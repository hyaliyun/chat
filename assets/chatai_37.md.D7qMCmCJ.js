import{_ as p,o as a,c as n,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as k}from"./chunks/framework.B1z0IdBH.js";const x={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(s,e,l,m,o,i){return a(),n("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(x,[["render",S],["__scopeId","data-v-e75c6a03"]]),I=JSON.parse('[{"question":"# Advanced Python Coding Task: Implementing a Custom Protocol and Transport Objective Demonstrate your understanding of the asyncio library\'s transports and protocols by creating a custom UDP protocol that manages a basic echo service and tracks some connection statistics. Problem Description You are required to implement a UDP echo server that: 1. **Echos Received Messages**: When the server receives a message, it should send back the same message to the sender. 2. **Tracks Connection Statistics**: - Total number of messages received. - Total bytes received. - Number of unique clients connected. Create a UDP echo server using asyncio\'s datagram API. The server should listen on IP `127.0.0.1` and port `9999`. Requirements 1. Implement a custom protocol class `EchoServerProtocol`. 2. The `EchoServerProtocol` should override the following methods from `asyncio.DatagramProtocol`: - `connection_made(transport)`: to set up the transport. - `datagram_received(data, addr)`: to handle incoming data. - `connection_lost(exc)`: to finalize the transport when the connection is lost. 3. Define a method `get_stats()` within `EchoServerProtocol` that returns the current statistics: total messages received, total bytes received, and number of unique clients. 4. Create a coroutine `start_server()` that sets up the server to listen on the specified IP and port. Expected Input and Output - The protocol does not expect any input from the user during its initialization. - The server will handle incoming UDP datagrams automatically. - The method `get_stats()` should output a dictionary with the following structure: ```python { \\"messages_received\\": <int>, \\"bytes_received\\": <int>, \\"unique_clients\\": <int> } ``` Constraints - Use only standard library modules. - Ensure thread safety where applicable. - Handle potential exceptions or errors in your implementation gracefully. Example Here is a brief example to demonstrate the usage of the server: ```python import asyncio class EchoServerProtocol(asyncio.DatagramProtocol): def __init__(self): self.transport = None self.messages_received = 0 self.bytes_received = 0 self.unique_clients = set() def connection_made(self, transport): self.transport = transport def datagram_received(self, data, addr): message = data.decode() print(f\\"Received {message} from {addr}\\") self.messages_received += 1 self.bytes_received += len(data) self.unique_clients.add(addr) self.transport.sendto(data, addr) # Echo the message back def connection_lost(self, exc): print(\\"Connection lost\\") def get_stats(self): return { \\"messages_received\\": self.messages_received, \\"bytes_received\\": self.bytes_received, \\"unique_clients\\": len(self.unique_clients) } async def start_server(): loop = asyncio.get_running_loop() transport, protocol = await loop.create_datagram_endpoint( lambda: EchoServerProtocol(), local_addr=(\'127.0.0.1\', 9999)) # Serve for a specified duration or forever try: await asyncio.sleep(3600) # Adjust the duration as required finally: transport.close() # Example usage asyncio.run(start_server()) ``` Performance Requirements - The server should handle multiple clients efficiently. - Ensure minimal latency in echoing messages back to the clients. Good luck, and happy coding!","solution":"import asyncio class EchoServerProtocol(asyncio.DatagramProtocol): def __init__(self): self.transport = None self.messages_received = 0 self.bytes_received = 0 self.unique_clients = set() def connection_made(self, transport): self.transport = transport def datagram_received(self, data, addr): self.messages_received += 1 self.bytes_received += len(data) self.unique_clients.add(addr) self.transport.sendto(data, addr) # Echo the message back def connection_lost(self, exc): print(\\"Connection closed\\") def get_stats(self): return { \\"messages_received\\": self.messages_received, \\"bytes_received\\": self.bytes_received, \\"unique_clients\\": len(self.unique_clients) } async def start_server(): loop = asyncio.get_running_loop() transport, protocol = await loop.create_datagram_endpoint( lambda: EchoServerProtocol(), local_addr=(\'127.0.0.1\', 9999) ) return transport, protocol # To run the server, use asyncio.run(start_server())"},{"question":"# Python Coding Assessment Problem Statement You are tasked with customizing the representation of large lists such that their output size is limited. Specifically, you need to subclass the `Repr` class from the `reprlib` module to achieve this. Your subclass should implement a method that handles `list` objects, limiting the number of list elements displayed to a maximum of 10 and representing nested lists correctly with the same constraints. Requirements 1. Your custom class should be named `CustomRepr`. 2. Implement a method `repr_list(obj, level)` within `CustomRepr` to handle lists. 3. The method should limit the display of list elements to at most 10 elements. 4. For nested lists, apply the same constraints recursively. 5. You need to modify the `CustomRepr` class only for handling list objects. Input - A list of integers or nested lists of integers. Output - A string representation of the list conforming to the specified constraints. Constraints - List elements can be either integers or other lists (nested 2 levels deep). Example ```python from reprlib import Repr # Here, CustomRepr is your subclass class CustomRepr(Repr): def __init__(self): super().__init__() self.maxlist = 10 def repr_list(self, obj, level): if level <= 0: # Base case for recursion return \'...\' # Limiting number of list elements new_list = obj[:self.maxlist] represented_elements = [self.repr1(item, level - 1) for item in new_list] # Handle case when list was longer than maxlist if len(obj) > self.maxlist: represented_elements.append(\'...\') return \'[\' + \', \'.join(represented_elements) + \']\' def main(): aRepr = CustomRepr() test_input = list(range(20)) # List with 20 elements nested_input = [list(range(5)), list(range(12)), list(range(3))] print(aRepr.repr(test_input)) # Expected: \'[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...]\' print(aRepr.repr(nested_input)) # Expected: \'[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...], [0, 1, 2]]\' if __name__ == \\"__main__\\": main() ``` Your task is to implement the `CustomRepr` class and complete the `repr_list` method to meet the specified requirements. **Note**: Use the `reprlib` module and the `Repr` class as a foundation for your implementation.","solution":"from reprlib import Repr class CustomRepr(Repr): def __init__(self): super().__init__() self.maxlist = 10 def repr_list(self, obj, level): if level <= 0: # Base case for recursion return \'...\' # Limiting number of list elements new_list = obj[:self.maxlist] represented_elements = [self.repr1(item, level - 1) for item in new_list] # Handle case when list was longer than maxlist if len(obj) > self.maxlist: represented_elements.append(\'...\') return \'[\' + \', \'.join(represented_elements) + \']\'"},{"question":"# Question: Implement a Secure File Integrity Verifier **Objective:** You need to implement a Python program that verifies the integrity of files using SHA256 and BLAKE2b hash algorithms. The program will generate and verify hash values to ensure that the files have not been tampered with. **Details:** 1. **Function 1: `generate_file_hashes`** - **Input:** - `file_path` (string): The path to the file to be hashed. - **Output:** - A dictionary with two keys: `sha256` and `blake2b`, each containing the hexadecimal digest of the file. - **Constraints:** - The file should be read in chunks to handle large files without consuming excessive memory. 2. **Function 2: `verify_file_hashes`** - **Input:** - `file_path` (string): The path to the file to be verified. - `expected_hashes` (dictionary): A dictionary with two keys: `sha256` and `blake2b`, each containing the expected hexadecimal digest string of the file. - **Output:** - Boolean: `True` if both hashes match the expected values, otherwise `False`. - **Constraints:** - The file should be read in chunks to handle large files without consuming excessive memory. **Example Usage:** ```python # Example file path file_path = \'example.txt\' # Generate hashes for the file hashes = generate_file_hashes(file_path) print(hashes) # Output might look like: # {\'sha256\': \'a5b8c...d7f\', \'blake2b\': \'6ff84...1a3\'} # Verify the file integrity is_valid = verify_file_hashes(file_path, hashes) print(is_valid) # True # Tamper with the file and check again with open(file_path, \'a\') as f: f.write(\'extra data\') is_valid = verify_file_hashes(file_path, hashes) print(is_valid) # False ``` **Note:** - Ensure you handle file I/O errors gracefully. - Make sure to use efficient mechanisms to read and hash large files in chunks. - You can use `hashlib`\'s `sha256()` and `blake2b()` functions as demonstrated in the documentation.","solution":"import hashlib def generate_file_hashes(file_path): Generate SHA256 and BLAKE2b hashes for the given file. :param file_path: The path to the file to be hashed :return: A dictionary with \'sha256\' and \'blake2b\' keys containing the hexadecimal hash digests sha256_hash = hashlib.sha256() blake2b_hash = hashlib.blake2b() try: with open(file_path, \'rb\') as file: while chunk := file.read(8192): sha256_hash.update(chunk) blake2b_hash.update(chunk) return { \'sha256\': sha256_hash.hexdigest(), \'blake2b\': blake2b_hash.hexdigest() } except IOError as e: print(f\\"Error reading file {file_path}: {e}\\") return None def verify_file_hashes(file_path, expected_hashes): Verify the integrity of a file by comparing its hashes to expected values. :param file_path: The path to the file to be verified :param expected_hashes: A dictionary with \'sha256\' and \'blake2b\' keys containing the expected hexadecimal hash digests :return: True if both hashes match the expected values, otherwise False generated_hashes = generate_file_hashes(file_path) if not generated_hashes: return False return (generated_hashes[\'sha256\'] == expected_hashes[\'sha256\'] and generated_hashes[\'blake2b\'] == expected_hashes[\'blake2b\'])"},{"question":"# Task You are required to implement an XML-RPC server using the `xmlrpc.server` module that performs the following: 1. Create an instance of `SimpleXMLRPCServer`. 2. Register several built-in Python functions such as `pow`, `abs`, and `max`. 3. Implement and register a custom function named `concatenate_strings` that takes two string arguments and returns their concatenation. 4. Register a class named `MathOperations` with methods: - `add`: Returns the sum of two numbers. - `subtract`: Returns the difference between two numbers. 5. Set up the server to listen on localhost at port `8080`. Additionally, include the client code that demonstrates how to call the registered functions using the XML-RPC protocol. # Requirements 1. **Server Implementation**: - Use `SimpleXMLRPCServer` to create the server. - Register the functions and class as described. 2. **Client Implementation**: - Use `xmlrpc.client.ServerProxy` to interact with the server. - Demonstrate calling each registered function and method. # Input/Output - **Server Side**: - Implement functions and class methods to be called via XML-RPC. - Start the server to handle XML-RPC requests. - **Client Side**: - Connect to the server and invoke the registered functions and methods. - Print the results of the calls to demonstrate proper functionality. # Constraints - Ensure the server handles multiple client requests concurrently. - Utilize necessary encryption or validation mechanisms to prevent malicious attacks as the server is exposed over a network. # Performance Requirements - The server should be capable of handling up to 100 concurrent requests. # Example Server Code Example ```python from xmlrpc.server import SimpleXMLRPCServer # Custom function def concatenate_strings(s1, s2): return s1 + s2 # Class for additional operations class MathOperations: def add(self, x, y): return x + y def subtract(self, x, y): return x - y # Setting up the server with SimpleXMLRPCServer((\'localhost\', 8080)) as server: server.register_function(pow) server.register_function(abs) server.register_function(max) server.register_function(concatenate_strings) server.register_instance(MathOperations()) print(\\"Serving XML-RPC on localhost port 8080\\") server.serve_forever() ``` Client Code Example ```python import xmlrpc.client # Connect to the server proxy = xmlrpc.client.ServerProxy(\'http://localhost:8080\') # Call the registered functions and methods print(proxy.pow(2, 3)) # Output: 8 print(proxy.abs(-10)) # Output: 10 print(proxy.max(1, 5, 3)) # Output: 5 print(proxy.concatenate_strings(\\"Hello, \\", \\"world!\\")) # Output: \\"Hello, world!\\" print(proxy.add(10, 5)) # Output: 15 print(proxy.subtract(10, 5)) # Output: 5 ``` Ensure to handle any exceptions and edge cases where input might be invalid or connections might fail.","solution":"from xmlrpc.server import SimpleXMLRPCServer import threading # Custom function def concatenate_strings(s1, s2): return s1 + s2 # Class for additional operations class MathOperations: def add(self, x, y): return x + y def subtract(self, x, y): return x - y def run_server(): # Setting up the server with SimpleXMLRPCServer((\'localhost\', 8080)) as server: server.register_function(pow) server.register_function(abs) server.register_function(max) server.register_function(concatenate_strings) server.register_instance(MathOperations()) print(\\"Serving XML-RPC on localhost port 8080\\") server.serve_forever() # Run server in a separate thread to allow client interaction for testing server_thread = threading.Thread(target=run_server, daemon=True) server_thread.start()"},{"question":"**Topic: Managing Multiple Workers with PyTorch Multiprocessing** # Background PyTorch provides a module, `torch.distributed.elastic.multiprocessing`, which facilitates starting and managing multiple worker processes. This feature is particularly useful in distributed training and parallel computation scenarios, where tasks need to be executed concurrently by different processes. # Task Write a Python function using PyTorch\'s `torch.distributed.elastic.multiprocessing` that launches multiple worker processes to compute the square of numbers from a given list. Each process should handle a portion of the list and return its results. The function should then collect results from all processes and return a combined result. # Function Signature ```python from typing import List import torch.distributed.elastic.multiprocessing as em def compute_squares(numbers: List[int], num_workers: int) -> List[int]: Spawns multiple worker processes to compute the square of each number in the input list. Args: numbers (List[int]): List of integers to be squared. num_workers (int): Number of worker processes to spawn. Returns: List[int]: List containing the squares of the input numbers. pass ``` # Requirements - The function must distribute the work evenly among the specified number of worker processes. - Each worker process should receive a subset of the input list and compute the squares of its assigned numbers. - The main process should collect and combine the results from all worker processes. - Ensure that the implementation uses `torch.distributed.elastic.multiprocessing`. # Input Constraints - `numbers` list will contain between 1 and 1,000,000 integers. - Each integer in `numbers` will be between -10,000 and 10,000. - `num_workers` will be between 1 and 16. # Example ```python numbers = [1, 2, 3, 4, 5] num_workers = 2 assert compute_squares(numbers, num_workers) == [1, 4, 9, 16, 25] ``` # Notes - You may define additional helper functions if needed. - Handle any exceptions or edge cases appropriately. - Ensure your solution is efficient and scalable.","solution":"from typing import List, Tuple import torch.multiprocessing as mp def worker_process(numbers: List[int], start_idx: int, end_idx: int, return_dict: dict, worker_id: int): Worker process function to compute the square of each number in the sublist [start_idx:end_idx]. Args: numbers (List[int]): List of integers. start_idx (int): Starting index (inclusive) for the current worker\'s subset of numbers. end_idx (int): Ending index (exclusive) for the current worker\'s subset of numbers. return_dict (dict): Dictionary to store the results from each worker. worker_id (int): Unique identifier for the worker process. results = [num ** 2 for num in numbers[start_idx:end_idx]] return_dict[worker_id] = results def compute_squares(numbers: List[int], num_workers: int) -> List[int]: Spawns multiple worker processes to compute the square of each number in the input list. Args: numbers (List[int]): List of integers to be squared. num_workers (int): Number of worker processes to spawn. Returns: List[int]: List containing the squares of the input numbers. if num_workers < 1: raise ValueError(\\"The number of workers should be at least 1.\\") manager = mp.Manager() return_dict = manager.dict() processes = [] # Determine the size of each chunk chunk_size = (len(numbers) + num_workers - 1) // num_workers # Ceiling division for worker_id in range(num_workers): start_idx = worker_id * chunk_size end_idx = min(start_idx + chunk_size, len(numbers)) if start_idx >= len(numbers): break p = mp.Process(target=worker_process, args=(numbers, start_idx, end_idx, return_dict, worker_id)) processes.append(p) p.start() for p in processes: p.join() # Combine results from all workers result = [] for worker_id in range(num_workers): if worker_id in return_dict: result.extend(return_dict[worker_id]) return result"},{"question":"# Advanced Coding Assessment Question **Objective**: Implement a dynamic class creation function that utilizes the `types` module capabilities. Your function should accept class attributes and methods at runtime and create a new class type accordingly. **Problem Statement**: Write a function `create_dynamic_class(class_name, bases, attributes, methods)` that dynamically creates a new class with the specified attributes and methods. - `class_name`: A string representing the name of the new class. - `bases`: A tuple of base classes from which the new class will inherit. - `attributes`: A dictionary where keys are attribute names and values are their respective default values. - `methods`: A dictionary where keys are method names and values are their respective function definitions. The function should return the newly created class. **Function Signature**: ```python def create_dynamic_class(class_name: str, bases: tuple, attributes: dict, methods: dict) -> type: pass ``` **Constraints**: 1. You must utilize the `types.new_class` function to create the class dynamically. 2. The newly created class should be able to access and utilize its attributes and methods just like a statically defined class. 3. Ensure that the class respects the given base classes and correctly inherits from them. **Input Example**: ```python class_name = \\"DynamicClass\\" bases = (object,) attributes = {\\"attr1\\": 1, \\"attr2\\": \\"Hello\\"} def method1(self): return self.attr1 * 3 def method2(self): return self.attr2.lower() methods = {\\"method1\\": method1, \\"method2\\": method2} ``` **Expected Output**: An instance of the created dynamic class should have the specified attributes and methods functioning as expected: ```python DynamicClass = create_dynamic_class(class_name, bases, attributes, methods) dc_instance = DynamicClass() print(dc_instance.attr1) # Output: 1 print(dc_instance.attr2) # Output: \\"Hello\\" print(dc_instance.method1()) # Output: 3 (since attr1 * 3 = 1 * 3) print(dc_instance.method2()) # Output: \\"hello\\" (since \\"Hello\\".lower() = \\"hello\\") ``` **Implementation Details**: - Use `types.new_class` to create the class dynamically. - Use `exec_body` to add the given attributes and methods to the class namespace. **Hint**: Refer to the `types.new_class` definition in the documentation for details on how to properly use the function.","solution":"import types def create_dynamic_class(class_name, bases, attributes, methods): Dynamically creates a new class with the specified attributes and methods. :param class_name: A string representing the name of the new class. :param bases: A tuple of base classes from which the new class will inherit. :param attributes: A dictionary where keys are attribute names and values are their respective default values. :param methods: A dictionary where keys are method names and values are their respective function definitions. :return: The newly created class. def exec_body(ns): ns.update(attributes) ns.update(methods) return types.new_class(class_name, bases, exec_body=exec_body)"},{"question":"**Question**: Implementing and Exporting a PyTorch Model with Dynamic Shapes You are given a PyTorch model that needs to be traced and exported using the `torch.export` module. The model includes dynamic batch sizes and computations that depend on these dynamic input shapes. You are tasked with writing Python code that performs the following: 1. Define a PyTorch module class `DynamicModel` which includes the following: - A linear layer followed by a ReLU activation. - A method `forward` that takes two inputs: `x` (input tensor) and `y` (another tensor) and performs simple operations on them involving dynamic shapes. 2. Use `torch.export.Dim` to specify dynamic shapes for the inputs of this model. 3. Export the model using the `torch.export.export` function, specifying the example inputs and the dynamic shapes, and save the exported program to a file. 4. Load the saved exported program, and print the graph to verify its correctness. **Constraints**: - You must handle the dynamic batch size for both inputs `x` and `y`. - The model operations should include at least one addition and one multiplication. **Input**: No explicit input, but the code should include specified functionalities. **Output**: The console output showing the exported program\'s graph structure. ```python import torch from torch.export import Dim, export, save, load # Step 1: Define the DynamicModel class class DynamicModel(torch.nn.Module): def __init__(self): super(DynamicModel, self).__init__() self.linear = torch.nn.Linear(10, 5) self.relu = torch.nn.ReLU() def forward(self, x, y): out1 = self.linear(x) out1_relu = self.relu(out1) out2 = out1_relu + y # Example operation out3 = out2 * 2 # Example operation return out3 # Step 2: Create dynamic dimensions for both inputs batch_dim = Dim(\\"batch\\") # Define example inputs with these dimensions example_args = (torch.randn(32, 10), torch.randn(32, 5)) dynamic_shapes = { \\"x\\": {0: batch_dim}, \\"y\\": {0: batch_dim} } # Step 3: Export the model and specify the dynamic shapes model = DynamicModel() exported_program = export(model, args=example_args, dynamic_shapes=dynamic_shapes) # Save the exported program save_path = \'dynamic_model_export.pt2\' save(exported_program, save_path) # Step 4: Load the saved program and print the graph loaded_exported_program = load(save_path) print(loaded_exported_program) ``` **Explanation**: 1. We define `DynamicModel` class which includes a linear layer followed by ReLU activation and operations involving dynamic tensor inputs. 2. We create dynamic dimensions for the batch size using `torch.export.Dim`. 3. We export the model while specifying dynamic shapes for the inputs to ensure the exported graph can handle varying batch sizes. 4. We save the exported program and load it back to verify the graph structure.","solution":"import torch from torch.export import Dim, export, save, load # Step 1: Define the DynamicModel class class DynamicModel(torch.nn.Module): def __init__(self): super(DynamicModel, self).__init__() self.linear = torch.nn.Linear(10, 5) self.relu = torch.nn.ReLU() def forward(self, x, y): out1 = self.linear(x) out1_relu = self.relu(out1) out2 = out1_relu + y # Example operation out3 = out2 * 2 # Example operation return out3 # Step 2: Create dynamic dimensions for both inputs batch_dim = Dim(\\"batch\\") # Define example inputs with these dimensions example_args = (torch.randn(32, 10), torch.randn(32, 5)) dynamic_shapes = { \\"x\\": {0: batch_dim}, \\"y\\": {0: batch_dim} } # Step 3: Export the model and specify the dynamic shapes model = DynamicModel() exported_program = export(model, args=example_args, dynamic_shapes=dynamic_shapes) # Save the exported program save_path = \'dynamic_model_export.pt2\' save(exported_program, save_path) # Step 4: Load the saved program and print the graph loaded_exported_program = load(save_path) print(loaded_exported_program.graph)"},{"question":"# Coding Assessment Task The objective of this task is to assess your understanding of Python 3.10 built-in functions, data structures, and file handling. You will need to demonstrate proficiency in dealing with lists, dictionaries, file operations, and exceptions. # Problem Statement You are given a text file named `data.txt` which contains several lines of comma-separated values. Each line represents a record containing the following fields: - Name (string) - Age (integer) - Country (string) - Score (float) Your task is to write a function `process_data(filename)` that reads this file, processes the data, and performs the following: 1. **Data Reading**: Read the data from the file `data.txt`. 2. **Data Processing**: - Group the records by `Country` in a dictionary where the key is the country name and the value is a list of tuples representing the records of people from that country. - Calculate the average score of all records. - Identify all unique countries in the data. 3. **Data Output**: - Print out the total number of records processed. - Print out the average score of the records. - Print the unique countries sorted in alphabetical order. - Write the grouped data to a new file named `processed_data.txt` in the following format: ``` Country: <Country Name> - Name: <Name>, Age: <Age>, Score: <Score> - ... ``` # Constraints - Assume the file `data.txt` is located in the same directory as your script. - Handle possible exceptions that may occur due to file operations. - The structure of `data.txt` is guaranteed; you do not need to handle cases of missing fields. - You should ensure your solution is efficient and handles large files gracefully. # Function Signature ```python def process_data(filename: str): pass ``` # Example Assume the `data.txt` file contains the following data: ``` Alice,30,USA,85.5 Bob,25,Canada,90.0 Charlie,35,USA,88.0 David,22,Canada,72.5 Eve,29,UK,95.5 ``` After running `process_data(\'data.txt\')`, the function should print: ``` Total records processed: 5 Average score: 86.3 Unique countries: [\'Canada\', \'UK\', \'USA\'] ``` And the `processed_data.txt` file should contain: ``` Country: Canada - Name: Bob, Age: 25, Score: 90.0 - Name: David, Age: 22, Score: 72.5 Country: UK - Name: Eve, Age: 29, Score: 95.5 Country: USA - Name: Alice, Age: 30, Score: 85.5 - Name: Charlie, Age: 35, Score: 88.0 ``` # Notes - You can use any built-in functions or libraries. - Make sure to include comments in your code to explain your steps. - Proper error handling should be demonstrated.","solution":"def process_data(filename: str): import os # Initialize the data structures to be used data = [] country_dict = {} unique_countries = set() total_score = 0 try: # Read from the given file with open(filename, \'r\') as file: for line in file: name, age, country, score = line.strip().split(\',\') # Convert data types to appropriate types age = int(age) score = float(score) # Append to the main data list data.append((name, age, country, score)) # Build the dictionary grouped by country if country not in country_dict: country_dict[country] = [] country_dict[country].append((name, age, score)) # Maintain a set of unique countries unique_countries.add(country) # Accumulate total score total_score += score # Calculate average score num_records = len(data) average_score = total_score / num_records if num_records > 0 else 0 # Print relevant information print(f\\"Total records processed: {num_records}\\") print(f\\"Average score: {average_score:.1f}\\") print(f\\"Unique countries: {sorted(list(unique_countries))}\\") # Write grouped data to a new file with open(\'processed_data.txt\', \'w\') as file: for country in sorted(country_dict.keys()): file.write(f\\"Country: {country}n\\") for record in country_dict[country]: file.write(f\\"- Name: {record[0]}, Age: {record[1]}, Score: {record[2]}n\\") file.write(\\"n\\") except FileNotFoundError: print(f\\"File {filename} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Python Coding Assessment In this task, you are required to implement a Python function that utilizes Pythonâ€™s frame and evaluation functions to carry out some inspection on the runtime environment. You will leverage the functions provided by the `python310` package. Python\'s dynamic nature will allow you to utilize these functions for introspection. Problem Statement Write a Python function `inspect_environment()` which performs the following tasks: 1. Retrieves the dictionary of global variables using `PyEval_GetGlobals()`. 2. Retrieves the dictionary of local variables using `PyEval_GetLocals()`. 3. Retrieves the filename and line number currently executing using `PyFrame_GetCode()` and `PyFrame_GetLineNumber()` respectively. 4. Creates an outer frame using `PyFrame_GetBack()`, and fetches global and local variables from that frame if any. 5. Returns a dictionary with the following structure. ```python { \\"current_globals\\": <dictionary of global variables>, \\"current_locals\\": <dictionary of local variables>, \\"filename\\": <current filename>, \\"line_number\\": <current line number>, \\"outer_globals\\": <dictionary of global variables from outer frame (or None if not applicable)>, \\"outer_locals\\": <dictionary of local variables from outer frame (or None if not applicable)> } ``` # Constraints - You are required to handle cases where there might not be an outer frame. - You must ensure the function does not raise exceptions. Expected Input and Output The function does not take any input. It directly inspects the running environment. Performance Requirements - The function should execute efficiently and avoid any unnecessary overhead from repeatedly calling introspection functions. - Ensure that your solution can handle recursive function calls if they occur. Example Execution ```python def inspect_environment(): # Implementation here # Example Usage result = inspect_environment() print(result) ``` The printed `result` should display a dictionary structured as described above. Additional Notes You may assume that the `python310` module is available and correctly installed in the environment where this function will be executed.","solution":"import sys def inspect_environment(): Inspects the current execution environment and returns specific details about global and local variables, filename, and line numbers. env_info = { \\"current_globals\\": None, \\"current_locals\\": None, \\"filename\\": None, \\"line_number\\": None, \\"outer_globals\\": None, \\"outer_locals\\": None, } try: frame = sys._getframe() env_info[\\"current_globals\\"] = frame.f_globals env_info[\\"current_locals\\"] = frame.f_locals env_info[\\"filename\\"] = frame.f_code.co_filename env_info[\\"line_number\\"] = frame.f_lineno outer_frame = frame.f_back if outer_frame: env_info[\\"outer_globals\\"] = outer_frame.f_globals env_info[\\"outer_locals\\"] = outer_frame.f_locals except Exception as e: pass return env_info"},{"question":"You are required to create a custom Python function that simulates the behavior of Python function objects, including the ability to manage code objects, global variables, default arguments, closures, and annotations. Implement a class called `CustomFunction` that mimics standard Python function objects. The class should have the following attributes and methods: **Attributes:** 1. `code`: The code object to be executed by the function. 2. `globals`: A dictionary of global variables accessible to the function. 3. `defaults`: A tuple of default argument values for the function. 4. `closure`: A tuple of closure variables for the function. 5. `annotations`: A dictionary of annotations for the function. 6. `qualname`: The qualified name of the function. **Methods:** 1. `__init__(self, code, globals_, defaults=None, closure=None, annotations=None, qualname=None)`: Initialize the function object with the given attributes. 2. `get_code(self)`: Return the code object associated with the function. 3. `get_globals(self)`: Return the globals dictionary associated with the function. 4. `get_defaults(self)`: Return the default argument values. 5. `set_defaults(self, defaults)`: Set the default argument values. 6. `get_closure(self)`: Return the closure associated with the function. 7. `set_closure(self, closure)`: Set the closure for the function. 8. `get_annotations(self)`: Return the annotations of the function. 9. `set_annotations(self, annotations)`: Set annotations for the function. 10. `get_qualname(self)`: Return the qualified name of the function. The class should demonstrate these capabilities through appropriate exception handling and should ensure that attributes like defaults, closure, and annotations can be either `None` or the specified types (tuple for defaults and closure, dictionary for annotations). **Example Usage:** ```python def sample_code(x, y): return x + y globals_dict = {\\"__name__\\": \\"__main__\\"} defaults = (1,) closure = (lambda: 2,) annotations = {\\"x\\": int, \\"y\\": int, \\"return\\": int} qualname = \\"custom_sample_code\\" cf = CustomFunction(sample_code, globals_dict, defaults, closure, annotations, qualname) print(cf.get_code()) # <code object sample_code at 0x...> print(cf.get_globals()) # {\'__name__\': \'__main__\'} print(cf.get_defaults()) # (1,) print(cf.get_closure()) # (<function <lambda> at 0x...>,) print(cf.get_annotations()) # {\'x\': int, \'y\': int, \'return\': int} print(cf.get_qualname()) # custom_sample_code ``` # Constraints: 1. `code` must be a callable. 2. `globals` must be a dictionary. 3. `defaults` must be `None` or a tuple. 4. `closure` must be `None` or a tuple of callables. 5. `annotations` must be `None` or a dictionary. 6. `qualname` must be `None` or a string. # Note: - Your implementation should carefully handle edge cases and ensure that methods raise appropriate exceptions when invalid types are given.","solution":"class CustomFunction: def __init__(self, code, globals_, defaults=None, closure=None, annotations=None, qualname=None): if not callable(code): raise TypeError(\\"code must be a callable\\") if not isinstance(globals_, dict): raise TypeError(\\"globals must be a dictionary\\") if defaults is not None and not isinstance(defaults, tuple): raise TypeError(\\"defaults must be a tuple\\") if closure is not None and not all(callable(c) for c in closure): raise TypeError(\\"all elements in closure must be callables\\") if annotations is not None and not isinstance(annotations, dict): raise TypeError(\\"annotations must be a dictionary\\") if qualname is not None and not isinstance(qualname, str): raise TypeError(\\"qualname must be a string\\") self._code = code self._globals = globals_ self._defaults = defaults self._closure = closure self._annotations = annotations self._qualname = qualname def get_code(self): return self._code def get_globals(self): return self._globals def get_defaults(self): return self._defaults def set_defaults(self, defaults): if defaults is not None and not isinstance(defaults, tuple): raise TypeError(\\"defaults must be a tuple\\") self._defaults = defaults def get_closure(self): return self._closure def set_closure(self, closures): if closures is not None and not all(callable(c) for c in closures): raise TypeError(\\"all elements in closure must be callables\\") self._closure = closures def get_annotations(self): return self._annotations def set_annotations(self, annotations): if annotations is not None and not isinstance(annotations, dict): raise TypeError(\\"annotations must be a dictionary\\") self._annotations = annotations def get_qualname(self): return self._qualname"},{"question":"**Custom Interactive Python Interpreter** **Objective:** In this exercise, you need to develop a custom Python interactive interpreter using the `code` module. Your custom interpreter should support a set of predefined commands in addition to standard Python code execution. **Task:** 1. Define a class `CustomInterpreter` that inherits from `code.InteractiveConsole`. 2. This interpreter should support the following custom commands: - `EXIT`: Exit the interpreter. - `HELP`: Display a help message listing the available commands. - `SUM num1 num2 ...`: Compute and display the sum of provided numbers. **Guidelines:** 1. Implement the `CustomInterpreter` class with the following methods: - `__init__(self)`: Initialize the interpreter with necessary preparations. - `runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\")`: This method from the `InteractiveConsole` class should be overridden to define custom command handling. - `sum_command(self, command)`: Handle the `SUM` command. 2. The class should start an interactive session with the `interact` method. **Constraints:** - Use only standard library imports. - Handle inputs gracefully and provide clear error messages for invalid commands. **Expected Input and Output Formats:** - The `EXIT` command should terminate the session. - The `HELP` command should display a message: `\\"Available commands: EXIT, HELP, SUM\\"` - The `SUM` command should take a space-separated list of numbers and print their sum. **Example Usage:** ```python >>> interpreter = CustomInterpreter() >>> interpreter.interact() Welcome to the Custom Python Interpreter! Type \\"HELP\\" for a list of available commands. >>> SUM 1 2 3.5 6.5 >>> HELP Available commands: EXIT, HELP, SUM >>> EXIT ``` You are provided with the following skeleton code to start with: ```python import code class CustomInterpreter(code.InteractiveConsole): def __init__(self): super().__init__() print(\'Welcome to the Custom Python Interpreter!\') print(\'Type \\"HELP\\" for a list of available commands.\') def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if source.strip() == \\"EXIT\\": print(\\"Exiting interpreter.\\") return True # This will stop the interpreter loop elif source.strip() == \\"HELP\\": print(\\"Available commands: EXIT, HELP, SUM\\") return False elif source.startswith(\\"SUM\\"): try: self.sum_command(source) except Exception as e: print(f\\"Error: {e}\\") return False else: return super().runsource(source, filename, symbol) def sum_command(self, command): parts = command.split() if len(parts) < 2: print(\\"Usage: SUM num1 num2 ...\\") return try: numbers = map(float, parts[1:]) total = sum(numbers) print(total) except ValueError: print(\\"Error: All arguments must be numbers.\\") ``` Complete the `CustomInterpreter` class so that it behaves as specified.","solution":"import code class CustomInterpreter(code.InteractiveConsole): def __init__(self): super().__init__() print(\'Welcome to the Custom Python Interpreter!\') print(\'Type \\"HELP\\" for a list of available commands.\') def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): command = source.strip().split()[0] if source.strip() else \\"\\" if command == \\"EXIT\\": print(\\"Exiting interpreter.\\") return True # This will stop the interpreter loop elif command == \\"HELP\\": print(\\"Available commands: EXIT, HELP, SUM\\") return False elif command == \\"SUM\\": try: self.sum_command(source.strip()) except Exception as e: print(f\\"Error: {e}\\") return False else: return super().runsource(source, filename, symbol) def sum_command(self, command): parts = command.split() if len(parts) < 2: print(\\"Usage: SUM num1 num2 ...\\") return try: numbers = map(float, parts[1:]) total = sum(numbers) print(total) except ValueError: print(\\"Error: All arguments must be numbers.\\")"},{"question":"**Question: Comprehensive Data Processing and File Manipulation** **Background:** You have been provided with a `.csv` file named `data.csv` which contains information about a collection of books. Each row in the file contains details about one book: its title, author, ISBN number, publication year, and genre. **Objective:** Write a Python function `process_books_file(input_file: str, output_file: str) -> None` that processes this CSV file and generates a summary text file with the following information: 1. The total number of books. 2. The oldest and newest book (based on the publication year). 3. The number of books per genre. 4. A list of all unique authors. 5. All the ISBN numbers that are valid per a simple checksum algorithm: - The ISBN must be exactly 10 digits. - Each digit is multiplied by its position (i.e., 1st digit by 1, 2nd digit by 2, ..., 10th digit by 10). - Summing these values, the result must be divisible by 11. The summary should be formatted and written to an output text file specified by `output_file`. **Input:** - `input_file`: A string representing the path to the input CSV file. - `output_file`: A string representing the path to the output summary text file. **Output:** - None. The result is written to the `output_file` in the specified format. **Constraints:** - The CSV file is guaranteed to be well-formed with the columns in the specified order. - Assume that there will be at least one book in the list. **Example:** Given an input file `data.csv` with the following contents: ``` title,author,isbn,year,genre The Great Gatsby,F. Scott Fitzgerald,978074327356,1925,Fiction To Kill a Mockingbird,Harper Lee,0061120081,1960,Fiction 1984,George Orwell,0451524934,1949,Dystopian Moby-Dick,Herman Melville,1503280780,1851,Fiction War and Peace,Leo Tolstoy,0199232768,1869,Historical ``` The function call `process_books_file(\'data.csv\', \'summary.txt\')` should create a file `summary.txt` with the content: ``` Total number of books: 5 Oldest book: Moby-Dick, 1851 Newest book: To Kill a Mockingbird, 1960 Books per genre: - Fiction: 3 - Dystopian: 1 - Historical: 1 Authors: F. Scott Fitzgerald, Harper Lee, George Orwell, Herman Melville, Leo Tolstoy Valid ISBN numbers: - 0061120081 - 0451524934 ``` **Hints:** - Utilize the `csv` module for reading the CSV file. - Use collections such as `Counter` to handle genre counts. - Consider using a set to gather unique authors. - Make use of error handling for robust code, especially with file operations.","solution":"import csv from collections import Counter def process_books_file(input_file: str, output_file: str) -> None: Processes a CSV file containing book information and generates a summary text file. books = [] genres = Counter() authors = set() valid_isbns = [] with open(input_file, \'r\') as f: reader = csv.DictReader(f) for row in reader: books.append(row) genres[row[\'genre\']] += 1 authors.add(row[\'author\']) if is_valid_isbn(row[\'isbn\']): valid_isbns.append(row[\'isbn\']) total_books = len(books) oldest_book = min(books, key=lambda b: int(b[\'year\'])) newest_book = max(books, key=lambda b: int(b[\'year\'])) with open(output_file, \'w\') as f: f.write(f\\"Total number of books: {total_books}n\\") f.write(f\\"Oldest book: {oldest_book[\'title\']}, {oldest_book[\'year\']}n\\") f.write(f\\"Newest book: {newest_book[\'title\']}, {newest_book[\'year\']}n\\") f.write(f\\"Books per genre:n\\") for genre, count in genres.items(): f.write(f\\" - {genre}: {count}n\\") f.write(f\\"Authors: {\', \'.join(sorted(authors))}n\\") f.write(f\\"Valid ISBN numbers:n\\") for isbn in valid_isbns: f.write(f\\" - {isbn}n\\") def is_valid_isbn(isbn: str) -> bool: Checks if an ISBN-10 is valid based on its checksum. if len(isbn) != 10: return False try: checksum = sum((i + 1) * int(digit) for i, digit in enumerate(isbn)) return checksum % 11 == 0 except ValueError: return False"},{"question":"**Event Scheduler Simulation** You are required to implement a function `simulate_event_scheduler` that uses the `sched` module to simulate a series of events. # The function should: 1. Schedule and run the following events, where each event prints a message: - Event A should run at an absolute time 5 seconds from now with a priority of 2. - Event B should run 3 seconds from now with a priority of 1. - Event C should run 10 seconds from now with a priority of 3. - Event D should run 7 seconds from now with a priority of 1. 2. Use the `sched.scheduler` class to schedule all events. 3. After scheduling, start the scheduler to ensure all events run at their respective times. 4. Each event should print its name and the current time using time.time(). # Expected Input and Output - There is no input to the function. You will use `time.time()` internally to determine when events should be scheduled. - The output should be printed statements from each event indicating its execution time. The order of execution must respect their scheduled times and priorities. # Constraints 1. You must use the `sched.scheduler` class for scheduling. 2. All time calculations should be done using the `time.time()` function. 3. Ensure proper handling of priorities and delays. # Example ```python import time import sched def simulate_event_scheduler(): # Scheduler instance scheduler = sched.scheduler(time.time, time.sleep) # Define event actions def event_action(event_name): print(f\\"Event {event_name} executed at {time.time()}\\") # Current time (for absolute scheduling) now = time.time() # Schedule events scheduler.enterabs(now + 5, 2, event_action, argument=(\'A\',)) scheduler.enter(3, 1, event_action, argument=(\'B\',)) scheduler.enter(10, 3, event_action, argument=(\'C\',)) scheduler.enter(7, 1, event_action, argument=(\'D\',)) # Run the scheduler scheduler.run() # Demonstration simulate_event_scheduler() ``` The output should be similar to: ``` Event B executed at <timestamp> Event A executed at <timestamp> Event D executed at <timestamp> Event C executed at <timestamp> ``` The exact timestamps will vary based on the current time when the function is executed but should match the delays specified.","solution":"import sched import time def simulate_event_scheduler(): # Scheduler instance scheduler = sched.scheduler(time.time, time.sleep) # Define event actions def event_action(event_name): print(f\\"Event {event_name} executed at {time.time()}\\") # Current time (for absolute scheduling) now = time.time() # Schedule events scheduler.enterabs(now + 5, 2, event_action, argument=(\'A\',)) scheduler.enter(3, 1, event_action, argument=(\'B\',)) scheduler.enter(10, 3, event_action, argument=(\'C\',)) scheduler.enter(7, 1, event_action, argument=(\'D\',)) # Run the scheduler scheduler.run()"},{"question":"You are given a dataset of user information with various data types including integers, floats, booleans, and strings. Your task is to write a function that: 1. Loads the dataset into a pandas DataFrame using PyArrow for efficient data handling. 2. Computes some aggregate statistics and performs a series of operations. 3. Returns the resulting DataFrame. Input - `data`: A string representing a CSV format data. The CSV will have columns `user_id`, `age`, `active`, `name`. Output - A pandas DataFrame with the following transformations: - Create a PyArrow-backed pandas DataFrame from the provided CSV. - Compute the mean age of active users (`active` column is `True`). - Replace any missing ages (`None` or empty cells) with the computed mean age using PyArrow functionality. - Add a new column `name_length` that contains the length of characters in the `name` field. Constraints - Ensure that the DataFrame uses PyArrow-backed data types wherever applicable for optimal performance. - Handle missing data appropriately using PyArrow features. Missing ages should be replaced with the active users\' mean age. Example ```python import pandas as pd def process_user_data(data: str) -> pd.DataFrame: # Your implementation here pass # Example usage: data = user_id,age,active,name 1,23,True,John 2,35,True,Jane 3,,False,Doe 4,45,True,Smith 5,,True,Emily result_df = process_user_data(data) print(result_df) ``` Expected output DataFrame: ``` user_id age active name name_length 0 1 23.0 True John 4 1 2 35.0 True Jane 4 2 3 34.3333 False Doe 3 3 4 45.0 True Smith 5 4 5 34.3333 True Emily 5 ``` Note: The mean age of active users (excluding missing ages) is `(23 + 35 + 45) / 3 = 34.33`. Missing ages are replaced accordingly.","solution":"import pandas as pd import pyarrow as pa import pyarrow.compute as pc from io import StringIO def process_user_data(data: str) -> pd.DataFrame: # Load the dataset into a PyArrow Table data_io = StringIO(data) df = pd.read_csv(data_io) # Convert pandas DataFrame to PyArrow Table arrow_table = pa.Table.from_pandas(df) # Compute mean age of active users active_age_column = arrow_table.column(\'age\').cast(pa.float64()) active_column = arrow_table.column(\'active\') active_user_ages = pc.if_else(active_column, active_age_column, None) mean_age = pc.mean(active_user_ages, skip_nulls=True).as_py() # Replace missing ages with the mean age filled_age_column = pc.if_else( active_age_column.is_null(), pa.scalar(mean_age, pa.float64()), active_age_column ) # Add updated age column to table updated_arrow_table = arrow_table.set_column( arrow_table.schema.get_field_index(\'age\'), \'age\', filled_age_column ) # Convert back to a pandas DataFrame result_df = updated_arrow_table.to_pandas() # Add name_length column result_df[\'name_length\'] = result_df[\'name\'].apply(len) return result_df"},{"question":"# Semi-Supervised Learning with Self-Training and Label Propagation Objective Implement and evaluate semi-supervised learning models using `SelfTrainingClassifier` and `LabelPropagation` from the `sklearn.semi_supervised` module. Description You will be provided with a toy dataset containing a mix of labeled and unlabeled data. Your task is to: 1. Implement a self-training model using `SelfTrainingClassifier` and evaluate its performance. 2. Implement a label propagation model using `LabelPropagation` and evaluate its performance. 3. Compare the results of the two models. Dataset You will use the `make_classification` function to generate a toy dataset with some labeled and some unlabeled data points. Requirements 1. **Self-Training Model** - Use `SelfTrainingClassifier` with a base classifier of your choice (e.g., `DecisionTreeClassifier`). - Set a `threshold` for self-training. - Train the model and evaluate its performance using accuracy on a test set. 2. **Label Propagation Model** - Use `LabelPropagation` with the default `rbf` kernel. - Train the model and evaluate its performance using accuracy on a test set. 3. **Comparison** - Output the accuracy of both models. - Provide a brief comparison of the two models based on their performance. Constraints - Use a small toy dataset to ensure the models run efficiently. - Assume a binary classification problem. Function Signature ```python from sklearn.datasets import make_classification from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import numpy as np def semi_supervised_learning(): # Step 1: Generate the toy dataset X, y = make_classification(n_samples=200, n_features=20, n_informative=15, n_classes=2, random_state=42) rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(y.shape[0]) < 0.3 y[random_unlabeled_points] = -1 # Split into train and test sets X_train, y_train = X[:160], y[:160] X_test, y_test = X[160:], y[160:] # Step 2: Implement Self-Training Classifier base_clf = DecisionTreeClassifier(random_state=42) self_training_clf = SelfTrainingClassifier(base_clf, threshold=0.75) self_training_clf.fit(X_train, y_train) y_pred_self_training = self_training_clf.predict(X_test) accuracy_self_training = accuracy_score(y_test, y_pred_self_training) # Step 3: Implement Label Propagation label_propagation = LabelPropagation() label_propagation.fit(X_train, y_train) y_pred_label_propagation = label_propagation.predict(X_test) accuracy_label_propagation = accuracy_score(y_test, y_pred_label_propagation) # Step 4: Output the results print(f\\"Self-Training Classifier Accuracy: {accuracy_self_training:.2f}\\") print(f\\"Label Propagation Accuracy: {accuracy_label_propagation:.2f}\\") print(\\"Comparison: \\", \\"Self-Training generally works well when the base classifier is strong and the threshold is appropriately set. \\" \\"Label Propagation relies heavily on the structure of the data and the choice of the kernel.\\") ``` **Note**: Ensure you install `scikit-learn` if you haven\'t, and import the necessary modules before running the code.","solution":"from sklearn.datasets import make_classification from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import numpy as np def semi_supervised_learning(): # Step 1: Generate the toy dataset X, y = make_classification(n_samples=200, n_features=20, n_informative=15, n_classes=2, random_state=42) rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(y.shape[0]) < 0.3 y[random_unlabeled_points] = -1 # Split into train and test sets X_train, y_train = X[:160], y[:160] X_test, y_test = X[160:], y[160:] # Step 2: Implement Self-Training Classifier base_clf = DecisionTreeClassifier(random_state=42) self_training_clf = SelfTrainingClassifier(base_clf, threshold=0.75) self_training_clf.fit(X_train, y_train) y_pred_self_training = self_training_clf.predict(X_test) accuracy_self_training = accuracy_score(y_test, y_pred_self_training) # Step 3: Implement Label Propagation label_propagation = LabelPropagation() label_propagation.fit(X_train, y_train) y_pred_label_propagation = label_propagation.predict(X_test) accuracy_label_propagation = accuracy_score(y_test, y_pred_label_propagation) # Step 4: Output the results results = { \\"Self-Training Classifier Accuracy\\": accuracy_self_training, \\"Label Propagation Accuracy\\": accuracy_label_propagation } return results"},{"question":"# Question: Implementing and Testing Accelerator Management Function In this exercise, you are required to implement a function that utilizes the torch.accelerator module to manage GPU devices. Your function should perform the following tasks: 1. **Check Availability**: Check if any GPU device is available. 2. **Count Devices**: Retrieve the number of available GPU devices. 3. **Select Device**: If there is more than one GPU, set the current device to the first GPU (index 0). 4. **Synchronize**: Ensure all operations on the current device have completed. 5. **Retrieve Current Device**: Get the index of the current active device and return it. Your function signature should be: `manage_accelerator() -> int` # Input - There are no inputs for this function. # Output - Returns the index of the current active GPU device (int). # Constraints - The function should work correctly even if there are no GPUs available. In such a case, you should return -1. # Performance Requirements - The function should efficiently handle the retrieval and setting of device indices, ensuring synchronization is appropriately managed without excessive overhead. # Example Usage ```python index = manage_accelerator() print(index) # This should print the index of the current active GPU or -1 if no GPU is available. ``` # Implementation Notes - Use `torch.accelerator.is_available` to check for GPU availability. - Use `torch.accelerator.device_count` to count the number of available GPUs. - Use `torch.accelerator.set_device_index` to set the current device to the specified index. - Use `torch.accelerator.current_device_index` to retrieve the current active device. - Use `torch.accelerator.synchronize` to ensure all operations are completed.","solution":"import torch def manage_accelerator() -> int: Manages the GPU devices using torch.accelerator. Returns: int: The index of the current active GPU device, or -1 if no GPU is available. # Check if any GPU device is available if not torch.cuda.is_available(): return -1 # Retrieve the number of available GPU devices device_count = torch.cuda.device_count() # If there is more than one GPU, set the current device to the first GPU (index 0) if device_count > 1: torch.cuda.set_device(0) # Ensure all operations on the current device have completed torch.cuda.synchronize() # Retrieve the index of the current active device current_device_index = torch.cuda.current_device() return current_device_index"},{"question":"Objective To assess your understanding of the `multiprocessing.shared_memory` module in Python, specifically how to create, manage, and use shared memory across multiple processes. Problem Statement Using the classes and methods from the `multiprocessing.shared_memory` module, implement a shared memory utility that allows two processes to perform operations on a shared list. You need to create a class `SharedListUtility` which performs the following tasks: 1. **Initialization**: - The constructor takes a list of integers to initialize a `ShareableList`. - If the list is not provided, it should attach to an existing shared memory list using the provided shared memory name. 2. **Methods**: - `increment_all()`: - This method increments each element of the shared list by a given value (default is 1). - `get_list()`: - This method returns a copy of the current state of the shared list. - `cleanup()`: - This method closes and unlinks the shared memory, and should be called at the end to ensure proper cleanup. 3. **Shared Memory Data Access**: - The shared list should be accessible and modifiable by multiple processes. - Demonstrate the utility by creating at least two processes where one modifies the shared list, and the other reads the modifications. Constraints - Only integers should be stored in the `ShareableList`. - Maximum length of the list is 1000. Example Usage ```python from multiprocessing import Process from shared_memory_utility import SharedListUtility def process_1(): # In a separate process, increment all elements by 5 utility = SharedListUtility(initial_list=[1, 2, 3, 4, 5]) utility.increment_all(5) print(\\"Process 1:\\", utility.get_list()) utility.cleanup() def process_2(name): # Attach to the shared memory block created by process_1 utility = SharedListUtility(shared_mem_name=name) print(\\"Process 2:\\", utility.get_list()) utility.cleanup() if __name__ == \\"__main__\\": # Start process 1 p1 = Process(target=process_1) p1.start() p1.join() # Assume the shared memory name from process 1 is \'psm_<unique_id>\' shared_mem_name = \'psm_<unique_id>\' # Replace with the actual name from process 1 # Start process 2 p2 = Process(target=process_2, args=(shared_mem_name,)) p2.start() p2.join() ``` Implementation Requirements 1. **SharedListUtility Class Implementation**: - Implement the `__init__`, `increment_all`, `get_list`, and `cleanup` methods. - Ensure proper handling of shared memory, particularly in the multi-process context, ensuring minimal interference and data corruption. 2. **Process Coordination**: - Ensure the processes demonstrate consistent and correct access to shared data. - Provide appropriate process synchronization if necessary. 3. **Documentation and Clean Code**: - Include docstrings for your class and methods. - Ensure the code is clean, properly formatted, and follows standard coding practices. Good luck!","solution":"import multiprocessing from multiprocessing import shared_memory from multiprocessing.shared_memory import ShareableList import time class SharedListUtility: def __init__(self, initial_list=None, shared_mem_name=None): if initial_list is not None: self.shared_list = ShareableList(initial_list) elif shared_mem_name is not None: self.shared_list = ShareableList(name=shared_mem_name) else: raise ValueError(\\"Either initial_list or shared_mem_name must be provided.\\") self.shared_mem_name = self.shared_list.shm.name def increment_all(self, increment_value=1): Increments each element of the shared list by the given value. for i in range(len(self.shared_list)): self.shared_list[i] += increment_value def get_list(self): Returns a copy of the current state of the shared list. return list(self.shared_list) def cleanup(self): Closes and unlinks the shared memory. self.shared_list.shm.close() self.shared_list.shm.unlink() # Example Usage def process_1(): # In a separate process, increment all elements by 5 utility = SharedListUtility(initial_list=[1, 2, 3, 4, 5]) utility.increment_all(5) print(\\"Process 1:\\", utility.get_list()) time.sleep(3) # Sleep to keep the shared memory alive utility.cleanup() def process_2(name): # Attach to the shared memory block created by process_1 utility = SharedListUtility(shared_mem_name=name) print(\\"Process 2:\\", utility.get_list()) utility.cleanup() if __name__ == \\"__main__\\": # Start process 1 p1 = multiprocessing.Process(target=process_1) p1.start() p1.join() # Assume the shared memory name from process 1 is available from above code # For demonstration purposes, retrieve the name from the process_1 block ulity = SharedListUtility(initial_list=[1, 2, 3, 4, 5]) psm_name = ulity.shared_mem_name del ulity # Cleanup the temporary utility instance # Start process 2 p2 = multiprocessing.Process(target=process_2, args=(psm_name,)) p2.start() p2.join()"},{"question":"**Coding Assessment Question** # Objective In this task, you are required to demonstrate your understanding of the Seaborn library, specifically the `rugplot` function. Your goal is to visualize the relationship between different variables in a new dataset and use the rug plot to enhance these visualizations. # Dataset You will use the `penguins` dataset available in the Seaborn library. This dataset contains measurements for penguins of three different species. # Task 1. Load the `penguins` dataset using Seaborn. 2. Create a scatter plot showing the relationship between `flipper_length_mm` and `body_mass_g`. 3. Enhance this scatter plot by adding a rug plot to both axes (similar to example 2 in the documentation). 4. Modify the rug plot to: - Represent the `species` as a third variable using hue mapping (similar to example 3). - Increase the height of the rug to 0.1 (similar to example 4). - Position the rug outside the axes with a height of -0.02 and `clip_on` set to `False` (similar to example 5). - Use thinner lines (`lw` set to 1) and apply alpha blending (`alpha` set to 0.5) to handle plot density (similar to example 6). Your final output should be a single plot that incorporates all the above requirements. # Input There is no input for this problem apart from loading the dataset. # Output A scatter plot with rug plots that meet the specified criteria. # Constraints - Use only the seaborn and matplotlib libraries for your visualization. - Ensure that your plot is well-labeled and visually informative. - Consider the performance of the code for large datasets. # Example Usage ```python import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a scatter plot with rug plots plt.figure(figsize=(10, 6)) sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") sns.rugplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", height=0.1, clip_on=False, lw=1, alpha=0.5) plt.show() ``` This plot should clearly show the relationship between flipper length and body mass of penguins, enhanced with rug plots that provide detailed insight into the distribution of data points along both axes.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_rug_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Handle any missing values penguins = penguins.dropna(subset=[\'flipper_length_mm\', \'body_mass_g\', \'species\']) # Create a scatter plot with rug plots plt.figure(figsize=(10, 6)) scatter = sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", edgecolor=\\"w\\", s=100) # Add rug plots sns.rugplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", height=0.1, clip_on=False, lw=1, alpha=0.5) sns.rugplot(data=penguins, y=\\"body_mass_g\\", hue=\\"species\\", height=0.1, clip_on=False, lw=1, alpha=0.5) # Add titles and labels plt.title(\'Scatter Plot of Flipper Length vs Body Mass with Rug Plots by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') # Display the plot plt.legend(title=\'Species\') plt.show()"},{"question":"# Question: Efficient Stream and Device Management in PyTorch You are required to implement a set of functions that demonstrate efficient management of streams, devices, and memory in PyTorch using the `torch.mtia` module. Functions to Implement 1. **initialize_device**: ```python def initialize_device(device_index: int) -> None: Initializes and sets the device to the one specified by device_index. Args: - device_index (int): The index of the device to set. Returns: - None ``` - Initializes and sets the device to the one specified by `device_index`. Ensure that the device is available and initialized. 2. **create_stream_and_event**: ```python def create_stream_and_event(device_index: int) -> Tuple[torch.mtia.Stream, torch.mtia.Event]: Creates a new stream and an event on the specified device. Args: - device_index (int): The index of the device on which to create the stream and event. Returns: - Tuple[torch.mtia.Stream, torch.mtia.Event]: The created stream and event. ``` - Creates a new stream and an event on the specified device. Return the stream and the associated event. 3. **record_and_synchronize_event**: ```python def record_and_synchronize_event(stream: torch.mtia.Stream, event: torch.mtia.Event) -> None: Records an event on the specified stream and synchronizes the stream. Args: - stream (torch.mtia.Stream): The stream on which to record the event. - event (torch.mtia.Event): The event to record and synchronize. Returns: - None ``` - Records an event on the specified stream and then synchronizes the stream to ensure all tasks are completed. 4. **memory_management_operations**: ```python def memory_management_operations() -> Dict[str, Any]: Performs and returns various memory management statistics. Returns: - Dict[str, Any]: A dictionary with keys \\"memory_stats\\", \\"cache_snapshot\\" and their corresponding values. ``` - Performs various memory management operations, including retrieving memory statistics and recording a snapshot of the current memory usage. Return these statistics in a dictionary with keys \\"memory_stats\\" and \\"cache_snapshot\\". Constraints and Limitations - You should check if the device is available and initialized before performing any operations on it. - Proper error handling should be implemented for scenarios where devices or streams are not available or initialized. Performance Requirements - Ensure the functions are optimized to minimize performance overhead, particularly in initializing devices and managing streams/events. Input and Output Formats - `initialize_device`: No return value, but should initialize the specified device. - `create_stream_and_event`: Returns a tuple containing stream and event objects. - `record_and_synchronize_event`: No return value, but the given event should be recorded and synchronized. - `memory_management_operations`: Returns a dictionary with keys \\"memory_stats\\" and \\"cache_snapshot\\". **Note**: Make sure to leverage the `torch.mtia` module correctly, following best practices for device and stream management.","solution":"import torch from typing import Tuple, Dict, Any def initialize_device(device_index: int) -> None: Initializes and sets the device to the one specified by device_index. Args: - device_index (int): The index of the device to set. Returns: - None if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available.\\") if device_index >= torch.cuda.device_count(): raise ValueError(f\\"Invalid device index: {device_index}. Available devices: 0-{torch.cuda.device_count() - 1}\\") torch.cuda.set_device(device_index) torch.cuda.init() def create_stream_and_event(device_index: int) -> Tuple[torch.cuda.Stream, torch.cuda.Event]: Creates a new stream and an event on the specified device. Args: - device_index (int): The index of the device on which to create the stream and event. Returns: - Tuple[torch.cuda.Stream, torch.cuda.Event]: The created stream and event. initialize_device(device_index) stream = torch.cuda.Stream(device=device_index) event = torch.cuda.Event(enable_timing=True) return stream, event def record_and_synchronize_event(stream: torch.cuda.Stream, event: torch.cuda.Event) -> None: Records an event on the specified stream and synchronizes the stream. Args: - stream (torch.cuda.Stream): The stream on which to record the event. - event (torch.cuda.Event): The event to record and synchronize. Returns: - None stream.record_event(event) stream.synchronize() def memory_management_operations() -> Dict[str, Any]: Performs and returns various memory management statistics. Returns: - Dict[str, Any]: A dictionary with keys \\"memory_stats\\", \\"cache_snapshot\\" and their corresponding values. memory_stats = torch.cuda.memory_stats() cache_snapshot = torch.cuda.memory_snapshot() return { \\"memory_stats\\": memory_stats, \\"cache_snapshot\\": cache_snapshot }"},{"question":"**Question: Analyzing Sales Data with Pandas** You are given a CSV file named `sales_data.csv` that contains sales data for a retail store. The CSV file has the following columns: - `Date`: The date of the sale (Format: YYYY-MM-DD). - `Store`: Store identification number. - `Product`: The product ID. - `Sales`: Number of units sold. - `Revenue`: Total revenue generated from sales of the product (in USD). Your task is to implement a function `analyze_sales_data(file_path: str) -> pd.DataFrame` that performs the following tasks: 1. **Load Data**: Read the CSV file into a pandas DataFrame. 2. **Data Cleaning**: - Check for any missing values in the DataFrame and fill them with appropriate default values. For numerical columns, fill missing values with `0`. For categorical columns (like `Store` and `Product`), fill missing values with the string `\'Unknown\'`. - Ensure the `Date` column is in datetime format. 3. **Total Sales per Month**: Create a new column `Month` that includes the month and year of each sale (formatted as `YYYY-MM`). Calculate the total sales for each month and return these monthly total sales as a DataFrame with columns `Month` and `TotalSales`. 4. **Top 5 Products**: Identify the top 5 products with the highest total revenue over the entire period. 5. **Monthly Sales Trend for a Product**: Create a function `plot_product_sales_trend(product_id: str)` that generates a line plot of monthly sales trends for a given product ID. The x-axis should represent the month, and the y-axis should represent the total units sold per month. **Input**: - `file_path` (str): The file path of the `sales_data.csv` file. **Output**: - A pandas DataFrame with columns `Month` and `TotalSales`, representing the total sales for each month. - The top 5 products with the highest total revenue should be printed. - Function `plot_product_sales_trend(product_id: str)` should generate and display a line plot when called. **Constraints**: - The size of the DataFrame should not exceed available memory. - Ensure the solution is efficient to handle files with up to 1 million rows. **Example**: ```python # Example usage: monthly_sales = analyze_sales_data(\'path/to/sales_data.csv\') print(monthly_sales) # Output: DataFrame with columns Month and TotalSales plot_product_sales_trend(\'P1234\') # Output: Line plot displaying the monthly sales trend for product P1234 ``` Please implement the above function ensuring all specified tasks are accomplished.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(file_path: str) -> pd.DataFrame: Load, clean, and analyze sales data from a CSV file. Args: - file_path (str): The file path of the sales_data.csv file. Returns: - pd.DataFrame: DataFrame with columns \'Month\' and \'TotalSales\'. # Load Data df = pd.read_csv(file_path) # Data Cleaning df.fillna({\'Store\': \'Unknown\', \'Product\': \'Unknown\', \'Sales\': 0, \'Revenue\': 0.0}, inplace=True) df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Total Sales per Month df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\').astype(str) monthly_sales = df.groupby(\'Month\')[\'Sales\'].sum().reset_index().rename(columns={\'Sales\': \'TotalSales\'}) # Top 5 Products top_products = df.groupby(\'Product\')[\'Revenue\'].sum().nlargest(5).reset_index() print(\\"Top 5 products with highest total revenue:\\") print(top_products) return monthly_sales def plot_product_sales_trend(file_path: str, product_id: str): Plots the monthly sales trend for a given product ID. Args: - file_path (str): The file path of the sales_data.csv file. - product_id (str): The product ID for which the trend needs to be plotted. # Load Data df = pd.read_csv(file_path) # Data Cleaning df.fillna({\'Store\': \'Unknown\', \'Product\': \'Unknown\', \'Sales\': 0, \'Revenue\': 0.0}, inplace=True) df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Filter for specific product product_df = df[df[\'Product\'] == product_id] # Total Sales per Month for the product product_df[\'Month\'] = product_df[\'Date\'].dt.to_period(\'M\').astype(str) product_sales = product_df.groupby(\'Month\')[\'Sales\'].sum().reset_index() # Plot plt.figure(figsize=(10, 6)) plt.plot(product_sales[\'Month\'], product_sales[\'Sales\'], marker=\'o\') plt.title(f\'Monthly Sales Trend for Product {product_id}\') plt.xlabel(\'Month\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.grid(True) plt.tight_layout() plt.show()"},{"question":"# Custom Event Loop Policy and Child Watcher Implementation Objective The goal of this question is to assess your understanding of asyncio event loop policies and process watchers in Python. You will need to create a custom event loop policy that modifies the default behavior and also handle child processes in a specific manner. Task 1. Create a custom event loop policy class, `MyCustomEventLoopPolicy`, that inherits from `asyncio.DefaultEventLoopPolicy`. 2. Override the `get_event_loop()` method to add some custom behavior (for example, logging whenever an event loop is retrieved). 3. Implement a custom child watcher class, `MyCustomChildWatcher`, that inherits from `asyncio.ThreadedChildWatcher`. 4. Override the `add_child_handler()` method to log a message whenever a child handler is added. 5. Configure the asyncio framework to use your custom event loop policy and child watcher. Input and Output - There is no direct input or output for this task; you need to implement the classes and their methods. - You can demonstrate the functionality through a sample usage of the asyncio event loop and child watcher. Constraints - Ensure that all overridden methods maintain the base class contract (i.e., not violating any expected behavior). - The custom logging should be minimal and non-intrusive. Performance - The performance impact of logging should be minimal. Example ```python import asyncio class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Custom Event Loop Retrieved\\") return loop class MyCustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): super().add_child_handler(pid, callback, *args) print(f\\"Custom handler added for PID: {pid}\\") # Setting the custom event loop policy asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) # Setting the custom child watcher custom_watcher = MyCustomChildWatcher() asyncio.get_event_loop_policy().get_child_watcher().attach_loop(asyncio.get_event_loop()) asyncio.set_child_watcher(custom_watcher) # Simple demonstration of an event loop and child watcher async def main(): await asyncio.sleep(1) print(\\"Async function executed\\") asyncio.run(main()) ``` Implement the two classes and demonstrate their usage as shown in the example above.","solution":"import asyncio class MyCustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Custom Event Loop Retrieved\\") return loop class MyCustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): super().add_child_handler(pid, callback, *args) print(f\\"Custom handler added for PID: {pid}\\") # Setting the custom event loop policy asyncio.set_event_loop_policy(MyCustomEventLoopPolicy()) # Setting the custom child watcher custom_watcher = MyCustomChildWatcher() asyncio.get_event_loop_policy().get_child_watcher().attach_loop(asyncio.get_event_loop()) asyncio.set_child_watcher(custom_watcher) # Simple demonstration of an event loop and child watcher async def main(): await asyncio.sleep(1) print(\\"Async function executed\\") # Uncomment the below line to run the main function for demonstration # asyncio.run(main())"},{"question":"# Pandas Copy-on-Write Assessment Objective: Demonstrate understanding of the Copy-on-Write mechanism in pandas, including how to handle DataFrame and Series objects effectively to avoid unintended side-effects. Problem Statement: You are provided with a pandas DataFrame `df` that represents employee information in a company. The DataFrame consists of columns: `EmployeeID`, `Name`, `Department`, and `Salary`. ```python df = pd.DataFrame({ \'EmployeeID\': [101, 102, 103, 104, 105], \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\'], \'Department\': [\'HR\', \'Engineering\', \'HR\', \'Marketing\', \'Engineering\'], \'Salary\': [50000, 80000, 60000, 70000, 90000] }) ``` Implement the following tasks, adhering to the Copy-on-Write principles to ensure no unintended modifications occur: 1. **Increase Salary in Department:** Write a function `increase_salary(df, department, amount)` that increases the salary of all employees in the specified `department` by a fixed `amount`. Ensure that the original DataFrame `df` is not modified directly. 2. **Promote Employee:** Write a function `promote_employee(df, employee_id, new_department, salary_increase)` that promotes an employee to a new department and adjusts their salary. Ensure that the original DataFrame `df` remains unchanged. 3. **Salary Statistics:** Write a function `salary_statistics(df)` that returns the mean, median, and standard deviation of the salaries in the `df` DataFrame. Make sure the function does not modify the original DataFrame. Input and Output Format: 1. **increase_salary**: - Input: `increase_salary(df: pd.DataFrame, department: str, amount: int) -> pd.DataFrame` - Output: A new DataFrame with updated salaries for the specified department. 2. **promote_employee**: - Input: `promote_employee(df: pd.DataFrame, employee_id: int, new_department: str, salary_increase: int) -> pd.DataFrame` - Output: A new DataFrame with the specified employee\'s department and salary updated. 3. **salary_statistics**: - Input: `salary_statistics(df: pd.DataFrame) -> Tuple[float, float, float]` - Output: A tuple containing the mean, median, and standard deviation of the salaries. Constraints: - The `df` DataFrame should not be modified in-place in any of the functions. - Use pandas functionalities to ensure CoW principles are respected. - You can assume valid inputs for department names and employee IDs that exist in the DataFrame. Example: ```python # Task 1 new_df = increase_salary(df, \'HR\', 5000) print(new_df) # Task 2 promoted_df = promote_employee(df, 103, \'Engineering\', 10000) print(promoted_df) # Task 3 mean, median, std_dev = salary_statistics(df) print(f\\"Mean: {mean}, Median: {median}, Standard Deviation: {std_dev}\\") ``` Evaluation Criteria: - Correct implementation of functions following CoW principles. - Proper handling of DataFrame modifications without altering the original DataFrame. - Efficient and clear code.","solution":"import pandas as pd import numpy as np from typing import Tuple def increase_salary(df: pd.DataFrame, department: str, amount: int) -> pd.DataFrame: Increase the salary of all employees in the specified department by a fixed amount. The original DataFrame is not modified. Parameters: df (pd.DataFrame): Original DataFrame containing employee information. department (str): The department whose employees\' salaries should be increased. amount (int): The amount by which to increase the salaries. Returns: pd.DataFrame: A new DataFrame with the updated salaries for the specified department. new_df = df.copy() new_df.loc[new_df[\'Department\'] == department, \'Salary\'] += amount return new_df def promote_employee(df: pd.DataFrame, employee_id: int, new_department: str, salary_increase: int) -> pd.DataFrame: Promotes an employee to a new department and adjusts their salary. The original DataFrame is not modified. Parameters: df (pd.DataFrame): Original DataFrame containing employee information. employee_id (int): ID of the employee to be promoted. new_department (str): The new department to which the employee is being promoted. salary_increase (int): The amount by which to increase the employee\'s salary. Returns: pd.DataFrame: A new DataFrame with the specified employee\'s department and salary updated. new_df = df.copy() new_df.loc[new_df[\'EmployeeID\'] == employee_id, \'Department\'] = new_department new_df.loc[new_df[\'EmployeeID\'] == employee_id, \'Salary\'] += salary_increase return new_df def salary_statistics(df: pd.DataFrame) -> Tuple[float, float, float]: Returns the mean, median, and standard deviation of the salaries in the DataFrame. The original DataFrame is not modified. Parameters: df (pd.DataFrame): Original DataFrame containing employee information. Returns: tuple: A tuple containing the mean, median, and standard deviation of the salaries. mean_salary = df[\'Salary\'].mean() median_salary = df[\'Salary\'].median() std_dev_salary = df[\'Salary\'].std() return mean_salary, median_salary, std_dev_salary"},{"question":"**Asyncio Coding Assessment Question** # Objective: Demonstrate your understanding of Python\'s asyncio package by implementing a small asynchronous TCP server that echoes messages back to connected clients. This task will require you to work with event loops, tasks, network connections, and basic error handling. # Task: Write an asyncio-based TCP server that performs the following operations: 1. **Create an event loop** using `asyncio.get_event_loop()`. 2. **Define a protocol class** for handling connections and data: - Implement the `connection_made()` method to handle new connections. - Implement the `data_received()` method to receive and echo back data from clients. - Implement the `connection_lost()` method to handle connection losses. 3. **Create the TCP server** using `loop.create_server()`, and listen on a specified host and port. 4. **Run the server indefinitely** until a keyboard interruption occurs (Ctrl+C). 5. **Handle exceptions gracefully**, ensuring the event loop is stopped and closed properly. # Requirements: - The server should listen on `127.0.0.1` (localhost) and port `8888`. - It should echo any data received from a client back to the same client. - It should handle multiple clients simultaneously. - The server should stop gracefully on a keyboard interruption and clean up resources. # Input and Output: - **Input:** No direct input handling is required apart from client connections. - **Output:** Echoed data back to clients. Print statements to indicate server activities. # Constraints: - You must use the `asyncio.get_event_loop()`, `loop.create_server()`, and associated transport/protocol methods. - No external libraries other than asyncio should be used. # Sample Code Structure: ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(\'Connection made:\', transport) def data_received(self, data): message = data.decode() print(\'Data received:\', message) self.transport.write(data) print(\'Data sent:\', message) def connection_lost(self, exc): print(\'Connection lost:\', exc) async def main(host, port): loop = asyncio.get_event_loop() server = await loop.create_server(EchoServerProtocol, host, port) async with server: await server.serve_forever() if __name__ == \\"__main__\\": try: asyncio.run(main(\'127.0.0.1\', 8888)) except KeyboardInterrupt: print(\\"Server shutting down.\\") ``` # Performance Expectations: - The server should efficiently handle multiple client connections. - It should not hang or crash on invalid operations from clients. - Memory and CPU usage should be reasonable, without excessive resource consumption. Test your implementation thoroughly, simulating multiple client connections and data exchanges.","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport client_address = transport.get_extra_info(\'peername\') print(f\'Connection made from {client_address}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') self.transport.write(data) print(f\'Data sent: {message}\') def connection_lost(self, exc): client_address = self.transport.get_extra_info(\'peername\') if exc: print(f\'Connection lost with {client_address} due to {exc}\') else: print(f\'Connection closed with {client_address}\') async def main(host, port): loop = asyncio.get_event_loop() server = await loop.create_server(EchoServerProtocol, host, port) async with server: await server.serve_forever() if __name__ == \\"__main__\\": try: asyncio.run(main(\'127.0.0.1\', 8888)) except KeyboardInterrupt: print(\\"Server shutting down.\\")"},{"question":"Your task is to implement a simple task scheduling system using the `asyncio.queues` module. You are required to design a `TaskScheduler` class and perform unit testing on it. # Objectives: 1. Implement the `TaskScheduler` class using the `asyncio.queues.Queue` class. 2. Add tasks with priorities to the scheduler and distribute them among workers. # Specifications: - The `TaskScheduler` class should be initialized with a maximum queue size. - Implement an `add_task` method that accepts a task and its priority (an integer, lower number means higher priority). - Implement a `run` method that creates a specified number of worker coroutines to process tasks from the queue. - Each worker should follow: - Fetch a task from the queue. - Perform a dummy processing step (e.g., sleep for a time). - Mark the task as done. - Ensure that the `run` method waits until all tasks are processed. - Custom exceptions should handle attempts to add tasks when the queue is full. # Constraints: - Function implementations should use async/await syntax where appropriate. - The `add_task` method should be non-blocking if the queue is full. # Input and Output: - Input: Various tasks with priorities. - Output: Order in which tasks were processed to verify the priority. # Example Usage: ```python import asyncio class TaskScheduler: def __init__(self, maxsize): self.queue = asyncio.PriorityQueue(maxsize) def add_task(self, task, priority): try: self.queue.put_nowait((priority, task)) except asyncio.QueueFull: raise TaskQueueFullError(\\"Queue is full. Cannot add more tasks.\\") async def worker(self, name): while True: priority, task = await self.queue.get() await asyncio.sleep(task) self.queue.task_done() print(f\'{name} has completed the task with priority {priority}\') async def run(self, worker_count): tasks = [asyncio.create_task(self.worker(f\'worker-{i}\')) for i in range(worker_count)] await self.queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) class TaskQueueFullError(Exception): pass async def main(): scheduler = TaskScheduler(maxsize=10) scheduler.add_task(1, 3) # priority 3 scheduler.add_task(2, 1) # priority 1 scheduler.add_task(3, 2) # priority 2 await scheduler.run(worker_count=2) asyncio.run(main()) ``` # Expected Output: The tasks will be processed in the following order: ``` worker-0 has completed the task with priority 1 worker-1 has completed the task with priority 2 worker-0 has completed the task with priority 3 ``` Implement the `TaskScheduler` class as described, ensuring to handle exceptions appropriately.","solution":"import asyncio from asyncio import PriorityQueue class TaskQueueFullError(Exception): pass class TaskScheduler: def __init__(self, maxsize): self.queue = PriorityQueue(maxsize) def add_task(self, task, priority): try: self.queue.put_nowait((priority, task)) except asyncio.QueueFull: raise TaskQueueFullError(\\"Queue is full. Cannot add more tasks.\\") async def worker(self, name): while True: priority, task = await self.queue.get() await asyncio.sleep(task) print(f\'{name} has completed the task with priority {priority}\') self.queue.task_done() async def run(self, worker_count): workers = [asyncio.create_task(self.worker(f\'worker-{i}\')) for i in range(worker_count)] await self.queue.join() for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True) # Example of how the TaskScheduler class could be used: # async def main(): # scheduler = TaskScheduler(maxsize=10) # scheduler.add_task(1, 3) # priority 3 # scheduler.add_task(2, 1) # priority 1 # scheduler.add_task(3, 2) # priority 2 # await scheduler.run(worker_count=2) # asyncio.run(main())"},{"question":"**Dynamic Python Code Execution** In this task, you are required to design and implement a function using the Python C API to compile and execute Python code from provided strings. Your primary goal is to utilize `Py_CompileStringFlags` and `PyEval_EvalCodeEx` to handle the given task. # Function Signature ```python def dynamic_code_execution(python_code: str, global_vars: dict, local_vars: dict) -> Any: Compiles and executes the given Python code with provided global and local variables. Args: python_code (str): A string containing the Python code to be executed. global_vars (dict): A dictionary of global variables. local_vars (dict): A dictionary of local variables. Returns: Any: The result of the executed code. ``` # Input - `python_code` (str): A string containing valid Python code to be executed. - `global_vars` (dict): A dictionary containing global variables to be used during code execution. - `local_vars` (dict): A dictionary containing local variables to be used during code execution. # Output - The function should return whatever the given Python code returns or produces. # Constraints - Ensure proper handling of exceptions and errors during the compilation and execution phases by raising appropriate errors in Python. - Assume `python_code` is always a valid Python code snippet, so there is no need to validate the syntax in your function. # Performance Requirements - The function should efficiently handle the execution of valid Python code provided in the form of strings. - The function should be capable of handling small to moderately complex Python code without significant performance overhead. # Example ```python global_vars = {\\"x\\": 1, \\"y\\": 2} local_vars = {} code = \\"result = x + ynresult\\" output = dynamic_code_execution(code, global_vars, local_vars) print(output) # Output should be 3 local_vars = {\\"a\\": 10, \\"b\\": 20} code = \\"result = a * bnresult\\" output = dynamic_code_execution(code, global_vars, local_vars) print(output) # Output should be 200 ``` In this example, the provided `python_code` string is dynamically compiled and executed within the context of given global and local variables, producing the expected results. # Implementation Guidance - Utilize the `Py_CompileStringFlags` to compile the provided code string into a Python code object. - Use `PyEval_EvalCodeEx` to execute the compiled code object with the given global and local variables. Ensure your implementation is robust and can handle typical exceptions and errors gracefully. Good luck!","solution":"def dynamic_code_execution(python_code: str, global_vars: dict, local_vars: dict): Compiles and executes the given Python code with provided global and local variables. Args: python_code (str): A string containing the Python code to be executed. global_vars (dict): A dictionary of global variables. local_vars (dict): A dictionary of local variables. Returns: Any: The result of the executed code. try: # Compile the code string provided compiled_code = compile(python_code, \'<string>\', \'exec\') # Execute the compiled code and capture the context exec(compiled_code, global_vars, local_vars) # The last expression defined in the local_vars dict will be the result return local_vars except Exception as e: # If there are any errors during compilation or execution, raise an appropriate error raise RuntimeError(\\"Error occurred in dynamic code execution: \\" + str(e))"},{"question":"Question # Objective Write a Python function using scikit-learn that fits multiple linear regression models on a given dataset, selects the best model based on cross-validation score, and evaluates its performance on a test dataset. The function should handle Ordinary Least Squares, Ridge, Lasso, and Elastic-Net regression. # Function Signature ```python def best_linear_model(X_train, y_train, X_test, y_test): Parameters: - X_train (numpy.ndarray): 2D array of training features - y_train (numpy.ndarray): 1D array of training targets - X_test (numpy.ndarray): 2D array of testing features - y_test (numpy.ndarray): 1D array of testing targets Returns: - best_model (sklearn.base.BaseEstimator): The best fit model - performance (dict): A dictionary containing keys \'model\', \'alpha\', \'l1_ratio\', \'rmse\' detailing the best model, its hyperparameters, and RMSE on the test set. pass ``` # Instructions 1. **Import Required Libraries:** - `from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet` - `from sklearn.model_selection import cross_val_score, KFold` - `from sklearn.metrics import mean_squared_error` - `import numpy as np` 2. **Models to Train:** - Ordinary Least Squares (OLS) - Ridge Regression with `alpha` values of `[0.1, 1, 10]` - Lasso Regression with `alpha` values of `[0.1, 1, 10]` - Elastic-Net Regression with `alpha` of `[0.1, 1, 10]` and `l1_ratio` of `[0.1, 0.5, 0.9]` 3. **Procedure:** - Create and fit each model using the training data. - Use 5-fold cross-validation to find the model with the best mean cross-validation score. - Evaluate the selected model on the test set using Root Mean Squared Error (RMSE). - Return the best model and its performance metrics. # Example Usage ```python from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split # Generate a random regression problem X, y = make_regression(n_samples=1000, n_features=20, noise=0.1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) best_model, performance = best_linear_model(X_train, y_train, X_test, y_test) print(performance) # Output example (values will differ based on data generated): # {\'model\': \'ElasticNet\', \'alpha\': 0.1, \'l1_ratio\': 0.5, \'rmse\': 0.276} ``` # Constraints - Do not use any additional machine learning libraries other than sklearn. - Ensure the function runs efficiently for datasets with up to 10,000 samples and 100 features. - Handle any exceptions or errors gracefully to ensure the function does not fail prematurely. # Notes - Model selection should be based on the best mean cross-validation score. - The function should be flexible enough to handle different datasets and return consistent results. - Document any assumptions or design choices made when implementing the function.","solution":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.model_selection import cross_val_score, KFold from sklearn.metrics import mean_squared_error import numpy as np def best_linear_model(X_train, y_train, X_test, y_test): Parameters: - X_train (numpy.ndarray): 2D array of training features - y_train (numpy.ndarray): 1D array of training targets - X_test (numpy.ndarray): 2D array of testing features - y_test (numpy.ndarray): 1D array of testing targets Returns: - best_model (sklearn.base.BaseEstimator): The best fit model - performance (dict): A dictionary containing keys \'model\', \'alpha\', \'l1_ratio\', \'rmse\' detailing the best model, its hyperparameters, and RMSE on the test set. models = { \'OLS\': LinearRegression(), \'Ridge\': [Ridge(alpha=a) for a in [0.1, 1, 10]], \'Lasso\': [Lasso(alpha=a) for a in [0.1, 1, 10]], \'ElasticNet\': [ElasticNet(alpha=a, l1_ratio=r) for a in [0.1, 1, 10] for r in [0.1, 0.5, 0.9]] } best_score = -np.inf best_model = None best_performance = {} kf = KFold(n_splits=5, shuffle=True, random_state=42) for model_name, model_list in models.items(): if not isinstance(model_list, list): model_list = [model_list] for model in model_list: scores = cross_val_score(model, X_train, y_train, cv=kf, scoring=\'neg_mean_squared_error\') mean_score = np.mean(scores) # higher score is better (since scoring is neg_mean_squared_error) if mean_score > best_score: best_score = mean_score best_model = model best_model.fit(X_train, y_train) y_pred = best_model.predict(X_test) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) best_performance[\'model\'] = type(best_model).__name__ best_performance[\'alpha\'] = getattr(best_model, \'alpha\', None) best_performance[\'l1_ratio\'] = getattr(best_model, \'l1_ratio\', None) best_performance[\'rmse\'] = rmse return best_model, best_performance"},{"question":"Implement a function that simulates the recording of a distributed rendezvous event using the PyTorch\'s `torch.distributed.elastic.events` module. You are required to create a custom event and log it using the provided API. Function Signature ```python def simulate_rdzv_event(node_id: int, event_name: str, message: str) -> None: pass ``` Input - `node_id` (int): The ID of the node where the event is taking place. - `event_name` (str): The name of the event. - `message` (str): A message or description related to the event. Constraints - `node_id` will be a non-negative integer. - `event_name` and `message` will be non-empty strings. Output - The function does not need to return anything. It should handle the logging of the event internally. Requirements 1. Use the `construct_and_record_rdzv_event` method to log the event. 2. Include relevant metadata (node_id, event_name, and message) in the logged event. Example Hereâ€™s how you can test your function: ```python simulate_rdzv_event(node_id=1, event_name=\\"RDZV_START\\", message=\\"Rendezvous started for node 1\\") ``` This should log an event named \\"RDZV_START\\" with a message \\"Rendezvous started for node 1\\" for node 1. Notes - Ensure you have installed torch and have the necessary imports in place. - Add error handling to manage potential issues with logging. - This question assesses your ability to use PyTorch event APIs effectively and integrate them into a distributed system simulation.","solution":"import torch.distributed.elastic.events as torch_events from torch.distributed.elastic.events import Event def simulate_rdzv_event(node_id: int, event_name: str, message: str) -> None: Simulates recording of a distributed rendezvous event. Args: node_id (int): The ID of the node where the event is taking place. event_name (str): The name of the event. message (str): A message or description related to the event. try: event = Event( name=event_name, metadata={ \\"node_id\\": node_id, \\"message\\": message } ) torch_events.construct_and_record_rdzv_event(event) except Exception as e: print(f\\"Failed to log event: {e}\\")"},{"question":"# Memory Management in Python Background Python\'s memory management involves complex mechanisms to ensure efficient and safe memory allocation and deallocation. The Python memory manager uses several allocator domains to handle different types of memory needs, such as raw memory allocation for general-purpose buffers, memory allocation for Python-specific buffers, and memory allocation specifically for Python objects. You are required to demonstrate your understanding of these memory allocation functions and ensure proper management of memory in custom implementations. Problem Statement Create a Python C extension module that provides a custom function to allocate and free memory using the raw memory allocation functions. This module should expose a method called `custom_buffer_operation` which: - Allocates a buffer of a given size using `PyMem_RawMalloc`. - Initializes the buffer with a given character. - Returns the buffer as a Python bytes object. - Frees the allocated buffer using `PyMem_RawFree`. Requirements - **Input** - `size` (int): The size of the buffer to allocate. - `char` (str): A single character to initialize the buffer with. - **Output** - Returns a Python bytes object representing the allocated and initialized buffer. - **Constraints** - The `size` parameter must be a positive integer. - The `char` parameter must be a single character string. - You must handle memory allocation failures gracefully and ensure no memory leaks. Performance Requirements - The function should be efficient enough to handle buffer sizes up to `10^6` bytes without significant performance degradation. Notes - Ensure you use the raw memory allocation functions `PyMem_RawMalloc` and `PyMem_RawFree`. - Properly handle memory allocation errors and ensure that all allocated memory is freed appropriately to avoid memory leaks. Additional Instructions - Implement the code in a file named `custom_memory.c`. - Provide instructions on how to compile the C extension module and how to use the `custom_buffer_operation` method in Python. ```c #include <Python.h> // Function to perform the custom buffer operation static PyObject* custom_buffer_operation(PyObject* self, PyObject* args) { int size; char fill_char; // Parse the input arguments if (!PyArg_ParseTuple(args, \\"ic\\", &size, &fill_char)) { return NULL; } // Ensure the size is positive if (size <= 0) { PyErr_SetString(PyExc_ValueError, \\"Size must be a positive integer\\"); return NULL; } // Allocate raw memory void* buffer = PyMem_RawMalloc(size); if (buffer == NULL) { return PyErr_NoMemory(); } // Fill the buffer with the provided character memset(buffer, fill_char, size); // Create a Python bytes object from the buffer PyObject* result = PyBytes_FromStringAndSize(buffer, size); // Free the allocated raw memory PyMem_RawFree(buffer); // Return the result return result; } // Method definitions static PyMethodDef CustomMemoryMethods[] = { {\\"custom_buffer_operation\\", custom_buffer_operation, METH_VARARGS, \\"Allocate and initialize a buffer\\"}, {NULL, NULL, 0, NULL} }; // Module definition static struct PyModuleDef custommemorymodule = { PyModuleDef_HEAD_INIT, \\"custom_memory\\", NULL, -1, CustomMemoryMethods }; // Module initialization PyMODINIT_FUNC PyInit_custom_memory(void) { return PyModule_Create(&custommemorymodule); } ``` Compilation Instructions 1. Save the above code in a file named `custom_memory.c`. 2. Create a `setup.py` file with the following content: ```python from distutils.core import setup, Extension module = Extension(\'custom_memory\', sources=[\'custom_memory.c\']) setup(name=\'custom_memory\', version=\'1.0\', description=\'Custom memory management module\', ext_modules=[module]) ``` 3. Compile the module by running the following command in the terminal: ```bash python setup.py build_ext --inplace ``` Usage Instructions 1. Import the module in your Python script: ```python import custom_memory ``` 2. Use the `custom_buffer_operation` method: ```python buffer = custom_memory.custom_buffer_operation(10, \'A\') print(buffer) # Outputs: b\'AAAAAAAAAA\' ```","solution":"def custom_buffer_operation(size, char): Allocates a buffer of a given size, initializes it with a given character, and returns the buffer as a Python bytes object. if not isinstance(size, int) or size <= 0: raise ValueError(\\"Size must be a positive integer\\") if not isinstance(char, str) or len(char) != 1: raise ValueError(\\"Char must be a single character string\\") buffer = bytes([ord(char)] * size) return buffer"},{"question":"**Question:** You are provided with a dataset representing information about different movies. Each row corresponds to a movie and contains details such as the title, genre, year of release, and ratings. Your task is to write a Python function using pandas that can perform various data selection and index manipulation tasks on this dataset. # Function Signature ```python import pandas as pd def movie_data_analysis(df: pd.DataFrame) -> dict: Analyzes the movie data provided in the DataFrame df Parameters: df (pd.DataFrame): DataFrame with movie data. The DataFrame contains the following columns: - \'title\': The title of the movie (string) - \'genre\': The genre of the movie (string) - \'year\': The year of release (integer) - \'ratings\': The ratings of the movie (float) Returns: dict: A dictionary containing the following information: - \'new_index\': DataFrame with \'title\' set as the new index. - \'comedy_movies\': DataFrame with all data for movies that belong to the \'Comedy\' genre. - \'top_rated\': A Series of the top 5 highest-rated movies. - \'movies_after_2000\': DataFrame with all movies released after the year 2000. - \'rating_counts\': A dictionary with years as keys and the number of movies released in that year as values. pass ``` # Tasks 1. **Set Index:** - Set the \'title\' column as the index of the DataFrame. 2. **Filter by Genre:** - Extract all movies that belong to the \'Comedy\' genre. 3. **Top Rated Movies:** - Identify the top 5 highest-rated movies. 4. **Filter by Year:** - Extract all movies released after the year 2000. 5. **Count Movies by Year:** - Count the number of movies released each year and return as a dictionary. # Example Assume `df` has the following data: | Title | Genre | Year | Ratings | |------------------|---------|------|---------| | Movie A | Comedy | 2001 | 7.9 | | Movie B | Drama | 1999 | 6.5 | | Movie C | Comedy | 2003 | 8.3 | | Movie D | Action | 2005 | 7.0 | | Movie E | Drama | 2000 | 5.4 | | Movie F | Comedy | 2015 | 7.8 | | Movie G | Action | 2010 | 8.7 | | Movie H | Comedy | 1993 | 6.2 | The return dict would be: ```python { \\"new_index\\": <DataFrame with \'title\' as index>, \\"comedy_movies\\": <DataFrame with just comedy movies>, \\"top_rated\\": <Series with top 5 ratings>, \\"movies_after_2000\\": <DataFrame with movies after 2000>, \\"rating_counts\\": <Dict with years and respective movie counts> } ``` # Constraints - The DataFrame will have at least one row of data. - The \'ratings\' column will only have float values between 0 and 10. - The \'year\' column will have integer values representing valid years (e.g., 1990, 2001). # Notes - Do not modify the input DataFrame `df` in place. Return the results in the specified format. - Use pandas DataFrame methods and indexing techniques for efficient solutions.","solution":"import pandas as pd def movie_data_analysis(df: pd.DataFrame) -> dict: Analyzes the movie data provided in the DataFrame df Parameters: df (pd.DataFrame): DataFrame with movie data. The DataFrame contains the following columns: - \'title\': The title of the movie (string) - \'genre\': The genre of the movie (string) - \'year\': The year of release (integer) - \'ratings\': The ratings of the movie (float) Returns: dict: A dictionary containing the specified information. # Task 1: Set the \'title\' column as the index of the DataFrame new_index_df = df.set_index(\'title\') # Task 2: Filter the DataFrame to get only \'Comedy\' movies comedy_movies_df = df[df[\'genre\'] == \'Comedy\'] # Task 3: Get the top 5 highest-rated movies as a Series top_rated_series = df.nlargest(5, \'ratings\').set_index(\'title\')[\'ratings\'] # Task 4: Filter the DataFrame to get movies released after the year 2000 movies_after_2000_df = df[df[\'year\'] > 2000] # Task 5: Count the number of movies released each year rating_counts_dict = df[\'year\'].value_counts().to_dict() return { \'new_index\': new_index_df, \'comedy_movies\': comedy_movies_df, \'top_rated\': top_rated_series, \'movies_after_2000\': movies_after_2000_df, \'rating_counts\': rating_counts_dict }"},{"question":"**Objective:** Demonstrate your understanding of PyTorch\'s serialization and deserialization functionalities by implementing a function that saves and loads a complex model with multiple layers, ensuring data integrity. **Problem Statement:** You are tasked with implementing a neural network model with a specific architecture and writing functions to save and load the model\'s state. The model should contain the following: - One linear layer that maps from an input of size 4 to an output of size 2. - A ReLU activation function after the first linear layer. - Another linear layer that maps from 2 to 1. Your task is to: 1. Define the model class `MyComplexModel`. 2. Implement the function `save_model(model: torch.nn.Module, filepath: str)` to save the model\'s `state_dict` to the given file path. 3. Implement the function `load_model(filepath: str) -> torch.nn.Module` to load the model\'s `state_dict` from the given file path and return the restored model. 4. Implement the function `clone_and_save_tensors(tensors: List[torch.Tensor], filepath: str)` to save a list of cloned tensors to the given file path. 5. Implement the function `load_and_verify_tensors(filepath: str) -> List[torch.Tensor]` to load the tensors from the file path and ensure their storage is minimized by cloning before saving. **Constraints and Considerations:** - The `save_model` function should handle the serialization of the `state_dict`. - The `load_model` function should ensure that the loaded model matches the original architecture and state. - The `clone_and_save_tensors` function should save clones of each tensor in the provided list to minimize storage. - The `load_and_verify_tensors` should load tensors ensuring their storages are verified to have minimal size. **Requirements:** - Use PyTorch\'s provided functions `torch.save` and `torch.load`. - Ensure that the functionality preserves views where necessary. - Return appropriate models and tensors ensuring data integrity. **Example Usage:** ```python # Define, save, and load a model model = MyComplexModel() save_model(model, \'model.pt\') loaded_model = load_model(\'model.pt\') assert model.state_dict().keys() == loaded_model.state_dict().keys() # Save and load tensors with minimized storage tensors = [torch.arange(100), torch.arange(50,100)] clone_and_save_tensors(tensors, \'tensors.pt\') loaded_tensors = load_and_verify_tensors(\'tensors.pt\') for t in loaded_tensors: assert t.storage().size() == t.numel() ``` **Expected Functions:** ```python import torch import torch.nn as nn class MyComplexModel(nn.Module): def __init__(self): super(MyComplexModel, self).__init__() self.linear1 = nn.Linear(4, 2) self.relu = nn.ReLU() self.linear2 = nn.Linear(2, 1) def forward(self, x): x = self.linear1(x) x = self.relu(x) x = self.linear2(x) return x def save_model(model: nn.Module, filepath: str): # Save the model state_dict torch.save(model.state_dict(), filepath) def load_model(filepath: str) -> nn.Module: # Create a new instance of the model model = MyComplexModel() # Load the state_dict state_dict = torch.load(filepath) # Apply the state_dict to the model model.load_state_dict(state_dict) return model def clone_and_save_tensors(tensors: List[torch.Tensor], filepath: str): # Clone each tensor and save cloned_tensors = [tensor.clone() for tensor in tensors] torch.save(cloned_tensors, filepath) def load_and_verify_tensors(filepath: str) -> List[torch.Tensor]: # Load tensors and verify storage size loaded_tensors = torch.load(filepath) for tensor in loaded_tensors: assert tensor.storage().size() == tensor.numel() return loaded_tensors ```","solution":"import torch import torch.nn as nn from typing import List class MyComplexModel(nn.Module): def __init__(self): super(MyComplexModel, self).__init__() self.linear1 = nn.Linear(4, 2) self.relu = nn.ReLU() self.linear2 = nn.Linear(2, 1) def forward(self, x): x = self.linear1(x) x = self.relu(x) x = self.linear2(x) return x def save_model(model: nn.Module, filepath: str): # Save the model state_dict torch.save(model.state_dict(), filepath) def load_model(filepath: str) -> nn.Module: # Create a new instance of the model model = MyComplexModel() # Load the state_dict state_dict = torch.load(filepath) # Apply the state_dict to the model model.load_state_dict(state_dict) return model def clone_and_save_tensors(tensors: List[torch.Tensor], filepath: str): # Clone each tensor and save cloned_tensors = [tensor.clone() for tensor in tensors] torch.save(cloned_tensors, filepath) def load_and_verify_tensors(filepath: str) -> List[torch.Tensor]: # Load tensors and verify storage size loaded_tensors = torch.load(filepath) for tensor in loaded_tensors: assert tensor.storage().size() == tensor.numel() return loaded_tensors"},{"question":"You are required to implement a custom class `CallableClass` that supports Python function call semantics and utilizes both the *tp_call* and vectorcall protocols to ensure efficient handling of function calls. # Requirements 1. Implement the `tp_call` protocol: - Define a method `tp_call` that behaves similarly to a typical function call accepting positional and keyword arguments. - This method should compute and return the sum of all its positional arguments as well as all its keyword arguments if they are numeric. Otherwise, it should correctly handle and raise appropriate exceptions for non-numeric arguments. 2. Implement the vectorcall protocol: - Ensure that your class supports the vectorcall protocol for efficient calls. Implement the respective `vectorcall` function. - Follow the guidelines to ensure that your implementation adheres to the optimization expectations (e.g., do not use vectorcall if converting arguments back to a list or dict internally). 3. Demonstrate the functionality: - Create an instance of `CallableClass`. - Demonstrate calling this instance with both positional and keyword arguments, ensuring that both protocols are exercised and their efficiency implications are showcased. # Constraints - You must handle non-numeric arguments appropriately by raising a `TypeError`. - Ensure that any implementations for recursion handling are in place for the vectorcall if needed. # Performance Your implementation should prioritize the performance benefits of the vectorcall protocol where applicable. Make sure to use appropriate CPython APIs and minimize unnecessary object conversions. # Example ```python class CallableClass: # Your implementation here # Example usage obj = CallableClass() result = obj(1, 2, 3, a=4, b=5) # Should output 15 ```","solution":"class CallableClass: def __call__(self, *args, **kwargs): return self.tp_call(args, kwargs) def tp_call(self, args, kwargs): total = 0 for arg in args: if not isinstance(arg, (int, float)): raise TypeError(f\\"Invalid argument {arg}: All arguments must be numeric.\\") total += arg for key, value in kwargs.items(): if not isinstance(value, (int, float)): raise TypeError(f\\"Invalid argument {key}={value}: All arguments must be numeric.\\") total += value return total # Example usage obj = CallableClass() result = obj(1, 2, 3, a=4, b=5) # Should output 15 print(result) # 15"},{"question":"**Task: Visualize the `tips` dataset using a customized clustermap in Seaborn** The `tips` dataset is available within Seaborn\'s sample datasets. You are required to write a function `custom_clustermap()` that performs the following: 1. Loads the `tips` dataset. 2. Uses the `clustermap` function from Seaborn to visualize the dataset. 3. Customizes the clustermap with the following requirements: - The figure should have a size of 10 inches in width and 8 inches in height. - The columns of the dataset should be standardized. - Apply row and column clustering. - Use the `coolwarm` colormap. - Adjust the color range limits to be between -3 and 3. - Include a legend that indicates the sex of the individuals in the dataset. # Function Signature ```python def custom_clustermap() -> None: ... ``` # Constraints and Notes - The function does not take any input parameters. - The function should display the clustermap when run. - You should use the appropriate Seaborn and Pandas functions to manipulate and visualize the data. - Make sure to handle any preprocessing required to fit the dataset into the clustermap function. # Example Result The output should be a displayed clustermap with customized settings as described above. **Evaluation Criteria:** - Correct implementation of the required visualization. - Appropriate use of Seaborn and Pandas functionalities. - Clear and effective customization of the clustermap according to the given specifications. - Proper handling of data preprocessing steps.","solution":"import seaborn as sns import pandas as pd from matplotlib import pyplot as plt from sklearn.preprocessing import StandardScaler def custom_clustermap() -> None: # Load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Select numerical features and standardize the columns features = tips.select_dtypes(include=\'number\') scaler = StandardScaler() standardized_features = scaler.fit_transform(features) standardized_df = pd.DataFrame(standardized_features, columns=features.columns) # Create a clustermap with the specified customizations g = sns.clustermap( standardized_df, figsize=(10, 8), cmap=\'coolwarm\', vmin=-3, vmax=3, col_cluster=True, row_cluster=True ) # Add a legend for the \'sex\' column for label in tips[\'sex\'].unique(): g.ax_col_dendrogram.bar(0, 0, color=sns.color_palette()[tips[\'sex\'].unique().tolist().index(label)], label=label, linewidth=0) g.ax_col_dendrogram.legend(loc=\\"center\\", ncol=2) # Show the plot plt.show()"},{"question":"**Question: Personal Finance Tracker** You are tasked with implementing a personal finance tracker that helps users keep track of their income and expenses. The program will involve creating a class to manage the user\'s transactions, calculate the balance, and categorize the transactions. Moreover, input and output operations will be performed to save and load user data. # Requirements: 1. Implement a class named `FinanceTracker` that manages user transactions. 2. The class should have the following methods: - `__init__(self)`: Initializes a new tracker object with an empty list of transactions. - `add_transaction(self, amount: float, description: str, category: str)`: Adds a new transaction to the list. A transaction is a dictionary with keys: \'amount\', \'description\', and \'category\'. - `get_balance(self) -> float`: Returns the current balance. The balance is computed as the sum of all transaction amounts. - `get_transactions_by_category(self, category: str) -> list`: Returns a list of all transactions filtered by the provided category. - `save_to_file(self, filepath: str)`: Saves the list of transactions to a file in JSON format. - `load_from_file(self, filepath: str)`: Loads the list of transactions from a file in JSON format and updates the internal list of transactions. 3. You must handle exceptions for possible I/O operations, such as file not found, and invalid file formats. 4. Ensure that you handle edge cases, such as an empty list of transactions when calculating balance or filtering by category. # Example Usage: ```python tracker = FinanceTracker() tracker.add_transaction(1000.0, \\"Salary\\", \\"Income\\") tracker.add_transaction(-100.0, \\"Groceries\\", \\"Expense\\") tracker.add_transaction(-50.0, \\"Utility Bill\\", \\"Expense\\") print(tracker.get_balance()) # Output: 850.0 print(tracker.get_transactions_by_category(\\"Expense\\")) # Output: [{\'amount\': -100.0, \'description\': \'Groceries\', \'category\': \'Expense\'}, {\'amount\': -50.0, \'description\': \'Utility Bill\', \'category\': \'Expense\'}] tracker.save_to_file(\\"transactions.json\\") tracker.load_from_file(\\"transactions.json\\") ``` # Constraints: - Transaction amounts can be positive (income) or negative (expense). - Categories are user-defined strings. - File paths for save/load operations are valid paths to JSON files. # Performance Requirements: - The `get_balance` and `get_transactions_by_category` methods should operate efficiently even with a large number of transactions. - The size of the JSON file should be manageable by maintaining clear and concise data in the transactions list. Implement the `FinanceTracker` class below: ```python import json class FinanceTracker: def __init__(self): # Implement initialization of the class pass def add_transaction(self, amount: float, description: str, category: str): # Implement the method to add transaction pass def get_balance(self) -> float: # Implement the method to calculate balance pass def get_transactions_by_category(self, category: str) -> list: # Implement the method to filter transactions by category pass def save_to_file(self, filepath: str): # Implement the method to save transactions to file pass def load_from_file(self, filepath: str): # Implement the method to load transactions from file pass # You can add additional helper methods if needed ```","solution":"import json class FinanceTracker: def __init__(self): Initializes a new FinanceTracker object with an empty list of transactions. self.transactions = [] def add_transaction(self, amount: float, description: str, category: str): Adds a new transaction to the list. Parameters: amount (float): The amount of the transaction. description (str): The description of the transaction. category (str): The category of the transaction. transaction = { \'amount\': amount, \'description\': description, \'category\': category } self.transactions.append(transaction) def get_balance(self) -> float: Returns the current balance. Returns: float: The balance computed as the sum of all transaction amounts. return sum(transaction[\'amount\'] for transaction in self.transactions) def get_transactions_by_category(self, category: str) -> list: Returns a list of all transactions filtered by the provided category. Parameters: category (str): The category to filter transactions by. Returns: list: A list of transactions that match the provided category. return [transaction for transaction in self.transactions if transaction[\'category\'] == category] def save_to_file(self, filepath: str): Saves the list of transactions to a file in JSON format. Parameters: filepath (str): The path to the file where transactions will be saved. with open(filepath, \'w\') as file: json.dump(self.transactions, file) def load_from_file(self, filepath: str): Loads the list of transactions from a file in JSON format and updates the internal list of transactions. Parameters: filepath (str): The path to the file where transactions are stored. with open(filepath, \'r\') as file: self.transactions = json.load(file)"},{"question":"**Objective**: Demonstrate your understanding of the `importlib.metadata` module in Python 3.10 by creating a utility function that processes metadata of installed packages. **Function to Implement**: `package_analysis(package_name: str) -> dict` **Description**: Write a function named `package_analysis` that takes a single string argument `package_name`, which is the name of an installed package, and returns a dictionary with the following structured information: 1. **package_version**: The version number of the package. 2. **metadata**: A dictionary of all available metadata fields and their values for the package. 3. **entry_points**: A dictionary where the keys are the groups of entry points (such as \\"console_scripts\\") and the values are dictionaries mapping entry point names to their respective module paths and callable attributes. 4. **file_paths**: A list of paths to all files contained within the package. 5. **requirements**: A list of package requirements. **Constraints**: - If the package does not exist or any requested metadata cannot be retrieved, handle the exception appropriately and return `None` for the respective fields. - Ensure that the function performs efficiently with a clear structure. **Function Signature**: ```python def package_analysis(package_name: str) -> dict: pass ``` **Example**: ```python result = package_analysis(\'wheel\') # A possible structure of the result dictionary { \'package_version\': \'0.36.2\', \'metadata\': { \'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', \'Version\': \'0.36.2\', \'Summary\': \'A built-package format for Python.\', \'Home-page\': \'https://github.com/pypa/wheel\', \'Author\': \'Daniel Holth\', \'Author-email\': \'dholth@fastmail.fm\', # Other metadata fields... }, \'entry_points\': { \'console_scripts\': { \'wheel\': \'wheel.cli:main\', # Other entry points... }, # Other entry point groups... }, \'file_paths\': [ \'path/to/file1\', \'path/to/file2\', # Other file paths... ], \'requirements\': [ \\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\" # Other requirements... ] } ``` **Notes**: - Make sure to modularize the code by creating helper functions if necessary. - Include appropriate error handling to manage non-existent packages or other issues. **Test Cases**: 1. `package_analysis(\'non_existent_package\')` should return a dictionary with all fields set to `None`. 2. `package_analysis(\'wheel\')` should return a structured dictionary with real metadata as shown in the example.","solution":"import os from importlib.metadata import distribution, PackageNotFoundError def package_analysis(package_name: str) -> dict: try: dist = distribution(package_name) package_version = dist.version metadata = dict(dist.metadata) entry_points = {} for entry_point in dist.entry_points: if entry_point.group not in entry_points: entry_points[entry_point.group] = {} entry_points[entry_point.group][entry_point.name] = entry_point.value file_paths = [file.locate() for file in dist.files] requirements = list(dist.requires or []) return { \'package_version\': package_version, \'metadata\': metadata, \'entry_points\': entry_points, \'file_paths\': file_paths, \'requirements\': requirements, } except PackageNotFoundError: return { \'package_version\': None, \'metadata\': None, \'entry_points\': None, \'file_paths\': None, \'requirements\': None, }"},{"question":"**Problem Statement:** Write a Python function to calculate the age of a person given the birthdate and the current date. The function should handle both naive and aware datetime objects correctly, including proper handling of timezone differences. It should also be able to format the output in a specified way. **Function Signature:** ```python def calculate_age(birthdate: datetime, current_date: datetime, output_format: str) -> str: ``` **Input:** - `birthdate` (datetime): A datetime object representing the birthdate of the person. - `current_date` (datetime): A datetime object representing the current date. - `output_format` (str): A string specifying the desired output format of the age. The format should follow the Python\'s `strftime` directives for formatting datetime objects. **Output:** - A formatted string representing the age of the person according to the specified output_format. **Constraints:** - The birthdate and current_date can be either naive or aware datetime objects. If both are naive, they should be treated as being in the same timezone. If one is naive and the other is aware, raise a `TypeError`. - Ensure that the timezones are adjusted correctly if both datetimes are aware. - The `output_format` should support directives such as `%Y`, `%m`, `%d` for years, months, and days respectively, and be able to format the calculated duration correctly. **Example Usage:** ```python from datetime import datetime, timedelta, timezone birthdate = datetime(1990, 5, 15, tzinfo=timezone.utc) current_date = datetime(2023, 10, 1, tzinfo=timezone.utc) output_format = \\"%Y years, %m months, %d days\\" print(calculate_age(birthdate, current_date, output_format)) # Output: \\"33 years, 4 months, 16 days\\" # Handling naive datetime objects birthdate_naive = datetime(1990, 5, 15) current_date_naive = datetime(2023, 10, 1) print(calculate_age(birthdate_naive, current_date_naive, output_format)) # Output: \\"33 years, 4 months, 16 days\\" ``` **Notes:** 1. You may need to handle leap years and ensure that the days are calculated correctly. 2. Be careful with the timezone differences and ensure the calculations are accurate. 3. Make sure the function is efficient in its operations.","solution":"from datetime import datetime, timezone, timedelta from dateutil.relativedelta import relativedelta def calculate_age(birthdate: datetime, current_date: datetime, output_format: str) -> str: if (birthdate.tzinfo is None and current_date.tzinfo is not None) or (birthdate.tzinfo is not None and current_date.tzinfo is None): raise TypeError(\\"Both datetimes should be either naive or aware\\") # Convert timezone-aware datetimes to the same timezone if different if birthdate.tzinfo and current_date.tzinfo and birthdate.tzinfo != current_date.tzinfo: current_date = current_date.astimezone(birthdate.tzinfo) # Calculate the difference difference = relativedelta(current_date, birthdate) # Format the duration into the required output format age_dict = { \'Y\': difference.years, \'m\': difference.months, \'d\': difference.days, } output = output_format for key, value in age_dict.items(): output = output.replace(f\\"%{key}\\", str(value)) return output"},{"question":"# **DBM Database Manipulation** You are required to implement a function to manage a simple database using Python\'s `dbm` module. Your task is to create a database, store some key-value pairs, retrieve values, update values, and delete keys, simulating a basic CRUD (Create, Read, Update, Delete) operation in a database. # **Function Signature** ```python def manage_db(operation: str, key: str = None, value: str = None) -> str: Function to manage a simple DBM database. Args: - operation: A string specifying the operation to perform. It can be one of the following: - \'create\': Create a new DBM database or open an existing one. - \'read\': Read the value of a given key. Return \'Key not found\' if the key does not exist. - \'update\': Update the value of a given key. If the key does not exist, create it. - \'delete\': Delete a given key. If the key does not exist, return \'Key not found\'. - key: The key to use for the read, update, and delete operations. - value: The value to set for the update operation. Returns: - The result of the \'read\' operation or deletion status for \'delete\'. Note: Handle any DBM-related exceptions appropriately. pass ``` # **Implementation Guidelines** 1. Create a DBM database or open an existing one using `dbm.open()`. Use `dbm.dumb` if other modules are not available. 2. Implement the following operations: * **Create**: Create a new database or open an existing one with the option to read and write. * **Read**: Retrieve the value associated with a specified key. Return \\"Key not found\\" if the key does not exist. * **Update**: Add a new key-value pair or update the value of an existing key. * **Delete**: Remove a key from the database. Return \\"Key not found\\" if the key is not present. 3. Use exception handling to manage DBM-related errors gracefully and ensure the database is closed properly. # **Example Usage** ```python # Example usage of the function manage_db(\'create\') # Initialize the database manage_db(\'update\', \'username\', \'john_doe\') # Add a new key-value pair print(manage_db(\'read\', \'username\')) # Returns \'john_doe\' manage_db(\'update\', \'username\', \'jane_doe\') # Update the value of the key \'username\' print(manage_db(\'read\', \'username\')) # Returns \'jane_doe\' print(manage_db(\'delete\', \'username\')) # Returns \'Key deleted\' print(manage_db(\'read\', \'username\')) # Returns \'Key not found\' ``` # **Notes** - Assume the database file is named `mydatabase.db`. - Remember to close the database file properly after each operation to prevent data corruption. **Constraints:** - Ensure your solution handles keys and values as strings, converting them to bytes for storage as DBM expects.","solution":"import dbm def manage_db(operation: str, key: str = None, value: str = None) -> str: db_filename = \'mydatabase\' try: with dbm.open(db_filename, \'c\') as db: if operation == \'create\': return \'Database created or opened successfully\' elif operation == \'read\': if key in db: return db[key].decode(\'utf-8\') else: return \'Key not found\' elif operation == \'update\': if key is not None and value is not None: db[key] = value return \'Key updated successfully\' else: return \'Key or value is missing\' elif operation == \'delete\': if key in db: del db[key] return \'Key deleted\' else: return \'Key not found\' else: return \'Invalid operation\' except dbm.error as e: return f\'An error occurred: {str(e)}\'"},{"question":"Objective You are required to implement a Ridge Regression model using the `scikit-learn` library. This implementation should include hyperparameter tuning using cross-validation. The goal is to fit the model to a given dataset, predict outcomes, and identify the best regularization parameter using cross-validation. Instructions 1. **Dataset**: - Assume you are provided with a dataset `data.csv` having features `X` and a target variable `y`. - Load the dataset into pandas DataFrame. 2. **Implement Ridge Regression**: - Implement Ridge Regression using the `Ridge` class from `scikit-learn`. - Perform hyperparameter tuning by selecting the optimal `alpha` from `[0.1, 1, 10, 100]` using 5-fold cross-validation. 3. **Cross-Validation**: - Use `RidgeCV` for internal cross-validation to determine the best `alpha`. 4. **Model Training and Evaluation**: - Fit the model with the entire dataset using the optimal `alpha`. - Obtain the coefficients and intercept of the model. - Make predictions on the same dataset. 5. **Output**: - Output the best `alpha` value found. - Output the model coefficients and the intercept. - Output the predictions for the dataset. Constraints - Use the provided list `[0.1, 1, 10, 100]` for potential alphas. - Use a random state of 42 for reproducibility where applicable. Expected Input and Output Formats **Input**: - A CSV file `data.csv` with the necessary data. **Output**: - The best alpha value. - The coefficients of the model. - The intercept. - Predictions for the dataset. Performance Requirements - The cross-validation process should efficiently handle a moderate-sized dataset (up to 10,000 samples and 100 features). Sample Code Framework ```python import pandas as pd from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error # Load the dataset data = pd.read_csv(\'data.csv\') X = data.drop(columns=[\'target\']) y = data[\'target\'] # Define the alphas to be tested alphas = [0.1, 1, 10, 100] # Implementing Ridge Regression with Cross-Validation ridge_cv = RidgeCV(alphas=alphas, cv=5).fit(X, y) # Getting the best alpha best_alpha = ridge_cv.alpha_ print(f\\"Best alpha: {best_alpha}\\") # Printing the coefficients and intercept print(f\\"Coefficients: {ridge_cv.coef_}\\") print(f\\"Intercept: {ridge_cv.intercept_}\\") # Predicting on the dataset predictions = ridge_cv.predict(X) print(f\\"Predictions: {predictions}\\") # Optional: Evaluating performance mse = mean_squared_error(y, predictions) print(f\\"Mean Squared Error: {mse}\\") ``` **Note**: Make sure your final implementation includes proper handling of inputs and outputs as specified and good coding practices.","solution":"import pandas as pd from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error def ridge_regression_with_cv(file_path): Perform Ridge Regression with Cross-Validation on the given dataset. Args: - file_path (str): The file path to the CSV file containing the dataset. Returns: - dict: A dictionary containing best alpha, coefficients, intercept, and predictions. # Load the dataset data = pd.read_csv(file_path) X = data.drop(columns=[\'target\']) y = data[\'target\'] # Define the alphas to be tested alphas = [0.1, 1, 10, 100] # Implementing Ridge Regression with Cross-Validation ridge_cv = RidgeCV(alphas=alphas, cv=5).fit(X, y) # Getting the best alpha best_alpha = ridge_cv.alpha_ # Getting the coefficients and intercept coefficients = ridge_cv.coef_ intercept = ridge_cv.intercept_ # Predicting on the dataset predictions = ridge_cv.predict(X) return { \\"best_alpha\\": best_alpha, \\"coefficients\\": coefficients, \\"intercept\\": intercept, \\"predictions\\": predictions }"},{"question":"# Configuration File Parser Assessment **Objective:** Write a Python function utilizing the `configparser` module to read, manipulate, and write configuration files. **Question:** You are provided with a configuration file in the classic INI format. Your task is to write a function `process_config(file_path: str, output_path: str, section_to_add: str, options_to_add: dict) -> None` that performs the following actions: 1. **Read** the input configuration file from `file_path`. 2. **Add** a new section with the name given by `section_to_add`, and populate it with the key-value pairs provided in `options_to_add`. 3. **Modify** an existing section: - If a section called \\"Settings\\" exists, change the value of an option called \\"theme\\" to \\"dark\\". - If the option \\"theme\\" doesn\'t exist, add it with the value \\"dark\\". 4. **Write** the modified configuration back to a new file defined by `output_path`. # Constraints: - The configuration file will follow the INI file format. - You should ensure that your function handles cases where the file could have missing sections or options gracefully. - Appropriate error handling should be implemented for file reading/writing operations. # Input and Output Formats: - `file_path`: A string representing the path to the input configuration file. - `output_path`: A string representing the path where the modified configuration file should be saved. - `section_to_add`: A string representing the name of the section to be added to the configuration. - `options_to_add`: A dictionary containing key-value pairs to be added to the new section. # Example: Given an input file `config.ini` with the following content: ```ini [General] username=admin password=admin123 [Settings] theme=light ``` Calling `process_config(\'config.ini\', \'new_config.ini\', \'Database\', {\'host\': \'localhost\', \'port\': \'5432\'})` should produce a `new_config.ini` file with the following content: ```ini [General] username=admin password=admin123 [Settings] theme=dark [Database] host=localhost port=5432 ``` # Function Signature: ```python import configparser def process_config(file_path: str, output_path: str, section_to_add: str, options_to_add: dict) -> None: # Your implementation here ``` # Notes: - You are encouraged to refer to the `configparser` module documentation to understand the different functionalities. - Make sure to handle different edge cases such as missing sections or options, empty input files, etc.","solution":"import configparser def process_config(file_path: str, output_path: str, section_to_add: str, options_to_add: dict) -> None: Reads a configuration file, modifies it by adding a new section and updating an existing section, then writes the modified configuration to a new file. :param file_path: str, path to the input configuration file :param output_path: str, path where the modified configuration file should be saved :param section_to_add: str, name of the section to be added :param options_to_add: dict, key-value pairs to be added to the new section config = configparser.ConfigParser() # Read the configuration file config.read(file_path) # Add the new section with provided options if not config.has_section(section_to_add): config.add_section(section_to_add) for key, value in options_to_add.items(): config.set(section_to_add, key, value) # Modify the existing \'Settings\' section if not config.has_section(\'Settings\'): config.add_section(\'Settings\') config.set(\'Settings\', \'theme\', \'dark\') # Write the modified configuration to the output file with open(output_path, \'w\') as configfile: config.write(configfile)"},{"question":"**Pandas and PyArrow Integration Exercise** # Objective: This exercise is designed to assess your comprehension of using PyArrow with pandas for data manipulation and IO operations. # Task: 1. Create a pandas DataFrame with PyArrow-backed data types. 2. Perform various data manipulation operations on the DataFrame. 3. Read a data sample from a string using PyArrow as the engine. 4. Convert an existing pandas DataFrame to PyArrow. # Input and Output Formats: 1. **Creating DataFrame:** - Input: None (Use the data provided in the task) - Output: pandas DataFrame with specified PyArrow-backed data types. 2. **Data Manipulation:** - Input: pandas DataFrame from step 1 - Output: Results of various operations (mean, sum, string operations, etc.) 3. **Reading Data:** - Input: A multi-line string with structured data - Output: pandas DataFrame read using PyArrow engine. 4. **Converting DataFrame:** - Input: pandas DataFrame with NumPy-backed data types - Output: pandas DataFrame with PyArrow-backed data types. # Instructions: 1. **Create a DataFrame with PyArrow Data Types:** - Create a DataFrame with the following data: ``` | a | b | c | d | |-------|--------|---------|---------| | 1 | 4.5 | True | hello | | 2 | 5.5 | False | world | | None | None | None | None | ``` - Use PyArrow-backed data types: int64[pyarrow], float64[pyarrow], bool[pyarrow], string[pyarrow]. 2. **Perform Data Manipulation:** - Calculate the mean of column \'b\'. - Perform a string operation where you concatenate string \' PyArrow\' to column \'d\'. - Handle missing data by filling `NaN` or `None` values in column \'b\' with the value 0. 3. **Read Data Using PyArrow Engine:** - Given the following string of data: ``` a,b,c,d 1,2.5,True,alpha 2,3.6,False,beta ``` - Read this data into a pandas DataFrame using the PyArrow engine. 4. **Convert an Existing DataFrame:** - Provided with a DataFrame: ```python import pandas as pd data = { \'x\': [1, 2, None], \'y\': [7.5, 3.4, None], \'z\': [\'foo\', \'bar\', None] } df = pd.DataFrame(data) ``` - Convert this DataFrame to have PyArrow-backed data types. # Constraints: - Ensure you handle any missing data appropriately. - Use PyArrow functionality wherever possible. - Performance optimizations are appreciated but not required for this task. # Solution Template: ```python import pandas as pd import pyarrow as pa def create_pyarrow_dataframe(): # Step 1: Create DataFrame with PyArrow-backed data types data = { \'a\': [1, 2, None], \'b\': [4.5, 5.5, None], \'c\': [True, False, None], \'d\': [\'hello\', \'world\', None] } df = pd.DataFrame(data, dtype={ \'a\': \'int64[pyarrow]\', \'b\': \'float64[pyarrow]\', \'c\': \'bool[pyarrow]\', \'d\': \'string[pyarrow]\' }) return df def manipulate_data(df): # Step 2: Data Manipulation mean_b = df[\'b\'].mean() df[\'d\'] = df[\'d\'].str.cat([\' PyArrow\']*len(df), na_rep=\'MISSING\') df[\'b\'] = df[\'b\'].fillna(0) return mean_b, df def read_data_with_pyarrow_engine(data_str): # Step 3: Read Data Using PyArrow Engine data = pd.read_csv(pd.compat.StringIO(data_str), engine=\'pyarrow\') return data def convert_to_pyarrow(df): # Step 4: Convert DataFrame to PyArrow-backed data types df = df.convert_dtypes(dtype_backend=\'pyarrow\') return df # Example Usage: df = create_pyarrow_dataframe() mean_value, updated_df = manipulate_data(df) data_str = \\"a,b,c,dn1,2.5,True,alphan2,3.6,False,betan\\" df_from_string = read_data_with_pyarrow_engine(data_str) data = {\'x\': [1, 2, None], \'y\': [7.5, 3.4, None], \'z\': [\'foo\', \'bar\', None]} df = pd.DataFrame(data) pyarrow_df = convert_to_pyarrow(df) print(df.head()) print(mean_value) print(updated_df) print(df_from_string.head()) print(pyarrow_df.head()) ``` # Important Notes: - Ensure PyArrow is installed in your environment. - Use appropriate pandas and PyArrow functions and methods to achieve the desired results. - Test your functions thoroughly and verify the outputs for correctness.","solution":"import pandas as pd import pyarrow as pa def create_pyarrow_dataframe(): Create a pandas DataFrame with PyArrow-backed data types. data = { \'a\': [1, 2, None], \'b\': [4.5, 5.5, None], \'c\': [True, False, None], \'d\': [\'hello\', \'world\', None] } df = pd.DataFrame(data).convert_dtypes(dtype_backend=\'pyarrow\') return df def manipulate_data(df): Perform data manipulation operations on the DataFrame. # Calculate mean of column \'b\' mean_b = df[\'b\'].mean() # Concatenate \' PyArrow\' to the end of each string in column \'d\' df[\'d\'] = df[\'d\'].fillna(\\"MISSING\\").astype(\\"string[pyarrow]\\") df[\'d\'] = df[\'d\'] + \' PyArrow\' # Fill missing values in column \'b\' with 0 df[\'b\'] = df[\'b\'].fillna(0) return mean_b, df def read_data_with_pyarrow_engine(data_str): Read data from a multi-line string using the PyArrow engine. from io import StringIO data = pd.read_csv(StringIO(data_str), engine=\'pyarrow\') return data def convert_to_pyarrow(df): Convert an existing pandas DataFrame to PyArrow-backed data types. df = df.convert_dtypes(dtype_backend=\'pyarrow\') return df"},{"question":"**Objective:** Implement a custom module and package management system that mimics elements of Python\'s import system, with a focus on handling both regular and namespace packages. Additionally, extend the import machinery by creating custom finders and loaders. **Background:** When Python imports a module or a package, it searches for the requisite module or package, creates a module object (if necessary), and loads the code within that module. Regular packages are directories containing an `__init__.py` file, whereas namespace packages may not have an `__init__.py` file and can span multiple directories. **Task:** 1. **Create a custom module and package setup:** - Create a directory structure that represents a package with both regular subpackages and namespace subpackages. - Implement an `__init__.py` file for regular packages which initializes some variables or functions. - Ensure that namespace packages can span multiple directories. 2. **Extend the import system:** - Implement a custom meta path finder. - Implement a custom loader. - Add your custom finder to `sys.meta_path` and demonstrate importing modules using your custom finder. **Requirements:** 1. **Directory Setup:** - Create a package named `my_package` with the following structure: ``` my_package/ __init__.py subpackage1/ __init__.py moduleA.py subpackage2/ __init__.py moduleB.py my_namespace_package/ subpackage3/ moduleC.py another_namespace_package/ subpackage3/ moduleD.py ``` 2. **Custom Meta Path Finder:** - Implement a class `CustomFinder` that adheres to the meta path finder protocol. - Implement the `find_spec` method which will be used to find modules in the `my_namespace_package`. 3. **Custom Loader:** - Implement a class `CustomLoader` that adheres to the loader protocol. - Implement the `create_module` and `exec_module` methods. 4. **Demonstration:** - Add your `CustomFinder` to `sys.meta_path`. - Demonstrate importing modules from both `my_package` and `my_namespace_package` using your custom finder. - Ensure that regular packages initialize correctly with the `__init__.py` file and that namespace packages allow for contributions from multiple directories. **Example Input and Output:** ```python # Directory setup: # See the structure provided above. # Import your custom finder and add it to sys.meta_path import sys from custom_import_system import CustomFinder sys.meta_path.insert(0, CustomFinder()) # Import modules from the custom package import my_package.subpackage1.moduleA import my_namespace_package.subpackage3.moduleC import another_namespace_package.subpackage3.moduleD # Access variables or functions defined in __init__.py of my_package print(my_package.some_variable) # Output: Variable from regular package print(my_namespace_package.subpackage3.moduleC.some_function()) # Output from moduleC print(another_namespace_package.subpackage3.moduleD.some_function()) # Output from moduleD ``` # Constraints: - Ensure that your `find_spec` method only tries to locate modules within the custom namespace packages. - The custom loader should be capable of handling both the creation and execution of modules. # Performance Requirements: - The custom finder should cache results to avoid repeated expensive filesystem operations. - The custom loader should handle exceptions gracefully, ensuring partial imports donâ€™t leave the system in an inconsistent state.","solution":"import sys import importlib.abc import importlib.util class CustomFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname.startswith(\\"my_namespace_package\\") or fullname.startswith(\\"another_namespace_package\\"): print(f\\"CustomFinder: Attempting to find module {fullname}\\") sub_module_parts = fullname.split(\'.\') if sub_module_parts[0] in [\'my_namespace_package\', \'another_namespace_package\']: for base_path in [\'my_namespace_package\', \'another_namespace_package\']: possible_path = f\\"{base_path}/{sub_module_parts[-1]}.py\\" try: with open(possible_path) as f: return importlib.util.spec_from_file_location(fullname, possible_path, loader=CustomLoader(possible_path)) except FileNotFoundError: continue return None class CustomLoader(importlib.abc.Loader): def __init__(self, path): self.path = path def create_module(self, spec): module = importlib.util.module_from_spec(spec) return module def exec_module(self, module): with open(self.path) as f: code = f.read() exec(code, vars(module)) sys.meta_path.insert(0, CustomFinder())"},{"question":"Coding Assessment Question # Task You are tasked with writing a function `evaluate_expression` that takes a string as input, representing a Python expression, and evaluates it. Your implementation should handle different input formats and ensure safe evaluation by restricting the environment in which `eval` operates. # Function Signature ```python def evaluate_expression(expr: str) -> any: pass ``` # Input - `expr`: A string representing a Python expression that needs to be evaluated. It can include arithmetic operations, function calls, and standard Python expressions. # Output - The evaluated result of the expression. The return type can vary (e.g., `int`, `float`, `str`, etc.) depending on the expression. # Constraints - You should only allow a restricted set of built-in functions, operators, and variables to ensure safety. Specifically: - Allowed operators: `+`, `-`, `*`, `/`, `**` (exponentiation), `%` (modulus) - Allowed functions: `abs`, `pow`, `round` - No access to modules, classes, or functions not explicitly permitted. - You need to handle invalid or unsafe inputs gracefully and raise appropriate exceptions if the expression cannot be evaluated securely. # Requirements - Your solution should not use any external libraries. - You need to ensure that no unsafe code can be executed (e.g., accessing files, network operations). # Examples ```python assert evaluate_expression(\\"2 + 3 * 5\\") == 17 assert evaluate_expression(\\"abs(-3.5)\\") == 3.5 assert evaluate_expression(\\"pow(2, 3)\\") == 8 assert evaluate_expression(\\"round(3.14159, 2)\\") == 3.14 # The following should raise exceptions due to unsafe operations try: evaluate_expression(\\"__import__(\'os\').system(\'ls\')\\") except ValueError: print(\\"Unsafe operation detected!\\") try: evaluate_expression(\\"open(\'somefile.txt\', \'r\')\\") except ValueError: print(\\"Unsafe operation detected!\\") ``` # Note You may use the `ast` module to parse and analyze the expression safely before evaluation.","solution":"import ast import operator def evaluate_expression(expr: str) -> any: Evaluates a given Python expression safely. :param expr: A string representing a Python expression. :return: The result of the evaluated expression. allowed_functions = { \'abs\': abs, \'pow\': pow, \'round\': round } allowed_operators = { ast.Add: operator.add, ast.Sub: operator.sub, ast.Mult: operator.mul, ast.Div: operator.truediv, ast.Pow: operator.pow, ast.Mod: operator.mod, ast.USub: operator.neg, } def _eval(node): if isinstance(node, ast.Expression): return _eval(node.body) elif isinstance(node, ast.BinOp): left = _eval(node.left) right = _eval(node.right) return allowed_operators[type(node.op)](left, right) elif isinstance(node, ast.UnaryOp): operand = _eval(node.operand) return allowed_operators[type(node.op)](operand) elif isinstance(node, ast.Call): func = allowed_functions.get(node.func.id, None) if func is None: raise ValueError(f\\"Unauthorized function {node.func.id} used\\") args = [_eval(arg) for arg in node.args] return func(*args) elif isinstance(node, ast.Num): # <number> return node.n else: raise ValueError(f\\"Unsupported operation {node}\\") try: parsed_expr = ast.parse(expr, mode=\'eval\') return _eval(parsed_expr.body) except Exception as e: raise ValueError(f\\"Invalid expression: {e}\\") # You can add additional logic here to further scrutinize the structure # of the parsed abstract syntax tree (AST) to ensure it\'s safe."},{"question":"**Objective:** Demonstrate your understanding of the `importlib.metadata` module by creating and using functions to analyze installed Python packages. **Task:** Write a Python program that performs the following tasks using `importlib.metadata`: 1. Defines a function `get_package_info(package_name: str) -> dict` that: - Takes the name of an installed package as input. - Returns a dictionary containing the package\'s version, list of entry points under the `console_scripts` group, and its installation files. 2. Defines a function `find_packages_with_entry_point(entry_point_name: str) -> List[str]` that: - Takes the name of an entry point as input. - Returns a list of package names that define this entry point under any group. **Constraints:** - Do not use any libraries other than `importlib` and standard libraries. - Ensure that appropriate error handling is in place, particularly for cases where a package or an entry point might not exist. **Input Format:** - For `get_package_info`: A single string representing the package name. - For `find_packages_with_entry_point`: A single string representing the entry point name. **Output Format:** - For `get_package_info`: A dictionary with keys \'version\', \'entry_points\', and \'files\'. The value for \'version\' should be a string, \'entry_points\' a list of strings, and \'files\' a list of file paths. - For `find_packages_with_entry_point`: A list of package names as strings. **Example:** ```python # Example outputs for the installed \'wheel\' package get_package_info(\'wheel\') # Output: { # \'version\': \'0.32.3\', # \'entry_points\': [\'wheel\', \'another_command\', ...], # These will depend on the actual package. # \'files\': [\'path/to/file1.py\', \'path/to/file2.py\', ...] # These are illustrative examples. # } find_packages_with_entry_point(\'wheel\') # Output: [\'wheel\'] # Assuming \'wheel\' is a unique entry point name. ``` **Note:** - The output for `get_package_info` for \'entry_points\' and \'files\' will vary depending on the actual package installed and its metadata. **Implementation:** Make sure your code is organized, readable, and demonstrates good practices in exception handling and function separation.","solution":"import importlib.metadata from typing import List, Dict def get_package_info(package_name: str) -> Dict[str, object]: Returns package information for a given package name. Includes version, entry points under `console_scripts` group, and installation files. try: # Get version version = importlib.metadata.version(package_name) # Get entry points distribution = importlib.metadata.distribution(package_name) entry_points = [ep.name for ep in distribution.entry_points if ep.group == \'console_scripts\'] # Get files files = [str(file_path) for file_path in distribution.files] return { \'version\': version, \'entry_points\': entry_points, \'files\': files } except importlib.metadata.PackageNotFoundError: # Package not found return { \'version\': None, \'entry_points\': [], \'files\': [] } def find_packages_with_entry_point(entry_point_name: str) -> List[str]: Returns a list of package names that define the specified entry point under any group. packages = [] for dist in importlib.metadata.distributions(): for ep in dist.entry_points: if ep.name == entry_point_name: packages.append(dist.metadata[\\"Name\\"]) break return packages"},{"question":"**Title:** Advanced File Processing with Generators and itertools Problem Statement: You are given a log file containing multiple lines. Each line represents a log entry with a timestamp followed by a message. Your task is to process this file in a memory-efficient manner using Python\'s iterator and generator features. 1. Write a generator function `log_reader(file_path)` that takes a file path as input and yields each line from the file. 2. Using the `log_reader` generator, create another generator function `error_filter(log_gen)` that filters and yields only those log entries that contain the word \\"ERROR\\". 3. Write a function `timestamp_extractor(filtered_log_gen)` that takes the filtered logs generator as input and uses `itertools` to create and return a list of unique timestamps found in the filtered logs. 4. Implement a `main()` function to demonstrate the usage of the above functions. The `main()` function should: - Read and filter the log file. - Extract and print the unique error timestamps. Input: - Path to the log file. Output: - A list of unique timestamps of error messages. Constraints: - The log file may be large, and your solution should be memory efficient. - Use only the standard library (`itertools`, `functools`, `operator`). Sample Log File: ``` 2010-01-01 00:00:01 INFO Starting process 2010-01-01 00:01:23 ERROR An error occurred 2010-01-01 00:02:45 ERROR Another error occurred 2010-01-01 00:03:56 INFO Process completed 2010-01-01 00:04:00 ERROR Critical failure ``` Function Definitions: ```python def log_reader(file_path): A generator function to read lines from a log file. Args: file_path (str): The path to the log file. Yields: str: A line from the log file. pass def error_filter(log_gen): A generator function to filter log entries containing the word \\"ERROR\\". Args: log_gen (generator): A generator yielding log entries. Yields: str: A log entry containing \\"ERROR\\". pass def timestamp_extractor(filtered_log_gen): Extracts unique timestamps from filtered log entries. Args: filtered_log_gen (generator): A generator yielding filtered log entries. Returns: list: A list of unique timestamps. pass def main(): Main function to read, filter logs and extract unique error timestamps. Args: None Returns: None pass ``` Implement and test these functions. Ensure your code is well-documented and handle edge cases where necessary.","solution":"import itertools def log_reader(file_path): A generator function to read lines from a log file. Args: file_path (str): The path to the log file. Yields: str: A line from the log file. with open(file_path, \'r\') as file: for line in file: yield line.strip() def error_filter(log_gen): A generator function to filter log entries containing the word \\"ERROR\\". Args: log_gen (generator): A generator yielding log entries. Yields: str: A log entry containing \\"ERROR\\". for log_entry in log_gen: if \\"ERROR\\" in log_entry: yield log_entry def timestamp_extractor(filtered_log_gen): Extracts unique timestamps from filtered log entries. Args: filtered_log_gen (generator): A generator yielding filtered log entries. Returns: list: A list of unique timestamps. timestamps = set() for log_entry in filtered_log_gen: timestamp = log_entry.split(\' \')[0] + \' \' + log_entry.split(\' \')[1] timestamps.add(timestamp) return list(timestamps) def main(file_path): Main function to read, filter logs and extract unique error timestamps. Args: file_path (str): The path to the log file. Returns: list: A list of unique error timestamps. logs = log_reader(file_path) error_logs = error_filter(logs) unique_timestamps = timestamp_extractor(error_logs) return unique_timestamps"},{"question":"Using seaborn, create a visualization that analyzes the relationship between `bill_length_mm` and `bill_depth_mm` of the `penguins` dataset from seaborn. Your visualization should meet the following requirements: 1. **Load Dataset**: Load the `penguins` dataset using `sns.load_dataset(\\"penguins\\")`. 2. **Plot Regression**: Create a scatter plot with a regression line for `bill_length_mm` and `bill_depth_mm`. 3. **Condition by Species and Island**: - Use color (`hue`) to differentiate between various `species`. - Split the plot into subplots (`col`) based on different `island`. - Ensure that all subplots have their axis limits independent of one another (`facet_kws=dict(sharex=False, sharey=False)`). 4. **Customize Axes**: - Set the x-axis label to \\"Bill Length (mm)\\". - Set the y-axis label to \\"Bill Depth (mm)\\". 5. **Add Title**: Add a title to each subplot indicating the island name. Implementation * Function Name: `create_penguin_plots` * Input: None * Output: This function should directly display the plots using seaborn/matplotlib functionalities. Constraints * You should not use global variables. * Optimize the function for readability and performance. * Use seaborn functions primarily, but you may use matplotlib for title customizations. Example Here\'s the structure you might start with: ```python import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the lmplot with specified conditions g = sns.lmplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"island\\", facet_kws=dict(sharex=False, sharey=False), height=4 ) # Customize axes and add titles g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") for ax, title in zip(g.axes.flat, penguins[\'island\'].unique()): ax.set_title(f\'Island: {title}\') plt.show() # Call the function to create the plot create_penguin_plots() ``` Make sure your solution produces clean, readable plots and meets all the requirements listed above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the lmplot with specified conditions g = sns.lmplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"island\\", facet_kws=dict(sharex=False, sharey=False), height=4 ) # Customize axes and add titles g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") for ax, title in zip(g.axes.flat, penguins[\'island\'].unique()): ax.set_title(f\'Island: {title}\') plt.show() # Call the function to create the plot create_penguin_plots()"},{"question":"# Question: Synthetic Dataset Generation and Basic Classification You are given a task to evaluate the effectiveness of a classification algorithm on various types of synthetic datasets. You will use the dataset generators provided by the `sklearn.datasets` module to create several datasets with specific characteristics. Then, you will apply a simple classification algorithm to test its performance on these datasets. Requirements: 1. **Generate three different datasets**: - A dataset with 3 normally-distributed clusters (`make_blobs`). - A complex dataset with noise, informative, and redundant features (`make_classification`). - A 2D binary classification dataset that is challenging for linear classification models (`make_moons`). 2. **Apply a k-nearest neighbors (KNN) classifier** to these datasets and evaluate its performance using accuracy. 3. **Visualize** the datasets and the classification boundaries. Implementation: 1. Write a function `generate_and_classify()` that: - Generates the three specified datasets. - Applies a KNN classifier with `k=3` on each dataset. - Evaluates the accuracy of the classifier on each dataset. - Displays the datasets with their classification boundaries. 2. **Input**: There are no specific inputs; the datasets should be generated within the function with fixed parameters. 3. **Output**: Output the accuracy scores for each dataset and display the plots. 4. **Constraints**: - Use `random_state=0` for reproducibility. - Use `n_samples=300` for generating all datasets. 5. **Performance requirements**: - The code should run efficiently within reasonable time on standard hardware. ```python import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split import numpy as np def plot_decision_boundary(clf, X, y): h = .02 # Step size in the mesh x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.3) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', marker=\\"o\\") plt.show() def generate_and_classify(): # Generate datasets X_blobs, y_blobs = make_blobs(n_samples=300, centers=3, random_state=0) X_classification, y_classification = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=0) X_moons, y_moons = make_moons(n_samples=300, noise=0.3, random_state=0) datasets = [(X_blobs, y_blobs), (X_classification, y_classification), (X_moons, y_moons)] titles = [\\"Blobs\\", \\"Complex Classification\\", \\"Moons\\"] for i, (X, y) in enumerate(datasets): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy for {titles[i]}: {accuracy}\') plt.title(f\'{titles[i]}: Decision Boundary\') plot_decision_boundary(knn, X, y) # Call the function to demonstrate generate_and_classify() ``` **Hint**: You may use the `train_test_split` method from `sklearn.model_selection` to split datasets into training and testing sets. Use `KNeighborsClassifier` from `sklearn.neighbors` for classification.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split import numpy as np def plot_decision_boundary(clf, X, y): h = .02 # Step size in the mesh x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.3) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', marker=\\"o\\") plt.show() def generate_and_classify(): # Generate datasets X_blobs, y_blobs = make_blobs(n_samples=300, centers=3, random_state=0) X_classification, y_classification = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=0) X_moons, y_moons = make_moons(n_samples=300, noise=0.3, random_state=0) datasets = [(X_blobs, y_blobs), (X_classification, y_classification), (X_moons, y_moons)] titles = [\\"Blobs\\", \\"Complex Classification\\", \\"Moons\\"] for i, (X, y) in enumerate(datasets): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy for {titles[i]}: {accuracy}\') plt.title(f\'{titles[i]}: Decision Boundary\') plot_decision_boundary(knn, X, y)"},{"question":"Title: Secure Management of Temporary Data Objective: To assess the comprehension of Python\'s `tempfile` module by writing a function that creates and manages temporary files and directories securely and correctly. Question: Write a Python function `create_and_manage_tempfiles()` that performs the following tasks: 1. Creates a temporary directory using `tempfile.TemporaryDirectory`. 2. Inside this directory, creates a temporary file using `tempfile.TemporaryFile` and writes the string \\"Hello, Tempfile!\\" into it. 3. Demonstrates reading from this temporary file by seeking to the beginning and reading the content back. 4. Additionally, creates a named temporary file using `tempfile.NamedTemporaryFile` within the same temporary directory, writes the string \\"This is a NamedTemporaryFile\\" to it, seeks to the start, and reads the content. 5. Ensures that all temporary files and directories are properly cleaned up after their usage. # Function Signature: ```python def create_and_manage_tempfiles() -> str: pass ``` # Expected Output: - The function should return a string that concatenates the read content from both the `TemporaryFile` and the `NamedTemporaryFile`. # Constraints: - You must use the context manager (`with` statement) to handle automatic cleanup of temporary files and directories. - The function should handle any necessary exceptions to ensure cleanup occurs even in case of errors. # Example Usage: ```python result = create_and_manage_tempfiles() print(result) ``` **Expected Output**: ``` \\"Hello, Tempfile!This is a NamedTemporaryFile\\" ``` # Explanation: In this task, students are expected to demonstrate their understanding of: - Creating and using temporary files and directories. - Writing to and reading from temporary files. - Managing temporary resources securely and ensuring proper cleanup by using context managers.","solution":"import tempfile def create_and_manage_tempfiles() -> str: try: with tempfile.TemporaryDirectory() as temp_dir: temp_file_content = \\"\\" named_temp_file_content = \\"\\" # Create a temporary file and write to it with tempfile.TemporaryFile(dir=temp_dir, mode=\'w+t\') as temp_file: temp_file.write(\\"Hello, Tempfile!\\") temp_file.seek(0) temp_file_content = temp_file.read() # Create a named temporary file and write to it with tempfile.NamedTemporaryFile(dir=temp_dir, mode=\'w+t\', delete=False) as named_temp_file: named_temp_file.write(\\"This is a NamedTemporaryFile\\") named_temp_file.seek(0) named_temp_file_content = named_temp_file.read() return temp_file_content + named_temp_file_content except Exception as e: return str(e)"},{"question":"# PyTorch Coding Challenge: GPU Profiling with TorchInductor Objective: Demonstrate your understanding of PyTorch\'s TorchInductor GPU profiling by analyzing the performance of a given model. The task will involve breaking down model execution times into individual kernel times, and then optimizing specific kernels using environment variables. Task: Given a PyTorch model\'s code and the instructions for profiling provided above, you are to perform the following steps: 1. **Profile the Model**: - Run the benchmarking script to profile the given model. Use the environment variables `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1` and `TORCHINDUCTOR_BENCHMARK_KERNEL=1`. - Save the profile output and identify the most time-consuming kernel during the forward pass. 2. **Analyze Kernel Performance**: - Isolate the identified kernel by running its standalone benchmarking script. - Report its execution time and bandwidth. 3. **Optimize the Kernel**: - Enable `TORCHINDUCTOR_MAX_AUTOTUNE=1` and re-run the kernel\'s benchmarking script. - Report the new execution time and bandwidth. - Provide an analysis comparing the original and optimized performance of the kernel. Input: - Model Python script (example: `mixnet_l.py`). - Paths to the python modules for forward graph and identified individual kernels. Output: - Percent of time when GPU is busy during the forward graph. - Detailed execution times and bandwidths for the identified kernel before and after optimization. - A brief analysis report comparing the two sets of performance metrics. Constraints: - Ensure all steps are completed within a time-bound environment (e.g., 2 hours maximum). Example Workflow: 1. **Profiling the model**: ```bash TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 python -u mixnet_l.py --backend inductor --amp --performance --dashboard --only mixnet_l --disable-cudagraphs --training > profile_output.log ``` 2. **Extracting and Benchmarking the Kernel**: - Parse `profile_output.log` to find the path to the forward graph module (e.g., `fwd.py`). - Run the forward graph module to obtain the profile trace. ```bash python fwd.py -p > fwd_profile.log ``` - Find the most time-consuming kernel in `fwd_profile.log` and note its path. ```bash python /path_to_kernel.py > kernel_benchmark.log ``` 3. **Optimizing and Re-benchmarking**: ```bash TORCHINDUCTOR_MAX_AUTOTUNE=1 python /path_to_kernel.py > kernel_benchmark_optimized.log ``` 4. **Comparative Analysis**: - Compare `kernel_benchmark.log` and `kernel_benchmark_optimized.log`. - Note differences in execution times and bandwidths. Please ensure to follow the steps accurately and document your findings.","solution":"import subprocess import re from pathlib import Path def run_command(command): result = subprocess.run(command, shell=True, capture_output=True, text=True) return result.stdout def profile_model(script_name): command = f\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 python {script_name} --backend inductor --amp --performance --dashboard --only mixnet_l --disable-cudagraphs --training\' profile_output = run_command(command) return profile_output def extract_forward_graph_path(profile_output): match = re.search(r\'Forward graph module saved at: (.*.py)\', profile_output) if match: return match.group(1) return None def benchmark_forward_graph(forward_graph_path): command = f\'python {forward_graph_path} -p\' fwd_profile_output = run_command(command) return fwd_profile_output def extract_time_consuming_kernel(fwd_profile_output): match = re.search(r\'Most time-consuming kernel: (.*.py)\', fwd_profile_output) if match: return match.group(1) return None def benchmark_kernel(kernel_path, autotune=False): env_options = \\"TORCHINDUCTOR_MAX_AUTOTUNE=1 \\" if autotune else \\"\\" command = f\'{env_options}python {kernel_path}\' kernel_benchmark_output = run_command(command) return kernel_benchmark_output def analyze_performance(original_output, optimized_output): original_time = re.search(r\'Execution time: (d+.d+)\', original_output).group(1) original_bandwidth = re.search(r\'Bandwidth: (d+.d+)\', original_output).group(1) optimized_time = re.search(r\'Execution time: (d+.d+)\', optimized_output).group(1) optimized_bandwidth = re.search(r\'Bandwidth: (d+.d+)\', optimized_output).group(1) report = { \'original\': { \'time\': float(original_time), \'bandwidth\': float(original_bandwidth) }, \'optimized\': { \'time\': float(optimized_time), \'bandwidth\': float(optimized_bandwidth) } } return report def main(): script_name = \'mixnet_l.py\' profile_output = profile_model(script_name) forward_graph_path = extract_forward_graph_path(profile_output) if forward_graph_path and Path(forward_graph_path).exists(): fwd_profile_output = benchmark_forward_graph(forward_graph_path) kernel_path = extract_time_consuming_kernel(fwd_profile_output) if kernel_path and Path(kernel_path).exists(): original_benchmark = benchmark_kernel(kernel_path) optimized_benchmark = benchmark_kernel(kernel_path, autotune=True) performance_report = analyze_performance(original_benchmark, optimized_benchmark) return performance_report return {} if __name__ == \'__main__\': report = main() if report: print(\\"Original vs Optimized Kernel Performance:\\") print(f\\"Original - Time: {report[\'original\'][\'time\']} ms, Bandwidth: {report[\'original\'][\'bandwidth\']} GB/s\\") print(f\\"Optimized - Time: {report[\'optimized\'][\'time\']} ms, Bandwidth: {report[\'optimized\'][\'bandwidth\']} GB/s\\")"},{"question":"# Advanced Python Mocking with `unittest.mock` In this coding question, you will create a set of unit tests for a hypothetical module using the `unittest` and `unittest.mock` libraries to demonstrate your understanding of mocking, patching, and making assertions about mock interactions. Hypothetical Module Consider the following module `weather.py` that provides functionality to fetch weather data from an external service: ```python # weather.py import requests class WeatherClient: def __init__(self, api_key): self.api_key = api_key def fetch_weather(self, location): response = requests.get(f\'https://api.weather.com/v3/wx/forecast/daily/5day?{location}&apiKey={self.api_key}\') if response.status_code == 200: return response.json() else: return None class WeatherService: def __init__(self, client): self.client = client def get_weather_forecast(self, location): data = self.client.fetch_weather(location) if data: # Simulate some processing of data... return data[\'forecasts\'] return \\"No Data\\" ``` Task 1. Write unit tests for `WeatherService` class methods using the `unittest` framework and `unittest.mock` package. 2. Patch the `requests.get` method to simulate API responses. 3. Create mock objects for `WeatherClient` to provide controlled behavior for the `fetch_weather` method. 4. Ensure that your tests validate both positive and negative scenarios, including assertions on: - Correct API endpoint usage. - Correct handling of successful and failed API calls. Requirements - Write your tests in a new file, `test_weather.py`. - Use the `patch` decorator or context manager where appropriate. - Use `MagicMock` or `Mock` for classes and methods to simulate their behavior. - Your tests should be comprehensive and cover the following cases: 1. Successful fetching of weather data. 2. Handling failed API calls (e.g., non-200 status codes). 3. Verifying correct API URL construction with the given location and API key. Input There is no direct input; you will create mock objects and configure their behavior. Output Your unit tests should produce output indicating whether the tests pass or fail, typical of a `unittest` run. ```python # test_weather.py import unittest from unittest.mock import patch, Mock from weather import WeatherClient, WeatherService class TestWeatherService(unittest.TestCase): @patch(\'weather.requests.get\') def test_successful_weather_fetch(self, mock_get): # Simulate a successful API response mock_response = Mock() mock_response.status_code = 200 mock_response.json.return_value = {\'forecasts\': \'Sunny\'} mock_get.return_value = mock_response client = WeatherClient(api_key=\'testapikey\') service = WeatherService(client) # Test if fetch_weather works as expected result = service.get_weather_forecast(\'valid_location\') self.assertEqual(result, \'Sunny\') mock_get.assert_called_once_with(\'https://api.weather.com/v3/wx/forecast/daily/5day?valid_location&apiKey=testapikey\') @patch(\'weather.requests.get\') def test_failed_weather_fetch(self, mock_get): # Simulate a failed API response mock_response = Mock() mock_response.status_code = 404 mock_get.return_value = mock_response client = WeatherClient(api_key=\'testapikey\') service = WeatherService(client) # Test if fetch_weather handles failure as expected result = service.get_weather_forecast(\'invalid_location\') self.assertEqual(result, \'No Data\') mock_get.assert_called_once_with(\'https://api.weather.com/v3/wx/forecast/daily/5day?invalid_location&apiKey=testapikey\') if __name__ == \\"__main__\\": unittest.main() ``` Ensure that your implementation correctly handles different cases and uses the full capability of mocking and patching provided by the `unittest.mock` library.","solution":"from unittest.mock import patch, Mock # Hypothetical weather module import requests class WeatherClient: def __init__(self, api_key): self.api_key = api_key def fetch_weather(self, location): response = requests.get(f\'https://api.weather.com/v3/wx/forecast/daily/5day?{location}&apiKey={self.api_key}\') if response.status_code == 200: return response.json() else: return None class WeatherService: def __init__(self, client): self.client = client def get_weather_forecast(self, location): data = self.client.fetch_weather(location) if data: # Simulate some processing of data... return data.get(\'forecasts\', \\"No Data\\") return \\"No Data\\""},{"question":"# Question Problem Statement Your task is to implement a function `combinations_product` that generates a list of all possible combinations of elements from multiple input iterables, followed by computing the Cartesian product of each combination. The function should leverage the `combinations` and `product` tools from the itertools module. Function Signature ```python def combinations_product(*iterables: List[List[int]], comb_len: int) -> List[Tuple[int, ...]]: pass ``` Input - `*iterables`: Variable number of input lists, where each list contains integers. - `comb_len`: An integer specifying the length of combinations to generate. Output - A list of tuples, where each tuple represents the Cartesian product of a combination of elements from the input lists. Example Consider the input: ```python combinations_product([1, 2], [3, 4, 5], [6, 7], comb_len=2) ``` The expected output should be: ``` [(1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (1, 6), (1, 7), (2, 6), (2, 7), (3, 6), (3, 7), (4, 6), (4, 7), (5, 6), (5, 7)] ``` Constraints 1. Each input list will have at least one element, but there can be up to 4 lists. 2. `comb_len` will always be less than or equal to the length of the list of input lists. 3. Each list contains distinct integers between 1 and 100. Performance Requirement - The solution should be efficient and execute within a reasonable timespan for larger inputs. Explanation The solution has to: 1. Generate all combinations of the input lists of length `comb_len`. 2. For each combination, generate the Cartesian product of the elements within these lists. 3. Return all Cartesian products as a list of tuples. Use the functionalities from itertools: `combinations` and `product`. # Implementation Use the `itertools` module to develop the function. This will test the student\'s ability to utilize and combine different iterator tools in solving the problem efficiently.","solution":"from itertools import combinations, product from typing import List, Tuple def combinations_product(*iterables: List[List[int]], comb_len: int) -> List[Tuple[int, ...]]: Generates all possible combinations of elements from multiple input iterables, followed by computing the Cartesian product of each combination. Parameters: *iterables (List[List[int]]): Variable number of input lists. comb_len (int): Length of combinations to generate. Returns: List[Tuple[int, ...]]: List of tuples representing the Cartesian product. results = [] for combo in combinations(iterables, comb_len): for prod in product(*combo): results.append(prod) return results"},{"question":"You are developing an application that needs to handle different file types based on their MIME type. You want to create a utility function using the `mailcap` module that will generate the appropriate command line to handle a given file. # Function: `create_command_line` Inputs: 1. `mime_type` (string): The MIME type of the file (e.g., `\'video/mpeg\'`). 2. `key` (string): The activity type field name (e.g., `\'view\'`). Default is `\'view\'`. 3. `filename` (string): The name of the file to be processed. 4. `parameters` (list of strings): Named parameters to be replaced in the command line, where each string is in the format `name=value`. Default is an empty list. Output: - A string that contains the command line to be executed. If no match is found, return `None`. Constraints: - The function should use the `mailcap.getcaps()` to get the mailcap dictionary and `mailcap.findmatch()` to find the appropriate command line. - Ensure that the generated command line does not contain any disallowed ASCII characters (other than alphanumerics and `@+=:,./-_`). - Handle cases where the mailcap entry is not found gracefully. Example: Given the following mailcap entry in your system: ``` video/mpeg; xmpeg %s ``` Calling the function with: ```python create_command_line(\'video/mpeg\', filename=\'video.mp4\') ``` Should return: ``` \'xmpeg video.mp4\' ``` If the `filename` contains disallowed characters: ```python create_command_line(\'video/mpeg\', filename=\'videofile.mp4\') ``` Should return: ``` None ``` # Notes: 1. Read the mailcap documentation from the `mailcap` module in Python v3.10 to understand `findmatch()` and `getcaps()`. 2. This module is deprecated in Python v3.11, so ensure your solution targets Python v3.10. Provide your implementation in the form of a function as described above.","solution":"import mailcap import re def create_command_line(mime_type, filename, key=\'view\', parameters=[]): Generates the appropriate command line for a given file based on its MIME type. Args: mime_type (str): The MIME type of the file (e.g., \'video/mpeg\'). key (str): The activity type field name (e.g., \'view\'). Default is \'view\'. filename (str): The name of the file to be processed. parameters (list of str): Named parameters to be replaced in the command line. Default is an empty list. Returns: str: The command line to be executed, or None if no match is found or the filename contains disallowed characters. # Define allowed characters for filenames allowed_chars = re.compile(r\'^[w@+=:,./-_]+\') # Check if the filename contains only allowed characters if not allowed_chars.match(filename): return None # Get mailcap entries caps = mailcap.getcaps() # Build dictionary from parameters param_dict = {kv.split(\'=\')[0]: kv.split(\'=\')[1] for kv in parameters} # Try to find a match in the mailcap entries command, entry = mailcap.findmatch(caps, mime_type, key, filename=filename, plist=param_dict) if command: return command else: return None"},{"question":"<|Analysis Begin|> The documentation provided explains the `getopt` module, which is used for parsing command line options and arguments in Python scripts. The `getopt` module provides functionality similar to the C `getopt()` function and is used to handle both short and long options supplied via the command line. Key points from the documentation: - There are two primary functions: `getopt.getopt()` and `getopt.gnu_getopt()`. - `getopt.getopt(args, shortopts, longopts=[])` parses command line options and parameters. - `getopt.gnu_getopt(args, shortopts, longopts=[])` allows GNU-style parsing, where option and non-option arguments can be intermixed. - An exception `getopt.GetoptError` (or its alias `getopt.error`) is raised for unrecognized options or missing arguments. Examples and typical usage scenarios are also included, demonstrating how to use the module to parse different types of command line options. Given this context, a challenging coding question can be designed to assess students\' understanding of the `getopt` module, requiring them to implement a function to parse command line arguments and handle different options and scenarios, including error handling. <|Analysis End|> <|Question Begin|> # Command Line Options Parser **Objective:** Write a Python function using the `getopt` module that parses a list of command line arguments for a script. The function should handle specific short and long options and appropriately process and return the parsed options and remaining arguments. **Function Signature:** ```python def parse_command_line_arguments(args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: pass ``` **Input:** - `args` (List[str]): A list of command line arguments (equivalent to `sys.argv[1:]`). **Output:** - (List[Tuple[str, str]], List[str]): A tuple where the first element is a list of `(option, value)` pairs and the second element is a list of remaining non-option arguments. **Options to be handled:** - Short options: `-a`, `-b`, and `-c` (where `-c` requires an argument). - Long options: `--help`, `--output=` (requires an argument), and `--verbose`. **Behavior:** - If the `--help` or `-h` option is present, raise a `HelpOptionTriggered` exception with the message `\\"Help option triggered\\"`. - If an unrecognized option or missing argument is present, raise a `getopt.GetoptError` exception. **Example Usage:** ```python args = [\'--output=file.txt\', \'-a\', \'-c\', \'value\', \'file1\', \'file2\'] opts, remaining_args = parse_command_line_arguments(args) # Expected output: # opts = [(\'--output\', \'file.txt\'), (\'-a\', \'\'), (\'-c\', \'value\')] # remaining_args = [\'file1\', \'file2\'] ``` **Constraints:** - The function should correctly handle and return the parsed options in the order they appear. - Ensure proper error handling with appropriate exceptions. **Additional Requirements:** - Define a custom exception `HelpOptionTriggered` when the help option is encountered. - Use the `getopt` module as described in the documentation provided. **Notes:** - The implementation should follow best practices in terms of readability and error handling. - Consider edge cases, such as missing arguments for required options or unrecognized options.","solution":"import getopt from typing import List, Tuple class HelpOptionTriggered(Exception): pass def parse_command_line_arguments(args: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: try: shortopts = \\"habc:\\" longopts = [\\"help\\", \\"output=\\", \\"verbose\\"] opts, remaining_args = getopt.getopt(args, shortopts, longopts) for opt, _ in opts: if opt in (\\"-h\\", \\"--help\\"): raise HelpOptionTriggered(\\"Help option triggered\\") return opts, remaining_args except getopt.GetoptError as e: raise e"},{"question":"# Question: Implementing and Comparing Covariance Estimators Problem Statement You are provided with a dataset, and your task is to implement and compare various covariance estimation techniques provided by the `sklearn.covariance` module. Specifically, the techniques to be implemented and compared are: 1. Empirical Covariance 2. Ledoit-Wolf Shrinkage 3. Oracle Approximating Shrinkage (OAS) 4. Robust Covariance using Minimum Covariance Determinant (MCD) You need to perform the following steps: 1. **Load the Dataset**: Load the provided dataset into a DataFrame. 2. **Estimators Implementation**: - Compute the empirical covariance matrix. - Compute the Ledoit-Wolf shrinkage covariance matrix. - Compute the Oracle Approximating Shrinkage covariance matrix. - Compute the robust covariance matrix using MCD. 3. **Compare Estimators**: Compare the performance of these estimators using appropriate metrics. You should present the covariance matrices and any relevant metrics (like Mean Squared Error if applicable) in a tabular form. Input Format You will receive a CSV file named `data.csv` containing the dataset. Each row represents an observation, and each column represents a feature. Output Format Your code should output the covariance matrices estimated by each method and the comparison metrics in a structured format (Tabular format preferred). Example This example demonstrates how to fit an `EmpiricalCovariance` object to data and compare it with other estimators. ```python import pandas as pd from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS, MinCovDet import numpy as np # Load the dataset data = pd.read_csv(\'data.csv\') X = data.values # Empirical Covariance emp_cov = EmpiricalCovariance().fit(X) emp_cov_matrix = emp_cov.covariance_ # Ledoit-Wolf Shrinkage lw = LedoitWolf().fit(X) lw_cov_matrix = lw.covariance_ # Oracle Approximating Shrinkage oas = OAS().fit(X) oas_cov_matrix = oas.covariance_ # Robust Covariance using MCD robust_cov = MinCovDet().fit(X) robust_cov_matrix = robust_cov.covariance_ # Compare Estimators # Here, you need to create a table comparing the covariance matrices and any performance metrics like MSE def compute_mse(cov1, cov2): return np.mean((cov1 - cov2) ** 2) # Example of printing the results print(\\"Empirical Covariance Matrix:n\\", emp_cov_matrix) print(\\"Ledoit-Wolf Covariance Matrix:n\\", lw_cov_matrix) print(\\"Oracle Approximating Shrinkage Covariance Matrix:n\\", oas_cov_matrix) print(\\"Robust Covariance Matrix:n\\", robust_cov_matrix) # Compute MSE with a true covariance matrix if available (for illustration) # true_cov = True Covariance Matrix # emp_mse = compute_mse(emp_cov_matrix, true_cov) # lw_mse = compute_mse(lw_cov_matrix, true_cov) # oas_mse = compute_mse(oas_cov_matrix, true_cov) # robust_mse = compute_mse(robust_cov_matrix, true_cov) # print(f\\"MSE Empirical: {emp_mse}, MSE LW: {lw_mse}, MSE OAS: {oas_mse}, MSE Robust: {robust_mse}\\") # Note: Ensure the used metrics are well-defined and directly comparable ``` Constraints 1. The dataset is assumed to be clean with no missing values. 2. Ensure to handle the `assume_centered` parameter as appropriate based on the data characteristics. Performance Requirements - Your solution should efficiently handle datasets with up to 10,000 samples and 50 features. - Compare the computational cost of each estimator.","solution":"import pandas as pd from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS, MinCovDet import numpy as np # Function to load dataset and estimate covariance matrices def compare_covariance_estimators(csv_file): # Load the dataset data = pd.read_csv(csv_file) X = data.values # Empirical Covariance emp_cov = EmpiricalCovariance().fit(X) emp_cov_matrix = emp_cov.covariance_ # Ledoit-Wolf Shrinkage lw = LedoitWolf().fit(X) lw_cov_matrix = lw.covariance_ # Oracle Approximating Shrinkage oas = OAS().fit(X) oas_cov_matrix = oas.covariance_ # Robust Covariance using MCD robust_cov = MinCovDet().fit(X) robust_cov_matrix = robust_cov.covariance_ # Collect results results = { \\"Empirical Covariance\\": emp_cov_matrix, \\"Ledoit-Wolf Shrinkage\\": lw_cov_matrix, \\"OAS\\": oas_cov_matrix, \\"Robust Covariance (MCD)\\": robust_cov_matrix } return results # Function to compute Mean Squared Error between two covariance matrices def compute_mse(cov1, cov2): return np.mean((cov1 - cov2) ** 2)"},{"question":"**Problem Statement:** You are given a dataset in the form of a list of dictionaries where each dictionary represents a student with the following keys: \'name\', \'score\', and \'grade\'. The \'score\' is an integer representing the student\'s score out of 100, and the \'grade\' is initially an empty string. Your task is to implement a function that categorizes students into grades based on their score and demonstrates the use of control flow tools as documented. The grading system is as follows: - \'A\' for scores 90 and above. - \'B\' for scores between 80 and 89. - \'C\' for scores between 70 and 79. - \'D\' for scores between 60 and 69. - \'F\' for scores below 60. Your function should satisfy the following requirements: 1. Update the \'grade\' field for each student based on their score. 2. Return a new list of dictionaries that includes only those students who have scored above 60. 3. Print the name and grade of each student sorted by their scores in descending order. 4. Use the match statement to determine the grade. Implement the following function: ```python def assign_grades_and_filter_students(students: list) -> list: This function updates the \'grade\' of each student in the list based on their score, filters out students with scores above 60, and prints the name and grade of each student sorted by score in descending order. Parameters: students (list): A list of dictionaries with \'name\', \'score\', and \'grade\' keys. Returns: list: A list of dictionaries containing students with scores above 60. # Your code here # Example usage: students = [ {\'name\': \'Alice\', \'score\': 95, \'grade\': \'\'}, {\'name\': \'Bob\', \'score\': 82, \'grade\': \'\'}, {\'name\': \'Charlie\', \'score\': 72, \'grade\': \'\'}, {\'name\': \'David\', \'score\': 55, \'grade\': \'\'}, {\'name\': \'Eve\', \'score\': 68, \'grade\': \'\'} ] result = assign_grades_and_filter_students(students) print(result) ``` **Expected Output:** 1. Printed output should display each student\'s name and grade sorted by scores in descending order. 2. The function should return a list of dictionaries only including students with scores above 60. **Constraints:** - The list of students will contain at least one student. - The score for each student will be an integer between 0 and 100 inclusive. - The \'grade\' field will initially be an empty string. **Performance Requirements:** - Your implementation should efficiently handle up to 1000 student records.","solution":"def assign_grades_and_filter_students(students: list) -> list: This function updates the \'grade\' of each student in the list based on their score, filters out students with scores above 60, and prints the name and grade of each student sorted by score in descending order. Parameters: students (list): A list of dictionaries with \'name\', \'score\', and \'grade\' keys. Returns: list: A list of dictionaries containing students with scores above 60. for student in students: score = student[\'score\'] match score: case n if n >= 90: student[\'grade\'] = \'A\' case n if 80 <= n < 90: student[\'grade\'] = \'B\' case n if 70 <= n < 80: student[\'grade\'] = \'C\' case n if 60 <= n < 70: student[\'grade\'] = \'D\' case _: student[\'grade\'] = \'F\' # Filter out students with scores above 60 filtered_students = [student for student in students if student[\'score\'] > 60] # Sort students by score in descending order sorted_students = sorted(students, key=lambda x: x[\'score\'], reverse=True) # Print the name and grade of each student for student in sorted_students: print(f\\"{student[\'name\']} - {student[\'grade\']}\\") return filtered_students"},{"question":"# Advanced Python Coding Challenge: Multi-Client Chat Server Problem Statement You are required to implement a multi-client chat server using Python\'s `socket` and `select` modules. The server should handle multiple clients simultaneously, allowing them to communicate with each other in real-time. Function Specifications: 1. **create_server_socket(port: int) -> socket.socket**: - Create an INET, STREAMing server socket, bind it to the provided port on localhost, and set it to listen for incoming connections. 2. **accept_new_connection(server_socket: socket.socket) -> Tuple[socket.socket, Tuple[str, int]]**: - Accept a new connection from the server socket and return the client socket and address. 3. **broadcast_message(sender: socket.socket, message: str, clients: List[socket.socket]) -> None**: - Send the given message from the sender to all other clients in the provided list. 4. **handle_client_message(client_socket: socket.socket, clients: List[socket.socket]) -> None**: - Receive a message from a client socket, print the received message to the server console, and broadcast the message to all other connected clients. Implementation Requirements: - The server should handle incoming client connections and messages concurrently using non-blocking sockets and the `select` module. - Use the `recv` method to read messages from clients and the `send` method to broadcast messages. - Ensure there is proper error handling and cleanup to close sockets when a client disconnects or an error occurs. Input/Output - **Input**: - No direct inputs are taken; the server should run and accept connections on a specific port (e.g., 12345). - **Output**: - No direct outputs are required; appropriate print statements can be added for debugging and logging purposes. - **Constraints**: - The server should handle message sizes up to 1024 bytes. - The server should not crash if a client suddenly disconnects. - Use port number 12345 for creating the server socket. Example Usage: ```python import socket import select def create_server_socket(port: int) -> socket.socket: # Your implementation here pass def accept_new_connection(server_socket: socket.socket) -> tuple: # Your implementation here pass def broadcast_message(sender: socket.socket, message: str, clients: list) -> None: # Your implementation here pass def handle_client_message(client_socket: socket.socket, clients: list) -> None: # Your implementation here pass def main(): server_socket = create_server_socket(12345) server_socket.listen(5) clients = [] running = True while running: ready_to_read, _, __ = select.select([server_socket] + clients, [], []) for s in ready_to_read: if s is server_socket: client_socket, address = accept_new_connection(server_socket) clients.append(client_socket) print(f\\"Client {address} connected.\\") else: try: message = s.recv(1024).decode(\'utf-8\') if message: print(f\\"Received message from {s.getpeername()}: {message}\\") broadcast_message(s, message, clients) else: s.close() clients.remove(s) print(f\\"Client {s.getpeername()} disconnected.\\") except Exception as e: print(f\\"Error with client {s.getpeername()}: {str(e)}\\") s.close() clients.remove(s) if __name__ == \\"__main__\\": main() ```","solution":"import socket from typing import Tuple, List import select def create_server_socket(port: int) -> socket.socket: server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'localhost\', port)) server_socket.listen(5) server_socket.setblocking(False) return server_socket def accept_new_connection(server_socket: socket.socket) -> Tuple[socket.socket, Tuple[str, int]]: client_socket, address = server_socket.accept() client_socket.setblocking(False) return client_socket, address def broadcast_message(sender: socket.socket, message: str, clients: List[socket.socket]) -> None: for client in clients: if client != sender: try: client.send(message.encode(\'utf-8\')) except Exception as e: print(f\\"Error broadcasting message to {client.getpeername()}: {str(e)}\\") client.close() clients.remove(client) def handle_client_message(client_socket: socket.socket, clients: List[socket.socket]) -> None: try: message = client_socket.recv(1024).decode(\'utf-8\') if message: print(f\\"Received message from {client_socket.getpeername()}: {message}\\") broadcast_message(client_socket, message, clients) else: client_socket.close() clients.remove(client_socket) print(f\\"Client {client_socket.getpeername()} disconnected.\\") except Exception as e: print(f\\"Error handling message from {client_socket.getpeername()}: {str(e)}\\") client_socket.close() clients.remove(client_socket) def main(): server_socket = create_server_socket(12345) clients = [] running = True while running: try: ready_to_read, _, __ = select.select([server_socket] + clients, [], []) for s in ready_to_read: if s is server_socket: client_socket, address = accept_new_connection(server_socket) clients.append(client_socket) print(f\\"Client {address} connected.\\") else: handle_client_message(s, clients) except KeyboardInterrupt: print(\\"Server shutting down.\\") running = False except Exception as err: print(f\\"Server error: {err}\\") for client_socket in clients: client_socket.close() server_socket.close() if __name__ == \\"__main__\\": main()"},{"question":"# Custom Event Loop Policy and Process Watcher Implementation **Objective:** As a seasoned Python developer, your knowledge of the asyncio library is crucial in managing asynchronous I/O operations. This task requires you to demonstrate your skills by implementing custom event loop policies and process watchers. **Task 1**: Implement a custom event loop policy by subclassing `asyncio.DefaultEventLoopPolicy`. The custom policy should log every time an event loop is obtained and limit the number of event loops created to a maximum of 5. The custom policy should raise an error if more than 5 event loops are created. **Task 2**: Implement a custom child process watcher by subclassing `asyncio.AbstractChildWatcher`. The custom watcher should provide additional logging for each registered child process handler and ensure that no more than 3 child processes can be watched concurrently. If a fourth child process is attempted to be watched, raise a `RuntimeError`. **Function Signatures**: - `class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy):` - `class MyChildWatcher(asyncio.AbstractChildWatcher):` **Expected Behavior**: 1. **Event Loop Policy Usage**: - When `get_event_loop()` is called, it should log the event and increment the count of created loops. - If more than 5 event loops are attempted, raise a `RuntimeError`. 2. **Child Process Watcher Usage**: - When `add_child_handler()` is called, it should log the event. - It should enforce a limit of 3 child processes, beyond which a `RuntimeError` is raised. **Constraints**: - Ensure thread safety for all logging and counter increments. - The `get_event_loop()` method should log which thread is making the call. - Must be compatible with both Unix and Windows systems. **Example**: ```python import asyncio import threading class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self): super().__init__() self._loop_counter = 0 self._lock = threading.Lock() def get_event_loop(self): with self._lock: self._loop_counter += 1 if self._loop_counter > 5: raise RuntimeError(\\"Maximum number of event loops created.\\") print(f\\"[LOG] Event Loop obtained in thread: {threading.current_thread().name}\\") return super().get_event_loop() # Other overridden methods as necessary class MyChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): super().__init__() self._processes = {} self._lock = threading.Lock() def add_child_handler(self, pid, callback, *args): with self._lock: if len(self._processes) >= 3: raise RuntimeError(\\"Maximum number of child processes watched.\\") print(f\\"[LOG] Adding child handler for PID: {pid}\\") self._processes[pid] = (callback, args) # Implement other required methods from AbstractChildWatcher # Usage: # asyncio.set_event_loop_policy(MyEventLoopPolicy()) # asyncio.get_child_watcher().attach_loop(asyncio.get_event_loop()) # Additional code to test the behavior ``` Ensure your code is thoroughly tested and demonstrates understanding of the core functionalities described.","solution":"import asyncio import threading class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def __init__(self): super().__init__() self._loop_counter = 0 self._lock = threading.Lock() def get_event_loop(self): with self._lock: self._loop_counter += 1 if self._loop_counter > 5: raise RuntimeError(\\"Maximum number of event loops created.\\") print(f\\"[LOG] Event Loop obtained in thread: {threading.current_thread().name}\\") return super().get_event_loop() class MyChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): super().__init__() self._processes = {} self._lock = threading.Lock() def add_child_handler(self, pid, callback, *args): with self._lock: if len(self._processes) >= 3: raise RuntimeError(\\"Maximum number of child processes watched.\\") print(f\\"[LOG] Adding child handler for PID: {pid}\\") self._processes[pid] = (callback, args) def remove_child_handler(self, pid): with self._lock: if pid in self._processes: del self._processes[pid] return True return False def attach_loop(self, loop): pass def detach_loop(self): pass def close(self): pass # Usage: # asyncio.set_event_loop_policy(MyEventLoopPolicy()) # asyncio.set_child_watcher(MyChildWatcher()) # Additional code to test the behavior"},{"question":"# Advanced Python Generics and Type Hinting You have been tasked with creating a custom generic container in Python that ensures type-safety during runtime, using the `GenericAlias` type introduced in Python 3.9. Your task is to implement a `GenericList` class that mimics the behavior of a generic list, constrained to contain elements of a specified type only. # Requirements: 1. Implement the `GenericList` class. 2. The class should use `GenericAlias` to enforce that only elements of the specified type are added. 3. Provide methods to: - Add an element to the list. - Retrieve an element from the list. - Return the entire list. 4. Ensure that any attempt to add an element of a type different from the specified type raises a `TypeError`. # Example: ```python from MyGenericPackage import GenericList # Example usage int_list = GenericList[int]() int_list.add_element(1) int_list.add_element(2) print(int_list.get_element(0)) # Output: 1 print(int_list.get_all_elements()) # Output: [1, 2] int_list.add_element(\\"string\\") # Should raise TypeError str_list = GenericList[str]() str_list.add_element(\\"hello\\") print(str_list.get_all_elements()) # Output: [\\"hello\\"] ``` # Input and Output Formats: - **add_element** method: - **Input**: A single element of the type specified at the time of `GenericList` instantiation. - **Output**: Adds the element to the internal list if it matches the specified type or raises a `TypeError`. - **get_element** method: - **Input**: An integer index. - **Output**: The element at the specified index. - **get_all_elements** method: - **Input**: None. - **Output**: List of all elements. # Constraints: - The index provided to `get_element` should be within the bounds of the list. - The `GenericList` should only accept types that are hashable and callable. # Performance Requirements: - This `GenericList` should efficiently handle up to 10^6 elements without significant performance degradation. # Provided Code: Build your solution by leveraging the `GenericAlias` type as described in the documentation provided.","solution":"from typing import Generic, TypeVar, List, _GenericAlias T = TypeVar(\'T\') class GenericList(Generic[T]): def __init__(self): self._list: List[T] = [] def add_element(self, element: T) -> None: if not isinstance(element, self._generic_type()): raise TypeError(f\\"Expected element of type {self._generic_type().__name__}, got {type(element).__name__}\\") self._list.append(element) def get_element(self, index: int) -> T: return self._list[index] def get_all_elements(self) -> List[T]: return self._list def _generic_type(self): return self.__orig_class__.__args__[0]"},{"question":"You are tasked with designing a simplified task scheduling system using the `asyncio` queues in Python 3.10. Your goal is to create a system that manages tasks with different priorities and ensures that higher priority tasks are handled first. **Requirements:** 1. Implement a `Task` class that includes: - `priority`: An integer representing the task\'s priority (lower numbers indicate higher priority). - `description`: A string describing the task. - Method `__lt__(self, other)` to compare tasks based on their priority (to facilitate priority queue functionality). 2. Implement the `task_scheduler` coroutine that: - Uses an `asyncio.PriorityQueue` to manage tasks. - Enqueues a given list of tasks into the priority queue. - Processes tasks by simulating work using `asyncio.sleep()` based on task priority (sleep time is inversely proportional to the priority number, e.g., priority 1 sleeps for 0.1 seconds). - Marks each task as done using `task_done()`. - Ensures all tasks are processed before returning. 3. Main coroutine to: - Create and enqueue tasks into the scheduler. - Use at least three workers to process the queue concurrently. - Measure the total time taken to process all tasks. Ensure your implementation handles asyncioâ€™s `QueueEmpty` and `QueueFull` exceptions appropriately. **Input and Output Formats:** - Input: A list of tuples, where each tuple contains an integer representing the task\'s priority and a string describing the task. - Output: Print statements indicating when a task is being processed, along with the total time taken to process all tasks. **Example Input:** ```python tasks = [ (1, \\"High Priority Task\\"), (3, \\"Medium Priority Task\\"), (2, \\"Medium-High Priority Task\\"), (5, \\"Low Priority Task\\") ] ``` **Example Output:** ```plaintext Processing task: High Priority Task with priority 1 Processing task: Medium-High Priority Task with priority 2 Processing task: Medium Priority Task with priority 3 Processing task: Low Priority Task with priority 5 Total time to process tasks: X.XX seconds ``` **Constraints:** - Maximum number of tasks: 1000. - Maximum priority value: 100. - Use asyncio queue functionalities only. **Notes:** - Properly handle the blocking behavior when putting/getting tasks if necessary. - Consider the performance and ensure efficient task scheduling. Implement your solution in Python, demonstrating a comprehensive understanding of asynchronous programming and queue management.","solution":"import asyncio import time from typing import List, Tuple class Task: def __init__(self, priority: int, description: str): self.priority = priority self.description = description def __lt__(self, other): return self.priority < other.priority async def worker(name: str, queue: asyncio.PriorityQueue): while True: try: task = await queue.get() except asyncio.QueueEmpty: continue print(f\'Processing task: {task.description} with priority {task.priority}\') # Simulate task processing time await asyncio.sleep(task.priority * 0.1) queue.task_done() async def task_scheduler(task_list: List[Tuple[int, str]]): queue = asyncio.PriorityQueue() for priority, description in task_list: task = Task(priority, description) await queue.put(task) workers = [asyncio.create_task(worker(f\'worker_{i}\', queue)) for i in range(3)] start_time = time.perf_counter() await queue.join() total_time = time.perf_counter() - start_time print(f\'Total time to process tasks: {total_time:.2f} seconds\') for w in workers: w.cancel() # Main function to simulate the task scheduler async def main(tasks: List[Tuple[int, str]]): await task_scheduler(tasks) if __name__ == \\"__main__\\": tasks = [ (1, \\"High Priority Task\\"), (3, \\"Medium Priority Task\\"), (2, \\"Medium-High Priority Task\\"), (5, \\"Low Priority Task\\"), ] asyncio.run(main(tasks))"},{"question":"You are given a dataset about the average temperature across multiple cities over several years. You need to use seaborn to accomplish the following tasks: 1. Load the dataset into a pandas DataFrame. 2. Create a scatter plot to show the relationship between the year and the average temperature. 3. Customize the plot by adding appropriate axis labels (`Year` and `Average Temperature`), a title (`Average Temperature Over Years`), and setting a theme. 4. Partition the scatter plot into two columns based on a third variable (e.g., hemisphere: Northern or Southern). 5. Highlight the temperature data points based on a fourth variable (e.g., city name) using different colors for each city. 6. Save the final plot to a file named `temperature_plot.png`. # Input - A CSV file named `temperature_data.csv` with the following columns: `year`, `avg_temperature`, `hemisphere`, and `city`. # Output - A PNG file named `temperature_plot.png`. # Constraints - The dataset can have up to 10,000 rows. - The `year` values range from 1900 onwards. - The task should be accomplished using seaborn and pandas libraries only. # Example Suppose the `temperature_data.csv` has the following content: ```csv year,avg_temperature,hemisphere,city 1900,15.0,Northern,London 1901,15.2,Northern,London 1900,22.0,Southern,Sydney 1901,21.8,Southern,Sydney ... ``` # Requirements 1. **Loading Data**: Use `pandas` to load the CSV file. 2. **Creating Plots**: Use `seaborn` to create and customize plots. 3. **Handling Data**: Use appropriate seaborn functions to handle partitioning and coloring based on columns. 4. **Saving Figures**: Ensure the figure is saved in the specified file format. # Solution Template ```python import pandas as pd import seaborn as sns # Step 1: Load the dataset data = pd.read_csv(\'temperature_data.csv\') # Step 2: Set the default theme sns.set_theme() # Step 3: Create a scatter plot g = sns.relplot( data=data, x=\'year\', y=\'avg_temperature\', col=\'hemisphere\', hue=\'city\', kind=\'scatter\' ) # Step 4: Customize the plot g.set_axis_labels(\'Year\', \'Average Temperature\') g.set_titles(\'Average Temperature Over Years\') g.add_legend() # Step 5: Save the final plot to a file g.savefig(\'temperature_plot.png\') ``` This question assesses the students\' understanding of seaborn for data visualization, including the ability to: - Load data into pandas DataFrames. - Create customized scatter plots using seaborn. - Partition data into subplots. - Use color encoding for different categories. - Save the output to a file.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_temperature_plot(file_path, output_file): # Load the dataset data = pd.read_csv(file_path) # Set the default theme sns.set_theme() # Create a scatter plot g = sns.relplot( data=data, x=\'year\', y=\'avg_temperature\', col=\'hemisphere\', hue=\'city\', kind=\'scatter\' ) # Customize the plot g.set_axis_labels(\'Year\', \'Average Temperature\') g.set_titles(\'Average Temperature Over Years\') g.add_legend() # Save the final plot to a file plt.savefig(output_file)"},{"question":"**Title: Working with Pre-trained Models in PyTorch** **Objective:** Implement a function that uses a pre-trained model from `torch.hub` to classify an image and assess the students\' understanding of model loading, pre-processing, and inference in PyTorch. **Question:** You are given the task of creating an image classification function using a pre-trained model from the PyTorch `torch.hub`. The function should download a pre-trained model, process an input image, perform inference, and return the predicted class. # Function Signature ```python def classify_image(image_path: str, model_name: str, class_names: list) -> str: pass ``` # Inputs - `image_path` (str): Path to the input image file. - `model_name` (str): The name of the pre-trained model to be loaded from `torch.hub`. - `class_names` (list): A list of class names that the model can predict. # Outputs - (str): The predicted class name. # Steps and Constraints 1. **Model Loading**: - Use `torch.hub.load` to load the specified pre-trained model. - Ensure the model is set to evaluation mode (`model.eval()`). 2. **Image Processing**: - Load the image from the given `image_path`. - Apply necessary transformations to convert the image into a format suitable for the model (resize, normalize, create a batch, etc.). - The transformations should be consistent with the steps generally required by the model\'s pre-trained version. 3. **Inference**: - Run the pre-processed image through the model to obtain predictions. - Convert the model\'s output to a human-readable class name using `class_names`. 4. **Performance**: - The inference process should be efficient and suitable for execution on a regular CPU. # Example ```python class_names = [\'cat\', \'dog\', \'car\', \'plane\'] # Assume the function classify_image is correctly implemented result = classify_image(\'path/to/cat_image.jpg\', \'resnet18\', class_names) print(result) # Output example: \'cat\' ``` # Additional Information - You may use libraries like `PIL`, `torchvision.transforms`, etc., for image loading and processing. - Check `https://pytorch.org/hub/` for model names and their requirements for image transformations. # Notes - The provided `class_names` list indexes should correspond exactly to the model\'s output. - Handling exceptions and edge cases (e.g., invalid image_path or model_name) is encouraged for robustness. **Your implementation should be clear, documented, and leverage PyTorch capabilities effectively.**","solution":"import torch from PIL import Image from torchvision import transforms def classify_image(image_path: str, model_name: str, class_names: list) -> str: # Load the pre-trained model from torch.hub model = torch.hub.load(\'pytorch/vision:v0.6.0\', model_name, pretrained=True) model.eval() # Define the required transformations preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) # Load and preprocess the image input_image = Image.open(image_path).convert(\'RGB\') input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # Perform inference with torch.no_grad(): output = model(input_batch) # Get the index of the highest probability _, predicted_idx = torch.max(output, 1) # Map the predicted index to the class name return class_names[predicted_idx.item()]"},{"question":"Implementing a Custom Generator **Objective:** Demonstrate your understanding of Python\'s generator objects by implementing a custom generator function according to specified requirements. **Problem Statement:** You are required to create a generator function that yields values from 1 to n in a specific pattern. The pattern is described as follows: 1. On the first call to the generator, yield 1. 2. On the second call to the generator, yield 2 twice. 3. On the third call to the generator, yield 3 three times. 4. Continue this pattern for all integers up to `n`. Thus, if `n` is 4, the generator should yield the sequence: `1, 2, 2, 3, 3, 3, 4, 4, 4, 4`. **Function Signature:** ```python def custom_generator(n: int): pass ``` **Input:** - `n` (int): A positive integer (1 â‰¤ n â‰¤ 1000). **Output:** - A generator that yields the described sequence of integers. **Constraints:** - The function should be implemented as a generator function using the `yield` keyword. - You should ensure that the solution is efficient in terms of memory and execution time. **Example:** ```python # Example usage: gen = custom_generator(4) print(list(gen)) # Output: [1, 2, 2, 3, 3, 3, 4, 4, 4, 4] ``` # Additional Notes: - Remember that each call to `yield` in a generator function resumes where it left off after the last `yield`. It should maintain its state across calls. - The function should be clear and make use of Python\'s generator capabilities effectively. - Consider edge cases such as when `n` is 1 or when `n` is at its maximum value.","solution":"def custom_generator(n: int): Generator that yields values from 1 to n in a specific pattern: - 1 on the first call - 2 twice on the second call - 3 three times on the third call - ... up to n. for i in range(1, n + 1): for _ in range(i): yield i"},{"question":"**Coding Assessment Question:** # Title: Working with Unicode Objects in Python # Objective: Demonstrate your understanding of Unicode object manipulation and encoding/decoding in Python by implementing functions that handle these operations. # Problem Statement: You are required to implement a function `analyze_and_process_unicode` which takes a list of strings and performs the following tasks: 1. **Filter Valid Identifiers**: Identify and return all strings from the list that are valid Python identifiers. 2. **Calculate Length**: For each valid identifier, calculate its length and store the lengths in a dictionary with the identifiers as keys. 3. **Encode to UTF-8**: Encode these valid identifiers to UTF-8 and store them in another dictionary with the identifiers as keys. # Function Signature: ```python def analyze_and_process_unicode(str_list: list) -> tuple: Analyzes and processes a list of strings based on their Unicode properties. Parameters: str_list (list): The list of strings to be processed. Returns: tuple: A tuple containing two dictionaries: - The first dictionary maps each valid identifier to its length. - The second dictionary maps each valid identifier to its UTF-8 encoded bytes. ``` # Input: - `str_list`: A list of strings. Each string is guaranteed to be a valid Unicode string. # Output: - A tuple of two dictionaries: 1. A dictionary that maps each string, if it is a valid Python identifier, to its length. 2. A dictionary that maps each valid identifier to its UTF-8 encoded bytes. # Constraints: - A string is considered a valid identifier if it follows Python\'s identifier naming rules (see `str.isidentifier()`). - All operations should be done in a memory-efficient manner, using appropriate Python and Unicode handling functions. # Example: ```python # Example usage: input_list = [\'validName1\', \'2invalid\', \'another_valid_name\', \'inv@lid\', \'Python3\'] # Expected output: # ( # { # \'validName1\': 10, # \'another_valid_name\': 18, # \'Python3\': 7 # }, # { # \'validName1\': b\'validName1\', # \'another_valid_name\': b\'another_valid_name\', # \'Python3\': b\'Python3\' # } # ) print(analyze_and_process_unicode(input_list)) ``` Make sure your solution handles edge cases and efficiently processes the list. # Evaluation Criteria: - Correctness of the solution. - Efficient and proper use of Unicode handling functions. - Code readability and use of Pythonic constructs.","solution":"def analyze_and_process_unicode(str_list): Analyzes and processes a list of strings based on their Unicode properties. Parameters: str_list (list): The list of strings to be processed. Returns: tuple: A tuple containing two dictionaries: - The first dictionary maps each valid identifier to its length. - The second dictionary maps each valid identifier to its UTF-8 encoded bytes. length_dict = {} encoded_dict = {} for item in str_list: if item.isidentifier(): length_dict[item] = len(item) encoded_dict[item] = item.encode(\'utf-8\') return length_dict, encoded_dict"},{"question":"**Objective**: Demonstrate your understanding of creating and customizing bar plots using the seaborn `objects` module. You are provided with two datasets, `penguins` and `flights`, from seaborn\'s built-in datasets. **Task**: 1. Create a bar plot using the `flights` dataset that shows the number of passengers for each month of the year 1960. 2. Customize the plot so that the bars are oriented horizontally and color each bar based on the month. 3. Create another bar plot using the `penguins` dataset to show the distribution of body mass for each species. 4. Further customize this plot to: - Color the bars by sex. - Add error bars to show the standard deviation of the body mass for each species and sex. - Use the `Dodge` transformation to differentiate the bars by sex. **Input**: - You do not need to read input from the user. The datasets can be loaded directly using the seaborn `load_dataset` function. **Output**: - The output should consist of two plots: 1. A horizontal bar plot for the `flights` data. 2. A bar plot for the `penguins` data showing the distribution of body mass, with appropriate customizations. **Constraints**: - Use the seaborn `objects` module for creating the plots. - Ensure that the plots are clear and well-labeled. **Sample Code to Load Datasets**: ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") ``` **Performance Requirements**: - The code should efficiently handle the manipulation and visualization of the provided datasets. **Example**: Your final implementation should look like this: ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Part 1: Horizontal bar plot for flights data plot1 = so.Plot(flights[\\"passengers\\"], flights[\\"month\\"]).add(so.Bar()) # Part 2: Bar plot for penguins data with customizations plot2 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Display the plots plot1.show() plot2.show() ``` Make sure to follow the task requirements and implement the plots accordingly.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets penguins = load_dataset(\\"penguins\\") flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Part 1: Horizontal bar plot for flights data plot1 = ( so.Plot(flights, y=\\"month\\", x=\\"passengers\\", color=\\"month\\") .add(so.Bar()) .label(title=\\"Number of Passengers for Each Month in 1960\\", xlabel=\\"Passengers\\", ylabel=\\"Month\\") ) # Part 2: Bar plot for penguins data with customizations plot2 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=0.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .label(title=\\"Distribution of Body Mass for Each Species\\", xlabel=\\"Species\\", ylabel=\\"Body Mass (g)\\") ) # Display the plots plot1.show() plot2.show()"},{"question":"# Objective Create custom palettes using seaborn\'s `husl_palette` function and apply them to different plots. This exercise will assess your understanding of customizing color palettes and applying them to visualizations in seaborn. # Problem Statement You are given the task of creating visualizations using seaborn and customizing your color palette using the `husl_palette` function. Your task is to write a function `custom_palette_visualization()` that: 1. Generates a customized color palette based on the parameters provided. 2. Applies this palette to a scatter plot of the tips dataset from seaborn. 3. Returns the figure object. # Function Signature ```python def custom_palette_visualization(number_of_colors: int, lightness: float, saturation: float, hue_start: float, as_continuous: bool) -> sns.axisgrid.FacetGrid: pass ``` # Parameters - `number_of_colors` (int): The number of colors to generate in the palette. It must be a positive integer. - `lightness` (float): A float value between 0 and 1 representing the lightness of the colors in the palette. - `saturation` (float): A float value between 0 and 1 representing the saturation of the colors in the palette. - `hue_start` (float): A float value representing the starting point for hue sampling. - `as_continuous` (bool): A boolean specifying whether to return a continuous colormap. # Output - Returns a `sns.axisgrid.FacetGrid` object representing the scatter plot of the `tips` dataset with the custom color palette applied. The scatter plot should plot `total_bill` on the x-axis and `tip` on the y-axis, with the data points colored based on the `day` field from the `tips` dataset. # Constraints 1. Ensure that the `lightness` and `saturation` values are within the range of 0 to 1. 2. Handle any invalid input gracefully by raising appropriate exceptions. # Example ```python # Example call to the function fig = custom_palette_visualization(8, 0.5, 0.7, 0.1, False) fig.savefig(\\"custom_palette_scatter_plot.png\\") ``` This will generate a scatter plot of the `tips` dataset with a custom color palette that has 8 colors, a lightness of 0.5, saturation of 0.7, hue starting at 0.1, and not as a continuous colormap. # Notes - Make sure to import the necessary libraries, including seaborn and matplotlib, at the beginning of your code. - You can use seaborn\'s `load_dataset` function to load the `tips` dataset. - The output figure should be created but not displayed within the function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_palette_visualization(number_of_colors: int, lightness: float, saturation: float, hue_start: float, as_continuous: bool): Generates a customized color palette and applies it to a scatter plot of the tips dataset from seaborn. Parameters: number_of_colors (int): The number of colors to generate in the palette. lightness (float): A float value between 0 and 1 representing the lightness of the colors. saturation (float): A float value between 0 and 1 representing the saturation of the colors. hue_start (float): A float value representing the starting point for hue sampling. as_continuous (bool): A boolean specifying whether to return a continuous colormap. Returns: sns.axisgrid.FacetGrid: A FacetGrid object representing the scatter plot. # Validate inputs if not (0 <= lightness <= 1): raise ValueError(\\"Lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"Saturation must be between 0 and 1\\") if number_of_colors <= 0: raise ValueError(\\"Number of colors must be a positive integer\\") # Generate custom palette if as_continuous: palette = sns.color_palette(\\"husl\\", number_of_colors) else: palette = sns.husl_palette(number_of_colors, h=hue_start, s=saturation, l=lightness) # Load dataset tips = sns.load_dataset(\\"tips\\") # Create scatter plot scatter_plot = sns.relplot( x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", palette=palette, data=tips ) return scatter_plot"},{"question":"<|Analysis Begin|> The provided documentation is comprehensive and provides extensive information about the `cmath` module in Python, which is used for mathematical operations on complex numbers. The `cmath` module includes functions for exponential and logarithmic calculations, trigonometric functions, hyperbolic functions, and classification functions, among others. It also includes constants specific to complex numbers. Our task is to design a coding question that tests the student\'s ability to utilize the functionalities of the `cmath` module, demonstrating their understanding of both fundamental and advanced concepts. Specifically, the question should challenge the student to apply the conversions, mathematical operations, and functions from the `cmath` module in a practical coding scenario. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Title:** Complex Number Operations and Transformations with `cmath` **Objective:** Demonstrate your understanding of the `cmath` module in Python by implementing a function that performs a series of operations on complex numbers and returns the results in both rectangular and polar coordinates. **Problem Statement:** You are tasked with implementing a function `complex_operations(z)` where `z` is a complex number. The function should perform the following steps: 1. Calculate the square root of `z`. 2. Calculate the exponential of `z`. 3. Calculate the natural logarithm of `z`. 4. Convert the results from steps 1 to 3 to polar coordinates. 5. Convert the results from steps 1 to 3 back into rectangular coordinates from polar coordinates. The function should return a dictionary with the following keys and their corresponding values: - `\\"sqrt_rect\\"`: The square root of `z` in rectangular coordinates. - `\\"exp_rect\\"`: The exponential of `z` in rectangular coordinates. - `\\"log_rect\\"`: The natural logarithm of `z` in rectangular coordinates. - `\\"sqrt_polar\\"`: The square root of `z` in polar coordinates as a tuple (modulus, phase). - `\\"exp_polar\\"`: The exponential of `z` in polar coordinates as a tuple (modulus, phase). - `\\"log_polar\\"`: The natural logarithm of `z` in polar coordinates as a tuple (modulus, phase). - `\\"sqrt_rect_from_polar\\"`: The square root of `z` back into rectangular coordinates from polar coordinates. - `\\"exp_rect_from_polar\\"`: The exponential of `z` back into rectangular coordinates from polar coordinates. - `\\"log_rect_from_polar\\"`: The natural logarithm of `z` back into rectangular coordinates from polar coordinates. **Function Signature:** ```python import cmath def complex_operations(z: complex) -> dict: pass ``` **Input:** - `z` : A complex number. **Output:** - A dictionary containing the results of the operations, as described above. **Constraints:** - The input `z` will always be a non-zero complex number. **Example:** ```python result = complex_operations(complex(1, 1)) print(result) # Expected output: # { # \'sqrt_rect\': (1.0986841134678099+0.45508986056222733j), # \'exp_rect\': (1.4686939399158851+2.2873552871788423j), # \'log_rect\': (0.3465735902799727+0.7853981633974483j), # \'sqrt_polar\': (1.189207115002721+0.39269908169872414), # \'exp_polar\': (2.710459584548771+1.0038848218538872), # \'log_polar\': (0.8555996771675537+1.1502619915109316), # \'sqrt_rect_from_polar\': (1.098684113467810+0.455089860562227j), # \'exp_rect_from_polar\': (1.468693939915885+2.287355287178841j), # \'log_rect_from_polar\': (0.346573590279972+0.785398163397448j) # } ``` **Notes:** 1. You may use any functions from the `cmath` module to complete this task. 2. Ensure that the resulting rectangular coordinates after conversion from polar coordinates are identical (or very close) to the initial rectangular coordinates. **Evaluation Criteria:** - Correctness of the implementation. - Proper use of the `cmath` module functions. - Accurate conversion between rectangular and polar coordinates. - Code readability and comments explaining the logic.","solution":"import cmath def complex_operations(z: complex) -> dict: # Calculate the square root of z sqrt_z = cmath.sqrt(z) # Calculate the exponential of z exp_z = cmath.exp(z) # Calculate the natural logarithm of z log_z = cmath.log(z) # Convert to polar coordinates sqrt_polar = cmath.polar(sqrt_z) exp_polar = cmath.polar(exp_z) log_polar = cmath.polar(log_z) # Convert back to rectangular coordinates sqrt_rect_from_polar = cmath.rect(sqrt_polar[0], sqrt_polar[1]) exp_rect_from_polar = cmath.rect(exp_polar[0], exp_polar[1]) log_rect_from_polar = cmath.rect(log_polar[0], log_polar[1]) return { \\"sqrt_rect\\": sqrt_z, \\"exp_rect\\": exp_z, \\"log_rect\\": log_z, \\"sqrt_polar\\": sqrt_polar, \\"exp_polar\\": exp_polar, \\"log_polar\\": log_polar, \\"sqrt_rect_from_polar\\": sqrt_rect_from_polar, \\"exp_rect_from_polar\\": exp_rect_from_polar, \\"log_rect_from_polar\\": log_rect_from_polar }"},{"question":"You are tasked to create a Python program that uses the `enum` module to model a basic user permissions system in which each permission uses a combination of different basic permissions. The permissions include: - READ (value: 1) - WRITE (value: 2) - EXECUTE (value: 4) - FULL_ACCESS - which combines READ, WRITE, and EXECUTE permissions You are required to implement the following: 1. **Permission IntFlag**: Create an `IntFlag` enumeration called `Permission` that includes: - READ permission with a value of 1 - WRITE permission with a value of 2 - EXECUTE permission with a value of 4 - FULL_ACCESS permission which combines READ, WRITE, and EXECUTE permissions 2. **Permission Check Function**: Implement a function `check_permission` that takes two arguments: - `user_permission`: an instance of the `Permission` enum representing the permissions a user has. - `required_permission`: an instance of the `Permission` enum representing the permission required to perform an action. The function should return `True` if the user has all of the required permissions, otherwise `False`. 3. **Permission Remove Function**: Implement a function `remove_permission` that takes two arguments: - `user_permission`: an instance of the `Permission` enum representing the permissions a user has. - `permission_to_remove`: an instance of the `Permission` enum representing the permission to be removed. The function should return a new `Permission` enum instance representing the user\'s permissions after removing the specified permissions. # Function Signature ```python from enum import IntFlag, auto class Permission(IntFlag): READ = 1 WRITE = 2 EXECUTE = 4 FULL_ACCESS = READ | WRITE | EXECUTE def check_permission(user_permission: Permission, required_permission: Permission) -> bool: pass def remove_permission(user_permission: Permission, permission_to_remove: Permission) -> Permission: pass ``` # Examples ```python user_permission = Permission.READ | Permission.WRITE required_permission = Permission.READ # Example 1: Checking permissions print(check_permission(user_permission, required_permission)) # Output: True # Example 2: Removing permissions removed = remove_permission(Permission.FULL_ACCESS, Permission.WRITE) print(removed) # Output: <Permission.READ|EXECUTE: 5> # Example 3: Checking permissions after removal print(check_permission(removed, Permission.WRITE)) # Output: False ``` **Constraints**: - Permissions should be combined using bitwise operators. - You are allowed to only use members from the `enum` module.","solution":"from enum import IntFlag class Permission(IntFlag): READ = 1 WRITE = 2 EXECUTE = 4 FULL_ACCESS = READ | WRITE | EXECUTE def check_permission(user_permission: Permission, required_permission: Permission) -> bool: return (user_permission & required_permission) == required_permission def remove_permission(user_permission: Permission, permission_to_remove: Permission) -> Permission: return user_permission & ~permission_to_remove"},{"question":"# Advanced Coding Challenge: Nested Context Management with ChainMap Objective: Implement a class utilizing `ChainMap` to manage nested configurations for a multi-user application where user-specific settings can override department-wide settings, which in turn override global settings. Problem Statement: Your task is to create a class `ConfigManager` which manages nested configurations using `ChainMap`. 1. **Initialization**: - The `ConfigManager` should be initialized with three dictionaries representing global settings, department settings, and user settings. - If any dictionary is not provided, it should default to an empty dictionary. 2. **Methods to Implement**: - `get_config()`: Return the effective setting for a given key across all layers of settings. - `set_user_config(key, value)`: Set a user-specific setting. - `set_department_config(key, value)`: Set a department-specific setting. - `set_global_config(key, value)`: Set a global setting. - `get_all_settings()`: Return a flattened dictionary representing the combined settings from all layers. 3. **Input and Output**: - `__init__(self, global_settings: dict = None, department_settings: dict = None, user_settings: dict = None)` - Inputs: Three optional dictionaries: `global_settings`, `department_settings`, and `user_settings`. - `get_config(self, key: str) -> Any` - Input: A string key. - Output: The effective value of the key, considering all levels. - `set_user_config(self, key: str, value: Any) -> None` - Input: A string key and value to set in user-specific settings. - `set_department_config(self, key: str, value: Any) -> None` - Input: A string key and value to set in department-specific settings. - `set_global_config(self, key: str, value: Any) -> None` - Input: A string key and value to set in global settings. - `get_all_settings(self) -> dict` - Output: A dictionary representing the merged settings from all levels. 4. **Constraints**: - Key names are unique across all configuration levels. - The operations should perform efficiently, leveraging the `ChainMap` for fast lookups. Example Usage: ```python # Initialize the manager with some settings global_config = {\'theme\': \'light\', \'timeout\': 30} department_config = {\'timeout\': 20, \'language\': \'English\'} user_config = {\'language\': \'French\', \'notifications\': \'enabled\'} config_manager = ConfigManager(global_config, department_config, user_config) # Retrieve a configuration setting print(config_manager.get_config(\'timeout\')) # Should return: 20 # Set and update settings at different levels config_manager.set_user_config(\'theme\', \'dark\') print(config_manager.get_config(\'theme\')) # Should return: \'dark\' (user setting overrides) config_manager.set_department_config(\'priority\', \'high\') print(config_manager.get_config(\'priority\')) # Should return: \'high\' (set at department level) # Get all combined settings print(config_manager.get_all_settings()) # Expected output: # {\'theme\': \'dark\', \'timeout\': 20, \'language\': \'French\', \'notifications\': \'enabled\', \'priority\': \'high\'} ``` Validation: Ensure your implementation uses the `ChainMap` effectively for managing the layered configurations and that the methods perform as described.","solution":"from collections import ChainMap class ConfigManager: def __init__(self, global_settings=None, department_settings=None, user_settings=None): self.global_settings = global_settings or {} self.department_settings = department_settings or {} self.user_settings = user_settings or {} self.configs = ChainMap(self.user_settings, self.department_settings, self.global_settings) def get_config(self, key): return self.configs.get(key) def set_user_config(self, key, value): self.user_settings[key] = value def set_department_config(self, key, value): self.department_settings[key] = value def set_global_config(self, key, value): self.global_settings[key] = value def get_all_settings(self): result = {} for config in reversed(self.configs.maps): result.update(config) return result"},{"question":"**Objective:** Implement a function that processes a list of paths, determines their file system representation, and checks if they are interactive streams, utilizing functions available in the python310 package. **Function:** ```python def process_paths_and_interactivity(paths): This function takes a list of paths and performs the following operations: 1. Converts each path to its file system representation. 2. Checks if the file stream associated with each path is interactive. Parameters: paths (list of str): List of file paths to be processed. Returns: dict: A dictionary with paths as keys and a tuple of (file_system_representation, is_interactive) as values Raises: TypeError: If any path cannot be converted to its file system representation. pass ``` **Input:** - `paths`: A list of strings where each string represents a file path. (e.g., `[\\"/path/to/file1\\", \\"/path/to/file2\\"]`) **Output:** - A dictionary: - Keys: The original file paths. - Values: A tuple where: - The first element is the file system representation of the path. - The second element is a boolean indicating if the file stream is interactive. **Constraints:** - The provided paths must be convertible to their file system representations using `PyOS_FSPath`. - You can assume that every path provided in the input list is valid and accessible. - The function should handle any `TypeError` raised due to invalid path conversion by raising the error again. **Additional Requirements:** 1. Use `PyOS_FSPath` to get the file system representation. 2. Use `Py_FdIsInteractive` to determine if the file is interactive. Since `Py_FdIsInteractive` requires a file pointer, you should open each path and get the required information. **Example:** ```python paths = [\\"/dev/tty\\", \\"/non/interactive/file.txt\\"] output = process_paths_and_interactivity(paths) # Expected output (actual values may vary based on system configuration): # { # \\"/dev/tty\\": (\\"/dev/tty\\", True), # \\"/non/interactive/file.txt\\": (\\"/non/interactive/file.txt\\", False) # } ``` **Performance Requirements:** - Your solution should be efficient enough to handle up to 1000 paths in a reasonable time frame. - Ensure proper resource management, such as closing file streams after checking their interactivity. You are required to implement the function and handle any exceptions as described.","solution":"import os from _io import TextIOWrapper def pyos_fspath(path): Mock implementation of PyOS_FSPath for converting path to filesystem representation. In a real setup, this would be part of the python310 package. return os.fspath(path) def py_fd_is_interactive(fp): Mock implementation of Py_FdIsInteractive for determining if a given file pointer is interactive. In a real setup, this would be part of the python310 package. return isinstance(fp, TextIOWrapper) and fp.isatty() def process_paths_and_interactivity(paths): This function takes a list of paths and performs the following operations: 1. Converts each path to its file system representation. 2. Checks if the file stream associated with each path is interactive. Parameters: paths (list of str): List of file paths to be processed. Returns: dict: A dictionary with paths as keys and a tuple of (file_system_representation, is_interactive) as values Raises: TypeError: If any path cannot be converted to its file system representation. result = {} for path in paths: try: fs_repr = pyos_fspath(path) except TypeError: raise is_interactive = False try: with open(path, \'r\') as fp: is_interactive = py_fd_is_interactive(fp) except Exception: is_interactive = False result[path] = (fs_repr, is_interactive) return result"},{"question":"Objective Your task is to identify and fix a performance issue in the provided Python code. You will then profile the code to measure its execution time before and after optimization. Problem Description You are given a Python function `find_primes_upto(n)` that returns a list of all prime numbers up to `n`. The current implementation has some performance issues. Your goal is to: 1. Identify and fix the performance issue. 2. Measure and compare the execution time of the function before and after the optimization. You should use the `cProfile` module to measure the execution time. Function Signature ```python def find_primes_upto(n: int) -> list: # Original implementation ``` Input - `n` (integer): The upper limit (inclusive) to find prime numbers. `1 <= n <= 10^5` Output - A list of integers representing all prime numbers up to `n`. Constraints - The function should handle input values up to 10^5 efficiently. - The solution should use `cProfile` to compare the execution time before and after optimization. Performance Requirements - Time complexity of the optimized function should be better than the initial implementation. Example ```python # Original function def find_primes_upto(n): primes = [] for num in range(2, n + 1): if all(num % i != 0 for i in range(2, num)): primes.append(num) return primes # Before optimization profiling import cProfile cProfile.run(\'find_primes_upto(10000)\') # Optimize the function def find_primes_upto(n): if n < 2: return [] sieve = [True] * (n + 1) sieve[0] = sieve[1] = False for start in range(2, int(n**0.5) + 1): if sieve[start]: for multiples in range(start * start, n + 1, start): sieve[multiples] = False return [num for num, is_prime in enumerate(sieve) if is_prime] # After optimization profiling cProfile.run(\'find_primes_upto(10000)\') ``` Use the `cProfile` module to demonstrate the performance improvements. Comment on your observations and the improvement in execution time. Submission Submit the following: 1. The fixed and optimized implementation of `find_primes_upto`. 2. The profiling results using `cProfile` before and after the optimization. 3. A brief commentary on the performance improvements observed.","solution":"# Original function def find_primes_upto(n): primes = [] for num in range(2, n + 1): if all(num % i != 0 for i in range(2, num)): primes.append(num) return primes # Optimized function using Sieve of Eratosthenes def find_primes_upto(n): if n < 2: return [] sieve = [True] * (n + 1) sieve[0] = sieve[1] = False for start in range(2, int(n**0.5) + 1): if sieve[start]: for multiples in range(start * start, n + 1, start): sieve[multiples] = False return [num for num, is_prime in enumerate(sieve) if is_prime]"},{"question":"# Question: Advanced Jitter Plot In this exercise, you\'re required to demonstrate your understanding of the `seaborn` package, specifically focusing on the `seaborn.objects` module and its capabilities to add jitter to plots. You will create a scatter plot that addresses potential overlapping of data points using jitter. Task 1. **Load the Dataset**: Load the \\"penguins\\" dataset from seaborn. 2. **Generate Jitter Plot**: Create a jitter plot using the following constraints: - X-axis: `species` - Y-axis: `body_mass_g` - Add jitter only along the Y-axis with a fixed value of `y=200`. 3. **Additional Plot with Numeric Axis**: Create another plot with the following constraints: - X-axis: `body_mass_g` rounded to the nearest 1000. - Y-axis: `flipper_length_mm` rounded to the nearest 10. - Add jitter with a relative `width` of 0.2. 4. **Combined Jitter Settings Plot**: Create a plot with combined jitter settings: - X-axis: `body_mass_g` rounded to the nearest 1000. - Y-axis: `flipper_length_mm` rounded to the nearest 10. - Add jitter along both axes with `x=150` and `y=5`. # Evaluation Criteria: - **Correctness**: Adherence to task specification. - **Clarity**: Code should be well-organized, and each plot appropriately labeled. - **Usage of Seaborn Objects API**: Ensure the use of `seaborn.objects` module especially `so.Jitter`. # Code Template: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Task 1: Generate Jitter Plot plot1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(y=200)) ) # Task 2: Additional Plot with Numeric Axis plot2 = ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1)) .add(so.Dots(), so.Jitter(width=0.2)) ) # Task 3: Combined Jitter Settings Plot plot3 = ( so.Plot(penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1)) .add(so.Dots(), so.Jitter(x=150, y=5)) ) # You can display the plots like this: plot1.show() plot2.show() plot3.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Task 1: Generate Jitter Plot plot1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(y=200)) ) # Task 2: Additional Plot with Numeric Axis plot2 = ( so.Plot( x=penguins[\\"body_mass_g\\"].round(-3), y=penguins[\\"flipper_length_mm\\"].round(-1) ) .add(so.Dots(), so.Jitter(width=0.2)) ) # Task 3: Combined Jitter Settings Plot plot3 = ( so.Plot( x=penguins[\\"body_mass_g\\"].round(-3), y=penguins[\\"flipper_length_mm\\"].round(-1) ) .add(so.Dots(), so.Jitter(x=150, y=5)) ) # Display the plots plot1.show() plot2.show() plot3.show()"},{"question":"# **Task: Creating Customized Boxen Plots with Seaborn** You are required to write a function `create_custom_boxenplot` that takes in a dataframe and creates multiple customized boxen plots using seaborn. Your function should demonstrate your understanding of seaborn\'s advanced plotting techniques. # **Function Signature** ```python def create_custom_boxenplot(data: pd.DataFrame): pass ``` # **Input** - `data`: A pandas DataFrame containing the following columns: - `feature1`: Numerical data representing the prices of different items. - `feature2`: A categorical feature representing the clarity of items (e.g., \'SI1\', \'VS2\', \'VVS2\', etc.). - `feature3`: Another numerical feature representing the size of items. # **Output** - Your function does not return anything. It should generate and display four types of boxen plots: 1. A single horizontal boxen plot representing distribution of `feature1`. 2. A vertical boxen plot grouped by `feature2`. 3. A vertical boxen plot grouped by `feature2` with an additional color group based on whether `feature3` is above a certain threshold (e.g., `feature3` > 50). 4. A customized vertical boxen plot with the following requirements: - Linear reduction in box widths. - A specific maximum box width. - Custom line color for box outlines and median, custom appearance for outliers. - Unfilled boxes using the `feature2` variable for grouping. # **Performance Requirements** - Use seaborn\'s `boxenplot` function efficiently to minimize unnecessary computations. - Ensure readability and maintainability of your code. # **Example** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Assume `data` is provided as described in the Input section def create_custom_boxenplot(data: pd.DataFrame): # Plot 1: Single horizontal boxen plot plt.figure(figsize=(10, 6)) sns.boxenplot(x=data[\\"feature1\\"]) plt.title(\'Horizontal Boxen Plot of Feature1\') plt.show() # Plot 2: Vertical boxen plot grouped by Feature2 plt.figure(figsize=(10, 6)) sns.boxenplot(x=\\"feature1\\", y=\\"feature2\\", data=data) plt.title(\'Vertical Boxen Plot grouped by Feature2\') plt.show() # Plot 3: Vertical boxen plot with color group threshold = 50 large_item = data[\\"feature3\\"].gt(threshold).rename(\\"large_item\\") plt.figure(figsize=(10, 6)) sns.boxenplot(x=\\"feature1\\", y=\\"feature2\\", hue=large_item, data=data, gap=.2) plt.title(\'Vertical Boxen Plot grouped by Feature2 with color group\') plt.show() # Plot 4: Customized vertical boxen plot plt.figure(figsize=(10, 6)) sns.boxenplot( x=\\"feature1\\", y=\\"feature2\\", data=data, width_method=\\"linear\\", width=.5, linewidth=.5, linecolor=\\".7\\", line_kws=dict(linewidth=1.5, color=\\"#cde\\"), flier_kws=dict(facecolor=\\".7\\", linewidth=.5), fill=False ) plt.title(\'Customized Vertical Boxen Plot with Feature2 and unfilled boxes\') plt.show() # Example usage # data = pd.read_csv(\\"path_to_your_dataframe.csv\\") # create_custom_boxenplot(data) ``` Ensure you have the seaborn and matplotlib libraries installed before running the function. # **Additional Information** - This task assesses your understanding of seaborn, particularly the customization options available for the `boxenplot` function. - Make sure to thoroughly comment your code.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_boxenplot(data: pd.DataFrame): This function generates four different boxen plots using seaborn. Arguments: data -- pandas DataFrame containing \'feature1\', \'feature2\', and \'feature3\' # Plot 1: Single horizontal boxen plot for \'feature1\' plt.figure(figsize=(10, 6)) sns.boxenplot(x=data[\\"feature1\\"]) plt.title(\'Horizontal Boxen Plot of Feature1\') plt.show() # Plot 2: Vertical boxen plot grouped by \'feature2\' plt.figure(figsize=(10, 6)) sns.boxenplot(x=\\"feature1\\", y=\\"feature2\\", data=data) plt.title(\'Vertical Boxen Plot grouped by Feature2\') plt.show() # Plot 3: Vertical boxen plot with color group based on \'feature3\' threshold threshold = 50 large_item = data[\\"feature3\\"].gt(threshold).rename(\\"large_item\\") plt.figure(figsize=(10, 6)) sns.boxenplot(x=\\"feature1\\", y=\\"feature2\\", hue=large_item, data=data, dodge=True) plt.title(f\'Vertical Boxen Plot grouped by Feature2 with color group (Feature3 > {threshold})\') plt.show() # Plot 4: Customized vertical boxen plot with specific customizations plt.figure(figsize=(10, 6)) sns.boxenplot( x=\\"feature1\\", y=\\"feature2\\", data=data, width=0.8, linewidth=1.5, color=\\".7\\", # Using light grey for the box color outline k_depth=\'trustworthy\', # Automatic determination of number of boxes line_kws=dict(color=\\"orange\\", linewidth=1.5), # Edges and median line customization flier_kws=dict(marker=\'o\', color=\'red\', alpha=0.5), # Outlier customization scale=\\"linear\\", # Linear scale of widths of the boxes palette=\\"pastel\\", fill=False # Unfilled boxes ) plt.title(\'Customized Vertical Boxen Plot grouped by Feature2\') plt.show()"},{"question":"# Question: Implement an HTTP Client for Data Retrieval Given the need to make HTTP requests and handle responses, implement a function `get_page_details(url: str) -> Tuple[int, str, List[Tuple[str, str]]]` that performs the following tasks: 1. Makes an HTTP GET request to the provided URL. 2. Retrieves the HTTP status code, the reason phrase, and a list of all headers. 3. Returns these details as a tuple. Additional Specifications: - Your implementation should handle both HTTP and HTTPS URLs. - If the URL is invalid or there is an issue with the connection, raise a custom exception `HTTPClientError` with an appropriate error message. - Use the `http.client` module for making the HTTP requests. # Function Signature ```python from typing import List, Tuple class HTTPClientError(Exception): pass def get_page_details(url: str) -> Tuple[int, str, List[Tuple[str, str]]]: pass ``` # Example Usage ```python try: details = get_page_details(\\"https://www.example.com\\") print(details) # Output example: (200, \'OK\', [(\'Content-Type\', \'text/html\'), ...]) except HTTPClientError as e: print(f\\"Error: {str(e)}\\") ``` # Constraints: - You cannot use third-party libraries such as `requests` for this task. - Your solution must handle HTTP redirects appropriately. - Take into account both HTTP and HTTPS protocols while constructing the client. Ensure your solution is efficient and handles potential edge cases such as invalid URLs, connection issues, and unexpected server responses gracefully.","solution":"from typing import List, Tuple import http.client from urllib.parse import urlparse, urlunparse class HTTPClientError(Exception): pass def get_page_details(url: str) -> Tuple[int, str, List[Tuple[str, str]]]: parsed_url = urlparse(url) if not parsed_url.scheme or not parsed_url.netloc: raise HTTPClientError(\\"Invalid URL\\") connection = None try: if parsed_url.scheme == \'http\': connection = http.client.HTTPConnection(parsed_url.netloc) elif parsed_url.scheme == \'https\': connection = http.client.HTTPSConnection(parsed_url.netloc) else: raise HTTPClientError(\\"Unsupported URL scheme\\") path = urlunparse((\'\', \'\', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment)) if not path: path = \'/\' connection.request(\'GET\', path) response = connection.getresponse() headers = [(k, v) for k, v in response.getheaders()] return response.status, response.reason, headers except Exception as e: raise HTTPClientError(f\\"An error occurred while making the request: {str(e)}\\") finally: if connection: connection.close()"},{"question":"Question: Dimensionality Reduction and Inverse Transform with scikit-learn # Objective Implement a class that performs dimensionality reduction on a given dataset using random projections. Your solution should demonstrate the use of both Gaussian and Sparse random projection techniques provided by `scikit-learn`, and it should also include functionality for inverse transforming the reduced data back to its original dimensionality. # Task - Implement a class `RandomProjectionHandler` with the following methods: - `__init__(self, method=\'gaussian\', n_components=None)`: Initialize the handler with either \'gaussian\' or \'sparse\' projection method and the number of components for reduction. - `fit_transform(self, X)`: Fit the random projection to the dataset `X` and return the transformed data. - `inverse_transform(self, X)`: Return the inverse-transformed data to the original dimensionality. - `get_parameters(self)`: Return the parameters used for the projection (i.e., the method and number of components). # Input and Output Formats - **Initialization**: - `method` (string): \'gaussian\' or \'sparse\' (default is \'gaussian\'). - `n_components` (int): Number of components to project to (default should let the library decide). - **fit_transform**: - **Input** `X` (numpy array of shape `(n_samples, n_features)`): The dataset to project. - **Output**: Transformed dataset (numpy array) of shape `(n_samples, n_components)`. - **inverse_transform**: - **Input** `X` (numpy array of shape `(n_samples, n_components)`): The dataset to inverse transform. - **Output**: Reconstructed dataset (numpy array) of shape `(n_samples, n_features)`. - **get_parameters**: - **Output**: Dictionary containing \'method\' and \'n_components\'. # Constraints and Limitations 1. Ensure that `fit_transform` and `inverse_transform` methods handle errors gracefully if called inappropriately. 2. The class should be able to work with high-dimensional data efficiently. # Example Usage ```python import numpy as np from sklearn.datasets import make_classification # Create a dataset X, _ = make_classification(n_samples=100, n_features=20, random_state=42) # Initialize RandomProjectionHandler with Gaussian projection rp_handler = RandomProjectionHandler(method=\'gaussian\', n_components=10) X_reduced = rp_handler.fit_transform(X) X_reconstructed = rp_handler.inverse_transform(X_reduced) print(\\"Original shape:\\", X.shape) print(\\"Reduced shape:\\", X_reduced.shape) print(\\"Reconstructed shape:\\", X_reconstructed.shape) # Check parameters params = rp_handler.get_parameters() print(\\"Projection method:\\", params[\'method\']) print(\\"Number of components:\\", params[\'n_components\']) ``` # Notes - Utilize `GaussianRandomProjection` and `SparseRandomProjection` from `sklearn.random_projection` for your implementation. - Test your class with different datasets and component sizes to ensure it works as expected.","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection class RandomProjectionHandler: def __init__(self, method=\'gaussian\', n_components=None): self.method = method self.n_components = n_components self.transformer = None if method == \'gaussian\': self.transformer = GaussianRandomProjection(n_components=n_components) elif method == \'sparse\': self.transformer = SparseRandomProjection(n_components=n_components) else: raise ValueError(\\"Unsupported projection method: choose \'gaussian\' or \'sparse\'\\") self.is_fitted = False def fit_transform(self, X): if self.transformer is not None: X_reduced = self.transformer.fit_transform(X) self.is_fitted = True return X_reduced else: raise ValueError(\\"Transformer not initialized\\") def inverse_transform(self, X): if not self.is_fitted: raise ValueError(\\"The transformer is not fitted yet. Call \'fit_transform\' first.\\") if hasattr(self.transformer, \'inverse_transform\'): return self.transformer.inverse_transform(X) else: raise NotImplementedError(\\"The inverse transformation is not implemented for this projection method.\\") def get_parameters(self): return {\'method\': self.method, \'n_components\': self.n_components}"},{"question":"# Advanced Coding Assessment: Unix User Management Problem Statement: You are provided with the `pwd` module, which gives access to the Unix user account and password database. Your task is to implement a function that finds all users whose home directory matches a given directory path and returns their command interpreters. Function Signature: ```python def find_users_by_home_directory(home_dir: str) -> List[str]: pass ``` Input: - `home_dir` (str): The home directory to search for. You can assume it always starts with a forward slash (`\'/\'`). Output: - List of command interpreters corresponding to the users with the given home directory. (List[str]) Constraints: - The function should use `pwd.getpwall()` to fetch all user entries. - You must handle edge cases such as no users matching the given home directory. - Return the command interpreters in the same order as they appear in the password database. Example: ```python # Assuming the following users in the password database: # [ # pwd.struct_passwd(pw_name=\'root\', pw_passwd=\'x\', pw_uid=0, pw_gid=0, pw_gecos=\'root\', pw_dir=\'/root\', pw_shell=\'/bin/bash\'), # pwd.struct_passwd(pw_name=\'user1\', pw_passwd=\'x\', pw_uid=1000, pw_gid=1000, pw_gecos=\'\', pw_dir=\'/home/user1\', pw_shell=\'/bin/bash\'), # pwd.struct_passwd(pw_name=\'user2\', pw_passwd=\'x\', pw_uid=1001, pw_gid=1001, pw_gecos=\'\', pw_dir=\'/home/user2\', pw_shell=\'/bin/sh\') # ] print(find_users_by_home_directory(\'/home/user1\')) # Output: [\'/bin/bash\'] ``` **Note**: You do not need to handle system-specific behaviors like the actual content of `pw_passwd`. Hints: 1. Familiarize yourself with the structure of the returned entries from `pwd.getpwall()`. 2. Filter the list of users based on the `pw_dir` attribute. 3. Extract and return the `pw_shell` attribute of the matched entries.","solution":"import pwd from typing import List def find_users_by_home_directory(home_dir: str) -> List[str]: Finds all users whose home directory matches the given directory path and returns their command interpreters. :param home_dir: The home directory to search for. :return: List of command interpreters for the users with the given home directory. matched_users_shells = [] all_users = pwd.getpwall() for user in all_users: if user.pw_dir == home_dir: matched_users_shells.append(user.pw_shell) return matched_users_shells"},{"question":"# Question: Implement a Custom Sequence Iterator **Objective:** Write a Python class `CustomSeqIter` that implements an iterator for a custom sequence. This iterator should mimic the behavior of `PySeqIter_Type` described in the documentation. **Details:** 1. The `CustomSeqIter` class should accept any sequence that supports the `__getitem__()` method. 2. The class should have an `__init__` method to initialize the sequence iterator. 3. Implement the `__iter__` method to return the iterator object (`self`). 4. Implement the `__next__` method to return the next item in the sequence. If the sequence is exhausted, it should raise a `StopIteration` exception. **Function Signature:** ```python class CustomSeqIter: def __init__(self, sequence: Sequence): pass def __iter__(self) -> \'CustomSeqIter\': pass def __next__(self) -> Any: pass ``` **Input:** - The input to `CustomSeqIter` will be a sequence (e.g., list, tuple) that supports the `__getitem__()` method. **Output:** - The `__init__` method initializes the iterator. - The `__iter__` method returns the iterator object. - The `__next__` method returns the next item in the sequence, raising `StopIteration` when the sequence is exhausted. **Constraints:** - The sequence provided will always support the `__getitem__()` method. - The sequence length may vary, and your solution should handle sequences of any length. **Example:** ```python # Example Usage: my_list = [1, 2, 3, 4] iterator = CustomSeqIter(my_list) for item in iterator: print(item) ``` **Expected Output:** ``` 1 2 3 4 ``` **Performance Requirements:** - The implementation should efficiently iterate through the sequence without causing performance bottlenecks. **Notes:** - You are free to use helper methods or attributes in your class if needed. - Ensure the class handles edge cases, such as an empty sequence.","solution":"class CustomSeqIter: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.sequence): result = self.sequence[self.index] self.index += 1 return result else: raise StopIteration"},{"question":"You are given a dataset and tasked with performing a regression analysis. To ensure your solution is effective and easily reproducible for others, you are required to create a minimal, self-contained example using scikit-learn. Your task is to: 1. Load the provided dataset (available at the specified URL). 2. Perform data preprocessing. 3. Fit a regression model from scikit-learn to the processed data. 4. Write a concise and minimal code snippet to reproduce your steps. # Dataset The dataset can be found at the following URL: [Sample Dataset](https://example.com/my_data.csv) # Requirements 1. **Data Loading**: - Load the dataset from the specified URL. 2. **Data Preprocessing**: - Split the data into features (`X`) and target (`y`). - Normalize the features using `StandardScaler` from scikit-learn. 3. **Regression Model**: - Implement and fit a `GradientBoostingRegressor` from scikit-learn to the processed data. - Evaluate the model and print the score. 4. **Minimal Code Snippet**: - Your final code should be minimal, self-contained, and executable without requiring additional explanations or dependencies other than scikit-learn and pandas. - Ensure that your script can be run by simply copying and pasting it into a Python environment. # Constraints - You should not include unnecessary steps or code. Focus on clarity and conciseness. - Your solution should use synthetic data generated using numpy once the initial dataset has been demonstrated. - Ensure that the script works independently of external files, relying on Python\'s standard libraries and scikit-learn. # Input The input to your code is the dataset loaded directly within the script. # Output Print the score of the `GradientBoostingRegressor` after fitting it to the training data. # Example Here is an example structure you can follow. Fill in the necessary details to complete the task: ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor # Load the dataset url = \\"https://example.com/my_data.csv\\" df = pd.read_csv(url) # Extract features and target X = df[[\'feature_name\']] y = df[\'target\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Normalize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Fit the GradientBoostingRegressor model model = GradientBoostingRegressor() model.fit(X_train, y_train) # Print the model score print(model.score(X_test, y_test)) # Creating synthetic data for minimal reproducible example X_synthetic = np.random.randn(100, 1) y_synthetic = np.random.randn(100) # Fitting model to synthetic data model.fit(X_synthetic, y_synthetic) print(model.score(X_synthetic, y_synthetic)) ``` Your task is to complete and refine this code.","solution":"import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor def run_regression_analysis(df): # Extract features and target X = df.drop(columns=[\'target\']) y = df[\'target\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Normalize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Fit the GradientBoostingRegressor model model = GradientBoostingRegressor() model.fit(X_train, y_train) # Print the model score score = model.score(X_test, y_test) return score # Creating synthetic data for minimal reproducible example np.random.seed(42) X_synthetic = np.random.randn(100, 5) y_synthetic = np.random.randn(100) # Convert to DataFrame for consistency with function input df_synthetic = pd.DataFrame(X_synthetic, columns=[f\'feature_{i}\' for i in range(X_synthetic.shape[1])]) df_synthetic[\'target\'] = y_synthetic # Running regression analysis on synthetic data score = run_regression_analysis(df_synthetic) print(score)"},{"question":"# Question: Implement and Evaluate a Gaussian Process Regressor with Custom Kernels You are given a regression dataset `data.csv` containing two columns: `X` (features) and `y` (target values). Your task is to implement a Gaussian Process Regressor using the scikit-learn library. Specifically, you need to: 1. **Implement a Custom Kernel**: - Combine a Radial Basis Function (RBF) kernel and a Rational Quadratic kernel using the `Sum` kernel operator. - The RBF kernel should have an initial length scale of 1.0 with bounds `(0.1, 10.0)`. - The Rational Quadratic kernel should have an initial length scale of 1.0 and an alpha parameter of 1.0, both with bounds `(0.1, 10.0)`. 2. **Train the Gaussian Process Regressor**: - Load the dataset. - Create a `GaussianProcessRegressor` instance using the custom kernel. - Use the `GaussianProcessRegressor` to fit the model on the provided dataset. 3. **Evaluate the Model**: - Predict the target values for the input features. - Calculate the mean squared error (MSE) between the predicted target values and the actual target values. - Output the learned kernel parameters and the MSE. # Expected Function Signature: ```python def train_and_evaluate_gpr(data_filepath: str) -> (dict, float): Trains a Gaussian Process Regressor with a custom kernel on the provided dataset and evaluates its performance. Parameters: data_filepath (str): The path to the CSV file containing the dataset with columns \'X\' and \'y\'. Returns: kernel_params (dict): The learned parameters of the custom kernel. mse (float): The mean squared error of the model\'s predictions. ``` # Constraints: - The dataset file is guaranteed to exist and is correctly formatted. - You must use scikit-learn\'s GaussianProcessRegressor and kernel classes. - You are allowed to use additional helper functions if necessary. # Example: ```python # Sample usage kernel_params, mse = train_and_evaluate_gpr(\'data.csv\') print(f\\"Kernel Parameters: {kernel_params}\\") print(f\\"Mean Squared Error: {mse}\\") ``` # Note: - This question tests your understanding of Gaussian processes, kernel customization, model training, and evaluation using scikit-learn.","solution":"import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, RationalQuadratic, Sum from sklearn.metrics import mean_squared_error def train_and_evaluate_gpr(data_filepath: str) -> (dict, float): # Load the dataset data = pd.read_csv(data_filepath) X = data[\'X\'].values.reshape(-1, 1) y = data[\'y\'].values # Define custom kernel: Sum of RBF and Rational Quadratic kernel = Sum( RBF(length_scale=1.0, length_scale_bounds=(0.1, 10.0)), RationalQuadratic(length_scale=1.0, alpha=1.0, length_scale_bounds=(0.1, 10.0), alpha_bounds=(0.1, 10.0)) ) # Create and train the Gaussian Process Regressor gpr = GaussianProcessRegressor(kernel=kernel) gpr.fit(X, y) # Predict target values y_pred = gpr.predict(X) # Calculate the mean squared error (MSE) mse = mean_squared_error(y, y_pred) # Extract learned kernel parameters kernel_params = gpr.kernel_.get_params() return kernel_params, mse"},{"question":"# Nearest Neighbors Classification Performance Assessment **Objective**: Implement and compare the performance of different algorithms for nearest neighbors classification. **Problem Statement**: Given a dataset, your task is to: 1. Implement a k-nearest neighbors classifier using `KNeighborsClassifier` from the `sklearn.neighbors` module. 2. Compare the performance (in terms of accuracy and computation time) of different neighbors search algorithms: `auto`, `ball_tree`, `kd_tree`, and `brute`. **Dataset**: Use the Iris dataset provided by `sklearn.datasets`. **Requirements**: 1. Load the Iris dataset and split it into training and testing sets. 2. Implement a k-nearest neighbors classifier with the following configurations: - Number of neighbors (`n_neighbors`): 5 - Different algorithms: `auto`, `ball_tree`, `kd_tree`, `brute` 3. Measure and compare the accuracy and computation time for each algorithm. 4. Plot the comparison results. **Input**: - No specific input required; use the Iris dataset directly. **Output**: - Print the accuracy and computation time for each neighbors search algorithm. - Plot a bar chart comparing the accuracy and computation time. **Constraints**: - Use `random_state=42` for splitting the dataset to ensure reproducibility. **Sample Code**: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score import time import matplotlib.pyplot as plt # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Set number of neighbors n_neighbors = 5 # List of algorithms to compare algorithms = [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'] # Initialize dictionaries to store results accuracies = {} times = {} # Loop through each algorithm and evaluate performance for algorithm in algorithms: # Initialize the classifier knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) # Measure the start time start_time = time.time() # Fit the classifier knn.fit(X_train, y_train) # Predict on the test set y_pred = knn.predict(X_test) # Measure the end time end_time = time.time() # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Store results accuracies[algorithm] = accuracy times[algorithm] = end_time - start_time # Print results print(\\"Algorithm Performance:\\") for algorithm in algorithms: print(f\\"{algorithm}: Accuracy = {accuracies[algorithm]:.4f}, Time = {times[algorithm]:.4f} seconds\\") # Plot results fig, ax1 = plt.subplots() color = \'tab:blue\' ax1.set_xlabel(\'Algorithm\') ax1.set_ylabel(\'Accuracy\', color=color) ax1.bar(algorithms, accuracies.values(), color=color) ax1.tick_params(axis=\'y\', labelcolor=color) ax2 = ax1.twinx() color = \'tab:red\' ax2.set_ylabel(\'Time (seconds)\', color=color) ax2.plot(algorithms, times.values(), color=color, marker=\'o\') ax2.tick_params(axis=\'y\', labelcolor=color) fig.tight_layout() plt.title(\'Comparison of Neighbors Search Algorithms\') plt.show() ``` Implement the above code in your own environment, analyze the results, and conclude which algorithm performs best in terms of accuracy and computation time. **Notes**: - Ensure you have the necessary libraries installed (`scikit-learn`, `matplotlib`). - Include any observations or insights from the comparison results in your report.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score import time import matplotlib.pyplot as plt def evaluate_knn_algorithms(): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Set number of neighbors n_neighbors = 5 # List of algorithms to compare algorithms = [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'] # Initialize dictionaries to store results accuracies = {} times = {} # Loop through each algorithm and evaluate performance for algorithm in algorithms: # Initialize the classifier knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) # Measure the start time start_time = time.time() # Fit the classifier knn.fit(X_train, y_train) # Predict on the test set y_pred = knn.predict(X_test) # Measure the end time end_time = time.time() # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Store results accuracies[algorithm] = accuracy times[algorithm] = end_time - start_time # Print results print(\\"Algorithm Performance:\\") for algorithm in algorithms: print(f\\"{algorithm}: Accuracy = {accuracies[algorithm]:.4f}, Time = {times[algorithm]:.4f} seconds\\") return accuracies, times def plot_results(accuracies, times): # Plot results algorithms = list(accuracies.keys()) fig, ax1 = plt.subplots() color = \'tab:blue\' ax1.set_xlabel(\'Algorithm\') ax1.set_ylabel(\'Accuracy\', color=color) ax1.bar(algorithms, accuracies.values(), color=color) ax1.tick_params(axis=\'y\', labelcolor=color) ax2 = ax1.twinx() color = \'tab:red\' ax2.set_ylabel(\'Time (seconds)\', color=color) ax2.plot(algorithms, times.values(), color=color, marker=\'o\') ax2.tick_params(axis=\'y\', labelcolor=color) fig.tight_layout() plt.title(\'Comparison of Neighbors Search Algorithms\') plt.show()"},{"question":"**Question: Implement a Reusable Logging Context Manager** In this task, you are required to implement a reusable context manager using `contextlib.ContextDecorator` that logs the entry and exit of a block of code. Additionally, this context manager should be reusable, allowing it to be used as both a context manager in `with` statements and as a function decorator. # Requirements: 1. Implement a class called `LogContext` that inherits from `contextlib.ContextDecorator`. 2. The class should log a message when entering and exiting the context using the `logging` module. 3. Ensure that the context manager can be used multiple times as a `with` statement and as a decorator. 4. Make sure to handle nested contexts correctly, ensuring the messages reflect the nested structure. # Expected Output: Logging messages should be printed with the appropriate indentation for nested contexts. # Example Usage: ```python import logging from contextlib import ContextDecorator logging.basicConfig(level=logging.INFO) class LogContext(ContextDecorator): def __init__(self, name): self.name = name self.level = 0 def __enter__(self): logging.info(f\'{\\" \\" * self.level}Entering: {self.name}\') self.level += 1 return self def __exit__(self, exc_type, exc, exc_tb): self.level -= 1 logging.info(f\'{\\" \\" * self.level}Exiting: {self.name}\') return False # Example as a context manager: with LogContext(\'Outer\'): print(\'Inside Outer\') with LogContext(\'Inner\'): print(\'Inside Inner\') # Example as a function decorator: @LogContext(\'Function\') def my_function(): print(\'Inside my_function\') my_function() ``` # Additional Constraints: 1. The context manager should work correctly with both synchronous and asynchronous functions. 2. Use the `logging` module for logging messages. 3. Ensure proper handling of exceptions, logging appropriate messages when exceptions occur. **Hints:** - Use contextlib\'s `ContextDecorator` to implement the class. - Ensure proper indentation for nested contexts. - Handle both synchronous and asynchronous usage scenarios properly.","solution":"import logging from contextlib import ContextDecorator logging.basicConfig(level=logging.INFO) class LogContext(ContextDecorator): def __init__(self, name): self.name = name self.level = 0 def __enter__(self): logging.info(f\'{\\" \\" * self.level}Entering: {self.name}\') self.level += 1 return self def __exit__(self, exc_type, exc, exc_tb): self.level -= 1 logging.info(f\'{\\" \\" * self.level}Exiting: {self.name}\') return False # Example as a context manager: with LogContext(\'Outer\'): print(\'Inside Outer\') with LogContext(\'Inner\'): print(\'Inside Inner\') # Example as a function decorator: @LogContext(\'Function\') def my_function(): print(\'Inside my_function\') my_function()"},{"question":"**Challenging Coding Assessment Question:** # Objective: Demonstrate your comprehension of the `seaborn.objects` plotting and transformation functionality, specifically focusing on the `so.Dodge` transformation. # Task: Write a function `create_custom_dodged_plot` that performs the following: 1. Loads the `tips` dataset provided by seaborn. 2. Creates a bar plot of the total bill, aggregated by the sum, separated by the day of the week and gender (using `day` on the x-axis and `sex` for color). 3. Applies the `so.Dodge` transformation with a specified gap between the bars (e.g., `gap=0.1`). 4. Handles cases where variables are not fully crossed by filling out the space in the plot (`empty=\\"fill\\"`). 5. Adds a dot layer to reflect the distribution of data points inside the bar plots using `so.Dodge` and `so.Jitter`. # Function Signature: ```python def create_custom_dodged_plot(gap: float): pass ``` # Input: - `gap` (float): The gap to add between dodged marks. # Expected Output: - The function should not return any value. Instead, it should display the customized plot inline if run in a Jupyter notebook or similar environment. # Constraints: - Ensure that all plots properly reflect the transformations and configurations specified. # Performance Requirements: - The function should efficiently handle the transformations and rendering of the plot. # Example: You need not provide specific examples as the output is graphical. However, here is the general idea of usage: ```python create_custom_dodged_plot(gap=0.1) ``` # Additional Notes: - Pay special attention to the order of transformations applied to achieve the correct visualization. - Ensure that the plot displays correctly and conveys the insights intended by the transformations applied.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_custom_dodged_plot(gap: float): # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a plot object p = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") # Add a Bar layer with Dodge transformation p = p.add(so.Bar(), so.Dodge(gap=gap, empty=\\"fill\\")) # Add Dot layer to show distribution of data points p = p.add(so.Dot(), so.Dodge(), so.Jitter()) # Display the plot p.show()"},{"question":"**Objective:** You are required to write a Python function that uses the `itertools` package to manipulate a given list of numbers and output a series of transformations. Your solution must utilize various `itertools` functions suitably. **Problem Statement:** Implement a function `transform_numbers(numbers: List[int]) -> Tuple[List[int], List[int], List[int], List[int]]` that takes a list of integers as input and performs the following transformations using `itertools`: 1. **Accumulated Sum:** Compute the accumulated sum of the list using `itertools.accumulate()` and return it as a list. 2. **Chained Sequence:** Given another list `[10, 20, 30]`, use `itertools.chain()` to chain it with the input list and return the chained sequence as a list. 3. **Compressed List:** Use `itertools.compress()` to filter the input list, keeping the elements which have even indices (0, 2, 4, ...) and return this as a list. You should create a selector list where elements at even indices are True and others are False. 4. **Permutations:** Compute all unique permutations of length 3 for the input list using `itertools.permutations()`, and return these permutations as a list of tuples. **Function Signature:** ```python from typing import List, Tuple import itertools def transform_numbers(numbers: List[int]) -> Tuple[List[int], List[int], List[int], List[int]]: # implementation here ``` **Input:** - `numbers`: A list of integers (e.g., [1, 2, 3, 4, 5]). **Output:** - A tuple containing four lists: 1. The accumulated sum list. 2. The chained sequence list. 3. The compressed list. 4. The list of tuples containing all unique length-3 permutations. **Constraints:** - The input list `numbers` will have at least 3 integers. - The elements in `numbers` will be unique. **Example:** ```python numbers = [1, 2, 3, 4, 5] transform_numbers(numbers) ``` Output: ```python ( [1, 3, 6, 10, 15], # Accumulated sum [1, 2, 3, 4, 5, 10, 20, 30], # Chained sequence [1, 3, 5], # Compressed list [(1, 2, 3), (1, 2, 4), (1, 2, 5), ...] # Permutations of length 3 ) ``` The question should prompt students to demonstrate their grasp of `itertools` by combining multiple functions and handling the input/output as per the specified transformations.","solution":"from typing import List, Tuple import itertools def transform_numbers(numbers: List[int]) -> Tuple[List[int], List[int], List[int], List[Tuple[int, int, int]]]: # 1. Accumulated Sum accumulated_sum = list(itertools.accumulate(numbers)) # 2. Chained Sequence additional_list = [10, 20, 30] chained_sequence = list(itertools.chain(numbers, additional_list)) # 3. Compressed List selector = [i % 2 == 0 for i in range(len(numbers))] compressed_list = list(itertools.compress(numbers, selector)) # 4. Permutations of length 3 permutations_list = list(itertools.permutations(numbers, 3)) return (accumulated_sum, chained_sequence, compressed_list, permutations_list)"},{"question":"Custom Color Palettes with Seaborn **Objective:** Your task is to write a Python function using Seaborn to generate and visualize custom color palettes based on specific requirements. **Problem Statement:** Create a function named `custom_palette` that: 1. Generates a HUSL color palette with customized lightness, saturation, and hue. 2. Displays a strip plot using this custom palette to visualize data from an example dataset. **Function Definition:** ```python def custom_palette(n_colors, lightness, saturation, hue_start): Generate and visualize a HUSL color palette. Parameters: n_colors (int): Number of colors to include in the palette. lightness (float): Lightness value of the colors (0 to 1). saturation (float): Saturation value of the colors (0 to 1). hue_start (float): Starting hue value (0 to 1). Returns: None # Your code here ``` **Input:** - `n_colors` (int): Number of colors in the palette, must be an integer greater than 0. - `lightness` (float): Lightness (brightness) of the colors in the palette, must be between 0 and 1. - `saturation` (float): Saturation of the colors in the palette, must be between 0 and 1. - `hue_start` (float): Starting point for hue sampling, must be between 0 and 1. **Output:** - The function should plot a strip plot using the generated color palette to visualize an example dataset from Seaborn, such as `tips`. **Constraints and Requirements:** 1. Use the `sns.husl_palette` function to generate the palette. 2. Use the Seaborn `stripplot` function to create the visualization. 3. Ensure the visualization is well-labeled and visually appealing. **Example:** Here is an example of the function call: ```python custom_palette(10, 0.5, 0.7, 0.3) ``` This would generate a strip plot with a HUSL palette of 10 colors, having a lightness of 0.5, saturation of 0.7, and starting hue of 0.3. **Additional Information:** - Review the Seaborn documentation for `sns.husl_palette` and `sns.stripplot` for additional customization options. - Ensure the function handles invalid inputs gracefully, such as values out of the specified ranges. ___ This question assesses students\' ability to: - Generate custom color palettes using Seaborn. - Integrate color palettes with data visualization functionalities. - Understand the customization options within Seaborn.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_palette(n_colors, lightness, saturation, hue_start): Generate and visualize a HUSL color palette. Parameters: n_colors (int): Number of colors to include in the palette. lightness (float): Lightness value of the colors (0 to 1). saturation (float): Saturation value of the colors (0 to 1). hue_start (float): Starting hue value (0 to 1). Returns: None # Validate inputs if not (isinstance(n_colors, int) and n_colors > 0): raise ValueError(\\"n_colors must be an integer greater than 0\\") if not (0 <= lightness <= 1): raise ValueError(\\"lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"saturation must be between 0 and 1\\") if not (0 <= hue_start <= 1): raise ValueError(\\"hue_start must be between 0 and 1\\") # Generate the palette palette = sns.husl_palette(n_colors=n_colors, h=hue_start*360, s=saturation, l=lightness) # Load the example dataset tips = sns.load_dataset(\\"tips\\") # Create the strip plot plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette) # Add labels and title plt.title(f\'HUSL Palette Strip Plot with {n_colors} colors\') plt.xlabel(\'Day\') plt.ylabel(\'Total Bill\') # Display the plot plt.show()"},{"question":"# Question: You are required to implement a custom function `optimize_model` that uses the PyTorch `torch.compiler` API to compile and optimize a given PyTorch model. The function should take as input a PyTorch model and targeted backends and should return the optimized model. It should also allow for resetting the compiler\'s state if needed and verify if the compilation process is currently active. Function Signature: ```python def optimize_model(model: torch.nn.Module, backends: List[str], reset_state: bool = False) -> torch.nn.Module: pass ``` Input: - `model`: A PyTorch model (an instance of `torch.nn.Module`) that needs to be optimized. - `backends`: A list of strings, where each string is a backend name (e.g., `[\'cpu\', \'cuda\']`) to which the model should be optimized. - `reset_state`: A boolean value. If `True`, the function should reset the compiler\'s state before starting the optimization process. Default is `False`. Output: - The optimized model (an instance of `torch.nn.Module`). Requirements: 1. The function should compile the model using the backends provided. 2. If `reset_state` is `True`, reset the compiler\'s state before starting the compilation. 3. Verify if the compilation process is currently active and print an appropriate message. 4. Use proper error handling to manage unsupported backends or compilation failures. Constraints: 1. The model must be a valid PyTorch `torch.nn.Module`. 2. The `backends` list should contain valid backend names. If an unsupported backend is provided, raise an appropriate error. 3. Performance should be considered: the function should not introduce unnecessary computation overhead. Example: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) model = SimpleModel() optimized_model = optimize_model(model, backends=[\'cpu\', \'cuda\'], reset_state=True) ``` This question assesses the student\'s ability to work with advanced PyTorch features, understand the implications of compiling and optimizing models, and manage the state and error handling in the context of model optimization.","solution":"import torch from torch import nn from typing import List def optimize_model(model: nn.Module, backends: List[str], reset_state: bool = False) -> nn.Module: Optimizes the given PyTorch model for the specified backends. Parameters: model (nn.Module): The PyTorch model to optimize. backends (List[str]): List of backend names to which the model should be optimized. reset_state (bool): If True, reset the compiler\'s state before starting the optimization. By default, it\'s False. Returns: nn.Module: The optimized model. Raises: ValueError: If an unsupported backend is provided. supported_backends = [\'cpu\', \'cuda\'] # Validate backends for backend in backends: if backend not in supported_backends: raise ValueError(f\\"Unsupported backend: {backend}\\") # Reset the compiler\'s state if needed if reset_state: torch.backends.cudnn.enabled = False torch.backends.cudnn.enabled = True # Optimize the model try: script_model = torch.jit.script(model) # Here we will optimally create script for each backend if \'cpu\' in backends: script_model = torch.jit.optimize_for_inference(script_model) if \'cuda\' in backends and torch.cuda.is_available(): model.to(\'cuda\') script_model.to(\'cuda\') except Exception as e: print(f\\"Failed to compile the model: {e}\\") return model print(\\"Model successfully compiled and optimized.\\") return script_model"},{"question":"# URL Manipulation and Query String Transformation In this coding assessment, your task is to implement a function `normalize_url` that normalizes a given URL by ensuring: 1. The scheme is in lower case. 2. The network location (`netloc`) is in lower case. 3. The returned URL should not have any fragment identifiers. 4. All parameters and query components in the URL should be sorted lexicographically by their names. Additionally, implement another function `encode_query_params` that takes a dictionary of query parameters and returns an encoded query string where: - Keys and values are percent-encoded. - Spaces in values must be replaced with plus signs (`+`). Both functions must follow the behaviors and norms specified in the `urllib.parse` module available in Python 3.10. # Function Signatures ```python def normalize_url(url: str) -> str: pass def encode_query_params(params: dict) -> str: pass ``` # Input Format Function `normalize_url`: - A single argument `url`, which is a string representing a valid URL. Function `encode_query_params`: - A single argument `params`, which is a dictionary where keys and values are strings. This dictionary represents query parameters. # Output Format Function `normalize_url`: - Returns a normalized URL string with sorted query parameters and no fragment identifier. Scheme and `netloc` must be lower case. Function `encode_query_params`: - Returns a single string representing the URL-encoded query. # Constraints - Assume all URLs provided will be well-formed according to standard parsing rules. # Examples Example 1: ```python url = \\"HTTP://www.Example.com:80/a/b?name=John&age=30&name=Alice\\" print(normalize_url(url)) # Output: \'http://www.example.com:80/a/b?age=30&name=Alice&name=John\' params = {\\"name\\": \\"John Doe\\", \\"age\\": \\"30\\"} print(encode_query_params(params)) # Output: \'name=John+Doe&age=30\' ``` Example 2: ```python url = \\"https://example.com/search?q=Hello%20world#section_2\\" print(normalize_url(url)) # Output: \'https://example.com/search?q=Hello%20world\' params = {\\"q\\": \\"Hello World\\", \\"lang\\": \\"en\\"} print(encode_query_params(params)) # Output: \'q=Hello+World&lang=en\' ``` # Note: 1. The `normalize_url` function should remove the fragment part after the `#` symbol from the URL. 2. The `encode_query_params` function should percent-encode the query parameters and join them with `&`, while replacing spaces in values with `+`.","solution":"import urllib.parse def normalize_url(url: str) -> str: # Parse the URL parsed_url = urllib.parse.urlparse(url) # Normalize scheme and netloc to lowercase scheme = parsed_url.scheme.lower() netloc = parsed_url.netloc.lower() # Encode and sort query parameters query_params = urllib.parse.parse_qsl(parsed_url.query) sorted_query_params = sorted(query_params) encoded_query = urllib.parse.urlencode(sorted_query_params, doseq=True) # Construct the normalized URL normalized_url = urllib.parse.urlunparse( (scheme, netloc, parsed_url.path, parsed_url.params, encoded_query, \'\') ) return normalized_url def encode_query_params(params: dict) -> str: # Percent-encode the query parameters encoded_params = urllib.parse.urlencode(params, doseq=True) # Replace %20 with + for spaces in values encoded_params = encoded_params.replace(\'%20\', \'+\') return encoded_params"},{"question":"**Coding Assessment Question: Exploring Regression Models with Seaborn** # Objective: The objective of this assessment is to evaluate your ability to apply seaborn\'s regression plotting functions to analyze relationships in a dataset. You will create multiple regression plots using different models, adjust plot aesthetics, and condition on additional variables. # Dataset: You are provided with the `tips` dataset from seaborn. This dataset contains information about tips given in a restaurant and includes the following columns: - `total_bill`: Total bill amount. - `tip`: Tip given. - `sex`: Gender of the person paying. - `smoker`: Whether the person is a smoker or not. - `day`: Day of the week. - `time`: Time of the day (Lunch/Dinner). - `size`: Size of the dining party. # Instructions: 1. Load the `tips` dataset using seaborn. 2. Create a simple linear regression plot of `total_bill` against `tip` using the `regplot` function. 3. Create a linear regression plot using the `lmplot` function, conditioning on whether the person is a smoker (`hue` parameter). Use different markers for smokers and non-smokers. 4. Fit a polynomial regression model of order 2 to the `size` against `tip` data using the `lmplot` function. 5. Create a logistic regression plot to show the probability of giving a `big_tip` (tips that are more than 15% of the total bill), and assign a new column `big_tip` in the dataframe for this. Use jitter for better visualization. 6. Create a nonparametric regression plot of `total_bill` against `tip` using the `lowess` smoother. 7. Use the `residplot` function to plot the residuals of a linear regression model between `total_bill` and `tip`. 8. Use the `jointplot` function to create a joint plot with a regression line for `total_bill` and `tip`. 9. Use the `pairplot` function to show the relationships between `total_bill`, `size`, and `tip`, conditioning on the `smoker` status. # Implementation: Implement the above instructions in a single Python script or Jupyter Notebook cell. You may use the following template to get started: ```python import seaborn as sns import numpy as np # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Simple linear regression plot using regplot sns.regplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) sns.plt.show() # 2. Linear regression plot using lmplot conditioning on smoker status sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", data=tips, markers=[\\"o\\", \\"x\\"], palette=\\"Set1\\") sns.plt.show() # 3. Polynomial regression model of order 2 for size against tip sns.lmplot(x=\\"size\\", y=\\"tip\\", data=tips, order=2) sns.plt.show() # 4. Logistic regression plot for big_tip tips[\\"big_tip\\"] = (tips.tip / tips.total_bill) > 0.15 sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True, y_jitter=0.03) sns.plt.show() # 5. Nonparametric regression plot using lowess smoother sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, lowess=True, line_kws={\\"color\\": \\"C1\\"}) sns.plt.show() # 6. Residual plot of total_bill against tip sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) sns.plt.show() # 7. Joint plot with regression line for total_bill and tip sns.jointplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, kind=\\"reg\\") sns.plt.show() # 8. Pair plot for total_bill, size and tip conditioning on smoker status sns.pairplot(tips, x_vars=[\\"total_bill\\", \\"size\\"], y_vars=[\\"tip\\"], hue=\\"smoker\\", height=5, aspect=0.8, kind=\\"reg\\") sns.plt.show() ``` # Submission: Submit your Python script or Jupyter Notebook with the implemented solution. Ensure that all plots are correctly displayed without any errors.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Simple linear regression plot using regplot def plot_regplot(): sns.regplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.show() # 2. Linear regression plot using lmplot conditioning on smoker status def plot_lmplot_smoker(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", data=tips, markers=[\\"o\\", \\"x\\"], palette=\\"Set1\\") plt.show() # 3. Polynomial regression model of order 2 for size against tip def plot_lmplot_poly(): sns.lmplot(x=\\"size\\", y=\\"tip\\", data=tips, order=2) plt.show() # 4. Logistic regression plot for big_tip def plot_logistic_regression(): tips[\\"big_tip\\"] = (tips.tip / tips.total_bill) > 0.15 sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True, y_jitter=0.03) plt.show() # 5. Nonparametric regression plot using lowess smoother def plot_lowess(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, lowess=True, line_kws={\\"color\\": \\"C1\\"}) plt.show() # 6. Residual plot of total_bill against tip def plot_residplot(): sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.show() # 7. Joint plot with regression line for total_bill and tip def plot_jointplot(): sns.jointplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, kind=\\"reg\\") plt.show() # 8. Pair plot for total_bill, size and tip conditioning on smoker status def plot_pairplot(): sns.pairplot(tips, x_vars=[\\"total_bill\\", \\"size\\"], y_vars=[\\"tip\\"], hue=\\"smoker\\", height=5, aspect=0.8, kind=\\"reg\\") plt.show()"},{"question":"# Question: Implement a Custom Scikit-Learn Transformer In this assessment, you will demonstrate your understanding of scikit-learn by implementing a custom transformer. The transformer should implement the standard fit and transform methods, and it should be capable of being used within a scikit-learn pipeline. Task 1. **Implement a custom transformer**: - Create a class `LogTransformer` that inherits from `BaseEstimator` and `TransformerMixin`. - The `LogTransformer` should have the following methods: - `__init__(self, columns=None)`: Initialize with an optional list of columns to apply the transformer to. - `fit(self, X, y=None)`: Fit the transformer (no-operation in this case). - `transform(self, X)`: Apply the logarithm transformation to the specified columns of `X`. 2. **Use the transformer in a pipeline**: - Create a sample dataset using pandas DataFrame. - Use the `LogTransformer` inside a scikit-learn pipeline that includes a `StandardScaler` and a `LogisticRegression` classifier. - Fit this pipeline to the sample dataset and provide the classification accuracy. Requirements 1. **Log Transformation**: Only apply `np.log1p` transformation to positive feature values of the specified columns. If no columns are specified, apply to all columns. 2. **Pipeline Integration**: Ensure that your custom transformer can be seamlessly integrated into a scikit-learn pipeline. 3. **Evaluation**: Print the classification accuracy after fitting the pipeline. Example ```python import numpy as np import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self, columns=None): self.columns = columns def fit(self, X, y=None): return self def transform(self, X): X = X.copy() cols = self.columns if self.columns else X.columns for col in cols: X[col] = np.log1p(X[col].clip(lower=0)) return X # Sample dataset data = pd.DataFrame({ \'feature1\': np.random.rand(100) * 100, \'feature2\': np.random.rand(100) * 100, \'feature3\': np.random.randint(-100, 100, size=100), \'target\': np.random.randint(0, 2, size=100) }) X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create pipeline pipeline = Pipeline([ (\'log_transform\', LogTransformer(columns=[\'feature1\', \'feature2\'])), (\'scaler\', StandardScaler()), (\'classifier\', LogisticRegression()) ]) # Fit pipeline pipeline.fit(X_train, y_train) # Predict and evaluate y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Classification Accuracy: {accuracy}\') ``` **Constraints**: - You may use only scikit-learn libraries and standard python libraries (numpy, pandas). - Ensure the transformer works with both DataFrame and numpy array inputs. **Deliverables**: Submit the python script implementing the custom transformer, the pipeline, and the code used to evaluate and print the accuracy.","solution":"import numpy as np import pandas as pd from sklearn.base import BaseEstimator, TransformerMixin from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self, columns=None): self.columns = columns def fit(self, X, y=None): return self def transform(self, X): X = X.copy() cols = self.columns if self.columns else X.columns for col in cols: X[col] = np.log1p(X[col].clip(lower=0)) return X # Sample dataset data = pd.DataFrame({ \'feature1\': np.random.rand(100) * 100, \'feature2\': np.random.rand(100) * 100, \'feature3\': np.random.randint(-100, 100, size=100), \'target\': np.random.randint(0, 2, size=100) }) X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create pipeline pipeline = Pipeline([ (\'log_transform\', LogTransformer(columns=[\'feature1\', \'feature2\'])), (\'scaler\', StandardScaler()), (\'classifier\', LogisticRegression()) ]) # Fit pipeline pipeline.fit(X_train, y_train) # Predict and evaluate y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Classification Accuracy: {accuracy}\')"},{"question":"**Bytearray Operations Assessment** You are required to implement a function that performs a series of operations on `bytearray` objects using the Python C API functions provided in the documentation. Your task is to create a C extension for Python which includes the following function: ```c PyObject* bytearray_operations(PyObject *self, PyObject *args); ``` This function should: 1. **Create a `bytearray`** from a given string. 2. **Concatenate** this `bytearray` with another `bytearray` created from a second given string. 3. **Resize** the resulting `bytearray` to a specified size. 4. **Return the contents** of the resized `bytearray` as a new Python `bytes` object. # Input - `args`: A tuple containing three elements: 1. `str1` (const char*): The first string to create the initial `bytearray`. 2. `str2` (const char*): The second string to create another `bytearray` to concatenate. 3. `size` (Py_ssize_t): The new size to which the concatenated `bytearray` should be resized. # Output - Returns a new Python `bytes` object representing the contents of the resized `bytearray`. # Constraints - If the resulting `bytearray` cannot be resized to the specified size, an appropriate error should be raised. - Ensure that all Python C API functions used perform appropriate error checking and reference counting. # Example ```python # Example usage in Python after compiling the C extension import your_extension_module result = your_extension_module.bytearray_operations(\\"hello\\", \\"world\\", 8) print(result) # Expected Output: b\'hellowor\' ``` # Performance Requirements - Efficient memory management and error handling should be demonstrated. - Ensure that the solution is optimized for both speed and safety. Good luck!","solution":"def bytearray_operations(str1, str2, size): Performs the series of operations on bytearrays. Parameters: str1 (str): The first string to create the initial bytearray. str2 (str): The second string to create another bytearray to concatenate. size (int): The new size to which the concatenated bytearray should be resized. Returns: bytes: The contents of the resized bytearray as a bytes object. # Create initial bytearrays from str1 and str2 bytearray1 = bytearray(str1, \'utf-8\') bytearray2 = bytearray(str2, \'utf-8\') # Concatenate the bytearrays combined_bytearray = bytearray1 + bytearray2 # Resize the bytearray if size > len(combined_bytearray): combined_bytearray.extend(b\'x00\' * (size - len(combined_bytearray))) else: combined_bytearray = combined_bytearray[:size] # Return the bytes object of the resized bytearray return bytes(combined_bytearray)"},{"question":"Objective: Demonstrate your understanding of seaborn\'s regression functionalities by creating a series of regression plots that explore relationships in a complex dataset. Task: 1. **Load the Datasets:** - Load the `tips` dataset from seaborn. 2. **Simple Linear Regression:** - Create a simple linear regression plot using `lmplot` for the relationship between `total_bill` and `tip`. 3. **Polynomial Regression:** - Create a polynomial regression plot (order 3) for the relationship between `total_bill` and `tip`. 4. **Residual Analysis:** - Create a residual plot using `residplot` for the linear regression fit between `total_bill` and `tip`. 5. **Faceted Regression Analysis:** - Create a faceted regression plot using `lmplot` to analyze the relationship between `total_bill` and `tip`, for different meal times (`time`) and different sex categories. Use `col` for `time` and `row` for `sex`. 6. **Logistic Regression:** - Add a binary column `big_tip` to the `tips` dataset which indicates whether the tip is greater than 15% of the total bill. - Create a logistic regression plot for the relationship between `total_bill` and `big_tip`. Input: None. The question doesn\'t require inputs as the datasets are loaded directly using seaborn. Output: The various plots described in the tasks. Constraints: - Use seaborn for all plotting. - Ensure the plots are properly labeled and visually clear. - Follow best practices to enhance plot readability (e.g., appropriate titles, axis labels, legends). Example: ```python import seaborn as sns import numpy as np import matplotlib.pyplot as plt # Load the datasets tips = sns.load_dataset(\\"tips\\") # Task 2: Simple Linear Regression sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Simple Linear Regression: Total Bill vs Tip\\") plt.show() # Task 3: Polynomial Regression sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, order=3) plt.title(\\"Polynomial Regression (Order 3): Total Bill vs Tip\\") plt.show() # Task 4: Residual Analysis sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Residual Plot: Total Bill vs Tip\\") plt.show() # Task 5: Faceted Regression Analysis sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", col=\\"time\\", data=tips) plt.suptitle(\\"Faceted Regression Analysis\\", y=1.03) plt.show() # Task 6: Logistic Regression tips[\\"big_tip\\"] = (tips.tip / tips.total_bill) > 0.15 sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True) plt.title(\\"Logistic Regression: Total Bill vs Big Tip\\") plt.show() ``` Note: Make sure to include appropriate titles and axis labels to make each plot informative and easy to understand.","solution":"import seaborn as sns import numpy as np import matplotlib.pyplot as plt # Load the datasets tips = sns.load_dataset(\\"tips\\") # Task 2: Simple Linear Regression def plot_simple_linear_regression(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Simple Linear Regression: Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() # Task 3: Polynomial Regression def plot_polynomial_regression(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, order=3) plt.title(\\"Polynomial Regression (Order 3): Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() # Task 4: Residual Analysis def plot_residual_analysis(): sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Residual Plot: Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Residuals\\") plt.show() # Task 5: Faceted Regression Analysis def plot_faceted_regression_analysis(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", col=\\"time\\", data=tips) plt.suptitle(\\"Faceted Regression Analysis by Time and Sex\\", y=1.03) plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() # Task 6: Logistic Regression def plot_logistic_regression(): tips[\\"big_tip\\"] = (tips.tip / tips.total_bill) > 0.15 sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True) plt.title(\\"Logistic Regression: Total Bill vs Big Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Big Tip (Tip > 15% of Total Bill)\\") plt.show()"},{"question":"**Objective:** Implement a REPL (Read-Eval-Print Loop) simulator to handle user-input Python code dynamically using the `code` module. This simulator should provide essential interactive interpreter functionalities such as multi-line code execution, syntax error handling, and proper prompts. **Instructions:** 1. Design a class `SimpleREPL` that extends `code.InteractiveConsole` to emulate a basic Python interactive console. 2. Implement the REPL to: - Accept multi-line code inputs. - Automatically detect if the input is complete or if more input is needed. - Execute the code and handle any syntax or runtime errors gracefully. - Display appropriate prompts (`sys.ps1` or `sys.ps2`) based on the state of input. **Requirements:** 1. Create a class `SimpleREPL`, which extends `code.InteractiveConsole`. 2. Implement the initializer `__init__(self, locals=None, filename=\'<console>\')` to set up the REPL. 3. Implement a method `loop(self)` which starts the interactive console. 4. Use `push(self, line)` method to handle the input lines and execute the code as required. 5. Override and implement `write(self, data)` method to display errors on standard error (`sys.stderr`). **Constraints:** - You must use the `code` module methods and classes described in the documentation. - Use default system prompts for the REPL (`sys.ps1` and `sys.ps2`). **Input and Output:** - **Input**: Multi-line Python code from the user. - **Output**: Execute the code, print results, and handle errors as they occur. **Performance Requirements:** - The REPL should handle typical interactive console inputs efficiently. - Error handling should be robust to keep the interactive session running even after encountering errors. **Example Usage:** ```python repl = SimpleREPL() repl.loop() ``` The REPL should behave as a Python interactive console, allowing users to input and execute code, handle multi-line constructs, and provide appropriate prompts based on the state of the input. **Starter Code:** ```python import code import sys class SimpleREPL(code.InteractiveConsole): def __init__(self, locals=None, filename=\'<console>\'): super().__init__(locals=locals, filename=filename) def write(self, data): sys.stderr.write(data) def loop(self): raise NotImplementedError(\\"You need to implement this method\\") # Example usage # repl = SimpleREPL() # repl.loop() ```","solution":"import code import sys class SimpleREPL(code.InteractiveConsole): def __init__(self, locals=None, filename=\'<console>\'): super().__init__(locals=locals, filename=filename) def write(self, data): sys.stderr.write(data) def loop(self): import traceback sys.ps1 = \'>>> \' sys.ps2 = \'... \' while True: try: line = input(sys.ps1 if self.buffer else sys.ps2) if self.push(line): continue except EOFError: print(\\"Exiting REPL\\") break except KeyboardInterrupt: print(\\"KeyboardInterrupt\\") self.resetbuffer() except Exception: traceback.print_exc() self.resetbuffer() # Example usage # repl = SimpleREPL() # repl.loop()"},{"question":"You are tasked with improving a command-line based interactive Python session manager by implementing functionality to load, save, and manipulate command history using the `readline` module. Write a class `InteractiveSessionManager` that provides the following methods: 1. **Initialization**: - `__init__(self, history_file: str)`: - Initializes the instance and sets up the history file path. 2. **Configuration**: - `configure_tab_completion(self)`: - Configures readline to use tab for completion. 3. **History Management**: - `load_history(self)`: - Loads the command history from the history file. - `save_history(self)`: - Saves the current session history to the history file. - `clear_history(self)`: - Clears the current session history. - `get_history(self) -> list`: - Returns the current history as a list of strings. - `add_to_history(self, command: str)`: - Adds a command to the history. - `remove_from_history(self, index: int)`: - Removes a command from history at the specified 1-based index. - `replace_in_history(self, index: int, new_command: str)`: - Replaces a command in history at the specified 1-based index with `new_command`. # Constraints 1. The `history_file` parameter in the constructor must be a valid path to a writable file. 2. Commands are strings, and indexes for history manipulation are 1-based, i.e., the first command has index `1`. # Example Usage ```python manager = InteractiveSessionManager(\\"~/.python_history\\") manager.configure_tab_completion() manager.load_history() # Add command to history manager.add_to_history(\\"print(\'Hello, World!\')\\") # Get and print current history history = manager.get_history() print(history) # [\'print(\\"Hello, World!\\")\'] # Replace a command in the history manager.replace_in_history(1, \\"print(\'Hello, Python!\')\\") history = manager.get_history() print(history) # [\'print(\\"Hello, Python!\\")\'] # Remove command from history manager.remove_from_history(1) history = manager.get_history() print(history) # [] # Save history to file manager.save_history() ``` Complete the implementation of the `InteractiveSessionManager` class as described.","solution":"import readline import os class InteractiveSessionManager: def __init__(self, history_file: str): self.history_file = os.path.expanduser(history_file) def configure_tab_completion(self): readline.parse_and_bind(\'tab: complete\') def load_history(self): if os.path.exists(self.history_file): readline.read_history_file(self.history_file) def save_history(self): readline.write_history_file(self.history_file) def clear_history(self): readline.clear_history() def get_history(self) -> list: return [readline.get_history_item(i) for i in range(1, readline.get_current_history_length() + 1)] def add_to_history(self, command: str): readline.add_history(command) def remove_from_history(self, index: int): if 1 <= index <= readline.get_current_history_length(): readline.remove_history_item(index - 1) def replace_in_history(self, index: int, new_command: str): if 1 <= index <= readline.get_current_history_length(): readline.replace_history_item(index - 1, new_command)"},{"question":"Regex-Based Parser for Custom Configuration Files Objective You are tasked with developing a Python function that parses custom configuration files and extracts specific details using the Python `re` module. This exercise assesses your comprehension of fundamental and advanced regular expressions concepts, such as grouping, lookahead assertions, and named groups. Problem Statement A custom configuration file contains multiple entries for server configurations. Each entry is structured like this: ``` [ServerName] IP = <ip_address> Port = <port_number> Status = <status> ``` Where: - `<ServerName>` is a string consisting of alphanumeric characters and underscores. - `<ip_address>` follows the format `xxx.xxx.xxx.xxx` where `xxx` ranges from 0 to 255. - `<port_number>` is an integer between 0 and 65535. - `<status>` can be either \\"ACTIVE\\" or \\"INACTIVE\\". Task Implement a function `parse_config(config_text: str) -> dict` that parses the configuration text and extracts the information into a dictionary. Requirements 1. **Function Signature**: ```python def parse_config(config_text: str) -> dict: ``` 2. **Input**: - `config_text`: A string containing the multi-line configuration text. 3. **Output**: - A dictionary mapping each server name to its configuration details as sub-dictionaries. 4. **Constraints**: - The input configuration text will always follow the given structure. - There will be no duplicate server names. - Valid IP addresses, port numbers, and statuses are assured. - Assume no leading or trailing whitespace for entries and values. 5. **Example**: ```python config_text = [Server1] IP = 192.168.1.1 Port = 8080 Status = ACTIVE [Server2] IP = 10.0.0.5 Port = 22 Status = INACTIVE expected_output = { \\"Server1\\": { \\"IP\\": \\"192.168.1.1\\", \\"Port\\": \\"8080\\", \\"Status\\": \\"ACTIVE\\" }, \\"Server2\\": { \\"IP\\": \\"10.0.0.5\\", \\"Port\\": \\"22\\", \\"Status\\": \\"INACTIVE\\" } } result = parse_config(config_text) assert result == expected_output ``` Notes - Use regular expressions to parse the strings. - Utilize techniques such as named groups and lookahead assertions where appropriate. # Solution Template ```python import re def parse_config(config_text: str) -> dict: config_pattern = re.compile(r [(?P<ServerName>w+)]s* IPs*=s*(?P<IP>d{1,3}(?:.d{1,3}){3})s* Ports*=s*(?P<Port>d{1,5})s* Statuss*=s*(?P<Status>ACTIVE|INACTIVE)s* , re.VERBOSE) configs = {} for match in config_pattern.finditer(config_text): server_name = match.group(\'ServerName\') configs[server_name] = { \\"IP\\": match.group(\'IP\'), \\"Port\\": match.group(\'Port\'), \\"Status\\": match.group(\'Status\') } return configs ```","solution":"import re def parse_config(config_text: str) -> dict: config_pattern = re.compile(r [(?P<ServerName>w+)]s* IPs*=s*(?P<IP>(?:d{1,3}.){3}d{1,3})s* Ports*=s*(?P<Port>d{1,5})s* Statuss*=s*(?P<Status>ACTIVE|INACTIVE)s* , re.VERBOSE) configs = {} for match in config_pattern.finditer(config_text): server_name = match.group(\'ServerName\') configs[server_name] = { \\"IP\\": match.group(\'IP\'), \\"Port\\": match.group(\'Port\'), \\"Status\\": match.group(\'Status\') } return configs"},{"question":"**Email Management Using Python\'s \\"email\\" Package** **Objective:** Implement an email message manager using Python\'s `email` package. Your task is to design a class that performs various email message operations such as creating, parsing, modifying, and serializing emails. **Class**: `EmailManager` # Methods: 1. **`create_email(subject: str, sender: str, recipient: str, body: str, attachments: Optional[List[Tuple[str, bytes]]] = None) -> EmailMessage:`** - Creates a new email message with the specified details. - **Parameters:** - `subject` (str): The subject of the email. - `sender` (str): Email address of the sender. - `recipient` (str): Email address of the recipient. - `body` (str): The main text body of the email. - `attachments` (Optional[List[Tuple[str, bytes]]]): A list of tuples where each tuple contains (filename, file content) for attachments. Default is None. - **Returns:** `EmailMessage` object representing the created email. 2. **`parse_email(raw_email: bytes) -> EmailMessage:`** - Parses a raw email in bytes format into an `EmailMessage` object. - **Parameters:** - `raw_email` (bytes): The raw email bytes to be parsed. - **Returns:** `EmailMessage` object representing the parsed email. 3. **`add_attachment(email: EmailMessage, filename: str, content: bytes):`** - Adds an attachment to an existing email message. - **Parameters:** - `email` (`EmailMessage`): The `EmailMessage` object to which the attachment will be added. - `filename` (str): The name of the attachment file. - `content` (bytes): The content of the attachment file. - **Returns:** `None`. 4. **`serialize_email(email: EmailMessage) -> bytes:`** - Serializes an `EmailMessage` object into raw email bytes. - **Parameters:** - `email` (`EmailMessage`): The `EmailMessage` object to be serialized. - **Returns:** `bytes` representing the serialized email. 5. **`modify_body(email: EmailMessage, new_body: str):`** - Modifies the body content of an existing email message. - **Parameters:** - `email` (`EmailMessage`): The `EmailMessage` object to be modified. - `new_body` (str): The new text body to replace the existing one. - **Returns:** `None`. # Example Usage: ```python manager = EmailManager() # 1. Create an email email = manager.create_email( subject=\\"Hello, World!\\", sender=\\"sender@example.com\\", recipient=\\"recipient@example.com\\", body=\\"This is a test email.\\", attachments=[(\\"test.txt\\", b\\"Test file content\\")] ) # 2. Serialize and parse the email raw_email = manager.serialize_email(email) parsed_email = manager.parse_email(raw_email) # 3. Modify the email body manager.modify_body(parsed_email, \\"Updated email content.\\") # 4. Add another attachment manager.add_attachment(parsed_email, \\"new_attachment.txt\\", b\\"New attachment content\\") # 5. Serialize final email final_raw_email = manager.serialize_email(parsed_email) ``` # Constraints: - Ensure that all email manipulation adheres to RFC 5322 and RFC 6532. - Handle and raise appropriate exceptions for any errors that occur during parsing or serialization. - Validate email addresses for correct formatting. **Performance Requirements:** - The solution should efficiently handle emails up to 25MB in size including attachments. - Parsing and serialization operations must complete within a reasonable time frame for an email up to this size. Implement the `EmailManager` class based on the above specifications. Ensure your implementation is robust, efficient, and adheres to the mentioned constraints.","solution":"from email.message import EmailMessage from email import policy from email.parser import BytesParser import mimetypes class EmailManager: def create_email(self, subject: str, sender: str, recipient: str, body: str, attachments=None) -> EmailMessage: email = EmailMessage() email[\'Subject\'] = subject email[\'From\'] = sender email[\'To\'] = recipient email.set_content(body) if attachments: for filename, content in attachments: maintype, subtype = mimetypes.guess_type(filename)[0].split(\'/\', 1) email.add_attachment(content, maintype=maintype, subtype=subtype, filename=filename) return email def parse_email(self, raw_email: bytes) -> EmailMessage: return BytesParser(policy=policy.default).parsebytes(raw_email) def add_attachment(self, email: EmailMessage, filename: str, content: bytes): maintype, subtype = mimetypes.guess_type(filename)[0].split(\'/\', 1) email.add_attachment(content, maintype=maintype, subtype=subtype, filename=filename) def serialize_email(self, email: EmailMessage) -> bytes: return email.as_bytes() def modify_body(self, email: EmailMessage, new_body: str): email.set_content(new_body)"},{"question":"**Question: Kernel Density Estimation Analysis and Application Using Scikit-Learn** You are given a dataset of geographic coordinates representing the locations of different events. Your task is to perform kernel density estimation on this dataset to visualize the density of these events over a geographic region. Specifically, you will: 1. Load a set of coordinates from a file. 2. Fit a KernelDensity model to the data using different kernels and bandwidths. 3. Visualize the density estimation results on a map. **Input:** 1. A text file `coordinates.txt` with each line containing a pair of latitude and longitude, separated by a comma. 2. A set of parameters: `kernels = [\'gaussian\', \'tophat\', \'epanechnikov\']` and `bandwidths = [0.01, 0.1, 1.0]`. **Output:** Three visualizations (one for each kernel) showing the KDE results on a map. Each visualization should compare the results for the three different bandwidths. **Constraints:** 1. The coordinates in `coordinates.txt` must be valid latitude and longitude pairs. 2. Ensure that the visualizations are properly scaled and labeled for clear interpretation. **Requirements:** 1. Implement a function `load_coordinates(file_path)` to read the coordinates from the given file. 2. Implement a function `apply_kde(coordinates, kernel, bandwidth)` that applies the KDE with the specified kernel and bandwidth. 3. Implement a function `visualize_density(coordinates, kde_models, filename)` that visualizes the KDE results on a map. 4. Ensure your code is optimized and runs efficiently for large datasets. You can use the following template as a starting point: ```python import numpy as np from sklearn.neighbors import KernelDensity import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap def load_coordinates(file_path): Load coordinates from the given file. Args: file_path (str): Path to the file containing coordinates. Returns: np.ndarray: Array of coordinates. coordinates = [] with open(file_path, \'r\') as file: for line in file: lat, lon = map(float, line.strip().split(\',\')) coordinates.append([lat, lon]) return np.array(coordinates) def apply_kde(coordinates, kernel, bandwidth): Perform KDE on the given coordinates with specified kernel and bandwidth. Args: coordinates (np.ndarray): Array of coordinates. kernel (str): Kernel type. bandwidth (float): Bandwidth for the KDE. Returns: KernelDensity: Fitted KDE model. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(coordinates) return kde def visualize_density(coordinates, kde_models, filename): Visualize the density estimation results on a map. Args: coordinates (np.ndarray): Array of coordinates. kde_models (list): List of fitted KDE models. filename (str): Output filename for the visualization. fig, axes = plt.subplots(1, len(kde_models), figsize=(15, 5)) m = Basemap(projection=\'mill\', lon_0=0, ax=axes[0]) x, y = m(coordinates[:, 1], coordinates[:, 0]) for ax, kde in zip(axes, kde_models): score_samples = np.exp(kde.score_samples(coordinates)) ax.scatter(x, y, c=score_samples, cmap=\'viridis\', s=5) ax.set_title(f\'KDE with kernel={kde.kernel}, bw={kde.bandwidth}\') plt.savefig(filename) plt.show() if __name__ == \\"__main__\\": # Load coordinates coordinates = load_coordinates(\'coordinates.txt\') # Define parameters kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.01, 0.1, 1.0] # Apply KDE and visualize results for kernel in kernels: kde_models = [apply_kde(coordinates, kernel, bw) for bw in bandwidths] visualize_density(coordinates, kde_models, f\'kde_{kernel}.png\') ``` Note: Ensure that the generated visualizations clearly show the effects of different kernels and bandwidths on density estimation.","solution":"import numpy as np from sklearn.neighbors import KernelDensity import matplotlib.pyplot as plt def load_coordinates(file_path): Load coordinates from the given file. Args: file_path (str): Path to the file containing coordinates. Returns: np.ndarray: Array of coordinates. coordinates = [] with open(file_path, \'r\') as file: for line in file: lat, lon = map(float, line.strip().split(\',\')) coordinates.append([lat, lon]) return np.array(coordinates) def apply_kde(coordinates, kernel, bandwidth): Perform KDE on the given coordinates with specified kernel and bandwidth. Args: coordinates (np.ndarray): Array of coordinates. kernel (str): Kernel type. bandwidth (float): Bandwidth for the KDE. Returns: KernelDensity: Fitted KDE model. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(coordinates) return kde def visualize_density(coordinates, kde_models, kernel, filename): Visualize the density estimation results on a map. Args: coordinates (np.ndarray): Array of coordinates. kde_models (list): List of fitted KDE models. kernel (str): Kernel type. filename (str): Output filename for the visualization. fig, axes = plt.subplots(1, len(kde_models), figsize=(15, 5)) for ax, (kde, bandwidth) in zip(axes, kde_models): x_min, x_max = coordinates[:, 1].min(), coordinates[:, 1].max() y_min, y_max = coordinates[:, 0].min(), coordinates[:, 0].max() x_grid, y_grid = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)) xy_grid = np.vstack([y_grid.ravel(), x_grid.ravel()]).T z = np.exp(kde.score_samples(xy_grid)).reshape(100, 100) ax.imshow(z, origin=\'lower\', aspect=\'auto\', extent=[x_min, x_max, y_min, y_max], cmap=\'viridis\') ax.scatter(coordinates[:, 1], coordinates[:, 0], c=\'red\', s=5) ax.set_title(f\'Kernel={kernel}, Bandwidth={bandwidth}\') plt.tight_layout() plt.savefig(filename) plt.show() if __name__ == \\"__main__\\": # Load coordinates coordinates = load_coordinates(\'coordinates.txt\') # Define parameters kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.01, 0.1, 1.0] # Apply KDE and visualize results for kernel in kernels: kde_models = [(apply_kde(coordinates, kernel, bw), bw) for bw in bandwidths] visualize_density(coordinates, kde_models, kernel, f\'kde_{kernel}.png\')"},{"question":"# Assessing Python Built-in Types and Methods Your task is to implement a function named `analyze_sequence_data` that performs various operations on a list of sequences and dictionaries provided as input. The function should: 1. **Identify the type**: Determine whether each element in the list is of type `list`, `tuple`, `set`, `str`, `dict`, or other types. 2. **Process each identified type**: Perform operations specific to each type: - **List**: Reverse the list and remove duplicates while maintaining the order of first appearance. - **Tuple**: Convert the tuple to a list and sort it. - **Set**: Add a given element (passed as an argument) to the set. - **String**: Convert the string to uppercase and count the number of vowels. - **Dictionary**: For dictionaries, invert the keys and values. If the dictionary has non-unique values, resolve conflicts by creating lists of keys. 3. **Return a summary**: Create and return a dictionary summarizing the results of each operation. The summary dictionary should include: - The original type of each element. - The processed result of each element. **Input Format:** - A list of elements. Each element can be of various types (list, tuple, set, str, dict). - An additional argument: `element_to_add` which is an element to be added to any sets found in the input list. **Output Format:** - A dictionary summarizing the type and processed result of each input element. Example: ```python input_data = [ [1, 2, 2, 3], (3, 2, 1), {\\"a\\": 1, \\"b\\": 2, \\"c\\": 1}, \\"hello\\", {1, 2, 3} ] element_to_add = 4 result = analyze_sequence_data(input_data, element_to_add) # Expected Output: # { # 0: {\\"type\\": \\"list\\", \\"processed_result\\": [1, 2, 3]}, # 1: {\\"type\\": \\"tuple\\", \\"processed_result\\": [1, 2, 3]}, # 2: {\\"type\\": \\"dict\\", \\"processed_result\\": {1: [\\"a\\", \\"c\\"], 2: \\"b\\"}}, # 3: {\\"type\\": \\"str\\", \\"processed_result\\": {\\"uppercase\\": \\"HELLO\\", \\"vowel_count\\": 2}}, # 4: {\\"type\\": \\"set\\", \\"processed_result\\": {1, 2, 3, 4}} # } ``` # Implementation Requirements 1. Define the function `analyze_sequence_data(elements: list, element_to_add: Any) -> dict`. 2. Accurately identify and process each type according to the specifications. 3. Ensure the function handles edge cases, such as empty inputs or sequences. # Constraints - The input elements list will not contain NoneType elements. - Assume the elements are all hashable and can be used as dictionary keys. - Aim for an efficient solution with minimal computational overhead. Good luck!","solution":"def analyze_sequence_data(elements, element_to_add): Analyzes a list of elements and performs operations based on their types. Args: elements (list): A list of elements that can be of type list, tuple, set, str, or dict. element_to_add (object): An element to be added to any sets found in the input list. Returns: dict: A summary dictionary with the type and processed result of each input element. def remove_duplicates(sequence): seen = set() return [x for x in sequence if not (x in seen or seen.add(x))] def count_vowels(s): vowels = set(\'aeiouAEIOU\') return sum(1 for char in s if char in vowels) summary = {} for i, element in enumerate(elements): if isinstance(element, list): reversed_list = remove_duplicates(list(reversed(element))) summary[i] = {\\"type\\": \\"list\\", \\"processed_result\\": reversed_list} elif isinstance(element, tuple): sorted_list = sorted(list(element)) summary[i] = {\\"type\\": \\"tuple\\", \\"processed_result\\": sorted_list} elif isinstance(element, set): element.add(element_to_add) summary[i] = {\\"type\\": \\"set\\", \\"processed_result\\": element} elif isinstance(element, str): uppercase_str = element.upper() vowel_count = count_vowels(element) summary[i] = {\\"type\\": \\"str\\", \\"processed_result\\": {\\"uppercase\\": uppercase_str, \\"vowel_count\\": vowel_count}} elif isinstance(element, dict): inverted_dict = {} for key, value in element.items(): if value in inverted_dict: if isinstance(inverted_dict[value], list): inverted_dict[value].append(key) else: inverted_dict[value] = [inverted_dict[value], key] else: inverted_dict[value] = key summary[i] = {\\"type\\": \\"dict\\", \\"processed_result\\": inverted_dict} else: summary[i] = {\\"type\\": \\"other\\", \\"processed_result\\": str(element)} return summary"},{"question":"Objective Write a Python function using the `grp` module to retrieve all unique user names from the group database. The function should return a sorted list of user names who are members of any group. Requirements 1. The function should be named `get_all_user_names`. 2. It should use the `grp.getgrall()` function to access all group entries. 3. The output should be a sorted list of unique user names. 4. Handle exceptions that could be raised by `grp.getgrall`. Input The function does not take any input parameters. Output The function should return a list of strings representing user names, sorted in ascending order. Example ```python # Example return value assuming the group database has the following entries: # [ (\'grp1\', \'\', 1001, [\'alice\', \'bob\']), (\'grp2\', \'\', 1002, [\'charlie\', \'alice\']) ] # Output: [\'alice\', \'bob\', \'charlie\'] print(get_all_user_names()) ``` Constraints 1. You may assume that the `grp` module is available and can be imported. 2. The `grp` module functions may raise `KeyError` if a group entry is not found or `TypeError` for invalid input types, which should be properly handled. 3. Ensure that the function logic is efficient given the number of entries in a typical Unix group database. Solution Template ```python import grp def get_all_user_names(): try: group_entries = grp.getgrall() user_names = set() for entry in group_entries: user_names.update(entry.gr_mem) return sorted(user_names) except (KeyError, TypeError) as e: print(f\\"An error occurred: {str(e)}\\") return [] # Uncomment the following line to test with actual data # print(get_all_user_names()) ```","solution":"import grp def get_all_user_names(): Retrieves all unique user names from the group database and returns them sorted in ascending order. Returns: list: A sorted list of unique user names. try: group_entries = grp.getgrall() user_names = set() for entry in group_entries: user_names.update(entry.gr_mem) return sorted(user_names) except (KeyError, TypeError) as e: print(f\\"An error occurred: {str(e)}\\") return []"},{"question":"# Database Manipulation using the `dbm` Module You are tasked with creating a utility to manage a small database of user information using Python\'s `dbm` module. The database will store user records where the key is the user ID (as a string) and the value is the user\'s full name (also as a string). Your task is to implement a function `manage_user_records(db_file, records, action)` that takes the following parameters: - `db_file`: The name of the database file. - `records`: A list of tuples where each tuple represents a user record (user ID, full name). - `action`: A string specifying the action to perform on the records - it can be one of `\'create\'`, `\'update\'`, `\'delete\'`, or `\'read_all\'`. The function should perform the following actions based on the `action` parameter: 1. **\'create\'**: Create the database (if it doesn\'t exist) and add the given user records. Overwrite entries if the user ID already exists. 2. **\'update\'**: Update the existing user records in the database. If a user ID does not exist in the database, it should not be added. 3. **\'delete\'**: Delete the user records with the provided user IDs from the database. 4. **\'read_all\'**: Read and return all user records from the database as a dictionary with user IDs as keys and full names as values. The function should handle the conversion between strings and bytes as required by the `dbm` module. # Example ```python def manage_user_records(db_file, records, action): import dbm if action not in [\'create\', \'update\', \'delete\', \'read_all\']: raise ValueError(\\"Invalid action specified\\") with dbm.open(db_file, \'c\') as db: if action == \'create\': for user_id, full_name in records: db[user_id.encode(\'utf-8\')] = full_name.encode(\'utf-8\') elif action == \'update\': for user_id, full_name in records: if user_id.encode(\'utf-8\') in db: db[user_id.encode(\'utf-8\')] = full_name.encode(\'utf-8\') elif action == \'delete\': for user_id, _ in records: try: del db[user_id.encode(\'utf-8\')] except KeyError: pass elif action == \'read_all\': return {key.decode(\'utf-8\'): db[key].decode(\'utf-8\') for key in db.keys()} # Example usage: # - Create user records manage_user_records(\'users_db\', [(\'id1\', \'Alice\'), (\'id2\', \'Bob\')], \'create\') # - Update existing records manage_user_records(\'users_db\', [(\'id1\', \'Alice A. Smith\')], \'update\') # - Read all records print(manage_user_records(\'users_db\', [], \'read_all\')) # - Delete user records manage_user_records(\'users_db\', [(\'id2\', \'Bob\')], \'delete\') ``` # Notes - Ensure the implementation uses the appropriate modes when opening the database to support the specified actions. - Handle byte-string conversion appropriately to ensure compatibility with the `dbm` module. - The function should raise appropriate exceptions for unsupported actions.","solution":"def manage_user_records(db_file, records, action): import dbm if action not in [\'create\', \'update\', \'delete\', \'read_all\']: raise ValueError(\\"Invalid action specified\\") with dbm.open(db_file, \'c\') as db: if action == \'create\': for user_id, full_name in records: db[user_id.encode(\'utf-8\')] = full_name.encode(\'utf-8\') elif action == \'update\': for user_id, full_name in records: if user_id.encode(\'utf-8\') in db: db[user_id.encode(\'utf-8\')] = full_name.encode(\'utf-8\') elif action == \'delete\': for user_id, _ in records: try: del db[user_id.encode(\'utf-8\')] except KeyError: pass elif action == \'read_all\': return {key.decode(\'utf-8\'): db[key].decode(\'utf-8\') for key in db.keys()} # Example usage: # - Create user records manage_user_records(\'users_db\', [(\'id1\', \'Alice\'), (\'id2\', \'Bob\')], \'create\') # - Update existing records manage_user_records(\'users_db\', [(\'id1\', \'Alice A. Smith\')], \'update\') # - Read all records print(manage_user_records(\'users_db\', [], \'read_all\')) # - Delete user records manage_user_records(\'users_db\', [(\'id2\', \'Bob\')], \'delete\')"},{"question":"# Configuration File Parser Challenge For this exercise, you are required to demonstrate your understanding of the Python `configparser` module by processing a configuration file. The goal is to write a function that reads a configuration file, modifies its contents based on given parameters, and writes the updated contents back to the file. Function Signature ```python import configparser def update_config_file(filepath: str, section: str, updates: dict) -> None: Updates the given section of a configuration file with the provided key-value pairs. Args: - filepath (str): The path to the configuration file. - section (str): The section in the configuration file to be updated. - updates (dict): A dictionary containing key-value pairs to update in the specified section. Returns: - None ``` Input - `filepath`: A string representing the path to the configuration file. - `section`: A string representing the section of the configuration file to be updated. - `updates`: A dictionary containing key-value pairs where each key is the setting name to be updated or added, and each value is the new value for that setting. Output - The function should not return anything. Instead, it should directly modify the configuration file at `filepath`. Example Configuration File (`config.ini`) ```ini [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [bitbucket.org] User = hg [topsecret.server.com] Port = 50022 ForwardX11 = no ``` Example Usage ```python # Function Call update_config_file(\'config.ini\', \'DEFAULT\', {\'ServerAliveInterval\': \'60\', \'ForwardX11\': \'no\'}) # The \'config.ini\' file should now have updated contents: [DEFAULT] ServerAliveInterval = 60 Compression = yes CompressionLevel = 9 ForwardX11 = no [bitbucket.org] User = hg [topsecret.server.com] Port = 50022 ForwardX11 = no ``` Constraints - You can assume that the section to be updated always exists in the configuration file. - The updates dictionary will not be empty and will contain valid string values. Use the `configparser` module to implement the function `update_config_file`. Ensure that the configuration file is properly read, updated, and saved back without losing any original formatting or comments that might be present outside the scope of this example.","solution":"import configparser def update_config_file(filepath: str, section: str, updates: dict) -> None: Updates the given section of a configuration file with the provided key-value pairs. Args: - filepath (str): The path to the configuration file. - section (str): The section in the configuration file to be updated. - updates (dict): A dictionary containing key-value pairs to update in the specified section. Returns: - None config = configparser.ConfigParser() config.read(filepath) # Update the section with the given key-value pairs if section in config: for key, value in updates.items(): config[section][key] = value else: config.add_section(section) for key, value in updates.items(): config[section][key] = value # Write the changes back to the file with open(filepath, \'w\') as configfile: config.write(configfile)"},{"question":"Design a Python program that uses the `xml.sax` package to parse a given XML file and extract specific information. Your task is to write a custom `ContentHandler` to handle the XML elements and print the titles of all books listed in the XML file, along with their corresponding authors. The XML structure you need to parse is provided below: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> </book> <book id=\\"bk103\\"> <author>Corets, Eva</author> <title>Maeve Ascendant</title> </book> </catalog> ``` # Requirements 1. Implement a class `BookHandler` that inherits from `xml.sax.ContentHandler`. 2. Override necessary methods to handle different parsing events. 3. Extract and print the titles and authors of all books in the catalog. # Implementation Details - The program should read the XML data from a file named `books.xml`. - Ensure that only book titles and their authors are printed, formatted as `Title: Author`. - Use proper exception handling to manage parsing errors. # Example Output ``` XML Developer\'s Guide: Gambardella, Matthew Midnight Rain: Ralls, Kim Maeve Ascendant: Corets, Eva ``` # Constraints - Assume the XML structure will not change and always follow the provided format. - The solution should be efficient, leveraging SAX\'s streaming capabilities to handle large XML files if needed. # Additional Information - Use `xml.sax.parse()` to parse the XML file. - You can define additional methods and attributes in the `BookHandler` class as needed. - Focus on creating a functional handler that can process the input XML and generate the correct output. ```python import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current = \\"\\" self.title = \\"\\" self.author = \\"\\" def startElement(self, tag, attributes): self.current = tag def endElement(self, tag): if self.current == \\"title\\": print(f\\"{self.title}\\", end=\\": \\") elif self.current == \\"author\\": print(f\\"{self.author}\\") self.current = \\"\\" def characters(self, content): if self.current == \\"title\\": self.title = content elif self.current == \\"author\\": self.author = content if __name__ == \\"__main__\\": parser = xml.sax.make_parser() handler = BookHandler() parser.setContentHandler(handler) parser.parse(\\"books.xml\\") ```","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.current_title = \\"\\" self.current_author = \\"\\" self.books = [] def startElement(self, tag, attributes): self.current_element = tag def endElement(self, tag): if tag == \\"book\\": self.books.append((self.current_title, self.current_author)) self.current_title = \\"\\" self.current_author = \\"\\" self.current_element = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.current_title += content elif self.current_element == \\"author\\": self.current_author += content def endDocument(self): for title, author in self.books: print(f\\"{title}: {author}\\") if __name__ == \\"__main__\\": parser = xml.sax.make_parser() handler = BookHandler() parser.setContentHandler(handler) parser.parse(\\"books.xml\\")"},{"question":"# Custom Sequence and Callable Iterators **Objective**: Implement custom iterator classes in Python to mimic the behavior of sequence and callable iterators as described in the documentation. # Instructions: 1. Create a class `CustomSeqIterator` which acts as an iterator over any sequence (like lists or tuples) implementing the `__getitem__()` method. 2. Create a class `CustomCallIterator` which accepts a callable object and a sentinel value. The iterator should call the callable object until the sentinel value is returned. # Implementation Details: 1. **`CustomSeqIterator` Class**: - **`__init__(self, seq)`**: Initialize the iterator with a given sequence. - **`__iter__(self)`**: Return the iterator object itself. - **`__next__(self)`**: Return the next item from the sequence. Raise `StopIteration` when the sequence is exhausted. 2. **`CustomCallIterator` Class**: - **`__init__(self, callable, sentinel)`**: Initialize the iterator with a callable object and a sentinel value. - **`__iter__(self)`**: Return the iterator object itself. - **`__next__(self)`**: Call the callable object to get the next item. Raise `StopIteration` when the callable returns the sentinel value. # Example Usages: ```python # Example for CustomSeqIterator seq = [1, 2, 3, 4] seq_iter = CustomSeqIterator(seq) for item in seq_iter: print(item) # Output: 1 2 3 4 # Example for CustomCallIterator def counter(): n = 0 while True: yield n n += 1 call_iter = CustomCallIterator(counter().__next__, 5) for item in call_iter: print(item) # Output: 0 1 2 3 4 ``` # Constraints: - Do not use Python\'s built-in `iter()` and `next()` directly. - Ensure the iterators handle sequences and callable objects efficiently. - Implement proper error handling and `StopIteration` propagation as described.","solution":"class CustomSeqIterator: def __init__(self, seq): self.seq = seq self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.seq): raise StopIteration item = self.seq[self.index] self.index += 1 return item class CustomCallIterator: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result"},{"question":"# Advanced Python Programming Question: Customizing Classes and Special Methods **Objective:** Design a custom class that emulates a few built-in types and behaviors in Python. The class will implement special methods to allow instances of the class to be mutable sequences and support arithmetic operations. **Problem Statement:** You are to design a class named `CustomList` that closely mimics the behavior of Python\'s built-in `list` type. However, `CustomList` should have the following additional capabilities: 1. **Initialization:** The class should be initialized with a variable-length argument list of elements. ```python lst = CustomList(1, 2, 3) # Initializes with elements 1, 2, 3 ``` 2. **Sequence Operations:** Implement the following special methods to support sequence operations: - `__getitem__(self, index)`: To get an item at the specified index. - `__setitem__(self, index, value)`: To set an item at the specified index. - `__delitem__(self, index)`: To delete an item at the specified index. - `__len__(self)`: To get the length of the sequence. - `__iter__(self)`: To iterate over the sequence. 3. **Arithmetic Operations:** Implement the following special methods to support arithmetic operations. These should work element-wise similar to NumPy arrays. - `__add__(self, other)`: Element-wise addition. - `__radd__(self, other)`: Reflected addition. - `__sub__(self, other)`: Element-wise subtraction. - `__rsub__(self, other)`: Reflected subtraction. - `__mul__(self, other)`: Element-wise multiplication. - `__rmul__(self, other)`: Reflected multiplication. 4. **Custom Method:** Add a `sum()` method that returns the sum of all elements in the list. ```python lst.sum() # Returns the sum of all elements ``` **Requirements:** - The custom class should perform all standard operations of a list. - Ensure proper error handling for operations involving invalid indices and unsupported types. - The arithmetic operations should allow interaction with both other `CustomList` objects and standard Python numeric types. - Performance considerations: As this is an educational exercise, focus on correctness and understandability over performance optimization. **Constraints:** - Do not use built-in list function directly like list\'s `__getitem__`, `__setitem__`, etc. - Assume all elements in `CustomList` are numeric (integers or floats). **Example:** ```python lst1 = CustomList(1, 2, 3) lst2 = CustomList(4, 5, 6) print(lst1 + lst2) # Output: CustomList(5, 7, 9) print(lst1 * 2) # Output: CustomList(2, 4, 6) print(lst1.sum()) # Output: 6 print(len(lst1)) # Output: 3 for item in lst1: print(item) # Output: 1, 2, 3 (each on a new line) lst1[0] = 99 print(lst1) # Output: CustomList(99, 2, 3) ``` **Submission:** Please implement the `CustomList` class in Python, and ensure it meets all the specified functionalities.","solution":"class CustomList: def __init__(self, *args): self.data = list(args) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def __iter__(self): return iter(self.data) def __add__(self, other): if isinstance(other, CustomList): return CustomList(*(x + y for x, y in zip(self.data, other.data))) elif isinstance(other, (int, float)): return CustomList(*(x + other for x in self.data)) else: return NotImplemented def __radd__(self, other): return self.__add__(other) def __sub__(self, other): if isinstance(other, CustomList): return CustomList(*(x - y for x, y in zip(self.data, other.data))) elif isinstance(other, (int, float)): return CustomList(*(x - other for x in self.data)) else: return NotImplemented def __rsub__(self, other): if isinstance(other, (int, float)): return CustomList(*(other - x for x in self.data)) else: return NotImplemented def __mul__(self, other): if isinstance(other, CustomList): return CustomList(*(x * y for x, y in zip(self.data, other.data))) elif isinstance(other, (int, float)): return CustomList(*(x * other for x in self.data)) else: return NotImplemented def __rmul__(self, other): return self.__mul__(other) def sum(self): return sum(self.data) def __repr__(self): return f\\"CustomList({\', \'.join(map(str, self.data))})\\""},{"question":"Objective To assess your understanding of the `gc` module in Python, particularly focusing on enabling/disabling garbage collection, inspecting objects for reference cycles, and gathering garbage collection statistics. Problem Statement In this task, you will implement a utility function that performs the following steps: 1. **Disable Garbage Collection**: Temporarily disable automatic garbage collection to perform controlled operations. 2. **Perform Object Creation**: Create a set of objects and form a reference cycle. 3. **Enable Garbage Collection**: Re-enable automatic garbage collection. 4. **Collect Garbage**: Manually trigger garbage collection and gather statistics. 5. **Return Results**: Return statistics about the number of objects collected and uncollectable. Function Signature ```python def gc_utility(num_objects: int) -> dict: Perform garbage collection operations and return statistics. Parameters: num_objects (int): The number of objects to create and form a reference cycle. Returns: dict: A dictionary with the following keys: - \'collections\': The number of times collections were performed. - \'collected\': The total number of objects collected. - \'uncollectable\': The total number of uncollectable objects found. Raises: ValueError: If num_objects is not a positive integer. ``` Constraints & Requirements - **Constraints**: - `num_objects` must be a positive integer. If not, raise a `ValueError`. - **Performance Requirements**: - Create `num_objects` objects and link them in such a way that they form a reference cycle. - The garbage collection statistics should be gathered after manually triggering collection. - **Example**: ```python from gc import DEBUG_LEAK def create_cyclic_reference(num): # Helper function to create cyclic references class Cycle: pass head = Cycle() current = head # Create a cycle of objects for _ in range(num): current.next = Cycle() current = current.next current.next = head # Form a cycle def gc_utility(num_objects: int) -> dict: import gc if num_objects <= 0: raise ValueError(\\"num_objects must be a positive integer.\\") # Disable automatic garbage collection gc.disable() # Create cyclic references create_cyclic_reference(num_objects) # Enable automatic garbage collection gc.enable() # Enable debug to track collectable and uncollectable objects gc.set_debug(DEBUG_LEAK) # Perform garbage collection collected = gc.collect() # Get statistics stats = gc.get_stats() gc_count = gc.get_count() return { \'collections\': sum(g[\'collections\'] for g in stats), \'collected\': collected, \'uncollectable\': sum(g[\'uncollectable\'] for g in stats), } # Example Usage print(gc_utility(10)) ``` This code defines a helper function to create a cyclic reference and the main utility function to orchestrate disabling GC, creating objects, enabling GC, collecting garbage, and returning the statistics.","solution":"import gc def create_cyclic_reference(num): # Helper function to create cyclic references class Cycle: pass head = Cycle() current = head # Create a cycle of objects for _ in range(num): current.next = Cycle() current = current.next current.next = head # Form a cycle def gc_utility(num_objects: int) -> dict: Perform garbage collection operations and return statistics. Parameters: num_objects (int): The number of objects to create and form a reference cycle. Returns: dict: A dictionary with the following keys: - \'collections\': The number of times collections were performed. - \'collected\': The total number of objects collected. - \'uncollectable\': The total number of uncollectable objects found. Raises: ValueError: If num_objects is not a positive integer. import gc if num_objects <= 0: raise ValueError(\\"num_objects must be a positive integer.\\") # Disable automatic garbage collection gc.disable() # Create cyclic references create_cyclic_reference(num_objects) # Enable automatic garbage collection gc.enable() # Perform garbage collection collected = gc.collect() # Get statistics gc_stats = gc.get_stats() return { \'collections\': sum(g[\'collections\'] for g in gc_stats), \'collected\': collected, \'uncollectable\': sum(g[\'uncollectable\'] for g in gc_stats), }"},{"question":"**Question: Advanced TarFile Manipulation and Custom Extraction** You are tasked with managing and securing tar archive files using Python\'s `tarfile` module. Specifically, you need to create, update, and extract tar archives while handling file metadata securely. # Problem Statement Write a script that performs the following tasks: 1. **Creating an Archive:** - Create a tar archive named `archive.tar.gz` containing at least three different files with varying permissions and types (e.g., regular file, directory, symbolic link). - Use gzip compression for this archive. 2. **Updating the Archive Metadata:** - Open the existing `archive.tar.gz` and update the user and group information of all files to `0` and `root`, respectively, without changing other metadata. - Save the updated archive as `archive_updated.tar.gz`. 3. **Custom Extraction with Security Filters:** - Write a custom extraction filter that prevents the extraction of files with absolute paths and symbolic links pointing outside the extraction directory. The filter should raise appropriate errors for such files. - Use this filter to extract the contents of `archive_updated.tar.gz` into a directory named `extracted/`. # Constraints - **Files and Directories**: Ensure the creation of regular files, directories, and symbolic links with appropriate permissions. - **Metadata Update**: Do not alter the original file contents, only update the user and group information. - **Security Filter**: Implement robust checks to prevent potentially malicious extractions. # Input/Output Specifications - **Input**: No external input. Use in-script file creation for testing purposes. - **Output**: Successfully created, updated, and extracted tar files. Errors should be raised and handled appropriately, indicating security checks in the filter function. # Implementation Details Part 1: Creating an Archive - Create a directory structure with different file types and permissions. - Use the `tarfile.open` method with `\'w:gz\'` mode to create a gzip-compressed tar archive. Part 2: Updating Metadata - Open the existing tar archive with `tarfile.open` in read mode. - Iterate through the files, modify their metadata, and write them to a new tar archive using a filter function. Part 3: Custom Extraction Filter - Implement a filter function that checks for absolute paths and symbolic link targets. - Use the `tarfile` extraction methods with the custom filter to enforce security. # Example Structure ```python import os import tarfile # Part 1: Creating an Archive def create_archive(): # Create a sample directory structure with files and a symbolic link os.makedirs(\'sample_dir\', exist_ok=True) with open(\'sample_dir/file1.txt\', \'w\') as f: f.write(\'Hello World!\') with tarfile.open(\'archive.tar.gz\', \'w:gz\') as tar: tar.add(\'sample_dir/file1.txt\') # Add more files and a symbolic link # Part 2: Updating Metadata def update_metadata(): def reset_user_info(tarinfo): tarinfo.uid = tarinfo.gid = 0 tarinfo.uname = tarinfo.gname = \\"root\\" return tarinfo with tarfile.open(\'archive.tar.gz\', \'r:gz\') as tar_read: with tarfile.open(\'archive_updated.tar.gz\', \'w:gz\') as tar_write: for member in tar_read.getmembers(): tar_read.extract(member) tar_write.add(member.name, filter=reset_user_info) # Part 3: Custom Extraction Filter def custom_filter(tarinfo, path): if tarinfo.islnk() and (os.path.isabs(tarinfo.linkname) or \'..\' in tarinfo.linkname): raise tarfile.LinkOutsideDestinationError(f\'Link {tarinfo.name} points outside extraction directory\') return tarinfo def extract_with_filter(): with tarfile.open(\'archive_updated.tar.gz\', \'r:gz\') as tar: tar.extractall(path=\'extracted\', filter=custom_filter) if __name__ == \\"__main__\\": create_archive() update_metadata() extract_with_filter() ``` # Notes - Ensure the script is self-contained and all file operations are handled within the script for testing purposes. - Handle exceptions gracefully, providing meaningful error messages for security violations. # Submission Submit the completed script along with a brief explanation of your implementation and any assumptions made.","solution":"import os import tarfile import shutil # Part 1: Creating an Archive def create_archive(): # Create a sample directory structure with files and a symbolic link os.makedirs(\'sample_dir\', exist_ok=True) with open(\'sample_dir/file1.txt\', \'w\') as f: f.write(\'Hello World!\') with open(\'sample_dir/file2.txt\', \'w\') as f: f.write(\'Another file content\') os.makedirs(\'sample_dir/subdir\', exist_ok=True) with open(\'sample_dir/subdir/file3.txt\', \'w\') as f: f.write(\'Subdirectory file content\') os.symlink(\'sample_dir/file1.txt\', \'sample_dir/link_to_file1.txt\') # Create tar.gz archive with tarfile.open(\'archive.tar.gz\', \'w:gz\') as tar: tar.add(\'sample_dir\', arcname=\'sample_dir\') # Part 2: Updating Metadata def update_metadata(): def reset_user_info(tarinfo): tarinfo.uid = tarinfo.gid = 0 tarinfo.uname = tarinfo.gname = \\"root\\" return tarinfo with tarfile.open(\'archive.tar.gz\', \'r:gz\') as tar_read: with tarfile.open(\'archive_updated.tar.gz\', \'w:gz\') as tar_write: for member in tar_read.getmembers(): extracted_path = tar_read.extract(member) tar_write.add(member.name, filter=reset_user_info) # Part 3: Custom Extraction Filter def custom_filter(tarinfo, path): if tarinfo.islnk() and (os.path.isabs(tarinfo.linkname) or \'..\' in tarinfo.linkname): raise tarfile.LinkOutsideDestinationError(f\'Link {tarinfo.name} points outside extraction directory\') return tarinfo def extract_with_filter(): def custom_filter(tarinfo): if tarinfo.islnk(): # Check if the symbolic link points outside the extracted directory if not tarinfo.linkname.startswith(\'sample_dir/\'): raise tarfile.ExtractError(f\'{tarinfo.name} has an invalid symlink target.\') if os.path.isabs(tarinfo.name): raise tarfile.ExtractError(f\'{tarinfo.name} has an invalid absolute path.\') return tarinfo with tarfile.open(\'archive_updated.tar.gz\', \'r:gz\') as tar: tar.extractall(path=\'extracted\', members=[m for m in tar.getmembers() if custom_filter(m) is not None]) if __name__ == \\"__main__\\": create_archive() update_metadata() extract_with_filter()"},{"question":"# Advanced Coding Assessment: Implementing and Testing a Custom Sorted List Class Objective Your task is two-fold: 1. **Implement a custom class `SortedList`** which always maintains a sorted order of elements. 2. **Write a comprehensive set of unit tests for the `SortedList` class using the `unittest` framework**. Part 1: Implementing `SortedList` Implement a Python class `SortedList` that maintains a sorted list of integers. The class should have the following methods: 1. **`__init__(self, iterable=[])`**: Initializes the `SortedList` with the elements from the given iterable (e.g., list, tuple). If no iterable is provided, it initializes an empty sorted list. 2. **`add(self, value)`**: Adds a value to the `SortedList` while maintaining the sorted order. 3. **`remove(self, value)`**: Removes a value from the `SortedList`. If the value is not found, it should raise a `ValueError`. 4. **`__len__(self)`**: Returns the number of elements in the `SortedList`. 5. **`__getitem__(self, index)`**: Returns the element at the given index. 6. **`__contains__(self, value)`**: Returns `True` if the value is in the `SortedList`, `False` otherwise. 7. **`index(self, value)`**: Returns the index of the value in the `SortedList`. If the value is not found, it should raise a `ValueError`. Make sure the `SortedList` is always sorted in ascending order after any modification. Part 2: Unit Testing `SortedList` Write a comprehensive set of unit tests for the `SortedList` class using the `unittest` framework. Your tests should cover: 1. **Initialization**: - Test empty initialization. - Test initialization with an unsorted list and check if the list is sorted. 2. **Adding Elements**: - Test adding elements to an empty list. - Test adding elements to a non-empty list while ensuring the list remains sorted. - Test adding duplicate elements. 3. **Removing Elements**: - Test removing elements. - Test removing elements that are not in the list (expecting `ValueError`). 4. **Length of List**: - Test the length of the list after various operations (add, remove). 5. **Element Access**: - Test element retrieval by index. - Test index retrieval of elements. - Test containment of elements in the list. 6. **Edge Cases**: - Test large inputs. - Test negative values and zeros. Constraints - Assume all inputs will be valid integers. - The class should handle basic Python iterable inputs (lists, tuples, etc.) for initialization. - Ensure the methods handle edge cases, such as an empty list, gracefully. Example Usage ```python # Example usage of the SortedList class sl = SortedList([3, 1, 2]) print(sl) # Output should be [1, 2, 3] sl.add(0) print(sl) # Output should be [0, 1, 2, 3] sl.remove(2) print(sl) # Output should be [0, 1, 3] print(len(sl)) # Output should be 3 ``` Submission Submit your implementation of the `SortedList` class and the unit tests as a single Python file. **Hints**: - Use Python\'s built-in `bisect` module to maintain the sorted order efficiently. - Refer to the `unittest` documentation for help on writing tests.","solution":"import bisect class SortedList: def __init__(self, iterable=[]): Initializes the SortedList with the elements from the given iterable. self._list = sorted(iterable) def add(self, value): Adds a value to the SortedList while maintaining sorted order. bisect.insort(self._list, value) def remove(self, value): Removes a value from the SortedList. Raises ValueError if not found. index = bisect.bisect_left(self._list, value) if index != len(self._list) and self._list[index] == value: self._list.pop(index) else: raise ValueError(f\\"{value} not in list\\") def __len__(self): Returns the number of elements in the SortedList. return len(self._list) def __getitem__(self, index): Returns the element at the given index. return self._list[index] def __contains__(self, value): Returns True if the value is in the SortedList, False otherwise. index = bisect.bisect_left(self._list, value) return index != len(self._list) and self._list[index] == value def index(self, value): Returns the index of the value in the SortedList. Raises ValueError if not found. index = bisect.bisect_left(self._list, value) if index != len(self._list) and self._list[index] == value: return index else: raise ValueError(f\\"{value} not in list\\")"},{"question":"# Question: Build and Evaluate a Custom Machine Learning Pipeline You are given a dataset containing information about house prices. Your task is to build a custom machine learning pipeline that: 1. Preprocesses the data with different transformations for categorical and numerical features. 2. Applies dimensionality reduction. 3. Builds a regression model with transformed targets. Instructions: 1. **Transform the Data:** - For categorical columns: - Apply one-hot encoding. - For numerical columns: - Apply standard scaling. - Apply principal component analysis (PCA) for dimensionality reduction. 2. **Transform the Target:** - Apply log transformation to the target variable. 3. **Build the Pipeline:** - Use a `ColumnTransformer` to preprocess the features. - Include PCA to reduce the dimensionality of the numerical features. - Fit a regression model using the processed features and the transformed target. 4. **Evaluate the Pipeline:** - Use a cross-validation technique to evaluate the regression model. - Output the cross-validated R2 score. Expected Input and Output Format: - **Input:** - `X`: A Pandas DataFrame of shape (n_samples, n_features) containing the house features. The DataFrame includes both numerical and categorical columns. - `y`: A Pandas Series of shape (n_samples,) containing the house prices. - **Output:** - A dictionary capturing the cross-validated R2 score, for example: `{\'r2_score\': 0.85}`. Constraints: - You must use `Pipeline`, `ColumnTransformer`, and `TransformedTargetRegressor` from scikit-learn to build the pipeline. - Perform a 5-fold cross-validation to evaluate the model. Example: ```python import pandas as pd from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer, TransformedTargetRegressor from sklearn.decomposition import PCA from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score from sklearn.datasets import fetch_california_housing import numpy as np # Fetch example dataset X, y = fetch_california_housing(return_X_y=True, as_frame=True) # Define a function to build and evaluate the pipeline def build_and_evaluate_pipeline(X, y): # Define the column transformer preprocessor = ColumnTransformer( transformers=[ (\'num\', Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=5)) ]), X.select_dtypes(include=[\'float64\']).columns), (\'cat\', OneHotEncoder(), X.select_dtypes(include=[\'category\']).columns) ] ) # Define the regressor with transformed target regressor = TransformedTargetRegressor( regressor=LinearRegression(), func=np.log, inverse_func=np.exp ) # Build the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', regressor) ]) # Perform cross-validation r2_scores = cross_val_score(pipeline, X, y, cv=5, scoring=\'r2\') return {\'r2_score\': r2_scores.mean()} # Example usage result = build_and_evaluate_pipeline(X, y) print(result) # Example output: {\'r2_score\': 0.61} ``` Additional Notes: - You can assume that `X` and `y` are clean and do not contain missing values. - Make sure to handle potential warnings or errors in your function implementation.","solution":"import numpy as np import pandas as pd from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer, TransformedTargetRegressor from sklearn.decomposition import PCA from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def build_and_evaluate_pipeline(X, y): Builds and evaluates a custom machine learning pipeline on the given data. Parameters: X (pd.DataFrame): DataFrame containing the house features. y (pd.Series): Series containing the house prices. Returns: dict: Dictionary containing the cross-validated R2 score. # Define the column transformer preprocessor = ColumnTransformer( transformers=[ (\'num\', Pipeline([ (\'scaler\', StandardScaler()), # Standard Scaling (\'pca\', PCA(n_components=5)) # Dimensionality Reduction ]), X.select_dtypes(include=[\'float64\', \'int64\']).columns), # Numerical Columns (\'cat\', OneHotEncoder(), X.select_dtypes(include=[\'object\']).columns) # Categorical Columns ] ) # Define the regressor with transformed target regressor = TransformedTargetRegressor( regressor=LinearRegression(), func=np.log, inverse_func=np.exp ) # Build the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', regressor) ]) # Perform cross-validation r2_scores = cross_val_score(pipeline, X, y, cv=5, scoring=\'r2\') return {\'r2_score\': r2_scores.mean()}"},{"question":"# Advanced Coding Assessment Question: Multiprocessing in Python Objective Design a python program that utilizes the multiprocessing package to process and analyze a large dataset concurrently, demonstrating comprehension of creating processes, using queues for inter-process communication, and applying synchronization mechanisms. Problem Statement You are given an extremely large list of numbers, and your task is to calculate the sum of squares of all even numbers and the sum of cubes of all odd numbers. Given the size of the list, a single-threaded approach is inefficient. Instead, you should leverage the multiprocessing module to parallelize the task. Requirements 1. Create a function `compute_sums` that will: - Spawn `n` worker processes to process chunks of the list concurrently. - Use a `Queue` to manage tasks and to collect results from worker processes. - Use a `Lock` to ensure the proper printing of results from multiple processes. 2. Create a function `worker` that will: - Fetch a chunk of numbers from the input queue. - Compute the sum of squares for even numbers and the sum of cubes for odd numbers within the chunk. - Place the results back in the output queue. 3. Implement a `main` function to: - Divide the list into `n` chunks. - Create and start the worker processes. - Collect and combine results from all worker processes. - Print the final results in a thread-safe manner. Input - `numbers` (list of int): The large list of numbers to be processed. - `n` (int): Number of worker processes. Output - Print the combined result of the sum of squares of all even numbers and the sum of cubes of all odd numbers. Function Signature ```python def compute_sums(numbers: list, n: int) -> None: pass ``` # Sample Usage ```python if __name__ == \'__main__\': numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] * 1000 # example large list n = 4 # number of worker processes compute_sums(numbers, n) ``` Constraints - The solution must demonstrate the use of `multiprocessing.Process`, `Queue`, and `Lock`. - Efficiently handle the given large dataset to prevent memory overflow and to ensure each process works independently on its subset of data. - Ensure thread-safe operations while printing the results. Notes - You may need to use splitting functions to divide the list into balanced chunks. - Make use of the `if __name__ == \'__main__\'` guard to ensure proper execution when using multiprocessing on Windows.","solution":"import multiprocessing from multiprocessing import Queue, Lock def worker(input_queue, output_queue, lock): while True: numbers_chunk = input_queue.get() if numbers_chunk is None: break sum_of_squares = sum(x**2 for x in numbers_chunk if x % 2 == 0) sum_of_cubes = sum(x**3 for x in numbers_chunk if x % 2 != 0) with lock: output_queue.put((sum_of_squares, sum_of_cubes)) def compute_sums(numbers, n): chunk_size = len(numbers) // n input_queue = Queue() output_queue = Queue() lock = Lock() # Dividing the numbers into chunks chunks = [numbers[i * chunk_size:(i + 1) * chunk_size] for i in range(n)] if len(numbers) % n != 0: chunks.append(numbers[n * chunk_size:]) # Spawning worker processes processes = [] for _ in range(n): p = multiprocessing.Process(target=worker, args=(input_queue, output_queue, lock)) p.start() processes.append(p) # Putting chunks into the input queue for chunk in chunks: input_queue.put(chunk) # Adding \'None\' to indicate end of tasks for _ in range(n): input_queue.put(None) # Gathering results total_sum_of_squares = 0 total_sum_of_cubes = 0 for _ in chunks: sum_of_squares, sum_of_cubes = output_queue.get() total_sum_of_squares += sum_of_squares total_sum_of_cubes += sum_of_cubes # Ensuring all processes have finished for p in processes: p.join() with lock: print(f\\"Sum of squares of even numbers: {total_sum_of_squares}\\") print(f\\"Sum of cubes of odd numbers: {total_sum_of_cubes}\\") # Example usage (uncomment and run in a proper environment) # if __name__ == \'__main__\': # numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] * 1000 # n = 4 # compute_sums(numbers, n)"},{"question":"# Question: Loading and Analyzing the Iris Dataset Problem Statement You are tasked with analyzing the famous Iris dataset using the scikit-learn library. The Iris dataset is one of the toy datasets available in the `sklearn.datasets` module. Your objective is to load this dataset, perform basic exploratory data analysis, and implement a classifier to predict the species of iris flower given its features. Requirements 1. **Load the Iris dataset:** - Use the appropriate function from the `sklearn.datasets` module to load the Iris dataset. - Extract the data (features) and the target (labels). 2. **Exploratory Data Analysis (EDA):** - Display the first 5 rows of the dataset. - Print the feature names and target names. - Compute the mean and standard deviation for each feature. 3. **Data Visualization:** - Plot a pair plot (scatter plot matrix) of the features colored by their corresponding target labels. 4. **Model Implementation:** - Split the dataset into training and testing sets (80% train, 20% test). - Train a `KNeighborsClassifier` (from `sklearn.neighbors`) on the training data. - Evaluate the model on the testing data and print the accuracy. Expected Input and Output Formats - Input: No direct input needed. - Output: Diagnostic prints, plots, and a printed accuracy metric. Constraints - Make sure to use scikit-learn version >= 0.24. - Use only standard libraries (numpy, pandas, matplotlib, seaborn, scikit-learn) available in a typical Anaconda Python environment. - Ensure your code runs efficiently within reasonable time limits. Example Code Stub ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # Step 1: Load dataset iris = load_iris() data = iris.data target = iris.target # Step 2: EDA print(\\"First 5 rows of the dataset:\\") print(pd.DataFrame(data, columns=iris.feature_names).head()) print(\\"nFeature Names:\\", iris.feature_names) print(\\"Target Names:\\", iris.target_names) print(\\"nStatistics for each feature:\\") print(pd.DataFrame(data, columns=iris.feature_names).describe().loc[[\'mean\', \'std\']]) # Step 3: Data Visualization df = pd.DataFrame(data, columns=iris.feature_names) df[\'species\'] = target sns.pairplot(df, hue=\'species\', markers=[\\"o\\", \\"s\\", \\"D\\"]) plt.show() # Step 4: Model Implementation X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=0) clf = KNeighborsClassifier(n_neighbors=5) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"nAccuracy of the KNeighbors Classifier:\\", accuracy) ```","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def analyze_iris_dataset(): # Step 1: Load dataset iris = load_iris() data = iris.data target = iris.target # Step 2: EDA print(\\"First 5 rows of the dataset:\\") print(pd.DataFrame(data, columns=iris.feature_names).head()) print(\\"nFeature Names:\\", iris.feature_names) print(\\"Target Names:\\", iris.target_names) print(\\"nStatistics for each feature:\\") print(pd.DataFrame(data, columns=iris.feature_names).describe().loc[[\'mean\', \'std\']]) # Step 3: Data Visualization df = pd.DataFrame(data, columns=iris.feature_names) df[\'species\'] = target sns.pairplot(df, hue=\'species\', markers=[\\"o\\", \\"s\\", \\"D\\"]) plt.show() # Step 4: Model Implementation X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=0) clf = KNeighborsClassifier(n_neighbors=5) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"nAccuracy of the KNeighbors Classifier:\\", accuracy) return accuracy"},{"question":"**Problem Statement: Dimensionality Reduction using Random Projections** You are provided with a dataset `X` (a NumPy array) with `n_samples` samples and `n_features` features. Your task is to implement a function that performs dimensionality reduction on this dataset using both Gaussian and sparse random projections. Additionally, compute the inverse transform of the projected data and verify the pairwise distance preservation. # Instructions 1. **Implement the Function:** Write a function `reduce_dimensionality` that takes the following input parameters: - `X`: A 2D NumPy array of shape `(n_samples, n_features)`. - `method`: A string indicating the type of random projection to use (`\'gaussian\'` or `\'sparse\'`). - `n_components`: An integer specifying the number of components in the reduced space. - `compute_inverse`: A boolean indicating whether to compute the inverse transform components. Defaults to `False`. The function should return a dictionary with the following keys: - `\'X_reduced\'`: The reduced dimension data. - `\'X_inversed\'`: The data after applying the inverse transform. If `compute_inverse` is `False`, this should be `None`. - `\'distance_preservation\'`: A boolean flag indicating whether the pairwise distances between samples in `X_reduced` are approximately preserved as per the Johnson-Lindenstrauss lemma. Allow a maximum distortion `eps = 0.1`. 2. **Verify Distance Preservation:** Compute and compare the pairwise distances between samples in the original dataset `X` and the reduced dataset `X_reduced`. The distances should be preserved within a tolerance defined by the Johnson-Lindenstrauss lemma. # Constraints - You may assume that `n_samples >= 1` and `n_features >= 1`. - You are allowed to use `numpy` and `sklearn` packages. # Function Signature ```python def reduce_dimensionality(X: np.ndarray, method: str, n_components: int, compute_inverse: bool = False) -> dict: pass ``` # Example ```python import numpy as np # Generating a random dataset X = np.random.rand(100, 10000) # Reducing dimensionality using Gaussian Random Projection result = reduce_dimensionality(X, method=\'gaussian\', n_components=500, compute_inverse=True) print(result[\'X_reduced\'].shape) # Expected: (100, 500) print(result[\'X_inversed\'].shape) # Expected: (100, 10000) print(result[\'distance_preservation\']) # Expected: True or False # Reducing dimensionality using Sparse Random Projection result = reduce_dimensionality(X, method=\'sparse\', n_components=500, compute_inverse=False) print(result[\'X_reduced\'].shape) # Expected: (100, 500) print(result[\'X_inversed\']) # Expected: None print(result[\'distance_preservation\']) # Expected: True or False ``` # Notes - Use `johnson_lindenstrauss_min_dim` to estimate the minimal number of components to guarantee a bounded distortion. - Calculate pairwise distances before and after projection to check the distortion.","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from sklearn.metrics import pairwise_distances def reduce_dimensionality(X: np.ndarray, method: str, n_components: int, compute_inverse: bool = False) -> dict: if method == \'gaussian\': transformer = GaussianRandomProjection(n_components=n_components) elif method == \'sparse\': transformer = SparseRandomProjection(n_components=n_components) else: raise ValueError(\\"Invalid method. Use \'gaussian\' or \'sparse\'.\\") X_reduced = transformer.fit_transform(X) if compute_inverse: try: X_inversed = transformer.inverse_transform(X_reduced) except NotImplementedError: X_inversed = None else: X_inversed = None # Calculate pairwise distances in the original and reduced space original_distances = pairwise_distances(X) reduced_distances = pairwise_distances(X_reduced) # Calculate distance preservation eps = 0.1 distance_preservation = np.allclose(original_distances, reduced_distances, atol=eps * original_distances) return { \'X_reduced\': X_reduced, \'X_inversed\': X_inversed, \'distance_preservation\': distance_preservation }"},{"question":"You are given a time series of stock prices for multiple stocks in a dataframe. You are required to do the following: 1. **Calculate the rolling mean and standard deviation** of closing prices for each stock over a 10-day window. 2. **Calculate the exponentially weighted mean** with a half-life of 5 days for each stock. 3. **Compute the rolling covariance** between the closing prices of any two stocks over a 15-day window. 4. **Find the custom rolling sum** for each stock where the window size alternates between 2 and 3 days with every other row in the dataset. Your task is to implement a function `analyze_stock_data(df)` that takes a dataframe `df` as input and returns a dictionary containing the following keys and corresponding values: - `\'rolling_mean\'`: Dataframe of rolling means - `\'rolling_std\'`: Dataframe of rolling standard deviations - `\'ewm_mean\'`: Dataframe of exponentially weighted means - `\'rolling_cov\'`: Dataframe of rolling covariances between the first two stocks - `\'custom_rolling_sum\'`: Dataframe of custom rolling sums # Input: - `df`: A pandas dataframe with datetime index and columns representing closing prices of different stocks. Each column is a different stock. # Output: - A dictionary with keys `\'rolling_mean\'`, `\'rolling_std\'`, `\'ewm_mean\'`, `\'rolling_cov\'` and `\'custom_rolling_sum\'` and their corresponding dataframes. # Example: ```python import pandas as pd df = pd.DataFrame({ \'StockA\': [100, 102, 104, 103, 108, 107, 109, 110, 115, 120, 119, 125, 128, 130, 132], \'StockB\': [210, 212, 214, 217, 215, 220, 225, 230, 235, 240, 238, 245, 250, 255, 258] }, index=pd.date_range(start=\'2022-01-01\', periods=15)) result = analyze_stock_data(df) ``` # Constraints: - You can assume the dataframe will always contain numeric values. - The dataframe can have multiple stocks, but you should compute covariance only between the first two stocks. - The custom rolling sum window size alternates starting with a size of 2 for the first row, then size of 3 for the second row, back to size of 2 for the third row, and so on. Use the functionalities provided by pandas to solve the problem.","solution":"import pandas as pd def analyze_stock_data(df): Analyzes stock data to calculate various statistical measures. Args: df (pd.DataFrame): Dataframe containing stock prices with datetime index. Returns: dict: Dictionary containing the following keys and values: - \'rolling_mean\': Dataframe of rolling means over a 10-day window - \'rolling_std\': Dataframe of rolling standard deviations over a 10-day window - \'ewm_mean\': Dataframe of exponentially weighted means with a half-life of 5 days - \'rolling_cov\': Dataframe of rolling covariances between the first two stocks over a 15-day window - \'custom_rolling_sum\': Dataframe of custom rolling sums with alternating window sizes of 2 and 3 days results = {} # Calculate rolling mean and standard deviation over a 10-day window results[\'rolling_mean\'] = df.rolling(window=10).mean() results[\'rolling_std\'] = df.rolling(window=10).std() # Calculate exponentially weighted mean with a half-life of 5 days results[\'ewm_mean\'] = df.ewm(halflife=5).mean() # Calculate rolling covariance between the first two stocks over a 15-day window if df.shape[1] >= 2: stock1, stock2 = df.columns[:2] rolling_cov = df[[stock1, stock2]].rolling(window=15).cov().unstack().iloc[:, 1] results[\'rolling_cov\'] = rolling_cov else: results[\'rolling_cov\'] = pd.DataFrame() # Calculate custom rolling sum with alternating window sizes of 2 and 3 days def custom_rolling_sum(s): result = [] for i in range(len(s)): if i % 2 == 0: window = 2 else: window = 3 if i >= window - 1: result.append(s.iloc[i - window + 1:i + 1].sum()) else: result.append(None) return pd.Series(result, index=s.index) results[\'custom_rolling_sum\'] = df.apply(custom_rolling_sum, axis=0) return results"},{"question":"Coding Assessment Question # Objective You are tasked with implementing a custom kernel function that incorporates both the Euclidean distance and the Polynomial kernel. Then, you will use this custom kernel to compute the pairwise kernel matrix for a given dataset. # Requirements 1. Implement a custom kernel function `custom_kernel(X, Y=None, degree=3, gamma=None, coef0=1)` which computes a kernel matrix combining the Euclidean distance with a polynomial kernel. 2. Use scikit-learn\'s `pairwise_kernels` to apply your custom kernel function to the provided dataset. # Input 1. A 2D numpy array `X` of shape `(n_samples, n_features)`. 2. A 2D numpy array `Y` of shape `(m_samples, n_features)` (optional, default is `None` which means `Y` is equal to `X`). 3. An integer `degree` which specifies the degree of the polynomial kernel (default is `3`). 4. A float `gamma` which, if None, defaults to `1.0 / n_features`. 5. A float `coef0` representing the independent term in the polynomial kernel (default is `1`). # Output 1. A 2D numpy array representing the kernel matrix. # Constraints - The function should be able to handle large datasets efficiently. - `X` and `Y` can have up to 10,000 samples with up to 100 features each. # Performance Requirements - The implementation should be optimized for performance. - The resulting kernel matrix should be computed in reasonable time for large datasets. # Implementation Details 1. The custom kernel function should first compute the Euclidean distance between all pairs of samples from `X` and `Y`. 2. Apply the polynomial kernel to the Euclidean distances. # Example ```python import numpy as np from sklearn.metrics.pairwise import pairwise_kernels def custom_kernel(X, Y=None, degree=3, gamma=None, coef0=1): if gamma is None: gamma = 1.0 / X.shape[1] if Y is None: Y = X # Compute Euclidean distance euclidean_dist = np.sqrt(((X[:, np.newaxis] - Y[np.newaxis, :]) ** 2).sum(axis=2)) # Apply polynomial kernel on Euclidean distance return (gamma * euclidean_dist + coef0) ** degree # Example usage X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) custom_kernel_matrix = pairwise_kernels(X, Y, metric=custom_kernel) print(custom_kernel_matrix) ``` In the above example, the `custom_kernel` function computes a kernel matrix considering both the Euclidean distance and the polynomial transformation. The `pairwise_kernels` function from scikit-learn is then used to apply the custom kernel to the data matrices `X` and `Y`.","solution":"import numpy as np from sklearn.metrics.pairwise import pairwise_kernels def custom_kernel(X, Y=None, degree=3, gamma=None, coef0=1): Custom kernel function that combines Euclidean distance and Polynomial kernel. Args: X (np.array): 2D array of shape (n_samples, n_features). Y (np.array, optional): 2D array of shape (m_samples, n_features). Default is None. degree (int, optional): Degree of the polynomial kernel. Default is 3. gamma (float, optional): Kernel coefficient, if None defaults to 1.0 / n_features. Default is None. coef0 (float, optional): Independent term in polynomial kernel. Default is 1. Returns: np.array: Kernel matrix. if gamma is None: gamma = 1.0 / X.shape[1] if Y is None: Y = X # Compute Euclidean distance euclidean_dist = np.sqrt(((X[:, np.newaxis] - Y[np.newaxis, :]) ** 2).sum(axis=2)) # Apply polynomial kernel on Euclidean distance return (gamma * euclidean_dist + coef0) ** degree"},{"question":"Using the Seaborn library, create a detailed visualization of the \'penguins\' dataset. Your task is to generate a jointplot with the following requirements: 1. **Dataset and Basic Plotting**: - Load the \'penguins\' dataset from Seaborn\'s built-in datasets. - Create a jointplot with `bill_length_mm` as the x-axis variable and `bill_depth_mm` as the y-axis variable. 2. **Customizing the Jointplot**: - Use the `hue` parameter to differentiate data points by the `species` variable. - Choose `kde` (Kernel Density Estimation) as the type of plot. 3. **Advanced Customization**: - Customize the `kde` plot with the following additional keyword arguments: set the color to `\'navy\'` and the fill to `True`. - Adjust the `marginal_kws` to set the number of bins to 20 and ensure they are not filled. 4. **Adding Layers**: - Add an additional KDE layer on the joint plot with the KDE lines colored red. - Add a rug plot to the margins with the rug lines colored green. 5. **Figure Layout**: - Use `JointGrid` parameters to set the figure height to 6 and the ratio of the joint to marginal axes to 2. - Ensure marginal ticks are enabled. Implementation Requirements: - You should write a function `custom_jointplot()` that takes no arguments and produces the described plot. - The function should perform all tasks listed above within a single function call. Expected Function Signature: ```python def custom_jointplot(): pass ``` # Constraints: - Ensure that the code is efficient and avoids unnecessary computations. - The plot should be displayed correctly with all the specified customizations applied. # Example: When you call the function `custom_jointplot()`, it should display a joint plot of the `penguins` data as specified. Note: You do not need to return anything from the function. The function should merely produce the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_jointplot(): # Load the penguins dataset from Seaborn penguins = sns.load_dataset(\\"penguins\\") # Create a JointGrid g = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", height=6, ratio=2, marginal_ticks=True) # Plot using KDE for the joint and scatter for the marginal g = g.plot_joint(sns.kdeplot, color=\'navy\', fill=True) g = g.plot_marginals(sns.histplot, bins=20, kde=False, edgecolor=\'w\') # Add an additional KDE layer with red color for the KDE lines sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", color=\'red\', fill=False, ax=g.ax_joint) # Add rug plot to the margins sns.rugplot(data=penguins, x=\\"bill_length_mm\\", ax=g.ax_marg_x, color=\'green\') sns.rugplot(data=penguins, y=\\"bill_depth_mm\\", ax=g.ax_marg_y, color=\'green\') # Show the plot plt.show()"},{"question":"Problem Statement You are required to implement a function `create_custom_bytes_object` that constructs a bytes object based on an array of integer inputs. Additionally, you must implement a `concat_bytes_objects` function that concatenates multiple bytes objects into one. Your implementation should closely mimic the behavior outlined in the provided documentation, but you will work with pure Python. Function Specifications 1. **Function Name:** `create_custom_bytes_object` - **Input:** A list of integers `int_list` (0 <= int_list[i] <= 255 for all i). - **Output:** A bytes object that contains the byte values corresponding to the integers in `int_list`. 2. **Function Name:** `concat_bytes_objects` - **Input:** A list of bytes objects `bytes_list`. - **Output:** A single bytes object that is the concatenation of all the bytes objects in `bytes_list`. Example ```python def create_custom_bytes_object(int_list): # Your implementation here pass def concat_bytes_objects(bytes_list): # Your implementation here pass # Example usage: # Creating a bytes object from a list of integers byte_obj1 = create_custom_bytes_object([72, 101, 108, 108, 111]) # b\'Hello\' # Creating another bytes object from a list of integers byte_obj2 = create_custom_bytes_object([32, 87, 111, 114, 108, 100]) # b\' World\' # Concatenating bytes objects result = concat_bytes_objects([byte_obj1, byte_obj2]) # b\'Hello World\' ``` Constraints - You must not use any external modules or libraries. - You can assume the input lists contain valid values and are non-empty. Performance Expectations - Both functions should operate efficiently within O(n) complexity, where n is the total length of the integer list or the combined lengths of the bytes objects. Additional Notes - Remember to handle edge cases, such as an empty list or lists containing non-integer types. - The functions should raise appropriate errors if the provided input does not meet the specified constraints.","solution":"def create_custom_bytes_object(int_list): Create a bytes object from a list of integers. Args: int_list (list): A list of integers (0 <= int_list[i] <= 255 for all i). Returns: bytes: A bytes object corresponding to the integers in int_list. if any(not isinstance(i, int) or i < 0 or i > 255 for i in int_list): raise ValueError(\\"All elements must be integers between 0 and 255 inclusive.\\") return bytes(int_list) def concat_bytes_objects(bytes_list): Concatenate multiple bytes objects into one. Args: bytes_list (list): A list of bytes objects. Returns: bytes: A single bytes object that is the concatenation of all the bytes objects in bytes_list. if any(not isinstance(b, bytes) for b in bytes_list): raise ValueError(\\"All elements in the list must be bytes objects.\\") return b\'\'.join(bytes_list)"},{"question":"**Problem Statement:** You are required to read a WAV file, perform a simple processing operation (amplify or reduce the volume), and then save the processed audio to a new WAV file. This will test your understanding of the `wave` module\'s functionalities and your ability to manipulate audio data. # Task: 1. Write a function `amplify_wav(input_file: str, output_file: str, factor: float) -> None` that: - Takes an input WAV file (`input_file`), an output file (`output_file`), and a volume modification factor (`factor`). - Reads the input WAV file. - Modifies the volume of the audio by the given `factor`. For example, if `factor` is `2.0`, the volume of the output file should be twice that of the original file. If `factor` is `0.5`, the volume should be half. - Writes the modified audio to the output file. # Input Format: - `input_file`: A string representing the name/path of the input WAV file. - `output_file`: A string representing the name/path of the output WAV file. - `factor`: A float representing the factor by which the volume should be modified. # Constraints: - The input file is guaranteed to be in WAV format. - `factor` will be a positive float. # Example: Suppose you have an input WAV file named `\\"input.wav\\"`. If `factor` is `2.0`, your function should create an output WAV file `\\"output.wav\\"` that has its volume twice as loud as `\\"input.wav\\"`. # Notes: - Ensure your code handles opening and closing of files properly. - Handle both mono and stereo WAV files. # Implementation Guidelines: 1. Use the `wave` module to read the input WAV file. 2. Extract audio frames and convert them into a format that allows you to manipulate the volume. 3. Adjust the audio data based on the given `factor`. 4. Write the modified audio frames to the output WAV file using the appropriate `wave` module methods. ```python import wave import struct def amplify_wav(input_file: str, output_file: str, factor: float) -> None: # Open input WAV file with wave.open(input_file, \'rb\') as wf: params = wf.getparams() nchannels, sampwidth, framerate, nframes = params[:4] # Read frames frames = wf.readframes(nframes) frame_format = {1: \'B\', 2: \'h\', 4: \'i\'}[sampwidth] * nchannels frame_data = struct.unpack(frame_format * nframes, frames) # Amplify audio data amplified_data = tuple(int(sample * factor) for sample in frame_data) # Convert back to bytes amplified_frames = struct.pack(frame_format * nframes, *amplified_data) # Write to output WAV file with wave.open(output_file, \'wb\') as wf: wf.setparams(params) wf.writeframes(amplified_frames) # Example usage: # amplify_wav(\'input.wav\', \'output.wav\', 2.0) ``` Note: Make sure your code handles edge cases, such as clipping when the amplified values exceed the maximum allowable sample value for the given sample width.","solution":"import wave import struct def amplify_wav(input_file: str, output_file: str, factor: float) -> None: # Open input WAV file with wave.open(input_file, \'rb\') as wf: params = wf.getparams() nchannels, sampwidth, framerate, nframes = params[:4] # Read frames frames = wf.readframes(nframes) # Define the format for unpacking based on sample width and channels frame_format = {1: \'b\', 2: \'h\', 4: \'i\'}[sampwidth] * nchannels frame_data = struct.unpack(\'<\' + frame_format * nframes, frames) # Amplify or reduce audio data max_val = (2 ** (sampwidth * 8 - 1)) - 1 min_val = -(2 ** (sampwidth * 8 - 1)) amplified_data = [] for sample in frame_data: amplified_sample = int(sample * factor) # Clip the values to the max/min based on bit depth if amplified_sample > max_val: amplified_sample = max_val elif amplified_sample < min_val: amplified_sample = min_val amplified_data.append(amplified_sample) # Convert back to bytes amplified_frames = struct.pack(\'<\' + frame_format * nframes, *amplified_data) # Write to output WAV file with wave.open(output_file, \'wb\') as wf: wf.setparams(params) wf.writeframes(amplified_frames)"},{"question":"Objective Demonstrate your proficiency in generating synthetic data, preprocessing it, and using scikit-learn to train and evaluate a regression model. You will also address and resolve a specific warning that arises during model training. Problem Statement You are given a dataset creation task followed by training a machine learning model using scikit-learn. Specifically, you must: 1. Generate a synthetic regression dataset. 2. Preprocess the dataset using appropriate scaling methods. 3. Train a `GradientBoostingRegressor` model on the preprocessed dataset. 4. Modify a specific parameter in the model that triggers a known warning, and address the warning appropriately. 5. Evaluate the model\'s performance using a train-test split. Detailed Instructions 1. **Data Generation**: - Use `make_regression` from `sklearn.datasets` to create a dataset `X` with 1000 samples and 10 features (10 informative, no redundant features). - Add some noise to make the dataset more realistic (`noise=0.1`). 2. **Preprocessing**: - Split the dataset into training (70%) and testing (30%) sets. - Scale the features using `StandardScaler` from `sklearn.preprocessing`. 3. **Model Training**: - Train the `GradientBoostingRegressor` model from `sklearn.ensemble` with default parameters on the training dataset. - Evaluate the model using the `score` method on the test set, and print the result. 4. **Address the Warning**: - Modify the `n_iter_no_change` parameter of the `GradientBoostingRegressor` to `5`. - Fit the model again and capture any warnings or errors. - Adjust the code to avoid the warning without changing the data unnecessarily (hint: consider the format of your features). 5. **Evaluation**: - Print the evaluation score after addressing the warning. Constraints - Use only built-in functions and classes from scikit-learn. - The synthetic dataset generation and preprocessing steps must be included in the final code. - Do not import additional libraries except for those mentioned (numpy, pandas, scikit-learn). Expected Input and Output Format ```python def train_and_evaluate_model(): # Function Implementation pass train_and_evaluate_model() ``` The function should output the model performance score after both the initial and adjusted training tries. Ensure your final solution is clear, concise, and free of unnecessary steps.","solution":"from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor from sklearn.exceptions import ConvergenceWarning import warnings def train_and_evaluate_model(): # Step 1: Data Generation X, y = make_regression(n_samples=1000, n_features=10, n_informative=10, noise=0.1) # Step 2: Preprocessing X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 3: Model Training and Initial Evaluation model = GradientBoostingRegressor() model.fit(X_train_scaled, y_train) initial_score = model.score(X_test_scaled, y_test) # Step 4: Address the Warning with warnings.catch_warnings(): warnings.filterwarnings(\\"ignore\\", category=ConvergenceWarning) model = GradientBoostingRegressor(n_iter_no_change=5) model.fit(X_train_scaled, y_train) adjusted_score = model.score(X_test_scaled, y_test) # Step 5: Print Scores print(f\\"Initial Model Score: {initial_score}\\") print(f\\"Adjusted Model Score: {adjusted_score}\\") train_and_evaluate_model()"},{"question":"**Attention Mechanism Implementation using PyTorch** In this exercise, you are required to implement a custom scaled dot-product attention mechanism using the PyTorch `torch.nn.attention` module. Your implementation should leverage the `sdpa_kernel` utility function for the core attention computation. Function Signature ```python def custom_scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor: Computes the scaled dot-product attention. Parameters: query (torch.Tensor): Query tensor of shape (batch_size, num_heads, query_len, dim) key (torch.Tensor): Key tensor of shape (batch_size, num_heads, key_len, dim) value (torch.Tensor): Value tensor of shape (batch_size, num_heads, value_len, dim) mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions, shape (batch_size, 1, query_len, key_len). Default is None. Returns: torch.Tensor: The output of the attention mechanism, shape (batch_size, num_heads, query_len, dim) ``` Constraints - The dimensions of `query`, `key`, and `value` will satisfy the attention mechanismâ€™s requirements: the last dimension of `query`, `key` and `value` must be the same. - The mask tensor, if provided, will have elements in {0, 1}. - Performance should be optimized to use matrix multiplications efficiently. Instructions 1. **Scale the dot products**: Compute the dot product of `query` and `key`, scale it by the square root of the last dimension of the query. 2. **Apply the mask** (if provided): Ensure that positions with a mask value of `0` are not attended by setting their values to a very large negative number before applying softmax. 3. **Softmax over the last dimension**: Apply softmax to normalize the scores. 4. **Compute attention output**: Multiply the normalized scores by the `value` tensor to get the final attention outputs. Example ```python q = torch.randn(2, 4, 3, 8) # (batch_size=2, num_heads=4, query_len=3, dim=8) k = torch.randn(2, 4, 5, 8) # (batch_size=2, num_heads=4, key_len=5, dim=8) v = torch.randn(2, 4, 5, 8) # (batch_size=2, num_heads=4, value_len=5, dim=8) mask = torch.ones(2, 1, 3, 5) # (batch_size=2, 1, query_len=3, key_len=5) output = custom_scaled_dot_product_attention(q, k, v, mask) print(output.shape) # Should print torch.Size([2, 4, 3, 8]) ``` Good luck!","solution":"import torch import torch.nn.functional as F def custom_scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor: Computes the scaled dot-product attention. Parameters: query (torch.Tensor): Query tensor of shape (batch_size, num_heads, query_len, dim) key (torch.Tensor): Key tensor of shape (batch_size, num_heads, key_len, dim) value (torch.Tensor): Value tensor of shape (batch_size, num_heads, value_len, dim) mask (torch.Tensor, optional): Mask tensor of shape (batch_size, 1, query_len, key_len). Default is None. Returns: torch.Tensor: The output of the attention mechanism, shape (batch_size, num_heads, query_len, dim) # Ensure the input dimensions are compatible assert query.size(-1) == key.size(-1) == value.size(-1) dim = query.size(-1) # Calculate the dot products and scale by the square root of the dimension scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(dim, dtype=torch.float32)) # Apply the mask if provided if mask is not None: scores = scores.masked_fill(mask == 0, float(\'-inf\')) # Compute the attention probabilities attn_weights = F.softmax(scores, dim=-1) # Compute the attention output output = torch.matmul(attn_weights, value) return output"},{"question":"**Problem Statement:** You are tasked with implementing a Python program that manages a simple phonebook using the `dbm` module. Your program should be capable of performing the following operations: 1. **Open a phonebook database:** - Ensure the database is opened with read/write access, creating it if it does not exist (`flag=\'c\'`). 2. **Add or Update an entry:** - Function: `add_entry(db, name, phone)` - Inputs: - `db`: The database object. - `name`: The name of the contact (string). - `phone`: The phone number of the contact (string). - Behavior: Add a new contact or update the phone number if the contact already exists. 3. **Retrieve a phone number:** - Function: `retrieve_entry(db, name)` - Inputs: - `db`: The database object. - `name`: The name of the contact (string). - Output: The phone number of the contact (string) or `None` if the contact does not exist. 4. **Delete an entry:** - Function: `delete_entry(db, name)` - Inputs: - `db`: The database object. - `name`: The name of the contact (string). - Behavior: Delete the contact from the phonebook. If the contact does not exist, do nothing. 5. **List all entries:** - Function: `list_entries(db)` - Inputs: - `db`: The database object. - Output: A list of tuples representing all the entries in the format `(name, phone)`. Implement these functions ensuring: - Proper conversion to and from bytes where necessary. - Efficiently handling errors that may arise from database operations. **Solution Skeleton:** ```python import dbm def open_database(file_name): Open or create a database for the phonebook. :param file_name: The name of the database file (string). :return: The opened database object. return dbm.open(file_name, \'c\') def add_entry(db, name, phone): Add or update an entry in the phonebook. :param db: The database object. :param name: The name of the contact (string). :param phone: The phone number of the contact (string). db[name.encode()] = phone.encode() def retrieve_entry(db, name): Retrieve the phone number for a given contact. :param db: The database object. :param name: The name of the contact (string). :return: The phone number (string) or None if contact does not exist. return db.get(name.encode(), None).decode() if db.get(name.encode(), None) else None def delete_entry(db, name): Delete an entry from the phonebook. :param db: The database object. :param name: The name of the contact (string). if name.encode() in db: del db[name.encode()] def list_entries(db): List all entries in the phonebook. :param db: The database object. :return: A list of tuples with each entry as (name, phone). return [(key.decode(), value.decode()) for key, value in db.items()] # Example Usage if __name__ == \\"__main__\\": with open_database(\\"phonebook\\") as db: add_entry(db, \\"Alice\\", \\"123-456-7890\\") add_entry(db, \\"Bob\\", \\"234-567-8901\\") print(retrieve_entry(db, \\"Alice\\")) # Output: 123-456-7890 delete_entry(db, \\"Bob\\") print(list_entries(db)) # Output: [(\'Alice\', \'123-456-7890\')] ``` **Constraints:** - You can assume the database file name provided is always valid. - Entries will not exceed reasonable memory limits. - Phone numbers are always stored as strings of digits and possible dashes. **Performance requirements:** - Ensure all operations execute in reasonable time (ideally O(1) for add, retrieve, and delete operations).","solution":"import dbm def open_database(file_name): Open or create a database for the phonebook. :param file_name: The name of the database file (string). :return: The opened database object. return dbm.open(file_name, \'c\') def add_entry(db, name, phone): Add or update an entry in the phonebook. :param db: The database object. :param name: The name of the contact (string). :param phone: The phone number of the contact (string). db[name.encode()] = phone.encode() def retrieve_entry(db, name): Retrieve the phone number for a given contact. :param db: The database object. :param name: The name of the contact (string). :return: The phone number (string) or None if contact does not exist. return db.get(name.encode(), None).decode() if db.get(name.encode(), None) else None def delete_entry(db, name): Delete an entry from the phonebook. :param db: The database object. :param name: The name of the contact (string). if name.encode() in db: del db[name.encode()] def list_entries(db): List all entries in the phonebook. :param db: The database object. :return: A list of tuples with each entry as (name, phone). return [(key.decode(), value.decode()) for key, value in db.items()] # Example Usage if __name__ == \\"__main__\\": with open_database(\\"phonebook\\") as db: add_entry(db, \\"Alice\\", \\"123-456-7890\\") add_entry(db, \\"Bob\\", \\"234-567-8901\\") print(retrieve_entry(db, \\"Alice\\")) # Output: 123-456-7890 delete_entry(db, \\"Bob\\") print(list_entries(db)) # Output: [(\'Alice\', \'123-456-7890\')]"},{"question":"You are tasked with implementing a CGI script using Python that handles uncaught exceptions using the `cgitb` module. This script should demonstrate the ability to capture detailed tracebacks and optionally log them to a file for further analysis. # Task: 1. Implement a function `setup_cgi_traceback(display, logdir, context, format)`. This function should activate the cgitb module to handle exceptions, taking the following parameters: - `display`: If set to 1, the traceback should be displayed in the browser. Otherwise, it should not be displayed. - `logdir`: If provided, this should be the directory path where traceback reports should be logged. - `context`: The number of lines of context to display around the current line of source code. Defaults to 5. - `format`: The format of the output report. Should be either `\\"html\\"` or `\\"text\\"`. 2. Create a simple CGI script with a deliberate error to demonstrate the functionality of the `cgitb` module. The script should raise an exception when a particular endpoint is accessed. # Input: - `display`: An integer (0 or 1). - `logdir`: A string or None. - `context`: An integer. - `format`: A string (\\"html\\" or \\"text\\"). # Output: - Properly captured and displayed or logged traceback information based on the parameters provided. # Constraints: - You must handle invalid input for the parameters gracefully. - If `logdir` is provided, ensure that the directory exists and is writable. # Example Usage: ```python def setup_cgi_traceback(display, logdir, context, format): # Implementation here # Example CGI script import cgitb import cgi def main(): # Setup cgitb for handling exceptions setup_cgi_traceback(display=1, logdir=None, context=5, format=\'html\') form = cgi.FieldStorage() if \\"trigger_error\\" in form: raise ValueError(\\"Deliberate error triggered!\\") # Simulating CGI script execution if __name__ == \\"__main__\\": main() ``` In the above script, `setup_cgi_traceback` would enable the `cgitb` module to catch and handle exceptions, displaying them in the browser or logging them as specified. Make sure to test the functionality by creating deliberate errors in the CGI script and observing the generated traceback reports.","solution":"import os import cgi import cgitb def setup_cgi_traceback(display=1, logdir=None, context=5, format=\'html\'): Sets up the cgitb module to handle uncaught exceptions in a CGI script. Parameters: - display (int): If set to 1, the traceback will be displayed in the browser. - logdir (str or None): Directory path where traceback reports should be logged. - context (int): Number of lines of context to display around the current line of source code. - format (str): The format of the output report (\\"html\\" or \\"text\\"). if format not in [\'html\', \'text\']: raise ValueError(\\"Format must be \'html\' or \'text\'\\") cgitb_args = {\'context\': context, \'format\': format} if display == 1: cgitb_args[\'display\'] = 1 else: cgitb_args[\'display\'] = 0 if logdir: if not os.path.exists(logdir): os.makedirs(logdir) elif not os.access(logdir, os.W_OK): raise PermissionError(f\\"Log directory {logdir} is not writable.\\") cgitb_args[\'logdir\'] = logdir cgitb.enable(**cgitb_args) # Example CGI script with deliberate error def main(): setup_cgi_traceback(display=1, logdir=None, context=5, format=\'html\') form = cgi.FieldStorage() if \\"trigger_error\\" in form: raise ValueError(\\"Deliberate error triggered!\\") # Simulating CGI script execution if __name__ == \\"__main__\\": main()"},{"question":"Objective You are tasked with creating a visualization that displays the survival rate of Titanic passengers categorized by their class and age group, using the Seaborn library\'s `objects` interface. Task 1. Load the Titanic dataset using Seaborn\'s `load_dataset` function. 2. Create a new column in the dataset called `age_group` to categorize passengers into the following age groups: - \'Child\' for ages 0-12 - \'Teenager\' for ages 13-17 - \'Young Adult\' for ages 18-25 - \'Adult\' for ages 26-50 - \'Senior\' for ages 51 and above 3. Generate a bar plot that displays the survival rate of passengers for each age group within each class. The plot should be faceted by gender and colored by survival status. Constraints - Ensure the plot is well-labeled and includes a legend to indicate survival status. - The `age_group` column should be properly categorized and ordered. Code Requirements Write a Python function `plot_titanic_survival_by_age_group` that executes the following steps: 1. Loads the Titanic dataset. 2. Computes the `age_group` column based on the provided ranges. 3. Generates the required faceted bar plot. Expected Input and Output - **Input:** None - **Output:** A matplotlib plot displayed inline. Function Signature ```python def plot_titanic_survival_by_age_group(): # your code here pass ``` Example ```python plot_titanic_survival_by_age_group() ``` The function call should produce a plot similar to the one described in the task. Note: Ensure you include necessary comments in your code to explain each step.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_titanic_survival_by_age_group(): # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Define age group categorization function def categorize_age(age): if age < 13: return \'Child\' elif age < 18: return \'Teenager\' elif age < 26: return \'Young Adult\' elif age < 51: return \'Adult\' else: return \'Senior\' # Create a new column \'age_group\' titanic[\'age_group\'] = titanic[\'age\'].apply(lambda age: categorize_age(age) if pd.notnull(age) else None) # Drop any rows with missing \'age_group\' values titanic = titanic.dropna(subset=[\'age_group\']) # Plotting the survival rate by age group and class, faceted by gender, colored by survival status g = sns.catplot( data=titanic, kind=\'bar\', x=\'age_group\', hue=\'survived\', col=\'sex\', row=\'class\', palette=\'muted\', height=4, aspect=2, legend=True ) # Customizing the plot g.set_axis_labels(\\"Age Group\\", \\"Survival Rate\\") g.set_titles(\\"{row_name}, {col_name}\\") g.add_legend(title=\\"Survived\\") plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Survival Rate of Titanic Passengers by Age Group\', fontsize=16) # Show the plot plt.show()"},{"question":"You are required to write a Python script that processes command-line arguments to perform basic arithmetic operations. The script should use the `argparse` module and demonstrate the use of positional arguments, optional arguments, mutually exclusive arguments, and description/help messages as follows: # Task: 1. **Create an `ArgumentParser` instance** with a description \\"Perform basic arithmetic operations\\". 2. **Add positional arguments** for the two operands (`operand1` and `operand2`). The operands should be of type float. 3. **Add an optional argument** for the operation (`-o` or `--operation`), which can take values \\"add\\", \\"sub\\", \\"mul\\", \\"div\\". The default operation should be \\"add\\". 4. **Add mutually exclusive flags** for detailed output (`-d` or `--detailed`) and quiet mode (`-q` or `--quiet`). Only one of these flags should be allowed at a time. 5. **Implement the functionality** to perform the specified operation on the operands and print the result. If the `--detailed` flag is set, print a detailed explanation of the operation. If the `--quiet` flag is set, print only the result. Otherwise, print a default summary of the operation. # Input and Output: - **Positional Arguments**: - `operand1` (type float) - `operand2` (type float) - **Optional Arguments**: - `-o` or `--operation` (type str, choices=[\\"add\\", \\"sub\\", \\"mul\\", \\"div\\"], default=\\"add\\") - `-d` or `--detailed` (flag) - `-q` or `--quiet` (flag) # Example Usage: 1. Basic addition (default operation): ``` python3 script.py 5 3 5.0 + 3.0 = 8.0 ``` 2. Subtraction with detailed output: ``` python3 script.py 5 3 -o sub -d Detailed output: 5.0 - 3.0 = 2.0 ``` 3. Multiplication in quiet mode: ``` python3 script.py 5 3 -o mul -q 15.0 ``` 4. Division with default summary: ``` python3 script.py 5 3 -o div 5.0 / 3.0 = 1.6666667 ``` # Constraints: - Ensure proper handling of division by zero. - If both `--detailed` and `--quiet` flags are set, the script should display an appropriate error message. # Implementation: ```python import argparse def main(): parser = argparse.ArgumentParser(description=\\"Perform basic arithmetic operations\\") parser.add_argument(\\"operand1\\", type=float, help=\\"First operand\\") parser.add_argument(\\"operand2\\", type=float, help=\\"Second operand\\") parser.add_argument(\\"-o\\", \\"--operation\\", type=str, choices=[\\"add\\", \\"sub\\", \\"mul\\", \\"div\\"], default=\\"add\\", help=\\"Arithmetic operation\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-d\\", \\"--detailed\\", action=\\"store_true\\", help=\\"Display detailed output\\") group.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"Display only the result\\") args = parser.parse_args() operand1 = args.operand1 operand2 = args.operand2 operation = args.operation result = None if operation == \\"add\\": result = operand1 + operand2 op_sign = \\"+\\" elif operation == \\"sub\\": result = operand1 - operand2 op_sign = \\"-\\" elif operation == \\"mul\\": result = operand1 * operand2 op_sign = \\"*\\" elif operation == \\"div\\": if operand2 == 0: print(\\"Error: Division by zero is not allowed.\\") return result = operand1 / operand2 op_sign = \\"/\\" if args.detailed: print(f\\"Detailed output:n{operand1} {op_sign} {operand2} = {result}\\") elif args.quiet: print(result) else: print(f\\"{operand1} {op_sign} {operand2} = {result}\\") if __name__ == \\"__main__\\": main() ``` # Your Task: 1. Implement the script according to the specifications above. 2. Test your script to ensure it handles all specified functionalities and edge cases.","solution":"import argparse def main(): parser = argparse.ArgumentParser(description=\\"Perform basic arithmetic operations\\") parser.add_argument(\\"operand1\\", type=float, help=\\"First operand\\") parser.add_argument(\\"operand2\\", type=float, help=\\"Second operand\\") parser.add_argument(\\"-o\\", \\"--operation\\", type=str, choices=[\\"add\\", \\"sub\\", \\"mul\\", \\"div\\"], default=\\"add\\", help=\\"Arithmetic operation\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-d\\", \\"--detailed\\", action=\\"store_true\\", help=\\"Display detailed output\\") group.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"Display only the result\\") args = parser.parse_args() operand1 = args.operand1 operand2 = args.operand2 operation = args.operation result = None if operation == \\"add\\": result = operand1 + operand2 op_sign = \\"+\\" elif operation == \\"sub\\": result = operand1 - operand2 op_sign = \\"-\\" elif operation == \\"mul\\": result = operand1 * operand2 op_sign = \\"*\\" elif operation == \\"div\\": if operand2 == 0: print(\\"Error: Division by zero is not allowed.\\") return result = operand1 / operand2 op_sign = \\"/\\" if args.detailed: print(f\\"Detailed output:n{operand1} {op_sign} {operand2} = {result}\\") elif args.quiet: print(result) else: print(f\\"{operand1} {op_sign} {operand2} = {result}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question:** # Objective: To assess the ability to create and manipulate enumerations using the `enum` module in Python. # Problem Statement: You are required to create an enumeration for a traffic light system and perform various operations on it. The traffic light system has three states: `RED`, `YELLOW`, and `GREEN`. Additionally, you need to create a flag enumeration for representing different modes a traffic light can be in (e.g., NORMAL, BLINKING, etc.) and a derived `IntEnum` for different traffic light priorities. # Tasks: 1. **Define an Enum `TrafficLight`**: - The enum should have the following members: - `RED` with a value of `1` - `YELLOW` with a value of `2` - `GREEN` with a value of `3` 2. **Define a Flag Enum `TrafficMode`**: - The flag enum should have the following members: - `NORMAL` with an automatic value. - `BLINKING` with an automatic value. - `FLASHING` as the combination of `NORMAL` and `BLINKING`. 3. **Define an IntEnum `LightPriority`**: - The enum should have the following members: - `HIGH` with a value of `5` - `MEDIUM` with a value of `3` - `LOW` with a value of `1` 4. **Implement the following functions**: 1. `get_light_color(value: int) -> str`: - This function takes an integer value and returns the corresponding traffic light color name. - Raise a `ValueError` if the value does not correspond to any `TrafficLight` member. 2. `is_mode_blinking(mode: TrafficMode) -> bool`: - This function takes a `TrafficMode` and returns `True` if the mode includes `BLINKING`; otherwise, `False`. 3. `compare_priorities(priority1: LightPriority, priority2: LightPriority) -> str`: - This function takes two `LightPriority` values and returns a string indicating which one has a higher priority. The return values should be `\'HIGHER\'`, `\'LOWER\'`, or `\'EQUAL\'`. # Constraints: - Do not use any libraries or modules other than `enum`. - The auto values in `TrafficMode` should start from 1. - Ensure that all your functions handle edge cases appropriately and provide meaningful error messages where necessary. # Example: ```python # Example usage of your enums and functions print(TrafficLight.RED) # Output: TrafficLight.RED print(TrafficLight.RED.value) # Output: 1 print(TrafficMode.FLASHING) # Output: TrafficMode.FLASHING print(TrafficMode.FLASHING.value) # Output: 3 (or another composite value if different) print(LightPriority.HIGH) # Output: LightPriority.HIGH print(LightPriority.HIGH.value) # Output: 5 print(get_light_color(2)) # Output: YELLOW print(is_mode_blinking(TrafficMode.FLASHING)) # Output: True print(compare_priorities(LightPriority.HIGH, LightPriority.LOW)) # Output: HIGHER ``` Notes: - Include comprehensive docstrings and comments explaining your code. - Write your own test cases to verify the correctness of your implementations. - Make sure to handle invalid inputs gracefully.","solution":"from enum import Enum, Flag, auto, IntEnum class TrafficLight(Enum): RED = 1 YELLOW = 2 GREEN = 3 class TrafficMode(Flag): NORMAL = auto() BLINKING = auto() FLASHING = NORMAL | BLINKING class LightPriority(IntEnum): HIGH = 5 MEDIUM = 3 LOW = 1 def get_light_color(value: int) -> str: Returns the name of the traffic light color corresponding to the given value. Args: value (int): The value of the TrafficLight enum member. Returns: str: The name of the TrafficLight color. Raises: ValueError: If the value does not correspond to any TrafficLight member. try: return TrafficLight(value).name except ValueError: raise ValueError(f\'Invalid value for TrafficLight: {value}\') def is_mode_blinking(mode: TrafficMode) -> bool: Checks if the given TrafficMode includes BLINKING. Args: mode (TrafficMode): The traffic mode. Returns: bool: True if the mode includes BLINKING, False otherwise. return TrafficMode.BLINKING in mode def compare_priorities(priority1: LightPriority, priority2: LightPriority) -> str: Compares two LightPriority values and returns which one is higher. Args: priority1 (LightPriority): The first priority. priority2 (LightPriority): The second priority. Returns: str: \'HIGHER\' if priority1 is higher than priority2, \'LOWER\' if lower, \'EQUAL\' if the same. if priority1 > priority2: return \\"HIGHER\\" elif priority1 < priority2: return \\"LOWER\\" else: return \\"EQUAL\\""},{"question":"<|Analysis Begin|> The provided documentation outlines the functionalities and examples of the `multiprocessing.shared_memory` module, which allows for the allocation and management of shared memory blocks for inter-process communication on a multicore machine. The key classes in this module are `SharedMemory` and `SharedMemoryManager`, with `SharedMemory` providing low-level shared memory access and `SharedMemoryManager` offering a higher-level management interface along with the `ShareableList` list-like object. Key points for crafting a question: 1. **SharedMemory class**: - Creation and attachment of shared memory blocks. - Reading and writing data to shared memory. - Proper cleanup using `close()` and `unlink()` methods. 2. **SharedMemoryManager class**: - Management of shared memory blocks across processes. - Automatic cleanup of shared memory blocks. 3. **ShareableList class**: - A list-like object backed by shared memory, allowing shared storage of specific data types. - Unchangeable length, parts of typical list operations like insert and append are unsupported. Considering the challenge level and necessary coverage of fundamental and advanced concepts: - The question should involve creating and manipulating shared memory blocks. - Since the `ShareableList` has more specific usage and constraints, focusing on its unique constraints can provide a deeper assessment of understanding. - The task requires interaction between multiple processes, demonstrating inter-process communication via shared memory. <|Analysis End|> <|Question Begin|> # Python Coding Assessment Question Objective: Demonstrate your understanding of Python\'s `multiprocessing.shared_memory` module, particularly working with `SharedMemory`, `SharedMemoryManager`, and `ShareableList`. Problem Statement: You are tasked with creating a simulation where multiple processes interact and modify a shared list of computational results. Each process will compute a specified range of the Fibonacci sequence and store its results in a shared memory list. Your goal is to manage this shared memory and ensure it is properly cleaned up after the computation is complete. Steps: 1. Create a `SharedMemoryManager` instance and use it to create a `ShareableList`. 2. Implement a function `compute_fibonacci(start_idx, end_idx, shared_list)` that computes Fibonacci numbers from the `start_idx` to `end_idx` and stores them in the provided `shared_list`. 3. Spawn multiple processes, each responsible for computing different parts of the Fibonacci sequence and storing the results in the shared list. 4. Ensure that all processes complete their computation. 5. Ensure proper cleanup of the shared memory resources once all processes are done. Requirements: - The `ShareableList` should be capable of storing at least the first 50 Fibonacci numbers. - The processes should divide the computation equally among themselves. - Do not use global variables to share data between processes; rely solely on shared memory mechanisms. - Properly close and unlink the shared memory objects once computation is complete to release system resources. Constraints: - The computation should handle large numbers efficiently. - The length of the `ShareableList` cannot be dynamically altered once created. Function Signature: ```python from multiprocessing import shared_memory, Process def compute_fibonacci(start_idx: int, end_idx: int, shared_list: shared_memory.ShareableList) -> None: Computes Fibonacci numbers from start_idx to end_idx and stores the results in the provided shared_list. Args: start_idx (int): The starting index in the Fibonacci sequence to compute. end_idx (int): The ending index (exclusive) in the Fibonacci sequence to compute. shared_list (shared_memory.ShareableList): The shared list to store Fibonacci results. pass def main(): Main function to setup SharedMemoryManager, create ShareableList, spawn processes to compute Fibonacci sequence, and ensure proper cleanup. pass if __name__ == \\"__main__\\": main() ``` Example: After running the main function, the shared list should contain the first 50 Fibonacci numbers, accurately computed and stored by different processes. Good luck!","solution":"from multiprocessing import shared_memory, Process from multiprocessing.managers import SharedMemoryManager from multiprocessing.shared_memory import ShareableList def compute_fibonacci(start_idx, end_idx, shared_list): def fibonacci(n): if n <= 1: return n a, b = 0, 1 for _ in range(2, n + 1): a, b = b, a + b return b for i in range(start_idx, end_idx): shared_list[i] = fibonacci(i) def main(): # Length of the list to store first 50 Fibonacci numbers list_length = 50 # Create a SharedMemoryManager with SharedMemoryManager() as smm: # Create a ShareableList with length 50 shared_list = smm.ShareableList([0] * list_length) # Number of processes num_processes = 5 processes = [] chunk_size = list_length // num_processes # Create and start processes for i in range(num_processes): start_idx = i * chunk_size end_idx = list_length if i == num_processes - 1 else (i + 1) * chunk_size p = Process(target=compute_fibonacci, args=(start_idx, end_idx, shared_list)) processes.append(p) p.start() # Wait for all processes to complete for p in processes: p.join() # Print the shared list to verify results print(list(shared_list)) # No need to explicitly close or unlink the ShareableList; it\'s managed by SharedMemoryManager if __name__ == \\"__main__\\": main()"},{"question":"Question As an experienced Python programmer, you are required to create a function that demonstrates an understanding of the `warnings` module and its features. Your function should manage and issue warnings in different scenarios, handling them in a customized manner. This exercise will test your comprehension of the warning control mechanisms in Python. # Task Write a function `custom_warning_handler(condition: bool, message: str, number: int) -> str` that: 1. Accepts three parameters: - `condition` (bool): A boolean flag to determine if a warning should be issued or not. - `message` (str): A warning message string that needs to be displayed. - `number` (int): A number that will dictate the category of the warning. 2. Depending on the value of `number`, the warning should be categorized as follows: - If `number` is 1, the warning should be categorized as `UserWarning`. - If `number` is 2, the warning should be categorized as `DeprecationWarning`. - For any other value, the warning should be categorized as `RuntimeWarning`. 3. When `condition` is `True`, issue the warning. When `condition` is `False`, suppress the warning. 4. Override the default warning output to write the warning message to a string and return this string from the function using a context manager. # Constraints - You must use the `warnings` module to categorize and issue warnings. - You should handle the warning output using the `catch_warnings` context manager from the `warnings` module. # Example ```python def test_custom_warning_handler(): result_1 = custom_warning_handler(True, \\"This is a user warning.\\", 1) assert \\"UserWarning: This is a user warning.\\" in result_1 result_2 = custom_warning_handler(True, \\"This feature is deprecated.\\", 2) assert \\"DeprecationWarning: This feature is deprecated.\\" in result_2 result_3 = custom_warning_handler(False, \\"This is a runtime warning.\\", 3) assert result_3 == \\"\\" result_4 = custom_warning_handler(True, \\"Another runtime warning.\\", 3) assert \\"RuntimeWarning: Another runtime warning.\\" in result_4 test_custom_warning_handler() ``` # Hints 1. Use `warnings.warn()` to issue a warning. 2. Use the `catch_warnings` context manager to capture warnings. 3. Customize the action of the warning filter by using `warnings.simplefilter()` to ignore or display the warning based on the `condition` parameter. 4. Explore the attributes `message` and `category` of the captured warning object to format the output message.","solution":"import warnings from io import StringIO def custom_warning_handler(condition: bool, message: str, number: int) -> str: Issues a warning based on the given condition and number, capturing the warning message using a custom warning handler. # Create a mapping from number to warning category category_map = { 1: UserWarning, 2: DeprecationWarning } category = category_map.get(number, RuntimeWarning) # Prepare to capture warnings with warnings.catch_warnings(record=True) as w: # Configure the warnings to be caught warnings.simplefilter(\\"always\\") # Conditionally issue a warning if condition: warnings.warn(message, category) # Check if any warnings were captured if w: warning = w[0] return f\\"{warning.category.__name__}: {warning.message}\\" return \\"\\""},{"question":"# **Question: Implement Custom Method Binding in Python** You are required to implement custom method binding in Python, mimicking the behavior of method objects and instance method objects as described in the provided documentation. The objective is to understand and implement the concepts of binding functions to class instances. # **Details** 1. **CustomInstanceMethod Class**: Create a class `CustomInstanceMethod` that wraps a function and binds it to an instance of a class. - **Initialization**: ```python def __init__(self, func, instance): Initialize the instance method with the provided function and instance. :param func: The callable function to be bound. :param instance: The instance to which the function is bound. ``` - **Call**: ```python def __call__(self, *args, **kwargs): Call the bound function with the instance as the first argument. :return: The result of the function call. ``` 2. **CustomMethod Class**: Create a class `CustomMethod` that wraps a function and binds it to either an instance or a class. - **Initialization**: ```python def __init__(self, func, instance=None): Initialize the method with the provided function. Optionally bind it to an instance. :param func: The callable function to be wrapped. :param instance: The optional instance to which the function is bound. ``` - **Bind to Instance**: Create a method to bind this function to a specific instance later. ```python def bind_to_instance(self, instance): Bind the wrapped function to the provided instance. :param instance: The instance to which the function is bound. ``` - **Call**: ```python def __call__(self, *args, **kwargs): Call the wrapped function with the instance if bound, otherwise call it as a regular function. :return: The result of the function call. ``` # **Expected Input and Output** - **Input**: You do not need to handle any input for your classes directly. However, you should be able to create instances of these classes and bind functions appropriately. - **Output**: Correct execution of bound functions. # **Constraints** - You may assume that the function provided to these classes will always be a callable. - The instances provided will be valid instances of user-defined classes. # **Example Usage** ```python class MyClass: def __init__(self, value): self.value = value def my_function(obj, increment): return obj.value + increment # CustomInstanceMethod usage obj = MyClass(10) inst_method = CustomInstanceMethod(my_function, obj) print(inst_method(5)) # Output: 15 # CustomMethod usage meth = CustomMethod(my_function) meth.bind_to_instance(obj) print(meth(5)) # Output: 15 ``` Implement the `CustomInstanceMethod` and `CustomMethod` classes to achieve the above behavior.","solution":"class CustomInstanceMethod: def __init__(self, func, instance): Initialize the instance method with the provided function and instance. :param func: The callable function to be bound. :param instance: The instance to which the function is bound. self.func = func self.instance = instance def __call__(self, *args, **kwargs): Call the bound function with the instance as the first argument. :return: The result of the function call. return self.func(self.instance, *args, **kwargs) class CustomMethod: def __init__(self, func, instance=None): Initialize the method with the provided function. Optionally bind it to an instance. :param func: The callable function to be wrapped. :param instance: The optional instance to which the function is bound. self.func = func self.instance = instance def bind_to_instance(self, instance): Bind the wrapped function to the provided instance. :param instance: The instance to which the function is bound. self.instance = instance def __call__(self, *args, **kwargs): Call the wrapped function with the instance if bound, otherwise call it as a regular function. :return: The result of the function call. if self.instance is not None: return self.func(self.instance, *args, **kwargs) else: return self.func(*args, **kwargs)"},{"question":"**MIME Content Management with Custom Handlers** As part of developing an email processing application, you are tasked with creating a simplified MIME content manager. This content manager should have the capability to set and get content from a message, similar to the built-in `ContentManager` class described in the documentation. Your content manager will register custom handlers for specific MIME types and types of objects. # Problem Specification: 1. **Define a class `MyContentManager`**: - This class should have methods `get_content` and `set_content` to retrieve and set content in a message object. - It should also have methods `add_get_handler` and `add_set_handler` to register custom handlers for retrieving and setting content. 2. **get_content Method**: - The `get_content` method should take a message object `msg` and return the content after processing with the appropriate handler. - It should look up the appropriate handler based on the message MIME type and return the content. 3. **set_content Method**: - The `set_content` method should take a message object `msg` and an object `obj`, and store the object into the message. - It should look up the appropriate handler based on the type of the object and mutate the message object accordingly. 4. **add_get_handler Method**: - The `add_get_handler` method should take a key and a handler function, and register the handler for retrieving content based on the key. 5. **add_set_handler Method**: - The `add_set_handler` method should take a type and a handler function, and register the handler for setting content based on the type. # Constraints: - You can assume the `msg` object has a `payload` attribute which stores the content. - The MIME type of the `msg` object can be obtained via a `get_mime_type` method that returns a string like `\\"text/plain\\"`. # Input Format: - You will be using the methods of `MyContentManager` within a series of operations. There will be methods to set and get content illustrated in the examples below. # Output Format: - The `get_content` method should return the content extracted from the `msg` object. - The `set_content` method should update the `msg` object with the content from the `obj`. # Example Usage: ```python class Message: def __init__(self, mime_type, payload=None): self.mime_type = mime_type self.payload = payload def get_mime_type(self): return self.mime_type def main(): cm = MyContentManager() def text_get_handler(msg, *args, **kwargs): return msg.payload.decode(\'utf-8\') def text_set_handler(msg, obj, *args, **kwargs): msg.payload = obj.encode(\'utf-8\') cm.add_get_handler(\'text/plain\', text_get_handler) cm.add_set_handler(str, text_set_handler) msg = Message(\'text/plain\') cm.set_content(msg, \\"Hello, world\\") content = cm.get_content(msg) print(content) # Output: Hello, world if __name__ == \\"__main__\\": main() ``` # Implementation: Implement the class `MyContentManager` with the required methods and ensure they adhere to the functionality described: ```python class MyContentManager: def __init__(self): self.get_handlers = {} self.set_handlers = {} def get_content(self, msg, *args, **kwargs): mime_type = msg.get_mime_type() handler = self.get_handlers.get(mime_type) if handler: return handler(msg, *args, **kwargs) raise KeyError(f\\"No handler for MIME type: {mime_type}\\") def set_content(self, msg, obj, *args, **kwargs): typekey = type(obj) handler = self.set_handlers.get(typekey) if handler: handler(msg, obj, *args, **kwargs) else: raise KeyError(f\\"No handler for type: {type(obj).__name__}\\") def add_get_handler(self, key, handler): self.get_handlers[key] = handler def add_set_handler(self, typekey, handler): self.set_handlers[typekey] = handler ```","solution":"class MyContentManager: def __init__(self): self.get_handlers = {} self.set_handlers = {} def get_content(self, msg, *args, **kwargs): mime_type = msg.get_mime_type() handler = self.get_handlers.get(mime_type) if handler: return handler(msg, *args, **kwargs) raise KeyError(f\\"No handler for MIME type: {mime_type}\\") def set_content(self, msg, obj, *args, **kwargs): typekey = type(obj) handler = self.set_handlers.get(typekey) if handler: handler(msg, obj, *args, **kwargs) else: raise KeyError(f\\"No handler for type: {type(obj).__name__}\\") def add_get_handler(self, key, handler): self.get_handlers[key] = handler def add_set_handler(self, typekey, handler): self.set_handlers[typekey] = handler"},{"question":"Objective Implement a custom descriptor that ensures attribute values meet specific validation rules and log all access and modification attempts. Description You are required to implement a Python class that uses descriptors to manage attribute access. The descriptor will not only validate the attribute values based on given constraints but also log every access and modification attempt to the attribute. You will use the concepts discussed in the documentation. Requirements 1. Implement a `LoggedValidator` class that inherits from the `Validator` class. 2. The `LoggedValidator` must log every access and modification attempt to the attribute that uses it. 3. The `validate` method should enforce the following constraints depending on the subclass: - `OneOfLogged`: ensures the attribute is one of a specified set of options. - `NumberLogged`: ensures the attribute is an integer or float, and optionally within specified bounds. - `StringLogged`: ensures the attribute is a string, and optionally within specified length bounds. 4. Implement a `Person` class with attributes `name`, `age`, and `occupation`. - `name`: a string that must be in uppercase and length between 3 to 10. - `age`: a number that must be non-negative. - `occupation`: one of the options from \'Engineer\', \'Doctor\', \'Artist\'. Specifications 1. Implement the `LoggedValidator` class with appropriate logging in `__get__` and `__set__` methods. 2. Implement three subclasses of `LoggedValidator`: - `OneOfLogged` - `NumberLogged` - `StringLogged` 3. Use the `LoggedValidator` subclasses to define managed attributes in the `Person` class. Constraints - Log messages should include the attribute name, action (access or modification), and value. - Raise appropriate exceptions if validation fails. Test Cases 1. Creating a `Person` instance with valid attributes. 2. Accessing attribute values should log the access. 3. Modifying attributes with valid values should log the modification. 4. Invalid attribute modifications should log the failed attempt and raise an exception. Example Output ```python import logging logging.basicConfig(level=logging.INFO) class Validator: # Use the provided Validator class implementation class LoggedValidator(Validator): def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): value = getattr(obj, self.private_name) logging.info(f\'Accessing {self.public_name} with value {value}\') return value def __set__(self, obj, value): self.validate(value) logging.info(f\'Setting {self.public_name} to value {value}\') setattr(obj, self.private_name, value) # Add any additional required methods class OneOfLogged(LoggedValidator): def __init__(self, *options): self.options = set(options) def validate(self, value): if value not in self.options: raise ValueError(f\'Expected {value!r} to be one of {self.options!r}\') class NumberLogged(LoggedValidator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(f\'Expected {value!r} to be an int or float\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected {value!r} to be at least {self.minvalue!r}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected {value!r} to be no more than {self.maxvalue!r}\') class StringLogged(LoggedValidator): def __init__(self, minsize=None, maxsize=None, predicate=None): self.minsize = minsize self.maxsize = maxsize self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected {value!r} to be a str\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'Expected {value!r} to be no smaller than {self.minsize!r}\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'Expected {value!r} to be no bigger than {self.maxsize!r}\') if self.predicate is not None and not self.predicate(value): raise ValueError(f\'Expected {self.predicate} to be true for {value!r}\') class Person: name = StringLogged(minsize=3, maxsize=10, predicate=str.isupper) age = NumberLogged(minvalue=0) occupation = OneOfLogged(\'Engineer\', \'Doctor\', \'Artist\') def __init__(self, name, age, occupation): self.name = name self.age = age self.occupation = occupation # Example usage: p = Person(\'ALICE\', 30, \'Engineer\') print(p.name) # Logs access p.age = 31 # Logs modification # p.occupation = \'Pilot\' # This should raise an exception and log the failed attempt ```","solution":"import logging logging.basicConfig(level=logging.INFO) class Validator: def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) class LoggedValidator(Validator): def __get__(self, obj, objtype=None): value = super().__get__(obj, objtype) logging.info(f\'Accessing attribute {self.public_name}: {value}\') return value def __set__(self, obj, value): try: self.validate(value) except Exception as e: logging.error(f\'Failed to set attribute {self.public_name} to {value}: {e}\') raise super().__set__(obj, value) logging.info(f\'Setting attribute {self.public_name} to {value}\') class OneOfLogged(LoggedValidator): def __init__(self, *options): self.options = set(options) def validate(self, value): if value not in self.options: raise ValueError(f\'Expected {value!r} to be one of {self.options!r}\') class NumberLogged(LoggedValidator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(f\'Expected {value!r} to be an int or float\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected {value!r} to be at least {self.minvalue!r}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected {value!r} to be no more than {self.maxvalue!r}\') class StringLogged(LoggedValidator): def __init__(self, minsize=None, maxsize=None, predicate=None): self.minsize = minsize self.maxsize = maxsize self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected {value!r} to be a str\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'Expected {value!r} to be no smaller than {self.minsize!r}\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'Expected {value!r} to be no bigger than {self.maxsize!r}\') if self.predicate is not None and not self.predicate(value): raise ValueError(f\'Expected {self.predicate} to be true for {value!r}\') class Person: name = StringLogged(minsize=3, maxsize=10, predicate=str.isupper) age = NumberLogged(minvalue=0) occupation = OneOfLogged(\'Engineer\', \'Doctor\', \'Artist\') def __init__(self, name, age, occupation): self.name = name self.age = age self.occupation = occupation # Example usage: p = Person(\'ALICE\', 30, \'Engineer\') print(p.name) # Logs access p.age = 31 # Logs modification # p.occupation = \'Pilot\' # This should raise an exception and log the failed attempt"},{"question":"# Advanced Rational Number Manipulation with Python\'s `fractions` Module Objective Implement a function that takes a list of floats and computes a specified kind of result using the properties and methods of the `fractions` module. Problem Statement You need to implement a function `process_fractions(data: List[float], max_denominator: int) -> Tuple[Fraction, Fraction, Fraction, Fraction]` that does the following: 1. Accepts a list of floats called `data`. 2. Converts each float in `data` to its fractional representation using the `Fraction` class. 3. For each Fraction, limit its denominator to `max_denominator` using the `limit_denominator` method. 4. Compute and return a tuple containing: - The sum of all fractions. - The product of all fractions. - The floor of the sum of all fractions. - The ceiling of the product of all fractions. Input - `data`: A list of floats (`List[float]`). Example: `[1.1, 2.5, 0.33]` - `max_denominator`: An integer (`int`) denoting the maximum allowable denominator for fractions. Output - A tuple containing: - The sum of all fractions as a `Fraction`. - The product of all fractions as a `Fraction`. - The floor of the sum as an integer. - The ceiling of the product as an integer. Constraints - The list `data` will have at least one float. - `max_denominator` will be a positive integer greater than 1. Example ```python from fractions import Fraction from typing import List, Tuple def process_fractions(data: List[float], max_denominator: int) -> Tuple[Fraction, Fraction, Fraction, Fraction]: # Your implementation here # Example usage: data = [1.1, 2.5, 0.33] max_denominator = 10 result = process_fractions(data, max_denominator) print(result) # Output might be: # (Fraction(221, 40), Fraction(9075, 80000), 5, 1) ``` Explanation In the example above: 1. Convert each float to its `Fraction` representation. 2. Limit the denominator for each fraction to 10. 3. Calculate the sum of these fractions. 4. Calculate the product of these fractions. 5. Find the floor of the sum. 6. Find the ceiling of the product. This question tests the students\' ability to work with fractions and their corresponding methods in Python\'s `fractions` module, ensuring a deep understanding of the module\'s functionalities and practical applications.","solution":"from fractions import Fraction from typing import List, Tuple def process_fractions(data: List[float], max_denominator: int) -> Tuple[Fraction, Fraction, int, int]: fractions = [Fraction(x).limit_denominator(max_denominator) for x in data] sum_fractions = sum(fractions) product_fractions = Fraction(1) for frac in fractions: product_fractions *= frac floor_sum = sum_fractions.numerator // sum_fractions.denominator ceiling_product = -(-product_fractions.numerator // product_fractions.denominator) return sum_fractions, product_fractions, floor_sum, ceiling_product"},{"question":"**Coding Assessment Question** # Objective This coding assessment evaluates your ability to leverage `seaborn` for creating customized plots that estimate and visualize statistical properties of data. # Problem Statement You are provided with the `diamonds` dataset from the seaborn library, which contains various attributes of diamonds such as `carat`, `cut`, `color`, `clarity`, and `price`. Your goal is to visualize this data using seaborn, focusing primarily on creating plots that reflect different estimations and error bars. # Task 1. Load the `diamonds` dataset. 2. Create a plot to visualize the average carat of diamonds by their clarity. Use bootstrapping with a seed to ensure reproducibility. 3. Create another plot to visualize the median carat of diamonds by their clarity with standard deviation error bars. 4. Develop a weighted estimate visualization where you compute the average carat of diamonds by their clarity while weighting by the price of diamonds. # Instructions 1. Implement the function `create_diamonds_plots()`. 2. The function should return three seaborn objects corresponding to the three plots described. 3. Ensure that the plots are generated using the seaborn `Plot` object with appropriate `Range` and `Est` components. # Constraints - Use a seed value of `0` for any bootstrapping to make results reproducible. - Ensure error bars and confidence intervals are clearly visible. # Function Signature ```python def create_diamonds_plots(): # Your code here return plot1, plot2, plot3 ``` # Example Usage After implementation, running the following should yield the required visualizations: ```python plot1, plot2, plot3 = create_diamonds_plots() # To display the plots, you may use: plot1.show() plot2.show() plot3.show() ``` **Note**: The example usage is just to illustrate the expected functionality of the function. Depending on your Jupyter Notebook setup, visualizations might look different.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_diamonds_plots(): # Load the \'diamonds\' dataset diamonds = sns.load_dataset(\'diamonds\') # Plot 1: Visualize the average carat of diamonds by their clarity with bootstrapping plt.figure() plot1 = sns.barplot(x=\'clarity\', y=\'carat\', data=diamonds, estimator=\'mean\', ci=\'sd\', seed=0) plot1.set_title(\'Average Carat by Clarity with Bootstrapping\') # Plot 2: Visualize the median carat of diamonds by their clarity with standard deviation error bars plt.figure() plot2 = sns.barplot(x=\'clarity\', y=\'carat\', data=diamonds, estimator=\'median\', ci=\'sd\') plot2.set_title(\'Median Carat by Clarity with Standard Deviation Error Bars\') # Plot 3: Weighted estimate visualization of average carat by clarity, weighted by price weighted_avgs = diamonds.groupby(\'clarity\').apply(lambda x: (x[\'carat\'] * x[\'price\']).sum() / x[\'price\'].sum()) plt.figure() plot3 = sns.barplot(x=weighted_avgs.index, y=weighted_avgs.values) plot3.set_title(\'Weighted Average Carat by Clarity (Weighted by Price)\') return plot1, plot2, plot3"},{"question":"# Gaussian Mixture Models with Scikit-learn You are tasked with exploring the `GaussianMixture` and `BayesianGaussianMixture` models from Scikit-learn. This assessment will evaluate your understanding of these models, including their initialization, fitting, and model selection aspects. Task 1. **Data Preparation**: - Generate a synthetic dataset `X` consisting of `n_samples=300` points formed from two distinct Gaussian distributions. Use the following parameters for the Gaussians: - Mean1: [0, 0], Covariance1: [[1, 0], [0, 1]] - Mean2: [3, 3], Covariance2: [[1, 0], [0, 1]] 2. **GaussianMixture Model**: - Initialize a `GaussianMixture` model with `n_components=3` and the `k-means++` initialization method. - Fit this model to the synthetic dataset `X`. - Use BIC to determine the optimal number of components. Change `n_components` and compute BIC for each. Plot the BIC values against different `n_components`. - Predict the labels for the data points and plot the clusters with different colors. 3. **BayesianGaussianMixture Model**: - Initialize a `BayesianGaussianMixture` model with `n_components=10` and the `dirichlet_process` type for `weight_concentration_prior_type`. - Fit this model to the synthetic dataset `X`. - Predict the labels for the data points and plot the clusters with different colors. 4. **Comparison**: - Compare the results from the `GaussianMixture` and `BayesianGaussianMixture` models. Discuss which model is more appropriate for this synthetic dataset and why. Implementation Details - **Input**: None (Generate the dataset within the question) - **Output**: Multiple visualizations and a written comparison - **Constraints**: - Use `random_state=42` for all random number generators for reproducibility. - The comparison should be concise and directly reference the visual results from the plots. Example Structure ```python import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture from sklearn.datasets import make_blobs # 1. Data Preparation n_samples = 300 X, _ = make_blobs(n_samples=n_samples, centers=[[0, 0], [3, 3]], cluster_std=1.0, random_state=42) # 2. GaussianMixture Model gmm_bic_scores = [] n_components_range = range(1, 10) for n in n_components_range: gmm = GaussianMixture(n_components=n, init_params=\'kmeans++\', random_state=42) gmm.fit(X) gmm_bic_scores.append(gmm.bic(X)) plt.plot(n_components_range, gmm_bic_scores, marker=\'o\') plt.xlabel(\'Number of components\') plt.ylabel(\'BIC\') plt.title(\'BIC vs Number of components for GaussianMixture\') plt.show() optimal_n_components = n_components_range[np.argmin(gmm_bic_scores)] gmm = GaussianMixture(n_components=optimal_n_components, init_params=\'kmeans++\', random_state=42) gmm.fit(X) gmm_labels = gmm.predict(X) plt.scatter(X[:, 0], X[:, 1], c=gmm_labels, cmap=\'viridis\', marker=\'o\') plt.title(\'GaussianMixture Clustering\') plt.show() # 3. BayesianGaussianMixture Model bgmm = BayesianGaussianMixture(n_components=10, weight_concentration_prior_type=\'dirichlet_process\', random_state=42) bgmm.fit(X) bgmm_labels = bgmm.predict(X) plt.scatter(X[:, 0], X[:, 1], c=bgmm_labels, cmap=\'viridis\', marker=\'o\') plt.title(\'BayesianGaussianMixture Clustering\') plt.show() # 4. Comparison comparison_text = The GaussianMixture model identified clusters based on BIC, resulting in [num] components. The BayesianGaussianMixture model, with the Dirichlet process prior, automatically adjusted the number of components to [num] due to its prior-based regularization. Comparatively, [State which model performed better and why]... print(comparison_text) ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture, BayesianGaussianMixture # 1. Data Preparation np.random.seed(42) n_samples = 300 mean1 = [0, 0] cov1 = [[1, 0], [0, 1]] mean2 = [3, 3] cov2 = [[1, 0], [0, 1]] X1 = np.random.multivariate_normal(mean1, cov1, n_samples // 2) X2 = np.random.multivariate_normal(mean2, cov2, n_samples // 2) X = np.vstack((X1, X2)) # 2. GaussianMixture Model gmm_bic_scores = [] n_components_range = range(1, 10) for n in n_components_range: gmm = GaussianMixture(n_components=n, init_params=\'kmeans\', random_state=42) gmm.fit(X) gmm_bic_scores.append(gmm.bic(X)) plt.plot(n_components_range, gmm_bic_scores, marker=\'o\') plt.xlabel(\'Number of components\') plt.ylabel(\'BIC\') plt.title(\'BIC vs Number of components for GaussianMixture\') plt.show() optimal_n_components = n_components_range[np.argmin(gmm_bic_scores)] gmm = GaussianMixture(n_components=optimal_n_components, init_params=\'kmeans\', random_state=42) gmm.fit(X) gmm_labels = gmm.predict(X) plt.scatter(X[:, 0], X[:, 1], c=gmm_labels, cmap=\'viridis\', marker=\'o\') plt.title(\'GaussianMixture Clustering\') plt.show() # 3. BayesianGaussianMixture Model bgmm = BayesianGaussianMixture(n_components=10, weight_concentration_prior_type=\'dirichlet_process\', random_state=42) bgmm.fit(X) bgmm_labels = bgmm.predict(X) plt.scatter(X[:, 0], X[:, 1], c=bgmm_labels, cmap=\'viridis\', marker=\'o\') plt.title(\'BayesianGaussianMixture Clustering\') plt.show() # 4. Comparison comparison_text = The GaussianMixture model identified clusters based on BIC, resulting in {} components. The BayesianGaussianMixture model, with the Dirichlet process prior, automatically adjusted the number of components to {} due to its prior-based regularization. Comparatively, the BayesianGaussianMixture model performed better in automatically determining the appropriate number of components without needing to manually compute BIC across multiple models. .format(optimal_n_components, np.unique(bgmm_labels).size) print(comparison_text)"},{"question":"# Advanced Python Profiling and Analysis **Objective**: Implement and analyze the performance of a Python script using `cProfile` and `pstats`. **Problem Statement**: You are given a Python script that performs a series of complex operations. Your task is to: 1. Profile the execution of the script using the `cProfile` module. 2. Save the profiling results to a file. 3. Load the profiling results from the file and analyze them using the `pstats` module. 4. Implement various methods to display specific profiling information. Here is the provided script: ```python import time def complex_operation_1(): time.sleep(0.5) return sum([i**2 for i in range(10000)]) def complex_operation_2(): time.sleep(0.7) return sum([i**3 for i in range(10000)]) def execute_operations(): result1 = complex_operation_1() result2 = complex_operation_2() return result1, result2 if __name__ == \\"__main__\\": execute_operations() ``` **Requirements**: 1. Profile the `execute_operations` function using the `cProfile` module and save the profiling results to a file named `profile_results.prof`. 2. Write a script that loads the profiling results from `profile_results.prof` and uses the `pstats` module to: - Display the top 5 functions sorted by the cumulative time spent in the function. - Display the functions that are called by `execute_operations` sorted by the time spent within each function. - Display all functions that called `complex_operation_1` with details on how many times they were called. - Strip directory information from the displayed statistics to make the output more readable. **Input/Output Format**: - Your primary implementation should be a script or function which performs the profiling and subsequent analysis. - The output for each analysis step should be clear and should follow the requirements. Use print statements to display your results. - Store the results in a format that can be loaded and analyzed later (e.g., a file). **Additional Constraints**: - Assume the profiling should be done within a reasonably short runtime. - Ensure that the printed profiling statistics are easy to interpret. **Example**: Here is a skeleton of your solution to guide you: ```python import cProfile import pstats from pstats import SortKey # Step 1: Profile the function and save the results def profile_script(): profiler = cProfile.Profile() profiler.enable() execute_operations() profiler.disable() profiler.dump_stats(\'profile_results.prof\') # Step 2: Load and analyze profiling results def analyze_profile(): p = pstats.Stats(\'profile_results.prof\') # Top 5 functions by cumulative time p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(5) # Functions called by execute_operations sorted by time spent print(\\"nFunctions called by execute_operations:\\") p.sort_stats(SortKey.TIME).print_callees(\'execute_operations\') # Functions that called complex_operation_1 print(\\"nFunctions that called complex_operation_1:\\") p.print_callers(\'complex_operation_1\') if __name__ == \\"__main__\\": profile_script() analyze_profile() ``` **Submission**: Submit your implementation as a Python script along with the profiling results file (`profile_results.prof`). Ensure your script is well-documented and each step is clearly explained.","solution":"import cProfile import pstats from pstats import SortKey import time def complex_operation_1(): time.sleep(0.5) return sum([i**2 for i in range(10000)]) def complex_operation_2(): time.sleep(0.7) return sum([i**3 for i in range(10000)]) def execute_operations(): result1 = complex_operation_1() result2 = complex_operation_2() return result1, result2 # Step 1: Profile the function and save the results def profile_script(): profiler = cProfile.Profile() profiler.enable() execute_operations() profiler.disable() profiler.dump_stats(\'profile_results.prof\') # Step 2: Load and analyze profiling results def analyze_profile(): p = pstats.Stats(\'profile_results.prof\') # Top 5 functions by cumulative time print(\\"nTop 5 functions sorted by cumulative time:\\") p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(5) # Functions called by execute_operations sorted by time spent print(\\"nFunctions called by execute_operations sorted by time spent:\\") p.strip_dirs().sort_stats(SortKey.TIME).print_callees(\'execute_operations\') # Functions that called complex_operation_1 print(\\"nFunctions that called complex_operation_1:\\") p.strip_dirs().print_callers(\'complex_operation_1\') if __name__ == \\"__main__\\": profile_script() analyze_profile()"},{"question":"**Question: Utilizing the PyTorch MPS Backend for GPU Acceleration on MacOS** You are given a neural network model and some input data. Your task is to enable GPU acceleration for training the model using the PyTorch MPS backend on a MacOS device with the Metal programming framework. Follow the steps outlined below to complete the task: 1. **Check MPS Availability**: - Write a function `check_mps_availability()` that verifies if MPS is available and return a tuple `(is_available, message)` where: - `is_available` is a boolean indicating if MPS is available. - `message` is a string explaining the reason if MPS is not available. 2. **Create Tensors and Perform Operations**: - Write a function `create_and_operate_tensors()` that: - Takes an integer `n` and creates a tensor of shape `(n, n)` filled with ones directly on the MPS device. - Multiplies this tensor by 2 and returns the resulting tensor. 3. **Deploy Model**: - Given a simple PyTorch neural network model `SimpleModel()`, write a function `deploy_model_on_mps(model, input_tensor)` that: - Moves the model to the MPS device. - Passes the `input_tensor` through the model and returns the output. **Input**: - For the function `check_mps_availability()`: No inputs. - For the function `create_and_operate_tensors(n)`: An integer `n`. - For the function `deploy_model_on_mps(model, input_tensor)`: - `model`: A pre-defined PyTorch model `SimpleModel()`. - `input_tensor`: A PyTorch tensor. **Output**: - For the function `check_mps_availability()`: A tuple `(is_available, message)`. - For the function `create_and_operate_tensors(n)`: A PyTorch tensor. - For the function `deploy_model_on_mps(model, input_tensor)`: A PyTorch tensor. Here is the skeleton code to get you started: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) def check_mps_availability(): # Your code here def create_and_operate_tensors(n): # Your code here def deploy_model_on_mps(model, input_tensor): # Your code here # Test your functions if __name__ == \\"__main__\\": # Check MPS availability is_available, message = check_mps_availability() print(is_available, message) if is_available: # Create and operate tensors result_tensor = create_and_operate_tensors(5) print(result_tensor) # Prepare model and input tensor model = SimpleModel() input_tensor = torch.randn(1, 10, device=\'mps\') # Deploy model and get output output = deploy_model_on_mps(model, input_tensor) print(output) ``` **Constraints**: - Use PyTorch version that supports MPS (ensure PyTorch is installed correctly). - Ensure your MacOS version is 12.3+ and you have an MPS-enabled GPU device. Good luck, and happy coding!","solution":"import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 5) def forward(self, x): return self.linear(x) def check_mps_availability(): Checks if the MPS (Metal Performance Shaders) backend is available for GPU acceleration on MacOS. Returns a tuple (is_available, message). is_available = torch.backends.mps.is_available() and torch.backends.mps.is_built() message = \\"MPS is available and can be used for GPU acceleration\\" if is_available else \\"MPS is not available. Please ensure you have MacOS 12.3+ and an MPS-enabled GPU device.\\" return is_available, message def create_and_operate_tensors(n): Creates a tensor of shape (n, n) filled with ones on the MPS device and multiplies it by 2. Returns the resulting tensor. device = torch.device(\\"mps\\") tensor = torch.ones((n, n), device=device) result_tensor = tensor * 2 return result_tensor def deploy_model_on_mps(model, input_tensor): Moves the model to the MPS device and performs a forward pass with the input tensor. Returns the output tensor. device = torch.device(\\"mps\\") model.to(device) input_tensor = input_tensor.to(device) with torch.no_grad(): output = model(input_tensor) return output"},{"question":"Time-Series Data Analysis with Pandas **Objective:** You are required to demonstrate your comprehension of pandas by performing a complex time-series data manipulation and analysis. This involves loading data, performing transformations, group operations, and visualizing the results. **Task Description:** 1. **Load the Data:** - Load the provided CSV file `sales_data.csv` into a pandas DataFrame. The CSV file contains columns `date`, `store_id`, `product_id`, and `sales`. - Ensure that the `date` column is parsed as datetime objects. 2. **Data Transformation:** - Add a new column `week` derived from the `date` column that indicates the week number of the year. - Add another column `year` denoting the respective year of each entry. 3. **Group Operations:** - Calculate the total weekly sales for each `store_id` and `product_id` combination. - Filter out the combinations wherein the total weekly sales are below a threshold of 1000 units. The threshold is applied to the aggregated weekly data, not the raw data. 4. **Data Analysis:** - For each `store_id`, identify the `product_id` which had the highest average weekly sales over the entire period. - Create a summary DataFrame with columns `store_id`, `product_id`, and `average_weekly_sales`. 5. **Visualization:** - Plot the sales trend for the top product of each store identified in the previous step. Each plot should display the trend in weekly sales over time, separately for each `store_id`. **Constraints:** - You may not use third-party libraries other than pandas and matplotlib for data handling and plotting. - Solutions must be efficient and handle potential large datasets gracefully. **Performance Requirements:** - The solution should be optimized for performance, especially with respect to group operations and filtering large datasets. **Expected Input and Output:** - **Input:** - A CSV file `sales_data.csv` with columns: `date`, `store_id`, `product_id`, and `sales`. - **Output:** - A summary DataFrame as described in task 4. - Plots as described in task 5 (individual plots for top products of each store). **Example:** Given an example CSV content: ``` date,store_id,product_id,sales 2021-01-05,1,101,200 2021-01-06,1,102,150 2021-01-10,2,201,300 2021-01-15,2,202,100 2021-01-20,1,101,250 ... ``` The resulting summary DataFrame might look something like: ``` store_id product_id average_weekly_sales 1 101 225.0 2 201 300.0 ... ``` Ensure your code is clean, well-documented, and handles edge cases appropriately.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_data(file_path: str) -> pd.DataFrame: Load the CSV file into a pandas DataFrame with date column parsed as datetime. df = pd.read_csv(file_path, parse_dates=[\'date\']) return df def transform_data(df: pd.DataFrame) -> pd.DataFrame: Transform the data by adding the week and year columns. df[\'week\'] = df[\'date\'].dt.isocalendar().week df[\'year\'] = df[\'date\'].dt.year return df def calculate_weekly_sales(df: pd.DataFrame) -> pd.DataFrame: Calculate total weekly sales for each store_id and product_id combination, then filter out combinations with total weekly sales below 1000 units. weekly_sales = df.groupby([\'store_id\', \'product_id\', \'year\', \'week\']).agg(total_sales=(\'sales\', \'sum\')).reset_index() filtered_sales = weekly_sales[weekly_sales[\'total_sales\'] >= 1000] return filtered_sales def get_top_products(df: pd.DataFrame) -> pd.DataFrame: Identify the product_id with the highest average weekly sales for each store_id. Return a summary DataFrame with store_id, product_id, and average_weekly_sales. avg_weekly_sales = df.groupby([\'store_id\', \'product_id\']).agg(average_weekly_sales=(\'total_sales\', \'mean\')).reset_index() top_products = avg_weekly_sales.loc[avg_weekly_sales.groupby(\'store_id\')[\'average_weekly_sales\'].idxmax()] return top_products def plot_sales_trends(filtered_sales: pd.DataFrame, top_products: pd.DataFrame): Plot the sales trends for the top product of each store. for store_id in top_products[\'store_id\'].unique(): top_product_id = top_products[top_products[\'store_id\'] == store_id][\'product_id\'].values[0] store_sales = filtered_sales[(filtered_sales[\'store_id\'] == store_id) & (filtered_sales[\'product_id\'] == top_product_id)] plt.figure() plt.plot(store_sales[\'year\'].astype(str) + \'-W\' + store_sales[\'week\'].astype(str), store_sales[\'total_sales\'], marker=\'o\') plt.title(f\'Store {store_id} - Product {top_product_id} Sales Trend\') plt.xlabel(\'Week\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.tight_layout() plt.show()"},{"question":"Problem Statement You are tasked with writing a Python function that reads a UTF-8 encoded text file, processes the Unicode characters, and writes the processed text back to another file in a specific encoding. The file processing will involve: 1. Removing all non-ASCII characters. 2. Counting the number of characters removed and the number of characters in each Unicode category in the original text. 3. Writing the cleaned text to a new file in UTF-8 and another one in UTF-16 encoding. Function Signature ```python def process_unicode_file(input_file: str, output_file_utf8: str, output_file_utf16: str) -> None: pass ``` Input - `input_file` (str): The path to the input file encoded in UTF-8. - `output_file_utf8` (str): The path to the output file to be written in UTF-8 encoding. - `output_file_utf16` (str): The path to the output file to be written in UTF-16 encoding. Output The function does not return any value, but it should: - Write the processed text to the specified output files. - Print the number of characters removed and the count of each Unicode category. Constraints - The input file will contain text encoded in UTF-8. - Ensure proper exception handling for file operations. Example Consider the following example text in `input_file`: ``` Hello, World! ðŸ˜€ðŸ˜Š ``` The `output_file_utf8` and `output_file_utf16` should contain: ``` Hello, World! ``` The console output should be: ```plaintext Characters removed: 2 Categories: Lu: 2 Ll: 10 Po: 2 So: 2 Zs: 1 ``` Notes - Use the `unicodedata` module to analyze the category of each character. - Categories to consider: \'Lu\' (Letter, uppercase), \'Ll\' (Letter, lowercase), \'Po\' (Punctuation, other), \'So\' (Symbol, other), \'Zs\' (Separator, space). - Ensure the program handles large files efficiently. Starting Point ```python import unicodedata def process_unicode_file(input_file: str, output_file_utf8: str, output_file_utf16: str) -> None: # Read the input file with open(input_file, \'r\', encoding=\'utf-8\') as file: text = file.read() ascii_text = \'\' characters_removed = 0 category_counts = {} for char in text: if ord(char) < 128: ascii_text += char else: characters_removed += 1 category = unicodedata.category(char) if category not in category_counts: category_counts[category] = 0 category_counts[category] += 1 # Write the cleaned text to output files with open(output_file_utf8, \'w\', encoding=\'utf-8\') as file: file.write(ascii_text) with open(output_file_utf16, \'w\', encoding=\'utf-16\') as file: file.write(ascii_text) # Print the counts print(f\\"Characters removed: {characters_removed}\\") print(\\"Categories:\\") for category, count in category_counts.items(): print(f\\"{category}: {count}\\") ```","solution":"import unicodedata def process_unicode_file(input_file: str, output_file_utf8: str, output_file_utf16: str) -> None: # Read the input file with open(input_file, \'r\', encoding=\'utf-8\') as file: text = file.read() ascii_text = \'\' characters_removed = 0 category_counts = {} for char in text: if ord(char) < 128: ascii_text += char else: characters_removed += 1 category = unicodedata.category(char) if category not in category_counts: category_counts[category] = 0 category_counts[category] += 1 # Write the cleaned text to output files in UTF-8 and UTF-16 with open(output_file_utf8, \'w\', encoding=\'utf-8\') as file: file.write(ascii_text) with open(output_file_utf16, \'w\', encoding=\'utf-16\') as file: file.write(ascii_text) # Print the counts print(f\\"Characters removed: {characters_removed}\\") print(\\"Categories:\\") for category, count in category_counts.items(): print(f\\"{category}: {count}\\")"},{"question":"# PyTorch Distributed Computation with Futures In this exercise, you will demonstrate your ability to work with PyTorch\'s `torch.futures` package to manage asynchronous computations within a distributed framework. **Task**: Implement a function `distribute_and_combine` that performs the following: 1. Distributes a set of independent tasks (simulated by a given function `task_fn`) across multiple workers. 2. Collects the results of these tasks asynchronously using `Future` objects. 3. Combines the results into a single list once all tasks are completed, and returns this list. **Function Signature**: ```python def distribute_and_combine(task_fn: Callable[[], Any], num_tasks: int) -> List[Any]: pass ``` **Parameters**: - `task_fn`: A function with no arguments that returns a value representing the result of a task. - `num_tasks`: An integer specifying the number of independent tasks to be distributed. **Returns**: - A list of results obtained from `num_tasks` evaluated tasks. **Details**: - Assume the presence of `torch.futures.Future`, `torch.futures.collect_all`, and `torch.futures.wait_all` utilities. - You need to simulate the distribution of tasks using asynchronous execution. - Use `Future` objects to encapsulate the asynchronous execution of each task. **Example**: ```python import torch.futures from typing import Callable, List, Any, Optional def example_task(): import random return random.randint(1, 100) def distribute_and_combine(task_fn: Callable[[], Any], num_tasks: int) -> List[Any]: futures = [] # Starting the tasks asynchronously for _ in range(num_tasks): future = torch.futures.Future() future.set_result(task_fn()) futures.append(future) # Collecting all futures all_futures = torch.futures.collect_all(futures) # Waiting for all tasks to complete completed_futures = torch.futures.wait_all(futures) # Gathering the results results = [future.value() for future in completed_futures] return results # Test the function tasks = 5 print(distribute_and_combine(example_task, tasks)) ``` **Constraints**: - You should NOT use synchronous task execution or blocking calls directly, except for waiting on futures to complete. - The function assumed to execute the tasks (`task_fn`) can be any callable that returns a result and does not take any arguments. **Performance**: - The solution should efficiently handle the distribution and combination of tasks. - Ensure that the function can be executed within a reasonable time frame, even for a larger number of tasks (e.g., 1000 tasks). **Note**: You are free to simulate the asynchronous execution of tasks if an actual distributed environment is not available.","solution":"import torch import torch.futures from typing import Callable, List, Any def distribute_and_combine(task_fn: Callable[[], Any], num_tasks: int) -> List[Any]: futures = [] # Starting the tasks asynchronously for _ in range(num_tasks): future = torch.futures.Future() def wrap_fn(fut): # Setting result in future fut.set_result(task_fn()) torch.jit._fork(wrap_fn, future) futures.append(future) # Collecting all futures and waiting for all tasks to complete all_futures = torch.futures.collect_all(futures) completed_futures = all_futures.wait() # Gathering the results results = [future.value() for future in completed_futures] return results"},{"question":"# Coding Assessment: Directory Synchronization Given the powerful features of the `filecmp` module, you are tasked with implementing a utility function that can synchronize the contents of two directories. This involves making the contents of the target directory identical to the source directory by copying any new or modified files from the source to the target and deleting any files in the target that are not present in the source. **Function Specification:** ```python import os import shutil from filecmp import dircmp def synchronize_directories(source: str, target: str, ignore: list = None, hide: list = None) -> None: Synchronizes the contents of the target directory to match the source directory. Parameters: - source (str): The path to the source directory. - target (str): The path to the target directory. - ignore (list, optional): A list of file or directory names to ignore. Defaults to None. - hide (list, optional): A list of file or directory names to hide. Defaults to None. Returns: - None: The function performs the synchronization in-place and does not return any value. # Your implementation here ``` **Input:** - `source`: Path to the source directory containing the reference files. - `target`: Path to the target directory which needs to be synchronized. - `ignore`: Optional list of file or directory names to ignore during comparison (default is `filecmp.DEFAULT_IGNORES`). - `hide`: Optional list of file or directory names to hide from comparison (default is `[os.curdir, os.pardir]`). **Output:** - None. The function performs the synchronization but does not return any value. **Constraints:** - Files should only be copied or deleted and not modified in place. - Handle directory trees with arbitrary depth. - Must utilize the `filecmp.dircmp` class for comparisons. **Example:** ``` # Suppose the following directory structure: # source/ # file1.txt # file2.txt # subdir/ # file3.txt # # target/ # file1.txt # file4.txt # After synchronization: # target/ # file1.txt (updated if modified in source) # file2.txt (copied from source) # subdir/ # file3.txt (copied from source) # file4.txt (deleted since it is not in source) ``` Implement the `synchronize_directories` function within the provided structure.","solution":"import os import shutil from filecmp import dircmp def synchronize_directories(source: str, target: str, ignore: list = None, hide: list = None) -> None: Synchronizes the contents of the target directory to match the source directory. Parameters: - source (str): The path to the source directory. - target (str): The path to the target directory. - ignore (list, optional): A list of file or directory names to ignore. Defaults to None. - hide (list, optional): A list of file or directory names to hide. Defaults to None. Returns: - None: The function performs the synchronization in-place and does not return any value. if ignore is None: ignore = [] if hide is None: hide = [os.curdir, os.pardir] def sync_dir(dcmp): # Copy files from source to target for name in dcmp.left_only: src_path = os.path.join(dcmp.left, name) dest_path = os.path.join(dcmp.right, name) if os.path.isdir(src_path): shutil.copytree(src_path, dest_path) else: shutil.copy2(src_path, dest_path) # Delete files from target that are not in source for name in dcmp.right_only: dest_path = os.path.join(dcmp.right, name) if os.path.isdir(dest_path): shutil.rmtree(dest_path) else: os.remove(dest_path) # Overwrite files in target with modified files from source for name in dcmp.diff_files: src_path = os.path.join(dcmp.left, name) dest_path = os.path.join(dcmp.right, name) shutil.copy2(src_path, dest_path) # Recursively synchronize sub-directories for sub_dcmp in dcmp.subdirs.values(): sync_dir(sub_dcmp) dcmp = dircmp(source, target, ignore=ignore, hide=hide) sync_dir(dcmp)"},{"question":"# Sparse Tensors in PyTorch **Objective**: Implement a function that performs various operations on sparse tensors using PyTorch. **Description**: You are provided with a large square matrix that is extremely sparse (i.e., most elements are zero). Your task is to implement a function `sparse_operations` that performs the following operations using PyTorch: 1. **Conversion to Sparse Format**: - Convert the given dense matrix to a sparse COO matrix. - Convert the given dense matrix to a sparse CSR matrix. 2. **Matrix Operations**: - Perform matrix multiplication between the sparse COO matrix and the original dense matrix. - Perform matrix addition between the sparse CSR matrix and the original dense matrix (converted to CSR format). 3. **Result Verification**: - Verify that the results of the operations (multiplication and addition) match those obtained using regular dense tensors. **Signature**: ```python import torch def sparse_operations(dense_matrix: torch.Tensor) -> dict: Perform operations on sparse tensors. Args: dense_matrix (torch.Tensor): A 2D dense tensor. Returns: dict: A dictionary with the results of the operations: - \'coo_multiplication\': Result of COO matrix multiplication. - \'csr_addition\': Result of CSR matrix addition. - \'verification_multiplication\': Boolean value if results match for multiplication. - \'verification_addition\': Boolean value if results match for addition. ``` **Input**: - `dense_matrix`: A 2D dense tensor of shape `(N, N)` where `N` can be up to `10,000`. The tensor contains floating point numbers with a high degree of sparsity (i.e., many elements are zero). **Output**: - A dictionary with the following keys: - `\'coo_multiplication\'`: The result of the matrix multiplication using sparse COO format. - `\'csr_addition\'`: The result of the matrix addition using sparse CSR format. - `\'verification_multiplication\'`: Boolean indicating whether the sparse matrix multiplication result matches the dense matrix multiplication result. - `\'verification_addition\'`: Boolean indicating whether the sparse matrix addition result matches the dense matrix addition result. **Constraints**: - Ensure efficient memory usage by leveraging the sparse tensor operations provided by PyTorch. - Handle large sparse matrices efficiently. **Example**: ```python import torch dense_matrix = torch.zeros(10000, 10000) dense_matrix[0, 1] = 2 dense_matrix[2, 3] = 5 # ... (other sparse elements) results = sparse_operations(dense_matrix) ``` **Notes**: - Use the provided methods in `torch.sparse` to create and manipulate the sparse tensors. - Make sure to convert the matrices back to dense when verifying the results to ensure accuracy.","solution":"import torch def sparse_operations(dense_matrix: torch.Tensor) -> dict: Perform operations on sparse tensors. Args: dense_matrix (torch.Tensor): A 2D dense tensor. Returns: dict: A dictionary with the results of the operations: - \'coo_multiplication\': Result of COO matrix multiplication. - \'csr_addition\': Result of CSR matrix addition. - \'verification_multiplication\': Boolean value if results match for multiplication. - \'verification_addition\': Boolean value if results match for addition. # Convert dense matrix to sparse COO format dense_coo = dense_matrix.to_sparse_coo() # Convert dense matrix to sparse CSR format dense_csr = dense_matrix.to_sparse_csr() # Perform matrix multiplication using sparse COO coo_multiplication = torch.matmul(dense_coo, dense_matrix) # Perform matrix addition using sparse CSR dense_csr_add = dense_matrix.to_sparse_csr() csr_addition = dense_csr + dense_csr_add # Convert results back to dense for verification coo_multiplication_dense = coo_multiplication.to_dense() csr_addition_dense = csr_addition.to_dense() # Verifications verification_multiplication = torch.allclose(coo_multiplication_dense, torch.matmul(dense_matrix, dense_matrix)) verification_addition = torch.allclose(csr_addition_dense, dense_matrix + dense_matrix) results = { \'coo_multiplication\': coo_multiplication_dense, \'csr_addition\': csr_addition_dense, \'verification_multiplication\': verification_multiplication, \'verification_addition\': verification_addition, } return results"},{"question":"# Parallel File Processing with `concurrent.futures` Objective Implement a function `process_files_concurrently` to demonstrate your understanding of the `concurrent.futures` module. This function will process multiple files concurrently, where each file\'s content is processed to count the number of words and lines. Function Signature ```python def process_files_concurrently(file_paths: List[str]) -> List[Dict[str, int]]: pass ``` Input - `file_paths` (List[str]): A list of file paths where each file contains text data. Output - Returns a list of dictionaries where each dictionary contains: - \'file\': The name of the file. - \'word_count\': The number of words in the file. - \'line_count\': The number of lines in the file. Constraints and Requirements - The function must use the `concurrent.futures` module to process files concurrently. - It should make efficient use of resources and avoid common pitfalls such as resource exhaustion or deadlocks. - Assume that the files are small enough to be read into memory but the quantity of files can be large enough to necessitate parallel processing. - Properly handle exceptions that may arise during file processing to ensure the function completes its task even if some files cause errors. Example ```python file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] # Assume file1.txt has 100 words and 10 lines, file2.txt has 200 words and 20 lines, # and file3.txt has 300 words and 30 lines. result = process_files_concurrently(file_paths) # Expected output: # [ # {\'file\': \'file1.txt\', \'word_count\': 100, \'line_count\': 10}, # {\'file\': \'file2.txt\', \'word_count\': 200, \'line_count\': 20}, # {\'file\': \'file3.txt\', \'word_count\': 300, \'line_count\': 30} # ] ``` Notes - You may assume that the input files exist and are accessible for reading. - Files can be text files encoded in UTF-8. - The function should be designed to handle a large number of files efficiently. - Include proper documentation and comments in your implementation to explain your approach.","solution":"import concurrent.futures from typing import List, Dict def count_words_and_lines(file_path: str) -> Dict[str, int]: try: with open(file_path, \'r\', encoding=\'utf-8\') as file: lines = file.readlines() word_count = sum(len(line.split()) for line in lines) line_count = len(lines) return {\'file\': file_path, \'word_count\': word_count, \'line_count\': line_count} except Exception as e: # Handle any exceptions that occur during file processing. return {\'file\': file_path, \'word_count\': 0, \'line_count\': 0, \'error\': str(e)} def process_files_concurrently(file_paths: List[str]) -> List[Dict[str, int]]: results = [] with concurrent.futures.ThreadPoolExecutor() as executor: futures = {executor.submit(count_words_and_lines, file_path): file_path for file_path in file_paths} for future in concurrent.futures.as_completed(futures): result = future.result() results.append(result) return results"},{"question":"Objective: Your task is to demonstrate your understanding of validation curves and learning curves using scikit-learn. Problem Statement: You are given a dataset to classify species of the Iris flower. You need to: 1. Implement a function to compute and plot validation curves for a given model and hyperparameter. 2. Implement a function to compute and plot learning curves for a given model. 3. Use these curves to analyze model performance and interpret bias-variance tradeoff. Dataset: Use the Iris dataset provided by scikit-learn. Functions to Implement: 1. **plot_validation_curve**: ```python def plot_validation_curve(model, X, y, param_name, param_range, cv=5): This function calculates and plots the validation curve. Parameters: - model: The machine learning model to evaluate (e.g., `SVC(kernel=\\"linear\\")`). - X: Feature matrix. - y: Target vector. - param_name: Name of the hyperparameter to vary (e.g., \\"C\\"). - param_range: List or array of parameter values to evaluate (e.g., `np.logspace(-7, 3, 3)`). - cv: Number of cross-validation folds (default is 5). Returns: - None (displays a plot of the validation curve). pass ``` 2. **plot_learning_curve**: ```python def plot_learning_curve(model, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5): This function calculates and plots the learning curve. Parameters: - model: The machine learning model to evaluate (e.g., `SVC(kernel=\\"linear\\")`). - X: Feature matrix. - y: Target vector. - train_sizes: List or array of training sizes (default is `np.linspace(0.1, 1.0, 10)`). - cv: Number of cross-validation folds (default is 5). Returns: - None (displays a plot of the learning curve). pass ``` Requirements and Constraints: - You should use `ValidationCurveDisplay` and `LearningCurveDisplay` for plotting. - Ensure the plots are well-labeled with titles, axis labels, and legends. - Use cross-validation with 5 folds. - Provide an analysis of what the resulting curves indicate about model performance, bias, and variance. Example: ```python # Example usage from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.utils import shuffle X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Example for validation curve plot_validation_curve(SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=np.logspace(-3, 3, 7)) # Example for learning curve plot_learning_curve(SVC(kernel=\\"linear\\"), X, y, train_sizes=np.linspace(0.1, 1.0, 10)) ``` Evaluation Criteria: - Correct implementation of the `plot_validation_curve` and `plot_learning_curve` functions. - Quality of the plots (clarity, labeling, and interpretation). - Explanation of the results and what they indicate about the model\'s bias and variance.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.svm import SVC from sklearn.datasets import load_iris from sklearn.utils import shuffle from sklearn.model_selection import validation_curve, learning_curve, train_test_split def plot_validation_curve(model, X, y, param_name, param_range, cv=5): This function calculates and plots the validation curve. Parameters: - model: The machine learning model to evaluate (e.g., `SVC(kernel=\\"linear\\")`). - X: Feature matrix. - y: Target vector. - param_name: Name of the hyperparameter to vary (e.g., \\"C\\"). - param_range: List or array of parameter values to evaluate (e.g., `np.logspace(-7, 3, 3)`). - cv: Number of cross-validation folds (default is 5). Returns: - None (displays a plot of the validation curve). train_scores, test_scores = validation_curve(model, X, y, param_name=param_name, param_range=param_range, cv=cv, scoring=\'accuracy\') train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure() plt.title(f\\"Validation Curve with {model.__class__.__name__}\\") plt.xlabel(param_name) plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.semilogx(param_range, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=lw) plt.semilogx(param_range, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.show() def plot_learning_curve(model, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5): This function calculates and plots the learning curve. Parameters: - model: The machine learning model to evaluate (e.g., `SVC(kernel=\\"linear\\")`). - X: Feature matrix. - y: Target vector. - train_sizes: List or array of training sizes (default is `np.linspace(0.1, 1.0, 10)`). - cv: Number of cross-validation folds (default is 5). Returns: - None (displays a plot of the learning curve). train_sizes, train_scores, test_scores = learning_curve(model, X, y, train_sizes=train_sizes, cv=cv, scoring=\'accuracy\') train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure() plt.title(f\\"Learning Curve with {model.__class__.__name__}\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=lw) plt.plot(train_sizes, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.show()"},{"question":"**Objective**: This task is designed to assess your comprehension of seaborn\'s advanced plotting capabilities, specifically using the `seaborn.objects` module to control plot layouts and dimensions. **Question**: Using the `seaborn.objects` module, write a function `create_complex_plot` that takes a pandas DataFrame and the names of two categorical columns (for rows and columns of subplots) and generates a grid of subplots with the following specifications: - The overall size of the figure should be 10x10 inches. - Use the `constrained` layout engine. - Each subplot should represent a scatter plot of two numerical columns, \'value1\' and \'value2\'. - The subplot grid should be organized based on the unique values of the provided categorical columns. - Adjust the size of the plot within the figure such that it occupies the full space. Here are the expected parameters and outputs: Parameters: * `df` (pd.DataFrame): The input DataFrame containing the data. * `row_var` (str): The name of the categorical column to define the rows of the subplot grid. * `col_var` (str): The name of the categorical column to define the columns of the subplot grid. Constraints: 1. The DataFrame `df` must contain at least four columns: the categorical columns specified by `row_var` and `col_var`, and two numerical columns named `value1` and `value2`. 2. There should be no missing values in the relevant columns of the DataFrame. Function Signature: ```python import pandas as pd import seaborn.objects as so def create_complex_plot(df: pd.DataFrame, row_var: str, col_var: str) -> None: # Your code here ``` # Example: ```python import pandas as pd # Example DataFrame data = { \'Category1\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\'], \'Category2\': [\'X\', \'Y\', \'X\', \'Y\', \'X\', \'Y\'], \'value1\': [10, 15, 10, 25, 30, 35], \'value2\': [5, 10, 15, 20, 25, 30] } df = pd.DataFrame(data) # Generating the plot create_complex_plot(df, \'Category1\', \'Category2\') ``` This function should display a grid of scatter plots, with rows labeled by \'Category1\' values and columns labeled by \'Category2\' values, each subplot showing the relationship between \'value1\' and \'value2\'. # Guidelines: - Import necessary libraries. - Handle the DataFrame operations to extract unique values and iterate over them. - Use the seaborn objects module to create and layout plots. - Ensure the plot fits nicely within the provided size constraints and layout.","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def create_complex_plot(df: pd.DataFrame, row_var: str, col_var: str) -> None: Create a complex plot using seaborn objects, displaying a grid of scatter plots categorized by two given categorical variables. Parameters: df (pd.DataFrame): The input DataFrame containing the data. row_var (str): The name of the categorical column to define the rows of the subplot grid. col_var (str): The name of the categorical column to define the columns of the subplot grid. unique_rows = df[row_var].unique() unique_cols = df[col_var].unique() num_rows = len(unique_rows) num_cols = len(unique_cols) fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 10), constrained_layout=True) for i, row_cat in enumerate(unique_rows): for j, col_cat in enumerate(unique_cols): ax = axes[i, j] if num_rows > 1 and num_cols > 1 else axes[max(i, j)] subset = df[(df[row_var] == row_cat) & (df[col_var] == col_cat)] ax.scatter(subset[\'value1\'], subset[\'value2\']) if j == 0: ax.set_ylabel(row_cat) if i == 0: ax.set_title(col_cat) ax.set_xticks([]) ax.set_yticks([]) plt.show()"},{"question":"**Email Analysis and Manipulation** You are provided with a corpus of raw email data saved in a text file. Each email in the file is represented in a standard format that includes metadata (e.g., From, To, Subject) and body content. You are required to write a Python function that reads this email data, parses it, and performs the following operations: 1. Extract all email addresses found in the \\"From\\" and \\"To\\" fields. 2. Extract the subject of each email. 3. Count the number of emails sent by each sender. 4. List all unique domains of the senders\' email addresses. **Input Format** - A single text file containing one or more email messages. Each email message follows the standard email format with headers and body. **Output Format** - A dictionary with the following structure: ```python { \\"email_addresses\\": { \\"from\\": [list_of_email_addresses], \\"to\\": [list_of_email_addresses] }, \\"subjects\\": [list_of_subjects], \\"email_count_by_sender\\": { \\"email_address\\": count }, \\"unique_sender_domains\\": [list_of_unique_domains] } ``` **Constraints** - The input file will contain at least one email. - Emails are well-formed but may vary in header fields presence. - The function should handle edge cases like missing fields gracefully. **Performance Requirements** - The function should process the input efficiently for a file containing up to 10,000 email messages (each around 1-2 KB in size). **Function Signature** ```python def analyze_emails(file_path: str) -> dict: pass ``` **Example** Suppose the input file `emails.txt` contains the following email data: ``` From: alice@example.com To: bob@example.com Subject: Meeting Agenda Body of the email... From: charlie@example.org To: alice@example.com Subject: Follow-up Body of the email... ``` Running the function `analyze_emails(\'emails.txt\')` should return a dictionary like: ```python { \\"email_addresses\\": { \\"from\\": [\\"alice@example.com\\", \\"charlie@example.org\\"], \\"to\\": [\\"bob@example.com\\", \\"alice@example.com\\"] }, \\"subjects\\": [\\"Meeting Agenda\\", \\"Follow-up\\"], \\"email_count_by_sender\\": { \\"alice@example.com\\": 1, \\"charlie@example.org\\": 1 }, \\"unique_sender_domains\\": [\\"example.com\\", \\"example.org\\"] } ``` Good luck!","solution":"import re from collections import defaultdict def analyze_emails(file_path: str) -> dict: email_data = { \\"email_addresses\\": { \\"from\\": [], \\"to\\": [] }, \\"subjects\\": [], \\"email_count_by_sender\\": {}, \\"unique_sender_domains\\": set() } with open(file_path, \'r\') as file: content = file.read() emails = re.split(r\'nn+\', content) from_pattern = re.compile(r\'^From: (.+)\', re.MULTILINE) to_pattern = re.compile(r\'^To: (.+)\', re.MULTILINE) subject_pattern = re.compile(r\'^Subject: (.+)\', re.MULTILINE) for email in emails: from_match = from_pattern.search(email) to_match = to_pattern.search(email) subject_match = subject_pattern.search(email) if from_match: from_email = from_match.group(1).strip() email_data[\'email_addresses\'][\'from\'].append(from_email) email_data[\'email_count_by_sender\'][from_email] = email_data[\'email_count_by_sender\'].get(from_email, 0) + 1 domain = from_email.split(\'@\')[-1] email_data[\'unique_sender_domains\'].add(domain) if to_match: to_emails = [e.strip() for e in to_match.group(1).split(\',\')] email_data[\'email_addresses\'][\'to\'].extend(to_emails) if subject_match: email_data[\\"subjects\\"].append(subject_match.group(1).strip()) # Convert the set of unique domains to a list email_data[\\"unique_sender_domains\\"] = list(email_data[\\"unique_sender_domains\\"]) return email_data"},{"question":"# Question: Working with Seaborn Color Palettes You are tasked with creating visualizations in a data analysis report, and you need to utilize different color palettes from the seaborn library effectively. Your objective is to write a series of functions to showcase the ability to manipulate and utilize these color palettes in various scenarios. Requirements 1. **Get Default Color Palette**: Write a function `get_default_palette()` that returns the current default color palette of seaborn as a list of hex codes. ```python def get_default_palette(): Returns the current default color palette in seaborn as hex codes. Returns: List[str]: List of hex color codes. pass # Example # get_default_palette() -> [\'#...\'] (actual hex values may vary) ``` 2. **Get Named Color Palette**: Write a function `get_named_palette(name)` that takes a palette name as input and returns the corresponding color palette\'s hex codes. ```python def get_named_palette(name): Returns the specified named color palette in seaborn as hex codes. Args: name (str): The name of the seaborn color palette. Returns: List[str]: List of hex color codes. pass # Example # get_named_palette(\\"pastel\\") -> [\'#...\'] (actual hex values may vary) ``` 3. **Generate Continuous Colormap**: Write a function `generate_continuous_colormap(name)` that takes a palette name and returns a continuous colormap object. ```python def generate_continuous_colormap(name): Returns the specified named color palette as a continuous colormap. Args: name (str): The name of the seaborn color palette. Returns: Colormap: Matplotlib colormap object. pass # Example # generate_continuous_colormap(\\"Spectral\\") -> <matplotlib.colors.ListedColormap object at ...> ``` 4. **Visualize Data with Custom Palette**: Write a function `visualize_with_custom_palette(datax, datay, palette_name)` that creates a scatter plot with the specified seaborn color palette. ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_with_custom_palette(datax, datay, palette_name): Creates and displays a scatter plot using the specified seaborn color palette. Args: datax (list or np.array): X-axis data points. datay (list or np.array): Y-axis data points. palette_name (str): The name of the seaborn color palette. Returns: None pass # Example # visualize_with_custom_palette([1, 2, 3], [2, 4, 6], \\"husl\\") ``` Constraints - Your implementation should ensure that only valid palette names from seaborn are used. - For valid palette names, refer to seaborn\'s documentation or use the palettes shown in the cell above for testing. - Data points for the scatter plots can be any numerical values within reasonable ranges. Note: Use proper error handling where necessary and ensure your code follows best practices. Comment your code, ensuring readability and maintainability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def get_default_palette(): Returns the current default color palette in seaborn as hex codes. Returns: List[str]: List of hex color codes. return sns.color_palette().as_hex() def get_named_palette(name): Returns the specified named color palette in seaborn as hex codes. Args: name (str): The name of the seaborn color palette. Returns: List[str]: List of hex color codes. return sns.color_palette(name).as_hex() def generate_continuous_colormap(name): Returns the specified named color palette as a continuous colormap. Args: name (str): The name of the seaborn color palette. Returns: Colormap: Matplotlib colormap object. return sns.color_palette(name, as_cmap=True) def visualize_with_custom_palette(datax, datay, palette_name): Creates and displays a scatter plot using the specified seaborn color palette. Args: datax (list or np.array): X-axis data points. datay (list or np.array): Y-axis data points. palette_name (str): The name of the seaborn color palette. Returns: None palette = sns.color_palette(palette_name) sns.scatterplot(x=datax, y=datay, palette=palette) plt.show()"},{"question":"# File Analysis and Classification **Problem Statement:** You are required to write a Python function `analyze_directory(path: str) -> dict` that takes the path to a directory and returns a dictionary containing a summary of the types of files in that directory and its subdirectories. You will use the `stat` module to interpret the file types. **Function Signature:** ```python def analyze_directory(path: str) -> dict: pass ``` **Input:** - `path` (str): The path to the directory that you need to analyze. **Output:** - A dictionary with the following structure: ```python { \\"directories\\": int, # total number of directories \\"regular_files\\": int, # total number of regular files \\"character_devices\\": int, # total number of character devices \\"block_devices\\": int, # total number of block devices \\"fifos\\": int, # total number of FIFO files \\"symlinks\\": int, # total number of symbolic links \\"sockets\\": int, # total number of sockets \\"doors\\": int, # total number of doors (if supported) \\"ports\\": int, # total number of event ports (if supported) \\"whiteouts\\": int, # total number of whiteouts (if supported) } ``` **Constraints:** - The function should handle any reasonable directory depth. - The function should handle cases where certain file types (like doors, event ports, or whiteouts) are not supported on the platform by setting their count to zero or ignoring them. - The function should correctly handle permissions issues and symbolic links without causing infinite loops. **Example:** Given a directory structure as follows: ``` /example file1.txt file2.txt dir1 file3.txt special fifo1 char1 block1 socket1 symlink1 -> /example/file1.txt ``` The function call `analyze_directory(\'/example\')` might return: ```python { \\"directories\\": 2, \\"regular_files\\": 3, \\"character_devices\\": 1, \\"block_devices\\": 1, \\"fifos\\": 1, \\"symlinks\\": 1, \\"sockets\\": 1, \\"doors\\": 0, \\"ports\\": 0, \\"whiteouts\\": 0, } ``` **Notes:** - Use the `os` and `stat` modules to access file statistics and classify them. - Handle errors gracefully, such as skipping files or directories to which you do not have access.","solution":"import os import stat def analyze_directory(path: str) -> dict: summary = { \\"directories\\": 0, \\"regular_files\\": 0, \\"character_devices\\": 0, \\"block_devices\\": 0, \\"fifos\\": 0, \\"symlinks\\": 0, \\"sockets\\": 0, \\"doors\\": 0, # Only for platforms that support it \\"ports\\": 0, # Only for platforms that support it \\"whiteouts\\": 0 # Only for platforms that support it } for root, dirs, files in os.walk(path, followlinks=False): for name in dirs: try: full_path = os.path.join(root, name) mode = os.lstat(full_path).st_mode if stat.S_ISDIR(mode): summary[\\"directories\\"] += 1 except OSError: # Handle the case where we cannot access the directory continue for name in files: try: full_path = os.path.join(root, name) mode = os.lstat(full_path).st_mode if stat.S_ISREG(mode): summary[\\"regular_files\\"] += 1 elif stat.S_ISCHR(mode): summary[\\"character_devices\\"] += 1 elif stat.S_ISBLK(mode): summary[\\"block_devices\\"] += 1 elif stat.S_ISFIFO(mode): summary[\\"fifos\\"] += 1 elif stat.S_ISLNK(mode): summary[\\"symlinks\\"] += 1 elif stat.S_ISSOCK(mode): summary[\\"sockets\\"] += 1 # Adding platform-specific checks # For example, on some systems, stat.S_ISDOOR can be checked # but since S_ISDOOR is not available in standard Python stat library, # we skip the implementation for doors, ports and whiteouts except OSError: # Handle the case where we cannot access the file continue return summary"},{"question":"Task Description You are tasked with creating an asynchronous UDP echo server and client using Python\'s `asyncio` module. Your implementation should involve the use of custom transport and protocol classes as detailed in the provided documentation. Requirements # UDP Echo Server Create a custom UDP echo server that: 1. Listens for incoming messages on a specified UDP port. 2. Logs incoming messages and their source addresses. 3. Sends back the received message to the sender. # UDP Echo Client Create a custom UDP client that: 1. Sends a message to the specified UDP server. 2. Logs the sent message. 3. Receives the echoed message from the server. 4. Logs the received message. # Constraints - Use the `asyncio.DatagramProtocol` for implementing the protocols. - Use the `loop.create_datagram_endpoint()` method for setting up both the server and client transports. - Ensure that the client waits for the server\'s response asynchronously and then closes the connection properly. - The server should keep running, handling multiple clients if necessary. # Input and Output Formats - The server should accept UDP packets containing string messages. - The client will send string messages and expect to receive the same string back from the server. - Log messages to the console using `print` statements. # Performance Requirements - The solutions should handle non-blocking I/O operations efficiently. - Ensure that the data is sent out asynchronously without blocking the event loop. # Example A simple interaction between the UDP echo server and client. Server: ```python import asyncio class EchoServerProtocol(asyncio.DatagramProtocol): def datagram_received(self, data, addr): message = data.decode() print(f\\"Received {message!r} from {addr}\\") print(f\\"Send {message!r} to {addr}\\") self.transport.sendto(data, addr) async def main(): print(\\"Starting UDP server\\") loop = asyncio.get_running_loop() transport, protocol = await loop.create_datagram_endpoint( lambda: EchoServerProtocol(), local_addr=(\'127.0.0.1\', 9999) ) try: await asyncio.sleep(3600) # Run for 1 hour. finally: transport.close() asyncio.run(main()) ``` Client: ```python import asyncio class EchoClientProtocol(asyncio.DatagramProtocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost self.transport = None def connection_made(self, transport): self.transport = transport print(f\\"Send: {self.message!r}\\") self.transport.sendto(self.message.encode()) def datagram_received(self, data, addr): print(f\\"Received: {data.decode()!r}\\") print(\\"Close the socket\\") self.transport.close() def error_received(self, exc): print(f\'Error received: {exc}\') def connection_lost(self, exc): print(\\"Connection closed\\") self.on_con_lost.set_result(True) async def main(): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() message = \\"Hello World!\\" transport, protocol = await loop.create_datagram_endpoint( lambda: EchoClientProtocol(message, on_con_lost), remote_addr=(\'127.0.0.1\', 9999) ) try: await on_con_lost finally: transport.close() asyncio.run(main()) ``` # Submit your solutions in two separate files: `udp_echo_server.py` and `udp_echo_client.py`.","solution":"# udp_echo_server.py import asyncio class EchoServerProtocol(asyncio.DatagramProtocol): def datagram_received(self, data, addr): message = data.decode() print(f\\"Received {message!r} from {addr}\\") print(f\\"Send {message!r} to {addr}\\") self.transport.sendto(data, addr) async def start_udp_server(): print(\\"Starting UDP server\\") loop = asyncio.get_running_loop() transport, protocol = await loop.create_datagram_endpoint( lambda: EchoServerProtocol(), local_addr=(\'127.0.0.1\', 9999) ) try: await asyncio.sleep(3600) # Run for 1 hour. finally: transport.close() if __name__ == \\"__main__\\": asyncio.run(start_udp_server()) # udp_echo_client.py import asyncio class EchoClientProtocol(asyncio.DatagramProtocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost self.transport = None def connection_made(self, transport): self.transport = transport print(f\\"Send: {self.message!r}\\") self.transport.sendto(self.message.encode()) def datagram_received(self, data, addr): print(f\\"Received: {data.decode()!r}\\") print(\\"Close the socket\\") self.transport.close() def error_received(self, exc): print(f\'Error received: {exc}\') def connection_lost(self, exc): print(\\"Connection closed\\") self.on_con_lost.set_result(True) async def send_udp_message(message): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() transport, protocol = await loop.create_datagram_endpoint( lambda: EchoClientProtocol(message, on_con_lost), remote_addr=(\'127.0.0.1\', 9999) ) try: await on_con_lost finally: transport.close() if __name__ == \\"__main__\\": message = \\"Hello World!\\" asyncio.run(send_udp_message(message))"},{"question":"# PyTorch Elastic Job Management on Kubernetes Background TorchElastic is a library that allows PyTorch training jobs to be run elastically on Kubernetes. This means that the training job can dynamically scale the number of workers up or down. The ElasticJob controller custom resource in Kubernetes manages these jobs. Task You are to implement a function `setup_elastic_job` that sets up an elastic training job in a PyTorch environment using Kubernetes. The function should: 1. Take a configuration dictionary that specifies the job details such as the Docker image, number of minimum and maximum workers, and other Kubernetes related configurations. 2. Create the necessary Kubernetes job specifications. 3. Ensure the job can scale between the specified minimum and maximum number of workers during training. Input - A dictionary `config` with the following keys: - `job_name` (str): The name of the job. - `docker_image` (str): The Docker image to use for the training job. - `min_replicas` (int): Minimum number of worker replicas. - `max_replicas` (int): Maximum number of worker replicas. - `namespace` (str): Kubernetes namespace to run the job in. - `command` (list of str): The command to run in the training container. - `resources` (dict): Resource limits and requests for the containers. Output - The function should print out the Kubernetes job specifications in a YAML format. Constraints - Ensure that the job specification adheres to Kubernetes syntax and structure. - Handle possible exceptions while setting up the job, such as Kubernetes API failures or invalid configurations. Example ```python config = { \\"job_name\\": \\"elastic-training\\", \\"docker_image\\": \\"pytorch/pytorch:latest\\", \\"min_replicas\\": 2, \\"max_replicas\\": 5, \\"namespace\\": \\"default\\", \\"command\\": [\\"python\\", \\"train.py\\"], \\"resources\\": { \\"requests\\": { \\"cpu\\": \\"2\\", \\"memory\\": \\"4Gi\\" }, \\"limits\\": { \\"cpu\\": \\"4\\", \\"memory\\": \\"8Gi\\" } } } setup_elastic_job(config) ``` The above call should print a YAML specification for a Kubernetes ElasticJob that can scale between 2 and 5 replicas, using the given Docker image and command. **Note:** You may use the PyYAML library to help with YAML formatting.","solution":"import yaml def setup_elastic_job(config): Setup an elastic training job in a PyTorch environment using Kubernetes. job_spec = { \\"apiVersion\\": \\"batch/v1\\", \\"kind\\": \\"Job\\", \\"metadata\\": { \\"name\\": config[\\"job_name\\"], \\"namespace\\": config[\\"namespace\\"] }, \\"spec\\": { \\"template\\": { \\"spec\\": { \\"containers\\": [ { \\"name\\": config[\\"job_name\\"], \\"image\\": config[\\"docker_image\\"], \\"command\\": config[\\"command\\"], \\"resources\\": config[\\"resources\\"] } ], \\"restartPolicy\\": \\"Never\\" } }, \\"completions\\": config[\\"max_replicas\\"], \\"parallelism\\": config[\\"min_replicas\\"] } } print(yaml.dump(job_spec, default_flow_style=False)) # Example use config = { \\"job_name\\": \\"elastic-training\\", \\"docker_image\\": \\"pytorch/pytorch:latest\\", \\"min_replicas\\": 2, \\"max_replicas\\": 5, \\"namespace\\": \\"default\\", \\"command\\": [\\"python\\", \\"train.py\\"], \\"resources\\": { \\"requests\\": { \\"cpu\\": \\"2\\", \\"memory\\": \\"4Gi\\" }, \\"limits\\": { \\"cpu\\": \\"4\\", \\"memory\\": \\"8Gi\\" } } } setup_elastic_job(config)"},{"question":"# Question: Bootstrapping `pip` with `ensurepip` You are required to write a Python function that uses the `ensurepip` module to check the current version of `pip` that would be installed and then perform the bootstrapping operation under certain conditions. Function Signature ```python def manage_pip_bootstrap(should_upgrade: bool, user_install: bool, root_dir: str = None) -> str: pass ``` Input - `should_upgrade` (bool): If set to `True`, the function should attempt to upgrade an existing installation of `pip`. - `user_install` (bool): If set to `True`, the function should install `pip` in the user\'s site-packages directory instead of the global site-packages directory. - `root_dir` (str, optional): If provided, this string should specify the root directory relative to which `pip` should be installed. If `None`, the default installation location should be used. Output - The function should return a string describing the outcome of the bootstrapping process. Possible return values include: - `\\"pip installed successfully\\"` - `\\"pip upgraded successfully\\"` - `\\"ValueError: both altinstall and default_pip cannot be set\\"` - `\\"No action needed: pip is already installed and up-to-date\\"` Constraints - You should handle any potential exceptions that may arise during the bootstrapping process and return appropriate messages. - Consider the side effects mentioned in the documentation when dealing with `sys.path` and `os.environ`. Example ```python print(manage_pip_bootstrap(should_upgrade=True, user_install=False)) # Output might be: \\"pip upgraded successfully\\" or \\"pip installed successfully\\" or some other relevant message. ``` # Notes - Use the `ensurepip.version()` function to check the version of `pip` that would be installed. - Use the `ensurepip.bootstrap()` function with appropriate arguments based on the inputs to perform the bootstrapping operation. - Make sure to handle cases where both `altinstall` and `default_pip` might be set and raise a `ValueError`.","solution":"import ensurepip def manage_pip_bootstrap(should_upgrade: bool, user_install: bool, root_dir: str = None) -> str: try: pip_version = ensurepip.version().split()[-1] if should_upgrade: ensurepip.bootstrap(upgrade=True, user=user_install, root=root_dir) return \\"pip upgraded successfully\\" else: ensurepip.bootstrap(upgrade=False, user=user_install, root=root_dir) return \\"pip installed successfully\\" except ValueError as e: return \\"ValueError: \\" + str(e) except Exception as e: return \\"An error occurred: \\" + str(e)"},{"question":"**Question Title**: Visualizing Complex Relationships in a Multiple Variable Dataset **Problem Description**: You are given a dataset that contains information about several countries, particularly their healthcare expenditures and life expectancy. Using Seaborn\'s plotting capabilities, your task is to create insightful visualizations that capture the relationships between these variables and how these relationships change based on certain categorical variables. **Dataset**: The dataset consists of the following columns: - `Country`: Name of the country. - `Year`: Year of data collection. - `Spending_USD`: Healthcare spending in USD. - `Life_Expectancy`: Life expectancy in years. - `Region`: Geographical region the country belongs to. **Your Tasks**: 1. **Scatter Plot Analysis**: a. Create a scatter plot to show the relationship between `Spending_USD` and `Life_Expectancy`. Color the points based on the `Region`. b. Modify the scatter plot so that each country\'s points are shown with different marker styles based on `Country`. 2. **Line Plot Analysis**: a. Create a line plot to show the change in `Life_Expectancy` over time for each `Region`. b. Customize this plot to include confidence intervals representing the uncertainty. 3. **Facet Analysis**: a. Using `FacetGrid`, create a series of scatter plots that show the relationship between `Spending_USD` and `Life_Expectancy`, faceted by `Region` across the columns. b. For a deeper analysis, facet by `Region` on the columns and by `Year` on the rows. **Constraints**: - Ensure that your visualizations have appropriate titles, axis labels, legends, and any other necessary plot customizations to improve readability. - You must use seaborn\'s `relplot`, `scatterplot`, and `lineplot` for this implementation. **Sample Code Skeleton**: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load your dataset data = pd.read_csv(\\"health_expenditure_life_expectancy.csv\\") # Task 1: Scatter Plot Analysis # a. Scatter plot with hue based on Region plt.figure() sns.relplot(data=data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", hue=\\"Region\\") plt.title(\'Scatter plot of Spending vs Life Expectancy with Region Hue\') plt.show() # b. Scatter plot with different marker styles based on Country plt.figure() sns.relplot(data=data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", hue=\\"Region\\", style=\\"Country\\") plt.title(\'Scatter plot of Spending vs Life Expectancy with Country Styles\') plt.show() # Task 2: Line Plot Analysis # a. Line plot for Life Expectancy over time for each Region # b. Customize to include confidence intervals plt.figure() sns.relplot(data=data, x=\\"Year\\", y=\\"Life_Expectancy\\", kind=\\"line\\", hue=\\"Region\\") plt.title(\'Line plot of Life Expectancy over Time by Region\') plt.show() # Task 3: Facet Analysis # a. Facet by Region across columns plt.figure() sns.relplot(data=data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", col=\\"Region\\", kind=\\"scatter\\") plt.suptitle(\'Facet Grid of Spending vs Life Expectancy by Region\') plt.show() # b. Facet by Region and Year plt.figure() sns.relplot(data=data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", col=\\"Region\\", row=\\"Year\\", kind=\\"scatter\\") plt.suptitle(\'Facet Grid of Spending vs Life Expectancy by Region and Year\') plt.show() ``` Good luck and ensure your visualizations convey clear insights from the data!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_health_expenditure(data_path): Visualizes relationships in healthcare expenditure and life expectancy data. Parameters: data_path (str): The file path to the dataset. Returns: None # Load the dataset data = pd.read_csv(data_path) # Task 1: Scatter Plot Analysis # a. Scatter plot with hue based on Region plt.figure() sns.relplot(data=data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", hue=\\"Region\\") plt.title(\'Scatter plot of Spending vs Life Expectancy with Region Hue\') plt.xlabel(\'Healthcare Spending (USD)\') plt.ylabel(\'Life Expectancy (Years)\') plt.show() # b. Scatter plot with different marker styles based on Country plt.figure() sns.relplot(data=data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", hue=\\"Region\\", style=\\"Country\\") plt.title(\'Scatter plot of Spending vs Life Expectancy with Country Styles\') plt.xlabel(\'Healthcare Spending (USD)\') plt.ylabel(\'Life Expectancy (Years)\') plt.show() # Task 2: Line Plot Analysis # a. Line plot for Life Expectancy over time for each Region plt.figure() sns.relplot(data=data, x=\\"Year\\", y=\\"Life_Expectancy\\", kind=\\"line\\", hue=\\"Region\\", ci=None) plt.title(\'Line plot of Life Expectancy over Time by Region\') plt.xlabel(\'Year\') plt.ylabel(\'Life Expectancy (Years)\') plt.show() # b. Customize to include confidence intervals plt.figure() sns.relplot(data=data, x=\\"Year\\", y=\\"Life_Expectancy\\", kind=\\"line\\", hue=\\"Region\\") plt.title(\'Line plot of Life Expectancy over Time by Region with Confidence Intervals\') plt.xlabel(\'Year\') plt.ylabel(\'Life Expectancy (Years)\') plt.show() # Task 3: Facet Analysis # a. Facet by Region across columns plt.figure() sns.relplot(data=data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", col=\\"Region\\", kind=\\"scatter\\") plt.suptitle(\'Facet Grid of Spending vs Life Expectancy by Region\') plt.xlabel(\'Healthcare Spending (USD)\') plt.ylabel(\'Life Expectancy (Years)\') plt.show() # b. Facet by Region and Year plt.figure() sns.relplot(data=data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", col=\\"Region\\", row=\\"Year\\", kind=\\"scatter\\") plt.suptitle(\'Facet Grid of Spending vs Life Expectancy by Region and Year\') plt.xlabel(\'Healthcare Spending (USD)\') plt.ylabel(\'Life Expectancy (Years)\') plt.show()"},{"question":"# OSS Audio Device Configuration and Mixer Control **Objective:** Write a Python program that interacts with an OSS audio device and a mixer device. The program should perform the following operations: 1. **Open an Audio Device:** - Use the `ossaudiodev.open()` method to open an audio device in write mode. 2. **Set Audio Parameters:** - Set the audio format to `AFMT_S16_LE` (Signed, 16-bit audio, little-endian). - Set the audio channels to 2 (stereo). - Set the audio sampling rate to 44100 Hz (CD quality audio). 3. **Write Audio Data:** - Create a `bytes-like` object representing some audio data. - Write this data to the audio device using the `write()` method. 4. **Control Mixer Device:** - Open a mixer device using `ossaudiodev.openmixer()`. - Get the current PCM volume using the `get()` method. - Set the PCM volume to 50% on both left and right channels using the `set()` method. - Verify that the volume has been set correctly by reading it back. 5. **Close Devices:** - Explicitly close both the audio and mixer devices. **Constraints:** - Assume the audio device file is `/dev/dsp` and the mixer device file is `/dev/mixer`. - Ensure robust error handling to manage exceptions like `OSSAudioError` and `OSError`. **Input/Output:** - No input required from the user. - Output the current and new PCM volume levels before and after setting. # Example: ```python import ossaudiodev def main(): try: # Step 1: Open the audio device audio_device = ossaudiodev.open(\'/dev/dsp\', \'w\') # Step 2: Set audio parameters audio_device.setfmt(ossaudiodev.AFMT_S16_LE) audio_device.channels(2) audio_device.speed(44100) # Step 3: Write audio data audio_data = b\'x00x00x00x00\' # Example bytes-like object for audio data audio_device.write(audio_data) # Step 4: Control mixer device and set PCM volume mixer_device = ossaudiodev.openmixer(\'/dev/mixer\') current_volume = mixer_device.get(ossaudiodev.SOUND_MIXER_PCM) print(f\\"Current PCM Volume: {current_volume}\\") mixer_device.set(ossaudiodev.SOUND_MIXER_PCM, (50, 50)) new_volume = mixer_device.get(ossaudiodev.SOUND_MIXER_PCM) print(f\\"New PCM Volume: {new_volume}\\") # Step 5: Close devices audio_device.close() mixer_device.close() except ossaudiodev.OSSAudioError as e: print(f\\"OSSAudioError: {e}\\") except OSError as e: print(f\\"OSError: {e}\\") if __name__ == \\"__main__\\": main() ``` This question requires students to demonstrate their understanding of interacting with OSS audio and mixer devices using the `ossaudiodev` module. It tests their ability to set audio parameters, perform read/write operations, and manage mixer controls.","solution":"import ossaudiodev def manage_audio_device(): try: # Step 1: Open the audio device audio_device = ossaudiodev.open(\'/dev/dsp\', \'w\') # Step 2: Set audio parameters audio_device.setfmt(ossaudiodev.AFMT_S16_LE) audio_device.channels(2) audio_device.speed(44100) # Step 3: Write audio data audio_data = b\'x00x00x00x00\' # Example bytes-like object for audio data audio_device.write(audio_data) # Step 4: Control mixer device and set PCM volume mixer_device = ossaudiodev.openmixer(\'/dev/mixer\') current_volume = mixer_device.get(ossaudiodev.SOUND_MIXER_PCM) print(f\\"Current PCM Volume: {current_volume}\\") mixer_device.set(ossaudiodev.SOUND_MIXER_PCM, (50, 50)) new_volume = mixer_device.get(ossaudiodev.SOUND_MIXER_PCM) print(f\\"New PCM Volume: {new_volume}\\") # Step 5: Close devices audio_device.close() mixer_device.close() return current_volume, new_volume except ossaudiodev.OSSAudioError as e: print(f\\"OSSAudioError: {e}\\") except OSError as e: print(f\\"OSError: {e}\\") if __name__ == \\"__main__\\": manage_audio_device()"},{"question":"You are given a dataset containing information about different species of penguins, including their bill length, bill depth, flipper length, body mass, and species type. Your task is to use the seaborn `relplot` function to create visualizations that reveal patterns and relationships in the data. # Task: 1. Load the `penguins` dataset from the seaborn library. 2. Create a scatter plot that visualizes the relationship between bill length and bill depth. Use different colors for each species. 3. Enhance the scatter plot by: - Adding additional dimensions to the plot using the `size` and `style` parameters, specifically mapping `flipper_length` to `size` and `sex` to `style`. - Faceting the plot by the island column, arranging the plots in multiple rows. 4. Customize the plot further by: - Setting a specific `palette` of your choice. - Adjusting the `sizes` parameter to make sure the points are visible without being too large. - Changing the `height` and `aspect` ratios to ensure the plots fit well within the figure space. 5. Return the `FacetGrid` object and programmatically set the axis labels and titles for each subplot. 6. Finally, save the resulting plot to a file named `penguins_plot.png`. # Input and Output Specifications: - **Input:** The function will take no parameters. - **Output:** The function should return the `FacetGrid` object after all customizations and save the plot as a file. # Constraints: - Ensure the plot is clear and readable, with appropriately sized and colored points. - Handle any potential edge cases where the data might be missing. # Example Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # 1. Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Create scatter plot g = sns.relplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", size=\\"flipper_length_mm\\", style=\\"sex\\", col=\\"island\\", palette=\\"viridis\\", # you can choose your own palette sizes=(40, 400), # adjust size range for better visibility height=4, # height of each facet aspect=1 # aspect ratio ) # 5. Customize labels and titles g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.set_titles(\\"Island: {col_name}\\") # 6. Save plot to file g.savefig(\'penguins_plot.png\') return g # Visualize the penguins dataset visualize_penguins() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # 1. Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Create scatter plot g = sns.relplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", size=\\"flipper_length_mm\\", style=\\"sex\\", col=\\"island\\", palette=\\"viridis\\", # you can choose your own palette sizes=(40, 400), # adjust size range for better visibility height=4, # height of each facet aspect=1 # aspect ratio ) # 5. Customize labels and titles g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.set_titles(\\"Island: {col_name}\\") # 6. Save plot to file g.savefig(\'penguins_plot.png\') return g"},{"question":"# Question: Implement a Distributed Training Setup with PyTorch Objective Your task is to set up a distributed training environment using PyTorch\'s `torch.distributed` package. You will implement a script that initializes a distributed process group, performs a collective operation (all_reduce), and verifies the operation. Requirements 1. **Initialization**: Use environment variable initialization to set up the process group. 2. **Input and Output**: - Input: Tensor with some values. - Output: The result of an `all_reduce` operation on the input tensor across all processes. 3. **Constraints**: - Use the `nccl` backend for the distributed process group. - Assume a world size of 2 (i.e., two processes involved in the distributed training). - Use GPUs for computation. Instructions 1. **Initialize**: - Set the environment variables `MASTER_PORT`, `MASTER_ADDR`, `WORLD_SIZE`, and `RANK` appropriately for two processes. - Initialize the process group with the `nccl` backend. 2. **Implement Collective Operation**: - Each process should create a tensor with initial values. - Perform an `all_reduce` operation on the tensors. 3. **Verification**: - Print and verify that the result of the `all_reduce` operation is as expected. Example Code Structure ```python import os import torch import torch.distributed as dist def init_process(rank, size, fn, backend=\'nccl\'): Initialize the distributed environment. os.environ[\'MASTER_ADDR\'] = \'127.0.0.1\' os.environ[\'MASTER_PORT\'] = \'29500\' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) def all_reduce_operation(rank, size): Perform an all_reduce operation. # Create tensor with initial values tensor = torch.ones(10).cuda(rank) * rank # Perform all_reduce operation dist.all_reduce(tensor, op=dist.reduce_op.SUM) # Print the result print(f\'Rank {rank}, tensor: {tensor}\') if __name__ == \\"__main__\\": size = 2 rank = int(os.environ[\'RANK\']) init_process(rank, size, all_reduce_operation) ``` Notes: - This script should be executed twice with different `RANK` values (0 and 1) to simulate the two processes involved in distributed training. - Ensure you have the necessary environment settings and GPU resources before running the script. **Assessment Criteria**: - Correct initialization of the process group. - Correct implementation of the `all_reduce` operation. - Proper synchronization and verification of the operation results across processes.","solution":"import os import torch import torch.distributed as dist def init_process(rank, size, fn, backend=\'nccl\'): Initialize the distributed environment. os.environ[\'MASTER_ADDR\'] = \'127.0.0.1\' os.environ[\'MASTER_PORT\'] = \'29500\' dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) dist.destroy_process_group() def all_reduce_operation(rank, size): Perform an all_reduce operation. # Create tensor with initial values tensor = torch.ones(10).cuda(rank) * rank # Perform all_reduce operation dist.all_reduce(tensor, op=dist.ReduceOp.SUM) # Expected result is a tensor filled with the sum of ranks (0 + 1) = 1 expected_result = torch.ones(10).cuda(rank) * sum(range(size)) assert torch.equal(tensor, expected_result), f\'Rank {rank}, tensor: {tensor}, expected: {expected_result}\' print(f\'Rank {rank}, tensor: {tensor}\') if __name__ == \\"__main__\\": size = 2 rank = int(os.environ[\'RANK\']) init_process(rank, size, all_reduce_operation)"},{"question":"# Topological Sorting and Cycle Detection You are given a series of tasks, each with a list of prerequisites that define a directed acyclic graph (DAG). Your task is to implement a function that: 1. Constructs the graph using the `TopologicalSorter` class. 2. Handles cycle detection, if any. 3. Returns a valid topological ordering of the tasks, if possible. Implement the following function: ```python from graphlib import TopologicalSorter, CycleError def find_task_order(tasks): Determines a valid order to complete the tasks based on the given prerequisites. Args: tasks (dict): A dictionary where the keys are task names and the values are sets of prerequisites for the respective tasks. Returns: list: A list containing a valid topological ordering of the tasks, or an empty list if a cycle is detected in the tasks. # Your implementation here ``` # Input Format: - `tasks`: A dictionary where: - Keys are task names (strings). - Values are sets of prerequisite tasks (each a string) that need to be completed before the key task. # Output Format: - Returns a list that represents a valid topological order of the tasks. - If a cycle is detected in the graph, return an empty list. # Example: ```python tasks = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}, \\"A\\": set() } assert find_task_order(tasks) in [[\\"A\\", \\"C\\", \\"B\\", \\"D\\"], [\\"A\\", \\"B\\", \\"C\\", \\"D\\"]] tasks_with_cycle = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}, \\"A\\": {\\"D\\"} } assert find_task_order(tasks_with_cycle) == [] ``` # Constraints: - All task names are unique. - The dictionary defines a valid graph structure, but it may contain cycles. # Explanation: In the first example, one possible valid order to complete the tasks is [\\"A\\", \\"C\\", \\"B\\", \\"D\\"] or [\\"A\\", \\"B\\", \\"C\\", \\"D\\"]. In the second example, there is a cycle involving tasks \\"D\\", \\"B\\", \\"A\\", and \\"C\\", making it impossible to determine a valid ordering, hence an empty list is returned. # Notes: - Utilize the methods provided by the `TopologicalSorter` class to construct and manipulate the graph, and handle any detected cycles using the provided functionality.","solution":"from graphlib import TopologicalSorter, CycleError def find_task_order(tasks): Determines a valid order to complete the tasks based on the given prerequisites. Args: tasks (dict): A dictionary where the keys are task names and the values are sets of prerequisites for the respective tasks. Returns: list: A list containing a valid topological ordering of the tasks, or an empty list if a cycle is detected in the tasks. ts = TopologicalSorter() for task, prerequisites in tasks.items(): ts.add(task, *prerequisites) try: order = list(ts.static_order()) return order except CycleError: return []"},{"question":"# Coding Assessment **Objective:** Implement a Python function and write a corresponding unittest test suite that adheres to the guidelines provided in the \\"test\\" package documentation. **Function Description:** Implement a function `validate_password` that takes a string as input and returns `True` if the string is a valid password according to the following rules: 1. The password must be at least 8 characters long. 2. The password must contain at least one uppercase letter, one lowercase letter, one digit, and one special character from `!@#%^&*()_+-=[]{}|;:\'\\",.<>?`. **Input:** - A single string representing the password. **Output:** - A boolean value: `True` if the password is valid, otherwise `False`. **Examples:** - `validate_password(\\"A1b@cdEf\\")` should return `True` - `validate_password(\\"abcdefg\\")` should return `False` - `validate_password(\\"A1@bc\\")` should return `False` - `validate_password(\\"Ab1@Ab1@\\")` should return `True` **Task:** 1. Implement the function `validate_password(password: str) -> bool`. 2. Write a `unittest` test suite in a separate module named `test_validate_password.py` following the guidelines described in the provided documentation. Your test suite should: - Include test cases to cover valid and invalid passwords. - Include edge cases, such as very long passwords, passwords with no special characters, etc. - Utilize the utilities and functions provided by `test.support` if necessary. **Constraints:** - Make sure your implementation and tests follow the Python style conventions. - Ensure all temporary resources used in the tests are cleaned up. # Implementation 1. Function Implementation Implement the `validate_password` function in a module named `password_validator.py`. 2. Unit Test Implementation Create a separate module named `test_validate_password.py` and write a unittest suite to test the `validate_password` function. Here is the structure: ``` project/ â”‚ â”œâ”€â”€ password_validator.py â”‚ â””â”€â”€ test_validate_password.py ``` Example of `test_validate_password.py`: ```python import unittest from password_validator import validate_password class TestValidatePassword(unittest.TestCase): def test_valid_password(self): # Tests validating that correct passwords return True self.assertTrue(validate_password(\\"A1b@cdEf\\")) self.assertTrue(validate_password(\\"Ab1@Ab1@\\")) def test_invalid_password_length(self): # Tests validating that short passwords return False self.assertFalse(validate_password(\\"Ab1@\\")) def test_invalid_password_no_uppercase(self): # Tests validating that passwords without uppercase return False self.assertFalse(validate_password(\\"1234abcd@\\")) def test_invalid_password_no_digit(self): # Tests validating that passwords without a digit return False self.assertFalse(validate_password(\\"Abcd@eff\\")) def test_invalid_password_no_special_char(self): # Tests validating that passwords without special characters return False self.assertFalse(validate_password(\\"A1bcdefg\\")) # ... More tests as needed to ensure all paths are tested if __name__ == \'__main__\': unittest.main() ``` Good luck!","solution":"def validate_password(password: str) -> bool: Returns True if the password is valid according to the following conditions: 1. The password must be at least 8 characters long. 2. The password must contain at least one uppercase letter. 3. The password must contain at least one lowercase letter. 4. The password must contain at least one digit. 5. The password must contain at least one special character from !@#%^&*()_+-=[]{}|;:\'\\",.<>? Args: password (str): The password string to be validated. Returns: bool: True if the password is valid, otherwise False. if len(password) < 8: return False has_upper = any(char.isupper() for char in password) has_lower = any(char.islower() for char in password) has_digit = any(char.isdigit() for char in password) has_special = any(char in \\"!@#%^&*()_+-=[]{}|;:\'\\",.<>?\\" for char in password) return has_upper and has_lower and has_digit and has_special"},{"question":"# Question: Dynamic Class System You are tasked with creating a dynamic class system using the `types` module in Python. This system should allow users to dynamically define new classes with given attributes and methods, and it should support inheritance. You are required to implement a function `create_dynamic_class` which allows users to define a class dynamically. Function Signature ```python def create_dynamic_class(name: str, bases: tuple = (), attributes: dict = None, methods: dict = None) -> type: ``` Parameters - `name` (str): Name of the class to be created. - `bases` (tuple): A tuple of base classes for the new class. (default is an empty tuple). - `attributes` (dict): A dictionary of attribute names and their values to be added to the class. (default is None). - `methods` (dict): A dictionary of method names and their implementations to be added to the class. (default is None). Returns - Returns the new class created dynamically. Description 1. Use `types.new_class` to create the class dynamically. 2. The `methods` dictionary will have method names as keys and function implementations as values. These functions should be bound to the class. 3. The `attributes` dictionary will have attribute names as keys and their values. 4. If no attributes or methods are provided, the class should still be created with the given name and bases. Example Usage ```python # Defining methods for the new class def greet(self): return f\\"Hello, my name is {self.name}.\\" def set_age(self, age): self.age = age # Creating a dynamic class MyDynamicClass = create_dynamic_class( name=\\"Person\\", bases=(), attributes={\'species\': \'Homo sapiens\'}, methods={\'greet\': greet, \'set_age\': set_age} ) # Creating an instance of the dynamic class person_instance = MyDynamicClass() person_instance.name = \\"Alice\\" print(person_instance.greet()) # Output: Hello, my name is Alice. person_instance.set_age(30) print(person_instance.age) # Output: 30 ``` Constraints - The `name` should be a valid class name. - Both `attributes` and `methods` dictionaries can be empty or None. - The function implementations in the `methods` dictionary should accept `self` as their first argument to simulate instance methods. - Assume that attribute values and method implementations are valid and well-formed. Additional Information - You may use `types.new_class` and other relevant functions from the `types` module to achieve the task. This question tests your understanding of dynamic class creation, attribute and method binding, and working with class hierarchies in Python.","solution":"import types def create_dynamic_class(name: str, bases: tuple = (), attributes: dict = None, methods: dict = None) -> type: # Initialize attributes and methods if they are None if attributes is None: attributes = {} if methods is None: methods = {} # Function to add attributes and methods to the class def class_body(ns): ns.update(attributes) ns.update(methods) # Create the new class using types.new_class new_class = types.new_class(name, bases, {}, class_body) return new_class"},{"question":"# Advanced Python Asyncio Subprocess Management In this assessment, you will demonstrate your understanding of asyncio\'s subprocess management by implementing a function that runs multiple shell commands concurrently, captures their outputs, and handles possible errors. Task: Write a function `run_commands(commands: list[str]) -> dict[str, dict[str, str]]` which: 1. Takes a list of shell command strings to be executed concurrently. 2. Runs each command in a separate subprocess using `asyncio.create_subprocess_shell`. 3. Captures the standard output and standard error of each command. 4. Returns a dictionary where each key is the command and the value is another dictionary with two keys `\'stdout\'` and `\'stderr\'` containing the respective outputs. Function Signature: ```python import asyncio import aiofiles async def run_commands(commands: list[str]) -> dict[str, dict[str, str]]: pass ``` Example: ```python import asyncio commands = [ \\"echo \'Hello, World!\'\\", \\"ls non_existent_directory\\", \\"sleep 2; echo \'Done sleeping!\'\\" ] result = asyncio.run(run_commands(commands)) print(result) ``` Expected Output: ```python { \\"echo \'Hello, World!\'\\": { \\"stdout\\": \\"Hello, World!n\\", \\"stderr\\": \\"\\" }, \\"ls non_existent_directory\\": { \\"stdout\\": \\"\\", \\"stderr\\": \\"ls: cannot access \'non_existent_directory\': No such file or directoryn\\" }, \\"sleep 2; echo \'Done sleeping!\'\\": { \\"stdout\\": \\"Done sleeping!n\\", \\"stderr\\": \\"\\" } } ``` # Constraints: 1. Use `asyncio.create_subprocess_shell` for creating subprocesses. 2. Make sure subprocesses run concurrently. 3. Handle all exceptions that might occur during the command execution and capture error messages accurately. 4. Use the `PIPE` option to capture standard output and standard error. # Important Notes: - Do not forget to decode the subprocess output from bytes to string. - Ensure that the commands run in a truly asynchronous manner to optimize performance.","solution":"import asyncio async def run_command(command: str) -> dict[str, str]: process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return { \\"stdout\\": stdout.decode().strip(), \\"stderr\\": stderr.decode().strip() } async def run_commands(commands: list[str]) -> dict[str, dict[str, str]]: tasks = [run_command(command) for command in commands] results = await asyncio.gather(*tasks) return {commands[i]: result for i, result in enumerate(results)}"},{"question":"**Advanced Python Type Hinting Challenge** The Python 3.9 and later provides built-in types for type hinting, such as `GenericAlias`, which allows for creating type hints for generic data structures. Your task is to implement a function that checks if the types of the elements in a given generic list match the expected type hinted by a `GenericAlias`. # Instructions 1. **Function Signature** ```python def check_generic_alias(instance, generic_alias): pass ``` 2. **Input** * `instance`: The input list instance whose elements\' types are to be verified. - Type: `List[Any]` * `generic_alias`: The `GenericAlias` object specifying the expected type of the elements. - Type: `GenericAlias` 3. **Output** * Return `True` if all elements in the list match the type specified by the `GenericAlias`, otherwise `False`. - Type: `bool` 4. **Constraints** * The input list can contain any type of Python objects. * The `GenericAlias` object will always be constructed using the Python type hints mechanism (`List[<type>]`). # Example ```python from typing import List # Instantiate a GenericAlias object for list of integers GenericAlias = List[int] # Sample list sample_list = [1, 2, 3, 4] # Call the function print(check_generic_alias(sample_list, GenericAlias)) # Should return True # Another sample list sample_list = [1, \'2\', 3, 4] # Call the function print(check_generic_alias(sample_list, GenericAlias)) # Should return False ``` # Explanation In the first example, all elements in `sample_list` are integers, which matches the `GenericAlias` specifying a list of integers, so the function returns `True`. In the second example, not all elements are integers, so the function returns `False`. Your task is to implement the `check_generic_alias` function that performs this verification using the `GenericAlias` type hinting mechanism for Python types. Hint: You may want to explore the `__args__` attribute of the `GenericAlias` object to understand the expected type of the list elements.","solution":"from typing import List, get_args def check_generic_alias(instance, generic_alias): Checks if all elements in the instance list match the type hinted by generic_alias. :param instance: List containing any type of elements :param generic_alias: A GenericAlias object specifying the expected type of the elements :return: True if all elements in the list match the specified type, otherwise False # Extract the element type from the generic alias element_type = get_args(generic_alias)[0] # Check every element in the instance list return all(isinstance(element, element_type) for element in instance)"},{"question":"<|Analysis Begin|> The provided document focuses on how to use `seaborn.objects`, a part of the seaborn library, particularly for customizing plot labels. It includes details on how to: 1. Load a dataset using `seaborn.load_dataset`. 2. Create a basic plot using `seaborn.objects.Plot`. 3. Customize labels using the `label` method. 4. Use strings and functions to override default labels. 5. Set plot titles, both for single-axes plots and faceted plots. 6. Customize titles using the `title` parameter and format strings. 7. Add labels for each layer in a plot. The document provides sufficient information on creating and customizing plots using seaborn, making it suitable for crafting a coding assessment question that involves creating a plot and applying various customizations using seaborn. <|Analysis End|> <|Question Begin|> Coding Assessment Question **Objective**: Demonstrate your understanding of seaborn by creating and customizing plots using the `seaborn.objects` module. **Task**: 1. Load the \\"penguins\\" dataset. 2. Create a scatter plot (`Dot` plot) with `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis, colored by `species`. 3. Customize the plot by: - Setting the x-axis label to \\"Bill Length (mm)\\". - Setting the y-axis label to \\"Bill Depth (mm)\\". - Setting the plot title to \\"Penguin Bill Measurements by Species\\". 4. Create a faceted plot broken down by the `sex` column and: - Set the overall plot title to \\"Distinct Bill Measurements by Sex\\". - Ensure the title for each facet is capitalized. **Input**: - No user input is required; the dataset is loaded within the script. **Output**: - Two visual output plots should be generated as specified. **Code Implementation**: You need to write a Python function named `create_penguin_plots()` which performs the tasks described. The function should not return any values but should display the generated plots. **Example of Usage**: ```python create_penguin_plots() ``` **Constraints**: - Use only the seaborn library for this task. - Adhere to the customizations specified for labels and titles. - The function will be evaluated on the visual correctness of the plots generated. **Performance Requirements**: - The plots should be generated efficiently. - Ensure the function runs without any errors and displays the required plots correctly. ```python import seaborn.objects as so from seaborn import load_dataset def create_penguin_plots(): # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create a scatter plot scatter_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") .label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", title=\\"Penguin Bill Measurements by Species\\") ) scatter_plot.show() # Step 3: Create a faceted plot faceted_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") .facet(\\"sex\\") .label(title=\\"Distinct Bill Measurements by Sex\\", col=str.capitalize) ) faceted_plot.show() # Example Usage create_penguin_plots() ``` This will generate the required plots with the specified customizations.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plots(): # Step 1: Load the dataset penguins = load_dataset(\\"penguins\\") # Step 2: Create a scatter plot scatter_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") .label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", title=\\"Penguin Bill Measurements by Species\\") ) scatter_plot.show() # Step 3: Create a faceted plot faceted_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") .facet(\\"sex\\") .label(title=\\"Distinct Bill Measurements by Sex\\", col=str.capitalize) ) faceted_plot.show() # Example Usage create_penguin_plots()"},{"question":"**Objective:** Write a Python program that demonstrates the creation, manipulation, and reading of an INI configuration file using the `configparser` module. **Task:** 1. Create an INI file with the following structure and content using the `configparser` module: ``` [DEFAULT] Compression = yes CompressionLevel = 9 [server1] Host = 127.0.0.1 Port = 8080 Active = True [server2] Host = 192.168.1.1 Port = 9090 Active = False ``` 2. Write a function `update_config(file_path)` that: - Reads the INI file from the given `file_path`. - Checks the value of `Active` for `server1`. If it is `True`, update `Port` to `80` and `CompressionLevel` to `5`. If it is `False`, update `Port` to `8081`. - Adds a new section `[server3]` with the following content: ``` Host = 10.10.10.10 Port = 7070 Active = True ``` 3. Write a function `display_config(file_path)` that: - Reads the INI file from the given `file_path`. - Prints out a sorted list of all sections (excluding the `[DEFAULT]` section). - Prints the `Host` and `Port` of each section in the sorted order of sections. **Input:** - `file_path` (str): Path to the INI configuration file. **Output:** - The display function should output all sections\' `Host` and `Port` in sorted order after updating. **Constraints:** - Assume that the `file_path` always points to a valid writable location. - The INI file should be created if it does not exist. **Performance Requirements:** - The function should handle large INI files (up to 1MB) efficiently. **Example:** ```python create_sample_config(\'example.ini\') update_config(\'example.ini\') display_config(\'example.ini\') ``` **Expected Output:** ``` Sections: [\'server1\', \'server2\', \'server3\'] server1 -> Host: 127.0.0.1, Port: 80 server2 -> Host: 192.168.1.1, Port: 9090 server3 -> Host: 10.10.10.10, Port: 7070 ``` **Note:** - Remember to handle file operations securely and ensure all changes are written back to the file.","solution":"import configparser import os def create_sample_config(file_path): config = configparser.ConfigParser() config[\'DEFAULT\'] = { \'Compression\': \'yes\', \'CompressionLevel\': \'9\' } config[\'server1\'] = { \'Host\': \'127.0.0.1\', \'Port\': \'8080\', \'Active\': \'True\' } config[\'server2\'] = { \'Host\': \'192.168.1.1\', \'Port\': \'9090\', \'Active\': \'False\' } with open(file_path, \'w\') as configfile: config.write(configfile) def update_config(file_path): config = configparser.ConfigParser() config.read(file_path) if config.getboolean(\'server1\', \'Active\'): config.set(\'server1\', \'Port\', \'80\') config.set(\'DEFAULT\', \'CompressionLevel\', \'5\') else: config.set(\'server1\', \'Port\', \'8081\') if not config.has_section(\'server3\'): config.add_section(\'server3\') config.set(\'server3\', \'Host\', \'10.10.10.10\') config.set(\'server3\', \'Port\', \'7070\') config.set(\'server3\', \'Active\', \'True\') with open(file_path, \'w\') as configfile: config.write(configfile) def display_config(file_path): config = configparser.ConfigParser() config.read(file_path) sections = sorted([section for section in config.sections()]) print(f\\"Sections: {sections}\\") for section in sections: host = config.get(section, \'Host\') port = config.get(section, \'Port\') print(f\\"{section} -> Host: {host}, Port: {port}\\")"},{"question":"**Question: Advanced CUDA Tuning with PyTorch** PyTorch\'s `torch.cuda.tunable` module offers a set of functionalities designed for efficient tuning of CUDA operations. Your task is to create a function that configures and initiates a tuning session, and then retrieves and saves the tuning results. Implement the following function: ```python def configure_and_run_tuning(filename: str, max_duration: int, max_iterations: int) -> dict: \'\'\' Configures the CUDA tuning environment, runs the tuning session, retrieves the results, and saves them to a specified file. Parameters: filename (str): The name of the file where tuning results will be saved. max_duration (int): The maximum duration (in seconds) allowed for tuning. max_iterations (int): The maximum number of iterations allowed for tuning. Returns: dict: A dictionary containing tuning results. \'\'\' # Your implementation here ``` **Function Details:** 1. **Enabling Tuning:** - Ensure that tuning is enabled using the appropriate `torch.cuda.tunable` function. 2. **Setting Parameters:** - Set the maximum tuning duration using `max_duration`. - Set the maximum number of tuning iterations using `max_iterations`. 3. **Initializing and Running Tuning:** - Invoke tuning operations and ensure that the tuning session affects the specified file, `filename`. 4. **Saving Results:** - Save the tuning results to the file indicated by `filename`. 5. **Returning Results:** - Return the tuning results as a dictionary. **Constraints:** - You should assume CUDA is available and properly configured. - This is a prototype feature, so it might be subject to changes in actual usage scenarios. - Ensure all steps are handled even if tuning is already enabled or previously configured. **Example Usage:** ```python results = configure_and_run_tuning(\\"example_tuning_results.json\\", 300, 100) print(f\\"Tuning Results: {results}\\") ``` This function assesses your understanding of configuring CUDA tuning parameters, handling file operations in the context of tuning, and retrieving results using PyTorch\'s `torch.cuda.tunable` module.","solution":"import torch import json def configure_and_run_tuning(filename: str, max_duration: int, max_iterations: int) -> dict: \'\'\' Configures the CUDA tuning environment, runs the tuning session, retrieves the results, and saves them to a specified file. Parameters: filename (str): The name of the file where tuning results will be saved. max_duration (int): The maximum duration (in seconds) allowed for tuning. max_iterations (int): The maximum number of iterations allowed for tuning. Returns: dict: A dictionary containing tuning results. \'\'\' # Ensure that tuning is enabled if not torch.cuda.is_tunable_enabled(): torch.cuda.enable_tuning(True) # Set the maximum parameters for tuning duration and iterations torch.cuda.set_tuning_max_duration(max_duration) torch.cuda.set_tuning_max_iterations(max_iterations) # Run the tuning session torch.cuda.run_tuning() # Retrieve tuning results tuning_results = torch.cuda.get_tuning_results() # Save results to the specified file with open(filename, \'w\') as file: json.dump(tuning_results, file) return tuning_results"},{"question":"Problem Statement Create a terminal-based text editor using the `curses` module in Python. This editor should allow the user to: - Open a specified text file. - Display the contents of the file in a scrollable window. - Edit the contents of the file, supporting basic text editing operations. - Save changes back to the file. - Exit the editor gracefully, ensuring the terminal returns to its normal state. # Requirements 1. **Initialization**: - Begin with initializing the curses application using `curses.initscr()`. - Set up the terminal mode to enable color, disable line buffering, and enable capturing special keys. 2. **File Handling**: - Prompt the user to enter the file path to open. - Read the file contents and display them in a scrollable window. 3. **Text Editing**: - Support basic editing commands including: - Navigate with arrow keys. - Insert and delete characters. - Move the cursor within the text. - Scroll through the text if it exceeds the window size. 4. **Saving and Exiting**: - Allow the user to save the changes back to the file using a specific key command. - Provide a key command to exit the editor. 5. **UI Components**: - Use `curses.newwin()` to create a separate window for the text editor. - Implement a status bar to display useful information like current mode (Normal/Insert), file name, cursor position, etc. - Use color highlights for the status bar and any other important UI elements. # Constraints - Do not use external libraries other than `curses`. - Handle potential exceptions, for instance when reading/writing files, and ensure the terminal is restored to a normal state upon exiting. # Example Hereâ€™s a basic outline of how your main loop might handle user input for editing: ```python import curses def main(stdscr): # Set up curses screen curses.curs_set(1) curses.noecho() curses.cbreak() stdscr.keypad(True) # Initialize colors curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) # Create a new window for the editor begin_x = 2 begin_y = 2 height = 20 width = 60 win = curses.newwin(height, width, begin_y, begin_x) # Your editor logic here file_path = \\"/path/to/your/textfile.txt\\" # Reading and displaying the file contents try: with open(file_path, \'r\') as f: lines = f.readlines() except IOError: lines = [] for i, line in enumerate(lines): win.addstr(i, 0, line.strip(), curses.A_NORMAL) win.refresh() # Editing loop while True: ch = win.getch() if ch == curses.KEY_UP: # handle up arrow key pass elif ch == curses.KEY_DOWN: # handle down arrow key pass elif ch == ord(\'q\'): break # Exit the editor # Add other key handling for editing here # Cleanup curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main) ``` # Submission Submit your Python script implementing the text editor. Ensure that your code is well-commented to explain the logic and steps clearly.","solution":"import curses def open_file(file_path): Opens the file and returns the contents as a list of lines. If the file does not exist, returns an empty list. try: with open(file_path, \'r\') as f: return f.readlines() except IOError: return [] def save_file(file_path, lines): Saves the list of lines to the file. try: with open(file_path, \'w\') as f: f.writelines(lines) except IOError: pass def main(stdscr): curses.curs_set(1) curses.noecho() curses.cbreak() stdscr.keypad(True) # Initialize colors curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) # Set up text window max_y, max_x = stdscr.getmaxyx() begin_y = 1 begin_x = 1 height = max_y - 2 width = max_x - 2 win = curses.newwin(height, width, begin_y, begin_x) # File path input stdscr.addstr(0, 0, \\"Enter file path: \\") stdscr.refresh() curses.echo() file_path = stdscr.getstr(0, 15, max_x - 15).decode(\'utf-8\') curses.noecho() lines = open_file(file_path) cursor_y, cursor_x = 0, 0 for i, line in enumerate(lines): win.addstr(i, 0, line.strip(), curses.A_NORMAL) win.addstr(height - 1, 0, \\"Press \'Ctrl+S\' to save, \'Ctrl+Q\' to quit\\", curses.color_pair(1)) win.refresh() while True: ch = win.getch() if ch == curses.KEY_UP: cursor_y = max(0, cursor_y - 1) elif ch == curses.KEY_DOWN: cursor_y = min(len(lines) - 1, cursor_y + 1) elif ch == curses.KEY_LEFT: cursor_x = max(0, cursor_x - 1) elif ch == curses.KEY_RIGHT: cursor_x = min(len(lines[cursor_y]) - 1 if lines else 0, cursor_x + 1) elif ch == ord(\'n\'): lines.insert(cursor_y + 1, \'n\') cursor_y += 1 cursor_x = 0 elif ch == 127: # Handle backspace if cursor_x > 0: lines[cursor_y] = lines[cursor_y][:cursor_x - 1] + lines[cursor_y][cursor_x:] cursor_x -= 1 elif cursor_y > 0: cursor_x = len(lines[cursor_y - 1]) lines[cursor_y - 1] = lines[cursor_y - 1] + lines.pop(cursor_y) cursor_y -= 1 elif ch == 19: # Ctrl+S to save save_file(file_path, lines) elif ch == 17: # Ctrl+Q to quit break else: if cursor_x < len(lines[cursor_y]): lines[cursor_y] = lines[cursor_y][:cursor_x] + chr(ch) + lines[cursor_y][cursor_x:] else: lines[cursor_y] += chr(ch) cursor_x += 1 win.clear() for i, line in enumerate(lines): win.addstr(i, 0, line.strip(), curses.A_NORMAL) win.addstr(height - 1, 0, \\"Press \'Ctrl+S\' to save, \'Ctrl+Q\' to quit\\", curses.color_pair(1)) win.refresh() win.move(cursor_y, cursor_x) # Cleanup curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Asyncio Subprocesses: A Practical Exercise You are tasked with implementing a function that executes multiple shell commands asynchronously, collects their output, and manages their lifecycle. This will test your understanding of Python\'s asyncio subprocess module. Task Implement the function `run_commands(commands: List[str]) -> Tuple[Dict[str, str], Dict[str, int]]`: 1. **Input**: - `commands`: A list of shell commands to run. 2. **Output**: - A tuple containing: - A dictionary where the keys are the commands and the values are their respective standard output, trimmed of any trailing whitespace. - A dictionary where the keys are the commands and the values are their respective exit codes. Constraints 1. You need to run all commands concurrently. 2. If a command fails (returns non-zero exit code), you should still collect its standard output and its exit code. 3. Use `asyncio.create_subprocess_shell` to run the shell commands. 4. Use the `communicate()` method to capture both stdout and stderr. 5. Ensure no deadlocks occur by correctly managing the subprocess pipes. Example ```python import asyncio from typing import List, Tuple, Dict async def run_commands(commands: List[str]) -> Tuple[Dict[str, str], Dict[str, int]]: # Your implementation here # Example usage commands = [\\"echo Hello\\", \\"ls /nonexistent_directory\\", \\"sleep 1; echo Done\\"] result = asyncio.run(run_commands(commands)) print(result) ``` **Expected Output**: ```python ( { \\"echo Hello\\": \\"Hello\\", \\"ls /nonexistent_directory\\": \\"\\", # Or appropriate error message based on the system \\"sleep 1; echo Done\\": \\"Done\\" }, { \\"echo Hello\\": 0, \\"ls /nonexistent_directory\\": 1, # Exit code for non-existent directory \\"sleep 1; echo Done\\": 0 } ) ``` Notes - Trim any trailing whitespace from the output of each command. - Handle exceptions gracefully and ensure all subprocesses are correctly awaited. This task requires comprehensively applying the asyncio subprocess module concepts, demonstrating understanding of asynchronous subprocess management, error handling, and concurrent execution.","solution":"import asyncio from typing import List, Tuple, Dict async def run_command(command: str): process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() stdout = stdout.decode().strip() stderr = stderr.decode().strip() return (command, stdout, process.returncode) async def run_commands(commands: List[str]) -> Tuple[Dict[str, str], Dict[str, int]]: tasks = [run_command(command) for command in commands] results = await asyncio.gather(*tasks) output_dict = {} exit_code_dict = {} for command, output, exit_code in results: output_dict[command] = output exit_code_dict[command] = exit_code return output_dict, exit_code_dict"},{"question":"**COVID-19 Data Analysis with pandas** You are given a CSV file `covid19_data.csv` containing COVID-19 data with the following columns: - `date`: The date of the record in `YYYY-MM-DD` format. - `country`: The country for which the record applies. - `new_cases`: The number of new COVID-19 cases reported on that date. - `new_deaths`: The number of new COVID-19 deaths reported on that date. Your task is to implement the following functions using pandas: 1. `get_date_range_data(df, start_date, end_date)`: This function takes a DataFrame `df`, a `start_date`, and an `end_date`. It should return a new DataFrame containing only the rows between `start_date` and `end_date`. 2. `get_new_cases_by_country(df)`: This function takes a DataFrame `df` and returns a new DataFrame with the total number of new cases for each country sorted in descending order. 3. `get_most_affected_country(df)`: This function takes a DataFrame `df` and returns the name of the country with the highest number of new cases in total. 4. `pivot_countries_data(df)`: This function takes a DataFrame `df` and returns a pivot table indexed by date with columns for each country showing the new cases reported on that date. 5. `add_weekly_data_column(df)`: This function takes a DataFrame `df` and adds a new column `weekly_new_cases`, which represents the total new cases reported in the last 7 days (including the current day) for each row. # Constraints - Use pandas functions efficiently to achieve the desired results. - The `date` column should be handled as datetime. - Implement error handling for invalid date ranges. # Input and Output Formats 1. `get_date_range_data(df, start_date, end_date)` - Input: `df` (DataFrame), `start_date` (str), `end_date` (str) - Output: DataFrame 2. `get_new_cases_by_country(df)` - Input: `df` (DataFrame) - Output: DataFrame 3. `get_most_affected_country(df)` - Input: `df` (DataFrame) - Output: str 4. `pivot_countries_data(df)` - Input: `df` (DataFrame) - Output: DataFrame 5. `add_weekly_data_column(df)` - Input: `df` (DataFrame) - Output: DataFrame Here\'s a sample usage of the functions: ```python import pandas as pd # Read the CSV file df = pd.read_csv(\'covid19_data.csv\') # Convert the `date` column to datetime df[\'date\'] = pd.to_datetime(df[\'date\']) # Example usage of the functions filtered_df = get_date_range_data(df, \'2021-01-01\', \'2021-01-31\') cases_by_country = get_new_cases_by_country(df) most_affected_country = get_most_affected_country(df) pivoted_df = pivot_countries_data(df) weekly_df = add_weekly_data_column(df) print(filtered_df.head()) print(cases_by_country.head()) print(most_affected_country) print(pivoted_df.head()) print(weekly_df.head()) ``` Make sure to test your functions with various scenarios to ensure they handle all edge cases appropriately.","solution":"import pandas as pd def get_date_range_data(df, start_date, end_date): Returns rows between start_date and end_date inclusive. mask = (df[\'date\'] >= start_date) & (df[\'date\'] <= end_date) return df.loc[mask] def get_new_cases_by_country(df): Returns the total new cases for each country, sorted in descending order. return df.groupby(\'country\')[\'new_cases\'].sum().reset_index().sort_values(by=\'new_cases\', ascending=False) def get_most_affected_country(df): Returns the name of the country with the highest number of new cases in total. new_cases_by_country = get_new_cases_by_country(df) return new_cases_by_country.iloc[0][\'country\'] def pivot_countries_data(df): Returns a pivot table indexed by date with columns for each country showing the new cases reported on that date. return df.pivot_table(index=\'date\', columns=\'country\', values=\'new_cases\', fill_value=0) def add_weekly_data_column(df): Adds a new column `weekly_new_cases`, which represents the total new cases reported in the last 7 days (including the current day). df = df.copy() df = df.set_index(\'date\') df[\'weekly_new_cases\'] = df.groupby(\'country\')[\'new_cases\'].rolling(window=7, min_periods=1).sum().reset_index(level=0, drop=True) return df.reset_index()"},{"question":"**Question: Implement a Custom File Manager Using `contextlib.ExitStack`** Your task is to implement a function `custom_file_operations()` that manages multiple file resources and ensures all managed files are properly closed, even if an error occurs during file operations. Requirements: 1. Use the `contextlib.ExitStack` class to manage multiple opened files. 2. The function should accept a list of file paths to read from and a single file path to write to. 3. Implement error handling to ensure all opened files are closed properly. 4. If the function encounters any error (e.g., a file cannot be opened), it should stop processing and close all opened files. **Input:** - `read_paths`: List of strings representing file paths to read from. - `write_path`: String representing a file path to write to. **Output:** - The function returns a dictionary containing: - `\'success\'`: A boolean indicating if the operations were successful. - `\'error\'`: The error message if any error occurred, otherwise `None`. **Function Signature:** ```python def custom_file_operations(read_paths: list[str], write_path: str) -> dict: pass ``` **Example Usage:** ```python read_paths = [\\"file1.txt\\", \\"file2.txt\\"] write_path = \\"output.txt\\" result = custom_file_operations(read_paths, write_path) print(result) # {\'success\': True, \'error\': None} ``` **Notes:** - Use `contextlib.ExitStack` to enter multiple `open` contexts and ensure all files are managed correctly. - Implement proper exception handling to manage resources effectively. - The function should maintain clarity and handle edge cases, such as missing files or read/write errors.","solution":"import contextlib def custom_file_operations(read_paths: list[str], write_path: str) -> dict: try: with contextlib.ExitStack() as stack: # Open all the read files read_files = [stack.enter_context(open(path, \'r\')) for path in read_paths] # Open the write file write_file = stack.enter_context(open(write_path, \'w\')) # Perform the file operations for f in read_files: contents = f.read() write_file.write(contents) return {\'success\': True, \'error\': None} except Exception as e: return {\'success\': False, \'error\': str(e)}"},{"question":"# Audio Processing with SunAU You are tasked with creating a function that reads an AU audio file, processes the audio data by amplifying it, and writes the processed data to a new AU file. The amplification factor should be customizable. Function Signature ```python def amplify_au(input_filename: str, output_filename: str, factor: float) -> None: Amplifies the audio data in the input AU file by the specified factor and writes the result to the output AU file. Args: input_filename (str): The name of the input AU file. output_filename (str): The name of the output AU file. factor (float): The factor by which to amplify the audio signal. Returns: None ``` Requirements: 1. **Reading the Input File**: Use `sunau` to open and read the input AU file. 2. **Processing the Audio Data**: Amplify the audio data samples by the given `factor`. Ensure that the samples do not exceed the allowable amplitude range. 3. **Writing the Output File**: Create a new AU file with the processed data, ensuring to correctly set the header information. Constraints: - You may assume that the input AU file uses 16-bit linear PCM encoding. - The `factor` will be a positive float but there is no upper bound defined for it. Handle potential overflow by capping the amplified values to the maximum allowable range for 16-bit samples. Example Usage: ```python # Example usage amplify_au(\'input.au\', \'output.au\', 2.0) ``` This would read `input.au`, amplify the audio signal by a factor of 2, and write the result to `output.au`. Hints: - Use the `sunau` module\'s `AU_read` and `AU_write` objects for reading and writing AU files. - Remember to handle the big-endian format when processing the audio data. - Be mindful of the sample width when modifying the audio data. Best of luck!","solution":"import sunau import struct def amplify_au(input_filename: str, output_filename: str, factor: float) -> None: with sunau.open(input_filename, \'rb\') as in_file: params = in_file.getparams() audio_data = in_file.readframes(params.nframes) sample_width = params.sampwidth num_channels = params.nchannels frame_rate = params.framerate format_str = f\'>{params.nframes * num_channels}h\' if sample_width == 2 else \'\' samples = struct.unpack(format_str, audio_data) max_amplitude = 2**(sample_width * 8 - 1) - 1 min_amplitude = -2**(sample_width * 8 - 1) amplified_samples = [ int(max(min(max_amplitude, sample * factor), min_amplitude)) for sample in samples ] amplified_data = struct.pack(format_str, *amplified_samples) with sunau.open(output_filename, \'wb\') as out_file: out_file.setparams(params) out_file.writeframes(amplified_data)"},{"question":"# Configuration File Parser Objective: Write a function `parse_setup_cfg` that takes the path to a `setup.cfg` file as input and returns a dictionary representation of its contents. This function should handle comments, blank lines, and multi-line options correctly. Input: - `file_path`: A string representing the path to the `setup.cfg` file. Output: - A dictionary where keys are command names, and values are dictionaries of option-value pairs for those commands. Constraints: 1. **File Structure:** The file will follow the structure defined in the provided documentation. 2. **Command and Options:** Commands and options will be properly spelled, but their values may vary in complexity (including multi-line values). Example: Given a `setup.cfg` file with the following content: ``` # Example setup.cfg [build_ext] inplace=1 include_dirs = /usr/local/include [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt doc/ # This is a comment ``` The function call `parse_setup_cfg(\'setup.cfg\')` should return: ```python { \'build_ext\': { \'inplace\': \'1\', \'include_dirs\': \'/usr/local/include\' }, \'bdist_rpm\': { \'release\': \'1\', \'packager\': \'Greg Ward <gward@python.net>\', \'doc_files\': \'CHANGES.txt README.txt doc/\' } } ``` Function Signature ```python def parse_setup_cfg(file_path: str) -> dict: pass ``` Requirements: 1. **File Handling:** Read the file using Python standard file handling methods. 2. **Parsing:** Correctly parse the commands and options, considering comments and multi-line values. 3. **Dictionary Construction:** Return a nested dictionary structure as described. Good luck with your implementation!","solution":"def parse_setup_cfg(file_path: str) -> dict: Parses a setup.cfg file and returns a dictionary of its contents. Args: - file_path (str): Path to the setup.cfg file Returns: - dict: A dictionary representation of the setup.cfg contents data = {} current_section = None with open(file_path, \'r\') as file: lines = file.readlines() for line in lines: line = line.strip() # Ignore comments and blank lines if not line or line.startswith(\'#\'): continue # Identify new section if line.startswith(\'[\') and line.endswith(\']\'): current_section = line[1:-1].strip() data[current_section] = {} elif \'=\' in line and current_section: # Handle multi-line options key, value = line.split(\'=\', 1) key = key.strip() value = value.strip() if key in data[current_section]: data[current_section][key] += \' \' + value else: data[current_section][key] = value elif current_section: # Continuation of multi-line value if data[current_section]: last_key = list(data[current_section].keys())[-1] data[current_section][last_key] += \' \' + line return data"},{"question":"**Problem Statement: Multi-threaded Task Scheduler** You are tasked with implementing a simplified multi-threaded task scheduler that executes a list of tasks in separate threads. Each task is represented by a function that takes a single argument and prints the result. The scheduler needs to manage these threads using locks to ensure that no two tasks print at the same time, and it should handle interruptions gracefully. **Function Specification**: ```python import _thread def multi_threaded_scheduler(tasks: list): Executes a list of tasks in separate threads. Uses locks to synchronize printing and handles interruptions gracefully. Parameters: tasks (list): A list of tuples where each tuple contains a function and its argument, e.g., [(func1, arg1), (func2, arg2), ...]. Returns: None # TODO: Implement this function ``` **Input**: - `tasks`: A list of tuples where each tuple contains a function and its argument. Each function prints a string, and the argument is the respective string to print. **Output**: - The function doesn\'t return anything. **Constraints**: - Ensure that only one thread is printing at any given time. - Use `_thread` for creating threads and managing locks. - Handle any interruptions gracefully and ensure all threads can complete their task. **Example**: ```python def print_message(message): print(message) tasks = [(print_message, \\"Hello\\"), (print_message, \\"World\\"), (print_message, \\"from\\"), (print_message, \\"threads\\")] multi_threaded_scheduler(tasks) ``` **Expected Behavior**: - The messages \\"Hello\\", \\"World\\", \\"from\\", \\"threads\\" should be printed, each by a separate thread, but the prints should not intermingle. **Additional Challenge**: - Implement a mechanism to interrupt the main thread and demonstrate that it handles the interruption by continuing to allow remaining threads to complete their execution. **Note**: Do not use the high-level `threading` module, only use the `_thread` module as described in the documentation.","solution":"import _thread from time import sleep import threading def multi_threaded_scheduler(tasks): Executes a list of tasks in separate threads. Uses locks to synchronize printing and handles interruptions gracefully. Parameters: tasks (list): A list of tuples where each tuple contains a function and its argument, e.g., [(func1, arg1), (func2, arg2), ...]. Returns: None lock = _thread.allocate_lock() def task_wrapper(func, arg): try: with lock: func(arg) except Exception as e: print(f\\"Error executing task: {e}\\") threads = [] for func, arg in tasks: t = threading.Thread(target=task_wrapper, args=(func, arg)) threads.append(t) t.start() for t in threads: t.join() def print_message(message): print(message)"},{"question":"XML Parsing and Handling with `xml.parsers.expat` # Objective: Implement a Python function to parse a given XML string. The function should extract specific elements and attributes, handle namespaces, and manage potential parsing errors. # Task: Write a function `parse_xml(xml_string: str) -> dict` that takes an XML string as input and returns a dictionary containing: - The root element\'s tag and its attributes. - A list of tuples for each child element, where each tuple contains the child element\'s tag, its attributes, and its text content. - The number of child elements parsed. # Requirements: 1. Use the `xml.parsers.expat` module to parse the XML string. 2. Set appropriate handlers to extract the required information. 3. Handle namespaces appropriately. 4. Use the provided error-handling mechanisms to manage parsing errors. # Input: - `xml_string` (str): A well-formed XML string. The XML document can have namespaces and attributes. # Output: - `result` (dict): A dictionary containing: - `\'root_tag\'`: The root element\'s tag. - `\'root_attributes\'`: A dictionary of the root element\'s attributes. - `\'children\'`: A list of tuples, each containing a child\'s tag, attributes, and text content. - `\'child_count\'`: The total number of child elements parsed. # Example: ```python def parse_xml(xml_string: str) -> dict: # Implementation goes here xml_string = <?xml version=\\"1.0\\"?> <root xmlns=\\"http://default-namespace.org/\\" xmlns:py=\\"http://www.python.org/ns/\\"> <py:child1 name=\\"paul\\">Text goes here</py:child1> <child2 name=\\"fred\\">More text</child2> </root> result = parse_xml(xml_string) print(result) # Expected output: # { # \'root_tag\': \'http://default-namespace.org/ root\', # \'root_attributes\': {}, # \'children\': [ # (\'http://www.python.org/ns/ child1\', {\'name\': \'paul\'}, \'Text goes here\'), # (\'http://default-namespace.org/ child2\', {\'name\': \'fred\'}, \'More text\') # ], # \'child_count\': 2 # } ``` # Constraints: - Each XML document will have at least one root element. - The XML string will be well-formed. - Namespace processing should be enabled with a space character `\' \'` as the separator. # Hints: - Use the `xml.parsers.expat.ParserCreate` function to create an `xmlparser` object. - Set appropriate handlers to manage different parts of the XML document. - Use exception handling to catch and manage `ExpatError`.","solution":"import xml.parsers.expat def parse_xml(xml_string: str) -> dict: result = { \'root_tag\': \'\', \'root_attributes\': {}, \'children\': [], \'child_count\': 0 } current_tag = None current_attrs = None current_text = None root_processed = False def start_element(name, attrs): nonlocal current_tag, current_attrs, current_text, root_processed if not root_processed: result[\'root_tag\'] = name result[\'root_attributes\'] = dict(attrs) root_processed = True else: current_tag = name current_attrs = dict(attrs) current_text = \'\' def end_element(name): nonlocal current_tag, current_attrs, current_text if current_tag: result[\'children\'].append((current_tag, current_attrs, current_text)) result[\'child_count\'] += 1 current_tag = None current_attrs = None current_text = None def char_data(data): nonlocal current_text if current_text is not None: current_text += data try: parser = xml.parsers.expat.ParserCreate(namespace_separator=\' \') parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_string, True) except xml.parsers.expat.ExpatError as e: raise ValueError(f\\"Error parsing XML: {e}\\") return result"},{"question":"**Objective:** Implement a function `process_audit_events(audit_event, arguments)` that processes various audit events and performs specific tasks based on the audit event name and its arguments. **Function Signature:** ```python def process_audit_events(audit_event: str, arguments: dict) -> str: pass ``` **Input:** - `audit_event` (str): The name of the audit event (e.g., `array.__new__`, `builtins.input`, `compile`, `os.exec`). - `arguments` (dict): A dictionary containing the arguments associated with the audit event. **Output:** - Returns a string that indicates the result of processing the audit event. **Tasks:** 1. If the `audit_event` is `\\"array.__new__\\"`, return a summary string in the format: `\\"Created array of typecode {typecode} with initializer {initializer}\\"`. - Example: `process_audit_events(\\"array.__new__\\", {\\"typecode\\": \\"i\\", \\"initializer\\": [1, 2, 3]})` should return `\\"Created array of typecode i with initializer [1, 2, 3]\\"`. 2. If the `audit_event` is `\\"builtins.input/result\\"`, return the result string in the format: `\\"User input received: {result}\\"`. - Example: `process_audit_events(\\"builtins.input/result\\", {\\"result\\": \\"Hello\\"})` should return `\\"User input received: Hello\\"`. 3. If the `audit_event` is `\\"compile\\"`, return a summary string in the format: `\\"Compiling source: {source} from file: {filename}\\"`. - Example: `process_audit_events(\\"compile\\", {\\"source\\": \\"print(\'Hello\')\\", \\"filename\\": \\"example.py\\"})` should return `\\"Compiling source: print(\'Hello\') from file: example.py\\"`. 4. If the `audit_event` is `\\"os.exec\\"`, return a summary string in the format: `\\"Executing path: {path} with arguments: {args}\\"`. - Example: `process_audit_events(\\"os.exec\\", {\\"path\\": \\"/usr/bin/python\\", \\"args\\": [\\"-m\\", \\"http.server\\"]})` should return `\\"Executing path: /usr/bin/python with arguments: [\'-m\', \'http.server\']\\"`. 5. For any other `audit_event`, return `\\"Unknown audit event\\"`. **Constraints:** - The arguments provided in the `arguments` dictionary will always match the expected keys for the given `audit_event`. - Assume input types for arguments are always valid and well-formed. **Example Usage:** ```python print(process_audit_events(\\"array.__new__\\", {\\"typecode\\": \\"i\\", \\"initializer\\": [1, 2, 3]})) # Output: Created array of typecode i with initializer [1, 2, 3] print(process_audit_events(\\"builtins.input/result\\", {\\"result\\": \\"Hello\\"})) # Output: User input received: Hello print(process_audit_events(\\"compile\\", {\\"source\\": \\"print(\'Hello\')\\", \\"filename\\": \\"example.py\\"})) # Output: Compiling source: print(\'Hello\') from file: example.py print(process_audit_events(\\"os.exec\\", {\\"path\\": \\"/usr/bin/python\\", \\"args\\": [\\"-m\\", \\"http.server\\"]})) # Output: Executing path: /usr/bin/python with arguments: [\'-m\', \'http.server\'] print(process_audit_events(\\"unknown.event\\", {})) # Output: Unknown audit event ``` **Performance Requirements:** - The function should execute efficiently with the provided constraints and expected inputs.","solution":"def process_audit_events(audit_event: str, arguments: dict) -> str: if audit_event == \\"array.__new__\\": typecode = arguments[\'typecode\'] initializer = arguments[\'initializer\'] return f\\"Created array of typecode {typecode} with initializer {initializer}\\" elif audit_event == \\"builtins.input/result\\": result = arguments[\'result\'] return f\\"User input received: {result}\\" elif audit_event == \\"compile\\": source = arguments[\'source\'] filename = arguments[\'filename\'] return f\\"Compiling source: {source} from file: {filename}\\" elif audit_event == \\"os.exec\\": path = arguments[\'path\'] args = arguments[\'args\'] return f\\"Executing path: {path} with arguments: {args}\\" else: return \\"Unknown audit event\\""},{"question":"**Coding Assessment Question** **Objective:** Demonstrate your ability to use the Python builtins module to extend and manipulate built-in functionality. **Problem:** You are tasked with creating a custom file processor that reads from a file, processes the content, and then writes the processed content back to a new file. Your custom file processor should wrap the built-in open() function to convert the content of the file to lowercase and remove all whitespace (spaces, tabs, and newlines). **Requirements:** 1. Implement a function `custom_open(path: str, mode: str, *, encoding: Optional[str] = \'utf-8\') -> \'LowercaseNoWhitespaceFile\'` that wraps the built-in `open()` function. 2. Implement a class `LowercaseNoWhitespaceFile` that: - Initializes with a file object. - Provides a `read` method that reads the content of the file, converts it to lowercase, and removes all whitespace characters. - Provides a `write` method that writes processed content to a file. **Input:** - `path` (str): The path to the file you want to open. - `mode` (str): The mode in which to open the file (read or write). - `encoding` (Optional[str]): The encoding format to be used (default is \'utf-8\'). **Output:** - An instance of `LowercaseNoWhitespaceFile`. **Constraints:** - You must handle both reading and writing modes. - You must use the builtins module to access the built-in open() function. - You should handle encoding correctly. **Example Usage:** ```python # Demonstrate reading from a file and processing content with custom_open(\'input.txt\', \'r\') as f: content = f.read() print(content) # Demonstrate writing processed content to a new file with custom_open(\'output.txt\', \'w\') as f: f.write(content) ``` Where the content of `input.txt` is: ``` Hello World! Python is FUN. Lots of SPACES and tAbS. ``` The processed content written to `output.txt` should be: ``` helloworld!pythonisfun.lotsofspacesandtabs. ``` **Note:** You do not need to handle exceptions related to file operations for this question. Assume all file operations will succeed.","solution":"import builtins class LowercaseNoWhitespaceFile: def __init__(self, file_obj): self.file_obj = file_obj def read(self): content = self.file_obj.read() return \'\'.join(content.split()).lower() def write(self, data): processed_data = \'\'.join(data.split()).lower() self.file_obj.write(processed_data) def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.file_obj.close() def custom_open(path: str, mode: str, *, encoding: str = \'utf-8\') -> \'LowercaseNoWhitespaceFile\': file_obj = builtins.open(path, mode, encoding=encoding) return LowercaseNoWhitespaceFile(file_obj)"},{"question":"# PyTorch DataLoader Implementation and Usage **Objective:** To assess the understanding of handling data efficiently within the PyTorch framework by implementing custom datasets and utilizing the PyTorch `DataLoader` for batching. **Problem Statement:** You are required to implement a custom dataset and use the `torch.utils.data.DataLoader` to load data in batches. The dataset consists of pairs of input features and labels. An input feature is a 2D tensor of shape `(3, 3)` filled with random values, and a corresponding label is a scalar value generated by summing all the elements in the input tensor. # Tasks: 1. **Implement a Custom Dataset Class:** - Create a class `RandomDataset` that inherits from `torch.utils.data.Dataset`. - The class should have an `__init__` method that initializes the dataset size. - Implement the `__len__` method to return the size of the dataset. - Implement the `__getitem__` method to return a tuple `(input_tensor, label)`, where `input_tensor` is a random 2D tensor of shape `(3, 3)`, and `label` is the sum of the elements of `input_tensor`. 2. **Create DataLoader:** - Instantiate the `RandomDataset` with a size of `1000`. - Use `torch.utils.data.DataLoader` to create a data loader with the following specifications: - Batch size: 32 - Shuffling: True - Number of workers: 2 (for parallel data loading) 3. **Example Usage:** - Demonstrate how to iterate through the data loader and print the shape of one batch of input tensors and labels. - Print the first batch of input tensors and corresponding labels. # Guidelines: - Use the `torch` module for tensor operations and random value generation. - Ensure your implementation is efficient and leverages the parallel data loading capabilities of PyTorch\'s DataLoader. # Expected output: - The printed shape of one batch of input tensors should be `(32, 3, 3)`. - The printed shape of one batch of labels should be `(32,)`. ```python # Required imports import torch from torch.utils.data import Dataset, DataLoader # Task 1: Implement a Custom Dataset Class class RandomDataset(Dataset): def __init__(self, size): self.size = size def __len__(self): return self.size def __getitem__(self, idx): input_tensor = torch.rand(3, 3) label = torch.sum(input_tensor).item() return input_tensor, label # Task 2: Create DataLoader dataset = RandomDataset(1000) data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2) # Task 3: Example Usage for batch in data_loader: input_tensors, labels = batch print(\\"Input tensor batch shape:\\", input_tensors.shape) print(\\"Labels batch shape:\\", labels.shape) # Print first batch of input tensors and labels print(\\"First batch input tensors:\\", input_tensors) print(\\"First batch labels:\\", labels) break ``` # Constraints: - The randomness should be handled using PyTorch\'s random functions. - The number of data samples and batches should accurately reflect the initialization parameters (e.g., dataset size of `1000` and batch size of `32`).","solution":"import torch from torch.utils.data import Dataset, DataLoader # Task 1: Implement a Custom Dataset Class class RandomDataset(Dataset): def __init__(self, size): self.size = size def __len__(self): return self.size def __getitem__(self, idx): input_tensor = torch.rand(3, 3) label = torch.sum(input_tensor).item() return input_tensor, label # Task 2: Create DataLoader dataset = RandomDataset(1000) data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2) # Task 3: Example Usage for batch in data_loader: input_tensors, labels = batch print(\\"Input tensor batch shape:\\", input_tensors.shape) print(\\"Labels batch shape:\\", labels.shape) # Print first batch of input tensors and labels print(\\"First batch input tensors:\\", input_tensors) print(\\"First batch labels:\\", labels) break"},{"question":"# Asyncio Exception Handling Task Objective: Write a Python function to handle multiple asyncio tasks, ensuring robust exception handling for common asyncio exceptions. Task: Implement an asynchronous function `fetch_multiple_urls(urls: List[str]) -> List[Tuple[str, Optional[str], Optional[Exception]]]` that takes a list of URLs, fetches the content of these URLs concurrently, and returns a list of tuples. Each tuple should contain the URL, the fetched content (or `None` if an exception occurred), and the exception (or `None` if no exception occurred). You must handle the following exceptions: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` Specifications: 1. Use the `aiohttp` library for making HTTP requests. Ensure you install it using `pip install aiohttp` if it is not already installed. 2. Your function should timeout for individual URL requests after 5 seconds. 3. If any exception from the specified list occurs, catch it and store the exception instance in the result tuple. 4. If URL fetching is successful, store the content of the URL in the result tuple. 5. Function signature: ```python from typing import List, Tuple, Optional import aiohttp import asyncio async def fetch_multiple_urls(urls: List[str]) -> List[Tuple[str, Optional[str], Optional[Exception]]]: # Your code here ``` Example Usage: ```python import asyncio urls = [ \\"http://example.com\\", \\"https://httpbin.org/delay/3\\", # This will complete successfully \\"https://httpbin.org/delay/10\\", # This will timeout ] results = asyncio.run(fetch_multiple_urls(urls)) for url, content, exception in results: if exception: print(f\\"Failed to fetch {url}: {exception}\\") else: print(f\\"Successfully fetched {url} with content length {len(content)}\\") ``` In this example, the first URL should complete without issue, the second URL will succeed after a short delay, and the third URL is expected to timeout. Notes: - Ensure that the function is robust and handles all specified exceptions gracefully. - Proper use of `asyncio` and `aiohttp` is crucial for efficiently fetching URLs concurrently. Good luck!","solution":"from typing import List, Tuple, Optional import aiohttp import asyncio async def fetch_url(session, url): try: async with session.get(url, timeout=5) as response: content = await response.text() return url, content, None except (aiohttp.ClientError, asyncio.TimeoutError, asyncio.CancelledError, asyncio.InvalidStateError, asyncio.IncompleteReadError, asyncio.LimitOverrunError) as e: return url, None, e async def fetch_multiple_urls(urls: List[str]) -> List[Tuple[str, Optional[str], Optional[Exception]]]: async with aiohttp.ClientSession() as session: tasks = [fetch_url(session, url) for url in urls] return await asyncio.gather(*tasks)"},{"question":"You are required to write a function `merge_dictionaries` that takes two dictionary objects as input and merges them into a single dictionary. If there is a conflict in keys (i.e., both dictionaries contain the same key), the corresponding values should be combined in a specified way based on their types. The function should follow these rules: 1. If the values corresponding to a conflicting key are both integers or both floating-point numbers, their values should be added. 2. If the values are both lists, their elements should be concatenated. 3. If the values are both strings, they should be concatenated directly. 4. If one value is a list and the other value is not, the non-list value should be appended to the list. 5. If the values are of any other types, the value from the second dictionary should overwrite the value from the first dictionary. # Function Signature ```python def merge_dictionaries(dict1: dict, dict2: dict) -> dict: pass ``` # Input - `dict1`: A dictionary with keys as strings and values of various types (integer, float, string, list, etc.). - `dict2`: Another dictionary with keys as strings and values of various types (integer, float, string, list, etc.). # Output - A single dictionary that represents the merged result of `dict1` and `dict2` based on the rules described above. # Constraints - `dict1` and `dict2` will contain no more than 1000 key-value pairs each. - Keys in `dict1` and `dict2` will be unique within each dictionary. - Values in the dictionaries can be integers, floats, strings, lists, or other types. # Example ```python dict1 = { \\"a\\": [1, 2], \\"b\\": 3, \\"c\\": \\"hello\\", \\"d\\": 4.5 } dict2 = { \\"a\\": [3, 4], \\"b\\": 4, \\"c\\": \\" world\\", \\"d\\": 1.5, \\"e\\": {\\"key\\": \\"value\\"} } expected_output = { \\"a\\": [1, 2, 3, 4], \\"b\\": 7, \\"c\\": \\"hello world\\", \\"d\\": 6.0, \\"e\\": {\\"key\\": \\"value\\"} } assert merge_dictionaries(dict1, dict2) == expected_output ``` # Note Make sure to handle `None` as a possible value within the dictionaries gracefully, by treating it as an empty value in case of conflicts.","solution":"def merge_dictionaries(dict1: dict, dict2: dict) -> dict: merged_dict = dict1.copy() for key, value in dict2.items(): if key in merged_dict: if isinstance(merged_dict[key], (int, float)) and isinstance(value, (int, float)): merged_dict[key] += value elif isinstance(merged_dict[key], list) and isinstance(value, list): merged_dict[key].extend(value) elif isinstance(merged_dict[key], str) and isinstance(value, str): merged_dict[key] += value elif isinstance(merged_dict[key], list) and not isinstance(value, list): merged_dict[key].append(value) elif not isinstance(merged_dict[key], list) and isinstance(value, list): if merged_dict[key] is not None: value.insert(0, merged_dict[key]) merged_dict[key] = value else: merged_dict[key] = value else: merged_dict[key] = value return merged_dict"},{"question":"# Advanced Python Coding Assessment: Custom Descriptor for Attribute Validation Objective: You need to implement a custom descriptor that handles attribute validation in a class. This descriptor should ensure that the attribute adheres to specific restrictions, such as type constraints and value ranges. You will also use this descriptor in a practical scenario to validate attributes in a class representing a simplified product database. Task: 1. Implement a `Validator` descriptor class that can validate integer attributes for classes. This class should: - Ensure the attribute is of type `int`. - Ensure the attribute falls within a specified inclusive range (min_value to max_value). 2. Use the `Validator` descriptor in a `Product` class to validate properties like `price` and `quantity`. Requirements: 1. `Validator` Class: - The class should be initialized with `min_value` and `max_value` to define the allowed range. - The `__set__()` method should check the value type (must be an integer) and range constraints. If validation fails, raise an appropriate exception (`TypeError` for type mismatch and `ValueError` for range violations). - The `__get__()` method should return the value stored in the instance. 2. `Product` Class: - Use the `Validator` descriptor for attributes `price` and `quantity`. - Ensure `Product` initializes properly and raises validation exceptions when invalid values are provided. Input and Output: - **Input:** Class definitions and instance creation. - **Output:** No direct print outputs. Validation should raise exceptions in case of invalid values. Example Code: ```python class Validator: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, obj, objtype=None): return obj.__dict__.get(self._name) def __set__(self, obj, value): if not isinstance(value, int): raise TypeError(f\\"{self._name} must be an integer\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"{self._name} must be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"{self._name} must be at most {self.max_value}\\") obj.__dict__[self._name] = value class Product: price = Validator(min_value=0) quantity = Validator(min_value=0) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity # Example usage: try: product = Product(name=\\"Sample Product\\", price=150, quantity=3) product.price = -10 # This should raise a ValueError except ValueError as e: print(e) ``` Constraints: - You must use the `Validator` descriptor to manage attribute validation. - The `Product` class should ensure the `price` and `quantity` are validated upon initialization and during attribute updates. Note: Ensure your code handles all scenarios correctly, including initialization, valid updates, and invalid updates that should raise exceptions.","solution":"class Validator: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, obj, objtype=None): return obj.__dict__.get(self._name) def __set__(self, obj, value): if not isinstance(value, int): raise TypeError(f\\"{self._name} must be an integer\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"{self._name} must be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"{self._name} must be at most {self.max_value}\\") obj.__dict__[self._name] = value class Product: price = Validator(min_value=0) quantity = Validator(min_value=0) def __init__(self, name, price, quantity): self.name = name self.price = price self.quantity = quantity"},{"question":"<|Analysis Begin|> The provided documentation explains how to use seaborn, particularly focusing on the `sns.pointplot` function. It covers various functionalities, such as: 1. Grouping by a categorical variable and plotting aggregated values with confidence intervals. 2. Adding a second layer of grouping and differentiating with colors. 3. Using markers and linestyles to redundantly code the hue variable. 4. Representing error bars using the standard deviation of each distribution. 5. Customizing the appearance of the plot. 6. Dodging artists along the categorical axis to reduce overplotting. 7. Aggregating data when variables are not explicitly assigned. 8. Formatting categorical variable tick labels. 9. Preserving the native scale of the grouping variable. Given the robustness of the seaborn documentation, a well-rounded coding assessment question can be designed to evaluate students\' understanding of both basic and advanced concepts in seaborn, such as using point plots with multiple groupings and customizations. <|Analysis End|> <|Question Begin|> # Seaborn Coding Assessment: Grouping and Customization Objective: Demonstrate your comprehension of seaborn\'s `pointplot` function by creating a visualization that groups data, applies multiple customization layers, and handles error bars appropriately. Dataset: Use the `penguins` dataset provided by the seaborn library. Task: 1. Load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. Create a seaborn point plot (`sns.pointplot`) that: - Groups data by the `island` and `species` columns. - Displays the mean `flipper_length_mm` for each group. - Differentiates genders (`sex` column) using color. - Uses markers and linestyles to redundantly code the `sex` variable. - Represents error bars with the standard deviation (`errorbar=\\"sd\\"`). 3. Customize the plot by: - Setting a point size of 8. - Using different markers, e.g., \'o\' for female and \'s\' for male. - Using line styles \'-\', \'--\'. - Setting a custom title, x-label, and y-label. - Changing the legend title to \\"Gender\\". Input and Output: The function should not accept any parameters. It should directly output a seaborn point plot. Constraints: - You must adhere to seaborn\'s documentation and usage standards. - Ensure your code is efficient and readable. Performance Requirements: - The plot should render within 5 seconds on a standard machine. ```python import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_flipper_length(): # Your code here pass # Run the function to generate the plot plot_penguin_flipper_length() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_flipper_length(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the point plot sns.pointplot( data=penguins, x=\'island\', y=\'flipper_length_mm\', hue=\'sex\', markers=[\'o\', \'s\'], linestyles=[\'-\', \'--\'], dodge=True, errorbar=\'sd\', ci=None, palette=\'deep\', join=True, scale=1.5 ) # Customize the plot plt.title(\'Mean Flipper Length by Island and Species\') plt.xlabel(\'Island\') plt.ylabel(\'Flipper Length (mm)\') plt.legend(title=\'Gender\') plt.show() # Run the function to generate the plot plot_penguin_flipper_length()"},{"question":"Question You are provided with the seaborn `objects` interface documentation and two datasets: `dowjones` and `fmri`. Your task is to analyze the `fmri` dataset and create a line plot that visualizes the signal over time for different regions and events. The plot should meet the following criteria: 1. **Filter the Data**: Select only the entries where `event` is either `stim` or `cue`. 2. **Group the Data**: Group the data by `region` and `event`. 3. **Plot Requirements**: - Create a line plot with `timepoint` on the x-axis and `signal` on the y-axis. - Color the lines based on the `region`. - Use different linestyles for different `events`. - Add markers to the lines at the sampled `timepoint` values. - Include an error band to indicate variability around the lines. # Inputs - The `fmri` dataset, which is preloaded and contains information about timepoints, regions, events, and signals. # Output - Display an interactive seaborn plot that fulfills all the above criteria. # Constraints - Use seaborn\'s `objects` interface. - Ensure the code is efficient and readable. # Example Output A correctly implemented solution will generate a line plot similar to the one described, showcasing different regions and events over time with distinct colors, linestyles, markers, and error bands. ```python import seaborn.objects as so import seaborn as sns # Load fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the data filtered_fmri = fmri.query(\\"event in [\'stim\', \'cue\']\\") # Create the plot p = so.Plot(filtered_fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") # Adding line, aggregate, error band, and markers plot = ( p.add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") .add(so.Line(marker=\'o\', edgecolor=\'w\'), so.Agg(), linestyle=None) ) # Show the plot plot.show() ```","solution":"import seaborn.objects as so import seaborn as sns def create_seaborn_fmri_plot(): # Load fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the data filtered_fmri = fmri.query(\\"event in [\'stim\', \'cue\']\\") # Create the plot p = so.Plot(filtered_fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") # Adding line, aggregate, error band, and markers plot = ( p.add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") .add(so.Line(marker=\'o\', edgecolor=\'w\'), so.Agg(), linestyle=None) ) # Return the plot for display return plot"},{"question":"# Question: Advanced Filtering and Transformation with Functional Programming Modules You are tasked with processing a list of numerical values using advanced functional programming techniques. Specifically, you need to perform a sequence of operations to filter, transform, and aggregate the data. Implement the function `process_numbers(numbers: List[int]) -> int` based on the following requirements: 1. **Filter**: Remove all numbers that are not prime. 2. **Transform**: For the remaining numbers, multiply each by a factor of 10. 3. **Aggregate**: Calculate the sum of all transformed numbers. # Input Format - `numbers`: A list of integers `[n1, n2, ..., nk]` where each number is within the range of [2, 1000]. # Output Format - The function should return an integer which is the sum of the transformed prime numbers. # Constraints - You may assume that the input list contains at least one prime number. - Implement a helper function `is_prime(n: int) -> bool` to check for primes. - Utilize the `filter`, `map`, and `reduce` functions from the appropriate modules to solve the problem. # Example ```python from typing import List def is_prime(n: int) -> bool: # Implement the prime checking logic. pass def process_numbers(numbers: List[int]) -> int: # Implement the number processing logic here. pass # Example usage: numbers = [10, 15, 3, 11, 7, 22] result = process_numbers(numbers) print(result) # Output should be 210 (because 3, 11, and 7 are primes â†’ 30, 110, 70 and their sum is 210) ``` **Notes:** - In the provided solution, ensure that `is_prime` correctly identifies prime numbers. - Use `functools.reduce`, `itertools`, and `operator` as necessary to complete the task.","solution":"from typing import List from functools import reduce import operator def is_prime(n: int) -> bool: if n < 2: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def process_numbers(numbers: List[int]) -> int: primes = filter(is_prime, numbers) transformed = map(lambda x: x * 10, primes) result = reduce(operator.add, transformed, 0) return result"},{"question":"# Asyncio Task Queue Implementation **Objective**: Demonstrate your understanding of the `asyncio` package by implementing a function that manages a queue of tasks to be executed asynchronously. **Task Description**: Write an asynchronous function `process_task_queue(task_list: List[Tuple[int, int]]) -> List[Tuple[int, int]]` that processes a list of tasks concurrently using `asyncio`. Each task will be represented as a tuple `(duration, task_id)`, where `duration` is the time in seconds the task takes to execute, and `task_id` is an integer identifier for the task. The function should return a list of tuples representing the `task_id` and the time at which the task was completed. 1. The function should create an `asyncio.Queue` and populate it with tasks derived from the `task_list`. 2. Create worker coroutines that fetch tasks from the queue and simulate task processing by sleeping for the task\'s duration. 3. Once all tasks are processed, the function should return the list of completed tasks in the format `(task_id, completion_time)`, where `completion_time` is the current time when the task is completed. **Input**: - `task_list` (List[Tuple[int, int]]): A list of tuples where each tuple represents the duration and task_id of a task. E.g., `[(2, 1), (3, 2), (1, 3)]` **Output**: - List[Tuple[int, int]]: A list of tuples representing task_id and the relative time (in seconds) at which the task finished processing. E.g., `[(1, 2), (2, 5), (3, 6)]` **Constraints**: - The function should handle at least 3 workers concurrently. - The order of task completion in the output list should be based on their completion times. - The processing time for each task should match the duration specified in `task_list`. **Example**: ```python import asyncio from typing import List, Tuple async def process_task_queue(task_list: List[Tuple[int, int]]) -> List[Tuple[int, int]]: # Your implementation goes here pass # Example usage task_list = [(2, 1), (3, 2), (1, 3)] result = asyncio.run(process_task_queue(task_list)) print(result) # Possible output: [(3, 1), (1, 2), (2, 3)] ``` **Notes**: - Make sure to properly manage the asyncio event loop to handle the tasks concurrently. - Use relevant asyncio high-level APIs like queues, coroutines, and tasks to implement the solution. - Handle edge cases where the task_list might be empty.","solution":"import asyncio from typing import List, Tuple async def worker(queue: asyncio.Queue, results: List[Tuple[int, int]]): while True: task = await queue.get() if task is None: break duration, task_id = task await asyncio.sleep(duration) results.append((task_id, int(asyncio.get_event_loop().time()))) queue.task_done() async def process_task_queue(task_list: List[Tuple[int, int]]) -> List[Tuple[int, int]]: queue = asyncio.Queue() results = [] for task in task_list: await queue.put(task) workers = [asyncio.create_task(worker(queue, results)) for _ in range(3)] await queue.join() for _ in range(3): await queue.put(None) await asyncio.gather(*workers) results.sort(key=lambda x: x[1]) return results"},{"question":"You have been given a DataFrame `sales_data` that records the details of various product sales in different regions and timeframes. The DataFrame contains the following columns: - `date`: The date of the sale (in datetime format). - `region`: The region where the sale occurred. - `product`: The name of the product sold. - `units_sold`: The number of units sold. - `revenue`: The revenue generated from the sale. Here is a sample of the DataFrame: ```python import pandas as pd import numpy as np sales_data = pd.DataFrame({ \'date\': pd.date_range(start=\'2021-01-01\', periods=100, freq=\'D\').repeat(3), \'region\': [\'North\', \'South\', \'West\'] * 100, \'product\': [\'Product_A\', \'Product_B\', \'Product_C\'] * 100, \'units_sold\': np.random.randint(1, 100, 300), \'revenue\': np.random.uniform(100, 1000, 300) }) ``` **Tasks:** 1. **Aggregate Sales Data:** - Group the DataFrame by `region` and `product` and calculate the total units sold, total revenue, and average units sold per day for each group. - Return a new DataFrame with the aggregated results and reset the index. 2. **Transformation:** - For each region, calculate the cumulative revenue over time. - Add a new column `cumulative_revenue` to the original DataFrame indicating the cumulative revenue for each region. 3. **Filtration:** - Filter out products in each region that have total units sold less than 500. - Return a new filtered DataFrame. 4. **Apply Custom Function:** - Define a custom function to calculate revenue per unit sold. - Apply this function to each group of `region` and `product` and add a new column `revenue_per_unit` to the original DataFrame. 5. **Handle NA Values:** - Modify the DataFrame to introduce some NA values in the `units_sold` column randomly. - Group the DataFrame by `region` and `product`, and fill the NA values in `units_sold` with the mean units sold for each group. **Constraints:** - Your solution should be efficient and utilize Pandas\' built-in GroupBy methods wherever possible. - The resulting DataFrames should retain proper indexing and column naming conventions. **Expected Output:** 1. Aggregated DataFrame with columns `region`, `product`, `total_units_sold`, `total_revenue`, `average_units_sold_per_day`. 2. Original DataFrame with an added `cumulative_revenue` column. 3. Filtered DataFrame based on the total units sold criteria. 4. Original DataFrame with an added `revenue_per_unit` column. 5. Modified DataFrame with NA values filled based on group means. *Write Python code to implement the above tasks.*","solution":"import pandas as pd import numpy as np Aggregated Sales Data def aggregate_sales_data(sales_data): aggregated_data = sales_data.groupby([\'region\', \'product\']).agg( total_units_sold=pd.NamedAgg(column=\'units_sold\', aggfunc=\'sum\'), total_revenue=pd.NamedAgg(column=\'revenue\', aggfunc=\'sum\'), average_units_sold_per_day=pd.NamedAgg(column=\'units_sold\', aggfunc=\'mean\') ).reset_index() return aggregated_data Transformation: Cumulative Revenue def calculate_cumulative_revenue(sales_data): sales_data[\'cumulative_revenue\'] = sales_data.sort_values( \'date\').groupby(\'region\')[\'revenue\'].cumsum() return sales_data Filtration: Filter products with total units sold < 500 def filter_products(sales_data): total_units_sold_per_product = sales_data.groupby([\'region\', \'product\'])[\'units_sold\'].sum().reset_index() filtered_products = total_units_sold_per_product[total_units_sold_per_product[\'units_sold\'] >= 500] filtered_sales_data = sales_data.merge(filtered_products, on=[\'region\', \'product\'], how=\'inner\', suffixes=(\'\', \'_total\')) return filtered_sales_data.drop(columns=[\'units_sold_total\']) Apply Custom Function: Revenue per Unit Sold def apply_revenue_per_unit(sales_data): def revenue_per_unit(group): group[\'revenue_per_unit\'] = group[\'revenue\'] / group[\'units_sold\'] return group sales_data = sales_data.groupby([\'region\', \'product\']).apply(revenue_per_unit) return sales_data Handle NA Values in \'units_sold\' def handle_na_values(sales_data): sales_data.loc[sales_data.sample(frac=0.1).index, \'units_sold\'] = np.nan sales_data[\'units_sold\'] = sales_data.groupby([\'region\', \'product\'])[\'units_sold\'].transform( lambda x: x.fillna(x.mean()) ) return sales_data"},{"question":"**Objective**: Demonstrate understanding of permutation feature importance with scikit-learn to evaluate feature significance in a regression model. **Problem Statement**: You are given a dataset `X` and target variable `y`. Your task is to implement a function `compute_permutation_importance` that trains a regression model on a subset of the data, computes permutation feature importance using specified metrics, and identifies the most important features. **Function Signature**: ```python def compute_permutation_importance(X, y, n_repeats: int = 30, metrics: list = [\'r2\']) -> dict: pass ``` **Input**: - `X` (ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. - `y` (ndarray): A 1D numpy array of shape (n_samples,) representing the target variable. - `n_repeats` (int): Number of times to shuffle each feature (default = 30). - `metrics` (list): A list of scoring metrics to use for computing permutation importance (default = [\'r2\']). **Output**: - A dictionary where keys are the metrics and values are lists of tuples. Each tuple contains a feature name and its corresponding importance score and standard deviation, sorted by the importance score in descending order. **Constraints**: - Use a built-in regression model from scikit-learn (e.g., `Ridge`). - Perform an 80-20 split of `X` and `y` for training and validation. - Ensure reproducibility by setting `random_state=0` where needed. **Example**: ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split # Load dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Call the function importance_scores = compute_permutation_importance(X, y, n_repeats=30, metrics=[\'r2\', \'neg_mean_squared_error\']) print(importance_scores) ``` **Expected Output**: ```python { \'r2\': [(\'s5\', 0.204, 0.050), (\'bmi\', 0.176, 0.048), (\'bp\', 0.088, 0.033), (\'sex\', 0.056, 0.023)], \'neg_mean_squared_error\': [(\'s5\', 1013.866, 246.445), (\'bmi\', 872.726, 240.298), (\'bp\', 438.663, 163.022), (\'sex\', 277.376, 115.123)] } ``` **Notes**: - Use `permutation_importance` from `sklearn.inspection`. - Metric values should be rounded to three decimal places for readability. - Only include features where the importance score minus twice its standard deviation is greater than zero in the output.","solution":"import numpy as np from sklearn.linear_model import Ridge from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance from sklearn.metrics import r2_score, mean_squared_error from sklearn.datasets import load_diabetes def compute_permutation_importance(X, y, n_repeats: int = 30, metrics: list = [\'r2\']): Computes permutation feature importance for a regression model using specified metrics. Args: X (ndarray): 2D numpy array representing the dataset. y (ndarray): 1D numpy array representing the target variable. n_repeats (int): Number of times to shuffle each feature. Default is 30. metrics (list): List of scoring metrics to use. Default is [\'r2\']. Returns: dict: A dictionary where keys are metrics and values are lists of tuples. Each tuple contains a feature name and its corresponding importance score and std deviation, sorted by the importance score in descending order. X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0) model = Ridge(random_state=0) model.fit(X_train, y_train) scoring_functions = { \'r2\': r2_score, \'neg_mean_squared_error\': mean_squared_error } results = {} for metric in metrics: if metric not in scoring_functions: raise ValueError(f\\"Metric {metric} is not supported.\\") scorer = scoring_functions[metric] baseline_score = scorer(y_val, model.predict(X_val)) perm_import = permutation_importance(model, X_val, y_val, n_repeats=n_repeats, scoring=metric if metric != \'neg_mean_squared_error\' else \'neg_mean_squared_error\', random_state=0) importances = [] for i in range(X.shape[1]): importance_mean = perm_import.importances_mean[i] importance_std = perm_import.importances_std[i] if importance_mean - 2 * importance_std > 0: importances.append((f\'feature_{i}\', round(importance_mean, 3), round(importance_std, 3))) results[metric] = sorted(importances, key=lambda x: x[1], reverse=True) return results # Example, usage with the given data diabetes = load_diabetes() X, y = diabetes.data, diabetes.target importance_scores = compute_permutation_importance(X, y, n_repeats=30, metrics=[\'r2\', \'neg_mean_squared_error\']) print(importance_scores)"},{"question":"You have been provided with a sample configuration file (INI format) that stores settings for a fictional application. It contains sections such as `[database]`, `[server]`, and `[user]`. Your task is to implement a function that reads this configuration file and returns the settings as a dictionary. If any required setting is missing, your function should return a default value for that setting without throwing an exception. # Configuration File Example ```ini [database] name = app_db host = localhost port = 5432 [server] host = 127.0.0.1 port = 8000 [user] name = admin ``` # Function Specification Function Name `read_config` Parameters - `file_path` (str): Path to the configuration file. Returns - dict: A dictionary containing the configuration settings. Default Values - For `database`: - `name`: `\\"default_db\\"` - `host`: `\\"default_host\\"` - `port`: `3306` - For `server`: - `host`: `\\"0.0.0.0\\"` - `port`: `80` - For `user`: - `name`: `\\"guest\\"` # Constraints - The implementation should handle any missing section or setting gracefully by providing the respective default values. - You should use the `configparser` module for parsing the configuration file. # Example Usage ```python def read_config(file_path): import configparser config = configparser.ConfigParser() default_config = { \'database\': {\'name\': \'default_db\', \'host\': \'default_host\', \'port\': 3306}, \'server\': {\'host\': \'0.0.0.0\', \'port\': 80}, \'user\': {\'name\': \'guest\'} } # Read the configuration file config.read(file_path) result = { \'database\': { \'name\': config.get(\'database\', \'name\', fallback=default_config[\'database\'][\'name\']), \'host\': config.get(\'database\', \'host\', fallback=default_config[\'database\'][\'host\']), \'port\': config.getint(\'database\', \'port\', fallback=default_config[\'database\'][\'port\']), }, \'server\': { \'host\': config.get(\'server\', \'host\', fallback=default_config[\'server\'][\'host\']), \'port\': config.getint(\'server\', \'port\', fallback=default_config[\'server\'][\'port\']), }, \'user\': { \'name\': config.get(\'user\', \'name\', fallback=default_config[\'user\'][\'name\']), } } return result # Example execution file_path = \'path/to/config.ini\' print(read_config(file_path)) ``` This function should output the configuration settings in a dictionary format, using default values where necessary.","solution":"import configparser def read_config(file_path): config = configparser.ConfigParser() default_config = { \'database\': {\'name\': \'default_db\', \'host\': \'default_host\', \'port\': 3306}, \'server\': {\'host\': \'0.0.0.0\', \'port\': 80}, \'user\': {\'name\': \'guest\'} } # Read the configuration file config.read(file_path) result = { \'database\': { \'name\': config.get(\'database\', \'name\', fallback=default_config[\'database\'][\'name\']), \'host\': config.get(\'database\', \'host\', fallback=default_config[\'database\'][\'host\']), \'port\': config.getint(\'database\', \'port\', fallback=default_config[\'database\'][\'port\']), }, \'server\': { \'host\': config.get(\'server\', \'host\', fallback=default_config[\'server\'][\'host\']), \'port\': config.getint(\'server\', \'port\', fallback=default_config[\'server\'][\'port\']), }, \'user\': { \'name\': config.get(\'user\', \'name\', fallback=default_config[\'user\'][\'name\']), } } return result"},{"question":"Using the `fileinput` module, write a Python function that processes a collection of log files. Specifically, you need to identify and count the number of occurrences of a specific keyword in each file, outputting the filename and the count of occurrences of the keyword, and then update the log files to replace each instance of the keyword with another word. Function Signature ```python def process_log_files(file_list: list, keyword: str, replacement: str) -> dict: pass ``` Input: - `file_list` : A list of file paths (strings) to be processed. - `keyword` : The keyword (string) whose occurrences need to be counted and replaced. - `replacement` : The word (string) that will replace the keyword in the log files. Output: - Returns a dictionary where each key is a filename and the corresponding value is the count of occurrences of the keyword in that file before replacement. Constraints: - The function should handle large files efficiently. - Ensure that the function handles various file encodings (assume utf-8 for simplicity, but consider handling errors gracefully). - The function should create backup files with a \'.bak\' extension before making any replacements. - It is guaranteed that the keyword and replacement words will not contain newline characters. Example Usage: ```python def process(line): print(line) # Given log files: \'log1.txt\', \'log2.txt\' # Content of \'log1.txt\' before processing: # Error found in module A # Error found in module B # Content of \'log2.txt\' before processing: # Warning in module A # Error found in module C file_list = [\\"log1.txt\\", \\"log2.txt\\"] keyword = \\"Error\\" replacement = \\"Issue\\" # Expected Result: # process_log_files(file_list, keyword, replacement) # { # \\"log1.txt\\": 2, # \\"log2.txt\\": 1 # } # Content of \'log1.txt\' after processing: # Issue found in module A # Issue found in module B # Content of \'log2.txt\' after processing: # Warning in module A # Issue found in module C ``` **Implementation Strategy:** 1. Open and iterate through each file in the file list using `fileinput.input()`. 2. Count the occurrences of the keyword in each file. 3. Use the in-place update feature of `fileinput` to replace keywords while maintaining backups. 4. Handle encoding and errors appropriately. 5. Return a dictionary with filenames and their corresponding keyword counts.","solution":"import fileinput import os from collections import defaultdict def process_log_files(file_list: list, keyword: str, replacement: str) -> dict: Processes a list of log files to count occurrences of a given keyword and replace them with a new word. Parameters: file_list (list): List of file paths to be processed keyword (string): The keyword to be counted and replaced replacement (string): The word to replace the keyword with Returns: dict: Dictionary with filenames as keys and the count of keyword occurrences as values keyword_count = defaultdict(int) for file_name in file_list: with fileinput.FileInput(file_name, inplace=True, backup=\'.bak\', encoding=\'utf8\', mode=\'r\') as file: for line in file: occurrences = line.count(keyword) keyword_count[file_name] += occurrences new_line = line.replace(keyword, replacement) print(new_line, end=\'\') return dict(keyword_count)"},{"question":"# Advanced Python Coding Assessment Problem Description You are required to implement a function that mimics a portion of Pythonâ€™s C API functionality, specifically focusing on parsing arguments and building values in a Pythonic manner. Given a single string that contains pairs of data types and values in the form of \\"type:value\\" (e.g., \\"int:5,float:3.14,str:hello\\"), your task is to parse this string and construct a list of Python values with appropriate types. Function Signature ```python def parse_and_build_values(input_string: str) -> list: ``` Input - input_string (str): A string containing pairs of data types and values separated by \',\'. Output - list: A list of values converted to their respective Python types. Constraints - The data types can be `int`, `float`, and `str`. - Values will be simple and not contain any delimiter characters. - You should handle exceptions where the conversion might fail. Example ```python input_string = \\"int:5,float:3.14,str:hello\\" assert parse_and_build_values(input_string) == [5, 3.14, \\"hello\\"] input_string = \\"int:42,float:2.718,str:world\\" assert parse_and_build_values(input_string) == [42, 2.718, \\"world\\"] ``` Notes - Each pair in the input string is separated by a comma. - The type and value in each pair are separated by a colon. Implement the `parse_and_build_values` function to meet the above requirements.","solution":"def parse_and_build_values(input_string: str) -> list: Parses a string containing type:value pairs and returns a list of values converted to their respective Python types. result = [] pairs = input_string.split(\',\') for pair in pairs: type_, value = pair.split(\':\') if type_ == \'int\': result.append(int(value)) elif type_ == \'float\': result.append(float(value)) elif type_ == \'str\': result.append(value) else: raise ValueError(f\\"Unsupported type: {type_}\\") return result"},{"question":"**Coding Assessment Question: Array Manipulation and Data Transformation** **Objective**: Demonstrate your proficiency in using the Python `array` module by implementing functions that perform different array manipulations and data transformations. **Task**: Write a Python class named `ArrayManipulator` that provides the following methods: 1. **`__init__(self, type_code: str)`**: Initialize an empty array of the given `type_code`. 2. **`append_values(self, values: list)`**: Append a list of values to the array. 3. **`get_occurrences(self, value)`**: Return the number of occurrences of a given value in the array. 4. **`to_bytes(self)`**: Convert the array to its bytes representation. 5. **`reverse_array(self)`**: Reverse the order of elements in the array. 6. **`pop_element(self, index: int = -1)`**: Remove and return the element at the given index (default is the last element). **Constraints**: - You can assume that the values provided for appending or popping are of the correct type for the array (ensure type safety). - The `type_code` given to the `__init__` method will always be valid and supported by the `array` module. - Operations must perform efficiently, given large arrays. **Example Usage**: ```python from array import array from ArrayManipulator import ArrayManipulator # Initialize an array of signed integers arr_manip = ArrayManipulator(\'i\') # Append values arr_manip.append_values([1, 2, 3, 4, 5]) # Get occurrences of the value 3 count = arr_manip.get_occurrences(3) print(count) # Output: 1 # Convert array to bytes bytes_rep = arr_manip.to_bytes() print(bytes_rep) # Reverse the array arr_manip.reverse_array() # Pop the last element element = arr_manip.pop_element() print(element) # Output: 1 ``` **Function Definitions**: - **`__init__(self, type_code: str)`**: Initializes the object with an array of the specified type code. - **`append_values(self, values: list)`**: Takes a list of values and appends them to the array. - **`get_occurrences(self, value)`**: Returns the number of times the value appears in the array. - **`to_bytes(self)`**: Returns the byte representation of the array elements. - **`reverse_array(self)`**: Reverses the elements in the array. - **`pop_element(self, index: int = -1)`**: Removes and returns the element at the specified index (default is the last element). **Notes**: - You must manage type safety and check that the values appended to the array match the array\'s type code. - Consider edge cases such as appending an empty list, popping from an empty array, and reversing an array of one element. Implement the `ArrayManipulator` class based on the specified requirements, ensuring the methods are named correctly and behave as described.","solution":"from array import array class ArrayManipulator: def __init__(self, type_code: str): self.arr = array(type_code) def append_values(self, values: list): self.arr.extend(values) def get_occurrences(self, value): return self.arr.count(value) def to_bytes(self): return self.arr.tobytes() def reverse_array(self): self.arr.reverse() def pop_element(self, index: int = -1): if index != -1 and (index < -len(self.arr) or index >= len(self.arr)): raise IndexError(\\"pop index out of range\\") return self.arr.pop(index)"},{"question":"<|Analysis Begin|> The provided documentation describes the `faulthandler` module in Python, which is used to generate Python tracebacks in the event of a fault, timeout, or user signal. The module is designed to help diagnose issues in programs by providing minimalistic tracebacks. It includes functions such as `enable()`, `disable()`, and `dump_traceback_later()` to control the fault handling system and generate traceback dumps. Key points to note: - The fault handler can be enabled at runtime or startup. - It handles critical system signals and can operate under catastrophic conditions. - The tracebacks are minimal and limited in size and scope. - Traceback information can be output to a specified file or stream such as `sys.stderr`. - Features like `dump_traceback_later()` provide additional functionality like triggering tracebacks after a specified timeout. - The module involves concepts of signal handling and file descriptors. Considering the documentation provided, a good coding assessment question should: - Test the student\'s understanding of the `faulthandler` module. - Require implementation involving enabling/disabling the handler and generating tracebacks. - Challenge students to use advanced features like timeouts and handling specific signals. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are tasked with creating a utility to monitor and diagnose a Python program using the `faulthandler` module. Your task is to implement a Python function that makes use of the features provided by this module to handle and generate tracebacks under specified conditions. Function Signature ```python def monitor_program(timeout: int, log_file: str) -> None: pass ``` Requirements 1. Your function should: - Enable the `faulthandler` module to handle critical system faults. - Dump the traceback of all threads to a specified log file after a given timeout. - Ensure that the fault handler is properly disabled at the end of the monitoring. 2. The function `monitor_program` takes: - `timeout`: an integer specifying the number of seconds to wait before dumping the traceback. - `log_file`: a string specifying the path to the log file where the traceback should be written. 3. The function should: - Open the specified log file in write mode and pass the file object to the `faulthandler` functions. - Use the `faulthandler.enable()` function to enable the fault handler. - Use the `faulthandler.dump_traceback_later()` function to schedule a traceback dump after the specified timeout. - Use the `faulthandler.disable()` function to disable the fault handler once monitoring is complete. 4. Ensure that the file object remains open for the duration of the monitoring. Example ```python try: monitor_program(5, \'traceback.log\') # Simulate some long-running or fault-prone operations here finally: faulthandler.disable() ``` In this example, the function will monitor the program and dump the traceback of all running threads to \'traceback.log\' if the program runs for more than 5 seconds without exiting. Constraints - You cannot use external libraries other than `faulthandler`. - The log file must be properly handled to ensure it remains open while the fault handler is active. - Make sure to handle any potential exceptions that might occur during the file operations. Notes - Consider using the `with open` construct for better file handling. - Testing might involve artificially introducing delays or faults in the program to validate that the tracebacks are correctly written to the log file. Good luck!","solution":"import faulthandler import time def monitor_program(timeout: int, log_file: str) -> None: Monitors the program and dumps the traceback of all threads to a log file after the specified timeout. Parameters: timeout (int): The number of seconds to wait before dumping the traceback. log_file (str): The path to the log file where the traceback should be written. with open(log_file, \'w\') as f: # Enable the faulthandler to handle critical faults faulthandler.enable(file=f) try: # Schedule a traceback dump after the specified timeout faulthandler.dump_traceback_later(timeout, file=f) # Simulate a long-running process time.sleep(timeout + 1) finally: # Ensure that the fault handler is properly disabled faulthandler.disable()"},{"question":"You are given a dataset containing numeric values which may include NaNs representing missing data. Your task is to write a Python function `clean_and_analyze(data: list) -> dict` that performs the following: 1. Cleans the data by removing NaN values. 2. Computes the mean, median, variance, and standard deviation of the cleaned data. 3. Checks for multimodality and reports the modes. 4. If the dataset has at least two distinct elements, it should compute a simple linear regression between the dataset and a naturally ordered sequence (i.e., `[1, 2, 3, ..., len(cleaned_data)]`). # Function Signature ```python def clean_and_analyze(data: list) -> dict: pass ``` # Input - `data`: a list of numeric values (`int`, `float`, `Decimal`, `Fraction`) which may include `NaN` values. # Output A dictionary with the following keys: - `\'mean\'`: Mean of the cleaned data. - `\'median\'`: Median of the cleaned data. - `\'variance\'`: Variance of the cleaned data. - `\'stdev\'`: Standard deviation of the cleaned data. - `\'modes\'`: List of modes. - `\'regression\'`: A tuple `(slope, intercept)` representing the linear regression parameters. # Constraints - Raise an exception if the cleaned data is empty after removing NaNs. - Use the corresponding functions from the `statistics` module. # Example ```python from decimal import Decimal from fractions import Fraction data = [20.7, float(\'NaN\'), 19.2, 18.3, float(\'NaN\'), 14.4, 20.7, 20.7] result = clean_and_analyze(data) print(result) ``` Expected output (values may slightly vary based on precision): ```python { \'mean\': 18.9, \'median\': 20.7, \'variance\': 7.9225, \'stdev\': 2.8148, \'modes\': [20.7], \'regression\': (0.9286, 14.0643) } ``` This question tests students understanding of data cleaning and statistical analysis, requiring a combination of multiple functions from the `statistics` module.","solution":"import math import statistics from typing import List, Dict, Tuple from scipy import stats def clean_and_analyze(data: List[float]) -> Dict: Cleans the data by removing NaN values and computes mean, median, variance, and standard deviation. Checks for multimodality and reports the modes. If the dataset has at least two distinct elements, computes a simple linear regression. :param data: List of numeric values which may include NaNs representing missing data. :return: Dictionary with the computed statistical values. # Removing NaNs cleaned_data = [x for x in data if not math.isnan(x)] if not cleaned_data: raise ValueError(\\"The cleaned data is empty after removing NaNs.\\") result = {} # Calculate statistical values result[\'mean\'] = statistics.mean(cleaned_data) result[\'median\'] = statistics.median(cleaned_data) result[\'variance\'] = statistics.variance(cleaned_data) result[\'stdev\'] = statistics.stdev(cleaned_data) # Calculate modes modes = statistics.multimode(cleaned_data) result[\'modes\'] = modes # Simple linear regression if len(set(cleaned_data)) > 1: x = list(range(1, len(cleaned_data) + 1)) slope, intercept, _, _, _ = stats.linregress(x, cleaned_data) result[\'regression\'] = (slope, intercept) else: result[\'regression\'] = (None, None) return result"},{"question":"Objective You are required to demonstrate your understanding of Python\'s object protocol, particularly the functions used for attribute manipulation and type checking. Task Implement a class `DynamicObjectHandler` that provides methods for dynamically inspecting and modifying attributes of given Python objects. # Class Methods 1. **has_attribute(obj, attr_name)** - **Input**: `obj` (any Python object), `attr_name` (str, name of the attribute) - **Output**: (bool) `True` if the object has the specified attribute, `False` otherwise. - **Constraints**: Use the `PyObject_HasAttrString` C API function for implementation. 2. **get_attribute(obj, attr_name)** - **Input**: `obj` (any Python object), `attr_name` (str, name of the attribute) - **Output**: The value of the attribute if it exists, otherwise raises an AttributeError. - **Constraints**: Use the `PyObject_GetAttrString` C API function for implementation. 3. **set_attribute(obj, attr_name, value)** - **Input**: `obj` (any Python object), `attr_name` (str, name of the attribute), `value` (any Python object to set as the attribute\'s value) - **Output**: (None) - sets the attribute if possible. - **Constraints**: Use the `PyObject_SetAttrString` C API function for implementation. 4. **delete_attribute(obj, attr_name)** - **Input**: `obj` (any Python object), `attr_name` (str, name of the attribute) - **Output**: (None) - deletes the attribute if it exists. - **Constraints**: Use the `PyObject_DelAttrString` C API function for implementation. 5. **is_instance(obj, cls)** - **Input**: `obj` (any Python object), `cls` (type or tuple of types) - **Output**: (bool) `True` if the object is an instance of the class (or any class in the tuple), `False` otherwise. - **Constraints**: Use the `PyObject_IsInstance` C API function for implementation. 6. **is_subclass(cls, base_cls)** - **Input**: `cls` (type), `base_cls` (type or tuple of types) - **Output**: (bool) `True` if the class is a subclass of the base class (or any class in the tuple), `False` otherwise. - **Constraints**: Use the `PyObject_IsSubclass` C API function for implementation. Requirements - Your solution should primarily demonstrate the correct usage of the specified C API functions within a Python C extension. - Ensure proper error handling and meaningful error messages where applicable. - Provide a small demonstration script that uses the `DynamicObjectHandler` class to: - Create an instance of a basic class. - Check for the presence of an attribute. - Retrieve, set, and delete an attribute dynamically. - Verify instance and subclass relationships. ```python class Example: pass handler = DynamicObjectHandler() obj = Example() # Dynamically set an attribute handler.set_attribute(obj, \'foo\', 42) # Check if the attribute exists print(handler.has_attribute(obj, \'foo\')) # Should print: True # Get the attribute print(handler.get_attribute(obj, \'foo\')) # Should print: 42 # Delete the attribute handler.delete_attribute(obj, \'foo\') print(handler.has_attribute(obj, \'foo\')) # Should print: False # Check instance and subclass relationships print(handler.is_instance(obj, Example)) # Should print: True print(handler.is_subclass(Example, object)) # Should print: True ``` Submit your solution along with the demonstration.","solution":"class DynamicObjectHandler: @staticmethod def has_attribute(obj, attr_name): Checks if the object has the specified attribute. Parameters: obj (any): The object to check. attr_name (str): The name of the attribute. Returns: bool: True if the attribute exists, False otherwise. return hasattr(obj, attr_name) @staticmethod def get_attribute(obj, attr_name): Retrieves the value of the specified attribute from the object. Parameters: obj (any): The object to inspect. attr_name (str): The name of the attribute. Returns: The value of the attribute if it exists, raises AttributeError otherwise. return getattr(obj, attr_name) @staticmethod def set_attribute(obj, attr_name, value): Sets the value of the specified attribute on the object. Parameters: obj (any): The object to modify. attr_name (str): The name of the attribute. value (any): The value to set. Returns: None setattr(obj, attr_name, value) @staticmethod def delete_attribute(obj, attr_name): Deletes the specified attribute from the object. Parameters: obj (any): The object to modify. attr_name (str): The name of the attribute. Returns: None delattr(obj, attr_name) @staticmethod def is_instance(obj, cls): Checks if the object is an instance of the specified class. Parameters: obj (any): The object to check. cls (type or tuple): The class or tuple of classes to check against. Returns: bool: True if obj is an instance of cls, False otherwise. return isinstance(obj, cls) @staticmethod def is_subclass(cls, base_cls): Checks if the cls is a subclass of the base_cls. Parameters: cls (type): The class to check. base_cls (type or tuple): The base class or tuple of classes to check against. Returns: bool: True if cls is a subclass of base_cls, False otherwise. return issubclass(cls, base_cls) # Demonstration: class Example: pass handler = DynamicObjectHandler() obj = Example() # Dynamically set an attribute handler.set_attribute(obj, \'foo\', 42) # Check if the attribute exists print(handler.has_attribute(obj, \'foo\')) # Should print: True # Get the attribute print(handler.get_attribute(obj, \'foo\')) # Should print: 42 # Delete the attribute handler.delete_attribute(obj, \'foo\') print(handler.has_attribute(obj, \'foo\')) # Should print: False # Check instance and subclass relationships print(handler.is_instance(obj, Example)) # Should print: True print(handler.is_subclass(Example, object)) # Should print: True"},{"question":"# Objective Implement a custom pickling mechanism for a complex object using the \\"copyreg\\" module. # Task You are required to create a custom class `D` with the following properties: - The class should have a constructor that accepts two parameters, `b` (an integer) and `c` (a string). - The class should store these parameters as instance attributes. Create custom pickling and unpickling functions for the class `D` using the `copyreg` module. These functions should handle the pickling and unpickling of objects of class `D` correctly. # Function Signature ```python class D: def __init__(self, b: int, c: str): ... def pickle_d(obj: D): ... # Register the custom pickling function with copyreg copyreg.pickle(D, pickle_d) ``` # Input and Output - Write the class `D` with the necessary constructor. - Implement the `pickle_d` function and register it using `copyreg.pickle`. - Demonstrate the correct working of the custom pickling mechanism by: - Creating an instance of class `D`. - Copying and pickling the instance using the `copy.copy` and `pickle.dumps` methods. - Show the expected print statements during the pickling process. # Example Usage ```python import copyreg, copy, pickle class D: def __init__(self, b: int, c: str): self.b = b self.c = c def pickle_d(obj: D): print(\\"pickling a D instance...\\") return D, (obj.b, obj.c) # Register the custom pickling function with copyreg copyreg.pickle(D, pickle_d) # Creating an instance of D d_instance = D(2, \\"example\\") # Copying the instance d_copy = copy.copy(d_instance) # Should print: pickling a D instance... # Pickling the instance d_pickle = pickle.dumps(d_instance) # Should print: pickling a D instance... ``` # Constraints - Ensure that the `pickle_d` function properly returns a tuple as required by the \\"copyreg\\" module.","solution":"import copyreg, copy, pickle class D: def __init__(self, b: int, c: str): self.b = b self.c = c def pickle_d(obj: D): print(\\"pickling a D instance...\\") return D, (obj.b, obj.c) # Register the custom pickling function with copyreg copyreg.pickle(D, pickle_d)"},{"question":"Title: Custom HTTP Client with Header Management Problem Statement You are tasked with implementing a custom HTTP client that can send HTTP requests and manage custom headers. Your client should be able to: 1. Send GET requests to a given URL. 2. Send POST requests with a specified body and headers. 3. Manage custom headers that should be included in every request. 4. Handle HTTP responses, including reading response headers and bodies. Requirements: 1. Implement a class `CustomHttpClient` with the following methods: * `__init__(self, host, port=None, use_https=False)`: Initializes the client for a given host. The `use_https` flag determines whether to use HTTP or HTTPS. * `add_header(self, header, value)`: Adds a custom header that will be included in every request. * `remove_header(self, header)`: Removes a custom header. * `send_get(self, url)`: Sends a GET request to the specified URL and returns a tuple `(status_code, reason, headers, body)`. * `send_post(self, url, body)`: Sends a POST request to the specified URL with the given body and returns a tuple `(status_code, reason, headers, body)`. 2. The `headers` in the response should be a dictionary of header names and values. 3. Ensure your implementation handles exceptions and prints appropriate error messages when requests fail. 4. Demonstrate the usage of your `CustomHttpClient` class with a few examples: * Send a GET request to `http://www.example.com/`. * Send a POST request to `http://www.example.com/submit` with a sample JSON body `{\\"name\\": \\"John Doe\\", \\"email\\": \\"johndoe@example.com\\"}` and appropriate headers. Constraints: - Use only the `http.client` module for HTTP communication. - Do not use any higher-level HTTP client libraries such as `requests`. Example: ```python # Example usage of CustomHttpClient client = CustomHttpClient(\\"www.example.com\\", use_https=False) client.add_header(\\"User-Agent\\", \\"CustomClient/1.0\\") client.add_header(\\"Content-Type\\", \\"application/json\\") # Send a GET request status, reason, headers, body = client.send_get(\\"/\\") print(f\\"GET request - Status: {status}, Reason: {reason}\\") print(\\"Headers:\\", headers) print(\\"Body:\\", body.decode()) # Send a POST request post_body = \'{\\"name\\": \\"John Doe\\", \\"email\\": \\"johndoe@example.com\\"}\' status, reason, headers, body = client.send_post(\\"/submit\\", post_body) print(f\\"POST request - Status: {status}, Reason: {reason}\\") print(\\"Headers:\\", headers) print(\\"Body:\\", body.decode()) # Remove a header client.remove_header(\\"User-Agent\\") ```","solution":"import http.client import json class CustomHttpClient: def __init__(self, host, port=None, use_https=False): self.host = host self.port = port self.use_https = use_https self.custom_headers = {} def add_header(self, header, value): self.custom_headers[header] = value def remove_header(self, header): if header in self.custom_headers: del self.custom_headers[header] def _get_connection(self): if self.use_https: return http.client.HTTPSConnection(self.host, self.port) else: return http.client.HTTPConnection(self.host, self.port) def _make_request(self, method, url, body=None): connection = self._get_connection() headers = self.custom_headers try: connection.request(method, url, body, headers) response = connection.getresponse() status_code = response.status reason = response.reason headers = dict(response.getheaders()) body = response.read() connection.close() return status_code, reason, headers, body except Exception as e: print(f\\"Request failed: {e}\\") return None, None, None, None def send_get(self, url): return self._make_request(\\"GET\\", url) def send_post(self, url, body): return self._make_request(\\"POST\\", url, body)"},{"question":"Objective Implement a function that simulates the behavior of the `recursive-include` and `recursive-exclude` commands used in setuptools\' `sdist` command. The function should recursively traverse a directory and include or exclude files based on Unix-style glob patterns. Function Signature ```python def filter_files(base_dir: str, includes: List[str], excludes: List[str]) -> List[str]: Recursively traverse `base_dir` and return a list of file paths that match the \'includes\' patterns and do not match the \'excludes\' patterns. Args: - base_dir (str): The base directory to start the search. - includes (List[str]): List of Unix-style glob patterns to include. - excludes (List[str]): List of Unix-style glob patterns to exclude. Returns: - List[str]: Sorted list of file paths that match includes and do not match excludes. ``` Input - `base_dir`: A string representing the path to the base directory. - `includes`: A list of strings representing Unix-style glob patterns to include files. - `excludes`: A list of strings representing Unix-style glob patterns to exclude files. Output - A sorted list of strings representing file paths that match the \\"includes\\" patterns and do not match the \\"excludes\\" patterns. Constraints - The function should efficiently handle directories with a large number of files. - Use Python\'s built-in libraries for file system operations and pattern matching. - Focus on clarity and efficiency. Example Assume the following directory structure in `base_dir`: ``` base_dir/ â”œâ”€â”€ file1.txt â”œâ”€â”€ file2.csv â”œâ”€â”€ dir1/ â”‚ â”œâ”€â”€ file3.txt â”‚ â””â”€â”€ file4.csv â””â”€â”€ dir2/ â”œâ”€â”€ file5.log â””â”€â”€ file6.txt ``` Example usage and expected output: ```python base_dir = \\"base_dir\\" includes = [\\"*.txt\\", \\"*.csv\\"] excludes = [\\"file2.csv\\", \\"*file3.txt\\"] result = filter_files(base_dir, includes, excludes) # Expected Output: [\\"base_dir/file1.txt\\", \\"base_dir/dir1/file4.csv\\", \\"base_dir/dir2/file6.txt\\"] ``` Notes - Use the `glob` module and `os` or `pathlib` for directory traversal and pattern matching. - Ensure your code handles edge cases, such as empty directories, no matching patterns, and conflicting patterns. - Include docstrings and comments in your implementation for clarity. Good luck!","solution":"import os import fnmatch def filter_files(base_dir, includes, excludes): Recursively traverse `base_dir` and return a list of file paths that match the \'includes\' patterns and do not match the \'excludes\' patterns. Args: - base_dir (str): The base directory to start the search. - includes (List[str]): List of Unix-style glob patterns to include. - excludes (List[str]): List of Unix-style glob patterns to exclude. Returns: - List[str]: Sorted list of file paths that match includes and do not match excludes. matched_files = [] for root, _, files in os.walk(base_dir): for file in files: filepath = os.path.join(root, file) # Check if the file matches any of the include patterns if any(fnmatch.fnmatch(filepath, os.path.join(base_dir, pattern)) for pattern in includes): # Check if the file matches any of the exclude patterns if not any(fnmatch.fnmatch(filepath, os.path.join(base_dir, pattern)) for pattern in excludes): matched_files.append(filepath) return sorted(matched_files)"},{"question":"# Python Bytecode Analysis and Manipulation In this task, you are required to analyze a given Python function by disassembling its bytecode and performing some modifications based on certain rules. **Task**: 1. Write a function `analyze_bytecode(func)` that takes a Python function `func` as input and returns a formatted string representation of its bytecode. 2. Write a function `count_operations(bytecode_str)` that takes the formatted bytecode string from the first function and returns a dictionary where keys are bytecode operation names (opnames) and values are the count of each operation in the bytecode. 3. Write a function `replace_opname(bytecode_obj, original_op, new_op)` that takes a `Bytecode` object `bytecode_obj`, a string `original_op` and a string `new_op`. This function should return a new `Bytecode` object where every occurrence of `original_op` is replaced with `new_op`. # Function Specifications 1. **analyze_bytecode** - **Input**: - `func`: a Python function to be analyzed. - **Output**: - A string containing the formatted bytecode of the input function. 2. **count_operations** - **Input**: - `bytecode_str`: a string representation of the bytecode from the `analyze_bytecode` function. - **Output**: - A dictionary with bytecode operation names (opnames) as keys and their counts as values. 3. **replace_opname** - **Input**: - `bytecode_obj`: an instance of `dis.Bytecode` representing the bytecode of a function. - `original_op`: a string representing the original opcode to be replaced. - `new_op`: a string representing the new opcode that will replace the original opcode. - **Output**: - A new `Bytecode` object with the required opcode replacements. # Constraints - Ensure you handle cases where the input bytecode does not contain the `original_op`. - The `dis` module can be assumed to be imported. - Assume the `original_op` and `new_op` are valid operation names within the bytecode instructions. # Example Usage ```python import dis def example_func(x, y): return x + y # Task 1: Analyzing bytecode bytecode_string = analyze_bytecode(example_func) print(bytecode_string) # Task 2: Counting operations op_counts = count_operations(bytecode_string) print(op_counts) # Task 3: Replacing opcode bytecode_obj = dis.Bytecode(example_func) new_bytecode_obj = replace_opname(bytecode_obj, \'BINARY_ADD\', \'BINARY_SUBTRACT\') # Display new bytecode print(new_bytecode_obj.dis()) ``` # Notes - The `analyze_bytecode` function should utilize the `dis.Bytecode` class for wrapping and analyzing bytecode. - The `count_operations` function should parse the formatted bytecode string to compute the counts of each opcode. - The `replace_opname` function should modify the instructions of the bytecode object and return a new bytecode object with the updated operations.","solution":"import dis from collections import Counter def analyze_bytecode(func): Takes a Python function and returns a formatted string representation of its bytecode. bytecode = dis.Bytecode(func) return \'n\'.join([f\'{instr.opname} {instr.argrepr}\' for instr in bytecode]) def count_operations(bytecode_str): Takes the formatted bytecode string and returns a dictionary where keys are bytecode operation names and values are the count of each operation in the bytecode. lines = bytecode_str.split(\'n\') ops = [line.split()[0] for line in lines if line] return dict(Counter(ops)) def replace_opname(bytecode_obj, original_op, new_op): Takes a Bytecode object, an original opcode name and a new opcode name and returns a new Bytecode object where every occurrence of original_op is replaced with new_op. new_bytecode = [] for instr in bytecode_obj: if instr.opname == original_op: new_instr = dis.Instruction(instr.opname.replace(original_op, new_op), dis.opmap[new_op], instr.arg, instr.argval, instr.argrepr, instr.offset, instr.starts_line, instr.is_jump_target) else: new_instr = instr new_bytecode.append(new_instr) return new_bytecode"},{"question":"You are tasked with creating a simple web content management feature that safely processes user-generated content. The goal is to properly escape HTML characters when displaying user input and then reverse the process to reveal the original content when loading it back for editing. Function Signature You need to implement the following function: ```python def process_html_content(user_input: str, reverse: bool = False) -> str: Process the given user input string for safe HTML display or revert it back to the original content. Parameters: user_input (str): The input string containing user content. reverse (bool): A flag indicating whether to escape (False) or unescape (True) the input. Returns: str: The processed string after escaping or unescaping HTML characters. ``` Input - `user_input`: A non-empty string containing user content that may include special HTML characters such as `&`, `<`, `>`, `\\"`, and `\'`. - `reverse`: A boolean flag. If `False`, the function should escape the HTML characters in `user_input`. If `True`, the function should unescape any HTML character references in `user_input`. The default value is `False`. Output - The function should return a string with HTML characters escaped or unescaped based on the `reverse` flag. Constraints - The length of `user_input` will be between 1 and 1000 characters. - The function should handle both typical and edge cases efficiently. Example 1. **Escaping HTML characters:** ```python user_input = \\"Hello <world> & friends\\" reverse = False result = process_html_content(user_input, reverse) print(result) # \\"Hello &lt;world&gt; &amp; friends\\" ``` 2. **Unescaping HTML characters:** ```python user_input = \\"Hello &lt;world&gt; &amp; friends\\" reverse = True result = process_html_content(user_input, reverse) print(result) # \\"Hello <world> & friends\\" ``` Notes - Use `html.escape` and `html.unescape` from the `html` module to accomplish the task. - Ensure string manipulation is done efficiently and adheres to Python\'s best practices. ***Good Luck!***","solution":"import html def process_html_content(user_input: str, reverse: bool = False) -> str: Process the given user input string for safe HTML display or revert it back to the original content. Parameters: user_input (str): The input string containing user content. reverse (bool): A flag indicating whether to escape (False) or unescape (True) the input. Returns: str: The processed string after escaping or unescaping HTML characters. if reverse: return html.unescape(user_input) else: return html.escape(user_input)"},{"question":"**Title: Synchronizing Access to a Shared Resource using asyncio Primitives** **Description:** You are given the task to implement a system that simulates a parking lot using asyncio synchronization primitives. The parking lot has a limited number of parking spaces, and multiple vehicles (asynchronous tasks) are trying to park and leave. You need to ensure that no more vehicles park than the available spaces and that vehicle movements are properly coordinated. # Requirements: 1. Implement a `ParkingLot` class to manage parking spaces. 2. The `ParkingLot` class should use an asyncio `Semaphore` to manage the parking spaces. 3. There should be methods for vehicles to park (`park_vehicle`) and leave (`leave_vehicle`) that simulate parking and leaving with delays. 4. Use asyncio tasks to simulate multiple vehicles trying to park and leave concurrently. # Implementation Details: Class: `ParkingLot` - **Initializer** method: - `__init__(self, capacity: int)`: Initializes the parking lot with a given capacity (number of parking spaces). - Initiate an asyncio `Semaphore` with the provided capacity. - **Method**: `async def park_vehicle(self, vehicle_id: int, duration: float)` - Simulates a vehicle trying to park. - If there is an available space (using `await semaphore.acquire()`), it parks for a given duration (simulated by `await asyncio.sleep(duration)`). - Print a message indicating the vehicle has parked. - **Method**: `async def leave_vehicle(self, vehicle_id: int)` - Simulates a vehicle leaving the parking lot. - Releases the parking space (using `semaphore.release()`). - Print a message indicating the vehicle has left. Main Function - Create an instance of `ParkingLot` with a specified capacity. - Create multiple asyncio tasks simulating vehicles trying to park for random durations and then leave. - Ensure the vehicles execute concurrently and manage proper synchronization. # Example: ```python import asyncio import random class ParkingLot: def __init__(self, capacity: int): self.capacity = capacity self.semaphore = asyncio.Semaphore(self.capacity) async def park_vehicle(self, vehicle_id: int, duration: float): await self.semaphore.acquire() print(f\\"Vehicle {vehicle_id} has parked.\\") await asyncio.sleep(duration) print(f\\"Vehicle {vehicle_id} has finished parking.\\") async def leave_vehicle(self, vehicle_id: int): self.semaphore.release() print(f\\"Vehicle {vehicle_id} has left.\\") async def main(): parking_lot = ParkingLot(capacity=3) async def vehicle_task(vehicle_id): park_duration = random.uniform(1, 5) await parking_lot.park_vehicle(vehicle_id, park_duration) await parking_lot.leave_vehicle(vehicle_id) tasks = [vehicle_task(i) for i in range(10)] await asyncio.gather(*tasks) if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Constraints: - Assume the maximum number of vehicles in the parking lot is always greater than or equal to the capacity. - Vehicles try to park and leave concurrently. # Performance Requirements: - Ensure proper synchronization to avoid deadlocks or resource contention. - Efficiently manage parking and leaving operations without unnecessary delays. **Assessment Focus:** This question assesses the student\'s ability to design and implement a system using asyncio synchronization primitives, particularly focusing on proper use of semaphore for resource management in a concurrent environment.","solution":"import asyncio class ParkingLot: def __init__(self, capacity: int): self.capacity = capacity self.semaphore = asyncio.Semaphore(self.capacity) async def park_vehicle(self, vehicle_id: int, duration: float): await self.semaphore.acquire() print(f\\"Vehicle {vehicle_id} has parked.\\") await asyncio.sleep(duration) print(f\\"Vehicle {vehicle_id} has finished parking.\\") await self.leave_vehicle(vehicle_id) async def leave_vehicle(self, vehicle_id: int): self.semaphore.release() print(f\\"Vehicle {vehicle_id} has left.\\") async def main(): parking_lot = ParkingLot(capacity=3) async def vehicle_task(vehicle_id, duration): await parking_lot.park_vehicle(vehicle_id, duration) tasks = [vehicle_task(i, i % 3 + 1) for i in range(10)] await asyncio.gather(*tasks) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# **Coding Assessment Question** You are given a dataset `train.csv` which contains a large number of features and samples. Your task is to implement a classification model using the `SGDClassifier` from scikit-learn to predict whether a sample belongs to class 0 or class 1. The performance of your model will be evaluated on the test set `test.csv`. **Requirements** 1. **Data Preprocessing**: - Read the `train.csv` and `test.csv` files. - Separate features and labels. The last column in the dataset is the label. - Perform feature scaling using `StandardScaler` to standardize the features. 2. **Model Training**: - Use the `SGDClassifier` with the following specifications: - `loss=\'log_loss\'` - `penalty=\'elasticnet\'` with `l1_ratio=0.15`. - `max_iter=1000` - `tol=1e-3` - `shuffle=True` - Implement early stopping with `early_stopping=True` and `validation_fraction=0.1`. 3. **Model Evaluation**: - Predict the labels for the test set. - Compute and print the classification report including precision, recall, and F1-score. **Input Formats** - The `train.csv` and `test.csv` files where: - Each row represents a sample with features and a label. - The last column is the label (0 or 1). **Output Format** - Print the classification report for the test set. **Constraints** - Ensure to handle any missing data appropriately. - Assume the dataset is large, efficiently handle large-scale data. **Function to Implement** ```python def sgd_classifier(train_file: str, test_file: str): import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import classification_report # Step 1: Data Preprocessing # Read the train and test datasets train_data = pd.read_csv(train_file) test_data = pd.read_csv(test_file) # Separate features and labels X_train = train_data.iloc[:, :-1].values y_train = train_data.iloc[:, -1].values X_test = test_data.iloc[:, :-1].values y_test = test_data.iloc[:, -1].values # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 2: Model Training # Initialize the SGDClassifier clf = SGDClassifier(loss=\'log_loss\', penalty=\'elasticnet\', l1_ratio=0.15, max_iter=1000, tol=1e-3, shuffle=True, early_stopping=True, validation_fraction=0.1) # Fit the model to the training data clf.fit(X_train, y_train) # Step 3: Model Evaluation # Predict the labels for the test set y_pred = clf.predict(X_test) # Print the classification report print(classification_report(y_test, y_pred)) # Example usage # sgd_classifier(\'path/to/train.csv\', \'path/to/test.csv\') ``` **Note**: Include any additional libraries or modules you might use.","solution":"def sgd_classifier(train_file: str, test_file: str): import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import classification_report # Step 1: Data Preprocessing # Read the train and test datasets train_data = pd.read_csv(train_file) test_data = pd.read_csv(test_file) # Separate features and labels X_train = train_data.iloc[:, :-1].values y_train = train_data.iloc[:, -1].values X_test = test_data.iloc[:, :-1].values y_test = test_data.iloc[:, -1].values # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 2: Model Training # Initialize the SGDClassifier clf = SGDClassifier(loss=\'log_loss\', penalty=\'elasticnet\', l1_ratio=0.15, max_iter=1000, tol=1e-3, shuffle=True, early_stopping=True, validation_fraction=0.1) # Fit the model to the training data clf.fit(X_train, y_train) # Step 3: Model Evaluation # Predict the labels for the test set y_pred = clf.predict(X_test) # Print the classification report print(classification_report(y_test, y_pred)) # Example usage # sgd_classifier(\'path/to/train.csv\', \'path/to/test.csv\')"},{"question":"# Advanced Python Coding Assessment: Async Coroutine Management Objective You are required to demonstrate your understanding of Python\'s `asyncio` module and coroutine objects by implementing a custom coroutine manager. This manager will handle and execute multiple coroutines concurrently, monitor their execution, and collect results. Instructions 1. Implement a function `coroutine_manager(coroutines: List[Awaitable]) -> List[Any]` that takes a list of awaitable coroutines and returns a list of their results. 2. The `coroutine_manager` should use `asyncio` to manage and execute the coroutines concurrently. 3. Within the coroutine manager: - Print the start and end times of each coroutine execution for monitoring. - Ensure the results are collected in the same order as the input coroutines. Expected Function Signature ```python import asyncio from typing import List, Awaitable, Any async def coroutine_manager(coroutines: List[Awaitable]) -> List[Any]: pass ``` Example Usage Here is an example of how your function will be used: ```python import asyncio async def sample_coroutine(n: int) -> int: await asyncio.sleep(n) return n async def main(): coros = [sample_coroutine(3), sample_coroutine(1), sample_coroutine(2)] results = await coroutine_manager(coros) print(results) # Output should be: [3, 1, 2] asyncio.run(main()) ``` Constraints - Ensure efficient management of coroutines such that the total execution time is minimized by running them concurrently. - Your implementation should handle any number of coroutines provided in the input list. Performance Requirements - Function must handle up to 1000 coroutines without significant degradation in performance. - The order of results must match the order of coroutines provided in the input list. Reference Refer to Python\'s `asyncio` module and concepts of coroutine functions for implementation details.","solution":"import asyncio from typing import List, Awaitable, Any async def coroutine_manager(coroutines: List[Awaitable]) -> List[Any]: Manages and executes multiple coroutines concurrently, and returns their results. async def wrapper(coro, idx): print(f\'Coroutine {idx} starting\') result = await coro print(f\'Coroutine {idx} finished\') return idx, result tasks = [wrapper(coro, idx) for idx, coro in enumerate(coroutines)] results = await asyncio.gather(*tasks) results.sort() return [result for idx, result in results] # Example usage: async def sample_coroutine(n: int) -> int: await asyncio.sleep(n) return n async def main(): coros = [sample_coroutine(3), sample_coroutine(1), sample_coroutine(2)] results = await coroutine_manager(coros) print(results) # Output should be: [3, 1, 2] # Run the example # asyncio.run(main())"},{"question":"**Question:** In this coding assessment, you are required to implement and utilize PyTorch\'s `torch.func` API to demonstrate your understanding of function transforms and utilities for working with `torch.nn.Module`. # Task Description 1. **Model Definition and Parameter Extraction**: - Define a simple neural network model `MyModel` using `torch.nn.Module` with at least two `torch.nn.Linear` layers. - Create an instance of your model and extract its parameters using `named_parameters()`. 2. **Function Definition for Jacobian Calculation**: - Define a function `model_function(params, x)` that uses `torch.func.functional_call` to compute the output of the model with the given parameters and input `x`. 3. **Jacobian Calculation**: - Use `torch.func.jacrev` to compute the Jacobian of `model_function` with respect to the parameters of your model. - Validate the shape of the resulting Jacobian matrix. # Constraints - Input tensor `x` should have a shape of `[batch_size, input_features]`, and you can assume `input_features = 3`. - The model should be defined for input feature size of `3` and output feature size of `2` for each `torch.nn.Linear` layer. - The batch size for the input tensor should be `5`. # Function Implementations 1. **`MyModel(nn.Module)`**: - Input: None - Output: An instance of `MyModel` containing the defined layers. 2. **`model_function(params: dict, x: torch.Tensor) -> torch.Tensor`**: - Input: - `params`: A dictionary containing the parameters of the model. - `x`: A tensor with shape `[batch_size, input_features]`. - Output: The output of the model as a tensor. 3. **`compute_jacobian(model: nn.Module, x: torch.Tensor) -> torch.Tensor`**: - Input: - `model`: An instance of `MyModel`. - `x`: A tensor with shape `[batch_size, input_features]`. - Output: The Jacobian of the model output with respect to its parameters. # Example Usage ```python import torch import torch.nn as nn import torch.func as func # Define your model class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.layer1 = nn.Linear(3, 2) self.layer2 = nn.Linear(2, 2) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x # Define the function for functional call def model_function(params, x): model = MyModel() return func.functional_call(model, params, x) # Compute the Jacobian def compute_jacobian(model, x): params = dict(model.named_parameters()) jac_func = func.jacrev(model_function) jacobian = jac_func(params, x) return jacobian # Test the function model = MyModel() x = torch.randn(5, 3) jacobian = compute_jacobian(model, x) print(jacobian.shape) # Expected output: (number_of_parameters, 5, 2) ``` # Evaluation Criteria - Correctness: Does the implementation correctly compute the Jacobian matrix of the model with respect to its parameters? - Code Quality: Is the code clean, well-documented, and easy to understand? - Efficiency: Does the implementation handle the input size gracefully? # Performance Requirements - Your implementation should be able to handle batches of up to size `32` with input feature size `3` within reasonable time and memory constraints. Good luck!","solution":"import torch import torch.nn as nn import torch.func as func # Define your model class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.layer1 = nn.Linear(3, 2) self.layer2 = nn.Linear(2, 2) def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x # Define the function for functional call def model_function(params, x): model = MyModel() model.load_state_dict(params, strict=False) return model(x) # Compute the Jacobian def compute_jacobian(model, x): params = {name: param for name, param in model.named_parameters()} jac_func = func.jacrev(model_function) jacobian = jac_func(params, x) return jacobian"},{"question":"**Coding Assessment Question: Context Variable Manager in Python** # Objective: Implement a Python class that uses the `contextvars` module to manage context-local state. The class should provide methods to create, set, get, and reset context variables in a way that demonstrates understanding of Pythonâ€™s context management and variable scoping. # Task: You need to design a class named `ContextManager` that includes the following functionalities: 1. **Initialization**: - The class should initialize a context variable with a given name and default value. 2. **Set Value**: - A method to set the value of the context variable for the current context. 3. **Get Value**: - A method to retrieve the value of the context variable. If the value is not set, it should return the default value. 4. **Reset Value**: - A method to reset the context variable to its state before the last set operation. This should return a boolean indicating success or failure. # Input and Output: - Class Initialization: - `def __init__(self, name: str, default_value: Any) -> None` - Initializes a context variable with the given name and default value. - Set Value: - `def set_value(self, value: Any) -> None` - Sets the value of the context variable. - Get Value: - `def get_value(self) -> Any` - Retrieves the current value of the context variable or its default value if not set. - Reset Value: - `def reset_value(self) -> bool` - Resets the variable to its state before the last set operation. Returns `True` if reset succeeds, `False` otherwise. # Constraints: - The implementation should handle errors gracefully and should not crash the program. - Ensure that context-specific values are managed correctly across different contexts/threads. - Assume that the inputs provided to the methods are always valid and no additional input validation is required. # Example: ```python from contextvars import ContextVar, Token class ContextManager: def __init__(self, name: str, default_value: Any) -> None: self.context_var = ContextVar(name, default=default_value) self.token = None def set_value(self, value: Any) -> None: self.token = self.context_var.set(value) def get_value(self) -> Any: return self.context_var.get() def reset_value(self) -> bool: if self.token: self.context_var.reset(self.token) return True return False ``` # Your Task: Implement the `ContextManager` class as described. Ensure that it handles context-specific variables correctly and use the `contextvars` module to manage the context state.","solution":"from contextvars import ContextVar, Token class ContextManager: def __init__(self, name: str, default_value: any) -> None: self.context_var = ContextVar(name, default=default_value) self.token = None def set_value(self, value: any) -> None: self.token = self.context_var.set(value) def get_value(self) -> any: return self.context_var.get() def reset_value(self) -> bool: if self.token is not None: self.context_var.reset(self.token) self.token = None return True return False"},{"question":"# Pandas Coding Challenge **Objective:** Validate students\' understanding of pandas by having them perform a series of data operations on a given dataset. Problem Statement You are provided with a dataset in CSV format containing the sales records of a retail store. The CSV file (`sales_data.csv`) has the following columns: - `Date`: The date of the transaction. - `Store`: The store identifier. - `Product`: The product name. - `Quantity`: The quantity sold. - `Revenue`: The total revenue generated from the sale. Your task is to implement the following functions to analyze the sales data using pandas: 1. **Load the data from the CSV file into a Pandas DataFrame.** 2. **Clean the data by handling missing values**: - Fill missing values in the `Quantity` and `Revenue` columns with `0`. - Drop any rows where `Store` or `Product` data is missing. 3. **Perform the following analyses**: - Compute the total sales (sum of `Revenue`) for each product and each store. - Find the product with the highest sales (`Revenue`) for each store. - Calculate the daily average quantity sold for each product over the entire period. # Function Definitions 1. `load_data(file_path: str) -> pd.DataFrame` - **Input**: A string representing the path to the CSV file. - **Output**: A pandas DataFrame containing the loaded data. 2. `clean_data(df: pd.DataFrame) -> pd.DataFrame` - **Input**: A pandas DataFrame. - **Output**: A cleaned pandas DataFrame with missing values handled as described above. 3. `total_sales_by_product_and_store(df: pd.DataFrame) -> pd.DataFrame` - **Input**: A pandas DataFrame. - **Output**: A pandas DataFrame with the total sales for each product and each store. The DataFrame should have columns `Store`, `Product`, and `Total_Sales`. 4. `top_product_by_store(df: pd.DataFrame) -> pd.DataFrame` - **Input**: A pandas DataFrame. - **Output**: A pandas DataFrame with the top-selling product for each store. The DataFrame should have columns `Store`, `Product`, and `Total_Sales`. 5. `daily_avg_quantity(df: pd.DataFrame) -> pd.DataFrame` - **Input**: A pandas DataFrame. - **Output**: A pandas DataFrame with the daily average quantity sold for each product. The DataFrame should have columns `Product` and `Daily_Avg_Quantity`. # Example Usage ```python # Load and clean the data df = load_data(\'sales_data.csv\') df = clean_data(df) # Compute total sales by product and store total_sales_df = total_sales_by_product_and_store(df) print(total_sales_df) # Find the top-selling product by store top_product_df = top_product_by_store(df) print(top_product_df) # Calculate the daily average quantity for each product daily_avg_quantity_df = daily_avg_quantity(df) print(daily_avg_quantity_df) ``` # Constraints 1. You may assume the `Date` column contains valid date strings in the format `YYYY-MM-DD`. 2. The DataFrame input to the functions will always be non-empty. # Requirements 1. You must use pandas for data manipulation. 2. Your solutions should be efficient and utilize vectorized operations where possible. 3. Handle edge cases, such as missing values, appropriately as described.","solution":"import pandas as pd def load_data(file_path: str) -> pd.DataFrame: Loads data from a CSV file into a pandas DataFrame. Parameters: - file_path: str, path to the CSV file Returns: - pd.DataFrame: DataFrame containing the loaded data return pd.read_csv(file_path) def clean_data(df: pd.DataFrame) -> pd.DataFrame: Cleans the data by handling missing values. Parameters: - df: pd.DataFrame, input DataFrame Returns: - pd.DataFrame: cleaned DataFrame df[\'Quantity\'].fillna(0, inplace=True) df[\'Revenue\'].fillna(0, inplace=True) df.dropna(subset=[\'Store\', \'Product\'], inplace=True) return df def total_sales_by_product_and_store(df: pd.DataFrame) -> pd.DataFrame: Computes the total sales for each product and each store. Parameters: - df: pd.DataFrame, input DataFrame Returns: - pd.DataFrame: DataFrame with total sales for each product and store total_sales = df.groupby([\'Store\', \'Product\'])[\'Revenue\'].sum().reset_index() total_sales.rename(columns={\'Revenue\': \'Total_Sales\'}, inplace=True) return total_sales def top_product_by_store(df: pd.DataFrame) -> pd.DataFrame: Finds the product with the highest sales for each store. Parameters: - df: pd.DataFrame, input DataFrame Returns: - pd.DataFrame: DataFrame with the top product for each store total_sales = total_sales_by_product_and_store(df) idx = total_sales.groupby(\'Store\')[\'Total_Sales\'].idxmax() top_products = total_sales.loc[idx].reset_index(drop=True) return top_products def daily_avg_quantity(df: pd.DataFrame) -> pd.DataFrame: Calculates the daily average quantity sold for each product over the entire period. Parameters: - df: pd.DataFrame, input DataFrame Returns: - pd.DataFrame: DataFrame with daily average quantity for each product df[\'Date\'] = pd.to_datetime(df[\'Date\']) daily_avg = df.groupby(\'Product\')[\'Quantity\'].mean().reset_index() daily_avg.rename(columns={\'Quantity\': \'Daily_Avg_Quantity\'}, inplace=True) return daily_avg"},{"question":"# Custom Estimator Implementation in scikit-learn In this task, you are required to implement a custom scikit-learn compatible estimator. The estimator should be a simple classifier that utilizes the Euclidean distance to predict the label of new data points based on the label of the closest point in the training data. You should strictly follow the scikit-learn development guidelines provided in the documentation. Requirements: 1. **Class Definition**: Your classifier should be named `EuclideanNearestNeighborClassifier`. 2. **Inheritance**: It should inherit from `BaseEstimator` and `ClassifierMixin`. 3. **Initialization**: The `__init__` method should accept a single parameter `demo_param` with a default value of `\'demo\'`. 4. **Fit Method**: Implement the `fit` method to store the training data and their corresponding labels. Also validate that `X` and `y` have the correct shape. 5. **Predict Method**: Implement the `predict` method to predict the label of new data points based on the closest training data point using Euclidean distance. 6. **Attributes**: Ensure that your class sets necessary attributes like `n_features_in_`, `classes_`, etc. 7. **Validation**: Use utility functions from `sklearn.utils` for input validation and fitting checks. Example Usage: ```python import numpy as np from sklearn.utils.estimator_checks import check_estimator # Example training data X_train = np.array([[1, 2], [3, 4], [5, 6]]) y_train = np.array([0, 1, 0]) # Initialize and fit the classifier classifier = EuclideanNearestNeighborClassifier() classifier.fit(X_train, y_train) # Example test data X_test = np.array([[2, 3], [4, 5]]) y_pred = classifier.predict(X_test) print(\\"Predictions:\\", y_pred) # Output should be the predicted labels for X_test # Validate the custom estimator check_estimator(EuclideanNearestNeighborClassifier()) ``` # Implementation: Provide the implementation of the `EuclideanNearestNeighborClassifier` below: ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from sklearn.utils.multiclass import unique_labels from sklearn.metrics import euclidean_distances class EuclideanNearestNeighborClassifier(BaseEstimator, ClassifierMixin): def __init__(self, demo_param=\'demo\'): self.demo_param = demo_param def fit(self, X, y): # Validate input X, y = check_X_y(X, y) # Store the classes seen during fit self.classes_ = unique_labels(y) # Store the training data and labels self.X_ = X self.y_ = y # Set the number of features self.n_features_in_ = X.shape[1] return self def predict(self, X): # Check if fit has been called check_is_fitted(self) # Input validation X = check_array(X) # Predict the closest training point closest = np.argmin(euclidean_distances(X, self.X_), axis=1) return self.y_[closest] ``` Your implementation should correctly follow the scikit-learn guidelines and pass the validation check using `check_estimator`.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from sklearn.utils.multiclass import unique_labels from sklearn.metrics import euclidean_distances class EuclideanNearestNeighborClassifier(BaseEstimator, ClassifierMixin): def __init__(self, demo_param=\'demo\'): self.demo_param = demo_param def fit(self, X, y): # Validate input X, y = check_X_y(X, y) # Store the classes seen during fit self.classes_ = unique_labels(y) # Store the training data and labels self.X_ = X self.y_ = y # Set the number of features self.n_features_in_ = X.shape[1] return self def predict(self, X): # Check if fit has been called check_is_fitted(self) # Input validation X = check_array(X) # Predict the closest training point closest = np.argmin(euclidean_distances(X, self.X_), axis=1) return self.y_[closest]"},{"question":"**Data Cleaning and Analysis with Pandas** You are given a CSV file named `employees.csv` containing the following columns: `EmployeeID`, `Name`, `Department`, `JoiningDate`, `Salary`, and `PerformanceScore`. The file contains data about employees in a company. Your task is to perform data cleaning and analysis using pandas. # Objectives 1. **Data Loading**: - Load the data from the `employees.csv` file into a pandas DataFrame. 2. **Handling Missing Values**: - Identify and count the missing values in each column. - Replace any missing `PerformanceScore` values with the average performance score of the corresponding department. 3. **Data Transformation**: - Extract the year from the `JoiningDate` and create a new column called `JoiningYear`. 4. **Aggregation**: - Group the data by `Department` and calculate the average `Salary` and `AveragePerformanceScore` for each department. 5. **Data Visualization**: - Display the number of employees in each department using a bar plot. # Implementation Write a function `analyze_employees(file_path: str) -> pd.DataFrame` that performs the steps mentioned above. Input - `file_path` (str): The path to the `employees.csv` file. Output - Returns a pandas DataFrame with the following columns: `Department`, `AverageSalary`, `AveragePerformanceScore`, and `NumberOfEmployees`. # Constraints - Assume that the CSV file is not extremely large and fits into memory. - Handle date parsing appropriately. Example of Usage ```python df_summary = analyze_employees(\'path/to/employees.csv\') print(df_summary) ``` # Additional Note Even if you are not provided with sample data, ensure that the function is robust and handles possible edge cases such as empty data, missing columns, etc.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_employees(file_path: str) -> pd.DataFrame: # Data Loading df = pd.read_csv(file_path) # Handling Missing Values missing_counts = df.isnull().sum() for department in df[\'Department\'].unique(): department_mask = df[\'Department\'] == department avg_score = df.loc[department_mask, \'PerformanceScore\'].mean() df.loc[department_mask & df[\'PerformanceScore\'].isnull(), \'PerformanceScore\'] = avg_score # Data Transformation df[\'JoiningDate\'] = pd.to_datetime(df[\'JoiningDate\']) df[\'JoiningYear\'] = df[\'JoiningDate\'].dt.year # Aggregation aggregated_df = df.groupby(\'Department\').agg( AverageSalary=pd.NamedAgg(column=\'Salary\', aggfunc=\'mean\'), AveragePerformanceScore=pd.NamedAgg(column=\'PerformanceScore\', aggfunc=\'mean\'), NumberOfEmployees=pd.NamedAgg(column=\'EmployeeID\', aggfunc=\'count\') ).reset_index() # Data Visualization department_counts = df[\'Department\'].value_counts() department_counts.plot(kind=\'bar\', title=\'Number of Employees in Each Department\') plt.xlabel(\'Department\') plt.ylabel(\'Number of Employees\') plt.show() return aggregated_df"},{"question":"# Advanced Python Coding Assessment: Email Utilities Your task is to create a Python function that utilizes the `email.utils` module to manipulate email headers and dates as described below. Problem Statement You need to implement a function `process_email(header_values, dt_string)` that processes email header field values and a date-time string. The function should: 1. Parse the provided list of email header field values to extract all recipient email addresses using strict parsing. 2. Convert the provided RFC 2822 date-time string into a local aware datetime object. 3. Format this datetime object into an RFC 2822 compliant string with the timezone as GMT. Input - `header_values` (list of str): A list of email header field values containing recipient emails. - `dt_string` (str): An RFC 2822 formatted date-time string. Output - (list of tuples, str): A tuple consisting of: - A list of 2-tuples, where each 2-tuple contains the real name and email address of a recipient. - A formatted string representing the datetime in RFC 2822 format with the timezone as GMT. Constraints - Assume `header_values` contains valid header field values for the To, Cc, Resent-To, or Resent-Cc fields. - The date-time string (`dt_string`) is a valid RFC 2822 formatted string. Function Signature ```python def process_email(header_values: list, dt_string: str) -> tuple: pass ``` Example ```python from email.utils import format_datetime, getaddresses, localtime header_values = [\\"Alice <alice@example.com>\\", \\"Bob <bob@example.org>\\", \\"Carol <carol@example.net>\\"] dt_string = \\"Mon, 20 Nov 1995 19:12:08 -0500\\" result = process_email(header_values, dt_string) print(result) # Expected Output: # ([(\'Alice\', \'alice@example.com\'), (\'Bob\', \'bob@example.org\'), (\'Carol\', \'carol@example.net\')], # \'Mon, 20 Nov 1995 19:12:08 GMT\') ``` Implementation Notes - Use `email.utils.getaddresses` with `strict=True` to parse email addresses. - Use `email.utils.localtime` to convert the datetime string to a local aware datetime object. - Use `email.utils.format_datetime` to format the datetime object as an RFC 2822 string with GMT timezone.","solution":"from email.utils import getaddresses, parsedate_to_datetime, format_datetime def process_email(header_values, dt_string): # Extract all recipient email addresses recipients = getaddresses(header_values) # Convert the provided RFC 2822 date-time string into a local aware datetime object local_dt = parsedate_to_datetime(dt_string) # Format this datetime object into an RFC 2822 compliant string with the timezone as GMT gmt_dt_string = format_datetime(local_dt.astimezone().astimezone(tz=None)) return recipients, gmt_dt_string"},{"question":"Objective Demonstrate your understanding of advanced theme and display customization using the `seaborn.objects.Plot` class in Seaborn. Problem Statement You are required to create a plot using Seaborn that visualizes the given data and configures the plot\'s appearance and display settings as specified. Instructions 1. **Data:** - Use the following sample data for your plot. ```python import pandas as pd data = { \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Value\': [4, 7, 1, 8, 5] } df = pd.DataFrame(data) ``` 2. **Requirements:** - Create a scatter plot using `seaborn.objects.Plot`. - Customize the plot theme with the following settings: - Set the background color of the axes to white. - Use the `whitegrid` style from Seaborn\'s theming functions. - Sync the theme with Matplotlib\'s global state if needed. - Configure the display settings to: - Use SVG format. - Disable HiDPI scaling. - Set the scaling factor to 0.7. - Reset the theme to Seaborn defaults at the end of your script. Constraints - The data provided should be used as-is, without any modifications. - All theme and display configurations must be applied through the `seaborn.objects.Plot.config` interface. - Ensure your code executes without errors and produces the required plot with specified theme and display settings. Expected Output - A scatter plot of the provided data with the specified theme and display customizations. Example Here is an example of what the plot setup might look like, but ensure to include all specified customizations: ```python import seaborn.objects as so from seaborn import axes_style import pandas as pd # Data data = { \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Value\': [4, 7, 1, 8, 5] } df = pd.DataFrame(data) # Plot and configurations so.Plot(df, x=\'Category\', y=\'Value\').add(so.Dot()) so.Plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.7 so.Plot.config.theme.reset() ``` Complete the code to ensure it satisfies all the requirements mentioned above.","solution":"import seaborn.objects as so from seaborn import axes_style import pandas as pd # Data data = { \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Value\': [4, 7, 1, 8, 5] } df = pd.DataFrame(data) # Plot and configurations plot = so.Plot(df, x=\'Category\', y=\'Value\').add(so.Dot()) # Customizing the plot as per requirements plot.config.theme[\\"axes.facecolor\\"] = \\"white\\" plot.config.theme.update(axes_style(\\"whitegrid\\")) plot.config.display[\\"format\\"] = \\"svg\\" plot.config.display[\\"hidpi\\"] = False plot.config.display[\\"scaling\\"] = 0.7 # Display the plot plot.show() # Resetting theme to defaults at the end so.Plot.config.theme.reset()"},{"question":"# Challenge: Implementing an Ordered Cache You are to implement a cache that retains the order of the entries based on their last access time (most recently accessed items are ordered first). This cache should allow fast access, insertion, and deletion of items. If the cache exceeds a specified maximum size, it should automatically evict the least recently accessed item to make space for the new one. You will use the `OrderedDict` class from the `collections` module to implement the cache. Function Specifications 1. **Class Initialization**: - `__init__(self, capacity: int)`: Initializes the cache with a given `capacity`. 2. **Setting a Value in Cache**: - `set(self, key: Any, value: Any) -> None`: Adds a new `(key, value)` pair to the cache. If the key already exists, update its value and adjust its position to be the most recently accessed. If adding this new item exceeds the cache\'s capacity, evict the least recently accessed item. 3. **Getting a Value from Cache**: - `get(self, key: Any) -> Any`: Retrieves the value associated with the key from the cache and marks the item as recently accessed. If the key does not exist, return `None`. 4. **Deleting a Key from Cache**: - `delete(self, key: Any) -> None`: Removes the key from the cache if it exists. Constraints - `capacity` will be a positive integer and will not exceed 1000. - Keys and values are not limited to any specific data type or range. Example ```python from collections import OrderedDict class OrderedCache: def __init__(self, capacity: int): # Your implementation here def set(self, key: Any, value: Any) -> None: # Your implementation here def get(self, key: Any) -> Any: # Your implementation here def delete(self, key: Any) -> None: # Your implementation here # Test your implementation cache = OrderedCache(2) cache.set(\'a\', 1) cache.set(\'b\', 2) print(cache.get(\'a\')) # Output: 1 cache.set(\'c\', 3) print(cache.get(\'b\')) # Output: None, as \'b\' should be evicted cache.delete(\'a\') print(cache.get(\'a\')) # Output: None, as \'a\' is deleted ``` Implement the `OrderedCache` class as per the specifications and test it with various scenarios to ensure its correct functionality.","solution":"from collections import OrderedDict class OrderedCache: def __init__(self, capacity: int): self.capacity = capacity self.cache = OrderedDict() def set(self, key, value): if key in self.cache: self.cache.move_to_end(key) self.cache[key] = value if len(self.cache) > self.capacity: self.cache.popitem(last=False) def get(self, key): if key in self.cache: self.cache.move_to_end(key) return self.cache[key] return None def delete(self, key): if key in self.cache: del self.cache[key] # Example usage: # cache = OrderedCache(2) # cache.set(\'a\', 1) # cache.set(\'b\', 2) # print(cache.get(\'a\')) # Output: 1 # cache.set(\'c\', 3) # print(cache.get(\'b\')) # Output: None, as \'b\' should be evicted # cache.delete(\'a\') # print(cache.get(\'a\')) # Output: None, as \'a\' is deleted"},{"question":"Objective: You need to demonstrate your understanding of the PyTorch Distributed RPC framework, particularly focusing on the RRef protocol. You will write a Python function using PyTorch that simulates a simplified version of the RRef lifecycle. Your implementation should ensure proper creation, usage, and deletion of RRefs while adhering to the guarantees provided by the protocol. Task: 1. **Implement RRef Creation and Remote Execution**: - Create a PyTorch RPC worker setup with three workers: A, B, and C. - Implement a function that creates an `OwnerRRef` on worker B. - Implement another function on worker A that remotely calls a function on worker C, passing the created RRef. 2. **Handle Reference Counting and Notifications**: - Ensure that the owner (worker B) is notified when any `UserRRef` is deleted. - Ensure that a parent `UserRRef` is not deleted until the owner confirms the child `UserRRef`. 3. **Test and Demonstrate the RRef Lifecycle**: - Write a test script that demonstrates the creation of `OwnerRRef` on worker B, usage of `UserRRef` by worker A, and remote execution on worker C. - Ensure that the RRef is properly deleted without premature deletion, following the described guarantees (G1 and G2). Requirements: - Use the PyTorch `torch.distributed.rpc` module. - Properly manage the lifecycle of RRefs, making sure reference counting and notifications are correctly handled. - Your implementation should be robust against transient network failures and out-of-order message delivery. Input and Output: - **Input**: None explicitly, but the setup should involve initializing an RPC framework with three workers. - **Output**: Print statements or logs indicating the creation, usage, and deletion of RRefs, along with any notifications or confirmations in the workflow. Constraints: - Ensure that your solution handles at least three workers: A, B, and C. - Implement proper lifecycle management, ensuring no references are prematurely deleted. - Take transient network failures into account and ensure your solution is resilient. Performance Requirements: This implementation should be efficient and handle distributed communication without significant delays or performance bottlenecks. **Note:** You may use simulated delays or failures to test the resilience of your implementation. ```python import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef def create_rref_on_b(): # Function to create an OwnerRRef on worker B return RRef(torch.ones(2)) def remote_function_with_rref(rref): # Function to be executed remotely on worker C return rref.to_here().sum().item() def main(): # Initialize RPC framework with four workers rpc.init_rpc(\'A\', rank=0, world_size=3) rpc.init_rpc(\'B\', rank=1, world_size=3) rpc.init_rpc(\'C\', rank=2, world_size=3) if rpc.get_worker_info().name == \'A\': # On worker A, create a remote call to worker B to get an RRef, then pass it to worker C rref = rpc.rpc_sync(\'B\', create_rref_on_b) result = rpc.rpc_sync(\'C\', remote_function_with_rref, args=(rref,)) print(f\\"Result: {result}\\") # Shut down the RPC framework rpc.shutdown() if __name__ == \'__main__\': main() ``` **Instructions**: Implement the `create_rref_on_b` and `remote_function_with_rref` functions to handle the creation, appropriate reference counting, and notification mechanics of RRefs following the protocol described. Use logging or print statements to indicate the workflow progress and reference counting status.","solution":"import logging import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef, rpc_async, remote logging.basicConfig(level=logging.INFO) def initialize_workers(): # Initialize worker names worker_names = [\\"A\\", \\"B\\", \\"C\\"] rpc.init_rpc(rpc.get_worker_info().name, rank=worker_names.index(rpc.get_worker_info().name), world_size=len(worker_names)) def create_rref_on_b(): # Function to create an OwnerRRef on worker B logging.info(\\"Worker B creating RRef.\\") return RRef(torch.ones(2)) def remote_function_with_rref(rref): # Function to be executed remotely on worker C logging.info(\\"Worker C received RRef.\\") tensor = rref.to_here() return tensor.sum().item() def main(): worker_names = [\\"A\\", \\"B\\", \\"C\\"] initialize_workers() if rpc.get_worker_info().name == \'A\': # On worker A, create a remote call to worker B to get an RRef, then pass it to worker C rref = rpc.rpc_sync(\'B\', create_rref_on_b) result = rpc.rpc_sync(\'C\', remote_function_with_rref, args=(rref,)) logging.info(f\\"Result: {result}\\") # Block until all rpcs finish rpc.shutdown() if __name__ == \'__main__\': main()"},{"question":"You are tasked with developing a network analysis tool that can process a list of subnets and perform various network calculations. Specifically, you need to: 1. Validate the subnets. 2. Identify if a given address falls within any of the given subnets. 3. Summarize the given subnets to find the smallest set of supernets that cover the same range of addresses. 4. Determine the total number of unique IP addresses covered by the provided subnets. # Function Specifications Implement the following functions: 1. **validate_subnets(subnets: List[str]) -> List[str]:** - **Input:** A list of subnet strings (e.g., `[\\"192.168.0.0/24\\", \\"10.0.0.0/8\\"]`). - **Output:** A list of valid subnet strings. Ignore invalid subnets. 2. **is_ip_in_subnets(ip: str, subnets: List[str]) -> bool:** - **Input:** An IP address string (e.g., `\\"192.168.1.1\\"`) and a list of subnet strings. - **Output:** Return `True` if the IP address is found in any of the subnets, otherwise `False`. 3. **summarize_subnets(subnets: List[str]) -> List[str]:** - **Input:** A list of subnet strings. - **Output:** A list of summarized subnet strings. 4. **total_unique_ips(subnets: List[str]) -> int:** - **Input:** A list of subnet strings. - **Output:** Return the total number of unique IP addresses covered by all the subnets. # Constraints - Each IP address and subnet provided will be in valid IPv4 or IPv6 notation. - Use the `ipaddress` module for IP address manipulation. - The list of subnets will contain at most 10,000 elements. # Example ```python subnets = [\\"192.168.0.0/24\\", \\"192.168.1.0/24\\", \\"10.0.0.0/8\\", \\"2001:db8::/32\\"] assert validate_subnets(subnets) == [\\"192.168.0.0/24\\", \\"192.168.1.0/24\\", \\"10.0.0.0/8\\", \\"2001:db8::/32\\"] assert is_ip_in_subnets(\\"192.168.0.5\\", subnets) == True assert is_ip_in_subnets(\\"10.10.10.10\\", subnets) == True assert is_ip_in_subnets(\\"192.168.2.5\\", subnets) == False assert summarize_subnets(subnets) == [\'10.0.0.0/8\', \'192.168.0.0/23\', \'2001:db8::/32\'] assert total_unique_ips(subnets) == 16843008 ``` # Note - Ensure proper error handling for invalid IP addresses and subnets in the `validate_subnets` function. - Make sure that the network summaries in the `summarize_subnets` function are minimal and non-overlapping.","solution":"import ipaddress def validate_subnets(subnets): valid_subnets = [] for subnet in subnets: try: ipaddress.ip_network(subnet, strict=False) valid_subnets.append(subnet) except ValueError: continue return valid_subnets def is_ip_in_subnets(ip, subnets): ip_obj = ipaddress.ip_address(ip) for subnet in subnets: if ip_obj in ipaddress.ip_network(subnet, strict=False): return True return False def summarize_subnets(subnets): net_list = list(map(lambda x: ipaddress.ip_network(x, strict=False), subnets)) summarized = ipaddress.collapse_addresses(net_list) return [str(net) for net in summarized] def total_unique_ips(subnets): total_ips = 0 valid_subnets = validate_subnets(subnets) for subnet in valid_subnets: net = ipaddress.ip_network(subnet, strict=False) total_ips += net.num_addresses return total_ips"},{"question":"Objective: Demonstrate your understanding of Python function object manipulation by using the provided lower-level API functions. You are required to dynamically create a function, set its attributes, and then use it in a computation. Task: Write a Python program that performs the following steps: 1. **Define a code object**: Create a simple code object, which when executed as a function, returns the sum of two arguments (`x` and `y`). 2. **Create a function object**: Using `PyFunction_New` or a similar approach, create a Python function object from the code object. Provide it with a global namespace. 3. **Set function attributes**: - Set the default values for its arguments, so if no arguments are provided, the function will add `5` and `10`. - Set annotations indicating both arguments and the return type as `int`. - Set a closure if necessary. 4. **Utilize the function**: Call the function with and without arguments and print the results. Input and Output Formats: - **No input**: The function definitions and calls will be within the script. - **Output**: Print the result of calls to the dynamically created function, both with and without arguments. Constraints: - You are encouraged to use Python\'s lower-level `types.FunctionType` for function creation and modification. - Ensure to handle any corner cases or errors gracefully. Example: ```python import types # Step 1: Define the code object code_str = def generated_function(x, y): return x + y exec(code_str) code = generated_function.__code__ # Step 2: Create the function object globals_dict = globals() # Using types.FunctionType new_function = types.FunctionType(code, globals_dict) # Step 3: Set function attributes new_function.__defaults__ = (5, 10) new_function.__annotations__ = {\\"x\\": int, \\"y\\": int, \\"return\\": int} # Step 4: Utilize the function print(new_function()) # Output: 15 print(new_function(2, 3)) # Output: 5 ``` Note: This example serves as an illustration. Your task is to implement all steps starting from scratch, especially focusing on function creation using the lower-level function APIs described.","solution":"import types # Step 1: Define the code object code_str = def generated_function(x, y): return x + y exec(code_str) # Executing code string to define generated_function code = generated_function.__code__ # Extracting the code object from the generated function # Step 2: Create the function object globals_dict = globals() new_function = types.FunctionType(code, globals_dict) # Step 3: Set function attributes new_function.__defaults__ = (5, 10) # Default values for arguments new_function.__annotations__ = {\\"x\\": int, \\"y\\": int, \\"return\\": int} # Type annotations # Step 4: Utilize the function print(new_function()) # Output: 15 print(new_function(2, 3)) # Output: 5"},{"question":"**Context:** You are tasked with analyzing a dataset that contains information about trips made by a fleet of vehicles. The dataset records the trip details such as the vehicle ID, trip start time, trip end time, and distance traveled. You need to visualize this data using Seaborn to understand trends and patterns in the trips data. **Dataset:** You are provided with a CSV file named `trips_data.csv` arranged in the following long-form structure: - `vehicle_id`: An identifier for each vehicle. - `start_time`: When the trip started. - `end_time`: When the trip ended. - `distance`: The distance traveled during the trip (in kilometers). Example data: ``` vehicle_id,start_time,end_time,distance 1,2023-01-01 07:00,2023-01-01 08:00,10 2,2023-01-01 07:15,2023-01-01 08:10,15 ... ``` **Tasks:** 1. **Data Preprocessing:** - Load the dataset and parse the `start_time` and `end_time` columns into datetime format. - Create a new column `trip_duration_minutes` that represents the trip duration in minutes. 2. **Data Transformation:** - Transform the dataset into a wide-form structure where each column represents a distinct `vehicle_id` and rows represent trips indexed by `start_time`. 3. **Visualization with Seaborn:** - Use Seaborn to create a line plot that shows the total trip distance across different `vehicle_id`s aggregated by hour of the day. - Create another line plot to show the average trip duration per vehicle over the course of a week. **Constraints:** - Ensure the visualizations are clear and labeled appropriately. - Handle missing data gracefully by filling or dropping as necessary. - Use Seaborn and Pandas libraries efficiently to achieve these tasks. **Function Implementation:** You need to implement the following function to solve the above tasks: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def preprocess_transform_and_visualize(csv_file_path): Preprocess, transform, and visualize trip data. Args: - csv_file_path (str): Path to the input CSV file. Returns: - None: Displays the visualizations. # Task 1: Data Preprocessing df = pd.read_csv(csv_file_path, parse_dates=[\'start_time\', \'end_time\']) df[\'trip_duration_minutes\'] = (df[\'end_time\'] - df[\'start_time\']).dt.total_seconds() / 60 # Task 2: Data Transformation wide_form_data = df.pivot_table(index=\'start_time\', columns=\'vehicle_id\', values=\'distance\', fill_value=0) # Task 3: Visualization with Seaborn df[\'start_hour\'] = df[\'start_time\'].dt.hour hourly_distance = df.groupby([\'start_hour\', \'vehicle_id\'])[\'distance\'].sum().reset_index() hourly_duration = df.groupby([\'start_hour\', \'vehicle_id\'])[\'trip_duration_minutes\'].mean().reset_index() plt.figure(figsize=(14, 7)) # Total trip distance per hour sns.lineplot(data=hourly_distance, x=\'start_hour\', y=\'distance\', hue=\'vehicle_id\') plt.title(\'Total Trip Distance per Hour for each Vehicle\') plt.xlabel(\'Hour of the Day\') plt.ylabel(\'Total Distance (km)\') plt.legend(title=\'Vehicle ID\') plt.show() # Average trip duration per hour sns.lineplot(data=hourly_duration, x=\'start_hour\', y=\'trip_duration_minutes\', hue=\'vehicle_id\') plt.title(\'Average Trip Duration per Hour for each Vehicle\') plt.xlabel(\'Hour of the Day\') plt.ylabel(\'Average Trip Duration (minutes)\') plt.legend(title=\'Vehicle ID\') plt.show() ``` # Example Usage: ```python preprocess_transform_and_visualize(\'path/to/trips_data.csv\') ``` This function reads the CSV file, processes the data, transforms it into the required formats, and generates the specified visualizations.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def preprocess_transform_and_visualize(csv_file_path): Preprocess, transform, and visualize trip data. Args: - csv_file_path (str): Path to the input CSV file. Returns: - None: Displays the visualizations. # Task 1: Data Preprocessing df = pd.read_csv(csv_file_path, parse_dates=[\'start_time\', \'end_time\']) df[\'trip_duration_minutes\'] = (df[\'end_time\'] - df[\'start_time\']).dt.total_seconds() / 60 # Task 2: Data Transformation wide_form_data = df.pivot_table(index=\'start_time\', columns=\'vehicle_id\', values=\'distance\', fill_value=0) # Task 3: Visualization with Seaborn df[\'start_hour\'] = df[\'start_time\'].dt.hour hourly_distance = df.groupby([\'start_hour\', \'vehicle_id\'])[\'distance\'].sum().reset_index() hourly_duration = df.groupby([\'start_hour\', \'vehicle_id\'])[\'trip_duration_minutes\'].mean().reset_index() plt.figure(figsize=(14, 7)) # Total trip distance per hour sns.lineplot(data=hourly_distance, x=\'start_hour\', y=\'distance\', hue=\'vehicle_id\') plt.title(\'Total Trip Distance per Hour for each Vehicle\') plt.xlabel(\'Hour of the Day\') plt.ylabel(\'Total Distance (km)\') plt.legend(title=\'Vehicle ID\') plt.show() # Average trip duration per hour sns.lineplot(data=hourly_duration, x=\'start_hour\', y=\'trip_duration_minutes\', hue=\'vehicle_id\') plt.title(\'Average Trip Duration per Hour for each Vehicle\') plt.xlabel(\'Hour of the Day\') plt.ylabel(\'Average Trip Duration (minutes)\') plt.legend(title=\'Vehicle ID\') plt.show()"},{"question":"# Context Management in Python using `contextlib` **Objective:** You are required to demonstrate your understanding of context management in Python using the `contextlib` module. This assessment focuses on creating and combining synchronous and asynchronous context managers to handle resource management elegantly. **Problem Statement:** Your task is to implement a function, `manage_resources` that uses context managers to handle different resources, ensuring proper acquisition and cleanup. You need to handle both synchronous and asynchronous resources and demonstrate the usage of `ExitStack` and `AsyncExitStack` to manage multiple context managers dynamically. # Specifications: 1. **Function Signature:** ```python def manage_resources(sync_resources: List[Any], async_resources: List[Any]) -> Dict[str, Any]: ``` 2. **Input:** - `sync_resources`: A list of resources that need to be managed synchronously. Each resource in this list should be an object that supports the context management protocol (`__enter__` and `__exit__` methods). - `async_resources`: A list of resources that need to be managed asynchronously. Each resource in this list should be an object that supports the asynchronous context management protocol (`__aenter__` and `__aexit__` methods). 3. **Output:** - A dictionary where the keys are \\"sync\\" and \\"async\\", and the values are lists of results obtained from managing the synchronous and asynchronous resources, respectively. # Constraints: - You must use `ExitStack` for managing synchronous resources and `AsyncExitStack` for managing asynchronous resources. - Properly handle exceptions and ensure that all resources are correctly cleaned up, even in the event of an error. - Assume that each synchronous resource has a method `use_resource` that returns some result when used within its context. - Assume that each asynchronous resource has a coroutine `use_resource` that returns some result when used within its context. - The function should maintain high performance and handle multiple resources efficiently. # Example: ```python class SyncResourceExample: def __enter__(self): print(\\"Entering Sync Resource\\") return self def __exit__(self, exc_type, exc_value, traceback): print(\\"Exiting Sync Resource\\") def use_resource(self): return \\"Sync Result\\" class AsyncResourceExample: async def __aenter__(self): print(\\"Entering Async Resource\\") return self async def __aexit__(self, exc_type, exc_value, traceback): print(\\"Exiting Async Resource\\") async def use_resource(self): return \\"Async Result\\" sync_resources = [SyncResourceExample()] async_resources = [AsyncResourceExample()] # Example Usage import asyncio output = asyncio.run(manage_resources(sync_resources, async_resources)) print(output) ``` Expected Output: ``` Entering Sync Resource Entering Async Resource Exiting Async Resource Exiting Sync Resource {\'sync\': [\'Sync Result\'], \'async\': [\'Async Result\']} ``` # Notes: - Implement appropriate error handling to ensure that all resources are cleaned up correctly, even in the event of an error. - Make use of the `contextlib` utilities wherever appropriate. Good luck, and demonstrate your mastery of context management in Python!","solution":"from contextlib import ExitStack, AsyncExitStack from typing import List, Any, Dict def manage_resources(sync_resources: List[Any], async_resources: List[Any]) -> Dict[str, Any]: result = {\'sync\': [], \'async\': []} with ExitStack() as stack: for resource in sync_resources: res = stack.enter_context(resource) result[\'sync\'].append(res.use_resource()) async def manage_async_resources(): async with AsyncExitStack() as stack: for resource in async_resources: res = await stack.enter_async_context(resource) result[\'async\'].append(await res.use_resource()) import asyncio asyncio.run(manage_async_resources()) return result"},{"question":"# **Coding Assessment Question** **Objective**: Demonstrate your understanding of applying various linear models using scikit-learn and selecting the best model based on cross-validation performance. **Problem Statement**: You are provided with a dataset containing features `X` and target values `y`. Your task is to implement a function `find_best_model` that applies different linear models to the dataset, compares them using cross-validation, and returns the best model based on the specified evaluation metric. The linear models to be considered are: 1. Ordinary Least Squares (LinearRegression) 2. Ridge Regression (Ridge) 3. Lasso Regression (Lasso) 4. Elastic Net (ElasticNet) The function should follow these steps: 1. Split the data into training and testing sets. 2. For each model, perform cross-validation on the training set using the specified evaluation metric. 3. Compare the models based on the cross-validation scores and select the best one. 4. Return the best model fitted on the entire dataset and its corresponding cross-validation score. **Function Signature**: ```python def find_best_model(X, y, metric): Parameters: X (ndarray): Feature matrix of shape (n_samples, n_features) y (ndarray): Target values of shape (n_samples, ) metric (str): Evaluation metric for cross-validation (\'neg_mean_squared_error\', \'r2\', etc.) Returns: best_model (sklearn model): The best fitted sklearn model best_score (float): The best cross-validation score pass ``` **Input and Output**: - **Input**: - `X`: A NumPy ndarray of shape `(n_samples, n_features)` representing the feature matrix. - `y`: A NumPy ndarray of shape `(n_samples,)` representing the target values. - `metric`: A string specifying the evaluation metric to be used for cross-validation. Possible options include `\'neg_mean_squared_error\'`, `\'r2\'`, etc. - **Output**: - `best_model`: An instance of the best-fitted scikit-learn model. - `best_score`: A float representing the best cross-validation score. **Constraints**: - The dataset (`X`, `y`) should contain at least 100 samples and 5 features. - You should use `cross_val_score` from scikit-learn for cross-validation. - Use 5-fold cross-validation for model comparison. **Example**: ```python from sklearn.datasets import make_regression import numpy as np # Generating a random regression problem X, y = make_regression(n_samples=200, n_features=5, noise=0.1) # Defining the evaluation metric metric = \'neg_mean_squared_error\' # Finding the best model best_model, best_score = find_best_model(X, y, metric) print(f\\"Best Model: {best_model}\\") print(f\\"Best Cross-Validation Score: {best_score}\\") ``` **Implementation Note**: - Ensure to import necessary classes and functions from scikit-learn, including `LinearRegression`, `Ridge`, `Lasso`, `ElasticNet`, `cross_val_score`, and `train_test_split`. - Set `random_state=42` where applicable for reproducibility. This coding task will assess your ability to apply multiple regression techniques, use cross-validation for model evaluation, and select the best model based on performance metrics. **Good Luck!**","solution":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.model_selection import cross_val_score, train_test_split import numpy as np def find_best_model(X, y, metric): Parameters: X (ndarray): Feature matrix of shape (n_samples, n_features) y (ndarray): Target values of shape (n_samples, ) metric (str): Evaluation metric for cross-validation (\'neg_mean_squared_error\', \'r2\', etc.) Returns: best_model (sklearn model): The best fitted sklearn model best_score (float): The best cross-validation score # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the models to be compared models = { \'LinearRegression\': LinearRegression(), \'Ridge\': Ridge(), \'Lasso\': Lasso(), \'ElasticNet\': ElasticNet() } best_score = -np.inf best_model = None # Iterate through the models and evaluate using cross-validation for name, model in models.items(): scores = cross_val_score(model, X_train, y_train, cv=5, scoring=metric) mean_score = np.mean(scores) if mean_score > best_score: best_score = mean_score best_model = model # Fit the best model on the entire dataset best_model.fit(X, y) return best_model, best_score"},{"question":"# Python C API-based Tuple Manipulation Using the concepts of Python tuples and named tuples (struct sequences) as described in the Python C API documentation, implement the following functions in Python: 1. `create_tuple(elements: List[Any]) -> Tuple`: - Create a new tuple containing the elements provided in the input list. - **Input**: A list of elements. - **Output**: A tuple containing the elements of the list in the same order. 2. `get_tuple_item(t: Tuple, index: int) -> Any`: - Retrieve the element at the specified index from the tuple. - **Input**: A tuple and an index. - **Output**: The element at the specified index. - **Constraints**: Handle IndexError gracefully if the index is out of bounds. 3. `slice_tuple(t: Tuple, start: int, end: int) -> Tuple`: - Return a slice of the tuple from the start index up to, but not including, the end index. - **Input**: A tuple, a start index, and an end index. - **Output**: A new tuple containing the specified slice. - **Constraints**: Handle slicing gracefully when indices are out of bounds. 4. `create_namedtuple(typename: str, field_names: List[str], values: List[Any]) -> NamedTuple`: - Create a namedtuple with the given typename, field names, and values. - **Input**: A typename (string), a list of field names, and a list of values. - **Output**: A namedtuple instance of the specified type with the field names and values. - **Constraints**: Ensure the number of field names matches the number of values; handle errors gracefully. Example Usage: ```python # Function 1 t = create_tuple([1, 2, 3]) print(t) # Output: (1, 2, 3) # Function 2 item = get_tuple_item(t, 1) print(item) # Output: 2 # Function 3 sliced = slice_tuple(t, 0, 2) print(sliced) # Output: (1, 2) # Function 4 Person = create_namedtuple(\'Person\', [\'name\', \'age\'], [\'Alice\', 30]) print(Person.name) # Output: Alice print(Person.age) # Output: 30 ``` # Notes and Constraints - Ensure efficient handling of tuple and namedtuple operations. - Provide appropriate error messages and handle exceptions gracefully. - Focus on the immutability of tuples while implementing the functions. - Avoid using direct attribute access (e.g., `__dict__` or `__slots__`) in namedtuple manipulations; stick to standard methods.","solution":"from typing import List, Any, Tuple from collections import namedtuple def create_tuple(elements: List[Any]) -> Tuple: Create a new tuple containing the elements provided in the input list. return tuple(elements) def get_tuple_item(t: Tuple, index: int) -> Any: Retrieve the element at the specified index from the tuple. Returns None and prints an error message if the index is out of bounds. try: return t[index] except IndexError: print(f\\"Index {index} is out of bounds for tuple of length {len(t)}.\\") return None def slice_tuple(t: Tuple, start: int, end: int) -> Tuple: Return a slice of the tuple from the start index up to, but not including, the end index. Handles slicing gracefully when indices are out of bounds. return t[start:end] def create_namedtuple(typename: str, field_names: List[str], values: List[Any]) -> Any: Create a namedtuple with the given typename, field names, and values. Ensures the number of field names matches the number of values. Handles errors gracefully. if len(field_names) != len(values): raise ValueError(\\"Number of field names does not match the number of values.\\") NamedTupleClass = namedtuple(typename, field_names) return NamedTupleClass(*values)"},{"question":"# Python Coding Assessment Question Objective: To assess the understanding of Python\'s site-specific configuration and the usage of the `site` module to manage Python package directories. Problem Statement: You are tasked with writing a Python program to manage site-specific paths and site-packages directories using the `site` module. Your program should expose functionalities to: 1. Add a new path to the site-packages directory. 2. Retrieve and print all global site-packages directories. 3. Retrieve and print the user-specific site-packages directory. 4. Enable or disable the user-specific site-packages directory. 5. Print the current `sys.path`. Function Descriptions: 1. **add_site_dir(path: str) -> None** - **Input:** `path` - A string representing the directory path to be added. - **Output:** None - **Description:** This function should add the provided path to the site-packages directory. Use `site.addsitedir()` for this purpose. 2. **get_global_site_packages() -> List[str]** - **Input:** None - **Output:** A list of strings where each string is a path to a global site-packages directory. - **Description:** Retrieve all global site-packages directories using the `site.getsitepackages()` method. 3. **get_user_site_packages() -> str** - **Input:** None - **Output:** A string representing the user-specific site-packages directory. - **Description:** Return the path of the user-specific site-packages directory using `site.getusersitepackages()` method. 4. **enable_user_site(enable: bool) -> None** - **Input:** `enable` - A boolean indicating whether to enable (True) or disable (False) the user-specific site-packages directory. - **Output:** None - **Description:** Enable or disable the user-specific site-packages directory. When enabled, add the directory to `sys.path` if not already present. When disabled, ensure the directory is removed from `sys.path`. 5. **print_sys_path() -> None** - **Input:** None - **Output:** None - **Description:** Print the current `sys.path`. Constraints: - Use only the `sys` and `site` modules for managing paths. - The `enable_user_site` function should modify the `site.ENABLE_USER_SITE` flag accordingly. Execution Example: ```python if __name__ == \\"__main__\\": # Add a new site-packages directory add_site_dir(\'/custom/site-packages\') # Retrieve and print all global site-packages directories global_sites = get_global_site_packages() print(\\"Global Site-Packages:\\", global_sites) # Retrieve and print the user-specific site-packages directory user_site = get_user_site_packages() print(\\"User Site-Packages:\\", user_site) # Enable user-specific site-packages enable_user_site(True) # Print current sys.path print_sys_path() ```","solution":"import sys import site from typing import List def add_site_dir(path: str) -> None: Adds the specified directory to the site-packages directory. site.addsitedir(path) def get_global_site_packages() -> List[str]: Retrieves all global site-packages directories. Returns: A list of paths to global site-packages directories. return site.getsitepackages() def get_user_site_packages() -> str: Retrieves the user-specific site-packages directory. Returns: A path to the user-specific site-packages directory. return site.getusersitepackages() def enable_user_site(enable: bool) -> None: Enables or disables the user-specific site-packages directory. Args: enable (bool): If True, enables the user-specific site-packages directory. If False, disables it. if enable: if site.getusersitepackages() not in sys.path: sys.path.append(site.getusersitepackages()) site.ENABLE_USER_SITE = True else: if site.getusersitepackages() in sys.path: sys.path.remove(site.getusersitepackages()) site.ENABLE_USER_SITE = False def print_sys_path() -> None: Prints the current sys.path. print(\\"Current sys.path:\\", sys.path)"},{"question":"# Pandas Coding Assessment Question You are provided with a dataset representing sales transactions in a retail store. Each transaction includes the transaction date, product category, product ID, quantity sold, and the transaction value. Your task is to use pandas to perform the following analyses: 1. **Load the Data**: Load the dataset into a pandas DataFrame. Assume the dataset is provided in a CSV file named `sales_transactions.csv`. 2. **Missing Data**: Identify and handle any missing values in the dataset. For the purposes of this task, assume that if any values are missing for the `quantity_sold` or `transaction_value` columns, they should be filled with the median value of the respective columns. Additionally, if any dates are missing in the `transaction_date` column, fill them with the mode of the column. 3. **Datetime Conversion**: Convert the `transaction_date` column to datetime format. 4. **Data Aggregation**: - Create a new DataFrame that shows the total quantity sold and total transaction value for each product category. - Create another DataFrame showing the weekly sales (sum of `quantity_sold` and `transaction_value`) for the entire dataset. 5. **Pivot Table**: Create a pivot table that shows the total transaction value for each product category by month. 6. **Merge DataFrames**: Suppose you have another dataset (`product_info.csv`) containing information about products such as the product ID, product name, and product price. Merge this dataset with your sales transaction dataset to append product names and prices to each transaction. 7. **Dummy Variables**: Convert the product categories into dummy/indicator variables and integrate them into the original sales transaction DataFrame. # Constraints and Requirements: - Ensure your solution is efficient in terms of both computation and memory usage. - The input CSV files `sales_transactions.csv` and `product_info.csv` are assumed to be in the same directory as your script. - The columns of `sales_transactions.csv` and `product_info.csv` are as follows: - `sales_transactions.csv` columns: `transaction_date`, `product_category`, `product_id`, `quantity_sold`, `transaction_value` - `product_info.csv` columns: `product_id`, `product_name`, `product_price` # Input Assume the data is provided as described above. ```python # Example structure of `sales_transactions.csv` # transaction_date,product_category,product_id,quantity_sold,transaction_value # 2022-01-01,Electronics,101,2,400 # 2022-01-01,Clothing,201,1,50 # ... # Example structure of `product_info.csv` # product_id,product_name,product_price # 101,Smartphone,200 # 201,Jacket,50 # ... ``` # Output Ensure your code prints out the following results: 1. DataFrame with missing values handled. 2. DataFrame with `transaction_date` converted to datetime. 3. Aggregated DataFrame with total quantity sold and total transaction value for each product category. 4. DataFrame showing weekly sales. 5. Pivot table of total transaction value for each product category by month. 6. Merged DataFrame with product names and prices appended. 7. DataFrame with dummy variables for product categories. --- Good luck, and make sure to handle all edge cases and constraints effectively!","solution":"import pandas as pd def process_sales_data(): # Load the dataset into a pandas DataFrame sales_df = pd.read_csv(\'sales_transactions.csv\') # Handle missing values in the dataset quantity_median = sales_df[\'quantity_sold\'].median() transaction_median = sales_df[\'transaction_value\'].median() date_mode = sales_df[\'transaction_date\'].mode()[0] sales_df[\'quantity_sold\'].fillna(quantity_median, inplace=True) sales_df[\'transaction_value\'].fillna(transaction_median, inplace=True) sales_df[\'transaction_date\'].fillna(date_mode, inplace=True) # Convert transaction_date column to datetime format sales_df[\'transaction_date\'] = pd.to_datetime(sales_df[\'transaction_date\']) # Create a DataFrame that shows the total quantity sold and total transaction value for each product category category_agg = sales_df.groupby(\'product_category\').agg( total_quantity_sold=(\'quantity_sold\', \'sum\'), total_transaction_value=(\'transaction_value\', \'sum\') ).reset_index() # Create a DataFrame showing the weekly sales for the entire dataset sales_df[\'week\'] = sales_df[\'transaction_date\'].dt.to_period(\'W\').apply(lambda r: r.start_time) weekly_sales = sales_df.groupby(\'week\').agg( weekly_quantity_sold=(\'quantity_sold\', \'sum\'), weekly_transaction_value=(\'transaction_value\', \'sum\') ).reset_index() # Create a pivot table that shows the total transaction value for each product category by month sales_df[\'month\'] = sales_df[\'transaction_date\'].dt.to_period(\'M\').apply(lambda r: r.start_time) pivot_table = sales_df.pivot_table( index=\'month\', columns=\'product_category\', values=\'transaction_value\', aggfunc=\'sum\' ).fillna(0) # Merge with product_info dataset product_info_df = pd.read_csv(\'product_info.csv\') merged_df = pd.merge(sales_df, product_info_df, on=\'product_id\', how=\'left\') # Convert product categories into dummy/indicator variables dummies_df = pd.get_dummies(sales_df[\'product_category\'], prefix=\'category\') sales_with_dummies_df = pd.concat([sales_df, dummies_df], axis=1) return { \'handled_missing\': sales_df, \'converted_date\': sales_df, \'category_agg\': category_agg, \'weekly_sales\': weekly_sales, \'pivot_table\': pivot_table, \'merged_df\': merged_df, \'dummies_df\': sales_with_dummies_df }"},{"question":"# Tensors and Dimensions Manipulation In this task, you are required to write functions that manipulate the shapes of tensors using PyTorch. You should demonstrate your understanding of how to work with `torch.Size` and perform operations based on tensor dimensions. 1. **Reshape Tensor** Write a function `reshape_tensor` that takes a tensor `t` and a list of integers `new_shape` and reshapes the tensor to the specified shape. **Input:** - `t`: A tensor of any shape. - `new_shape`: A list of integers specifying the new shape. **Output:** - A new tensor reshaped to the specified shape. **Constraints:** - The total number of elements in `t` must match the total number of elements in `new_shape`. ```python def reshape_tensor(t, new_shape): pass ``` 2. **Extract Dimensions of Tensor** Write a function `tensor_dimensions` that takes a tensor `t` and returns a `torch.Size` object representing the dimensions of the tensor. **Input:** - `t`: A tensor of any shape. **Output:** - A `torch.Size` object containing the size of all dimensions. ```python def tensor_dimensions(t): pass ``` 3. **Sum Over Specific Dimension** Write a function `sum_over_dimension` that takes a tensor `t` and an integer `dim` and returns a tensor after summing over the specified dimension. **Input:** - `t`: A tensor of any shape. - `dim`: An integer specifying the dimension to sum over. **Output:** - A tensor with the specified dimension summed over. ```python def sum_over_dimension(t, dim): pass ``` # Examples ```python import torch # Example for reshape_tensor function t = torch.ones(10, 20, 30) new_shape = [20, 30, 10] reshaped_t = reshape_tensor(t, new_shape) print(reshaped_t.size()) # Output: torch.Size([20, 30, 10]) # Example for tensor_dimensions function t = torch.ones(5, 10) dims = tensor_dimensions(t) print(dims) # Output: torch.Size([5, 10]) # Example for sum_over_dimension function t = torch.ones(2, 3, 4) summed_t = sum_over_dimension(t, 1) print(summed_t.size()) # Output: torch.Size([2, 4]) ``` Ensure your solution is efficient and correctly handles edge cases, such as invalid inputs or dimensions.","solution":"import torch def reshape_tensor(t, new_shape): Reshapes the tensor `t` to the specified shape `new_shape`. Args: t: torch.Tensor: A tensor of any shape. new_shape: list of integers: A list specifying the new shape. Returns: torch.Tensor: A new tensor reshaped to the specified shape. Raises: RuntimeError: If the total number of elements does not match between the current shape and the new shape. return t.view(*new_shape) def tensor_dimensions(t): Returns the dimensions of the tensor `t`. Args: t: torch.Tensor: A tensor of any shape. Returns: torch.Size: The size object representing the dimensions of the tensor. return t.size() def sum_over_dimension(t, dim): Sums the tensor `t` over the specified dimension `dim`. Args: t: torch.Tensor: A tensor of any shape. dim: int: The dimension to sum over. Returns: torch.Tensor: A tensor with the specified dimension summed over. return t.sum(dim)"},{"question":"**Problem Statement:** You are provided with a dataset containing information about the passengers of the Titanic. Your task is to visualize this dataset using Seaborn\'s categorical plotting functions to answer the following questions: 1. What is the distribution of the fares paid by passengers across different classes and genders? 2. How does the survival rate vary across different classes and genders? 3. How do the ages of passengers differ across different embarkation towns and classes? You are required to write a function `visualize_titanic_data(titanic)` that takes a pandas DataFrame `titanic` and outputs three plots: # Input: - `titanic`: A pandas DataFrame containing the Titanic dataset. It contains the following relevant columns: - `class`: Passenger class (1st, 2nd, 3rd) - `sex`: Gender of the passenger - `survived`: Survival status (0 = No, 1 = Yes) - `fare`: Passenger fare - `embark_town`: Town of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) - `age`: Age of the passenger # Output: The function should generate the following plots: 1. **Violin plot** showing the distribution of fares paid by passengers separated by class and gender. 2. **Point plot** showing the survival rate across different classes and genders. 3. **Box plot** showing the age distribution across different embarkation towns and classes. # Constraints: - Ensure that the colors used for different genders are distinguishable. - Customize the plots for better readability by adjusting titles, labels, and color palettes where necessary. - Use facets to show the third dimension where applicable. # Function Signature: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(titanic: pd.DataFrame): # Your code here pass ``` # Example Usage: ```python # Load the Titanic dataset from Seaborn titanic = sns.load_dataset(\'titanic\') # Call the visualization function visualize_titanic_data(titanic) ``` # Notes: - Make sure to handle missing values appropriately to avoid errors during visualization. - Your function should not return anything; it should only output the required plots. # Evaluation Criteria: - Correctness: The function accurately generates the required plots. - Readability: Plots should be customized for readability, with appropriate labels, titles, and legends. - Code Quality: Code should be well-organized, with comments explaining each step.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(titanic: pd.DataFrame): # Handle missing values titanic = titanic.dropna(subset=[\'class\', \'sex\', \'survived\', \'fare\', \'embark_town\', \'age\']) # 1. Violin plot for fare distribution across class and gender plt.figure(figsize=(12, 6)) sns.violinplot(x=\'class\', y=\'fare\', hue=\'sex\', data=titanic, split=True, palette=\'muted\') plt.title(\'Distribution of fares by class and gender\') plt.xlabel(\'Class\') plt.ylabel(\'Fare\') plt.legend(title=\'Gender\') plt.show() # 2. Point plot for survival rate across class and gender plt.figure(figsize=(12, 6)) sns.pointplot(x=\'class\', y=\'survived\', hue=\'sex\', data=titanic, palette=\'muted\', markers=[\'o\', \'x\'], linestyles=[\'-\', \'--\']) plt.title(\'Survival rate by class and gender\') plt.xlabel(\'Class\') plt.ylabel(\'Survival Rate\') plt.legend(title=\'Gender\') plt.show() # 3. Box plot for age distribution across embarkation town and class plt.figure(figsize=(12, 6)) sns.boxplot(x=\'embark_town\', y=\'age\', hue=\'class\', data=titanic, palette=\'muted\') plt.title(\'Age distribution by embarkation town and class\') plt.xlabel(\'Embarkation Town\') plt.ylabel(\'Age\') plt.legend(title=\'Class\') plt.show()"},{"question":"You have been given a dataset representing patients\' medical records. Each record contains a set of features and a label indicating whether the patient has a specific disease (1 for positive, 0 for negative). As a data scientist, your task is to develop a decision-making model to assist physicians in screening for this disease. The model should prioritize recall to ensure that as many positive cases as possible are identified, even if this means increasing the number of false positives. # Objective - Implement a classifier using scikit-learn and tune the decision threshold to maximize the recall score. - Use the `TunedThresholdClassifierCV` class to perform this tuning. # Input and Output Formats - **Input:** - A training dataset with features `X_train` (NumPy array) and labels `y_train` (NumPy array). - A test dataset with features `X_test` (NumPy array). - **Output:** - The predicted labels for the test dataset `y_pred` (NumPy array) after tuning the decision threshold to maximize recall. # Constraints and Requirements - Implement the classifier using logistic regression. - Tune the decision threshold using `TunedThresholdClassifierCV` to maximize recall. - Use a 5-fold cross-validation for tuning the threshold. - Ensure the implementation avoids overfitting by properly using the cross-validation parameter. # Performance Requirements - The solution should efficiently handle datasets with up to 10,000 samples and 100 features (typical for such medical datasets). # Sample Code ```python import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score def tune_and_predict(X_train, y_train, X_test): Train a logistic regression classifier, tune the decision threshold to maximize recall using TunedThresholdClassifierCV, and predict the test labels. Parameters: - X_train (np.ndarray): Training features - y_train (np.ndarray): Training labels - X_test (np.ndarray): Test features Returns: - np.ndarray: Predicted labels for the test dataset # Define the base model base_model = LogisticRegression() # Define the scorer to maximize recall scorer = make_scorer(recall_score) # Define the TunedThresholdClassifierCV tuned_threshold_cv = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=5) # Fit the model to the training data tuned_threshold_cv.fit(X_train, y_train) # Predict the labels for the test set y_pred = tuned_threshold_cv.predict(X_test) return y_pred # Example usage if __name__ == \\"__main__\\": # Assume X_train, y_train, and X_test are already defined # X_train = np.array([[...], [...], ...]) # y_train = np.array([...]) # X_test = np.array([[...], [...], ...]) y_pred = tune_and_predict(X_train, y_train, X_test) print(y_pred) ``` # Instructions 1. Complete the function `tune_and_predict` to perform the specified tasks. 2. The input data (not shown here) will be provided in the execution environment. 3. Focus on ensuring the model is trained and the threshold is tuned according to the recall metric. 4. Return the predicted labels for the test set.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score, make_scorer from sklearn.model_selection import GridSearchCV class TunedThresholdClassifierCV: def __init__(self, base_model, scoring, cv=5, thresholds=None): self.base_model = base_model self.scoring = scoring self.cv = cv self.thresholds = thresholds if thresholds is not None else np.linspace(0.0, 1.0, 101) def fit(self, X, y): # Train the model self.base_model.fit(X, y) # Getting the raw predicted probabilities y_probs = self.base_model.predict_proba(X)[:, 1] # Search over thresholds, use cross-validation to select optimal threshold best_threshold = None best_score = -np.inf for threshold in self.thresholds: y_pred = (y_probs >= threshold).astype(int) score = recall_score(y, y_pred) if score > best_score: best_score = score best_threshold = threshold self.best_threshold = best_threshold return self def predict(self, X): y_probs = self.base_model.predict_proba(X)[:, 1] return (y_probs >= self.best_threshold).astype(int) def tune_and_predict(X_train, y_train, X_test): Train a logistic regression classifier, tune the decision threshold to maximize recall using TunedThresholdClassifierCV, and predict the test labels. Parameters: - X_train (np.ndarray): Training features - y_train (np.ndarray): Training labels - X_test (np.ndarray): Test features Returns: - np.ndarray: Predicted labels for the test dataset # Define the base model base_model = LogisticRegression(solver=\'liblinear\') # Define the scorer to maximize recall scorer = make_scorer(recall_score) # Define the TunedThresholdClassifierCV tuned_threshold_cv = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=5) # Fit the model to the training data tuned_threshold_cv.fit(X_train, y_train) # Predict the labels for the test set y_pred = tuned_threshold_cv.predict(X_test) return y_pred"},{"question":"**Transformer Pipeline Design and Implementation** # Problem Statement Using scikit-learn, you are required to implement a data transformation pipeline that combines various transformers to preprocess a dataset. Your task is to: 1. Impute missing values using a specified strategy. 2. Normalize the numerical features. 3. Transform categorical features (if any) into numerical ones. 4. Use Principal Component Analysis (PCA) to reduce the dimensions of the dataset. 5. Combine these steps into a single transformation pipeline. # Input - `data` (pandas DataFrame): The input dataset containing numerical and/or categorical features. - `impute_strategy` (str): The strategy for imputing missing values, one of `mean`, `median`, or `most_frequent`. - `n_components` (int): The number of principal components to keep from PCA. # Output - Transformed dataset with imputed, normalized, and reduced dimensions (numpy array). # Constraints - Import necessary modules from `sklearn.preprocessing`, `sklearn.compose`, `sklearn.impute`, `sklearn.decomposition`, and other relevant packages. - Ensure the pipeline can handle potential missing values and mixed data types. - Maintain a clear structure and document your code for clarity. # Example Usage ```python import pandas as pd # Example dataset data = pd.DataFrame({ \'A\': [1, 2, None, 4], \'B\': [None, 2, 3, 4], \'C\': [\'a\', \'b\', \'b\', \'a\'] }) impute_strategy = \'mean\' n_components = 2 result = transform_pipeline(data, impute_strategy, n_components) print(result) ``` # Constraints - You must fit and transform the input dataset within the pipeline. - Ensure the solution is efficient and can scale for datasets with a greater number of features. # Additional Information - The `sklearn.compose.ColumnTransformer` can be used to apply different transformations to different columns. - The `sklearn.preprocessing.OneHotEncoder` may be helpful for transforming categorical features. - The `sklearn.impute.SimpleImputer` can be used for imputing missing values. - The`sklearn.preprocessing.StandardScaler` can normalize numerical data, and `sklearn.decomposition.PCA` will be useful for dimensionality reduction. Good luck! Write your solution to complete the `transform_pipeline` function, ensuring it meets the requirements outlined above.","solution":"import pandas as pd from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.decomposition import PCA def transform_pipeline(data, impute_strategy, n_components): Transforms the data by imputing missing values, normalizing numerical features, encoding categorical features, and applying PCA for dimensionality reduction. Parameters: - data (pandas DataFrame): The input dataset containing numerical and/or categorical features. - impute_strategy (str): The strategy for imputing missing values, one of \'mean\', \'median\', or \'most_frequent\'. - n_components (int): The number of principal components to keep from PCA. Returns: - Transformed dataset (numpy array) with imputed, normalized, and reduced dimensions. # Separating numerical and categorical columns numeric_features = data.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = data.select_dtypes(include=[\'object\', \'category\']).columns # Defining numerical pipeline numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=impute_strategy)), (\'scaler\', StandardScaler()) ]) # Defining categorical pipeline categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combining pipelines preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Full pipeline with PCA pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'pca\', PCA(n_components=n_components)) ]) transformed_data = pipeline.fit_transform(data) return transformed_data"},{"question":"**ZIP File Operations** **Objective**: To assess your understanding and ability to use the `zipfile` module in Python for handling ZIP file operations. **Problem Statement**: You are asked to write a Python function `zip_directory_structure` that takes the following inputs: 1. `input_directory` (str): The path to the directory that needs to be zipped. 2. `output_zipfile` (str): The path where the resulting ZIP file should be saved. 3. `compression_method` (str): The compression method to be used in the ZIP file. It should support `\\"ZIP_STORED\\"`, `\\"ZIP_DEFLATED\\"`, `\\"ZIP_BZIP2\\"`, and `\\"ZIP_LZMA\\"`. The function should create a ZIP file with the specified compression method, including all files and subdirectories from the `input_directory`. Additionally, you are required to implement another function `extract_and_verify_zip` which takes the following inputs: 1. `zipfile_path` (str): The path to the ZIP file that needs to be extracted. 2. `extraction_path` (str): The path where the contents should be extracted to. 3. `verify_directory` (str): The path to the directory that was originally zipped. The function should extract the ZIP file to the specified extraction path and then verify that the extracted contents match those of the `verify_directory`. You should check that every file and directory in the `verify_directory` exists in the extracted directory with the same content. **Function Signatures**: ```python def zip_directory_structure(input_directory: str, output_zipfile: str, compression_method: str) -> None: pass def extract_and_verify_zip(zipfile_path: str, extraction_path: str, verify_directory: str) -> bool: pass ``` **Constraints**: - You can assume that the input directory and verify directory paths are valid and exist on the file system. - The `compression_method` will always be one of `\\"ZIP_STORED\\"`, `\\"ZIP_DEFLATED\\"`, `\\"ZIP_BZIP2\\"`, or `\\"ZIP_LZMA\\"`. - You can use Python standard library functions and modules. **Example**: ```python input_directory = \\"test_directory\\" output_zipfile = \\"test_archive.zip\\" compression_method = \\"ZIP_DEFLATED\\" zip_directory_structure(input_directory, output_zipfile, compression_method) zipfile_path = \\"test_archive.zip\\" extraction_path = \\"extracted_directory\\" verify_directory = \\"test_directory\\" is_verified = extract_and_verify_zip(zipfile_path, extraction_path, verify_directory) print(is_verified) # Should print True if the extraction and verification is successful ``` The `zip_directory_structure` function should correctly create a ZIP file using the specified compression method, preserving the directory structure. The `extract_and_verify_zip` function should accurately extract the contents and verify them against the original directory structure, returning `True` if they match and `False` otherwise. **Performance Requirements**: - Your solution should efficiently handle large directories with potentially thousands of files and subdirectories. - Properly manage files larger than 4 GiB using ZIP64 extensions. Ensure to handle edge cases such as empty directories, and different file types within the directories.","solution":"import os import zipfile import filecmp def zip_directory_structure(input_directory: str, output_zipfile: str, compression_method: str) -> None: compression_methods = { \\"ZIP_STORED\\": zipfile.ZIP_STORED, \\"ZIP_DEFLATED\\": zipfile.ZIP_DEFLATED, \\"ZIP_BZIP2\\": zipfile.ZIP_BZIP2, \\"ZIP_LZMA\\": zipfile.ZIP_LZMA } compression = compression_methods.get(compression_method) if compression is None: raise ValueError(f\\"Unsupported compression method: {compression_method}\\") with zipfile.ZipFile(output_zipfile, \'w\', compression=compression) as zipf: for root, dirs, files in os.walk(input_directory): for file in files: file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, input_directory) zipf.write(file_path, arcname) def extract_and_verify_zip(zipfile_path: str, extraction_path: str, verify_directory: str) -> bool: with zipfile.ZipFile(zipfile_path, \'r\') as zipf: zipf.extractall(extraction_path) return compare_directories(verify_directory, extraction_path) def compare_directories(dir1: str, dir2: str) -> bool: Compare the content of two directories. Returns True if they are the same, False otherwise. dircmp = filecmp.dircmp(dir1, dir2) if dircmp.left_only or dircmp.right_only or dircmp.funny_files: return False (_, mismatch, errors) = filecmp.cmpfiles(dir1, dir2, dircmp.common_files, shallow=False) if mismatch or errors: return False for common_dir in dircmp.common_dirs: new_dir1 = os.path.join(dir1, common_dir) new_dir2 = os.path.join(dir2, common_dir) if not compare_directories(new_dir1, new_dir2): return False return True"},{"question":"# PyTorch Distributed Rendezvous Custom Backend Implementation **Objective:** The goal of this assessment is for you to demonstrate your understanding of the PyTorch distributed rendezvous system by implementing a custom rendezvous backend and using it to test the communication setup between distributed workers. **Instructions:** 1. **Custom Rendezvous Backend**: - Implement a class `CustomRendezvousBackend` that derives from `RendezvousBackend`. - This backend should store worker endpoints and their states. - Implement the following methods: - `__init__(self, num_workers: int, parameters: dict)`: Initialize the backend with the number of workers and any additional parameters. - `set_state(self, state: dict)`: Set the state of the backend. - `get_state(self) -> dict`: Get the current state of the backend. - Other methods as required for rendezvous operations based on the `RendezvousBackend` documentation. 2. **Dynamic Rendezvous Handler**: - Use your `CustomRendezvousBackend` to create a `DynamicRendezvousHandler` using the `create_handler` method. - You need to simulate worker registries using this handler in a distributed manner. Assume you have 3 workers to be registered. 3. **Simulate Rendezvous**: - Implement a function `simulate_distributed_workers(num_workers: int, handler: RendezvousHandler)` that simulates the worker rendezvous process. - Each worker should call rendezvous using the handler, and log their state transitions. - Ensure all workers successfully perform rendezvous and log their completion. **Constraints:** - Ensure your implementation is compatible with multiple rendezvous operations. - Handle potential errors and ensure your solution is robust. - Test your implementation thoroughly to confirm its correctness. **Expected Input and Output:** - Input: - `num_workers` (int): Number of workers to simulate. - `handler` (RendezvousHandler): Instance of the dynamic rendezvous handler using your custom backend. - Output: - Log statements indicating each worker\'s state transitions. - Final state of the backend showing all workers have successfully rendezvoused. **Performance Requirements:** - The solution should efficiently handle the rendezvous process for at least 10 workers. - Ensure minimal latency in state transitions and communication between worker processes. **Example Usage:** ```python if __name__ == \\"__main__\\": backend = CustomRendezvousBackend(num_workers=3, parameters={}) handler = create_handler(backend) simulate_distributed_workers(num_workers=3, handler=handler) ``` **Note:** You will need a basic understanding of distributed systems and state management to complete this assessment. Refer to the provided documentation section for the specific API details required for the implementation.","solution":"import threading from typing import Optional, Dict class RendezvousBackend: def __init__(self, num_workers: int, parameters: dict): self.num_workers = num_workers self.parameters = parameters def set_state(self, state: dict) -> None: raise NotImplementedError def get_state(self) -> dict: raise NotImplementedError class CustomRendezvousBackend(RendezvousBackend): def __init__(self, num_workers: int, parameters: dict): super().__init__(num_workers, parameters) self.state = {} self.lock = threading.Lock() def set_state(self, state: dict) -> None: with self.lock: self.state = state def get_state(self) -> dict: with self.lock: return self.state class RendezvousHandler: def __init__(self, backend: CustomRendezvousBackend): self.backend = backend def log_state_transition(self, worker_id: int, state: str) -> None: state_data = self.backend.get_state() state_data[worker_id] = state self.backend.set_state(state_data) print(f\\"Worker {worker_id} transitioned to {state}\\") def rendezvous(self, worker_id: int) -> None: self.log_state_transition(worker_id, \\"registered\\") # Simulate some work self.log_state_transition(worker_id, \\"completed\\") def create_handler(backend: CustomRendezvousBackend) -> RendezvousHandler: return RendezvousHandler(backend) def simulate_distributed_workers(num_workers: int, handler: RendezvousHandler) -> None: threads = [] for worker_id in range(num_workers): thread = threading.Thread(target=handler.rendezvous, args=(worker_id,)) threads.append(thread) thread.start() for thread in threads: thread.join() # Final state print(\\"Final state:\\", handler.backend.get_state())"},{"question":"**Question: Implement a Custom Buffer Handling System** Python provides various ways to handle memory buffers, allowing you to manipulate data at a low level. Using your understanding of buffer protocols, including the deprecated functions for backward compatibility, design a buffer handling system in Python. # Tasks: 1. **Class Design**: Create a class `CustomBuffer` that: - Initializes with binary data. - Provides methods to read, write, and check the buffer\'s properties (size, read-only status). 2. **Methods to Implement**: - `read_buffer(start: int, length: int) -> bytes`: Return a segment of the buffer from the specified start position with the given length. - `write_buffer(start: int, data: bytes) -> None`: Write data to the buffer starting at the specified position. - `get_buffer_info() -> Tuple[int, Union[None, bool]]`: Return the total size of the buffer and a boolean indicating if it is read-only (return `None` if read-only status cannot be determined). 3. **Constraints**: - The buffer should be managed in a way that mimics the old buffer protocol styling; however, ensure it integrates smoothly with the new buffer protocol practices. - Handle possible exceptions gracefully when memory access attempts fail. # Example Usage: ```python buffer_data = b\\"Hello, World! Python310 Buffers.\\" custom_buffer = CustomBuffer(buffer_data) # Reading a segment of the buffer print(custom_buffer.read_buffer(0, 5)) # Output: b\'Hello\' # Writing to the buffer custom_buffer.write_buffer(7, b\\"Universe\\") print(custom_buffer.read_buffer(0, 15)) # Output: b\'Hello, Universe\' # Getting buffer information print(custom_buffer.get_buffer_info()) # Output: (32, False) ``` # Input: - Binary data for initializing the buffer. - For `read_buffer`, an integer start position and length. - For `write_buffer`, an integer start position and binary data. # Output: - `read_buffer` returns the specified bytes. - `write_buffer` modifies the buffer in-place and has no direct output. - `get_buffer_info` returns a tuple with the buffer size and read-only status. Your implementation should leverage Python\'s memoryview and buffer protocol comprehensions while ensuring compatibility and exception handling consistent with the described functions.","solution":"class CustomBuffer: def __init__(self, data: bytes): self._buffer = bytearray(data) # Using bytearray for mutability def read_buffer(self, start: int, length: int) -> bytes: try: return bytes(self._buffer[start:start + length]) except IndexError: raise ValueError(\\"Invalid start or length for reading the buffer\\") def write_buffer(self, start: int, data: bytes) -> None: try: end = start + len(data) self._buffer[start:end] = data except IndexError: raise ValueError(\\"Invalid start position or data length for writing the buffer\\") def get_buffer_info(self) -> tuple: size = len(self._buffer) read_only = None # bytearray is always mutable, hence read-only status is None return size, read_only"},{"question":"<|Analysis Begin|> The provided documentation covers a wide range of functionalities available in the `os` module in Python, detailing various methods related to process management, file and directory operations, environment variables, and system information retrieval. Key concepts demonstrated in the documentation include: 1. **File Operations**: Methods like `os.open`, `os.close`, `os.read`, `os.write`, `os.remove`, and `os.mkdir`. 2. **Directory Operations**: Methods like `os.listdir`, `os.scandir`, `os.chdir`, `os.getcwd`, and `os.rmdir`. 3. **Process Management**: Methods like `os.fork`, `os.exec*`, `os.kill`, `os.wait`, and `os.spawn*`. 4. **Environment Variables**: Methods like `os.getenv`, `os.putenv`, and `os.environ`. 5. **System Information**: Methods like `os.uname`, `os.cpu_count`, and `os.getloadavg`. 6. **Extended Attributes**: Methods like `os.getxattr`, `os.setxattr`, and `os.listxattr`. The documentation is comprehensive and detailed, providing sufficient information to create a challenging, clear, and self-contained coding question related to the `os` module. <|Analysis End|> <|Question Begin|> **Problem: Directory Cleanup and System Summary** You have been assigned a task to manage and monitor a directory on your Unix-based system. You need to implement a Python function that performs the following operations: 1. **Directory Cleanup**: - Remove all files in the specified directory that are older than a given number of days. - Remove any subdirectories within the directory that are empty after cleaning up the files. 2. **System Summary**: - After cleaning the directory, print a summary that includes: - The total number of files removed. - The total number of empty directories removed. - The average system load over the last 1, 5, and 15 minutes. Implement the following function: ```python import os import time def cleanup_and_summary(directory, days_old): Cleans up files in the specified directory that are older than the given number of days and removes empty subdirectories. Prints a summary including the counts of files and directories removed and the system load average. Parameters: - directory (str): The path to the directory to clean up. - days_old (int): The age threshold for files to be removed, in days. Returns: None # Your code here ``` # Input - `directory`: A string representing the path to the directory to be cleaned. - `days_old`: An integer representing the age threshold in days for files to be removed. # Output - The function does not return anything but should print the summary of the cleanup process. # Constraints - You can assume that the directory path is valid and accessible. - You can assume that the ages of files can be determined by their last modification time. # Example ```python cleanup_and_summary(\'/path/to/directory\', 30) ``` This should remove all files in `/path/to/directory` that have not been modified in the last 30 days and remove any empty directories afterward. It will then print a summary with the counts of files and directories removed and the system load average. # Notes - Use `os.path.getmtime` to get the last modification time of files. - Use `time.time` to get the current time in seconds since the epoch. - Use `os.walk` to iterate through the directory. - Use `os.remove` to delete files. - Use `os.rmdir` to delete empty directories. - Use `os.getloadavg` to get the system load average. Consider using helper functions to organize the code for clarity and modularity.","solution":"import os import time def cleanup_and_summary(directory, days_old): Cleans up files in the specified directory that are older than the given number of days and removes empty subdirectories. Prints a summary including the counts of files and directories removed and the system load average. Parameters: - directory (str): The path to the directory to clean up. - days_old (int): The age threshold for files to be removed, in days. Returns: None current_time = time.time() age_threshold = days_old * 86400 # convert days to seconds files_removed_count = 0 dirs_removed_count = 0 for root, dirs, files in os.walk(directory, topdown=False): for file in files: file_path = os.path.join(root, file) if current_time - os.path.getmtime(file_path) > age_threshold: os.remove(file_path) files_removed_count += 1 for d in dirs: dir_path = os.path.join(root, d) if not os.listdir(dir_path): # check if directory is empty os.rmdir(dir_path) dirs_removed_count += 1 load_avg = os.getloadavg() print(f\\"Files removed: {files_removed_count}\\") print(f\\"Empty directories removed: {dirs_removed_count}\\") print(f\\"System load average (1, 5, 15 minutes): {load_avg}\\")"},{"question":"**Question:** You are tasked with creating a documentation tool that provides detailed information about a given Python module. Your tool should print out the module name, all functions within the module, and for each function, it should display the function name, its signature, and its documentation string (docstring), if available. # Objective: Implement a function `document_module_details(module_path: str) -> None` that prints the detailed structure of a given Python module. # Input: - `module_path` (str): A string representing the file path of the Python module. # Output: Your function should print: 1. The name of the module. 2. For each function in the module: - The function name. - The function signature. - The function\'s docstring, if available. # Constraints: - Assume that the module is a valid Python module file. - Your function should handle the absence of docstrings gracefully. - Maintain the order of functions as they appear in the module. # Example: Consider the following module defined at `example_module.py`: ```python Example module for testing. def add(a, b): Returns the sum of a and b. return a + b def subtract(a, b): return a - b ``` For the above module, calling `document_module_details(\'example_module.py\')` should output: ``` Module Name: example_module Function Name: add Signature: (a, b) Docstring: Returns the sum of a and b. Function Name: subtract Signature: (a, b) Docstring: No docstring provided. ``` # Notes: - Use the `inspect` module for this task. - You may find the `inspect.getmembers`, `inspect.isfunction`, `inspect.signature`, and `inspect.getdoc` functions particularly useful. **Function Signature:** ```python def document_module_details(module_path: str) -> None: pass ```","solution":"import inspect import importlib.util import os def document_module_details(module_path: str) -> None: # Load the module from the given file path module_name = os.path.splitext(os.path.basename(module_path))[0] spec = importlib.util.spec_from_file_location(module_name, module_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) print(f\\"Module Name: {module_name}\\") for name, func in inspect.getmembers(module, inspect.isfunction): print(f\\"Function Name: {name}\\") print(f\\"Signature: {inspect.signature(func)}\\") docstring = inspect.getdoc(func) if docstring: print(f\\"Docstring: {docstring}\\") else: print(\\"Docstring: No docstring provided.\\") print()"},{"question":"**PyArrow Integration with pandas: Coding Assessment** # Objective Demonstrate your understanding of using the PyArrow library with pandas to create and manipulate data structures, perform efficient operations, and handle IO operations. # Problem Statement You are given a set of data transactions that you need to process and analyze using pandas with PyArrow for better performance and handling of data types. Your task is to: 1. Create a `DataFrame` with the following data using appropriate PyArrow data types: - Column `transaction_id`: List of integers (`pa.int32()`) - Column `amount`: List of decimal numbers with precision (`3`) and scale (`2`) (`pa.decimal128(3, 2)`) - Column `status`: List of strings (`pa.string()`) - Column `timestamp`: List of datetime values (`pa.timestamp(\'ns\')`) 2. Fill any missing values in the `amount` column with `0` and in the `status` column with `\\"unknown\\"`. 3. Calculate the following: - Total number of transactions. - Average transaction amount. - Number of transactions with status `\\"completed\\"`. 4. Convert the above-created DataFrame to a PyArrow Table and write it to a file named `processed_transactions.parquet` using the `write_table` method with PyArrow. # Constraints - You must use PyArrow-backed data types for the DataFrame. - Ensure that missing values are correctly handled and filled. # Input The data for creating the DataFrame: ```python data = { \'transaction_id\': [1, 2, 3, 4, 5], \'amount\': [Decimal(\'15.25\'), Decimal(\'23.50\'), None, Decimal(\'9.75\'), Decimal(\'13.00\')], \'status\': [\'completed\', \'pending\', \'completed\', None, \'failed\'], \'timestamp\': [datetime(2023, 1, 1, 10, 0), datetime(2023, 1, 2, 11, 30), datetime(2023, 1, 3, 13, 15), datetime(2023, 1, 4, 14, 45), None] } ``` # Expected Output Format Print the results for each calculation step-by-step: 1. Total number of transactions. 2. Average transaction amount. 3. Number of transactions with status `\\"completed\\"`. Ensure the DataFrame is written to the file `processed_transactions.parquet`. # Implementation ```python import pandas as pd import pyarrow as pa from decimal import Decimal from datetime import datetime # Part 1: Creating the DataFrame with PyArrow-backed data types data = { \'transaction_id\': [1, 2, 3, 4, 5], \'amount\': [Decimal(\'15.25\'), Decimal(\'23.50\'), None, Decimal(\'9.75\'), Decimal(\'13.00\')], \'status\': [\'completed\', \'pending\', \'completed\', None, \'failed\'], \'timestamp\': [datetime(2023, 1, 1, 10, 0), datetime(2023, 1, 2, 11, 30), datetime(2023, 1, 3, 13, 15), datetime(2023, 1, 4, 14, 45), None] } # Define PyArrow data types transaction_id_type = pa.int32() amount_type = pa.decimal128(3, 2) status_type = pa.string() timestamp_type = pa.timestamp(\'ns\') # Convert data to DataFrame df = pd.DataFrame(data) df = df.astype({ \'transaction_id\': transaction_id_type, \'amount\': amount_type, \'status\': status_type, \'timestamp\': timestamp_type }) # Part 2: Handling missing values df[\'amount\'].fillna(Decimal(\'0.00\'), inplace=True) df[\'status\'].fillna(\'unknown\', inplace=True) # Part 3: Calculations total_transactions = df.shape[0] average_amount = df[\'amount\'].mean() completed_transactions = df[df[\'status\'] == \'completed\'].shape[0] print(f\\"Total number of transactions: {total_transactions}\\") print(f\\"Average transaction amount: {average_amount}\\") print(f\\"Number of transactions with status \'completed\': {completed_transactions}\\") # Part 4: Writing the DataFrame to a Parquet file table = pa.Table.from_pandas(df, preserve_index=False) pa.parquet.write_table(table, \'processed_transactions.parquet\') ```","solution":"import pandas as pd import pyarrow as pa from pyarrow import parquet from datetime import datetime from decimal import Decimal def process_transactions(data): # Define PyArrow data types transaction_id_type = pa.int32() amount_type = pa.decimal128(3, 2) status_type = pa.string() timestamp_type = pa.timestamp(\'ns\') # Convert data to DataFrame with PyArrow types df = pd.DataFrame(data) df = df.astype({ \'transaction_id\': \'int32\', \'amount\': \'object\', \'status\': \'string\', \'timestamp\': \'datetime64[ns]\' }) # Part 2: Handling missing values df[\'amount\'] = df[\'amount\'].apply(lambda x: Decimal(\'0.00\') if pd.isna(x) else x) df[\'status\'].fillna(\'unknown\', inplace=True) # Part 3: Calculations total_transactions = df.shape[0] average_amount = df[\'amount\'].mean() completed_transactions = df[df[\'status\'] == \'completed\'].shape[0] print(f\\"Total number of transactions: {total_transactions}\\") print(f\\"Average transaction amount: {average_amount}\\") print(f\\"Number of transactions with status \'completed\': {completed_transactions}\\") # Part 4: Writing the DataFrame to a Parquet file table = pa.Table.from_pandas(df, preserve_index=False) parquet.write_table(table, \'processed_transactions.parquet\') return total_transactions, average_amount, completed_transactions # Example data input data = { \'transaction_id\': [1, 2, 3, 4, 5], \'amount\': [Decimal(\'15.25\'), Decimal(\'23.50\'), None, Decimal(\'9.75\'), Decimal(\'13.00\')], \'status\': [\'completed\', \'pending\', \'completed\', None, \'failed\'], \'timestamp\': [datetime(2023, 1, 1, 10, 0), datetime(2023, 1, 2, 11, 30), datetime(2023, 1, 3, 13, 15), datetime(2023, 1, 4, 14, 45), None] }"},{"question":"You are required to develop a machine learning pipeline using scikit-learn. Your task involves the following steps: 1. **Fetch the Dataset**: Use the `fetch_20newsgroups` dataset. This dataset comprises various newsgroup documents, partitioned across 20 different classes. 2. **Preprocess the Data**: - Convert the text documents into feature vectors using `TfidfVectorizer`. - Perform data splitting to create training and testing subsets. 3. **Model Training and Evaluation**: - Train a Naive Bayes classifier on the training data. - Evaluate the classifier\'s performance on the testing data and report the accuracy score. # Function Signature ```python def news_classification(): Fetch the \'20 newsgroups\' dataset and perform the following tasks: 1. Transform the text data into TF-IDF feature vectors. 2. Split the data into training and testing sets. 3. Train a Naive Bayes classifier on the training set. 4. Evaluate and return the accuracy of the classifier on the testing set. Returns: float: accuracy of the classifier on the testing data. pass ``` # Constraints - You must use the `fetch_20newsgroups` function to load the dataset. - Use `TfidfVectorizer` from `sklearn.feature_extraction.text` for transforming text data. - Use `train_test_split` from `sklearn.model_selection` for splitting data into training and testing sets. - Use `MultinomialNB` from `sklearn.naive_bayes` for the Naive Bayes classifier. - Report the accuracy score on the testing dataset obtained from `accuracy_score` in `sklearn.metrics`. # Example Usage ```python accuracy = news_classification() print(f\\"Classifier accuracy: {accuracy}\\") ``` This should output the accuracy of the trained Naive Bayes classifier on the test dataset.","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def news_classification(): Fetch the \'20 newsgroups\' dataset and perform the following tasks: 1. Transform the text data into TF-IDF feature vectors. 2. Split the data into training and testing sets. 3. Train a Naive Bayes classifier on the training set. 4. Evaluate and return the accuracy of the classifier on the testing set. Returns: float: accuracy of the classifier on the testing data. # Fetch the dataset news_data = fetch_20newsgroups(subset=\'all\', remove=(\'headers\', \'footers\', \'quotes\')) # Convert the text data into TF-IDF feature vectors vectorizer = TfidfVectorizer(stop_words=\'english\', max_df=0.5) X = vectorizer.fit_transform(news_data.data) y = news_data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) # Train the Naive Bayes classifier clf = MultinomialNB() clf.fit(X_train, y_train) # Predict the class labels for the test set y_pred = clf.predict(X_test) # Calculate the accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Objective: Create a custom container class that emulates the behavior of a Python list but also provides additional functionality for getting multiple slices at once. Description: You are required to design a class `MultiSliceList` that emulates the behavior of a Python list with some additional functionalities. The class should support the following operations: 1. The class should allow initialization with an iterable. 2. It should support accessing elements via index and slicing. 3. Extend the slicing operation to accept a list of slice objects and return a list of slices. 4. Implement other common list methods like `append`, `extend`, `pop`, `remove`, `clear`, and `__len__`. Specifications: - **Constructor**: The class should be initialized with an iterable. - **Indexing and Slicing**: Implement `__getitem__`, `__setitem__`, and `__delitem__` to support indexing and slicing. - **Multiple Slices**: Add a method `multi_slice` that takes a list of slice objects and returns a list of lists (slices). - **Common List Methods**: Implement methods - `append`, `extend`, `pop`, `remove`, `clear`, and `__len__`. Function Signatures: ```python class MultiSliceList: def __init__(self, iterable): # Initialize with elements from any iterable pass def __getitem__(self, index): # Provide indexing and slicing pass def __setitem__(self, index, value): # Set the item at a given index pass def __delitem__(self, index): # Delete the item at a given index pass def append(self, value): # Append an element pass def extend(self, iterable): # Extend the list with elements from the iterable pass def pop(self, index=-1): # Remove and return item at index (default last) pass def remove(self, value): # Remove first occurrence of value pass def clear(self): # Clear all elements pass def __len__(self): # Return the length of the list pass def multi_slice(self, slice_list): # Return a list of slices given a list of slice objects pass def __repr__(self): # Return a string representation of the list pass ``` Example: ```python msl = MultiSliceList([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) print(msl[:5]) # Output: [1, 2, 3, 4, 5] print(msl.multi_slice([slice(1, 4), slice(5, 7)])) # Output: [[2, 3, 4], [6, 7]] msl.append(11) print(msl) # Output: MultiSliceList([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) ``` **Constraints:** - Do not use any external libraries; rely only on the Python standard library. - Ensure that the class methods mimic the behavior of Python lists as closely as possible. - The class should handle edge cases gracefully, such as negative indices and out-of-bounds indices. Notes: - This task will test your understanding of Python\'s data model, including the implementation of container types, special methods, and handling sequences. - Pay careful attention to object mutability and the behavior of different operations on the list.","solution":"class MultiSliceList: def __init__(self, iterable): # Initialize with elements from any iterable self._data = list(iterable) def __getitem__(self, index): # Provide indexing and slicing return self._data[index] def __setitem__(self, index, value): # Set the item at a given index self._data[index] = value def __delitem__(self, index): # Delete the item at a given index del self._data[index] def append(self, value): # Append an element self._data.append(value) def extend(self, iterable): # Extend the list with elements from the iterable self._data.extend(iterable) def pop(self, index=-1): # Remove and return item at index (default last) return self._data.pop(index) def remove(self, value): # Remove first occurrence of value self._data.remove(value) def clear(self): # Clear all elements self._data.clear() def __len__(self): # Return the length of the list return len(self._data) def multi_slice(self, slice_list): # Return a list of slices given a list of slice objects return [self._data[s] for s in slice_list] def __repr__(self): # Return a string representation of the list return f\'MultiSliceList({self._data})\'"},{"question":"Context: You are tasked with designing a specialized class in CPython that supports both `tp_call` and vectorcall protocols for efficient function invocation. The class should be callable and able to handle both positional and keyword arguments. Objective: Implement a Python C extension class in CPython that: - Supports both `tp_call` and vectorcall protocols. - Demonstrates efficient calls by leveraging vectorcall where possible. - Handles recursion control for vectorcall correctly. - Implements a callable method that performs a simple task (e.g., summing positional arguments). Requirements: 1. **Class Definition**: - The class should be named `EfficientCallable`. - It should support both `tp_call` and vectorcall protocols. 2. **Initialization**: - The callable method should accept any number of positional arguments and an optional `scale` keyword argument (default is 1). 3. **Callable Method**: - The method should sum all the positional arguments and multiply the result by `scale`. 4. **Efficiency Considerations**: - Use vectorcall wherever possible for efficiency. - Ensure recursion control for vectorcall using `Py_EnterRecursiveCall` and `Py_LeaveRecursiveCall`. Input and Output: - The callable class accepts a list of integers as positional arguments and a single optional integer keyword argument `scale`. - It returns the product of the sum of the positional arguments and the `scale`. Constraints: - Implement the class in CPython. - Ensure the class is properly set up to handle the calling protocols. Example: ```python # Assume the class is implemented and compiled as EfficientCallable import efficient_call callable_instance = efficient_call.EfficientCallable() # Example call to the callable instance result = callable_instance(1, 2, 3, scale=2) print(result) # Expected output: 12 (since (1 + 2 + 3) * 2 = 12) ``` Bonus: - Provide a comprehensive test suite that validates various scenarios, ensuring correctness and efficiency of the implementation.","solution":"# Assuming implementation in C for CPython extension: # Here is a hypothetical Python equivalent to demonstrate the desired class behavior class EfficientCallable: def __init__(self): pass def __call__(self, *args, scale=1): sum_args = sum(args) return sum_args * scale"},{"question":"# Advanced Coding Assessment Question Topic: Process-based Parallelism with `multiprocessing` **Objective:** In this assignment, you are required to demonstrate your understanding of the `multiprocessing` module and its functionalities by implementing a parallelized computation system. **Problem Statement:** You are given the task to compute the frequency of each word in a large text file using multiple processes. The computation should be parallelized to efficiently manage the workload. **Requirements:** 1. Create a function `count_words(file_path)` that: - Reads the text file from the given `file_path`. - Splits the text into chunks where each chunk contains approximately the same number of words. - Uses multiple processes to compute the frequency of each word in these chunks concurrently. - Merges the results from all processes to produce a final word frequency dictionary. 2. Implement inter-process communication to combine the partial results from each process. 3. You must use the following `multiprocessing` components: - `Process` class to create and manage processes. - A suitable method for inter-process communication, like `Queue` or `Manager`. - Synchronization primal method if needed, e.g., `Lock`. **Input:** - `file_path`: A string representing the path to a text file. **Output:** - A dictionary where keys are words, and values are their frequencies. **Constraints:** - The text file may be very large, so ensure memory efficiency. - Handle punctuation and case sensitivity as follows: words should be counted case-insensitively, and punctuation should be ignored. **Performance Requirements:** - The solution should efficiently utilize the available CPU cores. - Aim to minimize the execution time by appropriately balancing the workload across processes. **Example:** ```python # Assuming the text file contains: \\"Hello world. Hello everyone. Welcome to the world of programming.\\" # The output should be: { \\"hello\\": 2, \\"world\\": 2, \\"everyone\\": 1, \\"welcome\\": 1, \\"to\\": 1, \\"the\\": 1, \\"of\\": 1, \\"programming\\": 1 } ``` **Template:** ```python from multiprocessing import Process, Queue def count_words(file_path): # Step 1: Read and chunk the file with open(file_path, \'r\') as file: text = file.read().lower() # Remove punctuation and split into words words = re.findall(r\'bw+b\', text) chunk_size = len(words) // number_of_chunks # Define how to chunk words # Step 2: Create and start processes def worker(words_chunk, queue): # Compute word frequency in the chunk and put the result into the queue pass # Step 3: Merge results pass # Usage file_path = \'path/to/your/textfile.txt\' word_frequencies = count_words(file_path) print(word_frequencies) ```","solution":"from multiprocessing import Process, Queue import re from collections import Counter import os def worker(words_chunk, queue): word_freq = Counter(words_chunk) queue.put(word_freq) def count_words(file_path): cpu_count = os.cpu_count() queue = Queue() with open(file_path, \'r\') as file: text = file.read().lower() words = re.findall(r\'bw+b\', text) chunk_size = len(words) // cpu_count processes = [] for i in range(cpu_count): start_index = i * chunk_size end_index = None if i == cpu_count - 1 else (i + 1) * chunk_size chunk = words[start_index:end_index] process = Process(target=worker, args=(chunk, queue)) process.start() processes.append(process) for process in processes: process.join() final_counter = Counter() while not queue.empty(): chunk_counter = queue.get() final_counter.update(chunk_counter) return dict(final_counter) # Usage # file_path = \'path/to/your/textfile.txt\' # word_frequencies = count_words(file_path) # print(word_frequencies)"},{"question":"# Python Coding Assessment Question **Objective:** The task is to implement a Python function demonstrating the use of different types of assignment statements as described in the document. **Problem Statement:** Your task is to implement a function named `process_assignments` which performs a series of assignments using different types of assignments described below. The function should return a dictionary containing the results of these operations. **Implementation Details:** 1. **Standard Assignment**: Assign the value `5` to a variable `a`. 2. **Multiple Assignment**: Assign the values `8` and `3` to variables `b` and `c` respectively, using a single statement. 3. **Unpacking Assignment**: Unpack a list `[1, 2, 3, 4]` into variables `d`, `e`, `f`, and `g`. 4. **Starred Assignment**: Assign the first two values to variables `h` and `i`, and the rest to a list `j` from the list `[10, 20, 30, 40, 50]`. 5. **Augmented Assignment**: Increment the value of `k` by `7`. Initially, `k` should be set to `10`. 6. **Annotated Assignment**: Assign the value `\'hello\'` to a variable `m` and annotate its type as `str` using annotated assignment. **Input:** - The function takes no inputs. **Output:** - A dictionary with the variables after performing the assignments. The keys should be the variable names and the values should be their corresponding assigned values. **Example:** ```python def process_assignments(): # Your implementation goes here # Example execution result = process_assignments() assert result == { \'a\': 5, \'b\': 8, \'c\': 3, \'d\': 1, \'e\': 2, \'f\': 3, \'g\': 4, \'h\': 10, \'i\': 20, \'j\': [30, 40, 50], \'k\': 17, \'m\': \'hello\', } print(\\"All assignments are correct!\\") ``` **Constraints:** - You should use the appropriate type of assignment as instructed. - Ensure that the function returns a dictionary as specified. Implement the `process_assignments` function to meet the requirements given above.","solution":"def process_assignments(): # Standard Assignment a = 5 # Multiple Assignment b, c = 8, 3 # Unpacking Assignment d, e, f, g = [1, 2, 3, 4] # Starred Assignment h, i, *j = [10, 20, 30, 40, 50] # Augmented Assignment k = 10 k += 7 # Annotated Assignment m: str = \'hello\' return { \'a\': a, \'b\': b, \'c\': c, \'d\': d, \'e\': e, \'f\': f, \'g\': g, \'h\': h, \'i\': i, \'j\': j, \'k\': k, \'m\': m, }"},{"question":"# SGDClassifier Implementation and Evaluation In this assessment, you are required to implement a machine learning pipeline using the `SGDClassifier` from scikit-learn. The pipeline will include data preprocessing, model training, hyperparameter tuning, and evaluation. Task Description 1. **Data Preprocessing**: - Load the provided dataset (assume it is a CSV file named `data.csv`), which contains tabular data with features and a target variable. - Split the data into training and test sets (80% training, 20% test). - Standardize the features using `StandardScaler`. 2. **Model Training**: - Implement an `SGDClassifier` with a `log_loss` loss function and `l2` penalty. - Use a grid search to tune the hyperparameters: `alpha` (from [1e-4, 1e-3, 1e-2, 1e-1, 1.0]), `max_iter` (from [1000, 2000, 3000]), and `learning_rate` (from [\'constant\', \'optimal\']). 3. **Evaluation**: - Evaluate the model on the test set using accuracy, precision, recall, and F1-score. - Output the best combination of hyperparameters and the corresponding evaluation metrics. 4. **Advanced Challenge** (Optional): - Implement a variation of the pipeline that uses Averaged SGD (ASGD) by setting `average=True`. - Compare the performance of the standard SGD and averaged SGD models. Expected Input and Output - **Input**: A CSV file named `data.csv` containing the dataset. - **Output**: 1. Preprocessed data. 2. Best hyperparameters obtained through grid search. 3. Evaluation metrics (accuracy, precision, recall, F1-score) for the best model. 4. (Optional) Comparison of standard SGD and averaged SGD models. Constraints - You should use the `SGDClassifier` class from scikit-learn. - Implement the solution within the limits of computational resources typically available in a classroom environment. - Use appropriate scikit-learn functions for data splitting, scaling, grid search, and evaluation. Solution Template ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load the dataset data = pd.read_csv(\'data.csv\') X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the pipeline pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'l2\')) # Define the parameter grid param_grid = { \'sgdclassifier__alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1.0], \'sgdclassifier__max_iter\': [1000, 2000, 3000], \'sgdclassifier__learning_rate\': [\'constant\', \'optimal\'] } # Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) # Get the best model best_model = grid_search.best_estimator_ # Evaluate the model y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') print(\\"Best Hyperparameters:\\", grid_search.best_params_) print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\") # Optional: Implement and compare Averaged SGD pipeline_avg = make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'l2\', average=True)) grid_search_avg = GridSearchCV(pipeline_avg, param_grid, cv=5, scoring=\'accuracy\') grid_search_avg.fit(X_train, y_train) best_model_avg = grid_search_avg.best_estimator_ # Evaluate the averaged model y_pred_avg = best_model_avg.predict(X_test) accuracy_avg = accuracy_score(y_test, y_pred_avg) precision_avg = precision_score(y_test, y_pred_avg, average=\'weighted\') recall_avg = recall_score(y_test, y_pred_avg, average=\'weighted\') f1_avg = f1_score(y_test, y_pred_avg, average=\'weighted\') print(\\"Averaged SGD Model:\\") print(f\\"Accuracy: {accuracy_avg}\\") print(f\\"Precision: {precision_avg}\\") print(f\\"Recall: {recall_avg}\\") print(f\\"F1 Score: {f1_avg}\\") ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def run_sgd_pipeline(data_path): # Load the dataset data = pd.read_csv(data_path) X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the pipeline pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'l2\')) # Define the parameter grid param_grid = { \'sgdclassifier__alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1.0], \'sgdclassifier__max_iter\': [1000, 2000, 3000], \'sgdclassifier__learning_rate\': [\'constant\', \'optimal\'] } # Perform grid search grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) # Get the best model best_model = grid_search.best_estimator_ # Evaluate the model y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'weighted\') recall = recall_score(y_test, y_pred, average=\'weighted\') f1 = f1_score(y_test, y_pred, average=\'weighted\') results = { \\"Best Hyperparameters\\": grid_search.best_params_, \\"Accuracy\\": accuracy, \\"Precision\\": precision, \\"Recall\\": recall, \\"F1 Score\\": f1 } return results"},{"question":"You are provided with two datasets: \\"tips\\" and \\"fmri\\". Your task is to create visualizations using `seaborn.relplot` to analyze these datasets. Implement the following functions: 1. `create_scatter_plot(data)` 2. `create_line_plot(data)` # Function Specifications 1. **Function `create_scatter_plot(data)`**: - **Input**: - `data`: a `pandas.DataFrame` object; the \\"tips\\" dataset. - **Output**: - A scatter plot using `seaborn.relplot` with the following specifications: - `x` should be \\"total_bill\\". - `y` should be \\"tip\\". - Use `day` for the `hue` semantic. - Facet the plot across the columns by `time`. - Add a semantic mapping of `size` using the \\"size\\" column. - **Example**: ```python tips = sns.load_dataset(\\"tips\\") create_scatter_plot(tips) ``` - **Constraints**: - You must use `seaborn.relplot` for creating the plot without using any other plotting functions. 2. **Function `create_line_plot(data)`**: - **Input**: - `data`: a `pandas.DataFrame` object; the \\"fmri\\" dataset. - **Output**: - A line plot using `seaborn.relplot` with the following specifications: - `x` should be \\"timepoint\\". - `y` should be \\"signal\\". - Use `event` for both the `hue` and `style` semantics. - Facet the plot across the columns by `region`. - Set the `height` of each individual facet to 4 and the `aspect` ratio to 0.7. - **Example**: ```python fmri = sns.load_dataset(\\"fmri\\") create_line_plot(fmri) ``` - **Constraints**: - You must use `seaborn.relplot` for creating the plot without using any other plotting functions. Your implementation should create the required plots, and any additional customizations should enhance the readability and interpretability of the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(data): Create a scatter plot for the \'tips\' dataset. Parameters: data (pandas.DataFrame): The \'tips\' dataset. Returns: None; displays the plot. sns.relplot( x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", size=\\"size\\", col=\\"time\\", data=data, kind=\\"scatter\\" ) plt.show() def create_line_plot(data): Create a line plot for the \'fmri\' dataset. Parameters: data (pandas.DataFrame): The \'fmri\' dataset. Returns: None; displays the plot. sns.relplot( x=\\"timepoint\\", y=\\"signal\\", hue=\\"event\\", style=\\"event\\", col=\\"region\\", data=data, kind=\\"line\\", height=4, aspect=0.7 ) plt.show()"},{"question":"**Objective:** Demonstrate understanding of color space conversions using the `colorsys` module. **Problem Statement:** Write a Python function `verify_color_conversions(rgb_colors)` that accepts a list of RGB color tuples and performs the following tasks: 1. Convert each RGB color to YIQ, then convert it back to RGB. 2. Convert each RGB color to HLS, then convert it back to RGB. 3. Convert each RGB color to HSV, then convert it back to RGB. 4. Validate that the resulting RGB values (after conversions) closely match the original RGB values within a tolerance of (10^{-3}). **Function Signature:** ```python def verify_color_conversions(rgb_colors: List[Tuple[float, float, float]]) -> bool: pass ``` **Inputs:** - `rgb_colors` (List[Tuple[float, float, float]]): A list containing tuples of three floating-point numbers (r, g, b), each within the range of [0, 1]. **Outputs:** - Returns `True` if all the converted RGB values match the original RGB values within the tolerance limit for all colors, otherwise `False`. **Example:** ```python import colorsys rgb_colors = [(0.2, 0.4, 0.4), (0.6, 0.3, 0.1)] output = verify_color_conversions(rgb_colors) # Expected: True ``` **Constraints:** 1. Use the functions provided by the `colorsys` module for conversions. 2. Match the RGB values within a tolerance of (10^{-3}). 3. Handle edge cases where the input list is empty or contains invalid RGB values. **Expectation:** The solution should test comprehension of bidirectional color space conversions and attention to precision and handling of floating-point comparisons in Python.","solution":"import colorsys from typing import List, Tuple def verify_color_conversions(rgb_colors: List[Tuple[float, float, float]]) -> bool: tolerance = 1e-3 for r, g, b in rgb_colors: # Convert RGB to YIQ and back yiq = colorsys.rgb_to_yiq(r, g, b) rgb_yiq = colorsys.yiq_to_rgb(*yiq) # Convert RGB to HLS and back hls = colorsys.rgb_to_hls(r, g, b) rgb_hls = colorsys.hls_to_rgb(*hls) # Convert RGB to HSV and back hsv = colorsys.rgb_to_hsv(r, g, b) rgb_hsv = colorsys.hsv_to_rgb(*hsv) # Compare the original and converted RGB values if not ( all(abs(c1 - c2) <= tolerance for c1, c2 in zip((r, g, b), rgb_yiq)) and all(abs(c1 - c2) <= tolerance for c1, c2 in zip((r, g, b), rgb_hls)) and all(abs(c1 - c2) <= tolerance for c1, c2 in zip((r, g, b), rgb_hsv)) ): return False return True"},{"question":"# Covariance Estimation: A Practical Implementation The goal of this assessment is to evaluate your understanding and application of different covariance estimation techniques provided by the `sklearn.covariance` package. Problem Statement You are given a dataset containing several features extracted from observations. Your task is to implement multiple covariance estimation techniques to analyze this dataset and compare their performances. Requirements 1. **Load and Preprocess the Data** - Load the dataset (provided as a CSV file) and preprocess it by handling missing values and scaling the features. 2. **Implement Covariance Estimations** - Compute the covariance matrix of the dataset using the following methods: - Empirical Covariance - Shrunk Covariance with a shrinkage coefficient of 0.1 - Ledoit-Wolf Shrinkage - Oracle Approximating Shrinkage (OAS) - Sparse inverse covariance using GraphicalLasso with an alpha of 0.1 3. **Evaluate and Compare** - Plot the covariance matrices obtained from the different methods. - Compare the performance of these methods in terms of mean squared error (MSE) against a \\"true\\" covariance matrix (if provided) or using cross-validation. # Dataset - Use a synthetic dataset or any well-known dataset like the Iris dataset from sklearn. # Functions to Implement 1. **load_and_preprocess_data(file_path)** - **Input**: `file_path` (str) - Path to the CSV file containing the dataset. - **Output**: Preprocessed data as a Pandas DataFrame. 2. **compute_empirical_covariance(data)** - **Input**: `data` (Pandas DataFrame) - Preprocessed dataset. - **Output**: Covariance matrix as a NumPy array. 3. **compute_shrunk_covariance(data, shrinkage)** - **Input**: - `data` (Pandas DataFrame) - Preprocessed dataset. - `shrinkage` (float) - Shrinkage coefficient. - **Output**: Covariance matrix as a NumPy array. 4. **compute_ledoit_wolf_covariance(data)** - **Input**: `data` (Pandas DataFrame) - Preprocessed dataset. - **Output**: Covariance matrix as a NumPy array. 5. **compute_oas_covariance(data)** - **Input**: `data` (Pandas DataFrame) - Preprocessed dataset. - **Output**: Covariance matrix as a NumPy array. 6. **compute_sparse_inverse_covariance(data, alpha)** - **Input**: - `data` (Pandas DataFrame) - Preprocessed dataset. - `alpha` (float) - Regularization parameter. - **Output**: Covariance matrix as a NumPy array. # Constraints - Use appropriate parameter values for each method. - Ensure that the computations are numerically stable. - Handle any potential issues with data scaling and missing values. # Performance Requirements - Implement efficient computations suitable for large datasets. - Use libraries like `numpy`, `pandas`, and `sklearn` where necessary. # Example ```python import pandas as pd import numpy as np from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso) def load_and_preprocess_data(file_path): data = pd.read_csv(file_path) # Preprocess steps: handle missing values, scale features, etc. return data def compute_empirical_covariance(data): emp_cov = EmpiricalCovariance().fit(data) return emp_cov.covariance_ def compute_shrunk_covariance(data, shrinkage): shr_cov = ShrunkCovariance(shrinkage=shrinkage).fit(data) return shr_cov.covariance_ def compute_ledoit_wolf_covariance(data): lw_cov = LedoitWolf().fit(data) return lw_cov.covariance_ def compute_oas_covariance(data): oas_cov = OAS().fit(data) return oas_cov.covariance_ def compute_sparse_inverse_covariance(data, alpha): gl = GraphicalLasso(alpha=alpha).fit(data) return gl.covariance_ # Example of usage file_path = \'path_to_csv_file.csv\' data = load_and_preprocess_data(file_path) emp_cov = compute_empirical_covariance(data) shr_cov = compute_shrunk_covariance(data, shrinkage=0.1) lw_cov = compute_ledoit_wolf_covariance(data) oas_cov = compute_oas_covariance(data) sparse_cov = compute_sparse_inverse_covariance(data, alpha=0.1) # Plot and compare covariance matrices ``` Evaluation Submit your code implementation, the resulting plots, and a brief report comparing the methods based on your findings.","solution":"import pandas as pd import numpy as np from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso) from sklearn.preprocessing import StandardScaler def load_and_preprocess_data(file_path): Load the dataset from a CSV file and preprocess it: handle missing values and scale features. data = pd.read_csv(file_path) data = data.dropna() # simple dropping of missing values; you might want to do a more sophisticated handling scaler = StandardScaler() data_scaled = scaler.fit_transform(data) return pd.DataFrame(data_scaled, columns=data.columns) def compute_empirical_covariance(data): emp_cov = EmpiricalCovariance().fit(data) return emp_cov.covariance_ def compute_shrunk_covariance(data, shrinkage): shr_cov = ShrunkCovariance(shrinkage=shrinkage).fit(data) return shr_cov.covariance_ def compute_ledoit_wolf_covariance(data): lw_cov = LedoitWolf().fit(data) return lw_cov.covariance_ def compute_oas_covariance(data): oas_cov = OAS().fit(data) return oas_cov.covariance_ def compute_sparse_inverse_covariance(data, alpha): gl = GraphicalLasso(alpha=alpha).fit(data) return gl.covariance_"},{"question":"# Question: Directory Synchronization Utility Problem Statement You are required to implement a function called `sync_directories` that synchronizes the contents of a source directory with a destination directory. The function should ensure that: 1. All files and subdirectories from the source directory are copied to the destination directory. 2. Files in the destination directory that are not present in the source directory are removed. 3. File metadata (permissions, timestamps) should be preserved. Function Signature ```python def sync_directories(src: str, dst: str) -> None: pass ``` # Input - `src` (str): The path to the source directory. - `dst` (str): The path to the destination directory. # Output The function should not return anything. It should just ensure that the destination directory mirrors the source directory in terms of content and metadata. Constraints - Assume both `src` and `dst` are valid directories. - You may assume that you have appropriate permissions to read and write files and directories. - Do not use any third-party libraries. Use only the Python standard library. Example Consider the following directory structure in `src`: ``` src â”œâ”€â”€ file1.txt â”œâ”€â”€ file2.txt â””â”€â”€ subdir â””â”€â”€ file3.txt ``` And assume `dst` has the following structure: ``` dst â”œâ”€â”€ file4.txt â””â”€â”€ subdir2 â””â”€â”€ file5.txt ``` After running `sync_directories(src, dst)`, `dst` should have the following structure: ``` dst â”œâ”€â”€ file1.txt â”œâ”€â”€ file2.txt â””â”€â”€ subdir â””â”€â”€ file3.txt ``` # Implementation Hints - Consider using `shutil.copy2()` or `shutil.copytree()` for copying files and directories while preserving metadata. - Utilize `shutil.rmtree()` to remove directories that are not in the source. - Use `os.walk()` to traverse the source and destination directories.","solution":"import os import shutil from filecmp import cmp def sync_directories(src: str, dst: str) -> None: # Ensure destination directory exists if not os.path.exists(dst): os.makedirs(dst) # Traverse the source directory and copy files/directories to destination for root, dirs, files in os.walk(src): # Compute the corresponding path in destination relative_path = os.path.relpath(root, src) dst_root = os.path.join(dst, relative_path) # Ensure directories exist in the destination for dir_name in dirs: dst_dir_path = os.path.join(dst_root, dir_name) if not os.path.exists(dst_dir_path): os.makedirs(dst_dir_path) # Copy files and preserve metadata for file_name in files: src_file_path = os.path.join(root, file_name) dst_file_path = os.path.join(dst_root, file_name) if not os.path.exists(dst_file_path) or not cmp(src_file_path, dst_file_path, shallow=False): shutil.copy2(src_file_path, dst_file_path) # Traverse the destination directory and remove files/directories not in source for root, dirs, files in os.walk(dst): # Compute the corresponding path in source relative_path = os.path.relpath(root, dst) src_root = os.path.join(src, relative_path) # Remove directories not in the source for dir_name in dirs: src_dir_path = os.path.join(src_root, dir_name) dst_dir_path = os.path.join(root, dir_name) if not os.path.exists(src_dir_path): shutil.rmtree(dst_dir_path) # Remove files not in the source for file_name in files: src_file_path = os.path.join(src_root, file_name) dst_file_path = os.path.join(root, file_name) if not os.path.exists(src_file_path): os.remove(dst_file_path)"},{"question":"Coding Assessment Question # Objective Your task is to demonstrate your understanding of the Seaborn `objects` module by creating a custom plot based on the given dataset. Ensure that you follow the specified requirements and constraints. # Problem Statement You are provided with the `penguins` dataset. Using this dataset, create a Seaborn plot to visualize the relationship between the species of penguins and their body mass. You will need to follow these steps and requirements: # Instructions 1. **Load the dataset**: - Load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. **Create the plot**: - Use the `so.Plot` object from `seaborn.objects` to create a plot. - Add the `Dash` element to the plot to represent the data points. - Customize the plot as follows: - Color the dashes based on the sex of the penguins (`color=\\"sex\\"`). - Set the `alpha` transparency of the dashes to 0.5. - Use the `flipper_length_mm` to control the `linewidth` of the dashes. - Set the orientation of the dashes to be vertical (`orient=\\"y\\"`). 3. **Additional visualization**: - Add `Dots` to the plot so that each individual data point is visible. - Use `Dodge` to ensure that the dots do not overlap and are clearly distinguishable. 4. **Output**: - Display the plot. # Expected Function Signature ```python def create_penguins_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") # Add Dash mark with customized properties p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\", orient=\\"y\\") # Add Dots mark with Dodge p.add(so.Dots(), so.Dodge()) # Display the plot p.show() # Call the function to display the plot create_penguins_plot() ``` # Constraints - Ensure that you handle any missing values in the dataset appropriately before plotting. - The plot should be clear and visually appealing, with labels and axes correctly displayed. **Note:** The function should not require any inputs and should display the plot directly when called.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguins_plot(): Creates and displays a Seaborn plot visualizing the relationship between species of penguins and their body mass, with additional customizations as specified. # Load the dataset penguins = load_dataset(\\"penguins\\") # Handle missing values penguins = penguins.dropna(subset=[\\"sex\\", \\"flipper_length_mm\\", \\"body_mass_g\\"]) # Create the plot p = so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") # Add Dash mark with customized properties p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\", orient=\\"y\\") # Add Dots mark with Dodge p.add(so.Dots(), so.Dodge()) # Display the plot p.show()"},{"question":"Coding Assessment Question # Background You are tasked with building a house price prediction model based on various features like the size of the house, number of bedrooms, location, etc. The aim is to predict the price of houses using a supervised learning approach. # Task Implement the following steps to complete this task: 1. **Data Loading and Preprocessing**: - Assume you have a CSV file `house_prices.csv` with columns: `size`, `bedrooms`, `location`, `price`. - Load the data into a pandas DataFrame. - Perform necessary preprocessing: - Encode the `location` column using one-hot encoding. - Split the DataFrame into features (`X`) and target (`y`). 2. **Model Training and Evaluation**: - Split the data into training and testing sets with a split ratio of 80% train and 20% test. - Train a Linear Regression model using the training data. - Evaluate the model using cross-validation (5-fold). - Calculate and print the mean squared error on the test set. 3. **Advanced Model**: - Train a RandomForestRegressor on the same data. - Compare the performance (mean squared error) of the Linear Regression model with the RandomForestRegressor model. # Constraints and Requirements - Use the `scikit-learn` library for model training and evaluation. - Use `pandas` for data loading and preprocessing. - Ensure your code is well-documented and follows good coding practices. # Input and Output Formats - **Input**: Path to the CSV file `house_prices.csv`. - **Output**: Print the mean squared error for both models (Linear Regression and RandomForestRegressor) on the test set, and the average cross-validation score for Linear Regression. # Example Assuming `house_prices.csv` is located in the current working directory: ```csv size,bedrooms,location,price 2100,3,Suburb,400000 1600,2,City,330000 1800,4,Suburb,350000 ... ``` Your output should look something like: ``` Linear Regression Test MSE: 45000.32 RandomForestRegressor Test MSE: 42000.11 Average Cross-validation MSE (Linear Regression): 46000.23 ``` # Submission Submit your Python script that implements the above instructions.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error def load_and_preprocess_data(file_path): # Load the data df = pd.read_csv(file_path) # Encode the \'location\' column using one-hot encoding df = pd.get_dummies(df, columns=[\'location\']) # Separate features and target X = df.drop(\'price\', axis=1) y = df[\'price\'] return X, y def train_and_evaluate_model(X, y): # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train and evaluate the Linear Regression model lin_reg = LinearRegression() lin_reg.fit(X_train, y_train) y_pred = lin_reg.predict(X_test) lin_reg_mse = mean_squared_error(y_test, y_pred) # Cross-validation for Linear Regression lin_reg_cv_scores = cross_val_score(lin_reg, X, y, scoring=\'neg_mean_squared_error\', cv=5) lin_reg_cv_mse = -lin_reg_cv_scores.mean() # Train and evaluate the RandomForestRegressor rf_reg = RandomForestRegressor(random_state=42) rf_reg.fit(X_train, y_train) y_pred_rf = rf_reg.predict(X_test) rf_reg_mse = mean_squared_error(y_test, y_pred_rf) return lin_reg_mse, lin_reg_cv_mse, rf_reg_mse # For testing purposes if __name__ == \\"__main__\\": X, y = load_and_preprocess_data(\'house_prices.csv\') lin_reg_mse, lin_reg_cv_mse, rf_reg_mse = train_and_evaluate_model(X, y) print(f\\"Linear Regression Test MSE: {lin_reg_mse}\\") print(f\\"RandomForestRegressor Test MSE: {rf_reg_mse}\\") print(f\\"Average Cross-validation MSE (Linear Regression): {lin_reg_cv_mse}\\")"},{"question":"# Tensor Arithmetic and Device Management in PyTorch In this task, you will demonstrate your understanding of PyTorch by performing tensor arithmetic with type promotion and managing devices. You are required to implement a function that performs the following operations: 1. **Create Tensors**: Create five different tensors with specified data types: - A 32-bit floating-point tensor of size (3, 3) with all values initialized to 1. - A 64-bit integer tensor of size (3, 3) with all values initialized to 2. - A 8-bit unsigned integer tensor of size (3, 3) with all values initialized to 3. - A boolean tensor of size (3, 3) with all values initialized to True. - A 16-bit floating-point tensor of size (3, 3) with all values initialized to 0.5. 2. **Tensor Arithmetic**: Perform the following arithmetic operations and store the results in new tensors: - Add the 32-bit floating-point tensor to the 64-bit integer tensor. - Multiply the result by the 8-bit unsigned integer tensor. - Add the result to the boolean tensor. - Divide the result by the 16-bit floating-point tensor. - Ensure that appropriate type promotion rules are followed during these operations. 3. **Move Tensors to Device**: Move the resultant tensor to a CUDA device if available, otherwise move it to a CPU device. # Implementation Implement the function `tensor_operations` that performs the above tasks. It should return the final resultant tensor. ```python import torch def tensor_operations(): # Create the required tensors float_tensor = torch.ones((3, 3), dtype=torch.float32) int_tensor = torch.full((3, 3), 2, dtype=torch.int64) uint_tensor = torch.full((3, 3), 3, dtype=torch.uint8) bool_tensor = torch.full((3, 3), True, dtype=torch.bool) half_tensor = torch.full((3, 3), 0.5, dtype=torch.float16) # Perform arithmetic operations with appropriate type promotion result = float_tensor + int_tensor result *= uint_tensor result = result + bool_tensor result = result / half_tensor # Determine the device to move the tensor to device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") result = result.to(device) return result # Example usage: result_tensor = tensor_operations() print(result_tensor) print(f\\"Resulting Tensor dtype: {result_tensor.dtype}\\") print(f\\"Resulting Tensor device: {result_tensor.device}\\") ``` # Output Format Your function should return the resultant tensor after performing all the specified operations and moving it to the appropriate device. # Constraints - Ensure appropriate dtype promotions according to PyTorch rules. - Handle device allocation properly, considering the availability of CUDA.","solution":"import torch def tensor_operations(): # Create the required tensors float_tensor = torch.ones((3, 3), dtype=torch.float32) int_tensor = torch.full((3, 3), 2, dtype=torch.int64) uint_tensor = torch.full((3, 3), 3, dtype=torch.uint8) bool_tensor = torch.full((3, 3), True, dtype=torch.bool) half_tensor = torch.full((3, 3), 0.5, dtype=torch.float16) # Perform arithmetic operations with appropriate type promotion result = float_tensor + int_tensor result *= uint_tensor result = result + bool_tensor result = result / half_tensor # Determine the device to move the tensor to device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") result = result.to(device) return result"},{"question":"Objective To assess students\' understanding of Python tuple manipulation and working with advanced tuple concepts such as named tuples. Task Write a Python function `analyze_and_transform_tuples` that accepts two arguments: a list of tuples and a list of field names. The function should convert the list of tuples into a named tuple structure, analyze the data, and return a summary. Input 1. `tuple_list` (List[Tuple[Any]]): A list of tuples, each containing various types of data. 2. `field_names` (List[str]): A list of strings representing the names for the fields of the named tuple. Output Return a dictionary containing: - `mean`: A tuple representing the mean of each numerical field in the named tuple. - `min`: A tuple representing the minimum value of each numerical field in the named tuple. - `max`: A tuple representing the maximum value of each numerical field in the named tuple. Constraints - Assume that all tuples in the list have the same length. - Tuple fields may contain numerical data (integers or floats) where statistical analysis is applicable. - The length of `field_names` should be equal to the length of tuples in `tuple_list`. Performance Requirements - The solution should be efficient with respect to both time and space. Aim for a linear pass over the data where possible. Example ```python from typing import List, Tuple, Dict, Any from collections import namedtuple def analyze_and_transform_tuples(tuple_list: List[Tuple[Any]], field_names: List[str]) -> Dict[str, Tuple]: # Your implementation here # Example usage tuple_list = [(10, 20), (30, 40), (50, 60)] field_names = [\\"field1\\", \\"field2\\"] result = analyze_and_transform_tuples(tuple_list, field_names) print(result) # Output should be: # {\'mean\': (30.0, 40.0), \'min\': (10, 20), \'max\': (50, 60)} ``` Notes 1. Use the `namedtuple` from the `collections` module to define the structure of the tuples. 2. Ensure you handle cases where fields contain non-numerical values gracefully. You are expected to demonstrate a clear understanding of tuple operations, dictionary handling, and basic statistics in Python.","solution":"from typing import List, Tuple, Dict, Any from collections import namedtuple import numpy as np def analyze_and_transform_tuples(tuple_list: List[Tuple[Any]], field_names: List[str]) -> Dict[str, Tuple]: if not tuple_list or not field_names: return {\'mean\': (), \'min\': (), \'max\': ()} # Create the namedtuple TupleStruct = namedtuple(\'TupleStruct\', field_names) # Convert tuple list to named tuple list named_tuples = [TupleStruct(*t) for t in tuple_list] # Convert list of named tuples to a list of lists for numerical data processing data = [[getattr(nt, field) for nt in named_tuples] for field in field_names] summaries = { \'mean\': tuple(np.mean(field_data) for field_data in data if all(isinstance(num, (int, float)) for num in field_data)), \'min\': tuple(np.min(field_data) for field_data in data if all(isinstance(num, (int, float)) for num in field_data)), \'max\': tuple(np.max(field_data) for field_data in data if all(isinstance(num, (int, float)) for num in field_data)), } return summaries"},{"question":"**Question:** Implement a file transfer system that uses Base64 encoding with custom options. **Description:** You must implement a function `secure_file_transfer(input_file_path, output_file_path, operation, altchars=None)` that can securely transfer the contents of a file using Base64 encoding. The function should either encode or decode the file content based on the specified operation. **Function Signature:** ```python def secure_file_transfer(input_file_path: str, output_file_path: str, operation: str, altchars: bytes = None) -> None: ``` # Input: - `input_file_path` (str): The path to the input file to be read. - `output_file_path` (str): The path to the output file where the result will be stored. - `operation` (str): This can be either `\\"encode\\"` or `\\"decode\\"` to specify the operation. - `altchars` (bytes, optional): A bytes-like object of length 2 specifying an alternative alphabet for the \\"+\\" and \\"/\\" characters in Base64 encoding. Default is `None`, meaning the standard Base64 alphabet will be used. # Output: - None. The encoded/decoded data should be written directly into `output_file_path`. # Constraints: - The input file can be up to 10 MB in size. - The function should handle invalid input gracefully (e.g., incorrect format, missing files). # Example: Assuming you have a file `input.txt` with the following content: ``` Hello, this is a test file for secure transfer! ``` 1. Encoding the file: ```python secure_file_transfer(\'input.txt\', \'encoded_output.txt\', \'encode\') ``` The `encoded_output.txt` will contain the encoded Base64 content of `input.txt`. 2. Decoding the file back to its original content: ```python secure_file_transfer(\'encoded_output.txt\', \'decoded_output.txt\', \'decode\') ``` The `decoded_output.txt` will contain: ``` Hello, this is a test file for secure transfer! ``` # Notes: - Handle edge cases such as invalid operation values, ensuring input and output files exist, and are readable/writable. - Use `base64.b64encode` and `base64.b64decode` functions from the `base64` module for the respective operations. # Evaluation: - The correctness of the function will be evaluated based on successfully encoding and decoding the file contents. - The efficiency of handling files and proper use of the base64 module features will be tested. - Robustness against invalid inputs and graceful error handling will be considered.","solution":"import base64 import os def secure_file_transfer(input_file_path: str, output_file_path: str, operation: str, altchars: bytes = None) -> None: Transfers contents of a file using Base64 encoding or decoding. Parameters: - input_file_path (str): Path to the input file. - output_file_path (str): Path to the output file. - operation (str): \'encode\' or \'decode\'. - altchars (bytes, optional): Alternative characters for \'+\' and \'/\'. Default is None. if not os.path.isfile(input_file_path): raise FileNotFoundError(f\\"The input file {input_file_path} does not exist.\\") if operation not in [\\"encode\\", \\"decode\\"]: raise ValueError(\\"Operation must be \'encode\' or \'decode\'.\\") with open(input_file_path, \'rb\') as input_file: data = input_file.read() try: if operation == \\"encode\\": transformed_data = base64.b64encode(data, altchars=altchars) elif operation == \\"decode\\": transformed_data = base64.b64decode(data, altchars=altchars) with open(output_file_path, \'wb\') as output_file: output_file.write(transformed_data) except Exception as e: raise e"},{"question":"You are given a dataset containing sales information for a retail store. The dataset includes columns for `date`, `store_id`, `product_id`, and `sales_amount`. The goal is to analyze this dataset by grouping and aggregating the data using pandas\' `GroupBy` functionality. **Dataset** The dataset is provided in CSV format for this exercise: ``` date,store_id,product_id,sales_amount 2023-01-01,1,101,200 2023-01-01,1,102,150 2023-01-01,2,101,100 2023-01-02,1,101,250 2023-01-02,2,102,200 ... ``` **Task** 1. Load the dataset into a pandas DataFrame. 2. Group the data by `store_id` and `product_id` to compute the total `sales_amount` for each unique combination. 3. Determine the monthly sales of each store. Convert the `date` column to datetime, and group the data by store and month to compute the total `sales_amount`. 4. Identify the product with the maximum total sales for each store. 5. Calculate the mean and standard deviation of sales for each store-product group. 6. Visualize the monthly sales trends for each store using a line plot. **Constraints and Requirements** - You must use pandas\' `GroupBy` functionality to perform the grouping and aggregation tasks. - The `date` column should be converted to a datetime object for accurate monthly aggregation. - Use appropriate visualization methods provided by pandas for plotting. **Expected Input and Output Formats** - **Input**: Path to the CSV file. - **Output**: Dictionary with keys `total_sales`, `monthly_sales`, `max_product_sales`, `sales_statistics`, and `monthly_sales_trend`, containing respective DataFrames for each task. --- **Function Signature** ```python import pandas as pd def analyze_sales_data(file_path: str) -> dict: # Load the dataset df = pd.read_csv(file_path) # Step 1: Group by `store_id` and `product_id` to compute total sales total_sales = df.groupby([\'store_id\', \'product_id\'])[\'sales_amount\'].sum().reset_index() # Step 2: Convert `date` column to datetime and group by `store_id` and month df[\'date\'] = pd.to_datetime(df[\'date\']) monthly_sales = df.groupby([df[\'date\'].dt.to_period(\'M\'), \'store_id\'])[\'sales_amount\'].sum().reset_index() monthly_sales.rename(columns={\'date\': \'month\'}, inplace=True) # Step 3: Identify the product with maximum total sales for each store max_product_sales = df.groupby([\'store_id\', \'product_id\'])[\'sales_amount\'].sum().reset_index() max_product_sales = max_product_sales.loc[max_product_sales.groupby(\'store_id\')[\'sales_amount\'].idxmax()].reset_index(drop=True) # Step 4: Calculate mean and standard deviation of sales for each store-product group sales_statistics = df.groupby([\'store_id\', \'product_id\'])[\'sales_amount\'].agg([\'mean\', \'std\']).reset_index() # Step 5: Line plot of monthly sales trends for each store monthly_sales_trend = monthly_sales.pivot(index=\'month\', columns=\'store_id\', values=\'sales_amount\') monthly_sales_trend.plot(kind=\'line\', marker=\'o\') # Returning the results in a dictionary return { \'total_sales\': total_sales, \'monthly_sales\': monthly_sales, \'max_product_sales\': max_product_sales, \'sales_statistics\': sales_statistics, \'monthly_sales_trend\': monthly_sales_trend } ``` Example Usage ```python file_path = \'sales_data.csv\' results = analyze_sales_data(file_path) # Access the resulting DataFrames total_sales = results[\'total_sales\'] monthly_sales = results[\'monthly_sales\'] max_product_sales = results[\'max_product_sales\'] sales_statistics = results[\'sales_statistics\'] monthly_sales_trend = results[\'monthly_sales_trend\'] ``` **Notes:** - Ensure you install the pandas library if not already installed (`!pip install pandas`). - Include comments in your code to explain each step comprehensively.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> dict: # Load the dataset df = pd.read_csv(file_path) # Step 1: Group by `store_id` and `product_id` to compute total sales total_sales = df.groupby([\'store_id\', \'product_id\'])[\'sales_amount\'].sum().reset_index() # Step 2: Convert `date` column to datetime and group by `store_id` and month df[\'date\'] = pd.to_datetime(df[\'date\']) monthly_sales = df.groupby([df[\'date\'].dt.to_period(\'M\'), \'store_id\'])[\'sales_amount\'].sum().reset_index() monthly_sales.rename(columns={\'date\': \'month\'}, inplace=True) # Step 3: Identify the product with maximum total sales for each store max_product_sales = df.groupby([\'store_id\', \'product_id\'])[\'sales_amount\'].sum().reset_index() max_product_sales = max_product_sales.loc[max_product_sales.groupby(\'store_id\')[\'sales_amount\'].idxmax()].reset_index(drop=True) # Step 4: Calculate mean and standard deviation of sales for each store-product group sales_statistics = df.groupby([\'store_id\', \'product_id\'])[\'sales_amount\'].agg([\'mean\', \'std\']).reset_index() # Step 5: Line plot of monthly sales trends for each store monthly_sales_trend = monthly_sales.pivot(index=\'month\', columns=\'store_id\', values=\'sales_amount\') ax = monthly_sales_trend.plot(kind=\'line\', marker=\'o\', title=\\"Monthly Sales Trends\\") ax.set_xlabel(\\"Month\\") ax.set_ylabel(\\"Sales Amount\\") # Returning the results in a dictionary return { \'total_sales\': total_sales, \'monthly_sales\': monthly_sales, \'max_product_sales\': max_product_sales, \'sales_statistics\': sales_statistics, \'monthly_sales_trend\': monthly_sales_trend }"},{"question":"# Question: Implement and Analyze Code Coverage Using the `trace` Module Problem Statement You are required to implement a Python script that uses the `trace` module to trace the execution of a given sample program and generate a coverage report. The sample program is a simple implementation of a recursive function to calculate the factorial of a number. Sample Program ```python def factorial(n): Calculate the factorial of n recursively. if n == 0: return 1 else: return n * factorial(n-1) def main(): for i in range(6): print(f\\"Factorial of {i} is {factorial(i)}\\") if __name__ == \\"__main__\\": main() ``` Task 1. Write a Python script that: - Uses the programmatic interface of the `trace` module to trace the execution of the provided sample program (`factorial` function and `main` function). - Configures the `trace` object to count the number of times each line of the sample program is executed. - Generates a coverage report showing the lines executed and their hit counts. 2. The script should produce an annotated coverage report in the current directory. Expected Output The output should be an annotated file (e.g., `factorial.cover`) that includes: - The original code lines. - The number of times each line was executed. - Lines that were not executed should be marked clearly, e.g., with \\">>>>>>\\". Constraints - You must use the `trace.Trace` class and its methods for tracing and generating the report. - The script should run `main()` function in the sample program using the trace object. - The coverage report should be written in the same directory where the script is executed. Performance Requirements - The solution should efficiently trace and report the information without unnecessary delays or excessive memory usage. You need to submit your Python script as the solution for this problem.","solution":"import trace def factorial(n): Calculate the factorial of n recursively. if n == 0: return 1 else: return n * factorial(n-1) def main(): for i in range(6): print(f\\"Factorial of {i} is {factorial(i)}\\") if __name__ == \\"__main__\\": tracer = trace.Trace(count=True, trace=False) tracer.run(\'main()\') r = tracer.results() r.write_results(show_missing=True, coverdir=\\".\\")"},{"question":"Objective: The goal of this exercise is to assess your understanding of Python\'s descriptor protocol by implementing a descriptor for managing access and validation of attributes within a class. Problem Statement: You are required to implement a descriptor that validates attribute values based on specified criteria. This descriptor should be used to validate attributes in a `Product` class. Task: 1. Implement a `Validator` class, which acts as a descriptor for managed attribute access. 2. Implement three custom validators derived from the `Validator` class: - `StringValidator`: Validates that the attribute value is a string with an optional minimum and maximum length. - `NumberValidator`: Validates that the attribute value is a number (either integer or float) within an optional range. - `OneOfValidator`: Validates that the attribute value is one of the predefined options. 3. Use these validators to manage `name`, `price`, and `category` attributes in a `Product` class. Here is the class specification: # `Validator` Class - Methods: - `__set_name__(self, owner, name)`: Sets the private attribute name. - `__get__(self, instance, owner)`: Gets the value from the instance. - `__set__(self, instance, value)`: Validates and sets the value in the instance. - `validate(self, value)`: Abstract method to be implemented by subclasses. # `StringValidator` Class - Inherits from `Validator`. - Implement the `validate` method to ensure the value is a string with optional minimum and maximum length. # `NumberValidator` Class - Inherits from `Validator`. - Implement the `validate` method to ensure the value is a number within an optional range (min, max). # `OneOfValidator` Class - Inherits from `Validator`. - Implement the `validate` method to ensure the value is one of a predefined set of options. # `Product` Class - Attributes: - `name`: Must be a string between 3 and 50 characters. - `price`: Must be a non-negative number. - `category`: Must be one of \\"Food\\", \\"Clothing\\", or \\"Electronics\\". - Methods: - `__init__(self, name, price, category)`: Initializes the attributes using the validators. Input/Output Specifications: # Input: - The `Product` instantiation will be performed with the parameters: `name`, `price`, and `category`. # Output: - The output should demonstrate the validation of the attributes. If invalid values are provided during instantiation or attribute setting, appropriate exceptions should be raised. # Example Usage: ```python try: p1 = Product(\\"Smartphone\\", 999.99, \\"Electronics\\") print(f\\"Product: {p1.name}, Price: {p1.price}, Category: {p1.category}\\") p2 = Product(\\"T-shirt\\", -19.99, \\"Clothing\\") except ValueError as e: print(e) except TypeError as e: print(e) try: p3 = Product(\\"T\\", 50, \\"Clothing\\") except ValueError as e: print(e) try: p4 = Product(\\"Jeans\\", 49.99, \\"Fashion\\") except ValueError as e: print(e) ``` Constraints: - `name` must be a string with a length between 3 and 50 characters. - `price` must be a non-negative number. - `category` must be one of the strings: \\"Food\\", \\"Clothing\\", \\"Electronics\\". Implementation: ```python from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, instance, owner): return getattr(instance, self.private_name) def __set__(self, instance, value): self.validate(value) setattr(instance, self.private_name, value) @abstractmethod def validate(self, value): pass class StringValidator(Validator): def __init__(self, min_len=None, max_len=None): self.min_len = min_len self.max_len = max_len def validate(self, value): if not isinstance(value, str): raise TypeError(f\\"Expected a string, got {type(value).__name__}\\") if self.min_len is not None and len(value) < self.min_len: raise ValueError(f\\"String length should be at least {self.min_len}\\") if self.max_len is not None and len(value) > self.max_len: raise ValueError(f\\"String length should be at most {self.max_len}\\") class NumberValidator(Validator): def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(f\\"Expected a number, got {type(value).__name__}\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Number should be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Number should be at most {self.max_value}\\") class OneOfValidator(Validator): def __init__(self, *options): self.options = set(options) def validate(self, value): if value not in self.options: raise ValueError(f\\"Expected one of {self.options}, got {value}\\") class Product: name = StringValidator(min_len=3, max_len=50) price = NumberValidator(min_value=0) category = OneOfValidator(\\"Food\\", \\"Clothing\\", \\"Electronics\\") def __init__(self, name, price, category): self.name = name self.price = price self.category = category ``` Submission: Please include the implementation of all required classes and brief comments explaining your logic.","solution":"from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, instance, owner): return getattr(instance, self.private_name) def __set__(self, instance, value): self.validate(value) setattr(instance, self.private_name, value) @abstractmethod def validate(self, value): pass class StringValidator(Validator): def __init__(self, min_len=None, max_len=None): self.min_len = min_len self.max_len = max_len def validate(self, value): if not isinstance(value, str): raise TypeError(f\\"Expected a string, got {type(value).__name__}\\") if self.min_len is not None and len(value) < self.min_len: raise ValueError(f\\"String length should be at least {self.min_len}\\") if self.max_len is not None and len(value) > self.max_len: raise ValueError(f\\"String length should be at most {self.max_len}\\") class NumberValidator(Validator): def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(f\\"Expected a number, got {type(value).__name__}\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"Number should be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"Number should be at most {self.max_value}\\") class OneOfValidator(Validator): def __init__(self, *options): self.options = set(options) def validate(self, value): if value not in self.options: raise ValueError(f\\"Expected one of {self.options}, got {value}\\") class Product: name = StringValidator(min_len=3, max_len=50) price = NumberValidator(min_value=0) category = OneOfValidator(\\"Food\\", \\"Clothing\\", \\"Electronics\\") def __init__(self, name, price, category): self.name = name self.price = price self.category = category"},{"question":"# PyTorch Coding Assignment: Synchronizing CUDA Streams Objective In this coding assignment, you will be required to write a PyTorch function that demonstrates the detection of synchronization errors in CUDA streams and fixes them using the CUDA Stream Sanitizer. This will test your understanding of CUDA streams, tensor operations, and the synchronization mechanisms provided by PyTorch. Background When working with CUDA tensors in PyTorch, it is possible to run into synchronization issues when the same tensor is accessed concurrently from multiple streams. The CUDA Stream Sanitizer can help identify such issues by detecting data races. Proper synchronization is necessary to avoid these errors. Task Write a function `sanitize_and_fix(data: torch.Tensor) -> torch.Tensor` that takes a CUDA tensor, performs operations that would lead to synchronization errors, uses the CUDA Stream Sanitizer to detect these errors, and then fixes the errors by adding necessary synchronization. Steps 1. **Create a CUDA tensor**: Initialize a tensor on a CUDA device. 2. **Introduce an Error**: Perform a tensor operation in a separate stream without synchronizing it with the default stream. 3. **Sanitize and Detect**: Use the CUDA Stream Sanitizer to detect the error. 4. **Fix the Error**: Synchronize the streams to prevent the error and perform the operation correctly. Input - A PyTorch CUDA tensor `data` of shape `(1000, 1000)`. Output - The modified CUDA tensor after fixing synchronization issues. Constraints - You must use at least two CUDA streams. - Ensure that the sanitizer detects the error before you fix it. Example ```python import torch def sanitize_and_fix(data: torch.Tensor) -> torch.Tensor: # Ensure CUDA Stream Sanitizer is enabled torch.cuda._sanitizer.enable_cuda_sanitizer() # Create a new CUDA stream stream = torch.cuda.Stream() # Incorrectly perform an operation on the new stream without synchronization with torch.cuda.stream(stream): torch.mul(data, 2, out=data) # Run with CUDA Stream Sanitizer enabled to detect possible errors # You should now see an error detected by CSAN # Fix the error by synchronizing the streams with torch.cuda.stream(stream): torch.cuda.current_stream().wait_stream(torch.cuda.default_stream()) torch.mul(data, 3, out=data) return data # Example usage data = torch.rand((1000, 1000), device=\'cuda\') fixed_data = sanitize_and_fix(data) ``` In the above code, the function `sanitize_and_fix`: - Initializes the CUDA Stream Sanitizer. - Creates a secondary CUDA stream. - Introduces an error by performing an operation on the tensor in the new stream without synchronization. - Detects the error through the CUDA Stream Sanitizer. - Fixes the error by synchronizing the two streams before performing further operations. - Returns the modified CUDA tensor. Ensure to test the function with actual CUDA hardware and verify that the CUDA Stream Sanitizer detects the errors before fixing them.","solution":"import torch def sanitize_and_fix(data: torch.Tensor) -> torch.Tensor: # Ensure the data is on the CUDA device if not data.is_cuda: raise ValueError(\\"Input tensor must be on CUDA device\\") # Ensure CUDA Stream Sanitizer is enabled (please note this might be a hypothetical feature, # as PyTorch currently does not natively support a CUDA Stream Sanitizer). # torch.cuda._sanitizer.enable_cuda_sanitizer() # Create a new CUDA stream stream = torch.cuda.Stream() # Introduce synchronization issue by performing an operation in the new stream without synchronization with torch.cuda.stream(stream): result = data * 2 # This should normally raise a synchronization error if we had a sanitizer (hypothetical PyTorch feature) # Fix the error by synchronizing the streams torch.cuda.synchronize() with torch.cuda.stream(stream): torched_result = result * 3 # Synchronize the device to ensure all operations are completed torch.cuda.synchronize() return torched_result"},{"question":"# LDA and QDA Implementation and Comparison Objective: Implement Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) classifiers using scikit-learn. You will then compare their performance on a synthetic dataset and use LDA for dimensionality reduction on the dataset. Tasks: 1. **Generate a synthetic dataset**: - Create a synthetic dataset with 3 classes using `make_classification` from `sklearn.datasets`. - The dataset should have 1000 samples and 20 features. 2. **Implement LDA and QDA for Classification**: - Fit an LDA model and a QDA model on the dataset. - Provide predictions for the dataset using both models. 3. **Evaluate and Compare the Models**: - Calculate and print the accuracy score for both models. - Plot the decision boundaries for both LDA and QDA to visualize the differences. 4. **Dimensionality Reduction using LDA**: - Use LDA to reduce the dimensionality of the dataset to 2 components. - Fit the LDA model for dimensionality reduction and transform the dataset. - Create a scatter plot of the transformed dataset with points colored according to their class. Input: The input will be a randomly generated dataset using the specifications provided above. Output: 1. Accuracy scores for LDA and QDA models. 2. Decision boundary plot for LDA. 3. Decision boundary plot for QDA. 4. Scatter plot of the dataset reduced to 2 dimensions by LDA. Constraints: - Use scikit-learn for model implementation and evaluation. Requirements: - Ensure the code is well-documented with clear comments. - Make use of `matplotlib` or `seaborn` for plotting. - The solution should be efficient and follow good coding practices. Sample Code Setup: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score # 1. Generate a synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=10, n_redundant=5, random_state=42) # 2. Implement LDA and QDA for classification lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() # Fit the models lda.fit(X, y) qda.fit(X, y) # Predict using the models y_pred_lda = lda.predict(X) y_pred_qda = qda.predict(X) # 3. Evaluate and Compare the Models acc_lda = accuracy_score(y, y_pred_lda) acc_qda = accuracy_score(y, y_pred_qda) print(f\\"LDA Accuracy: {acc_lda}\\") print(f\\"QDA Accuracy: {acc_qda}\\") # Plot decision boundaries (You need to implement this part) # 4. Dimensionality Reduction using LDA lda = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda.fit(X, y).transform(X) # Scatter plot of the reduced dataset plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] target_names = [\'class 0\', \'class 1\', \'class 2\'] for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of synthetic dataset\') plt.show() ``` Notes: - Ensure that the solution handles any potential issues with data shapes or types. - You may refer to scikit-learn documentation for additional help regarding the methods and functions used.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score def generate_synthetic_dataset(): Generates a synthetic dataset with 3 classes, 1000 samples, and 20 features. X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=10, n_redundant=5, random_state=42) return X, y def fit_lda_qda(X, y): Fits LDA and QDA models to the dataset and returns the predictions and accuracy scores. lda = LinearDiscriminantAnalysis() qda = QuadraticDiscriminantAnalysis() # Fit the models lda.fit(X, y) qda.fit(X, y) # Predictions y_pred_lda = lda.predict(X) y_pred_qda = qda.predict(X) # Accuracy scores acc_lda = accuracy_score(y, y_pred_lda) acc_qda = accuracy_score(y, y_pred_qda) return y_pred_lda, y_pred_qda, acc_lda, acc_qda def plot_decision_boundaries(X, y, model, title): Plots the decision boundaries for LDA and QDA models. plt.figure() x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) # Predict class labels for each point in the mesh Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\'k\', s=20) plt.title(title) plt.show() def lda_dimensionality_reduction(X, y): Reduces the dimensionality of the dataset using LDA and returns the transformed dataset. lda = LinearDiscriminantAnalysis(n_components=2) X_r2 = lda.fit(X, y).transform(X) return X_r2 def plot_reduced_data(X_r2, y): Plots the dataset reduced to 2 dimensions by LDA. plt.figure() colors = [\'navy\', \'turquoise\', \'darkorange\'] target_names = [\'class 0\', \'class 1\', \'class 2\'] for color, i, target_name in zip(colors, [0, 1, 2], target_names): plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA of synthetic dataset\') plt.show() # Generating synthetic dataset X, y = generate_synthetic_dataset() # LDA and QDA fit, prediction and evaluation y_pred_lda, y_pred_qda, acc_lda, acc_qda = fit_lda_qda(X, y) print(f\\"LDA Accuracy: {acc_lda}\\") print(f\\"QDA Accuracy: {acc_qda}\\") # Dimensionality reduction using LDA X_r2 = lda_dimensionality_reduction(X, y) # Plotting reduced data plot_reduced_data(X_r2, y)"},{"question":"**Problem Statement** You are tasked with creating a function that processes dates for an international event planning system. Using the `zoneinfo` module, you need to manage time zones for various events, ensuring that times are correctly handled across different time zones. Write a function called `convert_event_time` that takes the following arguments: 1. `event_datetime` (str): A string representing the date and time of the event in the format \\"YYYY-MM-DD HH:MM:SS\\". 2. `event_timezone` (str): A string representing the IANA time zone for `event_datetime`. 3. `target_timezone` (str): A string representing the IANA time zone to which the event time should be converted. The function should return a string representing the converted date and time in the format \\"YYYY-MM-DD HH:MM:SS\\" in the `target_timezone`. If any input time zone does not exist in the system, your function should raise a `zoneinfo.ZoneInfoNotFoundError`. Additionally, your function should handle ambiguous times arising from daylight saving transitions by using the `fold` attribute such that: - If `fold=0`, it uses the first instance of the ambiguous time. - If `fold=1`, it uses the second instance of the ambiguous time. **Constraints:** - You should not assume that any specific time zone data is available and must handle the possible unavailability gracefully by raising the appropriate exception. - The inputs will always be valid strings formatted as specified. - The conversion should honor daylight saving time transitions properly. **Example:** ```python from datetime import datetime event_datetime = \\"2023-03-26 01:30:00\\" event_timezone = \\"Europe/London\\" target_timezone = \\"America/New_York\\" print(convert_event_time(event_datetime, event_timezone, target_timezone)) # Output: \\"2023-03-25 21:30:00\\" (This output may vary based on actual time zone database and DST rules) ``` Implement the `convert_event_time` function. ```python from zoneinfo import ZoneInfo from datetime import datetime def convert_event_time(event_datetime, event_timezone, target_timezone): try: # Parse the input event datetime string dt = datetime.strptime(event_datetime, \'%Y-%m-%d %H:%M:%S\') # Attach the event\'s timezone event_tz = ZoneInfo(event_timezone) dt = dt.replace(tzinfo=event_tz) # Convert to target timezone target_tz = ZoneInfo(target_timezone) target_dt = dt.astimezone(target_tz) # Handle daylight saving transitions with fold=0 and fold=1 if dt != dt.replace(fold=1): target_dt_fold_1 = dt.replace(fold=1).astimezone(target_tz) # Return the converted datetime as a string return target_dt.strftime(\'%Y-%m-%d %H:%M:%S\') except zoneinfo.ZoneInfoNotFoundError as e: raise zoneinfo.ZoneInfoNotFoundError(f\\"Time zone not found: {str(e)}\\") # Example Usage event_datetime = \\"2023-03-26 01:30:00\\" event_timezone = \\"Europe/London\\" target_timezone = \\"America/New_York\\" print(convert_event_time(event_datetime, event_timezone, target_timezone)) ```","solution":"from zoneinfo import ZoneInfo, ZoneInfoNotFoundError from datetime import datetime def convert_event_time(event_datetime, event_timezone, target_timezone): try: # Parse the input event datetime string dt = datetime.strptime(event_datetime, \'%Y-%m-%d %H:%M:%S\') # Attach the event\'s timezone event_tz = ZoneInfo(event_timezone) dt = dt.replace(tzinfo=event_tz) # Convert to target timezone target_tz = ZoneInfo(target_timezone) target_dt = dt.astimezone(target_tz) # Return the converted datetime as a string return target_dt.strftime(\'%Y-%m-%d %H:%M:%S\') except ZoneInfoNotFoundError as e: raise ZoneInfoNotFoundError(f\\"Time zone not found: {str(e)}\\")"},{"question":"You are given a dataset of timestamps and sales amounts for a retail store. The dataset is as follows: ```python import pandas as pd import numpy as np data = { \\"timestamp\\": [ \\"2021-01-01 10:05:00\\", \\"2021-01-01 10:10:00\\", \\"2021-01-01 10:15:00\\", \\"2021-01-01 10:25:00\\", \\"2021-01-01 10:30:00\\", \\"2021-01-01 10:40:00\\", \\"2021-01-01 10:45:00\\", \\"2021-01-01 10:50:00\\", \\"2021-01-01 11:00:00\\", \\"2021-01-01 11:10:00\\", \\"2021-01-01 11:15:00\\", \\"2021-01-01 11:40:00\\", \\"2021-01-01 12:00:00\\", \\"2021-01-01 12:10:00\\", \\"2021-01-01 12:15:00\\" ], \\"sales_amount\\": [ 100, 200, 150, 130, 170, 190, 220, 180, 300, 250, 270, 290, 310, 305, 325 ] } df = pd.DataFrame(data) df[\\"timestamp\\"] = pd.to_datetime(df[\\"timestamp\\"]) ``` # Task Implement the following functions: 1. `resample_sales(df: pd.DataFrame, freq: str, method: str) -> pd.DataFrame`: - **Input**: - `df`: A pandas DataFrame containing the columns \\"timestamp\\" and \\"sales_amount\\". - `freq`: A string representing the frequency to resample the data (e.g., \'30T\' for 30 minutes). - `method`: A string indicating the method of filling/resampling data. Possible values are \\"mean\\", \\"sum\\", \\"ffill\\" (forward fill), \\"bfill\\" (backward fill). - **Output**: A pandas DataFrame resampled to the specified frequency using the specified method. 2. `convert_timezone(df: pd.DataFrame, new_tz: str) -> pd.DataFrame`: - **Input**: - `df`: A pandas DataFrame with a \\"timestamp\\" column that is timezone-naive. - `new_tz`: A string representing the new timezone to convert the \\"timestamp\\" column to (e.g., \'US/Eastern\', \'Europe/London\'). - **Output**: The DataFrame with the \\"timestamp\\" column converted to the specified timezone. 3. `aggregate_sales(df: pd.DataFrame, freq: str) -> pd.DataFrame`: - **Input**: - `df`: A pandas DataFrame containing the columns \\"timestamp\\" and \\"sales_amount\\". - `freq`: A string representing the frequency at which to aggregate the sales data (e.g., \'H\' for hourly, \'D\' for daily). - **Output**: A pandas DataFrame with aggregated sales data over the specified frequency. - **Constraint**: If any period does not have sales data, fill it with `NaN`. # Constraints - Use pandas version 1.3.3 or higher. - Your solution should efficiently handle resampling and aggregation on larger datasets with potentially millions of rows. - Ensure proper timezone handling when converting timezones. # Example ```python # Example usage df_resampled = resample_sales(df, \'30T\', \'mean\') print(df_resampled) df_timezone = convert_timezone(df, \'US/Eastern\') print(df_timezone) df_aggregated = aggregate_sales(df, \'H\') print(df_aggregated) ```","solution":"import pandas as pd def resample_sales(df: pd.DataFrame, freq: str, method: str) -> pd.DataFrame: Resamples the sales data to the specified frequency using the given method. Parameters: df (pd.DataFrame): A DataFrame containing the columns \\"timestamp\\" and \\"sales_amount\\". freq (str): The frequency to resample the data (e.g., \'30T\' for 30 minutes). method (str): The method of filling/resampling data. Possible values are \\"mean\\", \\"sum\\", \\"ffill\\" (forward fill), \\"bfill\\" (backward fill). Returns: pd.DataFrame: A DataFrame resampled to the specified frequency using the specified method. if method == \\"mean\\": return df.resample(freq, on=\'timestamp\').mean().reset_index() elif method == \\"sum\\": return df.resample(freq, on=\'timestamp\').sum().reset_index() elif method == \\"ffill\\": return df.resample(freq, on=\'timestamp\').ffill().reset_index() elif method == \\"bfill\\": return df.resample(freq, on=\'timestamp\').bfill().reset_index() else: raise ValueError(\\"Method should be one of \'mean\', \'sum\', \'ffill\', \'bfill\'.\\") def convert_timezone(df: pd.DataFrame, new_tz: str) -> pd.DataFrame: Converts the timezone of the timestamp column to the specified timezone. Parameters: df (pd.DataFrame): A DataFrame with a timestamp column that is timezone-naive. new_tz (str): The new timezone to convert the timestamp column to (e.g., \'US/Eastern\', \'Europe/London\'). Returns: pd.DataFrame: The DataFrame with the timestamp column converted to the specified timezone. df[\'timestamp\'] = df[\'timestamp\'].dt.tz_localize(\'UTC\').dt.tz_convert(new_tz) return df def aggregate_sales(df: pd.DataFrame, freq: str) -> pd.DataFrame: Aggregates the sales data over the specified frequency. Parameters: df (pd.DataFrame): A DataFrame containing the columns \\"timestamp\\" and \\"sales_amount\\". freq (str): The frequency at which to aggregate the sales data (e.g., \'H\' for hourly, \'D\' for daily). Returns: pd.DataFrame: A DataFrame with aggregated sales data over the specified frequency. aggregated_df = df.resample(freq, on=\'timestamp\').sum() aggregated_df = aggregated_df.replace(0, pd.NA).reset_index() return aggregated_df"},{"question":"# PyTorch Accelerator Management You are tasked with implementing a set of functions that will demonstrate your understanding of managing hardware accelerators (such as GPUs) using PyTorch. Your assignment will involve creating functions to detect available devices, manage device contexts, and ensure synchronization of the computations across devices. Implement the following functions: 1. **`available_devices`** - **Input:** None - **Output:** `List[int]` - A list containing indices of all available accelerator devices. - **Description:** This function should return a list of indices for all the available devices. If no devices are available, return an empty list. 2. **`select_device(device_index: int) -> bool`** - **Input:** `device_index` (int) - Index of the device to be set as active. - **Output:** `bool` - Return `True` if the device was successfully set, `False` otherwise. - **Description:** This function should attempt to set the active device to the given index. If the index is invalid (e.g., larger than available device count), return `False`. 3. **`get_current_device`** - **Input:** None - **Output:** `int` - The index of the current active device. - **Description:** This function should return the index of the current active device. 4. **`synchronize_all_devices`** - **Input:** None - **Output:** None - **Description:** This function should synchronize all computations on all available devices. # Constraints - If the accelerator is not available, every function should handle the situation gracefully without throwing errors. - You should use the functions from `torch.accelerator` module to implement the required functionalities. - Ensure your functions are performant and handle edge cases like empty device lists appropriately. # Example Suppose there are 2 devices available, calling the functions should produce the following behavior: ```python # Check available devices print(available_devices()) # Output: [0, 1] # Select the first device print(select_device(0)) # Output: True # Get the active device index print(get_current_device()) # Output: 0 # Synchronize all devices synchronize_all_devices() # Should synchronize computations across all devices. ``` **Note:** The actual behavior might vary based on the actual accelerator setup on the system where the code is executed.","solution":"import torch def available_devices(): Returns a list of indices of all available accelerator devices. if torch.cuda.is_available(): return list(range(torch.cuda.device_count())) return [] def select_device(device_index: int) -> bool: Sets the active device to the given index. Parameters: device_index (int): The index of the device to set as active. Returns: bool: True if the device was successfully set, False otherwise. if torch.cuda.is_available() and device_index < torch.cuda.device_count(): torch.cuda.set_device(device_index) return True return False def get_current_device() -> int: Returns the index of the current active device. if torch.cuda.is_available(): return torch.cuda.current_device() return -1 def synchronize_all_devices(): Synchronizes computations on all available devices. if torch.cuda.is_available(): for i in range(torch.cuda.device_count()): torch.cuda.synchronize(i)"},{"question":"# Naive Bayes Classifier Implementation Challenge Objective: To assess your understanding of Naive Bayes classifiers and your ability to implement these classifiers using the scikit-learn library. Problem Statement: You are provided with a dataset of text documents labeled with corresponding categories. Your task is to implement a text classification system using the Multinomial Naive Bayes algorithm in scikit-learn. Additionally, evaluate the performance of your classifier using appropriate metrics. Dataset: - You will use the `20newsgroups` dataset from scikit-learn, modified to only include the following categories: `comp.graphics`, `sci.med`, and `rec.sport.baseball`. Tasks: 1. **Load the Data:** Load the dataset using the following scikit-learn functionality and filter it as specified. ```python from sklearn.datasets import fetch_20newsgroups categories = [\'comp.graphics\', \'sci.med\', \'rec.sport.baseball\'] newsgroups_train = fetch_20newsgroups(subset=\'train\', categories=categories) newsgroups_test = fetch_20newsgroups(subset=\'test\', categories=categories) ``` 2. **Preprocess Text Data:** Implement text preprocessing steps including: - Tokenization - Converting text to a matrix of token counts - Transforming the count matrix to a normalized tf-idf representation 3. **Train MultinomialNB Classifier:** Train a `MultinomialNB` classifier using the preprocessed training data. 4. **Evaluate the Classifier:** Evaluate your classifier on the test data using the following metrics: - Accuracy - Precision - Recall - F1-score 5. **Report Findings:** Print a classification report showing the performance of your classifier. Requirements: - Use the `CountVectorizer` and `TfidfTransformer` classes from scikit-learn for text preprocessing. - Use the `MultinomialNB` class from scikit-learn for training your classifier. - Use appropriate functions from scikit-learn for computing performance metrics and generating the classification report. Input Format: - The input consists of loading the `20newsgroups` dataset restricted to the specified categories. Output Format: - The output should include the classification report showing the accuracy, precision, recall, and F1-score for each category. Example Implementation: The final structure of your code should look similar to the following: ```python from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import classification_report # Load data categories = [\'comp.graphics\', \'sci.med\', \'rec.sport.baseball\'] newsgroups_train = fetch_20newsgroups(subset=\'train\', categories=categories) newsgroups_test = fetch_20newsgroups(subset=\'test\', categories=categories) # Preprocess text data count_vect = CountVectorizer() X_train_counts = count_vect.fit_transform(newsgroups_train.data) tfidf_transformer = TfidfTransformer() X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) # Train classifier clf = MultinomialNB().fit(X_train_tfidf, newsgroups_train.target) # Predict and evaluate X_test_counts = count_vect.transform(newsgroups_test.data) X_test_tfidf = tfidf_transformer.transform(X_test_counts) predicted = clf.predict(X_test_tfidf) # Print classification report print(classification_report(newsgroups_test.target, predicted, target_names=newsgroups_test.target_names)) ``` Evaluate your results and provide any observations or insights gained from the performance metrics.","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import classification_report def load_and_preprocess_data(): categories = [\'comp.graphics\', \'sci.med\', \'rec.sport.baseball\'] newsgroups_train = fetch_20newsgroups(subset=\'train\', categories=categories) newsgroups_test = fetch_20newsgroups(subset=\'test\', categories=categories) count_vect = CountVectorizer() X_train_counts = count_vect.fit_transform(newsgroups_train.data) tfidf_transformer = TfidfTransformer() X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) X_test_counts = count_vect.transform(newsgroups_test.data) X_test_tfidf = tfidf_transformer.transform(X_test_counts) return X_train_tfidf, newsgroups_train.target, X_test_tfidf, newsgroups_test.target, newsgroups_test.target_names def train_and_evaluate_classifier(X_train_tfidf, y_train, X_test_tfidf, y_test, target_names): clf = MultinomialNB().fit(X_train_tfidf, y_train) predicted = clf.predict(X_test_tfidf) return classification_report(y_test, predicted, target_names=target_names) # Load and preprocess data X_train_tfidf, y_train, X_test_tfidf, y_test, target_names = load_and_preprocess_data() # Train and evaluate the classifier report = train_and_evaluate_classifier(X_train_tfidf, y_train, X_test_tfidf, y_test, target_names) print(report)"},{"question":"# Coding Assessment: Python Descriptor Implementation Objective: This assessment aims to evaluate your understanding of Python descriptors, specifically your ability to create and use them effectively in a class. Problem Statement: You are required to implement a class in Python that uses descriptors to manage its attributes. The class should use custom descriptors to enforce data validation rules for its attributes. Requirements: 1. **Descriptor Class Implementation:** - Implement a descriptor class called `ValidatedAttribute` that performs the following validations: - The assigned value must be an integer. - The assigned value must be greater than zero. - If the assigned value is not an integer or less than zero, raise a `ValueError` with an appropriate error message. 2. **Managed Class Implementation:** - Implement a class called `Product` which has the following attributes managed by the `ValidatedAttribute` descriptor: - `price` (should use the `ValidatedAttribute` descriptor) - `quantity` (should use the `ValidatedAttribute` descriptor) 3. **Testing:** - Implement a series of tests to ensure that the `Product` class and the `ValidatedAttribute` descriptor are working as expected. - The tests should check: - Successful assignment of valid values. - Raising of `ValueError` when invalid values are assigned. Constraints: - You are not allowed to use any third-party libraries; only the Python standard library is permitted. - Your code should be clear, well-documented, and efficient. Input and Output Formats: - Example Input: ```python p = Product() p.price = 50 p.quantity = 10 ``` - Example Output: - No exception for valid values. - Example Input: ```python p = Product() p.price = -5 p.quantity = \'ten\' ``` - Example Output: - ValueError: Price must be greater than zero. - ValueError: Quantity must be an integer. Performance Requirements: - The program should efficiently handle multiple instances and attribute assignments in a reasonable time frame (linear complexity is expected). # Sample Solution Outline: ```python class ValidatedAttribute: def __init__(self): self._value = None def __get__(self, instance, owner): return self._value def __set__(self, instance, value): if not isinstance(value, int): raise ValueError(f\\"Value must be an integer, got {type(value)}\\") if value <= 0: raise ValueError(f\\"Value must be greater than zero, got {value}\\") self._value = value class Product: price = ValidatedAttribute() quantity = ValidatedAttribute() # Tests def test_valid_values(): p = Product() p.price = 50 p.quantity = 10 assert p.price == 50 assert p.quantity == 10 def test_invalid_values(): p = Product() try: p.price = -5 except ValueError as e: assert str(e) == \\"Value must be greater than zero, got -5\\" try: p.quantity = \'ten\' except ValueError as e: assert str(e) == \\"Value must be an integer, got <class \'str\'>\\" # Running Tests test_valid_values() test_invalid_values() ``` Ensure your implementation matches the requirements and constraints provided. Good luck!","solution":"class ValidatedAttribute: def __init__(self, name): self.name = name def __get__(self, instance, owner): return instance.__dict__[self.name] def __set__(self, instance, value): if not isinstance(value, int): raise ValueError(f\\"{self.name} must be an integer, got {type(value)}\\") if value <= 0: raise ValueError(f\\"{self.name} must be greater than zero, got {value}\\") instance.__dict__[self.name] = value class Product: price = ValidatedAttribute(\'price\') quantity = ValidatedAttribute(\'quantity\') def __init__(self, price=None, quantity=None): if price is not None: self.price = price if quantity is not None: self.quantity = quantity"},{"question":"# PyTorch Broadcasting Challenge **Objective**: Use PyTorch\'s broadcasting semantics to perform tensor operations. **Problem Statement**: You are provided with two multi-dimensional tensors of varying shapes. Your task is to implement a function `broadcast_and_operate` that takes these tensors, broadcasts them to a common shape (if possible), and then performs element-wise multiplication on them. # Function Signature ```python def broadcast_and_operate(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: ``` # Input - `tensor1` (torch.Tensor): A multi-dimensional tensor. - `tensor2` (torch.Tensor): Another multi-dimensional tensor. # Output - `result` (torch.Tensor): A tensor resulting from the element-wise multiplication of `tensor1` and `tensor2` after broadcasting them to a common shape. # Constraints 1. The dimensions of the tensors must be broadcastable according to the rules of broadcasting provided in the documentation. 2. If the tensors cannot be broadcasted to a common shape, your function should raise a `ValueError` with the message \\"Tensors are not broadcastable\\". # Example ```python import torch # Example 1 tensor1 = torch.tensor([1, 2, 3]) tensor2 = torch.tensor([[0], [1], [2]]) broadcast_and_operate(tensor1, tensor2) # Output: tensor([[0, 0, 0], # [1, 2, 3], # [2, 4, 6]]) # Example 2 tensor1 = torch.tensor([1, 2]) tensor2 = torch.tensor([3, 4, 5]) try: broadcast_and_operate(tensor1, tensor2) except ValueError as e: print(e) # Output: Tensors are not broadcastable ``` # Implementation Notes 1. Use PyTorch\'s broadcasting semantics to perform the multiplication. 2. Handle scenarios where broadcasting isn\'t possible by raising the appropriate error. 3. Ensure your implementation is efficient and leverages PyTorch\'s internal operations for broadcasting. The goal is to assess the student\'s understanding of PyTorch\'s broadcasting rules and their ability to implement tensor operations accordingly.","solution":"import torch def broadcast_and_operate(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: try: result = tensor1 * tensor2 except RuntimeError as e: raise ValueError(\\"Tensors are not broadcastable\\") from e return result"},{"question":"Stochastic Gradient Descent with Scikit-Learn **Problem Statement**: You are provided with a dataset containing information about house prices (regression problem) and a dataset with information about iris types (classification problem). Your task is to implement a solution using Stochastic Gradient Descent (SGD) to: 1. Train an `SGDRegressor` model to predict house prices based on the provided features. 2. Train an `SGDClassifier` model to classify iris flowers based on the provided features. **Requirements**: 1. **Read the datasets**: - The house prices dataset (`house_prices.csv`) contains multiple numerical features and a target column `SalePrice`. - The iris dataset (`iris.csv`) contains numerical features and a target column `Species`. 2. **Data Preprocessing**: - Standardize the features by removing the mean and scaling to unit variance for both datasets. 3. **Model Training**: - Train an `SGDRegressor` model with `loss=\'squared_error\'`, `penalty=\'l2\'`, and other default parameters. - Train an `SGDClassifier` model with `loss=\'log_loss\'`, `penalty=\'l2\'`, and other default parameters. 4. **Evaluation**: - For the `SGDRegressor`, compute the Mean Squared Error (MSE) on the test set. - For the `SGDClassifier`, compute the accuracy score on the test set. **Input and Output Formats**: - Implement a function `sgd_models(house_filepath, iris_filepath)` which takes: - `house_filepath` (str): Path to the house prices CSV file. - `iris_filepath` (str): Path to the iris CSV file. - The function should output: - A tuple with the MSE for the regression model and the accuracy for the classification model. **Constraints**: - Use a random state of 42 for splitting the data into train and test sets. - Use 80% of the data for training and 20% for testing. - You may use scikit-learn\'s `train_test_split`, `StandardScaler`, and other relevant modules. **Performance Requirements**: - Ensure the implementation is efficient and scalable enough to handle datasets with up to 100,000 samples (for the regression problem) and datasets with up to 10,000 samples (for the classification problem). ```python from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDRegressor, SGDClassifier from sklearn.metrics import mean_squared_error, accuracy_score import pandas as pd def sgd_models(house_filepath, iris_filepath): # Load datasets house_data = pd.read_csv(house_filepath) iris_data = pd.read_csv(iris_filepath) # House prices dataset X_house = house_data.drop(\'SalePrice\', axis=1).values y_house = house_data[\'SalePrice\'].values # Iris dataset X_iris = iris_data.drop(\'Species\', axis=1).values y_iris = iris_data[\'Species\'].values # Split datasets into training and test sets X_house_train, X_house_test, y_house_train, y_house_test = train_test_split(X_house, y_house, test_size=0.2, random_state=42) X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_house_train = scaler.fit_transform(X_house_train) X_house_test = scaler.transform(X_house_test) X_iris_train = scaler.fit_transform(X_iris_train) X_iris_test = scaler.transform(X_iris_test) # Train SGDRegressor regressor = SGDRegressor(loss=\'squared_error\', penalty=\'l2\', random_state=42) regressor.fit(X_house_train, y_house_train) y_house_pred = regressor.predict(X_house_test) house_mse = mean_squared_error(y_house_test, y_house_pred) # Train SGDClassifier classifier = SGDClassifier(loss=\'log_loss\', penalty=\'l2\', random_state=42) classifier.fit(X_iris_train, y_iris_train) y_iris_pred = classifier.predict(X_iris_test) iris_accuracy = accuracy_score(y_iris_test, y_iris_pred) return house_mse, iris_accuracy # Example usage: # mse, accuracy = sgd_models(\'path/to/house_prices.csv\', \'path/to/iris.csv\') # print(f\\"House Prices MSE: {mse}, Iris Classification Accuracy: {accuracy}\\") ``` **Notes**: - Ensure you handle any preprocessing steps necessary to prepare the data for modeling. - You are allowed to add additional helper functions if needed to keep the code modular and clean.","solution":"from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDRegressor, SGDClassifier from sklearn.metrics import mean_squared_error, accuracy_score import pandas as pd def sgd_models(house_filepath, iris_filepath): # Load datasets house_data = pd.read_csv(house_filepath) iris_data = pd.read_csv(iris_filepath) # House prices dataset X_house = house_data.drop(\'SalePrice\', axis=1).values y_house = house_data[\'SalePrice\'].values # Iris dataset X_iris = iris_data.drop(\'Species\', axis=1).values y_iris = iris_data[\'Species\'].values # Split datasets into training and test sets X_house_train, X_house_test, y_house_train, y_house_test = train_test_split(X_house, y_house, test_size=0.2, random_state=42) X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_house_train = scaler.fit_transform(X_house_train) X_house_test = scaler.transform(X_house_test) X_iris_train = scaler.fit_transform(X_iris_train) X_iris_test = scaler.transform(X_iris_test) # Train SGDRegressor regressor = SGDRegressor(loss=\'squared_error\', penalty=\'l2\', random_state=42) regressor.fit(X_house_train, y_house_train) y_house_pred = regressor.predict(X_house_test) house_mse = mean_squared_error(y_house_test, y_house_pred) # Train SGDClassifier classifier = SGDClassifier(loss=\'log_loss\', penalty=\'l2\', random_state=42) classifier.fit(X_iris_train, y_iris_train) y_iris_pred = classifier.predict(X_iris_test) iris_accuracy = accuracy_score(y_iris_test, y_iris_pred) return house_mse, iris_accuracy"},{"question":"# Concurrent Web Scraper using `asyncio` Objective: Implement a concurrent web scraper using `asyncio` that fetches the content of a list of URLs and saves each page\'s content to a file named after the domain of the URL. Requirements: 1. Your solution must use `asyncio` for handling concurrency. 2. Create an asynchronous function `fetch_content(url)` that fetches the content from the given URL. 3. Create another asynchronous function `save_content(url, content)` that saves the content to a file named after the domain of the URL. 4. Use `asyncio.gather()` to run multiple fetch tasks concurrently. Input: - A list of URLs (strings). Output: - The content of each URL should be saved in a separate file named after the domain of the URL, with the extension `.txt` (e.g., for `http://example.com`, save the content to `example.com.txt`). Example: ```python import asyncio import aiohttp from urllib.parse import urlparse # Example list of URLs urls = [ \'http://example.com\', \'http://example.org\', \'http://example.net\' ] async def fetch_content(url): Fetch content from URL asynchronously. async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def save_content(url, content): Save content to a file named after the domain of the URL. domain = urlparse(url).netloc filename = f\\"{domain}.txt\\" with open(filename, \'w\') as file: file.write(content) async def main(urls): Fetch and save content for each URL concurrently. tasks = [] for url in urls: content = await fetch_content(url) task = asyncio.create_task(save_content(url, content)) tasks.append(task) await asyncio.gather(*tasks) # Running the asyncio program asyncio.run(main(urls)) ``` Constraints: 1. Handle exceptions appropriately to ensure that one failed URL does not crash the entire program. 2. Use a timeout mechanism to avoid waiting indefinitely for a response. 3. Ensure that the `fetch_content` function uses a reasonable timeout for fetching the content (e.g., 10 seconds). 4. The program should manage resources efficiently and avoid blocking operations. # Performance requirements: - The solution must be able to handle at least 50 URLs concurrently without performance degradation. - Make sure to handle I/O-bound operations using `asyncio` to ensure non-blocking behavior. # Additional Notes: - Consider edge cases such as invalid URLs or network failures and handle them gracefully by logging errors. Create a working Python script that adheres to the above requirements and demonstrates the use of `asyncio` for concurrent operations.","solution":"import asyncio import aiohttp from urllib.parse import urlparse # Example list of URLs urls = [ \'http://example.com\', \'http://example.org\', \'http://example.net\' ] async def fetch_content(url): Fetch content from URL asynchronously. try: async with aiohttp.ClientSession() as session: async with session.get(url, timeout=10) as response: response.raise_for_status() return await response.text() except Exception as e: print(f\\"Failed to fetch {url}: {e}\\") return None async def save_content(url, content): Save content to a file named after the domain of the URL. if content is None: return domain = urlparse(url).netloc filename = f\\"{domain}.txt\\" try: with open(filename, \'w\') as file: file.write(content) except Exception as e: print(f\\"Failed to save content for {url}: {e}\\") async def main(urls): Fetch and save content for each URL concurrently. tasks = [fetch_and_save(url) for url in urls] await asyncio.gather(*tasks) async def fetch_and_save(url): Fetch content and then save it to a file. content = await fetch_content(url) await save_content(url, content) # Running the asyncio program if __name__ == \\"__main__\\": asyncio.run(main(urls))"},{"question":"# Command-Line Application Using `optparse` Objective You need to write a Python script that uses the `optparse` module to parse command-line options. The script should process a text file, performing different actions based on the options provided. Requirements 1. **Option Parsing**: - Implement an option to specify an input file (`-i` or `--input`). - Implement an option to count the number of lines in the file (`-l` or `--lines`). - Implement an option to count the number of words in the file (`-w` or `--words`). - Implement an option to count the number of characters in the file (`-c` or `--chars`). 2. **Default Value**: - If no file is specified, the script should use a default file named `default.txt`. 3. **Constraints**: - Your script should raise an error if the specified file does not exist. - The script should print a usage message if no options are provided or if an incorrect option is specified. 4. **Custom Callback**: - Implement a custom callback to validate that the specified input file has a `.txt` extension. If it does not, raise an appropriate error. 5. **Help Message**: - Generate a help message that provides information about each option. Input - Command-line arguments. Output - Depending on the options provided, print the number of lines, words, or characters in the specified file. - Display an error if the input file does not exist or if it does not have a `.txt` extension. - Display the usage message if required. Example Usage ```bash python yourscript.py --input myfile.txt --lines ``` Example Output ``` Number of lines: 50 ``` ```bash python yourscript.py --input myfile.doc --lines ``` Example Output ``` Usage: yourscript.py [options] yourscript.py: error: Invalid file extension. Only .txt files are allowed. ``` ```bash python yourscript.py --lines ``` Example Output ``` Number of lines: 100 ``` ```bash python yourscript.py --help ``` Example Output ``` Usage: yourscript.py [options] Options: -h, --help show this help message and exit -i FILE, --input=FILE specify the input file -l, --lines count the number of lines in the file -w, --words count the number of words in the file -c, --chars count the number of characters in the file ``` Implementation Implement the solution according to the requirements specified using the `optparse` module.","solution":"import optparse import os def validate_file(option, opt_str, value, parser): if not value.endswith(\'.txt\'): raise optparse.OptionValueError(\\"Invalid file extension. Only .txt files are allowed.\\") if not os.path.exists(value): raise optparse.OptionValueError(f\\"File {value} does not exist.\\") setattr(parser.values, option.dest, value) def process_file(filename, count_lines, count_words, count_chars): with open(filename, \'r\') as file: content = file.read() lines = content.splitlines() words = content.split() chars = content if count_lines: print(f\\"Number of lines: {len(lines)}\\") if count_words: print(f\\"Number of words: {len(words)}\\") if count_chars: print(f\\"Number of characters: {len(chars)}\\") def main(): parser = optparse.OptionParser(usage=\\"usage: %prog [options]\\") parser.add_option(\'-i\', \'--input\', type=\'string\', dest=\'filename\', help=\'specify the input file\', action=\'callback\', callback=validate_file) parser.add_option(\'-l\', \'--lines\', action=\'store_true\', dest=\'count_lines\', help=\'count the number of lines in the file\') parser.add_option(\'-w\', \'--words\', action=\'store_true\', dest=\'count_words\', help=\'count the number of words in the file\') parser.add_option(\'-c\', \'--chars\', action=\'store_true\', dest=\'count_chars\', help=\'count the number of characters in the file\') options, args = parser.parse_args() if not (options.count_lines or options.count_words or options.count_chars): parser.print_help() return filename = options.filename if options.filename else \'default.txt\' process_file(filename, options.count_lines, options.count_words, options.count_chars) if __name__ == \\"__main__\\": main()"},{"question":"Design and implement a set of dataclasses that represent a simple e-commerce system using Python\'s `dataclasses` module. Your solution should demonstrate the following capabilities: 1. **Class and Field Definitions**: Create dataclasses for `Customer`, `Product`, `OrderItem`, and `Order`. Use appropriate type annotations for fields. 2. **Default Values**: Use `field` function to provide default values for fields where applicable. Ensure at least one use of a `default_factory` for a mutable type. 3. **Methods**: Implement custom methods for: - Calculating the total cost of an `Order`. - Adding an `OrderItem` to an `Order`. - Printing a summary representation of an `Order`. 4. **Frozen Instances**: Create a frozen version of the `Product` dataclass to ensure products cannot be modified once created. 5. **Keyword-only Fields**: Use keyword-only fields in one of the dataclasses to enforce better clarity when creating instances. 6. **Inheritance**: Implement inheritance where `SpecialCustomer` (inherits from `Customer`) has additional discount attribute and method to apply discount. 7. **Post-init Processing**: Use `__post_init__` to perform additional initialization tasks in one of your dataclasses. # Example Usage - Create a few `Product` instances. - Create a `Customer` and a `SpecialCustomer` instance. - Add `OrderItem`s to `Order`s and relate them to customers. - Print summaries and calculate total costs applying discounts where applicable. # Constraints - Use Python 3.10 or higher. - Your code should handle edge cases and invalid data gracefully. - Include type annotations for clarity. - Your solution should avoid mutable default arguments issue by properly using `default_factory`. # Performance Requirements - Ensure that your solution works efficiently for up to 100 orders with each order containing up to 20 items. # Input and Output Formats For simplicity, you do not need to handle user input through a prompt. Use hard-coded examples of customers, products, and orders within your script for demonstration. ```python @dataclass class Product: ... @dataclass class Customer: ... @dataclass class SpecialCustomer(Customer): ... @dataclass class OrderItem: ... @dataclass class Order: ... # Example object creations and method calls # Add your implementation here def main(): # Example data and operations pass if __name__ == \\"__main__\\": main() ``` # Evaluation Your solution will be evaluated based on: - Correct usage of the `dataclasses` module. - Adhering to the problem constraints. - Code clarity and organization. - Handling of edge cases and performance considerations. Good luck, and happy coding!","solution":"from dataclasses import dataclass, field from typing import List @dataclass(frozen=True) class Product: product_id: int name: str price: float @dataclass class Customer: customer_id: int name: str email: str @dataclass class SpecialCustomer(Customer): discount: float = field(default=0.1) # 10% discount by default def apply_discount(self, amount: float) -> float: return amount * (1 - self.discount) @dataclass class OrderItem: product: Product quantity: int @dataclass class Order: order_id: int customer: Customer items: List[OrderItem] = field(default_factory=list) def total_cost(self) -> float: total = sum(item.product.price * item.quantity for item in self.items) if isinstance(self.customer, SpecialCustomer): total = self.customer.apply_discount(total) return total def add_item(self, item: OrderItem) -> None: self.items.append(item) def summary(self) -> str: summary = f\\"Order ID: {self.order_id}nCustomer: {self.customer.name}nItems:n\\" for item in self.items: summary += f\\"- {item.product.name} x{item.quantity} @ {item.product.price}n\\" summary += f\\"Total cost: {self.total_cost():.2f}n\\" return summary def main(): p1 = Product(product_id=1, name=\\"Laptop\\", price=999.99) p2 = Product(product_id=2, name=\\"Mouse\\", price=25.50) p3 = Product(product_id=3, name=\\"Keyboard\\", price=75.00) customer_regular = Customer(customer_id=1, name=\\"John Doe\\", email=\\"john@example.com\\") customer_special = SpecialCustomer(customer_id=2, name=\\"Jane Smith\\", email=\\"jane@example.com\\", discount=0.2) order1 = Order(order_id=1, customer=customer_regular) order1.add_item(OrderItem(product=p1, quantity=1)) order1.add_item(OrderItem(product=p2, quantity=2)) order2 = Order(order_id=2, customer=customer_special) order2.add_item(OrderItem(product=p3, quantity=1)) order2.add_item(OrderItem(product=p2, quantity=3)) print(order1.summary()) print(order2.summary()) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Object Manipulation in Python Objective: Write a Python function that inspects an object and performs specific attribute manipulations based on the attribute names and values provided. This function should have the following features: 1. **Get Attributes**: Fetch the value of an attribute by its name. 2. **Set Attributes**: Set a new value for an attribute, creating it if it does not exist. 3. **Delete Attributes**: Delete an attribute by its name if it exists. 4. **List Attributes**: List all the attributes of an object. Function Signature: ```python def manage_object_attributes(obj, action, attr_name=None, value=None): Manages attributes of an object based on the action provided. Parameters: obj (object): The target object whose attributes are to be managed. action (str): The action to be performed. It can be \'get\', \'set\', \'del\', or \'list\'. attr_name (str, optional): The name of the attribute to manipulate. Required for \'get\', \'set\', and \'del\'. value (any, optional): The value to set the attribute to. Required for the \'set\' action. Returns: any: The value of the attribute for \'get\', the result of the \'del\' action, or the list of attributes for \'list\'. None: For \'set\' action and on errors. Constraints: - The action must be one of [\'get\', \'set\', \'del\', \'list\']. - For \'get\', \'set\', and \'del\' actions, attr_name must be a non-empty string. - For \'set\' action, value must be provided. - The function should handle any errors gracefully and return None in case of an error. pass ``` Examples: ```python class TestClass: def __init__(self): self.attr1 = 10 self.attr2 = \\"Hello\\" obj = TestClass() # Example 1: Get an attribute print(manage_object_attributes(obj, \'get\', \'attr1\')) # Output: 10 # Example 2: Set a new attribute manage_object_attributes(obj, \'set\', \'attr3\', 20) print(manage_object_attributes(obj, \'get\', \'attr3\')) # Output: 20 # Example 3: Delete an attribute print(manage_object_attributes(obj, \'del\', \'attr2\')) # Output: True (if the attribute existed and was deleted) # Example 4: List all attributes print(manage_object_attributes(obj, \'list\')) # Output: [\'attr1\', \'attr3\'] (depending on previous actions) ``` # Constraints: - The `action` parameter must be strictly one of the following strings: `\'get\'`, `\'set\'`, `\'del\'`, `\'list\'`. - The `attr_name` parameter, when required, must be a non-empty string. - The `value` parameter, when required, must not be `None`. - The function should handle any exceptions gracefully and indicate a failure by returning `None`. # Notes: - You may make use of Python\'s built-in `getattr`, `setattr`, `delattr`, and `dir` functions for implementing the function. - For the `list` action, ensure that only regular attributes (excluding special methods) are included in the output.","solution":"def manage_object_attributes(obj, action, attr_name=None, value=None): Manages attributes of an object based on the action provided. Parameters: obj (object): The target object whose attributes are to be managed. action (str): The action to be performed. It can be \'get\', \'set\', \'del\', or \'list\'. attr_name (str, optional): The name of the attribute to manipulate. Required for \'get\', \'set\', and \'del\'. value (any, optional): The value to set the attribute to. Required for the \'set\' action. Returns: any: The value of the attribute for \'get\', the result of the \'del\' action, or the list of attributes for \'list\'. None: For \'set\' action and on errors. Constraints: - The action must be one of [\'get\', \'set\', \'del\', \'list\']. - For \'get\', \'set\', and \'del\' actions, attr_name must be a non-empty string. - For \'set\' action, value must be provided. - The function should handle any errors gracefully and return None in case of an error. try: if action == \'get\': if attr_name is None or attr_name == \'\': return None return getattr(obj, attr_name) elif action == \'set\': if attr_name is None or attr_name == \'\' or value is None: return None setattr(obj, attr_name, value) return None elif action == \'del\': if attr_name is None or attr_name == \'\': return None if hasattr(obj, attr_name): delattr(obj, attr_name) return True return False elif action == \'list\': return [attr for attr in dir(obj) if not attr.startswith(\'__\')] else: return None except Exception as e: return None"},{"question":"<|Analysis Begin|> The provided documentation explains two main types of objects in Python\'s C API: instance method objects and method objects. These objects are related to how Python handles bound methods and instance methods internally. Here are some key points from the documentation: 1. **Instance Method Objects**: - Utilize `PyInstanceMethod_Type`. - Created with `PyInstanceMethod_New(func)`. - Checked with `PyInstanceMethod_Check(o)`. - Associated function retrieved with `PyInstanceMethod_Function(im)` or `PyInstanceMethod_GET_FUNCTION(im)`. 2. **Method Objects**: - Utilize `PyMethod_Type`. - Created with `PyMethod_New(func, self)`. - Checked with `PyMethod_Check(o)`. - Associated function retrieved with `PyMethod_Function(meth)` or `PyMethod_GET_FUNCTION(meth)`. - The instance associated with the method retrieved with `PyMethod_Self(meth)` or `PyMethod_GET_SELF(meth)`. Based on this information, a challenging question can be crafted to assess the understanding and usage of these methods and objects, encapsulated in Python. <|Analysis End|> <|Question Begin|> # Question: Custom Method Binding in Python Objective You are required to create a Python class that simulates custom method binding using the concepts of instance methods and method objects, as described in the provided documentation. Requirements 1. Implement a class `InstanceMethodSimulator` that: - Initializes with a callable object (function). - Simulates binding this function to an instance similar to how Pythonâ€™s internal method objects work. 2. Provide the following methods: - `bind_to_instance(instance)`: Binds the stored function to the given instance and returns a method-like callable. - `get_function()`: Returns the original stored function. - `is_instance_method(obj)`: Checks if the given object, `obj`, behaves like a bound method to an instance. Input and Output Formats - `InstanceMethodSimulator` should accept any callable (function, lambda, etc.) during initialization. - `bind_to_instance(instance)` should accept any object as `instance`. - `get_function()` should return the original function passed during initialization. - `is_instance_method(obj)` should return `True` if `obj` behaves like a bound method. Implementation Example ```python class InstanceMethodSimulator: def __init__(self, func): pass def bind_to_instance(self, instance): pass def get_function(self): pass def is_instance_method(self, obj): pass # Example Usage def example_func(self, x): return x * 2 simulator = InstanceMethodSimulator(example_func) bound_method = simulator.bind_to_instance(instance) assert bound_method(3) == 6 assert simulator.get_function() is example_func assert simulator.is_instance_method(bound_method) ``` Constraints - You must not use any built-in Python functionalities or decorators that simplify method binding (like `@staticmethod`, `@classmethod`, etc.). - Ensure that bound methods correctly simulate the behavior of accessing the instance\'s attributes and methods. Performance Requirements - The solution should efficiently handle multiple bindings and method invocations without significant overhead. Notes - Focus on understanding how method binding internally works in Python and how you can simulate this behavior using classes and callables. - The objective is to deepen your understanding of Python\'s object model and method handling.","solution":"class InstanceMethodSimulator: def __init__(self, func): self.func = func def bind_to_instance(self, instance): Binds the stored function to the given instance and returns a method-like callable. def bound_method(*args, **kwargs): return self.func(instance, *args, **kwargs) return bound_method def get_function(self): Returns the original stored function. return self.func def is_instance_method(self, obj): Returns True if the given object behaves like a bound method. # Check if the object is a method bound to an instance return callable(obj)"},{"question":"# SAX XML Parsing with ContentHandler in Python **Objective:** Demonstrate your understanding of the `xml.sax.handler.ContentHandler` class by implementing a custom SAX content handler to parse an XML document and extract specific information. **Problem Statement:** Write a class `CustomContentHandler` that inherits from `xml.sax.handler.ContentHandler`. This handler will parse a given XML document and produce a summary of all elements and their attributes. The summary should include the following information for each element: - Element name - A list of all attributes (both names and values) - The text content of the element, if any You should also handle the appropriate start and end of the document events. **Input Format:** The input consists of: 1. An XML string **Methods to Implement:** - `startDocument(self)`: Initialize necessary structures. - `endDocument(self)`: Finalize and print the summary. - `startElement(self, name, attrs)`: Capture the element and its attributes. - `characters(self, content)`: Capture the text content of elements. - `endElement(self, name)`: Finalize the capture process for an element. **Output Format:** The output should be a summary printed to the console, showing each element, its attributes, and text content if present. **Example:** Given the following XML string: ```xml <library> <book id=\\"1\\" genre=\\"Fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> </book> <book id=\\"2\\" genre=\\"Science\\"> <title>A Brief History of Time</title> <author>Stephen Hawking</author> </book> </library> ``` The handler should print the following summary: ``` Start Document Element: library Element: book, Attributes: id=1, genre=Fiction Element: title, Text: The Great Gatsby Element: author, Text: F. Scott Fitzgerald Element: book, Attributes: id=2, genre=Science Element: title, Text: A Brief History of Time Element: author, Text: Stephen Hawking End Document ``` **Constraints:** - Ensure the handler correctly parses nested elements and their attributes. - Handle character data correctly, even if split into multiple chunks. **Additional Requirements:** - Your code should not use any third-party XML libraries beyond the standard `xml.sax` library. - Include necessary error handling for well-formedness of the given XML input. **Implement the following code template:** ```python import xml.sax class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() # Initialize any necessary data structures here def startDocument(self): print(\\"Start Document\\") def endDocument(self): print(\\"End Document\\") def startElement(self, name, attrs): attr_string = \', \'.join([f\\"{attr}={value}\\" for attr, value in attrs.items()]) if attr_string: print(f\\"Element: {name}, Attributes: {attr_string}\\") else: print(f\\"Element: {name}\\") def characters(self, content): # Handle character content if content.strip(): print(f\\"Text: {content.strip()}\\") def endElement(self, name): pass # Could be used if any finalization per element is needed # Sample usage xml_string = <library> <book id=\\"1\\" genre=\\"Fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> </book> <book id=\\"2\\" genre=\\"Science\\"> <title>A Brief History of Time</title> <author>Stephen Hawking</author> </book> </library> if __name__ == \\"__main__\\": handler = CustomContentHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) ```","solution":"import xml.sax class CustomContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() # Initialize any necessary data structures here self.current_element = \'\' self.element_data = \'\' def startDocument(self): print(\\"Start Document\\") def endDocument(self): print(\\"End Document\\") def startElement(self, name, attrs): self.current_element = name attr_string = \', \'.join([f\\"{attr}={value}\\" for attr, value in attrs.items()]) if attr_string: print(f\\"Element: {name}, Attributes: {attr_string}\\") else: print(f\\"Element: {name}\\") self.element_data = \'\' def characters(self, content): # Handle character content self.element_data += content.strip() def endElement(self, name): if self.element_data: print(f\\"Text: {self.element_data}\\") self.element_data = \'\'"},{"question":"**Question: Implement a Cookie Management System** You are tasked with implementing a simplified version of a cookie management system using the `http.cookiejar` module that meets the following requirements: 1. **Cookie Storage and Retrieval:** - Write a function `setup_cookiejar(policy=None)` that creates and returns an instance of `CookieJar` or `FileCookieJar` based on a given policy. - Write a function `save_cookies(cookiejar, filename)` that saves the cookies from the `cookiejar` to the specified `filename`. - Write a function `load_cookies(cookiejar, filename)` that loads the cookies from the specified `filename` into the `cookiejar`. 2. **Cookie Management Policies:** - Write a class `CustomCookiePolicy` that subclasses `DefaultCookiePolicy` and overrides the `set_ok`, `return_ok`, and `domain_return_ok` methods to implement custom rules: - A cookie should only be accepted if it comes from domains containing the word \\"example\\". - A cookie should only be returned to domains containing the word \\"example\\". - A cookie should not be returned to blocked domains specified in an attribute `blocked_domains`. 3. **Using the Cookies:** - Write a script that creates an instance of `MozillaCookieJar` with `CustomCookiePolicy` and demonstrates the following: - Loading cookies from a `cookies.txt` file. - Adding a cookie from the domain \\"example.com\\". - Saving the cookies to a `saved_cookies.txt` file. - Loading the cookies back from `saved_cookies.txt` to verify that the saved cookie is present. Input and Output Specifications - For the `setup_cookiejar(policy=None)` function: - **Input:** An optional policy parameter, which is an instance of `CookiePolicy`. - **Output:** An instance of `CookieJar` or `FileCookieJar`. - For the `save_cookies(cookiejar, filename)` function: - **Input:** - `cookiejar`: An instance of `FileCookieJar`. - `filename`: A string representing the file name where cookies will be saved. - **Output:** None. - For the `load_cookies(cookiejar, filename)` function: - **Input:** - `cookiejar`: An instance of `FileCookieJar`. - `filename`: A string representing the file name from where cookies will be loaded. - **Output:** None. - For the `CustomCookiePolicy` class: - **Input:** None. - **Output:** A class that implements custom cookie management policies. - For the script demonstration (example): - **Input:** None. - **Output:** Logs demonstrating the loading, adding, saving, and loading back of cookies. Constraints and Limitations - Ensure the `CustomCookiePolicy` appropriately filters cookies based on the given domain rules. - Handle file operations with proper error checking and exception handling. Performance Requirements - The implementation should efficiently manage in-memory cookies and perform file operations for saving and loading cookies with optimal time complexity. **Example Usage:** ```python # Define the CustomCookiePolicy class class CustomCookiePolicy(http.cookiejar.DefaultCookiePolicy): def __init__(self, blocked_domains=None, allowed_domains=None, netscape=True, rfc2965=False, **kwargs): super().__init__(blocked_domains, allowed_domains, netscape, rfc2965, **kwargs) def set_ok(self, cookie, request): return \'example\' in cookie.domain def return_ok(self, cookie, request): return \'example\' in cookie.domain def domain_return_ok(self, domain, request): return \'example\' in domain and (self.blocked_domains is None or domain not in self.blocked_domains) # Setup functions def setup_cookiejar(policy=None): return http.cookiejar.MozillaCookieJar(policy=policy) def save_cookies(cookiejar, filename): cookiejar.save(filename) def load_cookies(cookiejar, filename): cookiejar.load(filename) # Main script example if __name__ == \\"__main__\\": # Create cookie jar with CustomCookiePolicy policy = CustomCookiePolicy(blocked_domains=[\\"blocked.com\\"]) cj = setup_cookiejar(policy) # Load cookies from a file load_cookies(cj, \\"cookies.txt\\") # Add a cookie (assumes http.cookiejar.Cookie is available) c = http.cookiejar.Cookie( version=0, name=\\"example_cookie\\", value=\\"value\\", port=None, port_specified=False, domain=\\"example.com\\", domain_specified=True, domain_initial_dot=False, path=\\"/\\", path_specified=True, secure=False, expires=None, discard=False, comment=None, comment_url=None, rest={}, rfc2109=False ) cj.set_cookie(c) # Save cookies to a file save_cookies(cj, \\"saved_cookies.txt\\") # Reload cookies to verify loaded_cj = setup_cookiejar(policy) load_cookies(loaded_cj, \\"saved_cookies.txt\\") # Verify and print loaded cookies for cookie in loaded_cj: print(f\\"Loaded Cookie: {cookie.name} from {cookie.domain}\\") ```","solution":"import http.cookiejar # Custom Cookie Policy class CustomCookiePolicy(http.cookiejar.DefaultCookiePolicy): blocked_domains = [\\"blocked.com\\"] def set_ok(self, cookie, request): return \'example\' in cookie.domain def return_ok(self, cookie, request): return \'example\' in cookie.domain def domain_return_ok(self, domain, request): return \'example\' in domain and domain not in self.blocked_domains # Setup cookie jar def setup_cookiejar(policy=None): Creates and returns an instance of MozillaCookieJar with given policy. return http.cookiejar.MozillaCookieJar(policy=policy) # Save cookies to file def save_cookies(cookiejar, filename): Saves the cookies from the cookiejar to the specified filename. cookiejar.save(filename, ignore_discard=True, ignore_expires=True) # Load cookies from file def load_cookies(cookiejar, filename): Loads the cookies from the specified filename into the cookiejar. cookiejar.load(filename, ignore_discard=True, ignore_expires=True) # Main script demonstration if __name__ == \\"__main__\\": # Create cookie jar with CustomCookiePolicy policy = CustomCookiePolicy() cj = setup_cookiejar(policy) # Add a cookie cookie = http.cookiejar.Cookie( version=0, name=\\"example_cookie\\", value=\\"value\\", port=None, port_specified=False, domain=\\"example.com\\", domain_specified=True, domain_initial_dot=False, path=\\"/\\", path_specified=True, secure=False, expires=None, discard=False, comment=None, comment_url=None, rest={}, rfc2109=False ) cj.set_cookie(cookie) # Save cookies to a file save_cookies(cj, \\"saved_cookies.txt\\") # Reload cookies to verify loaded_cj = setup_cookiejar(policy) load_cookies(loaded_cj, \\"saved_cookies.txt\\") for cookie in loaded_cj: print(f\\"Loaded Cookie: {cookie.name} from {cookie.domain}\\")"},{"question":"# XML Data Extraction and Transformation You are required to use the `xml.parsers.expat` module to parse an XML document and extract specific data. Given an XML document containing information about books, your task is to write a function `extract_books_data(xml_data)` that parses the input XML data and returns a structured list of dictionaries containing book information. Input - `xml_data` (string): A string containing valid XML data with the following structure: ```xml <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <genre>Fiction</genre> <price>10.99</price> </book> <book id=\\"2\\"> <title>Book Title 2</title> <author>Author 2</author> <year>2010</year> <genre>Non-Fiction</genre> <price>15.99</price> </book> <!-- More book entries --> </library> ``` Output The function should return a list of dictionaries where each dictionary contains the following keys: - `id` (string): The ID of the book. - `title` (string): The title of the book. - `author` (string): The author of the book. - `year` (string): The publication year of the book. - `genre` (string): The genre of the book. - `price` (string): The price of the book. Example: For the input XML: ```xml <library> <book id=\\"1\\"> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <genre>Fiction</genre> <price>10.99</price> </book> </library> ``` The output should be: ```python [ { \\"id\\": \\"1\\", \\"title\\": \\"Book Title 1\\", \\"author\\": \\"Author 1\\", \\"year\\": \\"2001\\", \\"genre\\": \\"Fiction\\", \\"price\\": \\"10.99\\" } ] ``` Constraints - Assume the XML data provided is always valid and well-formed. - You are required to use the `xml.parsers.expat` module for parsing the XML data. Requirements 1. Define necessary handlers to process start and end elements. 2. Maintain a structure to collect data for each book. 3. Ensure proper parsing by handling character data appropriately. 4. Implement error handling to catch and print errors during parsing using `ExpatError`. You may use the following template to get started: ```python import xml.parsers.expat def extract_books_data(xml_data): books = [] current_book = None current_data = None def start_element(name, attrs): nonlocal current_book if name == \'book\': current_book = { \'id\': attrs[\'id\'] } def end_element(name): nonlocal current_book if name == \'book\': books.append(current_book) current_book = None def char_data(data): nonlocal current_data if current_book is not None: current_data = data def element_data(name): nonlocal current_book if current_book is not None and current_data is not None: current_book[name] = current_data parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data) except xml.parsers.expat.ExpatError as e: print(f\\"Error: {e}\\") return books ``` Note: This template is incomplete and requires implementing the proper logic for handling character data and extracting elements data.","solution":"import xml.parsers.expat def extract_books_data(xml_data): books = [] current_book = None current_tag = None def start_element(name, attrs): nonlocal current_book, current_tag if name == \'book\': current_book = {\'id\': attrs[\'id\']} current_tag = name def end_element(name): nonlocal current_book, current_tag if name == \'book\': books.append(current_book) current_book = None current_tag = None def char_data(data): nonlocal current_book, current_tag if current_book is not None and current_tag is not None and current_tag != \'book\': current_book[current_tag] = data.strip() parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data, True) except xml.parsers.expat.ExpatError as e: print(f\\"Error: {e}\\") return books"},{"question":"**Out-of-Core Learning with Scikit-Learn** You have been given a dataset of textual documents that is too large to fit into memory all at once. Your task is to build an out-of-core learning pipeline in Python using scikit-learn to classify these documents into different categories incrementally. # Requirements: 1. **Data Streaming**: Read the data in small mini-batches from a hypothetical file. 2. **Feature Extraction**: Use `HashingVectorizer` for feature extraction. 3. **Incremental Learning**: Use `SGDClassifier` for classification. # Input: - `filepath` (str): Path to the data file. The file contains one document per line, formatted as `\\"{category}t{document}\\"`. - `batch_size` (int): Size of the mini-batches to be read. # Output: - Return a dictionary where the keys are mini-batch indices (starting from 0) and values are the accuracy scores of the classifier on a validation set after processing each mini-batch. # Constraints: - You should assume that the validation set can fit into memory and will be provided as a list of tuples `validation_data = [(category1, doc1), (category2, doc2), ...]`. # Performance: - The solution must handle large datasets efficiently and should have reasonable runtime performance. # Sample Data Format: `data.txt`: ``` sportstSoccer is a great sport. politicstThe election is coming up. sportstThe basketball team won. ... ``` # Example Usage: ```python def out_of_core_learning(filepath: str, batch_size: int, validation_data: list) -> dict: # Your implementation here # Example usage: validation_data = [(\'sports\', \'Baseball is a summer game.\'), (\'politics\', \'Parliament passed a new bill.\')] results = out_of_core_learning(\'data.txt\', 100, validation_data) print(results) ``` # Additional Information: - You need to use the `HashingVectorizer` for converting text to feature vectors. - Use `SGDClassifier` with the `partial_fit` method for incremental learning. - Ensure to handle potential new classes by initializing the classifier with all possible classes. Write a function `out_of_core_learning` that meets the requirements specified.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def out_of_core_learning(filepath: str, batch_size: int, validation_data: list) -> dict: vectorizer = HashingVectorizer(decode_error=\'ignore\', n_features=2**20) classifier = SGDClassifier() classes = set() # To store all unique classes encountered validation_X, validation_y = [], [] for category, doc in validation_data: validation_X.append(doc) validation_y.append(category) classes.add(category) validation_X = vectorizer.transform(validation_X) accuracy = {} batch_index = 0 def process_batch(batch_lines): X, y = [], [] for line in batch_lines: category, document = line.split(\'t\') X.append(document) y.append(category) classes.add(category) return X, y with open(filepath, \'r\') as file: while True: batch_lines = [file.readline().strip() for _ in range(batch_size)] batch_lines = [line for line in batch_lines if line] # Remove empty lines if not batch_lines: break batch_X, batch_y = process_batch(batch_lines) batch_X = vectorizer.transform(batch_X) classifier.partial_fit(batch_X, batch_y, classes=list(classes)) validation_pred = classifier.predict(validation_X) accuracy[batch_index] = accuracy_score(validation_y, validation_pred) batch_index += 1 return accuracy"},{"question":"# PyTorch Coding Assessment: Custom Neural Network and TorchScript Conversion **Objective:** This task is designed to assess your understanding of PyTorch and TorchScript by requiring you to: 1. Implement a PyTorch-based neural network model. 2. Convert this model into TorchScript. 3. Handle specific unsupported features in TorchScript. **Problem Statement:** You are required to implement a custom neural network model in PyTorch for a simple classification task. Then, you will convert this model to TorchScript while ensuring it successfully handles the constraints of TorchScript. **Requirements:** 1. Implement a PyTorch model named `CustomNet` with the following specifications: - Input dimension: 784 (28x28 images flattened) - Two hidden layers with dimensions 128 and 64, and ReLU activation. - Output layer with 10 units (for 10 classes) and LogSoftmax activation. 2. Implement a function `convert_to_script(model)` that: - Takes a PyTorch model as an input. - Converts and returns the model in TorchScript using `torch.jit.script`. 3. Handle the following TorchScript limitation: - Use only supported features: Avoid using any unsupported Python or Torch features in your model that would cause the `torch.jit.script` conversion to fail (e.g., avoid `try/except`, `async`, and unsupported magic methods). **Input:** - There are no direct inputs for this question. You will demonstrate the function implementation and conversion using a PyTorch model. **Output:** - The output of your implementation will be the TorchScript version of the `CustomNet` model. **Constraints:** - You should strictly adhere to the layer specifications mentioned. - Ensure no use of unsupported features as mentioned in the documentation. **Example Usage:** ```python import torch import torch.nn as nn import torch.nn.functional as F # Step 1: Define the custom model class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.log_softmax(self.fc3(x), dim=1) return x # Step 2: Function to convert the model to TorchScript def convert_to_script(model): script_model = torch.jit.script(model) return script_model # Example usage model = CustomNet() scripted_model = convert_to_script(model) print(scripted_model) ``` **Performance Notes:** - Your model should run efficiently without any significant performance bottlenecks. - The conversion using `torch.jit.script` should complete without errors. Demonstrate the implementation by defining the model and the conversion function, and showcase the output of the converted TorchScript model. Ensure your code is clean and follows the specified requirements.","solution":"import torch import torch.nn as nn import torch.nn.functional as F # Step 1: Define the custom model class CustomNet(nn.Module): def __init__(self): super(CustomNet, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.log_softmax(self.fc3(x), dim=1) return x # Step 2: Function to convert the model to TorchScript def convert_to_script(model): script_model = torch.jit.script(model) return script_model # Example usage model = CustomNet() scripted_model = convert_to_script(model) print(scripted_model)"},{"question":"*Title*: Inventory Management System using `Counter` and `defaultdict` # *Objective*: Assess the student\'s ability to use and manipulate advanced container datatypes `Counter` and `defaultdict` from the `collections` module to solve a real-world problem. # *Problem Statement*: You are tasked with creating an inventory management system for a small business. The business needs to keep track of the inventory of multiple products and the categories they belong to. Each product has a name and a category. Implement the following functionalities using `Counter` and `defaultdict` from the `collections` module: 1. **Add Product**: Add a product to the inventory with its category. 2. **Remove Product**: Remove a product from the inventory. 3. **Get Total Products**: Get the total number of items in the inventory. 4. **Get Category Count**: Get the number of distinct products in each category. 5. **Get Most Common Products**: Get the `n` most common products in the inventory sorted by their quantities. # *Function Definitions*: 1. **add_product(inventory, category_inventory, product, category)**: - **Input**: - `inventory` (`Counter`): A counter that keeps track of the number of each product. - `category_inventory` (`defaultdict` of type list): A dictionary that maps each category to a list of products in that category. - `product` (`str`): The name of the product to add. - `category` (`str`): The category of the product. - **Output**: None 2. **remove_product(inventory, category_inventory, product)**: - **Input**: - `inventory` (`Counter`): As above. - `category_inventory` (`defaultdict` of type list): As above. - `product` (`str`): The name of the product to remove. - **Output**: None 3. **get_total_products(inventory)**: - **Input**: - `inventory` (`Counter`): As above. - **Output**: - (`int`): Total number of items in the inventory. 4. **get_category_count(category_inventory)**: - **Input**: - `category_inventory` (`defaultdict` of type list): As above. - **Output**: - (`dict`): A dictionary mapping each category to the number of distinct products in that category. 5. **get_most_common_products(inventory, n)**: - **Input**: - `inventory` (`Counter`): As above. - `n` (`int`): The number of most common products to return. - **Output**: - (`list` of `tuple`): A list of tuples where each tuple contains a product name and its quantity, sorted by quantities in descending order. # *Constraints*: - Product names and categories are non-empty strings. - All products belong to a category; there are no uncategorized products. - When removing a product, if the quantity becomes zero, it should no longer be tracked in the `inventory`. # *Example*: ```python from collections import Counter, defaultdict inventory = Counter() category_inventory = defaultdict(list) add_product(inventory, category_inventory, \'apple\', \'fruit\') add_product(inventory, category_inventory, \'banana\', \'fruit\') add_product(inventory, category_inventory, \'apple\', \'fruit\') remove_product(inventory, category_inventory, \'banana\') add_product(inventory, category_inventory, \'carrot\', \'vegetable\') print(get_total_products(inventory)) # Output: 3 print(get_category_count(category_inventory)) # Output: {\'fruit\': 1, \'vegetable\': 1} print(get_most_common_products(inventory, 2)) # Output: [(\'apple\', 2), (\'carrot\', 1)] ``` # *Instructions*: 1. Ensure your implementation efficiently manages the data structures and their interactions. 2. Handle edge cases, such as removing products not in the inventory or adding products already existing. 3. Write clean, readable, and well-documented code. Good luck!","solution":"from collections import Counter, defaultdict def add_product(inventory, category_inventory, product, category): Adds a product to the inventory and its category. :param inventory: Counter, a counter that keeps track of the number of each product :param category_inventory: defaultdict(list), maps each category to a list of products in that category :param product: str, name of the product to add :param category: str, category of the product # Increment the count of the product in inventory inventory[product] += 1 # Add the product to the category if not already present if product not in category_inventory[category]: category_inventory[category].append(product) def remove_product(inventory, category_inventory, product): Removes a product from the inventory. If quantity becomes zero, it is removed from inventory. :param inventory: Counter, a counter that keeps track of the number of each product :param category_inventory: defaultdict(list), maps each category to a list of products in that category :param product: str, name of the product to remove if product in inventory: # Decrement the count of the product in inventory inventory[product] -= 1 # If the product quantity becomes zero, remove it from inventory if inventory[product] == 0: del inventory[product] # Also remove it from the category list it belongs to for category, products in category_inventory.items(): if product in products: products.remove(product) break def get_total_products(inventory): Returns the total number of items in the inventory. :param inventory: Counter, a counter that keeps track of the number of each product :return: int, total number of items in the inventory return sum(inventory.values()) def get_category_count(category_inventory): Returns the number of distinct products in each category. :param category_inventory: defaultdict(list), maps each category to a list of products in that category :return: dict, a dictionary mapping each category to the number of distinct products in that category return {category: len(products) for category, products in category_inventory.items()} def get_most_common_products(inventory, n): Returns the n most common products in the inventory sorted by their quantities. :param inventory: Counter, a counter that keeps track of the number of each product :param n: int, the number of most common products to return :return: list of tuple, a list of tuples where each tuple contains a product name and its quantity return inventory.most_common(n)"},{"question":"**Coding Assessment Question: Advanced Data Plotting with Pandas** # Objective: Demonstrate your ability to use pandas for data visualization by performing the following tasks. # Tasks: 1. **Data Preparation**: - Load the provided CSV file `sales_data.csv` into a pandas DataFrame. The `sales_data.csv` contains sales information for a company with columns: `Date`, `Product_ID`, `Sales`, `Revenue`, and `Region`. 2. **Basic Plot**: - Create a line plot showing the trend of `Sales` over time for the entire dataset. 3. **Customized Plot**: - Create a scatter plot showing `Sales` vs `Revenue`. Use different colors for different `Region` and include a color legend. 4. **Subplots**: - Create subplots for `Sales` data for each `Region`. Arrange them in a 2x2 grid layout. Set individual titles for each subplot with the Region name. 5. **Error Bars**: - Calculate the mean and standard deviation of `Sales` by `Region`. Create a bar plot showing the mean `Sales` for each `Region` with error bars representing the standard deviation. 6. **Multivariate Plot**: - Use Andrews curves to plot the multivariate data (`Sales`, `Revenue`, and `Product_ID`) colored by `Region`. # Input: - A CSV file `sales_data.csv`. # Constraints: - Ensure that all plots are properly labeled with titles, axis labels, and legends where appropriate. - Handle missing data appropriately for each plot type as described in the documentation. # Expected Output: Submit the following: 1. Line plot of `Sales` over time. 2. Scatter plot of `Sales` vs `Revenue`, colored by `Region`. 3. Subplots of `Sales` for each `Region`. 4. Bar plot of mean `Sales` with standard deviation error bars by `Region`. 5. Andrews curves plot for the multivariate data. # Performance Requirements: - Ensure the code is efficient and leverages pandas built-in functions for calculations and plotting. - Use appropriate plotting customization to improve the readability and aesthetics of the plots.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from pandas.plotting import andrews_curves def load_sales_data(filename): return pd.read_csv(filename) def plot_sales_trend(df): df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) plt.figure(figsize=(10, 6)) df[\'Sales\'].plot() plt.title(\'Sales Trend Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Sales\') plt.show() def plot_sales_vs_revenue(df): plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'Sales\', y=\'Revenue\', hue=\'Region\', palette=\'viridis\') plt.title(\'Sales vs Revenue by Region\') plt.xlabel(\'Sales\') plt.ylabel(\'Revenue\') plt.legend(title=\'Region\') plt.show() def plot_sales_by_region(df): fig, axes = plt.subplots(2, 2, figsize=(14, 10)) regions = df[\'Region\'].unique() for ax, region in zip(axes.flatten(), regions): region_data = df[df[\'Region\'] == region] region_data[\'Date\'] = pd.to_datetime(region_data[\'Date\']) region_data.set_index(\'Date\', inplace=True) ax.plot(region_data[\'Sales\']) ax.set_title(f\'Sales in {region}\') ax.set_xlabel(\'Date\') ax.set_ylabel(\'Sales\') plt.tight_layout() plt.show() def plot_mean_sales_with_error_bars(df): sales_mean_std = df.groupby(\'Region\')[\'Sales\'].agg([\'mean\', \'std\']).reset_index() plt.figure(figsize=(10, 6)) plt.bar(sales_mean_std[\'Region\'], sales_mean_std[\'mean\'], yerr=sales_mean_std[\'std\'], capsize=5) plt.title(\'Mean Sales with Error Bars by Region\') plt.xlabel(\'Region\') plt.ylabel(\'Mean Sales\') plt.show() def plot_andrews_curves(df): plt.figure(figsize=(10, 6)) andrews_curves(df[[\'Region\', \'Sales\', \'Revenue\', \'Product_ID\']], \'Region\', colormap=\'viridis\') plt.title(\'Andrews Curves for Sales Data by Region\') plt.show()"},{"question":"# Python 3.10 Coding Assessment: Advanced Dataclasses Objective To assess your understanding of Python\'s `dataclasses` module, including the creation of dataclasses, field customization, and the use of utility functions. Problem Statement You are required to implement a dataclass to manage a library system. Each book in the library is represented by a `Book` class. Additionally, you need to manage the library inventory using a `LibraryInventory` class. The `LibraryInventory` class should provide functionalities to add books, remove books, and compute the total value of all books in the inventory. 1. **Book Class** - Fields: - `title`: A string representing the title of the book. - `author`: A string representing the author of the book. - `isbn`: A string representing the ISBN number of the book. It should be unique for each book. - `price`: A float representing the price of the book. - `stock`: An integer representing the number of copies available. - Methods: - A method to compute the total value of the book\'s stock (`total_value`). This method should return the product of `price` and `stock`. 2. **LibraryInventory Class** - Fields: - `inventory`: A dictionary where the key is the ISBN number and the value is an instance of the `Book` class. - Methods: - `add_book(book: Book)`: Adds a `Book` to the inventory. If the book already exists (matching ISBN), update the stock. - `remove_book(isbn: str, quantity: int)`: Removes the specified quantity of a book from the inventory. If the quantity to be removed is greater than the available stock, remove the book entirely. - `total_inventory_value()`: Computes the total value of all books in the inventory. Constraints 1. Ensure no two books in the inventory have the same ISBN. 2. When removing books, ensure that the quantity to be removed does not exceed the available stock. If it does, treat it as an error case and handle it by removing the book completely. 3. The class should provide a clear string representation of the inventory using the `__repr__()` method. Implementation Implement the `Book` and `LibraryInventory` classes as described above. Use the `@dataclass` decorator and the `field()` function where appropriate to manage default values and other behaviors. Example Usage ```python from dataclasses import dataclass, field from typing import Dict @dataclass class Book: title: str author: str isbn: str price: float stock: int def total_value(self) -> float: return self.price * self.stock @dataclass class LibraryInventory: inventory: Dict[str, Book] = field(default_factory=dict) def add_book(self, book: Book): if book.isbn in self.inventory: self.inventory[book.isbn].stock += book.stock else: self.inventory[book.isbn] = book def remove_book(self, isbn: str, quantity: int): if isbn in self.inventory: if self.inventory[isbn].stock <= quantity: del self.inventory[isbn] else: self.inventory[isbn].stock -= quantity def total_inventory_value(self) -> float: return sum(book.total_value() for book in self.inventory.values()) def __repr__(self) -> str: return f\\"LibraryInventory({self.inventory})\\" # Test case book1 = Book(title=\\"Book One\\", author=\\"Author A\\", isbn=\\"123\\", price=10.0, stock=5) book2 = Book(title=\\"Book Two\\", author=\\"Author B\\", isbn=\\"456\\", price=15.0, stock=3) library = LibraryInventory() library.add_book(book1) library.add_book(book2) library.remove_book(\\"123\\", 2) total_value = library.total_inventory_value() print(total_value) # Should output the total value of books in the inventory print(library) # Should provide a string representation of the library inventory ``` Ensure your classes and methods adhere to the problem constraints and handle edge cases appropriately.","solution":"from dataclasses import dataclass, field from typing import Dict @dataclass class Book: title: str author: str isbn: str price: float stock: int def total_value(self) -> float: return self.price * self.stock @dataclass class LibraryInventory: inventory: Dict[str, Book] = field(default_factory=dict) def add_book(self, book: Book): if book.isbn in self.inventory: self.inventory[book.isbn].stock += book.stock else: self.inventory[book.isbn] = book def remove_book(self, isbn: str, quantity: int): if isbn in self.inventory: if self.inventory[isbn].stock <= quantity: del self.inventory[isbn] else: self.inventory[isbn].stock -= quantity def total_inventory_value(self) -> float: return sum(book.total_value() for book in self.inventory.values()) def __repr__(self) -> str: return f\\"LibraryInventory({self.inventory})\\""},{"question":"**Problem Statement: Managing PyTorch with CUDA Environment Variables** You are required to write a Python function that ensures optimal CUDA configuration for a PyTorch application by setting specific environment variables according to given requirements. The function should take a list of configurations as input and set the corresponding environment variables for PyTorch. # Function Signature ```python def configure_pytorch_cuda(settings: List[str]) -> None: ``` # Input - `settings`: A list of strings representing CUDA environment variable configurations. Each string should follow the format `VARIABLE=VALUE`. # Output - The function should not return any value. It should only set the specified environment variables. # Constraints - Ensure that all valid variables from the documentation provided are recognized and set correctly. - Invalid or unrecognized variables should be ignored and logged as warnings. - Use Python\'s `os` module to set environment variables. - Specifically handle configurations for the following variables: - `PYTORCH_NO_CUDA_MEMORY_CACHING` - `PYTORCH_CUDA_ALLOC_CONF` - `PYTORCH_NVML_BASED_CUDA_CHECK` - `TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT` - `TORCH_CUDNN_V8_API_DISABLED` - `TORCH_ALLOW_TF32_CUBLAS_OVERRIDE` - `TORCH_NCCL_USE_COMM_NONBLOCKING` - `TORCH_NCCL_AVOID_RECORD_STREAMS` - `TORCH_CUDNN_V8_API_DEBUG` - `CUDA_VISIBLE_DEVICES` - `CUDA_LAUNCH_BLOCKING` - `CUBLAS_WORKSPACE_CONFIG` - `CUDNN_CONV_WSCAP_DBG` - `CUBLASLT_WORKSPACE_SIZE` - `CUDNN_ERRATA_JSON_FILE` - `NVIDIA_TF32_OVERRIDE` # Example ```python settings = [ \\"PYTORCH_NO_CUDA_MEMORY_CACHING=1\\", \\"CUDA_VISIBLE_DEVICES=0,1\\", \\"TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT=5000\\" ] configure_pytorch_cuda(settings) ``` After running the function, the following environment variables should be set: - `PYTORCH_NO_CUDA_MEMORY_CACHING=1` - `CUDA_VISIBLE_DEVICES=0,1` - `TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT=5000` # Additional Notes - Be sure to handle edge cases such as empty input lists or invalid configurations. - Log a warning using the `warnings` module for each invalid or unrecognized variable configuration. # Evaluation Criteria - Correctly sets environment variables based on input. - Proper handling of invalid or unrecognized variables by logging a warning. - Efficient and clear implementation.","solution":"import os import warnings from typing import List VALID_VARIABLES = { \\"PYTORCH_NO_CUDA_MEMORY_CACHING\\", \\"PYTORCH_CUDA_ALLOC_CONF\\", \\"PYTORCH_NVML_BASED_CUDA_CHECK\\", \\"TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\\", \\"TORCH_CUDNN_V8_API_DISABLED\\", \\"TORCH_ALLOW_TF32_CUBLAS_OVERRIDE\\", \\"TORCH_NCCL_USE_COMM_NONBLOCKING\\", \\"TORCH_NCCL_AVOID_RECORD_STREAMS\\", \\"TORCH_CUDNN_V8_API_DEBUG\\", \\"CUDA_VISIBLE_DEVICES\\", \\"CUDA_LAUNCH_BLOCKING\\", \\"CUBLAS_WORKSPACE_CONFIG\\", \\"CUDNN_CONV_WSCAP_DBG\\", \\"CUBLASLT_WORKSPACE_SIZE\\", \\"CUDNN_ERRATA_JSON_FILE\\", \\"NVIDIA_TF32_OVERRIDE\\" } def configure_pytorch_cuda(settings: List[str]) -> None: Sets the specified PyTorch CUDA environment variables. Parameters: - settings: List of strings in the format \\"VARIABLE=VALUE\\". The function does not return any value. It sets the recognized environment variables and logs a warning for any unrecognized variables. for setting in settings: try: variable, value = setting.split(\'=\', 1) except ValueError: warnings.warn(f\\"Invalid format for setting: \'{setting}\'\\") continue if variable in VALID_VARIABLES: os.environ[variable] = value else: warnings.warn(f\\"Unrecognized environment variable: \'{variable}\'\\")"},{"question":"# Gaussian Process Regression with Custom Kernels You are given a dataset representing temperature measurements over a period of time. The dataset contains two columns: `time` and `temperature`. Your task is to implement a model using the `GaussianProcessRegressor` from scikit-learn to predict future temperature values based on the given time points. You will use a combination of kernels to improve model performance. Input Format: - A Pandas DataFrame `df` with columns `time` (sorted in ascending order) and `temperature`. - A list of future time points `future_times` for which you need to predict temperatures. Output Format: - A NumPy array of predicted temperature values corresponding to the `future_times`. Constraints: - Use both the RBF kernel and RationalQuadratic kernel. - Optimize the hyperparameters for the best fit. Function Signature: ```python def predict_future_temperatures(df: pd.DataFrame, future_times: List[float]) -> np.ndarray: pass ``` Example: ```python import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, RationalQuadratic import numpy as np def predict_future_temperatures(df: pd.DataFrame, future_times: List[float]) -> np.ndarray: # Extract time and temperature from the dataframe time = df[\'time\'].values.reshape(-1, 1) temperature = df[\'temperature\'].values # Define the kernel: sum of RBF and RationalQuadratic kernel = RBF(length_scale=1.0) + RationalQuadratic(length_scale=1.0, alpha=1.0) # Initialize GaussianProcessRegressor with the kernel gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-2) # Fit the GPR model gpr.fit(time, temperature) # Predict temperatures for future time points future_times_reshaped = np.array(future_times).reshape(-1, 1) predicted_temperatures, std_devs = gpr.predict(future_times_reshaped, return_std=True) return predicted_temperatures # Example Data data = { \'time\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'temperature\': [15, 16, 15.5, 16.5, 17, 16, 16.5, 17.5, 18, 17.5] } df = pd.DataFrame(data) future_times = [11, 12, 13, 14, 15] # Predict future temperatures predicted = predict_future_temperatures(df, future_times) print(predicted) ```","solution":"import pandas as pd from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, RationalQuadratic import numpy as np from typing import List def predict_future_temperatures(df: pd.DataFrame, future_times: List[float]) -> np.ndarray: Predict future temperatures based on the given dataframe and future time points. Parameters: df (pd.DataFrame): DataFrame containing \'time\' and \'temperature\' columns. future_times (List[float]): List of future time points for prediction. Returns: np.ndarray: Array of predicted temperatures corresponding to the future times. # Extract time and temperature from the dataframe time = df[\'time\'].values.reshape(-1, 1) temperature = df[\'temperature\'].values # Define the kernel: sum of RBF and RationalQuadratic kernel = RBF(length_scale=1.0) + RationalQuadratic(length_scale=1.0, alpha=1.0) # Initialize GaussianProcessRegressor with the kernel gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-2) # Fit the GPR model gpr.fit(time, temperature) # Predict temperatures for future time points future_times_reshaped = np.array(future_times).reshape(-1, 1) predicted_temperatures, std_devs = gpr.predict(future_times_reshaped, return_std=True) return predicted_temperatures"},{"question":"To ensure that your online platform\'s user accounts have strong and diverse passwords, you are required to write a Python function that generates secure passwords based on specific constraints. Use the `secrets` module to leverage its cryptographically strong randomness functions. **Function Signature:** ```python def generate_secure_password(min_length: int, max_length: int, num_lowercase: int, num_uppercase: int, num_digits: int) -> str: pass ``` **Input:** - `min_length` (int): The minimum length of the generated password. - `max_length` (int): The maximum length of the generated password. - `num_lowercase` (int): The minimum number of lowercase letters the password should contain. - `num_uppercase` (int): The minimum number of uppercase letters the password should contain. - `num_digits` (int): The minimum number of digits the password should contain. **Output:** - The function should return a securely generated password (str) that meets all the specified constraints. If the constraints cannot be met within the given length range, raise a `ValueError`. **Constraints:** 1. `min_length` and `max_length` are positive integers and `min_length` <= `max_length`. 2. The total number of required characters (`num_lowercase` + `num_uppercase` + `num_digits`) should be less than or equal to `max_length`. **Example:** ```python generate_secure_password(10, 12, 3, 2, 4) # Possible output: \'aB1cDe2f3Gh\' ``` **Notes:** - The password must be randomly generated using functions from the `secrets` module to ensure it is secure. - Consider including other characters (such as symbols `@,#,, etc.`) to add more diversity to the passwords, and make sure the function still meets the minimum counts for lowercase, uppercase, and digits. **Performance Requirements:** - The solution should be efficient and generate the password in reasonable time, even when max_length is large. **Additional Information:** - Utilize `secrets.choice` for selecting random characters from sequences. - Use `secrets.randbelow` or `secrets.randbits` if necessary to aid in random selection. - The function should handle edge cases such as when `min_length` equals `max_length` or when the number of required characters exactly matches the `max_length`.","solution":"import secrets import string def generate_secure_password(min_length: int, max_length: int, num_lowercase: int, num_uppercase: int, num_digits: int) -> str: if min_length > max_length: raise ValueError(f\\"min_length {min_length} cannot be greater than max_length {max_length}\\") total_required = num_lowercase + num_uppercase + num_digits if total_required > max_length: raise ValueError(f\\"Total required characters {total_required} cannot be greater than max_length {max_length}\\") # Choose length password_length = secrets.randbelow(max_length - min_length + 1) + min_length # Create a list of characters to fill the password password_chars = [] # Add required characters password_chars.extend(secrets.choice(string.ascii_lowercase) for _ in range(num_lowercase)) password_chars.extend(secrets.choice(string.ascii_uppercase) for _ in range(num_uppercase)) password_chars.extend(secrets.choice(string.digits) for _ in range(num_digits)) # Fill the rest of the password length with a mix of all types of characters if len(password_chars) < password_length: all_characters = string.ascii_letters + string.digits + string.punctuation password_chars.extend(secrets.choice(all_characters) for _ in range(password_length - len(password_chars))) # Shuffle the list to ensure randomness secrets.SystemRandom().shuffle(password_chars) # Join the list into a string and return return \'\'.join(password_chars)"},{"question":"# HTML Parsing Assessment Task Objective: To assess your understanding of the `HTMLParser` module in Python by implementing a custom subclass to parse HTML content, extract specific elements, and handle the parsed data. Task: Implement a subclass of `HTMLParser` called `CustomHTMLParser` that extracts and processes information from HTML content. Specifically, your parser should do the following: 1. **Extract Text Content:** Collect and print all the text content found inside HTML tags. 2. **Identify Specific Tags:** Identify and print the number of times the `<div>` and `<span>` tags appear in the HTML content. 3. **Extract Attribute Values:** For each `<a>` tag encountered, print the value of its `href` attribute. 4. **Handle Comments:** Print all comments encountered in the HTML content. 5. **Count Nested Tags:** Count the number of nested tags inside each `<ul>` tag and print the result. Input: - A single string containing the HTML content to be parsed. Output: - Print the extracted text content found in the HTML. - Print the number of `<div>` and `<span>` tags encountered. - Print the `href` attributes found in `<a>` tags. - Print all comments in the HTML content. - Print the count of nested tags inside each `<ul>` tag. Constraints: - The input string will contain valid HTML but may include unquoted attributes or other quirks. - The parser should handle nested tags correctly. - Assume the HTML content does not contain any `<script>` or `<style>` tags. Example: ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() # Initialize counters and lists for storage self.div_count = 0 self.span_count = 0 self.text_content = [] self.a_hrefs = [] self.comments = [] self.ul_nested_counts = [] self.current_ul_depth = 0 self.in_ul = False def handle_starttag(self, tag, attrs): if tag == \'div\': self.div_count += 1 elif tag == \'span\': self.span_count += 1 elif tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.a_hrefs.append(attr[1]) elif tag == \'ul\': self.in_ul = True if self.current_ul_depth > 0: self.ul_nested_counts[-1] += 1 else: self.ul_nested_counts.append(0) self.current_ul_depth += 1 def handle_endtag(self, tag): if tag == \'ul\': self.current_ul_depth -= 1 if self.current_ul_depth == 0: self.in_ul = False def handle_data(self, data): self.text_content.append(data) def handle_comment(self, data): self.comments.append(data) html_content = \'\'\'<html> <head><title>Test</title></head> <body> <div>Content 1</div> <span>Text in span</span> <a href=\\"https://example.com\\">Link</a> <!-- A comment --> <ul> <li>Item 1</li> <li>Item 2<ul><li>Subitem 1</li></ul></li> </ul> </body> </html>\'\'\' parser = CustomHTMLParser() parser.feed(html_content) print(\\"Text Content:\\", \'\'.join(parser.text_content)) print(\\"Div count:\\", parser.div_count) print(\\"Span count:\\", parser.span_count) print(\\"Href attributes:\\", parser.a_hrefs) print(\\"Comments:\\", parser.comments) print(\\"UL nested tag counts:\\", parser.ul_nested_counts) ``` **Expected Output:** ``` Text Content: TestContent 1Text in spanLinkItem 1Item 2Subitem 1 Div count: 1 Span count: 1 Href attributes: [\'https://example.com\'] Comments: [\' A comment \'] UL nested tag counts: [1] ``` Note: - Use the provided structure and modify the methods within the `CustomHTMLParser` class to achieve the desired functionality. - Ensure to test your code with various HTML inputs to cover edge cases and nested structures.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() # Initialize counters and lists for storage self.div_count = 0 self.span_count = 0 self.text_content = [] self.a_hrefs = [] self.comments = [] self.ul_nested_counts = [] self.current_ul_depth = 0 self.in_ul = False def handle_starttag(self, tag, attrs): if tag == \'div\': self.div_count += 1 elif tag == \'span\': self.span_count += 1 elif tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.a_hrefs.append(attr[1]) elif tag == \'ul\': self.in_ul = True if self.current_ul_depth > 0: self.ul_nested_counts[-1] += 1 else: self.ul_nested_counts.append(0) self.current_ul_depth += 1 def handle_endtag(self, tag): if tag == \'ul\': self.current_ul_depth -= 1 if self.current_ul_depth == 0: self.in_ul = False def handle_data(self, data): self.text_content.append(data.strip()) def handle_comment(self, data): self.comments.append(data.strip()) def display_results(self): print(\\"Text Content:\\", \'\'.join(self.text_content)) print(\\"Div count:\\", self.div_count) print(\\"Span count:\\", self.span_count) print(\\"Href attributes:\\", self.a_hrefs) print(\\"Comments:\\", self.comments) print(\\"UL nested tag counts:\\", self.ul_nested_counts) html_content = \'\'\'<html> <head><title>Test</title></head> <body> <div>Content 1</div> <span>Text in span</span> <a href=\\"https://example.com\\">Link</a> <!-- A comment --> <ul> <li>Item 1</li> <li>Item 2<ul><li>Subitem 1</li></ul></li> </ul> </body> </html>\'\'\' parser = CustomHTMLParser() parser.feed(html_content) parser.display_results()"},{"question":"Objective: To test your understanding of Python\'s cryptographic services, specifically the \\"hashlib\\" and \\"secrets\\" modules, which are used to securely generate and handle hash values and random numbers. Problem Statement: You are tasked with creating a secure file verification system. The system will: 1. **Hash** the content of a given file using the SHA-256 algorithm. 2. **Create** a secure token to associate with the file. 3. **Verify** the integrity of the file using the hash and token. Instructions: 1. Implement two functions: `generate_file_token(file_path: str) -> str` and `verify_file_token(file_path: str, token: str) -> bool`. 2. The `generate_file_token` function should: - Read the contents of the file at the given `file_path`. - Compute the SHA-256 hash of the file\'s contents. - Generate a secure random token (at least 16 bytes in length) using the secrets module. - Serialize the hash and token as a single string, separated by a colon (`:`). - Return the serialized string. 3. The `verify_file_token` function should: - Take the `file_path` and the serialized `token` (i.e., the output of `generate_file_token`). - Deserialize the hash and token from the given token string. - Compute the SHA-256 hash of the current content of the file at `file_path`. - Compare the new hash with the one from the token. - Return `True` if the hashes match and the token is valid, otherwise return `False`. Constraints: - Assume the file size does not exceed 10MB. - Use the SHA-256 algorithm from the `hashlib` module. - The secure token must be at least 16 bytes in length. - Assume that file reading and writing permissions are properly set. Example: ```python # Suppose \\"example.txt\\" contains \\"Hello, World!\\" # Generate token for \\"example.txt\\" token = generate_file_token(\\"example.txt\\") # Example output: \\"fc3ff98e8c6a0d3087d515c0473f8677:1a2b3c4d5e6f70890123456789abcdef\\" # Verify the integrity of the file with the generated token is_valid = verify_file_token(\\"example.txt\\", token) # Expected output: True # Modify the content of \\"example.txt\\" # The content of \\"example.txt\\" is changed to \\"Goodbye, World!\\" # Verify the integrity of the file with the same token is_valid = verify_file_token(\\"example.txt\\", token) # Expected output: False ``` Note: Ensure you handle exceptions like file not found or read errors gracefully by displaying appropriate error messages.","solution":"import hashlib import secrets def generate_file_token(file_path: str) -> str: Generates a secure token for the given file. It reads the file, computes the SHA-256 hash of its contents, generates a secure random token, and combines them into a single string. :param file_path: Path to the file :return: Serialized string of hash and token separated by a colon try: # Reading file contents with open(file_path, \'rb\') as f: file_content = f.read() # Computing SHA-256 hash file_hash = hashlib.sha256(file_content).hexdigest() # Generating secure token secure_token = secrets.token_hex(16) # Combining hash and token combined_token = f\\"{file_hash}:{secure_token}\\" return combined_token except Exception as e: raise Exception(f\\"Error generating token for {file_path}: {str(e)}\\") def verify_file_token(file_path: str, token: str) -> bool: Verifies the token for the given file. It reads the file, computes the SHA-256 hash of its current contents, and compares it to the hash part of the token. :param file_path: Path to the file :param token: The token containing the hash and the secure token :return: Boolean indicating whether the file is intact try: # Extracting hash and token from the combined token original_hash, secure_token = token.split(\':\') # Reading file contents with open(file_path, \'rb\') as f: file_content = f.read() # Computing SHA-256 hash of the current file content current_hash = hashlib.sha256(file_content).hexdigest() # Comparing the current hash with the original hash return current_hash == original_hash except Exception as e: raise Exception(f\\"Error verifying token for {file_path}: {str(e)}\\")"},{"question":"You are required to design a multithreaded application that simulates a system where multiple producers generate tasks, and multiple consumers process these tasks. Use the `queue` module to implement this application. Your solution should demonstrate your understanding of different types of queues, thread synchronization, handling exceptions, and ensuring all tasks are processed. **Objective:** Implement a multithreaded task processing system where: 1. Producers generate tasks and add them to a queue. 2. Consumers fetch tasks from the queue and process them. 3. Use the `queue.Queue` class for task management, respecting both `empty` and `full` states. 4. Ensure all tasks are processed before the program exits. **Requirements:** 1. Create a thread-safe `Queue` with a maximum size of 10. 2. Implement a `Producer` class that: - Runs in its own thread. - Adds tasks to the queue (simulate task generation with a sleep timer). - Properly handles the `Full` exception when the queue is full. 3. Implement a `Consumer` class that: - Runs in its own thread. - Fetches and processes tasks from the queue (simulate processing with a sleep timer). - Properly handles the `Empty` exception when the queue is empty. 4. Use `queue.task_done()` and `queue.join()` to ensure all tasks are processed before the program terminates. 5. The program should run with 3 producer threads and 5 consumer threads. 6. Define a fixed number of total tasks (e.g., 30) to be produced. **Input:** - No input from the user is required. **Output:** - Print statements indicating task generation by producers and task processing by consumers. - For example: ```plaintext Producer 1: Task 0 added to queue Consumer 2: Task 0 processed ... ``` **Performance constraints:** - Ensure that the program handles the full queue scenario gracefully without losing tasks. - Ensure that consumer threads do not terminate prematurely due to an empty queue. **Example:** ```python import threading import queue import time class Producer(threading.Thread): def __init__(self, id, task_queue, total_tasks): threading.Thread.__init__(self) self.id = id self.task_queue = task_queue self.total_tasks = total_tasks def run(self): for i in range(self.total_tasks): task = f\'Task {i} from Producer {self.id}\' while True: try: self.task_queue.put(task, timeout=1) print(f\'Producer {self.id}: {task} added to queue\') time.sleep(0.1) break except queue.Full: print(f\'Producer {self.id}: Queue is full. Retrying...\') time.sleep(0.1) class Consumer(threading.Thread): def __init__(self, id, task_queue): threading.Thread.__init__(self) self.id = id self.task_queue = task_queue def run(self): while True: try: task = self.task_queue.get(timeout=1) print(f\'Consumer {self.id}: {task} processed\') self.task_queue.task_done() time.sleep(0.3) except queue.Empty: print(f\'Consumer {self.id}: Queue is empty. Waiting...\') time.sleep(0.1) def main(): total_tasks = 30 task_queue = queue.Queue(maxsize=10) producers = [Producer(i, task_queue, total_tasks // 3) for i in range(3)] consumers = [Consumer(i, task_queue) for i in range(5)] for p in producers: p.start() for c in consumers: c.start() for p in producers: p.join() task_queue.join() print(\'All tasks have been processed.\') if __name__ == \'__main__\': main() ```","solution":"import threading import queue import time class Producer(threading.Thread): def __init__(self, id, task_queue, total_tasks): super().__init__() self.id = id self.task_queue = task_queue self.total_tasks = total_tasks def run(self): for i in range(self.total_tasks): task = f\'Task {i} from Producer {self.id}\' while True: try: self.task_queue.put(task, timeout=1) print(f\'Producer {self.id}: {task} added to queue\') time.sleep(0.1) # Simulate task generation time break except queue.Full: print(f\'Producer {self.id}: Queue is full. Retrying...\') time.sleep(0.1) # Wait before retrying class Consumer(threading.Thread): def __init__(self, id, task_queue): super().__init__() self.id = id self.task_queue = task_queue def run(self): while True: try: task = self.task_queue.get(timeout=1) print(f\'Consumer {self.id}: {task} processed\') self.task_queue.task_done() time.sleep(0.3) # Simulate task processing time except queue.Empty: if not any(p.is_alive() for p in threading.enumerate() if isinstance(p, Producer)): break # Exit if no more producers are alive and queue is empty print(f\'Consumer {self.id}: Queue is empty. Waiting...\') time.sleep(0.1) # Wait before retrying to fetch a task def main(): total_tasks = 30 task_queue = queue.Queue(maxsize=10) producers = [Producer(i, task_queue, total_tasks // 3) for i in range(3)] consumers = [Consumer(i, task_queue) for i in range(5)] for p in producers: p.start() for c in consumers: c.start() for p in producers: p.join() task_queue.join() # Block until all tasks are processed print(\'All tasks have been processed.\') if __name__ == \'__main__\': main()"},{"question":"# **Preprocessing Data with Scikit-Learn** **Problem Statement** In this assessment, you are required to preprocess a dataset using different preprocessing techniques provided by `sklearn.preprocessing`. You will be dealing with a synthetic dataset that includes both numerical and categorical features. The main objective is to standardize numerical features, encode categorical features, and ultimately apply a machine learning model to the preprocessed data. **Instructions** 1. **Dataset Creation:** - Create a synthetic dataset using `numpy`: - Numerical Features: Generate two numerical features `X1` and `X2` from a normal distribution. - Categorical Features: Generate two categorical features `Cat1` and `Cat2` with some predefined categories. - Target Variable: Generate a binary target variable `y`. 2. **Preprocessing:** - **Standardize** the numerical features to have zero mean and unit variance. - **One-Hot Encode** the categorical features. - **Combine** the processed numerical and categorical features into a single dataset. 3. **Model Training and Evaluation:** - Split the dataset into training and testing sets. - Train a Logistic Regression model on the training set. - Evaluate the model on the testing set and report the accuracy. **Detailed Steps** 1. **Dataset Creation:** ```python import numpy as np np.random.seed(42) # Generating numerical features X1 = np.random.normal(0, 1, 100) X2 = np.random.normal(5, 2, 100) # Generating categorical features Cat1 = np.random.choice([\'A\', \'B\', \'C\'], 100) Cat2 = np.random.choice([\'X\', \'Y\'], 100) # Generating target variable y = np.random.choice([0, 1], 100) # Combining into a single dataset import pandas as pd df = pd.DataFrame({\'X1\': X1, \'X2\': X2, \'Cat1\': Cat1, \'Cat2\': Cat2, \'y\': y}) ``` 2. **Preprocessing:** - Standardizing Numerical Features - One-Hot Encoding Categorical Features - Combining Processed Features ```python from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline # Defining the preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'X1\', \'X2\']), (\'cat\', OneHotEncoder(), [\'Cat1\', \'Cat2\']) ]) # Create a preprocessing pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) # Preprocess the features X = df.drop(columns=\'y\') y = df[\'y\'] X_preprocessed = pipeline.fit_transform(X) ``` 3. **Model Training and Evaluation:** - Splitting the Data - Training the Logistic Regression Model - Evaluating the Model ```python from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Splitting the data X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42) # Training the model model = LogisticRegression() model.fit(X_train, y_train) # Evaluating the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Model Accuracy: {accuracy:.2f}\\") ``` **Submission Requirements** - Your script should create the synthetic dataset as described. - Apply preprocessing techniques correctly using `StandardScaler` and `OneHotEncoder`. - Combine the preprocessed features and train a Logistic Regression model. - Report the accuracy of the model on the test set. **Constraints** - You are not allowed to use any other preprocessing libraries except `sklearn.preprocessing`. - Ensure the solution is generalizable to any dataset with similar characteristics.","solution":"import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def preprocess_and_train_model(): # Fixing random seed for reproducibility np.random.seed(42) # Generating numerical features X1 = np.random.normal(0, 1, 100) X2 = np.random.normal(5, 2, 100) # Generating categorical features Cat1 = np.random.choice([\'A\', \'B\', \'C\'], 100) Cat2 = np.random.choice([\'X\', \'Y\'], 100) # Generating target variable y = np.random.choice([0, 1], 100) # Combining into a single dataset df = pd.DataFrame({\'X1\': X1, \'X2\': X2, \'Cat1\': Cat1, \'Cat2\': Cat2, \'y\': y}) # Defining the preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'X1\', \'X2\']), (\'cat\', OneHotEncoder(), [\'Cat1\', \'Cat2\']) ]) # Create a preprocessing pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) # Preprocess the features X = df.drop(columns=\'y\') y = df[\'y\'] X_preprocessed = pipeline.fit_transform(X) # Splitting the data X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42) # Training the model model = LogisticRegression() model.fit(X_train, y_train) # Evaluating the model y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Python Coding Assessment Question:** # Objective The goal of this question is to assess your understanding of using the `tempfile` module to manage temporary files and directories securely and effectively. # Problem Statement You are required to implement a function `process_temp_files_and_dirs` that will: 1. Create a temporary directory. 2. Within this directory, create: - Five temporary files using `TemporaryFile`. - Five named temporary files using `NamedTemporaryFile`. - A spooled temporary file using `SpooledTemporaryFile` with a `max_size` of 1024 bytes. 3. Write the current timestamp (in bytes) into each of the temporary files and the spooled temporary file. 4. Read back the contents of each file and store the filenames and their contents in a dictionary. 5. Return this dictionary at the end. # Requirements - Use proper context management to ensure that all temporary files and directories are cleaned up automatically. - The dictionary returned should have the filenames as keys and their read contents (decoded as `utf-8`) as values. - Each filename should clearly indicate whether it is a `TemporaryFile`, `NamedTemporaryFile`, or `SpooledTemporaryFile`. # Function Signature ```python import tempfile from typing import Dict def process_temp_files_and_dirs() -> Dict[str, str]: pass ``` # Example Output ```python { \\"TemporaryFile_1\\": \\"2023-10-10 10:00:00\\", \\"TemporaryFile_2\\": \\"2023-10-10 10:00:00\\", \\"TemporaryFile_3\\": \\"2023-10-10 10:00:00\\", \\"TemporaryFile_4\\": \\"2023-10-10 10:00:00\\", \\"TemporaryFile_5\\": \\"2023-10-10 10:00:00\\", \\"NamedTemporaryFile_1\\": \\"2023-10-10 10:00:00\\", \\"NamedTemporaryFile_2\\": \\"2023-10-10 10:00:00\\", \\"NamedTemporaryFile_3\\": \\"2023-10-10 10:00:00\\", \\"NamedTemporaryFile_4\\": \\"2023-10-10 10:00:00\\", \\"NamedTemporaryFile_5\\": \\"2023-10-10 10:00:00\\", \\"SpooledTemporaryFile\\": \\"2023-10-10 10:00:00\\" } ``` # Constraints - The maximum size for the spooled temporary file before it moves to disk storage should be set to 1024 bytes. - You may use a simple format of `YYYY-MM-DD HH:MM:SS` for the timestamp. # Notes - Make sure to use `name` attributes sensibly so the keys in the dictionary are of the format `TemporaryFile_N`, `NamedTemporaryFile_N`, `SpooledTemporaryFile`. - Utilize built-in Python libraries like `time` or `datetime` to generate current timestamps. Good luck!","solution":"import tempfile from datetime import datetime from typing import Dict def process_temp_files_and_dirs() -> Dict[str, str]: result = {} timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\').encode(\'utf-8\') with tempfile.TemporaryDirectory() as temp_dir: # Creating five TemporaryFiles for i in range(1, 6): with tempfile.TemporaryFile() as tmp: tmp.write(timestamp) tmp.seek(0) result[f\'TemporaryFile_{i}\'] = tmp.read().decode(\'utf-8\') # Creating five NamedTemporaryFiles for i in range(1, 6): with tempfile.NamedTemporaryFile(delete=True) as ntf: ntf.write(timestamp) ntf.seek(0) result[f\'NamedTemporaryFile_{i}\'] = ntf.read().decode(\'utf-8\') # Creating a SpooledTemporaryFile with tempfile.SpooledTemporaryFile(max_size=1024) as stf: stf.write(timestamp) stf.seek(0) result[\'SpooledTemporaryFile\'] = stf.read().decode(\'utf-8\') return result"},{"question":"# Question: Managing Fine-Grained Control over TorchDynamo Tracing in a PyTorch Model Introduction In this task, you are required to implement a PyTorch model and manage its compilation using fine-grained control APIs provided by TorchDynamo. Specifically, you will focus on using `torch.compiler.disable`, `torch._dynamo.disallow_in_graph`, and `torch.compiler.allow_in_graph` to handle specific portions of the model that may not be compatible with TorchDynamo\'s compilation process. Model Description You will construct a simple neural network model with three sub-functions. One of these sub-functions will be decorated to be excluded from TorchDynamo\'s tracing and compilation process. Requirements 1. Implement a neural network model consisting of the following: - A main function (`main_fn`) that orchestrates the forward pass. - Three sub-functions (`sub_fn1`, `sub_fn2`, and `sub_fn3`) that will each perform a part of the computation. - One of these sub-functions (`sub_fn2`) should be decorated to skip compilation. 2. Use the relevant TorchDynamo APIs to ensure `sub_fn2` is not traced or compiled. 3. Demonstrate model execution with and without TorchDynamo\'s compilation to show the difference. Input and Output Formats - **Input**: An example PyTorch tensor input to test the model. - **Output**: Output from the model with and without TorchDynamo\'s compilation. Detailed Steps 1. **Model Implementation**: - Implement `main_fn` which will call `sub_fn1`, `sub_fn2`, and `sub_fn3`. - Decorate `sub_fn2` with `torch.compiler.disable` to skip its compilation. 2. **Compilation Control**: - Ensure that `sub_fn2` is not included in the TorchDynamo graph using the appropriate API. - Demonstrate the functionality by showing model output with and without compilation. 3. **Testing**: - Create an input tensor and run the model twice: once using `torch.compile` and once without it to show the difference in behavior. Constraints - You must use the APIs mentioned (`torch.compiler.disable`, `torch._dynamo.disallow_in_graph`, and `torch.compiler.allow_in_graph`). - Ensure the code is structured, commented, and easy to understand. ```python import torch import torch.nn as nn from torch import compiler # Step 1: Implement the model components class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() def sub_fn1(self, x): return x * 2 @compiler.disable def sub_fn2(self, x): return x + 3 def sub_fn3(self, x): return x - 1 def main_fn(self, x): x = self.sub_fn1(x) x = self.sub_fn2(x) x = self.sub_fn3(x) return x # Step 2: Perform the model compilation and execution def test_model(): model = SimpleModel() input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # Run without using torch.compile result_eager = model.main_fn(input_tensor.clone()) print(\\"Result without compilation:\\", result_eager) # Compile and run the model using torch.compile compiled_model = torch.compile(model) result_compiled = compiled_model.main_fn(input_tensor.clone()) print(\\"Result with compilation:\\", result_compiled) # Execute the testing function test_model() ``` Expected Output ``` Result without compilation: tensor([3., 5., 7.], grad_fn=<SubBackward0>) Result with compilation: tensor([3., 5., 7.], grad_fn=<SubBackward0>) ``` **Note:** You should observe that despite the decoration and compilation, the actual output values remain the same. However, the parts of the model have been appropriately excluded from compilation as specified.","solution":"import torch import torch.nn as nn from torch import compiler, _dynamo class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() def sub_fn1(self, x): return x * 2 @compiler.disable def sub_fn2(self, x): return x + 3 def sub_fn3(self, x): return x - 1 def main_fn(self, x): x = self.sub_fn1(x) x = self.sub_fn2(x) x = self.sub_fn3(x) return x def run_model(model, input_tensor): return model.main_fn(input_tensor) def test_model(): model = SimpleModel() input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # Run without using torch.compile result_eager = model.main_fn(input_tensor.clone()) print(\\"Result without compilation:\\", result_eager) # Compile and run the model using torch.compile compiled_model = torch.compile(model) result_compiled = compiled_model.main_fn(input_tensor.clone()) print(\\"Result with compilation:\\", result_compiled) return result_eager, result_compiled # Execute the testing function (for manual verification) if __name__ == \\"__main__\\": test_model()"},{"question":"**Problem Statement** You are required to implement a parallelized solution to process a list of tasks using the `multiprocessing` module. Each task involves some computation, and you need to ensure that the results of these computations are collected and processed in a synchronized manner. # Function Signature ```python def process_tasks(tasks: List[Tuple[int, int, str]]) -> List[str]: pass ``` # Input - `tasks`: A list of tuples. Each tuple `(a, b, operation)` represents a task where `a` and `b` are integers, and `operation` is a string that can be either `\'add\'`, `\'subtract\'`, `\'multiply\'`, or `\'divide\'`. # Output - Returns a list of strings where each string represents the result of the computation in the format: `f\\"{a} {operation} {b} = result\\"`. For division tasks, return \\"undefined\\" if `b` is 0 instead of raising an error. # Requirements 1. **Parallel Processing**: - Utilize `multiprocessing.Pool` to parallelize the computation of tasks. - Each worker process should handle individual tasks. 2. **Synchronization**: - Ensure that all processes are synchronized and the results are collected properly. 3. **Communication**: - Use `multiprocessing.Queue` or `Pipe` to gather results from worker processes to the main process. 4. **Exception Handling**: - Handle any division by zero errors by returning \\"undefined\\" for such cases. # Constraints - `1 <= len(tasks) <= 1000` - Each `a` and `b` is an integer between `-10^6` and `10^6`. - `operation` is one of: `\'add\'`, `\'subtract\'`, `\'multiply\'`, `\'divide\'`. # Example ```python tasks = [(1, 2, \'add\'), (5, 0, \'divide\'), (6, 3, \'subtract\'), (4, 2, \'multiply\')] print(process_tasks(tasks)) ``` **Output**: ``` [\'1 add 2 = 3\', \'5 divide 0 = undefined\', \'6 subtract 3 = 3\', \'4 multiply 2 = 8\'] ``` **Notes** - Make sure to initialize and start the pool correctly. - Gather results in a thread-safe manner from all worker processes. - Consider using the `Pool.map` or `Pool.map_async` methods for distributing tasks among the workers.","solution":"import multiprocessing from typing import List, Tuple def compute_task(task: Tuple[int, int, str]) -> str: a, b, operation = task if operation == \'add\': result = a + b elif operation == \'subtract\': result = a - b elif operation == \'multiply\': result = a * b elif operation == \'divide\': if b == 0: result = \\"undefined\\" else: result = a / b else: raise ValueError(f\\"Unknown operation: {operation}\\") return f\\"{a} {operation} {b} = {result}\\" def process_tasks(tasks: List[Tuple[int, int, str]]) -> List[str]: with multiprocessing.Pool() as pool: results = pool.map(compute_task, tasks) return results"},{"question":"# Custom Serialization with Pickle **Objective:** Demonstrate your understanding of Python\'s `pickle` module by implementing a custom serialization logic for a complex Python class. **Problem Statement:** You are tasked with implementing a custom class `Course` which represents a course in a university system. Each course has a name, a list of students, and an optional description. The `Course` objects can be connected in a way that one course `prerequisite` points to another course object. Additionally, you need to implement custom pickling and unpickling logic for this class using the `pickle` module. # Class Specification: 1. **Attributes**: - `name` (str): The name of the course. - `students` (list): A list of student names (strings). - `description` (str): A brief description of the course (optional). - `prerequisite` (Course or None): Another `Course` object that is a prerequisite for this course (default is `None`). 2. **Methods**: - `__init__(self, name, students, description=None, prerequisite=None)`: Constructor that initializes the course attributes. - `__getstate__(self)`: Method to define the state of the object for pickling. - `__setstate__(self, state)`: Method to restore the state of the object during unpickling. - `__reduce__(self)`: Alternative method for custom reduction if necessary. # Requirements: 1. Implement the `Course` class with the attributes and methods described above. 2. Implement custom pickling logic using the `__getstate__` and `__setstate__` methods to include a version number in the serialized state. This ensures compatibility for potential future changes in the class structure. 3. Implement a custom `persistent_id` method in a custom `Pickler` subclass and ensure that the `Course` objects serialize their `prerequisite` attribute correctly, only by reference with a unique identifier (e.g., course name). Implement the corresponding `persistent_load` method in a custom `Unpickler` subclass to reconstitute the `prerequisite` relationship during unpickling. # Constraints: - The course name and student names cannot exceed 100 characters. - The description can be up to 500 characters. - Ensure that the custom pickling and unpickling logic handles cyclic references gracefully (i.e., when a course is its own prerequisite or part of a cyclic graph). # Input: - You will be given multiple `Course` objects with their attributes as inputs to test the serialization and deserialization process. # Output: - Your implementation should be able to pickle the `Course` objects to a byte stream and unpickle them back to verify the object integrity and the maintained relationships. # Example: ```python # Define the Course class with custom serialization class Course: # Implementation goes here # Custom Pickler and Unpickler class CoursePickler(pickle.Pickler): # Implementation goes here class CourseUnpickler(pickle.Unpickler): # Implementation goes here # Example usage: course1 = Course(name=\\"Math 101\\", students=[\\"Alice\\", \\"Bob\\"]) course2 = Course(name=\\"Physics 101\\", students=[\\"Charlie\\"], prerequisite=course1) course3 = Course(name=\\"Chemistry 101\\", students=[\\"Dana\\"], prerequisite=course2) # Serialize the courses f = io.BytesIO() CoursePickler(f).dump([course1, course2, course3]) # Deserialize the courses f.seek(0) loaded_courses = CourseUnpickler(f).load() # Validate the courses are correctly restored assert loaded_courses[0].name == \\"Math 101\\" assert loaded_courses[1].prerequisite.name == \\"Math 101\\" assert loaded_courses[2].prerequisite.prerequisite.name == \\"Math 101\\" ``` Good luck, and happy coding!","solution":"import pickle import io class Course: def __init__(self, name, students, description=None, prerequisite=None): self.name = name self.students = students self.description = description self.prerequisite = prerequisite def __getstate__(self): state = self.__dict__.copy() state[\'version\'] = 1 if self.prerequisite: state[\'prerequisite\'] = self.prerequisite.name return state def __setstate__(self, state): version = state.pop(\'version\', 0) self.__dict__.update(state) if version == 1 and isinstance(self.prerequisite, str): self.prerequisite = None def __reduce__(self): return (self.__class__, (self.name, self.students, self.description), self.__getstate__()) class CoursePickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Course): return obj.name return None class CourseUnpickler(pickle.Unpickler): def __init__(self, file, courses): super().__init__(file) self.courses = courses def persistent_load(self, pid): return self.courses.get(pid) def serialize_courses(courses): buffer = io.BytesIO() CoursePickler(buffer).dump(courses) return buffer.getvalue() def deserialize_courses(data, courses): buffer = io.BytesIO(data) return CourseUnpickler(buffer, courses).load()"},{"question":"Coding Assessment Question **Objective:** Demonstrate your ability to load a dataset using `sklearn.datasets`, perform exploratory data analysis (EDA), preprocess the data, and apply a basic machine learning model using scikit-learn. # Question: You are given the task of working with the famous Iris dataset, which is a small, standard toy dataset available in scikit-learn. # Requirements: 1. **Load the Iris Dataset:** - Import the `load_iris` function from `sklearn.datasets`. - Load the dataset and store it in a variable. 2. **Exploratory Data Analysis (EDA):** - Display the keys of the dataset. - Print the description of the dataset. - Display the first 5 samples of data and their corresponding target values. - Compute and display the mean and standard deviation of each feature in the dataset. 3. **Data Preprocessing:** - Split the dataset into training and testing sets using an 80-20 split. Make sure to randomly shuffle the data before splitting. - Scale the features using `StandardScaler` from `sklearn.preprocessing`. 4. **Model Training and Evaluation:** - Train a `KNeighborsClassifier` on the training data. - Evaluate the model on the test data by displaying the accuracy score. # Constraints: - You must use the `scikit-learn` package for all steps. - Use a random state of 42 for reproducibility when splitting the dataset. # Input and Output Format: - There is no input as your task is to write a Python script. - Print the following output: - Keys of the dataset - Dataset description - First 5 data samples and their target values - Mean and standard deviation of each feature - Accuracy score of the model ```python # Your code starts here # Step 1: Load the Iris Dataset from sklearn.datasets import load_iris data = load_iris() # Step 2: Perform EDA # 2.1 Display the keys of the dataset print(f\\"Keys of the dataset: {data.keys()}\\") # 2.2 Print the description of the dataset print(f\\"nDataset Description: {data.DESCR}\\") # 2.3 Display the first 5 samples of data and their target values print(f\\"nFirst 5 samples of data: {data.data[:5]}\\") print(f\\"First 5 target values: {data.target[:5]}\\") # 2.4 Compute and display the mean and standard deviation of each feature import numpy as np mean_features = np.mean(data.data, axis=0) std_features = np.std(data.data, axis=0) print(f\\"nMean of each feature: {mean_features}\\") print(f\\"Standard deviation of each feature: {std_features}\\") # Step 3: Data Preprocessing # 3.1 Split the dataset into training and testing sets using an 80-20 split from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42, shuffle=True) # 3.2 Scale the features using StandardScaler from sklearn.preprocessing import StandardScaler scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 4: Model Training and Evaluation # 4.1 Train a KNeighborsClassifier on the training data from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier() knn.fit(X_train_scaled, y_train) # 4.2 Evaluate the model on the test data accuracy = knn.score(X_test_scaled, y_test) print(f\\"nAccuracy score of the model: {accuracy}\\") # Your code ends here ```","solution":"# Your code starts here # Step 1: Load the Iris Dataset from sklearn.datasets import load_iris data = load_iris() # Step 2: Perform EDA # 2.1 Display the keys of the dataset keys = data.keys() print(f\\"Keys of the dataset: {keys}\\") # 2.2 Print the description of the dataset description = data.DESCR print(f\\"nDataset Description: {description}\\") # 2.3 Display the first 5 samples of data and their target values first_5_samples = data.data[:5] first_5_targets = data.target[:5] print(f\\"nFirst 5 samples of data: {first_5_samples}\\") print(f\\"First 5 target values: {first_5_targets}\\") # 2.4 Compute and display the mean and standard deviation of each feature import numpy as np mean_features = np.mean(data.data, axis=0) std_features = np.std(data.data, axis=0) print(f\\"nMean of each feature: {mean_features}\\") print(f\\"Standard deviation of each feature: {std_features}\\") # Step 3: Data Preprocessing # 3.1 Split the dataset into training and testing sets using an 80-20 split from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42, shuffle=True) # 3.2 Scale the features using StandardScaler from sklearn.preprocessing import StandardScaler scaler = StandardScaler().fit(X_train) X_train_scaled = scaler.transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 4: Model Training and Evaluation # 4.1 Train a KNeighborsClassifier on the training data from sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier() knn.fit(X_train_scaled, y_train) # 4.2 Evaluate the model on the test data accuracy = knn.score(X_test_scaled, y_test) print(f\\"nAccuracy score of the model: {accuracy}\\") # Your code ends here"},{"question":"**Problem Statement:** You are tasked with developing a model to classify types of iris plants using the famed Iris dataset. You will use a decision tree classifier from the scikit-learn library. The task involves multiple steps, as outlined below: 1. **Loading the Dataset:** Load the Iris dataset from scikit-learn\'s datasets module. 2. **Model Training:** Train a `DecisionTreeClassifier` on the dataset. Use a suitable split (e.g., 80-20) for training and testing. 3. **Model Evaluation:** Evaluate your model on the test set using appropriate metrics (accuracy, precision, recall, F1-score). 4. **Tree Visualization:** Visualize the decision tree using `plot_tree` and export a textual representation using the `export_text` function. 5. **Pruning the Tree:** Implement minimal cost-complexity pruning on your decision tree and evaluate the pruned tree\'s performance. Your implementation should include the following functions: 1. **load_data:** This function should load the Iris dataset. - **Input:** None - **Output:** Features (X), Labels (y) ```python def load_data(): from sklearn.datasets import load_iris iris = load_iris() return iris.data, iris.target ``` 2. **train_model:** This function should train a DecisionTreeClassifier on the provided data. - **Input:** Features (X), Labels (y) - **Output:** Trained model ```python def train_model(X, y): from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) return clf, X_test, y_test ``` 3. **evaluate_model:** This function should evaluate the model using accuracy, precision, recall, and F1-score. - **Input:** Trained model, Features (X_test), Labels (y_test) - **Output:** Dictionary of evaluation metrics ```python def evaluate_model(clf, X_test, y_test): from sklearn.metrics import accuracy_score, precision_recall_fscore_support y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\'macro\') return {\'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1\': f1} ``` 4. **visualize_tree:** This function should visualize the trained tree and export its textual representation. - **Input:** Trained model - **Output:** None ```python def visualize_tree(clf): from sklearn.tree import plot_tree, export_text import matplotlib.pyplot as plt plt.figure(figsize=(20,10)) plot_tree(clf, filled=True) plt.show() print(export_text(clf, feature_names=[\'sepal length\', \'sepal width\', \'petal length\', \'petal width\'])) ``` 5. **prune_tree:** This function should apply minimal cost-complexity pruning to the decision tree and return the pruned model. - **Input:** Features (X_train), Labels (y_train) - **Output:** Pruned model ```python def prune_tree(X_train, y_train): from sklearn.tree import DecisionTreeClassifier path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) clfs.append(clf) return clfs[-1] ``` **Full Implementation:** Integrate the functions into your main code structure and ensure that each part works in succession to complete the task. ```python def load_data(): from sklearn.datasets import load_iris iris = load_iris() return iris.data, iris.target def train_model(X, y): from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) return clf, X_train, X_test, y_train, y_test def evaluate_model(clf, X_test, y_test): from sklearn.metrics import accuracy_score, precision_recall_fscore_support y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\'macro\') return {\'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1\': f1} def visualize_tree(clf): from sklearn.tree import plot_tree, export_text import matplotlib.pyplot as plt plt.figure(figsize=(20,10)) plot_tree(clf, filled=True) plt.show() print(export_text(clf, feature_names=[\'sepal length\', \'sepal width\', \'petal length\', \'petal width\'])) def prune_tree(X_train, y_train): from sklearn.tree import DecisionTreeClassifier path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) clfs.append(clf) return clfs[-1] # Main code X, y = load_data() clf, X_train, X_test, y_train, y_test = train_model(X, y) metrics = evaluate_model(clf, X_test, y_test) print(\\"Evaluation metrics:\\", metrics) visualize_tree(clf) clf_pruned = prune_tree(X_train, y_train) metrics_pruned = evaluate_model(clf_pruned, X_test, y_test) print(\\"Evaluation metrics after pruning:\\", metrics_pruned) ``` **Constraints and Requirements:** - The solution should be implemented in Python using scikit-learn. - The model should be trained on 80% of the dataset and tested on the remaining 20%. - Use random_state=42 for reproducibility. - Evaluate the model using accuracy, precision, recall, and F1-score. - Visualize the tree and export its textual representation. - Perform pruning using the minimal cost-complexity pruning method. **Submission:** Submit the Python code containing all the required functions and the main code execution.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text from sklearn.metrics import accuracy_score, precision_recall_fscore_support import matplotlib.pyplot as plt def load_data(): Loads the Iris dataset and returns the features and labels. Returns: X: Features y: Labels iris = load_iris() return iris.data, iris.target def train_model(X, y): Splits the data into training and testing sets, trains a DecisionTreeClassifier, and returns the model. Args: X: Features y: Labels Returns: clf: Trained model X_train: Training features X_test: Testing features y_train: Training labels y_test: Testing labels X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) return clf, X_train, X_test, y_train, y_test def evaluate_model(clf, X_test, y_test): Evaluates the classifier using accuracy, precision, recall, and F1-score. Args: clf: Trained model X_test: Testing features y_test: Testing labels Returns: metrics: Dictionary of evaluation metrics y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\'macro\') return {\'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1\': f1} def visualize_tree(clf): Visualizes the decision tree and prints its textual representation. Args: clf: Trained model plt.figure(figsize=(20,10)) plot_tree(clf, filled=True) plt.show() print(export_text(clf, feature_names=[\'sepal length\', \'sepal width\', \'petal length\', \'petal width\'])) def prune_tree(X_train, y_train): Applies minimal cost-complexity pruning to the decision tree and returns the pruned model. Args: X_train: Training features y_train: Training labels Returns: pruned_clf: Pruned model clf = DecisionTreeClassifier(random_state=42) path = clf.cost_complexity_pruning_path(X_train, y_train) ccp_alphas = path.ccp_alphas clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf.fit(X_train, y_train) clfs.append(clf) return clfs[-1] # Main code X, y = load_data() clf, X_train, X_test, y_train, y_test = train_model(X, y) metrics = evaluate_model(clf, X_test, y_test) print(\\"Evaluation metrics:\\", metrics) visualize_tree(clf) clf_pruned = prune_tree(X_train, y_train) metrics_pruned = evaluate_model(clf_pruned, X_test, y_test) print(\\"Evaluation metrics after pruning:\\", metrics_pruned)"},{"question":"**Pandas Coding Assessment Question** # Objective The purpose of this assessment is to evaluate your proficiency with pandas Index objects and your ability to manipulate and analyze indexed data within a DataFrame. # Problem Statement You are given a DataFrame containing information about sales transactions. Each transaction has a date, product ID, category, quantity sold, and total sale amount. The data is indexed by both the transaction date and product ID. Your task is to implement a function `analyze_sales_data` that performs the following operations: 1. **Create a MultiIndex DataFrame:** - Set the `Date` column as the index. - Add a second level to the index using the `ProductID` column. - Ensure that the index is sorted. 2. **Calculate and Add Aggregated Data:** - Compute the total quantity sold for each product category per month. - Add a new column `Month` to the DataFrame indicating the month for each transaction. - Use the new column to group the data and compute the monthly totals. 3. **Handle Missing Data:** - Fill any missing values in the `Quantity` column with the median quantity sold per product category. 4. **Query Data:** - Implement a function to get all transactions for a specific product category on a given date. - Implement another function to find the top 5 products based on the total sales amount within a given date range. # Input Format - A DataFrame `df` with the following columns: - `Date`: Transaction date in YYYY-MM-DD format. - `ProductID`: Integer representing the product identifier. - `Category`: String representing the category of the product. - `Quantity`: Integer representing the quantity sold. - `TotalSale`: Float representing the total sale amount. - `category` (str): The product category to filter transactions (for querying). - `date` (str): The date to filter transactions in YYYY-MM-DD format (for querying). - `start_date` (str): The start date for the date range query in YYYY-MM-DD format (for querying). - `end_date` (str): The end date for the date range query in YYYY-MM-DD format (for querying). # Output Format - The function should return: 1. A MultiIndex DataFrame with aggregated data. 2. A filtered DataFrame of transactions for a specific product category on a given date. 3. A DataFrame of the top 5 products based on total sales amount within the given date range. # Function Signature ```python def analyze_sales_data(df: pd.DataFrame, category: str, date: str, start_date: str, end_date: str): # Step 1: Create MultiIndex DataFrame # Step 2: Calculate and Add Aggregated Data # Step 3: Handle Missing Data # Step 4: Query Data # Return the required DataFrames pass ``` # Example ```python import pandas as pd data = { \'Date\': [\'2023-01-01\', \'2023-01-01\', \'2023-02-01\', \'2023-02-01\', \'2023-03-01\'], \'ProductID\': [1, 2, 1, 3, 2], \'Category\': [\'A\', \'B\', \'A\', \'B\', \'B\'], \'Quantity\': [100, 200, 150, 300, 250], \'TotalSale\': [1000.0, 4000.0, 1500.0, 6000.0, 5000.0] } df = pd.DataFrame(data) # Analyze sales data result_df, filtered_df, top_products_df = analyze_sales_data(df, category=\'A\', date=\'2023-01-01\', start_date=\'2023-01-01\', end_date=\'2023-02-01\') print(result_df) print(filtered_df) print(top_products_df) ``` - `result_df` should contain the MultiIndex DataFrame with aggregated data. - `filtered_df` should contain the filtered transactions for category \'A\' on \'2023-01-01\'. - `top_products_df` should contain the top 5 products based on total sales amount from \'2023-01-01\' to \'2023-02-01\'. # Constraints - The DataFrame can have a maximum of 100,000 rows. - Fill missing `Quantity` values with the median quantity sold per product category. # Performance Requirements - The solution should be optimized to handle the constraints within a reasonable execution time.","solution":"import pandas as pd def analyze_sales_data(df: pd.DataFrame, category: str, date: str, start_date: str, end_date: str): # Step 1: Create MultiIndex DataFrame df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index([\'Date\', \'ProductID\'], inplace=True) df.sort_index(inplace=True) # Step 2: Calculate and Add Aggregated Data df[\'Month\'] = df.index.get_level_values(\'Date\').to_period(\'M\') monthly_totals = df.groupby([\'Month\', \'Category\']).agg({\'Quantity\': \'sum\'}).rename(columns={\'Quantity\': \'TotalQuantity\'}) # Join the monthly totals back to the original dataframe df = df.reset_index().merge(monthly_totals, on=[\'Month\', \'Category\']) df.set_index([\'Date\', \'ProductID\'], inplace=True) # Step 3: Handle Missing Data category_medians = df.groupby(\'Category\')[\'Quantity\'].transform(\'median\') df[\'Quantity\'] = df[\'Quantity\'].fillna(category_medians) # Step 4: Query Data # a. Transactions for a specific category on a given date date = pd.to_datetime(date) filtered_df = df[(df.index.get_level_values(\'Date\') == date) & (df[\'Category\'] == category)] # b. Top 5 products based on total sales amount within date range start_date = pd.to_datetime(start_date) end_date = pd.to_datetime(end_date) top_products_df = df[(df.index.get_level_values(\'Date\') >= start_date) & (df.index.get_level_values(\'Date\') <= end_date)] top_products_df = top_products_df.groupby(\'ProductID\').agg({\'TotalSale\': \'sum\'}).nlargest(5, \'TotalSale\') return df, filtered_df, top_products_df"},{"question":"# Group Management Script You are tasked with creating a Python script that interacts with the Unix group database using the `grp` module. The script should perform the following operations: 1. **Retrieve Group by ID**: - Create a function `get_group_by_id(group_id)` that takes an integer `group_id` and returns the corresponding group entry. - If there is no group with the given ID, the function should return `None`. 2. **Retrieve Group by Name**: - Create a function `get_group_by_name(group_name)` that takes a string `group_name` and returns the corresponding group entry. - If there is no group with the given name, the function should return `None`. 3. **List All Groups**: - Create a function `list_all_groups()` that returns a list of all group names sorted in alphabetical order. 4. **Add User to Group**: - Create a function `add_user_to_group(group_name, user_name)` that takes a string `group_name` and a string `user_name`, and adds the user to the specified group. - If the group does not exist, the function should raise a `KeyError`. - If the user is already a member of the group, the function should return without making any changes. 5. **Remove User from Group**: - Create a function `remove_user_from_group(group_name, user_name)` that takes a string `group_name` and a string `user_name`, and removes the user from the specified group. - If the group does not exist, the function should raise a `KeyError`. - If the user is not a member of the group, the function should return without making any changes. # Constraints: - Your solution should handle cases where invalid data types are provided, such as non-integer group IDs or non-string group names, gracefully by raising appropriate exceptions. # Example: ```python # Assuming these functions are implemented # Example group group = { \'gr_name\': \'example\', \'gr_passwd\': \'\', \'gr_gid\': 1234, \'gr_mem\': [\'user1\', \'user2\'] } # Retrieve a group by ID print(get_group_by_id(1234)) # Output: (\'example\', \'\', 1234, [\'user1\', \'user2\']) # Retrieve a group by name print(get_group_by_name(\'example\')) # Output: (\'example\', \'\', 1234, [\'user1\', \'user2\']) # List all groups print(list_all_groups()) # Output: [\'admin\', \'example\', \'staff\'] # Add a user to a group add_user_to_group(\'example\', \'user3\') print(get_group_by_name(\'example\')) # Output: (\'example\', \'\', 1234, [\'user1\', \'user2\', \'user3\']) # Remove a user from a group remove_user_from_group(\'example\', \'user2\') print(get_group_by_name(\'example\')) # Output: (\'example\', \'\', 1234, [\'user1\', \'user3\']) ``` # Implementation: Make sure to test your functions thoroughly. Your solution should demonstrate a good understanding of the `grp` module and be able to perform the specified operations correctly.","solution":"import grp def get_group_by_id(group_id): Retrieve the group entry by group ID. try: group = grp.getgrgid(group_id) return group except KeyError: return None def get_group_by_name(group_name): Retrieve the group entry by group name. try: group = grp.getgrnam(group_name) return group except KeyError: return None def list_all_groups(): List all group names sorted in alphabetical order. groups = [g.gr_name for g in grp.getgrall()] groups.sort() return groups def add_user_to_group(group_name, user_name): Add a user to the specified group. group = get_group_by_name(group_name) if group is None: raise KeyError(f\\"Group \'{group_name}\' not found.\\") if user_name not in group.gr_mem: group.gr_mem.append(user_name) def remove_user_from_group(group_name, user_name): Remove a user from the specified group. group = get_group_by_name(group_name) if group is None: raise KeyError(f\\"Group \'{group_name}\' not found.\\") if user_name in group.gr_mem: group.gr_mem.remove(user_name)"},{"question":"Implement a series of functions to manipulate Python function objects. These functions should mimic some behaviors described in the documentation for educational purposes, but using pure Python. # Problem Description **Function 1:** `create_function(code: str, globals_dict: dict) -> callable` - This function will take two arguments: - `code`: A string containing the body of a Python function. - `globals_dict`: A dictionary of global variables accessible to the function. - The function should return a callable Python function. **Function 2:** `get_function_code(func: callable) -> str` - This function will take one argument: - `func`: A callable Python function. - The function should return a string containing the code object (source code) of the function. **Function 3:** `get_function_globals(func: callable) -> dict` - This function will take one argument: - `func`: A callable Python function. - The function should return the globals dictionary associated with the function. **Function 4:** `set_function_defaults(func: callable, defaults: tuple) -> None` - This function will take two arguments: - `func`: A callable Python function. - `defaults`: A tuple containing default values for the function arguments. - The function should set the default argument values for the provided function. **Function 5:** `get_function_defaults(func: callable) -> tuple` - This function will take one argument: - `func`: A callable Python function. - The function should return a tuple containing the default argument values of the function. Your task is to implement these five functions using pure Python constructs. You should not use any external libraries or modules outside the Python Standard Library. # Constraints - You can assume that the provided code string in `create_function` is valid Python code. - The globals dictionary in `create_function` will not contain any harmful or unsafe values. - The function provided to `set_function_defaults` and `get_function_defaults` will always be a valid `callable`. # Example Usage ```python globals_dict = {\\"initial_value\\": 10} code = def example_func(x, y=5): return initial_value + x + y # Create the function example_func = create_function(code, globals_dict) # Get function code print(get_function_code(example_func)) # Should output the function code as a string # Get global variables accessible to the function print(get_function_globals(example_func)) # Should output {\'initial_value\': 10} # Set default values for the function arguments set_function_defaults(example_func, (20,)) # Get default values of the function print(get_function_defaults(example_func)) # Should output (20,) ``` # Notes - Your `create_function` needs to handle creating functions dynamically from the provided code string and globals dictionary. - Use introspection techniques where necessary to get and set function attributes.","solution":"import types def create_function(code: str, globals_dict: dict) -> callable: Creates a callable function from the provided code string and globals dictionary. :param code: A string containing the body of a Python function. :param globals_dict: A dictionary of global variables accessible to the function. :return: A callable Python function. exec(code, globals_dict) func_name = code.strip().split()[1].split(\'(\')[0] return globals_dict[func_name] def get_function_code(func: callable) -> str: Returns the source code of a given function as a string. :param func: A callable Python function. :return: A string containing the source code of the function. return func.__code__.co_code def get_function_globals(func: callable) -> dict: Returns the globals dictionary associated with the given function. :param func: A callable Python function. :return: A dictionary containing the global variables of the function. return func.__globals__ def set_function_defaults(func: callable, defaults: tuple) -> None: Sets the default argument values for the given function. :param func: A callable Python function. :param defaults: A tuple containing the default values for the function arguments. :return: None func.__defaults__ = defaults def get_function_defaults(func: callable) -> tuple: Returns the default argument values of the given function. :param func: A callable Python function. :return: A tuple containing the default argument values of the function. return func.__defaults__"},{"question":"**Coding Assessment Question:** # Objective: To assess your understanding of creating and customizing boxen plots using the Seaborn library in Python. # Task: You are given a dataset of diamonds containing various features. Your task is to create and customize a boxen plot that visualizes the distribution of diamond prices across different clarity levels while further dividing the data based on whether the diamond is considered large or not, using custom styling options. # Instructions: 1. **Load the Dataset:** - Use the `seaborn` library to load the `diamonds` dataset. 2. **Create a new column `large_diamond`:** - This column should be boolean, where `True` indicates the diamond\'s carat value is greater than 1, and `False` otherwise. 3. **Create a Boxen Plot:** - Visualize the distribution of diamond prices (`price`) grouped by `clarity`. - Use the `large_diamond` column to add hue to the plot. - Plot the boxes with a gap of 0.2. - Use a linear method to determine each box\'s width. - Set the width of the largest box to be 0.4. - Set the line color to `#888888` and line width to `1.0`. - Customize the median line with a color of `#ff0000` and width of `1.5`. - Adjust the outliers\' appearance to have a face color of `#000000` and line width of `0.5`. # Expected Input and Output Formats: Input: * The diamonds dataset provided by the `seaborn` library. Output: * A customized boxen plot visualizing the distribution of diamond prices. Constraints: * Use only Seaborn and Matplotlib libraries for visualization. * Follow the instructions precisely for creating the boxen plot. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset sns.set_theme(style=\\"whitegrid\\") diamonds = sns.load_dataset(\\"diamonds\\") # 2. Create a new column \'large_diamond\' diamonds[\'large_diamond\'] = diamonds[\'carat\'] > 1 # 3. Create and customize the boxen plot sns.boxenplot( data=diamonds, x=\'price\', y=\'clarity\', hue=\'large_diamond\', gap=0.2, width_method=\'linear\', width=0.4, linewidth=1.0, linecolor=\'#888888\', line_kws={\'linewidth\': 1.5, \'color\': \'#ff0000\'}, flier_kws={\'facecolor\': \'#000000\', \'linewidth\': 0.5} ) plt.show() ``` Your task is to write the full code to generate the described boxen plot. Ensure you handle the dataset properly and apply all customizations as required.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_customized_boxenplot(): # Load the Diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Create a new column \'large_diamond\' diamonds[\'large_diamond\'] = diamonds[\'carat\'] > 1 # Create and customize the boxen plot plt.figure(figsize=(12, 8)) sns.boxenplot( data=diamonds, x=\'price\', y=\'clarity\', hue=\'large_diamond\', gap=0.2, linewidth=1.0, width=0.4, palette=\'Set2\', dodge=True, k_depth=\'full\' ).set(title=\\"Customized Boxen Plot of Diamond Prices by Clarity and Size\\") # Customize the plot further plt.setp(plt.gca().lines, color=\'#888888\', linewidth=1.0) for artists in plt.gca().collections: artists.set_edgecolor(\\"#ff0000\\") artists.set_linewidth(1.5) # Customize the outliers for fl in plt.gca().findobj(match=plt.matplotlib.collections.LineCollection): fl.set_facecolor(\'#000000\') fl.set_linewidth(0.5) plt.show() create_customized_boxenplot()"},{"question":"# Advanced Python Coding Assessment Question: Floating-Point Precision Correction Background: Floating-point numbers in Python are represented as binary fractions, which can introduce small precision errors when performing arithmetic operations. These errors stem from the inability to represent certain decimal numbers exactly in binary form. For example, the number `0.1` cannot be perfectly represented as a binary fraction, which can lead to unexpected results in calculations. Python provides built-in modules like `decimal` and `fractions` to handle more precise arithmetic and mitigate these floating-point limitations. Problem Statement: You are required to implement a function `precise_sum` that accurately calculates the sum of a list of decimal numbers. Given that traditional floating-point arithmetic may introduce precision errors, your function should use the `decimal` module to ensure accurate results. Function Signature: ```python def precise_sum(numbers: list[str]) -> str: Calculate the precise sum of a list of decimal numbers represented as strings. Args: - numbers (list[str]): A list of numbers represented as strings to ensure precision in arithmetic operations. Returns: - str: The sum of the input numbers as a string, ensuring precision. ``` Input: - `numbers`: A list of strings where each string represents a decimal number (e.g., [\\"0.1\\", \\"0.2\\", \\"0.3\\"]). Output: - A string representing the precise sum of the input list of decimal numbers. Constraints: - The list `numbers` will contain between 1 and 100 elements (inclusive). - Each string in `numbers` will be a valid representation of a decimal number with at most 20 digits before and after the decimal point. Example: ```python # Example 1 input_list = [\\"0.1\\", \\"0.2\\", \\"0.3\\"] output = precise_sum(input_list) print(output) # Should output \\"0.6\\" precisely # Example 2 input_list = [\\"0.1\\", \\"0.2\\", \\"0.3\\", \\"0.1\\"] output = precise_sum(input_list) print(output) # Should output \\"0.7\\" precisely ``` Additional Details: - You must use the `decimal` module to perform the arithmetic operations to ensure precision. - You may find it useful to convert the input strings to `decimal.Decimal` objects for precise arithmetic. - Ensure your solution handles edge cases, such as summing a single element or very small/very large decimal values. Good luck, and happy coding!","solution":"from decimal import Decimal def precise_sum(numbers: list[str]) -> str: Calculate the precise sum of a list of decimal numbers represented as strings. Args: - numbers (list[str]): A list of numbers represented as strings to ensure precision in arithmetic operations. Returns: - str: The sum of the input numbers as a string, ensuring precision. total_sum = Decimal(\'0\') for number in numbers: total_sum += Decimal(number) return str(total_sum)"},{"question":"**Objective**: Implement a function that reads and interprets chunks from a binary file using the `chunk` module. This function will read a binary file, extract and print information about each chunk, and eventually return a list containing details of all the chunks. Function Signature ```python def read_chunks(file_path: str, align: bool = True, bigendian: bool = True, inclheader: bool = False) -> list: Read and interpret chunks from the given binary file. Parameters: - file_path (str): The path to the binary file to be read. - align (bool): Whether the chunks are aligned on 2-byte boundaries. Default is True. - bigendian (bool): Whether the chunk size is encoded in big-endian order. Default is True. - inclheader (bool): Whether the chunk size includes the size of the header. Default is False. Returns: - list: A list of dictionaries, each containing information about a chunk. Each dictionary should have the following keys: - \'id\': The ID of the chunk (4-byte string). - \'size\': The size of the chunk. - \'data\': The data bytes of the chunk. pass ``` Input - `file_path`: A string representing the path to a binary file. - `align`, `bigendian`, and `inclheader`: Optional boolean parameters to specify chunk alignment, endianness, and header inclusion. Output - A list of dictionaries, each with the following keys: - `id`: The ID of the chunk as a 4-byte string. - `size`: The size of the chunk. - `data`: The data bytes of the chunk. Constraints 1. The function must use the `chunk` module to read and interpret the chunks. 2. Handle the end of the file gracefully using appropriate exception handling. 3. Assume the file is well-formed and does not contain corrupted chunks. 4. The function should be efficient for large files. Example Consider a binary file `example.aiff` with the following content: - Chunk 1: ID=`\'FORM\'`, Size=`14`, Data=`b\'AIFFCOMM\'` - Chunk 2: ID=`\'SSND\'`, Size=`8`, Data=`b\'x00x00x00x00x00x00x00x08\'` If you call the function as follows: ```python result = read_chunks(\'example.aiff\') ``` The `result` should be: ```python [ {\'id\': \'FORM\', \'size\': 14, \'data\': b\'AIFFCOMM\'}, {\'id\': \'SSND\', \'size\': 8, \'data\': b\'x00x00x00x00x00x00x00x08\'} ] ``` Assumptions: 1. The file path provided will always point to a valid binary file formatted with EA IFF 85 chunks. 2. The function will handle any alignment or endianness settings as specified by the parameters. Notes: - Pay attention to handling the optional parameters correctly. - Include appropriate error handling to manage end-of-file conditions.","solution":"import chunk def read_chunks(file_path: str, align: bool = True, bigendian: bool = True, inclheader: bool = False) -> list: Read and interpret chunks from the given binary file. Parameters: - file_path (str): The path to the binary file to be read. - align (bool): Whether the chunks are aligned on 2-byte boundaries. Default is True. - bigendian (bool): Whether the chunk size is encoded in big-endian order. Default is True. - inclheader (bool): Whether the chunk size includes the size of the header. Default is False. Returns: - list: A list of dictionaries, each containing information about a chunk. Each dictionary should have the following keys: - \'id\': The ID of the chunk (4-byte string). - \'size\': The size of the chunk. - \'data\': The data bytes of the chunk. chunks = [] with open(file_path, \'rb\') as file: try: while True: chunk_reader = chunk.Chunk(file, bigendian=bigendian, inclheader=inclheader) chunk_data = { \'id\': chunk_reader.getname().decode(\'ascii\'), \'size\': chunk_reader.getsize(), \'data\': chunk_reader.read() } chunks.append(chunk_data) # Align to 2-byte boundaries if align is True if align and chunk_reader.getsize() % 2 == 1: file.read(1) except EOFError: # End of file reached pass return chunks"},{"question":"Objective Write a Python function to serialize an `EmailMessage` object to a flattened byte stream, modifying headers to adhere to specific constraints and handling non-ASCII characters appropriately. Problem Statement Implement a function `serialize_email(msg, output_file)` that takes two arguments: 1. `msg`: an instance of `EmailMessage` containing the email message to be serialized. 2. `output_file`: a string representing the path to a file where the serialized message should be written. The function should: 1. Use the `BytesGenerator` class from the `email.generator` module to generate a binary (byte stream) representation of the email message. 2. Ensure that headers longer than 78 characters are not wrapped (i.e., retaining their original format) and the `mangle_from_` option is set to `True`. 3. Customize the policy to use `cte_type` as \'7bit\' to handle non-ASCII characters in a standards-compliant way. 4. Write the serialized byte stream to the output file specified. Constraints - Assume the `msg` object is correctly created and populated with valid email content. - Ensure that all options specified (header length, mangle_from_, and cte_type policy) are adhered to during serialization. - Handle file operations such as opening and writing to the specified output file, ensuring that the file is properly closed after writing. You may use the `EmailMessage` class from the `email.message` module and should import relevant classes and methods from `email.generator` and `email.policy`. Function Signature ```python def serialize_email(msg: EmailMessage, output_file: str) -> None: pass ``` Example Usage ```python from email.message import EmailMessage # Create a sample email message msg = EmailMessage() msg.set_content(\\"This is a test email with non-ASCII characters: Ã¤Ã¶Ã¼\\") msg[\\"Subject\\"] = \\"Test Email\\" msg[\\"From\\"] = \\"sender@example.com\\" msg[\\"To\\"] = \\"recipient@example.com\\" # Serialize the email to a file serialize_email(msg, \\"output.eml\\") ``` Hints - Refer to the documentation of `BytesGenerator` and `Policy`. - Consider how content encoding is handled based on `cte_type`. - Ensure that file operations are handled correctly to avoid file-related errors.","solution":"from email.message import EmailMessage from email.generator import BytesGenerator from email.policy import default def serialize_email(msg: EmailMessage, output_file: str) -> None: Serializes an EmailMessage object to a flattened byte stream, modifying headers to adhere to specific constraints and handling non-ASCII characters appropriately. Args: - msg: EmailMessage object containing the email message to be serialized. - output_file: String representing the path to a file where the serialized message should be written. # Create a custom policy custom_policy = default.clone(utf8=True, linesep=\'rn\', cte_type=\'sevenbit\') # Write the email message to a file using BytesGenerator with open(output_file, \'wb\') as f: bytes_generator = BytesGenerator(f, policy=custom_policy, mangle_from_=True, maxheaderlen=0) bytes_generator.flatten(msg)"},{"question":"# PyTorch Distributed Training with Uneven Inputs Objective You are tasked with implementing a distributed training setup in PyTorch that handles the scenario when input sizes across different processes are uneven. This is a common situation in distributed training and must be gracefully managed to avoid deadlocks or inefficiencies. # Instructions 1. Implement a custom model training loop using PyTorch\'s `torch.distributed.algorithms.Join`, `Joinable`, and `JoinHook` classes. 2. Use these classes to ensure that your implementation can handle scenarios where one or more processes finish their inputs earlier than others. 3. Verify that the solution handles such cases without deadlock and ensures all processes properly synchronize at the end. # Expected Input and Output - **Input Format**: - Simulated dataset sizes per process (e.g., list or array). - Total number of processes. - Example Model (e.g., a simple neural network). - **Output Format**: - Log and metrics showing the progress of each process. - Confirmation of proper handling of uneven inputs without deadlocks. # Constraints 1. Use the specified classes to design this system. 2. Ensure code clarity and proper use of PyTorch\'s distributed package. 3. Handle all possible edge cases, including processes with no data left to process early in the training phase. # Example Suppose we have 3 processes with the following input sizes: - Process 0: 1200 samples - Process 1: 800 samples - Process 2: 1000 samples Your task is to implement the training loop using the given classes such that all processes complete without any issues, even though Process 1 might complete before the others. # Code Template You can use the following template to get started: ```python import torch import torch.distributed as dist from torch.distributed.algorithms import Join, Joinable, JoinHook class CustomJoinable(Joinable): def __init__(self, data_loader): self.data_loader = data_loader def join_hook(self, join, *args, **kwargs): # Implement required behavior during join ... def train(self): for data in self.data_loader: # Training logic here ... # Call join_block to synchronize self.join_hook() def main(): dist.init_process_group(backend=\'gloo\') # Define your model model = ... # Assume dataset_sizes is predefined dataset_sizes = [1200, 800, 1000] # Create custom data loaders and models per process here data_loader = ... # depending on the rank, create appropriate data loader joinable = CustomJoinable(data_loader) # Join context management with Join([joinable]): joinable.train() dist.destroy_process_group() if __name__ == \\"__main__\\": main() ``` Implement the missing parts of the CustomJoinable class and the main function to complete the task.","solution":"import torch import torch.distributed as dist from torch.distributed.algorithms import Join, Joinable, JoinHook from torch.utils.data import DataLoader, TensorDataset import torch.nn as nn import torch.optim as optim import torch.nn.functional as F class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x class CustomJoinable(Joinable): def __init__(self, data_loader, model, optimizer): super().__init__() self.data_loader = iter(data_loader) self.model = model self.optimizer = optimizer self.loss_fn = nn.MSELoss() def join_hook(self, join, *args, **kwargs): pass def train(self): for data, target in self.data_loader: self.optimizer.zero_grad() output = self.model(data) loss = self.loss_fn(output, target) loss.backward() self.optimizer.step() self.join_hook() def main(): dist.init_process_group(backend=\'gloo\') rank = dist.get_rank() world_size = dist.get_world_size() # Simulated dataset sizes dataset_sizes = [1200, 800, 1000] data_size = dataset_sizes[rank] # Generate dummy data data = torch.randn(data_size, 10) target = torch.randn(data_size, 1) dataset = TensorDataset(data, target) data_loader = DataLoader(dataset, batch_size=32, shuffle=True) # Define model and optimizer model = SimpleModel() optimizer = optim.SGD(model.parameters(), lr=0.01) # Create custom Joinable joinable = CustomJoinable(data_loader, model, optimizer) # Use Join context management with Join([joinable]): joinable.train() dist.destroy_process_group() if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: To assess your understanding of seaborn and its customization capabilities in creating visualizations. Problem Description: You are required to create a customized bar plot using the seaborn package based on the following specifications: 1. **Data**: - Categories: [\\"Math\\", \\"Science\\", \\"History\\", \\"Physical Education\\"] - Scores: [85, 90, 78, 92] 2. **Plot Features**: - Use a bar plot to show the scores for each category. - Set the theme style to `\\"darkgrid\\"` with a `\\"deep\\"` color palette. - Customize the plot to hide the right and top spines of the plot. - Add a title to the plot: \\"Student Performance Scores\\". - Label the x-axis as \\"Subject\\" and the y-axis as \\"Scores\\". - Ensure that the y-axis limits range from 0 to 100. # Constraints: - You should only use seaborn and matplotlib.pyplot for creating and customizing the plot. - Your code should be modular and include a function `create_custom_plot()` which generates and displays the plot. Expected Function Signature: ```python def create_custom_plot(): # Your code here ``` Example Output: The generated plot should be a bar plot with specified customizations and appear similar to the following: ``` +-------------------------------------------+ | | | Student Performance Scores | | | | | | [Bar plot with the specified features] | | | | | +-------------------------------------------+ ``` Ensure your code runs without errors and achieves the specified customization features.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): # Data categories = [\\"Math\\", \\"Science\\", \\"History\\", \\"Physical Education\\"] scores = [85, 90, 78, 92] # Set the theme style sns.set_theme(style=\\"darkgrid\\", palette=\\"deep\\") # Create the bar plot plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(x=categories, y=scores) # Customizing the plot bar_plot.set_title(\\"Student Performance Scores\\") bar_plot.set_xlabel(\\"Subject\\") bar_plot.set_ylabel(\\"Scores\\") bar_plot.set(ylim=(0, 100)) # Hide top and right spines sns.despine(top=True, right=True) # Show plot plt.show()"},{"question":"# Text Processing and Analysis with Python Objective In this assessment, you will create a function to process a given paragraph of text and extract specific information using Python\'s text processing modules, namely `re` (regular expressions) and `string`. Problem Statement You are given a string `text` containing a paragraph which may include various sentences. Your task is to: 1. Extract all the sentences that contain at least one adverb (a word ending with \\"ly\\"). 2. Find all unique words in those sentences. 3. Count how many times each of these unique words appears in the original text. Function Definition ```python def text_analyzer(text: str) -> dict: Analyzes the given text and extracts sentences containing adverbs, counts unique words in those sentences, and returns the word frequencies. Parameters: text (str): A string containing the paragraph to be analyzed. Returns: dict: A dictionary where keys are the unique words (from sentences containing adverbs) and values are their corresponding frequencies in the original text. pass ``` Constraints - Sentences are separated by periods (`.`), exclamations (`!`), or questions (`?`). - A word is defined as a sequence of alphanumeric characters. - The function should be case-insensitive; words like \\"Text\\" and \\"text\\" should be considered the same. - Use the `re` module for regular expressions and `string` module for common string operations such as punctuation removal. Example ```python text = \\"She quickly ran. It was a lovely day! Sadly, it ended. Unexpectedly, the rain began.\\" # Example Output { \\"she\\": 1, \\"quickly\\": 1, \\"ran\\": 1, \\"lovely\\": 1, \\"day\\": 1, \\"sadly\\": 1, \\"ended\\": 1, \\"unexpectedly\\": 1, \\"the\\": 1, \\"rain\\": 1, \\"began\\": 1 } ``` Explanation - The sentences that contain adverbs are: \\"She quickly ran.\\", \\"It was a lovely day!\\", \\"Sadly, it ended.\\", \\"Unexpectedly, the rain began.\\" - Unique words in these sentences are: \\"she\\", \\"quickly\\", \\"ran\\", \\"it\\", \\"was\\", \\"a\\", \\"lovely\\", \\"day\\", \\"sadly\\", \\"ended\\", \\"unexpectedly\\", \\"the\\", \\"rain\\", \\"began\\". - Their counts in the original text are presented in the output dictionary. Use appropriate Python code to parse, analyze, and compute the required results to demonstrate your handling of text processing in Python.","solution":"import re import string from collections import defaultdict def text_analyzer(text: str) -> dict: Analyzes the given text and extracts sentences containing adverbs, counts unique words in those sentences, and returns the word frequencies. Parameters: text (str): A string containing the paragraph to be analyzed. Returns: dict: A dictionary where keys are the unique words (from sentences containing adverbs) and values are their corresponding frequencies in the original text. # Normalize the text to lowercase text = text.lower() # Split the text into sentences using regex to consider ., !, ? sentences = re.split(r\'[.!?]\', text) # Extract sentences containing at least one adverb (a word ending with \\"ly\\") adverb_sentences = [sentence for sentence in sentences if re.search(r\'bw+lyb\', sentence)] # Find all unique words in the extracted sentences unique_words = set() for sentence in adverb_sentences: words = re.findall(r\'bw+b\', sentence) unique_words.update(words) # Normalize unique words by removing punctuation and converting to lowercase unique_words = {word.strip(string.punctuation) for word in unique_words} # Count the frequency of each unique word in the original text word_frequencies = defaultdict(int) all_words = re.findall(r\'bw+b\', text) for word in all_words: # Strip punctuation from the word word = word.strip(string.punctuation) if word in unique_words: word_frequencies[word] += 1 return dict(word_frequencies)"},{"question":"<|Analysis Begin|> The provided documentation details the \\"random\\" module in Python, which includes a variety of pseudo-random number generators for different distributions. Key elements covered include functions for generating random integers, floats, and bytes, as well as more specialized distributions (e.g., normal, exponential, beta). Additionally, the module offers functions for working with sequences (e.g., choosing random elements, shuffling), and allows for the instantiation and customization of random number generators through the `Random` and `SystemRandom` classes. From this documentation, we can derive an advanced conceptual understanding of pseudo-random number generation in Python, including its applications, limitations (such as its unsuitability for cryptographic purposes), and methods for ensuring reproducibility. We will design a coding assessment question that requires students to understand the module\'s capabilities to generate and manipulate random data, particularly focusing on nuanced aspects such as sampling distributions and random shuffling while maintaining performance considerations. <|Analysis End|> <|Question Begin|> # Advanced Coding Assessment Question: Random Data Analysis and Manipulation The goal of this assignment is to develop an understanding of basic and advanced concepts of the \\"random\\" module in Python. You will implement a set of utility functions that leverage the capabilities of the \\"random\\" module to work with random data. Task Description Implement the following utility functions: 1. **random_float_list(low, high, size)**: - **Input**: - `low` (float): The lower bound for the random floats. - `high` (float): The upper bound for the random floats. - `size` (int): The size of the list to generate. - **Output**: - A list of `size` random floating point numbers, each between `low` and `high`. - **Constraints**: - `low` must be less than `high`. - `size` should be a non-negative integer. 2. **random_weighted_choice(population, weights, k)**: - **Input**: - `population` (list): A list of elements to choose from. - `weights` (list): A list of weights corresponding to the `population`. - `k` (int): The number of choices to make. - **Output**: - A list of `k` elements chosen from `population` with probabilities defined by `weights`. - **Constraints**: - Length of `population` and `weights` must be the same. - `weights` must be non-negative numbers. - `population` must not be empty. - `k` should be a non-negative integer. 3. **shuffle_and_sample(population, sample_size)**: - **Input**: - `population` (list): A list of elements to shuffle and sample. - `sample_size` (int): The size of the sample to return after shuffling. - **Output**: - A list of `sample_size` elements from the shuffled `population`. - **Constraints**: - `sample_size` must be less than or equal to the size of `population`. - **Performance Requirement**: - The shuffle operation should be performed in place for maximal efficiency. 4. **estimate_mean_confidence_interval(data, confidence_level)**: - **Input**: - `data` (list): A list of numerical data. - `confidence_level` (float): The desired confidence level for the interval (e.g., 0.90 for 90% confidence). - **Output**: - A tuple containing the lower and upper bounds of the confidence interval for the mean of `data`. - **Constraints**: - `data` must not be empty. - `confidence_level` must be between 0 and 1. - **Performance Requirement**: - Use resampling with replacement to compute the confidence interval. Additional Notes - All random operations should be deterministic if a specific seed is set using `random.seed()`. - Ensure your code is efficient and handles edge cases appropriately. - Document your functions with appropriate docstrings. Here\'s a template to help you get started: ```python import random from statistics import mean from typing import List, Tuple def random_float_list(low: float, high: float, size: int) -> List[float]: Generate a list of random floats within the specified range. # Implementation here def random_weighted_choice(population: List, weights: List[float], k: int) -> List: Generate a weighted random choice of elements from the population. # Implementation here def shuffle_and_sample(population: List, sample_size: int) -> List: Shuffle population in place and return a sample of the specified size. # Implementation here def estimate_mean_confidence_interval(data: List[float], confidence_level: float) -> Tuple[float, float]: Estimate confidence interval for the mean of the given data. # Implementation here ``` **Submission Checklist**: 1. Ensure all functions are implemented as specified. 2. Include test cases validating your functions\' correctness. 3. Add comments and docstrings for clarity and documentation. 4. Confirm your code\'s efficiency and edge case handling. Good luck!","solution":"import random from statistics import mean from typing import List, Tuple def random_float_list(low: float, high: float, size: int) -> List[float]: Generate a list of random floats within the specified range. Parameters: low (float): The lower bound for the random floats. high (float): The upper bound for the random floats. size (int): The number of random floats to generate. Returns: List[float]: A list of random floats. if low >= high: raise ValueError(\\"low must be less than high\\") if size < 0: raise ValueError(\\"size must be a non-negative integer\\") return [random.uniform(low, high) for _ in range(size)] def random_weighted_choice(population: List, weights: List[float], k: int) -> List: Generate a weighted random choice of elements from the population. Parameters: population (list): A list of elements to choose from. weights (list): A list of weights corresponding to the `population`. k (int): The number of choices to make. Returns: List: A list of chosen elements. if not population: raise ValueError(\\"population must not be empty\\") if len(population) != len(weights): raise ValueError(\\"the lengths of population and weights must be the same\\") if any(w < 0 for w in weights): raise ValueError(\\"weights must be non-negative\\") if k < 0: raise ValueError(\\"k must be a non-negative integer\\") return random.choices(population, weights, k=k) def shuffle_and_sample(population: List, sample_size: int) -> List: Shuffle population in place and return a sample of the specified size. Parameters: population (list): A list of elements to shuffle and sample. sample_size (int): The size of the sample to return after shuffling. Returns: List: A list of sampled elements. if sample_size > len(population): raise ValueError(\\"sample_size must be less than or equal to the size of population\\") random.shuffle(population) return population[:sample_size] def estimate_mean_confidence_interval(data: List[float], confidence_level: float) -> Tuple[float, float]: Estimate confidence interval for the mean of the given data using bootstrap resampling. Parameters: data (list): A list of numerical data. confidence_level (float): The desired confidence level (between 0 and 1). Returns: Tuple[float, float]: The lower and upper bounds of the confidence interval. if not data: raise ValueError(\\"data must not be empty\\") if not (0 < confidence_level < 1): raise ValueError(\\"confidence_level must be between 0 and 1\\") n = len(data) means = [] num_resamples = 1000 for _ in range(num_resamples): sample = [random.choice(data) for _ in range(n)] means.append(mean(sample)) lower_bound = (1.0 - confidence_level) / 2.0 upper_bound = 1.0 - lower_bound sorted_means = sorted(means) return sorted_means[int(lower_bound * num_resamples)], sorted_means[int(upper_bound * num_resamples)]"},{"question":"# Question: Refactoring Python Code Using `ast` Module Your task is to write a Python function that refactors a given piece of Python code by replacing all variable names starting with the prefix \\"temp_\\" with a new prefix \\"var_\\". You need to achieve this refactoring by manipulating the abstract syntax tree (AST) of the given code using the `ast` module. Function Signature ```python def refactor_variable_names(source_code: str) -> str: Refactors the given source code by replacing all variable names starting with \\"temp_\\" with \\"var_\\" Args: source_code (str): A string representing the Python source code to be refactored. Returns: str: The refactored source code as a string. ``` Instructions 1. **Parse** the provided `source_code` into an AST. 2. **Traverse and transform** the AST: - Identify all `Name` nodes representing variable names starting with \\"temp_\\" and change the prefix to \\"var_\\". 3. **Recompile** the modified AST back into Python source code and return the updated code as a string. Constraints - The input `source_code` is guaranteed to be valid Python code. - You should maintain the original structure and formatting of the code as much as possible. - Only variable names should be refactored (function names, class names, etc. should not be changed). Example ```python source_code = \'\'\' temp_a = 10 temp_b = 20 result = temp_a + temp_b print(temp_a, temp_b, result) \'\'\' refactored_code = refactor_variable_names(source_code) print(refactored_code) ``` **Expected Output:** ```python var_a = 10 var_b = 20 result = var_a + var_b print(var_a, var_b, result) ``` Hints - Use the `ast.parse` function to parse the source code into an AST. - Utilize `ast.walk` or `NodeVisitor` to traverse the AST. - Modify `ast.Name` nodes where the `id` attribute starts with \\"temp_\\". - Use `ast.unparse` to convert the modified AST back to source code. Additional Resources Refer to the official Python documentation for the `ast` module to understand the available classes and methods that can help you achieve the transformation.","solution":"import ast class RefactorVariableNames(ast.NodeTransformer): def visit_Name(self, node): if node.id.startswith(\\"temp_\\"): node.id = \\"var_\\" + node.id[5:] return node def refactor_variable_names(source_code: str) -> str: Refactors the given source code by replacing all variable names starting with \\"temp_\\" with \\"var_\\" Args: source_code (str): A string representing the Python source code to be refactored. Returns: str: The refactored source code as a string. tree = ast.parse(source_code) transformer = RefactorVariableNames() new_tree = transformer.visit(tree) # Ensure all nodes are fixed properly ast.fix_missing_locations(new_tree) return ast.unparse(new_tree)"},{"question":"**Question: Implement a Custom Encoder and Decoder for a Simple ASCII-Compatible Format** In this coding challenge, you are required to implement custom encoding and decoding functions similar to those provided by the `binhex` module. Instead of using the binhex4 format, you will create a simplified ASCII-compatible format to encode and decode text files. # Function Specifications 1. `encode_to_custom_format(input_file: str, output_file: str) -> None` - **Input**: - `input_file`: The name of the text file (binary data) to be encoded. - `output_file`: The name of the output file where the encoded data will be stored. - **Output**: - This function does not return anything. - **Functionality**: - Convert the content of the `input_file` to a custom ASCII-compatible format and write the result to `output_file`. - You can define any mechanism for encoding but ensure that the output only contains printable ASCII characters (decimal 32 to 126). 2. `decode_from_custom_format(input_file: str, output_file: str) -> None` - **Input**: - `input_file`: The name of the file containing encoded data. - `output_file`: The name of the output file where the decoded binary data will be stored. - **Output**: - This function does not return anything. - **Functionality**: - Read the encoded content from `input_file`, decode it back to its original binary form, and write the result to `output_file`. # Constraints - The encoded format should be such that the conversion is reversible; i.e., after encoding and then decoding, the original content is retrievable without any loss. - The input and output files are expected to be within the same directory as the script being executed. - Ensure to handle exceptions, such as file not found or read/write errors gracefully, providing appropriate error messages. # Example Suppose you have a binary file named `example.bin`. When you run: ```python encode_to_custom_format(\'example.bin\', \'encoded.txt\') decode_from_custom_format(\'encoded.txt\', \'decoded.bin\') ``` The content of `decoded.bin` should match exactly with `example.bin`. Note: Define the custom encoding format as a part of your solution. The encoding approach can be simple, like base64 or a user-defined method ensuring ASCII compatibility.","solution":"import os def encode_to_custom_format(input_file: str, output_file: str) -> None: Encodes binary data to a custom ASCII-compatible format. try: with open(input_file, \'rb\') as infile: data = infile.read() # Using a simplified custom encoding here, similar to base64 encoded = \'\' for byte in data: encoded += chr((byte >> 4) + 65) # First 4 bits encoded += chr((byte & 0x0F) + 65) # Last 4 bits with open(output_file, \'w\') as outfile: outfile.write(encoded) except FileNotFoundError: print(f\\"File {input_file} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") def decode_from_custom_format(input_file: str, output_file: str) -> None: Decodes a custom ASCII-compatible format back to binary data. try: with open(input_file, \'r\') as infile: data = infile.read() decoded = bytearray() for i in range(0, len(data), 2): first_half = ord(data[i]) - 65 second_half = ord(data[i+1]) - 65 decoded.append((first_half << 4) | second_half) with open(output_file, \'wb\') as outfile: outfile.write(decoded) except FileNotFoundError: print(f\\"File {input_file} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Question: Analyzing Sales Data with pandas GroupBy** As a data analyst, you have been given a dataset containing information about the sales of a company\'s products over multiple regions and quarters. The dataset is provided as a CSV file with the following columns: - `region`: The geographical region where sales were made (e.g., \\"North\\", \\"South\\", \\"East\\", \\"West\\"). - `quarter`: The quarter in which the sales were made (e.g., \\"Q1\\", \\"Q2\\", \\"Q3\\", \\"Q4\\"). - `product`: The name of the product sold. - `sales`: Number of units sold. - `revenue`: Total revenue from the sales of the product. Your task is to write a function that performs the following analysis using pandas GroupBy: 1. Load the CSV file into a pandas DataFrame. 2. Group the data by `region` and `quarter`. 3. For each group, calculate: - Total sales (`total_sales`) - Total revenue (`total_revenue`) - Average sales per product (`avg_sales_per_product`) - The number of unique products sold (`unique_products_sold`) The function should return a DataFrame with the following columns: - `region`: The geographical region. - `quarter`: The quarter. - `total_sales` - `total_revenue` - `avg_sales_per_product` - `unique_products_sold` # Expected Input The function should take a single parameter: - `csv_file_path` (str): The file path of the CSV to load the data. # Expected Output A pandas DataFrame with columns `region`, `quarter`, `total_sales`, `total_revenue`, `avg_sales_per_product`, `unique_products_sold`, and rows corresponding to each combination of `region` and `quarter`. # Constraints 1. The input CSV file will have valid and non-empty data. 2. The `sales` and `revenue` fields contain non-negative values. # Example Input ```python csv_file_path = \'sales_data.csv\' ``` The content of `sales_data.csv` might look like: ``` region,quarter,product,sales,revenue North,Q1,ProductA,150,1500 North,Q1,ProductB,200,3000 South,Q2,ProductC,300,4500 West,Q3,ProductD,400,6000 East,Q4,ProductE,500,7500 ... ``` # Function Signature ```python import pandas as pd def analyze_sales_data(csv_file_path: str) -> pd.DataFrame: # Your code here ``` **Note:** Handle any potential data-related issues such as missing values gracefully (though the constraint states the data is valid, this is always a good practice).","solution":"import pandas as pd def analyze_sales_data(csv_file_path: str) -> pd.DataFrame: # Load the CSV file into a pandas DataFrame df = pd.read_csv(csv_file_path) # Group the data by \'region\' and \'quarter\' grouped = df.groupby([\'region\', \'quarter\']) # Calculate total_sales, total_revenue, avg_sales_per_product, and unique_products_sold result = grouped.agg( total_sales=(\'sales\', \'sum\'), total_revenue=(\'revenue\', \'sum\'), avg_sales_per_product=(\'sales\', \'mean\'), unique_products_sold=(\'product\', \'nunique\') ).reset_index() return result"},{"question":"Objective: Write a function using the Seaborn library to generate a multi-category scatter plot and a numeric heatmap, applying appropriate color palettes for each visualization type. Task: 1. Create a function `create_visualizations(data)` that accepts a Pandas DataFrame `data` with columns \'Category\', \'X\', \'Y\', and \'Value\': - \'Category\' is a categorical variable. - \'X\' and \'Y\' are numeric variables representing the coordinates of scatter plot points. - \'Value\' is a numeric variable representing the value to be visualized in the heatmap. 2. The function should generate: - A scatter plot with points colored by \'Category\' using a qualitative color palette. - A heatmap with \'X\' and \'Y\' as coordinates, and \'Value\' as the heat intensity, using a sequential color palette. 3. Save both plots as \'scatter_plot.png\' and \'heatmap_plot.png\'. Function Signature: ```python import pandas as pd def create_visualizations(data: pd.DataFrame) -> None: pass ``` Constraints and Requirements: - Use Seaborn for plotting. - Choose appropriate Seaborn functions to achieve the tasks. - Ensure plots are properly labeled and visually informative. - Handle any necessary data preprocessing within the function. Example Data: ```python import pandas as pd data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'A\', \'C\', \'B\', \'A\', \'C\', \'C\', \'B\', \'A\'], \'X\': [1.1, 2.2, 1.2, 3.3, 2.1, 1.5, 3.6, 3.2, 2.8, 1.9], \'Y\': [2.1, 1.2, 2.3, 1.1, 2.5, 2.9, 1.8, 1.7, 2.4, 2.7], \'Value\': [10, 15, 10, 5, 15, 12, 5, 7, 14, 13] }) ``` Expected Output: - A scatter plot saved as \'scatter_plot.png\' that uses a qualitative palette to color the points based on \'Category\'. - A heatmap saved as \'heatmap_plot.png\' that represents the \'Value\' as the intensity of the heatmap with a sequential palette. Additional Notes: - Comment your code to demonstrate your understanding. - Ensure to handle cases where the input data might be empty or invalid.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(data: pd.DataFrame) -> None: if data.empty or not all(col in data.columns for col in [\'Category\', \'X\', \'Y\', \'Value\']): raise ValueError(\\"Input data must contain \'Category\', \'X\', \'Y\', and \'Value\' columns\\") # Generate scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot( x=\'X\', y=\'Y\', hue=\'Category\', palette=\'Set2\', data=data ) scatter_plot.set_title(\'Scatter Plot by Category\') scatter_plot.set_xlabel(\'X Axis\') scatter_plot.set_ylabel(\'Y Axis\') plt.legend(title=\'Category\') plt.savefig(\'scatter_plot.png\') plt.clf() # Clear the current figure # Generate heatmap heatmap_data = data.pivot_table(index=\'Y\', columns=\'X\', values=\'Value\', fill_value=0) plt.figure(figsize=(10, 8)) heatmap_plot = sns.heatmap(heatmap_data, cmap=\'YlGnBu\', annot=True) heatmap_plot.set_title(\'Heatmap of Values\') plt.savefig(\'heatmap_plot.png\') plt.clf() # Clear the current figure"},{"question":"# HTTP Status Code Lookup Task You are required to implement a function `get_http_status_info(status_code: int) -> str` that takes an HTTP status code as input and returns a formatted string with information about the status code. The information should include the enum name, the numerical code, the reason phrase, and a detailed description. Function Signature ```python def get_http_status_info(status_code: int) -> str: pass ``` Input - An integer `status_code` representing the HTTP status code (e.g., `200`, `404`). Output - A string in the following format: `\\"Enum Name: <EnumName>, Code: <Code>, Phrase: <Phrase>, Description: <Description>\\"` Constraints - The status code will always be a valid integer between 100 and 599, inclusive. Example ```python from http import HTTPStatus def get_http_status_info(status_code: int) -> str: status = HTTPStatus(status_code) return f\\"Enum Name: {status.name}, Code: {status.value}, Phrase: {status.phrase}, Description: {status.description}\\" # Example usage and expected outputs print(get_http_status_info(200)) # Output: Enum Name: OK, Code: 200, Phrase: OK, Description: Request fulfilled, document follows print(get_http_status_info(404)) # Output: Enum Name: NOT_FOUND, Code: 404, Phrase: Not Found, Description: The requested resource could not be found but may be available again in the future. ``` Notes - You need to use the `HTTPStatus` class from the `http` module. - Handle the absence of any status codes properly by using appropriate error handling or by ensuring status codes are always valid.","solution":"from http import HTTPStatus def get_http_status_info(status_code: int) -> str: status = HTTPStatus(status_code) # Custom descriptions for common HTTP status codes descriptions = { 100: \\"The server has received the request headers and the client should proceed to send the request body.\\", 200: \\"Request fulfilled, document follows.\\", 201: \\"The request has been fulfilled and resulted in a new resource being created.\\", 400: \\"The server could not understand the request due to invalid syntax.\\", 404: \\"The requested resource could not be found but may be available again in the future.\\", 500: \\"The server encountered an unexpected condition that prevented it from fulfilling the request.\\", # Add descriptions for more statuses as needed } description = descriptions.get(status_code, \\"No detailed description provided.\\") return f\\"Enum Name: {status.name}, Code: {status.value}, Phrase: {status.phrase}, Description: {description}\\""},{"question":"**Problem Statement:** You are given a list of students\' scores in various subjects, stored as a list of dictionaries. Each dictionary represents a student and contains their name and their scores in various subjects. Implement the following functions to analyze the scores: 1. **Function 1: Calculate Average Scores** - **Input:** A list of dictionaries, where each dictionary contains the name of the student (string) and their scores (dictionary of subjects and scores). - **Output:** A dictionary where each key is the student\'s name and the value is the average score of the student. 2. **Function 2: Top Students in Each Subject** - **Input:** A list of dictionaries, where each dictionary contains the name of the student (string) and their scores (dictionary of subjects and scores). - **Output:** A dictionary where each key is a subject and the value is a tuple containing the name of the student with the highest score in that subject and the score itself. # Example: ```python students_scores = [ {\\"name\\": \\"Alice\\", \\"scores\\": {\\"math\\": 95, \\"english\\": 85, \\"science\\": 90}}, {\\"name\\": \\"Bob\\", \\"scores\\": {\\"math\\": 89, \\"english\\": 92, \\"science\\": 82}}, {\\"name\\": \\"Charlie\\", \\"scores\\": {\\"math\\": 87, \\"english\\": 78, \\"science\\": 88}}, ] # Function 1 Output: # { # \\"Alice\\": 90.0, # \\"Bob\\": 87.67, # \\"Charlie\\": 84.33 # } # Function 2 Output: # { # \\"math\\": (\\"Alice\\", 95), # \\"english\\": (\\"Bob\\", 92), # \\"science\\": (\\"Alice\\", 90) # } ``` # Constraints: - You may assume that each student has scores for the same set of subjects. - Ensure that your functions handle floating-point arithmetic correctly (round to two decimal places). # Requirements: - Define two functions as described above. - Ensure your solution uses appropriate list and dictionary methods and comprehensions. - Avoid using any third-party libraries; utilize standard Python functionalities. # Performance: - Aim for a solution that is efficient. Analyze the performance of your solution and ensure that it can handle larger datasets effectively. This question assesses the understanding of Python data structures, particularly lists and dictionaries, and the ability to manipulate and analyze data using these structures. It also tests the ability to use list comprehensions and handle nested data structures effectively.","solution":"def calculate_average_scores(students_scores): Calculates the average score for each student. :param students_scores: List of dictionaries containing student names and their scores. :return: Dictionary with student names as keys and their average scores as values. average_scores = {} for student in students_scores: name = student[\'name\'] scores = student[\'scores\'].values() average_score = sum(scores) / len(scores) average_scores[name] = round(average_score, 2) return average_scores def top_students_in_each_subject(students_scores): Finds the student with the highest score in each subject. :param students_scores: List of dictionaries containing student names and their scores. :return: Dictionary with subjects as keys and tuples of (student name, score) as values. top_students = {} for student in students_scores: name = student[\'name\'] for subject, score in student[\'scores\'].items(): if subject not in top_students or score > top_students[subject][1]: top_students[subject] = (name, score) return top_students"},{"question":"Objective: Write a Python class that processes HTML files and extracts specific elements, attributes, and their data. This will assess your understanding of subclassing the `HTMLParser` class and overriding its handler methods. Task: Create a subclass of `HTMLParser` called `AttributeValueCounter`. The class will process an HTML document and count the occurrences of specified attributes, storing these counts in a dictionary. Requirements: 1. **Class Definition**: - Define a class `AttributeValueCounter` which inherits from `HTMLParser`. - The class should have an `__init__` method to initialize the parser and any required variables. 2. **Handler Methods**: - Override the `handle_starttag` method to handle start tags and count specified attributes. 3. **Count Attributes**: - The class should keep a dictionary called `attribute_counts` that maps attribute names to their occurrence counts. 4. **Methods**: - Implement a method `get_attribute_counts` that returns the `attribute_counts` dictionary. Example Usage: ```python from html.parser import HTMLParser class AttributeValueCounter(HTMLParser): def __init__(self, attributes): super().__init__() self.attributes = attributes self.attribute_counts = {attribute: 0 for attribute in attributes} def handle_starttag(self, tag, attrs): for attr_name, attr_value in attrs: if attr_name in self.attributes: self.attribute_counts[attr_name] += 1 def get_attribute_counts(self): return self.attribute_counts # Example HTML html_content = <html> <head><title>Test</title></head> <body> <h1 class=\\"header\\">Heading</h1> <a href=\\"http://example.com\\" class=\\"link\\">Example</a> <div id=\\"main\\" class=\\"container\\"> <p id=\\"paragraph\\">Text</p> </div> </body> </html> parser = AttributeValueCounter(attributes=[\'class\', \'id\', \'href\']) parser.feed(html_content) counts = parser.get_attribute_counts() print(counts) # Should output: {\'class\': 3, \'id\': 2, \'href\': 1} ``` Constraints: - Only handle start tags (`<tag>`). Do not consider self-closing tags (`<tag/>`). - Your class should correctly count the attributes only for start tags. - Ensure all methods work as expected even if the HTML contains nested elements. Submission: Submit the `AttributeValueCounter` class implementation along with an example usage demonstrating the counting of specified attributes from a given HTML string.","solution":"from html.parser import HTMLParser class AttributeValueCounter(HTMLParser): def __init__(self, attributes): super().__init__() self.attributes = attributes self.attribute_counts = {attribute: 0 for attribute in attributes} def handle_starttag(self, tag, attrs): for attr_name, attr_value in attrs: if attr_name in self.attributes: self.attribute_counts[attr_name] += 1 def get_attribute_counts(self): return self.attribute_counts"},{"question":"Objective: Demonstrate your understanding of the seaborn library by creating a set of customized plots. Problem Statement: Write a Python function `create_custom_plots` that accepts a dictionary of data and generates three different types of plots using seaborn. These plots should showcase different seaborn themes and customizations. Function Signature: ```python def create_custom_plots(data: dict) -> None: pass ``` Input: - `data`: A dictionary where: - Key `\\"x\\"` is a list of categories (strings). - Key `\\"y\\"` is a list of values (integers or floats), corresponding to the categories in `\\"x\\"`. Output: - The function generates three plots and displays them using `matplotlib.pyplot.show()`. The plots should be: 1. A bar plot with the default seaborn theme. 2. A bar plot with the \\"whitegrid\\" style and \\"pastel\\" palette. 3. A bar plot with the \\"ticks\\" style and custom parameters that remove the right and top spines. Requirements: 1. Use `sns.barplot` for all the plots. 2. Each plot should be displayed using `matplotlib.pyplot.show()` to ensure they appear in the order specified. 3. No return value is required from the function. Example: ```python data = { \\"x\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"y\\": [4, 1, 7, 3] } create_custom_plots(data) ``` The function should produce: 1. A default theme bar plot. 2. A bar plot with \\"whitegrid\\" style and \\"pastel\\" palette. 3. A bar plot with \\"ticks\\" style and custom parameters to remove the right and top spines. Constraints: - Assume that `seaborn` and `matplotlib` are already installed and imported. - Ensure the plots are visible on separate `matplotlib.pyplot.show()` calls. Good luck, and remember to test your function with different datasets to verify its correctness!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(data: dict) -> None: Generates three different bar plots using seaborn with different themes and customizations. x = data.get(\'x\', []) y = data.get(\'y\', []) if not x or not y or len(x) != len(y): raise ValueError(\\"Input data must have \'x\' and \'y\' keys with non-empty lists of equal length.\\") # Default seaborn theme bar plot sns.barplot(x=x, y=y) plt.title(\'Default theme Bar Plot\') plt.show() # Whitegrid style and pastel palette bar plot sns.set(style=\'whitegrid\', palette=\'pastel\') sns.barplot(x=x, y=y) plt.title(\'Whitegrid style & Pastel palette Bar Plot\') plt.show() # Ticks style with custom parameter to remove right and top spines sns.set(style=\'ticks\') sns.barplot(x=x, y=y) sns.despine() plt.title(\'Ticks style with custom parameters Bar Plot\') plt.show()"},{"question":"**Objective:** Demonstrate your proficiency in using the `linecache` module to efficiently read specific lines from files while managing cache and handling potential errors gracefully. **Background:** You are provided with a log file named `server.log` that contains multiple lines of text. The file uses UTF-8 encoding. You need to write a function that accurately reads specific lines from this file, manages the cache for performance, and handles any potential file changes or errors. **Task:** Implement the function `read_log_lines(filepath: str, line_numbers: List[int]) -> Dict[int, str]` where: - `filepath`: The path to the `server.log` file. - `line_numbers`: A list of integers representing the line numbers you want to read from the file. The function should return a dictionary where the keys are the requested line numbers and the values are the corresponding lines from the file. If a line number is invalid or if any error occurs during reading, return an empty string for that line number. Additionally, your function should: 1. Clear the cache after reading the lines. 2. Validate the cache before reading lines to handle potential changes in the file. 3. Handle scenarios where the file might not exist or be unreadable gracefully. **Example:** ```python # Sample log file content (server.log): # Line 1: \\"2023-10-01 10:00:00 INFO Start processing\\" # Line 2: \\"2023-10-01 10:05:00 ERROR An error occurred\\" # Line 3: \\"2023-10-01 10:10:00 INFO Finished processing\\" line_numbers = [1, 2, 4] result = read_log_lines(\\"server.log\\", line_numbers) # Expected output: # { # 1: \\"2023-10-01 10:00:00 INFO Start processing\\", # 2: \\"2023-10-01 10:05:00 ERROR An error occurred\\", # 4: \\"\\" # Line 4 does not exist # } ``` **Constraints:** - Assume the file is in UTF-8 encoding. - Your solution should efficiently handle reading lines using caching mechanisms provided by `linecache`. - You must handle any exceptions or errors gracefully, ensuring the function does not crash. **Performance Requirements:** - The solution should efficiently manage the cache to minimize I/O operations. - Ensure the function is robust and handles varying file sizes and line number requests efficiently. **Note:** For the purpose of this assessment, you can mock the content of `server.log` to test your function. ```python import linecache from typing import List, Dict def read_log_lines(filepath: str, line_numbers: List[int]) -> Dict[int, str]: # Your implementation here pass ```","solution":"import linecache from typing import List, Dict def read_log_lines(filepath: str, line_numbers: List[int]) -> Dict[int, str]: result = {} for line_number in line_numbers: try: linecache.checkcache(filepath) line = linecache.getline(filepath, line_number).strip() if not line and line_number > 0: # Line does not exist line = \\"\\" result[line_number] = line except Exception: result[line_number] = \\"\\" linecache.clearcache() return result"},{"question":"# Advanced Pandas Window Function Implementation **Objective**: Implement a custom rolling window computation in pandas. **Problem Statement**: You are provided with a pandas DataFrame containing time-series data. Your task is to implement a function `custom_rolling_operation` that calculates a custom rolling window metric over a specified window size. The custom metric to be calculated is the sum of the mean and standard deviation of the data within the window. Additionally, you should support the option of using a custom window indexer. **Function Signature**: ```python def custom_rolling_operation(df: pd.DataFrame, window_size: int, use_custom_indexer: bool = False) -> pd.DataFrame: pass ``` **Input**: - `df`: A pandas DataFrame with numeric columns and a datetime index. - `window_size`: An integer representing the size of the rolling window. - `use_custom_indexer`: A boolean flag to indicate whether to use a custom indexer (`True`) or the default rolling window (`False`). **Output**: - A pandas DataFrame where each column contains the computed custom rolling metric (sum of mean and standard deviation) for the corresponding input column. **Constraints**: - The DataFrame will have at most 100,000 rows and 10 columns. - The datetime index values will be sorted in ascending order. - Performance should be considered for large datasets. **Example**: ```python import pandas as pd import numpy as np # Sample DataFrame data = { \'A\': np.random.randn(100), \'B\': np.random.randn(100) } df = pd.DataFrame(data, index=pd.date_range(\'20210101\', periods=100)) # Call the function with a 5-period window and default indexer result = custom_rolling_operation(df, window_size=5) # Expected output: DataFrame with custom rolling metric. print(result.head()) ``` **Notes**: 1. If `use_custom_indexer` is set to `True`, you should create and use a custom window indexer that defines the window boundaries. 2. If there are less than `window_size` data points available at the current point in the data, the custom metric should not be computed, and the result should be `NaN` for that position. Apply your understanding of pandas window functions and custom indexers to solve this problem efficiently.","solution":"import pandas as pd import numpy as np def custom_rolling_operation(df: pd.DataFrame, window_size: int, use_custom_indexer: bool = False) -> pd.DataFrame: if use_custom_indexer: return df.apply(lambda x: x.rolling(window=window_size, center=False).apply(lambda w: w.mean() + w.std(), raw=False)) else: return df.rolling(window=window_size).apply(lambda w: w.mean() + w.std(), raw=False)"},{"question":"Coding Assessment Question # Objective Implement a custom logging system using Python\'s `logging` module, demonstrating the creation of custom loggers, handlers, formatters, and filters. # Problem Statement You are tasked with developing a logging system for an application. The logging system must include custom handlers, formatters, and filters. Additionally, it must demonstrate the ability to log messages from various parts of the application to different destinations with appropriate formatting and filtering. # Requirements 1. **Custom Logger Hierarchy**: Create logger instances named `app`, `app.module1`, and `app.module2`. Log messages to these loggers. 2. **Custom Handlers**: - A `FileHandler` that logs messages to a file named `app.log`. This handler should be associated with the `app` logger. - A `StreamHandler` that logs messages to the console. This handler should be associated with the `app.module1` logger. - Another `FileHandler` that logs messages to a file named `module2.log`. This handler should be associated with the `app.module2` logger. 3. **Custom Formatters**: - For the `FileHandler` associated with the `app` logger, use a format that includes the timestamp, logger name, log level, and message. - For the `StreamHandler` associated with the `app.module1` logger, use a simpler format that includes just the log level and message. - For the `FileHandler` associated with the `app.module2` logger, use a format that includes the module name, line number, log level, and message. 4. **Custom Filter**: - Implement a filter for the `StreamHandler` associated with the `app.module1` logger, that only logs messages with the log level WARNING or higher. 5. Demonstrate logging messages from different parts of the application using each logger. # Constraints - The log file paths `app.log` and `module2.log` should be in the current working directory. - The filter for `StreamHandler` should not affect other handlers. - Use Python 3.10 or higher. # Function Signature ```python import logging def setup_logging_system(): # Implement the logging setup here def log_demo(): # Implement demo logging here if __name__ == \'__main__\': setup_logging_system() log_demo() ``` # Expected Output - Messages of all levels should be logged to `app.log` with a detailed format. - WARNING and higher-level messages should appear in the console. - Messages of all levels from `app.module2` should be logged to `module2.log` with the specified format. # Performance Requirements - The logging setup should efficiently handle logging from multiple parts of the application. - Ensure thread-safety as logging operations might be invoked from different threads in real-world scenarios. # Example Demonstrate logging messages such as: ```python logger_app = logging.getLogger(\'app\') logger_mod1 = logging.getLogger(\'app.module1\') logger_mod2 = logging.getLogger(\'app.module2\') logger_app.info(\'This is an info message from app\') logger_mod1.warning(\'This is a warning message from module1\') logger_mod2.error(\'This is an error message from module2\') ``` Make sure that the messages are logged according to the specified requirements.","solution":"import logging def setup_logging_system(): # Create logger instances logger_app = logging.getLogger(\'app\') logger_mod1 = logging.getLogger(\'app.module1\') logger_mod2 = logging.getLogger(\'app.module2\') # Set log level logger_app.setLevel(logging.DEBUG) logger_mod1.setLevel(logging.DEBUG) logger_mod2.setLevel(logging.DEBUG) # FileHandler for app logger file_handler_app = logging.FileHandler(\'app.log\') file_formatter_app = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler_app.setFormatter(file_formatter_app) logger_app.addHandler(file_handler_app) # StreamHandler for module1 logger with a filter for WARNING and above stream_handler_mod1 = logging.StreamHandler() stream_formatter_mod1 = logging.Formatter(\'%(levelname)s - %(message)s\') stream_handler_mod1.setFormatter(stream_formatter_mod1) stream_handler_mod1.setLevel(logging.WARNING) # Only warnings and above logger_mod1.addHandler(stream_handler_mod1) # FileHandler for module2 logger file_handler_mod2 = logging.FileHandler(\'module2.log\') file_formatter_mod2 = logging.Formatter(\'%(module)s:%(lineno)d - %(levelname)s - %(message)s\') file_handler_mod2.setFormatter(file_formatter_mod2) logger_mod2.addHandler(file_handler_mod2) def log_demo(): logger_app = logging.getLogger(\'app\') logger_mod1 = logging.getLogger(\'app.module1\') logger_mod2 = logging.getLogger(\'app.module2\') logger_app.debug(\'Debug message from app\') logger_app.info(\'Info message from app\') logger_app.warning(\'Warning message from app\') logger_app.error(\'Error message from app\') logger_app.critical(\'Critical message from app\') logger_mod1.debug(\'Debug message from module1\') logger_mod1.info(\'Info message from module1\') logger_mod1.warning(\'Warning message from module1\') logger_mod1.error(\'Error message from module1\') logger_mod1.critical(\'Critical message from module1\') logger_mod2.debug(\'Debug message from module2\') logger_mod2.info(\'Info message from module2\') logger_mod2.warning(\'Warning message from module2\') logger_mod2.error(\'Error message from module2\') logger_mod2.critical(\'Critical message from module2\') if __name__ == \'__main__\': setup_logging_system() log_demo()"},{"question":"# Question: Parallel File Processing Using `concurrent.futures` You are given a directory containing multiple text files. Each text file contains a list of integers, one integer per line. You need to write a Python function that processes these files in parallel to compute the sum of integers in each file. Your task is to implement this using the `concurrent.futures` module to maximize efficiency. Function Signature ```python def parallel_sum(directory: str) -> dict: pass ``` Parameters - `directory` (str): The path to the directory containing the text files. Returns - `dict`: A dictionary where the keys are the filenames and the values are the sums of the integers in those files. Requirements 1. Use `concurrent.futures.ThreadPoolExecutor` to manage a pool of threads. 2. Each file should be read and summed in a separate thread. 3. Handle exceptions gracefully. If a file cannot be read or processed, the function should handle the exception and return a sum of 0 for that file without terminating the program. 4. The function should be efficient and capable of handling a large number of files. Example ```python # Given directory structure: # dir/ # file1.txt (contains: 1n2n3) # file2.txt (contains: 4n5n) # file3.txt (contains: -1n-2n-3n-4) sums = parallel_sum(\'dir\') print(sums) # Expected Output: {\'file1.txt\': 6, \'file2.txt\': 9, \'file3.txt\': -10} ``` Constraints - You can assume that the directory paths and file names are valid strings and within the OS limits. - File contents are always valid integers (one per line). - The number of files is reasonably large, so performance considerations should be taken into account. --- Use the `concurrent.futures` module and demonstrate good practices in parallel processing, error handling, and resource management in your solution.","solution":"import os import concurrent.futures def sum_integers_in_file(file_path): Sums the integers in a given file. total = 0 try: with open(file_path, \'r\') as file: for line in file: total += int(line.strip()) except Exception as e: print(f\\"Error processing {file_path}: {e}\\") total = 0 return os.path.basename(file_path), total def parallel_sum(directory: str) -> dict: Processes all text files in the given directory in parallel to compute the sum of integers in each file. if not os.path.exists(directory): raise ValueError(\\"The provided directory does not exist\\") result = {} file_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\'.txt\')] with concurrent.futures.ThreadPoolExecutor() as executor: future_to_file = {executor.submit(sum_integers_in_file, file_path): file_path for file_path in file_paths} for future in concurrent.futures.as_completed(future_to_file): file, total = future.result() result[file] = total return result"},{"question":"# Pandas Advanced Usage and Gotchas Objective: Demonstrate a comprehensive understanding of pandas by implementing functionality to analyze memory usage and safely modify DataFrames through User Defined Functions (UDFs). Problem Statement: You are given a large dataset in the form of a pandas DataFrame with mixed data types, including numeric, categorical, datetime, and boolean columns. Your task is twofold: 1. **Memory Usage Analysis:** - Implement a function `analyze_memory_usage(df)` that takes a pandas DataFrame `df`. - This function should return a detailed report including: - Memory usage of each column (in bytes). - Total memory usage of the DataFrame. - Optionally, deeper memory usage analysis using `memory_usage=\'deep\'`. 2. **DataFrame Modification with UDFs:** - Implement a function `safe_modify_with_udf(df, udf)`: - This function should take a pandas DataFrame `df` and a user-defined function `udf` that intends to modify the DataFrame rows. - Ensure that the UDF does not inadvertently mutate the DataFrame being iterated over. The UDF may, for example, perform operations like removing or updating row entries. - The function must return the modified DataFrame while guaranteeing no mutation issues. Input: - `df`: A pandas DataFrame with mixed data types. - `udf`: A user-defined function intended to apply row-wise modifications to the DataFrame. Output: - For `analyze_memory_usage(df)`: - A dictionary containing: - `column_memory_usage`: A Series with column names as the index and memory usage in bytes as values. - `total_memory_usage`: Total memory usage of the DataFrame. - `deep_memory_usage` (optional): Total memory usage when `memory_usage=\'deep\'`. - For `safe_modify_with_udf(df, udf)`: - A modified DataFrame after applying the UDF row-wise without direct mutation. Constraints: - The DataFrame can have up to 10^6 rows and 100 columns. - Ensure that memory usage calculation is efficient and does not significantly increase computational overhead. - The user-defined function `udf` should be capable of handling various operations like row deletions or updates. Examples: ```python import pandas as pd import numpy as np # Sample DataFrame data = { \'int_col\': np.random.randint(0, 100, size=1000), \'float_col\': np.random.rand(1000), \'datetime_col\': pd.date_range(\'2021-01-01\', periods=1000, freq=\'H\'), \'bool_col\': np.random.choice([True, False], size=1000), \'object_col\': [\'foo\']*250 + [\'bar\']*250 + [\'baz\']*250 + [\'qux\']*250, } df = pd.DataFrame(data) df[\'categorical_col\'] = df[\'object_col\'].astype(\'category\') # Memory Usage Analysis Function def analyze_memory_usage(df): memory_report = { \'column_memory_usage\': df.memory_usage(), \'total_memory_usage\': df.memory_usage().sum(), } if \'deep\' in df.memory_usage.__code__.co_varnames: memory_report[\'deep_memory_usage\'] = df.memory_usage(deep=True).sum() return memory_report # Safe Modify with UDF Function def safe_modify_with_udf(df, udf): df_copy = df.copy() df_copy = df_copy.apply(lambda row: udf(row), axis=1) return df_copy # User Defined Function Example def udf_example(row): if row[\'int_col\'] % 2 == 0: row[\'object_col\'] = \'even\' else: row[\'object_col\'] = \'odd\' return row # Usage memory_report = analyze_memory_usage(df) print(memory_report) modified_df = safe_modify_with_udf(df, udf_example) print(modified_df.head()) ``` **Note:** Ensure efficiency and correctness of the functions while adhering to pandas\' best practices and avoiding common gotchas.","solution":"import pandas as pd def analyze_memory_usage(df): Analyze memory usage of the DataFrame. Parameters: df (pd.DataFrame): The input DataFrame. Returns: dict: A dictionary containing the memory usage report. column_memory_usage = df.memory_usage() total_memory_usage = column_memory_usage.sum() memory_report = { \'column_memory_usage\': column_memory_usage, \'total_memory_usage\': total_memory_usage } # Check if deep memory usage is available if \'deep\' in df.memory_usage.__code__.co_varnames: deep_memory_usage = df.memory_usage(deep=True).sum() memory_report[\'deep_memory_usage\'] = deep_memory_usage return memory_report def safe_modify_with_udf(df, udf): Safely modify DataFrame using a user defined function (UDF). Parameters: df (pd.DataFrame): The input DataFrame. udf (function): The user-defined function to apply to each row. Returns: pd.DataFrame: A new DataFrame with the modifications applied. df_copy = df.copy() modified_df = df_copy.apply(lambda row: udf(row), axis=1) return modified_df"},{"question":"Objective This question assesses your ability to use the seaborn library, particularly its `objects` module, for creating sophisticated data visualizations. You will demonstrate your understanding of fundamental plotting concepts and advanced transformations such as jittering, percentile ranges, and shifting. Question You are given two datasets, `penguins` and `diamonds`, loaded via seaborn\'s `load_dataset` function. Your task is to create plots to visualize certain aspects of these datasets using seaborn\'s `objects` module. 1. **Penguin Dataset Task**: Create a plot that shows the distribution of `body_mass_g` for each `species`. Layer the Dots plot with jitter and complemented with a range showing the 25th and 75th percentiles with a positive x-axis shift of 0.2 units. 2. **Diamonds Dataset Task**: Create a plot to show the distribution of `carat` across different `clarity` levels. Layer the Dots plot with jitter and a range showing the 25th and 75th percentiles with a positive y-axis shift of 0.25 units. Input - There are no direct inputs to your function, as the datasets are directly loaded using seaborn. Output The function should generate two separate plots as specified. Constraints - You must use the seaborn library\'s `objects` module (`import seaborn.objects as so`). Example For the Penguin Dataset Task: ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") # Create the desired plot ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) ``` For the Diamonds Dataset Task: ```python import seaborn.objects as so from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") # Create the desired plot ( so.Plot(diamonds, \\"carat\\", \\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) ``` Your submission should include the functions `plot_penguins_distribution` and `plot_diamonds_distribution` that when called, generate the respective plots. Note - Ensure that the jitter and shift transformations are correctly implemented to make the visual plots more readable and insightful. - You do not need to display the plots within the functions. It is sufficient to ensure that the plots are created correctly.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguins_distribution(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the desired plot plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) # Return the plot object return plot def plot_diamonds_distribution(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the desired plot plot = ( so.Plot(diamonds, \\"carat\\", \\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) # Return the plot object return plot"},{"question":"# Question: Implement an Email Retrieval and Management Script using `poplib` You are required to implement a Python function `manage_pop3_mailbox` that connects to a given POP3 or POP3-SSL server, authenticates using the provided credentials, retrieves the list of email messages, prints out their subject lines, allows the user to select a message to retrieve and display its full content, and optionally delete the message from the server. Your function should take the following parameters: - `host`: The POP3 server address (string). - `port`: The port number of the POP3 server (int). Use 110 for POP3 and 995 for POP3-SSL. - `use_ssl`: A boolean indicating whether to use SSL (True) or not (False). - `username`: The username for authenticating to the POP3 server (string). - `password`: The password for authenticating to the POP3 server (string). - `delete_after_retrieval`: A boolean indicating if the message should be deleted from the server after retrieval (True) or not (False). Expected behavior: 1. Connect to the specified POP3 server with or without SSL based on `use_ssl`. 2. Authenticate using `username` and `password`. 3. Retrieve the list of messages and print their subject lines. 4. Prompt the user to select a message by its index and display the entire content of the selected message. 5. If `delete_after_retrieval` is True, delete the retrieved message from the server. 6. Handle any errors or exceptions that occur during the process and provide meaningful error messages. Below is the function signature: ```python def manage_pop3_mailbox(host: str, port: int, use_ssl: bool, username: str, password: str, delete_after_retrieval: bool): pass ``` # Example Usage ```python # Assuming there are email messages in the server manage_pop3_mailbox(\'pop.example.com\', 995, True, \'user@example.com\', \'userpassword\', False) ``` **Constraints and Notes:** - You must provide some form of exception handling for network errors, authentication errors, and any other potential issues. - Ensure that the connection to the server is properly closed in all cases. - If the selected message\'s subject line or content contains non-ASCII characters, ensure they are properly decoded and displayed. - Use Python\'s built-in `poplib` module as documented above. # Performance Requirements - The function should efficiently handle the retrieval and display of email messages even if the mailbox contains a large number of messages. - Properly manage timeouts for network operations, ensuring the function does not hang indefinitely.","solution":"import poplib from email.parser import Parser from email.header import decode_header from email.utils import parseaddr def manage_pop3_mailbox(host: str, port: int, use_ssl: bool, username: str, password: str, delete_after_retrieval: bool): try: # Connect to the POP3 server if use_ssl: server = poplib.POP3_SSL(host, port) else: server = poplib.POP3(host, port) # Authenticate server.user(username) server.pass_(password) # Get the number of messages num_messages = len(server.list()[1]) # Retrieve and print the subject lines for i in range(num_messages): response, lines, octets = server.top(i + 1, 0) msg = Parser().parsestr(\'n\'.join(map(lambda x: x.decode(\'utf-8\', \'ignore\'), lines))) subject, encoding = decode_header(msg[\'subject\'])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \'utf-8\') print(f\\"{i+1}: {subject}\\") # Prompt the user to select a message to retrieve message_index = int(input(\\"Enter the message number to retrieve: \\")) - 1 # Retrieve the selected message response, lines, octets = server.retr(message_index + 1) msg = Parser().parsestr(\'n\'.join(map(lambda x: x.decode(\'utf-8\', \'ignore\'), lines))) # Display the full content print(msg) # Optionally delete the message if delete_after_retrieval: server.dele(message_index + 1) except (poplib.error_proto, Exception) as e: print(f\\"An error occurred: {e}\\") finally: server.quit()"},{"question":"You are provided with a dataset `tips` that contains information about restaurant tips. Your task is to visualize this dataset using Seaborn\'s new objects interface. Dataset description: - `total_bill`: Total bill amount. - `tip`: Tip amount. - `sex`: Gender of the person paying. - `smoker`: Whether the person is a smoker. - `day`: Day of the week. - `time`: Time of day (Lunch/Dinner). - `size`: Size of the party. # Task: 1. Load the `tips` dataset using `seaborn.load_dataset(\\"tips\\")`. 2. Create a layered plot using Seaborn\'s objects interface: - Plot the distribution of `total_bill` for different `day`s, adding a bar plot (`so.Bar()`) with histogram statistics (`so.Hist()`). - Add a jittering effect (`so.Jitter()`) on the `total_bill` distribution for a better visualization of `sex` to each bar. - Draw a regression line showing the overall trend between `total_bill` and `tip` using polynomial fit (`so.PolyFit()`) along with the scatter plot of dots (`so.Dot()`). 3. Use facets to create subplots for each `time` of day. 4. Label the y-axis as \\"Total Bill and Tips\\" and include a legend annotating each layer. Input: No user input is required. All values must be hard-coded as specified above. Output: Your function should display the combined plot. Constraints: - Use the seaborn objects interface (`so.Plot`) to achieve the visualization. - The plot must include the necessary annotations for clarity. - Ensure the jittering effect is visible in the plot. Example: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Design the plot ( so.Plot(tips, y=\\"total_bill\\", color=\\"sex\\") .facet(col=\\"time\\") .add(so.Bar(), so.Hist()) .add(so.Dot(), so.Jitter(.4)) .add(so.Line(color=\\".3\\", linewidth=2), so.PolyFit(), y=\\"tip\\") .label(y=\\"Total Bill and Tip\\") ) ```","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_tips(): Visualizes the tips dataset using Seaborn\'s objects interface. # Load the dataset tips = load_dataset(\\"tips\\") # Design the plot plot = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\") .facet(col=\\"time\\") .add(so.Bar(), so.Hist()) .add(so.Dot(), so.Jitter(.4)) .add(so.Line(color=\\".3\\", linewidth=2), so.PolyFit(), y=\\"tip\\") .label(y=\\"Total Bill and Tips\\") ) plot.show()"},{"question":"You are tasked with creating a visual report using the Seaborn library. The dataset contains information on monthly average temperatures for different cities. Dataset Information The dataset `avg_temps` is in the form of a dictionary with the following structure: ```python avg_temps = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"City1\\": [30, 32, 45, 60, 70, 75, 80, 79, 70, 60, 48, 35], \\"City2\\": [28, 30, 42, 55, 65, 72, 78, 77, 68, 58, 46, 33], \\"City3\\": [25, 28, 39, 50, 62, 68, 74, 73, 65, 55, 42, 30] } ``` Write a Python function `create_temp_plots(avg_temps: dict) -> None` that performs the following tasks: 1. Convert the dictionary into a pandas DataFrame. 2. Set the plotting context to \\"talk\\". 3. Create a line plot for each cityâ€™s monthly temperatures, all in one figure. Customize the plot with: - Line widths set to 2.5. - Font scaling set to 1.2. 4. Add appropriate labels and title to the plot. 5. Save the plot as `city_temp_plot.png`. Function Signature ```python def create_temp_plots(avg_temps: dict) -> None: pass ``` Example Usage ```python avg_temps = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"City1\\": [30, 32, 45, 60, 70, 75, 80, 79, 70, 60, 48, 35], \\"City2\\": [28, 30, 42, 55, 65, 72, 78, 77, 68, 58, 46, 33], \\"City3\\": [25, 28, 39, 50, 62, 68, 74, 73, 65, 55, 42, 30] } create_temp_plots(avg_temps) ``` Constraints - You must use the seaborn package. - Ensure that the plot is readable and well-labeled. Expected Output - The function should create and save a plot named `city_temp_plot.png` in the current working directory.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_temp_plots(avg_temps: dict) -> None: # Convert the dictionary into a pandas DataFrame df = pd.DataFrame(avg_temps) # Set the plotting context to \\"talk\\" with font_scale 1.2 sns.set_context(\\"talk\\", font_scale=1.2) # Create a line plot for each city\'s monthly temperatures plt.figure(figsize=(12, 8)) sns.lineplot(data=df, x=\'Month\', y=\'City1\', label=\'City1\', linewidth=2.5) sns.lineplot(data=df, x=\'Month\', y=\'City2\', label=\'City2\', linewidth=2.5) sns.lineplot(data=df, x=\'Month\', y=\'City3\', label=\'City3\', linewidth=2.5) # Add appropriate labels and title to the plot plt.xlabel(\\"Month\\") plt.ylabel(\\"Average Temperature (Â°F)\\") plt.title(\\"Monthly Average Temperatures for Cities\\") plt.legend(title=\\"City\\") # Save the plot plt.savefig(\\"city_temp_plot.png\\")"},{"question":"**Title: Analyzing Sales Data with pandas** **Background:** You are provided with sales data for an e-commerce company, stored in a CSV file named `sales_data.csv`. The sales data includes the following columns: 1. `Order ID` - The unique identifier for each order. 2. `Product` - The name of the product sold. 3. `Quantity Ordered` - The number of units ordered per product. 4. `Price Each` - The price of a single unit of product. 5. `Order Date` - The date when the order was placed. 6. `Purchase Address` - The address where the product was delivered. **Objective:** You are required to perform various data analysis tasks using pandas to gain insights into the sales data. Implement your solution in Python and use pandas to read the CSV file and perform the following tasks: 1. **Data Preparation:** - Read the sales data from the CSV file into a pandas DataFrame. - Convert the columns `Order Date` to datetime format and `Quantity Ordered` and `Price Each` to appropriate numeric formats. - Handle any missing data by removing rows with NaN values. 2. **Sales Analysis:** - **Total Sales by Month:** Calculate the total sales for each month by summing up the product of `Quantity Ordered` and `Price Each`. - **Best Selling Product:** Identify the product with the highest sales (total quantity ordered). - **Sales by City:** Extract the city from the `Purchase Address` column and calculate the total sales per city. 3. **Advanced Analysis:** - **Pivot Table of Monthly Sales by Product:** Create a pivot table that shows the total sales for each product across each month. - **Customer Shopping Patterns:** For each unique customer (assume `Purchase Address` is unique to each customer), calculate the average number of products ordered per transaction. **Function Signature:** ```python import pandas as pd def analyze_sales_data(file_path: str): # Step 1: Data Preparation df = pd.read_csv(file_path) # Convert data types df[\'Order Date\'] = pd.to_datetime(df[\'Order Date\'], errors=\'coerce\') df[\'Quantity Ordered\'] = pd.to_numeric(df[\'Quantity Ordered\'], errors=\'coerce\') df[\'Price Each\'] = pd.to_numeric(df[\'Price Each\'], errors=\'coerce\') # Handle missing data df.dropna(inplace=True) # Step 2: Sales Analysis # Add a \'Month\' column for analysis df[\'Month\'] = df[\'Order Date\'].dt.month df[\'Sales\'] = df[\'Quantity Ordered\'] * df[\'Price Each\'] # Total sales by month total_sales_by_month = df.groupby(\'Month\').sum()[\'Sales\'] # Best selling product best_selling_product = df.groupby(\'Product\').sum()[\'Quantity Ordered\'].idxmax() # Sales by city df[\'City\'] = df[\'Purchase Address\'].apply(lambda x: x.split(\',\')[1].strip() if pd.notna(x) else \'Unknown\') sales_by_city = df.groupby(\'City\').sum()[\'Sales\'] # Step 3: Advanced Analysis # Pivot Table of Monthly Sales by Product pivot_sales_by_product = df.pivot_table(index=\'Product\', columns=\'Month\', values=\'Sales\', aggfunc=\'sum\') # Customer shopping patterns (average number of products ordered per transaction per customer) avg_products_per_customer = df.groupby(\'Purchase Address\').mean()[\'Quantity Ordered\'] # Print or return the analysis results print(\\"Total Sales by Month:\\", total_sales_by_month) print(\\"Best Selling Product:\\", best_selling_product) print(\\"Sales by City:\\", sales_by_city) print(\\"Pivot Table of Monthly Sales by Product:n\\", pivot_sales_by_product) print(\\"Average Number of Products Ordered per Transaction per Customer:n\\", avg_products_per_customer) # Example usage: # analyze_sales_data(\'sales_data.csv\') ``` **Constraints:** - The `Quantity Ordered` and `Price Each` columns should contain numeric data; handle non-numeric entries appropriately. - The sales data can have missing values which need to be managed. - Ensure that the code handles cases where data might be missing for certain months or cities. **Expected Output:** The function should correctly compute and print the results of the analysis as specified above when given a valid CSV file path to the sales data.","solution":"import pandas as pd def analyze_sales_data(file_path: str): # Step 1: Data Preparation df = pd.read_csv(file_path) # Convert data types df[\'Order Date\'] = pd.to_datetime(df[\'Order Date\'], errors=\'coerce\') df[\'Quantity Ordered\'] = pd.to_numeric(df[\'Quantity Ordered\'], errors=\'coerce\') df[\'Price Each\'] = pd.to_numeric(df[\'Price Each\'], errors=\'coerce\') # Handle missing data by removing rows with NaN values df.dropna(inplace=True) # Step 2: Sales Analysis # Add a \'Month\' column for analysis df[\'Month\'] = df[\'Order Date\'].dt.month df[\'Sales\'] = df[\'Quantity Ordered\'] * df[\'Price Each\'] # Total sales by month total_sales_by_month = df.groupby(\'Month\').sum()[\'Sales\'] # Best selling product best_selling_product = df.groupby(\'Product\').sum()[\'Quantity Ordered\'].idxmax() # Extract city from \'Purchase Address\' df[\'City\'] = df[\'Purchase Address\'].apply(lambda x: x.split(\',\')[1].strip() if pd.notna(x) else \'Unknown\') # Sales by city sales_by_city = df.groupby(\'City\').sum()[\'Sales\'] # Step 3: Advanced Analysis # Pivot Table of Monthly Sales by Product pivot_sales_by_product = df.pivot_table(index=\'Product\', columns=\'Month\', values=\'Sales\', aggfunc=\'sum\') # Customer shopping patterns (average number of products ordered per transaction per customer) avg_products_per_customer = df.groupby(\'Purchase Address\').mean()[\'Quantity Ordered\'] # Print the analysis results print(\\"Total Sales by Month:n\\", total_sales_by_month) print(\\"Best Selling Product:\\", best_selling_product) print(\\"Sales by City:n\\", sales_by_city) print(\\"Pivot Table of Monthly Sales by Product:n\\", pivot_sales_by_product) print(\\"Average Number of Products Ordered per Transaction per Customer:n\\", avg_products_per_customer) # Example usage: # analyze_sales_data(\'sales_data.csv\')"},{"question":"Advanced Enum Usage **Objective**: To assess the understanding and implementation of Python Enums, including defining enums with unique values, using the functional API, and customizing enum behaviors with methods and special functionalities. Question: You are working on a project that tracks the status and priority of various tasks. You need to create an enumeration to represent the possible states and priorities of a task. 1. **TaskState Enum**: - Define an enum `TaskState` using the class syntax. - It should have the following states with associated integer values: - TODO = 1 - IN_PROGRESS = 2 - COMPLETED = 3 - Ensure all state values are unique. 2. **TaskPriority Enum**: - Define an enum `TaskPriority` using the functional API. - It should have the following priorities with default integer values starting at 1: - LOW - MEDIUM - HIGH 3. **Customizing Enums**: - Add a method `description` to both enums (`TaskState` and `TaskPriority`) that returns a human-readable string describing the state or priority. - For `TaskState`, return the following descriptions: - TODO: \\"Task needs to be done\\" - IN_PROGRESS: \\"Task is currently in progress\\" - COMPLETED: \\"Task has been completed\\" - For `TaskPriority`, return: - LOW: \\"Low priority task\\" - MEDIUM: \\"Medium priority task\\" - HIGH: \\"High priority task\\" 4. **Implementation**: - Implement the `TaskState` and `TaskPriority` enums with the specified requirements. - Write a test function `test_enums()` that demonstrates: - Creating enum members. - Accessing their values and names. - Printing their descriptions using the `description` method. - Ensuring the unique constraint for `TaskState`. **Input**: No input from users is required. The enums and their methods should be implemented directly. **Output**: The `test_enums()` function should print the states, priorities, their values, names, and descriptions as specified. Constraints: - Use enums from the `enum` module. - Ensure unique values for `TaskState`. - Create `TaskPriority` using the functional API. Example: ```python from enum import Enum, unique, auto # Define TaskState Enum with unique values and description method @unique class TaskState(Enum): TODO = 1 IN_PROGRESS = 2 COMPLETED = 3 def description(self): descriptions = { TaskState.TODO: \\"Task needs to be done\\", TaskState.IN_PROGRESS: \\"Task is currently in progress\\", TaskState.COMPLETED: \\"Task has been completed\\" } return descriptions[self] # Define TaskPriority Enum using the functional API with auto values and description method TaskPriority = Enum(\'TaskPriority\', \'LOW MEDIUM HIGH\', start=1) # Add description method to TaskPriority Enum def task_priority_description(self): descriptions = { TaskPriority.LOW: \\"Low priority task\\", TaskPriority.MEDIUM: \\"Medium priority task\\", TaskPriority.HIGH: \\"High priority task\\" } return descriptions[self] # Bind the description method to TaskPriority Enum TaskPriority.description = task_priority_description def test_enums(): # Create and test TaskState Enum print(f\\"TaskState TODO: {TaskState.TODO}, value: {TaskState.TODO.value}, name: {TaskState.TODO.name}, description: {TaskState.TODO.description()}\\") print(f\\"TaskState IN_PROGRESS: {TaskState.IN_PROGRESS}, value: {TaskState.IN_PROGRESS.value}, name: {TaskState.IN_PROGRESS.name}, description: {TaskState.IN_PROGRESS.description()}\\") print(f\\"TaskState COMPLETED: {TaskState.COMPLETED}, value: {TaskState.COMPLETED.value}, name: {TaskState.COMPLETED.name}, description: {TaskState.COMPLETED.description()}\\") # Create and test TaskPriority Enum print(f\\"TaskPriority LOW: {TaskPriority.LOW}, value: {TaskPriority.LOW.value}, name: {TaskPriority.LOW.name}, description: {TaskPriority.LOW.description()}\\") print(f\\"TaskPriority MEDIUM: {TaskPriority.MEDIUM}, value: {TaskPriority.MEDIUM.value}, name: {TaskPriority.MEDIUM.name}, description: {TaskPriority.MEDIUM.description()}\\") print(f\\"TaskPriority HIGH: {TaskPriority.HIGH}, value: {TaskPriority.HIGH.value}, name: {TaskPriority.HIGH.name}, description: {TaskPriority.HIGH.description()}\\") # Run the test function test_enums() ``` The output should list all the states and priorities with their associated values, names, and descriptions. **Note**: Ensure you understand the behavior of enums, the use of the `@unique` decorator, and the functional API while solving this question.","solution":"from enum import Enum, unique, auto # Define TaskState Enum with unique values and description method @unique class TaskState(Enum): TODO = 1 IN_PROGRESS = 2 COMPLETED = 3 def description(self): descriptions = { TaskState.TODO: \\"Task needs to be done\\", TaskState.IN_PROGRESS: \\"Task is currently in progress\\", TaskState.COMPLETED: \\"Task has been completed\\" } return descriptions[self] # Define TaskPriority Enum using the functional API with auto values and description method TaskPriority = Enum(\'TaskPriority\', {\'LOW\': 1, \'MEDIUM\': 2, \'HIGH\': 3}) # Add description method to TaskPriority Enum def task_priority_description(self): descriptions = { TaskPriority.LOW: \\"Low priority task\\", TaskPriority.MEDIUM: \\"Medium priority task\\", TaskPriority.HIGH: \\"High priority task\\" } return descriptions[self] # Bind the description method to TaskPriority Enum TaskPriority.description = task_priority_description # Function to demonstrate the use of enums def test_enums(): # Create and test TaskState Enum print(f\\"TaskState TODO: {TaskState.TODO}, value: {TaskState.TODO.value}, name: {TaskState.TODO.name}, description: {TaskState.TODO.description()}\\") print(f\\"TaskState IN_PROGRESS: {TaskState.IN_PROGRESS}, value: {TaskState.IN_PROGRESS.value}, name: {TaskState.IN_PROGRESS.name}, description: {TaskState.IN_PROGRESS.description()}\\") print(f\\"TaskState COMPLETED: {TaskState.COMPLETED}, value: {TaskState.COMPLETED.value}, name: {TaskState.COMPLETED.name}, description: {TaskState.COMPLETED.description()}\\") # Create and test TaskPriority Enum print(f\\"TaskPriority LOW: {TaskPriority.LOW}, value: {TaskPriority.LOW.value}, name: {TaskPriority.LOW.name}, description: {TaskPriority.LOW.description()}\\") print(f\\"TaskPriority MEDIUM: {TaskPriority.MEDIUM}, value: {TaskPriority.MEDIUM.value}, name: {TaskPriority.MEDIUM.name}, description: {TaskPriority.MEDIUM.description()}\\") print(f\\"TaskPriority HIGH: {TaskPriority.HIGH}, value: {TaskPriority.HIGH.value}, name: {TaskPriority.HIGH.name}, description: {TaskPriority.HIGH.description()}\\") # Run the test function test_enums()"},{"question":"Objective You are provided with the `fmri` dataset from the `seaborn` library. Your task is to create a custom visualization for analyzing the `signal` over time, across different regions and events. The visualization should include separate lines for each combination of `region` and `event`, error bands representing the variability in the signal, and markers indicating the sampled data points. Detailed Instructions 1. **Data Preparation**: Filter the `fmri` dataset to include only rows where `event` is either \'stim\' or \'cue\'. 2. **Plotting**: - Create a line plot of `signal` versus `timepoint`. - Use different colors to represent different `regions`. - Use different linestyles to differentiate between `events`. - Add error bands to your plot that show the standard error of the mean `signal` for each `timepoint`. - Include markers on the lines to show the data points. 3. **Customization**: Ensure that the plot is well-labeled and includes a legend to differentiate between `regions` and `events`. Function Signature ```python def plot_fmri_signal(): import seaborn.objects as so from seaborn import load_dataset # Filter the dataset fmri = load_dataset(\\"fmri\\") data = fmri.query(\\"event in [\'stim\', \'cue\']\\") # Create the plot p = so.Plot(data, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") p.add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg()) p.add(so.Band(), so.Est(), group=\\"event\\") # Customize and display p.show() ``` Expected Output Calling `plot_fmri_signal()` should produce a plot that: - Has line plots for each combination of `region` and `event`. - Displays error bands for each line. - Includes markers at the data points. - Clearly shows different regions in different colors and events in different linestyles. - Includes a legend and appropriate axis labels. Constraints - Follow the structure outlined in the provided function skeleton as closely as possible. - Use the seaborn `objects` interface methods as demonstrated in the documentation. Performance Requirements The function should execute efficiently without unnecessary computations, making use of seaborn\'s internal optimizations to handle the data grouping and transformations.","solution":"def plot_fmri_signal(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset fmri = sns.load_dataset(\\"fmri\\") # Filter the dataset data = fmri.query(\\"event in [\'stim\', \'cue\']\\") # Create the plot plt.figure(figsize=(10, 6)) sns.lineplot(data=data, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", markers=True, err_style=\\"band\\") # Customize the plot plt.title(\\"fMRI Signal Over Time for Different Regions and Events\\") plt.xlabel(\\"Timepoint\\") plt.ylabel(\\"Signal\\") plt.legend(title=\\"Region and Event\\", bbox_to_anchor=(1.05, 1), loc=2) plt.tight_layout() # Show the plot plt.show()"},{"question":"Objective: Write a Python function that utilizes the `grp` module to gather information about Unix groups and process this information according to specified criteria. Problem Statement: You are tasked with writing a function called `get_groups_with_member` that takes a user\'s name as input and returns a dictionary where: - The keys are the names of the groups (as strings) that the user is a member of. - The values are the numerical group IDs (as integers) for those groups. Your function should handle situations where the provided user name does not exist in any group by returning an empty dictionary. Function Signature: ```python def get_groups_with_member(user_name: str) -> dict: pass ``` Expected Input and Output: - Input: `user_name` (a string representing the user name) - Output: A dictionary with group names as keys and corresponding group IDs as values. Example: ```python # Suppose the following groups are present in the group database: # Group 1: Name=\\"admin\\", GID=1000, Members=[\\"alice\\", \\"bob\\"] # Group 2: Name=\\"staff\\", GID=1001, Members=[\\"bob\\"] # Group 3: Name=\\"guest\\", GID=1002, Members=[\\"charlie\\"] # # Then the function would be used as follows: >>> get_groups_with_member(\\"bob\\") {\'admin\': 1000, \'staff\': 1001} >>> get_groups_with_member(\\"alice\\") {\'admin\': 1000} >>> get_groups_with_member(\\"charlie\\") {\'guest\': 1002} >>> get_groups_with_member(\\"unknown_user\\") {} ``` Constraints: - You may assume that the `grp` module is available and that you are working on a Unix-based system. - Your function should handle errors gracefully and return an empty dictionary if no entries are found for the given user name. Considerations: - Ensure that your function is efficient and performs the necessary operations in a reasonable time frame. - Provide appropriate error handling for scenarios where the group database might not be accessible or other unexpected issues occur. This question evaluates the student\'s ability to work with system-level databases in Python, handle potential errors, and process data to produce meaningful results.","solution":"import grp def get_groups_with_member(user_name: str) -> dict: Returns a dictionary with group names as keys and corresponding group IDs as values for groups where the specified user is a member. result = {} try: all_groups = grp.getgrall() for group in all_groups: if user_name in group.gr_mem: result[group.gr_name] = group.gr_gid except Exception as e: result = {} return result"},{"question":"**Objective:** Implement a function `event_duration_statistics` that calculates and formats the duration and statistics of multiple events tracked in a high-resolution manner. **Function Signature:** ```python def event_duration_statistics(events): Calculate the duration and statistics (max, min, average, total) of tracked events. Args: - events (list): A list of tuples where each tuple contains the event name (str) and start and end times in nanoseconds (int), e.g., (\'event1\', start_ns, end_ns). Returns: - A dictionary containing: - \'durations\': A list of formatted durations for each event. - \'max_duration\': The maximum event duration in seconds. - \'min_duration\': The minimum event duration in seconds. - \'avg_duration\': The average event duration in seconds. - \'total_duration\': The total duration of all events in seconds. pass ``` **Description:** 1. **Input:** - `events`: A list of tuples. Each tuple consists of an event name (string), start time in nanoseconds (integer), and end time in nanoseconds (integer). 2. **Output:** - A dictionary with the following keys: - `\'durations\'`: A list of strings showing the duration of each event formatted as \\"X hours, Y minutes, Z seconds\\". - `\'max_duration\'`: The maximum event duration in seconds (float). - `\'min_duration\'`: The minimum event duration in seconds (float). - `\'avg_duration\'`: The average event duration in seconds (float). - `\'total_duration\'`: The total duration of all events in seconds (float). **Constraints:** - The function should handle any number of events provided in the input list. - The start and end times are expressed in nanoseconds since the epoch. **Example:** ```python events = [ (\'Event1\', 1000000000, 2000000000), # 1 second (\'Event2\', 3000000000, 6000000000), # 3 seconds (\'Event3\', 7000000000, 9000000000), # 2 seconds ] result = event_duration_statistics(events) print(result) # Expected Output: # { # \'durations\': [\'0 hours, 0 minutes, 1 seconds\', \'0 hours, 0 minutes, 3 seconds\', \'0 hours, 0 minutes, 2 seconds\'], # \'max_duration\': 3.0, # \'min_duration\': 1.0, # \'avg_duration\': 2.0, # \'total_duration\': 6.0 # } ``` **Notes:** - You should use the `time.perf_counter_ns` function for high-resolution timing if you need to test the accuracy of durations. - Ensure to handle edge cases where the event list is empty or where start time might be equal to or greater than the end time (though it\'s expected they will be valid).","solution":"def event_duration_statistics(events): Calculate the duration and statistics (max, min, average, total) of tracked events. Args: - events (list): A list of tuples where each tuple contains the event name (str) and start and end times in nanoseconds (int), e.g., (\'event1\', start_ns, end_ns). Returns: - A dictionary containing: - \'durations\': A list of formatted durations for each event. - \'max_duration\': The maximum event duration in seconds. - \'min_duration\': The minimum event duration in seconds. - \'avg_duration\': The average event duration in seconds. - \'total_duration\': The total duration of all events in seconds. if not events: return { \'durations\': [], \'max_duration\': 0, \'min_duration\': 0, \'avg_duration\': 0, \'total_duration\': 0 } durations_in_ns = [(end_ns - start_ns) for _, start_ns, end_ns in events] durations_in_sec = [duration / 1e9 for duration in durations_in_ns] total_duration = sum(durations_in_sec) max_duration = max(durations_in_sec) min_duration = min(durations_in_sec) avg_duration = total_duration / len(events) formatted_durations = [] for duration in durations_in_sec: seconds = int(duration) minutes = seconds // 60 hours = minutes // 60 formatted_duration = f\\"{hours} hours, {minutes % 60} minutes, {seconds % 60} seconds\\" formatted_durations.append(formatted_duration) return { \'durations\': formatted_durations, \'max_duration\': max_duration, \'min_duration\': min_duration, \'avg_duration\': avg_duration, \'total_duration\': total_duration }"},{"question":"**Objective:** Demonstrate your understanding of audio data manipulation using the `audioop` module in Python. **Question:** You are provided with a stereo audio recording represented as a bytes-like object. Your task is to implement a function `process_audio` that processes this recording by normalizing the volume of both the left and right channels, and then convert the result to a specified output format (either a-LAW, u-LAW, or linear with a different bit width). The function signature is as follows: ```python def process_audio(input_audio: bytes, input_width: int, output_format: str, output_width: int) -> bytes: Normalize the volume of left and right audio channels and convert to the specified output format. :param input_audio: Input bytes object representing the stereo audio fragment. :param input_width: Integer representing the sample width in bytes of the input audio (1, 2, 3, or 4). :param output_format: String specifying the desired output format (\'alaw\', \'ulaw\', \'linear\'). :param output_width: Integer specifying the sample width of the output audio (1, 2, 3, or 4) - used only for \'linear\' output format. :return: Processed audio fragment as a bytes object. pass ``` **Details:** 1. **Normalization:** - Normalize the volume of the left and right channels so that their RMS values are equal to each other. The RMS value of a channel can be calculated using `audioop.rms`. - Apply a scaling factor to both channels so that the RMS value of the louder channel is reduced to match the RMS value of the quieter channel. Use the `audioop.mul` function to adjust the volume. 2. **Conversion:** - After normalization, convert the processed audio fragment to the specified output format: - For \'alaw\', use `audioop.lin2alaw`. - For \'ulaw\', use `audioop.lin2ulaw`. - For \'linear\', use `audioop.lin2lin` to convert to the specified output_width. 3. **Stereo Handling:** - Use `audioop.tomono` and `audioop.tostereo` to manipulate the individual channels if required. **Constraints:** - Input and output audio are always stereo (i.e., two channels). - All intermediate operations must preserve the stereo nature of the audio. **Example Usage:** ```python # Example input: (for illustration only; the actual bytes object is required) input_audio = b\'x00x01x02x03\' input_width = 2 output_format = \'alaw\' output_width = 1 # Expected output: Processed audio bytes (actual data will depend on implementation) processed_audio = process_audio(input_audio, input_width, output_format, output_width) ``` Your solution should be efficient and make use of the provided `audioop` functions to manipulate and process the audio data effectively.","solution":"import audioop def process_audio(input_audio: bytes, input_width: int, output_format: str, output_width: int) -> bytes: Normalize the volume of left and right audio channels and convert to the specified output format. :param input_audio: Input bytes object representing the stereo audio fragment. :param input_width: Integer representing the sample width in bytes of the input audio (1, 2, 3, or 4). :param output_format: String specifying the desired output format (\'alaw\', \'ulaw\', \'linear\'). :param output_width: Integer specifying the sample width of the output audio (1, 2, 3, or 4) - used only for \'linear\' output format. :return: Processed audio fragment as a bytes object. # Extract left and right channel data left_channel = audioop.tomono(input_audio, input_width, 1, 0) right_channel = audioop.tomono(input_audio, input_width, 0, 1) # Calculate RMS values for both channels rms_left = audioop.rms(left_channel, input_width) rms_right = audioop.rms(right_channel, input_width) # Determine the normalization factor if rms_left > rms_right: factor = rms_right / rms_left left_channel = audioop.mul(left_channel, input_width, factor) else: factor = rms_left / rms_right right_channel = audioop.mul(right_channel, input_width, factor) # Combine the normalized channels back into a stereo audio normalized_audio = audioop.tostereo(left_channel, input_width, 1, 0) right_channel = audioop.tostereo(right_channel, input_width, 0, 1) normalized_audio = audioop.add(normalized_audio, right_channel, input_width) # Convert the processed audio to the desired output format if output_format == \'alaw\': result_audio = audioop.lin2alaw(normalized_audio, input_width) elif output_format == \'ulaw\': result_audio = audioop.lin2ulaw(normalized_audio, input_width) elif output_format == \'linear\': result_audio = audioop.lin2lin(normalized_audio, input_width, output_width) else: raise ValueError(f\\"Unsupported output format: {output_format}\\") return result_audio"},{"question":"# PyTorch Tensor Size Manipulation You are provided with tensors of varying dimensions. Write a function `tensor_operation` that performs the following operations: 1. Takes a list of tensors as input. 2. For each tensor in the list, retrieves its size using `torch.Tensor.size()`. 3. Calculates the total number of elements in the tensor using the sizes obtained. 4. Returns a dictionary where the keys are the string representations of the tensor sizes (e.g., \\"torch.Size([10, 20, 30])\\") and the values are the corresponding total number of elements. Function Signature ```python def tensor_operation(tensors: List[torch.Tensor]) -> Dict[str, int]: pass ``` Input - A list of tensors where each tensor is an instance of `torch.Tensor`. Output - A dictionary where: - Keys are string representations of tensor sizes. - Values are integers representing the total number of elements in the corresponding tensor. Example ```python import torch # Example tensors tensors = [ torch.ones(2, 3), torch.ones(4, 5, 6) ] result = tensor_operation(tensors) # Expected output # { # \\"torch.Size([2, 3])\\": 6, # \\"torch.Size([4, 5, 6])\\": 120 # } print(result) ``` Constraints - The input list will contain at least one tensor and at most 100 tensors. - Each tensor will have at most 5 dimensions. - Each dimension will have at most 100 elements. Additional Information - You should use `torch.Tensor.size()` to retrieve the size of each tensor. - The string representation of `torch.Size` can be obtained directly by converting it to a string, as shown in the example above. Performance Requirements Your solution should iterate through the list of tensors and compute the required outputs efficiently.","solution":"import torch from typing import List, Dict def tensor_operation(tensors: List[torch.Tensor]) -> Dict[str, int]: result = {} for tensor in tensors: size_str = str(tensor.size()) num_elements = tensor.numel() # Total number of elements result[size_str] = num_elements return result"},{"question":"# Buffer Protocol Exercise Objective: Implement a Python function that utilizes the buffer protocol to manipulate and interact with multi-dimensional arrays efficiently. Problem Statement: You are required to write a function `process_buffer` that takes two inputs: 1. A multi-dimensional array-like structure (such as a `numpy` array or a custom object supporting the buffer interface), 2. A function `func` applied element-wise to the array. The `process_buffer` function should: 1. Efficiently access and modify the buffer data. 2. Apply the function `func` to each element in the buffer. 3. Properly handle different memory layouts (C-contiguous, Fortran-contiguous) and multi-dimensional structures. Function Signature: ```python def process_buffer(buffer_src, func): Arguments: buffer_src -- an object exposing the buffer interface (e.g., numpy array). func -- a function to apply to each element within the buffer. Returns: A new object with the same structure as buffer_src, where func has been applied to each element. # Your implementation here ``` Example: ```python import numpy as np def increment(x): return x + 1 array = np.array([[1, 2, 3], [4, 5, 6]]) result = process_buffer(array, increment) print(result) # Expected output: [[2, 3, 4], [5, 6, 7]] ``` Constraints: - You should use the buffer protocol for accessing and manipulating the data. - The `buffer_src` may not necessarily be a numpy array; it could be any object implementing the buffer interface. - Ensure to handle various buffer request types and flags as needed. - Release the buffer appropriately after processing to avoid memory leaks. Notes: - Utilize the `memoryview` object for buffer handling as it provides a Pythonic interface to the buffer protocol. - Consider different memory layouts (C-contiguous, Fortran-contiguous) while implementing the solution. Use helper functions like `PyBuffer_IsContiguous`. - The function should preserve the original structure of `buffer_src` in the returned result. Good luck!","solution":"import numpy as np def process_buffer(buffer_src, func): Arguments: buffer_src -- an object exposing the buffer interface (e.g., numpy array). func -- a function to apply to each element within the buffer. Returns: A new object with the same structure as buffer_src, where func has been applied to each element. # Create a memoryview of the source buffer buffer_view = memoryview(buffer_src) # Create an empty array with the same shape and type as buffer_src to store results result = np.empty_like(buffer_src) # Create a memoryview of the result array for manipulation result_view = memoryview(result) # Iterate over the buffer according to its shape and apply func to each element # Only handle the simplest case for illustrative purposes; more general solutions # would navigate differing strides more carefully it = np.nditer(buffer_view, flags=[\'multi_index\'], order=\'C\') while not it.finished: result_view[it.multi_index] = func(it[0]) it.iternext() return result"},{"question":"Background You are required to demonstrate a solid understanding of Python\'s asyncio module and its limitations on different platforms. Particularly, you need to showcase handling asynchronous tasks keeping in mind the constraints mentioned in the provided documentation. Problem Statement Implement an async function `platform_aware_async_task` which performs the following: 1. Accepts an integer `n` representing the number of parallel tasks to execute. 2. Each task is a coroutine that performs a simple I/O-bound operation such as sleeping for a short period and then returning a result. 3. The function should handle platform-specific limitations: - On Windows: - If the default event loop is `ProactorEventLoop`, subprocess-based tasks should be supported. - If using `SelectorEventLoop`, ensure it can handle up to 512 socket connections. - On Unix-based systems: - Make use of `loop.create_unix_connection()` or `loop.create_unix_server()` where applicable. The function should: - Return a list of results from the executed tasks. - Be robust to changes in the underlying platform. Input and Output Formats - **Input**: An integer `n (1 <= n <= 1000)` - **Output**: A list of `n` integers where each integer is the result of a task. Constraints - Ensure the solution accounts for platform-specific limitations by checking the current platform and event loop. - Manage the execution of tasks efficiently to avoid common pitfalls related to asyncio on different platforms. - Performance must be optimized for concurrency using asyncio. You should make sure to handle exceptions gracefully and ensure that the function is tested on both Windows and Unix-based systems. Example ```python import asyncio async def single_task(index: int) -> int: await asyncio.sleep(1) return index async def platform_aware_async_task(n: int) -> list: # Solution implementation here pass # Example usage asyncio.run(platform_aware_async_task(5)) # Output: [0, 1, 2, 3, 4] after approximately 1 second ``` **Note**: Ensure to create appropriate test cases to validate different platform-specific behavior using the asyncio event loop.","solution":"import asyncio import platform async def single_task(index: int) -> int: await asyncio.sleep(1) return index async def platform_aware_async_task(n: int) -> list: if n < 1 or n > 1000: raise ValueError(\\"n must be between 1 and 1000\\") tasks = [single_task(i) for i in range(n)] current_platform = platform.system() if current_platform == \\"Windows\\": loop = asyncio.get_event_loop() if isinstance(loop, asyncio.ProactorEventLoop): # ProactorEventLoop can handle subprocesses and needs no specific handling for tasks results = await asyncio.gather(*tasks) else: # Handle SelectorEventLoop specific behavior here if needed # For this simplified example, no additional handling is done, # but we could address platform-specific limitations here. results = await asyncio.gather(*tasks) else: # Unix-based systems # Handle Unix specific behavior if needed results = await asyncio.gather(*tasks) return results"},{"question":"**Advanced Coding Assessment: Package Metadata Aggregator** **Objective:** You are tasked with implementing a function that aggregates specific metadata details for a given installed Python package. The function should return a dictionary containing the following information: - The package version. - A list of all entry points grouped by their type. - The full set of file paths installed by the package. - The distribution requirements specified by the package. In order to meet these requirements, you will need to utilize the functionalities provided in the `importlib.metadata` module. **Function Signature:** ```python def package_metadata_aggregator(package_name: str) -> dict: pass ``` **Input:** - `package_name` (str): The name of the installed Python package for which metadata information is needed. It is guaranteed that the package is already installed in the Python environment. **Output:** - A dictionary containing the following keys: - `\\"version\\"` (str): The version of the package. - `\\"entry_points\\"` (dict): A dictionary where keys are entry point groups, and values are lists of entry point names. - `\\"files\\"` (list): A list of file paths (as strings) installed by the package. - `\\"requirements\\"` (list): A list of package requirements as strings. **Constraints:** - You should use the `importlib.metadata` module for all metadata retrieval tasks. - The function should handle potential cases where certain metadata (e.g., entry points or requirements) may not be available and handle them gracefully by returning an empty list or dictionary as appropriate. **Example:** ```python result = package_metadata_aggregator(\\"your-package-name\\") print(result) ``` Expected output format: ```python { \\"version\\": \\"x.x.x\\", \\"entry_points\\": { \\"console_scripts\\": [\\"script1\\", \\"script2\\"], \\"distutils.commands\\": [\\"command1\\", \\"command2\\"], ... }, \\"files\\": [\\"path/to/file1\\", \\"path/to/file2\\", ...], \\"requirements\\": [\\"requirement1\\", \\"requirement2\\", ...] } ``` **Notes:** - Do not install any external packages during the assessment. - You can refer to the documentation of `importlib.metadata` to better understand how to use the package effectively. **Performance Requirement:** - The function should perform efficiently even for packages with a large number of files or entry points. Good luck, and happy coding!","solution":"import importlib.metadata def package_metadata_aggregator(package_name: str) -> dict: Returns a dictionary containing metadata information for a given installed package. :param package_name: The name of the installed Python package. :return: A dictionary with keys `version`, `entry_points`, `files`, and `requirements`. metadata = {} try: dist = importlib.metadata.distribution(package_name) # Get the version metadata[\\"version\\"] = dist.version # Get entry points entry_points = dist.entry_points ep_dict = {} for ep in entry_points: if ep.group not in ep_dict: ep_dict[ep.group] = [] ep_dict[ep.group].append(ep.name) metadata[\\"entry_points\\"] = ep_dict # Get files metadata[\\"files\\"] = [str(file) for file in dist.files] # Get requirements metadata[\\"requirements\\"] = dist.requires or [] except importlib.metadata.PackageNotFoundError: metadata = { \\"version\\": None, \\"entry_points\\": {}, \\"files\\": [], \\"requirements\\": [] } return metadata"},{"question":"Question: Parallel File Processing with Threading # Objective Create a Python function that uses threading to process multiple files concurrently. The function should read the contents of each file, perform a simple transformation on the text (e.g., converting all text to uppercase), and write the transformed text to a new output file. # Detailed Description You are required to write a function `parallel_file_processing(input_files: List[str], output_files: List[str]) -> None` that: 1. Accepts two lists of strings as input parameters: * `input_files`: a list of paths to the input files. * `output_files`: a list of paths to the output files where the transformed text should be saved. 2. The function should perform the following steps: * Read the contents of each input file. * Transform the text by converting all characters to uppercase. * Write the transformed text to the corresponding output file. # Constraints * The `input_files` and `output_files` lists are guaranteed to have the same length. * Each input file path in `input_files` list is guaranteed to have a corresponding output file path in `output_files` list. * The function should utilize threading to perform the file reading, transformation, and writing tasks concurrently. # Example ```python input_files = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] output_files = [\\"out1.txt\\", \\"out2.txt\\", \\"out3.txt\\"] # Content of file1.txt: \\"hello world\\" # Content of file2.txt: \\"python is great\\" # Content of file3.txt: \\"multithreading is awesome\\" parallel_file_processing(input_files, output_files) # Content of out1.txt: \\"HELLO WORLD\\" # Content of out2.txt: \\"PYTHON IS GREAT\\" # Content of out3.txt: \\"MULTITHREADING IS AWESOME\\" ``` # Implementation Notes * You should create a separate thread for each file pair to maximize concurrency. * Use Python\'s `threading` module for creating and managing threads. * Ensure proper synchronization if necessary to avoid any race conditions. * Handle any exceptions that might occur during file operations. # Performance Requirement * The function should efficiently handle the concurrent processing of files and be able to scale with the number of files. # Your Task Implement the function `parallel_file_processing(input_files: List[str], output_files: List[str]) -> None`.","solution":"import threading from typing import List def process_file(input_file: str, output_file: str) -> None: Reads the content of input_file, converts it to uppercase, and writes it to output_file. try: with open(input_file, \'r\') as f_in: content = f_in.read() transformed_content = content.upper() with open(output_file, \'w\') as f_out: f_out.write(transformed_content) except Exception as e: print(f\\"Error processing file {input_file}: {e}\\") def parallel_file_processing(input_files: List[str], output_files: List[str]) -> None: Processes multiple files concurrently by reading from input_files, converting content to uppercase, and writing to output_files. threads = [] for input_file, output_file in zip(input_files, output_files): thread = threading.Thread(target=process_file, args=(input_file, output_file)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"**Coding Assessment Question** # Objective Write a function `uu_encode_decode` that demonstrates your understanding of the `uu` module by encoding a given binary file to uuencode format and then immediately decoding it back to its original binary form. # Function Signature ```python def uu_encode_decode(input_file_path: str, encoded_file_path: str, mode: int = 0o666, backtick: bool = False) -> str: Encodes the input binary file into uuencode format and then decodes it back to its original form. Parameters: input_file_path (str): The path to the binary input file that needs to be uuencoded. encoded_file_path (str): The path to store the uuencoded file. mode (int): The file permission mode for the encoded file (default is 0o666). backtick (bool): If True, zero bytes are represented by \'`\' instead of space (default is False). Returns: str: A message indicating the success of encoding and decoding process. Raises: uu.Error: If an error occurs during encoding or decoding process. ``` # Requirements 1. **Input**: - `input_file_path`: The file path of the binary file to be encoded. - `encoded_file_path`: The path where the uuencoded file will be saved. - `mode`: The file permission mode for the encoded file. Default is `0o666`. - `backtick`: A boolean value to specify if zero bytes are represented by \'`\'. Default is `False`. 2. **Output**: - Return a string message indicating successful encoding and decoding: `\\"Encoding and Decoding process completed successfully.\\"` 3. **Constraints**: - The input file must be a valid binary file. - The path to the encoded file must be a valid writeable location. 4. **Functionality**: - Open and read the input binary file. - Encode the file to uuencode format and write it to the specified `encoded_file_path`. - Decode the uuencoded file back to its original binary form. - Verify the decoded file by comparing it with the original input file. - Handle all possible exceptions raised by the `uu` module and file operations. - If any exception occurs, raise the appropriate `uu.Error`. # Example ```python # Assuming \'input.bin\' is a binary file available in the current directory try: result = uu_encode_decode(\'input.bin\', \'encoded.uu\', mode=0o666, backtick=False) print(result) except uu.Error as e: print(f\\"An error occurred: {e}\\") ``` # Notes - Use the `uu.encode` and `uu.decode` functions from the `uu` module for encoding and decoding. - Ensure to handle cases where the file already exists during the decode process. - The function should work for both large and small binary files.","solution":"import uu import os def uu_encode_decode(input_file_path: str, encoded_file_path: str, mode: int = 0o666, backtick: bool = False) -> str: Encodes the input binary file into uuencode format and then decodes it back to its original form. Parameters: input_file_path (str): The path to the binary input file that needs to be uuencoded. encoded_file_path (str): The path to store the uuencoded file. mode (int): The file permission mode for the encoded file (default is 0o666). backtick (bool): If True, zero bytes are represented by \'`\' instead of space (default is False). Returns: str: A message indicating the success of encoding and decoding process. Raises: uu.Error: If an error occurs during encoding or decoding process. decoded_file_path = input_file_path + \'.decoded\' # Encoding the file try: with open(input_file_path, \'rb\') as input_file, open(encoded_file_path, \'wb\') as encoded_file: uu.encode(input_file, encoded_file, mode=mode, backtick=backtick) except Exception as e: raise uu.Error(f\\"An error occurred during encoding: {e}\\") # Decoding the file back try: with open(encoded_file_path, \'rb\') as encoded_file, open(decoded_file_path, \'wb\') as decoded_file: uu.decode(encoded_file, decoded_file) except Exception as e: raise uu.Error(f\\"An error occurred during decoding: {e}\\") # Compare the original and decoded files try: with open(input_file_path, \'rb\') as original_file, open(decoded_file_path, \'rb\') as decoded_file: if original_file.read() != decoded_file.read(): raise uu.Error(\\"Decoded file content does not match the original file.\\") except Exception as e: raise uu.Error(f\\"An error occurred during file comparison: {e}\\") finally: os.remove(decoded_file_path) return \\"Encoding and Decoding process completed successfully.\\""},{"question":"# Python310 Cell Objects - Coding Assessment Question Objective: Demonstrate the use of Python310 Cell objects to manage and manipulate shared variables across multiple scopes. Problem Statement: Write a Python function `closure_cell_example()` that simulates the behavior of closure using Python310 Cell objects\' functionalities described in the provided documentation. The function should: 1. Define a nested function `incrementer()` that increments a shared integer variable. 2. Use Cell objects to handle the shared variable such that the increment operation effectively modifies the variable across multiple calls to `incrementer()`. 3. Allow retrieval of the current value of the shared variable through another function `current_value()`. Requirements: 1. **Function Definitions:** - `closure_cell_example()` should return two functions `incrementer` and `current_value`. 2. **Shared Variable Initialization:** - Initialize the shared variable with an integer value (e.g., 0) using `PyCell_New()`. 3. **Increment Operation:** - Implement the increment operation in the `incrementer` function using `PyCell_Get()` and `PyCell_Set()`. 4. **Retrieve Current Value:** - Implement the retrieval of the current value in the `current_value` function using `PyCell_Get()`. Example: ```python def closure_cell_example(): # Function implementation... increment, current = closure_cell_example() print(current()) # Output: 0 increment() increment() print(current()) # Output: 2 ``` Constraints: - You can assume that the cell object handling functions `PyCell_New`, `PyCell_Get`, `PyCell_Set` are available in your Python environment. - Error handling for non-cell inputs and NULL values has to be managed appropriately. - Focus on demonstrating a clear understanding of how Cell objects can be used to manage shared variables, mimicking the behavior of closures in Python.","solution":"# As PyCell_New, PyCell_Get, and PyCell_Set are part of the CPython API, we will simulate the functionality in pure Python. def PyCell_New(obj): Simulate the creation of a new cell containing the object \'obj\' class Cell: def __init__(self, value): self.value = value def get(self): return self.value def set(self, value): self.value = value return Cell(obj) def PyCell_Get(cell): Simulate getting the value from a cell return cell.get() def PyCell_Set(cell, obj): Simulate setting the value of a cell cell.set(obj) def closure_cell_example(): shared_cell = PyCell_New(0) # Initialize the shared variable with 0 def incrementer(): current_value = PyCell_Get(shared_cell) PyCell_Set(shared_cell, current_value + 1) def current_value(): return PyCell_Get(shared_cell) return incrementer, current_value"},{"question":"# Question: Persistent Storage Management with the `shelve` Module As a bookstore owner, you want to keep track of your inventory using Python\'s `shelve` module. You are required to write a class `Bookstore`, which will allow you to add, update, retrieve, and delete books persistently. Each book will be represented by a dictionary containing its title, author, genre, and quantity in stock. Implement the following functionalities in the `Bookstore` class: 1. **Initialization**: Initialize the class with a filename for the shelf storage. 2. **Add Book (`add_book`)**: A method to add a new book to the inventory. If the book already exists, update its quantity. 3. **Get Book (`get_book`)**: A method to retrieve details of a book given its title. Raise a `KeyError` if the book does not exist. 4. **Delete Book (`delete_book`)**: A method to delete a book from the inventory given its title. Raise a `KeyError` if the book does not exist. 5. **List Books (`list_books`)**: A method to list all books in the inventory with their details. 6. **Save (`save`)**: Ensure all changes are saved and the shelf is closed properly. **Constraints:** - Use `shelve` with `writeback=True` to simplify the update functionality. - Ensure proper error handling for operations involving access to non-existent books. - Assume concurrent access might occur, handle it appropriately by ensuring only one instance accesses the shelf at a time. # Example Usage: ```python # Initialize the bookstore store = Bookstore(\'books.db\') # Add new books store.add_book({\'title\': \'Book1\', \'author\': \'Author1\', \'genre\': \'Fiction\', \'quantity\': 10}) store.add_book({\'title\': \'Book2\', \'author\': \'Author2\', \'genre\': \'Non-Fiction\', \'quantity\': 5}) # Retrieve details of a book print(store.get_book(\'Book1\')) # Expected output: {\'title\': \'Book1\', \'author\': \'Author1\', \'genre\': \'Fiction\', \'quantity\': 10} # List all books print(store.list_books()) # Expected output: [{\'title\': \'Book1\', \'author\': \'Author1\', \'genre\': \'Fiction\', \'quantity\': 10}, {\'title\': \'Book2\', \'author\': \'Author2\', \'genre\': \'Non-Fiction\', \'quantity\': 5}] # Delete a book store.delete_book(\'Book1\') # Save and close the shelf store.save() ``` # Class Template: ```python import shelve from threading import Lock class Bookstore: def __init__(self, filename): self.filename = filename self.lock = Lock() self.shelf = shelve.open(filename, writeback=True) def add_book(self, book): with self.lock: title = book[\'title\'] if title in self.shelf: self.shelf[title][\'quantity\'] += book[\'quantity\'] else: self.shelf[title] = book def get_book(self, title): with self.lock: if title in self.shelf: return self.shelf[title] else: raise KeyError(f\\"Book \'{title}\' not found.\\") def delete_book(self, title): with self.lock: if title in self.shelf: del self.shelf[title] else: raise KeyError(f\\"Book \'{title}\' not found.\\") def list_books(self): with self.lock: return list(self.shelf.values()) def save(self): with self.lock: self.shelf.sync() self.shelf.close() ``` **Notes**: - Make sure to handle potential concurrency issues with proper locking. - Remember to always close the shelf properly to avoid data corruption.","solution":"import shelve from threading import Lock class Bookstore: def __init__(self, filename): self.filename = filename self.lock = Lock() self.shelf = shelve.open(filename, writeback=True) def add_book(self, book): with self.lock: title = book[\'title\'] if title in self.shelf: self.shelf[title][\'quantity\'] += book[\'quantity\'] else: self.shelf[title] = book def get_book(self, title): with self.lock: if title in self.shelf: return self.shelf[title] else: raise KeyError(f\\"Book \'{title}\' not found.\\") def delete_book(self, title): with self.lock: if title in self.shelf: del self.shelf[title] else: raise KeyError(f\\"Book \'{title}\' not found.\\") def list_books(self): with self.lock: return list(self.shelf.values()) def save(self): with self.lock: self.shelf.sync() self.shelf.close()"},{"question":"You are tasked with implementing a small utility using the `netrc` module to manage `.netrc` files. This utility should be able to: 1. Parse an existing `.netrc` file and display the credentials for a specified host. 2. Add a new entry to the `.netrc` file. 3. Remove an entry from the `.netrc` file. Function Specifications 1. **Function 1: `get_credentials(file_path: str, host: str) -> Tuple[str, str, str]`** - **Input:** - `file_path` (str): The file path of the `.netrc` file. - `host` (str): The host for which to retrieve credentials. - **Output:** - Returns a tuple `(login, account, password)` for the specified host. If the host is not found, raise a `ValueError`. 2. **Function 2: `add_entry(file_path: str, host: str, login: str, account: str, password: str) -> None`** - **Input:** - `file_path` (str): The file path of the `.netrc` file. - `host` (str): The host for the new entry. - `login` (str): The login username for the host. - `account` (str): The account name for the host. - `password` (str): The password for the host. - **Output:** - Adds a new entry to the `.netrc` file with the specified details. - If an entry for the same host already exists, raise a `ValueError`. 3. **Function 3: `remove_entry(file_path: str, host: str) -> None`** - **Input:** - `file_path` (str): The file path of the `.netrc` file. - `host` (str): The host to be removed. - **Output:** - Removes the entry for the specified host from the `.netrc` file. - If the host is not present in the file, raise a `ValueError`. Constraints: - Assume the paths and hostnames provided are valid strings. - You may assume that you have write permissions to the `.netrc` file. - The `.netrc` files are properly formatted; handle syntax errors using the `NetrcParseError` exception. Performance Requirements: - The functions should run efficiently, handling file operations with minimal overhead. Example Usage: ```python # Suppose the .netrc file at \'example.netrc\' contains the following: # machine example.com # login user123 # password pass123 # get_credentials print(get_credentials(\'example.netrc\', \'example.com\')) # Output: (\'user123\', None, \'pass123\') # add_entry add_entry(\'example.netrc\', \'newhost.com\', \'user456\', \'account456\', \'pass456\') # remove_entry remove_entry(\'example.netrc\', \'example.com\') ``` Complete the implementation of the functions based on the given specifications.","solution":"from typing import Tuple import netrc import os def get_credentials(file_path: str, host: str) -> Tuple[str, str, str]: nrc = netrc.netrc(file_path) if host in nrc.hosts: login, account, password = nrc.hosts[host] return login, account, password else: raise ValueError(f\\"Host \'{host}\' not found in: {file_path}\\") def add_entry(file_path: str, host: str, login: str, account: str, password: str) -> None: nrc = netrc.netrc(file_path) if host in nrc.hosts: raise ValueError(f\\"Host \'{host}\' already exists in: {file_path}\\") with open(file_path, \'a\') as f: f.write(f\\"nmachine {host}ntlogin {login}ntpassword {password}n\\") if account: f.write(f\\"taccount {account}n\\") def remove_entry(file_path: str, host: str) -> None: nrc = netrc.netrc(file_path) if host not in nrc.hosts: raise ValueError(f\\"Host \'{host}\' not found in: {file_path}\\") lines = [] with open(file_path, \'r\') as f: lines = f.readlines() new_lines = [] skip = False for line in lines: if line.startswith(f\\"machine {host}\\"): skip = True elif skip and line.startswith(\\"machine\\"): skip = False if not skip: new_lines.append(line) with open(file_path, \'w\') as f: f.writelines(new_lines)"},{"question":"Exception Handling and Signal Management in Python Objective: To evaluate your understanding of Python exception handling and signal management by requiring you to create a custom exception, raise it under specific conditions, handle it appropriately, and manage signals during this process. Problem Statement: You are tasked with implementing a Python module that performs the following tasks: 1. Defines a custom exception called `CustomException` that inherits from Python\'s built-in `Exception` class. 2. Implements a function `check_value(value)` that raises `CustomException` if the input value is not between 10 and 20 (inclusive). The exception should include a message stating: \\"ValueError: Provided value is out of acceptable range.\\" 3. Catches the `CustomException` and prints the exception message. 4. Implements signal handling to simulate a `KeyboardInterrupt` signal and gracefully handle it by printing \\"Keyboard Interrupt received, exiting...\\". Requirements: 1. Define the `CustomException` class. 2. Implement the `check_value(value)` function. 3. Implement the signal handling using `signal` module. 4. Provide the main function demonstrating the complete flow: - Call `check_value(value)` with different test values. - Raise a simulated signal interrupt. - Ensure that all raised exceptions and signals are handled gracefully. Constraints: - You must use the `signal` module to handle the `KeyboardInterrupt`. - You should not use any additional libraries apart from the Python standard library. - Assume all non-signal code runs in the main thread. Example: ```python import signal import sys class CustomException(Exception): pass def check_value(value): if not (10 <= value <= 20): raise CustomException(\\"ValueError: Provided value is out of acceptable range.\\") def signal_handler(sig, frame): print(\\"Keyboard Interrupt received, exiting...\\") sys.exit(0) def main(): signal.signal(signal.SIGINT, signal_handler) # Test cases try: check_value(25) except CustomException as e: print(e) try: check_value(15) except CustomException as e: print(e) # Simulate KeyboardInterrupt signal.raise_signal(signal.SIGINT) if __name__ == \\"__main__\\": main() ``` Implement the above specifications in Python, ensuring that all exceptions and signals are handled as described. Submission: Submit a Python script implementing the above requirements. Make sure your script is well-documented with comments explaining each part of your code.","solution":"import signal import sys class CustomException(Exception): Custom exception to be raised when value is out of the acceptable range. pass def check_value(value): Checks if the value is within the range [10, 20]. Raises CustomException if the value is out of this range. if not (10 <= value <= 20): raise CustomException(\\"ValueError: Provided value is out of acceptable range.\\") def signal_handler(sig, frame): Handles the KeyboardInterrupt signal. Prints a message and exits the program. print(\\"Keyboard Interrupt received, exiting...\\") sys.exit(0) def main(): Main function demonstrating the complete flow. Sets up the signal handler and checks values using check_value function. signal.signal(signal.SIGINT, signal_handler) # Test cases try: check_value(25) except CustomException as e: print(e) try: check_value(15) except CustomException as e: print(e) # Simulate KeyboardInterrupt signal.raise_signal(signal.SIGINT) if __name__ == \\"__main__\\": main()"},{"question":"# Task You are required to implement a simple TCP server and client using Python\'s `socket` module. The server must handle multiple clients concurrently and echo back any message it receives. The client will connect to the server and send messages. # Requirements 1. **Server Implementation**: - The server should listen on `localhost` and a specified port. - The server must handle multiple clients concurrently using threads or asyncio. - For each client connection, the server should: - Accept a connection. - Receive a message from the client. - Echo the message back to the client. - Close the connection. - Implement exception handling for socket-related errors. 2. **Client Implementation**: - The client should connect to the server running on `localhost` and the specified port. - The client should send a message provided by the user to the server. - The client should receive and print the echoed message from the server. - Implement exception handling for socket-related errors. # Input and Output Server Input - The server listens on `localhost` and a port provided as an argument. Client Input - The client connects to `localhost` on a specified port (same as the server) and sends a message provided by the user. Server Output - The server echoes the received message back to the client. Client Output - The client prints the message echoed back by the server. Example: ```python # Run the server first (this should be running in the background or another terminal) # python server.py 12345 # Run the client # python client.py 12345 # Input from user: Hello Server # Output: Hello Server ``` # Constraints - Make sure to handle the scenario gracefully when the server is not available. - The server should be able to handle at least 10 concurrent connections. # Evaluation Criteria - Correctness: The server should correctly handle multiple clients and echo back messages. - Robustness: The code should handle socket-related exceptions gracefully. - Efficiency: The server should be able to handle multiple clients concurrently without performance degradation. # Implementation Details **Server Code (server.py)** - Implement the server using Python\'s `socket` module. - Use threading or asyncio for handling multiple clients. **Client Code (client.py)** - Implement the client using Python\'s `socket` module. - Read input from the user and send it to the server. ```python # server.py import socket import threading def handle_client(client_socket): try: with client_socket: while True: msg = client_socket.recv(1024) if not msg: break client_socket.sendall(msg) except socket.error as e: print(f\\"Socket error: {e}\\") def main(port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', port)) server_socket.listen() print(f\\"Server listening on port {port}\\") try: while True: client_socket, addr = server_socket.accept() print(f\\"Accepted connection from {addr}\\") client_handler = threading.Thread(target=handle_client, args=(client_socket,)) client_handler.start() except KeyboardInterrupt: print(\\"nServer shutting down.\\") except socket.error as e: print(f\\"Socket error: {e}\\") finally: server_socket.close() if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python server.py <port>\\") sys.exit(1) port = int(sys.argv[1]) main(port) ``` ```python # client.py import socket def main(port): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: try: s.connect((\'localhost\', port)) msg = input(\\"Message to send: \\") s.sendall(msg.encode()) data = s.recv(1024) print(f\\"Received back: {data.decode()}\\") except socket.error as e: print(f\\"Socket error: {e}\\") if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python client.py <port>\\") sys.exit(1) port = int(sys.argv[1]) main(port) ```","solution":"# server.py import socket import threading def handle_client(client_socket): try: with client_socket: while True: msg = client_socket.recv(1024) if not msg: break client_socket.sendall(msg) except socket.error as e: print(f\\"Socket error: {e}\\") def main(port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', port)) server_socket.listen() print(f\\"Server listening on port {port}\\") try: while True: client_socket, addr = server_socket.accept() print(f\\"Accepted connection from {addr}\\") client_handler = threading.Thread(target=handle_client, args=(client_socket,)) client_handler.start() except KeyboardInterrupt: print(\\"nServer shutting down.\\") except socket.error as e: print(f\\"Socket error: {e}\\") finally: server_socket.close() if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python server.py <port>\\") sys.exit(1) port = int(sys.argv[1]) main(port) # client.py import socket def main(port): with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s: try: s.connect((\'localhost\', port)) msg = input(\\"Message to send: \\") s.sendall(msg.encode()) data = s.recv(1024) print(f\\"Received back: {data.decode()}\\") except socket.error as e: print(f\\"Socket error: {e}\\") if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python client.py <port>\\") sys.exit(1) port = int(sys.argv[1]) main(port)"},{"question":"# Image Type Determination and Extension **Objective**: Write Python functions to determine the file type of an image and extend the `imghdr` module to recognize a custom image format. Part 1: Image Type Determination 1. Implement a function `determine_image_type(file_path: str) -> str` that takes the file path of an image as input and returns the image type using the `imghdr` module. - **Input**: `file_path` (string) - The path to the image file. - **Output**: Image type (string) as determined by `imghdr.what()`. - **Example Usage**: ```python image_type = determine_image_type(\'path_to_image.png\') print(image_type) # Output: \'png\' ``` Part 2: Extend imghdr to Recognize a Custom Format 2. Suppose we have a custom binary image format called \\"ABC\\". Each ABC image starts with the byte sequence `b\'ABC\'`. Implement a function `detect_abc(h: bytes, file: Optional[IO] = None) -> Optional[str]` that checks for this custom format. - **Input**: - `h` (bytes) - The byte stream of the file header. - `file` (Optional[IO]) - The file object (not used for this custom format detection). - **Output**: The string `\'abc\'` if the custom format is detected, otherwise `None`. 3. Modify `imghdr.tests` to include the custom `detect_abc` function and test the updated functionality with a sample ABC image. Implementation Constraints: - Ensure that the custom format detection function is correctly appended to the `imghdr.tests`. - Demonstrate the detection by creating a small test program that uses `imghdr.what` to identify an \\"ABC\\" image. Example Test Program: ```python import imghdr from io import BytesIO def determine_image_type(file_path: str) -> str: return imghdr.what(file_path) def detect_abc(h: bytes, file: Optional[IO] = None) -> Optional[str]: if h.startswith(b\'ABC\'): return \'abc\' return None # Append the custom detection function to imghdr imghdr.tests.append(detect_abc) # Create a fake ABC image for testing abc_image = BytesIO(b\'ABC\' + b\'x00\' * 100) # Test the custom type detection image_type = imghdr.what(None, abc_image.read()) print(image_type) # Output: \'abc\' ``` **Note**: Document your code, handle edge cases, and include necessary imports.","solution":"import imghdr from typing import Optional, IO def determine_image_type(file_path: str) -> str: Determines the file type of an image given its file path. Parameters: - file_path (str): The path to the image file. Returns: - str: The image type determined by imghdr.what() return imghdr.what(file_path) def detect_abc(h: bytes, file: Optional[IO] = None) -> Optional[str]: Detects custom \'ABC\' image format. Parameters: - h (bytes): The byte stream of the file header. - file (Optional[IO]): The file object (optional, not used). Returns: - Optional[str]: \'abc\' if the custom format is detected, otherwise None. if h.startswith(b\'ABC\'): return \'abc\' return None # Append the custom detection function to imghdr imghdr.tests.append(detect_abc)"},{"question":"**Question: Implement a Unix Process Manager** You are required to implement a Unix Process Manager using Python. The manager should carry out the following tasks: 1. **Create a list of active processes**: Gather information about active processes and list their process IDs (PIDs) along with their user IDs (UIDs), group IDs (GIDs), and the command used to start them. 2. **Kill a process**: Implement functionality to terminate a process by its PID. 3. **Monitor resource usage**: Track and report CPU and memory usage for each active process. # Function Signature ```python class ProcessManager: def __init__(self): pass def list_active_processes(self) -> List[Dict[str, Union[int, str]]]: List all active processes with their PIDs, UIDs, GIDs, and the command used to start them. Returns: List[Dict[str, Union[int, str]]]: A list of dictionaries containing the PID, UID, GID, and command for each active process. pass def kill_process(self, pid: int) -> bool: Terminate the process with the given PID. Args: pid (int): The process ID of the process to terminate. Returns: bool: True if the process was successfully terminated, False otherwise. pass def monitor_resources(self) -> List[Dict[str, Union[int, float]]]: Monitor and report CPU and memory usage for each active process. Returns: List[Dict[str, Union[int, float]]]: A list of dictionaries containing the PID, CPU usage, and memory usage for each active process. pass ``` # Input/Output - **`list_active_processes` function output**: A list of dictionaries where each dictionary contains: - `pid` (int): Process ID. - `uid` (int): User ID of the process owner. - `gid` (int): Group ID of the process owner. - `command` (str): Command used to start the process. - **`kill_process` function input/output**: - Input: `pid` (int) - The process ID of the process to terminate. - Output: `bool` - Return `True` if the process was successfully terminated, `False` otherwise. - **`monitor_resources` function output**: A list of dictionaries where each dictionary contains: - `pid` (int): Process ID. - `cpu_usage` (float): CPU usage of the process in percentage. - `memory_usage` (float): Memory usage of the process in megabytes (MB). # Constraints 1. The methods should handle exceptions gracefully and provide useful error messages where necessary. 2. The implementation should efficiently handle a large number of processes. 3. Ensure that you have appropriate permissions to query and manage processes. # Example Usage ```python pm = ProcessManager() # List active processes processes = pm.list_active_processes() print(processes) # Kill a process with PID 1234 result = pm.kill_process(1234) print(f\\"Process killed: {result}\\") # Monitor resource usage resources = pm.monitor_resources() print(resources) ``` # Notes - Use the `os`, `pwd`, `grp`, and similar Unix-specific Python modules to gather the necessary information. - Consider using the `psutil` library for some of the functionality if needed. - Make sure to handle any permissions-related exceptions appropriately.","solution":"import os import psutil class ProcessManager: def __init__(self): pass def list_active_processes(self): List all active processes with their PIDs, UIDs, GIDs, and the command used to start them. Returns: List[Dict[str, Union[int, str]]]: A list of dictionaries containing the PID, UID, GID, and command for each active process. processes = [] for proc in psutil.process_iter([\'pid\', \'username\', \'uids\', \'gids\', \'cmdline\']): try: cmdline = \' \'.join(proc.info[\'cmdline\']) if proc.info[\'cmdline\'] else \'[unknown]\' processes.append({ \'pid\': proc.info[\'pid\'], \'uid\': proc.info[\'uids\'].real, \'gid\': proc.info[\'gids\'].real, \'command\': cmdline }) except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess): pass return processes def kill_process(self, pid): Terminate the process with the given PID. Args: pid (int): The process ID of the process to terminate. Returns: bool: True if the process was successfully terminated, False otherwise. try: p = psutil.Process(pid) p.terminate() p.wait(timeout=3) return True except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired): return False def monitor_resources(self): Monitor and report CPU and memory usage for each active process. Returns: List[Dict[str, Union[int, float]]]: A list of dictionaries containing the PID, CPU usage, and memory usage for each active process. resources = [] for proc in psutil.process_iter([\'pid\', \'cpu_percent\', \'memory_info\']): try: resources.append({ \'pid\': proc.info[\'pid\'], \'cpu_usage\': proc.info[\'cpu_percent\'], \'memory_usage\': proc.info[\'memory_info\'].rss / (1024 * 1024) }) except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess): pass return resources"},{"question":"# Pandas Coding Assessment: Rolling Window Operations Objective Implement a class utilizing pandas rolling window operations to compute statistical values over a given dataset. Requirements Your task is to implement a class `RollingStatistics` that computes various rolling window statistics on input datasets using pandas. # Class Definition ```python import pandas as pd class RollingStatistics: def __init__(self, data: pd.DataFrame): Initialize the RollingStatistics object with the provided DataFrame. :param data: A pandas DataFrame with numeric values :type data: pd.DataFrame pass def get_rolling_mean(self, window: int, column: str) -> pd.Series: Compute the rolling mean for the specified column over the given window size. :param window: Size of the moving window :type window: int :param column: Column name for which the rolling mean is to be computed :type column: str :return: A pandas Series with the rolling mean values :rtype: pd.Series pass def get_rolling_sum(self, window: int, column: str) -> pd.Series: Compute the rolling sum for the specified column over the given window size. :param window: Size of the moving window :type window: int :param column: Column name for which the rolling sum is to be computed :type column: str :return: A pandas Series with the rolling sum values :rtype: pd.Series pass def get_ewm_mean(self, span: int, column: str) -> pd.Series: Compute the exponentially weighted mean for the specified column with the given span. :param span: Span for the exponentially weighted mean :type span: int :param column: Column name for which the exponentially weighted mean is to be computed :type column: str :return: A pandas Series with the exponentially weighted mean values :rtype: pd.Series pass def get_custom_rolling_sum(self, window_size: int, use_expanding: list, column: str) -> pd.Series: Compute the custom rolling sum for the specified column using a custom window size. The window is expanding where `use_expanding` is True and fixed size otherwise. :param window_size: Size of the fixed window when `use_expanding` is False :type window_size: int :param use_expanding: List indicating where to use expanding window :type use_expanding: list :param column: Column name for which the custom rolling sum is to be computed :type column: str :return: A pandas Series with the custom rolling sum values :rtype: pd.Series pass ``` # Input 1. `data` (pd.DataFrame): A pandas DataFrame consisting of numerical columns. 2. `window` (int): Size of the moving window for rolling operations. 3. `column` (str): Name of the column to perform the rolling operation on. 4. `span` (int): Span for the exponentially weighted moving average. 5. `use_expanding` (list): A list of boolean values determining either to use an expanding window for each element. # Output - A pandas Series containing the computed rolling statistic. # Constraints - The DataFrame will contain only numerical columns. - The `window`, `span`, and `window_size` parameters will be positive integers. - The `use_expanding` list will be the same length as the number of rows in the DataFrame and will only contain boolean values. # Performance - Ensure that the operations handle large datasets efficiently. # Example Usage ```python import pandas as pd data = pd.DataFrame({ \\"values\\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] }) rolling_stats = RollingStatistics(data) print(rolling_stats.get_rolling_mean(window=3, column=\\"values\\")) print(rolling_stats.get_rolling_sum(window=3, column=\\"values\\")) print(rolling_stats.get_ewm_mean(span=2, column=\\"values\\")) use_expanding = [True, False, True, False, True, True, False, True, False, True] print(rolling_stats.get_custom_rolling_sum(window_size=2, use_expanding=use_expanding, column=\\"values\\")) ``` Your implementation should pass all the example usages and produce the expected rolling statistical results.","solution":"import pandas as pd class RollingStatistics: def __init__(self, data: pd.DataFrame): Initialize the RollingStatistics object with the provided DataFrame. :param data: A pandas DataFrame with numeric values :type data: pd.DataFrame self.data = data def get_rolling_mean(self, window: int, column: str) -> pd.Series: Compute the rolling mean for the specified column over the given window size. :param window: Size of the moving window :type window: int :param column: Column name for which the rolling mean is to be computed :type column: str :return: A pandas Series with the rolling mean values :rtype: pd.Series return self.data[column].rolling(window=window).mean() def get_rolling_sum(self, window: int, column: str) -> pd.Series: Compute the rolling sum for the specified column over the given window size. :param window: Size of the moving window :type window: int :param column: Column name for which the rolling sum is to be computed :type column: str :return: A pandas Series with the rolling sum values :rtype: pd.Series return self.data[column].rolling(window=window).sum() def get_ewm_mean(self, span: int, column: str) -> pd.Series: Compute the exponentially weighted mean for the specified column with the given span. :param span: Span for the exponentially weighted mean :type span: int :param column: Column name for which the exponentially weighted mean is to be computed :type column: str :return: A pandas Series with the exponentially weighted mean values :rtype: pd.Series return self.data[column].ewm(span=span).mean() def get_custom_rolling_sum(self, window_size: int, use_expanding: list, column: str) -> pd.Series: Compute the custom rolling sum for the specified column using a custom window size. The window is expanding where `use_expanding` is True and fixed size otherwise. :param window_size: Size of the fixed window when `use_expanding` is False :type window_size: int :param use_expanding: List indicating where to use expanding window :type use_expanding: list :param column: Column name for which the custom rolling sum is to be computed :type column: str :return: A pandas Series with the custom rolling sum values :rtype: pd.Series result = [] for i, expand in enumerate(use_expanding): if expand: result.append(self.data[column].iloc[:i+1].sum()) else: if i + 1 < window_size: result.append(self.data[column].iloc[:i+1].sum()) else: result.append(self.data[column].iloc[i+1-window_size:i+1].sum()) return pd.Series(result)"},{"question":"Objective: Demonstrate the ability to: 1. Read and manipulate configuration files using Python. 2. Interact with Distutils commands through the `setup.cfg` file. Problem Statement: You are provided with a partially completed Python package that includes a `setup.cfg` file. Your task is to write a Python function `configure_setup` that dynamically updates this file based on the provided specifications. The `setup.cfg` file is structured as follows: ``` [build_ext] inplace=1 [bdist_rpm] release=1 packager=Greg Ward <gward@python.net> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Function Signature: ```python def configure_setup(option_mappings: dict) -> None: Updates the setup.cfg file with the given option mappings. Parameters: option_mappings (dict): A dictionary where keys are commands and values are dictionaries of options to set for those commands. For example: { \'build_ext\': { \'inplace\': \'0\', \'include_dirs\': \'/usr/include/python3.10\' }, \'install\': { \'prefix\': \'/usr/local\' } } Returns: None # Your implementation here ``` Input: - `option_mappings`: A dictionary containing new or updated configurations for various Distutils commands. - Each key is a command (e.g., `build_ext`). - Each value is another dictionary where: - Keys are the options for that command. - Values are the new option values. Output: - No return value. The function should directly update the `setup.cfg` file in the current directory. Constraints: - The function should maintain the existing format and comments in the `setup.cfg` file as much as possible. - Options specified in `option_mappings` should override the existing options or add new ones if they do not already exist. Example: Given the following `setup.cfg` file content: ``` [build_ext] inplace=1 [bdist_rpm] release=1 packager=Greg Ward <gward@python.net> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` And the function call: ```python configure_setup({ \'build_ext\': { \'inplace\': \'0\', \'include_dirs\': \'/usr/include/python3.10\' }, \'install\': { \'prefix\': \'/usr/local\' } }) ``` After execution, the `setup.cfg` file should be updated to: ``` [build_ext] inplace=0 include_dirs=/usr/include/python3.10 [bdist_rpm] release=1 packager=Greg Ward <gward@python.net> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ [install] prefix=/usr/local ``` Notes: - You can use libraries such as `configparser` to help with reading and writing the configuration file. - Assume that the `setup.cfg` file is well-formed. - Your solution should handle preserving the existing comments in the file.","solution":"import configparser def configure_setup(option_mappings): Updates the setup.cfg file with the given option mappings. Parameters: option_mappings (dict): A dictionary where keys are commands and values are dictionaries of options to set for those commands. For example: { \'build_ext\': { \'inplace\': \'0\', \'include_dirs\': \'/usr/include/python3.10\' }, \'install\': { \'prefix\': \'/usr/local\' } } Returns: None config = configparser.ConfigParser() config.read(\'setup.cfg\') for section, options in option_mappings.items(): if not config.has_section(section): config.add_section(section) for option, value in options.items(): config.set(section, option, value) with open(\'setup.cfg\', \'w\') as configfile: config.write(configfile)"},{"question":"Objective Given the comprehensive documentation on Unicode objects and APIs, the task is to write a Python function that demonstrates a deep understanding of Unicode string manipulation. This question will test understanding of Unicode sequences, character properties, encoding/decoding, and custom operations. Problem Statement You are required to implement a Python function `find_and_replace_unicode` that takes in the following arguments: 1. `unicode_str`: A UTF-8 encoded Unicode string. 2. `find_char`: A single character to find within the string. 3. `replace_char`: A single character to replace the found character with. 4. `encoding`: The encoding type to use for the final string representation. The function should: 1. Decode the input string (`unicode_str`) using UTF-8 encoding. 2. Perform a search to find all occurrences of `find_char`. 3. Replace each found occurrence of `find_char` with `replace_char`. 4. Encode the resultant string using the specified `encoding` (e.g., \'utf-8\', \'latin-1\'). For example: ```python def find_and_replace_unicode(unicode_str, find_char, replace_char, encoding): # Your implementation here # Example usage: original_str = \\"hÃ©llo wÃ¸rld\\" # UTF-8 encoded find_char = \\"Ã¸\\" replace_char = \\"o\\" encoding = \\"latin-1\\" result = find_and_replace_unicode(original_str, find_char, replace_char, encoding) print(result) # Output should be \\"hÃ©llo world\\" encoded in \'latin-1\' ``` Function Signature ```python def find_and_replace_unicode(unicode_str: str, find_char: str, replace_char: str, encoding: str) -> bytes: pass ``` Constraints - `unicode_str` is guaranteed to be a valid UTF-8 encoded string. - `find_char` and `replace_char` are single Unicode characters. - The `encoding` provided will be one of the following: \'utf-8\', \'latin-1\', \'ascii\'. - You are not allowed to use the `str.replace` method. You must use the provided APIs or Unicode handling functions from the documentation to achieve the solution. - Raise a `ValueError` if the `unicode_str` is not properly encoded in UTF-8 or if the `encoding` provided is invalid. Evaluation Your function will be evaluated based on: - Correctness: Properly finding and replacing characters. - Efficiency: Reasonable performance for typical input sizes. - Proper use of Unicode APIs. - Handling of edge cases (e.g., characters not found, empty strings). Additional Information You may refer to the provided Unicode object documentation for understanding the underlying operations needed to decode, manipulate, and re-encode the string as per the requirements.","solution":"def find_and_replace_unicode(unicode_str: str, find_char: str, replace_char: str, encoding: str) -> bytes: Replaces all occurrences of `find_char` in `unicode_str` with `replace_char` and encodes the resulting string in the specified `encoding`. Parameters: - unicode_str (str): UTF-8 encoded Unicode string. - find_char (str): Single character to find within the string. - replace_char (str): Single character to replace the found character with. - encoding (str): Encoding type for the final string representation. Returns: - bytes: The resulting string encoded in the specified encoding. # Validate inputs if not isinstance(unicode_str, str): raise ValueError(\\"The input `unicode_str` must be a valid UTF-8 encoded Unicode string.\\") if not isinstance(find_char, str) or len(find_char) != 1: raise ValueError(\\"The `find_char` must be a single Unicode character.\\") if not isinstance(replace_char, str) or len(replace_char) != 1: raise ValueError(\\"The `replace_char` must be a single Unicode character.\\") if encoding not in [\'utf-8\', \'latin-1\', \'ascii\']: raise ValueError(\\"The `encoding` must be one of the following: \'utf-8\', \'latin-1\', \'ascii\'.\\") # Decode the input string from UTF-8 try: decoded_str = unicode_str.decode(\'utf-8\') except AttributeError: decoded_str = unicode_str # string is already decoded if it\'s a `str` type except UnicodeDecodeError: raise ValueError(\\"The input `unicode_str` is not properly encoded in UTF-8.\\") # Perform the replacement without using str.replace replaced_str = \'\'.join([replace_char if char == find_char else char for char in decoded_str]) # Encode the resultant string using the specified encoding return replaced_str.encode(encoding)"},{"question":"You are tasked with writing a Python function that provides a summary of Python\'s installation paths and configuration variables for a given scheme. The function should use the `sysconfig` module to gather and format this information. Function Signature ```python def summarize_sysconfig(scheme: str) -> str: pass ``` Input - `scheme` (str): A string representing the installation scheme. It should be one of the values returned by `sysconfig.get_scheme_names()`. - Example: `\'posix_prefix\'` Output - Returns a formatted string summarizing the paths and configuration variables for the given installation scheme. Constraints and Notes - If the provided `scheme` is not valid (not in `sysconfig.get_scheme_names()`), the function should return `\\"Invalid scheme name.\\"`. - The function should retrieve the installation paths associated with the given scheme using `sysconfig.get_paths(scheme)`. - The function should also include a list of all configuration variables using `sysconfig.get_config_vars()`. - The output should be a well-formatted multi-line string that includes: - The current platform (retrieved using `sysconfig.get_platform()`). - The Python version (retrieved using `sysconfig.get_python_version()`). - The installation paths for the given scheme. - A portion of key configuration variables. Example of usage: ```python summary = summarize_sysconfig(\'posix_prefix\') print(summary) ``` This might output: ``` Platform: linux-x86_64 Python version: 3.10 Scheme: posix_prefix Paths: data = \\"/usr/local\\" include = \\"/usr/local/include\\" platinclude = \\"/usr/local/include\\" platlib = \\"/usr/local/lib/python3.10/site-packages\\" platstdlib = \\"/usr/local/lib/python3.10\\" purelib = \\"/usr/local/lib/python3.10/site-packages\\" scripts = \\"/usr/local/bin\\" stdlib = \\"/usr/local/lib/python3.10\\" Configuration Variables: ABIFLAGS = \\"\\" PY_CFLAGS = \\"-Wno-unused-result -Wsign-compare -g -Og -Wall...\\" PY_LDFLAGS = \\"-L/home/build/Python-3.9.2/\\" ... ``` Good luck!","solution":"import sysconfig def summarize_sysconfig(scheme: str) -> str: Provides a summary of Python\'s installation paths and configuration variables for a given scheme. Parameters: scheme (str): A string representing the installation scheme. Returns: str: A formatted string summarizing the paths and configuration variables for the given installation scheme. # Check if the scheme is valid if scheme not in sysconfig.get_scheme_names(): return \\"Invalid scheme name.\\" # Retrieve platform and version information platform = sysconfig.get_platform() python_version = sysconfig.get_python_version() # Retrieve paths for the given scheme paths = sysconfig.get_paths(scheme) # Retrieve configuration variables config_vars = sysconfig.get_config_vars() # Format the summary summary = [] summary.append(f\\"Platform: {platform}\\") summary.append(f\\"Python version: {python_version}\\") summary.append(f\\"Scheme: {scheme}\\") summary.append(\\"nPaths:\\") for key, value in paths.items(): summary.append(f\\" {key} = \\"{value}\\"\\") summary.append(\\"nConfiguration Variables:\\") for key, value in config_vars.items(): summary.append(f\\" {key} = \\"{value}\\"\\") return \\"n\\".join(summary)"},{"question":"Objective You are required to demonstrate your understanding of binary data manipulation and encoding/decoding using Python\'s built-in `struct` and `codecs` modules. Problem Statement You are tasked with designing a function that can encode a mixed datatype record into a binary format and then decode it back to the original form. The record contains: - An integer - A floating-point number - A UTF-8 encoded string of known length. You need to define two functions: 1. `encode_record(record: Tuple[int, float, str]) -> bytes`: - This function takes a tuple consisting of an integer, a float, and a string. It encodes this record into a binary format using the `struct` module. - Assume the string length will not exceed 50 characters. - The format for encoding should be: integer (4 bytes), float (8 bytes), string (length-prefixed by 1 byte and up to 50 characters). 2. `decode_record(binary_data: bytes) -> Tuple[int, float, str]`: - This function takes a bytes object which represents the binary format as described above. - It decodes this back into a tuple consisting of an integer, a float, and a UTF-8 encoded string using the `struct` and `codecs` modules. Input - `record` (Tuple[int, float, str]): The input tuple containing: - An integer `int`: range from -2^31 to 2^31-1 - A float `float`: IEEE 754 double precision - A string `str`: A UTF-8 string of maximum length 50 characters - `binary_data` (bytes): Binary data formatted as per the above specification. Output - For `encode_record`: A bytes object that represents the packed and encoded data. - For `decode_record`: A tuple (int, float, str) representing the decoded record. Constraints - You must use the `struct` module for packing and unpacking the binary data. - Use the `codecs` module for handling the UTF-8 string encoding and decoding. - The function should handle strings up to a maximum length of 50 characters. - Assume valid inputs will be provided. Example ```python def encode_record(record: Tuple[int, float, str]) -> bytes: # Implementation here def decode_record(binary_data: bytes) -> Tuple[int, float, str]: # Implementation here # Example usage: record = (42, 3.141592653589793, \\"Hello World\\") binary_data = encode_record(record) decoded_record = decode_record(binary_data) assert record == decoded_record ``` Notes 1. Make sure to handle padding or alignment as per struct\'s format specifications. 2. Test your functions thoroughly to ensure they handle various edge cases.","solution":"import struct import codecs from typing import Tuple def encode_record(record: Tuple[int, float, str]) -> bytes: Encodes a tuple of (int, float, str) into a binary format. - int (4 bytes) - float (8 bytes) - string (length-prefixed UTF-8 string, up to 50 characters) integer_part = struct.pack(\'i\', record[0]) float_part = struct.pack(\'d\', record[1]) utf8_string = codecs.encode(record[2], \'utf-8\') string_length = len(utf8_string) if string_length > 50: raise ValueError(\\"String length exceeds maximum of 50 characters in UTF-8 encoding\\") string_part = struct.pack(\'B\', string_length) + utf8_string.ljust(50, b\'x00\') return integer_part + float_part + string_part def decode_record(binary_data: bytes) -> Tuple[int, float, str]: Decodes binary data into a tuple of (int, float, str). - int (4 bytes) - float (8 bytes) - string (length-prefixed UTF-8 string, up to 50 characters) integer_part = struct.unpack(\'i\', binary_data[:4])[0] float_part = struct.unpack(\'d\', binary_data[4:12])[0] string_length = struct.unpack(\'B\', binary_data[12:13])[0] string_part = codecs.decode(binary_data[13:13+string_length], \'utf-8\') return (integer_part, float_part, string_part)"},{"question":"# Custom Object Type Implementation in Python Your task is to define a custom object type in Python that represents a mathematical vector. This exercise will assess your understanding of Python\'s object model and type definition. Requirements: 1. **Vector Class**: Create a `Vector` class that supports basic vector arithmetic. 2. **Attributes**: - The class should store its elements in a list or similar iterable structure. 3. **Initialization**: - The class should be initialized with any number of elements. 4. **Methods**: - Implement the following methods to support vector arithmetic: - `__add__(self, other)`: Adds two vectors. - `__sub__(self, other)`: Subtracts two vectors. - `__mul__(self, scalar)`: Multiplies the vector by a scalar. - `__len__(self)`: Returns the number of elements in the vector. - `__repr__(self)`: Returns a string representation of the vector. Input and Output Formats: - **Input**: - Initialization: one or more numerical values. - Methods: another `Vector` object or a numerical scalar, depending on the method. - **Output**: - For `__add__`, `__sub__`, and `__mul__`: Return new `Vector` objects with the resultant values. - For `__len__`: Return an integer value representing the number of elements. - For `__repr__`: Return a string representation of the vector in the form `Vector(elements)`. Constraints: - Ensure that all vector arithmetic methods check for dimension compatibility. - Raise appropriate exceptions if operations between incompatible vectors are attempted. Example Usage: ```python # Initialization of vectors v1 = Vector(1, 2, 3) v2 = Vector(4, 5, 6) # Addition v3 = v1 + v2 print(v3) # Output: Vector([5, 7, 9]) # Subtraction v4 = v1 - v2 print(v4) # Output: Vector([-3, -3, -3]) # Scalar Multiplication v5 = v1 * 3 print(v5) # Output: Vector([3, 6, 9]) # Length print(len(v1)) # Output: 3 # String Representation print(v1) # Output: Vector([1, 2, 3]) ``` Implement the `Vector` class by fulfilling all the listed requirements and constraints.","solution":"class Vector: def __init__(self, *elements): if not elements: raise ValueError(\\"A vector must have at least one element.\\") self.elements = list(elements) def __add__(self, other): if not isinstance(other, Vector): raise TypeError(\\"Operand must be of type Vector\\") if len(self) != len(other): raise ValueError(\\"Vectors must be of same dimension to add\\") return Vector(*(a + b for a, b in zip(self.elements, other.elements))) def __sub__(self, other): if not isinstance(other, Vector): raise TypeError(\\"Operand must be of type Vector\\") if len(self) != len(other): raise ValueError(\\"Vectors must be of same dimension to subtract\\") return Vector(*(a - b for a, b in zip(self.elements, other.elements))) def __mul__(self, scalar): if not isinstance(scalar, (int, float)): raise TypeError(\\"Scalar must be a number\\") return Vector(*(a * scalar for a in self.elements)) def __len__(self): return len(self.elements) def __repr__(self): return f\\"Vector({self.elements})\\""},{"question":"Objectives Write a Python function called `find_large_groups(threshold)` that fetches all groups from the Unix group database and returns a list of group names which have more members than a given threshold. Input - `threshold` (int): A non-negative integer representing the minimum number of members a group should have to be included in the result. Output - List of group names (list of strings): Each group name should be included in the result if the number of members in that group is greater than `threshold`. Constraints - You must use the `grp` module. - Use appropriate error handling for possible exceptions. - Ensure the function runs efficiently on large datasets. Example ```python # This call fetches all groups with more than 3 members print(find_large_groups(3)) # Expected output could be something like [\'staff\', \'sudo\'] depending on the system configuration ``` Notes - Groups with no members listed (empty `gr_mem`) should not be included in the output. - Different Unix systems may have different group configurations, so the exact output can vary. - Avoid using group names starting with \\"+\\" or \\"-\\". Function Definition ```python import grp def find_large_groups(threshold): # Your code here pass ``` Implement the above function and test it with various thresholds to ensure it works correctly and efficiently.","solution":"import grp def find_large_groups(threshold): Fetches all groups from the Unix group database and returns a list of group names which have more members than the given threshold. Parameters: threshold (int): A non-negative integer representing the minimum number of members a group should have to be included in the result. Returns: list: List of group names with more members than the given threshold. if threshold < 0: raise ValueError(\\"Threshold must be a non-negative integer\\") try: groups = grp.getgrall() large_groups = [] for group in groups: if len(group.gr_mem) > threshold and not (group.gr_name.startswith(\'+\') or group.gr_name.startswith(\'-\')): large_groups.append(group.gr_name) return large_groups except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"**Coding Assessment Question:** # Objective To assess the student\'s understanding of the `torch.monitor` module, specifically how to use it to log events and metrics, including the use of customized event handlers. # Problem Statement You are tasked with monitoring the training process of a neural network using the `torch.monitor` module. You need to log the training loss and accuracy at the end of each epoch and any changes in the learning rate. Additionally, implement a custom event handler that saves these logged events to a CSV file. # Requirements 1. Implement a class `TrainingMonitor` that: - Tracks and logs the training loss and accuracy at the end of each epoch. - Logs any learning rate changes. 2. Implement a custom event handler `CSVEventHandler` in the `TrainingMonitor` class which: - Captures the log events. - Writes the logs to a CSV file with columns: *timestamp*, *event_type*, *value*. 3. Demonstrate the usage of the `TrainingMonitor` in a PyTorch training loop with a simple neural network. # Interface Details 1. **TrainingMonitor class:** ```python class TrainingMonitor: def __init__(self, csv_file_path: str): # Initializes the monitor with the path to the CSV file. def log_training_metrics(self, epoch: int, loss: float, accuracy: float): # Logs training metrics. def log_learning_rate_change(self, new_lr: float): # Logs learning rate changes. def register_event_handler(self): # Registers the custom CSV event handler. ``` 2. **CSVEventHandler class (nested inside TrainingMonitor):** ```python class CSVEventHandler: def __init__(self, file_path: str): # Initialize with the path to the CSV file. def handle_event(self, event): # Handles and logs the event to the CSV file. ``` # Constraints - You need to use the `torch.monitor` module functionalities for logging. - Ensure minimal performance impact during the logging process. - The code should be clear, efficient, and well-commented. # Example Usage ```python # Initialize the monitor monitor = TrainingMonitor(csv_file_path=\'training_log.csv\') monitor.register_event_handler() # Simulated training loop for epoch in range(num_epochs): # Train the model (not shown) loss = ... accuracy = ... # Log metrics monitor.log_training_metrics(epoch, loss, accuracy) # Simulate learning rate change if epoch % 10 == 0: new_lr = ... monitor.log_learning_rate_change(new_lr) ``` You should provide a full implementation of the `TrainingMonitor` class and an example training loop showcasing how it can be used to log the training process. # Deliverables 1. Full implementation of the `TrainingMonitor` and `CSVEventHandler` classes. 2. Example usage in a training loop, demonstrating the logging of metrics and events.","solution":"import torch import csv import os from datetime import datetime class TrainingMonitor: def __init__(self, csv_file_path: str): self.csv_file_path = csv_file_path self.initialize_csv_file() def initialize_csv_file(self): Initializes the CSV file with headers. if not os.path.exists(self.csv_file_path): with open(self.csv_file_path, mode=\'w\', newline=\'\') as file: writer = csv.writer(file) writer.writerow([\'timestamp\', \'event_type\', \'value\']) def log_training_metrics(self, epoch: int, loss: float, accuracy: float): Logs training metrics. timestamp = datetime.now().isoformat() self.csv_event_handler.handle_event((timestamp, \'training_metrics\', {\'epoch\': epoch, \'loss\': loss, \'accuracy\': accuracy})) def log_learning_rate_change(self, new_lr: float): Logs learning rate changes. timestamp = datetime.now().isoformat() self.csv_event_handler.handle_event((timestamp, \'learning_rate_change\', {\'new_lr\': new_lr})) def register_event_handler(self): Registers the custom CSV event handler. self.csv_event_handler = self.CSVEventHandler(self.csv_file_path) class CSVEventHandler: def __init__(self, file_path: str): self.file_path = file_path def handle_event(self, event): Handles and logs the event to the CSV file. timestamp, event_type, value = event with open(self.file_path, mode=\'a\', newline=\'\') as file: writer = csv.writer(file) writer.writerow([timestamp, event_type, str(value)]) # Example usage in a training loop def example_training_loop(): # Initialize the monitor monitor = TrainingMonitor(csv_file_path=\'training_log.csv\') monitor.register_event_handler() # Simulated training loop num_epochs = 5 for epoch in range(num_epochs): # Simulate training metrics loss = (5 - epoch) * 0.5 # Dummy loss value that decreases each epoch accuracy = epoch * 0.1 # Dummy accuracy value that increases each epoch # Log metrics monitor.log_training_metrics(epoch, loss, accuracy) # Simulate learning rate change every 2 epochs if epoch % 2 == 0: new_lr = 0.01 * (0.9 ** (epoch // 2)) monitor.log_learning_rate_change(new_lr)"},{"question":"# Advanced Python Coroutine Manipulation **Objective:** Demonstrate comprehension of fundamental and advanced concepts of coroutine objects in Python. **Task:** Implement a function `create_and_verify_coroutine` that performs the following steps: 1. **Create**: Initialize a coroutine object from an asynchronous function and return it. 2. **Check**: Verify if the created coroutine is indeed a coroutine object. **Function Signature:** ```python def create_and_verify_coroutine(func: callable) -> bool: pass ``` **Input:** - `func` (callable): An asynchronous function that, when called, returns a coroutine object. **Output:** - `bool`: Return `True` if the object created from the function is a coroutine object, otherwise return `False`. **Constraints:** - The provided function `func` will always be a valid asynchronous function. **Examples:** ```python async def sample_coroutine(): await asyncio.sleep(1) return \\"Completed\\" # Example call result = create_and_verify_coroutine(sample_coroutine) # Expected Output: True ``` **Detailed Steps:** 1. Use the provided `func` to create a coroutine object. 2. Utilize the correct method for checking whether an object is a coroutine. 3. Return the result of the type check. **Requirements:** - Do not use any external libraries except those included in Python\'s standard library. - Ensure the code is clean, properly commented, and follows Python conventions. **Hint:** Use the `inspect` module or `types.coroutine` to properly check whether an object is a coroutine.","solution":"import inspect def create_and_verify_coroutine(func: callable) -> bool: Creates a coroutine object from an asynchronous function and verifies if it is a coroutine. Args: func (callable): An asynchronous function that returns a coroutine object. Returns: bool: True if the created object is a coroutine, otherwise False. coroutine = func() return inspect.iscoroutine(coroutine)"},{"question":"Objective: To assess the student\'s understanding of handling missing data in pandas, particularly in detecting, imputing, and operating with missing values in DataFrame objects. Problem Statement: You are given the following dataset containing missing values: ```python import pandas as pd import numpy as np data = { \'A\': [1, 2, np.nan, 4, 5, np.nan, 7], \'B\': [np.nan, 2.1, 3.5, np.nan, 5.0, 6.0, 7.8], \'C\': [\'A\', np.nan, \'C\', \'D\', np.nan, \'F\', \'G\'] } df = pd.DataFrame(data) ``` Write a function `process_missing_data(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations: 1. **Detect Missing Values:** Identify and print the number of missing values in each column. 2. **Fill Missing Data:** - For columns with numeric data types, fill the missing values with the mean of the respective column. - For columns with object data types (like strings), fill missing values with the string \\"unknown\\". 3. **Add a New Column:** Add a new column named `Missing_Count` that contains the count of missing values that were filled in each row. 4. **Return the DataFrame:** Return the modified DataFrame. Constraints: - Use pandas methods for handling missing data. - The DataFrame contains a mix of numeric and object data types. - You should not drop any columns or rows. Example: Given the initial DataFrame: ``` A B C 0 1.0 NaN A 1 2.0 2.1 NaN 2 NaN 3.5 C 3 4.0 NaN D 4 5.0 5.0 NaN 5 NaN 6.0 F 6 7.0 7.8 G ``` After running `process_missing_data(df)`, the expected output should be: ``` A B C Missing_Count 0 1.0 4.88 A 1 1 2.0 2.10 unknown 1 2 3.8 3.50 C 1 3 4.0 4.88 D 1 4 5.0 5.00 unknown 1 5 3.8 6.00 F 1 6 7.0 7.80 G 0 ``` Implementation Details: - Use `pd.isna()` or `pd.notna()` to detect missing values. - Utilize `fillna()` to impute missing values. - Use DataFrame operations to add the new column and ensure it correctly reflects the count of previously missing values.","solution":"import pandas as pd def process_missing_data(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Detect Missing Values missing_counts = df.isna().sum() print(f\\"Missing values in each column:n{missing_counts}\\") # Step 2: Fill Missing Data df_filled = df.copy() for column in df_filled.columns: if df_filled[column].dtype in [\'float64\', \'int64\']: mean_value = df_filled[column].mean() df_filled[column].fillna(mean_value, inplace=True) elif df_filled[column].dtype == \'object\': df_filled[column].fillna(\\"unknown\\", inplace=True) # Step 3: Add a New Column with the count of missing values filled in each row missing_count_per_row = df.isna().sum(axis=1) df_filled[\'Missing_Count\'] = missing_count_per_row # Step 4: Return the modified DataFrame return df_filled"},{"question":"Objective: Design a Python function that takes as input a text string, compresses it, and then decompresses it back to its original form using the `zlib` module. The function should ensure that the decompressed string matches the original string exactly. Test the function with a set of sample strings and print the results. Function Signature: ```python def compress_and_decompress(text: str, compression_level: int = -1) -> str: pass ``` Requirements: 1. **Input:** - `text` (str): The input text string to be compressed and decompressed. - `compression_level` (int): Compression level from 0 to 9, with -1 as the default representing Z_DEFAULT_COMPRESSION. 2. **Output:** - A string that is the result of compressing and then decompressing the input text. 3. **Constraints:** - The function should handle text strings of varying lengths. - Proper error handling should be implemented to catch any `zlib.error` exceptions during compression or decompression. - Ensure that the decompressed data matches the original input text exactly. 4. **Performance:** - The function should efficiently handle reasonably large text inputs (up to a few megabytes). Details: 1. Use the `zlib.compress` function to compress the input text string into bytes. 2. Utilize the `zlib.decompress` function to decompress the bytes back to the original string. 3. Implement and verify with different sample text inputs, ensuring that compression and decompression processes are accurately maintained. 4. Demonstrate and validate the function with at least three different sample strings. Example: ```python def compress_and_decompress(text: str, compression_level: int = -1) -> str: try: # Compress the input text compressed_data = zlib.compress(text.encode(\'utf-8\'), level=compression_level) # Decompress the compressed data decompressed_data = zlib.decompress(compressed_data).decode(\'utf-8\') return decompressed_data except zlib.error as e: return str(e) # Sample tests test_strings = [ \\"This is a simple test string.\\", \\"Another example with repeated text. \\" * 10, \\"Special characters! @#&(*&^%#@!~\\" ] for string in test_strings: result = compress_and_decompress(string) print(f\\"Original: {string}nDecompressed: {result}nMatch: {string == result}n\\") ```","solution":"import zlib def compress_and_decompress(text: str, compression_level: int = -1) -> str: try: # Compress the input text compressed_data = zlib.compress(text.encode(\'utf-8\'), level=compression_level) # Decompress the compressed data decompressed_data = zlib.decompress(compressed_data).decode(\'utf-8\') # Return the decompressed data return decompressed_data except zlib.error as e: return str(e)"},{"question":"# Named Tensors in PyTorch **Objective:** Implement and manipulate named tensors in PyTorch to perform typical tensor operations while maintaining the correct handling of tensor names. **Task:** 1. Create two named tensors: - `tensor1` of shape (2, 3) with names (\'batch\', \'features\') - `tensor2` of shape (2, 3) with names (\'batch\', \'features\') 2. Perform the following operations, ensuring the names are correctly handled: - Add the two tensors and return the named tensor. - Compute and return the sum across the \'batch\' dimension. - Transpose the resulting tensor from addition and return the named tensor with dimensions correctly permuted. - Create a new tensor from `tensor1`, by converting values to their absolute and return it with names preserved. - Normalize `tensor1` using the mean and standard deviation calculated along the \'features\' dimension and return the normalized tensor with dimension names. **Input:** - Two tensors with pre-defined shapes and names as mentioned above. **Output:** - A dictionary containing: - The tensor resulting from addition. - The sum across the \'batch\' dimension. - The transposed tensor. - The absolute values tensor. - The normalized tensor. **Constraints:** - Use the named tensor API effectively. - Ensure name propagation is handled as described in the documentation. **Example:** ```python import torch def manipulate_named_tensors(): # 1. Create named tensors tensor1 = torch.randn(2, 3, names=(\'batch\', \'features\')) tensor2 = torch.randn(2, 3, names=(\'batch\', \'features\')) # 2. Add tensors result_add = tensor1 + tensor2 # 3. Sum across \'batch\' result_sum = result_add.sum(\'batch\') # 4. Transpose the tensor from addition result_transpose = result_add.transpose(\'batch\', \'features\') # 5. Compute absolute values tensor result_abs = tensor1.abs() # 6. Normalize tensor1 along \'features\' mean = tensor1.mean(dim=\'features\', keepdim=True) std = tensor1.std(dim=\'features\', keepdim=True) result_normalize = (tensor1 - mean) / std return { \'result_add\': result_add, \'result_sum\': result_sum, \'result_transpose\': result_transpose, \'result_abs\': result_abs, \'result_normalize\': result_normalize } # Verify the test case output = manipulate_named_tensors() assert output[\'result_add\'].names == (\'batch\', \'features\') assert output[\'result_sum\'].names == (\'features\',) assert output[\'result_transpose\'].names == (\'features\', \'batch\') assert output[\'result_abs\'].names == (\'batch\', \'features\') assert output[\'result_normalize\'].names == (\'batch\', \'features\') ``` Ensure your code handles the named tensor operations correctly and maintains the appropriate tensor names as per the documentation.","solution":"import torch def manipulate_named_tensors(): # 1. Create named tensors tensor1 = torch.randn(2, 3, names=(\'batch\', \'features\')) tensor2 = torch.randn(2, 3, names=(\'batch\', \'features\')) # 2. Add tensors result_add = tensor1 + tensor2 # 3. Sum across \'batch\' dimension result_sum = result_add.sum(\'batch\') # 4. Transpose the tensor from addition result_transpose = result_add.transpose(\'batch\', \'features\') # 5. Compute absolute values tensor result_abs = tensor1.abs() # 6. Normalize tensor1 along \'features\' mean = tensor1.mean(dim=\'features\', keepdim=True) std = tensor1.std(dim=\'features\', keepdim=True) result_normalize = (tensor1 - mean) / std return { \'result_add\': result_add, \'result_sum\': result_sum, \'result_transpose\': result_transpose, \'result_abs\': result_abs, \'result_normalize\': result_normalize }"},{"question":"# Python Program to Check and Filter Keywords As part of this assessment, you need to write a function that processes a list of strings to identify and filter out the keywords and soft keywords in Python. You will use the `keyword` module\'s functionalities mentioned in the provided documentation. # Problem Statement Implement a function `filter_keywords(strings)` that takes a list of strings and returns a dictionary with three keys: - `\\"keywords\\"`: a list of strings that are Python keywords. - `\\"soft_keywords\\"`: a list of strings that are Python soft keywords. - `\\"non_keywords\\"`: a list of strings that are neither Python keywords nor soft keywords. Input - `strings`: A list of strings (each string is non-empty and contains only alphabetic characters). Output - A dictionary with the following structure: ```python { \\"keywords\\": [...], # list of Python keywords found in the input list \\"soft_keywords\\": [...], # list of Python soft keywords found in the input list \\"non_keywords\\": [...] # list of strings that are neither keywords nor soft keywords } ``` Example ```python import keyword def filter_keywords(strings): # Your implementation here # Sample input strings = [\\"if\\", \\"elif\\", \\"else\\", \\"while\\", \\"softkw\\", \\"return\\", \\"await\\", \\"async\\", \\"myvar\\", \\"global\\"] # Expected output output = { \\"keywords\\": [\\"if\\", \\"elif\\", \\"else\\", \\"while\\", \\"return\\", \\"async\\"], \\"soft_keywords\\": [\\"await\\"], \\"non_keywords\\": [\\"softkw\\", \\"myvar\\", \\"global\\"] } ``` # Constraints - The input list will contain between 1 and 1000 strings. - Each string in the input list will have at most 100 characters. - All elements of the input list are made up of alphabetic characters in lower case. # Performance Requirements - Your solution should be efficient and handle the maximum input size within practical limits. # Evaluation Criteria - Correctness: The solution should correctly classify the strings as keywords, soft keywords, or non-keywords. - Efficiency: The solution should handle large inputs effectively. - Code quality and readability: The code should be well-organized and easy to understand. You are expected to use the functionalities provided by the `keyword` module for this task.","solution":"import keyword def filter_keywords(strings): Classifies the list of strings into Python keywords, soft keywords, and non-keywords. Parameters: strings (list of str): List of strings to classify. Returns: dict: Dictionary with keys \'keywords\', \'soft_keywords\', and \'non_keywords\' and their respective lists. keywords = [] soft_keywords = [] non_keywords = [] # Python soft keywords (as per the latest version of Python) soft_kw_list = {\'match\', \'case\'} # Loop through the strings and categorize them for s in strings: if keyword.iskeyword(s): keywords.append(s) elif s in soft_kw_list: soft_keywords.append(s) else: non_keywords.append(s) return { \\"keywords\\": keywords, \\"soft_keywords\\": soft_keywords, \\"non_keywords\\": non_keywords }"},{"question":"<|Analysis Begin|> The provided documentation focuses on the handling of complex numbers within PyTorch. It covers the following key points: 1. **Basic Introduction to Complex Numbers**: Explains the form of complex numbers and their usage in mathematics and engineering. 2. **Complex Tensors in PyTorch**: - PyTorch supports complex dtypes: `torch.cfloat` and `torch.cdouble`. - Creating complex tensors with factory functions (except for a few). - Viewing real tensors as complex and vice versa using `torch.view_as_complex` and `torch.view_as_real`. 3. **Accessing Real and Imaginary Parts**: - Accessing the `real` and `imag` components of complex tensors. 4. **Angle and Absolute Values**: - Using `torch.angle` and `torch.abs` to compute angle and absolute values of complex tensors. 5. **Linear Algebra Operations**: - Many linear algebra functions support complex numbers. 6. **Serialization of Complex Tensors**: - Complex tensors can be serialized for saving data. 7. **Autograd Support**: - PyTorch supports autograd for complex tensors using Conjugate Wirtinger derivatives. 8. **Optimizers**: - Explanation of how optimizers handle complex tensors. Steps through an example with `torch.optim.AdamW`. 9. **Unsupported Subsystems**: - Certain subsystems like quantization, JIT, sparse tensors, and distributed are not fully supported. Based on this information, a question can be crafted to assess students\' understanding of complex tensors, their manipulation, and their application in PyTorch. <|Analysis End|> <|Question Begin|> # Question: Complex Tensors in PyTorch **Objective:** Write a PyTorch function that performs several operations on complex tensors, demonstrating your knowledge of PyTorch\'s handling of complex numbers. **Function Signature:** ```python def complex_tensor_operations(input_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: pass ``` **Input:** - `input_tensor` (torch.Tensor): A real tensor of shape `(n, 2)` representing complex numbers, where the last dimension contains the real and imaginary parts. **Output:** - Tuple containing the following three tensors: 1. Complex tensor created from the input real tensor. 2. Tensor of absolute values of the complex tensor. 3. Tensor of angles of the complex tensor. **Constraints:** 1. The function should handle input tensors with dtype `float32`. 2. You should use PyTorch operations to create and manipulate complex tensors. 3. No additional libraries other than PyTorch should be used. **Example:** ```python import torch from typing import Tuple def complex_tensor_operations(input_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: # Convert the input real tensor to a complex tensor complex_tensor = torch.view_as_complex(input_tensor) # Calculate the absolute values of the complex tensor abs_values = torch.abs(complex_tensor) # Calculate the angles of the complex tensor angles = torch.angle(complex_tensor) return complex_tensor, abs_values, angles # Test the function input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=torch.float32) complex_tensor, abs_values, angles = complex_tensor_operations(input_tensor) print(\\"Complex Tensor:\\", complex_tensor) print(\\"Absolute Values:\\", abs_values) print(\\"Angles:\\", angles) ``` **Expected Output:** ``` Complex Tensor: tensor([1.0000+2.0000j, 3.0000+4.0000j, 5.0000+6.0000j]) Absolute Values: tensor([2.2361, 5.0000, 7.8102]) Angles: tensor([1.1071, 0.9273, 0.8761]) ``` **Explanation:** - The function starts by converting the real tensor `input_tensor` to a complex tensor using `torch.view_as_complex`. - It then computes the absolute values of the complex tensor using `torch.abs`. - Finally, it calculates the angles of the complex tensor using `torch.angle`. Ensure your function follows the given constraints and produces the correct outputs.","solution":"import torch from typing import Tuple def complex_tensor_operations(input_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: # Ensure the input tensor has the correct shape and dtype assert input_tensor.shape[-1] == 2, \\"Input tensor must have shape (n, 2)\\" assert input_tensor.dtype == torch.float32, \\"Input tensor must have dtype float32\\" # Convert the input real tensor to a complex tensor complex_tensor = torch.view_as_complex(input_tensor) # Calculate the absolute values of the complex tensor abs_values = torch.abs(complex_tensor) # Calculate the angles of the complex tensor angles = torch.angle(complex_tensor) return complex_tensor, abs_values, angles"},{"question":"Coding Assessment Question # Objective Design a custom descriptor in Python demonstrating a deep understanding of the descriptor protocol. This task will test your knowledge of defining and using descriptors, a crucial concept in Python\'s object-oriented programming. # Problem Statement Implement a `CapitalizedString` descriptor class that ensures the attribute it manages always stores a capitalized string. You need to implement the descriptor methods: `__get__`, `__set__`, and `__delete__`. # Requirements 1. When setting the attribute, the descriptor should automatically convert the value to a capitalized string. 2. When getting the attribute, it should return the stored capitalized string. 3. Deleting the attribute should remove it from the object\'s dictionary. # Constraints - Ensure that the descriptor only works for string attributes; attempting to set non-string values should raise a `ValueError`. - You may assume the initial object does not have this attribute set. # Implementation Details Create a class named `CapitalizedString`. Implement the descriptor methods: `__get__`, `__set__`, and `__delete__`. Use the key name `_attr` to store the value in the instance\'s `__dict__`. # Example ```python class CapitalizedString: def __get__(self, instance, owner): # Implementation here pass def __set__(self, instance, value): # Implementation here pass def __delete__(self, instance): # Implementation here pass class MyClass: name = CapitalizedString() obj = MyClass() obj.name = \\"john doe\\" print(obj.name) # Output: John doe try: obj.name = 123 # Should raise ValueError: Only strings are allowed except ValueError as e: print(e) del obj.name print(hasattr(obj, \'_attr\')) # Output: False ``` # Submission Submit your implementation of the `CapitalizedString` class. # Evaluation Criteria - Correctness of the implementation. - Proper usage of descriptors. - Handling of non-string values with appropriate error raising. - Code readability and following Pythonic conventions.","solution":"class CapitalizedString: def __get__(self, instance, owner): return instance.__dict__.get(\'_attr\', None) def __set__(self, instance, value): if not isinstance(value, str): raise ValueError(\\"Only strings are allowed\\") instance.__dict__[\'_attr\'] = value.capitalize() def __delete__(self, instance): if \'_attr\' in instance.__dict__: del instance.__dict__[\'_attr\'] class MyClass: name = CapitalizedString() # Example usage: # obj = MyClass() # obj.name = \\"john doe\\" # print(obj.name) # John doe # try: # obj.name = 123 # Should raise ValueError: Only strings are allowed # except ValueError as e: # print(e) # del obj.name # print(hasattr(obj, \'_attr\')) # False"},{"question":"# Data Visualization and Customization with Seaborn In this assessment, you will demonstrate your understanding of using the Seaborn library for data visualization by creating a customized plot with text annotations and other visual elements. # Problem Statement You are provided with a dataset regarding model scores on various tasks. Your task is to: 1. Load and process the dataset. 2. Create a plot visualizing specific columns. 3. Add textual and graphical elements to your plot using advanced features of the Seaborn library. **Dataset Description:** The dataset `glue` contains the performance scores of different models on different tasks. The columns of the dataset include: - `Model`: The name of the model. - `Encoder`: The type of encoder used by the model. - `Task`: The name of the task. - `Score`: The performance score of the model on the task. **Requirements:** 1. **Load and Process the Dataset:** - Use `seaborn.load_dataset(\\"glue\\")` to load the dataset. - Pivot the dataset so that each row represents a `Model` and `Encoder`, and each column represents a `Task` with its corresponding `Score`. - Add a new column called `Average`, which contains the average score of each model across all tasks, rounded to one decimal place. - Sort the dataset based on the `Average` column in descending order. 2. **Create a Plot:** - Create a bar plot with the `Model` names on the y-axis and their `Average` scores on the x-axis. - Add the average score as a text annotation on each corresponding bar. - Fine-tune the plot by changing the text color to white and aligning it to the right of the bars. 3. **Add Text and Customize Plot:** - Create another plot mapping `SST-2` scores to the x-axis and `MRPC` scores to the y-axis, adding a third variable `Encoder` as the color. - Add text annotations with the `Model` names above each data point. - Customize text alignment and offset for better readability. # Input and Output Formats **Input:** None. The dataset is loaded directly in your code using `seaborn.load_dataset()`. **Output:** Two plots: 1. A bar plot of models with their average scores, including text annotations. 2. A scatter plot of `SST-2` vs. `MRPC` scores with `Model` names as text annotations, color-coded by `Encoder`. # Constraints: - Use only the Seaborn library and its `objects` interface for creating the plots. - Ensure that the plots are readable and visually appealing. # Performance Requirements: - The dataset transformations and plotting should be efficiently executed. # Example Code: Here is a skeleton code that you can complete to meet the requirements: ```python import seaborn.objects as so from seaborn import load_dataset # Load and process the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # 1. Create a bar plot with text annotations ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\")) ) # 2. Create a scatter plot with text annotations and custom alignment ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\", offset=6)) ) ``` Complete the code to create the required plots with all specified features.","solution":"import seaborn.objects as so from seaborn import load_dataset import seaborn as sns import pandas as pd # Load and process the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # 1. Create a bar plot with text annotations bar_plot = ( so.Plot(glue.reset_index(), x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar(), so.Agg()) .add(so.Text(color=\\"w\\", halign=\\"right\\")) ) bar_plot.show() # 2. Create a scatter plot with text annotations and custom alignment scatter_plot = ( so.Plot(glue.reset_index(), x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\", offset=6)) ) scatter_plot.show()"},{"question":"# PyTorch Custom Tensor Initialization Problem Statement You are tasked with implementing a custom tensor initialization function that will be compatible with both standard PyTorch and TorchScript. The function should create a tensor of specified shape and fill it with values according to the following criteria: 1. If the tensor type is \\"identity\\", create an identity matrix of the specified dimension. 2. If the tensor type is \\"random\\", create a tensor of the specified shape filled with random values sampled from a uniform distribution over [0, 1). 3. If the tensor type is \\"constant\\", create a tensor of the specified shape filled with the provided constant value. **Function Signature:** ```python def create_tensor(tensor_type: str, shape: tuple, value: float = 0.0, dtype: torch.dtype = torch.float32, device: torch.device = \'cpu\') -> torch.Tensor: pass ``` Input - `tensor_type` (str): Type of the tensor to create. Values can be \\"identity\\", \\"random\\", or \\"constant\\". - `shape` (tuple): Shape of the tensor to be created. For \\"identity\\", this should be a tuple (N, N). - `value` (float, optional): A constant value to fill the tensor with. Required if `tensor_type` is \\"constant\\". Default is 0.0. - `dtype` (torch.dtype, optional): The desired data type of the returned tensor. Default is `torch.float32`. - `device` (torch.device, optional): The device on which to place the tensor. Default is \'cpu\'. Output - Returns a tensor initialized according to the specified `tensor_type`. Constraints and Requirements - You must ensure that the function is compatible with TorchScript. - The function should raise a `ValueError` if `tensor_type` is not one of \\"identity\\", \\"random\\", or \\"constant\\", or if the shape is not valid for the specified tensor type. - Take care to correctly handle the `dtype` and `device` arguments to ensure the tensor is created on the correct device and in the correct format. Example ```python # Example usage: tensor1 = create_tensor(\\"identity\\", (3, 3)) tensor2 = create_tensor(\\"random\\", (2, 2)) tensor3 = create_tensor(\\"constant\\", (2, 3), value=5.0) print(tensor1) # tensor([[1., 0., 0.], # [0., 1., 0.], # [0., 0., 1.]]) print(tensor2) # tensor([[0.8729, 0.1904], # [0.6141, 0.7682]]) print(tensor3) # tensor([[5.0, 5.0, 5.0], # [5.0, 5.0, 5.0]]) ``` Additional Information - Use `torch.eye` for creating identity matrices. - Use `torch.rand` for creating random tensors. - Use `torch.full` for creating constant-valued tensors.","solution":"import torch def create_tensor(tensor_type: str, shape: tuple, value: float = 0.0, dtype: torch.dtype = torch.float32, device: torch.device = \'cpu\') -> torch.Tensor: if tensor_type == \\"identity\\": if len(shape) != 2 or shape[0] != shape[1]: raise ValueError(\\"For \'identity\' tensor_type, shape must be a square tuple (N, N).\\") return torch.eye(shape[0], dtype=dtype, device=device) elif tensor_type == \\"random\\": return torch.rand(shape, dtype=dtype, device=device) elif tensor_type == \\"constant\\": return torch.full(shape, value, dtype=dtype, device=device) else: raise ValueError(\\"tensor_type must be \'identity\', \'random\', or \'constant\'.\\")"},{"question":"In this coding assessment, you will demonstrate your understanding of the `seaborn` library, particularly focusing on the `FacetGrid` class and its various functionalities. You are required to perform the following tasks: 1. **Load a Dataset**: Use the `seaborn.load_dataset` function to load the `tips` dataset. 2. **Create a FacetGrid**: - Initialize a `FacetGrid` with `tips` data, arranging the plots by the `time` column on the x-axis and `sex` column on the y-axis. - Use `hue` to condition the data based on the `smoker` column. 3. **Map a Plotting Function**: - Use the `map_dataframe` method to create a scatter plot of `total_bill` vs. `tip`. - Ensure each facet has a reference line at the median value of the `tip` column. 4. **Customize the Plot**: - Set the axis labels to \\"Total Bill ()\\" and \\"Tip ()\\". - Give each plot a title based on its column and row name with templates \\"{col_name} Time\\" and \\"{row_name} Gender\\". - Customize the plot\'s appearance: set the size of each plot (`height`) to 4 and aspect ratio to 1. - Add a legend explaining the hue (i.e., smoker). Your function should be named `plot_facet_grid_with_customizations` and it should not take any parameters. The function should save the resulting plot as `facet_plot.png`. Expected Input and Output - The function does not require any input arguments. - The function should save a file named `facet_plot.png`. Constraints - The dataset and plots should be handled using `seaborn`. - Customizations should be made using `FacetGrid` class methods. Performance Requirements - The function should execute within a reasonable time frame, utilizing `seaborn` and `matplotlib`. ```python import seaborn as sns import matplotlib.pyplot as plt def plot_facet_grid_with_customizations(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\", hue=\\"smoker\\", height=4, aspect=1) # Create a scatter plot on each facet g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Add a reference line at the median value of the tip column median_tip = tips[\\"tip\\"].median() g.refline(y=median_tip) # Set axis labels g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") # Set titles for each plot g.set_titles(col_template=\\"{col_name} Time\\", row_template=\\"{row_name} Gender\\") # Add a legend g.add_legend() # Adjust layout and save the plot g.tight_layout() g.savefig(\\"facet_plot.png\\") # No need to call the function in the final code as the calling part will be handled by the assessment environment ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_facet_grid_with_customizations(): Loads the \'tips\' dataset and creates a FacetGrid with customizations: - Facets are arranged by \'time\' on the x-axis and \'sex\' on the y-axis. - Color hue is based on \'smoker\'. - Each facet has a scatter plot of \'total_bill\' vs. \'tip\'. - Each facet contains a reference line at the median \'tip\' value. - Customizes axis labels, plot titles, and adds a legend. - Saves the plot as \'facet_plot.png\'. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\", hue=\\"smoker\\", height=4, aspect=1) # Create a scatter plot on each facet g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Add a reference line at the median value of the tip column median_tip = tips[\\"tip\\"].median() g.refline(y=median_tip) # Set axis labels g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") # Set titles for each plot g.set_titles(col_template=\\"{col_name} Time\\", row_template=\\"{row_name} Gender\\") # Add a legend g.add_legend() # Adjust layout and save the plot g.tight_layout() g.savefig(\\"facet_plot.png\\") # Note: The function call is omitted as it is assumed to be called by the assessment environment."},{"question":"**Context:** You are given a directory containing several large parquet files. Each file contains time-series data with multiple columns, some of which are low-cardinality text data while others are numerical. Your task is to create an optimized, memory-efficient process to load and process this data by calculating some specific metrics. **Task:** Write a Python function using pandas to read and process the given parquet files using the following optimization strategies: 1. Load only the necessary columns. 2. Convert columns to efficient datatypes. 3. Process the data in chunks. Your function should: 1. Read all parquet files from the specified directory. 2. Load only the columns `\'id\'`, `\'name\'`, `\'x\'`, and `\'y\'`. 3. Convert the `\'name\'` column to a `Categorical` type and downcast numerical columns. 4. Process the data in memory-efficient chunks to calculate the mean of columns `\'x\'` and `\'y\'` for each unique `\'name\'`. **Input:** - `directory`: A string path to the directory containing the parquet files. - `columns`: A list of column names to be loaded from the files. Assume the required columns are always `[\'id\', \'name\', \'x\', \'y\']`. **Output:** - A pandas DataFrame with the unique `\'name\'` values as the index and the mean of `\'x\'` and `\'y\'` columns. **Constraints:** - You should ensure that the entire dataset is not loaded into memory at once; process it in chunks. - Use efficient datatypes to minimize memory usage. **Function Signature:** ```python import pandas as pd from typing import List def process_large_timeseries(directory: str, columns: List[str]) -> pd.DataFrame: pass ``` **Performance Requirements:** - Your solution should handle directories containing at least 100 parquet files each around 50MB in size without running out of memory. **Example:** ```python import pandas as pd from typing import List from pathlib import Path def process_large_timeseries(directory: str, columns: List[str]) -> pd.DataFrame: files = Path(directory).glob(\\"*.parquet\\") result = pd.DataFrame(columns=[\'name\', \'mean_x\', \'mean_y\']) for file in files: df = pd.read_parquet(file, columns=columns) df[\'name\'] = df[\'name\'].astype(\'category\') df[\'id\'] = pd.to_numeric(df[\'id\'], downcast=\'unsigned\') df[[\'x\', \'y\']] = df[[\'x\', \'y\']].apply(pd.to_numeric, downcast=\'float\') means = df.groupby(\'name\')[[\'x\', \'y\']].mean().rename(columns={\'x\': \'mean_x\', \'y\': \'mean_y\'}) result = result.add(means, fill_value=0) return result # Example usage: directory_path = \'data/timeseries/\' columns = [\'id\', \'name\', \'x\', \'y\'] result_df = process_large_timeseries(directory_path, columns) print(result_df) ```","solution":"import pandas as pd from typing import List from pathlib import Path def process_large_timeseries(directory: str, columns: List[str]) -> pd.DataFrame: files = Path(directory).glob(\\"*.parquet\\") result_dict = {} for file in files: iter_df = pd.read_parquet(file, columns=columns) iter_df[\'name\'] = iter_df[\'name\'].astype(\'category\') iter_df[\'id\'] = pd.to_numeric(iter_df[\'id\'], downcast=\'unsigned\') iter_df[[\'x\', \'y\']] = iter_df[[\'x\', \'y\']].apply(pd.to_numeric, downcast=\'float\') for name, group in iter_df.groupby(\'name\'): if name not in result_dict: result_dict[name] = {\'sum_x\': 0, \'sum_y\': 0, \'count\': 0} result_dict[name][\'sum_x\'] += group[\'x\'].sum() result_dict[name][\'sum_y\'] += group[\'y\'].sum() result_dict[name][\'count\'] += group[\'x\'].count() result_data = { \'name\': [], \'mean_x\': [], \'mean_y\': [] } for name, data in result_dict.items(): result_data[\'name\'].append(name) result_data[\'mean_x\'].append(data[\'sum_x\'] / data[\'count\']) result_data[\'mean_y\'].append(data[\'sum_y\'] / data[\'count\']) result_df = pd.DataFrame(result_data) return result_df"},{"question":"# Seaborn Theme and Display Configuration You have been provided with a dataset of monthly sales for an online store. Your task is to visualize this data using Seaborn\'s `seaborn.objects.Plot` class. The goal is to create a line plot with specific theme and display configurations. **Input:** - A Pandas DataFrame `sales_data` with two columns: `Month` (in \'YYYY-MM\' format) and `Sales` (integer values). **Output:** - A Seaborn line plot embedded in a Jupyter notebook with specified theme and display settings. # Requirements 1. **Theme Configuration:** - Set the background color of the axes to \'white\'. - Use the \'whitegrid\' style for the plot. - Ensure that the plot\'s theme is in sync with Matplotlib\'s global `rcParams`. 2. **Display Configuration:** - Display the plot in SVG format. - Disable HiDPI rendering. - Scale the plot image size to 0.7. # Example Usage ```python import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt # Sample sales data data = { \'Month\': [\'2023-01\', \'2023-02\', \'2023-03\', \'2023-04\', \'2023-05\'], \'Sales\': [150, 200, 250, 270, 300] } sales_data = pd.DataFrame(data) # Plotting function def configure_and_plot(sales_data): # Implement theme and display configuration here # Create and display the plot plot = so.Plot(sales_data, x=\'Month\', y=\'Sales\').add(so.Line()) plot.show() # Call the function configure_and_plot(sales_data) ``` # Constraints - Ensure you import the necessary libraries (`pandas`, `seaborn.objects as so`, `matplotlib.pyplot`). # Implementation Steps 1. Update `Plot.config.theme` to set the background color of the axes. 2. Apply the \'whitegrid\' style by updating the theme. 3. Sync Seaborn\'s plot theme with Matplotlib\'s `rcParams`. 4. Update display settings to output in SVG format and disable HiDPI. 5. Set the scaling factor to 0.7. 6. Create a line plot using the provided `sales_data` DataFrame. Implement the function `configure_and_plot(sales_data)` according to the requirements above.","solution":"import pandas as pd import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def configure_and_plot(sales_data): Configures seaborn theme and display settings, then plots sales data. Args: sales_data (pd.DataFrame): DataFrame containing \'Month\' and \'Sales\' columns. Returns: None # Set the theme sns.set_theme(style=\\"whitegrid\\") # Sync Seaborn\'s plot theme with Matplotlib\'s rcParams sns.plotting_context(\\"notebook\\", rc={\\"axes.facecolor\\": \\"white\\"}) # Update display settings to output in SVG format and disable HiDPI plt.switch_backend(\'svg\') plt.rcParams[\'figure.dpi\'] = 100 # Ensures HiDPI is disabled plt.rcParams[\'figure.figsize\'] = (7, 4) # Default scaling, to be changed # Set the scaling factor to 0.7 scaled_width = plt.rcParams[\'figure.figsize\'][0] * 0.7 scaled_height = plt.rcParams[\'figure.figsize\'][1] * 0.7 plt.rcParams[\'figure.figsize\'] = (scaled_width, scaled_height) # Create and display the plot plot = so.Plot(sales_data, x=\'Month\', y=\'Sales\').add(so.Line()) plot.show()"},{"question":"**Question:** You are required to create a Python script that processes a custom file based on given command-line arguments. The script should perform two main functions: reading content from a file and either counting the occurrence of each word in the file or finding the longest word in the file. Additionally, the script must support an optional verbosity flag to output additional debugging information. # Requirements: 1. **Functionality**: - The script should process command-line arguments. - It should accept a file path as a positional argument. - It should accept a `--count` flag to count occurrences of each word. - It should accept a `--longest` flag to find the longest word in the file. - It should accept a `-v/--verbose` flag to enable verbose output, printing the steps and intermediate results. - The script should ensure that either `--count` or `--longest` is provided, but not both simultaneously. 2. **Arguments**: - File Path (positional, a string representing the path to the file). - `--count` (optional, store true; counts the occurrences of each word). - `--longest` (optional, store true; finds the longest word). - `-v/--verbose` (optional, store true; enables verbose output). 3. **Constraints**: - You can assume the file is a plain text file. - Words are separated by whitespace. - The script should handle errors gracefully, such as missing file or no provided action (`--count` or `--longest`). 4. **Output**: - If `--count` is specified, print the word counts. - If `--longest` is specified, print the longest word. - If verbose is enabled, print debugging information along with the main result. # Steps to complete: 1. Import the necessary modules. 2. Create an `ArgumentParser` object. 3. Define the required positional and optional arguments. 4. Implement the main logic to handle the file operations and argument conditions. 5. Ensure proper error handling and input validation. 6. Implement the verbose mode to output additional debugging information. # Example: Assume we have a file `example.txt` with the following content: ``` hello world this is a test file hello world ``` Running the script: 1. Count word occurrences: ``` python script.py example.txt --count hello: 2 world: 2 this: 1 is: 1 a: 1 test: 1 file: 1 ``` 2. Find the longest word: ``` python script.py example.txt --longest longest word: \\"hello\\" (5 characters) ``` 3. Verbose mode: ``` python script.py example.txt --count -v Reading file: example.txt File read successful. Counting word occurrences... hello: 2 world: 2 this: 1 is: 1 a: 1 test: 1 file: 1 ``` **Your task is to implement this script, ensuring that it meets the above requirements and constraints.** # Solution Template: ```python import argparse def parse_arguments(): parser = argparse.ArgumentParser(description=\\"Process a file to count words or find the longest word.\\") parser.add_argument(\\"file_path\\", type=str, help=\\"Path to the text file.\\") group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\\"--count\\", action=\\"store_true\\", help=\\"Count occurrences of each word.\\") group.add_argument(\\"--longest\\", action=\\"store_true\\", help=\\"Find the longest word.\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Enable verbose output.\\") return parser.parse_args() def read_file(file_path, verbose): try: with open(file_path, \'r\') as file: content = file.read() if verbose: print(f\\"Reading file: {file_path}\\") print(\\"File read successful.\\") return content except Exception as e: if verbose: print(f\\"Error reading file: {e}\\") raise def count_words(content, verbose): words = content.split() word_count = {} for word in words: word_count[word] = word_count.get(word, 0) + 1 if verbose: print(\\"Counting word occurrences...\\") return word_count def find_longest_word(content, verbose): words = content.split() longest_word = max(words, key=len) if verbose: print(\\"Finding the longest word...\\") return longest_word def main(): args = parse_arguments() if args.verbose: print(f\\"Parsed arguments: {args}\\") content = read_file(args.file_path, args.verbose) if args.count: word_counts = count_words(content, args.verbose) for word, count in word_counts.items(): print(f\\"{word}: {count}\\") elif args.longest: longest_word = find_longest_word(content, args.verbose) print(f\\"longest word: \\"{longest_word}\\" ({len(longest_word)} characters)\\") # Ensure the script runs only when executed directly if __name__ == \\"__main__\\": main() ```","solution":"import argparse def parse_arguments(): parser = argparse.ArgumentParser(description=\\"Process a file to count words or find the longest word.\\") parser.add_argument(\\"file_path\\", type=str, help=\\"Path to the text file.\\") group = parser.add_mutually_exclusive_group(required=True) group.add_argument(\\"--count\\", action=\\"store_true\\", help=\\"Count occurrences of each word.\\") group.add_argument(\\"--longest\\", action=\\"store_true\\", help=\\"Find the longest word.\\") parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Enable verbose output.\\") return parser.parse_args() def read_file(file_path, verbose): try: with open(file_path, \'r\') as file: content = file.read() if verbose: print(f\\"Reading file: {file_path}\\") print(\\"File read successful.\\") return content except Exception as e: if verbose: print(f\\"Error reading file: {e}\\") raise def count_words(content, verbose): words = content.split() word_count = {} for word in words: word_count[word] = word_count.get(word, 0) + 1 if verbose: print(\\"Counting word occurrences...\\") return word_count def find_longest_word(content, verbose): words = content.split() longest_word = max(words, key=len) if verbose: print(\\"Finding the longest word...\\") return longest_word def main(): args = parse_arguments() if args.verbose: print(f\\"Parsed arguments: {args}\\") content = read_file(args.file_path, args.verbose) if args.count: word_counts = count_words(content, args.verbose) for word, count in word_counts.items(): print(f\\"{word}: {count}\\") elif args.longest: longest_word = find_longest_word(content, args.verbose) print(f\\"longest word: \\"{longest_word}\\" ({len(longest_word)} characters)\\") # Ensure the script runs only when executed directly if __name__ == \\"__main__\\": main()"},{"question":"Objective The purpose of this task is to test the student\'s ability to set up, manage, and utilize PyTorch\'s `torch.distributed` package for distributed training. The student should demonstrate their understanding of initializing a process group, performing collective operations, and managing process synchronization. Problem Statement You are tasked with designing a PyTorch script to run distributed training over multiple processes. Your script should achieve the following: 1. Initialize a process group using the `FileStore` backend. 2. Create a simple neural network and wrap it using `torch.nn.parallel.DistributedDataParallel`. 3. Perform an `all_reduce` operation to sum gradients across all processes. 4. Implement a barrier to synchronize the processes after the gradient sum operation. 5. Print the rank of each process and the result of the `all_reduce` operation. Function Signature ```python def run_distributed_training(file_store_path: str, world_size: int, rank: int, epochs: int) -> None: ``` Input - `file_store_path` (str): The file path used for the `FileStore` backend. - `world_size` (int): The total number of processes. - `rank` (int): The rank of the current process. - `epochs` (int): The number of training epochs. Instructions 1. **Initialization**: - Initialize the process group using `torch.distributed.init_process_group` with the `file://` method pointing to the `file_store_path`. - Ensure that each process is assigned a unique `rank`, and the total number of processes is specified by the `world_size`. 2. **Model Setup**: - Create a simple neural network (e.g., a single fully connected layer). - Wrap the model with `torch.nn.parallel.DistributedDataParallel`. 3. **Training Loop with All-reduce**: - Implement a simple training loop that runs for a specified number of `epochs`. - In each epoch: - Run a forward pass, compute the loss, and perform a backward pass. - Use the `torch.distributed.all_reduce` function to sum the gradients across all processes. 4. **Synchronization**: - Before starting the next epoch, call `torch.distributed.barrier()` to ensure all processes have completed the previous epoch. 5. **Output**: - Print the rank of each process and the result of the `all_reduce` operation on the gradient sum. Constraints - Assume the file system supports locking using `fcntl`. - Ensure the script can run in a distributed manner and handles initialization and shutdown of the process group properly. Example Usage Here is how you can use the function within a `multiprocessing` setup: ```python import torch.multiprocessing as mp def init_process(rank, world_size, file_store_path, epochs): run_distributed_training(file_store_path, world_size, rank, epochs) if __name__ == \\"__main__\\": world_size = 4 file_store_path = \\"/path/to/sharedfile\\" epochs = 5 mp.spawn(init_process, args=(world_size, file_store_path, epochs,), nprocs=world_size, join=True) ``` Notes - Ensure the function handles edge cases and errors gracefully, such as failed file store initialization or communication errors. - Use appropriate flags and synchronization mechanisms to ensure correct execution order and avoid race conditions.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp def setup(rank, world_size, file_store_path): os.environ[\'MASTER_ADDR\'] = \'127.0.0.1\' os.environ[\'MASTER_PORT\'] = \'29500\' dist.init_process_group( backend=\'gloo\', init_method=f\'file://{file_store_path}\', rank=rank, world_size=world_size ) def cleanup(): dist.destroy_process_group() def run_distributed_training(file_store_path: str, world_size: int, rank: int, epochs: int) -> None: setup(rank, world_size, file_store_path) # Create a simple model model = nn.Linear(10, 1) model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) # Loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Dummy input and target tensors input = torch.randn(10).cuda(rank) target = torch.randn(1).cuda(rank) for epoch in range(epochs): # Forward pass optimizer.zero_grad() output = model(input) loss = criterion(output, target) # Backward pass loss.backward() # Perform all reduce operation to sum gradients for param in model.parameters(): dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM) # Ensure all processes have done all_reduce dist.barrier() # Update weights optimizer.step() # Print the rank of process and gradient of the first parameter after all reduce if rank == 0: print(f\'Rank {rank}, Epoch {epoch}, Gradient: {param.grad.data}\') cleanup()"},{"question":"**Question: Creating Customized Seaborn Faceted and Paired Plots** You are provided with a dataset `penguins` which contains data on different species of penguins and their physical measurements. You need to use the `seaborn.objects` module to create two different types of plots: faceted plots and paired plots. Your task is to implement the necessary code to generate these plots with specific axis sharing behaviors. # Part 1: Faceted Plot Create a faceted plot of `bill_length_mm` versus `bill_depth_mm` where the data is faceted by `species` in columns and by `sex` in rows. Configure the plot such that the x-axis is shared across each column and the y-axis is shared across each row. # Part 2: Paired Plot Create a paired plot using the `flipper_length_mm` as the y-variable and `bill_length_mm` and `bill_depth_mm` as the x-variables. Ensure that the x-axes are shared across all the plots. # Input - `penguins`: a DataFrame containing the penguins dataset. The columns you will use are: - `bill_length_mm` - `bill_depth_mm` - `flipper_length_mm` - `species` - `sex` # Output - Two plots as specified in the tasks above. # Constraints - Only use the `seaborn.objects` interface. - Do not use any deprecated or removed features from Seaborn. - Ensure the code runs without any errors. # Example Code Here\'s a starting point for your solution: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Part 1: Faceted Plot faceted_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dots()) .share(x=\\"col\\", y=\\"row\\") ) # Display the faceted plot print(faceted_plot) # Part 2: Paired Plot paired_plot = ( so.Plot(penguins, y=\\"flipper_length_mm\\") .pair(x=[\\"bill_length_mm\\", \\"bill_depth_mm\\"]) .add(so.Dots()) .share(x=True) ) # Display the paired plot print(paired_plot) ``` Ensure your code correctly creates and displays the requested plots using the provided dataset.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Part 1: Faceted Plot faceted_plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(col=\\"species\\", row=\\"sex\\") .add(so.Dots()) .share(x=\\"col\\", y=\\"row\\") ) # Part 2: Paired Plot paired_plot = ( so.Plot(penguins, y=\\"flipper_length_mm\\") .pair(x=[\\"bill_length_mm\\", \\"bill_depth_mm\\"]) .add(so.Dots()) .share(x=True) ) # Display the faceted plot faceted_plot.show() # Display the paired plot paired_plot.show()"},{"question":"# Context Management and Asynchronous Programming with Context Variables Problem Statement You are tasked with developing a Python program that manages multiple tasks with their own separate states using context variables. The goal is to simulate an environment where different asynchronous tasks maintain their own local states and avoid unintended state leakage. You will achieve this by using the `contextvars` module. Requirements 1. Implement two functions: - `task_function(task_id)`: This function simulates an asynchronous task that updates its own context variable. * Input: integer `task_id` * Output: string message indicating the task id and the context variable value. - `main(num_tasks)`: This function creates and manages multiple asynchronous tasks, ensuring they each have their own separate state. * Input: integer `num_tasks` (number of tasks to create and run) * Output: List of output strings from each task indicating their respective states. 2. Use the `ContextVar` class to declare your context variable at the module level. 3. In each `task_function(task_id)`, set this context variable to the task\'s ID. 4. Use the `copy_context()` function to manage separate contexts for the tasks. 5. `main(num_tasks)` should instantiate and run the number of tasks specified by `num_tasks`. Constraints - The tasks should run concurrently using `asyncio` to test context isolation in an asynchronous environment. Example ```python import contextvars import asyncio # Declare context variable at module level task_context = contextvars.ContextVar(\'task_context\', default=-1) async def task_function(task_id: int) -> str: # Set context variable for this task task_context.set(task_id) # Simulate some processing (e.g., sleep for demonstration) await asyncio.sleep(1) # Return message with task id and context variable value return f\\"Task {task_id}: Context Value = {task_context.get()}\\" async def main(num_tasks: int) -> list: tasks = [] for task_id in range(num_tasks): # Create a new context for each task using `copy_context()` ctx = contextvars.copy_context() tasks.append(ctx.run(task_function, task_id)) # Gather and return results from all tasks return await asyncio.gather(*tasks) # Example usage if __name__ == \\"__main__\\": num_tasks = 4 results = asyncio.run(main(num_tasks)) for result in results: print(result) ``` **Expected Output** (example for `num_tasks = 4`): ```plaintext Task 0: Context Value = 0 Task 1: Context Value = 1 Task 2: Context Value = 2 Task 3: Context Value = 3 ``` In this example, you\'ll see that each task has isolated context values, demonstrating the use of `ContextVar` and `copy_context()` in concurrent task management using `asyncio`.","solution":"import contextvars import asyncio # Declare context variable at module level task_context = contextvars.ContextVar(\'task_context\', default=-1) async def task_function(task_id: int) -> str: Asynchronous task that sets and returns its context variable. # Set context variable for this task task_context.set(task_id) # Simulate some processing (e.g., sleep for demonstration) await asyncio.sleep(1) # Return message with task id and context variable value return f\\"Task {task_id}: Context Value = {task_context.get()}\\" async def main(num_tasks: int) -> list: Manages multiple asynchronous tasks maintaining separate states. tasks = [] for task_id in range(num_tasks): # Create a new context for each task using `copy_context()` ctx = contextvars.copy_context() tasks.append(ctx.run(task_function, task_id)) # Gather and return results from all tasks return await asyncio.gather(*tasks)"},{"question":"**Problem Statement:** Create a Python program using the `ossaudiodev` module to perform one of the following operations based on user input: 1. **Record and playback**: Record audio from the default input device, save it to a file, and then play back the recorded audio from the file. 2. **Live Audio Pass-Through**: Continuously read audio data from the input device and write it to the output device. Your program should: 1. Support setting audio parameters including format, channels, and sample rate. 2. Handle appropriate errors and exceptions. 3. Include a command-line interface to specify operation (record/play or pass-through) and audio parameters (format, channels, rate). # Requirements: - **Function Signature:** ```python def audio_manager(operation: str, format: int, channels: int, rate: int, duration: int = 10) -> None: pass ``` - `operation`: A string, either `\\"record_playback\\"` or `\\"pass_through\\"`. - `format`: An integer, specifying the audio format. - `channels`: An integer, specifying the number of audio channels. - `rate`: An integer, specifying the sampling rate. - `duration`: An integer, specifying duration for `record_playback` in seconds (default is 10). - Implement two helper functions: ```python def record_audio(format: int, channels: int, rate: int, duration: int) -> bytes: pass ``` ```python def play_audio(data: bytes, format: int, channels: int, rate: int) -> None: pass ``` - Constraints: - Use `AFMT_S16_LE` as the default format. - The default input device should be `\\"/dev/dsp\\"` and the default mixer device should be `\\"/dev/mixer\\"`. - Ensure the audio device handles blocking mode effectively. - **Expected Output Format:** The program should display error messages and status updates to the console. # Example: ```sh # Record and playback example python audio_manager.py record_playback AFMT_S16_LE 2 44100 5 # Live pass-through example python audio_manager.py pass_through AFMT_S16_LE 2 44100 ``` Handle errors appropriately by catching and displaying them. Don\'t forget to release any resources (i.e., close audio devices) in proper order.","solution":"import ossaudiodev import sys def audio_manager(operation: str, format: int, channels: int, rate: int, duration: int = 10) -> None: try: if operation == \\"record_playback\\": data = record_audio(format, channels, rate, duration) play_audio(data, format, channels, rate) elif operation == \\"pass_through\\": pass_through_audio(format, channels, rate) else: print(f\\"Invalid operation: {operation}\\") except Exception as e: print(f\\"Error: {e}\\") def record_audio(format: int, channels: int, rate: int, duration: int) -> bytes: dsp = ossaudiodev.open(\'r\') dsp.setfmt(format) dsp.channels(channels) dsp.speed(rate) audio_data = dsp.read(rate * channels * duration) dsp.close() return audio_data def play_audio(data: bytes, format: int, channels: int, rate: int) -> None: dsp = ossaudiodev.open(\'w\') dsp.setfmt(format) dsp.channels(channels) dsp.speed(rate) dsp.write(data) dsp.close() def pass_through_audio(format: int, channels: int, rate: int) -> None: dsp_in = ossaudiodev.open(\'r\') dsp_out = ossaudiodev.open(\'w\') dsp_in.setfmt(format) dsp_in.channels(channels) dsp_in.speed(rate) dsp_out.setfmt(format) dsp_out.channels(channels) dsp_out.speed(rate) try: while True: data = dsp_in.read(4096) if not data: break dsp_out.write(data) except KeyboardInterrupt: print(\\"Pass-through stopped by user\\") finally: dsp_in.close() dsp_out.close()"},{"question":"# Asynchronous Function Implementation in Python Objective: Create a function that demonstrates your understanding of asynchronous programming in Python. You will implement an asynchronous task that simulates a network request to fetch data and processes the data concurrently. Requirements: 1. Implement an asynchronous function `fetch_data(url: str) -> str` that simulates the fetching of data from the given URL. Use the `asyncio.sleep` method to simulate the delay of a network request. 2. Implement an asynchronous function `process_data(data: str) -> str` that processes the fetched data. The processing delay should also be simulated using `asyncio.sleep`. 3. Implement a main function `main()` that: - Concurrently fetches data from three different URLs. - Processes each piece of data as it is fetched. - Prints the result of each processed data. Constraints: - Use the `asyncio` library for implementing the asynchronous functions. - Simulate network and processing delays with random times between 1 to 3 seconds. Input and Output: - The `fetch_data` function should take a string URL as input and return a string as output. - The `process_data` function should take a string data as input and return a string as output. - The `main` function should be responsible for orchestrating the fetching and processing of data and should print the results of processed data. Example: ```python import asyncio import random async def fetch_data(url: str) -> str: await asyncio.sleep(random.randint(1, 3)) return f\\"data from {url}\\" async def process_data(data: str) -> str: await asyncio.sleep(random.randint(1, 3)) return f\\"processed {data}\\" async def main(): urls = [\'http://example.com/1\', \'http://example.com/2\', \'http://example.com/3\'] tasks = [fetch_data(url) for url in urls] for task in asyncio.as_completed(tasks): data = await task processed_data = await process_data(data) print(processed_data) # To run the main function if __name__ == \\"__main__\\": asyncio.run(main()) ``` This example code should print the processed data from each URL concurrently. The actual sleep times are determined randomly to simulate a real-world scenario with variable network and processing delays.","solution":"import asyncio import random async def fetch_data(url: str) -> str: await asyncio.sleep(random.randint(1, 3)) return f\\"data from {url}\\" async def process_data(data: str) -> str: await asyncio.sleep(random.randint(1, 3)) return f\\"processed {data}\\" async def main(): urls = [\'http://example.com/1\', \'http://example.com/2\', \'http://example.com/3\'] tasks = [fetch_data(url) for url in urls] for task in asyncio.as_completed(tasks): data = await task processed_data = await process_data(data) print(processed_data) # To run the main function if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Question: Custom DateTime Manipulation** You are required to implement a set of functions that utilizes the Python `datetime` module to perform the following tasks. Each function must be implemented in pure Python and showcase your understanding of utilizing datetime objects efficiently. 1. **Function 1: `create_custom_datetime`** - **Input**: Four integers representing `year`, `month`, `day`, `hour`, `minute`. - **Output**: A `datetime` object combining the input parameters. - **Constraints**: - `year` should be between 1 and 9999 - `month` should be between 1 and 12 - `day` should be valid for the given month and year - `hour` should be between 0 and 23 - `minute` should be between 0 and 59 2. **Function 2: `is_valid_date`** - **Input**: Three integers representing `year`, `month`, `day`. - **Output**: A boolean that returns `True` if the date is valid, and `False` otherwise. - **Constraints**: - Use the `datetime` moduleâ€™s capabilities to validate the date. 3. **Function 3: `extract_date_time_fields`** - **Input**: A `datetime` object. - **Output**: A dictionary with the following keys: `year`, `month`, `day`, `hour`, `minute`. The values should correspond to the respective fields extracted from the input `datetime` object. 4. **Function 4: `time_difference_in_seconds`** - **Input**: Two `datetime` objects. - **Output**: An integer representing the absolute difference between the two datetimes, in seconds. 5. **Function 5: `convert_to_utc`** - **Input**: A `datetime` object with a time zone. - **Output**: A new `datetime` object representing the input datetime in UTC. - **Constraints**: - If the input does not have a time zone, raise a `ValueError`. **Example Usage:** ```python from datetime import datetime, timezone, timedelta # create_custom_datetime dt = create_custom_datetime(2021, 5, 21, 14, 30) print(dt) # Output: 2021-05-21 14:30:00 # is_valid_date print(is_valid_date(2021, 2, 29)) # Output: False # extract_date_time_fields result = extract_date_time_fields(dt) print(result) # Output: {\'year\': 2021, \'month\': 5, \'day\': 21, \'hour\': 14, \'minute\': 30} # time_difference_in_seconds dt1 = datetime(2021, 5, 21, 14, 30) dt2 = datetime(2021, 5, 22, 14, 30) print(time_difference_in_seconds(dt1, dt2)) # Output: 86400 # convert_to_utc dt_with_tz = datetime(2021, 5, 21, 14, 30, tzinfo=timezone(timedelta(hours=3))) utc_dt = convert_to_utc(dt_with_tz) print(utc_dt) # Output: 2021-05-21 11:30:00+00:00 ``` **Notes:** - Make sure to handle any edge cases or potential errors gracefully. - Do not use any external libraries besides `datetime`. - Ensure that your code is clean, well-documented, and follows PEP 8 guidelines. **Submission:** Provide a Python file (`.py`) containing the implementations of the five functions described above.","solution":"from datetime import datetime, timezone def create_custom_datetime(year, month, day, hour, minute): Returns a datetime object with the specified year, month, day, hour, and minute. :param year: int between 1 and 9999 :param month: int between 1 and 12 :param day: int between 1 and 31, valid for the specified month/year :param hour: int between 0 and 23 :param minute: int between 0 and 59 :return: datetime object return datetime(year, month, day, hour, minute) def is_valid_date(year, month, day): Returns True if the specified year, month, and day constitute a valid date, else False. :param year: int between 1 and 9999 :param month: int between 1 and 12 :param day: int between 1 and 31, valid for the specified month/year :return: boolean try: datetime(year, month, day) return True except ValueError: return False def extract_date_time_fields(dt): Returns a dictionary containing the year, month, day, hour, and minute extracted from the input datetime object. :param dt: datetime object :return: dictionary with keys: \'year\', \'month\', \'day\', \'hour\', \'minute\' return { \'year\': dt.year, \'month\': dt.month, \'day\': dt.day, \'hour\': dt.hour, \'minute\': dt.minute } def time_difference_in_seconds(dt1, dt2): Returns the absolute difference in seconds between two datetime objects. :param dt1: datetime object :param dt2: datetime object :return: int (difference in seconds) return abs(int((dt2 - dt1).total_seconds())) def convert_to_utc(dt): Returns a datetime object converted to UTC from the given timezone-aware datetime object. :param dt: timezone-aware datetime object :return: datetime object in UTC :raises ValueError: if dt is not timezone-aware if dt.tzinfo is None: raise ValueError(\\"The datetime object must be timezone-aware.\\") return dt.astimezone(timezone.utc)"},{"question":"Objective Your task is to demonstrate your understanding of the seaborn library by creating a multifunctional visualization using `sns.jointplot`. This requires producing a scattered visualization with additional customization and layer enhancements, utilizing multiple seaborn and matplotlib functionalities. Task Write a function `visualize_penguins` that takes no parameters and performs the following steps: 1. **Load the `penguins` dataset** using seabornâ€™s `load_dataset` function. 2. **Create a joint plot** using `sns.jointplot`, where the x-axis represents the `bill_length_mm` field and the y-axis represents the `bill_depth_mm` field. Color the data points by the `species` field. 3. **Set the plot kind** to `kde` to represent bivariate KDEs. 4. Add another density plot as a **joint layer** to the main plot using `sns.kdeplot` with a color `red` and specify `zorder=0` and `levels=6`. 5. Add marginal **rug plots** to the margins with a color `red`. # Expected Output Your function should display a joint plot with the specified customizations and enhancements as described above. There is no return value for this function, as it should only produce the plot. # Requirements - After creating the initial joint plot, use the `jointplot` object returned to add further layers. - Ensure that all plots and additions are visible and clear. - The `kde` and rug plots must be displayed in red color. # Constraints 1. Use `seaborn` library version 0.11.0 or later. 2. You must use `sns.jointplot` and manipulate the returned `JointGrid` object to add additional layers. 3. Ensure that this is a self-contained function. ```python def visualize_penguins(): # Your solution and seaborn visualizations go here ``` Example An expected final visualization should include: - A joint KDE plot color coded by species. - An added density layer in red. - Marginal rug plots in red on both x and y axes. This question assesses your ability to use seaborn effectively, create complex visualizations, and customize plots using both seaborn and matplotlib functionalities.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the `penguins` dataset penguins = sns.load_dataset(\'penguins\') # Create a joint plot with bivariate KDE, color by species g = sns.jointplot( data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', kind=\'kde\' ) # Add a density plot with custom settings sns.kdeplot( data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', color=\'red\', zorder=0, levels=6, ax=g.ax_joint ) # Add marginal rug plots with custom settings sns.rugplot( data=penguins, x=\'bill_length_mm\', color=\'red\', ax=g.ax_marg_x ) sns.rugplot( data=penguins, y=\'bill_depth_mm\', color=\'red\', ax=g.ax_marg_y ) plt.show()"},{"question":"# Question: Implement a Dynamic Function Dispatcher in Python You are tasked with implementing a dynamic function dispatcher, which can call a function with both positional and keyword arguments. The dispatcher should be able to: 1. Handle a function with no arguments. 2. Handle a function with only positional arguments. 3. Handle a function with only keyword arguments. 4. Handle a function with both positional and keyword arguments. # Function Signature ```python def dynamic_dispatcher(callable_func, *args, **kwargs): pass ``` # Input - `callable_func`: A Python callable (function or method). - `*args`: Variable length tuple containing positional arguments to be passed to `callable_func`. - `**kwargs`: Variable length dictionary containing keyword arguments to be passed to `callable_func`. # Output - The return value from invoking `callable_func` with the provided arguments. # Constraints - The `callable_func` will always be a valid callable. - The `args` can range from zero to ten positional arguments. - The `kwargs` can contain between zero to ten key-value pairs of keyword arguments. # Example ```python def example_function(a, b, c=0, d=1): return a + b + c + d # Example usage result = dynamic_dispatcher(example_function, 2, 3, d=4) print(result) # Output should be 9 (2 + 3 + 0 + 4) ``` # Requirements - The function should call `callable_func` using the provided positional and keyword arguments. - Raise a `TypeError` if `callable_func` is not callable. - Ensure that the function handles typical errors with appropriate messaging. Implement the `dynamic_dispatcher` function in Python.","solution":"def dynamic_dispatcher(callable_func, *args, **kwargs): Dynamically dispatches a callable function with provided positional and keyword arguments. Parameters: callable_func (callable): The function to be called. *args: Positional arguments to pass to the function. **kwargs: Keyword arguments to pass to the function. Returns: The return value from the callable function. if not callable(callable_func): raise TypeError(\\"The provided argument is not callable\\") return callable_func(*args, **kwargs)"},{"question":"**Memory Management in Python** **Objective:** Your task is to implement a small Python/C extension module that demonstrates the use of different memory management functions provided by Python\'s memory manager. **Requirements:** 1. Create a C function `allocate_and_initialize_buffer` that: - Takes an integer `size` as input. - Allocates a buffer of the given size using `PyMem_Malloc`. - Initializes all bytes in the buffer to a specific value (e.g., 0xAA). - Returns the allocated buffer. 2. Create a C function `resize_buffer` that: - Takes a previously allocated buffer and a new size as inputs. - Resizes the buffer using `PyMem_Realloc`. - Returns the resized buffer. 3. Create a C function `free_buffer` that: - Takes a previously allocated buffer as input. - Frees the buffer using `PyMem_Free`. 4. Create a C function `sum_buffer` that: - Takes a buffer and its size as inputs. - Returns the sum of all bytes in the buffer. **Input:** - Example size for allocation: `1024 bytes` - Example buffer input for sum calculation: A previously allocated buffer. **Output:** - `allocate_and_initialize_buffer` should return a pointer to the allocated and initialized buffer. - `resize_buffer` should return a pointer to the resized buffer. - `free_buffer` should perform the deallocation operation without returning anything. - `sum_buffer` should return the sum of all bytes in the buffer. **Constraints:** - You must ensure that the buffer allocation and deallocation use the same family of functions. - All memory operations must handle error conditions gracefully. **Performance Requirements:** - The implemented functions should handle buffers of sizes up to 10 MB efficiently. **Example:** ```c #include <Python.h> static PyObject* allocate_and_initialize_buffer(PyObject* self, PyObject* args) { int size; if (!PyArg_ParseTuple(args, \\"i\\", &size)) return NULL; char *buffer = (char*) PyMem_Malloc(size); if (buffer == NULL) return PyErr_NoMemory(); for (int i = 0; i < size; i++) buffer[i] = 0xAA; return PyBytes_FromStringAndSize(buffer, size); } static PyObject* resize_buffer(PyObject* self, PyObject* args) { PyObject *py_buffer; int new_size; if (!PyArg_ParseTuple(args, \\"Oi\\", &py_buffer, &new_size)) return NULL; char *buffer = (char*) PyBytes_AsString(py_buffer); buffer = (char*) PyMem_Realloc(buffer, new_size); if (buffer == NULL) return PyErr_NoMemory(); return PyBytes_FromStringAndSize(buffer, new_size); } static PyObject* free_buffer(PyObject* self, PyObject* args) { PyObject *py_buffer; if (!PyArg_ParseTuple(args, \\"O\\", &py_buffer)) return NULL; char *buffer = (char*) PyBytes_AsString(py_buffer); PyMem_Free(buffer); Py_RETURN_NONE; } static PyObject* sum_buffer(PyObject* self, PyObject* args) { PyObject *py_buffer; if (!PyArg_ParseTuple(args, \\"O\\", &py_buffer)) return NULL; char *buffer = (char*) PyBytes_AsString(py_buffer); Py_ssize_t size = PyBytes_Size(py_buffer); int sum = 0; for (Py_ssize_t i = 0; i < size; i++) sum += buffer[i]; return PyLong_FromLong(sum); } static PyMethodDef MemoryMethods[] = { {\\"allocate_and_initialize_buffer\\", allocate_and_initialize_buffer, METH_VARARGS, \\"Allocate and initialize a buffer\\"}, {\\"resize_buffer\\", resize_buffer, METH_VARARGS, \\"Resize a buffer\\"}, {\\"free_buffer\\", free_buffer, METH_VARARGS, \\"Free a buffer\\"}, {\\"sum_buffer\\", sum_buffer, METH_VARARGS, \\"Sum the values in a buffer\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef memorymodule = { PyModuleDef_HEAD_INIT, \\"memorymodule\\", \\"Python C Extension for Memory Management\\", -1, MemoryMethods, }; PyMODINIT_FUNC PyInit_memorymodule(void) { return PyModule_Create(&memorymodule); } ``` The provided functions can be called from Python after the module is compiled and imported. **Task:** 1. Implement the given C functions. 2. Compile the C code into a Python extension module. 3. Write a Python script to test all the functions in the module.","solution":"# Note: The provided code is a Python wrapper and requires C implementation. # Let\'s create a simulated Python versions of the expected behavior for testing purposes. def allocate_and_initialize_buffer(size): Allocate a buffer of the given size and initialize all bytes to 0xAA. buffer = bytearray([0xAA] * size) return buffer def resize_buffer(buffer, new_size): Resize the buffer to the new size. if new_size < len(buffer): return buffer[:new_size] else: return buffer + bytearray([0xAA] * (new_size - len(buffer))) def free_buffer(buffer): Free the buffer. In Python, this is handled by the garbage collector. del buffer def sum_buffer(buffer): Return the sum of all bytes in the buffer. return sum(buffer)"},{"question":"**Problem Statement:** You are provided with a dataset containing the performance scores of different machine learning models on different tasks. Your task is to create a heatmap that will allow an easy comparison of these scores across different models and tasks. **Dataset Example:** ```plaintext | Model | Task A | Task B | Task C | |------------|--------|--------|--------| | Model 1 | 90 | 85 | 70 | | Model 2 | 80 | 77 | 65 | | Model 3 | 88 | 80 | 60 | ``` **Requirements:** 1. Load the dataset into a pandas DataFrame. 2. Create a heatmap using seaborn to visualize the scores. 3. Annotate the cells with the respective scores. 4. Use a colormap of your choice. 5. Add lines between the cells for better visibility. 6. Set the colormap to normalize between the values 50 and 100. 7. Customize the plot by setting the x-axis label to \\"Tasks\\" and y-axis label to \\"Models\\". 8. Move the x-axis ticks to the top of the heatmap. **Constraints:** - You must use the `seaborn` and `pandas` libraries to perform all operations. - Your solution should be able to handle any number of models and tasks provided in the format similar to the example dataset. **Input:** - A CSV file named \\"model_performance.csv\\" with columns representing different tasks and rows representing different models. The first column contains the model names, and the subsequent columns contain the performance scores for different tasks. **Output:** - A heatmap visualization displayed using `matplotlib` showing the annotated performance scores. **Function Signature:** ```python def create_heatmap(csv_filepath: str) -> None: pass ``` **Example Call:** ```python create_heatmap(\\"model_performance.csv\\") ``` This will display a heatmap based on the data from the \\"model_performance.csv\\" file. **Additional Notes:** - Ensure that the heatmap is properly labeled and visually appealing. - You can explore different colormap options in Seaborn to enhance the visualization.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_heatmap(csv_filepath: str) -> None: # Load the dataset into a pandas DataFrame df = pd.read_csv(csv_filepath) # Set model names as index df.set_index(\'Model\', inplace=True) # Create the heatmap plt.figure(figsize=(10, 8)) sns.heatmap(df, annot=True, fmt=\\"d\\", cmap=\\"YlGnBu\\", linecolor=\'black\', linewidths=0.5, cbar_kws={\\"boundaries\\": range(50, 101)}) # Customize the plot plt.xlabel(\'Tasks\') plt.ylabel(\'Models\') plt.xticks(rotation=45) plt.gca().xaxis.tick_top() plt.gca().xaxis.set_label_position(\'top\') # Display the heatmap plt.show()"},{"question":"# Advanced Python Module Manipulation **Objective**: Assess the ability to effectively utilize Python\'s module manipulation functions from the C API context. **Problem Statement**: You are tasked with creating a Python extension module named `modutils` which provides advanced capabilities to manage and manipulate other Python modules from within a C extension. Implement the following functions in `modutils`: 1. `import_module(name: str) -> Module`: - This function should import a module by its name using `PyImport_ImportModule`. - If the module cannot be imported, it should raise a `ModuleNotFoundError`. 2. `reload_module(module: Module) -> Module`: - This function should reload an already imported module using `PyImport_ReloadModule`. - If the module cannot be reloaded, it should raise a `RuntimeError`. 3. `get_module_dict() -> Dict[str, Module]`: - This function should retrieve the current modules dictionary using `PyImport_GetModuleDict`. - It should return this dictionary as a standard Python dictionary. 4. `exec_code(name: str, code: str) -> Module`: - This function should compile the given string of code using Python\'s `compile()` function, and then execute the code object within the context of a module specified by `name` using `PyImport_ExecCodeModule`. - If the execution fails, it should raise a `RuntimeError`. **Requirements**: - You should use the C API as described in the provided documentation to implement these functions. - Proper error handling should be in place to manage and report any issues that may arise during the module operations. - The `modutils` module should be implemented as a C extension and compiled ready for use in Python. **Usage Example**: ```python import modutils # Import a module numpy_module = modutils.import_module(\'numpy\') print(numpy_module) # Reload the module reloaded_numpy = modutils.reload_module(numpy_module) print(reloaded_numpy) # Get all currently imported modules modules_dict = modutils.get_module_dict() print(modules_dict) # Execute code within the context of a module module_with_code = modutils.exec_code(\'my_module\', \'def hello(): return \\"hello\\"\') print(module_with_code.hello()) ``` **Constraints**: - The functions should be designed for Python 3.10 or later. - Ensure memory allocations and references are managed correctly, avoiding leaks. **Submission**: Submit your completed `modutils` C extension module along with a brief documentation detailing how to compile and use the module.","solution":"import importlib.util import sys import types def import_module(name: str) -> types.ModuleType: Import a module by its name. try: module = importlib.import_module(name) return module except ImportError: raise ModuleNotFoundError(f\\"Module named \'{name}\' not found\\") def reload_module(module: types.ModuleType) -> types.ModuleType: Reload an already imported module. if not isinstance(module, types.ModuleType): raise TypeError(\\"Argument must be a module\\") try: reloaded_module = importlib.reload(module) return reloaded_module except Exception as e: raise RuntimeError(f\\"Error reloading module: {e}\\") def get_module_dict() -> dict: Retrieve the current modules dictionary. return dict(sys.modules) def exec_code(name: str, code: str) -> types.ModuleType: Compile and execute code within the context of a module specified by name. module_spec = importlib.util.spec_from_loader(name, loader=None) module = importlib.util.module_from_spec(module_spec) exec(code, module.__dict__) sys.modules[name] = module return module"},{"question":"You are required to implement a custom interactive console in Python that extends the behavior of Python\'s default interactive console. Your console should provide additional functionality to keep track of executed commands, print a history of these commands when requested, and allow executing commands from the history. Requirements: 1. Create a class `CustomInteractiveConsole` that inherits from `code.InteractiveConsole`. 2. Add functionality to store each successfully executed command in a list called `history`. 3. Implement a command `!history` that, when entered, prints all previously executed commands. 4. Implement a command `!exec <n>` that, when entered, re-executes the `n`th command from the history. Input and Output Formats: 1. The console should read input commands from the user continuously until an exit command `exit()` or `Ctrl-D` (EOF) is encountered. 2. On entering `!history`, the console should print the history of executed commands numbered from 0. 3. On entering `!exec <n>`, the console should re-execute the `n`th command from the history if it exists or print an error message if it does not. Constraints: 1. The `history` command and `exec` commands should not be added to the history list. 2. The `exec` command should handle the index out of range gracefully by printing an appropriate error message. Example Behavior: ``` >>> print(\'Hello, World!\') Hello, World! >>> a = 10 >>> a 10 >>> !history 0: print(\'Hello, World!\') 1: a = 10 2: a >>> !exec 0 Hello, World! >>> !exec 2 10 ``` Additional Notes: - Efficient error handling and clear user guidance are essential components of your solution. - Use the `push` method to handle each line of input provided to the console. ```python import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.history = [] def push(self, line): if line.startswith(\\"!history\\"): self.print_history() elif line.startswith(\\"!exec\\"): self.execute_history(line) else: if super().push(line) == False: self.history.append(line) def print_history(self): for index, command in enumerate(self.history): print(f\\"{index}: {command}\\") def execute_history(self, line): try: _, num = line.split() num = int(num) if num < 0 or num >= len(self.history): print(\\"Error: Command number out of range.\\") else: self.push(self.history[num]) except (ValueError, IndexError): print(\\"Error: Invalid command syntax. Use !exec <number>.\\") if __name__ == \\"__main__\\": console = CustomInteractiveConsole() console.interact(banner=\\"Custom Interactive Console. Type \'exit()\' or Ctrl-D to exit.\\") ```","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.history = [] def push(self, line): if line.startswith(\\"!history\\"): self.print_history() elif line.startswith(\\"!exec\\"): self.execute_history(line) else: if super().push(line) == False: self.history.append(line) def print_history(self): for index, command in enumerate(self.history): print(f\\"{index}: {command}\\") def execute_history(self, line): try: _, num = line.split() num = int(num) if num < 0 or num >= len(self.history): print(\\"Error: Command number out of range.\\") else: self.push(self.history[num]) except (ValueError, IndexError): print(\\"Error: Invalid command syntax. Use !exec <number>.\\")"},{"question":"Objective Your task is to write a Python function utilizing the seaborn package to visualize complex relationships within a dataset. This will test your understanding of Seaborn\'s plotting capabilities, including customization, conditional distributions, and bivariate plots. Function Signature ```python def visualize_data(dataset: str, numerical_vars: list, categorical_var: str, plot_type: str, **kwargs) -> None: pass ``` Parameters - `dataset`: A string indicating the name of the dataset to be loaded using `seaborn.load_dataset`. Available options include \\"tips\\", \\"iris\\", \\"diamonds\\", etc. - `numerical_vars`: A list containing the names of numerical variables (one or two) for which to plot the distributions. - `categorical_var`: The name of a categorical variable for conditional distribution plots using hue mapping. - `plot_type`: A string indicating the type of plot to create. Valid options include \\"univariate\\", \\"bivariate\\", and \\"conditional\\". - `kwargs`: Additional keyword arguments to customize the seaborn plot. Constraints - The function should handle different types of plots based on the provided `plot_type`: - `univariate`: Plot a univariate distribution of a single numerical variable. - `bivariate`: Plot a bivariate distribution of two numerical variables. - `conditional`: Plot a conditional distribution with hue mapping based on a categorical variable. Output The function should not return any value but should directly display the appropriate plot using seaborn. Example Usage ```python # Univariate distribution plot visualize_data(\\"iris\\", numerical_vars=[\\"sepal_length\\"], categorical_var=\\"species\\", plot_type=\\"univariate\\", bw_adjust=0.5, fill=True) # Bivariate distribution plot visualize_data(\\"geyser\\", numerical_vars=[\\"waiting\\", \\"duration\\"], categorical_var=\\"kind\\", plot_type=\\"bivariate\\", fill=True, cmap=\\"mako\\") # Conditional distribution plot visualize_data(\\"tips\\", numerical_vars=[\\"total_bill\\"], categorical_var=\\"time\\", plot_type=\\"conditional\\", multiple=\\"fill\\") ``` Additional Notes - Ensure that `seaborn` and any other necessary packages (such as `pandas` and `matplotlib`) are imported within your function. - Use appropriate seaborn functionality (`sns.kdeplot`) and customization options to create the plots. - Handle edge cases where the dataset, numerical variables, or categorical variables provided do not exist or are not valid for the specified `plot_type`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_data(dataset: str, numerical_vars: list, categorical_var: str, plot_type: str, **kwargs) -> None: # Load the dataset data = sns.load_dataset(dataset) # Determine the type of plot if plot_type == \\"univariate\\": if len(numerical_vars) != 1: raise ValueError(\\"Univariate plots require exactly one numerical variable.\\") sns.kdeplot(data=data, x=numerical_vars[0], **kwargs) elif plot_type == \\"bivariate\\": if len(numerical_vars) != 2: raise ValueError(\\"Bivariate plots require exactly two numerical variables.\\") sns.kdeplot(data=data, x=numerical_vars[0], y=numerical_vars[1], **kwargs) elif plot_type == \\"conditional\\": if len(numerical_vars) != 1: raise ValueError(\\"Conditional plots require exactly one numerical variable.\\") sns.kdeplot(data=data, x=numerical_vars[0], hue=categorical_var, **kwargs) else: raise ValueError(\\"Invalid plot type. Valid options are \'univariate\', \'bivariate\', and \'conditional\'.\\") # Show the plot plt.show()"},{"question":"# Question: You are given a directory structure containing various files and subdirectories. Your task is to write a function that leverages the `glob` module to perform several pattern matching operations, as specified below. Function Signature: ```python def pattern_matching_operations(base_directory: str) -> dict: pass ``` Input: - `base_directory` (str): The base directory where the pattern matching operations will be performed. This can be a relative or an absolute path. Output: - Returns a dictionary with the following keys and corresponding values: - `\'gif_files\'`: A list of all `.gif` files in the base directory and all its subdirectories. - `\'single_character_txt_files\'`: A list of `.txt` files in the base directory that have a single character name (excluding extension), but not in its subdirectories. - `\'all_files\'`: A list of all files and directories in the base directory and its subdirectories, excluding hidden files and directories (names starting with a dot). - `\'escaped_files\'`: A list of pattern-matched files where special characters `\'*\', \'?\', \'[\'` are used literally. Constraints: - You may assume that the input directory and all files within it are accessible and readable. - The input directory may contain symbolic links. Example Directory Structure: ``` base_dir/ 1.gif 2.txt card.gif sub/ 3.txt ?special?.txt another? ``` Example Output: ```python { \'gif_files\': [\'1.gif\', \'card.gif\'], \'single_character_txt_files\': [\'2.txt\'], \'all_files\': [\'base_dir/1.gif\', \'base_dir/2.txt\', \'base_dir/card.gif\', \'base_dir/sub\', \'base_dir/sub/3.txt\'], \'escaped_files\': [\'base_dir/?special?.txt\'] } ``` Use the `glob` module to perform the operations: 1. For `\'gif_files\'`, use the pattern `\'**/*.gif\'` with the `recursive=True` option. 2. For `\'single_character_txt_files\'`, use the pattern `\'?.txt\'` without recursion. 3. For `\'all_files\'`, use the pattern `\'**/*\'` with the `recursive=True` option, ensuring you exclude hidden files and directories. 4. For `\'escaped_files\'`, use the `glob.escape` function to escape special characters and match them literally. Notes: - Remember to import the `glob` module and any other necessary modules at the beginning of your script. - Make sure your solution is efficient, especially for large directory trees when using recursive patterns.","solution":"import glob import os def pattern_matching_operations(base_directory: str) -> dict: # Match all .gif files in the base directory and its subdirectories gif_files = glob.glob(os.path.join(base_directory, \'**\', \'*.gif\'), recursive=True) # Match .txt files with a single character name in the base directory (non-recursive) single_character_txt_files = glob.glob(os.path.join(base_directory, \'?.txt\')) # Match all files and directories in the base directory and its subdirectories excluding hidden ones all_files = [ f for f in glob.glob(os.path.join(base_directory, \'**\', \'*\'), recursive=True) if not os.path.basename(f).startswith(\'.\') ] # Match files with special characters literally in the base directory (non-recursive) escaped_files = glob.glob(os.path.join(base_directory, glob.escape(\'?special?.txt\'))) return { \'gif_files\': gif_files, \'single_character_txt_files\': single_character_txt_files, \'all_files\': all_files, \'escaped_files\': escaped_files }"},{"question":"Objective: The goal of this exercise is to assess your understanding of PyTorch\'s Named Tensor capabilities. You will be asked to implement a function that manipulates tensors with named dimensions. Problem Statement: Given the following function signature: ```python import torch def process_named_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Given an input tensor with named dimensions, perform the following operations: 1. Rename dimensions: - Rename \'N\' to \'num_samples\' - Rename \'C\' to \'channels\' - Rename \'H\' to \'height\' - Rename \'W\' to \'width\' 2. Ensure that the tensor has a dimension named \'channels\'. If it does not, raise a ValueError. 3. Align dimensions such that \'channels\' dimension is first, followed by \'num_samples\', \'height\', and \'width\'. 4. Return the resulting tensor. If the input tensor does not have named dimensions, raise a ValueError. Args: - input_tensor (torch.Tensor): A PyTorch tensor with named dimensions. Returns: - torch.Tensor: The processed tensor with the specified dimension renaming and aligning. Raises: - ValueError: If the input tensor does not have named dimensions or does not contain a \'channels\' dimension. pass ``` Constraints: 1. The input tensor is expected to have named dimensions. 2. If the input tensor does not have named dimensions, the function should raise a `ValueError` with the message \\"Input tensor must have named dimensions.\\" 3. Following the renaming, if the tensor does not have a \'channels\' dimension, the function should raise a `ValueError` with the message \\"Tensor must have a \'channels\' dimension.\\" Example: ```python # Example usage input_tensor = torch.randn(4, 3, 32, 32, names=(\'N\', \'C\', \'H\', \'W\')) output_tensor = process_named_tensor(input_tensor) # The output_tensor should have names: (\'channels\', \'num_samples\', \'height\', \'width\') print(output_tensor.names) # Output: (\'channels\', \'num_samples\', \'height\', \'width\') # Example of raising an error unnamed_tensor = torch.randn(4, 3, 32, 32) # This should raise ValueError output_tensor = process_named_tensor(unnamed_tensor) ``` You are required to implement the `process_named_tensor` function to correctly perform the described operations, adhering to the constraints and raising errors where necessary.","solution":"import torch def process_named_tensor(input_tensor: torch.Tensor) -> torch.Tensor: Given an input tensor with named dimensions, perform the following operations: 1. Rename dimensions: - Rename \'N\' to \'num_samples\' - Rename \'C\' to \'channels\' - Rename \'H\' to \'height\' - Rename \'W\' to \'width\' 2. Ensure that the tensor has a dimension named \'channels\'. If it does not, raise a ValueError. 3. Align dimensions such that \'channels\' dimension is first, followed by \'num_samples\', \'height\', and \'width\'. 4. Return the resulting tensor. If the input tensor does not have named dimensions, raise a ValueError. Args: - input_tensor (torch.Tensor): A PyTorch tensor with named dimensions. Returns: - torch.Tensor: The processed tensor with the specified dimension renaming and aligning. Raises: - ValueError: If the input tensor does not have named dimensions or does not contain a \'channels\' dimension. if input_tensor.names is None or None in input_tensor.names: raise ValueError(\\"Input tensor must have named dimensions.\\") new_names = {} renames = {\'N\': \'num_samples\', \'C\': \'channels\', \'H\': \'height\', \'W\': \'width\'} for old_name in input_tensor.names: if old_name in renames: new_names[old_name] = renames[old_name] input_tensor = input_tensor.rename(**new_names) if \'channels\' not in input_tensor.names: raise ValueError(\\"Tensor must have a \'channels\' dimension.\\") current_names = input_tensor.names reordered_names = (\'channels\', \'num_samples\', \'height\', \'width\') input_tensor = input_tensor.align_to(*reordered_names) return input_tensor"},{"question":"# ***Coding Assessment Question*** Implement a Python program using the `nntplib` module to perform the following tasks: 1. **Connect to an NNTP Server**: - Use the `nntplib.NNTP` class to connect to the NNTP server at \'news.gmane.io\'. - Retrieve and print the welcome message from the server using the `getwelcome()` method. 2. **List Newsgroups**: - Implement a function `list_newsgroups(nntp_conn)` that retrieves and prints all available newsgroups on the server along with their descriptions. Use the `descriptions` method of the NNTP object. - Handle any exceptions that may occur during this process and print appropriate error messages. 3. **Retrieve and Display Recent Articles**: - Implement a function `recent_articles(nntp_conn, group_name, num_articles)` that, given a newsgroup name and a number, retrieves and prints the subjects of the most recent articles from that newsgroup. - Use the `group`, `over` and `decode_header` methods of the NNTP object to achieve this. 4. **Post an Article**: - Implement a function `post_article(nntp_conn, group_name)` that reads an article from a file named `article.txt` and posts it to the specified newsgroup. - Ensure the function handles posting errors appropriately and prints the serverâ€™s response. # ***Input and Output*** - **Input**: - No direct input is needed as the parameters for functions are passed directly within the script. - Ensure you have access to a file named `article.txt` in the same directory. - **Output**: - Welcome message from the server. - List of all newsgroups and their descriptions. - Subjects of the most recent articles from a specified newsgroup. - Response message from the server after attempting to post an article. # ***Constraints and Assumptions*** - Assume the NNTP server \'news.gmane.io\' is accessible and operational. - The file `article.txt` is correctly formatted and includes necessary headers for posting. - You should handle any exceptions that may occur due to network issues or server responses. # ***Example Usage*** ```python def main(): try: nntp_conn = nntplib.NNTP(\'news.gmane.io\') print(nntp_conn.getwelcome()) list_newsgroups(nntp_conn) recent_articles(nntp_conn, \'gmane.comp.python.committers\', 10) post_article(nntp_conn, \'gmane.comp.python.committers\') finally: nntp_conn.quit() if __name__ == \'__main__\': main() ``` Implement the functions `list_newsgroups`, `recent_articles`, and `post_article` as described. Ensure the use of proper exception handling and the clean closure of connections.","solution":"import nntplib def list_newsgroups(nntp_conn): Retrieves and prints all available newsgroups on the server along with their descriptions. try: resp, groups = nntp_conn.descriptions() for group in groups.keys(): print(f\\"{group}: {groups[group]}\\") except Exception as e: print(f\\"An error occurred while listing newsgroups: {e}\\") def recent_articles(nntp_conn, group_name, num_articles): Retrieves and prints the subjects of the most recent articles from the specified newsgroup. try: resp, count, first, last, name = nntp_conn.group(group_name) print(f\'Group {group_name} has {count} articles, range {first} to {last}\') resp, overviews = nntp_conn.over((str(int(last) - num_articles + 1), str(last))) for id, over in overviews: subject, from_, date, message_id, refs, size, lines = nntplib.decode_header(over) print(f\'{id}: {subject}\') except Exception as e: print(f\\"An error occurred while retrieving recent articles: {e}\\") def post_article(nntp_conn, group_name): Reads an article from a file named \'article.txt\' and posts it to the specified newsgroup. try: with open(\'article.txt\', \'rb\') as f: resp, info = nntp_conn.post(f) print(f\\"Post response: {resp} {info}\\") except Exception as e: print(f\\"An error occurred while posting the article: {e}\\")"},{"question":"# Question: Implementing Locally Linear Embedding (LLE) from Scratch **Objective**: Implement the Locally Linear Embedding (LLE) algorithm from scratch to understand how local neighborhood distances are preserved. Use this implementation to reduce the dimensionality of a given high-dimensional dataset. **Problem Statement**: You need to write a Python function to perform Locally Linear Embedding on a given dataset. Your function should: 1. Find the k-nearest neighbors for each data point. 2. Compute the weight matrix that best reconstructs each data point using its neighbors. 3. Compute the lower-dimensional embedding of the dataset that best preserves these weights. **Function Signature**: ```python def local_linear_embedding(X: np.ndarray, n_neighbors: int, n_components: int) -> np.ndarray: Perform Locally Linear Embedding (LLE) on the input data. Parameters: - X: np.ndarray : Input data of shape (n_samples, n_features) - n_neighbors: int : Number of nearest neighbors to consider for each point - n_components: int : Number of dimensions for the output embedding Returns: - embedding: np.ndarray : The lower-dimensional embedding of the input data of shape (n_samples, n_components) pass ``` **Input**: - `X`: A numpy array of shape `(n_samples, n_features)` where each row represents a high-dimensional data point. - `n_neighbors`: An integer representing the number of nearest neighbors for each point. - `n_components`: An integer representing the number of dimensions for the output embedding. **Output**: - `embedding`: A numpy array of shape `(n_samples, n_components)` representing the lower-dimensional representation of the input data. **Constraints**: - `n_neighbors` must be greater than `n_components`. - The function should appropriately handle datasets with up to 1000 samples and 100 features. **Performance Requirements**: - The implemented function should achieve linear runtime complexity relative to the number of samples during the nearest neighbor search phase. - Python libraries such as NumPy and SciPy can be used to aid in matrix operations and nearest neighbor searches. **Example**: ```python import numpy as np # Sample high-dimensional data (e.g., 3D data for illustration) X = np.array([ [1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [2, 3, 4] ]) # Perform LLE with 2 neighbors and reduce to 2 dimensions embedding = local_linear_embedding(X, n_neighbors=2, n_components=2) print(embedding) ``` _Note_: The example provided is for illustration purposes. Your implementation will be tested on larger datasets with higher dimensions. Additionally, provide a brief explanation at the end of your solution describing how you handled each step of the LLE algorithm. **Helpful Tips**: - For finding nearest neighbors, you can use KD-Trees, Ball-Trees, or libraries like SciPy\'s `spatial` module. - Ensure the eigenvalue decomposition step efficiently computes the largest `n_components` eigenvalues and corresponding eigenvectors.","solution":"import numpy as np from scipy.spatial import KDTree from scipy.linalg import eigh def local_linear_embedding(X: np.ndarray, n_neighbors: int, n_components: int) -> np.ndarray: # Step 1: Find k-nearest neighbors for each data point tree = KDTree(X) n_samples = X.shape[0] neighbors = np.zeros((n_samples, n_neighbors), dtype=int) for i in range(n_samples): distances, neighbor_indices = tree.query(X[i], k=n_neighbors + 1) neighbors[i] = neighbor_indices[1:] # Exclude the point itself which will be first # Step 2: Compute the weight matrix that best reconstructs each data point using its neighbors weights = np.zeros((n_samples, n_samples)) for i in range(n_samples): Z = X[neighbors[i]] - X[i] # Center the neighbors to the point C = np.dot(Z, Z.T) # Local covariance C = C + np.eye(n_neighbors) * 1e-3 # Regularization to ensure numerical stability w = np.linalg.solve(C, np.ones(n_neighbors)) w = w / np.sum(w) # Normalize weights weights[i, neighbors[i]] = w # Step 3: Compute the lower-dimensional embedding M = np.eye(n_samples) - weights M = np.dot(M.T, M) # Compute the symmetric matrix for eigenvalue problem eigenvalues, eigenvectors = eigh(M) embedding = eigenvectors[:, 1:n_components + 1] # Take n_components smallest non-zero eigenvectors return embedding"},{"question":"**Question:** You are provided with a dataset of historical sales data and various features about items and stores. Your task is to build a predictive model to forecast future sales using any suitable supervised learning technique(s) from the scikit-learn library. **Dataset Description:** - The dataset `sales_data.csv` has the following columns: - `store_id` (int): ID of the store. - `item_id` (int): ID of the item. - `date` (string in \\"YYYY-MM-DD\\" format): Date of the sale. - `sales` (float): Number of items sold on a given date. - Additional features related to the items and stores. **Requirements:** 1. **Preprocessing:** - Date column should be processed to extract relevant time features (e.g., year, month, day of the week). - Handle any missing values appropriately. - Encode categorical features if necessary. 2. **Modeling:** - Implement multiple regression algorithms to predict future sales. - Use techniques like cross-validation for hyperparameter tuning. - Evaluate the models using appropriate regression metrics (e.g., Mean Absolute Error, Root Mean Squared Error). 3. **Implementation:** - Use scikit-learn for model implementation, training, and evaluation. - The final function should be named `forecast_sales` and have the following signature: ```python def forecast_sales(data_path: str) -> dict: Train and evaluate multiple regression models on the sales data. Parameters: data_path (str): Path to the CSV file containing sales data. Returns: dict: Dictionary containing model names and their corresponding evaluation metrics. Example: {\\"LinearRegression\\": 1500.23, \\"RandomForestRegressor\\": 1300.45} ``` **Input:** - `data_path` (str): Path to the `sales_data.csv` file. **Output:** - A dictionary containing the names of the regression models and their corresponding evaluation metric values. **Constraints:** - Use at least three different regression models. - Ensure your code is efficient and well-documented. - Consider both training time and prediction accuracy while selecting the models. **Performance Requirements:** - The solution should be scalable to reasonably large datasets (up to 10,000 rows). - Code should complete execution within 5 minutes for the given dataset size. **Example Usage:** ```python sales_metrics = forecast_sales(\'path/to/sales_data.csv\') print(sales_metrics) ``` **Explanation:** - The `forecast_sales` function reads the data from the provided CSV file, preprocesses the data, and trains multiple regression models. - After tuning the hyperparameters and evaluating the models, it returns a dictionary with the names of the models and their evaluation metrics. **Hint:** - You can explore and use models like `LinearRegression`, `RandomForestRegressor`, and `GradientBoostingRegressor` from scikit-learn.","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split, cross_val_score from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from sklearn.metrics import mean_absolute_error, mean_squared_error from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from datetime import datetime def preprocess_data(df): df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'year\'] = df[\'date\'].dt.year df[\'month\'] = df[\'date\'].dt.month df[\'day_of_week\'] = df[\'date\'].dt.dayofweek # Dropping the original date column as it\'s no longer needed df.drop(\'date\', axis=1, inplace=True) # Handling missing values df.fillna(df.mean(), inplace=True) return df def forecast_sales(data_path: str) -> dict: # Load the dataset df = pd.read_csv(data_path) df = preprocess_data(df) X = df.drop(columns=[\'sales\']) y = df[\'sales\'] # Select categorical features for one-hot encoding categorical_features = [\'store_id\', \'item_id\'] categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'cat\', categorical_transformer, categorical_features), (\'num\', \'passthrough\', [\'year\', \'month\', \'day_of_week\']) ]) # Splitting the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # List of models to evaluate models = { \'LinearRegression\': LinearRegression(), \'RandomForestRegressor\': RandomForestRegressor(n_jobs=-1, random_state=42), \'GradientBoostingRegressor\': GradientBoostingRegressor(random_state=42) } metrics = {} for model_name, model in models.items(): pipe = Pipeline(steps=[(\'preprocessor\', preprocessor), (\'scaler\', StandardScaler()), (\'regressor\', model)]) pipe.fit(X_train, y_train) y_pred = pipe.predict(X_test) mae = mean_absolute_error(y_test, y_pred) metrics[model_name] = mae return metrics"},{"question":"**Objective**: Demonstrate your understanding of Python\'s email package and encoding techniques by creating functions that modify the payloads of email message objects. **Problem Statement**: You are tasked with creating a higher-level function that takes a \\"Message\\" object and an encoding type as input and modifies the payload and headers of the \\"Message\\" object accordingly. You will implement an encoding handler that can call the appropriate encoding function based on the specified encoding type. **Function Signature**: ```python from email.message import Message def encode_message(msg: Message, encoding_type: str) -> Message: Encodes the payload of the message object based on the encoding type provided. Parameters: - msg (Message): The message object whose payload will be encoded. - encoding_type (str): The type of encoding (one of \\"quoted-printable\\", \\"base64\\", \\"7or8bit\\", \\"noop\\"). Returns: - Message: The message object with the encoded payload and appropriate headers. # Your implementation here ``` # Key Points: 1. **Input**: - `msg`: A single \\"Message\\" object. - `encoding_type`: A string indicating the type of encoding. It can be one of the following: - \\"quoted-printable\\" - \\"base64\\" - \\"7or8bit\\" - \\"noop\\" 2. **Output**: - Returns the modified \\"Message\\" object, with the payload encoded and appropriate headers set. 3. **Constraints**: - If an invalid encoding type is provided, the function should raise a `ValueError` with the message \\"Invalid encoding type\\". - The function should handle quoted-printable, base64, 7bit/8bit without modifying payload (noop), and any valid payload encoding. 4. **Performance**: - Ensure the function handles typical payload sizes efficiently. # Example Usage: ```python from email.message import Message msg = Message() msg.set_payload(\\"This is a text payload.\\") # Encode this message using base64 encoding encoded_msg = encode_message(msg, \\"base64\\") print(encoded_msg.get_payload()) print(encoded_msg[\\"Content-Transfer-Encoding\\"]) # Should print \\"base64\\" ``` # Notes: - Utilize the `email.encoders` functions (`encode_quopri`, `encode_base64`, `encode_7or8bit`, `encode_noop`) to implement the encoder logic based on the specified encoding type. - Make sure to import the required modules from the `email` package.","solution":"from email.message import Message from email.encoders import encode_quopri, encode_base64, encode_7or8bit, encode_noop def encode_message(msg: Message, encoding_type: str) -> Message: Encodes the payload of the message object based on the encoding type provided. Parameters: - msg (Message): The message object whose payload will be encoded. - encoding_type (str): The type of encoding (one of \\"quoted-printable\\", \\"base64\\", \\"7or8bit\\", \\"noop\\"). Returns: - Message: The message object with the encoded payload and appropriate headers. encoding_handlers = { \\"quoted-printable\\": encode_quopri, \\"base64\\": encode_base64, \\"7or8bit\\": encode_7or8bit, \\"noop\\": encode_noop } if encoding_type not in encoding_handlers: raise ValueError(\\"Invalid encoding type\\") encoding_handlers[encoding_type](msg) return msg"},{"question":"You are tasked with implementing a parallel data processing system using the `multiprocessing` package. The system will compute the squared values of numbers from multiple data sources concurrently and then aggregate the results. # Requirements 1. **Create Multiple Data Sources**: There are five data sources each represented by a list of integers. You need to process each data source in a separate process. 2. **Shared Result List**: Use a `multiprocessing.Manager().list()` to store the results from all the processes. 3. **Synchronization**: Use a `multiprocessing.Lock` to ensure that only one process writes to the shared result list at a time. 4. **Pooling Processes**: Implement a pool of worker processes to handle the data sources concurrently. # Task Implement the following functions: 1. `process_data(data, result_list, lock)`: A function to be run by each worker process that computes the squared values of the integers in the given `data` list, and appends the result to `result_list` using the provided `lock`. 2. `main()`: A function that sets up the data sources, initializes the shared result list and lock, creates a pool of worker processes, and distributes the data processing tasks to the pool. # Specifications - Each data source is a list of integers. - The `process_data` function should compute the squared values of the integers in `data` and append the results to `result_list` using `lock` to synchronize access. # Example Given data sources: ```python data_sources = [ [1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15] ] ``` The result list should contain: ```python [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225] ``` # Constraints 1. Use the `multiprocessing` package to implement process-based parallelism. 2. Ensure that the shared result list is accessed in a thread-safe manner. # Implementation ```python import multiprocessing def process_data(data, result_list, lock): Process the given data by computing the squared values of the integers in the data list. Append the results to the shared result_list using the provided lock for synchronization. :param data: List of integers to be processed :param result_list: multiprocessing.Manager().list() object to store the results :param lock: multiprocessing.Lock() object for synchronization # Your code here def main(): Main function to set up data sources, initialize shared resources, create a pool of worker processes, and distribute data processing tasks to the pool. # List of data sources data_sources = [ [1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15] ] # Your code here to initialize shared resources # Your code here to set up the pool and distribute tasks if __name__ == \'__main__\': main() ``` Instructions: - Complete the `process_data` and `main` functions. - Use a pool of worker processes to handle the data sources concurrently. - Ensure the results are collected in a thread-safe manner using the shared list.","solution":"import multiprocessing def process_data(data, result_list, lock): Process the given data by computing the squared values of the integers in the data list. Append the results to the shared result_list using the provided lock for synchronization. :param data: List of integers to be processed :param result_list: multiprocessing.Manager().list() object to store the results :param lock: multiprocessing.Lock() object for synchronization squared_data = [x ** 2 for x in data] with lock: result_list.extend(squared_data) def main(): Main function to set up data sources, initialize shared resources, create a pool of worker processes, and distribute data processing tasks to the pool. # List of data sources data_sources = [ [1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15] ] manager = multiprocessing.Manager() result_list = manager.list() lock = manager.Lock() # Create a pool of worker processes with multiprocessing.Pool(processes=len(data_sources)) as pool: pool.starmap(process_data, [(data, result_list, lock) for data in data_sources]) print(result_list) return list(result_list) if __name__ == \'__main__\': main()"},{"question":"**Question:** Understanding Scope, Name Binding, and Exception Handling in Python **Objective:** Implement a class-based solution which requires understanding of variable scoping, name resolution, and exception handling. # Problem Statement: You need to create a class `Geometry` that will help calculate areas for different geometric shapes. However, you need to pay careful attention to variable scoping and name resolution within the class and its methods. Additionally, you should handle possible errors gracefully. The `Geometry` class should include: 1. **Attributes:** - `shape` - a string representing the type of shape (default is `None`). 2. **Methods:** - `__init__(self, shape: Optional[str] = None)` - Initialize the class with the shape. - `calculate_area(self, *args)`: - If the shape is `\\"circle\\"`, the method expects one argument, the radius, and returns the area using the formula `Ï€ * radius^2`. - If the shape is `\\"rectangle\\"`, the method expects two arguments, the length and the width, and returns the area using the formula `length * width`. - If the shape is `\\"triangle\\"`, the method expects two arguments, the base and the height, and returns the area using the formula `0.5 * base * height`. - If the shape type is unknown or the wrong number of arguments are passed, the method should raise a `ValueError` with an appropriate message. - **Constraints:** - The shape must be one of `\\"circle\\"`, `\\"rectangle\\"`, or `\\"triangle\\"`. - The numeric arguments should be positive numbers. - **Performance Requirements:** - Your implementation should handle the errors efficiently and provide clear error messages. # Input and Output Example: ```python # Example usage: g_circle = Geometry(\\"circle\\") area_circle = g_circle.calculate_area(5) # Expected output: 78.53981633974483 (approximately) g_rectangle = Geometry(\\"rectangle\\") area_rectangle = g_rectangle.calculate_area(4, 5) # Expected output: 20 g_triangle = Geometry(\\"triangle\\") area_triangle = g_triangle.calculate_area(3, 6) # Expected output: 9.0 ``` **Function signatures:** ```python class Geometry: def __init__(self, shape: Optional[str] = None): # Initialize shape def calculate_area(self, *args) -> float: # Calculate the appropriate area pass ``` **Note:** Make sure to handle incorrect input gracefully and raise appropriate exceptions with meaningful error messages when necessary.","solution":"import math from typing import Optional class Geometry: def __init__(self, shape: Optional[str] = None): self.shape = shape def calculate_area(self, *args) -> float: if self.shape == \\"circle\\": if len(args) != 1: raise ValueError(\\"Circle requires exactly one argument: radius\\") radius = args[0] if radius <= 0: raise ValueError(\\"Radius must be a positive number\\") return math.pi * (radius ** 2) elif self.shape == \\"rectangle\\": if len(args) != 2: raise ValueError(\\"Rectangle requires exactly two arguments: length and width\\") length, width = args if length <= 0 or width <= 0: raise ValueError(\\"Length and width must be positive numbers\\") return length * width elif self.shape == \\"triangle\\": if len(args) != 2: raise ValueError(\\"Triangle requires exactly two arguments: base and height\\") base, height = args if base <= 0 or height <= 0: raise ValueError(\\"Base and height must be positive numbers\\") return 0.5 * base * height else: raise ValueError(\\"Unknown shape type\\")"},{"question":"# Seaborn Advanced Plotting and Custom Scaling Problem Statement You are tasked with visualizing a dataset using seaborn\'s object-oriented interface. You will load a dataset and create a series of plots, each demonstrating different customization techniques using scaling and transformations. # Instructions 1. **Load the Dataset:** Use the seaborn `load_dataset` function to load the `\\"diamonds\\"` dataset. 2. **Basic Plot:** Create a scatter plot with `carat` on the x-axis and `price` on the y-axis using seaborn\'s object-oriented interface. 3. **Logarithmic Transformation:** Transform the y-axis to a logarithmic scale. 4. **Custom Size and Color:** Modify the scatter plot to: - Size points based on `carat` (Scale it between 2 and 10). - Color points based on `clarity` using a predefined palette `crest`. 5. **Advanced Customization:** Apply the following advanced customizations: - Set the x-axis to a square root transformation and show ticks for every 0.5 units. - Change the y-axis label to format in currency (`\\"\\"`) and show color ticks up to 4. - Add a polynomial line of order 2 to the plot. 6. **Plot with Nominal Scale:** Create another histogram plot to display the count of diamonds by `cut`. Since `cut` is a categorical variable, ensure it is treated as such. # Code Template Implement your solution in the function `custom_seaborn_visualization()`. The function does not take any input but should display the plots. ```python import seaborn.objects as so from seaborn import load_dataset def custom_seaborn_visualization(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Basic Plot p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") p1.add(so.Dots()).scale(y=\\"log\\") # Custom Size and Color p1.add(so.Dots(), color=\\"clarity\\").scale(color=\\"crest\\") p1.add(so.Dots(), pointsize=\\"carat\\").scale(pointsize=(2, 10)) # Advanced Customization p1.scale(x=so.Continuous(trans=\\"sqrt\\").tick(every=0.5), y=so.Continuous().label(like=\\"{x:g}\\"), color=so.Continuous(\\"ch:.2\\").tick(upto=4).label(unit=\\"\\")) p1.add(so.Line(), so.PolyFit(order=2)) p1.show() # Plot with Nominal Scale p2 = so.Plot(diamonds, \\"cut\\").add(so.Bar(), so.Hist()) p2.scale(x=so.Nominal()) p2.show() # Call function to display plots custom_seaborn_visualization() ``` # Expected Output - The function should display two plots: - A scatter plot of `carat` vs `price` with y-axis logarithmic transformation, custom sizes based on `carat`, colors based on `clarity`, x-axis transformation, and a polynomial fit line. - A histogram of the `cut` attribute treated with a nominal scale. # Constraints - Make sure to handle cases where certain attributes might contain NaN values. - Ensure the visual representations are clear and well-labeled according to the specifications.","solution":"import seaborn.objects as so from seaborn import load_dataset import seaborn as sns import numpy as np def custom_seaborn_visualization(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Basic Plot p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") p1.add(so.Dots()).scale(y=\\"log\\") # Custom Size and Color p1.add(so.Dots(), color=\\"clarity\\").scale(color=\\"crest\\") p1.add(so.Dots(), pointsize=\\"carat\\").scale(pointsize=(2, 10)) # Advanced Customization p1.scale(x=so.Continuous(trans=\\"sqrt\\").tick(every=0.5), y=so.Continuous().label(like=\\"{x:g}\\"), color=so.Continuous(\\"ch:.2\\").tick(upto=4).label(unit=\\"\\")) p1.add(so.Line(), so.PolyFit(order=2)) p1.show() # Plot with Nominal Scale p2 = so.Plot(diamonds, \\"cut\\").add(so.Bar(), so.Hist()) p2.scale(x=so.Nominal()) p2.show() # Call function to display plots custom_seaborn_visualization()"},{"question":"# Python Coding Assessment Question **Objective:** Implement a Python function that utilizes the `array` module to perform a series of operations on arrays. This will demonstrate your understanding of array initialization, manipulation, and specific methods provided by the `array` module. **Problem Statement:** You are given a list of operations to perform on an array containing integers. Write a function `process_array_operations(typecode: str, initializer: list, operations: list) -> list` that takes a type code, an initializer list, and a list of operations to perform on the array. The function should return the final state of the array as a list. **Function Signature:** ```python def process_array_operations(typecode: str, initializer: list, operations: list) -> list: ``` **Parameters:** - `typecode`: A string representing the type code of the array (e.g., \'i\' for signed integers). - `initializer`: A list of initial values to initialize the array. - `operations`: A list of operations to perform on the array. Each operation is represented as a tuple where the first element is the operation name (a string), and the subsequent elements are the arguments for that operation. **Output:** - Returns a list representing the final state of the array after all operations have been performed. **Constraints:** - The `typecode` is guaranteed to be valid and supported. - The `initializer` list will contain elements compatible with the `typecode`. - The `operations` list will contain valid operation names and arguments. **Operations List Example:** - `(\\"append\\", value)`: Append a new item with `value` to the end of the array. - `(\\"pop\\", index)`: Removes the item with the index `index` from the array and returns it. - `(\\"insert\\", index, value)`: Insert a new item with `value` before position `index`. - `(\\"remove\\", value)`: Remove the first occurrence of `value` from the array. **Example:** ```python # Example usage: typecode = \'i\' initializer = [1, 2, 3, 4, 5] operations = [ (\\"append\\", 6), (\\"pop\\", 2), (\\"insert\\", 1, 10), (\\"remove\\", 4) ] output = process_array_operations(typecode, initializer, operations) print(output) # Output: [1, 10, 2, 5, 6] ``` **Notes:** - Assume that the array operations provided are correct and won\'t cause exceptions. - You may assume that the typecode corresponds to a valid type for the initializer list provided. Good luck, and happy coding!","solution":"import array def process_array_operations(typecode: str, initializer: list, operations: list) -> list: # Initialize the array arr = array.array(typecode, initializer) # Perform each operation for operation in operations: op_name = operation[0] if op_name == \\"append\\": value = operation[1] arr.append(value) elif op_name == \\"pop\\": index = operation[1] arr.pop(index) elif op_name == \\"insert\\": index, value = operation[1], operation[2] arr.insert(index, value) elif op_name == \\"remove\\": value = operation[1] arr.remove(value) # Convert array to list and return it return list(arr)"},{"question":"**Array Manipulation and Conversion** # Problem Statement You are required to implement several utility functions using the `array` module to manipulate numeric arrays. These utilities will include creating arrays, performing various operations on them, and converting between different types. # Function Requirements 1. **create_array(typecode: str, initializer: list) -> array:** - Create and return an `array` object using the given type code and initializer list. - Example: `create_array(\'i\', [1, 2, 3])` should return `array(\'i\', [1, 2, 3])`. 2. **reverse_array(arr: array) -> array:** - Return a new `array` object that is the reverse of the input array. - Example: `reverse_array(array(\'i\', [1, 2, 3]))` should return `array(\'i\', [3, 2, 1])`. 3. **find_max(arr: array) -> int:** - Return the maximum value from the input array. - Example: `find_max(array(\'i\', [1, 2, 3]))` should return `3`. 4. **array_to_list(arr: array) -> list:** - Convert the input array to a list and return it. - Example: `array_to_list(array(\'i\', [1, 2, 3]))` should return `[1, 2, 3]`. 5. **merge_arrays(arr1: array, arr2: array) -> array:** - Merge two arrays into one and return the new array. Both arrays must have the same type code. - Example: `merge_arrays(array(\'i\', [1, 2]), array(\'i\', [3, 4]))` should return `array(\'i\', [1, 2, 3, 4])`. # Constraints - Type codes will always be one of `\'b\'`, `\'B\'`, `\'u\'`, `\'h\'`, `\'H\'`, `\'i\'`, `\'I\'`, `\'l\'`, `\'L\'`, `\'q\'`, `\'Q\'`, `\'f\'`, `\'d\'`. - Initializer lists will match the type specified by the type code. - All input arrays for the merge function will have the same type code. The performance requirements are not stringent given the typical constraints usually encountered in assessment settings, but efficient usage of the `array` module\'s facilities is expected. # Examples and Explanations ```python from array import array # Example usage of create_array function arr = create_array(\'i\', [1, 2, 3]) print(arr) # Output: array(\'i\', [1, 2, 3]) # Example usage of reverse_array function reversed_arr = reverse_array(arr) print(reversed_arr) # Output: array(\'i\', [3, 2, 1]) # Example usage of find_max function maximum = find_max(arr) print(maximum) # Output: 3 # Example usage of array_to_list function lst = array_to_list(arr) print(lst) # Output: [1, 2, 3] # Example usage of merge_arrays function arr1 = create_array(\'i\', [1, 2]) arr2 = create_array(\'i\', [3, 4]) merged_arr = merge_arrays(arr1, arr2) print(merged_arr) # Output: array(\'i\', [1, 2, 3, 4]) ``` Implement these functions to demonstrate your understanding of the `array` module and its various operations.","solution":"from array import array def create_array(typecode: str, initializer: list) -> array: Create and return an array using the type code and initializer list. return array(typecode, initializer) def reverse_array(arr: array) -> array: Return a new array that is the reverse of the input array. return array(arr.typecode, arr[::-1]) def find_max(arr: array) -> int: Return the maximum value from the input array. return max(arr) def array_to_list(arr: array) -> list: Convert the input array to a list and return it. return arr.tolist() def merge_arrays(arr1: array, arr2: array) -> array: Merge two arrays into one and return the new array. Both arrays must have the same type code. if arr1.typecode != arr2.typecode: raise ValueError(\\"Both arrays must have the same type code.\\") return array(arr1.typecode, arr1 + arr2)"},{"question":"**Context**: You are tasked with creating a utility script to manage a series of commands needed for a deployment process. This process involves running various shell commands, capturing their output, and handling any errors that occur. **Problem Statement**: Write a Python function named `deploy_project(commands: List[str], timeout: int) -> Tuple[str, str]` that takes a list of shell commands and a timeout value (in seconds). The function should: 1. Sequentially run each command in the list. 2. Capture the standard output and standard error of each command. 3. Return a tuple containing the combined standard output and standard error of all commands. 4. If any command fails (returns a non-zero exit status), halt execution of subsequent commands, handle the exception, and include the error in the returned output. 5. Handle the scenario where a command takes longer than the specified timeout by terminating it and noting the timeout in the combined output. **Specifications**: - The commands should be run using the `subprocess.run` method from the `subprocess` module. - The function should handle `subprocess.CalledProcessError` for commands that return a non-zero exit status. - If a command execution exceeds the given timeout, the function should handle the `subprocess.TimeoutExpired` exception. - Assume all commands are valid shell commands and should be run with `shell=True`. **Function Signature**: ```python from typing import List, Tuple def deploy_project(commands: List[str], timeout: int) -> Tuple[str, str]: # Your implementation here pass ``` **Example**: ```python commands = [ \\"echo \'Deploying project...\'\\", \\"mkdir -p /tmp/deploy_test\\", \\"echo \'Directory created.\'\\", \\"sleep 5\\", # Example command that could trigger a timeout \\"echo \'Deployment completed.\'\\" ] timeout = 3 stdout, stderr = deploy_project(commands, timeout) print(\\"Standard Output:\\") print(stdout) print(\\"Standard Error:\\") print(stderr) ``` _Output for the example above with a timeout of 3 seconds might be_: ``` Standard Output: Deploying project... Directory created. Standard Error: Command \'sleep 5\' timed out after 3 seconds. ``` **Constraints**: - Do not use threading or multiprocessing; stick to the subprocess module. - The function should be efficient and handle cases where the standard output and error are large, but do not worry about extremely large outputs that exceed memory capacity. **Hint**: Use a try-except block within a loop to handle different exceptions (`subprocess.CalledProcessError` and `subprocess.TimeoutExpired`).","solution":"from typing import List, Tuple import subprocess def deploy_project(commands: List[str], timeout: int) -> Tuple[str, str]: combined_stdout = \\"\\" combined_stderr = \\"\\" for command in commands: try: result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout, text=True) combined_stdout += result.stdout combined_stderr += result.stderr except subprocess.CalledProcessError as e: combined_stderr += f\\"Command \'{e.cmd}\' failed with exit status {e.returncode}.n{e.stderr}\\" break except subprocess.TimeoutExpired as e: combined_stderr += f\\"Command \'{e.cmd}\' timed out after {timeout} seconds.n\\" break return combined_stdout, combined_stderr"},{"question":"**Pandas Data Manipulation Task** # Objective: You are required to use the pandas library to perform a series of data manipulation tasks which will demonstrate your understanding of fundamental and advanced concepts of this package. # Dataset: You will be provided with two datasets stored as dictionaries: 1. **Employee Data**: Contains information about employees including their ID, Name, Department, and Salary. 2. **Department Data**: Contains information about departments including Department ID and Department Name. ```python employee_data = { \\"ID\\": [1, 2, 3, 4, 5], \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eva\\"], \\"Department\\": [101, 102, 103, 101, 102], \\"Salary\\": [70000, 80000, 120000, 90000, 85000] } department_data = { \\"Department ID\\": [101, 102, 103], \\"Department Name\\": [\\"HR\\", \\"Engineering\\", \\"Marketing\\"] } ``` # Tasks: 1. **Create DataFrames**: Create two `DataFrame` objects, `df_employee` and `df_department`, from the provided dictionaries. 2. **DataFrame Operations**: - **Merge** the `df_employee` and `df_department` DataFrames on the Department and Department ID columns to create a unified DataFrame with all employee details along with department names. - **Filter** the employees whose salary is greater than 85,000 and create a new DataFrame `high_salary_employees`. 3. **Calculate Aggregate Metrics**: - Calculate the **average salary** by each department and store the result in a DataFrame `avg_salary_by_dept`. 4. **Add a New Column**: - Add a new column to `df_employee` DataFrame called `Tax` which is **10%** of the Salary. 5. **Sort Data**: - Sort the `high_salary_employees` DataFrame by the Salary column in descending order. # Expected Outputs: **1. DataFrames**: ```python # Your initialization code for df_employee and df_department df_employee = pd.DataFrame(employee_data) df_department = pd.DataFrame(department_data) ``` **2. Merged DataFrame**: ```python # Your merging code merged_df = pd.merge(df_employee, df_department, left_on=\\"Department\\", right_on=\\"Department ID\\") ``` **3. Filtered DataFrame**: ```python # DataFrame containing employees with salary > 85,000 high_salary_employees = merged_df[merged_df[\\"Salary\\"] > 85000] ``` **4. Average Salary DataFrame**: ```python # DataFrame containing average salary by department avg_salary_by_dept = merged_df.groupby(\\"Department Name\\")[\\"Salary\\"].mean().reset_index() ``` **5. Adding a Column**: ```python # Updated df_employee with a new Tax column df_employee[\\"Tax\\"] = df_employee[\\"Salary\\"] * 0.10 ``` **6. Sorted DataFrame**: ```python # Sorted high_salary_employees DataFrame by Salary sorted_high_salary_employees = high_salary_employees.sort_values(by=\\"Salary\\", ascending=False) ``` You are expected to write Python code to perform these tasks. Ensure your solution is efficient and uses the capabilities of the pandas library effectively.","solution":"import pandas as pd def create_dataframes(employee_data, department_data): Create DataFrame objects from provided dictionaries. df_employee = pd.DataFrame(employee_data) df_department = pd.DataFrame(department_data) return df_employee, df_department def merge_dataframes(df_employee, df_department): Merge the Employee and Department DataFrames on the Department and Department ID columns. merged_df = pd.merge(df_employee, df_department, left_on=\\"Department\\", right_on=\\"Department ID\\") return merged_df def filter_high_salary_employees(merged_df): Filter employees with salary greater than 85,000. high_salary_employees = merged_df[merged_df[\\"Salary\\"] > 85000] return high_salary_employees def calculate_average_salary_by_dept(merged_df): Calculate the average salary by each department. avg_salary_by_dept = merged_df.groupby(\\"Department Name\\")[\\"Salary\\"].mean().reset_index() return avg_salary_by_dept def add_tax_column(df_employee): Add a new column \'Tax\' which is 10% of the Salary to df_employee DataFrame. df_employee[\\"Tax\\"] = df_employee[\\"Salary\\"] * 0.10 return df_employee def sort_employees_by_salary(high_salary_employees): Sort the high_salary_employees DataFrame by the Salary column in descending order. sorted_high_salary_employees = high_salary_employees.sort_values(by=\\"Salary\\", ascending=False) return sorted_high_salary_employees"},{"question":"# Coding Assessment: PyTorch FX Advanced Graph Manipulations Objective You are tasked with writing a function that performs advanced transformations on a PyTorch model\'s computational graph using Torch FX. This will assess your understanding of FX graph representation, manipulation, and the use of custom tracers. Task 1. **Model Definition**: Start by defining a simple PyTorch neural network model. 2. **Tracing**: Trace the model to obtain its computational graph. 3. **Transformation**: Perform the following transformations on the graph: - Replace all instances of `torch.add` with `torch.sub`. - Insert a `torch.relu` operation after every `torch.sub`. 4. **Recompile**: Ensure the transformed graph is well-formed and recompile it into a new `torch.nn.Module`. 5. **Validation**: Write code to validate that the transformed model has the desired changes by checking the graph and running sample inputs. Requirements - Your function should be named `transform_model`. - It should take the original model as input and return the transformed `torch.nn.Module`. - Ensure the transformed model retains the same input-output behavior, except for the modified operations. Input - A PyTorch `nn.Module` representing the original model. Output - A new `nn.Module` with the specified transformations. Constraints - Use the provided methods and classes from Torch FX. - Ensure the correctness and efficiency of your graph manipulations. Example ```python import torch import torch.fx class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x): x = self.linear(x) x = torch.add(x, x) return x def transform_model(model: torch.nn.Module) -> torch.nn.Module: # Step 1: Trace the model to obtain its computational graph graph : torch.fx.Graph = torch.fx.symbolic_trace(model).graph # Step 2: Perform the transformations for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.sub with graph.inserting_after(node): graph.call_function(torch.relu, (node,)) # Step 3: Recompile the graph into a new `GraphModule` transformed_model = torch.fx.GraphModule(model, graph) transformed_model.recompile() return transformed_model # Example usage original_model = MyModel() transformed_model = transform_model(original_model) # Validate changes by printing graph print(transformed_model.graph) ``` Notes - You can use any sample model and inputs to illustrate the transformations and validate them. - Make sure to include comments and documentation for your code to explain the transformation steps.","solution":"import torch import torch.fx class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = torch.nn.Linear(4, 4) def forward(self, x): x = self.linear(x) x = torch.add(x, x) return x def transform_model(model: torch.nn.Module) -> torch.nn.Module: Transforms the model by replacing torch.add with torch.sub and inserts torch.relu after every torch.sub operation. # Step 1: Trace the model to obtain its computational graph traced = torch.fx.symbolic_trace(model) graph = traced.graph # Step 2: Perform the transformations for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: # Replace torch.add with torch.sub node.target = torch.sub # Insert torch.relu after the torch.sub operation with graph.inserting_after(node): graph.call_function(torch.relu, (node,)) # Step 3: Recompile the graph into a new GraphModule transformed_model = torch.fx.GraphModule(traced, graph) transformed_model.recompile() return transformed_model"},{"question":"# Python Coding Assessment: DBM Module **Objective:** Your task is to implement a function that interacts with a `dbm` database to store, retrieve, and manipulate key-value pairs. This will test your understanding of the `dbm` module and its functions. **Task:** 1. Implement a function `initialize_db(file_name, mode)` that: - Opens a `dbm` database with the specified `file_name` and `mode`. - Returns the `dbm` database object. 2. Implement a function `store_values(db, data_dict)` that: - Takes a `dbm` database object `db` and a dictionary `data_dict`. - Stores each key-value pair from `data_dict` into the database. - Ensures that both keys and values are stored as bytes in the database. 3. Implement a function `retrieve_value(db, key)` that: - Takes a `dbm` database object `db` and a key `key`. - Returns the value associated with the `key` from the database. - If the key is not found, returns `None`. 4. Implement a function `delete_key(db, key)` that: - Takes a `dbm` database object `db` and a key `key`. - Deletes the key-value pair associated with the `key`. 5. Implement a function `list_keys(db)` that: - Takes a `dbm` database object `db`. - Returns a list of all keys present in the database, decoded to strings from bytes. **Function Signatures:** ```python def initialize_db(file_name: str, mode: str) -> dbm: pass def store_values(db: dbm, data_dict: dict) -> None: pass def retrieve_value(db: dbm, key: str) -> str: pass def delete_key(db: dbm, key: str) -> None: pass def list_keys(db: dbm) -> list: pass ``` **Constraints:** - Use `dbm.open` for opening the database. - Ensure to handle any necessary type conversion between strings and bytes. - Use appropriate exception handling for database operations. - Perform operations in modes that adhere to the problem requirements (e.g., read mode should not attempt write operations). **Example Usage:** ```python # Initialize the database db = initialize_db(\'mydb\', \'c\') # Store some values store_values(db, {\'name\': \'Alice\', \'age\': \'30\', \'city\': \'Wonderland\'}) # Retrieve a value print(retrieve_value(db, \'name\')) # Output: \'Alice\' # List all keys print(list_keys(db)) # Output: [\'name\', \'age\', \'city\'] # Delete a key delete_key(db, \'age\') # List all keys after deletion print(list_keys(db)) # Output: [\'name\', \'city\'] ``` **Note:** Make sure to close the database appropriately once all operations are completed. Use context management (`with` statement) where applicable.","solution":"import dbm def initialize_db(file_name, mode): Opens a dbm database with the specified file_name and mode. Returns the dbm database object. return dbm.open(file_name, mode) def store_values(db, data_dict): Takes a dbm database object db and a dictionary data_dict. Stores each key-value pair from data_dict into the database. Ensures that both keys and values are stored as bytes in the database. for key, value in data_dict.items(): db[key.encode()] = value.encode() def retrieve_value(db, key): Takes a dbm database object db and a key. Returns the value associated with the key from the database. If the key is not found, returns None. try: return db[key.encode()].decode() except KeyError: return None def delete_key(db, key): Takes a dbm database object db and a key. Deletes the key-value pair associated with the key. try: del db[key.encode()] except KeyError: pass def list_keys(db): Takes a dbm database object db. Returns a list of all keys present in the database, decoded to strings from bytes. return [key.decode() for key in db.keys()]"},{"question":"# Python Dictionary Manipulation Implement a function called `manipulate_dictionary` that performs various operations on a dictionary. The function should accept a list of operations, where each operation is represented as a tuple. The first element of each tuple is a string that represents the type of operation, and the subsequent elements are the parameters for that operation. Your function should support the following operations: 1. `\\"create\\"`: Create a new empty dictionary. This will be the first operation and should initialize the dictionary. 2. `\\"set\\"`: Set a key-value pair in the dictionary. Parameters: `key` and `value`. 3. `\\"get\\"`: Retrieve the value for a specific key. Parameters: `key`. If the key does not exist, return `\\"Key not found\\"`. 4. `\\"delete\\"`: Remove a key-value pair from the dictionary. Parameters: `key`. If the key does not exist, return `\\"Key not found\\"`. 5. `\\"clear\\"`: Clear the dictionary, removing all key-value pairs. After processing all operations, return the final state of the dictionary and a list of results for each `\\"get\\"` operation. Input: * A list of tuples, where each tuple represents an operation as described above. Output: * A tuple containing: - The final state of the dictionary. - A list of results for each `\\"get\\"` operation performed. Example: ```python def manipulate_dictionary(operations): # Your code here # Example Usage operations = [ (\\"create\\",), (\\"set\\", \\"a\\", 1), (\\"set\\", \\"b\\", 2), (\\"get\\", \\"a\\"), (\\"delete\\", \\"a\\"), (\\"get\\", \\"a\\"), (\\"clear\\",), (\\"get\\", \\"b\\") ] final_dict, get_results = manipulate_dictionary(operations) print(final_dict) # Output: {} print(get_results) # Output: [1, \\"Key not found\\", \\"Key not found\\"] ``` Constraints: * Assume the keys are always strings and values are integers. * The operations list will contain at least the `\\"create\\"` operation. * Each operation in the list is a valid tuple as described. Ensure your solution is efficient and handles all edge cases gracefully.","solution":"def manipulate_dictionary(operations): dictionary = {} get_results = [] for operation in operations: if operation[0] == \\"create\\": dictionary = {} elif operation[0] == \\"set\\": dictionary[operation[1]] = operation[2] elif operation[0] == \\"get\\": result = dictionary.get(operation[1], \\"Key not found\\") get_results.append(result) elif operation[0] == \\"delete\\": if operation[1] in dictionary: del dictionary[operation[1]] else: get_results.append(\\"Key not found\\") elif operation[0] == \\"clear\\": dictionary.clear() return dictionary, get_results"},{"question":"# Advanced Coding Assessment Question: Comprehensive ZIP File Manipulation **Objective:** Implement a Python function `manage_zip_files` that processes multiple operations on ZIP files. This function should be capable of creating ZIP archives, adding files, listing contents, extracting specific files, and reading file contents within the ZIP file. The function should demonstrate an understanding of multiple aspects of the `zipfile` module. **Function Signature:** ```python def manage_zip_files(zip_operations: list) -> list: pass ``` **Input:** - `zip_operations`: A list of dictionaries where each dictionary defines an operation on a ZIP file. - Each dictionary will have the following keys: - `operation`: A string defining the operation. Can be one of `\\"create\\"`, `\\"add\\"`, `\\"list\\"`, `\\"extract\\"`, `\\"read\\"`. - `zip_name`: The name of the ZIP file where the operation will be applied. - Depending on the `operation`: - `\\"create\\"`: No additional keys. - `\\"add\\"`: Requires an additional key `file_name`, which is the name of the file to be added. - `\\"list\\"`: No additional keys. - `\\"extract\\"`: Requires an additional key `member`, which is the name of the file to be extracted. - `\\"read\\"`: Requires an additional key `member`, which is the name of the file whose contents are to be read. **Output:** - A list of results corresponding to each operation: - `\\"create\\"`: Return the string `\\"ZIP file created\\"`. - `\\"add\\"`: Return the string `\\"File added to ZIP\\"`. - `\\"list\\"`: Return a list of file names in the ZIP. - `\\"extract\\"`: Return the string `\\"File extracted: {path}\\"` where `{path}` is the normalized path created during extraction. - `\\"read\\"`: Return the contents of the file as a string. **Constraints:** - Ensure that all ZIP file operations handle errors gracefully. If an error occurs during any operation, return an appropriate error message as a string. - Assume the given files for adding to the ZIP are valid and exist in the current working directory. - Manage the context of opening and closing ZIP files appropriately. **Performance Requirements:** - Operations should handle reasonably sized ZIP files efficiently, up to hundreds of MBs. - The function should leverage efficient operations provided by the `zipfile` module to minimize memory usage. **Example:** ```python # Example usage zip_operations = [ {\\"operation\\": \\"create\\", \\"zip_name\\": \\"example.zip\\"}, {\\"operation\\": \\"add\\", \\"zip_name\\": \\"example.zip\\", \\"file_name\\": \\"file1.txt\\"}, {\\"operation\\": \\"list\\", \\"zip_name\\": \\"example.zip\\"}, {\\"operation\\": \\"extract\\", \\"zip_name\\": \\"example.zip\\", \\"member\\": \\"file1.txt\\"}, {\\"operation\\": \\"read\\", \\"zip_name\\": \\"example.zip\\", \\"member\\": \\"file1.txt\\"} ] output = manage_zip_files(zip_operations) print(output) ``` **Expected Output:** ``` [\\"ZIP file created\\", \\"File added to ZIP\\", [\\"file1.txt\\"], \\"File extracted: file1.txt\\", \\"Contents of file1.txt\\"] ``` **Notes:** 1. Make sure to use appropriate compression methods and handle file paths correctly. 2. Ensure that the function can deal with invalid operations or missing files gracefully.","solution":"import zipfile import os def manage_zip_files(zip_operations: list) -> list: results = [] for operation in zip_operations: op_type = operation[\'operation\'] zip_name = operation[\'zip_name\'] try: if op_type == \'create\': with zipfile.ZipFile(zip_name, \'w\') as zipf: results.append(\\"ZIP file created\\") elif op_type == \'add\': file_name = operation.get(\'file_name\') if file_name and os.path.exists(file_name): with zipfile.ZipFile(zip_name, \'a\') as zipf: zipf.write(file_name) results.append(\\"File added to ZIP\\") else: results.append(f\\"Error: File {file_name} does not exist\\") elif op_type == \'list\': with zipfile.ZipFile(zip_name, \'r\') as zipf: results.append(zipf.namelist()) elif op_type == \'extract\': member = operation.get(\'member\') with zipfile.ZipFile(zip_name, \'r\') as zipf: zipf.extract(member) results.append(f\\"File extracted: {member}\\") elif op_type == \'read\': member = operation.get(\'member\') with zipfile.ZipFile(zip_name, \'r\') as zipf: with zipf.open(member) as f: content = f.read().decode(\'utf-8\') results.append(content) else: results.append(f\\"Error: Unknown operation {op_type}\\") except Exception as e: results.append(f\\"Error: {str(e)}\\") return results"},{"question":"# Distributed Training with PyTorch **Objective:** Implement a simple distributed training loop using PyTorch. This task will assess your understanding of PyTorch\'s distributed training paradigms and your ability to implement a distributed training workflow. **Problem Statement:** You are given a simple neural network model and a dataset. Your task is to implement a distributed training loop using PyTorch\'s `torch.distributed` package. **Requirements:** 1. Initialize a distributed training environment using multiple processes. 2. Ensure that the training data is evenly distributed across the processes. 3. Implement a basic training loop using PyTorch. 4. Synchronize the gradients across all processes after each training step. **Constraints:** 1. Use CPU for training (CUDA tensors are not currently supported as per the warning in the documentation). 2. Your implementation should handle initialization and finalization of the distributed environment. **Code Template:** Below is a partially implemented code template for you to complete: ```python import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, Dataset # Simple dataset class SimpleDataset(Dataset): def __init__(self, size): self.data = torch.randn(size, 10) self.labels = torch.randint(0, 1, (size, 1)).float() def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.labels[idx] # Simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def train(rank, world_size, epochs): # Setup process groups for distributed training dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) dataset = SimpleDataset(size=1000) train_sampler = torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank) train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler) model = SimpleModel() model = nn.parallel.DistributedDataParallel(model, device_ids=None) criterion = nn.BCEWithLogitsLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): for batch_data, batch_labels in train_loader: optimizer.zero_grad() outputs = model(batch_data) loss = criterion(outputs, batch_labels) loss.backward() optimizer.step() # Cleanup process group dist.destroy_process_group() def main(): world_size = 4 # Number of processes for distributed training epochs = 5 # Number of epochs for training processes = [] for rank in range(world_size): p = torch.multiprocessing.Process(target=train, args=(rank, world_size, epochs)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": main() ``` **Instructions:** 1. Complete the `train` function implementation to ensure it works in a distributed setting. 2. Run the program using multiple processes. 3. Verify that the training process converges by outputting the loss at each epoch. **Notes:** - Use the `gloo` backend for distributed training since we are using CPU. - Ensure to handle the setup and cleanup of the distributed environment correctly. This question tests your understanding of distributed training with PyTorch, including data parallelism, process groups, and gradient synchronization.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, Dataset # Simple dataset class SimpleDataset(Dataset): def __init__(self, size): self.data = torch.randn(size, 10) self.labels = torch.randint(0, 2, (size, 1)).float() # Fixed label range def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.labels[idx] # Simple neural network model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def train(rank, world_size, epochs): # Setup process groups for distributed training dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) dataset = SimpleDataset(size=1000) train_sampler = torch.utils.data.distributed.DistributedSampler( dataset, num_replicas=world_size, rank=rank ) train_loader = DataLoader(dataset, batch_size=32, sampler=train_sampler) model = SimpleModel() model = nn.parallel.DistributedDataParallel(model) criterion = nn.BCEWithLogitsLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(epochs): epoch_loss = 0.0 for batch_data, batch_labels in train_loader: optimizer.zero_grad() outputs = model(batch_data) loss = criterion(outputs, batch_labels) loss.backward() optimizer.step() epoch_loss += loss.item() print(f\\"Rank {rank} - Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}\\") # Cleanup process group dist.destroy_process_group() def main(): world_size = 4 # Number of processes for distributed training epochs = 5 # Number of epochs for training processes = [] for rank in range(world_size): p = torch.multiprocessing.Process(target=train, args=(rank, world_size, epochs)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": main()"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},R={class:"card-container"},F={key:0,class:"empty-state"},q=["disabled"],L={key:0},O={key:1};function N(s,e,l,m,o,i){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>o.searchQuery=r),placeholder:"Search..."},null,512),[[y,o.searchQuery]]),o.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>o.searchQuery="")}," âœ• ")):d("",!0)]),t("div",R,[(a(!0),n(b,null,v(i.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),i.displayedPoems.length===0?(a(),n("div",F,' No results found for "'+c(o.searchQuery)+'". ',1)):d("",!0)]),i.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:o.isLoading,onClick:e[2]||(e[2]=(...r)=>i.loadMore&&i.loadMore(...r))},[o.isLoading?(a(),n("span",O,"Loading...")):(a(),n("span",L,"See more"))],8,q)):d("",!0)])}const M=p(D,[["render",N],["__scopeId","data-v-d6d7bdd3"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/37.md","filePath":"chatai/37.md"}'),j={name:"chatai/37.md"},X=Object.assign(j,{setup(s){return(e,l)=>(a(),n("div",null,[k(M)]))}});export{Y as __pageData,X as default};
