import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,i,r){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-f12b5001"]]),A=JSON.parse('[{"question":"**Objective:** Assess the student\'s ability to apply cross-validation techniques, use pipelines for preprocessing, and evaluate multiple metrics on a dataset. **Problem Statement:** You are provided with a dataset containing information on flowers (the well-known iris dataset), and your task is to build a Support Vector Machine (SVM) classifier to predict the class of the flowers. To ensure the robustness of your model, you need to use cross-validation techniques for evaluation. **Tasks:** 1. **Load the Data:** - Load the iris dataset from Scikit-learn. - Check the shape and basic statistics of the dataset. 2. **Data Preprocessing:** - Standardize the features of the dataset using `StandardScaler`. 3. **Model Creation:** - Create a Support Vector Machine (SVM) classifier with a linear kernel. 4. **Cross-Validation Setup:** - Use Stratified K-Fold cross-validation with 5 splits to ensure class distribution is maintained. - Define multiple evaluation metrics: `accuracy`, `precision_macro`, and `recall_macro`. 5. **Pipeline Creation:** - Create a pipeline that includes the StandardScaler and the SVM classifier. 6. **Model Evaluation:** - Evaluate the model using cross-validation on the defined metrics. - Print the mean and standard deviation of each metric across the folds. **Constraints:** - The use of loops and printing inside cross-validation is not allowed; leverage Scikit-learn\'s cross-validation functions instead. - Ensure reproducibility by setting random states as needed. **Performance Requirement:** - Your solution must handle the dataset efficiently and complete the cross-validation within a reasonable time frame. **Expected Input and Output Format:** *Input:* - None (the dataset is loaded within the code). *Output:* - Mean and standard deviation of the cross-validation scores for each metric. ```python # Write your code here import numpy as np from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate from sklearn import datasets from sklearn import svm from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline # Step 1: Load the Data X, y = datasets.load_iris(return_X_y=True) print(\\"Dataset shape:\\", X.shape) # Step 2: Data Preprocessing is done within the pipeline # Step 3: Model Creation is done within the pipeline # Step 4: Cross-Validation Setup is integrated with the cross_validate function # Step 5: Pipeline Creation pipeline = make_pipeline(StandardScaler(), svm.SVC(kernel=\'linear\', random_state=42)) # Step 6: Model Evaluation scoring = [\'accuracy\', \'precision_macro\', \'recall_macro\'] cv_results = cross_validate(pipeline, X, y, cv=StratifiedKFold(n_splits=5), scoring=scoring) # Print the mean and standard deviation of each metric for metric in scoring: print(f\\"{metric.capitalize()} - Mean: {cv_results[\'test_\' + metric].mean():.2f}, Std Dev: {cv_results[\'test_\' + metric].std():.2f}\\") ```","solution":"import numpy as np from sklearn.model_selection import StratifiedKFold, cross_validate from sklearn import datasets from sklearn import svm from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline def evaluate_svm(): # Step 1: Load the Data X, y = datasets.load_iris(return_X_y=True) # Step 2: Data Preprocessing (handled in the pipeline) # Step 3: Create SVM model (handled in the pipeline) # Step 4: Cross-Validation Setup is integrated with the cross_validate function # Step 5: Pipeline Creation pipeline = make_pipeline(StandardScaler(), svm.SVC(kernel=\'linear\', random_state=42)) # Step 6: Model Evaluation scoring = [\'accuracy\', \'precision_macro\', \'recall_macro\'] cv_results = cross_validate(pipeline, X, y, cv=StratifiedKFold(n_splits=5), scoring=scoring) # Prepare the results results = {} for metric in scoring: results[metric] = { \'mean\': cv_results[\'test_\' + metric].mean(), \'std_dev\': cv_results[\'test_\' + metric].std() } return results"},{"question":"# Challenging Coding Assessment: Random Projection and Inverse Transformation Problem Description You are provided with a high-dimensional dataset. Your task is to perform the following: 1. Reduce the dimensionality of the dataset using both Gaussian and Sparse random projections. 2. Transform the reduced dataset back to its original dimensionality. 3. Compare the initial dataset with the inversely transformed dataset to ensure the transformations were correctly implemented. Input - `data`: A 2D NumPy array of shape `(n_samples, n_features)`, representing the high-dimensional dataset. - `projection_type`: A string, either `\\"gaussian\\"` or `\\"sparse\\"`, indicating the type of random projection to use. - `n_components`: An integer, the number of dimensions to project down to. Output - A tuple consisting of: - `X_projected`: A 2D NumPy array of shape `(n_samples, n_components)`, representing the lower-dimensional dataset. - `X_inversed`: A 2D NumPy array of shape `(n_samples, n_features)`, representing the dataset transformed back to the original dimension. - `reconstruction_error`: A float, the mean squared error between the original dataset and the inversely transformed dataset. Constraints - `1 <= n_samples <= 10000` - `1 <= n_features <= 10000` - `1 <= n_components < n_features` - Use `scikit-learn`\'s `random_projection` module for the transformations. Performance Requirements - The solution should efficiently handle datasets with dimensions up to `(10000, 10000)`. Example ```python import numpy as np data = np.random.rand(100, 10000) projection_type = \\"gaussian\\" n_components = 500 (X_projected, X_inversed, reconstruction_error) = random_projection_analysis(data, projection_type, n_components) print(X_projected.shape) # Expected: (100, 500) print(X_inversed.shape) # Expected: (100, 10000) print(reconstruction_error) # Expected: A float value close to 0 ``` Implementation Write a function `random_projection_analysis` that performs the described operations: ```python from sklearn import random_projection import numpy as np from sklearn.metrics import mean_squared_error def random_projection_analysis(data, projection_type, n_components): if projection_type == \\"gaussian\\": transformer = random_projection.GaussianRandomProjection(n_components=n_components) elif projection_type == \\"sparse\\": transformer = random_projection.SparseRandomProjection(n_components=n_components, compute_inverse_components=True) else: raise ValueError(\\"Invalid projection type. Use \'gaussian\' or \'sparse\'.\\") # Fit and transform the data X_projected = transformer.fit_transform(data) # Inverse transform X_inversed = transformer.inverse_transform(X_projected) # Calculate mean squared error reconstruction_error = mean_squared_error(data, X_inversed) return X_projected, X_inversed, reconstruction_error ``` Ensure your implementation correctly handles the specified input-output format and adheres to the constraints and performance requirements.","solution":"from sklearn import random_projection import numpy as np from sklearn.metrics import mean_squared_error def random_projection_analysis(data, projection_type, n_components): if projection_type == \\"gaussian\\": transformer = random_projection.GaussianRandomProjection(n_components=n_components) elif projection_type == \\"sparse\\": transformer = random_projection.SparseRandomProjection(n_components=n_components, compute_inverse_components=True) else: raise ValueError(\\"Invalid projection type. Use \'gaussian\' or \'sparse\'.\\") # Fit and transform the data X_projected = transformer.fit_transform(data) # Inverse transform X_inversed = transformer.inverse_transform(X_projected) # Calculate mean squared error reconstruction_error = mean_squared_error(data, X_inversed) return X_projected, X_inversed, reconstruction_error"},{"question":"# XML Parsing and Handling with `xml.parsers.expat` **Objective:** Implement an XML parser using the `xml.parsers.expat` module to parse a given XML string. Your implementation should set up handlers for the start and end of elements, and for character data. It should also handle errors appropriately and provide meaningful error messages. **Task:** 1. Create an XML parser using `xml.parsers.expat.ParserCreate()`. 2. Implement handlers for: - Start of an element - End of an element - Character data 3. Parse the provided XML string. 4. If any errors occur during parsing, handle them using `ExpatError` and return an appropriate error message. **Specifications:** 1. **Input:** - A string representing a well-formed or malformed XML. 2. **Output:** - Print the start and end of each element along with its attributes. - Print character data within elements. - In case of an error, return a string with the format: `Error: {error_message}`, where `{error_message}` is the explanatory string for the encountered error. **Example Input:** ```python xml_data = <?xml version=\\"1.0\\"?> <root> <child name=\\"example\\">This is some text.</child> <child name=\\"example2\\">This is more text.</child> </root> ``` **Expected Output:** ``` Start element: root {} Character data: \'n \' Start element: child {\'name\': \'example\'} Character data: \'This is some text.\' End element: child Character data: \'n \' Start element: child {\'name\': \'example2\'} Character data: \'This is more text.\' End element: child Character data: \'n\' End element: root ``` **Example Input with Error:** ```python xml_data = <?xml version=\\"1.0\\"?> <root> <child name=\\"example\\">This is some text.<child> <child name=\\"example2\\">This is more text.</child> </root> ``` **Expected Output with Error:** ``` Error: mismatched tag ``` **Hints:** - Use `ParserCreate()` to instantiate the parser object. - Set handler functions for `StartElementHandler`, `EndElementHandler`, and `CharacterDataHandler`. - Use a try-except block to catch `ExpatError` and handle it gracefully using `ErrorString()` function. **Constraints:** - The XML string might be well-formed or contain errors. - Performance is not a critical issue for this assessment but code clarity and proper error handling are essential. Good luck and happy coding!","solution":"import xml.parsers.expat def start_element(name, attrs): print(f\\"Start element: {name} {attrs}\\") def end_element(name): print(f\\"End element: {name}\\") def char_data(data): if data.strip(): # To avoid printing empty character data print(f\\"Character data: \'{data}\'\\") def parse_xml(xml_data): parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data, True) except xml.parsers.expat.ExpatError as e: return f\\"Error: {e}\\" return \\"Parsing completed successfully.\\""},{"question":"<|Analysis Begin|> The provided documentation is for the `asynchat` module in Python, which is used for asynchronous socket command/response handling. The module builds on the asyncore infrastructure and is designed to simplify handling protocols with varied or string-terminated elements. It includes an abstract class `async_chat`, which needs to be subclassed to provide implementations for `collect_incoming_data()` and `found_terminator()` methods. Key points from the documentation: 1. `async_chat` is a subclass of `asyncore.dispatcher`. 2. Essential methods to implement in subclasses: - `collect_incoming_data(data)` - `found_terminator()` 3. The class supports setting terminators for recognizing end of data. 4. There\'s a FIFO queue for producers that handle data to be sent over the network. 5. The provided example demonstrates handling HTTP requests. The analysis reveals that the module\'s design relies on extending the `async_chat` class and implementing specific methods to cater to custom protocols. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are tasked with creating a custom asynchronous chat server using the `asynchat` module. This chat server will handle multiple clients, receive messages from them, and broadcast these messages to all connected clients. **Requirements:** 1. **Subclass `asynchat.async_chat` and implement the necessary methods:** - `collect_incoming_data(data)`: This method should collect the incoming data from clients. - `found_terminator()`: This method should handle the logic for a complete message, particularly broadcasting to all other clients. 2. **Manage client connections:** - Implement a class `ChatServer` that will handle incoming client connections. - Maintain a list of connected clients. 3. **Broadcast messages:** - When a message is received from a client, broadcast it to all other connected clients. # Input and Output - **Input:** - A client connects to the server and sends a message. - **Output:** - The message should be received by all other connected clients. # Constraints - The terminator for each message will be the newline character (`n`). # Example ```python import asynchat import asyncore import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock, server): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.server = server self.ibuffer = [] self.server.clients.append(self) def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): Broadcast the incoming message to all clients message = b\'\'.join(self.ibuffer) self.ibuffer = [] for client in self.server.clients: if client != self: client.push(message + self.get_terminator()) class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): pair = self.accept() if pair is not None: sock, addr = pair print(f\\"Incoming connection from {addr}\\") handler = ChatHandler(sock, self) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 8080) asyncore.loop() ``` In this example, create a chat server using `asynchat`. Each client in the chat gets messages broadcasted to them. Implement the `ChatHandler` class by subclassing `asynchat.async_chat` and writing the custom logic for `collect_incoming_data` and `found_terminator`. The `ChatServer` class manages incoming connections and client lists.","solution":"import asynchat import asyncore import socket class ChatHandler(asynchat.async_chat): def __init__(self, sock, server): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.server = server self.ibuffer = [] self.server.clients.append(self) def collect_incoming_data(self, data): Buffer the data self.ibuffer.append(data) def found_terminator(self): Broadcast the incoming message to all clients message = b\'\'.join(self.ibuffer) self.ibuffer = [] for client in self.server.clients: if client != self: client.push(message + self.get_terminator()) def handle_close(self): Remove the client from the server\'s list and close the connection self.server.clients.remove(self) self.close() class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): pair = self.accept() if pair is not None: sock, addr = pair print(f\\"Incoming connection from {addr}\\") handler = ChatHandler(sock, self)"},{"question":"**Objective:** Implement a Python function that demonstrates a comprehensive understanding of the `dataclasses` module, including defining data classes, handling default values, using frozen instances, and performing post-init processing. **Question:** You are tasked with implementing a class to represent a `LibraryBook`. The class should use Pythonâ€™s `dataclasses` module and should include the following requirements: 1. **Attributes:** - `title` (string): The title of the book. - `author` (string): The author of the book. - `publication_year` (integer): The year the book was published. - `isbn` (string): The ISBN number of the book. - `checked_out` (boolean): Whether the book is currently checked out. Defaults to `False`. - `borrower` (string, optional): The name of the borrower. Defaults to `None`. 2. **Behavior:** - The class should be **frozen** to prevent modification after creation. - Include a method `checkout` which sets `checked_out` to `True` and assigns a borrower. - Include a method `return_book` which sets `checked_out` to `False` and clears the borrower. - Use a default factory function to generate a default ISBN if one is not provided. 3. **Post-Init Processing:** - Ensure that the `publication_year` is not in the future during initialization. If it is, raise a `ValueError`. **Constraints:** - You may assume the current year is 2023 for the purposes of validation. - ISBN should be a non-empty string if provided. If a default is used, it should generate a pseudo-unique identifier. **Input/Output:** - **Input:** The input consists of initializing a `LibraryBook` object with `title`, `author`, `publication_year`, `isbn` (optional), and `borrower` (optional). - **Output:** The class methods should handle the internal state change and return appropriate messages or raise exceptions as needed. **Sample Usage:** ```python from dataclasses import dataclass, field from typing import Optional import random import datetime @dataclass(frozen=True) class LibraryBook: title: str author: str publication_year: int isbn: str = field(default_factory=lambda: f\\"ISBN{random.randint(1000, 9999)}\\") checked_out: bool = False borrower: Optional[str] = None def __post_init__(self): current_year = 2023 if self.publication_year > current_year: raise ValueError(\\"Publication year cannot be in the future.\\") if not self.isbn: raise ValueError(\\"ISBN cannot be empty.\\") def checkout(self, borrower_name: str): if self.checked_out: raise ValueError(\\"The book is already checked out.\\") # Since the class is frozen, create a new instance to simulate state change object.__setattr__(self, \'checked_out\', True) object.__setattr__(self, \'borrower\', borrower_name) def return_book(self): if not self.checked_out: raise ValueError(\\"The book is not checked out.\\") # Since the class is frozen, create a new instance to simulate state change object.__setattr__(self, \'checked_out\', False) object.__setattr__(self, \'borrower\', None) ``` # Requirements to Pass: 1. Create the `LibraryBook` data class as described. 2. Handle frozen behavior properly using post-init methods. 3. Provide the `checkout` and `return_book` methods as described. 4. Validate inputs according to the provided constraints.","solution":"from dataclasses import dataclass, field from typing import Optional import random @dataclass(frozen=True) class LibraryBook: title: str author: str publication_year: int isbn: str = field(default_factory=lambda: f\\"ISBN{random.randint(1000, 9999)}\\") checked_out: bool = False borrower: Optional[str] = None def __post_init__(self): current_year = 2023 if self.publication_year > current_year: raise ValueError(\\"Publication year cannot be in the future.\\") if not self.isbn: raise ValueError(\\"ISBN cannot be empty.\\") def checkout(self, borrower_name: str): if self.checked_out: raise ValueError(\\"The book is already checked out.\\") # Since the class is frozen, we need to use `object.__setattr__` to simulate state change object.__setattr__(self, \'checked_out\', True) object.__setattr__(self, \'borrower\', borrower_name) def return_book(self): if not self.checked_out: raise ValueError(\\"The book is not checked out.\\") # Since the class is frozen, we need to use `object.__setattr__` to simulate state change object.__setattr__(self, \'checked_out\', False) object.__setattr__(self, \'borrower\', None)"},{"question":"Objective You are required to demonstrate your understanding of managing environment variables using the `os` module in Python. The task should cover creating, modifying, and deleting environment variables, as well as handling scenarios where the variables might not exist. Problem Statement Write a Python function `manage_environment_variable(action, var_name, var_value=None)` that performs different actions (create, modify, delete, retrieve) on environment variables based on the specified action. - `action`: A string that indicates the action to be performed. It can be one of the following: - `\\"create\\"`: Create a new environment variable with the name `var_name` and value `var_value`. - `\\"modify\\"`: Modify an existing environment variable with the name `var_name` to have the value `var_value`. - `\\"delete\\"`: Delete the environment variable with the name `var_name`. - `\\"retrieve\\"`: Retrieve the value of the environment variable with the name `var_name`. - `var_name`: The name of the environment variable. - `var_value` (optional): The value to set for the environment variable. This is required for the `\\"create\\"` and `\\"modify\\"` actions. Requirements 1. If the `action` is `\\"create\\"` and the environment variable already exists, the function should raise an `EnvironmentError` with the message `\\"Environment variable already exists\\"`. 2. If the `action` is `\\"modify\\"` and the environment variable does not exist, the function should raise an `EnvironmentError` with the message `\\"Environment variable does not exist\\"`. 3. If the `action` is `\\"delete\\"` and the environment variable does not exist, the function should raise an `EnvironmentError` with the message `\\"Environment variable does not exist\\"`. 4. If the `action` is `\\"retrieve\\"` and the environment variable does not exist, the function should return `None`. 5. The function should use the `os` module for all operations. Input and Output Formats - Input: - `action`: A string, one of `\\"create\\"`, `\\"modify\\"`, `\\"delete\\"`, `\\"retrieve\\"`. - `var_name`: A string representing the name of the environment variable. - `var_value` (optional): A string representing the value to set for the environment variable (only required for `\\"create\\"` and `\\"modify\\"` actions). - Output: - For `\\"retrieve\\"` action: Return the value of the environment variable or `None` if it does not exist. - For other actions: Return `None`. Raise appropriate errors as mentioned in the requirements. Example ```python # Example usage: manage_environment_variable(\\"create\\", \\"MY_VAR\\", \\"VALUE\\") print(manage_environment_variable(\\"retrieve\\", \\"MY_VAR\\")) # Output: VALUE manage_environment_variable(\\"modify\\", \\"MY_VAR\\", \\"NEW_VALUE\\") print(manage_environment_variable(\\"retrieve\\", \\"MY_VAR\\")) # Output: NEW_VALUE manage_environment_variable(\\"delete\\", \\"MY_VAR\\") print(manage_environment_variable(\\"retrieve\\", \\"MY_VAR\\")) # Output: None ``` Constraints - You may assume that the `action` parameter will always be one of the specified strings. - The environment variable names and values will be valid strings. Implement the `manage_environment_variable` function according to the given problem statement and requirements.","solution":"import os def manage_environment_variable(action, var_name, var_value=None): Manages environment variables by creating, modifying, deleting, or retrieving them. :param action: Action to perform (\\"create\\", \\"modify\\", \\"delete\\", \\"retrieve\\"). :param var_name: Name of the environment variable. :param var_value: Value for the environment variable (required for \\"create\\" and \\"modify\\"). :return: Value of the variable for \\"retrieve\\" action, None otherwise. if action == \\"create\\": if var_name in os.environ: raise EnvironmentError(\\"Environment variable already exists\\") os.environ[var_name] = var_value elif action == \\"modify\\": if var_name not in os.environ: raise EnvironmentError(\\"Environment variable does not exist\\") os.environ[var_name] = var_value elif action == \\"delete\\": if var_name not in os.environ: raise EnvironmentError(\\"Environment variable does not exist\\") del os.environ[var_name] elif action == \\"retrieve\\": return os.environ.get(var_name, None) return None"},{"question":"**Question: Dynamic Module Importing with `importlib`** The objective of this assignment is to write a function that dynamically imports modules provided as input and returns specific attributes or information about these modules. You are required to use the `importlib` library to perform these imports. # Function Specification ```python def dynamic_import(module_name: str, attribute_name: str): Dynamically imports a module and retrieves a specific attribute or provides information about the module. Args: - module_name (str): The name of the module to be imported. - attribute_name (str): The attribute from the module which needs to be accessed. Returns: - The requested attribute if it exists. - Raises Import Error if the module cannot be imported. - Raises AttributeError if the attribute does not exist in the module. pass ``` # Input - `module_name`: A string representing the name of the module. - `attribute_name`: A string representing the attribute to be retrieved from the module. # Output - Returns the value of the requested attribute if it exists. - If the module cannot be imported, raise an `ImportError`. - If the attribute does not exist in the module, raise an `AttributeError`. # Constraints - You must use the `importlib` module for importing. - You should handle the exceptions properly to provide meaningful error messages. - The module name provided could be any standard library module or a third-party module (if available in the environment). # Example ```python # Example Usage & Expected Output # Assuming `math` module and needing to retrieve the `pi` attribute. print(dynamic_import(\'math\', \'pi\')) # Expected Output: 3.141592653589793 # Attempting to import a non-existent module should raise an ImportError. print(dynamic_import(\'non_existent_module\', \'attr\')) # Expected Output: Raises ImportError # Attempting to retrieve a non-existent attribute should raise an AttributeError. print(dynamic_import(\'math\', \'non_existent_attr\')) # Expected Output: Raises AttributeError ``` # Additional Notes - Ensure you include docstrings specifying the function usage. - Demonstrate usage of your function with multiple test cases to verify correctness and error handling. - Optimize your function for readability and efficiency where possible.","solution":"import importlib def dynamic_import(module_name: str, attribute_name: str): Dynamically imports a module and retrieves a specific attribute or provides information about the module. Args: - module_name (str): The name of the module to be imported. - attribute_name (str): The attribute from the module which needs to be accessed. Returns: - The requested attribute if it exists. - Raises Import Error if the module cannot be imported. - Raises AttributeError if the attribute does not exist in the module. try: module = importlib.import_module(module_name) except ImportError: raise ImportError(f\\"Module \'{module_name}\' cannot be imported\\") try: return getattr(module, attribute_name) except AttributeError: raise AttributeError(f\\"Module \'{module_name}\' does not have an attribute \'{attribute_name}\'\\")"},{"question":"Objective You are required to demonstrate your comprehension of Kernel Ridge Regression and Support Vector Regression. Implement a function to perform regression using both methods on a given dataset, compare their performances, and visualize the results. Description Implement a function `compare_krr_svr(X_train, y_train, X_test, y_test)` that: 1. Fits both `KernelRidge` and `SVR` models on the provided training dataset (`X_train`, `y_train`). 2. Makes predictions on the test dataset (`X_test`). 3. Evaluates the mean squared error (MSE) of the predictions for both models. 4. Generates a plot that shows: - The true values vs. predicted values for both models. - The MSE values for both models in the plot\'s legend or title. Function Signature ```python def compare_krr_svr(X_train, y_train, X_test, y_test): pass ``` Input - `X_train`: numpy array of shape (n_samples_train, n_features), training data. - `y_train`: numpy array of shape (n_samples_train,), target values corresponding to `X_train`. - `X_test`: numpy array of shape (n_samples_test, n_features), test data. - `y_test`: numpy array of shape (n_samples_test,), true target values corresponding to `X_test`. Output - This function does not return any value but should display a plot as described. Constraints - Use an RBF kernel for both models. - Optimize the hyperparameters using grid search. - Assume the dataset fits in memory. Example ```python import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split # Generating synthetic dataset X, y = make_regression(n_samples=200, n_features=1, noise=0.1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Function call compare_krr_svr(X_train, y_train, X_test, y_test) ``` Notes - Use scikit-learn\'s `KernelRidge` and `SVR` estimators. - You may use matplotlib or seaborn for plotting. - Visualize prediction performance clearly and concisely. - Document your code properly.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.metrics import mean_squared_error from sklearn.model_selection import GridSearchCV def compare_krr_svr(X_train, y_train, X_test, y_test): # Define parameter grids param_grid_krr = {\'alpha\': [1e-3, 1e-2, 1e-1, 1, 10], \'kernel\': [\'rbf\'], \'gamma\': np.logspace(-2, 2, 5)} param_grid_svr = {\'C\': [1e-2, 1e-1, 1, 10, 100], \'kernel\': [\'rbf\'], \'gamma\': np.logspace(-2, 2, 5)} # Kernel Ridge Regression model krr = KernelRidge() krr_cv = GridSearchCV(krr, param_grid_krr, cv=5, scoring=\'neg_mean_squared_error\') krr_cv.fit(X_train, y_train) y_pred_krr = krr_cv.predict(X_test) mse_krr = mean_squared_error(y_test, y_pred_krr) # Support Vector Regression model svr = SVR() svr_cv = GridSearchCV(svr, param_grid_svr, cv=5, scoring=\'neg_mean_squared_error\') svr_cv.fit(X_train, y_train) y_pred_svr = svr_cv.predict(X_test) mse_svr = mean_squared_error(y_test, y_pred_svr) # Plotting plt.figure(figsize=(12, 6)) plt.scatter(y_test, y_pred_krr, color=\'red\', label=f\'KRR Predictions, MSE={mse_krr:.2f}\') plt.scatter(y_test, y_pred_svr, color=\'blue\', label=f\'SVR Predictions, MSE={mse_svr:.2f}\') plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color=\'black\', lw=3, ls=\'--\') plt.xlabel(\\"True Values\\") plt.ylabel(\\"Predicted Values\\") plt.legend() plt.title(\\"True vs Predicted Values for KRR and SVR\\") plt.show()"},{"question":"# Password Database Analysis Problem Statement: You are provided with access to the Unix password database through the `pwd` module in Python. Your task is to write a function `get_users_with_shell(shell_name)` that takes a string `shell_name` as input and returns a list of login names of all users who use the specified shell. Requirements: 1. The function should use the `pwd.getpwall()` method to fetch all user account entries. 2. Filter and return the login names of users whose `pw_shell` attribute matches the `shell_name` provided. 3. Ensure that the login names are unique and sorted in alphabetical order. Input: - `shell_name`: A string representing the shell (e.g., \'/bin/bash\', \'/usr/bin/zsh\'). Output: - A list of strings where each string is a login name of a user using the specified shell. Constraints: - You can assume that the system contains a reasonable number of user entries that can fit into memory. - The `shell_name` will be a non-empty string. - The function should handle the case where no users use the specified shell by returning an empty list. Example: ```python # Assuming the password database contains the following entries: # [ # pwd.struct_passwd(pw_name=\'user1\', pw_passwd=\'x\', pw_uid=1001, pw_gid=1001, pw_gecos=\'User One\', pw_dir=\'/home/user1\', pw_shell=\'/bin/bash\'), # pwd.struct_passwd(pw_name=\'user2\', pw_passwd=\'x\', pw_uid=1002, pw_gid=1002, pw_gecos=\'User Two\', pw_dir=\'/home/user2\', pw_shell=\'/bin/zsh\'), # pwd.struct_passwd(pw_name=\'user3\', pw_passwd=\'x\', pw_uid=1003, pw_gid=1003, pw_gecos=\'User Three\', pw_dir=\'/home/user3\', pw_shell=\'/bin/bash\') # ] print(get_users_with_shell(\'/bin/bash\')) # Output: [\'user1\', \'user3\'] print(get_users_with_shell(\'/bin/zsh\')) # Output: [\'user2\'] print(get_users_with_shell(\'/bin/fish\')) # Output: [] ```","solution":"import pwd def get_users_with_shell(shell_name): Returns a list of login names of all users who use the specified shell. The list is sorted in alphabetical order and contains unique login names. Parameters: shell_name (str): The name of the shell (e.g., \'/bin/bash\', \'/usr/bin/zsh\'). Returns: list: A sorted list of unique login names. all_users = pwd.getpwall() users_with_shell = [user.pw_name for user in all_users if user.pw_shell == shell_name] return sorted(set(users_with_shell))"},{"question":"**Question: GZIP File Operations** You are tasked with implementing a Python function that utilizes the **gzip** module to compress and decompress text data from files. Your implementation needs to demonstrate a good understanding of file operations, error handling, and the use of the **gzip** module. **Function Signature:** ```python def gzip_file_operations(input_filepath: str, output_filepath: str, operation: str, compresslevel: int = 9) -> None: Perform GZIP compression or decompression on the specified file. Args: input_filepath (str): Path to the input file. output_filepath (str): Path where the output file will be stored. operation (str): The operation to perform: \'compress\' or \'decompress\'. compresslevel (int): The compression level (default is 9). Raises: ValueError: If the `operation` parameter is not \'compress\' or \'decompress\'. gzip.BadGzipFile: If the input gzip file is invalid when decompressing. ``` **Requirements:** 1. If the `operation` is `\'compress\'`: - Read the input file in binary mode. - Compress the data using the **gzip** module. - Write the compressed data to the output file in binary mode. 2. If the `operation` is `\'decompress\'`: - Read the input gzip file in binary mode. - Decompress the data using the **gzip** module. - Write the decompressed data to the output file in binary mode. 3. Handle the scenario where the input gzip file is invalid by raising a `gzip.BadGzipFile` exception. 4. Raise a `ValueError` if the `operation` parameter is not `\'compress\'` or `\'decompress\'`. 5. Use a compression level from 0 to 9, with a default value of 9. 6. Ensure proper resource management by using context managers (`with` statement) for file operations. **Example Usage:** ```python # To compress a file: gzip_file_operations(\'example.txt\', \'example.txt.gz\', \'compress\') # To decompress a file: gzip_file_operations(\'example.txt.gz\', \'example_decompressed.txt\', \'decompress\') ``` **Constraints:** - The input file must exist and be readable. - For decompression, the input file must be a valid gzip file. - Assume the system has sufficient memory to handle the size of the files being processed. **Note:** Do not use any additional libraries other than the built-in **gzip** module. The implementation should not include any print statements or input handling from the user; the function should rely solely on the provided arguments.","solution":"import gzip import os def gzip_file_operations(input_filepath: str, output_filepath: str, operation: str, compresslevel: int = 9) -> None: Perform GZIP compression or decompression on the specified file. Args: input_filepath (str): Path to the input file. output_filepath (str): Path where the output file will be stored. operation (str): The operation to perform: \'compress\' or \'decompress\'. compresslevel (int): The compression level (default is 9). Raises: ValueError: If the `operation` parameter is not \'compress\' or \'decompress\'. gzip.BadGzipFile: If the input gzip file is invalid when decompressing. if operation not in [\'compress\', \'decompress\']: raise ValueError(\\"The `operation` parameter must be either \'compress\' or \'decompress\'\\") if operation == \'compress\': with open(input_filepath, \'rb\') as f_in: with gzip.open(output_filepath, \'wb\', compresslevel=compresslevel) as f_out: f_out.writelines(f_in) elif operation == \'decompress\': with gzip.open(input_filepath, \'rb\') as f_in: with open(output_filepath, \'wb\') as f_out: f_out.writelines(f_in)"},{"question":"You are tasked with creating a Python program that manages terminal settings to execute a sequence of controlled input and output operations. Your program should demonstrate the use of various \\"termios\\" functions to: 1. Modify terminal attributes for specific behaviors. 2. Control the flow of data through the terminal. 3. Handle any exceptions to ensure terminal settings are properly restored. # Task Implement a function `controlled_terminal_operations(prompt, input_data, break_duration)` that performs the following steps: 1. Disable echoing of characters typed by the user to ensure privacy (e.g., entering a password). 2. Prompt the user for input with the provided `prompt` string. 3. Validate the input against the provided `input_data`. If the input is incorrect, send a break signal with the specified `break_duration` and prompt the user again. 4. After 3 incorrect attempts, flush both the input and output queues. 5. If the input is correct within 3 attempts, proceed to suspend and then resume the terminal input/output to simulate controlled data flow. 6. Ensure the original terminal attributes are restored at the end, regardless of success or failure. # Input - `prompt` (str): The prompt message displayed to the user. - `input_data` (str): The correct input data that the user should provide. - `break_duration` (int): The duration (in seconds) for which to send a break signal when input is incorrect. # Output - (bool): Return `True` if correct input is received within 3 attempts, otherwise `False`. # Constraints - Handle terminal attributes and flow controls using the `termios` module. - Ensure that the terminal attributes are restored properly even after encountering exceptions. # Example ```python def controlled_terminal_operations(prompt, input_data, break_duration): import termios, sys fd = sys.stdin.fileno() old_attrs = termios.tcgetattr(fd) new_attrs = termios.tcgetattr(fd) new_attrs[3] = new_attrs[3] & ~termios.ECHO attempts = 0 success = False try: while attempts < 3: termios.tcsetattr(fd, termios.TCSADRAIN, new_attrs) user_input = input(prompt) if user_input == input_data: success = True break else: termios.tcsendbreak(fd, break_duration) attempts += 1 if not success: termios.tcflush(fd, termios.TCIOFLUSH) else: termios.tcflow(fd, termios.TCIOFF) termios.tcflow(fd, termios.TCION) finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_attrs) return success # Example usage: print(controlled_terminal_operations(\\"Enter password: \\", \\"secret\\", 1)) ``` # Notes: - Ensure to import necessary modules within the function. - Use proper exception handling to restore terminal attributes in the `finally` block.","solution":"def controlled_terminal_operations(prompt, input_data, break_duration): import termios, sys fd = sys.stdin.fileno() old_attrs = termios.tcgetattr(fd) new_attrs = termios.tcgetattr(fd) new_attrs[3] = new_attrs[3] & ~termios.ECHO attempts = 0 success = False try: while attempts < 3: termios.tcsetattr(fd, termios.TCSADRAIN, new_attrs) user_input = input(prompt) if user_input == input_data: success = True break else: termios.tcsendbreak(fd, break_duration) attempts += 1 if not success: termios.tcflush(fd, termios.TCIOFLUSH) else: termios.tcflow(fd, termios.TCIOFF) termios.tcflow(fd, termios.TCION) finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_attrs) return success"},{"question":"<|Analysis Begin|> The provided documentation offers detailed information about the `pdb` module, Python\'s interactive source code debugger. Key features include setting breakpoints, stepping through code, inspecting stack frames, and more. The `pdb` module also allows for the evaluation of arbitrary Python code in the context of any stack frame and supports post-mortem debugging. The module primarily consists of the `Pdb` class and several methods, such as `run`, `runeval`, `runcall`, `set_trace`, `post_mortem`, and several debugging commands like `step`, `next`, `continue`, `return`, among others. The `pdb` module enables users to set conditional breakpoints, list source code, print variable values, and execute arbitrary Python statements. A suitable coding assessment question should: 1. Challenge students to use multiple features of the `pdb` module. 2. Test both basic and advanced uses of debugging. 3. Require students to implement functions and demonstrate their understanding of debugging principles. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are given a buggy Python program named `example_program.py` with the implementation of a basic calculator. Your task is to debug and correct the errors using the `pdb` module. You will also need to provide a detailed explanation of how you identified and fixed each error using `pdb`. Here is the content of `example_program.py`: ```python class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): return a / b def test_calculator(): calc = Calculator() assert calc.add(1, 2) == 3 assert calc.subtract(5, 3) == 2 assert calc.multiply(4, 3) == 12 assert calc.divide(10, 2) == 5 assert calc.divide(10, 0) == \\"Undefined\\" # This line will generate an exception if __name__ == \\"__main__\\": test_calculator() ``` The above program contains an intentional bug in the `test_calculator` function, where the division by zero check is missing, resulting in a runtime exception. **Your tasks are:** 1. Use the `pdb` module to debug the above program and identify the exact cause of the error. 2. Modify the `divide` method of `Calculator` class to handle division by zero errors gracefully. 3. Document the debugging process and the steps you took to identify and correct the error. **Steps to follow:** 1. Insert necessary `pdb` breakpoints in the script to step through and analyze the code. 2. Identify where the error occurs using `pdb` commands such as `step`, `next`, `continue`, etc. 3. Modify the `divide` method to handle division by zero and return \\"Undefined\\" when the second argument is zero. 4. Verify that all assertions in the `test_calculator` function pass after applying the fix. **Output format:** 1. **Modified `example_program.py`**: ```python class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: return \\"Undefined\\" return a / b def test_calculator(): calc = Calculator() assert calc.add(1, 2) == 3 assert calc.subtract(5, 3) == 2 assert calc.multiply(4, 3) == 12 assert calc.divide(10, 2) == 5 assert calc.divide(10, 0) == \\"Undefined\\" if __name__ == \\"__main__\\": test_calculator() ``` 2. **Explanation**: - Describe where you set breakpoints and what pdb commands you used. - Explain how you identified the error. - Detail the changes you made to the `divide` method. - Confirm that the program works correctly after the modifications, with proof that all assertions pass. **Constraints:** - Do not remove the assertions or alter their conditions other than fixing the division by zero issue. - You must use the `pdb` module for debugging purposes; manual identification of the bug without `pdb` will not be accepted.","solution":"class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: return \\"Undefined\\" return a / b def test_calculator(): calc = Calculator() assert calc.add(1, 2) == 3 assert calc.subtract(5, 3) == 2 assert calc.multiply(4, 3) == 12 assert calc.divide(10, 2) == 5 assert calc.divide(10, 0) == \\"Undefined\\" if __name__ == \\"__main__\\": test_calculator() # Explanation: # 1. I started by setting a breakpoint at the start of the `test_calculator` function using pdb. # This helped to step into the test cases one by one. # 2. Running the script and stepping into the divide function call when the second argument is 0 # helped identify the exact line where the error occurs. # 3. Using `step` and `next` commands, I found that the issue is with division by zero which raises # a ZeroDivisionError. # 4. I modified the `divide` method of the Calculator class to check if the divisor is zero, if so, it returns \\"Undefined\\". # 5. After making this change, I verified all assertions in the `test_calculator` function to confirm they pass correctly."},{"question":"# Advanced Python Coding Assessment: Handling Multiple Context Managers with `ExitStack` Objective Create a Python function that efficiently manages multiple file resources using the `contextlib.ExitStack` class. This exercise tests your understanding of handling multiple context managers programmatically, ensuring proper resource management even when dealing with user input or conditional requirements. Problem Statement Write a function `process_files` that takes a list of file names and a flag indicating whether to create a special temporary file. Your function should: 1. Open and read all the files provided in the list of file names. 2. If the flag `create_special_file` is `True`, create a temporary file and write \\"Special file content\\" to it. 3. Store the content of each file in a dictionary where the key is the file name, and the value is the file\'s content. 4. Ensure all files are properly closed after reading, including the temporary file if created. 5. Return the dictionary containing the file contents. Use the `contextlib.ExitStack` to manage multiple context managers. Input - `file_names`: A list of strings containing file paths to open and read. - `create_special_file`: A boolean flag indicating whether to create and write to a temporary file. Output - A dictionary where the keys are the file names and the values are strings containing the content of each file. Function Signature ```python from typing import List, Dict import contextlib import tempfile def process_files(file_names: List[str], create_special_file: bool) -> Dict[str, str]: pass ``` Constraints - All files referenced in `file_names` will exist and be readable. - If `create_special_file` is `True`, ensure proper creation and cleanup of the temporary file. - Handle exceptions gracefully to ensure all resources are released even in the event of an error. Example ```python file_content = process_files([\\"file1.txt\\", \\"file2.txt\\"], create_special_file=True) print(file_content) # Output: {\'file1.txt\': \'content of file1\', ... \'tempfile\': \'Special file content\'} ``` Notes - Use the `contextlib.ExitStack()` to manage the opening and closing of multiple files. - Use the `tempfile` module to create the temporary file if needed.","solution":"from typing import List, Dict import contextlib import tempfile def process_files(file_names: List[str], create_special_file: bool) -> Dict[str, str]: contents = {} with contextlib.ExitStack() as stack: # Open all files and read their contents for file_name in file_names: file = stack.enter_context(open(file_name, \'r\')) contents[file_name] = file.read() # If create_special_file is True, create a temporary file if create_special_file: temp_file = stack.enter_context(tempfile.NamedTemporaryFile(delete=True)) temp_file_name = temp_file.name temp_file.write(b\\"Special file content\\") temp_file.seek(0) # Go back to the beginning of the file contents[temp_file_name] = temp_file.read().decode(\'utf-8\') return contents"},{"question":"# MIME Email Construction Objective: Design a Python function to create a complex MIME email containing different types of attachments such as text, image, and audio, using the `email.mime` module. Task: Write a function `create_mime_email(subject, sender, recipients, text_content, image_data, audio_data)` which performs the following steps: 1. Creates a `MIMEMultipart` message with a root MIME type of `mixed`. 2. Adds the subject, sender, and recipient headers to the root message. 3. Attaches the provided text content as a `MIMEText` part. 4. Attaches the provided image data as a `MIMEImage` part. 5. Attaches the provided audio data as a `MIMEAudio` part. 6. Returns the root `MIMEMultipart` message. Input: - `subject` (str): The email subject. - `sender` (str): The email sender. - `recipients` (list of str): The email recipients. - `text_content` (str): The plain text content of the email. - `image_data` (bytes): The raw data of the image to be attached. - `audio_data` (bytes): The raw data of the audio to be attached. Output: - A `MIMEMultipart` email message object containing the text, image, and audio parts. Constraints: - Properly handle the `Content-Type`, `Content-Transfer-Encoding`, and other relevant headers for each part. - Use base64 encoding for image and audio attachments. - Ensure that the email is correctly structured to be sent through an SMTP server after creation. Example: ```python from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio def create_mime_email(subject, sender, recipients, text_content, image_data, audio_data): # Create the root message msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) # Attach text content text_part = MIMEText(text_content, \'plain\') msg.attach(text_part) # Attach image image_part = MIMEImage(image_data) msg.attach(image_part) # Attach audio audio_part = MIMEAudio(audio_data) msg.attach(audio_part) return msg # Example Usage subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipients = [\\"recipient1@example.com\\", \\"recipient2@example.com\\"] text_content = \\"This is a test email with text, image, and audio.\\" image_data = open(\'test_image.png\', \'rb\').read() # replace with actual image file path audio_data = open(\'test_audio.wav\', \'rb\').read() # replace with actual audio file path email_message = create_mime_email(subject, sender, recipients, text_content, image_data, audio_data) ``` Notes: - Test the function with actual files for `image_data` and `audio_data` to ensure the MIME message is correctly formed. - The outputted `MIMEMultipart` message object can be further used with Python\'s `smtplib` to send email.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio def create_mime_email(subject, sender, recipients, text_content, image_data, audio_data): Creates a MIME email with text, image, and audio attachments. Args: subject (str): The email subject. sender (str): The email sender. recipients (list of str): The email recipients. text_content (str): The plain text content of the email. image_data (bytes): The raw data of the image to be attached. audio_data (bytes): The raw data of the audio to be attached. Returns: MIMEMultipart: The constructed MIME email. # Create the root message msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = \', \'.join(recipients) # Attach text content text_part = MIMEText(text_content, \'plain\') msg.attach(text_part) # Attach image image_part = MIMEImage(image_data) msg.attach(image_part) # Attach audio audio_part = MIMEAudio(audio_data) msg.attach(audio_part) return msg"},{"question":"on `unittest` Module You are required to write a Python script that defines tests for a sample class using the `unittest` module. The test should cover various functionalities and edge cases for the class. Your task involves creating both the sample class and an accompanying test suite. **Scenario**: You need to create a class named `MathOperations` which has the following methods: 1. `add(a, b)` - returns the sum of `a` and `b`. 2. `subtract(a, b)` - returns the subtraction result of `b` from `a`. 3. `multiply(a, b)` - returns the product of `a` and `b`. 4. `divide(a, b)` - returns the division result of `a` by `b`. If `b` is 0, it should raise a `ValueError` with the message \\"Cannot divide by zero\\". **Requirements**: 1. Define the class `MathOperations` with the methods listed above. 2. Create a separate test class named `TestMathOperations` which inherits from `unittest.TestCase`. 3. Implement the following test methods: - `test_add`: Verify the `add` method with multiple test cases. - `test_subtract`: Verify the `subtract` method with multiple test cases. - `test_multiply`: Verify the `multiply` method with multiple test cases. - `test_divide`: Verify the `divide` method with multiple test cases, including a test to check the `ValueError` when dividing by zero. 4. Use `setUp` method to initialize any prerequisites for the tests. 5. Include one example of subTest with `test_subtract` method. 6. Ensure the tests are run with all the assertions to validate correctness. 7. The test should be able to run when the script is executed directly. **Constraints**: - Ensure you follow the unittest structure and naming conventions. - Make use of assert methods like `assertEqual`, `assertTrue`, `assertRaises`, and `assertNotAlmostEqual` where appropriate. - Aim to achieve high code coverage by considering edge cases. **Sample Usage**: ```python if __name__ == \'__main__\': unittest.main(verbosity=2) ``` **Expected Output**: 1. Implementation of `MathOperations` class. 2. Comprehensive `TestMathOperations` class with required tests. 3. Ensure all tests pass, producing an output that shows the test cases run, and the results (pass/fail).","solution":"class MathOperations: Class that performs basic math operations. def add(self, a, b): Returns the sum of a and b. return a + b def subtract(self, a, b): Returns the subtraction result of b from a. return a - b def multiply(self, a, b): Returns the product of a and b. return a * b def divide(self, a, b): Returns the division result of a by b. Raises a ValueError when b is zero. if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b"},{"question":"**Coding Assessment Question: Python310 Bytearray Manipulation** **Objective:** You will write Python code that reinforces your understanding of handling bytearray objects using Python310\'s `PyByteArray_Type` and functions related to it. Your task is to implement a function that processes bytearrays in several steps. **Problem Statement:** Write a Python function `process_bytearrays` that takes two string inputs and an integer: ```python def process_bytearrays(str1: str, str2: str, new_size: int) -> bytearray: ``` 1. Convert `str1` and `str2` to bytearrays using the Python310 API functions. 2. Concatenate the two bytearrays. 3. Resize the concatenated bytearray to `new_size`. 4. Return this resized bytearray. **Inputs:** - `str1` (string): The first input string to be converted to a bytearray. - `str2` (string): The second input string to be converted to a bytearray. - `new_size` (int): The new size to which the concatenated bytearray should be resized. **Output:** - The function should return a `bytearray` object that is the result of the described process. **Constraints:** - `new_size` will be a non-negative integer and can be smaller or larger than the combined length of `str1` and `str2`. **Example:** ```python ba = process_bytearrays(\\"hello\\", \\"world\\", 8) # Initial bytearrays: b\'hello\' and b\'world\' # After concatenation: b\'helloworld\' # After resizing to 8, result: b\'hellowor\' assert ba == b\'hellowor\' ``` **Notes:** - Use the `PyByteArray_FromStringAndSize`, `PyByteArray_Concat`, and `PyByteArray_Resize` API functions to manipulate the bytearrays. - Ensure proper error checking and handling when using these API functions. You can assume the availability of the Python310 C API in your environment. Your task is to simulate this behavior with a provided Python interface, focusing particularly on the correct use of the API functions and the desired outcomes.","solution":"def process_bytearrays(str1: str, str2: str, new_size: int) -> bytearray: # Convert strings to bytearrays ba1 = bytearray(str1, \'utf-8\') ba2 = bytearray(str2, \'utf-8\') # Concatenate bytearrays concatenated_ba = ba1 + ba2 # Resize the concatenated bytearray concatenated_ba = concatenated_ba[:new_size] added_part = bytearray(new_size - len(concatenated_ba)) return concatenated_ba + added_part"},{"question":"**Objective**: The goal of this assessment is to evaluate your understanding of the Copy-on-Write (CoW) mechanism in pandas and your ability to manipulate pandas DataFrames while adhering to CoW principles. **Scenario**: You are given a DataFrame representing sales data with columns \\"product\\", \\"price\\", \\"quantity\\", and \\"discount\\". Your task is to implement functions to perform various operations on this DataFrame while ensuring any updates do not have unintended side effects on other DataFrame objects sharing the same underlying data. **Tasks**: 1. **Reading Data**: Define a function `load_data` that reads a CSV file into a pandas DataFrame. 2. **Applying Discounts**: Define a function `apply_discount(df: pd.DataFrame, threshold: float, discount_rate: float) -> pd.DataFrame` that sets the discount for products with a price above the `threshold` to `discount_rate`. Ensure no unintended modifications to other DataFrame objects sharing the same data. 3. **Calculating Total Sales**: Define a function `calculate_total_sales(df: pd.DataFrame) -> float` that calculates the total sales considering the `discount`. The formula to calculate the sale for each product is `quantity * price * (1 - discount)`. 4. **Resetting Discounts**: Define a function `reset_discounts(df: pd.DataFrame) -> pd.DataFrame` that resets all discounts in the DataFrame to zero without altering any other object sharing the same underlying data. **Constraints**: - The input DataFrame for all functions should be assumed to adhere to the schema specified (columns: \\"product\\", \\"price\\", \\"quantity\\", \\"discount\\"). - Use `iloc` or `loc` for any updates to avoid chained assignment issues. - Ensure functions do not inadvertently modify other DataFrame objects sharing the same data (compliant with CoW principles). **Input and Output Formats**: - `load_data(filepath: str) -> pd.DataFrame`: Load data from a CSV file at the specified path. - `apply_discount(df: pd.DataFrame, threshold: float, discount_rate: float) -> pd.DataFrame`: Returns a new DataFrame with applied discounts. - `calculate_total_sales(df: pd.DataFrame) -> float`: Returns the total sales as a float. - `reset_discounts(df: pd.DataFrame) -> pd.DataFrame`: Returns a new DataFrame with discounts reset to zero. **Example**: ```python # Assuming the CSV file \'sales_data.csv\' has the following content: # product,price,quantity,discount # A,10.0,5,0.0 # B,20.0,2,0.0 # C,30.0,7,0.0 import pandas as pd def load_data(filepath: str) -> pd.DataFrame: return pd.read_csv(filepath) def apply_discount(df: pd.DataFrame, threshold: float, discount_rate: float) -> pd.DataFrame: new_df = df.copy() new_df.loc[new_df[\'price\'] > threshold, \'discount\'] = discount_rate return new_df def calculate_total_sales(df: pd.DataFrame) -> float: return (df[\'quantity\'] * df[\'price\'] * (1 - df[\'discount\'])).sum() def reset_discounts(df: pd.DataFrame) -> pd.DataFrame: new_df = df.copy() new_df[\'discount\'] = 0.0 return new_df df_original = load_data(\'sales_data.csv\') df_discounted = apply_discount(df_original, 15.0, 0.1) total_sales = calculate_total_sales(df_discounted) df_reset = reset_discounts(df_discounted) print(f\\"Original DataFrame: n{df_original}\\") print(f\\"Discounted DataFrame: n{df_discounted}\\") print(f\\"Total Sales: {total_sales}\\") print(f\\"Discounts Reset DataFrame: n{df_reset}\\") ``` No copies of dataframes should be unintentionally modified.","solution":"import pandas as pd def load_data(filepath: str) -> pd.DataFrame: Loads data from a CSV file into a pandas DataFrame. return pd.read_csv(filepath) def apply_discount(df: pd.DataFrame, threshold: float, discount_rate: float) -> pd.DataFrame: Sets the discount for products with a price above the threshold to the discount_rate. Ensures no unintended modifications to other DataFrame objects sharing the same data. new_df = df.copy() new_df.loc[new_df[\'price\'] > threshold, \'discount\'] = discount_rate return new_df def calculate_total_sales(df: pd.DataFrame) -> float: Calculates the total sales considering the discount. Formula for each product: quantity * price * (1 - discount) return (df[\'quantity\'] * df[\'price\'] * (1 - df[\'discount\'])).sum() def reset_discounts(df: pd.DataFrame) -> pd.DataFrame: Resets all discounts in the DataFrame to zero. Ensures no unintended modifications to other DataFrame objects sharing the same data. new_df = df.copy() new_df[\'discount\'] = 0.0 return new_df"},{"question":"You are required to create a function that takes in an email message object and encodes its payload using one of the encoding methods from the `email.encoders` module based on the type of payload data. The function should determine the appropriate encoding method to use and apply it to the message object. # Function Signature ```python from email.message import Message def encode_message(msg: Message) -> None: pass ``` # Input: - `msg`: An email `Message` object. The payload can be of various types, including text and binary data. # Output: - The function should return `None`. It modifies the `msg` object in place by encoding its payload and setting the appropriate Content-Transfer-Encoding header. # Constraints: - If the payload is textual and contains few non-printable characters, use `encode_quopri`. - If the payload is mostly binary, use `encode_base64`. - If the payload is simple text that can be transferred in 7-bit or 8-bit form, use `encode_7or8bit`. - If the payload does not need any encoding, use `encode_noop`. # Example Usage: ```python msg = Message() msg.set_payload(\\"Example payload with some text.\\") encode_message(msg) # Should modify msg to use the appropriate encoding and set the Content-Transfer-Encoding header. ``` # Assumptions: - You may assume that the `msg` object has a valid payload. - You may assume to use the functions from the `email.encoders` module directly. Utilize the provided information and functions from the `email.encoders` module to implement the `encode_message` function effectively.","solution":"from email.message import Message from email.encoders import encode_base64, encode_quopri, encode_7or8bit, encode_noop def encode_message(msg: Message) -> None: Encodes the payload of an email message based on its type and content, and sets the appropriate Content-Transfer-Encoding header. payload = msg.get_payload() # Determine the type of payload and choose the appropriate encoding if isinstance(payload, str): # Determine if the payload contains non-printable characters if all(32 <= ord(char) <= 126 or char in \'rn\' for char in payload): # Use 7 or 8 bit encoding for simple text encode_7or8bit(msg) else: # Use Quoted-Printable encoding for text with non-printable characters encode_quopri(msg) else: # Assume the payload is binary and use Base64 encoding encode_base64(msg)"},{"question":"# Task You are tasked with designing a simulator to manage multiple worker threads processing jobs from a job queue. Each worker thread should periodically check for jobs in the queue, process them, and then wait for the next job. However, the number of jobs that a worker can pick up simultaneously is limited by a semaphore that controls access to the shared resource (the job queue). Requirements: 1. Implement a `JobQueue` class that uses a queue to store jobs and provides thread-safe methods to add and retrieve jobs. 2. Implement a `Worker` class extending `threading.Thread` that repeatedly picks up jobs from the `JobQueue` and processes them. 3. Use a `Semaphore` to control the number of concurrent jobs a worker can process. Specifications: - The `JobQueue` class must include: - A method `add_job(job)` to add a job to the queue. - A method `get_job()` to retrieve a job from the queue in a thread-safe manner. - The `Worker` class must include: - An overridden `run` method that continuously performs the following: - Acquires a semaphore before it fetches a job. - Fetches a job from the `JobQueue` and processes it. - Releases the semaphore after processing the job. - Implement the `process_job(job)` function which simulates job processing by the worker. Input and Output Format - Input: You don\'t need to provide input directly. The test script will create multiple worker threads, add jobs to the `JobQueue`, and start the workers. - Output: Each worker should print a statement indicating the start and completion of processing a job. Constraints: - Use the threading module. - Ensure that the methods in `JobQueue` are thread-safe. Example Usage: ```python import threading import time from queue import Queue class JobQueue: def __init__(self): self._queue = Queue() def add_job(self, job): self._queue.put(job) def get_job(self): return self._queue.get() class Worker(threading.Thread): def __init__(self, semaphore, job_queue): threading.Thread.__init__(self) self.semaphore = semaphore self.job_queue = job_queue def run(self): while True: self.semaphore.acquire() job = self.job_queue.get_job() self.process_job(job) self.semaphore.release() def process_job(self, job): print(f\'{self.name} started job {job}\') time.sleep(1) # Simulate job processing print(f\'{self.name} completed job {job}\') # Example Test (illustrative purposes) if __name__ == \'__main__\': job_queue = JobQueue() semaphore = threading.Semaphore(3) # Adding jobs to the queue for i in range(10): job_queue.add_job(f\'Job-{i}\') # Starting worker threads workers = [Worker(semaphore, job_queue) for _ in range(5)] for worker in workers: worker.start() # Join workers (In actual implementation handle the breaking condition to end workers gracefully) for worker in workers: worker.join() ``` Ensure that the provided class implementations properly use synchronization primitives to handle concurrent access and abide by the constraints and methods defined.","solution":"import threading import time from queue import Queue class JobQueue: def __init__(self): self._queue = Queue() def add_job(self, job): self._queue.put(job) def get_job(self): return self._queue.get() class Worker(threading.Thread): def __init__(self, semaphore, job_queue): threading.Thread.__init__(self) self.semaphore = semaphore self.job_queue = job_queue def run(self): while True: self.semaphore.acquire() job = self.job_queue.get_job() self.process_job(job) self.semaphore.release() def process_job(self, job): print(f\'{self.name} started job {job}\') time.sleep(1) # Simulate job processing print(f\'{self.name} completed job {job}\')"},{"question":"# Version Number Decomposition You are provided with a 32-bit integer representing a Python version number encoded in big-endian order. Your task is to implement a function that extracts the various components of the version number and returns them in human-readable format. Function Signature ```python def parse_python_version(hex_version: int) -> str: :param hex_version: A 32-bit integer representing the Python version in big-endian order. :return: A string in the format \\"X.Y.ZaN\\" where: - X is the major version. - Y is the minor version. - Z is the micro version. - a represents alpha (A), b represents beta (B), c represents release candidate (C), and nothing for final (F). - N is the release serial number, omitted for final releases. ``` Expected Input and Output - Input: An integer (e.g., `0x030401a2`) - Output: A string representing the version (e.g., `\\"3.4.1a2\\"`) Example Given `0x030401a2`, the function should return `\\"3.4.1a2\\"`. Given `0x030a00f0`, the function should return `\\"3.10.0\\"`. Constraints - You must use bitwise operations to extract the relevant parts of the integer. - The function should handle different release levels (`alpha`, `beta`, `release candidate`, and `final`) appropriately. - Release serial should be omitted if the release level is `final`. Implementation Details 1. Extract the 1st byte for the major version. 2. Extract the 2nd byte for the minor version. 3. Extract the 3rd byte for the micro version. 4. Extract the high 4 bits of the 4th byte for the release level. 5. Extract the low 4 bits of the 4th byte for the release serial. Translate these components into a human-readable version string. Ensure your solution is efficient and correctly handles edge cases.","solution":"def parse_python_version(hex_version: int) -> str: Parses a 32-bit integer representing a Python version in big-endian order. :param hex_version: A 32-bit integer representing the Python version in big-endian order. :return: A string in the format \\"X.Y.ZaN\\" where: - X is the major version. - Y is the minor version. - Z is the micro version. - a represents alpha (A), b represents beta (B), c represents release candidate (C), and nothing for final (F). - N is the release serial number, omitted for final releases. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF release = (hex_version >> 4) & 0xF serial = hex_version & 0xF release_level = { 0xA: \'a\', # Alpha 0xB: \'b\', # Beta 0xC: \'c\', # Release Candidate 0xF: \'\' # Final } version_str = f\\"{major}.{minor}.{micro}\\" if release in release_level: version_str += release_level[release] if release != 0xF: # Append serial number only if not final release version_str += str(serial) return version_str"},{"question":"**Objective:** Implement a Python function to programmatically generate HTML documentation for a given module and save it to a specified file. The function should use the \\"pydoc\\" module to achieve this, and handle exceptions appropriately. **Function Signature:** ```python def generate_html_doc(module_name: str, output_file: str) -> bool: pass ``` **Input:** - `module_name` (str): The name of the Python module for which to generate documentation. - `output_file` (str): The file path where the HTML documentation should be saved. **Output:** - Returns `True` if the documentation is successfully generated and saved. - Returns `False` in case of any errors (e.g., if the module doesn\'t exist or the file cannot be written). **Constraints:** - Assume that the module name provided is a valid Python module name that could be used in an import statement. - The `output_file` should be a writable file path. The function should handle cases where the path is invalid or lacks write permissions. **Example:** ```python assert generate_html_doc(\'sys\', \'sys_doc.html\') == True assert generate_html_doc(\'nonexistentmodule\', \'nonexistent_doc.html\') == False ``` **Requirements:** 1. Use the `pydoc` module to generate documentation. 2. Ensure that the resulting documentation is in HTML format. 3. Implement appropriate error handling for common exceptions. **Hints:** - Look into using `subprocess` or invoking `pydoc` programmatically. - Ensure that the `-w` flag is used to generate HTML documentation. - Handle output and errors carefully to deliver the correct boolean result. **Notes:** - You may need to use specific methods from the `pydoc` module to implement this function. - Consider writing utility functions if necessary to modularize your code.","solution":"import pydoc def generate_html_doc(module_name: str, output_file: str) -> bool: Generate HTML documentation for the specified module and save it to the output file. Args: - module_name (str): The name of the Python module. - output_file (str): The file path to save the HTML documentation. Returns: - bool: True if documentation is successfully generated and saved, False otherwise. try: doc = pydoc.HTMLDoc() module = pydoc.safeimport(module_name) if module is None: return False html_doc = doc.docroutine(module) with open(output_file, \'w\') as file: file.write(html_doc) return True except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"# Question You are provided with the \'diamonds\' dataset from the Seaborn package. This dataset contains various features of diamonds including attributes like carat, cut, color, clarity, depth, and price among others. Objective Create a function `visualize_diamond_trends()` using the Seaborn package. The function should achieve the following: 1. Draw a boxen plot to visualize the distribution of diamond prices, grouped by the clarity of the diamonds. 2. Further annotate the boxen plot by adding hue to differentiate between large diamonds (carat > 1) and small diamonds (carat â‰¤ 1). 3. Customize the appearance of the plot by setting: - The line thickness of the boxes to 0.7. - Different color for each median line. - The `line_kws` parameter to control the aesthetics of the line representing the median. 4. Show the plot. Input - None. The function should load the dataset internally using `sns.load_dataset(\\"diamonds\\")`. Output - The function should display the Seaborn plot directly without returning any values. Constraints - Use only the seaborn package for visualization. - Ensure that the plot displays correctly inline in a Jupyter notebook. - Handle any potential warnings or errors due to missing or misconfigured data gracefully. Function Signature ```python def visualize_diamond_trends(): pass ``` Example ```python visualize_diamond_trends() ``` This should display a well-annotated and customized boxen plot in Jupyter notebook. **Note**: The `visualize_diamond_trends` function should be able to execute without any additional input, produce plots that are meaningful and visually appealing, and effectively communicate the trends and distributions of diamond prices concerning their clarity and size.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_diamond_trends(): Visualizes the distribution of diamond prices, grouped by clarity, and differentiates between small and large diamonds (carat > 1). # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create a new column indicating the size category of the diamonds diamonds[\'size\'] = diamonds[\'carat\'].apply(lambda x: \'Large\' if x > 1 else \'Small\') # Draw the boxen plot plt.figure(figsize=(12, 6)) sns.boxenplot( data=diamonds, x=\'clarity\', y=\'price\', hue=\'size\', line_kws={\\"linewidth\\": 0.7}, palette=\'Set3\' ) # Customize the median lines color for line in plt.gca().lines[4::6]: # Skip to the median lines line.set_color(\'red\') # Set title and labels plt.title(\'Diamond Price Distribution by Clarity and Size\') plt.xlabel(\'Clarity\') plt.ylabel(\'Price\') # Display the plot plt.show()"},{"question":"# Custom Type with Mimicked PyTypeObject Behavior **Objective**: Create a Python class `MyType` that mimics the behavior of defining a type using `PyTypeObject` in Python\'s C API. This class should expose custom behavior for attribute access, addition, and string representation. # Requirements 1. **Initialization and Attributes** - Define a class `MyType` with attributes for `name` and `value`. - The class should have an `__init__` method that initializes these attributes. 2. **Custom Attribute Access** - Implement the `__getattr__`, `__setattr__`, and `__delattr__` methods to control access to attributes: - `__getattr__`: Should return a custom message if the attribute does not exist. - `__setattr__`: Should allow adding new attributes. - `__delattr__`: Should delete attributes if they exist. 3. **Arithmetic Operations** - Implement addition (`__add__`) and in-place addition (`__iadd__`) methods: - `__add__`: Adds the `value` attributes of two `MyType` instances. - `__iadd__`: Adds a number directly to the `value` of the `MyType` instance. 4. **String Representation** - Implement the `__repr__` and `__str__` methods to provide custom string representations for instances: - `__repr__`: Should return a string suitable for debugging (similar to default `repr`). - `__str__`: Should return a user-friendly description incorporating the `name`. # Constraints - Assume `name` is a string, and `value` is a numeric type (integer or float). - Handle exceptions and invalid operations gracefully. - Avoid using any external libraries. # Inputs and Outputs - Create instances of `MyType` and demonstrate the defined behaviors: - Attribute access and modification. - Custom addition operations. - String representations. # Example Usage ```python class MyType: ... # Example Initialization obj1 = MyType(name=\'Object1\', value=10) obj2 = MyType(name=\'Object2\', value=15) # Attribute Access print(obj1.non_existent_attr) # \\"Attribute \'non_existent_attr\' does not exist.\\" # Addition obj3 = obj1 + obj2 # MyType instance with value 25 # In-place Addition obj1 += 5 # MyType instance with value 15 # String Representations print(repr(obj1)) # \\"MyType(name=\'Object1\', value=15)\\" print(str(obj1)) # \\"Object1 with value: 15\\" ``` # Submission Submit a Python file defining the `MyType` class that passes the given example usage.","solution":"class MyType: def __init__(self, name, value): self.name = name self.value = value def __getattr__(self, attr): return f\\"Attribute \'{attr}\' does not exist.\\" def __setattr__(self, key, value): self.__dict__[key] = value def __delattr__(self, attr): if attr in self.__dict__: del self.__dict__[attr] def __add__(self, other): if isinstance(other, MyType): return MyType(self.name, self.value + other.value) return NotImplemented def __iadd__(self, other): if isinstance(other, (int, float)): self.value += other return self return NotImplemented def __repr__(self): return f\\"MyType(name=\'{self.name}\', value={self.value})\\" def __str__(self): return f\\"{self.name} with value: {self.value}\\""},{"question":"**Question: Implement a Concurrent HTTP/1.0 Server Using asyncio Streams** In this assessment, you will write a simplified HTTP/1.0 server using the asyncio streams library documented above. The server should handle multiple clients concurrently, serving static files from a directory. # Specifications: 1. **Server Socket:** - The server must listen for incoming TCP connections on a specified host and port. - The server should utilize `asyncio.start_server` to handle incoming connections. 2. **Request Handling:** - For each client, read the HTTP request starting with a request line followed by headers (`readuntil(b\'rnrn\')`). - Parse the request line to determine the method (`GET`), the requested path, and the HTTP version. - Construct a file path based on the requested path, considering the specified directory base. - If the requested file is found, respond with a status `200 OK` and the file\'s contents. - If the file is not found, respond with a status `404 Not Found`. 3. **Response Format:** - The server should follow the HTTP/1.0 protocol format. Each response must include a status line, headers for `Content-Type` and `Content-Length`, and the payload (file data or error message). 4. **Concurrency:** - Use `asyncio` to handle multiple clients simultaneously. 5. **Shutdown:** - Ensure all connections are properly closed. # Constraints: - You may assume all requested paths are relative to a defined directory, e.g., `./www`. - Handle only `GET` requests; respond with `405 Method Not Allowed` for other methods. - Implement appropriate error handling for invalid HTTP requests and other potential exceptions. # Input/Output: - Input: No direct input is given. The server responds to external HTTP client requests. - Output: The server responses are sent directly to clients. # Example: Assuming the given directory structure: ``` ./www/ index.html about.html ``` A `GET /index.html HTTP/1.0` request should return the contents of `index.html` with a `200 OK` status. A `GET /nonexistent.html HTTP/1.0` request should return a `404 Not Found` status. # Code Skeleton: ```python import asyncio from pathlib import Path BASE_DIR = Path(\'./www\') async def handle_client(reader, writer): try: # Read and parse request request_line = await reader.readuntil(b\'rn\') headers = await reader.readuntil(b\'rnrn\') # Decode and parse request line request_line = request_line.decode().strip() method, path, version = request_line.split() if method != \'GET\': response = \'HTTP/1.0 405 Method Not Allowedrnrn\' writer.write(response.encode()) await writer.drain() return # Construct the file path file_path = BASE_DIR / path.lstrip(\'/\') if file_path.exists() and file_path.is_file(): content = file_path.read_bytes() response_headers = [ \'HTTP/1.0 200 OK\', f\'Content-Length: {len(content)}\', \'Content-Type: text/html\', # Simplification, serve all as text/html \'\', \'\', ] response = \'rn\'.join(response_headers) writer.write(response.encode() + content) else: response = \'HTTP/1.0 404 Not Foundrnrn\' writer.write(response.encode()) await writer.drain() except Exception as e: print(f\'Error handling request: {e}\') finally: writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main()) ``` This skeleton divides the problem into manageable parts, helping you focus on request handling, concurrency, and response formatting. Implement the missing details and ensure the server adheres to the requirements.","solution":"import asyncio from pathlib import Path BASE_DIR = Path(\'./www\') # Mime type mapping for basic file types MIME_TYPES = { \'html\': \'text/html\', \'txt\': \'text/plain\', \'css\': \'text/css\', \'js\': \'application/javascript\', \'json\': \'application/json\', \'png\': \'image/png\', \'jpg\': \'image/jpeg\', \'jpeg\': \'image/jpeg\', \'gif\': \'image/gif\' } def get_mime_type(file_path): extension = file_path.suffix[1:] # strip the leading dot return MIME_TYPES.get(extension, \'application/octet-stream\') async def handle_client(reader, writer): try: # Read and parse request request_line = await reader.readuntil(b\'rn\') headers = await reader.readuntil(b\'rnrn\') # Decode and parse request line request_line = request_line.decode().strip() method, path, version = request_line.split() if method != \'GET\': response = \'HTTP/1.0 405 Method Not Allowedrnrn\' writer.write(response.encode()) else: # Construct the file path file_path = BASE_DIR / path.lstrip(\'/\') if file_path.exists() and file_path.is_file(): content = file_path.read_bytes() mime_type = get_mime_type(file_path) response_headers = [ \'HTTP/1.0 200 OK\', f\'Content-Length: {len(content)}\', f\'Content-Type: {mime_type}\', \'\', \'\' ] response = \'rn\'.join(response_headers) writer.write(response.encode() + content) else: response = \'HTTP/1.0 404 Not Foundrnrn\' writer.write(response.encode()) await writer.drain() except Exception as e: print(f\'Error handling request: {e}\') finally: writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"Objective: Write a Python class that extends the functionality of the `venv.EnvBuilder` class to create virtual environments with additional custom Python scripts that should be installed into the environment directory. This will assess your understanding of the \\"venv\\" module and your ability to work with and extend Python classes. Problem Statement: You are required to create a custom virtual environment builder class named `CustomEnvBuilder` that inherits from `venv.EnvBuilder`. Your class should be able to: 1. Create a virtual environment in a specified directory. 2. Install provided custom scripts into the environment\'s script directory (`bin` on POSIX or `Scripts` on Windows). Class Definition: Define a class `CustomEnvBuilder` which extends `venv.EnvBuilder` with the following additional methods: 1. `install_custom_script(context, script_name, script_content)`: This method should: - Take three parameters: - `context`: The context object returned by `ensure_directories`. - `script_name`: The name of the custom script file. - `script_content`: The content of the custom script. - Write the `script_content` into a file named `script_name` inside `context.bin_path`. Implementation Details: - You MUST use the existing `venv` module and `venv.EnvBuilder` class. - Ensure that the custom builder creates the required virtual environment and installs the custom scripts correctly. - Your solution should be able to handle any exceptions and edge cases (e.g., directory already exists, missing files). Example Usage: After defining your `CustomEnvBuilder` class, the example below demonstrates how to use it: ```python import os import venv # Define the custom script content custom_script = #!/usr/bin/env python print(\\"Hello from custom script!\\") # Instantiate the CustomEnvBuilder builder = CustomEnvBuilder(with_pip=True) # Directory where the environment will be created env_dir = \'/path/to/env\' # Create the environment builder.create(env_dir) # Install a custom script into the created environment builder.install_custom_script(builder.ensure_directories(env_dir), \'hello.py\', custom_script) ``` In this example, after running the provided code, a virtual environment should be created in `/path/to/env`, and a custom script named `hello.py` should be installed in the environment\'s script directory. Running `hello.py` should print \\"Hello from custom script!\\" Constraints: - The solution should be compatible with Python versions 3.3 and above. - Ensure proper error handling and resource management. Submissions: Submit a single Python file named `custom_env_builder.py` containing: 1. The `CustomEnvBuilder` class definition. 2. The example usage code (inside the `if __name__ == \\"__main__\\":` block).","solution":"import os import venv class CustomEnvBuilder(venv.EnvBuilder): def install_custom_script(self, context, script_name, script_content): Installs a custom script into the virtual environment. :param context: The context object returned by `ensure_directories`. :param script_name: The name of the custom script file. :param script_content: The content of the custom script. script_path = os.path.join(context.bin_path, script_name) with open(script_path, \'w\') as script_file: script_file.write(script_content) # Make the script executable os.chmod(script_path, 0o755) # Example usage if __name__ == \\"__main__\\": custom_script = #!/usr/bin/env python print(\\"Hello from custom script!\\") builder = CustomEnvBuilder(with_pip=True) env_dir = \'./test_env\' builder.create(env_dir) context = builder.ensure_directories(env_dir) builder.install_custom_script(context, \'hello.py\', custom_script)"},{"question":"**Title:** Multi-Functional Data Processor **Objective:** Design a set of Python functions to process a sequence of numbers. The tasks will demonstrate the comprehension of various compound statements and function definitions. **Problem Statement:** You need to implement the following functions: 1. `process_numbers(numbers: list) -> dict`: - This function takes a list of numbers and returns a dictionary with the following keys: - `\\"even\\"`: A list of all even numbers from the input list. - `\\"odd\\"`: A list of all odd numbers from the input list. - `\\"prime\\"`: A list of all prime numbers from the input list. - `\\"sum_of_squares\\"`: The sum of the squares of all numbers in the list. 2. `safe_division(a: int, b: int) -> float`: - This function takes two arguments and returns the result of division `a / b`. - If `b` is zero, it should return `float(\'inf\')`, and ensure that no exception is unhandled. 3. `contextualize(filename: str, numbers: list) -> None`: - This function writes the list of numbers to a file with the given filename using the \\"with\\" statement to handle the file contextually. - Each number should be written on a new line. 4. `match_pattern(value: Any) -> str`: - This function takes a single value and uses the `match` statement to determine the type of the value, returning one of the following strings: - `\\"integer\\"`, if the value is an integer, - `\\"float\\"`, if the value is a float, - `\\"string\\"`, if the value is a string, - `\\"list\\"`, if the value is a list, - `\\"other\\"`, if the value doesn\'t match any of the above types. **Function Signatures:** ```python def process_numbers(numbers: list) -> dict: pass def safe_division(a: int, b: int) -> float: pass def contextualize(filename: str, numbers: list) -> None: pass def match_pattern(value: Any) -> str: pass ``` **Constraints:** - For the `process_numbers` function: - Assume all elements in `numbers` are non-negative integers. - Numbers up to 10^3. - For the `safe_division` function: - The function should handle integer division only. - For the `contextualize` function: - The list of numbers is non-empty. - For the `match_pattern` function: - The value can be any valid Python data type. **Example Usage:** ```python # Example usage of process_numbers print(process_numbers([1, 2, 3, 4, 5, 6])) # Output: {\'even\': [2, 4, 6], \'odd\': [1, 3, 5], \'prime\': [2, 3, 5], \'sum_of_squares\': 91} # Example usage of safe_division print(safe_division(10, 2)) # Output: 5.0 print(safe_division(10, 0)) # Output: inf # Example usage of contextualize contextualize(\'numbers.txt\', [1, 2, 3, 4, 5]) # This should write \\"1n2n3n4n5n\\" to \'numbers.txt\' # Example usage of match_pattern print(match_pattern(10)) # Output: integer print(match_pattern(10.5)) # Output: float print(match_pattern(\'hello\')) # Output: string print(match_pattern([1, 2, 3])) # Output: list print(match_pattern({1: \'a\'})) # Output: other ``` **Note:** - Implement the functions correctly to match the expected inputs and outputs. - Use appropriate control flow constructs as instructed above.","solution":"def process_numbers(numbers): Processes a list of numbers and returns a dictionary with keys: \'even\', \'odd\', \'prime\', and \'sum_of_squares\'. def is_prime(n): if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True result = { \'even\': [n for n in numbers if n % 2 == 0], \'odd\': [n for n in numbers if n % 2 != 0], \'prime\': [n for n in numbers if is_prime(n)], \'sum_of_squares\': sum(n**2 for n in numbers) } return result def safe_division(a, b): Returns the result of dividing a by b. If b is zero, returns float(\'inf\'). try: return a / b except ZeroDivisionError: return float(\'inf\') def contextualize(filename, numbers): Writes a list of numbers to a file, each number on a new line. with open(filename, \'w\') as f: for number in numbers: f.write(f\\"{number}n\\") def match_pattern(value): Matches the type of value to a specific string (integer, float, string, list, other). match value: case int(): return \\"integer\\" case float(): return \\"float\\" case str(): return \\"string\\" case list(): return \\"list\\" case _: return \\"other\\""},{"question":"**Question: Customizing Seaborn Plot Context for Data Visualization** You are provided with a dataset that contains information about daily temperatures recorded in a city over a month. Your task is to visualize this data using Seaborn while experimenting with different plotting contexts and aesthetic parameters. # Dataset The dataset `daily_temperatures.csv` has the following columns: - `day`: The day of the month (1 to 31). - `temperature`: The recorded temperature on that day. # Tasks 1. **Load the dataset**: Import necessary libraries and load the `daily_temperatures.csv` dataset into a DataFrame. 2. **Plot with Default Context**: Create a line plot of `day` vs. `temperature` using the default seaborn context. 3. **Apply Different Contexts**: Create three additional line plots using the \\"paper\\", \\"talk\\", and \\"poster\\" contexts respectively. 4. **Scale Font and Line Width**: - Use the \\"talk\\" context with a font scale of 1.5 to create a plot. - Use the \\"poster\\" context with the line width set to 2 and the font scale set to 2 to create another plot. 5. **Custom Context with Parameters**: Create a custom context by overriding the default parameters to have a line width of 3 and font size of 1.2. Plot the data using this custom context. # Expected Output - Five separate plots as per the specifications above. # Constraints - You must use the Seaborn library for plotting. - Make sure the plots are clearly labeled (title, axis labels). # Performance Requirements - The code should run efficiently without unnecessary computations. # Example code snippet for reference ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Example of loading the dataset df = pd.read_csv(\'daily_temperatures.csv\') # Example of creating a line plot with default context sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.show() ``` Good luck, and remember to experiment with the contexts and parameters provided by Seaborn to produce high-quality visualizations!","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_dataset(filepath): Load the dataset from a given file path. return pd.read_csv(filepath) def plot_with_default_context(df): Plot with default context. sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.title(\\"Daily Temperatures with Default Context\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature\\") plt.show() def plot_with_context(df, context): Plot with a given seaborn context. sns.set_context(context) sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.title(f\\"Daily Temperatures with {context.capitalize()} Context\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature\\") plt.show() def plot_with_talk_context_font_scale(df): Plot with \'talk\' context and a font scale of 1.5. sns.set_context(\\"talk\\", font_scale=1.5) sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.title(\\"Daily Temperatures with Talk Context (Font Scale 1.5)\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature\\") plt.show() def plot_with_poster_context_custom_params(df): Plot with \'poster\' context, line width 2, and font scale 2. sns.set_context(\\"poster\\", font_scale=2, rc={\\"lines.linewidth\\": 2}) sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.title(\\"Daily Temperatures with Poster Context (Line Width 2, Font Scale 2)\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature\\") plt.show() def plot_with_custom_context(df): Plot with custom context (line width 3 and font size 1.2). sns.set_context(\\"notebook\\", rc={\\"lines.linewidth\\": 3, \\"font.size\\": 1.2}) sns.lineplot(x=\'day\', y=\'temperature\', data=df) plt.title(\\"Daily Temperatures with Custom Context (Line Width 3, Font Size 1.2)\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Temperature\\") plt.show()"},{"question":"Customizing Object Representations with `reprlib` **Objective**: Implement a subclass of the `reprlib.Repr` that modifies the string representations of common Python collections (`list`, `dict`, and `tuple`), restricting their maximum lengths and applying custom formatting rules. Requirements: 1. **Subclass `reprlib.Repr`**: Create a subclass named `CustomRepr` that inherits from `reprlib.Repr`. 2. **Override Representation Methods**: - Override the methods for list, dict, and tuple to apply a new format: - `[item1, item2, ...]` for lists, limiting to a maximum of 4 items. - `{key1: value1, key2: value2, ...}` for dicts, limiting to a maximum of 3 key-value pairs. - `(item1, item2, ...)` for tuples, limiting to a maximum of 5 items. 3. **Dynamic Dispatch**: Ensure the correct dynamic dispatching of types. 4. **Size Limits**: Conform to the size limits specified in the requirements. 5. **Performance Considerations**: Ensure the implementation efficiently handles collections near the size limits. Constraints: - The input objects can contain nested lists, dicts, and tuples. - For simplicity, you may assume the content of the objects does not include complex types beyond basic types (int, float, string, list, dict, tuple). # Example ```python import reprlib # Implement the CustomRepr class here class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxlist = 4 self.maxdict = 3 self.maxtuple = 5 def repr_list(self, obj, level): if level <= 0 or len(obj) == 0: return \'[]\' items = (self.repr1(item, level - 1) for item in obj[:self.maxlist]) return \'[\' + \', \'.join(items) + (\'...\' if len(obj) > self.maxlist else \'\') + \']\' def repr_dict(self, obj, level): if level <= 0 or len(obj) == 0: return \'{}\' items = ( f\\"{self.repr1(key, level - 1)}: {self.repr1(value, level - 1)}\\" for key, value in list(obj.items())[:self.maxdict] ) return \'{\' + \', \'.join(items) + (\'...\' if len(obj) > self.maxdict else \'\') + \'}\' def repr_tuple(self, obj, level): if level <= 0 or len(obj) == 0: return \'()\' items = (self.repr1(item, level - 1) for item in obj[:self.maxtuple]) return \'(\' + \', \'.join(items) + (\'...\' if len(obj) > self.maxtuple else \'\') + \')\' # Test the implementation custom_repr = CustomRepr() print(custom_repr.repr([1, 2, 3, 4, 5])) # Expected output: [1, 2, 3, 4...] print(custom_repr.repr({\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4})) # Expected output: {\'a\': 1, \'b\': 2, \'c\': 3...} print(custom_repr.repr((1, 2, 3, 4, 5, 6))) # Expected output: (1, 2, 3, 4, 5...) ``` Submission: Write your implementation of the `CustomRepr` class in the following code cell. Your submission will be evaluated based on correctness, efficiency, and code readability.","solution":"import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxlist = 4 self.maxdict = 3 self.maxtuple = 5 def repr_list(self, obj, level): if level <= 0 or len(obj) == 0: return \'[]\' items = (self.repr1(item, level - 1) for item in obj[:self.maxlist]) return \'[\' + \', \'.join(items) + (\'...\' if len(obj) > self.maxlist else \'\') + \']\' def repr_dict(self, obj, level): if level <= 0 or len(obj) == 0: return \'{}\' items = ( f\\"{self.repr1(key, level - 1)}: {self.repr1(value, level - 1)}\\" for key, value in list(obj.items())[:self.maxdict] ) return \'{\' + \', \'.join(items) + (\'...\' if len(obj) > self.maxdict else \'\') + \'}\' def repr_tuple(self, obj, level): if level <= 0 or len(obj) == 0: return \'()\' items = (self.repr1(item, level - 1) for item in obj[:self.maxtuple]) return \'(\' + \', \'.join(items) + (\'...\' if len(obj) > self.maxtuple else \'\') + \')\' custom_repr = CustomRepr()"},{"question":"# Objective Write a function `create_penguin_plots` that generates two advanced seaborn plots using the penguins dataset. # Input The function should not receive any direct input. Instead, it should load the penguins dataset internally. # Output The function should return two seaborn plot objects. # Constraints - You must use the `seaborn.objects` API to create the plots. - You should handle the facet, layer addition, and statistical transformations as demonstrated in the provided documentation. # Requirements 1. **First Plot:** - A Dot plot of `body_mass_g` (x-axis) versus `species` (y-axis) displaying data points. - Color should represent `sex`. - Add a second layer that displays error bars showing the standard deviation. - Use dodging to separate male and female dots within each species. 2. **Second Plot:** - A Line plot of `sex` (x-axis) versus `body_mass_g` (y-axis) faceted by `species`. - Customize the lines by adding circular markers. - Add a second layer to display error bars showing the standard deviation. - Use a different color for each species. # Example Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def create_penguin_plots(): # Load the dataset penguins = load_dataset(\\"penguins\\") # First Plot plot1 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Second Plot plot2 = ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", linestyle=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) ) return plot1, plot2 ``` The above solution should correctly implement the multi-layered seaborn plots required according to the question\'s requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plots(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Check if penguins dataset is loaded correctly if penguins is None or penguins.empty: print(\\"Failed to load penguins dataset\\") return None, None # First Plot: Dot plot with error bars and dodging plot1 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) # Second Plot: Line plot faceted by species with error bars and markers plot2 = ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", color=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) ) return plot1, plot2"},{"question":"# Question Objective You are required to implement a `serialize_data` function that serializes a given Python object and saves it to a file, and a `deserialize_data` function that reads the serialized object from a file and returns the original object. Details 1. Implement the `serialize_data(obj, file_path)` function: - Parameters: - `obj`: The Python object to be serialized. - `file_path`: The path (including the file name) where the serialized object will be stored. - Functionality: - Serialize the given Python object `obj` using the `pickle` module. - Save the serialized object to the specified `file_path`. 2. Implement the `deserialize_data(file_path)` function: - Parameter: - `file_path`: The path (including the file name) from where the serialized object will be read. - Functionality: - Read the serialized object from the specified `file_path`. - Deserialize it back into the original Python object. Input and Output Formats - The serialized data should be saved and read from a file in binary mode. - The solutions should handle exceptions like file not found, read/write errors, and any issues with pickling/unpickling. Example ```python import pickle def serialize_data(obj, file_path): with open(file_path, \'wb\') as file: pickle.dump(obj, file) def deserialize_data(file_path): with open(file_path, \'rb\') as file: return pickle.load(file) # Assuming you have an example class class ExampleClass: def __init__(self, a, b): self.a = a self.b = b # Prepare a sample object sample_obj = ExampleClass(10, {\'x\': 1, \'y\': 2}) # Serialize the object serialize_data(sample_obj, \'sample_obj.pkl\') # Deserialize the object result_obj = deserialize_data(\'sample_obj.pkl\') # Verifying the result assert isinstance(result_obj, ExampleClass) assert result_obj.a == 10 assert result_obj.b == {\'x\': 1, \'y\': 2} ``` Constraints - The Python object to be serialized can be a custom class instance, a dictionary, a list, or any picklable Python object. - The `file_path` will always be a valid file path in the string format. - You should handle possible IO errors and exceptions gracefully. Performance - The functions should efficiently handle the serialization and deserialization process without unnecessary computation. - The size of the object to be serialized should not be a constraint, but the functions should be able to handle objects up to a reasonable size (e.g., 10MB).","solution":"import pickle def serialize_data(obj, file_path): Serializes a Python object and saves it to a file. Parameters: obj (object): The Python object to be serialized. file_path (str): The path (including file name) where the serialized object will be stored. try: with open(file_path, \'wb\') as file: pickle.dump(obj, file) except Exception as e: print(f\\"An error occurred while serializing the object: {e}\\") def deserialize_data(file_path): Reads the serialized object from a file and returns the original object. Parameters: file_path (str): The path (including file name) from where the serialized object will be read. Returns: object: The deserialized Python object. try: with open(file_path, \'rb\') as file: return pickle.load(file) except Exception as e: print(f\\"An error occurred while deserializing the object: {e}\\") return None"},{"question":"# Complex Number Arithmetic in Python In this exercise, you will implement functions to perform basic arithmetic operations on complex numbers. You will utilize Python\'s `complex` type for these operations. You are required to implement four functions: `complex_sum`, `complex_diff`, `complex_prod`, and `complex_quot`. Each of these functions takes two complex numbers as input and returns the result of the corresponding arithmetic operation. **Function Signatures:** ```python def complex_sum(a: complex, b: complex) -> complex: Return the sum of two complex numbers. pass def complex_diff(a: complex, b: complex) -> complex: Return the difference between two complex numbers. pass def complex_prod(a: complex, b: complex) -> complex: Return the product of two complex numbers. pass def complex_quot(a: complex, b: complex) -> complex: Return the quotient of two complex numbers. If the divisor is zero, raise a ZeroDivisionError. pass ``` # Requirements: 1. **Input:** - `a` and `b` are complex numbers, e.g., `3+4j`, `1-2j`. 2. **Output:** - Each function should return a complex number that is the result of the arithmetic operation. 3. **Constraints:** - You should handle cases where the divisor in `complex_quot` is zero by raising a `ZeroDivisionError`. 4. **Performance:** - Each function should run in O(1) time complexity. # Example Usage: ```python x = complex(2, 3) y = complex(1, -1) print(complex_sum(x, y)) # Output: (3+2j) print(complex_diff(x, y)) # Output: (1+4j) print(complex_prod(x, y)) # Output: (5+1j) print(complex_quot(x, y)) # Output: (-0.5+2.5j) ``` Note: You are not allowed to use any external libraries other than Python\'s built-in complex type and basic arithmetic operations. Happy coding!","solution":"def complex_sum(a: complex, b: complex) -> complex: Return the sum of two complex numbers. return a + b def complex_diff(a: complex, b: complex) -> complex: Return the difference between two complex numbers. return a - b def complex_prod(a: complex, b: complex) -> complex: Return the product of two complex numbers. return a * b def complex_quot(a: complex, b: complex) -> complex: Return the quotient of two complex numbers. If the divisor is zero, raise a ZeroDivisionError. if b == 0: raise ZeroDivisionError(\\"The divisor is zero.\\") return a / b"},{"question":"You are asked to demonstrate your understanding of seaborn\'s `cubehelix_palette` function by creating custom color palettes and using them in a seaborn plot. # Task 1. **Create three custom color palettes** using the `sns.cubehelix_palette` function: - **Palette 1:** Should have 10 colors, starting at 1.5, rotating by -0.8, with increased gamma of 1.2. - **Palette 2:** Should be a continuous colormap with a starting point of 3, rotating by 0.5, with a hue of 0.8. - **Palette 3:** Should have the default number of colors but with reversed luminance direction, starting at 2, rotating by 0.4, dark level of 0.3, and light level of 0.7. 2. **Plot a seaborn barplot** using any dataset from seabornâ€™s built-in datasets (for instance, `tips`). Each palette should be used to color the bars in three subplots arranged in a single row. # Input - None for functions directly. Utilize seaborn\'s built-in dataset (`tips` or any suitable dataset). # Output - A matplotlib figure containing three side-by-side barplots using the custom palettes created. # Constraints - Make sure the code is efficient and follows good coding practices. # Example ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load dataset tips = sns.load_dataset(\\"tips\\") # Create custom palettes palette1 = sns.cubehelix_palette(n_colors=10, start=1.5, rot=-.8, gamma=1.2) palette2 = sns.cubehelix_palette(start=3, rot=.5, hue=.8, light=0.9, dark=0.2, as_cmap=True) palette3 = sns.cubehelix_palette(start=2, rot=.4, dark=.3, light=.7, reverse=True) # Plot the barplots fig, axes = plt.subplots(1, 3, figsize=(18, 5)) sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=palette1, ax=axes[0]) axes[0].set_title(\'Palette 1\') sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=palette2(np.linspace(0, 1, len(tips))), ax=axes[1]) axes[1].set_title(\'Palette 2\') sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=palette3, ax=axes[2]) axes[2].set_title(\'Palette 3\') plt.tight_layout() plt.show() ``` Above solution demonstrates the creation of the required palettes and their application to a seaborn barplot.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_palettes(): Creates three custom color palettes using seaborn\'s cubehelix_palette. Returns a tuple of three palettes. # Palette 1: 10 colors, starting at 1.5, rotating by -0.8, with gamma 1.2 palette1 = sns.cubehelix_palette(n_colors=10, start=1.5, rot=-.8, gamma=1.2) # Palette 2: Continuous colormap starting at 3, rotating by 0.5, with hue 0.8 palette2 = sns.cubehelix_palette(start=3, rot=.5, hue=.8, as_cmap=True) # Palette 3: Default number of colors, reversed luminance, starting at 2, rotating by 0.4, dark 0.3, light 0.7 palette3 = sns.cubehelix_palette(start=2, rot=.4, dark=.3, light=.7, reverse=True) return palette1, palette2, palette3 def plot_with_custom_palettes(): Plots a seaborn barplot in three subplots using the three custom palettes. # Load dataset tips = sns.load_dataset(\\"tips\\") # Create custom palettes palette1, palette2, palette3 = create_custom_palettes() # Plot the barplots fig, axes = plt.subplots(1, 3, figsize=(18, 5)) sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=palette1, ax=axes[0]) axes[0].set_title(\'Palette 1\') sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=palette2(np.linspace(0, 1, len(tips))), ax=axes[1]) axes[1].set_title(\'Palette 2\') sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", palette=palette3, ax=axes[2]) axes[2].set_title(\'Palette 3\') plt.tight_layout() plt.show()"},{"question":"# Question You are tasked with implementing a function that processes a mixed dataset containing dense arrays and sparse matrices to ensure they are compatible for further machine learning processing. The function should validate the input data and then perform a dot product operation on the validated data. Function Signature ```python def process_and_dot_product(X, Y): Parameters: X (array-like or sparse matrix): Input data array of shape (n_samples, n_features) Y (array-like or sparse matrix): Second input data array of shape (n_features, n_samples) Returns: numpy.ndarray or sparse matrix: Result of dot product between validated X and Y Raises: ValueError: If the input arrays do not meet the proper conditions. ``` Requirements 1. Implement the `process_and_dot_product` function to perform the following steps: - Validate that both `X` and `Y` are 2D arrays or matrices. - Ensure that there are no NaNs or Infs in the inputs. - The function should be able to handle both dense and sparse input formats. 2. If both inputs pass validation, compute their dot product. The output should match the input type (if inputs are dense arrays, the output should be a dense array; if inputs are sparse matrices, the output should be a sparse matrix). 3. Use the utilities provided by `sklearn.utils` for validation and processing. Constraints - You can assume that `X` and `Y` have compatible dimensions for matrix multiplication. - Use of the following utilities is expected: - `check_array` - `assert_all_finite` - `safe_sparse_dot` Example ```python import numpy as np from scipy.sparse import csr_matrix X_dense = np.array([[1, 2], [3, 4]]) Y_dense = np.array([[5, 6], [7, 8]]) print(process_and_dot_product(X_dense, Y_dense)) # Expected output: [[19 22] # [43 50]] X_sparse = csr_matrix(X_dense) Y_sparse = csr_matrix(Y_dense) print(process_and_dot_product(X_sparse, Y_sparse)) # Expected output: <2x2 sparse matrix of type \'<class \'numpy.int64\'>\' # with 4 stored elements in Compressed Sparse Row format> ```","solution":"import numpy as np from scipy import sparse from sklearn.utils import check_array, assert_all_finite from sklearn.utils.extmath import safe_sparse_dot def process_and_dot_product(X, Y): Parameters: X (array-like or sparse matrix): Input data array of shape (n_samples, n_features) Y (array-like or sparse matrix): Second input data array of shape (n_features, n_samples) Returns: numpy.ndarray or sparse matrix: Result of dot product between validated X and Y Raises: ValueError: If the input arrays do not meet the proper conditions. # Validate input X X_checked = check_array(X, accept_sparse=True, ensure_2d=True, dtype=[np.float64, np.float32, np.int64, np.int32]) assert_all_finite(X_checked) # Validate input Y Y_checked = check_array(Y, accept_sparse=True, ensure_2d=True, dtype=[np.float64, np.float32, np.int64, np.int32]) assert_all_finite(Y_checked) # Perform dot product result = safe_sparse_dot(X_checked, Y_checked) return result"},{"question":"You are tasked with creating a Python application that simulates a concurrent task processing system using asyncio. The system must read tasks from a queue, process them asynchronously, and handle proper logging and error detection. # Requirements 1. **Task Queue**: Implement a task queue using `asyncio.Queue`. 2. **Task Processing**: Simulate processing each task with a function that includes both I/O-bound and CPU-bound operations. 3. **Concurrency**: Ensure that multiple tasks can be processed concurrently. 4. **Logging**: Set up logging to capture debug-level information. 5. **Error Handling**: Detect and handle never-awaited coroutines and exceptions in tasks. 6. **Debug Mode**: Enable asyncio debug mode to aid in development and debugging. # Input and Output Formats - **Input**: - A list of tasks, where each task is a dictionary containing: - `id`: An integer representing the unique ID of the task. - `type`: A string, either \\"io\\" for I/O-bound tasks or \\"cpu\\" for CPU-bound tasks. - `data`: The data required for processing the task. - **Output**: - Print logs at debug level including task ID and processing states. - Print any exceptions detected during task processing. # Constraints 1. Tasks must be processed concurrently but respect the asyncio event loop mechanics. 2. The system should log relevant information with the debug-level logging enabled. 3. Use `asyncio.run` to run the main entry point. # Example ```python import asyncio import logging logging.basicConfig(level=logging.DEBUG) async def io_task(task_data): await asyncio.sleep(1) # Simulate I/O work return f\\"Processed IO task with data: {task_data}\\" async def cpu_task(task_data): import concurrent.futures loop = asyncio.get_running_loop() with concurrent.futures.ThreadPoolExecutor() as pool: result = await loop.run_in_executor(pool, lambda: sum(x*x for x in range(task_data))) return f\\"Processed CPU task with data: {task_data}\\" async def process_task(task): try: logging.debug(f\\"Starting task {task[\'id\']} of type {task[\'type\']}\\") if task[\'type\'] == \'io\': result = await io_task(task[\'data\']) elif task[\'type\'] == \'cpu\': result = await cpu_task(task[\'data\']) else: raise ValueError(\\"Unknown task type\\") logging.debug(f\\"Completed task {task[\'id\']}: {result}\\") except Exception as e: logging.error(f\\"Exception in task {task[\'id\']}: {e}\\") async def main(tasks): asyncio.get_event_loop().set_debug(True) task_queue = asyncio.Queue() for task in tasks: await task_queue.put(task) workers = [asyncio.create_task(worker(task_queue)) for _ in range(5)] await asyncio.gather(*workers) async def worker(queue): while not queue.empty(): task = await queue.get() await process_task(task) queue.task_done() # Sample tasks tasks = [ {\\"id\\": 1, \\"type\\": \\"io\\", \\"data\\": \\"http://example.com\\"}, {\\"id\\": 2, \\"type\\": \\"cpu\\", \\"data\\": 10000}, {\\"id\\": 3, \\"type\\": \\"io\\", \\"data\\": \\"http://example.com/resource\\"}, {\\"id\\": 4, \\"type\\": \\"cpu\\", \\"data\\": 20000}, ] asyncio.run(main(tasks)) ``` # Notes - You can adjust the logging level using `logging.getLogger(\\"asyncio\\").setLevel(logging.WARNING)` within the code. - Ensure to handle any pending exceptions or un-awaited coroutines in a proper way before program termination.","solution":"import asyncio import logging import concurrent.futures logging.basicConfig(level=logging.DEBUG) async def io_task(task_data): await asyncio.sleep(1) # Simulate I/O work return f\\"Processed IO task with data: {task_data}\\" async def cpu_task(task_data): loop = asyncio.get_running_loop() with concurrent.futures.ThreadPoolExecutor() as pool: result = await loop.run_in_executor(pool, lambda: sum(x*x for x in range(task_data))) return f\\"Processed CPU task with data: {task_data}\\" async def process_task(task): try: logging.debug(f\\"Starting task {task[\'id\']} of type {task[\'type\']}\\") if task[\'type\'] == \'io\': result = await io_task(task[\'data\']) elif task[\'type\'] == \'cpu\': result = await cpu_task(task[\'data\']) else: raise ValueError(\\"Unknown task type\\") logging.debug(f\\"Completed task {task[\'id\']}: {result}\\") except Exception as e: logging.error(f\\"Exception in task {task[\'id\']}: {e}\\") async def main(tasks): asyncio.get_event_loop().set_debug(True) task_queue = asyncio.Queue() for task in tasks: await task_queue.put(task) workers = [asyncio.create_task(worker(task_queue)) for _ in range(5)] await asyncio.gather(*workers) async def worker(queue): while not queue.empty(): task = await queue.get() await process_task(task) queue.task_done() tasks = [ {\\"id\\": 1, \\"type\\": \\"io\\", \\"data\\": \\"http://example.com\\"}, {\\"id\\": 2, \\"type\\": \\"cpu\\", \\"data\\": 10000}, {\\"id\\": 3, \\"type\\": \\"io\\", \\"data\\": \\"http://example.com/resource\\"}, {\\"id\\": 4, \\"type\\": \\"cpu\\", \\"data\\": 20000}, ] asyncio.run(main(tasks))"},{"question":"You are tasked with demonstrating your understanding of loading and manipulating real-world datasets using scikit-learn. You must fetch a dataset, perform some preprocessing, and then use it to create and evaluate a simple machine learning model. Specifically, focus on the \\"California Housing\\" dataset. Requirements: 1. **Fetch the Dataset**: - Use the `fetch_california_housing` function from `sklearn.datasets` to load the California Housing dataset. 2. **Data Preparation**: - Split the dataset into training and testing sets. The training set should contain 80% of the data, and the testing set should contain the remaining 20%. - Standardize the features by removing the mean and scaling to unit variance. 3. **Model Implementation**: - Implement a linear regression model using `sklearn.linear_model.LinearRegression`. - Train the model on the training set. - Evaluate the model on the testing set using the Root Mean Squared Error (RMSE) as the metric. 4. **Function Implementation**: - Create a function `load_and_train()` that performs the above tasks. - The function should return the RMSE of the model on the testing set. Expected Input and Output: - **Input**: The function `load_and_train()` does not take any input parameters. - **Output**: The function should return a single float value representing the RMSE of the model on the testing set. Constraints: - You must use scikit-learn for fetching the dataset, splitting the data, standardizing the features, and implementing the linear regression model. - Ensure that your solution is efficient and adheres to best coding practices. Example: ```python def load_and_train(): # Your code here # Example usage: rmse = load_and_train() print(f\\"RMSE of the model: {rmse:.4f}\\") ```","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error import numpy as np def load_and_train(): # Fetch the California Housing dataset cali_housing = fetch_california_housing() X, y = cali_housing.data, cali_housing.target # Split the dataset into training (80%) and testing (20%) sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Implement and train the linear regression model model = LinearRegression() model.fit(X_train_scaled, y_train) # Evaluate the model using RMSE y_pred = model.predict(X_test_scaled) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) return rmse # Example usage rmse_value = load_and_train() print(f\\"RMSE of the model: {rmse_value:.4f}\\")"},{"question":"**Objective:** Implement a function `process_and_copy_streams` that reads data from a text stream, performs some transformations, and writes the output to a binary stream. Specifically, this function must: 1. Read text data from an input stream `input_stream` (which can either be a file or an in-memory `StringIO` object). 2. Perform the following transformations on the text data: - Convert all text to uppercase. - Replace all newline characters with a space character. 3. Write the transformed text data to an output stream `output_stream` as bytes. The `output_stream` could either be a file or an in-memory `BytesIO` object. # Input: - `input_stream`: A text stream object. It can be either an opened file object in text mode or an `io.StringIO` object. - `output_stream`: A binary stream object. It can be either an opened file object in binary mode or an `io.BytesIO` object. - `encoding`: A string specifying the encoding of the text in the input stream. Default is `\'utf-8\'`. # Output: - The function does not return any value but writes the transformed bytes to the given `output_stream`. # Constraints: - The `input_stream` is guaranteed to be well-formed and valid according to the provided encoding. - Assume the text data is not excessively large, and all operations can fit into memory. # Function Signature: ```python def process_and_copy_streams(input_stream, output_stream, encoding=\'utf-8\'): pass ``` # Example: ```python import io # Example input and output streams input_text = \\"HellonWorld!nThis is a test.\\" input_stream = io.StringIO(input_text) output_stream = io.BytesIO() # Process and copy streams process_and_copy_streams(input_stream, output_stream) # Check the output output_stream.seek(0) print(output_stream.read()) # Expected Output (in bytes): # b\'HELLO WORLD! THIS IS A TEST.\' ``` # Notes: - You should handle opening and closing the streams if they are files, but if they are `StringIO` or `BytesIO` objects, do not close them. - Use exception handling to catch and handle any potential I/O errors. - Ensure your solution is efficient and leverages buffering correctly to minimize system calls where appropriate.","solution":"def process_and_copy_streams(input_stream, output_stream, encoding=\'utf-8\'): Reads text data from input_stream, transforms the text, and writes the output to output_stream as bytes. try: # Read the entire data from the input stream raw_data = input_stream.read() # Convert all text to uppercase and replace newlines with spaces transformed_data = raw_data.upper().replace(\'n\', \' \') # Encode the transformed data to bytes using the specified encoding byte_data = transformed_data.encode(encoding) # Write the bytes data to the output stream output_stream.write(byte_data) except (IOError, UnicodeEncodeError, UnicodeDecodeError) as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Question: Implementing a Custom Profiler Background: As part of your role in performance optimization, you are tasked with creating a custom profiler to measure execution time of different functions in a Python program. You are to leverage Python\'s `timeit` module for measuring the execution times and organize the results in a human-readable format. Task: Write a Python function called `profile_functions` that takes a list of function names (as strings) and their corresponding arguments and measures the execution time of each function. The function should then return a summary of the results in descending order of execution times. Requirements: 1. Use the `timeit` module to measure the execution time. 2. Each function should be run 1000 times to get a reliable average execution time. 3. The input to the `profile_functions` should be: - A list of tuples, where each tuple contains: - The function name (string) - A list of arguments for the function 4. The output should be a list of tuples, each containing: - The function name (string) - Its average execution time (float) Constraints: - Assume that the functions to be profiled are already defined in the global scope and are accessible by their names. Input: - `functions_to_profile`: List[Tuple[str, List[Any]]] - A list of tuples, where each tuple contains the function name and a list of arguments for that function. Output: - List[Tuple[str, float]] - A sorted list in descending order by execution time, where each element is a tuple comprising a function name and its average execution time. Example: ```python def add(a, b): return a + b def subtract(a, b): return a - b functions_to_profile = [ (\'add\', [1, 2]), (\'subtract\', [5, 3]) ] result = profile_functions(functions_to_profile) print(result) # [(\\"subtract\\", <execution_time>), (\\"add\\", <execution_time>)] ``` In this example, the `profile_functions` runs the `add` and `subtract` functions 1000 times each with given arguments, measures their average execution times, and returns the results sorted by descending order of execution time.","solution":"import timeit def profile_functions(functions_to_profile): Profiling the execution time of given functions and returning the results in descending order of execution times. :param functions_to_profile: List[Tuple[str, List[Any]]] :return: List[Tuple[str, float]] results = [] for func_name, args in functions_to_profile: func = globals()[func_name] timer = timeit.Timer(lambda: func(*args)) avg_time = timer.timeit(number=1000) / 1000 results.append((func_name, avg_time)) results.sort(key=lambda x: x[1], reverse=True) return results # Example functions to be used in the scope of solution def add(a, b): return a + b def subtract(a, b): return a - b"},{"question":"**Problem Statement:** You are tasked with writing utility functions for managing accelerator devices using PyTorch. Specifically, you must implement the following function: # Function: manage_accelerators Description: This function will check the availability of accelerator devices, set a specific device index as active, perform some dummy computation on it, and ensure synchronization between streams. Function Signature: ```python def manage_accelerators(device_idx: int) -> dict: pass ``` Input: - **device_idx** (`int`): The device index which you want to set as the active device. The index is assumed to be valid. Output: - Returns a dictionary containing: - `\\"device_count\\"` (`int`): The number of available accelerator devices. - `\\"is_available\\"` (`bool`): The availability status of the accelerator. - `\\"current_device_index\\"` (`int`): The global index of the currently active device after setting it. - `\\"current_stream\\"` (`stream`): The stream object representing the currently active stream. Constraints and Assumptions: - Assume that the necessary hardware and PyTorch configurations are correctly set up. - The dummy computation can be a simple tensor addition. - Ensure that the synchronization of streams is handled appropriately. Example: ```python result = manage_accelerators(0) print(result) # Expected Output (values will vary based on the actual hardware): # { # \\"device_count\\": 4, # \\"is_available\\": True, # \\"current_device_index\\": 0, # \\"current_stream\\": <stream object> # } ``` Notes: - Use `torch.accelerator.device_count()` to get the number of available devices. - Use `torch.accelerator.is_available()` to check if an accelerator is available. - Use `torch.accelerator.set_device_idx(device_idx)` to set the current device by its index. - Perform a simple tensor addition on the set device using the appropriate stream. - Use `torch.accelerator.current_device_index()` to get the global index of the current device. - Use `torch.accelerator.current_stream()` to get the current stream object. - Use `torch.accelerator.synchronize()` to synchronize the current stream. Your implementation should demonstrate a clear understanding of managing devices and streams within the torch accelerator module.","solution":"import torch def manage_accelerators(device_idx: int) -> dict: Manages accelerator devices and performs dummy computations on the specified device. Args: - device_idx (int): The device index to set as the active device. Returns: - dict: Dictionary containing device information and current stream. # Get the number of available devices device_count = torch.cuda.device_count() # Check if the accelerator (CUDA) is available is_available = torch.cuda.is_available() if is_available and device_idx < device_count: # Set the specified device as the current device torch.cuda.set_device(device_idx) # Perform a dummy computation on this device a = torch.ones((100, 100)).to(device_idx) b = torch.ones((100, 100)).to(device_idx) c = a + b # Dummy computation # Synchronize the current stream torch.cuda.synchronize() # Get the current stream and the current device index current_stream = torch.cuda.current_stream() current_device_index = torch.cuda.current_device() else: return { \\"device_count\\": device_count, \\"is_available\\": is_available, \\"current_device_index\\": None, \\"current_stream\\": None } return { \\"device_count\\": device_count, \\"is_available\\": is_available, \\"current_device_index\\": current_device_index, \\"current_stream\\": current_stream }"},{"question":"You have been given a dataset containing car details including their weight, horsepower, miles per gallon (mpg), and displacement. Your task is to analyze the relationship between these variables using seaborn residual plots. **Task:** 1. Load the `mpg` dataset from seaborn. 2. Create three residual plots to achieve the following: - Plot the residuals of a linear regression model between `weight` (x) and `displacement` (y). - Plot the residuals of a linear regression model between `horsepower` (x) and `mpg` (y). - Plot the residuals after fitting a second-order polynomial regression model (order=2) between `horsepower` (x) and `mpg` (y). - Add a LOWESS curve to the second-order polynomial residual plot to reveal any remaining structure, and customize the LOWESS curve to be red in color. **Input:** - The input dataset is internally loaded using seaborn, so there are no external input files to read from. **Output:** - Generate and display the three seaborn residual plots as described. # Constraints: - Use seaborn for creating the residual plots. - Ensure that the plots are visually distinguishable and appropriately labeled. # Example Code Usage: ```python import seaborn as sns # 1. Load the dataset mpg = sns.load_dataset(\\"mpg\\") # 2. Plot the residuals with weight (x) and displacement (y) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") # 3. Plot the residuals with horsepower (x) and mpg (y) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") # 4. Plot the residuals with horsepower (x) and mpg (y), and a second-order polynomial sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) # 5. Add a LOWESS curve to the polynomial residual plot sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) ``` Make sure to import any necessary libraries and use the appropriate seaborn functions to create the residual plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_residual_plots(): # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Create a figure for plotting with an appropriate size plt.figure(figsize=(15, 10)) # 1. Plot the residuals with weight (x) and displacement (y) plt.subplot(2, 2, 1) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residuals of weight vs displacement\') # 2. Plot the residuals with horsepower (x) and mpg (y) plt.subplot(2, 2, 2) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\'Residuals of horsepower vs mpg\') # 3. Plot the residuals with horsepower (x) and mpg (y), fitting a second-order polynomial plt.subplot(2, 2, 3) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\'Residuals of horsepower vs mpg (2nd-order polynomial)\') # 4. Plot the residuals with horsepower (x) and mpg (y), fitting a second-order polynomial, with a LOWESS line plt.subplot(2, 2, 4) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residuals of horsepower vs mpg (2nd-order polynomial with LOWESS)\') # Adjust layout for better spacing between plots plt.tight_layout() # Show the plots plt.show()"},{"question":"**Title:** Implement a Pseudo-Random Number Sequence Simulation and Analysis Tool **Objective:** To assess the student\'s ability to utilize the Python `random` module to generate and analyze pseudo-random number sequences and to simulate probability distributions. **Problem Statement:** You are tasked with creating a simulation tool that can generate pseudo-random number sequences and perform analysis on them. Your tool should meet the following requirements: 1. **Function: `generate_random_sequence`** - **Input:** - `length` (int): The number of random numbers to generate. - `distribution` (str): The type of distribution to use. It must be one of `\'uniform\'`, `\'normal\'`, `\'exponential\'`, or `\'binary\'`. - `params` (dict): Parameters for the chosen distribution. - For `\'uniform\'`: params should include `a` (float) and `b` (float) representing the range [a, b]. - For `\'normal\'`: params should include `mu` (float) and `sigma` (float) representing the mean and standard deviation. - For `\'exponential\'`: params should include `lambda` (float) representing the rate parameter. - For `\'binary\'`: params should be `None` as it will generate 0 or 1 with equal probability. - **Output:** - A list of random numbers generated based on the specified distribution and parameters. 2. **Function: `analyze_sequence`** - **Input:** - `sequence` (list): A list of numbers to analyze. - **Output:** - A dictionary with the following statistics: - `mean`: The mean of the sequence. - `std_dev`: The standard deviation of the sequence. - `min`: The minimum value in the sequence. - `max`: The maximum value in the sequence. - `element_counts`: A dictionary with each unique element and its corresponding count in the sequence. 3. **Function: `simulate_and_analyze`** - **Input:** - `length` (int): The number of random numbers to generate. - `distribution` (str): The type of distribution to use (same as above). - `params` (dict): Parameters for the chosen distribution (same as above). - **Output:** - A tuple with two elements: - The generated sequence (list). - The analysis result (dictionary as specified above). **Requirements:** 1. You must utilize the `random` module to implement the random number generation. 2. Ensure your code exhibits good coding practices and includes appropriate error handling. 3. Provide documentation and comments for clarity. 4. The overall performance should be efficient, especially for large input sizes. **Example Usage:** ```python # Example input parameters length = 1000 distribution = \'normal\' params = {\'mu\': 0, \'sigma\': 1} # Function call sequence, analysis = simulate_and_analyze(length, distribution, params) # Expected output # sequence: A list of 1000 random numbers following a normal distribution with mean 0 and standard deviation 1. # analysis: A dictionary with the calculated mean, standard deviation, min, max, and unique element counts of the sequence. ``` You should write the implementation of `generate_random_sequence`, `analyze_sequence`, and `simulate_and_analyze` functions based on the above specifications. Document your code and provide test cases to validate the correctness of your implementation. **Constraints:** - Assume valid input is provided for all parameters. - Use the `random` module exclusively for random number generation. **Performance Requirements:** - Your implementation should handle large sequences efficiently, with a time complexity approximately O(n) for sequence generation and analysis. Good luck!","solution":"import random import math from collections import Counter def generate_random_sequence(length, distribution, params): Generates a sequence of random numbers based on the specified distribution and parameters. Parameters: - length (int): The number of random numbers to generate. - distribution (str): The type of distribution to use. One of \'uniform\', \'normal\', \'exponential\', or \'binary\'. - params (dict): Parameters for the distribution. Returns: - list: A list of generated random numbers. random_sequence = [] if distribution == \'uniform\': a = params[\'a\'] b = params[\'b\'] random_sequence = [random.uniform(a, b) for _ in range(length)] elif distribution == \'normal\': mu = params[\'mu\'] sigma = params[\'sigma\'] random_sequence = [random.gauss(mu, sigma) for _ in range(length)] elif distribution == \'exponential\': lambd = params[\'lambda\'] random_sequence = [random.expovariate(lambd) for _ in range(length)] elif distribution == \'binary\': random_sequence = [random.randint(0, 1) for _ in range(length)] return random_sequence def analyze_sequence(sequence): Computes statistics for a given sequence of numbers. Parameters: - sequence (list): A list of numbers to analyze. Returns: - dict: A dictionary containing the mean, standard deviation, min, max, and element counts. mean_value = sum(sequence) / len(sequence) variance = sum((x - mean_value) ** 2 for x in sequence) / len(sequence) std_dev_value = math.sqrt(variance) min_value = min(sequence) max_value = max(sequence) element_counts = dict(Counter(sequence)) return { \'mean\': mean_value, \'std_dev\': std_dev_value, \'min\': min_value, \'max\': max_value, \'element_counts\': element_counts, } def simulate_and_analyze(length, distribution, params): Generates a random sequence and analyzes it. Parameters: - length (int): The number of random numbers to generate. - distribution (str): The type of distribution to use. - params (dict): Parameters for the distribution. Returns: - tuple: A tuple containing the generated sequence and its analysis. sequence = generate_random_sequence(length, distribution, params) analysis = analyze_sequence(sequence) return sequence, analysis"},{"question":"**MemoryView Objects in Python** **Context:** Memoryview objects allow you to access the internal data of an object that supports the buffer protocol without copying. This feature is particularly useful for large data arrays to prevent the overhead of copying data. In this task, you are required to implement a function using memoryview objects and related functionalities as described in the documentation. **Task:** You need to implement a function `filter_and_sum_bytes(data: bytearray, threshold: int) -> int` that performs the following steps: 1. Creates a writable memoryview from the provided `data` bytearray. 2. Iterates over each byte in the memoryview, keeps only the bytes that are greater than the `threshold` value, and ignores the rest. 3. Sums the values of the filtered bytes. 4. Returns the sum as an integer. **Input:** - `data`: A bytearray of integers ranging between 0-255. - `threshold`: An integer value against which bytes are compared. **Output:** - An integer representing the sum of byte values that are greater than the given threshold. **Function Signature:** ```python def filter_and_sum_bytes(data: bytearray, threshold: int) -> int: pass ``` **Example:** ```python data = bytearray([10, 20, 30, 40, 50]) threshold = 25 assert filter_and_sum_bytes(data, threshold) == 120 # Only 30, 40, and 50 are summed ``` **Constraints:** 1. The `data` bytearray length will be between 1 and 10^6. 2. The `threshold` will be between 0 and 255. Ensure that your implementation leverages memoryview objects to handle the input efficiently, especially with large data arrays. Aim to achieve an optimal solution concerning both time and space complexity.","solution":"def filter_and_sum_bytes(data: bytearray, threshold: int) -> int: This function creates a writable memoryview from the provided data bytearray, filters bytes greater than the given threshold, and sums their values. :param data: bytearray of integers between 0-255 :param threshold: an integer threshold value :return: integer representing the sum of byte values greater than the threshold memview = memoryview(data) total_sum = sum(byte for byte in memview if byte > threshold) return total_sum"},{"question":"**Coding Assessment Question:** Implement a custom binary-to-binhex4 encoding function and a binhex4-to-binary decoding function from scratch. The goal of this exercise is to evaluate your understanding of file handling, binary data manipulation, and exception management in Python. # Function 1: binary_to_binhex4 **Objective:** Convert a binary file to a binhex4 encoded file. **Function Signature:** ```python def binary_to_binhex4(input_file: str, output_file: str) -> None: ``` **Input:** - `input_file` (str): A string representing the path to the input binary file. - `output_file` (str): A string representing the path to the output binhex4 encoded file. **Output:** - The function writes the binhex4 encoded data to the specified output file. No return value. **Constraints:** - Assume the input file exists and is a valid binary file. - Handle file operations and ensure that files are properly closed after processing. - Implement minimal exception handling to catch and report errors during encoding. # Function 2: binhex4_to_binary **Objective:** Decode a binhex4 encoded file back to its original binary form. **Function Signature:** ```python def binhex4_to_binary(input_file: str, output_file: str) -> None: ``` **Input:** - `input_file` (str): A string representing the path to the input binhex4 encoded file. - `output_file` (str): A string representing the path to the output binary file. **Output:** - The function writes the binary data to the specified output file. No return value. **Constraints:** - Assume the input file exists and is a valid binhex4 encoded file. - Handle file operations and ensure that files are properly closed after processing. - Implement minimal exception handling to catch and report errors during decoding. # Example Usage: ```python # Encode a binary file to binhex4 format binary_to_binhex4(\'example.bin\', \'encoded.bh4\') # Decode a binhex4 encoded file back to binary format binhex4_to_binary(\'encoded.bh4\', \'decoded.bin\') ``` **Notes:** - You are required to handle the reading and writing of files. - Manual handling of binary-to-ASCII and ASCII-to-binary conversion is expected. - The usage of external libraries for binhex encoding/decoding is not allowed.","solution":"import binascii def binary_to_binhex4(input_file: str, output_file: str) -> None: try: with open(input_file, \'rb\') as f_in: binary_data = f_in.read() binhex4_data = binascii.b2a_hex(binary_data).decode(\'ascii\') with open(output_file, \'w\') as f_out: f_out.write(binhex4_data) except Exception as e: print(f\\"An error occurred during encoding: {e}\\") def binhex4_to_binary(input_file: str, output_file: str) -> None: try: with open(input_file, \'r\') as f_in: binhex4_data = f_in.read() binary_data = binascii.a2b_hex(binhex4_data.encode(\'ascii\')) with open(output_file, \'wb\') as f_out: f_out.write(binary_data) except Exception as e: print(f\\"An error occurred during decoding: {e}\\")"},{"question":"# Question: **Title**: Inventory Management System **Description**: You need to build an Inventory Management System for a retail store using Python. This system should track items in the inventory, their quantities, and their prices. The system should also handle adding new items, updating existing items, and retrieving information about the items. Your implementation should demonstrate your understanding of compound statements and advanced Python constructs, including handling exceptions and using context managers. **Requirements**: 1. **Class Definition**: - Define a class `Item` which stores: - the `name` of the item (string). - the `quantity` of the item (integer). - the `price` of the item (float). - Ensure proper encapsulation of attributes. - Include a method `update_quantity` to update the `quantity` of the item. - Include a method `update_price` to update the `price` of the item. 2. **Class Definition & Context Management**: - Define an `Inventory` class which will manage multiple items. It should: - Store a collection of `Item` objects. - Use a context manager to handle file operations for reading and saving the inventory data to a file called `inventory.dat`. 3. **Handling Operations**: - Implement a method `add_item` which takes an item and adds it to the inventory. - Implement a method `get_item` which takes an item name and returns the item from the inventory. - Implement a method `update_item` which takes an item name, a new quantity, and a new price to update the item\'s details. - Implement exception handling within `update_item` to handle cases where an item is not found. 4. **Asynchronous Functions**: - Implement an asynchronous method `save_inventory_to_disk` which saves the current state of the inventory to the file. Ensure that this operation is awaited properly. 5. **Pattern Matching**: - Use pattern matching within a method `search_item` which takes a search string and tries to match it to item names in the inventory. The matching should be done on the basis of: - Exact match - Partial match (substring) - Case insensitive match **Input/Output Examples**: ```python # Creating Inventory and Items inventory = Inventory() # Adding items item1 = Item(\\"Apple\\", 50, 0.5) inventory.add_item(item1) # Updating items try: inventory.update_item(\\"Apple\\", 60, 0.45) except ValueError as e: print(e) # Getting item details apple = inventory.get_item(\\"Apple\\") print(f\\"Item: {apple.name}, Quantity: {apple.quantity}, Price: {apple.price}\\") # Searching items match = inventory.search_item(\\"app\\") print(f\\"Found: {match}\\") # Saving to Disk await inventory.save_inventory_to_disk() ``` **Constraints**: - The `Inventory` class should ensure no duplicate items based on the name. - Use `with` statements to manage file operations. - Ensure that methods are appropriately handling exceptions. **Performance Considerations**: - Ensure that the search operation is efficient and handles cases with large inventories. - Asynchronous file operations should be properly awaited to avoid blocking. Your task is to implement all the required classes and methods fulfilling the above requirements, showcasing your proficiency with Python compound statements and advanced constructs.","solution":"import json import asyncio class Item: def __init__(self, name, quantity, price): self._name = name self._quantity = quantity self._price = price @property def name(self): return self._name @property def quantity(self): return self._quantity @property def price(self): return self._price def update_quantity(self, new_quantity): self._quantity = new_quantity def update_price(self, new_price): self._price = new_price class Inventory: def __init__(self): self.items = {} def add_item(self, item): if item.name in self.items: raise ValueError(f\\"Item with name {item.name} already exists.\\") self.items[item.name] = item def get_item(self, name): return self.items.get(name, None) def update_item(self, name, new_quantity, new_price): if name not in self.items: raise ValueError(f\\"Item with name {name} not found.\\") item = self.items[name] item.update_quantity(new_quantity) item.update_price(new_price) async def save_inventory_to_disk(self): async with asyncio.Lock(): with open(\'inventory.dat\', \'w\') as f: json.dump( { name: {\'quantity\': item.quantity, \'price\': item.price} for name, item in self.items.items() }, f, ) def search_item(self, search_str): matches = [ item for item in self.items.values() if search_str in item.name or search_str.lower() in item.name.lower() or search_str.upper() in item.name.upper() ] return matches"},{"question":"Objective You are tasked with implementing a function that utilizes the `netrc` class to handle authentication data from `.netrc` files. Your function should demonstrate the ability to parse `.netrc` files, retrieve authentication data for given hosts, and handle errors appropriately. Task Implement a function named `retrieve_auth_data` that takes a file path and a list of hostnames as input, and returns a dictionary mapping each hostname to its corresponding (login, account, password) tuple. If a hostname is not found in the file, the function should return `None` for that hostname. Your function must handle possible exceptions, particularly `FileNotFoundError` and `netrc.NetrcParseError`. Input - `file_path` (str): The path to the `.netrc` file. - `hostnames` (list of str): A list of hostnames for which authentication data is to be retrieved. Output - A dictionary where each key is a hostname from the input list, and the corresponding value is either: - a tuple (login, account, password) if the hostname is found in the `.netrc` file. - `None` if the hostname is not found in the file or if any error occurs during the process. Functions and Exceptions to Use - Use `netrc.netrc` to parse the file and retrieve data. - Handle `FileNotFoundError` and `netrc.NetrcParseError`. Constraints - The `.netrc` file may contain multiple hosts. - Ensure your implementation correctly handles default credentials if specified. Example ```python def retrieve_auth_data(file_path, hostnames): # Your implementation here # Sample .netrc content: # machine host1.com # login user1 # account myaccount # password pass1 # # machine host2.com # login user2 # account myaccount2 # password pass2 # # default # login default_user # account default_account # password default_pass # Example usage: file_path = \'path/to/.netrc\' hostnames = [\'host1.com\', \'host2.com\', \'unknownhost.com\'] output = retrieve_auth_data(file_path, hostnames) print(output) # Expected output: # { # \'host1.com\': (\'user1\', \'myaccount\', \'pass1\'), # \'host2.com\': (\'user2\', \'myaccount2\', \'pass2\'), # \'unknownhost.com\': (\'default_user\', \'default_account\', \'default_pass\') # } ``` Notes 1. Ensure that your function is resilient to errors and always returns a dictionary of the specified format. 2. Focus on clear, maintainable code and proper error handling.","solution":"import netrc import os def retrieve_auth_data(file_path, hostnames): Retrieve authentication data from a .netrc file for the specified hostnames. Args: file_path (str): Path to the .netrc file. hostnames (list of str): List of hostnames to retrieve authentication data for. Returns: dict: A dictionary mapping each hostname to a tuple (login, account, password), or None if the hostname is not found or if an error occurs. result = {} try: auth_data = netrc.netrc(file_path) for host in hostnames: try: machine_data = auth_data.authenticators(host) if machine_data: result[host] = (machine_data[0], machine_data[1], machine_data[2]) else: # Fallback to default if specified default_data = auth_data.authenticators(\'default\') result[host] = None if not default_data else (default_data[0], default_data[1], default_data[2]) except KeyError: result[host] = None except (FileNotFoundError, netrc.NetrcParseError): for host in hostnames: result[host] = None return result"},{"question":"You are given the task to write a function that uses the `email.generator.BytesGenerator` class to generate a standardized byte-stream representation of an email message. The email message is represented as an `EmailMessage` object, and your function must provide the functionality to generate its byte-stream according to the given specifications. # Function Signature ```python def generate_email_bytes(msg: EmailMessage, mangle_from: bool = False, maxheaderlen: int = None, policy: Optional[Policy] = None, unixfrom: bool = False, linesep: Optional[str] = None) -> bytes: pass ``` # Parameters - `msg` (EmailMessage): The email message object that needs to be converted to a byte-stream. - `mangle_from` (bool, default=False): Whether to mangle `From` lines in the body to prevent issues when storing messages in Unix format. - `maxheaderlen` (Optional[int], default=None): Maximum length of any header line. If `None`, use the default policy behavior. - `policy` (Optional[Policy], default=None): The policy to control message generation. If `None`, use the policy associated with the message object. - `unixfrom` (bool, default=False): Whether to include the Unix `From` envelope header. - `linesep` (Optional[str], default=None): The separator character between lines in the flattened message. # Returns - `bytes`: The byte-stream representation of the email message. # Example ```python from email.message import EmailMessage from email.policy import default msg = EmailMessage() msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email.\') output_bytes = generate_email_bytes(msg, mangle_from=True, policy=default) print(output_bytes) ``` # Constraints 1. You should use the `BytesGenerator` class from `email.generator`. 2. Ensure that your function handles the `mangle_from`, `maxheaderlen`, `policy`, `unixfrom`, and `linesep` parameters correctly. 3. The function should return a bytes object containing the serialized representation of the email message.","solution":"from email.message import EmailMessage from email.policy import Policy from email.generator import BytesGenerator from io import BytesIO from typing import Optional def generate_email_bytes(msg: EmailMessage, mangle_from: bool = False, maxheaderlen: Optional[int] = None, policy: Optional[Policy] = None, unixfrom: bool = False, linesep: Optional[str] = None) -> bytes: Generates a byte-stream representation of an EmailMessage object. Parameters: - msg: EmailMessage object that needs to be converted to a byte-stream. - mangle_from: Whether to mangle From lines in the body to prevent issues when storing messages in Unix format. - maxheaderlen: Maximum length of any header line. - policy: The policy to control message generation. - unixfrom: Whether to include the Unix From envelope header. - linesep: The separator character between lines in the flattened message. Returns: - bytes: The byte-stream representation of the email message. buffer = BytesIO() generator = BytesGenerator(buffer, mangle_from_=mangle_from, maxheaderlen=maxheaderlen, policy=policy) generator.flatten(msg, unixfrom=unixfrom, linesep=linesep) return buffer.getvalue()"},{"question":"You are tasked with building a secure file integrity verification system using the `hashlib` module in Python. Your system should be able to create hash digests for files using different algorithms, save these digests for later comparison, and verify the integrity of files by checking if they have been tampered with. Part 1: Creating Hash Digests Write a function `create_hash(filepath: str, algorithm: str = \'sha256\') -> str` that: 1. Takes in the path of a file (`filepath`) and the name of the hash algorithm (`algorithm`) to use. 2. Reads the file in binary mode and updates the hash object incrementally to handle large files efficiently. 3. Returns the hex digest of the file content. ```python def create_hash(filepath: str, algorithm: str = \'sha256\') -> str: Creates a hash digest for a given file using the specified algorithm. Parameters: filepath (str): The path to the file to be hashed. algorithm (str): The hash algorithm to use (default is \'sha256\'). Returns: str: The hex digest of the file content. pass ``` Part 2: Saving and Loading Hash Digests Write two functions `save_hash(filepath: str, hash_digest: str)` and `load_hash(filepath: str) -> str`: 1. `save_hash` saves the hash digest to a file (you can choose a standard format for storage). 2. `load_hash` loads the hash digest from the file. ```python def save_hash(filepath: str, hash_digest: str): Saves the hash digest to a file. Parameters: filepath (str): The path to the file where the hash digest will be saved. hash_digest (str): The hash digest to save. pass def load_hash(filepath: str) -> str: Loads the hash digest from a file. Parameters: filepath (str): The path to the file from which to load the hash digest. Returns: str: The loaded hash digest. pass ``` Part 3: Verifying File Integrity Write a function `verify_file(filepath: str, hash_filepath: str, algorithm: str = \'sha256\') -> bool` that: 1. Takes in the path of the file to verify (`filepath`), the path of the file containing the original hash digest (`hash_filepath`), and the name of the hash algorithm (`algorithm`) to use. 2. Computes the current hash digest of the file. 3. Compares the computed digest to the one loaded from `hash_filepath`. 4. Returns `True` if the digests match, `False` otherwise. ```python def verify_file(filepath: str, hash_filepath: str, algorithm: str = \'sha256\') -> bool: Verifies the integrity of a file by comparing its current hash digest with a saved digest. Parameters: filepath (str): The path to the file to verify. hash_filepath (str): The path to the file containing the original hash digest. algorithm (str): The hash algorithm to use (default is \'sha256\'). Returns: bool: True if the file\'s integrity is intact, False otherwise. pass ``` Constraints - You must handle potential exceptions, such as file not found or unsupported hash algorithms. - Your solution should efficiently handle large files. - Clearly document each function and provide examples of usage. Performance Requirements - The `create_hash` function should be able to process files of sizes up to several gigabytes without running out of memory. - Ensure minimal performance overhead by reading and updating the hash in chunks. This will test the student\'s ability to leverage the `hashlib` module to create secure and efficient hashing mechanisms, handle large data efficiently, and implement file integrity verification systems.","solution":"import hashlib def create_hash(filepath: str, algorithm: str = \'sha256\') -> str: Creates a hash digest for a given file using the specified algorithm. Parameters: filepath (str): The path to the file to be hashed. algorithm (str): The hash algorithm to use (default is \'sha256\'). Returns: str: The hex digest of the file content. hash_func = getattr(hashlib, algorithm) hasher = hash_func() with open(filepath, \'rb\') as file: for block in iter(lambda: file.read(4096), b\'\'): hasher.update(block) return hasher.hexdigest() def save_hash(filepath: str, hash_digest: str): Saves the hash digest to a file. Parameters: filepath (str): The path to the file where the hash digest will be saved. hash_digest (str): The hash digest to save. with open(filepath, \'w\') as file: file.write(hash_digest) def load_hash(filepath: str) -> str: Loads the hash digest from a file. Parameters: filepath (str): The path to the file from which to load the hash digest. Returns: str: The loaded hash digest. with open(filepath, \'r\') as file: return file.read().strip() def verify_file(filepath: str, hash_filepath: str, algorithm: str = \'sha256\') -> bool: Verifies the integrity of a file by comparing its current hash digest with a saved digest. Parameters: filepath (str): The path to the file to verify. hash_filepath (str): The path to the file containing the original hash digest. algorithm (str): The hash algorithm to use (default is \'sha256\'). Returns: bool: True if the file\'s integrity is intact, False otherwise. original_hash = load_hash(hash_filepath) current_hash = create_hash(filepath, algorithm) return current_hash == original_hash"},{"question":"# Question: Create a Seaborn Area Plot with Customized Facets and Stacked Areas You are given a dataset containing health expenditure data for multiple countries over several years. Your task is to create an area plot using seaborn that shows the spending trend of each country over the years, with customized facets for each country and a stacked area representing the part-whole relationship of spending. Input Format: - The dataset will have the following columns: - `Year`: The year of the expenditure. - `Country`: The name of the country. - `Spending_USD`: The amount spent in USD. Output: - An area plot using the seaborn.objects module. - Facet the plot by `Country` with 3 columns. - Color the areas by `Country`. - Stack the areas to show the part-whole relationships. - Show lines at the edges of the areas. Constraints: - Ensure that the areas are clearly distinguishable by color. - Ensure proper handling of missing data (hint: consider interpolation). - Sort the data by `Country` for consistent plotting. Performance Requirements: - The solution should efficiently handle datasets up to 10,000 rows. Example Dataset: ```python data = { \'Year\': [2000, 2001, 2002, 2000, 2001, 2002, 2000, 2001, 2002], \'Country\': [\'Country1\', \'Country1\', \'Country1\', \'Country2\', \'Country2\', \'Country2\', \'Country3\', \'Country3\', \'Country3\'], \'Spending_USD\': [3000, 3200, 3500, 2000, 2100, 2200, 1500, 1600, 1700] } ``` Example Code: ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd import numpy as np # Load and prepare the dataset data = pd.DataFrame({ \'Year\': [2000, 2001, 2002, 2000, 2001, 2002, 2000, 2001, 2002], \'Country\': [\'Country1\', \'Country1\', \'Country1\', \'Country2\', \'Country2\', \'Country2\', \'Country3\', \'Country3\', \'Country3\'], \'Spending_USD\': [3000, 3200, 3500, 2000, 2100, 2200, 1500, 1600, 1700] }) # Manipulate the data data = ( data.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() # Handle missing values .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the plot p = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(alpha=0.7), so.Stack(), color=\\"Country\\").add(so.Line()) # Show the plot p.show() ```","solution":"import seaborn.objects as so import pandas as pd import numpy as np def create_area_plot(data): Creates an area plot using seaborn.objects that shows the health expenditure trend of each country over the years, with customized facets and stacked areas. Parameters: data (pd.DataFrame): DataFrame containing columns \'Year\', \'Country\', and \'Spending_USD\'. Returns: p (seaborn.objects.Plot): The seaborn plot object. # Manipulate the data data = ( data.pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate(method=\'linear\', limit_direction=\'forward\', axis=0) # Handle missing values .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Create the plot p = so.Plot(data, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(alpha=0.7), so.Stack(), color=\\"Country\\").add(so.Line()) return p"},{"question":"Loading and Processing Datasets with Scikit-Learn **Objective**: Demonstrate your comprehension of loading and processing datasets using scikit-learn. **Description**: You are given the task of developing a utility function that downloads a dataset from OpenML, processes it, and loads a sample image. **Your task** is to implement the following functions: 1. **`load_and_process_openml_dataset(data_id: int)`**: - **Input**: - `data_id` (int): The identifier for the dataset on OpenML. - **Output**: - A tuple `(X, y)` where `X` is a pandas DataFrame containing the features and `y` is a pandas Series containing the target variable. - **Functionality**: - Use `fetch_openml` to download the dataset specified by `data_id`. - Ensure that the data is returned as a pandas DataFrame (use `as_frame=True`). - Handle cases where the dataset may have missing values by filling them with the mean of the column. 2. **`load_sample_image_and_display(image_name: str)`**: - **Input**: - `image_name` (str): The name of the sample image to load (e.g., \\"china.jpg\\" or \\"flower.jpg\\"). - **Output**: - None - **Functionality**: - Use `load_sample_image` to load the sample image specified by `image_name`. - Display the image using `matplotlib.pyplot.imshow` and ensure the correct scaling to the range 0 - 1 if required. **Constraints**: - You are not allowed to use any other libraries apart from `pandas`, `numpy`, `sklearn`, and `matplotlib`. - The dataset must be returned in a clean format with no missing values. - The image display should be clear with no axes shown. **Example Usage**: ```python # Load and process the Mice Protein dataset from OpenML X, y = load_and_process_openml_dataset(data_id=40966) print(X.head()) print(y.head()) # Load and display sample images load_sample_image_and_display(\\"china.jpg\\") ``` **Performance Requirements**: - Your implementation should efficiently handle datasets with up to 100,000 samples and 1,000 features. - Ensure that image loading and display operations are performed without significant delays. **Evaluation Criteria**: - Correctness of the implemented functions. - Efficiency and clarity of the code. - Proper handling of missing values. - Correct and clear display of the sample image.","solution":"import pandas as pd import numpy as np from sklearn.datasets import fetch_openml, load_sample_image import matplotlib.pyplot as plt def load_and_process_openml_dataset(data_id: int): Downloads a dataset from OpenML, processes it, and returns features and target variable. Parameters: data_id (int): The identifier for the dataset on OpenML. Returns: tuple: (X, y) where X is a pandas DataFrame containing the features and y is a pandas Series containing the target variable. # Fetch the dataset from OpenML dataset = fetch_openml(data_id=data_id, as_frame=True) # Extract features and target X = dataset.data y = dataset.target # Handle missing values by filling with the mean of the column X = X.apply(lambda col: col.fillna(col.mean()), axis=0) return X, y def load_sample_image_and_display(image_name: str): Loads a sample image and displays it using matplotlib. Parameters: image_name (str): The name of the sample image to load (e.g., \\"china.jpg\\" or \\"flower.jpg\\"). # Load the image image = load_sample_image(image_name) # Display the image plt.imshow(image) plt.axis(\'off\') # No axes for clearer display plt.show()"},{"question":"Objective: This question aims to assess your understanding of the `sys` module in Python, specifically focusing on command-line arguments (`sys.argv`), recursion limits, and tracing functionalities. Problem Statement: You have been given a Python script file named `recursive_calculator.py` that performs recursive calculations based on command-line input. Your task is to implement the following features: 1. **Command-Line Arguments Handling:** - Accept command-line arguments to perform a specific recursive operation. - The first command-line argument should specify the type of operation (\'factorial\' or \'fibonacci\'). - The second command-line argument should be the integer input for the operation. 2. **Recursion Limit Management:** - Adjust the recursion limit based on the depth required by the operation. - For the \'factorial\' operation, ensure that the recursion limit is sufficient to compute factorial up to `n!`. - For the \'fibonacci\' operation, ensure that the recursion limit is sufficient to compute the `n`th Fibonacci number. 3. **Tracing Execution:** - Implement tracing to show the entry and exit of each recursive call. - Print helpful debugging information showing the current depth of recursion and the values being processed. 4. **Exception Handling:** - Properly handle exceptions that might occur due to invalid inputs or excessive recursion limits. Input Format: - Command-line arguments in the following format: ``` python recursive_calculator.py <operation> <integer> ``` Output Format: - Print the result of the recursive operation. - Print the tracing information for each recursive call. Example Usage: 1. To compute the factorial of 5: ``` python recursive_calculator.py factorial 5 ``` Output: ``` Entering factorial 5 Entering factorial 4 Entering factorial 3 Entering factorial 2 Entering factorial 1 Entering factorial 0 Exiting factorial 0 => 1 Exiting factorial 1 => 1 Exiting factorial 2 => 2 Exiting factorial 3 => 6 Exiting factorial 4 => 24 Exiting factorial 5 => 120 Result: 120 ``` 2. To compute the 6th Fibonacci number: ``` python recursive_calculator.py fibonacci 6 ``` Output: ``` Entering fibonacci 6 Entering fibonacci 5 Entering fibonacci 4 Entering fibonacci 3 Entering fibonacci 2 Entering fibonacci 1 Exiting fibonacci 1 => 1 Entering fibonacci 0 Exiting fibonacci 0 => 0 Exiting fibonacci 2 => 1 Entering fibonacci 1 Exiting fibonacci 1 => 1 Exiting fibonacci 3 => 2 Entering fibonacci 2 Entering fibonacci 1 Exiting fibonacci 1 => 1 Entering fibonacci 0 Exiting fibonacci 0 => 0 Exiting fibonacci 2 => 1 Exiting fibonacci 4 => 3 Entering fibonacci 3 Entering fibonacci 2 Entering fibonacci 1 Exiting fibonacci 1 => 1 Entering fibonacci 0 Exiting fibonacci 0 => 0 Exiting fibonacci 2 => 1 Entering fibonacci 1 Exiting fibonacci 1 => 1 Exiting fibonacci 3 => 2 Exiting fibonacci 5 => 5 Entering fibonacci 4 Entering fibonacci 3 Entering fibonacci 2 Entering fibonacci 1 Exiting fibonacci 1 => 1 Entering fibonacci 0 Exiting fibonacci 0 => 0 Exiting fibonacci 2 => 1 Entering fibonacci 1 Exiting fibonacci 1 => 1 Exiting fibonacci 3 => 2 Exiting fibonacci 4 => 3 Exiting fibonacci 6 => 8 Result: 8 ``` Constraints: - The input integer for both operations can be assumed to be non-negative. - Properly handle cases where the input integer is too large and may cause recursion depth issues. Implementation: ```python import sys def trace_function(frame, event, arg): if event == \'call\': print(f\'Entering {frame.f_code.co_name} {frame.f_locals.get(\\"n\\", \\"\\")}\') elif event == \'return\': print(f\'Exiting {frame.f_code.co_name} {frame.f_locals.get(\\"n\\", \\"\\")} => {arg}\') return trace_function def factorial(n): if n == 0: return 1 return n * factorial(n - 1) def fibonacci(n): if n == 0: return 0 elif n == 1: return 1 return fibonacci(n - 1) + fibonacci(n - 2) def main(): sys.settrace(trace_function) try: operation = sys.argv[1] n = int(sys.argv[2]) if operation == \'factorial\': sys.setrecursionlimit(n + 10) result = factorial(n) elif operation == \'fibonacci\': sys.setrecursionlimit(2 * n + 10) result = fibonacci(n) else: raise ValueError(\'Invalid operation specified\') print(f\'Result: {result}\') except IndexError: print(\'Usage: python recursive_calculator.py <operation> <integer>\') except ValueError as ve: print(f\'Error: {ve}\') except RecursionError: print(\'Error: Recursion limit exceeded\') finally: sys.settrace(None) if __name__ == \'__main__\': main() ```","solution":"import sys def trace_function(frame, event, arg): if event == \'call\': print(f\'Entering {frame.f_code.co_name} {frame.f_locals.get(\\"n\\", \\"\\")}\') elif event == \'return\': print(f\'Exiting {frame.f_code.co_name} {frame.f_locals.get(\\"n\\", \\"\\")} => {arg}\') return trace_function def factorial(n): if n == 0: return 1 return n * factorial(n - 1) def fibonacci(n): if n == 0: return 0 elif n == 1: return 1 return fibonacci(n - 1) + fibonacci(n - 2) def main(): sys.settrace(trace_function) try: operation = sys.argv[1] n = int(sys.argv[2]) if operation == \'factorial\': sys.setrecursionlimit(n + 10) result = factorial(n) elif operation == \'fibonacci\': sys.setrecursionlimit(2 * n + 10) result = fibonacci(n) else: raise ValueError(\'Invalid operation specified\') print(f\'Result: {result}\') except IndexError: print(\'Usage: python recursive_calculator.py <operation> <integer>\') except ValueError as ve: print(f\'Error: {ve}\') except RecursionError: print(\'Error: Recursion limit exceeded\') finally: sys.settrace(None) if __name__ == \'__main__\': main()"},{"question":"# Python Initialization Configuration Task You have been provided with the documentation for Python Initialization Configuration, which includes details on setting up a custom Python environment using `PyConfig` and `PyPreConfig`. Your task is to write a Python function that simulates configuring and initializing a custom Python environment based on certain criteria. Requirements: 1. **Initialize Python Config**: Create an instance of `PyConfig` and initialize it for standard Python configuration. 2. **Set Command Line Arguments**: Add specific command line arguments to the configuration. 3. **Customize Settings**: Modify at least three fields in the configuration (`isolated`, `utf8_mode`, `buffered_stdio`) based on provided arguments: - `isolated`: 1 or 0. - `utf8_mode`: 1 or 0. - `buffered_stdio`: 1 or 0. 4. **Exception Handling**: Implement exception handling for each step and ensure to clear configurations if an exception occurs. 5. **Return Success**: The function should return a success message if all steps are completed correctly or an appropriate error message otherwise. Function Signature: ```python def configure_and_initialize_python(args: list[str], isolated: int, utf8_mode: int, buffered_stdio: int) -> str: Configures and initializes a custom Python environment. :param args: A list of command line arguments as strings. :param isolated: Integer flag for isolated mode (1 or 0). :param utf8_mode: Integer flag to enable/disable UTF-8 mode (1 or 0). :param buffered_stdio: Integer flag for buffered stdio (1 or 0). :return: A success message if completed correctly, or an error message if any exceptions occur. ``` Example Usage: ```python result = configure_and_initialize_python([\\"python_script.py\\", \\"-u\\"], 1, 1, 0) print(result) # Should print \\"Python initialized successfully\\" or appropriate error message. ``` # Constraints: - `args` should contain valid strings representing command-line arguments. - `isolated`, `utf8_mode`, and `buffered_stdio` should strictly be `1` or `0`. - Ensure that `PyConfig_Clear` is called to properly release configuration memory in case of exceptions. Note: Mock the necessary parts of the Python C API required to simulate this initialization within your function. Your implementation should demonstrate understanding of the provided documentation on Python Initialization Configuration.","solution":"def configure_and_initialize_python(args, isolated, utf8_mode, buffered_stdio): Configures and initializes a custom Python environment. :param args: A list of command line arguments as strings. :param isolated: Integer flag for isolated mode (1 or 0). :param utf8_mode: Integer flag to enable/disable UTF-8 mode (1 or 0). :param buffered_stdio: Integer flag for buffered stdio (1 or 0). :return: A success message if completed correctly, or an error message if any exceptions occur. try: # Mock instances of PyConfig config = { \'args\': args, \'isolated\': isolated, \'utf8_mode\': utf8_mode, \'buffered_stdio\': buffered_stdio, \'other_config_settings\': {} } # Simulate standard initialization procedure if not args or not all(isinstance(arg, str) for arg in args): raise ValueError(\\"Invalid command line arguments provided\\") if isolated not in [0, 1] or utf8_mode not in [0, 1] or buffered_stdio not in [0, 1]: raise ValueError(\\"Isolated, UTF-8 mode, and buffered stdio must be 0 or 1\\") # Simulating the initialization logic # Assuming initialization is successful if it reaches this point print(\\"Configured with args: {args}, isolated: {isolated}, utf8_mode: {utf8_mode}, buffered_stdio: {buffered_stdio}\\".format(**config)) return \\"Python initialized successfully\\" except Exception as e: return \\"Initialization failed: \\" + str(e)"},{"question":"# Socket Programming Challenge Objective The goal of this challenge is to assess your understanding of creating and managing client and server sockets in Python, handling data transmission, and managing multiple socket connections efficiently. Task You are required to implement a simple server-client architecture using Python sockets. The server should handle multiple clients concurrently using non-blocking sockets and the `select` function. The client will send messages to the server, and the server should echo back the received messages to the respective clients. Requirements 1. **Server Implementation**: - Create an INET (IPv4), STREAM (TCP) server socket. - Bind the socket to `localhost` on port `12345`. - Use non-blocking sockets and the `select` function to handle multiple clients simultaneously. - For each client connection, receive messages and echo them back to the client. - Properly handle socket disconnection and cleanup. 2. **Client Implementation**: - Create an INET (IPv4), STREAM (TCP) client socket. - Connect to the server at `localhost` on port `12345`. - Send a message to the server and receive the echoed message. - Properly handle socket closure. Input and Output Formats The client will send a string message to the server and the server will echo the same message back. *Example Communication*: ``` Client sends: \\"Hello, Server!\\" Server echoes: \\"Hello, Server!\\" ``` Constraints - Use Python\'s built-in `socket` module. - The server should handle at least 5 concurrent clients. Performance Requirements - Efficiently manage multiple socket connections without blocking the main execution thread. - Ensure that the server can handle rapid succession of client connections and disconnections. Implementation Details - Implement the server in a file named `server.py`. - Implement the client in a file named `client.py`. - Ensure the server can be started using: `python server.py`. - Ensure the client can be started using: `python client.py`. Code Structure ```python # server.py import socket import select def run_server(): # Write your server implementation here pass if __name__ == \\"__main__\\": run_server() # client.py import socket def run_client(message): # Write your client implementation here pass if __name__ == \\"__main__\\": message = \\"Hello, Server!\\" # You can change the message to test different scenarios run_client(message) ``` Notes - Make sure to handle partial sends and receives as discussed in the documentation. - Ensure proper handling of socket errors and exceptions. - Test your server and client thoroughly to ensure they meet the requirements.","solution":"# server.py import socket import select def run_server(): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'localhost\', 12345)) server_socket.listen(5) server_socket.setblocking(False) inputs = [server_socket] outputs = [] message_queues = {} print(\\"Server is running and listening on port 12345\\") try: while inputs: readable, writable, exceptional = select.select(inputs, outputs, inputs) for s in readable: if s is server_socket: client_socket, client_address = s.accept() print(f\'New connection from {client_address}\') client_socket.setblocking(False) inputs.append(client_socket) message_queues[client_socket] = [] else: data = s.recv(1024) if data: print(f\'Received data: {data.decode()} from {s.getpeername()}\') message_queues[s].append(data) if s not in outputs: outputs.append(s) else: print(f\'Closing connection to {s.getpeername()}\') if s in outputs: outputs.remove(s) inputs.remove(s) s.close() del message_queues[s] for s in writable: if message_queues[s]: next_message = message_queues[s].pop(0) s.send(next_message) else: outputs.remove(s) for s in exceptional: print(f\'Handling exceptional condition for {s.getpeername()}\') inputs.remove(s) if s in outputs: outputs.remove(s) s.close() del message_queues[s] finally: print(\\"Shutting down server\\") server_socket.close() if __name__ == \\"__main__\\": run_server()"},{"question":"# Objective The goal of this exercise is to assess your understanding of creating data visualizations using the seaborn `objects` module, specifically focusing on `so.Plot`, `so.Dots`, and `so.Jitter`. You are required to manipulate these elements to produce a specific plot from a given dataset. # Task 1. Load the `penguins` dataset using `seaborn.load_dataset`. 2. Create a scatter plot that displays the relationship between the `body_mass_g` and `flipper_length_mm` of penguins, with appropriate jitter applied to avoid overlapping points. # Requirements - Load the `penguins` dataset. - Create a scatter plot using `so.Plot` with `body_mass_g` on the x-axis and `flipper_length_mm` on the y-axis. - Add `so.Dots()` to visualize each data point. - Apply jitter along the x-axis and y-axis using `so.Jitter()`. The x-axis jitter should be 200 units, and the y-axis jitter should be 5 units. # Input `None` - The dataset should be loaded within the function. # Output Your function should display the plot directly using the Python plotting libraries. # Function Signature ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguins_scatter_plot(): # Your code here ``` # Example Usage When the function `create_penguins_scatter_plot` is called, it should load the dataset, create the scatter plot as specified, and display the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguins_scatter_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the scatter plot using so.Plot with jitter plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(200), so.Jitter(5)) ) plot.show()"},{"question":"# Semi-Supervised Learning with Label Propagation You are tasked with implementing a semi-supervised learning function using scikit-learn\'s `LabelPropagation`. The function should train a model on a dataset with both labeled and unlabeled examples and then predict the labels for the unlabeled examples. Function Signature ```python def semi_supervised_label_propagation(X: np.ndarray, y: np.ndarray, kernel: str = \'knn\', n_neighbors: int = 7, gamma: float = 20.0) -> np.ndarray: Trains a LabelPropagation model on the given dataset and predicts labels for the unlabeled examples. Parameters: X (np.ndarray): Feature matrix of shape (n_samples, n_features). y (np.ndarray): Target labels of shape (n_samples,). Unlabeled entries should be marked with -1. kernel (str): The kernel to use (\'knn\' or \'rbf\'). Defaults to \'knn\'. n_neighbors (int): Number of neighbors to use for the \'knn\' kernel. Defaults to 7. gamma (float): Parameter for the \'rbf\' kernel. Defaults to 20.0. Returns: np.ndarray: Predicted labels for the entire dataset. pass ``` # Inputs 1. `X`: A 2D NumPy array where each row is a sample, and each column is a feature. 2. `y`: A 1D NumPy array of labels with the same number of elements as rows in `X`. Unlabeled examples should be marked by -1. 3. `kernel`: A string, either `\'knn\'` or `\'rbf\'` that specifies the kernel to use in label propagation. 4. `n_neighbors`: An integer specifying the number of neighbors to consider if `kernel` is `\'knn\'`. 5. `gamma`: A float specifying the parameter for the `rbf` kernel. # Output - Return a 1D NumPy array with the predicted labels for the entire dataset (both labeled and unlabeled points). # Constraints 1. You must use scikit-learn\'s `LabelPropagation` class to build your model. 2. Assume that `X` will always be a valid 2D array and `y` will be a valid 1D array with some labels as `-1`. # Example Usage ```python import numpy as np X = np.array([[1, 1], [2, 1], [3, 2], [8, 8], [9, 8], [8, 9]]) y = np.array([0, 0, 0, -1, 1, 1]) predicted_labels = semi_supervised_label_propagation(X, y, kernel=\'knn\', n_neighbors=3) print(predicted_labels) # prints array with predicted labels, including for previously unlabeled points ``` # Notes - Testing your implementation with various datasets and parameters will help ensure that it is robust and works correctly under different conditions. - Make sure to handle any edge cases, such as having all points initially labeled or all points initially unlabeled.","solution":"import numpy as np from sklearn.semi_supervised import LabelPropagation def semi_supervised_label_propagation(X: np.ndarray, y: np.ndarray, kernel: str = \'knn\', n_neighbors: int = 7, gamma: float = 20.0) -> np.ndarray: Trains a LabelPropagation model on the given dataset and predicts labels for the unlabeled examples. Parameters: X (np.ndarray): Feature matrix of shape (n_samples, n_features). y (np.ndarray): Target labels of shape (n_samples,). Unlabeled entries should be marked with -1. kernel (str): The kernel to use (\'knn\' or \'rbf\'). Defaults to \'knn\'. n_neighbors (int): Number of neighbors to use for the \'knn\' kernel. Defaults to 7. gamma (float): Parameter for the \'rbf\' kernel. Defaults to 20.0. Returns: np.ndarray: Predicted labels for the entire dataset. model = LabelPropagation(kernel=kernel, n_neighbors=n_neighbors, gamma=gamma) model.fit(X, y) return model.transduction_"},{"question":"# Problem: Implementing a Custom Bzip2 Compressor and Decompressor As a data engineer, you are required to handle large data files efficiently. Compression is a key aspect of your work to save storage space and reduce transmission times. Your task is to implement a custom wrapper class around the `bz2` moduleâ€™s compressor and decompressor objects for incremental compression and decompression of data. Requirements: 1. **Class Definition:** - Define a `CustomBZ2` class that supports incremental compression and decompression. 2. **Methods**: - `compress_data(self, data: bytes) -> bytes`: - Compresses the given data incrementally and returns the compressed bytes. - `finalize_compression(self) -> bytes`: - Finishes the compression process and returns any remaining compressed data. - `decompress_data(self, data: bytes) -> bytes`: - Decompresses the given data incrementally and returns the decompressed bytes. - `finalize_decompression(self) -> bytes`: - Returns any remaining uncompressed data if any. 3. **Constructor Parameters**: - `compresslevel`: - Specify the level of compression. Values range from 1 (least compression) to 9 (most compression). Default is 9. Constraints: - The class needs to handle large data chunks efficiently. - Ensure that instances of `CustomBZ2` can be used to alternately compress and decompress without the need for multiple instances. Input: - Data to be compressed or decompressed as bytes-like objects. Output: - Compressed or decompressed data as bytes-like objects. Example Usage: ```python # Initialize compressor and decompressor bz2_wrapper = CustomBZ2(compresslevel=5) # Data to be compressed data = b\\"Example data that needs to be compressed in chunks.\\" # Perform incremental compression compressed_data = b\\"\\" compressed_data += bz2_wrapper.compress_data(data[:10]) compressed_data += bz2_wrapper.compress_data(data[10:]) compressed_data += bz2_wrapper.finalize_compression() # Perform incremental decompression decompressed_data = b\\"\\" decompressed_data += bz2_wrapper.decompress_data(compressed_data[:15]) decompressed_data += bz2_wrapper.decompress_data(compressed_data[15:]) decompressed_data += bz2_wrapper.finalize_decompression() assert data == decompressed_data print(\\"Compression and decompression were successful!\\") ``` # Additional Notes: - Make sure to handle any edge cases, such as empty data or partially processed chunks. - Optimize for memory efficiency and performance.","solution":"import bz2 class CustomBZ2: def __init__(self, compresslevel=9): self.compressor = bz2.BZ2Compressor(compresslevel) self.decompressor = bz2.BZ2Decompressor() def compress_data(self, data: bytes) -> bytes: return self.compressor.compress(data) def finalize_compression(self) -> bytes: return self.compressor.flush() def decompress_data(self, data: bytes) -> bytes: return self.decompressor.decompress(data) def finalize_decompression(self) -> bytes: # No finalization method is provided by bz2 for decompression, any remaining # data should already be decompressed by decompress method return b\\"\\""},{"question":"# PyTorch Coding Assessment Question **Problem Statement:** Write a function `reshape_tensor` that takes a PyTorch tensor `tensor` and a list of integers `new_shape` as input. The function should return a new tensor that is reshaped according to the specified `new_shape`. If the reshaping is not possible (i.e., the product of the dimensions does not match the total number of elements in the original tensor), the function should raise a ValueError. Additionally, write a function `extract_dimensions` that takes a PyTorch tensor `tensor` as input and returns a list containing the size of each dimension using `torch.Size`. **Function Signatures:** ```python def reshape_tensor(tensor: torch.Tensor, new_shape: list) -> torch.Tensor: pass def extract_dimensions(tensor: torch.Tensor) -> list: pass ``` **Input:** - `tensor` (torch.Tensor): A PyTorch tensor of any shape. - `new_shape` (list): A list of integers specifying the desired shape. **Output:** - `reshape_tensor` should return a new tensor with the specified shape if possible, otherwise raise ValueError. - `extract_dimensions` should return a list of integers representing the size of each dimension of the input `tensor`. **Constraints:** - Ensure the product of the dimensions in `new_shape` matches the total number of elements in `tensor`. - The input `tensor` will be a PyTorch tensor. - The `new_shape` list will contain positive integers only. **Examples:** ```python import torch # Example for reshape_tensor tensor = torch.ones(2, 3, 4) new_shape = [6, 4] reshaped_tensor = reshape_tensor(tensor, new_shape) print(reshaped_tensor.size()) # Output: torch.Size([6, 4]) new_shape_invalid = [7, 4] try: reshape_tensor(tensor, new_shape_invalid) except ValueError as e: print(e) # Output: \\"Total number of elements does not match the new shape.\\" # Example for extract_dimensions tensor = torch.ones(2, 3, 4) dimensions = extract_dimensions(tensor) print(dimensions) # Output: [2, 3, 4] ``` **Notes:** - Make sure to handle errors gracefully and provide meaningful error messages. - Utilize `torch.Size` to work with tensor dimensions for the `extract_dimensions` function.","solution":"import torch def reshape_tensor(tensor: torch.Tensor, new_shape: list) -> torch.Tensor: Reshapes the input tensor to the new shape if possible. If the reshaping is not possible due to mismatched number of elements, raises a ValueError. if tensor.numel() != torch.prod(torch.tensor(new_shape)).item(): raise ValueError(\\"Total number of elements does not match the new shape.\\") return tensor.view(*new_shape) def extract_dimensions(tensor: torch.Tensor) -> list: Extracts the dimensions of the input tensor and returns them as a list. return list(tensor.size())"},{"question":"**Objective:** Design a script that accepts a directory path and a file extension as command-line arguments, searches for all files with the given extension within that directory (including subdirectories), compresses them, and outputs the total time taken for the compression process. **Requirements:** 1. **Command-Line Arguments:** - **Required**: Directory path (`--dir`) - **Required**: File extension (`--ext`) 2. **Functionality:** - Traverse the given directory recursively to find all files with the specified extension. - Compress each of these files using zlib. - Measure and print the total time taken for the compression process. 3. **Constraints:** - The script should handle the case where the directory does not exist. - The script should handle cases where no files with the specified extension are found. 4. **Performance Requirements:** - Efficiency in file searching and compression. - Accurate time measurement. **Expected Solution Structure:** 1. **Imports:** - Import necessary modules (`os`, `sys`, `argparse`, `zlib`, `time`). 2. **Functions:** - `parse_args()`: Parse command-line arguments. - `find_files(directory, extension)`: Recursively find and return a list of files with the given extension. - `compress_files(file_list)`: Compress each file in the list and return the total compression time. 3. **Main Script:** - Parse the arguments. - Find files. - Perform compression. - Print the total time taken. **Sample Input:** ```sh python compress_files.py --dir /path/to/directory --ext .txt ``` **Sample Output:** ``` Number of files found: 10 Total compression time: 5.34 seconds ``` **Implementation Details:** Ensure the usage of proper error handling using `try` and `except` blocks. ```python import os import argparse import zlib import time def parse_args(): parser = argparse.ArgumentParser(description=\\"Search and compress files in a directory.\\") parser.add_argument(\\"--dir\\", required=True, help=\\"The directory path to search files in.\\") parser.add_argument(\\"--ext\\", required=True, help=\\"The file extension to search for.\\") return parser.parse_args() def find_files(directory, extension): files = [] for dirpath, _, filenames in os.walk(directory): for filename in filenames: if filename.endswith(extension): files.append(os.path.join(dirpath, filename)) return files def compress_files(file_list): start_time = time.time() for filepath in file_list: with open(filepath, \'rb\') as file: content = file.read() compressed_content = zlib.compress(content) with open(filepath + \'.zlib\', \'wb\') as compressed_file: compressed_file.write(compressed_content) end_time = time.time() return end_time - start_time def main(): args = parse_args() directory = args.dir extension = args.ext if not os.path.exists(directory): print(f\\"Error: The directory \'{directory}\' does not exist.\\") sys.exit(1) files = find_files(directory, extension) if not files: print(f\\"No files with extension \'{extension}\' found in directory \'{directory}\'.\\") sys.exit(1) print(f\\"Number of files found: {len(files)}\\") total_time = compress_files(files) print(f\\"Total compression time: {total_time:.2f} seconds\\") if __name__ == \\"__main__\\": main() ``` **Note:** Ensure to comment the code properly and provide necessary documentation for understanding.","solution":"import os import argparse import zlib import time def parse_args(): Parse command-line arguments parser = argparse.ArgumentParser(description=\\"Search and compress files in a directory.\\") parser.add_argument(\\"--dir\\", required=True, help=\\"The directory path to search files in.\\") parser.add_argument(\\"--ext\\", required=True, help=\\"The file extension to search for.\\") return parser.parse_args() def find_files(directory, extension): Recursively find all files with the given extension in the specified directory. Parameters: directory (str): The directory path to search files in. extension (str): The file extension to search for. Returns: list: A list of file paths that match the given extension. files = [] for dirpath, _, filenames in os.walk(directory): for filename in filenames: if filename.endswith(extension): files.append(os.path.join(dirpath, filename)) return files def compress_files(file_list): Compress each file in the list using zlib and measure the total compression time. Parameters: file_list (list): List of file paths to compress. Returns: float: Total time taken for the compression process in seconds. start_time = time.time() for filepath in file_list: with open(filepath, \'rb\') as file: content = file.read() compressed_content = zlib.compress(content) with open(filepath + \'.zlib\', \'wb\') as compressed_file: compressed_file.write(compressed_content) end_time = time.time() return end_time - start_time def main(): Main function to parse arguments, find files, compress them, and print the total compression time. args = parse_args() directory = args.dir extension = args.ext if not os.path.exists(directory): print(f\\"Error: The directory \'{directory}\' does not exist.\\") sys.exit(1) files = find_files(directory, extension) if not files: print(f\\"No files with extension \'{extension}\' found in directory \'{directory}\'.\\") sys.exit(1) print(f\\"Number of files found: {len(files)}\\") total_time = compress_files(files) print(f\\"Total compression time: {total_time:.2f} seconds\\") if __name__ == \\"__main__\\": main()"},{"question":"# Secure Log Verifier You are tasked with creating a secure log verifier for a system that logs user activities. The verifier ensures that logged data has not been tampered with. You will use the hashlib library in Python to accomplish this. # Requirements: 1. **Function Implementation:** Implement a function `secure_log_verifier(log_entries, secret_key, salt)` that accepts the following parameters: - `log_entries`: A list of strings, each representing a log entry. - `secret_key`: A bytes object representing the secret key for keyed hashing. - `salt`: A bytes object representing the salt for hashing. 2. **Hashing Mechanism:** - Initialize a `blake2b` hash object with the given `secret_key` and `salt`. - Hash each log entry sequentially using the `update()` method. - Retrieve the final hexadecimal digest using the `hexdigest()` method. 3. **Validation Function:** Implement another function `validate_log(log_entries, secret_key, salt, expected_digest)` that accepts: - `log_entries`: Same as above. - `secret_key`: Same as above. - `salt`: Same as above. - `expected_digest`: A string representing the expected hexadecimal digest obtained during the logging process. This function should return `True` if the computed digest matches the `expected_digest`, otherwise return `False`. # Constraints: - The `log_entries` list will contain up to 10,000 entries. - Each log entry is a string of up to 200 characters. - The `secret_key` and `salt` are always provided as non-empty byte objects. # Performance: - Ensure that the computations are efficient and handle the given constraints within a reasonable time frame. # Example: ```python def secure_log_verifier(log_entries, secret_key, salt): # Implement this function def validate_log(log_entries, secret_key, salt, expected_digest): # Implement this function # Example Usage log_entries = [\\"Login: User A\\", \\"Logout: User A\\", \\"Login: User B\\", ...] secret_key = b\'supersecretkey\' salt = b\'unique_salt\' # Generate the digest digest = secure_log_verifier(log_entries, secret_key, salt) # Validate the log entries with the expected digest is_valid = validate_log(log_entries, secret_key, salt, digest) print(is_valid) # Should print: True ``` # Notes: - Make sure to handle the hashing operations securely and avoid common pitfalls such as re-using the same hash object instance for different logs.","solution":"import hashlib def secure_log_verifier(log_entries, secret_key, salt): Generates a BLAKE2b hexadecimal digest for the given log entries using the provided secret key and salt. hasher = hashlib.blake2b(key=secret_key, salt=salt) for entry in log_entries: hasher.update(entry.encode()) return hasher.hexdigest() def validate_log(log_entries, secret_key, salt, expected_digest): Validates the log entries against the expected digest. generated_digest = secure_log_verifier(log_entries, secret_key, salt) return generated_digest == expected_digest"},{"question":"**Objective**: The purpose of this assessment is to evaluate your understanding of the `hashlib` module in Python, specifically focusing on creating and utilizing hash objects, keyed hashing, and key derivation functions. Problem Statement You are tasked with implementing a function that takes a list of filenames and their contents, hashes the contents using different algorithms, and returns a secure summary of the files\' contents. Additionally, implement functionalities for verifying the file integrity and securely hashing passwords. Below are the detailed requirements: 1. **Function: `hash_files`** - **Input**: `files` - A list of tuples `[(filename: str, content: bytes), ...]`. - **Output**: A dictionary where keys are filenames and values are another dictionary with keys \'sha256\', \'blake2b\', and \'sha3_256\' containing the corresponding hexdigests. 2. **Function: `verify_file`** - **Input**: `filename`, `content`, `hash_dict` - `filename` - The name of the file to check. - `content` - The contents of the file. - `hash_dict` - The dictionary returned by `hash_files`. - **Output**: `True` if the provided content matches the stored hash in `hash_dict`; `False` otherwise. 3. **Function: `hash_password`** - **Input**: `password` - The password to hash (string). - **Output**: A dictionary containing: - `pbkdf2` - A hexadecimal hash using PBKDF2 with SHA-256. - `scrypt` - A hexadecimal hash using scrypt with sensible defaults (parameters). 4. **Function: `verify_password`** - **Input**: `password`, `hash_dict` - `password` - The password to verify. - `hash_dict` - The dictionary returned by `hash_password`. - **Output**: `True` if the provided password matches the stored hash in `hash_dict`; `False` otherwise. Constraints - Use appropriate parameters for PBKDF2 and scrypt to ensure security. - Ensure efficient computation and avoid any blocking operations. Example ```python files = [(\\"file1.txt\\", b\\"Hello World\\"), (\\"file2.txt\\", b\\"Goodbye World\\")] hash_dict = hash_files(files) # { # \\"file1.txt\\": { # \\"sha256\\": \\"<sha256_hexdigest>\\", # \\"blake2b\\": \\"<blake2b_hexdigest>\\", # \\"sha3_256\\": \\"<sha3_256_hexdigest>\\" # }, # \\"file2.txt\\": { # \\"sha256\\": \\"<sha256_hexdigest>\\", # \\"blake2b\\": \\"<blake2b_hexdigest>\\", # \\"sha3_256\\": \\"<sha3_256_hexdigest>\\" # } # } assert verify_file(\\"file1.txt\\", b\\"Hello World\\", hash_dict) == True assert verify_file(\\"file1.txt\\", b\\"Wrong content\\", hash_dict) == False password = \\"secure_password!\\" password_hash_dict = hash_password(password) # { # \\"pbkdf2\\": \\"<pbkdf2_hexdigest>\\", # \\"scrypt\\": \\"<scrypt_hexdigest>\\" # } assert verify_password(password, password_hash_dict) == True assert verify_password(\\"wrong_password\\", password_hash_dict) == False ``` Implement the functions below: ```python import hashlib from hmac import compare_digest def hash_files(files): # Your code here pass def verify_file(filename, content, hash_dict): # Your code here pass def hash_password(password): # Your code here pass def verify_password(password, hash_dict): # Your code here pass ```","solution":"import hashlib from hmac import compare_digest from hashlib import pbkdf2_hmac, scrypt def hash_files(files): file_hashes = {} for filename, content in files: file_hashes[filename] = { \'sha256\': hashlib.sha256(content).hexdigest(), \'blake2b\': hashlib.blake2b(content).hexdigest(), \'sha3_256\': hashlib.sha3_256(content).hexdigest() } return file_hashes def verify_file(filename, content, hash_dict): if filename not in hash_dict: return False computed_hashes = { \'sha256\': hashlib.sha256(content).hexdigest(), \'blake2b\': hashlib.blake2b(content).hexdigest(), \'sha3_256\': hashlib.sha3_256(content).hexdigest() } for key in computed_hashes: if not compare_digest(computed_hashes[key], hash_dict[filename][key]): return False return True def hash_password(password): salt = b\'secure_salt\' # In a real use case, this should be randomly generated password_bytes = password.encode(\'utf-8\') password_hashes = { \'pbkdf2\': pbkdf2_hmac(\'sha256\', password_bytes, salt, 100000).hex(), \'scrypt\': scrypt(password_bytes, salt=salt, n=16384, r=8, p=1).hex() } return password_hashes def verify_password(password, hash_dict): salt = b\'secure_salt\' # Use the same salt as used during hash_password password_bytes = password.encode(\'utf-8\') computed_hashes = { \'pbkdf2\': pbkdf2_hmac(\'sha256\', password_bytes, salt, 100000).hex(), \'scrypt\': scrypt(password_bytes, salt=salt, n=16384, r=8, p=1).hex() } for key in computed_hashes: if not compare_digest(computed_hashes[key], hash_dict[key]): return False return True"},{"question":"**Objective**: Transition a given piece of code from using the deprecated `imp` module to the modern `importlib` module. **Challenge**: You are given a function that uses the deprecated `imp` module to load a specified module. Your task is to rewrite this function using the `importlib` module while maintaining equivalent functionality. Function Signature ```python def load_custom_module(module_name: str, module_path: str) -> object: Loads a module given its name and path. Parameters: - module_name (str): The name of the module. - module_path (str): The full path to the module file. Returns: - object: The loaded module object ``` Input - `module_name` (str): The name of the module. - `module_path` (str): The path to the module\'s `.py` file. For instance, it could be \\"/User/example/my_module.py\\". Output - An object representing the loaded module. Constraints - Ensure that the function throws appropriate errors if the module cannot be found or loaded. - Ensure that the loaded module is equivalent whether loaded through `imp` or `importlib`. Performance Requirements - The function should handle reasonable file reading times as dictated by common module imports. Example ```python # Example usage of load_custom_module module = load_custom_module(\\"my_module\\", \\"/User/example/my_module.py\\") print(module) ``` Deprecated Code Below is a deprecated implementation using `imp`. Your task is to rewrite this using `importlib`. ```python import imp def load_custom_module(module_name, module_path): fp, pathname, description = imp.find_module(module_name, [module_path]) try: return imp.load_module(module_name, fp, pathname, description) finally: if fp: fp.close() ``` # Requirements: 1. Implement the `load_custom_module()` function using `importlib` instead of `imp`. 2. Ensure that the new implementation handles errors gracefully and performs efficiently.","solution":"import importlib.util import sys def load_custom_module(module_name: str, module_path: str) -> object: Loads a module given its name and path using importlib. Parameters: - module_name (str): The name of the module. - module_path (str): The full path to the module file. Returns: - object: The loaded module object spec = importlib.util.spec_from_file_location(module_name, module_path) if spec is None: raise ImportError(f\\"Cannot find the module \'{module_name}\' at \'{module_path}\'\\") module = importlib.util.module_from_spec(spec) try: spec.loader.exec_module(module) except FileNotFoundError as e: raise ImportError(f\\"Could not load the module \'{module_name}\' from \'{module_path}\'. File not found.\\") from e except Exception as e: raise ImportError(f\\"Could not load the module \'{module_name}\' from \'{module_path}\'. An error occurred.\\") from e sys.modules[module_name] = module return module"},{"question":"# Python Development Mode and Resource Management You are tasked with designing a script that safely handles file operations in Python, especially paying attention to proper resource management to avoid common pitfalls. The script will count the number of words in a text file and print the result. However, the script provided to you has several issues that need addressing. Your task is to: 1. **Rewrite the script using best practices for file handling.** 2. **Ensure that the script will run without any issues when Python Development Mode is enabled.** Here is the original problematic script provided: ```python import sys import os def count_words(file_path): fp = open(file_path) data = fp.read() words = data.split() fp.close() return len(words) def main(): if len(sys.argv) < 2: print(\\"Usage: python script.py <file_path>\\") sys.exit(1) file_path = sys.argv[1] print(f\\"Number of words: {count_words(file_path)}\\") if __name__ == \\"__main__\\": main() ``` # Requirements 1. **Refactor the `count_words` function to ensure that any exceptions are safely handled and resources are properly closed.** 2. **The script must not leave any files unclosed or cause any \\"Bad file descriptor\\" errors.** 3. **Ensure that the script runs cleanly with no warnings or errors when executed with Python Development Mode enabled using `-X dev` option.** 4. **Use any additional features or practices recommended by Python Development Mode, such as context managers for file operations.** **Input:** The command line argument specifies the path to the text file (e.g., `python script.py somefile.txt`). **Output:** The script should output the number of words in the provided text file. # Constraints 1. The script should handle potential errors gracefully, such as file not found or file read errors. 2. Use modern Python features and good coding practices. # Performance Requirements 1. The script should efficiently handle large files, avoiding loading the entire content into memory at once, if possible. # Example ```shell cat test.txt Hello world Hello again python3 script.py test.txt Number of words: 4 python3 -X dev script.py test.txt Number of words: 4 ```","solution":"import sys def count_words(file_path): try: with open(file_path, \'r\') as file: data = file.read() words = data.split() return len(words) except FileNotFoundError: print(f\\"Error: The file \'{file_path}\' does not exist.\\") sys.exit(1) except Exception as e: print(f\\"An error occurred: {e}\\") sys.exit(1) def main(): if len(sys.argv) < 2: print(\\"Usage: python script.py <file_path>\\") sys.exit(1) file_path = sys.argv[1] print(f\\"Number of words: {count_words(file_path)}\\") if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of the Seaborn package\'s object-oriented interface by creating a complex plot. You will work with given datasets to generate a visualization that effectively communicates data insights. # Problem Statement Using the `seaborn` package, create a plot that visualizes the relationship between different variables in the **diamonds** dataset. Your task is to follow these specific requirements: 1. Load the `diamonds` dataset using the `seaborn.load_dataset` function. 2. Create a scatter plot that shows the relationship between the `carat` and `price` of the diamonds. 3. Use jitter to avoid overlap of the points in the scatter plot. 4. Overlay a range plot that shows the interquartile range (25th to 75th percentile) of the `price` for each `carat` value. 5. Apply a vertical shift to the range marks to improve clarity. # Constraints - **Input**: None. The data should be loaded directly in your solution. - **Output**: The function should produce and show the plot inline (use `plt.show()` if needed). - Use only the `seaborn`, `pandas`, and `matplotlib` libraries. # Example Here is an example of the expected steps your solution should follow in code form: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_diamond_plot(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create a scatter plot with jitter and an overlaid range plot ( so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=1000)) # Adjust the shift value as needed ) # Show the plot plt.show() # Calling the function create_diamond_plot() ``` Note: The values used in `.Shift(y=1000)` are arbitrary â€” you should adjust them as necessary to achieve clarity in your plot. # Evaluation Criteria Your solution will be evaluated on: - Correct loading and use of the dataset. - Appropriate application of jitter to scatter plot points. - Correct implementation of the range overlay showing interquartile ranges. - Proper usage of shift transformation to avoid overlapping of layers in the plot. - Overall clarity and visual appeal of the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_diamond_plot(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create a scatter plot with jitter and an overlaid range plot plot = ( so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=1000)) # Adjust the shift value as needed ) # Show the plot plot.show() # Calling the function create_diamond_plot()"},{"question":"Coding Assessment Question # Objective To assess your understanding of semi-supervised learning using the `scikit-learn` library, specifically focusing on implementing, tuning, and evaluating a semi-supervised learning model. # Question You are provided with a dataset containing labeled and unlabeled samples. Your task is to implement a semi-supervised learning pipeline using the `SelfTrainingClassifier`, and evaluate its performance. # Dataset The dataset consists of two parts: 1. **Labeled Data**: A subset of the dataset where each sample has a corresponding label. 2. **Unlabeled Data**: The rest of the dataset where labels are not provided. For simplicity, we will generate a synthetic classification dataset using `sklearn.datasets.make_classification`. # Steps to Follow 1. **Data Preparation:** - Generate a synthetic dataset using `make_classification` with 1000 samples and 20 features. - Split the dataset into labeled and unlabeled parts. Assume 10% of the data is labeled. 2. **Model Implementation:** - Implement a semi-supervised learning model using `SelfTrainingClassifier` with a `LogisticRegression` estimator. - Use a probability threshold of 0.8 for adding predicted labels. 3. **Training and Evaluation:** - Train the semi-supervised model on the combined labeled and unlabeled data. - Evaluate the model\'s performance on a separate test set using accuracy, precision, recall, and F1-score. # Requirements - **Input Format:** - **`train_data`**: A numpy array of shape (1000, 21) where the last column represents labels. Unlabeled data should have labels marked as -1. - **`test_data`**: A numpy array of shape (200, 20) representing the test samples without labels. - **`test_labels`**: A numpy array of shape (200,) representing correct labels for the `test_data`. - **Output Format:** - A dictionary with the following keys: - `\'accuracy\'` - `\'precision\'` - `\'recall\'` - `\'f1_score\'` # Constraints - Ensure that the model training is efficient. Use cross-validation if necessary. - The solution should be robust and handle different random states for data generation. # Performance Requirements - Accuracy should be > 0.7 - Precision, Recall, and F1-score should be reasonably balanced, without any metric dropping significantly below others. ```python from sklearn.datasets import make_classification import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.model_selection import train_test_split def semi_supervised_learning(train_data, test_data, test_labels): # Input validation assert train_data.shape[1] == 21, \\"train_data should have 21 columns: 20 features and 1 label column\\" assert test_data.shape[1] == 20, \\"test_data should have 20 features\\" assert test_labels.shape[0] == test_data.shape[0], \\"test_labels should match the length of test_data\\" # Split training data into features and labels X_train = train_data[:, :-1] y_train = train_data[:, -1] # Initialize SelfTrainingClassifier with LogisticRegression estimator base_estimator = LogisticRegression(solver=\'liblinear\') self_training_clf = SelfTrainingClassifier(base_estimator, threshold=0.8) # Train the model self_training_clf.fit(X_train, y_train) # Predict on the test data test_predictions = self_training_clf.predict(test_data) # Calculate performance metrics accuracy = accuracy_score(test_labels, test_predictions) precision = precision_score(test_labels, test_predictions, average=\'macro\') recall = recall_score(test_labels, test_predictions, average=\'macro\') f1 = f1_score(test_labels, test_predictions, average=\'macro\') return { \'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1_score\': f1 } # Example Usage (You don\'t need to implement this part, it\'s just for your reference) if __name__ == \\"__main__\\": # Generate synthetic dataset X, y = make_classification(n_samples=1200, n_features=20, n_informative=15, n_clusters_per_class=3, random_state=42) X_train_initial, X_test, y_train_initial, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Mark 90% of y_train_initial as unlabeled (-1) n_labeled = int(0.1 * len(y_train_initial)) y_train_initial[n_labeled:] = -1 # Combine X and y to form train data with shape (1000, 21) train_data = np.hstack((X_train_initial, y_train_initial.reshape(-1, 1))) # Call the function with prepared train_data, test_data, and test_labels results = semi_supervised_learning(train_data, X_test, y_test) print(results) ``` # Hints - Refer to the `sklearn` documentation for details on `SelfTrainingClassifier` and `LogisticRegression`. - Ensure `predict_proba` is supported by your base estimator. - Properly handle and initialize random states for reproducibility.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.semi_supervised import SelfTrainingClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def semi_supervised_learning(train_data, test_data, test_labels): # Split training data into features and labels X_train = train_data[:, :-1] y_train = train_data[:, -1] # Initialize SelfTrainingClassifier with LogisticRegression estimator base_estimator = LogisticRegression(solver=\'liblinear\') self_training_clf = SelfTrainingClassifier(base_estimator, threshold=0.8) # Train the model self_training_clf.fit(X_train, y_train) # Predict on the test data test_predictions = self_training_clf.predict(test_data) # Calculate performance metrics accuracy = accuracy_score(test_labels, test_predictions) precision = precision_score(test_labels, test_predictions, average=\'macro\') recall = recall_score(test_labels, test_predictions, average=\'macro\') f1 = f1_score(test_labels, test_predictions, average=\'macro\') return { \'accuracy\': accuracy, \'precision\': precision, \'recall\': recall, \'f1_score\': f1 }"},{"question":"# URL Data Extraction and Validation Task Problem Statement You are required to write a Python function `extract_and_validate_url_data(url: str) -> dict` that performs the following tasks: 1. **Open and Read the URL**: Use `urllib.request` to open and read the content from the given URL. 2. **Count Words**: Calculate the number of words in the page content. 3. **Parse URL**: Use `urllib.parse` to break down the URL into its components (scheme, netloc, path, params, query, fragment). 4. **Validate URL Access**: Use `urllib.robotparser` to check if the specified URL can be accessed by web crawlers. Input - `url (str)`: A string representing the URL to be processed. Output - Returns a dictionary with the following keys and corresponding values: * `\\"word_count\\"`: An integer representing the number of words in the page content. * `\\"parsed_url\\"`: A dictionary containing the components of the URL (keys: `scheme`, `netloc`, `path`, `params`, `query`, `fragment`). * `\\"can_fetch\\"`: A boolean indicating whether the URL can be accessed by web crawlers according to `robots.txt`. Constraints - Your function should handle potential exceptions, such as network errors or invalid URLs. - Assume that the given URL properly responds and is accessible. - Performance should be acceptable for standard web pages (i.e., processing time should not exceed 5 seconds). Example Usage ```python url = \\"http://www.example.com/path/page?query=param#fragment\\" result = extract_and_validate_url_data(url) print(result) # Expected output (example): # { # \\"word_count\\": 120, # \\"parsed_url\\": { # \\"scheme\\": \\"http\\", # \\"netloc\\": \\"www.example.com\\", # \\"path\\": \\"/path/page\\", # \\"params\\": \\"\\", # \\"query\\": \\"query=param\\", # \\"fragment\\": \\"fragment\\" # }, # \\"can_fetch\\": True # } ``` Notes - Ensure you install any necessary Python packages if not already available. - Make use of the `urllib` package exclusively to accomplish the tasks above. Good luck!","solution":"import urllib.request from urllib.parse import urlparse import urllib.robotparser import re def extract_and_validate_url_data(url: str) -> dict: result = { \\"word_count\\": 0, \\"parsed_url\\": {}, \\"can_fetch\\": False } try: # Open and read the URL content with urllib.request.urlopen(url) as response: page_content = response.read().decode(\'utf-8\') # Count words in the page content words = re.findall(r\'bw+b\', page_content) result[\\"word_count\\"] = len(words) # Parse the URL into its components parsed_url = urlparse(url) result[\\"parsed_url\\"] = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } # Validate URL access via robots.txt robot_parser = urllib.robotparser.RobotFileParser() robot_parser.set_url(f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\") robot_parser.read() result[\\"can_fetch\\"] = robot_parser.can_fetch(\\"*\\", url) except Exception as e: # Handle exceptions (e.g., network errors, invalid URLs) print(\\"An error occurred:\\", e) return result"},{"question":"# PyTorch JIT Compilation and Model Optimization **Objective**: The goal of this exercise is to assess your understanding of PyTorch\'s Just-In-Time (JIT) compilation capabilities. You will be required to demonstrate knowledge of JIT scripting, tracing, and saving/loading a model using the `torch.utils.jit` module. **Problem Statement**: You are given a simple PyTorch neural network model. Your task is to: 1. Implement a function to convert this model to a JIT scripted model. 2. Implement a function to convert the model to a JIT traced model. 3. Save the JIT scripted model to a file. 4. Load the JIT scripted model from the file and ensure it can make predictions. # Instructions: 1. **JIT Scripting Function**: - Write a function `jit_script_model(model: torch.nn.Module) -> torch.jit.ScriptModule` that takes a PyTorch model as input and returns the JIT scripted version of the model. 2. **JIT Tracing Function**: - Write a function `jit_trace_model(model: torch.nn.Module, example_input: torch.Tensor) -> torch.jit.TraceModule` that takes a PyTorch model and an example input tensor and returns the JIT traced version of the model. 3. **Save JIT Model**: - Write a function `save_jit_model(jit_model: torch.jit.ScriptModule, file_path: str) -> None` that saves the JIT scripted model to the specified file path. 4. **Load JIT Model and Predict**: - Write a function `load_jit_model_and_predict(file_path: str, input_tensor: torch.Tensor) -> torch.Tensor` that loads a JIT model from the specified file path and uses it to make a prediction on the input tensor. # Example Code ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Example use: model = SimpleModel() example_input = torch.randn(1, 10) scripted_model = jit_script_model(model) traced_model = jit_trace_model(model, example_input) save_jit_model(scripted_model, \'scripted_model.pt\') loaded_model = load_jit_model_and_predict(\'scripted_model.pt\', example_input) print(loaded_model) ``` # Constraints: - Assume the input tensors will be of appropriate dimensions and types. - Use PyTorch\'s JIT utilities to perform the scripting, tracing, saving, and loading operations. # Notes: - You are expected to demonstrate understanding of transforming, saving, and loading models using PyTorch JIT utilities. - The primary focus is on correct use of `torch.jit` functionalities. Good luck!","solution":"import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) def jit_script_model(model: torch.nn.Module) -> torch.jit.ScriptModule: Convert model to a JIT scripted model. return torch.jit.script(model) def jit_trace_model(model: torch.nn.Module, example_input: torch.Tensor) -> torch.jit.ScriptModule: Convert model to a JIT traced model. return torch.jit.trace(model, example_input) def save_jit_model(jit_model: torch.jit.ScriptModule, file_path: str) -> None: Save JIT scripted model to a file. jit_model.save(file_path) def load_jit_model_and_predict(file_path: str, input_tensor: torch.Tensor) -> torch.Tensor: Load JIT scripted model from file and make a prediction. loaded_model = torch.jit.load(file_path) with torch.no_grad(): return loaded_model(input_tensor)"},{"question":"**Coding Assessment Question:** # Multiclass Dataset Generation and Visualization Problem Statement You are tasked with generating and visualizing synthetic datasets using `scikit-learn`\'s sample generators. Your goal is to create three different datasets with specific properties, plot them, and then preprocess them for a machine learning classification task. Datasets 1. **Normal Clusters:** - Generate a dataset using `make_blobs` with 4 clusters centered at random locations. - Use a standard deviation of 1.0 for each cluster. - Standardize the dataset. 2. **Correlated Features:** - Create a dataset using `make_classification` with the following properties: - 3 classes. - 5 informative features. - 2 redundant features. - 2 clusters per class. - Perform Principal Component Analysis (PCA) to reduce the dataset to 2 dimensions for visualization. 3. **Non-linear Boundary:** - Generate a dataset using `make_moons` with added Gaussian noise (standard deviation of 0.2). - Apply a polynomial transformation to the dataset. Tasks 1. **Dataset Generation:** Implement the functions to generate the datasets as described above. 2. **Visualization:** Create scatter plots for all three datasets. Each plot must differentiate classes by color. 3. **Preprocessing:** - For the Normal Clusters dataset, standardize the features. - For the Correlated Features dataset, apply PCA and plot the first 2 principal components. - For the Non-linear Boundary dataset, apply a polynomial transformation of degree 3. # Expected Input and Output Formats Input The input to your functions is implicit through the required parameters described above. Output - Three scatter plots, each for the respective datasets generated. - Print the first 5 entries of the transformed features for each dataset after preprocessing. Example ```python import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons from sklearn.preprocessing import StandardScaler, PolynomialFeatures from sklearn.decomposition import PCA # Function to generate and preprocess datasets def generate_and_preprocess_datasets(): # Normal Clusters X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42) scaler = StandardScaler() X_blobs = scaler.fit_transform(X_blobs) # Visualize Normal Clusters plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"Normal Clusters\\") plt.show() print(\\"First 5 entries of Normal Clusters after standardization:n\\", X_blobs[:5]) # Correlated Features X_class, y_class = make_classification(n_samples=300, n_classes=3, n_informative=5, n_redundant=2, n_clusters_per_class=2, random_state=42) pca = PCA(n_components=2) X_class_pca = pca.fit_transform(X_class) # Visualize Correlated Features plt.scatter(X_class_pca[:, 0], X_class_pca[:, 1], c=y_class) plt.title(\\"Correlated Features with PCA\\") plt.show() print(\\"First 5 entries of Correlated Features after PCA:n\\", X_class_pca[:5]) # Non-linear Boundary X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42) poly = PolynomialFeatures(degree=3) X_moons_poly = poly.fit_transform(X_moons) # Visualize Non-linear Boundary (original space) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"Non-linear Boundary (make_moons)\\") plt.show() print(\\"First 5 entries of Non-linear Boundary after polynomial transformation:n\\", X_moons_poly[:5]) generate_and_preprocess_datasets() ``` Constraints and Requirements - Use `scikit-learn` for dataset generation, preprocessing, and visualization. - The datasets should be clearly visualized with distinct colors for different classes. - The preprocessing steps should be correctly implemented and verified by printing out sample entries. This question will assess your ability to work with `scikit-learn`\'s dataset generators, preprocess the data, and visualize the results effectively.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons from sklearn.preprocessing import StandardScaler, PolynomialFeatures from sklearn.decomposition import PCA def generate_and_preprocess_datasets(): # Normal Clusters X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42) scaler = StandardScaler() X_blobs = scaler.fit_transform(X_blobs) # Visualize Normal Clusters plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs) plt.title(\\"Normal Clusters\\") plt.show() print(\\"First 5 entries of Normal Clusters after standardization:n\\", X_blobs[:5]) # Correlated Features X_class, y_class = make_classification(n_samples=300, n_classes=3, n_informative=5, n_redundant=2, n_clusters_per_class=2, random_state=42) pca = PCA(n_components=2) X_class_pca = pca.fit_transform(X_class) # Visualize Correlated Features plt.scatter(X_class_pca[:, 0], X_class_pca[:, 1], c=y_class) plt.title(\\"Correlated Features with PCA\\") plt.show() print(\\"First 5 entries of Correlated Features after PCA:n\\", X_class_pca[:5]) # Non-linear Boundary X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42) poly = PolynomialFeatures(degree=3) X_moons_poly = poly.fit_transform(X_moons) # Visualize Non-linear Boundary (original space) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons) plt.title(\\"Non-linear Boundary (make_moons)\\") plt.show() print(\\"First 5 entries of Non-linear Boundary after polynomial transformation:n\\", X_moons_poly[:5]) generate_and_preprocess_datasets()"},{"question":"# Question Given a dataset of car attributes (`mpg`, `origin`, `cylinders`, `horsepower`), you are required to use seaborn to visualize the relationship between the car origins and their miles per gallon (MPG). You must group the data by the number of cylinders, apply different colors for each group, and include error bars representing the standard deviation. Dataset ```python cars = sns.load_dataset(\\"mpg\\") ``` Instructions: 1. Load the dataset described above. 2. Create a point plot that shows the `mpg` for each `origin` grouping by the number of cylinders (`cylinders`). 3. Differentiate the groups by `cylinders` using colors. 4. Represent the error bars using the standard deviation of each distribution. 5. Add appropriate labels and titles to the plot for clarity. Expected Function Signature: ```python def plot_car_mpg(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset cars = sns.load_dataset(\\"mpg\\") # Create the point plot as instructed sns.pointplot( data=cars, x=\\"origin\\", y=\\"mpg\\", hue=\\"cylinders\\", errorbar=\\"sd\\" ) # Add plot title and labels plt.title(\\"MPG by Origin grouped by Cylinders\\") plt.xlabel(\\"Origin\\") plt.ylabel(\\"Miles Per Gallon (MPG)\\") # Display the plot plt.show() ``` Constraints - Assume that all necessary packages are already installed. - Ensure the plot is visually clear and well-labeled. - Use standard deviation for the error bars.","solution":"def plot_car_mpg(): import seaborn as sns import matplotlib.pyplot as plt # Load the dataset cars = sns.load_dataset(\\"mpg\\") # Create the point plot as instructed sns.pointplot( data=cars, x=\\"origin\\", y=\\"mpg\\", hue=\\"cylinders\\", errorbar=(\\"sd\\", 1.96) # Use standard deviation for error bars ) # Add plot title and labels plt.title(\\"MPG by Origin grouped by Cylinders\\") plt.xlabel(\\"Origin\\") plt.ylabel(\\"Miles Per Gallon (MPG)\\") # Display the plot plt.show()"},{"question":"**Objective:** Your task is to utilize seaborn\'s objects interface to create a composite plot that meets specific requirements, demonstrating your understanding of seaborn\'s advanced visualization capabilities. **Datasets:** - `dowjones`: Represents the Dow Jones index data containing columns `Date` and `Price`. - `fmri`: Represents functional MRI data with columns `timepoint`, `signal`, `region`, `event`, and `subject`. **Task:** 1. Load the `dowjones` and `fmri` datasets using seaborn\'s `load_dataset` function. 2. Create a line plot for the `dowjones` dataset: - Plot the `Price` as a function of `Date`. - Change the plot orientation to connect observations along the y-axis. 3. Create a plot for the `fmri` dataset: - Filter the dataset to only include rows where `region` is `\'parietal\'` and `event` is `\'stim\'`. - Plot the `signal` data as a function of `timepoint` for each `subject`. - Group the data by `subject` and use a single color and linewidth for each line. 4. Enhance the `fmri` plot: - Map `region` to the color property and `event` to the linestyle property. - Incorporate statistical aggregation (mean) and error bands to show the spread of the data. - Add markers to indicate sampled data points clearly. **Expected Input and Output:** - **Input:** None (datasets are loaded directly from seaborn). - **Output:** Two separate plots: 1. A line plot of the `dowjones` dataset with `Price` on the x-axis and `Date` on the y-axis. 2. A detailed plot of the `fmri` dataset showcasing the signal data grouped by `subject` with additional enhancements, including color, linestyle mapping, statistical aggregations, error bands, and markers. **Constraints:** - You should use seaborn\'s objects interface exclusively. - Ensure that the plots are clear and well-labeled. - The entire code should execute successfully in one run. ```python # Your code starts here import seaborn.objects as so from seaborn import load_dataset # Load the datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # 1. Create a line plot for the dowjones dataset with changed orientation dowjones_plot = so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line(), orient=\\"y\\") # Display the dowjones plot dowjones_plot.show() # 2. Create a plot for the fmri dataset with the specified enhancements fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") fmri_plot = ( so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) ) # 3. Enhance the fmri plot further with color and linestyle mappings, and statistical aggregation fmri_enhanced_plot = ( so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line(), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") ) # Display the fmri plots fmri_plot.show() fmri_enhanced_plot.show() # Your code ends here ``` **Performance Requirements:** - The code should be optimized to run efficiently with the given datasets. - Ensure that the file handles and displays large data smoothly without significant performance degradation.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_dowjones_plot(): # Load the dowjones dataset dowjones = load_dataset(\\"dowjones\\") # Create a line plot for the dowjones dataset with changed orientation dowjones_plot = so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line(), orient=\\"y\\") return dowjones_plot def create_fmri_plot(): # Load the fmri dataset fmri = load_dataset(\\"fmri\\") # Filter the dataset fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") # Create the initial plot for the filtered fmri dataset fmri_plot = ( so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") ) # Enhance the plot with the specified parameters fmri_enhanced_plot = ( so.Plot(fmri, \\"timepoint\\", \\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Band(), so.Est(), group=\\"event\\") ) return fmri_plot, fmri_enhanced_plot"},{"question":"# Question You are tasked with implementing a Python utility using the `webbrowser` module that performs the following: 1. Registers a custom browser with a specific command. 2. Opens a given URL in a new browser window or tab, based on user preference. 3. Returns a status message indicating whether the URL was opened successfully or if an error occurred. You should implement the following functions: 1. `register_custom_browser(name: str, command: str) -> None` - **Input**: - `name` (str): The name to register the custom browser with. - `command` (str): The command to run the custom browser. - **Output**: None - **Description**: Registers a new browser type in the webbrowser module with the specified name and command. 2. `open_url(url: str, new_window: bool = False) -> str` - **Input**: - `url` (str): The URL to open. - `new_window` (bool): A flag indicating whether to open the URL in a new window (default is False, which indicates opening in a new tab). - **Output**: A message (str) indicating the status (\\"URL opened successfully\\" or \\"Error opening URL\\"). - **Description**: Opens the specified URL using the default web browser. If `new_window` is True, opens the URL in a new window; otherwise, opens it in a new tab. # Constraints - Assume the URL is always a valid string starting with \\"http://\\" or \\"https://\\". - The custom browser command line should correctly follow the format specified in the documentation. - Your function should handle exceptions raised by the `webbrowser` module and return an appropriate error message. # Requirements - You must use the `webbrowser` module to register the custom browser and open URLs. - The functionality should be compatible with at least one non-default browser type. Below is the structure of the implementation you need to follow: ```python import webbrowser def register_custom_browser(name: str, command: str) -> None: Registers a custom browser with the specified command. pass def open_url(url: str, new_window: bool = False) -> str: Opens the URL in a new browser window or tab, and returns a status message. pass ``` **Example Usage:** ```python register_custom_browser(\'mybrowser\', \'/path/to/mybrowser %s\') print(open_url(\'https://www.python.org\', new_window=True)) # Output: \\"URL opened successfully\\" or \\"Error opening URL\\" ``` Implement the `register_custom_browser` and `open_url` functions.","solution":"import webbrowser def register_custom_browser(name: str, command: str) -> None: Registers a custom browser with the specified command. webbrowser.register(name, None, webbrowser.BackgroundBrowser(command)) def open_url(url: str, new_window: bool = False) -> str: Opens the URL in a new browser window or tab, and returns a status message. try: if new_window: webbrowser.open_new(url) else: webbrowser.open_new_tab(url) return \\"URL opened successfully\\" except Exception as e: return f\\"Error opening URL: {e}\\""},{"question":"# Question: Implementing and Evaluating a Custom Kernel SVM for Multi-class Classification Objective: Demonstrate your understanding of implementing and using Support Vector Machines (SVMs) with custom kernels in scikit-learn for multi-class classification. Task: 1. Implement a custom kernel function for an SVM classifier using scikit-learn. 2. Create a multi-class dataset and split it into training and testing sets. 3. Train the SVM classifier using the custom kernel. 4. Evaluate the performance of the classifier on the testing set using appropriate metrics. Requirements: - The custom kernel function should implement a custom polynomial kernel. - The dataset should have at least 4 classes and 100 samples per class. - Use `train_test_split` from `sklearn.model_selection` to create training and testing sets. - Use `classification_report` from `sklearn.metrics` to evaluate and print the performance metrics. Custom Kernel Function: A polynomial kernel function can be defined as: [ K(x, y) = (gamma cdot langle x, y rangle + text{coef0})^d ] Where: - (gamma) is the kernel coefficient. - (text{coef0}) is a constant term. - (d) is the degree of the polynomial. Sample Code Structure: ```python import numpy as np from sklearn import svm from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report # Step 1: Define the custom polynomial kernel function def custom_polynomial_kernel(X, Y, gamma=1.0, coef0=0, degree=3): return (gamma * np.dot(X, Y.T) + coef0) ** degree # Step 2: Generate a multi-class dataset X, y = make_classification(n_samples=400, n_features=20, n_informative=15, n_classes=4, random_state=42) # Step 3: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 4: Train the SVM classifier using the custom kernel clf = svm.SVC(kernel=lambda X, Y: custom_polynomial_kernel(X, Y, gamma=1.0, coef0=1, degree=3)) clf.fit(X_train, y_train) # Step 5: Make predictions and evaluate the classifier y_pred = clf.predict(X_test) print(classification_report(y_test, y_pred)) ``` Constraints: - Ensure that your code is well-documented. - Use appropriate parameter values for the custom kernel function. - The performance evaluation should reflect accuracy, precision, recall, and F1-score for each class. Submission: Submit a Jupyter Notebook or a Python script with your implementation and results.","solution":"import numpy as np from sklearn import svm from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report # Step 1: Define the custom polynomial kernel function def custom_polynomial_kernel(X, Y, gamma=1.0, coef0=0, degree=3): return (gamma * np.dot(X, Y.T) + coef0) ** degree # Step 2: Generate a multi-class dataset X, y = make_classification(n_samples=400, n_features=20, n_informative=15, n_classes=4, random_state=42) # Step 3: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 4: Train the SVM classifier using the custom kernel clf = svm.SVC(kernel=lambda X, Y: custom_polynomial_kernel(X, Y, gamma=1.0, coef0=1, degree=3)) clf.fit(X_train, y_train) # Step 5: Make predictions and evaluate the classifier y_pred = clf.predict(X_test) performance_report = classification_report(y_test, y_pred) print(performance_report)"},{"question":"**Python Coding Assessment Question** You are tasked with creating a simple graphical user interface (GUI) program using the `tkinter` library and its `messagebox` module. The program should be a basic user interface that: 1. Displays a button labeled \\"Generate Report\\". 2. When the button is clicked, it checks if a specific file (e.g., \\"data.txt\\") exists in the current directory. 3. If the file does not exist, it should display a warning message box to the user indicating that the file is missing. 4. If the file exists, it should display an information message box confirming that the report generation was successful. The program should be designed with the following function: ```python import tkinter as tk import tkinter.messagebox import os def generate_report(): This function checks for the existence of \'data.txt\' file and displays the appropriate message box. - If \'data.txt\' does not exist, display a warning message box with the title \\"Warning\\" and the message \\"data.txt file is missing!\\". - If \'data.txt\' exists, display an information message box with the title \\"Success\\" and the message \\"Report generated successfully!\\". pass # Your code here if __name__ == \\"__main__\\": root = tk.Tk() root.title(\\"Report Generator\\") generate_button = tk.Button(root, text=\\"Generate Report\\", command=generate_report) generate_button.pack(pady=20) root.mainloop() ``` # Constraints - You must use the `tkinter` library and its `messagebox` module. - The button in the user interface should trigger the `generate_report` function. - The `generate_report` function should check for the presence of the \\"data.txt\\" file and display the appropriate message box based on its presence or absence. # Input Format - There is no direct input; interaction is through the GUI components. # Output Format - The output is displayed through the message boxes based on the file check condition. # Example If the \\"data.txt\\" file does not exist in the current directory, the program will display: - Warning Message Box: - Title: \\"Warning\\" - Message: \\"data.txt file is missing!\\" If the \\"data.txt\\" file exists, the program will display: - Information Message Box: - Title: \\"Success\\" - Message: \\"Report generated successfully!\\" Implement the `generate_report` function such that it handles the specified behaviors.","solution":"import tkinter as tk import tkinter.messagebox import os def generate_report(): This function checks for the existence of \'data.txt\' file and displays the appropriate message box. - If \'data.txt\' does not exist, display a warning message box with the title \\"Warning\\" and the message \\"data.txt file is missing!\\". - If \'data.txt\' exists, display an information message box with the title \\"Success\\" and the message \\"Report generated successfully!\\". if os.path.isfile(\\"data.txt\\"): tk.messagebox.showinfo(\\"Success\\", \\"Report generated successfully!\\") else: tk.messagebox.showwarning(\\"Warning\\", \\"data.txt file is missing!\\") if __name__ == \\"__main__\\": root = tk.Tk() root.title(\\"Report Generator\\") generate_button = tk.Button(root, text=\\"Generate Report\\", command=generate_report) generate_button.pack(pady=20) root.mainloop()"},{"question":"**Objective**: Demonstrate your understanding of the `wave` module in Python by performing operations on WAV files. **Problem Statement**: You are given a task to analyze an audio file in WAV format, perform some basic transformations, and save the transformed audio data into a new WAV file. **Instructions**: 1. Write a function `analyze_and_transform_wave(input_file: str, output_file: str) -> dict` that reads an input WAV file, performs basic analysis, and writes a modified version of the audio to an output file. 2. The function should: - Read the WAV file specified by `input_file`. - Retrieve and print the following properties of the WAV file: - Number of channels - Sample width (in bytes) - Frame rate (in Hz) - Number of frames - Reverse the audio frames. - Write the reversed audio frames into a new WAV file specified by `output_file` with the same properties as the input file. 3. The function should return a dictionary containing: - \'channels\': The number of channels. - \'sample_width\': The sample width (in bytes). - \'frame_rate\': The frame rate (in Hz). - \'num_frames\': The number of frames. **Constraints**: - You can assume the input WAV file is always a valid PCM WAV file. - The output WAV file should have the same properties (channels, sample width, frame rate) as the input WAV file. - The function should handle file input/output errors gracefully. **Example**: ```python input_file = \'input.wav\' output_file = \'output.wav\' result = analyze_and_transform_wave(input_file, output_file) print(result) # Output: # { # \'channels\': 2, # \'sample_width\': 2, # \'frame_rate\': 44100, # \'num_frames\': 88200 # } ``` **Notes**: - Ensure you use the `wave` module for reading and writing WAV files. - The function should manage file resources properly, ensuring files are closed after operations are complete. - Test the function with different WAV files to ensure it works correctly.","solution":"import wave import contextlib def analyze_and_transform_wave(input_file: str, output_file: str) -> dict: # Read the input WAV file with wave.open(input_file, \'rb\') as input_wav: # Extract properties of the WAV file channels = input_wav.getnchannels() sample_width = input_wav.getsampwidth() frame_rate = input_wav.getframerate() num_frames = input_wav.getnframes() # Read frames and reverse them audio_frames = input_wav.readframes(num_frames) reversed_audio_frames = audio_frames[::-1] # Write the reversed audio frames to the output WAV file with wave.open(output_file, \'wb\') as output_wav: output_wav.setnchannels(channels) output_wav.setsampwidth(sample_width) output_wav.setframerate(frame_rate) output_wav.writeframes(reversed_audio_frames) # Return the properties as a dictionary return { \'channels\': channels, \'sample_width\': sample_width, \'frame_rate\': frame_rate, \'num_frames\': num_frames }"},{"question":"Objective To evaluate your understanding of kernel approximation methods in scikit-learn, you will implement the Nystroem method for kernel approximation. This method is particularly useful for scaling up kernel methods by approximating the kernel matrix using a smaller subset of data points. Problem Statement You are required to implement a function `nystroem_kernel_approximation(X, n_components, kernel, gamma=1.0)` that approximates the feature mapping for a given kernel function using the Nystroem method. Inputs: 1. `X`: `numpy array` of shape (n_samples, n_features) - The input data. 2. `n_components`: `int` - The number of components to be used for approximation. 3. `kernel`: `str` - The kernel function to be used. It can be \'rbf\' or \'linear\'. 4. `gamma`: `float`, optional (default=1.0) - Parameter for the rbf kernel. Output: 1. `Phi_X`: `numpy array` of shape (n_samples, n_components) - The transformed data. Constraints: - `n_components` should be less than or equal to `n_samples`. - You are not allowed to use scikit-learn\'s `Nystroem` class directly but you can use other basic utilities from scikit-learn if needed. - Ensure the function is optimized for performance. Instructions: 1. Compute the kernel matrix for a subset of `n_components` samples from `X`. 2. Perform an eigenvalue decomposition on the subset kernel matrix. 3. Use the components obtained from the decomposition to transform the full dataset `X`. 4. Return the transformed data. Example: ```python import numpy as np def nystroem_kernel_approximation(X, n_components, kernel, gamma=1.0): # Your implementation here # Example usage: X = np.array([[0, 0], [1, 1], [1, 0], [0, 1]]) n_components = 2 kernel = \'rbf\' gamma = 1.0 Phi_X = nystroem_kernel_approximation(X, n_components, kernel, gamma) print(Phi_X) ``` Notes: - For the RBF kernel, the kernel function is given by: [ k(x, y) = exp(-gamma |x - y|^2) ] - For the linear kernel, the kernel function is given by: [ k(x, y) = x cdot y ] - Make sure you compute the normalization constant properly and use it during the transformation step. Good luck and happy coding!","solution":"import numpy as np from sklearn.metrics.pairwise import rbf_kernel, linear_kernel def nystroem_kernel_approximation(X, n_components, kernel, gamma=1.0): if n_components > X.shape[0]: raise ValueError(\\"n_components must be less than or equal to n_samples\\") rng = np.random.default_rng() indices = rng.choice(X.shape[0], n_components, replace=False) X_subset = X[indices] if kernel == \'rbf\': K_subset = rbf_kernel(X_subset, gamma=gamma) elif kernel == \'linear\': K_subset = linear_kernel(X_subset) else: raise ValueError(\\"Unsupported kernel type. Use \'rbf\' or \'linear\'.\\") # Eigenvalue decomposition U, S, V = np.linalg.svd(K_subset) S_diag = np.diag(S) sqrt_S_inv = np.diag(1.0 / np.sqrt(S)) G = np.dot(U, sqrt_S_inv) if kernel == \'rbf\': K_full = rbf_kernel(X, X_subset, gamma=gamma) elif kernel == \'linear\': K_full = linear_kernel(X, X_subset) Phi_X = np.dot(K_full, G) return Phi_X"},{"question":"# Question: Dataset Exploration and Custom Transformation In this assessment, you will demonstrate your understanding of the `sklearn.datasets` package. Your task is to load a dataset, explore its contents, and implement a custom transformation function. Specifically, you will: 1. Load the `Iris` dataset using the `datasets` utility from `scikit-learn`. 2. Extract and display the following information: - The number of samples (`n_samples`) and the number of features (`n_features`). - The feature names of the dataset. - The first five rows of the data. - The unique classes and their respective counts in the target array. 3. Implement a custom transformation function `add_feature_noise(X, noise_level)` that adds Gaussian noise to each feature of the dataset. The function should: - Take a 2D numpy array `X` and a float `noise_level` as input. - Return a new 2D numpy array with noise added to each feature. # Input - Dataset: `iris` (loaded using `sklearn.datasets.load_iris`) - Parameters for `add_feature_noise` function: - `X`: a 2D numpy array. - `noise_level`: a float value representing the standard deviation of the Gaussian noise to be added. # Output - Boolean value indicating whether all steps were performed correctly. # Constraints - Do not use any additional machine learning algorithms or preprocessors apart from those specified in the `sklearn.datasets` package. # Performance Requirements - Ensure that the custom transformation operation is efficiently handled and does not significantly increase the runtime for larger datasets. # Example ```python import numpy as np from sklearn.datasets import load_iris def load_and_explore_iris_dataset(): # Load dataset iris = load_iris() # Extract information n_samples = iris.data.shape[0] n_features = iris.data.shape[1] feature_names = iris.feature_names first_five_rows = iris.data[:5] unique_classes, class_counts = np.unique(iris.target, return_counts=True) # Print information print(f\\"Number of samples: {n_samples}\\") print(f\\"Number of features: {n_features}\\") print(f\\"Feature names: {feature_names}\\") print(f\\"First five rows of the dataset:n{first_five_rows}\\") print(f\\"Unique classes: {unique_classes}\\") print(f\\"Class counts: {dict(zip(unique_classes, class_counts))}\\") return True def add_feature_noise(X, noise_level): noise = np.random.normal(0, noise_level, X.shape) noisy_X = X + noise return noisy_X # Execute the functions if load_and_explore_iris_dataset(): print(\\"Dataset exploration completed successfully.\\") # Generate noise-added data iris = load_iris() X = iris.data noisy_X = add_feature_noise(X, noise_level=0.1) print(\\"Noise-added data (first five rows):\\") print(noisy_X[:5]) ``` # Notes - Ensure that the printed information for the dataset exploration matches the expected format. - The `add_feature_noise` function should be tested with the `iris` dataset\'s feature data.","solution":"import numpy as np from sklearn.datasets import load_iris def load_and_explore_iris_dataset(): # Load dataset iris = load_iris() # Extract information n_samples = iris.data.shape[0] n_features = iris.data.shape[1] feature_names = iris.feature_names first_five_rows = iris.data[:5] unique_classes, class_counts = np.unique(iris.target, return_counts=True) # Output information print(f\\"Number of samples: {n_samples}\\") print(f\\"Number of features: {n_features}\\") print(f\\"Feature names: {feature_names}\\") print(f\\"First five rows of the dataset:n{first_five_rows}\\") print(f\\"Unique classes: {unique_classes}\\") print(f\\"Class counts: {dict(zip(unique_classes, class_counts))}\\") return True def add_feature_noise(X, noise_level): Add Gaussian noise to each feature in the dataset. Parameters: - X: 2D numpy array of features. - noise_level: float, standard deviation of the Gaussian noise. Returns: - noisy_X: 2D numpy array with added noise. noise = np.random.normal(0, noise_level, X.shape) noisy_X = X + noise return noisy_X"},{"question":"**Objective**: Implementation of a list-based dynamic value updater. **Question**: Write a Python function `dynamic_list_updater` that takes a list of dictionaries, a key, and an update rule. The function should use the `operator` module to perform necessary dynamic operations based on the rule. **Function Signature**: ```python def dynamic_list_updater(data: list, key: str, rule: (str, any)) -> list: pass ``` **Input**: - `data` (list): A list of dictionaries. Each dictionary represents an object with various attributes. - `key` (str): The attribute of the dictionary on which the update rule will be applied. - `rule` (tuple): A tuple where the first element is the operator as a string (e.g., \'add\', \'sub\', \'mul\', etc.), and the second element is the value to use with the operator. **Output**: - A list of dictionaries with updated values based on the provided key and rule. **Example**: ```python data = [{\'value\': 1}, {\'value\': 2}, {\'value\': 3}] key = \'value\' rule = (\'mul\', 3) updated_data = dynamic_list_updater(data, key, rule) print(updated_data) # Output should be [{\'value\': 3}, {\'value\': 6}, {\'value\': 9}] ``` **Constraints**: - The list may contain up to 10,000 dictionaries. - Each dictionary will only contain simple key-value pairs where values are numeric. **Requirements**: - Use the corresponding functions from the `operator` module to handle the dynamic operation. - Ensure the function performs efficiently and effectively handles edge cases. **Note**: Include necessary import statements and provide a brief explanation (as comments) on how `operator` functions are used to accomplish the task.","solution":"import operator def dynamic_list_updater(data: list, key: str, rule: (str, any)) -> list: Updates the values in the list of dictionaries based on a provided rule. :param data: List of dictionaries :param key: The key in the dictionaries to update :param rule: A tuple where the first element is the operator as a string (e.g., \'add\', \'sub\', \'mul\', etc.), and the second element is the value :return: Updated list of dictionaries # Dictionary mapping operator strings to actual operator functions operators = { \'add\': operator.add, \'sub\': operator.sub, \'mul\': operator.mul, \'truediv\': operator.truediv, \'floordiv\': operator.floordiv, \'mod\': operator.mod, \'pow\': operator.pow } op_func = operators[rule[0]] update_value = rule[1] for item in data: if key in item: item[key] = op_func(item[key], update_value) return data # Example usage: # data = [{\'value\': 1}, {\'value\': 2}, {\'value\': 3}] # key = \'value\' # rule = (\'mul\', 3) # updated_data = dynamic_list_updater(data, key, rule) # print(updated_data) # Output: [{\'value\': 3}, {\'value\': 6}, {\'value\': 9}]"},{"question":"# HMAC-based Authentication System You are to implement a simple HMAC-based authentication system using the `hmac` module. The system will allow users to register by specifying a username and password, and subsequently authenticate by providing the same credentials. The password will be securely hashed using HMAC for storage. Function Signatures 1. **Register User** ```python def register_user(username: str, password: str, users: dict, key: bytes, digestmod: str) -> None: Registers a new user with the given username and password. Args: - username (str): The username of the new user. - password (str): The password of the new user. - users (dict): The dictionary storing usernames and their corresponding hashed passwords. - key (bytes): The secret key used for HMAC. - digestmod (str): The name of the hash algorithm (e.g., \'sha256\'). Returns: - None Raises: - ValueError: If the username already exists. ``` 2. **Authenticate User** ```python def authenticate_user(username: str, password: str, users: dict, key: bytes, digestmod: str) -> bool: Authenticates a user with the given username and password. Args: - username (str): The username of the user trying to authenticate. - password (str): The password of the user trying to authenticate. - users (dict): The dictionary storing usernames and their corresponding hashed passwords. - key (bytes): The secret key used for HMAC. - digestmod (str): The name of the hash algorithm (e.g., \'sha256\'). Returns: - bool: True if authentication is successful, otherwise False. Raises: - KeyError: If the username does not exist. ``` Requirements 1. The password must be hashed using HMAC with the provided key and digest algorithm. 2. Store the hashed password in the `users` dictionary with the username as the key. 3. Ensure that `register_user` raises a `ValueError` if the username already exists. 4. Ensure that `authenticate_user` raises a `KeyError` if the username does not exist. 5. `authenticate_user` should return `True` only if the provided password matches the stored hashed password for the given username. Use the `hmac.compare_digest` function for secure comparison. 6. Handle any potential edge cases (e.g., empty passwords, unusual usernames). Example usage: ```python users_db = {} register_user(\\"alice\\", \\"password123\\", users_db, b\\"supersecretkey\\", \\"sha256\\") register_user(\\"bob\\", \\"securepass\\", users_db, b\\"supersecretkey\\", \\"sha256\\") assert authenticate_user(\\"alice\\", \\"password123\\", users_db, b\\"supersecretkey\\", \\"sha256\\") == True assert authenticate_user(\\"bob\\", \\"securepass\\", users_db, b\\"supersecretkey\\", \\"sha256\\") == True assert authenticate_user(\\"alice\\", \\"wrongpassword\\", users_db, b\\"supersecretkey\\", \\"sha256\\") == False ```","solution":"import hmac import hashlib def register_user(username: str, password: str, users: dict, key: bytes, digestmod: str) -> None: if username in users: raise ValueError(\\"Username already exists.\\") hashed_password = hmac.new(key, password.encode(), digestmod).hexdigest() users[username] = hashed_password def authenticate_user(username: str, password: str, users: dict, key: bytes, digestmod: str) -> bool: if username not in users: raise KeyError(\\"Username does not exist.\\") hashed_password = hmac.new(key, password.encode(), digestmod).hexdigest() stored_hashed_password = users[username] return hmac.compare_digest(stored_hashed_password, hashed_password)"},{"question":"# Python Coding Assessment Question Objective Design and implement a function `complex_iterate` that takes a nested data structure, evaluates specific conditions, performs mathematical operations, and transforms the data into a new format using some of Python\'s built-in functions. Requirements Your function should: 1. **Input**: Accept a list of dictionaries where each dictionary represents a data entry with potential nested lists and dictionaries. Each dictionary will always have the keys `id`, `value`, and `nested`. - `id`: an integer representing the identifier of the entry. - `value`: a numerical value (integer/float). - `nested`: a list of integers. 2. **Processing**: - For each entry in the list: - Increase the `value` by applying a function (e.g., square root, exponential) using functional programming (map, filter). - Evaluate if all integers in the `nested` list are positive. - Create a tuple `(id, new_value, all_positive)` where `new_value` is the transformed `value` and `all_positive` is a boolean indicating if all integers in `nested` are positive. - Generate a final result list of these tuples. 3. **Output**: Return a list of tuples with each tuple having the format `(id, new_value, all_positive)`. 4. **Constraints and Performance**: - Use list comprehensions, `map()`, `filter()`, `all()`, and other functional programming constructs where appropriate. - Ensure the solution is modular, readable, and efficient. Example Here is a sample input and expected output: ```python data = [ {\\"id\\": 1, \\"value\\": 16, \\"nested\\": [1, 2, 3]}, {\\"id\\": 2, \\"value\\": 25, \\"nested\\": [-1, 5, 6]}, {\\"id\\": 3, \\"value\\": 9, \\"nested\\": [0, 3]}, ] def complex_iterate(data): # Your implementation here result = complex_iterate(data) # Expected output # [ # (1, 4.0, True), # sqrt(16) = 4.0, all_positive = True # (2, 5.0, False), # sqrt(25) = 5.0, all_positive = False # (3, 3.0, False), # sqrt(9) = 3.0, all_positive = False (0 is not positive) # ] ``` Additional Information - You can use the `math` module for mathematical operations. - The function should handle cases where the `nested` list is empty by considering all elements as positive. - Ensure to handle different types of numerical values correctly (integers and floats). Write the implementation of the `complex_iterate` function in the code cell below: ```python import math def complex_iterate(data): # Your implementation here pass ``` Test your function thoroughly to ensure it meets the requirements.","solution":"import math def complex_iterate(data): result = [] for entry in data: id_val = entry[\'id\'] value = entry[\'value\'] nested = entry[\'nested\'] # Apply square root to the value new_value = math.sqrt(value) # Check if all integers in the nested list are positive all_positive = all(n > 0 for n in nested) # Create tuple and append to result result.append((id_val, new_value, all_positive)) return result"},{"question":"Objective Write a Python function to create a customized scatter plot with rug plots in Seaborn that visualizes the relationship between `total_bill` and `tip` amounts in the popular `tips` dataset. Additionally, add a rug plot to both axes with height and position customization. Include a third variable `time` (when the meal occurred) for hue mapping. Function Signature ```python def customized_scatter_rugplot(data: pd.DataFrame) -> None: pass ``` Input - `data` (pandas.DataFrame): The DataFrame containing the `tips` dataset. Output - The function should display a scatter plot with the following features: 1. Scatter plot of `total_bill` vs. `tip`. 2. Rug plots along both axes (`x` as `total_bill`, `y` as `tip`). 3. Use `time` for hue mapping. 4. Customize the rug plots to have a height of 0.05 and positions outside the main axes. Constraints and Requirements - You must use Seaborn and Matplotlib for visualizations. - Ensure all necessary libraries are imported within the function. - The `tips` dataset will always be provided as input to your function; you do not need to load it inside the function. - Your function should not return any value, just display the plot. Example Usage ```python import pandas as pd import seaborn as sns # Load the dataset (this part is just for demonstration, not required in your function) tips = sns.load_dataset(\\"tips\\") # Call the function with the loaded dataset customized_scatter_rugplot(tips) ``` When the function is correctly implemented, calling it with the `tips` dataset will render the customized scatter plot with rug plots as described.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customized_scatter_rugplot(data: pd.DataFrame) -> None: Create a customized scatter plot with rug plots in Seaborn. Parameters: - data: pandas DataFrame containing the tips dataset. # Setting style for the plot sns.set(style=\\"whitegrid\\") # Creating the scatter plot with rug plots scatter_plot = sns.scatterplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", data=data) # Adding rug plots to the axes sns.rugplot(x=data[\\"total_bill\\"], y=data[\\"tip\\"], height=0.05, ax=scatter_plot, color=\\"gray\\", clip_on=False) # Adding titles and labels scatter_plot.set_title(\\"Scatter Plot of Total Bill vs Tip with Rug Plots\\") scatter_plot.set_xlabel(\\"Total Bill\\") scatter_plot.set_ylabel(\\"Tip\\") # Display the plot plt.show()"},{"question":"Objective: Write a Python function `is_valid_float_representation` that takes a single input `obj` and returns a boolean indicating whether `obj` can be represented as a valid floating-point number according to the rules encapsulated in the provided C API functions. Requirements: 1. **Input:** - `obj`: A Python object (could be of any type). 2. **Output:** - `True` if `obj` can be represented as a valid floating-point number. - `False` otherwise. Constraints: - You are not allowed to directly use Python\'s built-in `float()` function. - You must emulate the behavior of `PyFloat_Check()`, `PyFloat_CheckExact()`, `PyFloat_AsDouble()`, and the conversion fallbacks as described: - If the object has a `__float__()` method, use it. - If not, fallback to `__index__()`, if defined. Example Usage: ```python class Test: def __float__(self): return 10.5 class InvalidTest: pass print(is_valid_float_representation(10.5)) # True print(is_valid_float_representation(\\"10.5\\")) # False print(is_valid_float_representation(Test())) # True print(is_valid_float_representation(InvalidTest())) # False ``` Implementation Notes: - You may assume the `obj` parameter will be provided in most typical data types (e.g., int, float, str, custom objects). - Consider edge cases, such as objects with faulty `__float__()` implementations or invalid `__index__()` methods. This assessment will test your understanding of Python\'s type system, special methods (`__float__()` and `__index__()`), and your ability to create robust type checking and conversion logic.","solution":"def is_valid_float_representation(obj): Returns True if obj can be represented as a valid floating-point number, based on custom float representation rules, otherwise False. # Check directly if `obj` is a float type if isinstance(obj, float): return True # Check if `obj` has a `__float__` method if hasattr(obj, \'__float__\'): try: result = obj.__float__() if isinstance(result, float): return True except (TypeError, ValueError): pass # Check if `obj` has an `__index__` method if hasattr(obj, \'__index__\'): try: result = obj.__index__() if isinstance(result, int): return True except (TypeError, ValueError): pass # If none of the above conditions are met, return False return False"},{"question":"# Metadata-based Dependency Checker **Objective**: You are required to implement a Python function that, given a package name, determines its direct dependencies and recursively checks the dependencies of those dependencies, creating a full map of the package\'s dependency tree. # Function Signature ```python def get_dependency_tree(package_name: str) -> dict: Given a package name, returns its full dependency tree. Parameters: package_name (str): The name of the package to check dependencies for. Returns: dict: A nested dictionary representing the dependency tree, where keys are package names and values are dictionaries of their direct dependencies. Raises: ValueError: If the package is not installed or metadata cannot be retrieved. ``` # Input - `package_name`: A string representing the name of a package installed in the Python environment. # Output - A nested dictionary representing the dependency tree for the given package. # Constraints - You may assume that the provided package name exists and metadata can be retrieved from the environment where the function is executed. - The environment should have sufficient permissions to access installed package metadata. - Handle the case where a package has no dependencies (should return an empty dictionary for that package). # Performance Requirements - Ensure that unnecessary repeated metadata access is minimized by caching retrieved metadata. # Example Usage ```python # Assuming \'package_name\' has the following direct dependencies: # - \'dependency1\' # - \'dependency2\' # And \'dependency1\' has: # - \'subdependency1\' # - \'subdependency2\' # And \'dependency2\' has no dependencies # The output should be: expected_output = { \'package_name\': { \'dependency1\': { \'subdependency1\': {}, \'subdependency2\': {}, }, \'dependency2\': {} } } actual_output = get_dependency_tree(\'package_name\') assert actual_output == expected_output ``` # Example ```python # Example for the \'requests\' package might be: { \\"requests\\": { \\"chardet\\": {}, \\"idna\\": {}, \\"urllib3\\": {}, \\"certifi\\": {} } } ``` # Hints - Use `importlib.metadata.requires` to retrieve the dependencies of a package. - Use recursion to build the dependency tree. - Consider the `importlib.metadata.metadata` function to handle any additional package-related data extraction if necessary.","solution":"import importlib.metadata def get_dependency_tree(package_name: str) -> dict: Given a package name, returns its full dependency tree. Parameters: package_name (str): The name of the package to check dependencies for. Returns: dict: A nested dictionary representing the dependency tree, where keys are package names and values are dictionaries of their direct dependencies. Raises: ValueError: If the package is not installed or metadata cannot be retrieved. def get_dependencies(name): try: requirements = importlib.metadata.requires(name) if requirements: deps = {} for req in requirements: req_name = req.split(\' \')[0] deps[req_name] = get_dependencies(req_name) return deps else: return {} except importlib.metadata.PackageNotFoundError: raise ValueError(f\\"Package \'{name}\' is not installed or metadata cannot be retrieved.\\") return {package_name: get_dependencies(package_name)} # Example usage # print(get_dependency_tree(\'requests\'))"},{"question":"# Configuration Management in Python Packaging Objective You are required to write a `setup.cfg` configuration file for a Python package. This will test your understanding of customizing the build and distribution process using Distutils. Description Suppose you are developing a Python package named `mypackage` that includes: - Pure Python modules. - C extension modules. - Documentation and example files. Your task is to create a `setup.cfg` file with the following specifications: 1. The C extension modules should be built in-place, meaning the compiled extensions should be placed in the same source directory as the Python modules. 2. For generating an RPM distribution, the package should have the following attributes: - Release version: `2` - Packager: `Jane Doe <jane.doe@example.com>` - Documentation files should include `README.md`, `CHANGELOG.md`, all files in the `docs/` folder, and all files in the `examples/` folder. Requirements - Write a `setup.cfg` file following the syntax and format described in the documentation. - Ensure all configurations are correct and adhere to the specified requirements. Deliverables - A `setup.cfg` file with the required configurations. Example Below is an example `setup.cfg` file template. Fill in the necessary configurations as per the requirements. ``` [build_ext] # Your configurations here [bdist_rpm] # Your configurations here ``` Constraints - All configuration settings must be accurate and in the correct sections. - Follow the indentation and formatting rules for multi-line options. Submission Submit the `setup.cfg` file containing your configurations.","solution":"# This is the content of the setup.cfg file represented as a string format in Python setup_cfg_content = [build_ext] inplace=1 [bdist_rpm] release=2 packager=Jane Doe <jane.doe@example.com> doc_files=README.md CHANGELOG.md docs examples def generate_setup_cfg(): Generates the setup.cfg file with the specified configurations. with open(\'setup.cfg\', \'w\') as file: file.write(setup_cfg_content.strip())"},{"question":"# Multiprocessing in PyTorch Elastic In this task, you are required to demonstrate your understanding of PyTorch Elastic\'s multiprocessing capabilities. You will need to implement a function that starts multiple subprocesses using the PyTorch Elastic `start_processes` method and manages the subprocess context and logging. Function Signature ```python import torch.distributed.elastic.multiprocessing as em def run_multiprocessing(num_processes: int, function_to_run: callable, args_list: list): Starts up multiple subprocesses and runs a provided function in each subprocess. Args: - num_processes (int): Number of subprocesses to start. - function_to_run (callable): The function to run in each subprocess. - args_list (list): List of arguments to pass to the function, one element per process. Returns: - result_list (list): List of results from each subprocess. pass ``` Requirements: 1. **Function Arguments**: - `num_processes` is an integer denoting how many subprocesses to start. - `function_to_run` is a function that each subprocess will execute. - `args_list` is a list of arguments for the function, where each element corresponds to the arguments for one subprocess. 2. **Subprocess Management**: - Use the `start_processes` method from the `torch.distributed.elastic.multiprocessing` module to start the subprocesses. - Use an appropriate context class (e.g., `MultiprocessContext`, `SubprocessContext`) to manage these processes. 3. **Logging**: - Utilize the provided logging classes (`DefaultLogsSpecs`, `LogsDest`, `LogsSpecs`) to configure and handle logging for each subprocess. 4. **Return Values**: - Collect and return the results from all subprocesses as a list. Below is an example of how you can utilize the function: ```python def sample_function(x): return x * x # Suppose we want to run `sample_function` in 3 subprocesses with different inputs result = run_multiprocessing(3, sample_function, [1, 2, 3]) print(result) # should print [1, 4, 9] ``` Constraints: - Ensure your solution handles any exceptions that may arise during the execution of subprocesses. - The number of processes (`num_processes`) will always match the length of the `args_list`. You may refer to the PyTorch Elastic documentation for more details on the classes and methods involved.","solution":"import torch.distributed.elastic.multiprocessing as em from torch.distributed.elastic.multiprocessing.errors import record import torch.multiprocessing as mp import logging logging.basicConfig(level=logging.INFO) def run_multiprocessing(num_processes: int, function_to_run: callable, args_list: list): Starts up multiple subprocesses and runs a provided function in each subprocess. Args: - num_processes (int): Number of subprocesses to start. - function_to_run (callable): The function to run in each subprocess. - args_list (list): List of arguments to pass to the function, one element per process. Returns: - result_list (list): List of results from each subprocess. results = mp.Manager().list() def wrapper_function(index, *args): try: result = function_to_run(*args) results.append((index, result)) except Exception as e: logging.error(f\'Error in process {index}: {e}\') results.append((index, e)) processes = [] for index, args in enumerate(args_list): p = mp.Process(target=wrapper_function, args=(index, *args)) processes.append(p) p.start() for p in processes: p.join() result_list = [result for index, result in sorted(results, key=lambda x: x[0])] return result_list"},{"question":"<|Analysis Begin|> The provided documentation offers an overview of several Unix-specific services and modules available in Python 3.10. These modules allow interactions with various Unix system calls and functionalities, such as POSIX system calls (`posix`), password and group databases (`pwd` and `grp`), terminal control (`termios`, `tty`, `pty`), file control and input/output control system calls (`fcntl`), resource usage (`resource`), and syslog (`syslog`). Each of these modules provides unique interfaces for interacting with the underlying Unix operating system, and they offer various functions and examples demonstrating their usage. Given this context, a challenging coding assessment question could ask students to utilize one or more of these modules to perform a specific Unix-related task, ensuring that they understand how to interact with Unix system calls and handle system-level information in Python. <|Analysis End|> <|Question Begin|> # Problem: User and Group Information Reporting Write a Python function `user_group_report` that generates a report of all current users and their associated groups on a Unix system. The function should utilize the `pwd` and `grp` modules to retrieve and display the necessary information. Function Signature ```python def user_group_report() -> str: pass ``` Expected Output - The function should return a string representing the report. - Each line of the report should contain a user\'s name and a list of groups they belong to, formatted as follows: ``` username: [group1, group2, ..., groupN] ``` Constraints - You should handle the potential complexity of users belonging to multiple groups. - Ensure the output is sorted by username in alphabetical order. - The function should work efficiently, even with a large number of users and groups. Example ```python # Example output: # adam: [wheel, staff] # eve: [staff, users] # john: [admin, staff, users] print(user_group_report()) ``` Additional Requirements - You are not allowed to use any external libraries. Stick to the standard library (the modules described in the provided documentation). Good luck!","solution":"import pwd import grp def user_group_report() -> str: Generates a report of all current users and their associated groups on a Unix system. Returns: str: The formatted report listing users and their groups users = pwd.getpwall() user_groups = {} for user in users: username = user.pw_name groups = [g.gr_name for g in grp.getgrall() if username in g.gr_mem] primary_gid_name = grp.getgrgid(user.pw_gid).gr_name if primary_gid_name not in groups: groups.append(primary_gid_name) user_groups[username] = sorted(groups) sorted_usernames = sorted(user_groups.keys()) report_lines = [f\\"{username}: {user_groups[username]}\\" for username in sorted_usernames] return \\"n\\".join(report_lines)"},{"question":"**Title**: Advanced Configuration File Handling with `configparser` **Objective**: To assess the student\'s understanding of the `configparser` module in Python, specifically their ability to read, write, and manipulate configuration files. **Question**: You are tasked with developing a Python script that reads a configuration file, processes its content, and writes the output to another configuration file. The script should demonstrate the following capabilities: 1. **Reading a Configuration File**: - Read the given configuration from a file named `input_config.ini`. - Assume the file has multiple sections with various key-value pairs. 2. **Data Processing**: - Convert all integer values in the configuration to their hexadecimal representations. - Multiply all floating-point values by 100 and round to two decimal places. - Change all boolean values (`yes`, `no`, `true`, `false`, etc.) to their uppercase equivalents (`YES`, `NO`, `TRUE`, `FALSE`). - For any key that contains the string \\"path\\", replace any backslashes (``) with forward slashes (`/`). 3. **Writing the Processed Configuration**: - Write the modified configuration to a new file named `output_config.ini`. 4. **Interpolation**: - Add a new section named `[Paths]` with two keys: - `base_dir` with the value `/home/user` - `log_dir` with an interpolated value based on `base_dir`, resulting in `/home/user/logs` **Input**: - A sample configuration file `input_config.ini` is provided with the following structure: ```ini [DEFAULT] path_separator = [server] ServerAliveInterval = 45 Port = 8080 Compression = yes CompressionLevel = 9.5 LogPath = C:Logs [client] User = user MaxRetries = 3 EnableFeatureX = false Threshold = 0.75 ``` **Expected Output**: - A file `output_config.ini` with the processed content that adheres to the rules described above. **Constraints**: - The script should handle any valid configuration file that adheres to the INI file format. - Ensure that the script is robust and handles errors gracefully. **Implementation Details**: - Create a Python script with a function `process_config(input_file: str, output_file: str) -> None` that performs the required operations. - Use `configparser` methods and classes to implement the functionality. ```python import configparser def process_config(input_file: str, output_file: str) -> None: # Create a config parser object and read the input file config = configparser.ConfigParser() config.read(input_file) # Process the sections and their key-value pairs for section in config.sections(): for key in config[section]: value = config[section][key] if value.isdigit(): config[section][key] = hex(int(value)) elif value.replace(\'.\', \'\', 1).isdigit() and \'.\' in value: config[section][key] = \\"{:.2f}\\".format(float(value) * 100) elif value.lower() in [\'yes\', \'no\', \'true\', \'false\']: config[section][key] = value.upper() if \\"path\\" in key.lower(): config[section][key] = value.replace(\\"\\", \\"/\\") # Add the new section with interpolated values config[\'Paths\'] = { \'base_dir\': \'/home/user\', \'log_dir\': \'%(base_dir)s/logs\' } # Write the modified configuration to the output file with open(output_file, \'w\') as configfile: config.write(configfile) # Example usage process_config(\'input_config.ini\', \'output_config.ini\') ``` **Note**: Ensure you have the `input_config.ini` file in the same directory as your script or specify the correct path to it when testing your solution.","solution":"import configparser def process_config(input_file: str, output_file: str) -> None: # Create a config parser object and read the input file config = configparser.ConfigParser() config.read(input_file) # Process the sections and their key-value pairs for section in config.sections(): for key in config[section]: value = config[section][key] if value.isdigit(): config[section][key] = hex(int(value)) elif value.replace(\'.\', \'\', 1).isdigit() and \'.\' in value: config[section][key] = \\"{:.2f}\\".format(float(value) * 100) elif value.lower() in [\'yes\', \'no\', \'true\', \'false\']: config[section][key] = value.upper() if \\"path\\" in key.lower(): config[section][key] = value.replace(\\"\\", \\"/\\") # Add the new section with interpolated values config[\'Paths\'] = { \'base_dir\': \'/home/user\', \'log_dir\': \'%(base_dir)s/logs\' } # Write the modified configuration to the output file with open(output_file, \'w\') as configfile: config.write(configfile) # Example usage # process_config(\'input_config.ini\', \'output_config.ini\')"},{"question":"**CSV File Processing with Custom Dialects** **Objective**: Demonstrate proficiency with Python\'s `csv` module for reading, writing, and processing CSV files with custom dialects and formatting. **Task**: 1. Create a new custom dialect named `semicolon_dialect` that uses a semicolon (`;`) as the delimiter and double quotes (`\\"`) for quoting. 2. Write a function `read_csv_file(file_path)` that reads a CSV file using the `semicolon_dialect` and returns a list of dictionaries. 3. Write a function `write_csv_file(data, file_path)` that writes a list of dictionaries to a CSV file using the `semicolon_dialect`. **Details**: - The data to be read or written will consist of dictionaries where the keys represent column names and values represent the cell data. - Any empty fields in the dictionaries should be written as empty strings. - For simplicity, assume all CSV files have a header row that should be used as dictionary keys for reading and writing. **Function Signatures**: ```python import csv def register_semicolon_dialect(): Registers the semicolon_dialect with delimiter as semicolon (;) and quotechar as double quote (\\"). pass def read_csv_file(file_path: str) -> list: Reads a CSV file using the semicolon_dialect and returns a list of dictionaries. Parameters: - file_path: str : path to the CSV file to be read. Returns: - List of dictionaries representing the CSV data. pass def write_csv_file(data: list, file_path: str): Writes a list of dictionaries to a CSV file using the semicolon_dialect. Parameters: - data: list : List of dictionaries to be written to CSV. - file_path: str : path to the CSV file to be written. pass ``` **Constraints**: - The `register_semicolon_dialect` function must be called before any CSV reading or writing operations. - Handle any CSV errors gracefully and print appropriate error messages. **Example Usage**: ```python # Example data to write data_to_write = [ {\'name\': \'Alice\', \'age\': \'30\', \'city\': \'New York\'}, {\'name\': \'Bob\', \'age\': \'25\', \'city\': \'Los Angeles\'}, {\'name\': \'Charlie\', \'age\': \'\', \'city\': \'Chicago\'} ] # Register the custom dialect register_semicolon_dialect() # Writing the data to a CSV file write_csv_file(data_to_write, \'output.csv\') # Reading the data back from the CSV file data_read = read_csv_file(\'output.csv\') print(data_read) # Expected Output: # [{\'name\': \'Alice\', \'age\': \'30\', \'city\': \'New York\'}, # {\'name\': \'Bob\', \'age\': \'25\', \'city\': \'Los Angeles\'}, # {\'name\': \'Charlie\', \'age\': \'\', \'city\': \'Chicago\'}] ``` This task assesses the students\' understanding of: - CSV file operations. - Using custom CSV dialects. - Handling CSV data represented as dictionaries. - Practical error-handling for I/O operations.","solution":"import csv def register_semicolon_dialect(): Registers the semicolon_dialect with delimiter as semicolon (;) and quotechar as double quote (\\"). csv.register_dialect(\'semicolon_dialect\', delimiter=\';\', quotechar=\'\\"\', quoting=csv.QUOTE_MINIMAL) def read_csv_file(file_path: str) -> list: Reads a CSV file using the semicolon_dialect and returns a list of dictionaries. Parameters: - file_path: str - path to the CSV file to be read. Returns: - List of dictionaries representing the CSV data. try: with open(file_path, mode=\'r\', newline=\'\', encoding=\'utf-8\') as file: reader = csv.DictReader(file, dialect=\'semicolon_dialect\') return [row for row in reader] except Exception as e: print(f\\"Error reading CSV file: {e}\\") return [] def write_csv_file(data: list, file_path: str): Writes a list of dictionaries to a CSV file using the semicolon_dialect. Parameters: - data: list - List of dictionaries to be written to CSV. - file_path: str - path to the CSV file to be written. if not data: return try: with open(file_path, mode=\'w\', newline=\'\', encoding=\'utf-8\') as file: fieldnames = data[0].keys() writer = csv.DictWriter(file, fieldnames=fieldnames, dialect=\'semicolon_dialect\') writer.writeheader() for row in data: writer.writerow(row) except Exception as e: print(f\\"Error writing CSV file: {e}\\")"},{"question":"# Advanced Coding Assessment Question Objective The objective of this assignment is to evaluate your ability to use the `readline` module for creating a custom interactive command-line application. You will demonstrate understanding of key functions such as history management, line buffer handling, and custom word completion. Question **Implement a Python interactive command-line application with the following features:** 1. **Custom Command Completer:** - Create a custom completer function that suggests commands from a predefined list (`[\'start\', \'stop\', \'pause\', \'play\', \'exit\']`). - Use the `readline.set_completer` function to set this completer. 2. **History Management:** - Implement functionalities to read and write command history to a file named `\'.cmd_history\'` in the user\'s home directory. - Ensure that the application saves the history of the user\'s inputs upon exiting and loads it upon starting. 3. **Startup and Pre-input Hooks:** - Set a startup hook that displays a welcome message when the application starts. - Set a pre-input hook that displays the current line buffer before reading user inputs. 4. **Interactive Loop:** - Create an interactive loop that continuously reads user input (use `input()`). - Implement command functionalities for `start`, `stop`, `pause`, and `play` with simple print statements for their actions. - Implement an `exit` command that breaks the loop and properly saves the history before exiting. Input and Output Your program should ensure the following: - **Input:** User inputs command-line instructions interactively. - **Output:** The program should provide auto-completion for commands, save and load input history, and execute predefined commands with appropriate print statements. Constraints - The history length should be retained to the last 1000 commands. - Assume a Unix-like environment where `readline` is fully supported. - Handle any potential file-related exceptions properly. Performance Requirements - Ensure minimal latency for command completion and history lookup. - The application should remain interactive and responsive even with the maximum history length. # Example Workflow 1. **Startup:** - Display a welcome message. - Load history from `~/.cmd_history`. 2. **User Interaction:** - On typing `st` and pressing Tab, it should auto-complete to `start` or provide suggestions like `start stop`. - On entering `start`, it should print `Command start executed`. - The same applies for `stop`, `pause`, and `play`. 3. **Exiting:** - On entering `exit`, it should save the history to `~/.cmd_history` and exit the loop. **Your implementation should be contained in a single Python script. Include appropriate comments and documentation for clarity.**","solution":"import readline import os HISTORY_FILE = os.path.expanduser(\'~/.cmd_history\') COMMANDS = [\'start\', \'stop\', \'pause\', \'play\', \'exit\'] def save_history(): readline.write_history_file(HISTORY_FILE) def load_history(): if os.path.exists(HISTORY_FILE): readline.read_history_file(HISTORY_FILE) def completer(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] return None def startup_hook(): print(\\"Welcome to the command-line application!\\") def pre_input_hook(): buffer = readline.get_line_buffer() if buffer: print(f\\"Current input: {buffer}\\") def main(): readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") readline.set_history_length(1000) readline.set_startup_hook(startup_hook) readline.set_pre_input_hook(pre_input_hook) load_history() while True: try: user_input = input(\\">> \\").strip() if user_input == \'exit\': save_history() print(\\"Exiting... History saved.\\") break elif user_input == \'start\': print(\\"Command start executed\\") elif user_input == \'stop\': print(\\"Command stop executed\\") elif user_input == \'pause\': print(\\"Command pause executed\\") elif user_input == \'play\': print(\\"Command play executed\\") else: print(f\\"Unknown command: {user_input}\\") except (EOFError, KeyboardInterrupt): save_history() print(\\"nExiting... History saved.\\") break if __name__ == \\"__main__\\": main()"},{"question":"# MIME Email Construction Problem Statement You are tasked with creating a function that constructs an email with mixed content, including plain text and an image attachment. The function should use classes from the `email.mime` module to build the email structure. Function Signature ```python def create_mime_email( subject: str, sender: str, recipient: str, text_content: str, image_content: bytes, image_filename: str ) -> str: ``` Input Parameters - `subject` (str): The subject of the email. - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `text_content` (str): The plain text content of the email. - `image_content` (bytes): The binary content of the image to be attached. - `image_filename` (str): The filename for the attached image. Output - Returns a string representation of the MIME email message. Constraints 1. The email should have a subject. 2. The email must contain plain text content. 3. The email must have one image attachment. 4. The function must utilize appropriate classes from the `email.mime` module. 5. The MIME version should be `1.0`. Example ```python subject = \\"Hello\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" text_content = \\"This is a test email.\\" image_content = b\\"x89PNGrnx1anx00x00x00rIHDRx00x00x00x10...\\" image_filename = \\"test.png\\" email_message = create_mime_email(subject, sender, recipient, text_content, image_content, image_filename) print(email_message) ``` The expected output should be an RFC 5322 compliant MIME message as a string that includes headers, text content, and the image encoded in base64. Detailed Requirements 1. **Create the Base MIME Message**: - Use `MIMEMultipart` to create the base message with `_subtype=\'mixed\'`. 2. **Add Headers**: - Set the `From`, `To`, and `Subject` headers. 3. **Attach the Plain Text Content**: - Use `MIMEText` to create the text part and attach it to the base message. 4. **Attach the Image**: - Use `MIMEImage` to create the image part and attach it to the base message. - Ensure the image data is encoded properly, usually with base64 encoding. 5. **Return the Message as a String**: - Use the `as_string` method to return the entire message as a string representation. Implementation ```python from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage import email.encoders def create_mime_email( subject: str, sender: str, recipient: str, text_content: str, image_content: bytes, image_filename: str ) -> str: # Create the root message msg = MIMEMultipart(_subtype=\'mixed\') msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Attach the plain text message text_part = MIMEText(text_content, _subtype=\'plain\') msg.attach(text_part) # Attach the image image_part = MIMEImage(image_content, _subtype=\'png\', name=image_filename) email.encoders.encode_base64(image_part) # Encode the image data image_part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{image_filename}\\"\') msg.attach(image_part) # Return the formatted email as a string return msg.as_string() ```","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage import email.encoders def create_mime_email( subject: str, sender: str, recipient: str, text_content: str, image_content: bytes, image_filename: str ) -> str: # Create the root message msg = MIMEMultipart(_subtype=\'mixed\') msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Attach the plain text message text_part = MIMEText(text_content, _subtype=\'plain\') msg.attach(text_part) # Attach the image image_part = MIMEImage(image_content, name=image_filename) email.encoders.encode_base64(image_part) # Encode the image data image_part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{image_filename}\\"\') msg.attach(image_part) # Return the formatted email as a string return msg.as_string()"},{"question":"# Seaborn Customization and Plotting Challenge Objective: Write a function `customize_and_plot(data: pd.DataFrame) -> None` that: 1. Reads a pandas DataFrame where the columns contain numeric data. 2. Creates a figure with two subplots (1, 2). 3. Applies different seaborn themes and customizations to each subplot as described below. 4. Ensures proper display and aesthetics of the resulting plots. Inputs: - `data`: A pandas DataFrame with at least two different numeric columns named \'A\' and \'B\'. Assume that the DataFrame has at least 100 rows of data. Requirements: 1. **First Subplot**: - Use the \\"darkgrid\\" style. - Use the \\"deep\\" color palette. - Generate a bar plot for column \'A\'. - Include customization to remove the top and right spines of the plot. 2. **Second Subplot**: - Use the \\"white\\" style with default palette. - Generate a line plot for column \'B\'. - Ensure gridlines are present. - Customize the line plot to set the line width to 2 and the color to \\"blue\\". 3. Display the figure with a tight layout. Output: - The function does not return anything but must display the figure with the described subplots when called. Constraints: - Consider performance implications when handling large datasets with seaborn and matplotlib. - Ensure your solution is modular and readable. # Example Usage: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customize_and_plot(data: pd.DataFrame) -> None: sns.set_theme(style=\\"darkgrid\\", palette=\\"deep\\") fig, axs = plt.subplots(1, 2, figsize=(14, 7)) # First subplot sns.barplot(x=data.index, y=data[\'A\'], ax=axs[0]) axs[0].spines[\'top\'].set_visible(False) axs[0].spines[\'right\'].set_visible(False) # Second subplot sns.set_theme(style=\\"white\\", palette=None) sns.lineplot(x=data.index, y=data[\'B\'], ax=axs[1], linewidth=2, color=\\"blue\\") axs[1].grid(True) plt.tight_layout() plt.show() # Assuming df is a pandas DataFrame with the required structure df = pd.DataFrame({ \'A\': range(1, 101), \'B\': [value**0.5 for value in range(1, 101)] }) customize_and_plot(df) ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customize_and_plot(data: pd.DataFrame) -> None: sns.set_theme(style=\\"darkgrid\\", palette=\\"deep\\") fig, axs = plt.subplots(1, 2, figsize=(14, 7)) # First subplot sns.barplot(x=data.index, y=data[\'A\'], ax=axs[0]) axs[0].spines[\'top\'].set_visible(False) axs[0].spines[\'right\'].set_visible(False) # Second subplot sns.set_theme(style=\\"white\\", palette=None) sns.lineplot(x=data.index, y=data[\'B\'], ax=axs[1], linewidth=2, color=\\"blue\\") axs[1].grid(True) plt.tight_layout() plt.show()"},{"question":"# Advanced Python Programming Question: Iterator Protocols Objective Implement a custom class that functions as both a synchronous and asynchronous iterator using Python\'s iterator protocols. This task will assess your understanding of handling iterators and asynchronous iterators using the methods provided in the documentation. Task Description 1. **Custom Sync Iterator:** - Implement a class `CustomSyncIterator` that takes an iterable (like a list) as its input and provides synchronous iteration through it. - Implement the `__iter__()` and `__next__()` methods. - Use the `PyIter_Check()` and `PyIter_Next()` methods to ensure compliance with the iterator protocol. 2. **Custom Async Iterator:** - Implement a class `CustomAsyncIterator` that takes an iterable and provides asynchronous iteration through it. - Implement the `__aiter__()` and `__anext__()` methods, which should behave similarly to their synchronous counterparts. - Use the `PyAIter_Check()` and `PyIter_Send()` to ensure compliance with the async iterator protocol. Input and Output Formats 1. **CustomSyncIterator** - **Input:** An iterable object (e.g., a list `[1, 2, 3]`). - **Output:** Objects from the iterable in sequence when iterated upon using a `for` loop. ```python sync_iter = CustomSyncIterator([1, 2, 3]) for item in sync_iter: print(item) # Expected output: # 1 # 2 # 3 ``` 2. **CustomAsyncIterator** - **Input:** An iterable object (e.g., a list `[1, 2, 3]`). - **Output:** Objects from the iterable in sequence when iterated upon using an `async for` loop. ```python import asyncio async def main(): async_iter = CustomAsyncIterator([1, 2, 3]) async for item in async_iter: print(item) asyncio.run(main()) # Expected output: # 1 # 2 # 3 ``` Constraints - Your custom iterators should handle edge cases, such as empty iterables. - Your implementations should correctly propagate any exceptions that occur during iteration. Performance Requirements - The implementation should be efficient in terms of both time and space. - Both synchronous and asynchronous iteration should be achieved with minimal overhead. Notes - You may assume that the input iterable will not be modified during iteration. - Using the provided `PyIter_Check()`, `PyIter_Next()`, `PyAIter_Check()`, and `PyIter_Send()` functions is crucial to ensure compliance and correctness. Submission Submit your implementation of the `CustomSyncIterator` and `CustomAsyncIterator` classes.","solution":"class CustomSyncIterator: def __init__(self, iterable): self._iterable = iter(iterable) def __iter__(self): return self def __next__(self): try: return next(self._iterable) except StopIteration: raise StopIteration import asyncio class CustomAsyncIterator: def __init__(self, iterable): self._iterable = iter(iterable) def __aiter__(self): return self async def __anext__(self): try: return next(self._iterable) except StopIteration: raise StopAsyncIteration"},{"question":"**Title:** Advanced Generators for Efficient Data Processing **Objective:** Demonstrate your understanding and ability to implement and work with generator functions in Python for efficient data processing. **Problem Statement:** You are tasked to process a large dataset containing information about sales transactions. For this purpose, you will create a generator function that processes the data incrementally without loading the entire dataset into memory at once. Your input will be a list of dictionaries representing sales transactions. Each dictionary will contain the following keys: - `transaction_id` (str): A unique identifier for the transaction. - `amount` (float): The transaction amount. - `currency` (str): The currency in which the transaction was made. - `date` (str): The date of the transaction, formatted as \\"YYYY-MM-DD\\". Write a generator function `process_transactions(transactions)` that: 1. Receives the list of dictionaries `transactions`. 2. Iteratively processes each transaction. 3. Converts the transaction dates to Python `datetime` objects. 4. Extracts and yields the `transaction_id` and the converted date as a tuple `(\\"transaction_id\\", datetime_object)`. **Function Signature:** ```python import datetime from typing import List, Dict, Generator, Tuple def process_transactions(transactions: List[Dict[str, str]]) -> Generator[Tuple[str, datetime.datetime], None, None]: pass ``` **Input:** - `transactions`: A list of dictionaries, each dictionary containing keys `transaction_id`, `amount`, `currency`, and `date`. **Output:** - A generator that yields tuples containing the transaction ID and the converted date for each transaction. **Example:** ```python transactions = [ {\\"transaction_id\\": \\"TX1001\\", \\"amount\\": 150.75, \\"currency\\": \\"USD\\", \\"date\\": \\"2023-09-12\\"}, {\\"transaction_id\\": \\"TX1002\\", \\"amount\\": 99.99, \\"currency\\": \\"EUR\\", \\"date\\": \\"2023-09-13\\"} ] gen = process_transactions(transactions) print(next(gen)) # Expected output: (\\"TX1001\\", datetime.datetime(2023, 9, 12, 0, 0)) print(next(gen)) # Expected output: (\\"TX1002\\", datetime.datetime(2023, 9, 13, 0, 0)) ``` **Constraints:** - You must use a generator function to process the transactions incrementally. - You cannot load all the transactions into memory at once. - Ensure your solution correctly handles the date conversion to `datetime`. **Performance Requirements:** - The function should efficiently handle very large lists of transactions without excessive memory usage.","solution":"import datetime from typing import List, Dict, Generator, Tuple def process_transactions(transactions: List[Dict[str, str]]) -> Generator[Tuple[str, datetime.datetime], None, None]: for transaction in transactions: trans_id = transaction[\'transaction_id\'] trans_date = datetime.datetime.strptime(transaction[\'date\'], \'%Y-%m-%d\') yield (trans_id, trans_date)"},{"question":"# Question: Titanic Survival Analysis Using Seaborn **Objective:** Use the Seaborn library to analyze the Titanic dataset and create informative visualizations to assess survival rates based on different passenger attributes. Problem Statement: Given the Titanic dataset, write a Python function `plot_titanic_analysis()` that performs the following tasks: 1. **Load the Dataset:** Load the Titanic dataset using the `seaborn.load_dataset()` function. 2. **Create Boxplots:** a. Generate a horizontal boxplot to show the distribution of passengers\' fares. b. Generate a vertical boxplot that groups the age of passengers by their passenger class (`class`). c. Create a more complex boxplot that shows the survival (`alive`) as a hue, and the age distribution across different passenger classes (`class`). 3. **Customize Boxplots:** a. Create a boxplot showing the distribution of passenger ages grouped by deck, covering the full range of the data with the whiskers. b. Create a narrow boxplot showing the distribution of fares grouped by passenger class and layered by gender (`sex`), with customized colors and linewidth. 4. **Annotate the Plots:** Use the underlying matplotlib functions to add titles and modify the style attributes of each plot for better readability. **Constraints:** - You must use Seaborn for loading the dataset and creating the plots. - Use appropriate Seaborn and Matplotlib functions for customization and annotations. - Each plot should be displayed clearly with necessary labels and titles. **Function Signature:** ```python def plot_titanic_analysis(): pass ``` **Expected Output:** The function should create and display multiple plots that show various insights into the Titanic dataset based on the specified groupings and customizations. Example Implementation: Here is an example of how you might start the implementation, focusing on loading the dataset and creating the first boxplot: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_analysis(): # Load Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create horizontal boxplot for fares plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"fare\\"]) plt.title(\\"Distribution of Passenger Fares\\") plt.xlabel(\\"Fare\\") plt.show() # Add additional plots and customizations as specified # ... ``` Complete the function by adding the necessary plots and customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_analysis(): # Load Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create horizontal boxplot for fares plt.figure(figsize=(10, 6)) sns.boxplot(x=titanic[\\"fare\\"]) plt.title(\\"Distribution of Passenger Fares\\") plt.xlabel(\\"Fare\\") plt.show() # Create vertical boxplot for age grouped by passenger class plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"class\\", y=\\"age\\", data=titanic) plt.title(\\"Age Distribution by Passenger Class\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Age\\") plt.show() # Create boxplot for age distribution with survival as hue across passenger classes plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"class\\", y=\\"age\\", hue=\\"alive\\", data=titanic) plt.title(\\"Age Distribution by Passenger Class and Survival\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Age\\") plt.legend(title=\\"Survival\\") plt.show() # Create a boxplot for age grouped by deck covering the full range of data with whiskers plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"deck\\", y=\\"age\\", data=titanic) plt.title(\\"Age Distribution by Deck\\") plt.xlabel(\\"Deck\\") plt.ylabel(\\"Age\\") plt.show() # Create a narrow boxplot for fares grouped by passenger class and gender plt.figure(figsize=(10, 6)) sns.boxplot(x=\\"class\\", y=\\"fare\\", hue=\\"sex\\", data=titanic, palette=\\"Set2\\", linewidth=2.5) plt.title(\\"Fare Distribution by Passenger Class and Gender\\") plt.xlabel(\\"Passenger Class\\") plt.ylabel(\\"Fare\\") plt.legend(title=\\"Gender\\") plt.show()"},{"question":"**Problem Statement: Implementing a Secure File Integrity Checker** You are required to implement a secure file integrity checker using the `hashlib` module. The solution should handle the following requirements: 1. **Compute Hashes:** - Compute the SHA256 hash of the file content. - Compute a BLAKE2b hash using a specified digest size. 2. **Randomized Hashing:** - Implement a function that computes a randomized hash using BLAKE2b with a specified salt. 3. **Secure Password Storage:** - Implement a function to securely hash and store user passwords using `pbkdf2_hmac`. 4. **Verify File Integrity:** - Implement a function that verifies the integrity of a file by comparing stored hash values (SHA256 and BLAKE2b) with newly computed hashes. # Detailed Requirements **Part 1: Compute Hashes** - Create a function `compute_hashes(file_path: str, blake2b_digest_size: int, salt: bytes) -> Tuple[str, str, str]` that: - Computes the SHA256 hash of the file. - Computes the BLAKE2b hash of the file using the specified digest size. - Computes a randomized BLAKE2b hash of the file using the provided salt. - Returns a tuple with three hash values in hexadecimal format: `(\'sha256_hash\', \'blake2b_hash\', \'randomized_blake2b_hash\')`. **Part 2: Secure Password Storage** - Create a function `secure_password_hash(password: str, salt: bytes, iterations: int) -> str` that: - Uses `pbkdf2_hmac` with the SHA256 algorithm to securely hash the password. - Returns the hashed password in hexadecimal format. **Part 3: Verify File Integrity** - Create a function `verify_file_integrity(file_path: str, sha256_hash: str, blake2b_hash: str, blake2b_digest_size: int, salt: bytes) -> bool` that: - Recomputes the SHA256 hash and the randomized BLAKE2b hash of the file using the provided salt. - Compares the recomputed hashes with the provided hashes. - Returns `True` if both hashes match, `False` otherwise. # Example: ```python example_file_path = \'path/to/example.txt\' example_salt = b\'some_random_salt\' blake2b_digest_size = 32 # Compute hashes sha256_hash, blake2b_hash, randomized_blake2b_hash = compute_hashes(example_file_path, blake2b_digest_size, example_salt) # Securely store a password password_salt = b\'another_random_salt\' iterations = 100000 hashed_password = secure_password_hash(\'mypassword\', password_salt, iterations) # Verify file integrity is_valid = verify_file_integrity(example_file_path, sha256_hash, blake2b_hash, blake2b_digest_size, example_salt) print(is_valid) # Should print True if the file has not been altered ``` **Constraints:** - All file paths are valid and point to text files. - Salt values are always valid byte strings. - The digest size for BLAKE2b will be a positive integer up to 64. - The number of iterations for password hashing will be a positive integer. - The file content, passwords, and salt values are all UTF-8 encoded strings. **Performance Requirements:** - Ensure that the solution is efficient in terms of computation, especially for large files and multiple iterations in password hashing.","solution":"import hashlib import os def compute_hashes(file_path: str, blake2b_digest_size: int, salt: bytes) -> tuple: Computes SHA256, BLAKE2b, and salted BLAKE2b hashes of the file content. :param file_path: Path to the input file. :param blake2b_digest_size: Digest size for the BLAKE2b hash. :param salt: The salt to use for the BLAKE2b hash. :return: A tuple of hashes in hexadecimal format (SHA256, BLAKE2b, randomized BLAKE2b). sha256 = hashlib.sha256() blake2b = hashlib.blake2b(digest_size=blake2b_digest_size) blake2b_salted = hashlib.blake2b(digest_size=blake2b_digest_size, salt=salt) with open(file_path, \'rb\') as f: file_content = f.read() sha256.update(file_content) blake2b.update(file_content) blake2b_salted.update(file_content) return sha256.hexdigest(), blake2b.hexdigest(), blake2b_salted.hexdigest() def secure_password_hash(password: str, salt: bytes, iterations: int) -> str: Securely hash and store user password using pbkdf2_hmac with SHA256. :param password: The password to hash. :param salt: The salt to use for hashing. :param iterations: Number of iterations for pbkdf2_hmac. :return: The hashed password in hexadecimal format. dk = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, iterations) return dk.hex() def verify_file_integrity(file_path: str, sha256_hash: str, blake2b_hash: str, blake2b_digest_size: int, salt: bytes) -> bool: Verify the integrity of a file by comparing stored hash values with newly computed hashes. :param file_path: Path to the input file. :param sha256_hash: Precomputed SHA256 hash value. :param blake2b_hash: Precomputed BLAKE2b hash value. :param blake2b_digest_size: Digest size for the BLAKE2b hash. :param salt: The salt used for the BLAKE2b hash. :return: True if both hashes match, False otherwise. sha256, blake2b, blake2b_rand = compute_hashes(file_path, blake2b_digest_size, salt) return sha256_hash == sha256 and blake2b_hash == blake2b_rand"},{"question":"**Problem Statement:** You are required to implement a class called `ObjectRegistry` that uses the `weakref` module to manage a registry of objects. The registry should use weak references so that objects can be garbage collected when they are no longer in use. Your implementation should include the following functionalities: 1. **Add Object:** Method `add_object(obj)` to add an object to the registry. 2. **Get Object:** Method `get_object(obj_id)` to retrieve an object from the registry using its id. 3. **List Objects:** Method `list_objects()` to return a list of all currently registered objects. 4. **Remove Object:** Method `remove_object(obj_id)` to remove an object from the registry using its id. The class should internally use `weakref.WeakValueDictionary` to hold weak references to the objects. **Constraints:** - The `obj_id` should be the `id` of the object. - The registry should only hold references to objects that can be weakly referenced. - The operations for adding, retrieving, listing, and removing objects should be efficient, with average time complexity of O(1) where applicable. **Function Signature:** ```python import weakref class ObjectRegistry: def __init__(self): self.registry = weakref.WeakValueDictionary() def add_object(self, obj): pass def get_object(self, obj_id): pass def list_objects(self): pass def remove_object(self, obj_id): pass ``` **Example:** ```python # Define some example classes class ExampleObject: def __init__(self, name): self.name = name # Create instances of the ObjectRegistry registry = ObjectRegistry() # Create some objects obj1 = ExampleObject(\'Object 1\') obj2 = ExampleObject(\'Object 2\') # Add objects to the registry registry.add_object(obj1) registry.add_object(obj2) # List objects in the registry print(registry.list_objects()) # Output: [<ExampleObject name=\'Object 1\'>, <ExampleObject name=\'Object 2\'>] # Get an object by its ID obj_id = id(obj1) retrieved_obj = registry.get_object(obj_id) print(retrieved_obj.name) # Output: \'Object 1\' # Remove an object from the registry registry.remove_object(obj_id) print(registry.list_objects()) # Output: [<ExampleObject name=\'Object 2\'>] ``` **Note:** 1. The `list_objects` method should return the current live objects in the registry. 2. Ensure that the class handles the case where an object might have been garbage collected and is no longer available when attempting to retrieve or list objects.","solution":"import weakref class ObjectRegistry: def __init__(self): self.registry = weakref.WeakValueDictionary() def add_object(self, obj): obj_id = id(obj) self.registry[obj_id] = obj def get_object(self, obj_id): return self.registry.get(obj_id, None) def list_objects(self): return list(self.registry.values()) def remove_object(self, obj_id): if obj_id in self.registry: del self.registry[obj_id] # Example class for demonstrating the ObjectRegistry usage class ExampleObject: def __init__(self, name): self.name = name"},{"question":"**Question: Exception Handling and Custom Exception Classes** Implement a function `process_file(file_path: str) -> int` that reads a file, performs some processing on its contents, and returns an integer result. Your implementation should cover the following requirements: 1. **Reading the File:** - Use a `with` statement to open the file specified by `file_path`. - If the file does not exist, raise a `FileNotFoundError`. 2. **Processing the File:** - Read all lines from the file. - Each line is expected to contain an integer. - If a line does not contain a valid integer, raise a `ValueError` with the message `\\"Invalid integer in file\\"`. 3. **Aggregating Results:** - Sum all the integers in the file. - If the file is empty, raise a custom `EmptyFileError` (defined by you) with the message `\\"File is empty\\"`. 4. **Exception Handling:** - Handle the `FileNotFoundError` by printing `\\"Error: File not found\\"` to the console. - Handle the `ValueError` by printing the exception message to the console. - Ensure that the file is always properly closed regardless of whether an exception occurs using appropriate clean-up mechanisms. 5. **Returning the Result:** - Return the sum of the integers in the file if no exceptions occur. # Constraints: - Do not use external libraries except those provided by the standard Python library. - Assume the file contains at most 1000 lines. # Example Usage: ```python try: result = process_file(\\"path/to/file.txt\\") print(\\"Sum:\\", result) except EmptyFileError as e: print(e) ``` # Custom Exception Class: Define the custom exception class `EmptyFileError` derived from the built-in `Exception` class as follows: ```python class EmptyFileError(Exception): pass ``` Implement the function and the custom exception class in a single Python script.","solution":"class EmptyFileError(Exception): pass def process_file(file_path: str) -> int: try: with open(file_path, \'r\') as file: lines = file.readlines() if not lines: raise EmptyFileError(\\"File is empty\\") total = 0 for line in lines: try: total += int(line.strip()) except ValueError: raise ValueError(\\"Invalid integer in file\\") return total except FileNotFoundError: print(\\"Error: File not found\\") raise except ValueError as e: print(e) raise"},{"question":"Objective Demonstrate your understanding of Python\'s `token` module by implementing a function that analyzes a list of token values from a Python source code snippet. Problem Statement Write a function `analyze_tokens(token_values: List[int]) -> Dict[str, int]` that takes a list of token values as input and returns a dictionary summarizing the count of each type of token (both terminal and non-terminal). The dictionary keys should be the human-readable token names, and the values should be the counts of those tokens in the input list. Input - `token_values`: A list of integers, each representing a token value in the Python language grammar. Output - A dictionary mapping token names (as strings) to their counts (as integers). Constraints - All integers in `token_values` are guaranteed to be valid token constants as defined in the `token` module. - The list `token_values` can have a length between `1` and `10^6`. Example ```python from token import tok_name # Example token values corresponding to the sequence: \\"def ( ) :\\" token_values = [1, 7, 8, 9, 10, 11, 12, 55] assert analyze_tokens(token_values) == { \'NAME\': 1, \'NEWLINE\': 1, \'INDENT\': 1, \'DEDENT\': 1, \'LPAR\': 1, \'RPAR\': 1, \'COLON\': 1, \'ENDMARKER\': 1 } ``` Notes - Utilize the `token.tok_name` dictionary to convert numeric token values to their human-readable string representations. - Efficiently handle the input list, ensuring the function performs well even for the maximum input size. Performance Requirements - The function should execute in `O(n)` time complexity, where `n` is the number of elements in `token_values`.","solution":"import token def analyze_tokens(token_values): Analyzes a list of token values and returns a dictionary summarizing the count of each type of token. :param token_values: List[int] - A list of integers, each representing a token value. :return: Dict[str, int] - A dictionary mapping token names (strings) to their counts (integers). token_counts = {} for tok_val in token_values: tok_name = token.tok_name[tok_val] if tok_name in token_counts: token_counts[tok_name] += 1 else: token_counts[tok_name] = 1 return token_counts"},{"question":"# Custom Exception Handling in Python You are tasked with implementing a custom library for file operations that raises user-defined exceptions in certain conditions. You need to create custom exceptions, inherit from appropriate built-in exception classes, and demonstrate their use. Task Details 1. **Define Custom Exceptions** - `InvalidFileTypeError`: - Inherits from `ValueError`. - Should have an additional attribute `file_type` to store the type of file that was found invalid. - `FileTooLargeError`: - Inherits from `OSError`. - Should have an additional attribute `file_size` to store the size of the file that was found too large. 2. **Implement the function `process_file`** - Parameters: - `file_path`: A string representing the path of the file. - Behavior: - Check if the file is a `.txt` file. If not, raise `InvalidFileTypeError`. - Check if the file size is larger than 1MB. If so, raise `FileTooLargeError`. - If the file is valid, return the contents of the file. - Usage of exceptions: - Demonstrate how to handle these custom exceptions by printing a user-friendly message. Example Usage ```python try: content = process_file(\\"example.txt\\") print(content) except InvalidFileTypeError as e: print(f\\"Error: Invalid file type `{e.file_type}`. Only `.txt` files are allowed.\\") except FileTooLargeError as e: print(f\\"Error: The file size is {e.file_size} bytes, which exceeds the 1MB limit.\\") try: content = process_file(\\"large_file.pdf\\") print(content) except InvalidFileTypeError as e: print(f\\"Error: Invalid file type `{e.file_type}`. Only `.txt` files are allowed.\\") except FileTooLargeError as e: print(f\\"Error: The file size is {e.file_size} bytes, which exceeds the 1MB limit.\\") ``` Function and Class Definitions ```python # Define your custom exceptions here class InvalidFileTypeError(ValueError): def __init__(self, message, file_type): super().__init__(message) self.file_type = file_type class FileTooLargeError(OSError): def __init__(self, message, file_size): super().__init__(message) self.file_size = file_size def process_file(file_path): import os if not file_path.endswith(\\".txt\\"): raise InvalidFileTypeError(\\"Invalid file type.\\", os.path.splitext(file_path)[-1]) if os.path.getsize(file_path) > 1024 * 1024: raise FileTooLargeError(\\"File is too large.\\", os.path.getsize(file_path)) with open(file_path, \'r\') as file: content = file.read() return content ``` Constraints - Ensure the file exists before performing operations on it. - The implementation should handle other potential file errors gracefully, such as `FileNotFoundError`, `PermissionError`, etc. Assessment Criteria - Correctly defined custom exceptions with appropriate inheritance. - Accurate implementation of the function `process_file`. - Proper use and demonstration of exception handling. - Clear, readable, and maintainable code.","solution":"class InvalidFileTypeError(ValueError): def __init__(self, message, file_type): super().__init__(message) self.file_type = file_type class FileTooLargeError(OSError): def __init__(self, message, file_size): super().__init__(message) self.file_size = file_size def process_file(file_path): import os if not file_path.endswith(\\".txt\\"): raise InvalidFileTypeError(\\"Invalid file type.\\", os.path.splitext(file_path)[-1]) try: file_size = os.path.getsize(file_path) except FileNotFoundError: raise FileNotFoundError(f\\"The file at {file_path} does not exist.\\") except PermissionError: raise PermissionError(f\\"Permission denied for file at {file_path}.\\") if file_size > 1024 * 1024: raise FileTooLargeError(\\"File is too large.\\", file_size) try: with open(file_path, \'r\') as file: content = file.read() except FileNotFoundError: raise FileNotFoundError(f\\"The file at {file_path} does not exist.\\") except PermissionError: raise PermissionError(f\\"Permission denied for file at {file_path}.\\") return content"},{"question":"Objective: Implement a function that fetches and processes user account information using the Unix `pwd` database. Your solution should demonstrate your understanding of the `pwd` module\'s interface and ability to work with password database entries. Task: Write a function named `fetch_user_info` that takes either a username (string) or user ID (integer) as an input and returns a dictionary containing the following information for the user: - `login_name`: Login name - `user_id`: Numerical user ID - `group_id`: Numerical group ID - `full_name`: User name or comment field - `home_directory`: User home directory - `shell`: User command interpreter If the input is a username, the function should fetch the user\'s information using `pwd.getpwnam()`. If the input is a user ID, it should use `pwd.getpwuid()`. If the user cannot be found, the function should raise a `KeyError`. Input: - A single argument which can be either a username (string) or a user ID (integer). Output: - A dictionary containing the user\'s information with the specified keys. Constraints: - The function should handle both username (string) and user ID (integer) inputs correctly. - If the input is not found in the database, a `KeyError` should be raised. Example: ```python import pwd def fetch_user_info(user): if isinstance(user, int): entry = pwd.getpwuid(user) elif isinstance(user, str): entry = pwd.getpwnam(user) else: raise ValueError(\\"Input must be either a username (string) or a user ID (integer).\\") user_info = { \\"login_name\\": entry.pw_name, \\"user_id\\": entry.pw_uid, \\"group_id\\": entry.pw_gid, \\"full_name\\": entry.pw_gecos, \\"home_directory\\": entry.pw_dir, \\"shell\\": entry.pw_shell, } return user_info # Example usage: # fetch_user_info(\\"root\\") -> returns a dictionary with root user information # fetch_user_info(0) -> returns a dictionary with root user information ``` Note: Make sure to handle the cases where the username or user ID does not exist in the password database.","solution":"import pwd def fetch_user_info(user): Fetches and returns user account information as a dictionary. Parameters: user (str|int): Username (string) or user ID (integer). Returns: dict: A dictionary containing user information. Raises: KeyError: If the user cannot be found in the password database. ValueError: If the input type is not correct. try: if isinstance(user, int): entry = pwd.getpwuid(user) elif isinstance(user, str): entry = pwd.getpwnam(user) else: raise ValueError(\\"Input must be either a username (string) or a user ID (integer).\\") except KeyError: raise KeyError(f\\"User: {user} not found in the password database.\\") user_info = { \\"login_name\\": entry.pw_name, \\"user_id\\": entry.pw_uid, \\"group_id\\": entry.pw_gid, \\"full_name\\": entry.pw_gecos, \\"home_directory\\": entry.pw_dir, \\"shell\\": entry.pw_shell, } return user_info"},{"question":"# MemoryView Object Manipulation In this task, you are required to implement three Python functions that interface with features described in the `memoryview` object documentation. Each function utilizes the memoryview objects in different ways as specified below. Part 1: Creating a Readable MemoryView Write a function `create_readable_memoryview(data: bytes) -> memoryview` that takes a `bytes` object as input and returns a memoryview of this object. The memoryview should be read-only. Part 2: Creating a Writable MemoryView from Raw Memory Write a function `create_writable_memoryview(size: int) -> memoryview` that allocates a block of memory of specified `size` and returns a writable memoryview. Part 3: Copying Contiguous Memory Write a function `copy_contiguous_memory(src: memoryview, order: str) -> memoryview` that takes a memoryview `src` and a character `order` (either \'C\' for C-style contiguous or \'F\' for Fortran-style contiguous). The function should return a new memoryview pointing to a contiguous chunk of memory with the specified order. # Constraints - Ensure that the returned memoryviews are of the specified type (read-only or writable). - For the third function, simulate the behavior of having non-contiguous memory, if necessary, to enforce the creation of a contiguous copy. # Example Usage ```python # Part 1 data = b\\"example data\\" readonly_memview = create_readable_memoryview(data) try: readonly_memview[0] = 120 # This should raise an exception as the memoryview is read-only except TypeError: print(\\"Read-only memoryview\\") # Part 2 writable_memview = create_writable_memoryview(10) writable_memview[0] = 120 # This should work as the memoryview is writable # Part 3 src = memoryview(bytearray(b\\"1234\\")) contiguous_memview = copy_contiguous_memory(src, \'C\') print(bytes(contiguous_memview)) # Expected output should be b\\"1234\\" ``` # Notes - You may assume that the order provided in the third function will always be \'C\' or \'F\'. - You are not required to handle exceptions outside the constraints specified.","solution":"def create_readable_memoryview(data: bytes) -> memoryview: Returns a read-only memoryview of the provided bytes data. return memoryview(data).toreadonly() def create_writable_memoryview(size: int) -> memoryview: Allocates a block of memory of the specified size and returns a writable memoryview. return memoryview(bytearray(size)) def copy_contiguous_memory(src: memoryview, order: str) -> memoryview: Returns a new memoryview pointing to a contiguous chunk of memory with the specified order (\'C\' or \'F\'). if order == \'C\': return memoryview(bytes(src)) elif order == \'F\': return memoryview(bytearray(src.tobytes())) else: raise ValueError(\\"Order must be either \'C\' or \'F\'.\\") # Example usage demonstration data = b\\"example data\\" readonly_memview = create_readable_memoryview(data) try: readonly_memview[0] = 120 # This should raise an exception as the memoryview is read-only except TypeError: print(\\"Read-only memoryview\\") writable_memview = create_writable_memoryview(10) writable_memview[0] = 120 # This should work as the memoryview is writable src = memoryview(bytearray(b\\"1234\\")) contiguous_memview = copy_contiguous_memory(src, \'C\') print(bytes(contiguous_memview)) # Expected output should be b\\"1234\\""},{"question":"# Question Using the seaborn library, write a function named `customized_plot` that generates a series of sine wave plots with different aesthetic customizations. Your function should: 1. Accept the following parameters: - `n` (int): The number of sine waves to plot. Default is 10. - `theme` (str): The seaborn theme to use. Options are \'darkgrid\', \'whitegrid\', \'dark\', \'white\', and \'ticks\'. Default is \'darkgrid\'. - `style` (str): The seaborn style to use. Options are \'darkgrid\', \'whitegrid\', \'dark\', \'white\', and \'ticks\'. Default is \'darkgrid\'. - `context` (str): The seaborn context to scale the plot elements. Options are \'paper\', \'notebook\', \'talk\', and \'poster\'. Default is \'notebook\'. - `remove_spines` (bool): Whether to remove the top and right spines from the plot. Default is False. - `despine_params` (dict): Additional parameters for the `despine` function, such as `offset` and `trim`. Default is an empty dictionary `{}`. 2. Implement the function to: - Set the seaborn theme, style, and context based on the parameters provided. - Generate the sine wave plot using the `sinplot` function defined below. - If `remove_spines` is True, remove the top and right spines using the `despine` function and apply any additional `despine_params`. 3. Return the generated plot. `sinplot` function: ```python import numpy as np import matplotlib.pyplot as plt def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * 0.5) * (n + 2 - i) * flip) ``` Example Usage: ```python customized_plot(n=8, theme=\'whitegrid\', style=\'ticks\', context=\'talk\', remove_spines=True, despine_params={\'offset\': 10, \'trim\': True}) ``` # Constraints - Ensure that all plot customizations are appropriately set and applied. - Use appropriate default values for each parameter to provide a predefined behavior. # Expected Output A Matplotlib plot with the specified number of sine waves, customized according to the specified theme, style, context, and spine adjustments.","solution":"import seaborn as sns import numpy as np import matplotlib.pyplot as plt def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * 0.5) * (n + 2 - i) * flip) def customized_plot(n=10, theme=\'darkgrid\', style=\'darkgrid\', context=\'notebook\', remove_spines=False, despine_params={}): sns.set_theme(style=theme, context=context) sns.set_style(style) sinplot(n) if remove_spines: sns.despine(**despine_params) plt.show()"},{"question":"You are provided with a dataset in the form of a pandas DataFrame. The dataset captures sales information for a store over a period. It includes the following columns: 1. `date`: The date of the sale. 2. `product`: The product sold. 3. `quantity`: The quantity of the product sold. 4. `price`: The price of one unit of the product. 5. `revenue`: The total revenue from that sale (quantity * price). 6. `category`: The category to which the product belongs. The task involves several steps to analyze this data. Implement the following functions using pandas: # Function 1: Load Dataset ```python def load_dataset(file_path): Load a dataset from a given CSV file into a pandas DataFrame. Parameters: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame. ``` # Function 2: Data Cleaning ```python def clean_data(df): Perform data cleaning on the DataFrame. Specifically: - Ensure that there are no missing values in `date`, `product`, `quantity`, and `price` columns. - If there are missing values in the `revenue` column, fill them using `quantity * price`. Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The cleaned DataFrame. ``` # Function 3: Add New Columns ```python def add_columns(df): Add the following new columns to the DataFrame: - \'year\': Extract the year from the \'date\' column. - \'month\': Extract the month from the \'date\' column. - \'day\': Extract the day from the \'date\' column. Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The DataFrame with new columns added. ``` # Function 4: Monthly Sales Summary ```python def monthly_sales_summary(df): Group the data by \'year\' and \'month\' and calculate the following statistics for each group: - Total revenue - Total quantity sold - Number of sales (number of rows) Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: A DataFrame with the summary statistics. ``` # Function 5: Top Selling Products ```python def top_selling_products(df, n=5): Identify the top N selling products based on total revenue. Parameters: df (pd.DataFrame): The input DataFrame. n (int): The number of top products to return. Default is 5. Returns: pd.DataFrame: A DataFrame with columns \'product\' and \'total_revenue\' for the top N products. ``` # Function 6: Export Summary ```python def export_summary(df, file_path): Export the summary DataFrame to a CSV file. Parameters: df (pd.DataFrame): The input DataFrame. file_path (str): The path to the CSV file where the summary will be saved. ``` # Example Usage ```python if __name__ == \\"__main__\\": dataset_path = \'sales_data.csv\' output_path = \'sales_summary.csv\' df = load_dataset(dataset_path) df = clean_data(df) df = add_columns(df) summary_df = monthly_sales_summary(df) top_products_df = top_selling_products(df) print(\\"Monthly Sales Summary:\\") print(summary_df) print(\\"nTop Selling Products:\\") print(top_products_df) export_summary(summary_df, output_path) ``` **Constraints** 1. Assume the provided CSV file is in a correct and readable format. 2. Do not change the structure of the dataset apart from the specified modifications. 3. Handle any potential exceptions that might occur during file loading or exporting. **Performance Requirements** - Efficiently handle dataframes with up to 100,000 rows.","solution":"import pandas as pd def load_dataset(file_path): Load a dataset from a given CSV file into a pandas DataFrame. Parameters: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame. return pd.read_csv(file_path) def clean_data(df): Perform data cleaning on the DataFrame. Specifically: - Ensure that there are no missing values in `date`, `product`, `quantity`, and `price` columns. - If there are missing values in the `revenue` column, fill them using `quantity * price`. Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The cleaned DataFrame. df.dropna(subset=[\'date\', \'product\', \'quantity\', \'price\'], inplace=True) df[\'revenue\'].fillna(df[\'quantity\'] * df[\'price\'], inplace=True) return df def add_columns(df): Add the following new columns to the DataFrame: - \'year\': Extract the year from the \'date\' column. - \'month\': Extract the month from the \'date\' column. - \'day\': Extract the day from the \'date\' column. Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The DataFrame with new columns added. df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'year\'] = df[\'date\'].dt.year df[\'month\'] = df[\'date\'].dt.month df[\'day\'] = df[\'date\'].dt.day return df def monthly_sales_summary(df): Group the data by \'year\' and \'month\' and calculate the following statistics for each group: - Total revenue - Total quantity sold - Number of sales (number of rows) Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: A DataFrame with the summary statistics. summary = df.groupby([\'year\', \'month\']).agg(total_revenue=(\'revenue\', \'sum\'), total_quantity=(\'quantity\', \'sum\'), number_of_sales=(\'date\', \'count\')).reset_index() return summary def top_selling_products(df, n=5): Identify the top N selling products based on total revenue. Parameters: df (pd.DataFrame): The input DataFrame. n (int): The number of top products to return. Default is 5. Returns: pd.DataFrame: A DataFrame with columns \'product\' and \'total_revenue\' for the top N products. top_products = df.groupby(\'product\').agg(total_revenue=(\'revenue\', \'sum\')).nlargest(n, \'total_revenue\').reset_index() return top_products def export_summary(df, file_path): Export the summary DataFrame to a CSV file. Parameters: df (pd.DataFrame): The input DataFrame. file_path (str): The path to the CSV file where the summary will be saved. df.to_csv(file_path, index=False)"},{"question":"**Title:** Custom URL Opener with Advanced Handlers **Background:** The `urllib.request` module provides a high-level interface for fetching data across the web. It includes several classes to handle specific functionalities such as opening different types of URLs, managing cookies, handling authentication, and dealing with redirects. In this assessment, you will create a custom URL opener that leverages these capabilities. **Task:** Implement a custom URL opener class `CustomURLOpener` that extends `urllib.request.FancyURLopener`. Your class should: 1. Handle HTTP and HTTPS URLs. 2. Manage cookies using `HTTPCookieProcessor`. 3. Handle HTTP basic and digest authentication. 4. Follow redirects up to a maximum of 5 redirections. 5. Log all HTTP requests and responses to a file. **Specifications:** 1. **Inputs:** - `url` (string): The URL to be fetched. - `auth_info` (tuple, optional): A tuple `(username, password)` for HTTP basic or digest authentication. - `log_file_path` (string, optional): Path to the file where logs should be written. 2. **Outputs:** - Returns the fetched content as a string. - Raises an appropriate exception if the URL cannot be opened. 3. **Constraints:** - You must use built-in Python libraries (`urllib.request` and `http.cookiejar`). - The solution must handle redirects correctly and stop after 5 redirects. - Logging should capture all request headers, response headers, and HTTP status codes. 4. **Performance Requirements:** - The solution should handle large web pages efficiently. - The program should handle network errors gracefully and retry failed requests up to 3 times before failing. **Example Usage:** ```python opener = CustomURLOpener() content = opener.fetch( url=\\"http://example.com\\", auth_info=(\\"user\\", \\"pass\\"), log_file_path=\\"http_logs.txt\\" ) print(content) ``` **Starter Code:** ```python import urllib.request import http.cookiejar import logging class CustomURLOpener(urllib.request.FancyURLopener): def __init__(self): super().__init__() self.cookie_jar = http.cookiejar.CookieJar() self.opener = urllib.request.build_opener( urllib.request.HTTPCookieProcessor(self.cookie_jar) ) self.max_redirects = 5 def fetch(self, url, auth_info=None, log_file_path=None): if log_file_path: logging.basicConfig(filename=log_file_path, level=logging.INFO) if auth_info: # Code to handle authentication # Code to handle URL fetch with redirection and logging return content # Continue implementing the methods and additional functionalities ``` **Evaluation Criteria:** - Correctness: The implementation must fulfill all the requirements. - Error Handling: Appropriate handling of exceptions and network errors. - Code Quality: Code should be clean, well-documented, and follow Python conventions. - Efficiency: Efficient fetching and handling of web content.","solution":"import urllib.request import http.cookiejar import logging from urllib.error import URLError, HTTPError import time class CustomURLOpener(urllib.request.FancyURLopener): def __init__(self): super().__init__() self.cookie_jar = http.cookiejar.CookieJar() self.opener = urllib.request.build_opener( urllib.request.HTTPCookieProcessor(self.cookie_jar) ) self.max_redirects = 5 self.retry_limit = 3 def fetch(self, url, auth_info=None, log_file_path=None): if log_file_path: logging.basicConfig(filename=log_file_path, level=logging.INFO) if auth_info: username, password = auth_info password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) auth_handler = urllib.request.HTTPBasicAuthHandler(password_mgr) auth_handler = urllib.request.HTTPDigestAuthHandler(password_mgr) self.opener = urllib.request.build_opener( urllib.request.HTTPCookieProcessor(self.cookie_jar), auth_handler ) redirects = 0 retries = 0 while retries < self.retry_limit: try: response = self.opener.open(url) content = response.read().decode(\'utf-8\') self._log_request(response) return content except HTTPError as e: if e.code in [301, 302, 303, 307, 308]: if redirects < self.max_redirects: url = e.headers[\'Location\'] redirects += 1 continue else: raise else: logging.error(f\\"HTTPError: {e.code} - {e.reason}\\") raise except URLError as e: logging.error(f\\"URLError: {e.reason}\\") retries += 1 time.sleep(1) if retries == self.retry_limit: raise def _log_request(self, response): headers = response.info() request_details = f\\"Request URL: {response.geturl()}n\\" request_details += f\\"Status code: {response.getcode()}n\\" request_details += \\"Headers:n\\" for header, value in headers.items(): request_details += f\\"{header}: {value}n\\" logging.info(request_details)"},{"question":"**Title:** Custom Nested Configuration Management with `ChainMap` **Objective:** Demonstrate your understanding of the `ChainMap` container type from the `collections` module by implementing a nested configuration management system that supports hierarchical overrides. **Problem Statement:** You are tasked with creating a class `ConfigManager` that helps manage configurations in a nested manner using the `ChainMap` class. Your class should provide functionality to handle default configurations, environment-specific configurations, and user-specific configurations, all of which can be dynamically updated and queried. Requirements: 1. **Initialization:** The `ConfigManager` should be initialized with three optional dictionaries: `defaults`, `environment`, and `user`. These dictionaries represent default settings, environment-specific settings, and user-specific settings, respectively. 2. **Getter Method:** Implement a method `get(self, key)` that retrieves the value associated with `key`. The lookup priority should be: `user`, `environment`, then `defaults`. If the key does not exist in any dictionary, return `None`. 3. **Setter Method:** Implement a method `set(self, key, value, level=\'user\')` that sets the value for `key` in the specified `level`. The `level` parameter can be `\'user\'`, `\'environment\'`, or `\'defaults\'`. If a key is set in the `user` level, it should only update the `user` dictionary and so forth. 4. **Reset Method:** Implement a method `reset(self, key, level=None)` that deletes the key from the specified level. If `level` is `None`, the key should be removed from all levels (if it exists). 5. **Merge Method:** Implement a method `merge(self, other_config, level=\'user\')` that merges another dictionary `other_config` into the specified level. Example Usage: ```python # Initializing the ConfigManager defaults = {\'theme\': \'light\', \'language\': \'en\'} env = {\'language\': \'fr\'} user = {\'theme\': \'dark\'} config = ConfigManager(defaults, env, user) # Getting configuration values assert config.get(\'theme\') == \'dark\' assert config.get(\'language\') == \'fr\' assert config.get(\'timezone\') is None # Setting configuration values config.set(\'timezone\', \'GMT\', level=\'environment\') assert config.get(\'timezone\') == \'GMT\' # Resetting configuration values config.reset(\'timezone\', level=\'environment\') assert config.get(\'timezone\') is None # Merging configuration values config.merge({\'font_size\': \'12pt\'}, level=\'defaults\') assert config.get(\'font_size\') == \'12pt\' ``` Constraints: - Each dictionary can have no more than 1000 keys. - Keys and values are strings. Performance: - Ensure that all operations (`get`, `set`, `reset`, and `merge`) are efficiently implemented to handle large configurations. Implementation Details: - Use the `ChainMap` class from the `collections` module to manage the nested configurations. - It is important to maintain logical separation between the different levels of configuration. **Starter Code:** ```python from collections import ChainMap class ConfigManager: def __init__(self, defaults=None, environment=None, user=None): # Initialize the dictionaries if they are None self.defaults = defaults if defaults is not None else {} self.environment = environment if environment is not None else {} self.user = user if user is not None else {} self.configs = ChainMap(self.user, self.environment, self.defaults) def get(self, key): # Return the value associated with the key if it exists, otherwise return None pass def set(self, key, value, level=\'user\'): # Set the value for the key in the specified level pass def reset(self, key, level=None): # Delete the key from the specified level or from all levels if level is None pass def merge(self, other_config, level=\'user\'): # Merge the other_config dictionary into the specified level pass # Example testing code if __name__ == \\"__main__\\": defaults = {\'theme\': \'light\', \'language\': \'en\'} env = {\'language\': \'fr\'} user = {\'theme\': \'dark\'} config = ConfigManager(defaults, env, user) # Add more test cases as necessary ``` Implement the `ConfigManager` class by filling in the methods as described. Be sure to test your implementation thoroughly.","solution":"from collections import ChainMap class ConfigManager: def __init__(self, defaults=None, environment=None, user=None): # Initialize the dictionaries if they are None self.defaults = defaults if defaults is not None else {} self.environment = environment if environment is not None else {} self.user = user if user is not None else {} self.configs = ChainMap(self.user, self.environment, self.defaults) def get(self, key): # Return the value associated with the key if it exists, otherwise return None return self.configs.get(key, None) def set(self, key, value, level=\'user\'): # Set the value for the key in the specified level if level == \'user\': self.user[key] = value elif level == \'environment\': self.environment[key] = value elif level == \'defaults\': self.defaults[key] = value def reset(self, key, level=None): # Delete the key from the specified level or from all levels if level is None if level is None: self.user.pop(key, None) self.environment.pop(key, None) self.defaults.pop(key, None) else: if level == \'user\': self.user.pop(key, None) elif level == \'environment\': self.environment.pop(key, None) elif level == \'defaults\': self.defaults.pop(key, None) def merge(self, other_config, level=\'user\'): # Merge the other_config dictionary into the specified level if level == \'user\': self.user.update(other_config) elif level == \'environment\': self.environment.update(other_config) elif level == \'defaults\': self.defaults.update(other_config)"},{"question":"# Custom Event Loop Policy using Asyncio **Background:** An event loop policy is responsible for managing the event loop in Python\'s `asyncio` module. It allows different implementations to be used based on requirements. The default policy can be replaced or customized by subclassing `DefaultEventLoopPolicy`. **Objective:** You are required to create a custom event loop policy by subclassing `asyncio.DefaultEventLoopPolicy` and overriding necessary methods to implement custom behavior. This custom policy must implement the following: 1. **get_event_loop:** This method should retrieve the current event loop. If there is no event loop present, it should create and set a new event loop, and then return it. 2. **set_event_loop:** This method should set the current event loop for the context. 3. **new_event_loop:** This method should create and return a custom event loop. **Requirements:** - Implement a class `MyEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. - Override the `get_event_loop`, `set_event_loop`, and `new_event_loop` methods to add custom functionality. - Ensure the new event loop is created using `new_event_loop` if no current loop is set in `get_event_loop`. **Constraints:** - Use the `asyncio` module. - The `set_event_loop` method should use the `super()` function to call the base implementation and then print a message indicating that the loop was set. - The `new_event_loop` method should create a new event loop of type `asyncio.SelectorEventLoop`. # Example Usage ```python import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() if loop is None: loop = self.new_event_loop() self.set_event_loop(loop) return loop def set_event_loop(self, loop): super().set_event_loop(loop) print(f\\"Custom event loop set to {loop}\\") def new_event_loop(self): return asyncio.SelectorEventLoop() # Apply the custom policy asyncio.set_event_loop_policy(MyEventLoopPolicy()) # Retrieve the event loop to see the custom behavior loop = asyncio.get_event_loop() print(f\\"Current event loop: {loop}\\") ``` **Output:** ``` Custom event loop set to <SelectorEventLoop> Current event loop: <SelectorEventLoop> ``` **Input:** - No input is required for this question. **Output:** - The result of setting and getting the custom event loop will be printed as described in the example. **Note:** Ensure that you handle the case where a loop is already present and only create and set a new loop when necessary.","solution":"import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): try: loop = super().get_event_loop() except RuntimeError: loop = self.new_event_loop() self.set_event_loop(loop) return loop def set_event_loop(self, loop): super().set_event_loop(loop) print(f\\"Custom event loop set to {loop}\\") def new_event_loop(self): return asyncio.SelectorEventLoop() # Apply the custom policy asyncio.set_event_loop_policy(MyEventLoopPolicy()) # Function to demonstrate usage def demonstrate_get_event_loop(): loop = asyncio.get_event_loop() print(f\\"Current event loop: {loop}\\") # Call the demonstration function if __name__ == \'__main__\': demonstrate_get_event_loop()"},{"question":"**Problem Statement:** Suppose you are developing a command-line tool that assists users in automatically opening multiple URLs in their preferred browsers. You will implement a function called `batch_open_urls(urls, browser_type=None)` that takes a list of URLs and an optional browser type and opens each URL in a new tab. Your task is to define this function according to the following requirements: 1. The function must accept two arguments: - `urls`: A list of valid URLs (strings) to be opened. - `browser_type`: An optional string to specify a particular browser type (e.g., \\"firefox\\", \\"chrome\\") to be used for opening the URLs. If not provided, the default browser is used. 2. The function should attempt to open each URL in a new tab of the specified browser. 3. If the specified browser type is not available or in case of any error, raise a `webbrowser.Error` with an appropriate message. Your solution should handle possible exceptions and ensure that the user receives meaningful feedback if something goes wrong. **Function Signature**: ```python import webbrowser def batch_open_urls(urls, browser_type=None): pass ``` **Examples**: ```python # Example 1: Open URLs using the default browser urls = [ \\"https://www.google.com\\", \\"https://www.github.com\\" ] batch_open_urls(urls) # Example 2: Open URLs using Firefox urls = [ \\"https://www.google.com\\", \\"https://www.github.com\\" ] browser_type = \\"firefox\\" batch_open_urls(urls, browser_type) # Example 3: Handle error case when the browser type is not available urls = [ \\"https://www.python.org\\", \\"https://www.stackoverflow.com\\" ] browser_type = \\"nonexistent-browser\\" batch_open_urls(urls, browser_type) # Should raise webbrowser.Error with a message indicating the browser type is not available. ``` **Constraints**: - Each URL in the `urls` list is correctly formatted. - The optional `browser_type` argument, when provided, matches one of the predefined types. - Ensure all opened tabs are handled asynchronously where possible.","solution":"import webbrowser def batch_open_urls(urls, browser_type=None): Open a batch of URLs in a specified browser or the default browser. Args: urls (list): A list of URLs to be opened. browser_type (str): Optional. The type of browser to use (e.g., \\"firefox\\", \\"chrome\\"). Raises: webbrowser.Error: If the specified browser type is not available or any other error occurs. try: if browser_type: browser = webbrowser.get(using=browser_type) else: browser = webbrowser.get() for url in urls: browser.open_new_tab(url) except Exception as e: raise webbrowser.Error(f\\"Failed to open URLs: {e}\\")"},{"question":"You are required to implement a custom function to copy a directory tree with additional features such as selective copying based on file patterns and handling symbolic links in a specific manner. The function should leverage the `shutil` module\'s capabilities while ensuring robust error handling and efficient file operations. # Function Specification **Function Name**: `custom_copytree` **Parameters**: - `src` (str): Source directory path. - `dst` (str): Destination directory path. - `ignore_patterns` (Optional[List[str]]): List of glob-style patterns to ignore while copying (default is `None`). - `follow_symlinks` (bool): Whether to follow symbolic links or not (default is `True`). **Returns**: - `str`: The path of the destination directory. **Behavior**: - Recursively copy the contents of the directory from `src` to `dst`. - If `ignore_patterns` is provided, ignore files and directories matching these patterns. - If `follow_symlinks` is `True`, copy the contents of the files pointed to by the symbolic links, otherwise, recreate the symbolic links in the destination. - Preserve file metadata using `copy2` if `follow_symlinks` is `True`; otherwise, use `copy` to only copy file contents. - Raise an appropriate exception if the operation fails due to reasons like `OSError`, permission issues, or symlink problems that cannot be handled. # Example Usage ```python import os src = \\"/path/to/source\\" dst = \\"/path/to/destination\\" ignore_patterns = [\\"*.tmp\\", \\"*.log\\"] follow_symlinks = False try: result = custom_copytree(src, dst, ignore_patterns, follow_symlinks) print(f\\"Directory successfully copied to: {result}\\") except Exception as e: print(f\\"Failed to copy directory: {e}\\") ``` # Constraints - The source directory (`src`) must exist and be readable. - The destination directory (`dst`) should not pre-exist unless it\'s empty. - Efficient use of memory and processing time is required (avoid reading entire files into memory at once). Implement the `custom_copytree` function below: ```python import shutil import os from typing import List, Optional def custom_copytree(src: str, dst: str, ignore_patterns: Optional[List[str]] = None, follow_symlinks: bool = True) -> str: # Function to create an ignore function based on patterns ignore = shutil.ignore_patterns(*ignore_patterns) if ignore_patterns else None # Custom copy function to handle symbolic links def _custom_copy_function(src, dst): if follow_symlinks: shutil.copy2(src, dst) else: shutil.copy(src, dst) # Copying the directory tree shutil.copytree( src, dst, ignore=ignore, copy_function=_custom_copy_function, symlinks=not follow_symlinks ) return dst # Example usage (uncomment to run) # src = \\"/path/to/source\\" # dst = \\"/path/to/destination\\" # ignore_patterns = [\\"*.tmp\\", \\"*.log\\"] # follow_symlinks = False # result = custom_copytree(src, dst, ignore_patterns, follow_symlinks) # print(f\\"Directory successfully copied to: {result}\\") ``` # Notes - Ensure the function handles exceptions gracefully and provides meaningful error messages. - The function should be tested with various files and directory structures, including symbolic links, to confirm its correctness.","solution":"import shutil import os from typing import List, Optional def custom_copytree(src: str, dst: str, ignore_patterns: Optional[List[str]] = None, follow_symlinks: bool = True) -> str: # Function to create an ignore function based on patterns ignore = shutil.ignore_patterns(*ignore_patterns) if ignore_patterns else None # Custom copy function to handle symbolic links def _custom_copy_function(src, dst): if follow_symlinks: shutil.copy2(src, dst) else: shutil.copy(src, dst) # Copying the directory tree shutil.copytree( src, dst, ignore=ignore, copy_function=_custom_copy_function, symlinks=not follow_symlinks ) return dst"},{"question":"Shared Memory Synchronization for Parallel Matrix Multiplication You are tasked to implement a parallel matrix multiplication using the `multiprocessing.shared_memory` module. Specifically, you will: 1. Create shared memory blocks to hold the input matrices and the output matrix. 2. Use multiple processes to fill these shared blocks and perform the matrix multiplication in parallel. 3. Ensure proper synchronization and cleanup of resources. # Function Signature ```python def parallel_matrix_multiplication(A: List[List[int]], B: List[List[int]], num_workers: int) -> List[List[int]]: Multiplies two matrices A and B in parallel using shared memory and the specified number of workers. Args: A (List[List[int]]): The first input matrix. B (List[List[int]]): The second input matrix. num_workers (int): The number of parallel workers to use for computation. Returns: List[List[int]]: The resulting matrix multiplication of A and B. # Input - `A`: List of lists of integers representing a matrix of size `m x k`. - `B`: List of lists of integers representing a matrix of size `k x n`. - `num_workers`: Integer, number of parallel processes to employ for computation. # Output - Returns a list of lists of integers representing the resulting matrix of size `m x n` from the multiplication of A and B. # Constraints - Both matrices `A` and `B` consist of non-empty lists. - The number of columns in `A` should match the number of rows in `B`. - `1 <= len(A), len(B[0]), len(A[0]) <= 500` - `1 <= num_workers <= 8` - Ensure proper cleanup with `close()` and `unlink()`. # Example Input ```python A = [ [1, 2], [3, 4], [5, 6] ] B = [ [7, 8, 9], [10, 11, 12] ] num_workers = 4 ``` Output ```python [ [27, 30, 33], [61, 68, 75], [95, 106, 117] ] ``` # Implementation Notes - You should manage shared memory properly, such that once the multiplication is done, the memory should be properly cleaned. - Distributed computation should be implemented using Pythonâ€™s `multiprocessing.shared_memory`. - Utilize `SharedMemoryManager` to simplify the lifecycle management of shared memory. # Additional Guidelines - Handle the persistence and lifecycle of the shared memory between different processes. - Ensure that the results are properly aggregated from multiple workers. ```python import numpy as np from multiprocessing import shared_memory, Process, Barrier def worker(shared_A_name, shared_B_name, shared_C_name, A_shape, B_shape, num_workers, worker_id, barrier): # Attach to the shared memory blocks shm_A = shared_memory.SharedMemory(name=shared_A_name) shm_B = shared_memory.SharedMemory(name=shared_B_name) shm_C = shared_memory.SharedMemory(name=shared_C_name) # Create numpy array using the shared memory buffers A_shm = np.ndarray(A_shape, dtype=np.int, buffer=shm_A.buf) B_shm = np.ndarray(B_shape, dtype=np.int, buffer=shm_B.buf) m, n = A_shape[0], B_shape[1] C_shm = np.ndarray((m, n), dtype=np.int, buffer=shm_C.buf) # Define the range of rows for this worker to process rows_per_worker = (m + num_workers - 1) // num_workers # ceiling division start_row = worker_id * rows_per_worker end_row = min(start_row + rows_per_worker, m) # Perform the computation for the assigned rows for i in range(start_row, end_row): for j in range(n): C_shm[i, j] = np.dot(A_shm[i], B_shm[:, j]) # Synchronize with other workers barrier.wait() # Cleanup shared memory shm_A.close() shm_B.close() shm_C.close() def parallel_matrix_multiplication(A, B, num_workers): import numpy as np from multiprocessing import shared_memory, Process, Barrier # Convert input matrices to numpy arrays A_np = np.array(A) B_np = np.array(B) m, k = A_np.shape _, n = B_np.shape # Allocate shared memory for input matrices and result matrix shm_A = shared_memory.SharedMemory(create=True, size=A_np.nbytes) shm_B = shared_memory.SharedMemory(create=True, size=B_np.nbytes) shm_C = shared_memory.SharedMemory(create=True, size=m * n * 4) # assuming int32 entries # Create numpy arrays backed by the shared memory A_shm = np.ndarray(A_np.shape, dtype=np.int, buffer=shm_A.buf) B_shm = np.ndarray(B_np.shape, dtype=np.int, buffer=shm_B.buf) C_shm = np.ndarray((m, n), dtype=np.int, buffer=shm_C.buf) # Copy data to shared memory np.copyto(A_shm, A_np) np.copyto(B_shm, B_np) # Create a barrier for synchronization barrier = Barrier(num_workers + 1) # Create and start worker processes processes = [] for worker_id in range(num_workers): p = Process(target=worker, args=(shm_A.name, shm_B.name, shm_C.name, A_np.shape, B_np.shape, num_workers, worker_id, barrier)) p.start() processes.append(p) # Wait for all processes to complete barrier.wait() # Join all processes for p in processes: p.join() # Collect result result = C_shm.copy() # Cleanup shared memory shm_A.close() shm_A.unlink() shm_B.close() shm_B.unlink() shm_C.close() shm_C.unlink() return result.tolist() ```","solution":"import numpy as np from multiprocessing import shared_memory, Process, Barrier def worker(shared_A_name, shared_B_name, shared_C_name, A_shape, B_shape, num_workers, worker_id, barrier): shm_A = shared_memory.SharedMemory(name=shared_A_name) shm_B = shared_memory.SharedMemory(name=shared_B_name) shm_C = shared_memory.SharedMemory(name=shared_C_name) A_shm = np.ndarray(A_shape, dtype=np.int32, buffer=shm_A.buf) B_shm = np.ndarray(B_shape, dtype=np.int32, buffer=shm_B.buf) C_shm = np.ndarray((A_shape[0], B_shape[1]), dtype=np.int32, buffer=shm_C.buf) rows_per_worker = (A_shape[0] + num_workers - 1) // num_workers start_row = worker_id * rows_per_worker end_row = min(start_row + rows_per_worker, A_shape[0]) for i in range(start_row, end_row): for j in range(B_shape[1]): C_shm[i, j] = np.dot(A_shm[i, :], B_shm[:, j]) barrier.wait() shm_A.close() shm_B.close() shm_C.close() def parallel_matrix_multiplication(A, B, num_workers): import numpy as np from multiprocessing import shared_memory, Process, Barrier A_np = np.array(A, dtype=np.int32) B_np = np.array(B, dtype=np.int32) m, k = A_np.shape k2, n = B_np.shape assert k == k2, \\"Incompatible matrix dimensions for multiplication\\" shm_A = shared_memory.SharedMemory(create=True, size=A_np.nbytes) shm_B = shared_memory.SharedMemory(create=True, size=B_np.nbytes) shm_C = shared_memory.SharedMemory(create=True, size=m * n * 4) A_shm = np.ndarray(A_np.shape, dtype=np.int32, buffer=shm_A.buf) B_shm = np.ndarray(B_np.shape, dtype=np.int32, buffer=shm_B.buf) C_shm = np.ndarray((m, n), dtype=np.int32, buffer=shm_C.buf) np.copyto(A_shm, A_np) np.copyto(B_shm, B_np) barrier = Barrier(num_workers + 1) processes = [] for worker_id in range(num_workers): p = Process(target=worker, args=(shm_A.name, shm_B.name, shm_C.name, A_np.shape, B_np.shape, num_workers, worker_id, barrier)) p.start() processes.append(p) barrier.wait() for p in processes: p.join() result = C_shm.copy() shm_A.close() shm_A.unlink() shm_B.close() shm_B.unlink() shm_C.close() shm_C.unlink() return result.tolist()"},{"question":"**Question: Implement a Library Database System** Implement a Python program using the `sqlite3` module to manage a library database. This program should support the following functionalities: 1. **Initialize the Database**: - Create a new SQLite database called `library.db`. - Create a table named `books` with the following columns: - `id` (INTEGER PRIMARY KEY): Unique identifier for each book. - `title` (TEXT): Title of the book. - `author` (TEXT): Author of the book. - `published_year` (INTEGER): Year the book was published. - `available` (BOOLEAN): Availability status of the book. 2. **Add a New Book**: - Implement a function `add_book(title: str, author: str, published_year: int) -> None` that inserts a new book into the `books` table. - The `available` status of a new book should be set to `True`. 3. **Search Books**: - Implement a function `search_books(keyword: str) -> list` that searches for books by title or author using the provided keyword (case-insensitive). The function should return a list of tuples with book details (id, title, author, published_year, available). 4. **Borrow a Book**: - Implement a function `borrow_book(book_id: int) -> bool` that updates the `available` status of a book to `False` if the book is currently available. The function should return `True` if the operation is successful and `False` otherwise. 5. **Return a Book**: - Implement a function `return_book(book_id: int) -> bool` that updates the `available` status of a book to `True` if the book is currently borrowed. The function should return `True` if the operation is successful and `False` otherwise. 6. **Custom Row Factory**: - Use a custom row factory to return each row as a dictionary where the keys are the column names. **Constraints**: - Ensure that the `id` field of the `books` table is automatically incremented and unique for each book. - Use placeholders for parameter substitution to prevent SQL injection. - The search should be case-insensitive. **Input/Output Specification**: - The input to the functions will be of appropriate types as specified in their signatures. - For the `search_books` function, the output should be a list of dictionaries where each dictionary represents a book\'s details. **Example**: ```python # Example usage: initialize_database() add_book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925) add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) add_book(\\"1984\\", \\"George Orwell\\", 1949) print(search_books(\\"george\\")) # Expected to find \\"1984\\" by George Orwell print(borrow_book(1)) # Expected to return True print(borrow_book(1)) # Expected to return False since the book is now borrowed print(return_book(1)) # Expected to return True print(return_book(1)) # Expected to return False since the book is now returned print(search_books(\\"great\\")) # Expected to find \\"The Great Gatsby\\" by F. Scott Fitzgerald ``` Implement the functions and necessary setup to achieve the described behavior. **Notes**: - Remember to handle database connections and cursor objects appropriately. - Ensure proper error handling for database operations.","solution":"import sqlite3 from typing import List, Dict def initialize_database() -> None: Initializes the library database by creating a new SQLite database with the specified schema. conn = sqlite3.connect(\'library.db\') conn.row_factory = sqlite3.Row c = conn.cursor() # Create table c.execute(\'\'\' CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY, title TEXT, author TEXT, published_year INTEGER, available BOOLEAN ) \'\'\') conn.commit() conn.close() def add_book(title: str, author: str, published_year: int) -> None: Adds a new book to the library database. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\'\'\' INSERT INTO books (title, author, published_year, available) VALUES (?, ?, ?, ?) \'\'\', (title, author, published_year, True)) conn.commit() conn.close() def search_books(keyword: str) -> List[Dict]: Searches for books by title or author using the provided keyword (case-insensitive). conn = sqlite3.connect(\'library.db\') conn.row_factory = sqlite3.Row c = conn.cursor() keyword = f\\"%{keyword}%\\" c.execute(\'\'\' SELECT * FROM books WHERE title LIKE ? OR author LIKE ? \'\'\', (keyword, keyword)) rows = c.fetchall() results = [dict(row) for row in rows] conn.close() return results def borrow_book(book_id: int) -> bool: Updates the available status of a book to False if the book is currently available. conn = sqlite3.connect(\'library.db\') c = conn.cursor() # Check if the book is available c.execute(\'SELECT available FROM books WHERE id = ?\', (book_id,)) result = c.fetchone() if result and result[0]: # If the book is available c.execute(\'UPDATE books SET available = False WHERE id = ?\', (book_id,)) conn.commit() conn.close() return True conn.close() return False def return_book(book_id: int) -> bool: Updates the available status of a book to True if the book is currently borrowed. conn = sqlite3.connect(\'library.db\') c = conn.cursor() # Check if the book is borrowed c.execute(\'SELECT available FROM books WHERE id = ?\', (book_id,)) result = c.fetchone() if result and not result[0]: # If the book is borrowed c.execute(\'UPDATE books SET available = True WHERE id = ?\', (book_id,)) conn.commit() conn.close() return True conn.close() return False"},{"question":"# Pandas Time Series and Date Functionality: Data Analysis **Objective:** To evaluate your understanding of various pandas time series functionalities, including working with DatetimeIndex, resampling, timezone localization, and PeriodIndex manipulation. **Task:** You are given a dataset that records temperature readings every hour for a specific location. The data is in a CSV file called `temperature_data.csv` with the following structure: ``` timestamp,temperature 2022-01-01 00:00:00,5.2 2022-01-01 01:00:00,4.9 ... ``` Your tasks are as follows: 1. **Read and Parse the Data:** - Read the CSV file into a pandas DataFrame. - Convert the `timestamp` column to a `DatetimeIndex`. 2. **Timezone Localization:** - Localize the `DatetimeIndex` to UTC. - Convert the timezone to \'US/Eastern\'. 3. **Resample the Data:** - Resample the temperature data to daily frequency, calculating the mean temperature for each day. 4. **Period Index Conversion:** - Convert the `DatetimeIndex` to a `PeriodIndex` with monthly frequency. - Calculate the monthly mean temperature. 5. **Missing Data Handling:** - Introduce some missing values in the temperature data. - Fill the missing values using forward-fill method. 6. **Data Aggregation:** - Aggregate the data to find the mean, max, and min temperatures for each month. **Constraints and Performance Requirements:** - Ensure that all operations are efficient and performed using pandas built-in functions. - Handle any potential issues with time zone conversion and inconsistencies. **Input:** - A CSV file `temperature_data.csv` containing hourly temperature readings. **Output:** - Print the head of the DataFrame after each step to show the transformations. - Print the final aggregated data for monthly mean, max, and min temperatures. **Function Signature:** ```python import pandas as pd def analyze_temperature_data(file_path: str): # Step 1: Read and Parse the Data df = pd.read_csv(file_path) df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) df.set_index(\'timestamp\', inplace=True) print(\\"DataFrame after reading and parsing:\\") print(df.head()) # Step 2: Timezone Localization df = df.tz_localize(\'UTC\').tz_convert(\'US/Eastern\') print(\\"DataFrame after timezone localization and conversion:\\") print(df.head()) # Step 3: Resample the Data daily_mean = df.resample(\'D\').mean() print(\\"DataFrame after daily resampling:\\") print(daily_mean.head()) # Step 4: Period Index Conversion monthly_period = daily_mean.to_period(\'M\') monthly_mean = monthly_period.resample(\'M\').mean() print(\\"DataFrame after converting to PeriodIndex with monthly frequency:\\") print(monthly_mean.head()) # Step 5: Missing Data Handling daily_mean_missing = daily_mean.copy() daily_mean_missing.iloc[5:10] = None # Introduce missing values daily_mean_filled = daily_mean_missing.ffill() print(\\"DataFrame after forward-filling missing values:\\") print(daily_mean_filled.head(10)) # Step 6: Data Aggregation monthly_agg = daily_mean_filled.resample(\'M\').agg({\'temperature\': [\'mean\', \'max\', \'min\']}) print(\\"Monthly aggregated data:\\") print(monthly_agg) ``` **Implement this function and demonstrate its usage with the sample file provided.**","solution":"import pandas as pd def analyze_temperature_data(file_path: str): # Step 1: Read and Parse the Data df = pd.read_csv(file_path) df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) df.set_index(\'timestamp\', inplace=True) print(\\"DataFrame after reading and parsing:\\") print(df.head()) # Step 2: Timezone Localization df = df.tz_localize(\'UTC\').tz_convert(\'US/Eastern\') print(\\"DataFrame after timezone localization and conversion:\\") print(df.head()) # Step 3: Resample the Data daily_mean = df.resample(\'D\').mean() print(\\"DataFrame after daily resampling:\\") print(daily_mean.head()) # Step 4: Period Index Conversion monthly_period = daily_mean.to_period(\'M\') monthly_mean = monthly_period.resample(\'M\').mean() print(\\"DataFrame after converting to PeriodIndex with monthly frequency:\\") print(monthly_mean.head()) # Step 5: Missing Data Handling daily_mean_missing = daily_mean.copy() daily_mean_missing.iloc[5:10] = None # Introduce missing values daily_mean_filled = daily_mean_missing.ffill() print(\\"DataFrame after forward-filling missing values:\\") print(daily_mean_filled.head(10)) # Step 6: Data Aggregation monthly_agg = daily_mean_filled.resample(\'M\').agg({\'temperature\': [\'mean\', \'max\', \'min\']}) print(\\"Monthly aggregated data:\\") print(monthly_agg)"},{"question":"**Objective:** Your task is to write a function that visualizes a dataset using Seaborn\'s `objects` interface. You will need to load a dataset, perform some data manipulation, and create a customized plot that includes text annotations. **Requirements:** 1. Load the \'penguins\' dataset using `seaborn.load_dataset`. 2. Pivot the dataset to calculate the average bill length and bill depth for each species. 3. Create a bar plot showing the average bill length for each species. 4. Add text annotations to the bars indicating the average values. 5. Customize the text annotations to be white in color and right-aligned. **Function Signature:** ```python import seaborn.objects as so import seaborn as sns def visualize_penguins(): pass ``` # Expected Input and Output: - **Input:** None - **Output:** Display a bar plot with customized text annotations. # Constraints: - Use the Seaborn library version that supports the `seaborn.objects` module. # Performance Requirements: - The function should perform the necessary data manipulations using efficient pandas operations. - The generated plot should be correctly displayed with the required customizations. # Example Usage: When you run the function `visualize_penguins()`, it should generate and display a bar plot with the described specifications. ```python # Example usage visualize_penguins() ``` # Additional Notes: - Ensure you have the Seaborn library and its dependencies installed. - The generated plot should be visually informative and clear.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def visualize_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Pivot the dataset to calculate the average bill length and bill depth for each species pivot_table = penguins.pivot_table(index=\'species\', values=[\'bill_length_mm\', \'bill_depth_mm\'], aggfunc=\'mean\') # Select only the average bill length for visualization bill_length = pivot_table[\'bill_length_mm\'].reset_index() # Create a bar plot showing the average bill length for each species p = so.Plot(bill_length, x=\\"species\\", y=\\"bill_length_mm\\").add(so.Bar()) # Create the figure and axes for customization fig, ax = plt.subplots() p.on(ax) # Add text annotations to the bars indicating the average values for index, row in bill_length.iterrows(): ax.text(index, row[\\"bill_length_mm\\"] - 0.5, f\\"{row[\'bill_length_mm\']:.2f}\\", color=\'white\', ha=\'right\', va=\'bottom\') # Display the plot plt.show()"},{"question":"You have been given a task to build a classification model using the Nearest Neighbors method. The dataset you\'ll use for this task is the famous Iris dataset, which consists of 150 samples with four features each and belongs to one of three classes. Implement a system that: 1. Loads the Iris dataset using scikit-learn. 2. Splits the dataset into training and testing sets. 3. Implements the Neighborhood Components Analysis (NCA) algorithm for dimensionality reduction. 4. Uses the KNeighborsClassifier to classify the samples in the reduced dimension space. 5. Evaluates the performance of the classification model using accuracy. 6. Plots the decision boundaries for the classified data points if the data is reduced to 2 dimensions for visualization purposes. # Input - No user input is required as you are using the Iris dataset directly from scikit-learn. # Output - Print the model\'s accuracy score. - If the number of components is set to 2, display a plot showing the decision boundaries. # Requirements: - Use appropriate scikit-learn modules (`datasets`, `model_selection`, `pipeline`, `neighbors`, `metrics`). - Use Matplotlib for plotting the decision boundaries (only if `n_components` = 2). - Ensure the code is well documented and follows proper coding practices. # Constraints: - The `n_neighbors` parameter for the KNeighborsClassifier should be 3. - You should use 30% of the data for testing. # Example Code Snippet: Here\'s a template to help you get started. Replace `...` with your code: ```python import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.neighbors import NeighborhoodComponentsAnalysis, KNeighborsClassifier from sklearn.metrics import accuracy_score import numpy as np # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets (70% train, 30% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) # Initialize the NeighborhoodComponentsAnalysis and KNeighborsClassifier nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42) knn = KNeighborsClassifier(n_neighbors=3) # Create a pipeline with NCA and KNN nca_knn_pipeline = Pipeline([(\'nca\', nca), (\'knn\', knn)]) # Fit the pipeline with the training data nca_knn_pipeline.fit(X_train, y_train) # Predict the labels for the test set y_pred = nca_knn_pipeline.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') # Plot the decision boundaries if n_components is 2 if nca.n_components == 2: h = .02 # step size in the mesh # Create color maps cmap_light = plt.cm.RdYlBu cmap_bold = [\'darkorange\', \'c\', \'darkblue\'] # Mesh grid x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1 y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.figure() plt.contourf(xx, yy, Z, cmap=cmap_light) # Plot also the training points scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor=\'k\', s=20, marker=\'o\') plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, edgecolor=\'k\', s=50, marker=\'*\') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title(\\"3-Class classification (k = 3)\\") plt.xlabel(iris.feature_names[0]) plt.ylabel(iris.feature_names[1]) plt.show() ```","solution":"import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.neighbors import NeighborhoodComponentsAnalysis, KNeighborsClassifier from sklearn.metrics import accuracy_score import numpy as np def load_data(): iris = datasets.load_iris() return iris.data, iris.target def split_data(X, y): return train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) def train_model(X_train, y_train): nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42) knn = KNeighborsClassifier(n_neighbors=3) nca_knn_pipeline = Pipeline([(\'nca\', nca), (\'knn\', knn)]) nca_knn_pipeline.fit(X_train, y_train) return nca_knn_pipeline def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') return accuracy def plot_decision_boundaries(model, X_train, y_train, n_components=2): if n_components != 2: return # Plotting requires 2D data h = .02 # step size in the mesh # Transform the data X_train_transformed = model.named_steps[\'nca\'].transform(X_train) x_min, x_max = X_train_transformed[:, 0].min() - 1, X_train_transformed[:, 0].max() + 1 y_min, y_max = X_train_transformed[:, 1].min() - 1, X_train_transformed[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) knn = model.named_steps[\'knn\'] Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.figure() plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.6) # Plot also the training points scatter = plt.scatter(X_train_transformed[:, 0], X_train_transformed[:, 1], c=y_train, cmap=plt.cm.RdYlBu, edgecolor=\'k\', s=20) plt.title(\\"3-Class classification with Neighborhood Components Analysis\\") plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.show() def main(): X, y = load_data() X_train, X_test, y_train, y_test = split_data(X, y) model = train_model(X_train, y_train) evaluate_model(model, X_test, y_test) plot_decision_boundaries(model, X_train, y_train) if __name__ == \\"__main__\\": main()"},{"question":"# Custom Boolean Class Implementation **Objective**: Implement a custom Boolean class in Python that mimics the behavior of the standard Boolean type. This class should follow similar principles as described in the provided documentation of Boolean objects for Python. **Task**: Create a class `CustomBoolean` that: 1. Initializes with a value (`True` or `False`). 2. Implements methods to handle the following: - Boolean checks similar to `PyBool_Check`. - Custom methods that return `CustomBoolean` objects for logical operations (AND, OR, NOT). - String representation that matches the standard Python Boolean (`True` or `False`). - Proper handling of reference counts (Bonus - Advanced). **Details**: 1. **Initialization**: ```python class CustomBoolean: def __init__(self, value): # Initialize with True or False ``` 2. **Boolean Check**: ```python def is_custom_boolean(obj): # Return True if obj is an instance of CustomBoolean; else False ``` 3. **Logical Operations**: - Implement `__and__`, `__or__`, and `__invert__` methods: ```python def __and__(self, other): # Logical AND operation def __or__(self, other): # Logical OR operation def __invert__(self): # Logical NOT operation ``` 4. **String Representation**: ```python def __str__(self): # Return \'True\' or \'False\' as a string representation ``` 5. **Reference Counting** (Bonus - Advanced): - Implement internal reference counting to manage the creation and deletion of `CustomBoolean` objects. **Example Usage**: ```python cb_true = CustomBoolean(True) cb_false = CustomBoolean(False) print(is_custom_boolean(cb_true)) # True print(is_custom_boolean(123)) # False cb_and = cb_true & cb_false print(cb_and) # False cb_or = cb_true | cb_false print(cb_or) # True cb_not = ~cb_true print(cb_not) # False ``` **Constraints**: - Do not use the built-in `bool` type directly for logical operations inside your class methods. - Ensure that the class is robust and handles invalid inputs gracefully. **Performance Requirements**: - Implement methods with consideration of time complexity for logical operations (O(1)). Good luck!","solution":"class CustomBoolean: def __init__(self, value): if not isinstance(value, bool): raise ValueError(\\"Value must be of type bool\\") self.value = value def __and__(self, other): if not isinstance(other, CustomBoolean): raise ValueError(\\"Operand must be of type CustomBoolean\\") return CustomBoolean(self.value and other.value) def __or__(self, other): if not isinstance(other, CustomBoolean): raise ValueError(\\"Operand must be of type CustomBoolean\\") return CustomBoolean(self.value or other.value) def __invert__(self): return CustomBoolean(not self.value) def __str__(self): return \'True\' if self.value else \'False\' def is_custom_boolean(obj): return isinstance(obj, CustomBoolean)"},{"question":"**Objective** You are required to implement a function using the `asyncio` subprocess APIs provided. This function should execute multiple commands concurrently, handle their outputs, errors, and manage potential timeouts. **Function Signature** ```python import asyncio from typing import List, Tuple async def run_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str, str]]: pass # Example Usage async def main(): commands = [ \\"ls -l\\", \\"echo Hello World\\", \\"sleep 2; echo Done\\", ] results = await run_commands(commands, timeout=3) for cmd, returncode, stdout, stderr in results: print(f\\"Command: {cmd}nReturn Code: {returncode}nSTDOUT: {stdout}nSTDERR: {stderr}n\\") asyncio.run(main()) ``` **Requirements** 1. **Function Description**: - `run_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str, str]]`: takes a list of shell commands and a timeout value in seconds. 2. **Function Implementation**: - Use `asyncio.create_subprocess_shell` to create subprocesses. - Utilize `asyncio.gather` to run commands concurrently. - Implement error handling, ensuring that commands do not exceed the specified `timeout`. - Capture each command\'s output (both stdout and stderr) and return code. 3. **Expected Input**: - `commands`: a list of strings, where each string is a shell command to be executed (e.g., `[\\"ls -l\\", \\"echo Hello World\\"]`). - `timeout`: an integer specifying the maximum number of seconds to wait for each command to complete. 4. **Expected Output**: - A list of tuples, each containing the command executed, its return code (integer), its stdout output (string), and its stderr output (string). **Constraints**: - Use async/await functionalities provided by `asyncio`. - Ensure proper error handling for commands timing out. - Stdout and stderr should be captured correctly for each command. - The function must not block and should handle subprocesses concurrently. **Performance Requirements**: - The function should efficiently manage the provided timeout and handle multiple commands in parallel without blocking. - The implementation should avoid deadlocks by correctly managing subprocess IO pipes. # Note: Candidates should ensure to demonstrate: - Proper knowledge of the `asyncio` package, specifically with subprocesses. - Effective error handling, especially with respect to timeouts and IO management. - Efficient use of async/await for concurrent execution.","solution":"import asyncio from typing import List, Tuple async def run_command(command: str, timeout: int) -> Tuple[str, int, str, str]: try: proc = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE, ) try: stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=timeout) return (command, proc.returncode, stdout.decode(), stderr.decode()) except asyncio.TimeoutError: proc.kill() stdout, stderr = await proc.communicate() return (command, -1, stdout.decode(), \\"Command timed out\\") except Exception as e: return (command, -1, \\"\\", str(e)) async def run_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str, str]]: tasks = [run_command(command, timeout) for command in commands] return await asyncio.gather(*tasks)"},{"question":"# CGI Form Handling and File Upload **Objective:** You are required to implement a CGI script in Python that handles form submissions and file uploads. Your task is to create a function that processes form data, checks for required fields, handles multiple values for a field, manages file uploads, and ensures output is correctly formatted in HTML. **Details:** Write a function `process_cgi_form` that performs the following tasks: 1. **Input Handling:** - Parse the form data using the `FieldStorage` class. - Retrieve the value of a form field named \\"username\\". Ensure it is a single value. - Retrieve all values of form fields named \\"hobbies\\". These should be processed as a list. - Check if a file has been uploaded under the form field name \\"profile_picture\\". 2. **Validations:** - If \\"username\\" is not provided or is empty, return an HTML message indicating it is a required field. - If no hobbies are provided, return an HTML message indicating at least one hobby must be selected. 3. **File Handling:** - If a file is uploaded under \\"profile_picture\\", count the number of lines in this file. 4. **Output:** - Return an HTML response with the following: - The username in uppercase. - A comma-separated list of hobbies. - The number of lines in the uploaded profile picture file (if it exists). - Ensure to print the necessary headers for an HTML response. **Constraints:** - Assume the input comes from a CGI-formatted request. - Use the `cgi` and `cgitb` modules for processing and debugging. - Handle unexpected errors gracefully using cgitb. **Function Signature:** ```python def process_cgi_form(): pass ``` **Example Usage:** Consider an HTML form with: ```html <form method=\\"POST\\" enctype=\\"multipart/form-data\\" action=\\"/cgi-bin/process_form.cgi\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Hobbies: <input type=\\"checkbox\\" name=\\"hobbies\\" value=\\"Reading\\">Reading <input type=\\"checkbox\\" name=\\"hobbies\\" value=\\"Traveling\\">Traveling <input type=\\"checkbox\\" name=\\"hobbies\\" value=\\"Cooking\\">Cooking<br> Profile Picture: <input type=\\"file\\" name=\\"profile_picture\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` After submission: ```python process_cgi_form() ``` This code should generate a valid HTML response including header information and displaying the parsed form data and file details. **Note:** Write the complete function `process_cgi_form` including headers and HTML formatting in the response.","solution":"import cgi import cgitb cgitb.enable() # Enable detailed error messages for the web page def process_cgi_form(): form = cgi.FieldStorage() # Retrieve \'username\' field username = form.getfirst(\'username\', \'\') username = username.strip() # Retrieve \'hobbies\' field which can have multiple values hobbies = form.getlist(\'hobbies\') # Check if a file is uploaded under \'profile_picture\' fileitem = form[\'profile_picture\'] if \'profile_picture\' in form else None # Check for required fields and validations if not username: print(\\"Content-Type: text/htmln\\") print(\\"<html><body>\\") print(\\"<p>Error: \'username\' is a required field.</p>\\") print(\\"</body></html>\\") return if not hobbies: print(\\"Content-Type: text/htmln\\") print(\\"<html><body>\\") print(\\"<p>Error: At least one hobby must be selected.</p>\\") print(\\"</body></html>\\") return hobbies_list = \\", \\".join(hobbies) # Initialize the line count for the uploaded file line_count = None if fileitem and fileitem.file: line_count = sum(1 for _ in fileitem.file) # Print HTML response print(\\"Content-Type: text/htmln\\") print(\\"<html><body>\\") print(f\\"<p>Username: {username.upper()}</p>\\") print(f\\"<p>Hobbies: {hobbies_list}</p>\\") if line_count is not None: print(f\\"<p>Profile Picture Line Count: {line_count}</p>\\") else: print(\\"<p>No Profile Picture Uploaded.</p>\\") print(\\"</body></html>\\")"},{"question":"# Question: Customize and Plot Data with Seaborn You are provided with a dataset containing information about the performance of students in different subjects. Your task is to create a visualization using Seaborn to compare the average performance of students in each subject. Additionally, you are required to customize the style of the plots. **Dataset:** The dataset is a dictionary where keys are student names and values are dictionaries of subject-performance pairs. ```python students_performance = { \\"Alice\\": {\\"Math\\": 85, \\"English\\": 78, \\"Science\\": 92}, \\"Bob\\": {\\"Math\\": 79, \\"English\\": 85, \\"Science\\": 88}, \\"Charlie\\": {\\"Math\\": 92, \\"English\\": 89, \\"Science\\": 95}, \\"David\\": {\\"Math\\": 70, \\"English\\": 72, \\"Science\\": 78} } ``` **Tasks:** 1. Calculate the average performance of students for each subject. 2. Using Seaborn, create a bar plot to visualize the average performance in each subject. 3. Temporarily change the style of the plot to \\"darkgrid\\" using a context manager. **Function Signature:** ```python def plot_student_performance(data: dict) -> None: pass ``` **Constraints:** - You must use the `sns.barplot()` function to create the bar plot. - The style change to \\"darkgrid\\" must be done using a context manager. - The plot must include a title \\"Average Performance of Students in Subjects\\". **Expected Output:** A bar plot showing the average performance of students in Math, English, and Science, with the plot styled according to the \\"darkgrid\\" style. **Example:** When you call `plot_student_performance(students_performance)`, it should display a plot with three bars representing the average performances in Math, English, and Science, respectively, with the \\"darkgrid\\" style applied.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_student_performance(data: dict) -> None: Plots the average performance of students in each subject. # Calculate average performance for each subject subjects = list(next(iter(data.values())).keys()) averages = {subject: 0 for subject in subjects} student_count = len(data) for performance in data.values(): for subject, score in performance.items(): averages[subject] += score averages = {subject: score / student_count for subject, score in averages.items()} # Preparing data for Seaborn subjects = list(averages.keys()) scores = list(averages.values()) # Plotting with sns.axes_style(\\"darkgrid\\"): plt.figure(figsize=(8, 5)) sns.barplot(x=subjects, y=scores, palette=\\"viridis\\") plt.title(\\"Average Performance of Students in Subjects\\") plt.ylabel(\\"Average Score\\") plt.xlabel(\\"Subjects\\") plt.ylim(0, 100) plt.show()"},{"question":"You are provided with a dataset and need to create a series of visualizations to analyze it. Here, you will utilize Seaborn\'s `objects` API to generate insightful plots with specific customizations. # Dataset Assume you have a dataset `student_scores` which contains the following columns: - `Student`: Name of the student. - `Subject`: The subject the student was evaluated in. - `Score`: Numerical score of the student in the subject. - `Category`: The category of the evaluation (e.g., \'Midterm\', \'Final\'). # Tasks 1. **Data Preparation:** - Load the dataset into a DataFrame. - Pivot the dataset so that you have students as rows, subjects as columns, and their scores as values. Additionally, add a column for the average score of each student. - Ensure the DataFrame is sorted by the average score in descending order. 2. **Create a Bar Plot:** - Create a bar plot showing the average score for each student. - Annotate each bar with the respective average score. - Ensure that the text is white and horizontally aligned to the right of the bars with a small offset for better visibility. 3. **Create a Dot Plot:** - Create a dot plot to show the relationship between `Math` and `Science` scores. - Annotate each dot with the student\'s name. - Use a different color for the dot based on the category. - Ensure that the text is aligned above the dots. 4. **Add Advanced Text Alignment:** - In the dot plot, align the text based on the `Category`. Use specific alignment (`left`, `right`, `center`) for different categories to improve the plot\'s readability. - Apply additional matplotlib parameters to highlight the text. # Input and Output - **Input:** Assume the input dataset (`student_scores`) is a pandas DataFrame already loaded in the environment. - **Output:** The output should be a visual representation following the above specifications. # Constraints - You must use Seaborn\'s `objects` API for all tasks. - Handle any missing values appropriately. - Ensure that the plots are appropriately labeled and visually clear. # Example Code for Data Preparation Below is an example of data preparation. You need to integrate and adapt it as part of your solution: ```python import seaborn.objects as so import pandas as pd # Example dataset data = { \'Student\': [\'Alice\', \'Bob\', \'Charlie\', \'David\'], \'Subject\': [\'Math\', \'Science\', \'Math\', \'Science\'], \'Score\': [85, 90, 78, 88], \'Category\': [\'Midterm\', \'Midterm\', \'Final\', \'Final\'] } student_scores = pd.DataFrame(data) # Task 1: Data Preparation glue = ( student_scores .pivot(index=\'Student\', columns=\'Subject\', values=\'Score\') .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\'Average\', ascending=False) ) ``` # Expected Output - A bar plot with properly aligned annotations. - A customized dot plot with different alignments and text properties.","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def prepare_data(student_scores): Prepares the student_scores DataFrame by pivoting it and adding an average score column. Parameters: student_scores (pd.DataFrame): The input DataFrame with columns \'Student\', \'Subject\', \'Score\', and \'Category\'. Returns: pd.DataFrame: The pivoted DataFrame with students as rows, subjects as columns, and their scores as values, including an average score column. glue = ( student_scores .pivot(index=\'Student\', columns=\'Subject\', values=\'Score\') .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\'Average\', ascending=False) ) return glue def create_bar_plot(data): Creates a bar plot showing the average score for each student. Parameters: data (pd.DataFrame): A DataFrame containing students and their average scores. fig, ax = plt.subplots() bars = ax.barh(data.index, data[\'Average\'], color=\\"skyblue\\") for bar in bars: width = bar.get_width() ax.text(width - 5, bar.get_y() + bar.get_height()/2, f\'{width:.1f}\', va=\'center\', ha=\'right\', color=\'white\') ax.set_xlabel(\'Average Score\') ax.set_ylabel(\'Student\') ax.set_title(\'Average Score by Student\') plt.gca().invert_yaxis() plt.show() def create_dot_plot(student_scores, data): Creates a dot plot to show the relationship between Math and Science scores with customized annotations. Parameters: student_scores (pd.DataFrame): The original student_scores DataFrame. data (pd.DataFrame): A pivoted DataFrame containing scores and average scores for each student. fig, ax = plt.subplots() for idx, row in data.iterrows(): student = idx math_score = row[\'Math\'] science_score = row[\'Science\'] category = student_scores[student_scores[\'Student\'] == student][\'Category\'].values[0] if category == \'Midterm\': ha = \'left\' text_color = \'green\' elif category == \'Final\': ha = \'right\' text_color = \'red\' else: ha = \'center\' text_color = \'blue\' ax.plot(math_score, science_score, \'o\', label=category) ax.text(math_score, science_score + 0.5, student, ha=ha, color=text_color) ax.set_xlabel(\'Math Score\') ax.set_ylabel(\'Science Score\') ax.set_title(\'Math vs Science Scores by Student\') plt.show()"},{"question":"# Objective You are to demonstrate your understanding of the `ssl` module with a focus on secure socket communication, certificate handling, and context management. # Problem Statement You are required to write a Python program using the `ssl` module which establishes a secure client connection to a given server hostname on port 443 (HTTPS). Upon establishing the connection, your program should fetch and print out the server\'s SSL certificate details. # Requirements 1. **Create SSL Context:** - Create an SSL context using `ssl.create_default_context()`. 2. **Secure Socket Connection:** - Use the created context to wrap a socket and connect to the server securely. The server address is `hostname` and the port is `443`. - Fetch and print the SSL protocol version being used on this connection. 3. **Certificate Handling and Validation:** - Obtain the server\'s SSL certificate using `getpeercert()`. - Print the details of the certificate (especially the issuer and subject fields). 4. **Error Handling:** - Properly handle any potential exceptions (like `ssl.SSLError`) that might occur during the connection or certificate handling process. # Input - `hostname`: A string representing the server\'s hostname you are connecting to (e.g., \'www.python.org\'). # Output - Print the SSL protocol version used. - Print the server certificate details. # Example Execution If the hostname is \'www.python.org\', your program should print: ``` SSL Protocol Version: TLSv1.3 Issuer: [(\'countryName\', \'US\'), (\'organizationName\', \'DigiCert Inc\'), (\'organizationalUnitName\', \'www.digicert.com\'), (\'commonName\', \'DigiCert SHA2 High Assurance Server CA\')] Subject: [(\'businessCategory\', \'Private Organization\'), (\'serialNumber\', \'Society number\'), (\'countryName\', \'US\'), (\'stateOrProvinceName\', \'Delaware\'), (\'localityName\', \'Wilmington\'), (\'organizationName\', \'Python Software Foundation\'), (\'commonName\', \'www.python.org\')] ``` # Constraints - Ensure compatibility with Python 3.6+ and OpenSSL 1.1.1+. - You must handle and report SSL-related exceptions gracefully. # Implementation Complete the function `fetch_server_cert_details(hostname)`: ```python import ssl import socket from pprint import pprint def fetch_server_cert_details(hostname): try: # 1. Create SSL Context context = ssl.create_default_context() # 2. Create secure socket connection with socket.create_connection((hostname, 443)) as sock: with context.wrap_socket(sock, server_hostname=hostname) as ssock: # 3. Print SSL protocol version print(f\\"SSL Protocol Version: {ssock.version()}\\") # 4. Get and print server certificate cert = ssock.getpeercert() pprint(cert) except ssl.SSLError as e: print(f\\"SSL Error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") # Example usage fetch_server_cert_details(\'www.python.org\') ``` # Note You must test the function thoroughly to ensure it handles various cases; for example, invalid hostnames should trigger exception handling gracefully.","solution":"import ssl import socket from pprint import pprint def fetch_server_cert_details(hostname): try: # 1. Create SSL Context context = ssl.create_default_context() # 2. Create secure socket connection with socket.create_connection((hostname, 443)) as sock: with context.wrap_socket(sock, server_hostname=hostname) as ssock: # 3. Print SSL protocol version print(f\\"SSL Protocol Version: {ssock.version()}\\") # 4. Get and print server certificate cert = ssock.getpeercert() print(\\"Issuer:\\", cert.get(\'issuer\')) print(\\"Subject:\\", cert.get(\'subject\')) except ssl.SSLError as e: print(f\\"SSL Error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") # Example usage if __name__ == \\"__main__\\": fetch_server_cert_details(\'www.python.org\')"},{"question":"Objective Implement a function in Python that dynamically modifies the `sys.path` by adding new paths, validates the changes, and provides utility to handle warnings in Python. Task Write a Python function named `manage_sys_paths_and_warnings(new_paths)`, where: 1. `new_paths` is a list of strings, each representing a new path to be added to `sys.path`. 2. The function should add each path from `new_paths` to `sys.path`. 3. After updating `sys.path`, the function should validate that the paths have been correctly added. 4. Implement a utility within the function to handle warnings by resetting `sys.warnoptions` and adding a custom warning option. Requirements 1. The function `manage_sys_paths_and_warnings(new_paths)` should: - Add each path from the `new_paths` list to `sys.path`. - Ensure that each new path has been added correctly by printing the updated `sys.path`. 2. The function should also: - Reset the `sys.warnoptions` to an empty list. - Add a custom warning option (e.g., \\"default\\") to the `sys.warnoptions`. - Print the updated `sys.warnoptions`. Input - `new_paths`: A list of strings representing new paths to be added to `sys.path`. Output - The function should print the updated `sys.path` and `sys.warnoptions` after making the changes. Constraints - You can assume the input will always be a list of valid strings. - Use of Python\'s built-in `sys` module utilities such as `sys.path` and `sys.warnoptions` is required. Example ```python def manage_sys_paths_and_warnings(new_paths): pass # Your implementation goes here # Example usage new_paths = [\\"/new/path/one\\", \\"/new/path/two\\"] manage_sys_paths_and_warnings(new_paths) ``` Expected output: ``` Updated sys.path: [\'...\', \'/new/path/one\', \'/new/path/two\'] Updated sys.warnoptions: [\'default\'] ``` Note - Ensure your implementation considers performance and avoids redundant operations. - Your solution should handle potential exceptions when accessing or modifying system configurations.","solution":"import sys def manage_sys_paths_and_warnings(new_paths): This function adds new paths to sys.path, resets sys.warnoptions, and adds a custom warning option. Parameters: new_paths (list of str): List of new paths to be added to sys.path for path in new_paths: if path not in sys.path: sys.path.append(path) # Print the updated sys.path for validation print(\\"Updated sys.path:\\", sys.path) # Resetting the sys.warnoptions to an empty list and adding a custom warning option sys.warnoptions = [] sys.warnoptions.append(\\"default\\") # Print the updated sys.warnoptions for validation print(\\"Updated sys.warnoptions:\\", sys.warnoptions)"},{"question":"**Objective:** Demonstrate your ability to utilize pandas\' `Styler` object to format and export a DataFrame. Task: You are given sales data for a retail store in a DataFrame format. Your task is to apply several styling modifications to this DataFrame using the `Styler` object and then export it to an HTML file. Given below is a sample sales DataFrame: ``` python import pandas as pd data = { \'Product\': [\'Widget\', \'Gadget\', \'Doohickey\', \'Thingamajig\', \'Gizmo\'], \'Sales\': [150, 300, 80, 230, 120], \'Returns\': [5, 15, 2, 12, 7], \'Profit\': [140, 280, 75, 210, 113] } df = pd.DataFrame(data) ``` Tasks to perform: 1. **Highlight Maximum and Minimum Sales:** - Highlight the cell(s) with the maximum sales in green. - Highlight the cell(s) with the minimum sales in red. 2. **Gradient Background:** - Apply a blue-to-white background gradient to the \'Profit\' column, where higher profits are darker blue. 3. **Display Properties:** - Set the text alignment of the entire DataFrame to center. - Set the table caption to \\"Retail Store Sales Data\\". 4. **Export the DataFrame:** - Export the styled DataFrame to an HTML file named `styled_sales.html`. Constraints: - You should not modify the original DataFrame `df`. - The output HTML should be a valid HTML file that can be viewed in a web browser. Input: - A DataFrame `df` with columns: `Product`, `Sales`, `Returns`, and `Profit`. Output: - An HTML file named `styled_sales.html`. Example: ``` python # Apply your styling here styled_df = df.style # Add your formatting and styling code below... # Export the styled DataFrame to HTML styled_df.to_html(\'styled_sales.html\') ``` Use the methods outlined in the pandas `Styler` documentation to complete these tasks efficiently.","solution":"import pandas as pd def style_sales_dataframe(df): Apply styling to the sales DataFrame and export it as an HTML file. Parameters: df (pd.DataFrame): The sales DataFrame to be styled. Returns: pd.io.formats.style.Styler: The styled DataFrame. # Define a function to highlight maximum values in \'Sales\' column def highlight_max(s): is_max = s == s.max() return [\'background-color: green\' if v else \'\' for v in is_max] # Define a function to highlight minimum values in \'Sales\' column def highlight_min(s): is_min = s == s.min() return [\'background-color: red\' if v else \'\' for v in is_min] # Style the DataFrame styled_df = df.style .apply(highlight_max, subset=[\'Sales\']) .apply(highlight_min, subset=[\'Sales\']) .background_gradient(cmap=\'Blues\', subset=[\'Profit\']) .set_table_styles([{\'selector\': \'td\', \'props\': [(\'text-align\', \'center\')]}, {\'selector\': \'th\', \'props\': [(\'text-align\', \'center\')]}]) .set_caption(\\"Retail Store Sales Data\\") # Export to HTML styled_df.to_html(\'styled_sales.html\') return styled_df"},{"question":"**Objective**: You are required to demonstrate your understanding of fundamental and advanced concepts in the `os` module by performing operations related to file and directory management, process handling, and environment variable manipulation. # Problem Statement Your task is to implement a Python script that performs the following operations: 1. **Create a Directory Structure**: - Create a directory named `\\"my_projects\\"` in the current working directory. - Inside `\\"my_projects\\"`, create three subdirectories named: - `\\"project_A\\"` - `\\"project_B\\"` - `\\"project_C\\"` - Inside each subdirectory, create a text file named `\\"README.txt\\"` with the content: `\\"This is the README for [project_name]\\"`. 2. **Manage Environment Variables**: - Read an environment variable named `\\"USER\\"`. If it does not exist, create it with the value `\\"default_user\\"`. - Log the value of the `\\"USER\\"` environment variable in a file named `\\"user_info.txt\\"` inside the `\\"my_projects\\"` directory. 3. **File Operations with Exception Handling**: - Attempt to read the content of a file named `\\"notes.txt\\"` in the `\\"my_projects/project_A\\"` directory. If the file does not exist, create it with some default content `\\"Sample notes for project_A\\"` and then read the content. - Log the read content into a file named `\\"project_A_notes_log.txt\\"` in the `\\"my_projects/project_A\\"` directory. # Function Specifications 1. **create_directory_structure(base_dir)**: - **Input**: `base_dir (str)` - **Output**: None - **Functionality**: - Takes a `base_dir` as input and creates the required directory structure and files as specified. 2. **manage_user_environment_variable(base_dir)**: - **Input**: `base_dir (str)` - **Output**: None - **Functionality**: - Reads or sets the `\\"USER\\"` environment variable and logs it in a file in the specified `base_dir`. 3. **handle_file_operations(base_dir)**: - **Input**: `base_dir (str)` - **Output**: None - **Functionality**: - Reads from or creates the `\\"notes.txt\\"` file and logs the content appropriately. # Example Usage ```python base_directory = os.getcwd() create_directory_structure(base_directory) manage_user_environment_variable(base_directory) handle_file_operations(base_directory) ``` # Constraints - Ensure robust exception handling for file operations. - Implement appropriate directory and file creation permissions. - Ensure your code works for both Windows and Unix platforms. - Do not use third-party libraries; only rely on the standard `os` module. # Evaluation Criteria - Correctness: Does the solution meet the requirements and handle edge cases properly? - Code Quality: Is the code well-organized, readable, and maintainable? - Robustness: Does the solution handle errors and exceptions gracefully?","solution":"import os def create_directory_structure(base_dir): Creates the required directory structure and files as specified. projects_dir = os.path.join(base_dir, \\"my_projects\\") # Create main directory os.makedirs(projects_dir, exist_ok=True) project_names = [\\"project_A\\", \\"project_B\\", \\"project_C\\"] for project in project_names: project_dir = os.path.join(projects_dir, project) os.makedirs(project_dir, exist_ok=True) readme_path = os.path.join(project_dir, \\"README.txt\\") with open(readme_path, \\"w\\") as readme_file: readme_file.write(f\\"This is the README for {project}n\\") def manage_user_environment_variable(base_dir): Reads or sets the \\"USER\\" environment variable and logs it in a file. projects_dir = os.path.join(base_dir, \\"my_projects\\") user_info_path = os.path.join(projects_dir, \\"user_info.txt\\") user = os.getenv(\\"USER\\", \\"default_user\\") with open(user_info_path, \\"w\\") as user_info_file: user_info_file.write(f\\"USER: {user}n\\") def handle_file_operations(base_dir): Reads from or creates the \\"notes.txt\\" file and logs the content appropriately. project_a_dir = os.path.join(base_dir, \\"my_projects\\", \\"project_A\\") notes_path = os.path.join(project_a_dir, \\"notes.txt\\") notes_log_path = os.path.join(project_a_dir, \\"project_A_notes_log.txt\\") if not os.path.exists(notes_path): with open(notes_path, \\"w\\") as notes_file: notes_file.write(\\"Sample notes for project_An\\") with open(notes_path, \\"r\\") as notes_file: content = notes_file.read() with open(notes_log_path, \\"w\\") as notes_log_file: notes_log_file.write(content)"},{"question":"# Question: Implement a Syslog-Based Logging System in Python Using the `syslog` module provided in the documentation, write a Python function to implement a flexible logging system. Your function will need to take in log messages and appropriately configure the logger. The goal is to provide a detailed, customizable logging system using the `syslog` functionality. Function Signature ```python def flexible_syslog_logger(messages: list, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER): pass ``` Parameters - `messages`: A list of tuples where each tuple contains a message and its corresponding priority level. For example: `[(\\"Initialization complete\\", syslog.LOG_INFO), (\\"An error occurred\\", syslog.LOG_ERR)]`. - `ident`: (Optional) A string which is prepended to every log message. Defaults to `sys.argv[0]` with leading path components stripped. - `logoption`: (Optional) An integer representing a bit field to set various logging options. Defaults to `0`. - Possible values include `syslog.LOG_PID`, `syslog.LOG_CONS`, etc. - `facility`: (Optional) An integer specifying the default facility for messages without an explicitly encoded facility. Defaults to `syslog.LOG_USER`. Constraints - You must use the `syslog` module functions to send messages and configure the logger. - Ensure that log messages without a newline are correctly formatted with one by the logger. - Use appropriate default values and provide flexibility for configuration. Example Usage ```python import syslog def flexible_syslog_logger(messages, ident=None, logoption=0, facility=syslog.LOG_USER): syslog.openlog(ident, logoption, facility) for message, priority in messages: syslog.syslog(priority, message) syslog.closelog() # Example execution messages = [ (\\"System is booting up\\", syslog.LOG_INFO), (\\"System has booted successfully\\", syslog.LOG_NOTICE), (\\"An unexpected error occurred\\", syslog.LOG_ERR) ] flexible_syslog_logger(messages, ident=\'myapp\', logoption=syslog.LOG_PID | syslog.LOG_CONS) ``` In this example, `flexible_syslog_logger` is called with a list of messages and priorities. The function configures the logger by prepending \\"myapp\\" to each message, includes the process ID, and sends messages to both the console and the system logger. Finally, it sends log messages with their respective priorities and closes the logger.","solution":"import syslog def flexible_syslog_logger(messages, ident=None, logoption=0, facility=syslog.LOG_USER): Logs messages with specified priorities using the syslog module. :param messages: List of tuples where each tuple contains a (message, priority) pair. :param ident: Optional string to prefix to each message. Defaults to script name. :param logoption: Optional bit field to set logging options. Defaults to 0. :param facility: Optional logging facility. Defaults to syslog.LOG_USER. if ident is None: import os import sys ident = os.path.basename(sys.argv[0]) syslog.openlog(ident, logoption, facility) for message, priority in messages: syslog.syslog(priority, message) syslog.closelog()"},{"question":"# Custom Estimator Implementation **Objective:** Create a custom `scikit-learn` compatible estimator called `CustomKNeighborsClassifier` that implements a K-Nearest Neighbors (KNN) classification algorithm. Your estimator should inherit from `BaseEstimator` and `ClassifierMixin`, and it should follow `scikit-learn` conventions. **Requirements:** 1. **Initialization (`__init__` method):** - Accept parameters: `n_neighbors` (default=5), `weights` (default=\'uniform\'). Store these parameters without any logic or validation. 2. **Fitting (`fit` method):** - Accept arguments `X` (shape: `[n_samples, n_features]`) and `y` (shape: `[n_samples]`). - Validate and store the `X` and `y` data. - Calculate and store the classes seen during fit. 3. **Predicting (`predict` method):** - Accept argument `X` (shape: `[n_samples, n_features]`). - Check if the estimator has been fitted. - Implement a KNN prediction: For each sample in `X`, find the `n_neighbors` nearest samples from the fitted data and predict the class based on the majority class among the neighbors. If `weights` is set to \'distance\', the closer neighbors have a stronger impact on the prediction. 4. **Attributes and Methods:** - The estimator should store classes in an attribute `classes_`. - Implement `get_params` and `set_params` for compatibility with `GridSearchCV`. - Implement the `score` method to calculate the accuracy of the classifier. **Constraints:** - Do not use the built-in KNeighborsClassifier from `scikit-learn`. **Performance Considerations:** - The code should handle large datasets efficiently using vectorized operations where possible. **Example Usage:** ```python from CustomKNeighborsClassifier import CustomKNeighborsClassifier from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris from sklearn.metrics import accuracy_score # Load data data = load_iris() X, y = data.data, data.target # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize custom classifier classifier = CustomKNeighborsClassifier(n_neighbors=3, weights=\'distance\') # Fit the model classifier.fit(X_train, y_train) # Predict y_pred = classifier.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") ``` **Implementation:** ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, validate_data class CustomKNeighborsClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5, weights=\'uniform\'): self.n_neighbors = n_neighbors self.weights = weights def fit(self, X, y): X, y = validate_data(self, X, y) self.classes_ = np.unique(y) self.X_ = X self.y_ = y self.n_features_in_ = X.shape[1] return self def predict(self, X): check_is_fitted(self, [\'X_\', \'y_\']) X = validate_data(self, X, reset=False) def euclidean_distance(a, b): return np.sqrt(np.sum((a - b) ** 2, axis=1)) predictions = [] for x in X: distances = euclidean_distance(self.X_, x) nearest_indices = np.argsort(distances)[:self.n_neighbors] nearest_labels = self.y_[nearest_indices] if self.weights == \'distance\': nearest_distances = distances[nearest_indices] weights = 1 / (nearest_distances + 1e-8) weighted_vote = np.bincount(nearest_labels, weights=weights) else: weighted_vote = np.bincount(nearest_labels) predicted_label = self.classes_[np.argmax(weighted_vote)] predictions.append(predicted_label) return np.array(predictions) def get_params(self, deep=True): return {\\"n_neighbors\\": self.n_neighbors, \\"weights\\": self.weights} def set_params(self, **params): for param, value in params.items(): setattr(self, param, value) return self def score(self, X, y): return np.mean(self.predict(X) == y) ```","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_X_y, check_array class CustomKNeighborsClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5, weights=\'uniform\'): self.n_neighbors = n_neighbors self.weights = weights def fit(self, X, y): X, y = check_X_y(X, y) self.X_ = X self.y_ = y self.classes_ = np.unique(y) self.n_features_in_ = X.shape[1] return self def predict(self, X): check_is_fitted(self, [\'X_\', \'y_\']) X = check_array(X) def euclidean_distance(a, b): return np.sqrt(np.sum((a - b) ** 2, axis=1)) predictions = [] for x in X: distances = euclidean_distance(self.X_, x) nearest_indices = np.argsort(distances)[:self.n_neighbors] nearest_labels = self.y_[nearest_indices] if self.weights == \'distance\': nearest_distances = distances[nearest_indices] weights = 1 / (nearest_distances + 1e-8) weighted_vote = np.bincount(nearest_labels, weights=weights) else: weighted_vote = np.bincount(nearest_labels) predicted_label = self.classes_[np.argmax(weighted_vote)] predictions.append(predicted_label) return np.array(predictions) def get_params(self, deep=True): return {\\"n_neighbors\\": self.n_neighbors, \\"weights\\": self.weights} def set_params(self, **params): for param, value in params.items(): setattr(self, param, value) return self def score(self, X, y): predictions = self.predict(X) return np.mean(predictions == y)"},{"question":"**Objective**: Implement a function that compares the contents of two directories both shallowly and deeply, identifies differences, and generates a comprehensive report. Task Write a Python function `compare_directories(dir1, dir2, shallow=True)` that compares two directories `dir1` and `dir2`. Your function should perform the following tasks: 1. Compare files in `dir1` and `dir2` shallowly if `shallow` is set to `True`, otherwise perform a deep comparison. 2. Return a dictionary with the following keys: - `match`: List of files that match. - `mismatch`: List of files that differ. - `errors`: List of files that could not be compared. - `unique_to_dir1`: List of files and subdirectories only in `dir1`. - `unique_to_dir2`: List of files and subdirectories only in `dir2`. Input - `dir1`: Path to the first directory (string). - `dir2`: Path to the second directory (string). - `shallow`: Boolean flag indicating if the comparison should be shallow. Defaults to `True`. Output - A dictionary with the comparison results. Constraints - Directory paths will always be valid and readable. - Python `filecmp` module will be used for file comparisons. Example ```python def compare_directories(dir1, dir2, shallow=True): # Implement your function here # Example usage: result = compare_directories(\'path/to/dir1\', \'path/to/dir2\', shallow=False) print(result) ``` Output: ```python { \'match\': [\'file1.txt\', \'file2.txt\'], \'mismatch\': [\'file3.txt\'], \'errors\': [\'file4.txt\'], \'unique_to_dir1\': [\'only_in_dir1\'], \'unique_to_dir2\': [\'only_in_dir2\'] } ``` Notes - Consider using the `filecmp` module\'s `cmpfiles` function for file comparison and the `dircmp` class for directory comparison. - Handle potential exceptions and ensure your function does not crash due to inaccessible files or other I/O issues.","solution":"import filecmp import os def compare_directories(dir1, dir2, shallow=True): Compare the contents of two directories both shallowly and deeply. Args: - dir1: Path to the first directory. - dir2: Path to the second directory. - shallow: Boolean flag indicating if the comparison should be shallow. Defaults to True. Returns: - A dictionary with comparison results. result = { \'match\': [], \'mismatch\': [], \'errors\': [], \'unique_to_dir1\': [], \'unique_to_dir2\': [] } dcmp = filecmp.dircmp(dir1, dir2) # Handle matched, mismatched and errors match, mismatch, errors = filecmp.cmpfiles(dir1, dir2, dcmp.common_files, shallow=shallow) result[\'match\'] = match result[\'mismatch\'] = mismatch result[\'errors\'] = errors # Handle unique files and directories result[\'unique_to_dir1\'] = dcmp.left_only result[\'unique_to_dir2\'] = dcmp.right_only return result"},{"question":"**Question Title: Scope and Exception Handling in Python** **Problem Description:** You are required to implement a function `calculate_operations` which takes a list of tuples as input. Each tuple contains two integers. The function should perform the following operations: 1. Calculate the sum of the two integers. 2. Multiply the two integers. 3. Divide the first integer by the second integer. While performing these operations, your function must handle the following exceptions: - If the integers in the tuple are not valid (i.e., they are not integers), it should raise a `TypeError` with the message \\"Invalid input type\\". - If division by zero is attempted, it should raise a `ZeroDivisionError` with the message \\"Division by zero is not allowed\\". Given a list of tuples, your function should return a dictionary where each key is the tuple (as a string) and the corresponding value is another dictionary with the keys \\"sum\\", \\"product\\", and \\"quotient\\" containing the results of the respective operations. If an exception occurs for any tuple, the corresponding value should be the exception message. **Function Signature:** ```python def calculate_operations(data: list[tuple]) -> dict: ``` **Example:** ```python # Example input data = [(3, 5), (10, 2), (8, 0), (7, \'a\')] # Example output { \'(3, 5)\': {\'sum\': 8, \'product\': 15, \'quotient\': 0.6}, \'(10, 2)\': {\'sum\': 12, \'product\': 20, \'quotient\': 5.0}, \'(8, 0)\': \'Division by zero is not allowed\', \'(7, \'a\')\': \'Invalid input type\' } ``` **Constraints:** - You can assume the list will contain at most 100 tuples. - Each tuple will have exactly two elements. - You can use exception handling as needed to handle errors gracefully. **Notes:** - Think carefully about where to place your try-except blocks to effectively handle different types of exceptions. - Use clear and descriptive error messages as specified in the problem statement.","solution":"def calculate_operations(data): Calculates the sum, product, and quotient of each tuple of integers from the input list. If exceptions arise, returns appropriate error messages. result = {} for pair in data: key = str(pair) result[key] = {} try: # Ensure both elements are integers if not (isinstance(pair[0], int) and isinstance(pair[1], int)): raise TypeError(\\"Invalid input type\\") num1, num2 = pair # Calculate sum result[key][\'sum\'] = num1 + num2 # Calculate product result[key][\'product\'] = num1 * num2 # Calculate quotient try: result[key][\'quotient\'] = num1 / num2 except ZeroDivisionError: result[key] = \'Division by zero is not allowed\' except TypeError: result[key] = \'Invalid input type\' return result"},{"question":"# PyArrow Integration with Pandas Objective To evaluate your understanding of integrating PyArrow with Pandas for enhanced data handling and performance. Problem Statement You are provided with a CSV file containing data about various transactions. Your goal is to read this data into a PyArrow-backed Pandas DataFrame, perform several data manipulations, and output the results. CSV Content The CSV file (`transactions.csv`) has the following structure: ``` TransactionID,Amount,Date,Category,IsComplete 1,19.95,2023-01-15,Food,True 2,100.00,2023-01-16,Utilities,False 3,15.65,2023-01-16,Food,True ``` Tasks: 1. **Reading Data**: - Read the `transactions.csv` file using PyArrow as the engine and ensure the DataFrame is PyArrow-backed. - Print the data types of each column to verify. 2. **Data Type Casting**: - Convert the `Amount` column to a PyArrow `decimal128` type with precision 5 and scale 2. - Convert the `Date` column to a PyArrow timestamp of nanoseconds (`ns`). - Display the DataFrame with the new data types. 3. **Data Manipulation**: - Extract and print all transactions categorized as `Food`. - Calculate and print the total `Amount` for complete transactions (`IsComplete` is True). 4. **Data IO**: - Export the manipulated DataFrame to a Parquet file (`transactions_output.parquet`). Ensure the output maintains the PyArrow-backed data types. Constraints - You should use Pandas functions with PyArrow support as much as possible. - Ensure correct DataFrame creation and manipulation using the PyArrow integration. Expected Output Format 1. Printed data types of each column after reading the CSV. 2. DataFrame display with the new data types for `Amount` and `Date`. 3. List of transactions categorized as `Food`. 4. Total `Amount` for complete transactions. 5. Save the final DataFrame as a Parquet file named `transactions_output.parquet`. Note You can assume the file `transactions.csv` is located in the same directory as your script/notebook. Function Signatures ```python import pandas as pd import pyarrow as pa def read_and_verify_csv(file_path: str): # Your code here pass def cast_data_types(df: pd.DataFrame): # Your code here pass def filter_and_calculate(df: pd.DataFrame): # Your code here pass def save_to_parquet(df: pd.DataFrame, output_file: str): # Your code here pass # Implement these functions and call them accordingly. ```","solution":"import pandas as pd import pyarrow as pa import pyarrow.parquet as pq def read_and_verify_csv(file_path: str): # Read the file using PyArrow as the engine df = pd.read_csv(file_path, engine=\'pyarrow\') print(df.dtypes) return df def cast_data_types(df: pd.DataFrame): # Convert Amount to decimal128 with precision 5 and scale 2 df[\'Amount\'] = df[\'Amount\'].astype(pa.decimal128(precision=5, scale=2).to_pandas_dtype()) # Convert Date to timestamp nanoseconds df[\'Date\'] = pd.to_datetime(df[\'Date\'], format=\'%Y-%m-%d\').astype(\'datetime64[ns]\') print(df.dtypes) return df def filter_and_calculate(df: pd.DataFrame): # Extract and print all transactions categorized as Food food_transactions = df[df[\'Category\'] == \'Food\'] print(\\"Food transactions:\\") print(food_transactions) # Calculate and print the total Amount for complete transactions total_amount_complete = df[df[\'IsComplete\'] == True][\'Amount\'].sum() print(f\\"Total amount for complete transactions: {total_amount_complete}\\") return food_transactions, total_amount_complete def save_to_parquet(df: pd.DataFrame, output_file: str): # Save DataFrame to a Parquet file ensuring PyArrow-backed types table = pa.Table.from_pandas(df) pq.write_table(table, output_file)"},{"question":"# Bytearray Manipulation and Analysis You are required to implement a series of functions to manipulate and analyze bytearrays. Each function must leverage the capabilities described in the provided documentation. Function 1: `create_bytearray_from_string` Create a function that takes a string as input and returns a bytearray object created from this string. **Input:** - `input_string` (str): A non-empty string. **Output:** - `bytearray`: A new bytearray object created from the input string. ```python def create_bytearray_from_string(input_string): pass ``` Function 2: `concatenate_bytearrays` Create a function that takes two bytearrays and returns a new bytearray that concatenates them. **Input:** - `bytearray1` (bytearray): First bytearray. - `bytearray2` (bytearray): Second bytearray. **Output:** - `bytearray`: A new bytearray object that is the concatenation of `bytearray1` and `bytearray2`. ```python def concatenate_bytearrays(bytearray1, bytearray2): pass ``` Function 3: `bytearray_info` Create a function that takes a bytearray as input and returns a tuple containing its size and its content as a string. **Input:** - `bytearray_obj` (bytearray): A bytearray object. **Output:** - `tuple`: A tuple where the first element is the size of the bytearray and the second element is the bytearray content as a string. ```python def bytearray_info(bytearray_obj): pass ``` Function 4: `resize_bytearray` Create a function that takes a bytearray and a new size, and resizes the bytearray to the new size. If the new size is larger, the additional elements should be set to zero; if smaller, the bytearray is truncated. **Input:** - `bytearray_obj` (bytearray): A bytearray object. - `new_size` (int): The new size for the bytearray. **Output:** - `bytearray`: The resized bytearray. ```python def resize_bytearray(bytearray_obj, new_size): pass ``` **Constraints:** 1. Do not use any `import` statements. Utilize only core Python functionalities associated with bytearrays. 2. Ensure that any changes to the bytearray are reflected appropriately in the output. 3. Consider edge cases where input strings or bytearrays might be empty, especially when concatenating or resizing to zero. **Example Usage:** ```python # Example usage of create_bytearray_from_string ba = create_bytearray_from_string(\\"hello\\") print(ba) # bytearray(b\'hello\') # Example usage of concatenate_bytearrays ba1 = create_bytearray_from_string(\\"123\\") ba2 = create_bytearray_from_string(\\"456\\") concatenated = concatenate_bytearrays(ba1, ba2) print(concatenated) # bytearray(b\'123456\') # Example usage of bytearray_info info = bytearray_info(ba) print(info) # (5, \'hello\') # Example usage of resize_bytearray resized_ba = resize_bytearray(ba, 10) print(resized_ba) # bytearray(b\'hellox00x00x00x00x00\') ``` Implement these functions considering the constraints and examples provided to pass the assessment.","solution":"def create_bytearray_from_string(input_string): Creates a bytearray object from the given string. Parameters: input_string (str): A non-empty string. Returns: bytearray: A new bytearray object created from the input string. return bytearray(input_string, \'utf-8\') def concatenate_bytearrays(bytearray1, bytearray2): Concatenates two bytearrays into a new bytearray. Parameters: bytearray1 (bytearray): First bytearray. bytearray2 (bytearray): Second bytearray. Returns: bytearray: A new bytearray object that concatenates bytearray1 and bytearray2. return bytearray1 + bytearray2 def bytearray_info(bytearray_obj): Returns the size and content of a bytearray as a tuple. Parameters: bytearray_obj (bytearray): A bytearray object. Returns: tuple: A tuple containing the size and content of the bytearray. return (len(bytearray_obj), bytearray_obj.decode(\'utf-8\')) def resize_bytearray(bytearray_obj, new_size): Resizes a bytearray to a new size. Parameters: bytearray_obj (bytearray): A bytearray object. new_size (int): The new size for the bytearray. Returns: bytearray: The resized bytearray. bytearray_obj = bytearray(bytearray_obj) # Create a mutable copy current_size = len(bytearray_obj) if new_size > current_size: bytearray_obj.extend([0] * (new_size - current_size)) else: bytearray_obj = bytearray_obj[:new_size] return bytearray_obj"},{"question":"**Async Subprocess Management** # Problem Statement You are required to implement a function that takes a list of shell commands, executes them concurrently using asynchronous I/O operations, and collects their output. Each command may optionally have an input string that should be sent to its standard input. Your function should: 1. Execute multiple shell commands concurrently. 2. Handle the standard output and standard error streams asynchronously. 3. Return a dictionary where each key is the command and the value is a tuple containing its standard output and standard error. # Function Signature ```python import asyncio from typing import List, Dict, Tuple async def execute_commands(commands: List[Tuple[str, str]]) -> Dict[str, Tuple[str, str]]: pass ``` # Input - `commands` (List[Tuple[str, str]]): A list of tuples where each tuple contains: - A string representing the shell command to execute. - A string representing the input to send to the command\'s standard input (can be an empty string if no input is needed). # Output - Returns (Dict[str, Tuple[str, str]]): A dictionary where the key is the command string, and the value is a tuple containing: - The standard output from the command (as a string). - The standard error from the command (as a string). # Constraints - The number of commands will not exceed 10. - Each command\'s execution will not exceed 10 seconds. - Ensure proper error handling to avoid deadlocks and capture command execution failures. # Example ```python commands = [ (\'ls /\', \'\'), (\'echo \\"Hello, World!\\"\', \'\'), (\'cat non_existent_file\', \'\') ] result = await execute_commands(commands) print(result) ``` Expected Output: ``` { \'ls /\': (\'binnbootndevn...\', \'\'), # Assume the standard output is truncated for brevity. \'echo \\"Hello, World!\\"\': (\'Hello, World!n\', \'\'), \'cat non_existent_file\': (\'\', \'cat: non_existent_file: No such file or directoryn\') } ``` # Notes - Utilize `asyncio.create_subprocess_exec` or `asyncio.create_subprocess_shell` for creating subprocesses asynchronously. - Use appropriate error handling to manage command execution failures and avoid deadlocks. - Ensure all inputs and outputs are handled properly to facilitate correct and efficient asyncio operations. # Hint Make good use of `asyncio.gather` to execute the commands concurrently.","solution":"import asyncio from typing import List, Dict, Tuple async def run_command(command: str, input_data: str) -> Tuple[str, str]: Executes a single shell command asynchronously and returns the output and error. proc = await asyncio.create_subprocess_shell( command, stdin=asyncio.subprocess.PIPE, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate(input=input_data.encode()) return stdout.decode(), stderr.decode() async def execute_commands(commands: List[Tuple[str, str]]) -> Dict[str, Tuple[str, str]]: Executes multiple shell commands concurrently and returns their outputs. Each command is provided with input data that is sent to its STDIN. :param commands: List of tuples [(command, input_data)] :return: Dictionary { command: (stdout, stderr) } tasks = [run_command(command, input_data) for command, input_data in commands] results = await asyncio.gather(*tasks) return {command: result for (command, input_data), result in zip(commands, results)} # Example usage: # commands = [ # (\'ls /\', \'\'), # (\'echo \\"Hello, World!\\"\', \'\'), # (\'cat non_existent_file\', \'\') # ] # result = asyncio.run(execute_commands(commands)) # print(result)"},{"question":"**Objective**: Demonstrate your understanding of the \\"webbrowser\\" module by implementing and registering a custom browser controller. **Problem Statement**: You are tasked with enhancing a Python script to use a custom web browser for displaying URLs. Specifically, you will register a custom browser that simulates opening URLs by printing a statement to the console rather than launching an actual browser. # Instructions: 1. **Implement a Custom Browser Controller**: - Create a class `ConsoleBrowser` which includes methods corresponding to the following functionalities: - `open(url, new=0, autoraise=True)`: Print the URL along with information about whether it is a new window or tab. - `open_new(url)`: Print the URL with a message indicating it is opened in a new window. - `open_new_tab(url)`: Print the URL with a message indicating it is opened in a new tab. 2. **Register the Custom Browser**: - Register `ConsoleBrowser` under the name \\"console\\" using the `webbrowser.register` function. - Set this custom browser as the preferred browser. 3. **Demonstrate Usage**: - Using the `webbrowser.get` function, obtain an instance of your custom browser and use it to: - Open the URL \\"https://www.python.org\\" in the default mode. - Open the URL \\"https://www.python.org\\" in a new window. - Open the URL \\"https://www.python.org\\" in a new tab. # Code Constraints: 1. You should import and use the `webbrowser` module exclusively. 2. Your solution should ensure that the `ConsoleBrowser` is registered as the preferred browser when the script runs. # Expected Output: Your implementation should correctly simulate the browser actions using print statements. For example: ``` Opening URL https://www.python.org with new=0 and autoraise=True. Opening URL https://www.python.org in a new window. Opening URL https://www.python.org in a new tab. ``` # Performance Requirements: Since this is a simulation and does not involve actual browser launching, there are no special performance constraints beyond correct function implementation. # Deliverables: Submit the complete Python script that: - Defines the `ConsoleBrowser` class. - Registers it with `webbrowser.register`. - Demonstrates its usage through various `webbrowser` functions. ```python # Your implementation here ``` **Notes**: - Ensure your print statements clearly indicate the action being simulated. - Think carefully about how you manage the `new` and `autoraise` parameters.","solution":"import webbrowser class ConsoleBrowser: def open(self, url, new=0, autoraise=True): mode = \'default mode\' if new == 1: mode = \'new window\' elif new == 2: mode = \'new tab\' print(f\\"Opening URL {url} in {mode} with autoraise={autoraise}.\\") def open_new(self, url): print(f\\"Opening URL {url} in a new window.\\") def open_new_tab(self, url): print(f\\"Opening URL {url} in a new tab.\\") # Register the custom browser webbrowser.register(\'console\', None, ConsoleBrowser()) # Set the custom browser as the preferred browser console_browser = webbrowser.get(\'console\') # Demonstrate usage console_browser.open(\\"https://www.python.org\\") console_browser.open(\\"https://www.python.org\\", new=1) console_browser.open(\\"https://www.python.org\\", new=2)"},{"question":"Objective The goal of this question is to assess your understanding of the `cmath` module in Python, particularly with respect to handling complex numbers, converting between rectangular and polar coordinates, and utilizing trigonometric and power functions. Problem Statement Write a function `complex_analysis` that performs the following operations: 1. **Argument:** Takes a complex number `z` as input. 2. **Calculations:** 1. **Convert** the complex number `z` from rectangular coordinates to polar coordinates. 2. **Compute** the exponential of the phase angle of `z`. 3. **Find** the logarithm (base 10) of the modulus of `z`. 4. **Return** the results in a dictionary. Function Signature ```python def complex_analysis(z: complex) -> dict: pass ``` Input - `z`: A complex number. It can be a Python `complex` object, an integer, or a floating-point number. Output - A dictionary with the following keys: - `\'polar\'`: A tuple representing the polar coordinates `(r, phi)` of the input complex number `z`. - `\'exp_phi\'`: The exponential of the phase angle `phi` (`e ** phi` where `phi` is the phase of `z`). - `\'log10_r\'`: The base-10 logarithm of the modulus `r` (log10 of the absolute value of `z`). Example ```python z = 1 + 1j result = complex_analysis(z) print(result) ``` Expected output: ```python { \\"polar\\": (1.4142135623730951, 0.7853981633974483), \\"exp_phi\\": 2.1932800507380152, \\"log10_r\\": 0.15051499783199058 } ``` Constraints - You can assume that the input `z` will always be a valid number (integer, float or complex). Notes - Use the `cmath` module to perform the necessary calculations. - Ensure that your function handles edge cases, such as when the imaginary part is zero. - The output values should preserve the precision provided by `cmath`.","solution":"import cmath import math def complex_analysis(z: complex) -> dict: Analyzes a complex number by converting it to polar coordinates, computing the exponential of its phase angle, and finding the base-10 logarithm of its modulus. Args: z (complex): Complex number to analyze. Returns: dict: A dictionary with keys \'polar\', \'exp_phi\', and \'log10_r\'. # Convert to polar coordinates r, phi = cmath.polar(z) # Compute exponential of phase angle exp_phi = cmath.exp(phi).real # phi is a float, exp(phi) is also float # Logarithm base 10 of modulus log10_r = math.log10(r) return {\'polar\': (r, phi), \'exp_phi\': exp_phi, \'log10_r\': log10_r}"},{"question":"**Question: Loading, Managing, and Visualizing Data with Scikit-learn** Given the utility functions described in the scikit-learn datasets module, your task is to write functions that demonstrate the loading and basic manipulation of datasets. Specifically, you should: 1. Load the sample images provided by scikit-learn and display them. 2. Load a dataset from the openml repository. 3. Perform basic data analysis on the loaded dataset, including displaying the shape of the data, and a summary of the different target classes. # Requirements 1. **Function 1: `display_sample_images()`** - This function should load and display the sample images provided by scikit-learn. - There is no input to this function. - This function does not return anything; it only displays the images. 2. **Function 2: `load_openml_dataset(dataset_name: str, version: int)`** - This function should load a dataset from the openml repository using the dataset\'s name and version. - *Input*: - `dataset_name` (str): The name of the dataset to load. - `version` (int): The version of the dataset to load. - *Output*: A tuple containing two elements: `data` (a pandas DataFrame with the feature data) and `target` (a pandas Series with the target data). 3. **Function 3: `analyze_dataset(data: pd.DataFrame, target: pd.Series)`** - This function should accept the data loaded using `load_openml_dataset` and provide basic analysis. - *Input*: - `data` (pd.DataFrame): The dataframe containing feature data. - `target` (pd.Series): The series containing target data. - *Output*: - The shape of the dataset. - A list of unique target classes. # Example Usage ```python # Display sample images display_sample_images() # Load and analyze the \'miceprotein\' dataset, version 4 data, target = load_openml_dataset(\'miceprotein\', 4) dataset_shape, unique_classes = analyze_dataset(data, target) print(f\\"Dataset shape: {dataset_shape}\\") print(f\\"Unique target classes: {unique_classes}\\") ``` # Constraints - You must use `sklearn.datasets.load_sample_image` and `sklearn.datasets.fetch_openml` for loading respective datasets. - You can assume that all necessary imports are handled (e.g., `import pandas as pd`).","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_sample_image, fetch_openml import pandas as pd def display_sample_images(): Load and display the sample images provided by scikit-learn. # List of sample image filenames sample_images = [\'china\', \'flower\'] for image_name in sample_images: img = load_sample_image(image_name + \'.jpg\') plt.imshow(img) plt.title(image_name.title()) plt.axis(\'off\') plt.show() def load_openml_dataset(dataset_name: str, version: int): Load a dataset from the openml repository. Parameters: - dataset_name (str): The name of the dataset to load. - version (int): The version of the dataset to load. Returns: - data (pd.DataFrame): A dataframe containing the feature data. - target (pd.Series): A series containing the target data. dataset = fetch_openml(name=dataset_name, version=version, as_frame=True) data = dataset.data target = dataset.target return data, target def analyze_dataset(data: pd.DataFrame, target: pd.Series): Perform basic analysis on the loaded dataset. Parameters: - data (pd.DataFrame): The dataframe containing feature data. - target (pd.Series): The series containing target data. Returns: - shape (tuple): The shape of the dataset. - unique_classes (list): A list of unique target classes. shape = data.shape unique_classes = target.unique().tolist() return shape, unique_classes"},{"question":"Objective Implement and compare Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) for classification and dimensionality reduction. Evaluate the performance using different covariance estimators and visualize the results. Problem Statement 1. **Data Preparation**: Download the Iris dataset from `sklearn.datasets`. Use only two classes (`setosa` and `versicolor`) to simplify the problem into binary classification. 2. **Implement LDA and QDA Models**: - Fit both LDA and QDA models using the training data. - Use \'lsqr\', \'eigen\', and the default (\'svd\' for LDA, \'lsqr\' for QDA) solvers for LDA. - Apply and visualize dimensionality reduction using LDA with `n_components=1`. 3. **Regularization with Shrinkage**: - Use the `shrinkage` parameter with the \'lsqr\' solver in LDA. Test with \'auto\' and a range of manual shrinkage values (e.g., 0.1, 0.5, 0.9). 4. **Evaluate Performance**: - Compute and print the accuracy of each model on the test data. - Plot the decision boundaries and the dimensionality reduction results. - Compare performance with different covariance estimators (Empirical, Ledoit Wolf, and OAS) using `sklearn.covariance`. Input Format - None Output Format - Print the accuracy of each model. - Display plots comparing decision boundaries for each solver and shrinkage parameter. - Display plots of LDA dimensionality reduction results. Constraints - Ensure reproducibility by setting a random seed where applicable. - Use scikit-learn and matplotlib libraries. Example Solution Outline ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.covariance import LedoitWolf, OAS # Load the dataset iris = load_iris() X = iris.data[iris.target != 2] # Use only \'setosa\' and \'versicolor\' y = iris.target[iris.target != 2] # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit and evaluate LDA and QDA with different solvers lda_solvers = [\'svd\', \'lsqr\', \'eigen\'] qda = QuadraticDiscriminantAnalysis() results = {} for solver in lda_solvers: lda = LinearDiscriminantAnalysis(solver=solver) lda.fit(X_train, y_train) y_pred = lda.predict(X_test) results[f\'LDA ({solver})\'] = accuracy_score(y_test, y_pred) qda.fit(X_train, y_train) y_pred = qda.predict(X_test) results[\'QDA\'] = accuracy_score(y_test, y_pred) # Regularization with Shrinkage shrinkage_values = [\'auto\', 0.1, 0.5, 0.9] for shrinkage in shrinkage_values: lda = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=shrinkage) lda.fit(X_train, y_train) y_pred = lda.predict(X_test) results[f\'LDA (lsqr, shrinkage={shrinkage})\'] = accuracy_score(y_test, y_pred) # Dimensionality Reduction with LDA lda = LinearDiscriminantAnalysis(n_components=1) X_r2 = lda.fit(X, y).transform(X) plt.figure() for color, i, target_name in zip([\\"red\\", \\"blue\\"], [0, 1], iris.target_names[:2]): plt.scatter(X_r2[y == i, 0], np.zeros_like(X_r2[y == i]), alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA Dimensionality Reduction\') plt.show() # Compare different covariance estimators estimators = { \'Empirical\': None, \'LedoitWolf\': LedoitWolf(), \'OAS\': OAS() } for name, estimator in estimators.items(): lda = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=estimator) lda.fit(X_train, y_train) y_pred = lda.predict(X_test) results[f\'LDA (lsqr, {name} Covariance Estimator)\'] = accuracy_score(y_test, y_pred) # Print the results for key, value in results.items(): print(f\'{key}: {value:.2f}\') # Decision Boundary Visualization (simplified) # ... ``` Note Ensure you follow the coding conventions and provide visualization plots to validate your results.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.covariance import LedoitWolf, OAS from sklearn.preprocessing import StandardScaler # Load the dataset iris = load_iris() X = iris.data[iris.target != 2] # Use only \'setosa\' and \'versicolor\' y = iris.target[iris.target != 2] # Standardize the dataset scaler = StandardScaler() X = scaler.fit_transform(X) # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit and evaluate LDA and QDA with different solvers lda_solvers = [\'svd\', \'lsqr\', \'eigen\'] results = {} for solver in lda_solvers: lda = LinearDiscriminantAnalysis(solver=solver) lda.fit(X_train, y_train) y_pred = lda.predict(X_test) results[f\'LDA ({solver})\'] = accuracy_score(y_test, y_pred) # Fit and evaluate QDA qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) y_pred = qda.predict(X_test) results[\'QDA\'] = accuracy_score(y_test, y_pred) # Regularization with Shrinkage shrinkage_values = [\'auto\', 0.1, 0.5, 0.9] for shrinkage in shrinkage_values: lda = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=shrinkage) lda.fit(X_train, y_train) y_pred = lda.predict(X_test) results[f\'LDA (lsqr, shrinkage={shrinkage})\'] = accuracy_score(y_test, y_pred) # Dimensionality Reduction with LDA lda = LinearDiscriminantAnalysis(n_components=1) X_r2 = lda.fit(X, y).transform(X) plt.figure() for color, i, target_name in zip([\\"red\\", \\"blue\\"], [0, 1], iris.target_names[:2]): plt.scatter(X_r2[y == i, 0], np.zeros_like(X_r2[y == i]), alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\', shadow=False, scatterpoints=1) plt.title(\'LDA Dimensionality Reduction\') plt.show() # Compare different covariance estimators estimators = { \'Empirical\': None, \'LedoitWolf\': LedoitWolf(), \'OAS\': OAS() } for name, estimator in estimators.items(): lda = LinearDiscriminantAnalysis(solver=\'lsqr\', covariance_estimator=estimator) lda.fit(X_train, y_train) y_pred = lda.predict(X_test) results[f\'LDA (lsqr, {name} Covariance Estimator)\'] = accuracy_score(y_test, y_pred) # Print the results for key, value in results.items(): print(f\'{key}: {value:.2f}\')"},{"question":"Objective Implement a Python function using the multiprocessing package that reads and processes text data in parallel. The function should demonstrate the understanding of process creation, communication through Queues, and synchronization using Locks. Problem Statement Given a list of file paths containing text data, build a Python function `process_files_in_parallel(file_paths: List[str]) -> int` that: 1. Reads the text files in parallel using multiple processes. 2. Counts the total occurrences of the word \\"Python\\" (case-insensitive) across all files. 3. Uses a manager dictionary to store the result from each process. 4. Ensures that no two processes write to the manager dictionary at the same time using Locks. # Function Specification **Function Name**: `process_files_in_parallel` **Inputs**: - `file_paths` (List of strings): A list of file paths to text files. **Output**: - Returns an integer representing the total count of occurrences of the word \\"Python\\" across all files. **Constraints**: - You must utilize the multiprocessing package for parallel processing of files. - Processes should use a manager dictionary to store the intermediate results. - Ensure that the access to the shared dictionary is synchronized using Locks. **Example**: ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] result = process_files_in_parallel(file_paths) print(result) ``` # Example Execution Suppose `file1.txt` contains \\"Python is fun. Python makes life easier.\\", `file2.txt` contains \\"I love Python programming.\\", and `file3.txt` contains \\"Welcome to the world of Python and more PYTHON.\\". The function should return 5 after processing these files in parallel. # Hints - Use the `Process` class to create and manage separate processes for reading files. - Use a `Manager().dict()` to maintain a shared dictionary among the processes. - Use a `Lock()` to manage concurrent writes to the shared dictionary.","solution":"from multiprocessing import Process, Manager, Lock import os def count_python_occurrences(file_path, shared_dict, lock): with open(file_path, \'r\') as file: text = file.read().lower() count = text.count(\'python\') with lock: shared_dict[file_path] = count def process_files_in_parallel(file_paths): manager = Manager() shared_dict = manager.dict() lock = Lock() processes = [] for file_path in file_paths: p = Process(target=count_python_occurrences, args=(file_path, shared_dict, lock)) p.start() processes.append(p) for p in processes: p.join() total_count = sum(shared_dict.values()) return total_count"},{"question":"# Spectral Co-Clustering Implementation Objective: In this coding assessment, you are required to implement the Spectral Co-Clustering algorithm from scratch using the concepts provided in the scikit-learn documentation. Your implementation will involve preprocessing the input data matrix, applying singular value decomposition, and clustering the resulting vectors to identify biclusters. Task: Implement the Spectral Co-Clustering algorithm. You should create a function `spectral_co_clustering` that takes a matrix as input and returns the row and column clusters. Function Signature: ```python def spectral_co_clustering(data: np.ndarray, n_clusters: int) -> Tuple[np.ndarray, np.ndarray]: pass ``` Input: - `data` (np.ndarray): A 2D numpy array representing the data matrix (shape: (m, n)). - `n_clusters` (int): The number of clusters to form. Output: - Returns a tuple of two numpy arrays: - Row cluster labels (np.ndarray): An array of shape `(m,)` where each element represents the cluster label of the corresponding row. - Column cluster labels (np.ndarray): An array of shape `(n,)` where each element represents the cluster label of the corresponding column. Constraints: - The input matrix `data` may include positive and negative values. - Your implementation should handle cases where rows and columns may not have equal weight. Example: ```python import numpy as np data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) row_clusters, column_clusters = spectral_co_clustering(data, n_clusters=2) print(\\"Row Clusters:\\", row_clusters) # Output: array representing row cluster assignments print(\\"Column Clusters:\\", column_clusters) # Output: array representing column cluster assignments ``` Requirements: 1. **Preprocessing**: - Normalize the data matrix `A` as ( A_n = R^{-1/2} A C^{-1/2} ) where ( R ) and ( C ) are diagonal matrices. 2. **Singular Value Decomposition**: - Perform SVD on the normalized matrix ( A_n ) to obtain the singular vectors. 3. **Clustering**: - Use k-means clustering on the appropriate singular vectors to partition the rows and columns. 4. **Efficiency**: - Ensure your implementation is efficient and scalable to larger matrices. Note: Do not use any high-level scikit-learn biclustering functions directly. You may, however, use libraries like NumPy for matrix operations and SciPy for helper functions.","solution":"import numpy as np from sklearn.cluster import KMeans from scipy.linalg import svd def spectral_co_clustering(data: np.ndarray, n_clusters: int) -> tuple: Implements the Spectral Co-Clustering algorithm to identify biclusters in a data matrix. Parameters: data (np.ndarray): A 2D numpy array representing the data matrix (shape: (m, n)). n_clusters (int): The number of clusters to form. Returns: tuple: Row cluster labels (np.ndarray) and Column cluster labels (np.ndarray). # Step 1: Normalize the data matrix row_sums = np.sum(data, axis=1) col_sums = np.sum(data, axis=0) # Avoid division by zero row_factors = np.sqrt(row_sums) row_factors[row_factors == 0] = 1 col_factors = np.sqrt(col_sums) col_factors[col_factors == 0] = 1 row_norm_matrix = np.diag(1 / row_factors) col_norm_matrix = np.diag(1 / col_factors) normalized_data = row_norm_matrix @ data @ col_norm_matrix # Step 2: Singular Value Decomposition U, s, Vh = svd(normalized_data, full_matrices=False) # Step 3: Use k-means on the rows/columns of U and V to identify clusters row_vectors = U[:, :n_clusters] col_vectors = Vh.T[:, :n_clusters] kmeans_row = KMeans(n_clusters=n_clusters, random_state=0).fit(row_vectors) kmeans_col = KMeans(n_clusters=n_clusters, random_state=0).fit(col_vectors) row_clusters = kmeans_row.labels_ col_clusters = kmeans_col.labels_ return row_clusters, col_clusters"},{"question":"IP Address Block Checker # Objective In this assignment, you will implement a function that processes a list of IP addresses and determines various properties about them using the `ipaddress` module. Specifically, you\'ll determine which IP addresses belong to a particular network range and extract relevant details from these IP addresses. # Requirements You are required to implement the following function: ```python def check_ip_addresses(ip_list, network_str): This function takes a list of IP addresses (as strings) and a network (as a string) and returns a dictionary mapping each IP address to a dictionary of properties. Parameters: - ip_list (list of str): A list of IP addresses. - network_str (str): A network definition in CIDR format. Returns: - dict: A dictionary where each IP address (key) maps to another dictionary (value) containing properties such as \\"version\\", \\"network\\", \\"is_in_network\\", \\"network_size\\", \\"netmask\\", \\"hostmask\\", \\"exploded\\", and \\"compressed\\". ``` # Input Constraints - `ip_list` contains at least one and at most 10 IP addresses. - The IP addresses in `ip_list` and `network_str` are valid IPv4 or IPv6 addresses. # Properties to Determine 1. `version`: The IP version (4 or 6). 2. `network`: The network to which this IP belongs. 3. `is_in_network`: Boolean indicating whether this IP belongs to the given network. 4. `network_size`: The number of individual addresses in this network. 5. `netmask`: The netmask of the network. 6. `hostmask`: The hostmask of the network. 7. `exploded`: The exploded form of the IP address. 8. `compressed`: The compressed form of the IP address. # Example ```python ip_list = [\\"192.0.2.1\\", \\"2001:db8::1\\", \\"192.0.2.15\\"] network_str = \\"192.0.2.0/24\\" output = check_ip_addresses(ip_list, network_str) print(output) ``` Expected Output: ```python { \\"192.0.2.1\\": { \\"version\\": 4, \\"network\\": \\"192.0.2.0/24\\", \\"is_in_network\\": True, \\"network_size\\": 256, \\"netmask\\": \\"255.255.255.0\\", \\"hostmask\\": \\"0.0.0.255\\", \\"exploded\\": \\"192.0.2.1\\", \\"compressed\\": \\"192.0.2.1\\" }, ... } ``` # Notes - You must use the `ipaddress` module for all IP-related operations. - Ensure to handle both IPv4 and IPv6 addresses properly. # Constraints - Performance is not a primary concern for this task. - Raise appropriate exceptions if invalid input is provided.","solution":"import ipaddress def check_ip_addresses(ip_list, network_str): This function takes a list of IP addresses (as strings) and a network (as a string) and returns a dictionary mapping each IP address to a dictionary of properties. Parameters: - ip_list (list of str): A list of IP addresses. - network_str (str): A network definition in CIDR format. Returns: - dict: A dictionary where each IP address (key) maps to another dictionary (value) containing properties such as \\"version\\", \\"network\\", \\"is_in_network\\", \\"network_size\\", \\"netmask\\", \\"hostmask\\", \\"exploded\\", and \\"compressed\\". network = ipaddress.ip_network(network_str, strict=False) result = {} for ip_str in ip_list: ip = ipaddress.ip_address(ip_str) is_in_network = ip in network result[ip_str] = { \'version\': ip.version, \'network\': str(network), \'is_in_network\': is_in_network, \'network_size\': network.num_addresses, \'netmask\': str(network.netmask), \'hostmask\': str(network.hostmask), \'exploded\': ip.exploded, \'compressed\': ip.compressed } return result"},{"question":"**Question: Synthetic Data Generation and Model Implementation using Scikit-Learn** --- **Objective**: This assignment aims to assess your understanding of synthetic data generation using scikit-learn\'s dataset generators and your ability to implement and evaluate a supervised learning model using the generated data. **Task Description**: 1. **Data Generation**: - Generate a synthetic dataset using the `make_classification` function from scikit-learn. Parameters to be used: - `n_features=20`: The number of features. - `n_informative=15`: The number of informative features. - `n_redundant=5`: The number of redundant features. - `n_clusters_per_class=2`: The number of clusters per class. - `random_state=42`: Seed to ensure reproducibility. - Split the generated dataset into training (80%) and testing (20%) datasets. 2. **Model Implementation**: - Implement a Random Forest classifier to predict the class labels of the generated dataset. - Train the model on the training dataset and evaluate it using the testing dataset. 3. **Evaluation**: - Calculate and print out the following performance metrics: - Accuracy - Precision - Recall - F1-score - Visualize the feature importances from the Random Forest model. **Constraints and Requirements**: - Use `sklearn.ensemble.RandomForestClassifier` for the implementation of the classifier. - Use appropriate metrics from `sklearn.metrics` for evaluation. - Ensure that the feature importances are visualized using a bar plot. Provide your solution in a Python function named `run_classification_pipeline()` which should: - Generate and split the dataset. - Train the model. - Evaluate and print the specified performance metrics. - Plot the feature importances. **Expected Function Signature**: ```python def run_classification_pipeline(): # Your code here ``` **Hints**: - Refer to the following scikit-learn modules for assistance: - `sklearn.datasets.make_classification` - `sklearn.model_selection.train_test_split` - `sklearn.ensemble.RandomForestClassifier` - `sklearn.metrics` **Note**: Make sure your solution does not generate any errors and properly handles the required operations.","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score import matplotlib.pyplot as plt def run_classification_pipeline(): # Step 1: Data Generation X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_clusters_per_class=2, random_state=42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 2: Model Implementation clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Step 3: Evaluation y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) # Print out the performance metrics print(f\'Accuracy: {accuracy:.4f}\') print(f\'Precision: {precision:.4f}\') print(f\'Recall: {recall:.4f}\') print(f\'F1-score: {f1:.4f}\') # Visualize feature importances feature_importances = clf.feature_importances_ plt.figure(figsize=(10, 6)) plt.bar(range(20), feature_importances, align=\'center\') plt.xlabel(\'Feature index\') plt.ylabel(\'Feature importance\') plt.title(\'Feature Importances from Random Forest\') plt.show()"},{"question":"**Problem Statement:** You are tasked with analyzing the age and fare distribution of Titanic passengers based on their online dataset. Use Seaborn\'s `violinplot` function to create visual representations for this analysis. **Requirements:** 1. **Load the Titanic Dataset**: - Use the Seaborn `load_dataset` function to load the Titanic dataset. 2. **Visual Representation**: - Create a violin plot for the distribution of passenger ages. - Create another violin plot to compare passenger fares across different age groups. 3. **Customization**: - Modify your plots using at least three different parameters each, such as `hue`, `split`, `inner`, `fill`, etc. 4. **Advanced Customization**: - Use the `inner_kws` parameter to customize the inner elements of the violin plot. - Normalize the width of the violins to represent the number of observations (`density_norm=\\"count\\"`). **Input Formats**: None. The data will be loaded directly from Seaborn. **Output**: Two customized violin plots. **Constraints**: - You must use the Seaborn `violinplot` function. - Include proper titles and labels for the plots for better readability. **Performance Requirements**: - The solution should plot the graphs without any significant delay. **Example Solution**: ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the Titanic dataset data = sns.load_dataset(\\"titanic\\") # Step 2: Create a violin plot for the age distribution plt.figure(figsize=(10, 6)) sns.violinplot(x=data[\\"age\\"], inner=\\"quart\\", bw_adjust=0.5, cut=0) plt.title(\\"Violin Plot for Age Distribution of Titanic Passengers\\") plt.xlabel(\\"Age\\") plt.show() # Step 3: Create violin plots for fare distribution across age groups plt.figure(figsize=(10, 6)) decades = lambda x: f\\"{int(x)}â€“{int(x + 10)}\\" sns.violinplot(x=data[\\"age\\"].round(-1), y=data[\\"fare\\"], split=True, inner=\\"point\\", density_norm=\\"count\\", bw_adjust=0.5, cut=0, formatter=decades) plt.title(\\"Violin Plot for Fare Distribution Across Age Groups\\") plt.xlabel(\\"Age Group (Rounded to Decades)\\") plt.ylabel(\\"Fare\\") plt.show() # Step 4: Customize the inner elements using inner_kws plt.figure(figsize=(10, 6)) sns.violinplot(x=data[\\"age\\"], inner_kws=dict(box_width=15, whis_width=2, color=\\".8\\")) plt.title(\\"Customized Inner Elements Violin Plot for Age\\") plt.xlabel(\\"Age\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the Titanic dataset data = sns.load_dataset(\\"titanic\\") # Step 2: Create a violin plot for the age distribution plt.figure(figsize=(10, 6)) sns.violinplot(x=data[\\"age\\"], inner=\\"quartile\\", bw=0.5, cut=0, fill=True) plt.title(\\"Violin Plot for Age Distribution of Titanic Passengers\\") plt.xlabel(\\"Age\\") plt.show() # Step 3: Create violin plots for fare distribution across age groups plt.figure(figsize=(10, 6)) data[\'age_group\'] = data[\'age\'] // 10 * 10 # group ages by decade sns.violinplot(x=data[\\"age_group\\"], y=data[\\"fare\\"], split=True, inner=\\"point\\", scale=\\"count\\", bw=0.5, cut=0) plt.title(\\"Violin Plot for Fare Distribution Across Age Groups\\") plt.xlabel(\\"Age Group (Rounded to Decades)\\") plt.ylabel(\\"Fare\\") plt.show() # Step 4: Customize the inner elements using inner_kws plt.figure(figsize=(10, 6)) sns.violinplot(x=data[\\"age\\"], inner=\\"box\\", inner_kws=dict(box_width=15, whis_width=2, color=\\".8\\")) plt.title(\\"Violin Plot with Customized Inner Elements for Age\\") plt.xlabel(\\"Age\\") plt.show()"},{"question":"Implementing a Generic Function with `singledispatch` and A Memoized Function with `lru_cache` Background This assessment will test your understanding of advanced concepts in the `functools` module, specifically focusing on `singledispatch` and `lru_cache` decorators. You are required to implement a generic function that dispatches based on argument type and a memoized function that caches results to optimize performance. Task 1. **Implement a Generic Function using `singledispatch`** Write a function `process_data` that processes different types of input data using `singledispatch`. Implement specific logic for the following types: - `int`: Increment the integer value by 1. - `list`: Return a new list with each element squared. - `str`: Return the reversed string. ```python from functools import singledispatch @singledispatch def process_data(data): raise NotImplementedError(\\"Unsupported type\\") @process_data.register def _(data: int): return data + 1 @process_data.register def _(data: list): return [x ** 2 for x in data] @process_data.register def _(data: str): return data[::-1] ``` 2. **Implement a Memoized Function using `lru_cache`** Write a function `fibonacci` that computes the nth Fibonacci number. Use the `lru_cache` decorator to memoize the function and avoid redundant calculations. ```python from functools import lru_cache @lru_cache(maxsize=None) def fibonacci(n): if n < 2: return n return fibonacci(n-1) + fibonacci(n-2) ``` Constraints - The `process_data` function should handle any other types by raising a `NotImplementedError`. - The `fibonacci` function should use memoization to be efficient for large values of `n`. Examples ```python # Testing process_data assert process_data(7) == 8 assert process_data([1, 2, 3]) == [1, 4, 9] assert process_data(\\"hello\\") == \\"olleh\\" # Testing fibonacci assert fibonacci(0) == 0 assert fibonacci(1) == 1 assert fibonacci(10) == 55 assert fibonacci(20) == 6765 ``` Write your implementations of `process_data` and `fibonacci` functions. Ensure your solution is efficient and correctly handles the inputs specified in the example.","solution":"from functools import singledispatch, lru_cache @singledispatch def process_data(data): raise NotImplementedError(\\"Unsupported type\\") @process_data.register def _(data: int): return data + 1 @process_data.register def _(data: list): return [x ** 2 for x in data] @process_data.register def _(data: str): return data[::-1] @lru_cache(maxsize=None) def fibonacci(n): if n < 2: return n return fibonacci(n-1) + fibonacci(n-2)"},{"question":"# Advanced Coding Assessment: Manipulating Python Sets Objective: Design and implement a Python C extension function to manipulate set objects using the provided Python C API. Your task is to create a function that merges two sets and returns a new set containing unique elements from both input sets. Function Signature: ```python PyObject* merge_sets(PyObject* set1, PyObject* set2) ``` Input: - `set1`: A Python `set` object. - `set2`: A Python `set` object. Output: - Returns a new Python `set` object containing unique elements from both `set1` and `set2`. Constraints: 1. Both `set1` and `set2` are guaranteed to be valid Python `set` objects. 2. The function should handle large sets efficiently. Guidelines: 1. **Initialization**: You will need to validate the input sets and create a new set to store the result. 2. **Merging Elements**: Use appropriate C API functions to iterate through the input sets and add elements to the new set. 3. **Error Handling**: Ensure that your function properly handles any possible errors, such as memory allocation failures. 4. **Performance**: Your function should be optimized for performance, given that sets can be large. Example: ```c #include <Python.h> PyObject* merge_sets(PyObject* set1, PyObject* set2) { if (!PySet_Check(set1) || !PySet_Check(set2)) { PyErr_SetString(PyExc_TypeError, \\"Both arguments must be sets\\"); return NULL; } PyObject *result_set = PySet_New(NULL); if (!result_set) { return NULL; } PyObject *iterator = PyObject_GetIter(set1); if (!iterator) { Py_DECREF(result_set); return NULL; } PyObject *item; while ((item = PyIter_Next(iterator))) { if (PySet_Add(result_set, item) == -1) { Py_DECREF(item); Py_DECREF(iterator); Py_DECREF(result_set); return NULL; } Py_DECREF(item); } Py_DECREF(iterator); iterator = PyObject_GetIter(set2); if (!iterator) { Py_DECREF(result_set); return NULL; } while ((item = PyIter_Next(iterator))) { if (PySet_Add(result_set, item) == -1) { Py_DECREF(item); Py_DECREF(iterator); Py_DECREF(result_set); return NULL; } Py_DECREF(item); } Py_DECREF(iterator); return result_set; } ``` You will write the implementation of `merge_sets` function and ensure it works as expected. Remember to handle all edge cases and aim for efficiency.","solution":"def merge_sets(set1, set2): Returns a new set containing unique elements from both set1 and set2. # Check if both inputs are sets if not isinstance(set1, set) or not isinstance(set2, set): raise TypeError(\\"Both inputs must be sets\\") # Return the union of both sets return set1 | set2"},{"question":"# Python C-API Cell Object Manipulation Objective Your task is to create a Python class that mimics the behavior of the Python C-API\'s cell objects as described in the given documentation. This class should provide a way to store a value and allow it to be referenced or changed by multiple scopes. Requirements 1. Implement a class `PyCell` with the following methods: - `__init__(self, value)`: Initialize the cell object with a value. - `get(self)`: Return the current value contained in the cell. - `set(self, value)`: Set a new value to the cell. 2. Implement three functions outside the class: - `pycell_check(obj)`: Check if the object is an instance of `PyCell`. - `pycell_new(value)`: Create and return a new instance of `PyCell` containing the given value. - `pycell_get(cell)`: Get the value from the `PyCell` object. - `pycell_set(cell, value)`: Set a new value to the `PyCell` object. Constraints - You should not use any external libraries. - Your solution should be purely in Python without using actual cells from the C-API. Example Usage ```python # Creating a new cell object cell = pycell_new(10) assert pycell_check(cell) == True # Getting the value from cell assert pycell_get(cell) == 10 # Setting a new value in cell pycell_set(cell, 20) assert pycell_get(cell) == 20 # Working with the PyCell class directly my_cell = PyCell(5) assert my_cell.get() == 5 my_cell.set(15) assert my_cell.get() == 15 ``` By completing this task, you will demonstrate your understanding of class implementation, instance method definitions, and handling objects and their properties in Python.","solution":"class PyCell: def __init__(self, value): self.value = value def get(self): return self.value def set(self, value): self.value = value def pycell_check(obj): return isinstance(obj, PyCell) def pycell_new(value): return PyCell(value) def pycell_get(cell): return cell.get() def pycell_set(cell, value): cell.set(value)"},{"question":"# Question: Optimizing Non-Negative Matrix Factorization (NMF) Performance in scikit-learn Objective The goal is to assess your ability to apply performance optimization techniques to improve the efficiency of a scikit-learn algorithm implementation, specifically Non-Negative Matrix Factorization (NMF). Problem Statement Non-Negative Matrix Factorization (NMF) is a matrix factorization technique used in machine learning for dimensionality reduction. You are provided with an implementation of NMF. Your task is to: 1. **Profile the given NMF code** to identify the main bottlenecks. 2. **Optimize the identified bottleneck** using an appropriate algorithmic improvement. 3. **Implement parallel processing** using joblib to further enhance performance. 4. **Verify the correctness of your optimization** with the original implementation. Details 1. **Input and Output Formats**: - **Input**: - `X`: A 2D numpy array of shape (n_samples, n_features). The input data to factorize. - `n_components`: Integer. The number of components for NMF. - `tol`: Float. Tolerance of the stopping condition. - **Output**: - A 2D numpy array of shape (n_samples, n_components), which is the non-negative factors matrix H. 2. **Function Signature**: ```python import numpy as np def profile_and_optimize_nmf(X: np.ndarray, n_components: int, tol: float) -> np.ndarray: # Your optimized implementation here. pass ``` 3. **Constraints**: - Make sure to handle edge cases, such as negative values in input matrices (which should prompt an error). - Ensure the output H has non-negative values only. - The optimization should not change the correctness of the NMF results significantly. 4. **Performance Requirements**: - Utilize profiling tools to identify the performance bottlenecks. - Apply vectorized operations using numpy/scipy to minimize Python loops. - Use joblib for parallel operations where possible to speed up computation for large datasets. Instructions: 1. **Profiling**: - Provide a profiling analysis of the initial implementation using `%timeit`, `%prun`, or `line_profiler`. 2. **Optimization**: - Identify the bottleneck from the profiling results. - Implement an algorithmic improvement to optimize the identified bottleneck. Consider vectorized operations, condition checks, etc. 3. **Parallel Processing**: - Integrate `joblib.Parallel` to utilize multiple cores for further speed enhancements, especially in the iterative parts of the algorithm. 4. **Validation**: - Verify the correctness of your optimization by comparing the results of the optimized function with the original NMF implementation. Example Usage: ```python from sklearn.datasets import load_digits X, _ = load_digits(return_X_y=True) # Define parameters n_components = 16 tol = 1e-2 # Call the optimized function H_optimized = profile_and_optimize_nmf(X, n_components, tol) # Validate output shape print(H_optimized.shape) # Output: (n_samples, n_components) ``` Notes: - You might refer to the non-negative matrix factorization example in scikit-learn documentation for the initial implementation. - Ensure you maintain the readability of your code and add relevant comments for clarity.","solution":"import numpy as np from sklearn.decomposition import NMF from joblib import Parallel, delayed def profile_and_optimize_nmf(X: np.ndarray, n_components: int, tol: float) -> np.ndarray: Optimized Non-Negative Matrix Factorization (NMF) implementation using vectorized operations and parallel processing with joblib.Parallel. Parameters: X (np.ndarray): A 2D numpy array of shape (n_samples, n_features). The input data to factorize. n_components (int): The number of components for NMF. tol (float): Tolerance of the stopping condition. Returns: np.ndarray: A 2D numpy array of shape (n_samples, n_components), which is the non-negative factors matrix H. # Ensure X contains only non-negative values if np.any(X < 0): raise ValueError(\\"Input matrix X must contain only non-negative values.\\") # Initialize NMF model from scikit-learn with the given parameters model = NMF(n_components=n_components, tol=tol, random_state=0, max_iter=200) # Fit the model on the input matrix X W = model.fit_transform(X) H = model.components_ # Return the results return np.dot(W, H)"},{"question":"Problem Statement You are tasked with creating a simple internationalized greeting application using the `gettext` and `locale` modules in Python 3.10. Your program should be able to greet the user in different languages based on their locale settings. Implement the following function: ```python def international_greeting(name: str, lang: str) -> str: Returns the appropriate greeting message in the specified language. Parameters: - name (str): The name of the person to greet. - lang (str): The language code for the greeting (e.g., \'en\', \'es\', \'de\'). Returns: - str: The greeting message in the specified language. ``` Functionality 1. Write the function `international_greeting` which takes two parameters: - `name`: A string representing the name of the person. - `lang`: A string representing the language code (e.g., \'en\' for English, \'es\' for Spanish, \'de\' for German). 2. The function should utilize the `gettext` module to provide greetings in different languages. - English (\'en\'): \\"Hello, {name}!\\" - Spanish (\'es\'): \\"Hola, {name}!\\" - German (\'de\'): \\"Hallo, {name}!\\" - For any other language code, default to English. 3. The function should return the greeting message in the appropriate language, formatted with the person\'s name. Example ```python print(international_greeting(\\"Alice\\", \\"en\\")) # Output: \\"Hello, Alice!\\" print(international_greeting(\\"Bob\\", \\"es\\")) # Output: \\"Hola, Bob!\\" print(international_greeting(\\"Charlie\\", \\"de\\")) # Output: \\"Hallo, Charlie!\\" print(international_greeting(\\"Dana\\", \\"fr\\")) # Output: \\"Hello, Dana!\\" (Defaults to English) ``` Constraints - The `lang` parameter will always be a lowercase two-letter code. - The `name` parameter will be a non-empty string containing only alphabetical characters. Performance - Your function should be efficient and capable of handling multiple calls with different language codes seamlessly. Using these guidelines, implement the `international_greeting` function to demonstrate your understanding of internationalization concepts in Python.","solution":"import gettext def international_greeting(name: str, lang: str) -> str: Returns the appropriate greeting message in the specified language. Parameters: - name (str): The name of the person to greet. - lang (str): The language code for the greeting (e.g., \'en\', \'es\', \'de\'). Returns: - str: The greeting message in the specified language. translations = { \'en\': \\"Hello, {name}!\\", \'es\': \\"Hola, {name}!\\", \'de\': \\"Hallo, {name}!\\" } # Default to English if the language is not supported greeting = translations.get(lang, translations[\'en\']) return greeting.format(name=name)"},{"question":"Advanced File Compression and Decompression Objective Design a Python function that reads multiple uncompressed text data files from a specified directory, compresses them using the bzip2 compression algorithm, and then writes the compressed data to new files in a specified output directory. Additionally, provide functionality to decompress these files back to their original state and verify the correctness of the round-trip compression and decompression. Function Signature ```python import os import bz2 def compress_and_decompress_files(input_directory: str, output_directory: str) -> bool: Compresses all text files in the input_directory, writes them to the output_directory, and then decompresses them to verify that the decompressed files match the original files. Parameters: - input_directory (str): Path to the directory containing uncompressed text files. - output_directory (str): Path to the directory where compressed files will be stored. Returns: - bool: True if all decompressed files match the original files, False otherwise. pass ``` Details 1. **Inputs and Outputs**: - `input_directory`: A string path to the directory containing multiple text files for compression. - `output_directory`: A string path to the directory where compressed files should be stored. 2. **Constraints**: - Assume all files in the `input_directory` are text files. - The `output_directory` may or may not exist. Your function should handle both cases appropriately. 3. **Functionality**: - **Compression**: Read each file from the `input_directory`, compress its content using `bz2`, and save it with a `.bz2` extension in the `output_directory`. - **Decompression and Verification**: Decompress each compressed file and verify that its content matches the original file. Return `True` if all decompressed files match the original files exactly, otherwise return `False`. 4. **Performance Requirements**: - The function should handle a reasonable number of large text files efficiently without running into memory issues. - Proper error handling should be implemented for file operations (e.g., file not found, read/write errors). 5. **Example Usage**: ```python input_dir = \'/path/to/input_directory\' output_dir = \'/path/to/output_directory\' result = compress_and_decompress_files(input_dir, output_dir) print(result) # True if all files successfully round-tripped, otherwise False ``` Notes - This question tests students\' understanding of file handling, compression algorithms, and verification of data integrity. - Ensure your implementation follows best practices for readability and error handling.","solution":"import os import bz2 import filecmp def compress_and_decompress_files(input_directory: str, output_directory: str) -> bool: # Ensure the output directory exists os.makedirs(output_directory, exist_ok=True) # Store the list of original and decompressed file paths for comparison original_files = [] decompressed_files = [] for filename in os.listdir(input_directory): original_file_path = os.path.join(input_directory, filename) compressed_file_path = os.path.join(output_directory, filename + \'.bz2\') decompressed_file_path = os.path.join(output_directory, filename + \'.decompressed\') # Compress the file with open(original_file_path, \'rb\') as original_file: data = original_file.read() compressed_data = bz2.compress(data) with open(compressed_file_path, \'wb\') as compressed_file: compressed_file.write(compressed_data) # Decompress the file with open(compressed_file_path, \'rb\') as compressed_file: compressed_data = compressed_file.read() decompressed_data = bz2.decompress(compressed_data) with open(decompressed_file_path, \'wb\') as decompressed_file: decompressed_file.write(decompressed_data) # Track the file paths original_files.append(original_file_path) decompressed_files.append(decompressed_file_path) # Verify that the original files and decompressed files are the same all_match = all(filecmp.cmp(orig, decomp) for orig, decomp in zip(original_files, decompressed_files)) return all_match"},{"question":"# Coding Assessment: XML Parser with Error Handling Objective Implement a custom XML parser in Python using the `xml.parsers.expat` module that: 1. Parses a given XML string. 2. Handles start and end of elements. 3. Handles character data between elements. 4. Manages and reports errors encountered during the parsing process. Problem Statement Write a function `custom_xml_parser(xml_string: str) -> dict` that takes an XML string as input and parses it using the `xml.parsers.expat` module. The function should return a dictionary containing: - `elements`: A list of tuples where each tuple represents an element and its attributes in the form (`element_name`, `attributes`), called in the order they appear in the XML document. - `char_data`: A list of character data encountered inside elements. - `errors`: A list of error messages encountered during parsing. Function Signature ```python def custom_xml_parser(xml_string: str) -> dict: pass ``` Input - A single string, `xml_string`, containing the XML content to be parsed. Output - A dictionary with three keys: - `elements`: List of tuples representing parsed elements and their attributes. - `char_data`: List of strings with character data found within elements. - `errors`: List of strings with error messages encountered during parsing. Example ```python xml_string = <?xml version=\\"1.0\\"?> <root> <child1 name=\\"paul\\">Text goes here</child1> <child2 name=\\"fred\\">More text</child2> </root> result = custom_xml_parser(xml_string) print(result) ``` Expected `result`: ```python { \'elements\': [ (\'root\', {}), (\'child1\', {\'name\': \'paul\'}), (\'child2\', {\'name\': \'fred\'}) ], \'char_data\': [ \'Text goes here\', \'More text\' ], \'errors\': [] } ``` Constraints - Assume the XML string is UTF-8 encoded. - Handle at least the following elements of the XML: StartElement, EndElement, CharacterData, and Errors. - You should handle typical XML errors like mismatched tags, undefined entities, and incorrect syntax. Notes - Use the `xml.parsers.expat` module and ensure to set appropriate handler functions for the parser. - Ensure to handle and capture any `ExpatError` exceptions during parsing, adding relevant error messages to the `errors` list.","solution":"import xml.parsers.expat def custom_xml_parser(xml_string: str) -> dict: result = { \'elements\': [], \'char_data\': [], \'errors\': [] } def start_element(name, attrs): result[\'elements\'].append((name, attrs)) def end_element(name): pass # We don\'t need to do anything on end element def char_data(data): if data.strip(): result[\'char_data\'].append(data) def error_handler(exception): result[\'errors\'].append(str(exception)) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_string) except xml.parsers.expat.ExpatError as e: error_handler(e) return result"},{"question":"**Question: XML Playlist Processor** You are given an XML document representing a music playlist. The XML includes the following structure: ```xml <playlist> <song> <title>Song1</title> <artist>Artist1</artist> <duration>03:45</duration> </song> <song> <title>Song2</title> <artist>Artist2</artist> <duration>04:20</duration> </song> <!-- More song entries --> </playlist> ``` Your task is to write functions that process this playlist to perform the following tasks: 1. **Load and Parse the XML Document**: Load the XML content into an element tree. 2. **Query Songs by Artist**: Given an artist name, return a list of song titles by that artist. 3. **Modify the Playlist**: Add a new song entry to the playlist with `title`, `artist`, and `duration` parameters. 4. **Save Modifications**: Save the modified XML document back to a file. # Function Specifications: 1. **load_playlist(xml_content: str) -> ET.ElementTree**: - **Input**: A string containing XML content. - **Output**: An `ElementTree` object representing the parsed XML content. 2. **get_songs_by_artist(tree: ET.ElementTree, artist_name: str) -> List[str]**: - **Input**: An `ElementTree` object and a string `artist_name`. - **Output**: A list of titles of songs by `artist_name`. 3. **add_song(tree: ET.ElementTree, title: str, artist: str, duration: str) -> None**: - **Input**: An `ElementTree` object, and strings `title`, `artist`, and `duration`. - **Output**: None. The function will modify the `ElementTree` to include the new song. 4. **save_playlist(tree: ET.ElementTree, file_path: str) -> None**: - **Input**: An `ElementTree` object and a string `file_path`. - **Output**: None. The function will write the modified XML content to the file specified by `file_path`. # Constraints: - Assume valid XML content and well-formed tags. - Each song consists of a title, artist, and duration. - The duration is always in the format `mm:ss`. # Example Usage: ```python # Sample XML content xml_content = <playlist> <song> <title>Song1</title> <artist>Artist1</artist> <duration>03:45</duration> </song> <song> <title>Song2</title> <artist>Artist2</artist> <duration>04:20</duration> </song> </playlist> # Load and parse the XML tree = load_playlist(xml_content) # Query songs by artist songs_by_artist1 = get_songs_by_artist(tree, \\"Artist1\\") print(songs_by_artist1) # Output: [\'Song1\'] # Add a new song add_song(tree, \\"NewSong\\", \\"NewArtist\\", \\"05:00\\") # Save the modified playlist save_playlist(tree, \\"new_playlist.xml\\") ``` Implement these functions to demonstrate your ability to handle XML processing using the `xml.etree.ElementTree` module in Python.","solution":"import xml.etree.ElementTree as ET from typing import List def load_playlist(xml_content: str) -> ET.ElementTree: Loads and parses the XML content into an element tree. Args: xml_content (str): A string containing XML content. Returns: ET.ElementTree: An ElementTree object representing the parsed XML content. return ET.ElementTree(ET.fromstring(xml_content)) def get_songs_by_artist(tree: ET.ElementTree, artist_name: str) -> List[str]: Returns a list of song titles by the given artist. Args: tree (ET.ElementTree): An ElementTree object. artist_name (str): The name of the artist. Returns: List[str]: A list of song titles by the artist. root = tree.getroot() songs = root.findall(\\"song\\") return [song.find(\\"title\\").text for song in songs if song.find(\\"artist\\").text == artist_name] def add_song(tree: ET.ElementTree, title: str, artist: str, duration: str) -> None: Adds a new song entry to the playlist. Args: tree (ET.ElementTree): An ElementTree object. title (str): The title of the new song. artist (str): The artist of the new song. duration (str): The duration of the new song in mm:ss format. Returns: None root = tree.getroot() new_song = ET.Element(\\"song\\") title_element = ET.SubElement(new_song, \\"title\\") title_element.text = title artist_element = ET.SubElement(new_song, \\"artist\\") artist_element.text = artist duration_element = ET.SubElement(new_song, \\"duration\\") duration_element.text = duration root.append(new_song) def save_playlist(tree: ET.ElementTree, file_path: str) -> None: Saves the modified XML document to a file. Args: tree (ET.ElementTree): An ElementTree object. file_path (str): The file path where the modified XML should be written. Returns: None tree.write(file_path, encoding=\'utf-8\', xml_declaration=True)"},{"question":"**Problem Statement:** You are given a dataset about the average yearly sunshine hours of various cities (`\'sunshine_hours.csv\'`). The dataset has the following columns: - `City`: Name of the city. - `Year`: Year of the recorded data. - `Sunshine_hours`: Average yearly sunshine hours. Your task is to write a function `plot_sunshine_hours(data_file)` that loads this dataset, sorts the data by `City` and `Year`, and then plots the trajectories of sunshine hours over the years for each city. The plot should use different colors for different cities and have circle markers at each data point. # Function Signature ```python def plot_sunshine_hours(data_file: str) -> None: pass ``` # Input - `data_file` (str): The file path of the CSV file containing the sunshine hours data. # Output - Displays a plot with the trajectories of sunshine hours per city, with different colors for each city and circle markers for each data point. # Constraints - Use the `seaborn.objects` module and the `so.Path` class as demonstrated in the provided documentation. - Make sure to handle the data appropriately before plotting. - Your implementation should be efficient and handle datasets with up to 10,000 rows smoothly. # Example Usage ```python # Assuming \'sunshine_hours.csv\' contains the following data: # City,Year,Sunshine_hours # CityA,2000,1200 # CityA,2001,1300 # CityB,2000,1450 # CityB,2001,1500 plot_sunshine_hours(\'sunshine_hours.csv\') ``` # Example Result The plot would display two trajectories: - One for `CityA` showing points and lines connecting 1200, 1300 (for 2000 and 2001 respectively). - One for `CityB` showing points and lines connecting 1450, 1500 (for 2000 and 2001 respectively), with a different color than `CityA`. # Notes - Ensure that your plot has appropriate axis labels and a legend to clearly distinguish between different cities.","solution":"import pandas as np import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def plot_sunshine_hours(data_file: str) -> None: # Load the dataset data = pd.read_csv(data_file) # Sort the data by City and Year data = data.sort_values(by=[\'City\', \'Year\']) # Create point plot using seaborn objects interface p = ( so.Plot(data, x=\\"Year\\", y=\\"Sunshine_hours\\", color=\\"City\\") .add(so.Path(marker=\'o\')) .label(x=\\"Year\\", y=\\"Average Yearly Sunshine Hours\\", color=\\"City\\") ) # Display the plot p.show()"},{"question":"# **Seaborn Advanced Plotting with `seaborn.objects`** You are provided with a dataset `diamonds` that contains information about the prices and attributes of diamonds. Your task is to visualize the data using Seaborn\'s `objects` interface. Implement a function `visualize_diamond_data` that takes no input and generates the following plot: 1. **Create a plot object `p` with `diamonds` DataFrame**: - Set `cut` as the x-axis variable. - Set `price` as the y-axis variable. 2. **Add plot layers to `p`**: - Use dot elements to show individual data points. - Scale the y-axis to a logarithmic scale (`log`). - Add points representing the 10th, 25th, 50th, 75th, and 90th percentiles. - Add a jitter effect to the dots to avoid overlapping. - Include a range mark representing the 25th to 75th percentile interval with a small vertical shift and black color. 3. **Show the plot**. # Expected Output The plot should display: - Individual data points as dots. - Horizontal dispersion of dots due to the jitter effect. - Logarithmic scaling of the y-axis. - Dots for the specified percentiles and a range interval for the median percentile range. # Constraints - Use the seaborn library with `objects` interface. - Ensure the plot definitions are highly readable and follow Seaborn\'s best practices. # Function Signature ```python def visualize_diamond_data(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the Plot object p = ( so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") .scale(y=\\"log\\") ) # Add plot layers p = p.add(so.Dot(), so.Perc([10, 25, 50, 75, 90])) p = p.add(so.Dots(pointsize=1, alpha=0.2), so.Jitter(0.3)) p = p.add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=0.2)) # Show the plot p.show() ```","solution":"def visualize_diamond_data(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create the Plot object p = ( so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") .scale(y=\\"log\\") ) # Add plot layers p = p.add(so.Dot(), so.Perc([10, 25, 50, 75, 90])) p = p.add(so.Dots(pointsize=1, alpha=0.2), so.Jitter(0.3)) p = p.add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=0.2)) # Show the plot p.show()"},{"question":"Objective: Write a Python function `custom_pretty_print(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False)` that pretty-prints the provided data structure according to the given parameters. Function Signature: ```python def custom_pretty_print(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False) -> str: pass ``` Input: 1. `data`: An arbitrary Python data structure (e.g., nested lists, dictionaries). 2. `indent`: (optional) Integer value specifying the amount of indentation for each nesting level (default is 1). 3. `width`: (optional) Integer value specifying maximum number of characters per line (default is 80). 4. `depth`: (optional) Integer value specifying the number of nesting levels to print. Deeper levels will be indicated with ellipses (default is None, meaning no constraint). 5. `compact`: (optional) Boolean value indicating whether to format sequences as densely as possible (default is False). 6. `sort_dicts`: (optional) Boolean value indicating whether dictionaries should be pretty-printed with sorted keys (default is True). 7. `underscore_numbers`: (optional) Boolean value indicating whether to format integers with underscores as thousands separators (default is False). Output: A string that represents the pretty-printed version of the input data structure according to the specified formatting parameters. Constraints: - The function should handle large and deeply nested structures efficiently. - The function should correctly handle recursive structures and avoid infinite loops. - Sorting of dictionary keys should be implemented as specified by the parameters. Example: ```python data = [\'spam\', \'eggs\', \'lumberjack\', \'knights\', \'ni\'] data.insert(0, data[:]) result = custom_pretty_print(data, indent=4) print(result) ``` Expected Output: ``` [ [\'spam\', \'eggs\', \'lumberjack\', \'knights\', \'ni\'], \'spam\', \'eggs\', \'lumberjack\', \'knights\', \'ni\'] ``` This problem assesses the student\'s understanding of class usage, parameter handling, and working with nested and complex data structures.","solution":"import pprint def custom_pretty_print(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False): Pretty-prints a Python data structure according to the provided formatting parameters. printer = pprint.PrettyPrinter( indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts, underscore_numbers=underscore_numbers ) return printer.pformat(data)"},{"question":"**Pipeline Parallelism with PyTorch**: You are tasked with implementing a model training using Pipeline Parallelism in PyTorch to understand its functionality and efficiency. You will demonstrate both creating pipeline stages manually and using them in a pipeline schedule. # Part 1: Implementing and Using a Manual Pipeline Stage 1. **Manual PipelineStage Creation**: You need to create a simple model class and split it manually using PipelineStage. You can use the following simple feedforward neural network structure: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 30) self.fc3 = nn.Linear(30, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x ``` 2. **PipelineStage Creation**: Split this model into two pipeline stages: - Stage 1: Contains self.fc1 - Stage 2: Contains self.fc2 and self.fc3 Implement the splitting: ```python from torch.distributed.pipelining import PipelineStage def create_pipeline_stages(): model = SimpleModel() stage1 = nn.Sequential(model.fc1) stage2 = nn.Sequential(model.fc2, model.fc3) # Creating pipeline stages (Assume stage_index and num_stages are provided) stage_obj1 = PipelineStage(stage1, stage_index=0, num_stages=2, device=\'cpu\') stage_obj2 = PipelineStage(stage2, stage_index=1, num_stages=2, device=\'cpu\') return stage_obj1, stage_obj2 ``` # Part 2: Using PipelineSchedule for Execution 1. **Running PipelineSchedule**: Using the two stages created, implement a GPipe schedule to run a simple forward and backward pass. Assume the data input is a tensor of shape `(batch_size, 10)`. Example: ```python import torch from torch.distributed.pipelining import ScheduleGPipe def execute_pipeline(batch_size=32, n_microbatches=4): x = torch.randn(batch_size, 10) stage_obj1, stage_obj2 = create_pipeline_stages() schedule = ScheduleGPipe(stage_obj1, n_microbatches) schedule.step(x) # Normally, `schedule.step()` would be run on different ranks # Simulate the stage 2 step for demonstration output = stage_obj2.model(stage_obj1.stage.module(x)) return output ``` # Input The function `execute_pipeline` should take `batch_size` and `n_microbatches` as input parameters. # Output The function should return the output tensor after executing the forward pass through the pipeline schedule. # Constraints 1. Use `torch.distributed.pipelining` and related classes as presented in the PyTorch documentation. 2. The solution should demonstrate understanding of `PipelineStage` creation and using `PipelineSchedule`. 3. Assume devices are CPU only for simplicity. # Performance Requirements Since the model and stages are simple, there are no strict performance constraints. However, ensure that the pipeline stages and schedule are correctly used to demonstrate their functionality.","solution":"import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 30) self.fc3 = nn.Linear(30, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x def create_pipeline_stages(): model = SimpleModel() stage1 = nn.Sequential(model.fc1) stage2 = nn.Sequential(model.fc2, model.fc3) # Normally, PipelineStage would be imported from torch.distributed.pipelining, # but since it is not a part of the actual PyTorch API, let\'s define a mock: class PipelineStage: def __init__(self, model_part, stage_index, num_stages, device): self.model = model_part self.stage_index = stage_index self.num_stages = num_stages self.device = device stage_obj1 = PipelineStage(stage1, stage_index=0, num_stages=2, device=\'cpu\') stage_obj2 = PipelineStage(stage2, stage_index=1, num_stages=2, device=\'cpu\') return stage_obj1, stage_obj2 def execute_pipeline(batch_size=32, n_microbatches=4): x = torch.randn(batch_size, 10) stage_obj1, stage_obj2 = create_pipeline_stages() # Mocking ScheduleGPipe since it\'s not part of actual PyTorch API class ScheduleGPipe: def __init__(self, stage, n_microbatches): self.stage = stage self.n_microbatches = n_microbatches def step(self, input_tensor): microbatch_size = input_tensor.shape[0] // self.n_microbatches outputs = [] for _ in range(self.n_microbatches): microbatch = input_tensor[:microbatch_size, :] output = self.stage.model(microbatch) outputs.append(output) input_tensor = input_tensor[microbatch_size:, :] return torch.cat(outputs) schedule = ScheduleGPipe(stage_obj1, n_microbatches) intermediate_output = schedule.step(x) # Simulate the stage 2 step for demonstration output = stage_obj2.model(intermediate_output) return output"},{"question":"Consider the following scenario: You are building a data validation function for an application that requires processing various user inputs. One of the validations your function needs to handle is checking whether certain conditions are true or false. These conditions are expressed in a mixed list containing boolean values, integers, and strings. Write a function named `validate_conditions` that processes the input list and returns a list of boolean values indicating the validity of each element based on the following rules: 1. Boolean elements (`True`, `False`) should be returned as they are. 2. Integer elements should be considered `True` if they are non-zero and `False` if they are zero. 3. String elements should be considered `True` if they are non-empty and `False` if they are empty. 4. Any other type of element should be considered `False`. # Input - A single list `conditions` of length `n` where `1 <= n <= 1000`. Each element in the list could be a boolean, an integer, a string, or any other type. # Output - A list of boolean values of length `n` corresponding to the validity of each element based on the rules above. # Constraints - Consider only the types mentioned in the rules for processing. - The implementation must run efficiently within the provided input constraints. # Example ```python def validate_conditions(conditions): # your code here # Example test cases print(validate_conditions([True, 0, \\"Hello\\", \\"\\", 42, None, False])) # Expected output: [True, False, True, False, True, False, False] print(validate_conditions([1, \\"World\\", \\"\\", False, 0, \\"Python\\"])) # Expected output: [True, True, False, False, False, True] ``` # Tips - Consider using Python\'s `bool` casting function to simplify the processing of each list element.","solution":"def validate_conditions(conditions): Given a list of mixed types, returns a list of boolean values indicating the validity of each element based on the following rules: 1. Boolean elements should be returned as they are. 2. Integer elements should be considered True if they are non-zero and False if they are zero. 3. String elements should be considered True if they are non-empty and False if they are empty. 4. Any other type of element should be considered False. result = [] for element in conditions: if isinstance(element, bool): result.append(element) elif isinstance(element, int): result.append(element != 0) elif isinstance(element, str): result.append(len(element) > 0) else: result.append(False) return result"},{"question":"# Question: Implementing a Custom PyTorch Model with TorchScript **Problem Statement:** You are required to implement a custom neural network model in PyTorch and convert it to TorchScript. The model should: - Contain at least two layers. - Take a tensor as input and output a tensor after passing through the layers. - Utilize TorchScript annotations for all functions and variables. - Include a method for performing an element-wise multiplication of two tensors, along with necessary type checks and refinements. - Include at least one class that is defined using the `@torch.jit.script` decorator. **Function Specifications:** 1. `CustomModel.__init__(self)`: - Initializes the layers. - No parameters required. 2. `CustomModel.forward(self, x: torch.Tensor) -> torch.Tensor`: - Takes a tensor `x` and returns the output after passing through both layers. 3. `CustomModel.tensorwise_mult(self, a: Optional[torch.Tensor], b: Optional[torch.Tensor]) -> torch.Tensor`: - Takes two tensors `a` and `b`, checks if they are not None, performs element-wise multiplication and returns the result. - If either tensor is None, it should raise a `ValueError`. 4. Define a TorchScript class `CustomClass`: - Contains an attribute `attribute` of type `torch.Tensor`. - A method `increment_attribute` that increments this tensor by a given value. **Implementation Hints:** - Make use of the `@torch.jit.script` decorator to denote TorchScript functions and classes. - Use type annotations as described in the given documentation. - Properly handle `Optional` types and refine types using `assert` statements or conditional checks. **Example Usage:** ```python import torch import torch.nn as nn from typing import Optional class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 10) @torch.jit.script_method def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x @torch.jit.script_method def tensorwise_mult(self, a: Optional[torch.Tensor], b: Optional[torch.Tensor]) -> torch.Tensor: if a is None or b is None: raise ValueError(\\"Input tensors cannot be None\\") return a * b @torch.jit.script class CustomClass: def __init__(self, attribute: torch.Tensor): self.attribute = attribute def increment_attribute(self, value: float): self.attribute += value # Example model construction model = CustomModel() scripted_model = torch.jit.script(model) # Example custom class usage obj = CustomClass(torch.tensor([1.0, 2.0, 3.0])) obj.increment_attribute(1.0) ``` # Task: 1. Implement the `CustomModel` class according to the specifications provided. 2. Implement the `CustomClass` as a TorchScript class with the specified method. 3. Script the model and demonstrate an example forward pass and tensorwise multiplication. **Constraints:** - You should ensure type safety and correct usage of the TorchScript features. - The tensor operations must handle tensors of compatible shapes. **Note:** If you encounter any issues, consult the TorchScript documentation for guidance on supported types and functions.","solution":"import torch import torch.nn as nn from typing import Optional class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.layer1 = nn.Linear(10, 20) self.layer2 = nn.Linear(20, 10) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x def tensorwise_mult(self, a: Optional[torch.Tensor], b: Optional[torch.Tensor]) -> torch.Tensor: if a is None or b is None: raise ValueError(\\"Input tensors cannot be None\\") return a * b @torch.jit.script class CustomClass: attribute: torch.Tensor def __init__(self, attribute: torch.Tensor): self.attribute = attribute def increment_attribute(self, value: float): self.attribute += value # Example model construction model = CustomModel() scripted_model = torch.jit.script(model) # Example custom class usage obj = CustomClass(torch.tensor([1.0, 2.0, 3.0])) obj.increment_attribute(1.0)"},{"question":"**Datetime Manipulation and Analysis** You are tasked with creating a function that processes a list of datetimes and computes various statistics. The function should accept a list of string representations of dates and times and return a dictionary containing the following information: - The earliest date-time in the list. - The latest date-time in the list. - The total time span (difference between the latest and earliest) in days, hours, minutes, and seconds. - The most common hour (i.e., the hour of the day that occurs most frequently in the list). # Function Signature ```python def analyze_datetime_statistics(datetime_list: list[str]) -> dict: pass ``` # Parameters - `datetime_list` (list of str): A list of strings, where each string represents a date-time in the format `YYYY-MM-DD HH:MM:SS`. # Returns - dict: A dictionary containing the following key-value pairs: - `earliest` (str): The earliest date-time in `datetime_list`. - `latest` (str): The latest date-time in `datetime_list`. - `time_span` (dict): A dictionary with keys `\\"days\\"`, `\\"hours\\"`, `\\"minutes\\"`, and `\\"seconds\\"`, representing the time span between the latest and earliest date-times. - `common_hour` (int): The hour (0 - 23) that occurs most frequently in `datetime_list`. # Example ```python datetime_list = [ \\"2023-01-01 12:00:00\\", \\"2023-01-02 14:30:00\\", \\"2023-01-01 09:15:00\\", \\"2023-01-01 12:00:00\\", \\"2023-01-03 08:45:00\\" ] assert analyze_datetime_statistics(datetime_list) == { \'earliest\': \'2023-01-01 09:15:00\', \'latest\': \'2023-01-03 08:45:00\', \'time_span\': {\'days\': 1, \'hours\': 23, \'minutes\': 30, \'seconds\': 0}, \'common_hour\': 12 } ``` # Constraints - The input list will have at least two and at most 10^3 date-time strings. - All date-times in the input list will be in the correct format and represent valid dates and times. # Performance Requirements The solution should run efficiently with a complexity of approximately O(n log n) due to sorting, which is acceptable given the constraints. **Hints**: - Utilize the `datetime` module for parsing date-time strings and calculating differences. - Use a dictionary or counter to determine the most common hour.","solution":"from datetime import datetime from collections import Counter def analyze_datetime_statistics(datetime_list: list[str]) -> dict: # Parse the datetime strings into datetime objects datetime_objects = [datetime.strptime(dt, \'%Y-%m-%d %H:%M:%S\') for dt in datetime_list] # Find the earliest and latest date-time earliest_datetime = min(datetime_objects) latest_datetime = max(datetime_objects) earliest_str = earliest_datetime.strftime(\'%Y-%m-%d %H:%M:%S\') latest_str = latest_datetime.strftime(\'%Y-%m-%d %H:%M:%S\') # Calculate the total time span time_span_delta = latest_datetime - earliest_datetime time_span = { \'days\': time_span_delta.days, \'hours\': time_span_delta.seconds // 3600, \'minutes\': (time_span_delta.seconds % 3600) // 60, \'seconds\': time_span_delta.seconds % 60 } # Calculate the most common hour hours = [dt.hour for dt in datetime_objects] common_hour = Counter(hours).most_common(1)[0][0] return { \'earliest\': earliest_str, \'latest\': latest_str, \'time_span\': time_span, \'common_hour\': common_hour }"},{"question":"# Asynchronous Subprocess Management In this task, you are required to write an asynchronous Python function that demonstrates your understanding of creating and managing subprocesses using the `asyncio` library in Python 3.10. Objective: Write a function called `run_multiple_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str]]` which executes multiple shell commands concurrently, collects their outputs, and enforces a timeout for each command. Function Signature: ```python import asyncio from typing import List, Tuple async def run_multiple_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str]]: pass ``` Input: - `commands`: A list of shell command strings to be executed (`List[str]`). - `timeout`: An integer representing the maximum time (in seconds) allowed for each command\'s execution before termination (`int`). Output: - Returns a list of tuples, where each tuple contains: - The command string (`str`). - The exit status of the command (`int`): - 0 for successful execution. - 1 for timeout. - Any other non-zero value for different errors. - The combined output from stdout and stderr (`str`). Constraints: - The function should handle up to 10 commands simultaneously. - Ensure proper encoding and decoding of the command outputs. - Implement shell command execution using `asyncio.create_subprocess_shell` and manage their execution and collection of results asynchronously. Example: ```python commands = [\\"ls\\", \\"sleep 2\\", \\"echo \'hello world\'\\", \\"invalid_command\\"] timeout = 3 result = asyncio.run(run_multiple_commands(commands, timeout)) for cmd, exit_code, output in result: print(f\\"Command: {cmd}\\") print(f\\"Exit Code: {exit_code}\\") print(f\\"Output: {output}\\") print(\\"-\\" * 40) ``` Assume the example produces the following output: ``` Command: ls Exit Code: 0 Output: [list of files and directories] ---------------------------------------- Command: sleep 2 Exit Code: 0 Output: ---------------------------------------- Command: echo \'hello world\' Exit Code: 0 Output: hello world ---------------------------------------- Command: invalid_command Exit Code: 127 Output: /bin/sh: invalid_command: command not found ---------------------------------------- ``` Notes: - Use asyncio.gather to run commands concurrently. - Handle subprocess timeouts appropriately. - Merge stdout and stderr streams for the final output. Hints: - Utilize `asyncio.create_subprocess_shell` to create asynchronous subprocesses. - Use `proc.communicate()` to avoid deadlocks and capture outputs. - Enforce the timeout by using `asyncio.wait_for`.","solution":"import asyncio from typing import List, Tuple async def run_command(command: str, timeout: int) -> Tuple[str, int, str]: try: proc = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) try: stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout) output = (stdout + stderr).decode().strip() return (command, proc.returncode, output) except asyncio.TimeoutError: proc.kill() await proc.communicate() return (command, 1, \\"Command timeout\\") except Exception as e: return (command, -1, str(e)) async def run_multiple_commands(commands: List[str], timeout: int) -> List[Tuple[str, int, str]]: tasks = [run_command(command, timeout) for command in commands] results = await asyncio.gather(*tasks) return results"},{"question":"**Question: Advanced Data Visualization with Seaborn\'s `objects` API** You are given a dataset containing information about various bills and tips at a restaurant. Your task is to create a complex visualization using Seaborn\'s `objects` API that provides the following insights: 1. A scatter plot of `total_bill` vs `tip`. 2. An overlaid linear regression line that represents the trend between `total_bill` and `tip`. 3. A bar plot showing the distribution of total customers per day of the week. 4. The scatter plot should differentiate between genders (`sex`) using colors. 5. Include a separate subset of data where the `size` is equal to 2 and overlay it on top of the previous scatter plot with a different color for the points representing time of the day (`time`). **Instructions**: 1. Load the `tips` dataset using Seaborn. 2. Create the required visualizations using the Seaborn `objects` API. 3. Ensure each layer is properly configured as described. **Expected Input and Output Formats**: * Input: The `tips` dataset will be used from Seaborn\'s built-in datasets. * Output: A single visualization object composed of multiple layers as described above. **Constraints**: - Use the Seaborn `objects` API for all visual elements. - The visualization should be clear, properly labeled, and differentiated by the specified attributes. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot p = so.Plot(tips, \\"total_bill\\", \\"tip\\").add(so.Dot(color=\\"sex\\")) # Add a linear regression line p.add(so.Line(color=\\".3\\"), so.PolyFit(), label=\\"Linear Trend\\") # Add another layer for subset where size is 2 p.add(so.Dot(), data=tips.query(\\"size == 2\\"), color=\\"time\\") # Add a new plot for total customers per day p.add(so.Bar(), so.Hist(), weight=\\"size\\", x=\\"day\\").label(y=\\"Total patrons\\") # Display the plot p ``` Submit your implementation by providing the complete code that fulfills the above requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the base plot p = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add a scatter plot of total_bill vs tip, colored by sex p = p.add(so.Dot(), color=\\"sex\\") # Add a linear regression line p = p.add(so.Line(color=\\".3\\"), so.PolyFit(), label=\\"Linear Trend\\") # Add another scatter plot layer for subset where size is 2, colored by time of the day p = p.add(so.Dot(), data=tips.query(\\"size == 2\\"), color=\\"time\\") # Create the bar plot for total customers per day and combine it bar_plot = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count(), color=\\"day\\").label(y=\\"Total patrons\\") # Display the plots together p, bar_plot"},{"question":"**Coding Challenge: Extend the Functionality of `optparse`** **Objective:** You are tasked with creating a custom option for a command-line script using the deprecated `optparse` module. The custom option should extend the behavior of the `append` action to allow appending multiple comma-separated values from a single option string. **Task:** 1. Create a subclass of `optparse.Option` to implement a new action called \\"**extend**\\". 2. The `extend` action should split a comma-separated string into individual values and append them to a list. 3. Demonstrate the use of this custom action in a script that parses command-line arguments. **Specifications:** - Create a subclass `MyOption` that extends `optparse.Option`. - Add a custom action `extend` that processes comma-separated values and appends them to a list. - Implement a script that uses this custom option and demonstrates parsing the options correctly. **Input Format:** Your script should be able to accept command-line arguments in the following format: ``` python script_name.py --names=John,Doe --names=Jane,Doe ``` **Output Format:** The script should print the parsed list of names. For the above example, the output should look like: ``` [\'John\', \'Doe\', \'Jane\', \'Doe\'] ``` **Constraints:** - Use the deprecated `optparse` module as described. - Ensure the solution handles various edge cases (e.g., empty values, single values, no values). **Example Implementation:** ```python from optparse import OptionParser, Option class MyOption(Option): ACTIONS = Option.ACTIONS + (\'extend\',) STORE_ACTIONS = Option.STORE_ACTIONS + (\'extend\',) TYPED_ACTIONS = Option.TYPED_ACTIONS + (\'extend\',) ALWAYS_TYPED_ACTIONS = Option.ALWAYS_TYPED_ACTIONS + (\'extend\',) def take_action(self, action, dest, opt, value, values, parser): if action == \'extend\': lvalue = value.split(\',\') values.ensure_value(dest, []).extend(lvalue) else: Option.take_action(self, action, dest, opt, value, values, parser) def main(): parser = OptionParser(option_class=MyOption) parser.add_option(\'--names\', action=\'extend\', type=\'string\', dest=\'names\', help=\\"Comma-separated names to append to the list\\") (options, args) = parser.parse_args() if not options.names: print(\\"No names provided.\\") else: print(options.names) if __name__ == \'__main__\': main() ``` **Instructions:** - Implement the `MyOption` class and the script as demonstrated, ensuring it works for various input cases. - Add comments and documentation as necessary to explain the code logic. **Performance Requirements:** - The solution should efficiently handle varying lengths of input without significant performance degradation.","solution":"from optparse import OptionParser, Option class MyOption(Option): ACTIONS = Option.ACTIONS + (\'extend\',) STORE_ACTIONS = Option.STORE_ACTIONS + (\'extend\',) TYPED_ACTIONS = Option.TYPED_ACTIONS + (\'extend\',) ALWAYS_TYPED_ACTIONS = Option.ALWAYS_TYPED_ACTIONS + (\'extend\',) def take_action(self, action, dest, opt, value, values, parser): if action == \'extend\': lvalue = value.split(\',\') values.ensure_value(dest, []).extend(lvalue) else: Option.take_action(self, action, dest, opt, value, values, parser) def main(): parser = OptionParser(option_class=MyOption) parser.add_option(\'--names\', action=\'extend\', type=\'string\', dest=\'names\', help=\\"Comma-separated names to append to the list\\") (options, args) = parser.parse_args() if not options.names: print(\\"No names provided.\\") else: print(options.names) if __name__ == \'__main__\': main()"},{"question":"You are required to create an abstract base class and demonstrate the usage of the `abc` module functionality provided in Python 3.10. The goal is to assess your understanding of Abstract Base Classes (ABCs) and how to implement and utilize abstract methods. # Task 1. Create an abstract base class `AbstractOperation` using the `abc` module. 2. Define the following abstract methods within `AbstractOperation`: - `perform_operation(self, a: int, b: int) -> int`: Takes two integers and returns an integer. - `operation_name(self) -> str`: Returns the name of the operation as a string. 3. Implement two concrete classes `Addition`, and `Multiplication` that inherit from `AbstractOperation`. - `Addition`: Override `perform_operation` to return the sum of `a` and `b`. - `Multiplication`: Override `perform_operation` to return the product of `a` and `b`. - Both class should implement `operation_name` to return \\"Addition\\" and \\"Multiplication\\" respectively. 4. Create a function `operate` that takes an object of type `AbstractOperation` and two integers, and returns the result of `perform_operation`. 5. Ensure that `operate` works correctly with instances of `Addition` and `Multiplication`. # Input and Output Your solution should define the classes and the function in the following way: ```python from abc import ABC, abstractmethod class AbstractOperation(ABC): @abstractmethod def perform_operation(self, a: int, b: int) -> int: pass @abstractmethod def operation_name(self) -> str: pass class Addition(AbstractOperation): def perform_operation(self, a: int, b: int) -> int: return a + b def operation_name(self) -> str: return \\"Addition\\" class Multiplication(AbstractOperation): def perform_operation(self, a: int, b: int) -> int: return a * b def operation_name(self) -> str: return \\"Multiplication\\" def operate(operation: AbstractOperation, a: int, b: int) -> int: return operation.perform_operation(a, b) # Example usage: addition = Addition() print(operate(addition, 5, 3)) # Output: 8 multiplication = Multiplication() print(operate(multiplication, 5, 3)) # Output: 15 ``` # Constraints - Ensure your implementation strictly follows the guidelines of using abstract base classes and the `abc` module. - Do not instantiate the `AbstractOperation` class directly; it must remain abstract. Your solution must demonstrate the proper usage of abstract methods, class inheritance, and the overall structure provided by the `abc` module.","solution":"from abc import ABC, abstractmethod class AbstractOperation(ABC): @abstractmethod def perform_operation(self, a: int, b: int) -> int: pass @abstractmethod def operation_name(self) -> str: pass class Addition(AbstractOperation): def perform_operation(self, a: int, b: int) -> int: return a + b def operation_name(self) -> str: return \\"Addition\\" class Multiplication(AbstractOperation): def perform_operation(self, a: int, b: int) -> int: return a * b def operation_name(self) -> str: return \\"Multiplication\\" def operate(operation: AbstractOperation, a: int, b: int) -> int: return operation.perform_operation(a, b) # Example usage: addition = Addition() print(operate(addition, 5, 3)) # Output: 8 multiplication = Multiplication() print(operate(multiplication, 5, 3)) # Output: 15"},{"question":"# CSV File Manipulation and Analysis using Python\'s `csv` Module Problem Statement You have been provided with a CSV file called `products.csv` which contains information about various products in a store. The file has the following columns: - `ProductID`: Unique identifier for each product - `ProductName`: Name of the product - `Category`: Category to which the product belongs - `Price`: Price of the product - `Quantity`: Quantity of the product available in stock An example row might look like this: ``` ProductID,ProductName,Category,Price,Quantity 1001,Widget A,Widgets,25.50,100 1002,Widget B,Widgets,15.75,50 1003,Gadget C,Gadgets,45.00,20 ``` Your task is to implement several functions using the `csv` module to perform the following operations: 1. **read_csv_to_dict(filename)**: - **Input**: A string representing the filename of the CSV file. - **Output**: A list of dictionaries where each dictionary represents a row in the CSV file. - **Example Usage**: ```python products = read_csv_to_dict(\\"products.csv\\") # products would be a list of dictionaries each representing a row ``` 2. **write_dict_to_csv(filename, data)**: - **Input**: A string representing the filename of the CSV file and a list of dictionaries where each dictionary represents a row to be written to the CSV file. - **Output**: This function does not return anything. It writes the data to the given filename. - **Example Usage**: ```python new_products = [ {\\"ProductID\\": \\"1004\\", \\"ProductName\\": \\"Gizmo D\\", \\"Category\\": \\"Gizmos\\", \\"Price\\": \\"30.00\\", \\"Quantity\\": \\"40\\"} ] write_dict_to_csv(\\"new_products.csv\\", new_products) # new_products.csv will contain the data from the list of dictionaries ``` 3. **filter_by_category(products, category)**: - **Input**: A list of dictionaries (as returned by `read_csv_to_dict`) and a category string. - **Output**: A list of dictionaries containing only the products that belong to the specified category. - **Example Usage**: ```python widgets = filter_by_category(products, \\"Widgets\\") # widgets will only contain products where the \\"Category\\" is \\"Widgets\\" ``` 4. **calculate_total_stock_value(products)**: - **Input**: A list of dictionaries (as returned by `read_csv_to_dict`). - **Output**: A dictionary with keys being categories and values being total stock value for that category. The total stock value for a category is calculated as the sum of (Price * Quantity) for all products in that category. - **Example Usage**: ```python stock_value = calculate_total_stock_value(products) # stock_value would be a dictionary with category names as keys and total stock values as values ``` # Constraints - Assume the CSV file will always have valid data as per the mentioned columns. - Prices and Quantities will always be valid numbers. # Performance Requirements - The functions should be efficient enough to handle CSV files with up to 10,000 rows. # Submission Please implement these functions in Python and test them with appropriate example data.","solution":"import csv def read_csv_to_dict(filename): Reads a CSV file and converts it to a list of dictionaries with open(filename, mode=\'r\', newline=\'\') as csvfile: reader = csv.DictReader(csvfile) return [row for row in reader] def write_dict_to_csv(filename, data): Writes a list of dictionaries to a CSV file with open(filename, mode=\'w\', newline=\'\') as csvfile: if data: fieldnames = data[0].keys() writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(data) def filter_by_category(products, category): Filters the list of products by a specific category return [product for product in products if product[\'Category\'] == category] def calculate_total_stock_value(products): Calculates the total stock value for each category stock_value = {} for product in products: category = product[\'Category\'] price = float(product[\'Price\']) quantity = int(product[\'Quantity\']) if category not in stock_value: stock_value[category] = 0.0 stock_value[category] += price * quantity return stock_value"},{"question":"# Objective You need to demonstrate your understanding of the `xml.sax` module in Python by writing a program that: 1. Creates a SAX parser. 2. Implements custom handlers to handle the start and end of XML elements and character data. 3. Parses a given XML string using your handlers. 4. Handles any SAX parsing exceptions appropriately. # Task Write a Python program that accomplishes the following: 1. **Create a SAX Parser:** - Use the `xml.sax.make_parser()` function to create a SAX parser. 2. **Implement Custom Handlers:** - Create a custom handler class `MyHandler`, derived from `xml.sax.handler.ContentHandler`. - Override the `startElement`, `endElement`, and `characters` methods to handle XML start tags, end tags, and character data, respectively. - Print appropriate messages in each method to indicate the processing of different parts of the XML. 3. **Parse an XML String:** - Parse the following XML string using your custom handler and the SAX parser: ```xml <?xml version=\\"1.0\\"?> <root> <greeting>Hello, World!</greeting> <farewell>Goodbye, World!</farewell> </root> ``` 4. **Exception Handling:** - Handle any `xml.sax.SAXException` that might occur during parsing and print a meaningful error message. # Input - A hardcoded XML string: ```xml <?xml version=\\"1.0\\"?> <root> <greeting>Hello, World!</greeting> <farewell>Goodbye, World!</farewell> </root> ``` # Output - Print messages indicating the start and end of elements and the character data encountered. - If an exception occurs, print an error message indicating the problem. # Constraints - You must use the `xml.sax` module for parsing. - The custom handler must derive from `xml.sax.handler.ContentHandler`. - The SAX parser must be created using `xml.sax.make_parser()`. # Example Output ``` Start element: root Start element: greeting Characters: Hello, World! End element: greeting Start element: farewell Characters: Goodbye, World! End element: farewell End element: root ``` # Notes - Ensure your program handles the XML parsing robustly and catches any potential exceptions. - Your program should be clear and concise, demonstrating your understanding of SAX-based XML parsing in Python.","solution":"import xml.sax class MyHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Start element: {name}\\") def endElement(self, name): print(f\\"End element: {name}\\") def characters(self, content): if content.strip(): # Avoid printing empty strings print(f\\"Characters: {content.strip()}\\") def parse_xml_string(xml_string): parser = xml.sax.make_parser() handler = MyHandler() parser.setContentHandler(handler) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXException as e: print(f\\"An error occurred during XML parsing: {e}\\") xml_string = <?xml version=\\"1.0\\"?> <root> <greeting>Hello, World!</greeting> <farewell>Goodbye, World!</farewell> </root> # This call will output the parsed elements and characters parse_xml_string(xml_string)"},{"question":"# Objective The objective of this coding assignment is to assess your understanding of data distribution visualization using Seaborn. Your task is to implement a function that takes a dataset and generates customized distribution plots considering various visualization aspects as described in the provided documentation. # Problem Statement You are provided with a dataset of penguins which includes various measurements (like flipper length, bill length, etc.) and descriptive attributes (like species, island, etc.). Your task is to create a function that performs the following: 1. Load the `penguins` dataset. 2. Create a histogram of the `flipper_length_mm` variable. 3. Customize the histogram by setting the number of bins to 20. 4. Create a KDE plot for the `flipper_length_mm` variable and adjust the bandwidth to 0.25. 5. Generate a layered KDE plot conditioned on the `species` feature. 6. Use a joint plot to visualize the relationship between `bill_length_mm` and `bill_depth_mm` with KDE marginal distributions. 7. Create a pairwise plot for the dataset, customizing the upper diagonal plots with histograms, lower diagonal with filled KDE plots, and diagonal with combined histograms and KDE. # Function Signature ```python def customized_distributions(): pass ``` # Constraints and Assumptions - You may assume the `seaborn` library is already installed. - Customize the visual aesthetics to make the plots more readable if necessary. - Ensure that the plots are displayed within the function. - Follow best practices for code readability and maintainability. # Input - No input parameters for the function. # Output - The function should display the following plots: 1. Histogram of `flipper_length_mm` with 20 bins. 2. KDE plot of `flipper_length_mm` with bandwidth adjustment. 3. Layered KDE plot of `flipper_length_mm` conditioned on `species`. 4. Joint plot for `bill_length_mm` and `bill_depth_mm` with KDE marginals. 5. Pairwise plot for all numeric features in the dataset. # Example Usage ```python customized_distributions() ``` The above call should generate and display all the plots as specified. # Evaluation Criteria - **Correctness**: The plots should match the specified customization requirements. - **Clarity**: The code should be well-organized and readable. - **Comprehension**: Demonstrated understanding of Seaborn functionality and its various parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_distributions(): # Load the dataset penguins = sns.load_dataset(\'penguins\') # Create a histogram of the flipper_length_mm variable with 20 bins plt.figure(figsize=(8, 6)) sns.histplot(data=penguins, x=\'flipper_length_mm\', bins=20) plt.title(\'Flipper Length Histogram with 20 Bins\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') plt.show() # Create a KDE plot for the flipper_length_mm variable with a bandwidth adjustment plt.figure(figsize=(8, 6)) sns.kdeplot(data=penguins, x=\'flipper_length_mm\', bw_adjust=0.25) plt.title(\'Flipper Length KDE Plot with Bandwidth Adjustment\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() # Generate a layered KDE plot conditioned on the species feature plt.figure(figsize=(8, 6)) sns.kdeplot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', fill=True) plt.title(\'Flipper Length KDE Plot by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.legend(title=\'Species\') plt.show() # Use a joint plot to visualize the relationship between bill_length_mm and bill_depth_mm with KDE marginals sns.jointplot(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', kind=\'kde\', fill=True, marginal_ticks=True) plt.suptitle(\'Joint Plot of Bill Length and Bill Depth with KDE Marginals\', y=1.02) plt.show() # Create a pairwise plot for the dataset pair_plot = sns.pairplot(penguins, diag_kind=\'kde\', kind=\'hist\', plot_kws={\'alpha\': 0.5}, diag_kws={\'shade\': True}) pair_plot.fig.suptitle(\'Pairwise Plot of Penguin Measurements\', y=1.02) plt.show()"},{"question":"# Python Coding Assessment Question Objective: Design a function using the `turtle` module to draw a fractal tree. The function should use recursion to create branches and sub-branches. This problem assesses the students\' understanding of recursion, graphics, and the use of the `turtle` module. Problem Description: Implement a function `draw_fractal_tree(t, branch_length, angle, factor)` using the `turtle` module that draws a fractal tree. The tree should branch into smaller sub-branches recursively until a base case is reached. Function Signature: ```python def draw_fractal_tree(t, branch_length, angle, factor): # t: An instance of `turtle.Turtle` # branch_length: Initial length of the branch (int) # angle: Angle between branches (int) # factor: Reduction factor for branch length at each recursive step (float) ``` Input: - `t`: An instance of `turtle.Turtle` used for drawing. - `branch_length` (int): The length of the initial branch. - `angle` (int): The angle between the branches. - `factor` (float): The reduction factor for the branch length when a new branch is created. Output: - No return value. The function should display the fractal tree using the turtle graphics window. Example: ```python import turtle screen = turtle.Screen() screen.bgcolor(\\"white\\") t = turtle.Turtle() t.speed(0) t.left(90) # Ensure the tree is drawn upwards t.up() t.backward(100) t.down() t.color(\\"green\\") draw_fractal_tree(t, 100, 30, 0.7) turtle.done() ``` # Function Implementation: 1. If the `branch_length` is less than a certain threshold, stop the recursion (base case). 2. Move the turtle forward by `branch_length`. 3. Turn the turtle left by `angle`. 4. Recursively call `draw_fractal_tree` to draw the left branch with the reduced `branch_length` multiplied by the `factor`. 5. Turn the turtle right by `2 * angle`. 6. Recursively call `draw_fractal_tree` to draw the right branch with the reduced `branch_length` multiplied by the `factor`. 7. Turn the turtle back to the center by `angle`. 8. Move the turtle backward by `branch_length` to restore to the original position. Constraints: - 1 <= `branch_length` <= 300 - 10 <= `angle` <= 90 - 0.1 <= `factor` <= 1.0 Notes: - Use the turtle\'s `forward()`, `backward()`, `left()`, and `right()` methods to move the turtle. - The `speed(0)` method sets the fastest drawing speed for the turtle. - Ensure the base case is well-defined to prevent infinite recursion. This question assesses the ability to utilize the `turtle` graphics module, understand and implement recursion, and perform graphical transformations.","solution":"import turtle def draw_fractal_tree(t, branch_length, angle, factor): Draws a fractal tree using recursion. Parameters: t (turtle.Turtle): An instance of the turtle used for drawing. branch_length (int): Initial length of the branch. angle (int): Angle between branches. factor (float): Reduction factor for branch length at each recursive step. if branch_length < 5: # Base case: stop drawing if the branch length is too small return t.forward(branch_length) t.left(angle) # Recursively draw the left sub-branch draw_fractal_tree(t, branch_length * factor, angle, factor) t.right(2 * angle) # Recursively draw the right sub-branch draw_fractal_tree(t, branch_length * factor, angle, factor) t.left(angle) t.backward(branch_length) # Example of how to use this function if __name__ == \\"__main__\\": screen = turtle.Screen() screen.bgcolor(\\"white\\") t = turtle.Turtle() t.speed(0) # Fastest speed t.left(90) # Ensure the tree is drawn upwards t.up() t.backward(100) t.down() t.color(\\"green\\") draw_fractal_tree(t, 100, 30, 0.7) turtle.done()"},{"question":"# Covariance Matrix Estimation with Scikit-learn In this task, you will use the `sklearn.covariance` module to estimate the covariance matrix of a given dataset using different methods, and compare their performance. # Objective 1. **Compute the empirical covariance matrix** of the dataset. 2. **Compute the shrunk covariance matrix** using Ledoit-Wolf shrinkage. 3. **Compute the sparse inverse covariance (precision) matrix** using graphical lasso. 4. **Compute a robust covariance matrix** using the Minimum Covariance Determinant (MCD) estimator. 5. **Compare the estimated covariance matrices** and discuss the differences. # Input and Output - **Input**: - A 2D NumPy array `data` of shape (n_samples, n_features). - **Output**: - Print the following estimated matrices: - Empirical covariance matrix (shape: (n_features, n_features)) - Ledoit-Wolf shrunk covariance matrix (shape: (n_features, n_features)) - Sparse inverse covariance matrix (precision matrix, shape: (n_features, n_features)) - Minimum Covariance Determinant (MCD) matrix (shape: (n_features, n_features)) # Constraints - Use the `sklearn.covariance` module for implementations. Do not use other packages or libraries for covariance estimation. - Assume `n_samples > n_features` for simplicity. # Performance - Ensure the solution is optimized for performance, focusing on efficient use of scikit-learn functions and methods. # Example Usage ```python import numpy as np from sklearn.covariance import EmpiricalCovariance, LedoitWolf, GraphicalLasso, MinCovDet def compute_covariance_matrices(data): # Step 1: Empirical Covariance emp_cov = EmpiricalCovariance().fit(data).covariance_ print(\\"Empirical Covariance Matrix:\\") print(emp_cov) # Step 2: Ledoit-Wolf Shrunk Covariance lw = LedoitWolf().fit(data) lw_cov = lw.covariance_ print(\\"nLedoit-Wolf Shrunk Covariance Matrix:\\") print(lw_cov) # Step 3: Graphical Lasso (Sparse Inverse Covariance) graphical_lasso = GraphicalLasso(alpha=0.01).fit(data) precision_matrix = graphical_lasso.precision_ print(\\"nSparse Inverse Covariance (Precision) Matrix:\\") print(precision_matrix) # Step 4: Minimum Covariance Determinant (Robust Covariance) mcd = MinCovDet().fit(data) mcd_cov = mcd.covariance_ print(\\"nMinimum Covariance Determinant (MCD) Matrix:\\") print(mcd_cov) # Example data data = np.random.rand(100, 5) compute_covariance_matrices(data) ``` The provided example is a skeleton implementation that you are expected to enhance, if necessary, to complete the task as described. # Discussion Points - After computing the covariance matrices, compare and discuss: - Why and in what scenarios the empirical covariance matrix might fail. - The advantages of Ledoit-Wolf shrinkage. - The scenarios where sparse inverse covariance estimation is useful. - The robustness of the MCD estimator against outliers.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, LedoitWolf, GraphicalLasso, MinCovDet def compute_covariance_matrices(data): Computes various types of covariance matrices for the given data. Parameters: data (np.ndarray): A 2D array of shape (n_samples, n_features). Returns: dict: A dictionary containing the computed covariance matrices. # Step 1: Empirical Covariance emp_cov = EmpiricalCovariance().fit(data).covariance_ # Step 2: Ledoit-Wolf Shrunk Covariance lw = LedoitWolf().fit(data) lw_cov = lw.covariance_ # Step 3: Graphical Lasso (Sparse Inverse Covariance) graphical_lasso = GraphicalLasso(alpha=0.01).fit(data) precision_matrix = graphical_lasso.precision_ # Step 4: Minimum Covariance Determinant (Robust Covariance) mcd = MinCovDet().fit(data) mcd_cov = mcd.covariance_ # Return all covariance matrices in a dictionary return { \'empirical_covariance\': emp_cov, \'ledoitwolf_covariance\': lw_cov, \'sparse_inverse_covariance\': precision_matrix, \'mcd_covariance\': mcd_cov }"},{"question":"**Objective**: To assess the understanding of Python 3.10 built-in types, their operations, and appropriate usage. # Question You are tasked with writing a Python program that simulates a basic library system to track books and their availability using Python\'s built-in data types and collection methods. Requirements: 1. **Book Class**: - Create a class `Book` that has the following attributes: - `title` (str): The title of the book. - `author` (str): The author of the book. - `isbn` (str): The unique ISBN of the book. - Implement the `__str__` method to return a string representing the book in the format: `\\"Title by Author (ISBN: isbn)\\"`. - Implement the `__eq__` and `__hash__` methods so that books can be compared and work as keys in a dictionary/set. 2. **Library Class**: - Create a class `Library` that manages a collection of books. It should have the following methods: - `add_book(self, book: Book)`: Adds a book to the library. - `remove_book(self, isbn: str) -> bool`: Removes a book with the given ISBN. Returns `True` if the book was removed, `False` otherwise. - `find_book(self, isbn: str) -> Book`: Finds and returns the book with the given ISBN. Returns `None` if the book is not found. - `list_books(self)`: Prints all books in the library. 3. **Extra Functionality**: - Extend the `Library` class to handle borrowed books: - `borrow_book(self, isbn: str) -> bool`: Marks a book as borrowed. Returns `True` if the book was available and borrowed, `False` otherwise. - `return_book(self, isbn: str) -> bool`: Marks a book as returned. Returns `True` if the book was successfully returned, `False` otherwise. # Constraints: - The ISBN for a book is assumed to be unique across the entire library. - Borrowed books should not be available for borrowing until they are returned. - Handle edge cases appropriately, such as trying to borrow a book that doesn\'t exist or is already borrowed. # Input and Output: - There is no fixed input format; instead, define a main function to demonstrate adding, removing, finding, listing, borrowing, and returning books using the `Library` class. - Print relevant messages during each operation to demonstrate functionality. # Example: ```python def main(): b1 = Book(\\"1984\\", \\"George Orwell\\", \\"1234567890\\") b2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"2345678901\\") b3 = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", \\"3456789012\\") library = Library() library.add_book(b1) library.add_book(b2) library.add_book(b3) print(\\"Listing all books:\\") library.list_books() print(\\"Borrowing \'1984\':\\") library.borrow_book(\\"1234567890\\") print(\\"Trying to borrow \'1984\' again:\\") library.borrow_book(\\"1234567890\\") print(\\"Returning \'1984\':\\") library.return_book(\\"1234567890\\") print(\\"Listing all books after operations:\\") library.list_books() if __name__ == \\"__main__\\": main() ``` This functionality demonstrates a clear understanding of Python\'s built-in types and their operations, encapsulation principles in classes, handling edge cases, and working with collections effectively.","solution":"class Book: def __init__(self, title, author, isbn): self.title = title self.author = author self.isbn = isbn def __str__(self): return f\'\\"{self.title}\\" by {self.author} (ISBN: {self.isbn})\' def __eq__(self, other): if isinstance(other, Book): return self.isbn == other.isbn return False def __hash__(self): return hash(self.isbn) class Library: def __init__(self): self.books = {} self.borrowed_books = set() def add_book(self, book): if book.isbn not in self.books: self.books[book.isbn] = book def remove_book(self, isbn): if isbn in self.books: if isbn not in self.borrowed_books: del self.books[isbn] return True return False def find_book(self, isbn): return self.books.get(isbn, None) def list_books(self): for book in self.books.values(): print(book) def borrow_book(self, isbn): if isbn in self.books and isbn not in self.borrowed_books: self.borrowed_books.add(isbn) return True return False def return_book(self, isbn): if isbn in self.borrowed_books: self.borrowed_books.remove(isbn) return True return False"},{"question":"**Managing Context-Sensitive State with Context Variables** # Objective Your task is to implement a state management system that demonstrates the use of the `contextvars` module to manage context-sensitive variables in an asynchronous environment. # Problem Statement Create an asynchronous simulation where multiple \\"tasks\\" operate with their own context-sensitive state. Each task will manage a counter that it increments independently of other tasks. The goal is to ensure that each task\'s counter remains isolated from others using the `contextvars` module. # Requirements 1. **Context Variable Initialization**: - Initialize a `ContextVar` named `task_counter` with a default value of `0`. 2. **Task Function**: - Implement an asynchronous function `task_incrementer(task_id: int, increments: int)` which: - Receives a task identifier `task_id` and number of increments `increments`. - Increments the `task_counter` context variable by 1 in a loop for `increments` times. - Prints the task identifier and the final value of `task_counter`. 3. **Simulation Function**: - Create a function `run_simulation(task_configs: List[Tuple[int, int]])` that: - Accepts a list of tuples, where each tuple contains a task identifier and the number of increments. - For each tuple in `task_configs`, it should: - Use `copy_context()` to create a copy of the current context. - Run the `task_incrementer` function in the copied context using the context\'s `run` method. - Ensure proper handling of asynchronous execution by using asyncio. - Print the status of all tasks\' `task_counter` at the end of the simulation. # Example ```python import asyncio import contextvars # Initialize context variable task_counter = contextvars.ContextVar(\'task_counter\', default=0) async def task_incrementer(task_id: int, increments: int): for _ in range(increments): # Retrieve current counter value, increment it, and set the new value counter = task_counter.get() task_counter.set(counter + 1) print(f\\"Task {task_id} final counter value: {task_counter.get()}\\") def run_simulation(task_configs): loop = asyncio.get_event_loop() async def main(): tasks = [] for task_id, increments in task_configs: ctx = contextvars.copy_context() tasks.append(loop.create_task(ctx.run(task_incrementer, task_id, increments))) await asyncio.gather(*tasks) loop.run_until_complete(main()) print(\\"Simulation complete\\") # Example usage: task_configs = [ (1, 5), (2, 3), (3, 7), ] run_simulation(task_configs) ``` # Constraints - The solution should work for any valid `task_configs` input. - Ensure thread safety and correct handling of context variables. - `task_incrementer` should use the `task_counter` context variable effectively. # Evaluation Criteria - Correct implementation of `ContextVar` for maintaining isolated task states. - Ability to manage and run multiple tasks asynchronously. - Proper use of context copying to isolate contexts between tasks.","solution":"import asyncio import contextvars # Initialize context variable task_counter = contextvars.ContextVar(\'task_counter\', default=0) async def task_incrementer(task_id: int, increments: int): for _ in range(increments): counter = task_counter.get() task_counter.set(counter + 1) print(f\\"Task {task_id} final counter value: {task_counter.get()}\\") def run_simulation(task_configs): loop = asyncio.get_event_loop() async def main(): tasks = [] for task_id, increments in task_configs: ctx = contextvars.copy_context() tasks.append(loop.create_task(ctx.run(task_incrementer, task_id, increments))) await asyncio.gather(*tasks) loop.run_until_complete(main()) print(\\"Simulation complete\\")"},{"question":"# Socket Programming in Python Objective: Design a socket-based client-server application in Python. The server application should be able to accept multiple client connections and facilitate a simple message exchange system. The objective is to assess the understanding of creating, managing, and using sockets effectively for inter-process communication. Requirements: - **Server Application**: 1. The server should create a socket using `AF_INET` and `SOCK_STREAM`. 2. Bind the socket to any available host and a port number greater than 1024. 3. Listen for incoming connections. 4. Accept multiple client connections. 5. Handle each client connection in a separate thread. 6. Echo any message received from a client back to that client. 7. Gracefully handle errors, ensuring sockets are closed properly. - **Client Application**: 1. The client should create a socket using `AF_INET` and `SOCK_STREAM`. 2. Connect to the server\'s socket. 3. Read user input from the command line and send it to the server. 4. Display any message received from the server. 5. Gracefully close the connection when finished. Constraints: - The server should handle at least 5 simultaneous client connections without crashing. - Use appropriate error handling to deal with network-related issues. - Assume messages are simple strings of reasonable length (less than 1024 bytes). Additional Notes: - Use `select` to create a non-blocking version of the server if you aim for extra credit. - You are free to use any standard Python libraries. - Ensure your code is well-documented and formatted according to PEP 8 guidelines. Input/Output Formats: - **Server Output**: Print connection events and received messages from clients to the console. - **Client Input**: Users will input messages in the command line. - **Client Output**: Display server responses in the command line. Example: - **Server**: ```python Starting server on localhost:12345 Connection from 192.168.1.2:34567 Received message: \\"Hello, server!\\" ``` - **Client**: ```commandline Enter message: Hello, server! Server response: Hello, server! ``` Submission: - Submit both server and client programs in Python files with appropriate comments and documentation. # Evaluation Criteria: - Functional correctness. - Handling multiple client connections. - Proper usage of socket functions. - Error handling. - Code readability and documentation.","solution":"import socket import threading def handle_client(conn, addr): print(f\\"Connection from {addr}\\") with conn: while True: try: data = conn.recv(1024) if not data: break print(f\\"Received message from {addr}: {data.decode()}\\") conn.sendall(data) except ConnectionError: break print(f\\"Connection closed from {addr}\\") def start_server(): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'\', 12345)) server_socket.listen(5) print(\\"Server started on port 12345\\") try: while True: conn, addr = server_socket.accept() client_thread = threading.Thread(target=handle_client, args=(conn, addr)) client_thread.start() except KeyboardInterrupt: print(\\"Server shutting down.\\") finally: server_socket.close() if __name__ == \\"__main__\\": start_server()"},{"question":"# Config File Parsing and Validation using `configparser` **Objective**: Implement a function to parse a given configuration file and validate certain expected values using the `configparser` library. **Problem Statement**: You are given a configuration file in the INI format. Your task is to implement a Python function `parse_and_validate_config` that: 1. Parses the given configuration file. 2. Validates that the configuration contains certain required sections and keys. 3. Validates the values of specific keys with given constraints. **Function Signature**: ```python def parse_and_validate_config(filepath: str, required_sections: dict) -> bool: Parses the configuration file at the given filepath and validates its contents. Args: filepath (str): The path to the configuration file. required_sections (dict): A dictionary specifying the required sections and keys with their validation constraints. The dictionary format is: { \'SectionA\': { \'key1\': lambda x: x.isdigit(), # Value must be a digit \'key2\': lambda x: len(x) > 5 # Value length must be greater than 5 }, \'SectionB\': { \'key3\': lambda x: x in [\'yes\', \'no\'] # Value must be \'yes\' or \'no\' } } Returns: bool: True if the configuration file is valid, False otherwise. ``` **Input**: - `filepath` (str): Path to the INI configuration file. - `required_sections` (dict): A dictionary specifying required sections and keys with constraints as lambdas. **Output**: - Returns `True` if the configuration file is valid according to the `required_sections` constraints, `False` otherwise. **Constraints**: 1. The configuration file must be in a valid INI format. 2. Each key\'s value must pass the associated validation function provided in `required_sections`. # Example Given the configuration file `config.ini`: ``` [SectionA] key1=123 key2=abcdef [SectionB] key3=yes ``` ```python required_sections = { \'SectionA\': { \'key1\': lambda x: x.isdigit(), \'key2\': lambda x: len(x) > 5 }, \'SectionB\': { \'key3\': lambda x: x in [\'yes\', \'no\'] } } ``` For the `filepath` pointing to `config.ini`, `parse_and_validate_config(filepath, required_sections)` should return `True`. # Notes - Make sure to handle cases where the required sections or keys are missing by returning `False`. - Utilize the `configparser` module to parse the configuration file and handle errors gracefully. # Testing You can test your function with different configuration files and validation rules to ensure correctness.","solution":"import configparser def parse_and_validate_config(filepath: str, required_sections: dict) -> bool: Parses the configuration file at the given filepath and validates its contents. Args: filepath (str): The path to the configuration file. required_sections (dict): A dictionary specifying the required sections and keys with their validation constraints. Returns: bool: True if the configuration file is valid, False otherwise. config = configparser.ConfigParser() config.read(filepath) for section, keys in required_sections.items(): if section not in config: return False for key, validation_func in keys.items(): if key not in config[section]: return False value = config[section][key] if not validation_func(value): return False return True"},{"question":"# Question You are required to implement a function, `count_non_empty_lines(filename, start_lineno, end_lineno)`, which uses the `linecache` module to count the number of non-empty lines in a file between specified line numbers (inclusive). Function Signature ```python def count_non_empty_lines(filename: str, start_lineno: int, end_lineno: int) -> int: pass ``` Input - `filename` (str): The name of the file from which lines are to be read. - `start_lineno` (int): The line number to start reading from (inclusive). - `end_lineno` (int): The line number to read up to (inclusive). Output - Returns an integer indicating the number of non-empty lines in the specified range. Example ```python # Assume \'sample.txt\' contains the following lines: # Line 1: Hello # Line 2: # Line 3: This is a sample file. # Line 4: # Line 5: Python linecache module. print(count_non_empty_lines(\'sample.txt\', 1, 5)) # Output should be 3 print(count_non_empty_lines(\'sample.txt\', 2, 4)) # Output should be 1 ``` Constraints - `start_lineno` and `end_lineno` are positive integers and `start_lineno <= end_lineno`. - If the range provided exceeds the actual number of lines in the file, consider only the lines available within the file. Notes - Utilize the `linecache.getline` function to read lines. - An empty line is defined as a line containing only whitespace characters or nothing at all. Additional Requirements - Your solution should handle errors gracefully and not raise exceptions for invalid file names or line numbers out of range. If an error occurs, count the valid lines accessible. Implement the function `count_non_empty_lines` as specified.","solution":"import linecache def count_non_empty_lines(filename: str, start_lineno: int, end_lineno: int) -> int: Count the number of non-empty lines in a file from start_lineno to end_lineno inclusive. non_empty_lines_count = 0 for lineno in range(start_lineno, end_lineno + 1): line = linecache.getline(filename, lineno) if line.strip(): # Check if the line is non-empty non_empty_lines_count += 1 return non_empty_lines_count"},{"question":"# Question You are provided with a set of advanced functionalities in the `torch.backends.cuda` and `torch.backends.cudnn` modules. Your task is to implement a function that configures CUDA and cuDNN backends for optimal performance in a matrix multiplication operation on an Ampere or newer GPU. Your function should: 1. Enable TensorFloat-32 (TF32) tensor cores for matrix multiplications. 2. Enable reduced precision reductions with fp16 accumulation type. 3. Optimize cuDNN for the fastest convolution algorithms. 4. Check the availability of CUDA and cuDNN. 5. Configure cuFFT plan cache for CUDA devices with a specified maximum size, and clear it before setting the new size. **Function Signature:** ```python import torch def optimize_pytorch_backends(max_cufft_cache_size: int) -> bool: Configures CUDA and cuDNN backends for optimal performance. Parameters: max_cufft_cache_size (int): The maximum size for cuFFT plan cache. Returns: bool: True if both CUDA and cuDNN are available and configurations were successful, False otherwise. pass ``` # Expected Function Behavior * Enable TensorFloat-32 (TF32) tensor cores for matrix multiplications on Ampere or newer GPUs. * Enable reduced precision reductions with fp16 accumulation type. * Enable cuDNN convolution benchmarks and set the benchmark limit to try the fastest algorithm. * Check if CUDA and cuDNN are available. * Set the cuFFT plan cache for each available CUDA device to `max_cufft_cache_size` and clear the current cache before setting the new size. * Return `True` if both CUDA and cuDNN are available and configurations were successful, otherwise return `False`. # Constraints * The function should handle any exceptions that might be raised during backend configurations. * The function should work on a system where PyTorch is installed and compatible hardware is available. # Example Usage ```python result = optimize_pytorch_backends(1024) if result: print(\\"CUDA and cuDNN backends configured successfully.\\") else: print(\\"Failed to configure CUDA and cuDNN backends.\\") ``` **Tips:** - Use `torch.cuda.is_available()` to check CUDA availability. - Use `torch.backends.cudnn.is_available()` to check cuDNN availability. - Modify the required attributes and methods as documented.","solution":"import torch def optimize_pytorch_backends(max_cufft_cache_size: int) -> bool: Configures CUDA and cuDNN backends for optimal performance. Parameters: max_cufft_cache_size (int): The maximum size for cuFFT plan cache. Returns: bool: True if both CUDA and cuDNN are available and configurations were successful, False otherwise. try: # Check if CUDA is available if not torch.cuda.is_available(): return False # Enable TensorFloat-32 (TF32) tensor cores for matrix multiplications. torch.backends.cuda.matmul.allow_tf32 = True # Enable reduced precision reductions with fp16 accumulation type torch.backends.cudnn.allow_fp16_reduced_precision_reduction = True # Enable cuDNN convolution benchmarks torch.backends.cudnn.benchmark = True # Check if cuDNN is available if not torch.backends.cudnn.is_available(): return False # Configure cuFFT plan cache torch.cuda.cufft_plan_cache.clear() torch.cuda.cufft_plan_cache.set_max_size(max_cufft_cache_size) return True except Exception as e: print(f\\"Exception occurred while optimizing PyTorch backends: {e}\\") return False"},{"question":"# Seaborn Coding Assessment Question **Objective**: Demonstrate your understanding of seaborn\'s palette customization and its integration with data visualization. **Question**: You are tasked with visualizing the relationship between two continuous variables from a dataset. You should use the seaborn package to create a scatter plot with customized aesthetics. The customization should demonstrate your understanding of the `husl_palette` function and its various parameters. **Dataset**: Create a pandas DataFrame containing the following columns: - `x`: A sequence of 100 equally spaced values from 0 to 10. - `y`: A sine wave transformation of `x` with some added random noise for variance. **Requirements**: 1. **Scatter Plot Creation**: Create a scatter plot using the seaborn `scatterplot` function. 2. **Color Palette Customization**: - Use the `husl_palette` function to generate a custom palette. - Create the palette with 5 different hues. - Adjust the lightness and saturation of the palette according to your preference. 3. **Integration of Palette**: Apply the customized HUSL palette to the scatter plot. **Input and Output**: - Input: No explicit input function is required. The dataset should be created within your code. - Output: The scatter plot should be displayed as part of the output. **Hints**: - You can use the numpy and pandas libraries to generate and manipulate your data. - To apply the custom HUSL palette to the scatter plot, you can utilize the `palette` parameter in the seaborn `scatterplot` function. **Constraints**: - Ensure that the palette contains exactly 5 colors. - The lightness of the colors should be less than 0.5. - The code should be written in Python 3. **Example**: Here is a skeleton to help you get started: ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Generate the data x = np.linspace(0, 10, 100) y = np.sin(x) + np.random.normal(0, 0.1, 100) data = pd.DataFrame({\'x\': x, \'y\': y}) # Step 2: Customize the palette palette = sns.husl_palette(n_colors=5, l=.4, s=.7) # Step 3: Create the scatter plot sns.set_theme(style=\\"whitegrid\\") scatter_plot = sns.scatterplot(data=data, x=\'x\', y=\'y\', palette=palette) # Step 4: Show the plot plt.show() ``` **Deliverables**: A Python script that: - Generates and displays the required scatter plot. - Contains well-documented code explaining each step.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot_with_custom_palette(): This function creates a scatter plot with a custom HUSL palette to visualize the relationship between two continuous variables. # Step 1: Generate the data np.random.seed(0) x = np.linspace(0, 10, 100) y = np.sin(x) + np.random.normal(0, 0.1, 100) data = pd.DataFrame({\'x\': x, \'y\': y}) # Step 2: Customize the palette palette = sns.husl_palette(n_colors=5, l=.4, s=.7) # Step 3: Create the scatter plot sns.set_theme(style=\\"whitegrid\\") scatter_plot = sns.scatterplot(data=data, x=\'x\', y=\'y\', palette=palette) # Step 4: Show the plot plt.show()"},{"question":"# Asynchronous Network Server **Problem**: You are required to implement an asynchronous echo server using the `asyncio` event loop functionalities. The server should be able to handle multiple client connections concurrently and echo back any message received from a client. The server should log incoming connections and messages. **Specifications**: 1. **Function Signature**: ```python async def start_server(host: str, port: int): pass ``` 2. **Input**: - `host` (str): The IP address to bind the server. - `port` (int): The port number to bind the server. 3. **Functionality**: - The server should start accepting connections on the given `host` and `port`. - When a client connects, log the connection with the client\'s address. - For each received message from a client, log the message and echo it back to the client. - Handle multiple client connections concurrently. - If a client disconnects, log the disconnection and clean up resources. - The server should be stoppable (i.e., support a graceful shutdown). 4. **Constraints**: - Use `asyncio` and its event loop functionalities. - Utilize coroutines and `async`/`await` syntax. - Aim to efficiently handle multiple concurrent clients. 5. **Performance**: - Efficiently handle and process client connections using the `asyncio` framework. **Example**: ```python import asyncio async def start_server(host: str, port: int): pass # Your implementation here async def main(): await start_server(\'127.0.0.1\', 8888) await asyncio.sleep(3600) # Run for 1 hour to allow connections asyncio.run(main()) ``` *Note*: Ensure that the server can be tested locally by running the sample `main` function. **Hints**: - Look into `asyncio.start_server` for creating the server. - Investigate how to read and write asynchronously using `StreamReader` and `StreamWriter`. Good luck!","solution":"import asyncio import logging logging.basicConfig(level=logging.INFO) async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') logging.info(f\\"Connection from {addr}\\") try: while True: data = await reader.read(100) if not data: logging.info(f\\"Connection closed by {addr}\\") break message = data.decode() logging.info(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() except asyncio.CancelledError: logging.info(f\\"Connection cancelled by {addr}\\") finally: writer.close() await writer.wait_closed() async def start_server(host: str, port: int): server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() logging.info(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(start_server(\'127.0.0.1\', 8888))"},{"question":"<|Analysis Begin|> The `contextlib` module from Python\'s standard library provides utilities for context management in scripts and applications. Specifically, it helps in managing resources that need to be set up and then cleaned up reliably, such as files or network connections. Here are the key utilities and concepts provided by `contextlib`: 1. **Abstract Base Classes for Context Managers**: - `AbstractContextManager`: Used for classes that implement synchronous context management (`__enter__` and `__exit__` methods). - `AbstractAsyncContextManager`: Used for classes that implement asynchronous context management (`__aenter__` and `__aexit__` methods). 2. **Decorator Functions for Context Managers**: - `contextmanager`: Decorator to simplify the creation of context managers using generator functions, ensuring resources are properly handled. - `asynccontextmanager`: Decorator similar to `contextmanager`, but for asynchronous contexts. 3. **Convenience Functions to Create Context Managers**: - `closing`: Ensures that an object\'s `close` method is called when exiting the context. - `aclosing`: Ensures that an object\'s `aclose` coroutine is awaited upon exiting the context. - `nullcontext`: A context manager that does nothing, useful for creating conditional context management blocks. - `suppress`: Suppresses specified exceptions within a context. - `redirect_stdout` & `redirect_stderr`: Temporarily redirect outputs to another file-like object. 4. **Classes for Managing Complex Contexts**: - `ContextDecorator`: Allows a context manager to be used as both a decorator and a traditional context manager. - `AsyncContextDecorator`: Like `ContextDecorator`, but for asynchronous functions. - `ExitStack`: Provides a mechanism for programmatically combining multiple context managers and cleanup functions. - `AsyncExitStack`: Like `ExitStack`, but supports asynchronous context managers and coroutines. These utilities help manage resource allocation and cleanup consistently, whether working with synchronous or asynchronous code. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Context You are working with a codebase that requires the consistent management of multiple resources, some of which might be opened/allocated conditionally or in a sequence that might raise exceptions. To ensure proper allocation and cleanup of these resources, you are to use the `contextlib` module. Task Implement a class `ConditionalResourceManager`, which uses `contextlib.ExitStack` to manage a series of resources based on given conditions. The class should provide methods to add resources either conditionally or non-conditionally, ensuring that all resources are correctly released upon exit, even if allocations fail at some point. Function Signature ```python from contextlib import ExitStack class ConditionalResourceManager: def __init__(self): # Initializes the ExitStack self.stack = ExitStack() def add_resource(self, condition: bool, resource_func, *args, **kwargs): Adds a resource to the stack conditionally. If the condition is True, executes the resource_func with provided args and kwargs to get the resource, and ensures it is properly managed with the stack. :param condition: A boolean indicating if the resource should be added. :param resource_func: A function that when called, returns a context manager. if condition: resource = resource_func(*args, **kwargs) self.stack.enter_context(resource) def __enter__(self): Enter the stack context. :return: self for usage within a `with` statement. self.stack.__enter__() return self def __exit__(self, exc_type, exc_val, exc_tb): Exit the stack context and handle cleanup. return self.stack.__exit__(exc_type, exc_val, exc_tb) ``` Example ```python from contextlib import contextmanager @contextmanager def sample_resource(name): print(f\\"Acquiring resource {name}\\") yield print(f\\"Releasing resource {name}\\") with ConditionalResourceManager() as manager: manager.add_resource(True, sample_resource, \\"Resource 1\\") manager.add_resource(False, sample_resource, \\"Resource 2\\") manager.add_resource(True, sample_resource, \\"Resource 3\\") # Output should be: # Acquiring resource Resource 1 # Acquiring resource Resource 3 # Releasing resource Resource 3 # Releasing resource Resource 1 ``` Constraints - `resource_func` should be a callable that provides a context manager when called. - The class should ensure that the resources are acquired and released in the correct order. - Resources should be managed correctly even if an exception occurs during their acquisition or usage. Implement the `ConditionalResourceManager` and test it with various resources to ensure it works as expected.","solution":"from contextlib import ExitStack class ConditionalResourceManager: def __init__(self): # Initializes the ExitStack self.stack = ExitStack() def add_resource(self, condition: bool, resource_func, *args, **kwargs): Adds a resource to the stack conditionally. If the condition is True, executes the resource_func with provided args and kwargs to get the resource, and ensures it is properly managed with the stack. :param condition: A boolean indicating if the resource should be added. :param resource_func: A function that when called, returns a context manager. if condition: resource = resource_func(*args, **kwargs) self.stack.enter_context(resource) def __enter__(self): Enter the stack context. :return: self for usage within a `with` statement. self.stack.__enter__() return self def __exit__(self, exc_type, exc_val, exc_tb): Exit the stack context and handle cleanup. return self.stack.__exit__(exc_type, exc_val, exc_tb)"},{"question":"**Objective**: Assess student\'s ability to perform fundamental and advanced operations using `Timedelta` in pandas. **Problem Statement**: You are given a DataFrame containing a column with task durations. You need to perform various operations on these timedeltas to analyze the tasks. ```python import pandas as pd # Sample DataFrame data = {\'Task\': [\'Task A\', \'Task B\', \'Task C\'], \'Duration\': [\'1 days 05:30:00\', \'2 days 03:45:30\', \'3 days 01:15:20\']} df = pd.DataFrame(data) ``` **Tasks**: 1. Convert the \'Duration\' column to `Timedelta` objects. 2. Calculate the total duration of all tasks. 3. Find the average duration of the tasks. 4. Convert the average duration to seconds and days. 5. Add a new column \'Adjusted Duration\' by adding 2 hours to each task\'s duration. 6. Resample the DataFrame to summarize the durations on a daily basis using the \'sum\' aggregation. **Function Signature**: ```python def analyze_task_durations(df: pd.DataFrame) -> dict: Analyzes task durations in a DataFrame. Parameters: df (pd.DataFrame): DataFrame with columns \'Task\' and \'Duration\' as string representations of timedeltas. Returns: dict: Dictionary containing: - \'total_duration\': Total duration of all tasks (Timedelta). - \'average_duration\': Average duration of all tasks (Timedelta). - \'average_duration_seconds\': Average duration in seconds (int). - \'average_duration_days\': Average duration in days (float). - \'adjusted_df\': DataFrame with an additional column \'Adjusted Duration\'. - \'resampled_df\': DataFrame resampled daily with summed durations. pass ``` **Expected Input and Output**: - **Input**: A DataFrame `df` with the above structure. - **Output**: A dictionary with the results as specified, containing total duration, average duration, average duration in seconds and days, and the adjusted and resampled DataFrames. **Constraints**: - The DataFrame columns \'Task\' and \'Duration\' will always be present. - The \'Duration\' column contains valid string representations of timedeltas. **Instructions**: 1. Implement the `analyze_task_durations` function to perform the tasks specified. 2. Use appropriate pandas functions to handle `Timedelta`, conversions, and resampling. 3. Return the results in the specified dictionary format.","solution":"import pandas as pd def analyze_task_durations(df: pd.DataFrame) -> dict: # 1. Convert the \'Duration\' column to `Timedelta` objects. df[\'Duration\'] = pd.to_timedelta(df[\'Duration\']) # 2. Calculate the total duration of all tasks. total_duration = df[\'Duration\'].sum() # 3. Find the average duration of the tasks. average_duration = df[\'Duration\'].mean() # 4. Convert the average duration to seconds and days. average_duration_seconds = average_duration.total_seconds() average_duration_days = average_duration / pd.Timedelta(days=1) # 5. Add a new column \'Adjusted Duration\' by adding 2 hours to each task\'s duration. df[\'Adjusted Duration\'] = df[\'Duration\'] + pd.Timedelta(hours=2) # 6. Resample the DataFrame to summarize the durations on a daily basis using the \'sum\' aggregation. # For this, we create a new DataFrame with datetime index starting from some reference date. daily_df = df.set_index(pd.date_range(start=\'2021-01-01\', periods=len(df), freq=\'D\')) resampled_df = daily_df.resample(\'D\').sum() return { \'total_duration\': total_duration, \'average_duration\': average_duration, \'average_duration_seconds\': average_duration_seconds, \'average_duration_days\': average_duration_days, \'adjusted_df\': df, \'resampled_df\': resampled_df }"},{"question":"You are required to write a Python script that reads a given XML-formatted plist file, modifies its contents, and writes the updated data back to a new plist file. The script should handle different data types and ensure the correct format is maintained throughout the process. Task 1. **Read** the given plist file and load its contents into a dictionary. 2. **Modify** the dictionary by adding a new key-value pair: - Key: \\"updated\\" - Value: The current datetime in string format (e.g., \\"2023-10-14 12:34:56\\") 3. **Write** the updated dictionary back to a new plist file, ensuring it is in the XML format. Input - An XML-formatted plist file named `input.plist`. Output - A new XML-formatted plist file named `output.plist` containing the modified data. Requirements - Use the `plistlib` module to read and write plist files. - Ensure the new key \\"updated\\" is added to the root dictionary. - The datetime value should be formatted as a string in the format `YYYY-MM-DD HH:MM:SS`. Constraints - The input plist file is guaranteed to be well-formed and in XML format. - The dictionary in the input plist file may contain nested dictionaries and various data types supported by the `plistlib` module. Example If the `input.plist` contains: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>foo</key> <string>bar</string> </dict> </plist> ``` And the current datetime is `2023-10-14 12:34:56`, the `output.plist` should contain: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>foo</key> <string>bar</string> <key>updated</key> <string>2023-10-14 12:34:56</string> </dict> </plist> ``` **Note**: Ensure your code handles the necessary imports and exceptions appropriately.","solution":"import plistlib from datetime import datetime def update_plist(input_file, output_file): Reads an XML-formatted plist file, modifies its contents by adding a current datetime field, and writes the updated contents to a new plist file. :param input_file: Path to the input plist file. :param output_file: Path to the output plist file. with open(input_file, \'rb\') as fp: plist_data = plistlib.load(fp) # Add the new key-value pair current_time = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') plist_data[\\"updated\\"] = current_time with open(output_file, \'wb\') as fp: plistlib.dump(plist_data, fp)"},{"question":"# Asyncio Platform-Specific Implementation Challenge Objective: You are tasked with creating a function that utilizes the `asyncio` module to set up a simple server and client communication system. However, due to platform-specific limitations described in the documentation, you should ensure that your code can adapt to both Windows and macOS. Specifically, you need to ensure compatibility with both `SelectorEventLoop` and `ProactorEventLoop` where applicable. Input: - No direct input parameters. Output: - The server should print incoming messages from the client. - The client should print a confirmation message after sending each message. Requirements: 1. **Server Function:** - Create an asyncio-based server that listens for incoming connections. - The server should handle multiple client connections. - On receiving a message from a client, the server should print the message to the console. 2. **Client Function:** - Create an asyncio-based client that connects to the server. - The client should send a series of messages to the server. - After sending each message, the client should print a confirmation message. Constraints: - On **Windows**, use the `ProactorEventLoop` for the default event loop. - Ensure that your implementation does not use `loop.add_reader()` or `loop.add_writer()` methods explicitly. - On **macOS**, you can safely assume that modern versions (post 10.8) are used. - If needed, adjust the event loop to use `selectors.SelectSelector()` or the appropriate selector for older versions. Example Usage: ```python import asyncio async def run_server(): # Implement your server code here pass async def run_client(): # Implement your client code here pass # Example of launching the server and client asyncio.run(run_server()) asyncio.run(run_client()) ``` **Performance Requirements:** - Ensure the server and client can handle up to 100 concurrent connections/messages within a timeframe of 10 seconds without crashing. **Notes:** - Provide a solution that would dynamically choose the appropriate event loop based on the operating system in use. - The solution should be self-contained and executable on both Windows and macOS. Good luck! Ensure your solution adheres strictly to the platform-specific guidelines provided.","solution":"import asyncio import sys async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received from {addr}: {message}\\") writer.close() await writer.wait_closed() async def run_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def run_client(messages): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) for message in messages: writer.write(message.encode()) await writer.drain() print(f\'Sent: {message}\') print(\'Close the connection\') writer.close() await writer.wait_closed() def choose_event_loop(): if sys.platform == \\"win32\\": asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) elif sys.platform == \\"darwin\\": policy = asyncio.get_event_loop_policy() if isinstance(policy, asyncio.DefaultEventLoopPolicy): selector = selectors.SelectSelector() policy = asyncio.SelectorEventLoopPolicy(selector.__class__) asyncio.set_event_loop_policy(policy) choose_event_loop() # Example usage if __name__ == \\"__main__\\": server = asyncio.run(run_server()) # Interupt to stop the server"},{"question":"Out-of-Core Learning for Text Classification Objective You need to design an out-of-core learning algorithm to classify a large dataset of text documents that cannot fit into the main memory at once. Your task involves streaming the data, extracting features, and applying an incremental learning algorithm that updates its model as it processes each mini-batch of data. Problem Statement You are given a directory containing multiple text files. Each file represents one document, and its filename indicates its class label (e.g., \'sports_1.txt\', \'politics_2.txt\', etc.). Your goal is to build an out-of-core text classification system using scikit-learn. Requirements: 1. **Streaming Data**: Implement a generator function `data_stream(directory)` to yield tuples `(label, text)` for each document in the directory. 2. **Feature Extraction**: Use the `HashingVectorizer` from `scikit-learn` to transform the text data into feature vectors. 3. **Incremental Learning**: Use `SGDClassifier` or another suitable incremental learning classifier from scikit-learn. Specifications: - **Input**: - `directory`: A string path to the directory containing text documents. - `batch_size`: An integer specifying the number of documents to process in each mini-batch. - **Output**: - Print the accuracy of the model after processing all documents. You may use a portion of the data as a test set to evaluate the performance. - **Constraints**: - Assume the main memory is limited, and large data cannot fit into it all at once. - Classes are derived from file names and should be passed to the classifier during the first `partial_fit` call. ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import os import numpy as np def data_stream(directory): Generator function to stream text data and their labels from a directory for filename in os.listdir(directory): if filename.endswith(\\".txt\\"): label = filename.split(\'_\')[0] with open(os.path.join(directory, filename), \'r\') as file: text = file.read() yield (label, text) def out_of_core_text_classification(directory, batch_size=100): # Initialize a HashingVectorizer vectorizer = HashingVectorizer(non_negative=True) # Initialize SGDClassifier for incremental learning classifier = SGDClassifier() # Assuming binary classification for simplicity, you can expand this code for multi-class classification classes = np.array([\'sports\', \'politics\']) # Add all unique classes from your data batch_texts = [] batch_labels = [] test_texts = [] test_labels = [] # Split data into training and testing portions for i, (label, text) in enumerate(data_stream(directory)): if i % 10 == 0: # Use every 10th item for testing test_texts.append(text) test_labels.append(label) else: batch_texts.append(text) batch_labels.append(label) if len(batch_texts) == batch_size: X_batch = vectorizer.transform(batch_texts) y_batch = np.array(batch_labels) if not hasattr(classifier, \'classes_\'): classifier.partial_fit(X_batch, y_batch, classes=classes) else: classifier.partial_fit(X_batch, y_batch) batch_texts = [] batch_labels = [] # Transform test data X_test = vectorizer.transform(test_texts) y_test = np.array(test_labels) y_pred = classifier.predict(X_test) # Print accuracy accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') # Example usage directory_path = \'path/to/your/dataset\' out_of_core_text_classification(directory_path, batch_size=100) ``` # Instructions 1. Implement the `data_stream(directory)` generator function. 2. Implement the main function `out_of_core_text_classification(directory, batch_size=100)`. 3. Evaluate the classifier on a test set and print the accuracy. 4. Ensure that your implementation is memory efficient and can handle large datasets which don\'t fit in memory.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score import os import numpy as np def data_stream(directory): Generator function to stream text data and their labels from a directory for filename in os.listdir(directory): if filename.endswith(\\".txt\\"): label = filename.split(\'_\')[0] with open(os.path.join(directory, filename), \'r\', encoding=\'utf-8\') as file: text = file.read() yield (label, text) def out_of_core_text_classification(directory, batch_size=100): # Initialize a HashingVectorizer with a reasonable number of features vectorizer = HashingVectorizer(n_features=2**20, alternate_sign=False) # Initialize SGDClassifier for incremental learning classifier = SGDClassifier() # Collect all unique classes from the data classes = set() for label, _ in data_stream(directory): classes.add(label) classes = np.array(list(classes)) batch_texts = [] batch_labels = [] test_texts = [] test_labels = [] # Split data into training and testing portions for i, (label, text) in enumerate(data_stream(directory)): if i % 10 == 0: # Use every 10th item for testing test_texts.append(text) test_labels.append(label) else: batch_texts.append(text) batch_labels.append(label) if len(batch_texts) == batch_size: X_batch = vectorizer.transform(batch_texts) y_batch = np.array(batch_labels) if not hasattr(classifier, \'classes_\'): classifier.partial_fit(X_batch, y_batch, classes=classes) else: classifier.partial_fit(X_batch, y_batch) batch_texts = [] batch_labels = [] # Process any remaining data in batches if batch_texts: X_batch = vectorizer.transform(batch_texts) y_batch = np.array(batch_labels) classifier.partial_fit(X_batch, y_batch) # Transform test data X_test = vectorizer.transform(test_texts) y_test = np.array(test_labels) y_pred = classifier.predict(X_test) # Print accuracy accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\')"},{"question":"Background: You are tasked with creating a robust and flexible script to manage subprocesses using Python\'s `subprocess` module. Your task is to implement a function that runs multiple commands in a sequence, handles their inputs/outputs correctly, and ensures that resource management is handled properly (i.e., no unfinished subprocesses or open file descriptors). Objective: Implement a function `run_commands(commands, timeout)` that: 1. Accepts a list of commands where each command is a list of program arguments (e.g., `[[\'ls\', \'-l\'], [\'grep\', \'py\']]`). 2. Runs the commands in sequence such that the output of each command is piped as the input to the next command. 3. Enforces a timeout for the entire sequence. If the cumulative execution time exceeds the timeout, it should clean up by terminating any running processes and raising a `subprocess.TimeoutExpired` exception. 4. Collects and returns the final output and error of the executed command chain. 5. Handles any errors that occur during execution by raising appropriate exceptions. Function Signature: ```python import subprocess def run_commands(commands: list, timeout: int): pass ``` Inputs: - `commands`: A list of lists, each inner list represents a command and its arguments. - `timeout`: An integer representing the maximum allowed time (in seconds) for the entire sequence to complete. Outputs: - Returns a tuple of `(final_output, final_error)` where both elements are strings representing the standard output and error of the final executed command, respectively. Example: ```python try: commands = [[\'echo\', \'Hello, world!\'], [\'grep\', \'world\']] output, error = run_commands(commands, 10) print(\\"Output:\\", output) print(\\"Error:\\", error) except subprocess.TimeoutExpired: print(\\"The command sequence timed out.\\") except subprocess.CalledProcessError as e: print(f\\"Command \'{e.cmd}\' failed with return code {e.returncode}\\") ``` Constraints: - All subprocesses should be properly managed to prevent resource leaks. - The script should be platform-independent and work on both POSIX and Windows systems. - The function should handle both text and binary data properly. Note: - Make sure to use the `subprocess.PIPE` for piping outputs and `subprocess.run` or `subprocess.Popen` for advanced process handling. - Pay attention to possible edge cases, such as empty commands, non-zero return codes, and handling of signals/signals on different operating systems.","solution":"import subprocess def run_commands(commands: list, timeout: int): Run a sequence of commands where the output of each command is piped as the input to the next. :param commands: List of commands where each command is a list of arguments. :param timeout: Maximum allowed time for the sequence to complete. :return: Tuple of (final_output, final_error). :raises: subprocess.TimeoutExpired if the command sequence exceeds the given timeout. subprocess.CalledProcessError for non-zero exit statuses. if not commands: return \'\', \'\' # Chain the processes, using the previous process\' stdout as the next process\' stdin processes = [] prev_stdout = None try: for i, cmd in enumerate(commands): p = subprocess.Popen(cmd, stdin=prev_stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE) # Close the previous process\' stdout if present if prev_stdout is not None: prev_stdout.close() # Set the current process\' stdout to be the next process\' stdin prev_stdout = p.stdout processes.append(p) # Get the final process final_process = processes[-1] try: final_output, final_error = final_process.communicate(timeout=timeout) except subprocess.TimeoutExpired: for p in processes: p.kill() raise final_output = final_output.decode() final_error = final_error.decode() # Check for non-zero return codes for p in processes: if p.returncode: raise subprocess.CalledProcessError(p.returncode, p.args, output=final_output, stderr=final_error) return final_output, final_error finally: # Ensure all processes are terminated and file descriptors are closed for p in processes: if p.poll() is None: # Check if process is still running p.terminate() if p.stdout: p.stdout.close() if p.stderr: p.stderr.close()"},{"question":"You are given a dataset called `seaice`, which contains historical data on sea ice extent with columns \\"Date\\" and \\"Extent\\". Your task is to write a Python function that performs the following tasks: 1. Load the dataset using seaborn\'s `load_dataset` function. 2. Create a custom line plot using `seaborn.objects.Plot`: - The x-axis should represent the day of the year derived from the \\"Date\\" column. - The y-axis should represent the \\"Extent\\" column. - The color of the lines should differentiate between years. 3. Facet the plot by decades, and ensure that each plot within the facet represents data for a single year within that decade. 4. Customize the lines in the plot such that: - Each plot within a facet has a line width of 0.5 and a semi-transparent color `#bbca`. - The main lines should have a line width of 1. 5. Scale the colors with a specific rotation and light adjustment. 6. Layout the entire plot to have a size of 8x4 (width by height). 7. Add a title to each facet, indicating the decade it represents. # Function Definition ```python def custom_seaice_plot(): pass ``` # Input - The `seaice` dataset should be loaded within the function using `seaborn.load_dataset(\\"seaice\\")`. # Expected Output - The function should display a facetted plot with the specified customizations. # Example Executing `custom_seaice_plot()` should generate and display the facetted plot as per the instructions. # Notes - Focus on utilizing `seaborn.objects.Plot` and the methods associated with it for creating the plots. - Ensure proper handling of data transformations such as extracting the day of the year and faceting by decade. - Pay attention to the aesthetics and scaling parameters specified in the question.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def custom_seaice_plot(): # Load the dataset seaice = sns.load_dataset(\\"seaice\\") # Ensure the Date column is datetime type seaice[\'Date\'] = pd.to_datetime(seaice[\'Date\']) # Extract the day of the year and year from Date seaice[\'DayOfYear\'] = seaice[\'Date\'].dt.dayofyear seaice[\'Year\'] = seaice[\'Date\'].dt.year seaice[\'Decade\'] = (seaice[\'Year\'] // 10) * 10 # Initialize the FacetGrid g = sns.FacetGrid(seaice, col=\'Decade\', col_wrap=3, height=4, aspect=2) # Define a custom plotting function def lineplot(*args, **kwargs): data = kwargs.pop(\'data\') sns.lineplot(x=\'DayOfYear\', y=\'Extent\', hue=\'Year\', data=data, linewidth=0.5, alpha=0.8, palette=\'crest\', legend=None) # Map the plotting function to the FacetGrid g.map_dataframe(lineplot) # Adjust plot details g.set_axis_labels(\'Day of Year\', \'Extent\') for ax in g.axes.flat: ax.set_xlim(1, 365) ax.set_ylim(seaice[\'Extent\'].min(), seaice[\'Extent\'].max()) plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Sea Ice Extent by Decade\', fontsize=16) plt.show()"},{"question":"**Problem Description:** You are tasked with implementing a simulation of a simplified task scheduler using the `queue` module in Python. The scheduler will handle tasks that need to be processed by different worker threads. Each task has an associated priority, determining the order in which tasks are processed. Additionally, tasks can be dynamically added to the queue by worker threads. **Requirements:** 1. **Scheduler Initialization:** - Implement a class `TaskScheduler` with a constructor that initializes a `PriorityQueue` to manage tasks. - The constructor should accept an optional parameter `maxsize` to limit the number of tasks in the queue. If `maxsize` is not provided, the queue should be unbounded. 2. **Method to Add Task:** - Implement a method `add_task(priority: int, task: str)` to add a new task to the queue with the given priority. - If the queue is full, the method should block until a free slot is available. 3. **Worker Thread Functionality:** - Implement a method `worker_thread()` to be executed by worker threads. - The `worker_thread` method should continuously fetch and process tasks from the queue in priority order. - For each task, it should print the task string in the format `\\"Processing task: {task}\\"`. - Once the task is processed, it should call `task_done()`. 4. **Starting Worker Threads:** - Implement a method `start_workers(num_workers: int)` that starts a specified number of worker threads, each running the `worker_thread` method. 5. **Waiting for Completion:** - Implement a method `wait_for_completion()` that blocks until all tasks in the queue have been processed. **Input and Output Formats:** - The input to initialize the `TaskScheduler` class can optionally be an integer argument `maxsize`. - Input to `add_task` should be two parameters: an integer `priority` and a string `task`. - Output should be the printed task strings in the format `\\"Processing task: {task}\\"` as explained above. **Constraints:** - Ensure your implementation handles concurrency correctly. - You may assume that the priority values are integers, and smaller numbers indicate higher priority. **Example Usage:** ```python if __name__ == \\"__main__\\": import threading import time # Create a TaskScheduler with a max size of 10 scheduler = TaskScheduler(maxsize=10) # Adding tasks with various priorities scheduler.add_task(2, \\"Task 1\\") scheduler.add_task(1, \\"Task 2\\") scheduler.add_task(3, \\"Task 3\\") # Start 2 worker threads scheduler.start_workers(num_workers=2) # Wait for all tasks to be processed scheduler.wait_for_completion() print(\\"All tasks completed.\\") ``` **Expected Output:** ``` Processing task: Task 2 Processing task: Task 1 Processing task: Task 3 All tasks completed. ``` **Note:** The order of processing should depend on the priorities given.","solution":"import threading from queue import PriorityQueue class TaskScheduler: def __init__(self, maxsize=0): self.queue = PriorityQueue(maxsize=maxsize) def add_task(self, priority, task): self.queue.put((priority, task)) def worker_thread(self): while True: priority, task = self.queue.get() if task is None: break print(f\\"Processing task: {task}\\") self.queue.task_done() def start_workers(self, num_workers): self.threads = [] for _ in range(num_workers): t = threading.Thread(target=self.worker_thread) t.start() self.threads.append(t) def wait_for_completion(self): self.queue.join() for _ in self.threads: self.add_task(None, None) for t in self.threads: t.join()"},{"question":"Objective: Demonstrate your understanding of `MaskedTensor` in PyTorch by creating masked tensors, applying various unary and binary operations on them, and performing reduction operations while respecting the mask. Problem Statement You are given a tensor of data and a corresponding mask. Your task is to: 1. Create a `MaskedTensor` using the provided data and mask. 2. Apply a series of unary operations (`log`, `exp`) and binary operations (`add`, `multiply`) on the `MaskedTensor`. 3. Perform reduction operations (`sum`, `mean`) on the `MaskedTensor`. 4. Ensure that masked values are correctly handled and not included in the computation. 5. Write a function `masked_tensor_operations` that takes the data tensor and mask tensor as input and returns the results of these operations. Function Signature ```python def masked_tensor_operations(data: torch.Tensor, mask: torch.Tensor) -> dict: Perform specified operations on the given data and mask tensors and return the results in a dictionary. Parameters: data (torch.Tensor): The input data tensor. mask (torch.Tensor): The mask tensor where True indicates elements to be included and False indicates elements to be ignored. Returns: dict: A dictionary containing the results of the operations with the following keys: - \'log\': Result of applying the log operation on the MaskedTensor. - \'exp\': Result of applying the exp operation on the MaskedTensor. - \'add\': Result of adding the MaskedTensor with itself. - \'multiply\': Result of multiplying the MaskedTensor with itself. - \'sum\': Sum of the elements in the MaskedTensor. - \'mean\': Mean of the elements in the MaskedTensor. pass ``` Inputs - `data`: A `torch.Tensor` of any shape filled with numerical values. - `mask`: A `torch.Tensor` of the same shape as `data` with boolean values (True/False), indicating which elements to consider. Outputs - A dictionary with the following keys and their respective values: - `\'log\'`: A `MaskedTensor` after applying the logarithm operation. - `\'exp\'`: A `MaskedTensor` after applying the exponential operation. - `\'add\'`: A `MaskedTensor` that is the result of adding the `MaskedTensor` with itself. - `\'multiply\'`: A `MaskedTensor` that is the result of multiplying the `MaskedTensor` with itself. - `\'sum\'`: The sum of the elements in the `MaskedTensor` (excluding the masked elements). - `\'mean\'`: The mean of the elements in the `MaskedTensor` (excluding the masked elements). Example ```python import torch from torch.masked import masked_tensor data = torch.tensor([[1.0, 2.0, 3.0], [4.0, 0.0, 6.0], [7.0, 8.0, 9.0]]) mask = torch.tensor([[True, False, True], [True, True, False], [False, True, True]]) result = masked_tensor_operations(data, mask) print(result[\'log\']) print(result[\'exp\']) print(result[\'add\']) print(result[\'multiply\']) print(result[\'sum\']) print(result[\'mean\']) ``` Expected output should respect the mask during operations. Notes - You may assume that the data tensor does not contain negative values or values that would cause domain errors for the `log` operation. - Pay careful attention to handling masked values correctly in each operation.","solution":"import torch def masked_tensor_operations(data: torch.Tensor, mask: torch.Tensor) -> dict: Perform specified operations on the given data and mask tensors and return the results in a dictionary. Parameters: data (torch.Tensor): The input data tensor. mask (torch.Tensor): The mask tensor where True indicates elements to be included and False indicates elements to be ignored. Returns: dict: A dictionary containing the results of the operations with the following keys: - \'log\': Result of applying the log operation on the MaskedTensor. - \'exp\': Result of applying the exp operation on the MaskedTensor. - \'add\': Result of adding the MaskedTensor with itself. - \'multiply\': Result of multiplying the MaskedTensor with itself. - \'sum\': Sum of the elements in the MaskedTensor. - \'mean\': Mean of the elements in the MaskedTensor. # Create masked tensors by applying the mask masked_data = data.masked_fill(~mask, float(\'nan\')) log_result = torch.log(masked_data) exp_result = torch.exp(masked_data) add_result = masked_data + masked_data multiply_result = masked_data * masked_data valid_elements = masked_data[mask] sum_result = torch.sum(valid_elements) mean_result = torch.mean(valid_elements) # Output dictionary results = { \'log\': log_result, \'exp\': exp_result, \'add\': add_result, \'multiply\': multiply_result, \'sum\': sum_result, \'mean\': mean_result } return results"},{"question":"<|Analysis Begin|> The documentation provided focuses on the `sklearn.calibration` module of the scikit-learn library, which is used for probability calibration in classification tasks. It describes how some models might give poor estimates of class probabilities or may not support probability prediction at all. Calibration can address these issues. The documentation explains two main concepts: 1. **Calibration Curves**: Used to compare how well the probabilistic predictions of a binary classifier are calibrated. It details functions such as `CalibrationDisplay.from_estimator` and `calibration_curve`. 2. **CalibratedClassifierCV**: A class used for calibrating a classifier using cross-validation to achieve unbiased estimates. It also discusses different calibration methods, namely \\"sigmoid\\" and \\"isotonic\\". Key points from the documentation: - The importance of well-calibrated classifiers and their practical interpretation. - Use of calibration curves to measure calibration quality. - The `CalibratedClassifierCV` class and its usage with cross-validation. - Different calibration methods and their suitability under various conditions. Given the depth and focus of the content, an effective question should require students to: - Implement and interpret a calibration mechanism using the provided tools. - Compare the performance of different classifiers in terms of calibration quality. - Apply the `CalibratedClassifierCV` class with different calibration methods. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Calibration of Classifier Probabilities** You are provided with a dataset and are required to assess and improve the calibration of different classifiers. You need to perform the following tasks: 1. **Data Preparation**: - Load the dataset and preprocess it to a form suitable for classification. 2. **Model Building and Evaluation**: - Train a Logistic Regression model, Gaussian Naive Bayes model, and Random Forest model on the dataset. - Evaluate their performance using calibration curves. 3. **Calibration**: - Use `CalibratedClassifierCV` to calibrate each of these models using both \\"sigmoid\\" and \\"isotonic\\" methods. - Evaluate and compare the calibrated models using calibration curves and appropriate scoring metrics. 4. **Interpretation**: - Compare the calibration performance before and after applying `CalibratedClassifierCV`. - Provide insights on the effectiveness of each calibration method for each model. # Instructions - Implement the function `evaluate_calibration` which takes as input: - `X_train`: Training feature matrix. - `X_test`: Testing feature matrix. - `y_train`: Training labels. - `y_test`: Testing labels. - The function should return: - Dictionary containing calibration evaluation results for each model and method combination. - Use Brier score loss for evaluation. # Input and Output Formats 1. **Input**: - `X_train`: Training feature matrix (numpy array of shape `(n_samples, n_features)`). - `X_test`: Testing feature matrix (numpy array of shape `(n_samples, n_features)`). - `y_train`: Training labels (numpy array of shape `(n_samples,)`). - `y_test`: Testing labels (numpy array of shape `(n_samples,)`). 2. **Output**: - Dictionary with model names as keys and another dictionary as values containing the results for each calibration method. - Results should include: - `before_calibration_brier_score`: Brier score loss before calibration. - `after_calibration_sigmoid_brier_score`: Brier score loss after calibration using \\"sigmoid\\". - `after_calibration_isotonic_brier_score`: Brier score loss after calibration using \\"isotonic\\". Example: ```python { \\"LogisticRegression\\": { \\"before_calibration_brier_score\\": 0.13, \\"after_calibration_sigmoid_brier_score\\": 0.12, \\"after_calibration_isotonic_brier_score\\": 0.11 }, \\"GaussianNB\\": { ... }, \\"RandomForestClassifier\\": { ... } } ``` Use the following template as a starting point for your implementation: ```python from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.calibration import CalibratedClassifierCV from sklearn.metrics import brier_score_loss from sklearn.calibration import calibration_curve import matplotlib.pyplot as plt def evaluate_calibration(X_train, X_test, y_train, y_test): results = {} # Models models = { \\"LogisticRegression\\": LogisticRegression(), \\"GaussianNB\\": GaussianNB(), \\"RandomForestClassifier\\": RandomForestClassifier() } for model_name, model in models.items(): model.fit(X_train, y_train) # Predictions before calibration prob_pos = model.predict_proba(X_test)[:, 1] before_brier = brier_score_loss(y_test, prob_pos) # Calibration calibrator_sigmoid = CalibratedClassifierCV(model, method=\'sigmoid\', cv=5) calibrator_sigmoid.fit(X_train, y_train) prob_pos_sigmoid = calibrator_sigmoid.predict_proba(X_test)[:, 1] after_brier_sigmoid = brier_score_loss(y_test, prob_pos_sigmoid) calibrator_isotonic = CalibratedClassifierCV(model, method=\'isotonic\', cv=5) calibrator_isotonic.fit(X_train, y_train) prob_pos_isotonic = calibrator_isotonic.predict_proba(X_test)[:, 1] after_brier_isotonic = brier_score_loss(y_test, prob_pos_isotonic) # Store results results[model_name] = { \\"before_calibration_brier_score\\": before_brier, \\"after_calibration_sigmoid_brier_score\\": after_brier_sigmoid, \\"after_calibration_isotonic_brier_score\\": after_brier_isotonic } return results ``` # Additional Guidance - Use `calibration_curve`, `CalibratedClassifierCV`, and other tools mentioned in the provided scikit-learn documentation. - Plot the calibration curves using `CalibrationDisplay.from_estimator` to visualize the impact of calibration. Let me know if you need any more clarity or have additional questions!","solution":"from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.calibration import CalibratedClassifierCV from sklearn.metrics import brier_score_loss from sklearn.calibration import calibration_curve import matplotlib.pyplot as plt def evaluate_calibration(X_train, X_test, y_train, y_test): results = {} # Models models = { \\"LogisticRegression\\": LogisticRegression(), \\"GaussianNB\\": GaussianNB(), \\"RandomForestClassifier\\": RandomForestClassifier() } for model_name, model in models.items(): model.fit(X_train, y_train) # Predictions before calibration prob_pos = model.predict_proba(X_test)[:, 1] before_brier = brier_score_loss(y_test, prob_pos) # Calibration calibrator_sigmoid = CalibratedClassifierCV(model, method=\'sigmoid\', cv=5) calibrator_sigmoid.fit(X_train, y_train) prob_pos_sigmoid = calibrator_sigmoid.predict_proba(X_test)[:, 1] after_brier_sigmoid = brier_score_loss(y_test, prob_pos_sigmoid) calibrator_isotonic = CalibratedClassifierCV(model, method=\'isotonic\', cv=5) calibrator_isotonic.fit(X_train, y_train) prob_pos_isotonic = calibrator_isotonic.predict_proba(X_test)[:, 1] after_brier_isotonic = brier_score_loss(y_test, prob_pos_isotonic) # Store results results[model_name] = { \\"before_calibration_brier_score\\": before_brier, \\"after_calibration_sigmoid_brier_score\\": after_brier_sigmoid, \\"after_calibration_isotonic_brier_score\\": after_brier_isotonic } return results"},{"question":"# Asynchronous Programming in PyTorch using torch.futures In this coding assessment, you will demonstrate your understanding of asynchronous programming with PyTorch\'s `torch.futures` package. You will need to implement a function that performs a series of asynchronous operations and subsequently gather their results. Task 1. Create and run several asynchronous computations using `torch.futures.Future`. 2. Collect these `Future` objects and ensure you block until all are completed using the provided utilities. 3. Aggregate the results from these `Future` objects and return the aggregated result. Specifications 1. **Function Name:** `perform_async_operations` 2. **Input:** None 3. **Output:** Aggregated result (sum of results from all asynchronous operations) 4. **Constraints:** - Perform 10 asynchronous operations. - Each operation should return its index (0 through 9) after a simulated delay. ```python import torch import time import random def async_operation(index): Simulate an asynchronous operation that takes a random time to complete. time.sleep(random.random()) return index def perform_async_operations(): Perform 10 asynchronous operations using torch.futures.Future and aggregate their results. :return: Sum of the results from all asynchronous operations. futures = [] # Create 10 Future objects for asynchronous operations for i in range(10): future = torch.futures.Future() future.set_result(async_operation(i)) futures.append(future) # Collect all futures and block until all are completed completed_futures = torch.futures.wait_all(futures) # Aggregate results aggregated_result = sum(f.result() for f in completed_futures) return aggregated_result # Run the function and print the result if __name__ == \\"__main__\\": result = perform_async_operations() print(f\\"Aggregated result of async operations: {result}\\") ``` **Notes:** - You may use the provided `async_operation` function for simulating delays. - Ensure to properly handle the future objects using the utilities from `torch.futures`. - The aggregated result should be the sum of integers from 0 to 9, as each operation returns its own index. # Evaluation Criteria Your function will be evaluated based on the following criteria: 1. Correct usage of `torch.futures.Future` and the provided utility functions. 2. Proper handling of asynchronous operations and their results. 3. Correctness of the aggregated result. 4. Code style and readability.","solution":"import time import random import torch from concurrent.futures import ThreadPoolExecutor def async_operation(index): Simulate an asynchronous operation that takes a random time to complete. time.sleep(random.random()) return index def perform_async_operations(): Perform 10 asynchronous operations using torch.futures.Future and aggregate their results. :return: Sum of the results from all asynchronous operations. futures = [] executor = ThreadPoolExecutor(max_workers=10) # Create 10 Future objects for asynchronous operations for i in range(10): future = executor.submit(async_operation, i) futures.append(future) # Collect all futures and block until all are completed completed_futures = [future.result() for future in futures] # Aggregate results aggregated_result = sum(completed_futures) return aggregated_result # Run the function and print the result if __name__ == \\"__main__\\": result = perform_async_operations() print(f\\"Aggregated result of async operations: {result}\\")"},{"question":"**Background:** You are tasked with designing a simple file downloader using the `asyncio` library in Python. This should involve fetching multiple files concurrently from a list of URLs and storing them on the disk. You need to ensure that your program handles the concurrency properly and downloads all files efficiently. **Problem Statement:** Implement a function `async def download_files(url_list: List[str], save_dir: str) -> List[str]:` that takes in a list of URLs (`url_list`) and a directory path (`save_dir`) where the files should be saved. The function should download all the files concurrently and return a list of file paths for the downloaded files. **Requirements:** 1. For each URL in the `url_list`, download the file and save it in the directory specified by `save_dir`. The files should be saved with their original filenames. 2. Use the `asyncio` library to run the download tasks concurrently. 3. If any download fails, handle the error gracefully and continue downloading the remaining files. 4. The function should return a list of paths for successfully downloaded files. **Input:** - `url_list`: List of URLs to be downloaded. Example: `[\\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\"]` - `save_dir`: String representing the directory path where the files should be saved. **Output:** - List of strings representing the paths to successfully downloaded files. Example: `[\\"/path/to/save_dir/file1.txt\\", \\"/path/to/save_dir/file2.txt\\"]` **Constraints:** - Assume `url_list` will contain between 1 and 100 URLs. - Assume `save_dir` exists and is writable. - You may use `aiohttp` for asynchronous HTTP requests. Install it via `pip install aiohttp`. **Performance:** - The solution should efficiently handle concurrent downloads and avoid blocking operations. **Example Usage:** ```python import asyncio from typing import List async def download_files(url_list: List[str], save_dir: str) -> List[str]: # Implementation goes here pass # Example URLs url_list = [\\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\"] save_dir = \\"/path/to/save_dir\\" # Run the async function downloaded_files = asyncio.run(download_files(url_list, save_dir)) print(downloaded_files) ``` Your task is to provide the implementation for `download_files` that adheres to the above requirements.","solution":"import asyncio import aiohttp import os from aiohttp import ClientSession from typing import List async def download_file(session: ClientSession, url: str, save_dir: str) -> str: try: async with session.get(url) as response: # Extract file name from URL filename = os.path.basename(url) file_path = os.path.join(save_dir, filename) # Read and save file with open(file_path, \'wb\') as f: while True: chunk = await response.content.read(1024) if not chunk: break f.write(chunk) return file_path except Exception as e: print(f\\"Failed to download {url}: {e}\\") return None async def download_files(url_list: List[str], save_dir: str) -> List[str]: async with aiohttp.ClientSession() as session: tasks = [download_file(session, url, save_dir) for url in url_list] file_paths = await asyncio.gather(*tasks) # Filter out None entries from failed downloads return [file_path for file_path in file_paths if file_path is not None]"},{"question":"Context A software company wants to validate user input on their registration forms to ensure data consistency and security. Your task is to use regular expressions to ensure these inputs meet required patterns. Task Write a function `validate_registration(form)` that takes a dictionary `form` with the following keys and their associated values: - `\\"email\\"`: A string representing the user\'s email address. - `\\"password\\"`: A string representing the user\'s password. - `\\"username\\"`: A string representing the user\'s username. Your function should validate the inputs using regular expressions and return a dictionary with the validation results for each field. Validation Rules: 1. **Email**: - The email must follow the standard pattern: `[local-part]@[domain]`. - The local-part can contain alphanumeric characters, dots, hyphens, and underscores. - The domain must contain at least one dot after the \\"@\\" symbol, and each part of the domain should be composed of alphanumeric characters and hyphens. - The domain\'s last part should be between 2 to 6 characters. 2. **Password**: - The password must be at least 8 characters long. - It must contain at least one lowercase letter, one uppercase letter, one digit, and one special character (`!@#%^&*()`). 3. **Username**: - The username must be between 3 and 16 characters long. - It can contain alphanumeric characters, underscores, and hyphens. - It should not start or end with a hyphen or underscore, and it should not contain consecutive hyphens or underscores. Input: - `form` (dict): A dictionary containing keys `\\"email\\"`, `\\"password\\"`, and `\\"username\\"` with corresponding string values to validate. Output: - A dictionary with keys `\\"email\\"`, `\\"password\\"`, and `\\"username\\"`, where the value is `True` if the validation is successful or an error message string if the validation fails. Example: ```python form = { \\"email\\": \\"user@example.com\\", \\"password\\": \\"Password123!\\", \\"username\\": \\"valid_user-name\\" } result = validate_registration(form) print(result) # Output: {\\"email\\": True, \\"password\\": True, \\"username\\": True} ``` Constraints: - You must use the `re` module for validation. - Do not use any additional libraries or helper functions.","solution":"import re def validate_registration(form): Validates email, password, and username from the registration form. Parameters: form (dict): A dictionary with keys \\"email\\", \\"password\\", and \\"username\\" with corresponding string values to validate. Returns: dict: A dictionary with keys \\"email\\", \\"password\\", and \\"username\\", where the value is True if the validation is successful or an error message string if the validation fails. validation_results = {} # Validate email email_pattern = re.compile(r\'^[w.-]+@[a-zA-Zd.-]+.[a-zA-Z]{2,6}\') if email_pattern.match(form.get(\\"email\\", \\"\\")): validation_results[\\"email\\"] = True else: validation_results[\\"email\\"] = \\"Invalid email format.\\" # Validate password password_pattern = re.compile(r\'^(?=.*[a-z])(?=.*[A-Z])(?=.*d)(?=.*[!@#%^&*()]).{8,}\') if password_pattern.match(form.get(\\"password\\", \\"\\")): validation_results[\\"password\\"] = True else: validation_results[\\"password\\"] = \\"Password must be at least 8 characters long and contain at least one lowercase letter, one uppercase letter, one digit, and one special character.\\" # Validate username username_pattern = re.compile(r\'^(?!.*[_.]{2})[a-zA-Zd_-]{3,16}\') if username_pattern.match(form.get(\\"username\\", \\"\\")) and not form[\\"username\\"].startswith((\\"-\\", \\"_\\")) and not form[\\"username\\"].endswith((\\"-\\", \\"_\\")): validation_results[\\"username\\"] = True else: validation_results[\\"username\\"] = \\"Username must be 3-16 characters long, only contain alphanumeric characters, underscores, and hyphens, and not start or end with them.\\" return validation_results"},{"question":"# Complex CGI Script Implementation in Python **Objective**: Implement a CGI script using the `cgi` module to process form data and file uploads, with appropriate error handling and debugging. **Problem Statement**: You are required to create a CGI script that: 1. Processes an HTML form containing the following fields: - `username`: A text input for the username. - `email`: A text input for the user\'s email. - `profile_picture`: A file input for uploading a profile picture. - `interests`: Multiple checkbox inputs with the same name \\"interests\\" (values could be like \'coding\', \'reading\', \'gaming\', etc.). 2. The script should: - Validate that both `username` and `email` are present. If either field is missing, return an error message in HTML. - Ensure that the `email` field contains a valid email format. If invalid, return an error message in HTML. - Process the `profile_picture` file upload, and count the lines in the uploaded file (if any). - Collect all selected `interests` and display them. - Use a higher-level interface with `getfirst()` and `getlist()` for processing form data. - Include error handling using `cgitb` for detailed debugging. 3. The script should output the results in a neat HTML format displaying: - The entered `username` and `email`. - The total lines in the uploaded `profile_picture` file. - The list of selected `interests`. **Input Format**: Your script should read the form data submitted via an HTTP POST request. **Output Format**: The output should be in HTML format, adhering to the following structure: ```html <!DOCTYPE html> <html> <head> <title>CGI Script Output</title> </head> <body> <h1>Form Submission Results</h1> <p>Username: {username}</p> <p>Email: {email}</p> <p>Profile Picture Line Count: {line_count}</p> <p>Interests: {interests}</p> </body> </html> ``` **Constraints**: - Ensure that the script handles any exceptions gracefully and outputs an HTML error message if an issue arises. - The uploaded file should be read line by line to count the lines. **Example Usage**: Consider the following HTML form: ```html <form method=\\"post\\" action=\\"/cgi-bin/your_script.py\\" enctype=\\"multipart/form-data\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Email: <input type=\\"text\\" name=\\"email\\"><br> Profile Picture: <input type=\\"file\\" name=\\"profile_picture\\"><br> Interests:<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"coding\\"> Coding<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"reading\\"> Reading<br> <input type=\\"checkbox\\" name=\\"interests\\" value=\\"gaming\\"> Gaming<br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` **Implementation Notes**: 1. Initialize and enable `cgitb` at the beginning of the script for debugging. 2. Use `cgi.FieldStorage` to gather form data. 3. Validate and process each form field accordingly. 4. Ensure proper HTML output with results or error messages. **Submission**: Submit the complete CGI script as a Python (.py) file.","solution":"#!/usr/bin/env python3 import cgi import cgitb import re # Enable debugging cgitb.enable() def validate_email(email): Validate the email format. pattern = r\\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\\" return re.match(pattern, email) def count_file_lines(file_item): Count the number of lines in the uploaded file. if file_item.file: return sum(1 for line in file_item.file) return 0 # Gather form data form = cgi.FieldStorage() # Get form fields username = form.getfirst(\\"username\\", \\"\\").strip() email = form.getfirst(\\"email\\", \\"\\").strip() profile_picture = form[\\"profile_picture\\"] if \\"profile_picture\\" in form else None interests = form.getlist(\\"interests\\") # Start HTML output print(\\"Content-type: text/html\\") print() print(\\"<!DOCTYPE html>\\") print(\\"<html>\\") print(\\"<head>\\") print(\\" <title>CGI Script Output</title>\\") print(\\"</head>\\") print(\\"<body>\\") print(\\" <h1>Form Submission Results</h1>\\") # Validate required fields if not username: print(\\" <p style=\'color: red;\'>Error: Username is required.</p>\\") elif not email: print(\\" <p style=\'color: red;\'>Error: Email is required.</p>\\") elif not validate_email(email): print(\\" <p style=\'color: red;\'>Error: Invalid email format.</p>\\") else: line_count = count_file_lines(profile_picture) if profile_picture else 0 interests_display = \\", \\".join(interests) if interests else \\"None\\" # Display results print(f\\" <p>Username: {username}</p>\\") print(f\\" <p>Email: {email}</p>\\") print(f\\" <p>Profile Picture Line Count: {line_count}</p>\\") print(f\\" <p>Interests: {interests_display}</p>\\") print(\\"</body>\\") print(\\"</html>\\")"},{"question":"**Question: Implement and Test a User Management System** **Problem Statement:** You are tasked with implementing a simple user management system in Python and writing corresponding unit tests for it using the `unittest` framework. The system should support the following functionalities: - Adding a user with a unique username and email. - Retrieving user details by username. - Deleting a user by username. - Updating user email by username. Your task is to implement the `UserManager` class with the above functionalities and write comprehensive unit tests that verify the correctness of each method in the class. **Details and Constraints:** 1. Implement a `UserManager` class with the following methods: - `add_user(username: str, email: str) -> bool`: Adds a new user to the system. Returns `True` if the user is added successfully, `False` if a user with the same username already exists. - `get_user(username: str) -> dict`: Retrieves the details of the user with the given username as a dictionary `{ \'username\': <username>, \'email\': <email> }`. If the user does not exist, returns `None`. - `delete_user(username: str) -> bool`: Deletes the user with the given username. Returns `True` if the user is deleted successfully, `False` if the user does not exist. - `update_email(username: str, new_email: str) -> bool`: Updates the email address of the user with the given username. Returns `True` if the email is updated successfully, `False` if the user does not exist. 2. Implement unit tests for each method using the `unittest` framework in a separate test class named `TestUserManager`. - Test `add_user` method for adding users correctly and handling duplicates. - Test `get_user` method for retrieving user details and handling non-existent users. - Test `delete_user` method for deleting users correctly and handling non-existent users. - Test `update_email` method for updating the user\'s email and handling non-existent users. 3. You should not interact with any external database; use in-memory storage (e.g., a dictionary) to manage users within the `UserManager` class. **Performance Requirements:** - The functions should maintain an average time complexity of O(1) for all operations due to the use of a dictionary for user storage. **Example Usage:** ```python user_manager = UserManager() # Adding Users assert user_manager.add_user(\'alice\', \'alice@example.com\') == True assert user_manager.add_user(\'alice\', \'alice_duplicate@example.com\') == False # Duplicate user # Retrieving Users assert user_manager.get_user(\'alice\') == {\'username\': \'alice\', \'email\': \'alice@example.com\'} assert user_manager.get_user(\'bob\') == None # Non-existent user # Deleting Users assert user_manager.delete_user(\'alice\') == True assert user_manager.delete_user(\'alice\') == False # User already deleted # Updating Email assert user_manager.add_user(\'carol\', \'carol@example.com\') == True assert user_manager.update_email(\'carol\', \'carol_new@example.com\') == True assert user_manager.get_user(\'carol\') == {\'username\': \'carol\', \'email\': \'carol_new@example.com\'} assert user_manager.update_email(\'dan\', \'dan@example.com\') == False # Non-existent user ``` **Submission:** - Provide the implementation of the `UserManager` class. - Provide the `TestUserManager` class with all the necessary test cases.","solution":"class UserManager: A simple user management system. Users are stored in an in-memory dictionary. def __init__(self): self.users = {} def add_user(self, username: str, email: str) -> bool: if username in self.users: return False self.users[username] = {\'username\': username, \'email\': email} return True def get_user(self, username: str) -> dict: return self.users.get(username) def delete_user(self, username: str) -> bool: if username in self.users: del self.users[username] return True return False def update_email(self, username: str, new_email: str) -> bool: if username in self.users: self.users[username][\'email\'] = new_email return True return False"},{"question":"# PyCapsule Handling in Python You are tasked with writing several Python functions that interface with a C extension module using PyCapsule. Your goal is to ensure proper creation, modification, and validation of PyCapsules. Functions to Implement: 1. **create_capsule**: - **Input**: A C pointer (simulated as an integer) and a name (string). - **Output**: A PyCapsule object. - **Description**: Create a PyCapsule with the given pointer and name. Assume that the pointer is always a non-zero integer. 2. **set_capsule_name**: - **Input**: A PyCapsule object and a new name (string). - **Output**: None. - **Description**: Set a new name for the given PyCapsule. Handle any exceptions if the operation fails. 3. **get_capsule_pointer**: - **Input**: A PyCapsule object. - **Output**: The pointer stored in the PyCapsule. - **Description**: Retrieve and return the pointer from the given PyCapsule. Handle any exceptions if the operation fails. 4. **validate_capsule**: - **Input**: A PyCapsule object and a name (string). - **Output**: A boolean value (True if valid, False otherwise). - **Description**: Check if the PyCapsule is valid and its name matches the provided name. 5. **modify_capsule_pointer**: - **Input**: A PyCapsule object and a new pointer (simulated as an integer). - **Output**: None. - **Description**: Set a new pointer for the given PyCapsule. Handle any exceptions if the operation fails. Constraints: - You can assume that the C pointers are integer values for simulation purposes in Python. - Names are always non-empty strings. - If an operation fails, an appropriate exception message should be displayed. Example: ```python # Example usage of the functions capsule = create_capsule(12345, \\"module.attribute\\") print(get_capsule_pointer(capsule)) # Output should be 12345 set_capsule_name(capsule, \\"module.new_attribute\\") print(validate_capsule(capsule, \\"module.new_attribute\\")) # Output should be True modify_capsule_pointer(capsule, 67890) print(get_capsule_pointer(capsule)) # Output should be 67890 ``` Ensure your implementation handles potential exceptions and validations correctly.","solution":"import ctypes # Simulating C pointers in Python class PyCapsule: def __init__(self, pointer, name): self.pointer = pointer self.name = name def create_capsule(pointer, name): Create a PyCapsule with the given pointer and name. Assume pointer is a non-zero integer. if not isinstance(pointer, int) or pointer == 0: raise ValueError(\\"Pointer must be a non-zero integer\\") if not isinstance(name, str) or not name: raise ValueError(\\"Name must be a non-empty string\\") return PyCapsule(pointer, name) def set_capsule_name(capsule, new_name): Set a new name for the given PyCapsule. if not isinstance(capsule, PyCapsule): raise ValueError(\\"Invalid PyCapsule object\\") if not isinstance(new_name, str) or not new_name: raise ValueError(\\"Name must be a non-empty string\\") capsule.name = new_name def get_capsule_pointer(capsule): Retrieve and return the pointer from the given PyCapsule. if not isinstance(capsule, PyCapsule): raise ValueError(\\"Invalid PyCapsule object\\") return capsule.pointer def validate_capsule(capsule, name): Check if the PyCapsule is valid and its name matches the provided name. if not isinstance(capsule, PyCapsule): return False if not isinstance(name, str) or not name: raise ValueError(\\"Name must be a non-empty string\\") return capsule.name == name def modify_capsule_pointer(capsule, new_pointer): Set a new pointer for the given PyCapsule. if not isinstance(capsule, PyCapsule): raise ValueError(\\"Invalid PyCapsule object\\") if not isinstance(new_pointer, int) or new_pointer == 0: raise ValueError(\\"Pointer must be a non-zero integer\\") capsule.pointer = new_pointer"},{"question":"**Objective**: Implement a Python program that reads a list of text files, compresses their contents using the LZMA algorithm with custom filters, and then verifies the integrity of the compressed data by decompressing it and comparing it with the original content. **Problem Statement**: You are provided with a list of file paths containing text data. Your task is to: 1. Compress the contents of these files using the `lzma` module. 2. Implement a custom filter chain to be applied during compression. 3. Verify the integrity of the compressed content by decompressing it and comparing it with the original content. 4. Handle any potential errors gracefully and print appropriate error messages. **Requirements**: 1. Use the `lzma` module to perform compression and decompression. 2. Apply a custom filter chain during compression: - First filter: `FILTER_DELTA` with a distance of 1. - Second filter: `FILTER_LZMA` with a preset of `9`. 3. Ensure that the decompressed data matches the original content to verify integrity. 4. Handle potential errors like file not found, read/write errors, and compression/decompression errors. **Function Signature**: ```python def compress_and_verify_files(file_paths: list): pass ``` **Input**: - `file_paths` (List[str]): A list of file paths to be processed. **Output**: - No explicit output, but print appropriate messages indicating the success or failure of the operations. **Constraints**: - Assume file contents fit into memory. - Handle both text and binary file modes as needed. **Example Usage**: ```python file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] compress_and_verify_files(file_paths) ``` **Example Output**: ``` Compressed file1.txt successfully and verified integrity. Compressed file2.txt successfully and verified integrity. Compressed file3.txt successfully and verified integrity. ``` If there is an error: ``` Error: File file_not_found.txt not found. ``` **Notes**: - Use appropriate methods from the `lzma` module to achieve the tasks. - Consider edge cases such as empty files and very large files (though you can assume they fit in memory for this exercise). - Ensure your code is readable and well-documented.","solution":"import lzma import os def compress_and_verify_files(file_paths): for file_path in file_paths: try: # Read the original content of the file with open(file_path, \'rb\') as file: original_content = file.read() # Define LZMA custom filter chain filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] # Compress the content compressed_content = lzma.compress(original_content, format=lzma.FORMAT_XZ, filters=filters) # Decompress the content decompressed_content = lzma.decompress(compressed_content) # Verify integrity if decompressed_content == original_content: print(f\\"Compressed {file_path} successfully and verified integrity.\\") else: print(f\\"Verification failed for {file_path}.\\") except FileNotFoundError: print(f\\"Error: File {file_path} not found.\\") except lzma.LZMAError as e: print(f\\"Error in compression/decompression for {file_path}: {e}\\") except Exception as e: print(f\\"An unexpected error occurred while processing {file_path}: {e}\\")"},{"question":"**Objective:** You are required to create a custom logger that wraps Python\'s built-in `print` function to ensure all printed messages are logged to a file with a timestamp and converted to lowercase. **Problem Statement:** Create a custom logger class `CustomLogger` that wraps the built-in `print` function. The custom logger should log all messages to a specified log file with a timestamp and convert the messages to lowercase before printing them. **Detailed Requirements:** 1. **Class Definition:** - Define a class `CustomLogger`. 2. **Initialization:** - The `__init__` method should accept a parameter `log_file_path` which specifies the path to the log file. - Initialize the log file by opening it in append mode. 3. **Method: `log`** - Define a method `log`, which accepts any number of arguments similar to the built-in `print`. - Convert the message to lowercase. - Add a timestamp to the message. - Print the message to the console. - Write the message with the timestamp to the specified log file. **Class Signature:** ```python import builtins from datetime import datetime class CustomLogger: def __init__(self, log_file_path: str): # Your initialization code here def log(self, *args): # Your code to log the message here ``` **Sample Usage:** ```python # Initialize the logger logger = CustomLogger(\'log.txt\') # Log some messages logger.log(\\"Hello World!\\") logger.log(\\"Another Log Message.\\") ``` **Expected Result:** - Messages printed to the console should be in lowercase and prefixed with a timestamp. - The log file should also contain these messages with the same format. **Constraints:** - You cannot use any external libraries for logging and printing other than the standard Python libraries mentioned above. **Performance Requirements:** - The implementation should be efficient in terms of file operations, ensuring the file is opened only once during initialization and properly closed when the object is deleted or program ends. **Example:** If `log(\\"Hello World!\\")` is called at `2023-10-07 15:45:23`, the message printed and logged should be: ```plaintext 2023-10-07 15:45:23 hello world! ```","solution":"import builtins from datetime import datetime class CustomLogger: def __init__(self, log_file_path: str): self.log_file_path = log_file_path self._log_file = open(self.log_file_path, \'a\') def __del__(self): if self._log_file: self._log_file.close() def log(self, *args): # Combine all arguments into a single string with spaces message = \' \'.join(map(str, args)).lower() # Add timestamp timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_message = f\\"{timestamp} {message}\\" # Print to the console using the built-in print function builtins.print(log_message) # Write to the log file self._log_file.write(log_message + \'n\') self._log_file.flush() # Ensure the message is written immediately"},{"question":"# Question: Implement a custom logging system using Python\'s `logging` module. Your implementation should follow these requirements: 1. Configure a logger named `app` with the following characteristics: - Logs messages to both a file named `app.log` and the console. - Uses a `Formatter` to include the timestamp, logger name, log level, and message in the log output. The format should be `\\"[<timestamp>] <logger> - <level> - <message>\\"`. - Only logs messages with a level of `INFO` or higher. 2. Create a function `configure_logger()` that sets up the logger as described above. 3. Demonstrate the usage of your logging system: - Log messages of different levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) and show which ones appear in the output. # Function Signature: ```python def configure_logger() -> None: pass ``` # Example Usage: ```python if __name__ == \\"__main__\\": configure_logger() logger = logging.getLogger(\'app\') logger.debug(\\"This is a DEBUG message\\") # Should not appear in the output logger.info(\\"This is an INFO message\\") # Should appear in the output logger.warning(\\"This is a WARNING message\\") # Should appear in the output logger.error(\\"This is an ERROR message\\") # Should appear in the output logger.critical(\\"This is a CRITICAL message\\") # Should appear in the output ``` # Constraints: - Ensure the `app.log` file is created in the same directory as the script. - The console output should include color coding for different log levels if run on a compatible console (optional). **Note**: You may assume that Python\'s `logging` module is already imported.","solution":"import logging def configure_logger() -> None: Configures a logger named \'app\' to log messages to both a file \'app.log\' and the console with a specific format, and sets the logging level to INFO or higher. logger = logging.getLogger(\'app\') logger.setLevel(logging.INFO) # Create file handler fh = logging.FileHandler(\'app.log\') fh.setLevel(logging.INFO) # Create console handler ch = logging.StreamHandler() ch.setLevel(logging.INFO) # Create formatter and set it for handlers formatter = logging.Formatter(\'[%(asctime)s] %(name)s - %(levelname)s - %(message)s\') fh.setFormatter(formatter) ch.setFormatter(formatter) # Check if the handlers are not already added (to avoid duplication) if not logger.handlers: # Add handlers to the logger logger.addHandler(fh) logger.addHandler(ch) # Example Usage if __name__ == \\"__main__\\": configure_logger() logger = logging.getLogger(\'app\') logger.debug(\\"This is a DEBUG message\\") # Should not appear in the output logger.info(\\"This is an INFO message\\") # Should appear in the output logger.warning(\\"This is a WARNING message\\") # Should appear in the output logger.error(\\"This is an ERROR message\\") # Should appear in the output logger.critical(\\"This is a CRITICAL message\\") # Should appear in the output"},{"question":"**Objective:** This task assesses your ability to manipulate and process lists using Python\'s data structure methods, comprehensions, and basic conditional logic. **Problem Statement:** You are given a list of raw data elements, which are strings representing integers. Your task is to perform the following operations on this list: 1. Remove duplicate elements. 2. Convert each string element to an integer. 3. Sort the elements in ascending order. 4. Identify and remove any negative integers. 5. Use list comprehensions to create a new list where each element is the square of the corresponding integer in the sorted list. **Function Signature:** ```python def process_and_square_list(raw_data: List[str]) -> List[int]: pass ``` **Input:** - `raw_data` (List[str]): A list of strings, each representing an integer. Example: `[\\"3\\", \\"1\\", \\"-1\\", \\"2\\", \\"3\\", \\"2\\", \\"-4\\"]` **Output:** - Returns a list of integers where: 1. Duplicates are removed. 2. All strings are converted to integers. 3. The list is sorted in ascending order. 4. Negative integers are removed. 5. Each integer is squared. **Example:** ```python raw_data = [\\"3\\", \\"1\\", \\"-1\\", \\"2\\", \\"3\\", \\"2\\", \\"-4\\"] # Expected Output: # [1, 4, 9] ``` **Constraints:** 1. Assume the input list `raw_data` contains only valid integers in string format. 2. The results should not include any negative integers after the operations. **Performance Requirements:** - The function should handle input lists with up to (10^5) elements efficiently. # Notes: - You can utilize Python list methods (`append`, `remove`, `sort`, etc.), conditional statements, and list comprehensions to complete this task. - Focus on creating concise and readable code. Implement the `process_and_square_list` function that meets the above requirements. **Good Luck!**","solution":"from typing import List def process_and_square_list(raw_data: List[str]) -> List[int]: # Step 1: Convert the list elements to integers and remove duplicates unique_integers = list(set(int(item) for item in raw_data)) # Step 2: Remove negative integers non_negative_integers = [item for item in unique_integers if item >= 0] # Step 3: Sort the list in ascending order sorted_integers = sorted(non_negative_integers) # Step 4: Square each element using list comprehension squared_integers = [item ** 2 for item in sorted_integers] return squared_integers"},{"question":"Reading, Interpreting, and Encoding Binary Data **Objective:** You are asked to implement a function that reads binary data from a file, interprets specific parts of it using the `struct` module, and then encodes the interpreted data using a specified codec from the `codecs` module. **Problem Statement:** Implement a function `process_binary_data(file_path: str, codec_name: str) -> str`. This function should: 1. Read the first 12 bytes from the binary file located at `file_path`. 2. Interpret these bytes as follows: - The first 4 bytes as an integer. - The next 8 bytes as a double precision floating point number. 3. Encode the string representation of this interpreted data using the encoding specified by `codec_name`. - The string should be in the format: `\\"Integer: {int_value}, Float: {float_value}\\"` - For example, if the interpreted integer is `42` and the float is `3.14159`, the string should be `\\"Integer: 42, Float: 3.14159\\"`. 4. Return the encoded string. **Input:** - `file_path` (str): The path to the binary file. - `codec_name` (str): The name of the codec to be used for encoding the interpreted data. **Output:** - A string that is the encoded representation of the interpreted data. **Constraints:** - The binary file will always have at least 12 bytes. - The codec name provided will be a valid codec that exists in the `codecs` module. **Example:** Suppose the first 12 bytes of the binary file contain: ``` x00x00x00x2ax40x09x21xfbx54x44x2dx18 ``` where: - `x2a` represents the integer `42` - `x40x09x21xfbx54x44x2dx18` represents the float `3.14159`. Given `codec_name` as `\\"utf-8\\"`, the function call: ```python encoded_str = process_binary_data(\'path_to_file\', \'utf-8\') ``` should return the string: ``` \'Integer: 42, Float: 3.14159\' ``` encoded using UTF-8. **Hint:** Refer to the documentation of the `struct` and `codecs` modules for details on how to unpack binary data and encode strings respectively.","solution":"import struct import codecs def process_binary_data(file_path: str, codec_name: str) -> str: Reads the first 12 bytes from the binary file located at file_path. Interprets them as an integer and a double precision float, then encodes the string representation of this interpreted data using the specified codec. :param file_path: Path to the binary file :param codec_name: Name of the codec for encoding the interpreted data :return: Encoded string of interpreted data with open(file_path, \'rb\') as f: data = f.read(12) # Unpack the first 4 bytes as an integer and the next 8 bytes as a double int_value = struct.unpack(\'i\', data[:4])[0] float_value = struct.unpack(\'d\', data[4:])[0] # Create the string representation result_str = f\\"Integer: {int_value}, Float: {float_value}\\" # Encode the string using the specified codec encoded_result = result_str.encode(codec_name) return encoded_result"},{"question":"**Asyncio Task Manager** You are tasked with implementing a Task Manager using Python\'s `asyncio` module. This Task Manager will handle the creation, execution, and management of multiple tasks concurrently. The tasks will perform a simple job - printing the name of the task and its execution time at regular intervals. Additionally, the Task Manager should support canceling tasks and handling timeouts. # Requirements: 1. Implement a coroutine `task(name, duration)` that: - Prints the task name and its execution time every second until the given `duration` is reached. - Returns the total execution time of the task. 2. Implement a coroutine `main()` that: - Creates and runs multiple `task` coroutines concurrently. - Uses `asyncio.gather` to manage these tasks. - Cancels any task that exceeds a given timeout period and handles the cancellation gracefully. - Waits for the completion of all tasks and gathers their results. 3. The `main()` coroutine should: - Accept a list of tuples where each tuple contains a task name and its duration. - Accept a timeout value which will be used to cancel tasks that exceed this period. - Return the results of all completed tasks, indicating which tasks were canceled due to timeout. # Input: - A list of tuples `tasks = [(name1, duration1), (name2, duration2), ..., (namen, durationn)]` - An integer `timeout` representing the timeout period in seconds. # Output: - A list of results where each result is a dictionary with: - `\\"name\\"`: The name of the task. - `\\"status\\"`: \\"completed\\" or \\"cancelled\\". - `\\"execution_time\\"`: The total execution time if completed, or Time when it was cancelled. # Example: ```python import asyncio import time async def task(name, duration): start_time = time.time() try: for i in range(duration): print(f\\"Task {name} running at second {i+1}\\") await asyncio.sleep(1) execution_time = time.time() - start_time return {\\"name\\": name, \\"status\\": \\"completed\\", \\"execution_time\\": execution_time} except asyncio.CancelledError: execution_time = time.time() - start_time return {\\"name\\": name, \\"status\\": \\"cancelled\\", \\"execution_time\\": execution_time} async def main(tasks, timeout): task_objects = [asyncio.create_task(task(name, duration)) for name, duration in tasks] done, pending = await asyncio.wait(task_objects, timeout=timeout, return_when=asyncio.ALL_COMPLETED) results = [] for t in done: results.append(t.result()) for t in pending: t.cancel() try: await t except asyncio.CancelledError as e: results.append(t.result()) return results if __name__ == \\"__main__\\": tasks = [(\\"Task1\\", 3), (\\"Task2\\", 5), (\\"Task3\\", 2)] timeout = 4 result = asyncio.run(main(tasks, timeout)) print(result) ``` **Expected Output:** ```python Task Task1 running at second 1 Task Task2 running at second 1 Task Task3 running at second 1 Task Task1 running at second 2 Task Task2 running at second 2 Task Task3 running at second 2 Task Task1 running at second 3 Task Task2 running at second 3 Task Task1 has completed Task Task3 has completed Task Task2 running at second 4 timeout reached, cancelling task Task2 [ {\\"name\\": \\"Task1\\", \\"status\\": \\"completed\\", \\"execution_time\\": 3.001}, {\\"name\\": \\"Task3\\", \\"status\\": \\"completed\\", \\"execution_time\\": 2.001}, {\\"name\\": \\"Task2\\", \\"status\\": \\"cancelled\\", \\"execution_time\\": 4.001} ] ``` Constraints: - The duration of tasks cannot exceed 100 seconds. - The timeout value should be a positive integer. - Handle all exceptions gracefully and ensure the proper completion of the asyncio event loop.","solution":"import asyncio import time async def task(name, duration): start_time = time.time() try: for i in range(duration): print(f\\"Task {name} running at second {i+1}\\") await asyncio.sleep(1) execution_time = time.time() - start_time return {\\"name\\": name, \\"status\\": \\"completed\\", \\"execution_time\\": execution_time} except asyncio.CancelledError: execution_time = time.time() - start_time return {\\"name\\": name, \\"status\\": \\"cancelled\\", \\"execution_time\\": execution_time} async def main(tasks, timeout): task_objects = [asyncio.create_task(task(name, duration)) for name, duration in tasks] done, pending = await asyncio.wait(task_objects, timeout=timeout, return_when=asyncio.ALL_COMPLETED) results = [] for t in done: results.append(t.result()) for t in pending: t.cancel() try: await t except asyncio.CancelledError as e: results.append(t.result()) return results if __name__ == \\"__main__\\": tasks = [(\\"Task1\\", 3), (\\"Task2\\", 5), (\\"Task3\\", 2)] timeout = 4 result = asyncio.run(main(tasks, timeout)) print(result)"},{"question":"**Question: Implement a Custom Interactive Python Prompt** Using the \\"readline\\" module in Python, write a custom interactive prompt that includes the following features: 1. **Initialization from a Custom Configuration**: The prompt should read an initialization file (named `.myreadlineconfig` in the user\'s home directory) when it starts, if the file exists. Implement this using `readline.parse_and_bind()`. 2. **Command History Handling**: The prompt should support command history such that: - On startup, it reads a history file named `.my_python_history` from the user\'s home directory (if it exists) and appends new commands to this history during the session. - It saves the updated history back to the file on exit, ensuring the file does not grow larger than 500 entries. 3. **Command Auto-completion**: Implement auto-completion using the `Tab` key for a predefined list of commands, such as [\'start\', \'stop\', \'list\', \'exit\']. **Constraints**: - The history should persist between different sessions. - The history file should not exceed 500 entries. - Commands are case-sensitive. **Input**: - The prompt should continually read user input until the \'exit\' command is entered. **Output**: - The prompt should display the user\'s input back to them (like an echo), but only for demonstration purposes during this exercise. **Function Signatures**: ```python def initialize_readline_config(): Reads and applies configuration from ~/.myreadlineconfig if it exists. pass def load_command_history(file_path: str) -> int: Loads the command history from the specified file and returns the history length before loading. pass def save_command_history(file_path: str, previous_history_length: int): Saves the command history, ensuring the total number of entries does not exceed 500. pass def setup_autocompletion(commands: list): Sets up the autocompletion mechanism for the specified list of commands. pass def start_custom_prompt(): Starts the custom interactive prompt, with history and autocomplete features. pass ``` **Example**: - Place a file `.myreadlineconfig` in your home directory with the following content: ``` tab: complete set editing-mode vi ``` - Before starting the prompt, ensure your home directory has a file `.my_python_history` (can be empty initially). - Run `start_custom_prompt()`. The prompt should provide auto-completion for \'start\', \'stop\', \'list\', \'exit\' and manage history as specified. *Your task is to implement these functions based on the features described.*","solution":"import os import readline def initialize_readline_config(): Reads and applies configuration from ~/.myreadlineconfig if it exists. config_file = os.path.expanduser(\\"~/.myreadlineconfig\\") if os.path.exists(config_file): with open(config_file, \'r\') as f: for line in f: readline.parse_and_bind(line.strip()) def load_command_history(file_path: str) -> int: Loads the command history from the specified file and returns the history length before loading. if os.path.exists(file_path): readline.read_history_file(file_path) return readline.get_current_history_length() def save_command_history(file_path: str, previous_history_length: int): Saves the command history, ensuring the total number of entries does not exceed 500. current_history_length = readline.get_current_history_length() history_to_save = [] for i in range(previous_history_length, current_history_length): history_to_save.append(readline.get_history_item(i + 1)) # Only keep the last 500 entries max_entries = 500 all_history = [] if os.path.exists(file_path): with open(file_path, \'r\') as f: all_history.extend(f.readlines()) new_history = all_history + [entry + \'n\' for entry in history_to_save] new_history = new_history[-max_entries:] with open(file_path, \'w\') as f: f.writelines(new_history) def setup_autocompletion(commands: list): Sets up the autocompletion mechanism for the specified list of commands. def completer(text, state): matches = [cmd for cmd in commands if cmd.startswith(text)] return matches[state] if state < len(matches) else None readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") def start_custom_prompt(): Starts the custom interactive prompt, with history and autocomplete features. initialize_readline_config() history_file = os.path.expanduser(\\"~/.my_python_history\\") previous_history_length = load_command_history(history_file) commands = [\'start\', \'stop\', \'list\', \'exit\'] setup_autocompletion(commands) try: while True: user_input = input(\\">>> \\") if user_input == \'exit\': break print(user_input) except EOFError: pass finally: save_command_history(history_file, previous_history_length)"},{"question":"# Question: File Content Reverser Implement a function `reverse_file_content_in_place` that takes in the path to a text file and reverses its contents in place using the `mmap` module without loading the entire file into memory. The function should handle large files efficiently by memory-mapping the file. Function Signature ```python def reverse_file_content_in_place(file_path: str) -> None: pass ``` # Detailed Instructions 1. Open the given file in read-plus-binary mode (`r+b`). 2. Memory-map the file, ensuring the whole file is mapped to memory. 3. Reverse the content of the file in place using the memory-mapped object. 4. The reversal must be done efficiently without creating large intermediate objects or using excessive memory. 5. Use appropriate error handling to manage cases like empty files, file access permissions, etc. 6. After processing, ensure to close the file and the memory map properly. Constraints - You can assume that the file path given is valid. - Handle files of significant size without consuming a large amount of memory. - The file will contain plain ASCII text. # Example Usage If `input.txt` contains: ``` Hello, World! ``` After calling `reverse_file_content_in_place(\'input.txt\')`, the content of `input.txt` should be: ``` !dlroW ,olleH ``` Performance Requirements - The implementation should be efficient and respect the constraints imposed by potentially large file sizes. - Think about both time and space complexities, and ensure the method operates with minimal memory overhead. # Hints - Consider using slicing and/or indexing provided by the `mmap` object to efficiently reverse parts of the file. - Be mindful of the types of `access` modes and they are used correctly as per the requirements. # Important: Do not use other libraries or read the entire file into memory directly using Python\'s standard file I/O operations. # Submission Requirements - The implementation of the function `reverse_file_content_in_place`. - A brief explanation of the approach used in the solution.","solution":"import mmap def reverse_file_content_in_place(file_path: str) -> None: Reverses the contents of a file in place using the mmap module. Parameters: file_path (str): The path to the file to reverse. try: with open(file_path, \'r+b\') as f: # Memory-map the file with mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_WRITE) as mm: left, right = 0, mm.size() - 1 while left < right: # Swap the characters at the left and right positions left_char = mm[left] right_char = mm[right] mm[left] = right_char mm[right] = left_char left += 1 right -= 1 except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective This question assesses your understanding of seaborn by requiring you to work with different data formats and create visualizations. You will need to transform the dataset, differentiate between data formats, and correctly use seaborn plotting functions. Question You are provided with the \\"tips\\" dataset from seaborn, which contains information about tips received by waiters in a restaurant, including day, time, total bill amount, tip amount, and more. You will perform the following tasks: 1. Load the \\"tips\\" dataset from seaborn. 2. Transform the dataset from its original format to a wide-format where the index is the `day` of the week and columns are `time` (Lunch or Dinner). The values should be the average `tip` amount. 3. Create a line plot using seaborn that shows the average tip amount for each day, with separate lines for Lunch and Dinner. 4. Transform the original dataset to a long-format where each row represents a single observation and includes the day, time, and tip amount. Use this transformed dataset to create a box plot showing the distribution of tips for each day, separated by time (Lunch or Dinner). Instructions 1. Implement the function `transform_and_plot_tips`. 2. The function should not take any arguments and should: - Load and transform the \\"tips\\" dataset as described. - Create and display the required plots. 3. Ensure your plots are appropriately labeled. Expected Output - Two seaborn plots: - A line plot showing the average tips by day for Lunch and Dinner. - A box plot showing the distribution of tips by day for Lunch and Dinner. Function Signature ```python def transform_and_plot_tips(): # Your implementation here ``` Example ```python def transform_and_plot_tips(): import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Wide-format transformation wide_tips = tips.pivot_table(index=\\"day\\", columns=\\"time\\", values=\\"tip\\", aggfunc=\\"mean\\") # Line plot for wide-format data sns.relplot(data=wide_tips, kind=\\"line\\", markers=True, dashes=False) plt.title(\\"Average Tips by Day (Lunch vs Dinner)\\") plt.ylabel(\\"Average Tip Amount\\") plt.xlabel(\\"Day\\") plt.show() # Long-format transformation long_tips = tips.melt( id_vars=[\\"day\\", \\"time\\"], value_vars=[\\"tip\\"], var_name=\\"variable\\", value_name=\\"value\\" ) # Box plot for long-format data sns.catplot(data=long_tips, x=\\"day\\", y=\\"value\\", hue=\\"time\\", kind=\\"box\\") plt.title(\\"Distribution of Tips by Day (Lunch vs Dinner)\\") plt.ylabel(\\"Tip Amount\\") plt.xlabel(\\"Day\\") plt.show() ```","solution":"def transform_and_plot_tips(): import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Wide-format transformation wide_tips = tips.pivot_table(index=\\"day\\", columns=\\"time\\", values=\\"tip\\", aggfunc=\\"mean\\") # Line plot for wide-format data sns.relplot(data=wide_tips, kind=\\"line\\", markers=True, dashes=False) plt.title(\\"Average Tips by Day (Lunch vs Dinner)\\") plt.ylabel(\\"Average Tip Amount\\") plt.xlabel(\\"Day\\") plt.show() # Long-format transformation long_tips = tips.melt( id_vars=[\\"day\\", \\"time\\"], value_vars=[\\"tip\\"], var_name=\\"variable\\", value_name=\\"value\\" ) # Box plot for long-format data sns.catplot(data=long_tips, x=\\"day\\", y=\\"value\\", hue=\\"time\\", kind=\\"box\\") plt.title(\\"Distribution of Tips by Day (Lunch vs Dinner)\\") plt.ylabel(\\"Tip Amount\\") plt.xlabel(\\"Day\\") plt.show()"},{"question":"# Custom Complex Number Class in Python You are tasked with implementing a custom complex number class in Python that encapsulates complex number arithmetic using the provided C API functions and integrates smoothly with Python\'s complex type. Implement the class `CComplex` with the following requirements: 1. **Initialization**: - The constructor should accept two arguments: `real` and `imag`, which are the real and imaginary parts of the complex number, respectively. 2. **Methods**: - **Addition**: Implement the `__add__` method to add two `CComplex` objects. - **Subtraction**: Implement the `__sub__` method to subtract two `CComplex` objects. - **Multiplication**: Implement the `__mul__` method to multiply two `CComplex` objects. - **Division**: Implement the `__truediv__` method to divide two `CComplex` objects. - **Negation**: Implement the `__neg__` method to negate a `CComplex` object. - **Exponentiation**: Implement the `__pow__` method to raise a `CComplex` object to the power of another `CComplex` object. 3. **Conversion Methods**: - Implement the `to_python_complex` method to convert the `CComplex` object to a native Python complex object. 4. **String Representation**: - Implement the `__str__` method to provide a string representation of the `CComplex` object in the form `\\"(real + imag j)\\"`. **Constraints**: - You must use the provided C API functions where applicable. - Handle any potential errors gracefully, especially divisions or exponentiations involving zero. - Ensure the class integrates well with Python\'s built-in complex type for operations and conversions. # Example: ```python # Example usage: c1 = CComplex(3, 4) c2 = CComplex(1, 2) print(c1 + c2) # Output: (4.0 + 6.0j) print(c1 - c2) # Output: (2.0 + 2.0j) print(c1 * c2) # Output: (-5.0 + 10.0j) print(c1 / c2) # Output: (2.2 - 0.4j) print(-c1) # Output: (-3.0 - 4.0j) print(c1 ** c2) # Should implement complex exponentiation print(c1.to_python_complex()) # Output: (3+4j) ``` # Input and Output Format - The constructor takes two floats as input for the real and imaginary parts. - Each arithmetic method takes another `CComplex` object as input and returns a new `CComplex` object. - The `to_python_complex` method returns a Python `complex` object. - The `__str__` method returns a string. Use appropriate C API functions from the documentation for implementing these methods.","solution":"class CComplex: def __init__(self, real, imag): self.real = real self.imag = imag def __add__(self, other): return CComplex(self.real + other.real, self.imag + other.imag) def __sub__(self, other): return CComplex(self.real - other.real, self.imag - other.imag) def __mul__(self, other): real = self.real * other.real - self.imag * other.imag imag = self.real * other.imag + self.imag * other.real return CComplex(real, imag) def __truediv__(self, other): if other.real == 0 and other.imag == 0: raise ValueError(\\"Cannot divide by zero complex number.\\") denom = other.real ** 2 + other.imag ** 2 real = (self.real * other.real + self.imag * other.imag) / denom imag = (self.imag * other.real - self.real * other.imag) / denom return CComplex(real, imag) def __neg__(self): return CComplex(-self.real, -self.imag) def __pow__(self, other): import cmath result = cmath.exp(other.to_python_complex() * cmath.log(self.to_python_complex())) return CComplex(result.real, result.imag) def to_python_complex(self): return complex(self.real, self.imag) def __str__(self): return f\\"({self.real} + {self.imag}j)\\""},{"question":"# Color Conversion Function Implementation **Objective:** Your task is to implement a Python function that converts an RGB color to all other mentioned color spaces (YIQ, HLS, HSV) and returns them in a dictionary format. **Function Signature:** ```python def convert_rgb_to_all_colorspaces(r: float, g: float, b: float) -> dict: pass ``` **Expected Input and Output Format:** - Input: Three floating-point numbers between 0 and 1, representing red, green, and blue components of a color. - Output: A dictionary with keys as \\"YIQ\\", \\"HLS\\", and \\"HSV\\", and values as tuples containing the respective converted color coordinates. **Example:** ```python >>> convert_rgb_to_all_colorspaces(0.2, 0.4, 0.4) { \'YIQ\': (0.35320000000000004, 0.03124, -0.094), \'HLS\': (0.5, 0.30000000000000004, 0.3333333333333333), \'HSV\': (0.5, 0.5, 0.4) } ``` **Constraints:** - You must use the functions provided by the colorsys module for conversion. - The input values will always be valid floating-point numbers between 0 and 1. - Ensure that the output dictionary contains correctly named keys \\"YIQ\\", \\"HLS\\", and \\"HSV\\". **Notes:** - Do not round the output values; retain full floating-point precision. This question will test your ability to use the provided functions, manage floating-point operations, and handle dictionary data structures effectively.","solution":"import colorsys def convert_rgb_to_all_colorspaces(r: float, g: float, b: float) -> dict: Converts RGB to YIQ, HLS, and HSV color spaces and returns them in a dictionary. Parameters: r (float): Red component [0, 1] g (float): Green component [0, 1] b (float): Blue component [0, 1] Returns: dict: A dictionary with keys \'YIQ\', \'HLS\', and \'HSV\' and corresponding color space values as tuples. yiq = colorsys.rgb_to_yiq(r, g, b) hls = colorsys.rgb_to_hls(r, g, b) hsv = colorsys.rgb_to_hsv(r, g, b) return { \'YIQ\': yiq, \'HLS\': hls, \'HSV\': hsv }"},{"question":"You are required to implement a function that calculates the difference in time between two datetime objects and represents the result in days, hours, minutes, and seconds. # Function Signature ```python def calculate_time_difference(dt1: datetime.datetime, dt2: datetime.datetime) -> dict: pass ``` # Input - `dt1` and `dt2`: These are two datetime objects. # Output - A dictionary with the following keys: - `\'days\'`: an integer representing the difference in days. - `\'hours\'`: an integer representing the difference in hours. - `\'minutes\'`: an integer representing the difference in minutes. - `\'seconds\'`: an integer representing the difference in seconds. # Constraints - The datetime objects are provided in UTC. - The difference calculation should be absolute (i.e., the output should always be a non-negative duration). # Example ```python import datetime dt1 = datetime.datetime(2023, 1, 1, 12, 0, 0) dt2 = datetime.datetime(2023, 1, 3, 14, 30, 15) result = calculate_time_difference(dt1, dt2) # Expected output: # { # \'days\': 2, # \'hours\': 2, # \'minutes\': 30, # \'seconds\': 15 #} ``` # Performance Requirements - The solution should be efficient and handle large date ranges up to several years apart. # Additional Requirements - Do not use any external libraries other than the `datetime` module. - Ensure the code is clean, well-documented, and includes proper error handling.","solution":"import datetime def calculate_time_difference(dt1: datetime.datetime, dt2: datetime.datetime) -> dict: Calculate the difference between two datetime objects and represent it as a dictionary with days, hours, minutes, and seconds. :param dt1: First datetime object :param dt2: Second datetime object :return: Dictionary with keys \'days\', \'hours\', \'minutes\', \'seconds\' # Calculate absolute difference delta = abs(dt1 - dt2) # Extract days, seconds, hours, and minutes days = delta.days total_seconds = delta.seconds hours, remainder = divmod(total_seconds, 3600) minutes, seconds = divmod(remainder, 60) return { \'days\': days, \'hours\': hours, \'minutes\': minutes, \'seconds\': seconds }"},{"question":"# Seaborn Advanced Plotting Challenge Given the `diamonds` dataset from the seaborn library, use seaborn objects (`seaborn.objects`) to visualize the dataset. Your task is to create a robust and informative plot based on custom requirements for analyzing diamond clarity vs carat weight with custom error bars and bootstrapping. Instructions 1. **Load the Dataset:** - Load the `diamonds` dataset using `seaborn.load_dataset(\\"diamonds\\")`. 2. **Create a Plot:** - Using seaborn objects, create a plot to visualize the relationship between `clarity` (x-axis) and `carat` (y-axis). 3. **Customization Requirements:** - Compute the median as the central tendency measure for the plot. - Use bootstrapping with a seed value of `42` to ensure reproducibility. - Add error bars representing the standard deviation around the median. - Apply a weight using the `price` variable. - Ensure the plot is clear and well-labeled. 4. **Output:** - Save the plot as `diamond_clarity_vs_carat.png` file. - Verify that the plot meets all the listed requirements. Constraints 1. You should not change the dataset in any other way except for plotting. 2. Plotting should be done using seaborn\'s object-oriented interface (`seaborn.objects`). Example of Expected Code: ```python import seaborn.objects as so from seaborn import load_dataset def plot_diamond_clarity_vs_carat(): diamonds = load_dataset(\\"diamonds\\") p = so.Plot(diamonds, \\"clarity\\", \\"carat\\") p.add(so.Range(), so.Est(\\"median\\"), errorbar=\\"sd\\", seed=42, weight=\\"price\\") p.savefig(\\"diamond_clarity_vs_carat.png\\") plot_diamond_clarity_vs_carat() ``` Ensure that your implementation matches the given example and fulfills all the specified customizations.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_diamond_clarity_vs_carat(): diamonds = load_dataset(\\"diamonds\\") # Creating the plot using seaborn\'s object-oriented interface p = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") p.add(so.Range(), so.Est(\\"median\\"), errorbar=\\"sd\\", seed=42, weight=\\"price\\") p.label(x=\\"Clarity\\", y=\\"Carat Weight\\", title=\\"Diamond Clarity vs Carat Weight with Error Bars\\") p.save(\\"diamond_clarity_vs_carat.png\\") plot_diamond_clarity_vs_carat()"},{"question":"# Seaborn Coding Assessment You are provided with a dataset containing information about various cars. The dataset includes attributes such as \'weight\', \'displacement\', \'horsepower\', and \'mpg\' (miles per gallon). Your task is to use the seaborn library to create and analyze residual plots, demonstrating your understanding of `residplot` and its parameters. **Dataset:** Use `sns.load_dataset(\\"mpg\\")` to load the dataset. # Part 1: Basic Residual Plot 1. Load the `mpg` dataset from seaborn. 2. Create a basic residual plot to visualize the residuals of a simple regression model with \'weight\' as the independent variable and \'displacement\' as the dependent variable. ```python import seaborn as sns sns.set_theme() mpg = sns.load_dataset(\\"mpg\\") # Create a basic residual plot sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") ``` # Part 2: Detecting Non-Linear Trends Extend the plot from Part 1 by creating a residual plot with a second-order polynomial regression (quadratic) to check for non-linear trends. ```python # Create a residual plot with a second-order polynomial regression sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\", order=2) ``` # Part 3: Adding a LOWESS Curve Add a LOWESS (locally weighted scatterplot smoothing) curve to the residual plot from Part 2, to further emphasize linear or non-linear patterns. Customize the LOWESS curve to be red. ```python # Create a residual plot with a second-order polynomial regression and a LOWESS curve sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) ``` # Deliverables 1. **Code Implementation**: Submit the Python code used to generate all three residual plots. 2. **Analysis**: - Explain any patterns or structures you observe in each of the three residual plots. - Discuss how the addition of a higher-order polynomial regression and LOWESS curve affects your interpretation of the residuals. **Note**: Ensure your code is well-commented and follows best practices for readability and maintainability.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the mpg dataset from seaborn mpg = sns.load_dataset(\\"mpg\\") # Part 1: Create a basic residual plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\'Residual Plot (Weight vs Displacement)\') plt.show() # Part 2: Create a residual plot with a second-order polynomial regression plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\", order=2) plt.title(\'Residual Plot with Second-Order Polynomial Regression (Weight vs Displacement)\') plt.show() # Part 3: Create a residual plot with a second-order polynomial regression and a LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\", order=2, lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Residual Plot with Second-Order Polynomial Regression and LOWESS Curve (Weight vs Displacement)\') plt.show()"},{"question":"# Description You are given a stream of numerical data, representing transaction amounts being processed by a bank. To efficiently manage fraud detection and reporting, you need to maintain a list of these transaction amounts in sorted order. Additionally, you should be able to efficiently: 1. Insert new transaction amounts while maintaining the order. 2. Calculate the median transaction amount at any point in the transaction stream. Task Write a Python function `process_transactions(transactions)` that takes a list of integers `transactions` and returns a list `medians` where `medians[i]` is the median of the first `i+1` transaction amounts after processing the `transactions[i]` item. # Input - `transactions` (a list of integers): A list of transaction amounts. # Output - `medians` (a list of floats): A list where each element at index `i` is the median of the first `i+1` transaction amounts. # Constraints - The transaction list contains at least one and at most `10^4` amounts. - Each transaction amount is an integer in the range `-10^6` to `10^6`. # Performance requirements Your solution should efficiently handle insertions and median calculations, aiming for an overall complexity better than `O(n^2)`. # Examples Example 1: Input: ```python transactions = [5, 2, 1, 4, 3] ``` Output: ```python [5.0, 3.5, 2.0, 3.0, 3.0] ``` Example 2: Input: ```python transactions = [1, 7, 9, 2, 0, 3] ``` Output: ```python [1.0, 4.0, 7.0, 4.5, 2.0, 2.5] ``` # Implementation Notes: - Use the `bisect` module to keep the transaction list sorted with efficient insertions. - Calculate the median for the current streamed data accurately.","solution":"import bisect def process_transactions(transactions): sorted_transactions = [] medians = [] for t in transactions: bisect.insort(sorted_transactions, t) n = len(sorted_transactions) if n % 2 == 1: medians.append(float(sorted_transactions[n // 2])) else: medians.append((sorted_transactions[n // 2 - 1] + sorted_transactions[n // 2]) / 2) return medians"},{"question":"# Problem: Log File Analysis and Statistics You are tasked with writing a Python program that reads a log file, processes its contents, and computes certain statistics. The log file contains multiple lines, and each line is formatted as follows: ``` [TIMESTAMP] - LEVEL - Message ``` Where: - `TIMESTAMP` is a string in the ISO 8601 format, e.g., `2023-10-11T14:23:45`. - `LEVEL` is the log level, which can be one of `INFO`, `WARNING`, `ERROR`. - `Message` is a string containing the log message. Your program must perform the following tasks: 1. Read the log file and parse its contents. 2. Count the number of log messages for each log level (`INFO`, `WARNING`, and `ERROR`). 3. Compute the earliest and latest timestamps in the log. 4. Identify the unique log levels present in the log file. 5. Calculate the frequency of the words in log messages. Words are defined as sequences of characters separated by whitespace and are case-insensitive. Punctuation should be ignored. # Input - The input to your function `log_analysis(file_path: str) -> dict` is the path to the log file. # Output - The output is a dictionary with the following structure: ```python { \\"info_count\\": int, # Number of INFO log entries \\"warning_count\\": int, # Number of WARNING log entries \\"error_count\\": int, # Number of ERROR log entries \\"earliest_timestamp\\": str, # Earliest timestamp in ISO 8601 format \\"latest_timestamp\\": str, # Latest timestamp in ISO 8601 format \\"unique_levels\\": List[str], # List of unique log levels, sorted alphabetically \\"word_frequencies\\": Dict[str, int] # Dictionary with word frequencies } ``` # Constraints - The log file may contain a large number of entries, so your solution should be efficient in terms of time and space. - You should handle potential edge cases, such as an empty log file or invalid lines in the log file. # Example Consider a log file with the following contents: ``` 2023-10-11T14:23:45 - INFO - System started successfully. 2023-10-11T14:24:15 - WARNING - Low disk space. 2023-10-11T14:25:01 - INFO - User logged in. 2023-10-11T14:26:05 - ERROR - Disk read error. 2023-10-11T14:27:09 - WARNING - High memory usage. ``` Calling `log_analysis(\'path/to/logfile.log\')` might return: ```python { \\"info_count\\": 2, \\"warning_count\\": 2, \\"error_count\\": 1, \\"earliest_timestamp\\": \'2023-10-11T14:23:45\', \\"latest_timestamp\\": \'2023-10-11T14:27:09\', \\"unique_levels\\": [\'ERROR\', \'INFO\', \'WARNING\'], \\"word_frequencies\\": { \\"system\\": 1, \\"started\\": 1, \\"successfully\\": 1, \\"low\\": 1, \\"disk\\": 2, \\"space\\": 1, \\"user\\": 1, \\"logged\\": 1, \\"in\\": 1, \\"read\\": 1, \\"error\\": 1, \\"high\\": 1, \\"memory\\": 1, \\"usage\\": 1 } } ``` # Requirements - Use the `datetime`, `collections`, and `re` modules from the Python standard library. - Ensure your implementation is robust and handles edge cases gracefully. # Note You should not include any external libraries other than those available in the standard library.","solution":"import re from collections import defaultdict from datetime import datetime def log_analysis(file_path: str) -> dict: log_levels = {\\"INFO\\": 0, \\"WARNING\\": 0, \\"ERROR\\": 0} timestamps = [] word_frequencies = defaultdict(int) unique_levels_set = set() with open(file_path, \'r\') as file: for line in file: match = re.match(r\'([.*]) - (w+) - (.*)\', line) if not match: continue timestamp_str, level, message = match.groups() timestamp_str = timestamp_str.strip(\'[]\') timestamps.append(timestamp_str) if level in log_levels: log_levels[level] += 1 unique_levels_set.add(level) words = re.findall(r\'bw+b\', message.lower()) for word in words: word_frequencies[word] += 1 timestamps.sort() result = { \\"info_count\\": log_levels[\\"INFO\\"], \\"warning_count\\": log_levels[\\"WARNING\\"], \\"error_count\\": log_levels[\\"ERROR\\"], \\"earliest_timestamp\\": timestamps[0] if timestamps else None, \\"latest_timestamp\\": timestamps[-1] if timestamps else None, \\"unique_levels\\": sorted(unique_levels_set), \\"word_frequencies\\": dict(word_frequencies) } return result"},{"question":"Advanced Bytearray Handling You are tasked with implementing a function that takes three arguments: two bytearrays and an integer. The function will perform specific operations on these bytearrays using the Python C API and return a result based on these operations. Function Specification ```python def advanced_bytearray_manipulation(arr1: bytearray, arr2: bytearray, target_size: int) -> bytearray: This function performs the following operations: 1. Concatenates arr1 and arr2. 2. Resizes the concatenated bytearray to the specified target_size. 3. Returns the resized bytearray. Args: - arr1 (bytearray): The first input bytearray. - arr2 (bytearray): The second input bytearray. - target_size (int): The target size for the resulting bytearray. Returns: - bytearray: The resulting bytearray after concatenation and resizing. Constraints: - target_size >= 0 - If target_size is greater than the length of the concatenated array, it should be filled with null bytes. - If PyObject* inputs are NULL, handle the error appropriately. pass ``` Explanation 1. **Concatenate `arr1` and `arr2`**: Utilize `PyByteArray_Concat` to concatenate the two input arrays. 2. **Resize to `target_size`**: Use `PyByteArray_Resize` to resize the concatenated array to `target_size`. 3. **Return the resulting bytearray**: The final resized bytearray should be returned. Constraints - `target_size` is non-negative. - Handle cases where the concatenated bytearray needs to be padded with null bytes if `target_size` is greater than the total length of `arr1` and `arr2`. - Properly handle invalid inputs and potential errors (e.g., null pointers in the context of the C API). Example ```python arr1 = bytearray(b\\"hello\\") arr2 = bytearray(b\\"world\\") target_size = 15 result = advanced_bytearray_manipulation(arr1, arr2, target_size) print(result) # Expected output: bytearray(b\'helloworld00000\') ``` Your goal is to implement the `advanced_bytearray_manipulation` function in a Python context, ensuring it accurately replicates the described behavior using the specified C API functions.","solution":"def advanced_bytearray_manipulation(arr1: bytearray, arr2: bytearray, target_size: int) -> bytearray: concatenated = arr1 + arr2 if target_size > len(concatenated): concatenated.extend(b\'0\' * (target_size - len(concatenated))) else: concatenated = concatenated[:target_size] return concatenated"},{"question":"# Coding Challenge: Analyze Password Policy Strength You are tasked with writing a function that analyzes the password policies of users in the Unix shadow password database using the `spwd` module. Specifically, you need to identify users with weak password policies based on certain criteria. Function Specification - **Function Name**: `find_weak_policies` - **Input**: None - **Output**: List of usernames that do not meet the following strength criteria: - The maximum number of days between password changes (`sp_max`) should not exceed 90 days. - The number of days to warn users before password expiration (`sp_warn`) should be at least 7 days. Constraints - Ensure you handle any permissions issues gracefully. If the current user does not have access to the shadow password database, return an empty list. - If a user\'s entry does not have the required attributes (indicating a potential misconfiguration), assume they have a weak policy. Example ```python def find_weak_policies(): # Your implementation here # Example Usage weak_users = find_weak_policies() print(weak_users) ``` **Note**: You are to assume that necessary permissions are already granted to access the shadow password database. Hints - Use the `spwd.getspall()` to get a list of all shadow password entries. - Iterate through the entries and check for the conditions mentioned. - Handle exceptions such as `PermissionError`.","solution":"import spwd def find_weak_policies(): Returns a list of usernames with weak password policies. Weak password policies are defined as: - The maximum number of days between password changes (sp_max) exceeds 90 days. - The number of days to warn users before password expiration (sp_warn) is less than 7 days. try: shadow_entries = spwd.getspall() except PermissionError: # If permissions are not adequate to access shadow password database, return an empty list return [] weak_users = [] for entry in shadow_entries: username = entry.sp_namp sp_max = entry.sp_max sp_warn = entry.sp_warn if sp_max > 90 or sp_warn < 7: weak_users.append(username) return weak_users"},{"question":"You are tasked with demonstrating your understanding of method binding and callable objects in Python. Specifically, you will create a class and dynamically add methods to instances of this class. # Objective Implement a class `DynamicMethods` that allows adding and calling methods dynamically. # Description 1. **Class Definition:** Define a class `DynamicMethods`. 2. **Method to Add Functions:** Implement a method `add_method` that takes: - `method_name` (string): Name of the method to be added. - `func` (callable): A function that will serve as the method. - This method should bind `func` to instances of `DynamicMethods` with the name `method_name`. 3. **Method to Call Functions:** Implement a method `call_method` that takes: - `method_name` (string): Name of the method to be called. - `*args` and `**kwargs`: Arguments and keyword arguments to pass to the method. - This method should call the dynamically bound method with the provided arguments. # Requirements - Methods added using `add_method` should behave as if they were defined within the class. - The added methods should be callable with any arguments required by the original function. - Ensure appropriate error handling if a method name does not exist. # Example ```python class DynamicMethods: def __init__(self): # Your code here def add_method(self, method_name, func): # Your code here def call_method(self, method_name, *args, **kwargs): # Your code here # Example Usage def greet(self, name): return f\\"Hello, {name}!\\" dm = DynamicMethods() dm.add_method(\'greet\', greet) print(dm.call_method(\'greet\', \'Alice\')) # Output: \\"Hello, Alice!\\" def add(self, x, y): return x + y dm.add_method(\'add\', add) print(dm.call_method(\'add\', 5, 3)) # Output: 8 ``` **Constraints:** - You should not use any libraries or modules other than Python\'s standard library. - Ensure that `add_method` can handle any callable, not just functions defined with `def`. Good luck!","solution":"class DynamicMethods: def __init__(self): self.methods = {} def add_method(self, method_name, func): self.methods[method_name] = func.__get__(self, self.__class__) def call_method(self, method_name, *args, **kwargs): if method_name in self.methods: return self.methods[method_name](*args, **kwargs) else: raise AttributeError(f\\"Method \'{method_name}\' not found\\") # Example Usage def greet(self, name): return f\\"Hello, {name}!\\" dm = DynamicMethods() dm.add_method(\'greet\', greet) print(dm.call_method(\'greet\', \'Alice\')) # Output: \\"Hello, Alice!\\" def add(self, x, y): return x + y dm.add_method(\'add\', add) print(dm.call_method(\'add\', 5, 3)) # Output: 8"},{"question":"**Custom Binhex Encoder/Decoder Implementation** In this task, you are required to implement your own version of a simplified binhex encoding and decoding process. This will help you demonstrate your understanding of file handling, encoding/decoding processes, and managing exceptions. **Function 1: custom_binhex(input_file: str, output_file: str) -> None** - **Description**: This function should read a binary file and convert its contents to a binhex4-style encoded string, writing the result to a specified output file. - **Parameters**: - `input_file` (str): The path to the input binary file. - `output_file` (str): The path to the output file where the binhex data will be written. - **Constraints**: - You should handle exceptions if the input file does not exist. - You should manually convert the binary contents to a hex representation. **Function 2: custom_hexbin(input_file: str, output_file: str) -> None** - **Description**: This function should read a binhex4-style encoded file and decode its contents back to binary, writing the result to a specified output file. - **Parameters**: - `input_file` (str): The path to the binhex encoded input file. - `output_file` (str): The path to the output file where the binary data will be written. - **Constraints**: - You should handle exceptions if the input file does not exist. - You should manually convert the hex representation back to binary. **Exception Handling**: - Implement a custom exception `BinhexError` that is raised whenever the input data for encoding or decoding is improper. **Example Usage**: ```python try: custom_binhex(\\"example_binary.bin\\", \\"encoded_output.hqx\\") custom_hexbin(\\"encoded_output.hqx\\", \\"decoded_binary.bin\\") except BinhexError as e: print(f\\"An error occurred: {e}\\") ``` **Notes**: 1. You should not use the deprecated `binhex` module functions directly in your implementations. 2. Ensure to handle file opening and closing operations properly. 3. Test your implementation with sample binary files to verify the correctness of the encoding and decoding processes. **Deliverables**: - `custom_binhex` function definition - `custom_hexbin` function definition - `BinhexError` exception class definition","solution":"class BinhexError(Exception): pass def custom_binhex(input_file: str, output_file: str) -> None: import os if not os.path.exists(input_file): raise BinhexError(f\\"Input file {input_file} does not exist\\") try: with open(input_file, \'rb\') as infile: binary_data = infile.read() hex_string = binary_data.hex() # Convert binary data to a hex representation with open(output_file, \'w\') as outfile: outfile.write(hex_string) except Exception as e: raise BinhexError(f\\"An error occurred during binhex encoding: {e}\\") def custom_hexbin(input_file: str, output_file: str) -> None: import os if not os.path.exists(input_file): raise BinhexError(f\\"Input file {input_file} does not exist\\") try: with open(input_file, \'r\') as infile: hex_string = infile.read() # Validate if the input contents are proper hex if not all(c in \'0123456789abcdefABCDEF\' for c in hex_string): raise BinhexError(\\"Invalid hex data in input file\\") binary_data = bytes.fromhex(hex_string) # Convert hex representation back to binary with open(output_file, \'wb\') as outfile: outfile.write(binary_data) except Exception as e: raise BinhexError(f\\"An error occurred during hexbin decoding: {e}\\")"},{"question":"# Python Coding Assessment Objective The objective of this assessment is to evaluate your ability to work with Pythonâ€™s `lzma` module to compress and decompress data, using both built-in methods and custom filter chains. Problem Statement You are required to write two functions: 1. **compress_file(input_file, output_file)**: This function should compress a given input text file using the LZMA compression algorithm and save the compressed data to the specified output file. 2. **decompress_file(input_file, output_file)**: This function should decompress the given input file (which is LZMA-compressed) and save the decompressed data to the specified output file. Additionally, implement a custom filter chain that involves a delta filter and a high compression preset for the compress_file function. Input and Output Format 1. **compress_file** * Input: * `input_file` (str): the path to the text file to be compressed. * `output_file` (str): the path where the compressed file will be stored. * Output: This function does not return anything. It simply creates a file at `output_file` with the compressed data. 2. **decompress_file** * Input: * `input_file` (str): the path to the LZMA-compressed file to be decompressed. * `output_file` (str): the path where the decompressed text file will be stored. * Output: This function does not return anything. It simply creates a file at `output_file` with the decompressed data. Constraints * The input files will be text files (e.g., `.txt`) and the output of the compression will be an LZMA file (e.g., `.xz`). * You may assume the input files are not empty and that file paths are valid. * For compressing files, apply a custom filter chain comprising a delta filter with a distance of 1 and a high compression preset (level 9 with extreme setting). Example ```python compress_file(\\"example.txt\\", \\"example_compressed.xz\\") decompress_file(\\"example_compressed.xz\\", \\"example_decompressed.txt\\") ``` Here: - The `compress_file` function reads the content from `example.txt`, compresses it with LZMA (using a custom filter), and writes the result to `example_compressed.xz`. - The `decompress_file` function reads the compressed data from `example_compressed.xz`, decompresses it, and writes the result to `example_decompressed.txt`. Performance Notes You should ensure that your implementation is efficient and handles large files gracefully. The use of the custom filter chain should reflect a good understanding of the various options provided by the `lzma` module. ```python # Your implementation goes here def compress_file(input_file, output_file): import lzma filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] with open(input_file, \'rb\') as f_in, lzma.open(output_file, \'wb\', filters=filters) as f_out: f_out.write(f_in.read()) def decompress_file(input_file, output_file): import lzma with lzma.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.write(f_in.read()) ```","solution":"def compress_file(input_file, output_file): import lzma filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME}, ] with open(input_file, \'rb\') as f_in, lzma.open(output_file, \'wb\', filters=filters) as f_out: f_out.write(f_in.read()) def decompress_file(input_file, output_file): import lzma with lzma.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: f_out.write(f_in.read())"},{"question":"Objective: Write a Python program that securely hashes a user\'s password and validates it using the `crypt` module. The program should incorporate best practices for security, such as constant-time comparison for checking hashed passwords. Specifications: 1. Implement a function `hash_password(plaintext: str) -> str` that: - Takes a user\'s plaintext password and returns the hashed password using the strongest method available in the `crypt` module. - Implements proper error handling for invalid inputs. 2. Implement a function `is_password_valid(plaintext: str, hashed: str) -> bool` that: - Takes the user\'s plaintext password and a hashed password. - Returns `True` if the plaintext password matches the hashed password, `False` otherwise. - Uses a constant-time comparison method to prevent timing attacks. 3. **Input Format:** - `plaintext`: A string representing the user\'s password to be hashed or validated. - `hashed`: A string representing the hashed password, used for validation. 4. **Output Format:** - `hash_password`: Returns the hashed password as a string. - `is_password_valid`: Returns a boolean value (`True` or `False`). 5. **Constraints:** - You may assume that the input passwords are non-empty strings containing printable ASCII characters only. - The program should terminate gracefully, providing appropriate error messages for invalid inputs. Example: ```python import crypt from hmac import compare_digest as compare_hash def hash_password(plaintext: str) -> str: # Your implementation here def is_password_valid(plaintext: str, hashed: str) -> bool: # Your implementation here # Example usage: plaintext = \\"securepassword123\\" hashed_password = hash_password(plaintext) print(hashed_password) # Prints the hashed password is_valid = is_password_valid(\\"securepassword123\\", hashed_password) print(is_valid) # Should print True is_valid = is_password_valid(\\"wrongpassword\\", hashed_password) print(is_valid) # Should print False ``` Feel free to add additional helper functions or classes if necessary. Be sure to include inline comments to explain your code.","solution":"import crypt import os from hmac import compare_digest as compare_hash def hash_password(plaintext: str) -> str: Takes a user\'s plaintext password and returns the hashed password using the strongest method available in the crypt module. if not isinstance(plaintext, str) or not plaintext: raise ValueError(\\"Invalid input: plaintext must be a non-empty string\\") salt = crypt.mksalt(crypt.METHOD_SHA512) hashed_password = crypt.crypt(plaintext, salt) return hashed_password def is_password_valid(plaintext: str, hashed: str) -> bool: Takes the user\'s plaintext password and a hashed password. Returns True if the plaintext password matches the hashed password, False otherwise. Uses a constant-time comparison method to prevent timing attacks. if not isinstance(plaintext, str) or not plaintext: raise ValueError(\\"Invalid input: plaintext must be a non-empty string\\") if not isinstance(hashed, str) or not hashed: raise ValueError(\\"Invalid input: hashed must be a non-empty string\\") return compare_hash(crypt.crypt(plaintext, hashed), hashed)"},{"question":"Problem Statement: Your task is to design a class `Student` that allows binding and manipulation of instance methods and methods dynamically. # Requirements: 1. Implement the `Student` class with the following specifications: - The class should have an attribute `name` to store the student\'s name. - The class should have an attribute `grade` to store the student\'s grade. 2. Implement a method `add_instance_method` that binds a new instance method to the class. This method should take the following parameters: - `method_name`: The name of the method to be added. - `func`: A callable (function) that will be bound as an instance method to the class. 3. Implement a method `add_method` that binds a new method to the class. This method should take the following parameters: - `method_name`: The name of the method to be added. - `func`: A callable (function) that will be bound as a method to the class. 4. Implement a method `call_method` that dynamically calls an instance method or method. This method should take the following parameters: - `method_name`: The name of the method to be called. - Additional arguments and keyword arguments that need to be passed to the method. # Example: ```python class Student: def __init__(self, name, grade): # Initialize name and grade def add_instance_method(self, method_name, func): # Add instance method dynamically def add_method(self, method_name, func): # Add method dynamically def call_method(self, method_name, *args, **kwargs): # Call a specific method dynamically # Implementation Example # Define a function to be added def get_grade(self): return self.grade # Create a Student object student = Student(name=\'John\', grade=90) # Add instance method `get_grade` student.add_instance_method(\'get_grade\', get_grade) # Call the dynamically added instance method result = student.call_method(\'get_grade\') print(result) # Output: 90 # Define another function to be added def set_grade(self, new_grade): self.grade = new_grade # Add method `set_grade` student.add_method(\'set_grade\', set_grade) # Call the dynamically added method student.call_method(\'set_grade\', 95) print(student.grade) # Output: 95 ``` # Constraints: - Ensure that only valid callables are added as methods. - Attempting to call a method that does not exist should raise an `AttributeError`. - The solutions should handle edge cases such as binding methods with the same name and re-binding existing methods. # Evaluation: - Correctness: The solution must correctly implement dynamic binding and calling of methods. - Performance: The solution should efficiently manage memory and computation. - Code readability and documentation: The implemented code should be easy to read and well-documented.","solution":"class Student: def __init__(self, name, grade): self.name = name self.grade = grade self._methods = {} def add_instance_method(self, method_name, func): if not callable(func): raise ValueError(f\\"{func} is not callable\\") self._methods[method_name] = func.__get__(self) def add_method(self, method_name, func): if not callable(func): raise ValueError(f\\"{func} is not callable\\") setattr(self, method_name, func.__get__(self, self.__class__)) def call_method(self, method_name, *args, **kwargs): if method_name in self._methods: return self._methods[method_name](*args, **kwargs) elif hasattr(self, method_name): method = getattr(self, method_name) return method(*args, **kwargs) else: raise AttributeError(f\\"Method {method_name} does not exist\\")"},{"question":"# PyTorch `torch.func` Assessment Objective Implement and utilize the composable function transforms from the `torch.func` module to solve a mathematical problem involving gradients and batching. Task Given a multi-variable function defined as `f(x, y) = x^2 + y^2`, you are to perform the following tasks: 1. Implement a function `f` in PyTorch that computes the value of `f(x, y)` for given tensors `x` and `y`. 2. Implement a function `compute_gradients` that uses `torch.func.grad` to compute the gradient of `f` with respect to its inputs. 3. Implement a function `batched_gradients` that uses `torch.func.vmap` to compute the gradients of `f` for a batch of input pairs `(x, y)`. Input and Output Formats - Input: - For `compute_gradients(f, x, y)`: - `f`: The function to compute gradients for. - `x`: A tensor representing the first input variable. - `y`: A tensor representing the second input variable. - For `batched_gradients(f, x_batch, y_batch)`: - `f`: The function to compute gradients for. - `x_batch`: A tensor representing multiple instances of the first input variable, with shape `(batch_size, )`. - `y_batch`: A tensor representing multiple instances of the second input variable, with shape `(batch_size, )`. - Output: - For `compute_gradients(f, x, y)`: - A tuple of gradients with respect to `x` and `y`, each as a tensor. - For `batched_gradients(f, x_batch, y_batch)`: - A tuple of batched gradients with respect to `x_batch` and `y_batch`, each as a tensor with shape `(batch_size, )`. Constraints - Assume the inputs will always be tensors of appropriate shapes. - You may assume a fixed batch size of 4 for testing purposes. Performance Requirements - The implementations should leverage the `torch.func` module\'s capabilities for efficient computation. - Ensure the gradient computations reflect the correct mathematical derivatives. Example ```python import torch # Define the function f def f(x, y): return x**2 + y**2 # Function to compute gradients def compute_gradients(f, x, y): from torch.func import grad df_dx, df_dy = grad(f, argnums=(0, 1))(x, y) return df_dx, df_dy # Function to compute batched gradients def batched_gradients(f, x_batch, y_batch): from torch.func import vmap batched_grad_fn = vmap(grad(f, argnums=(0, 1))) gradients = batched_grad_fn(x_batch, y_batch) return gradients # Example usage x = torch.tensor(1.0, requires_grad=True) y = torch.tensor(2.0, requires_grad=True) grad_x, grad_y = compute_gradients(f, x, y) print(grad_x, grad_y) # Expected output: tensor(2.) tensor(4.) x_batch = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True) y_batch = torch.tensor([2.0, 3.0, 4.0, 5.0], requires_grad=True) grad_batch_x, grad_batch_y = batched_gradients(f, x_batch, y_batch) print(grad_batch_x, grad_batch_y) # Expected output: tensor([2., 4., 6., 8.]) tensor([4., 6., 8., 10.]) ``` Implement the functions according to the descriptions and ensure they pass the example usage provided.","solution":"import torch from torch.func import grad, vmap # Define the function f as per the problem statement def f(x, y): return x**2 + y**2 # Function to compute gradients def compute_gradients(f, x, y): df_dx, df_dy = grad(f, argnums=(0, 1))(x, y) return df_dx, df_dy # Function to compute batched gradients def batched_gradients(f, x_batch, y_batch): batched_grad_fn = vmap(grad(f, argnums=(0, 1))) gradients = batched_grad_fn(x_batch, y_batch) return gradients # Example usage for clarification # x = torch.tensor(1.0, requires_grad=True) # y = torch.tensor(2.0, requires_grad=True) # grad_x, grad_y = compute_gradients(f, x, y) # print(grad_x, grad_y) # Expected output: tensor(2.) tensor(4.) # x_batch = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True) # y_batch = torch.tensor([2.0, 3.0, 4.0, 5.0], requires_grad=True) # grad_batch_x, grad_batch_y = batched_gradients(f, x_batch, y_batch) # print(grad_batch_x, grad_batch_y) # Expected output: tensor([2., 4., 6., 8.]) tensor([4., 6., 8., 10.])"},{"question":"**Title: Implementing Custom Collections Using Protocols** **Objective:** Implement a custom class that behaves like a Python list and dictionary combined, demonstrating the use of the Sequence Protocol and the Mapping Protocol. **Problem Statement:** You need to create a custom collection class named `CustomCollection` that mimics the behavior of both a list and a dictionary. Specifically, the `CustomCollection` class should support: 1. Sequence operations (indexing, slicing) like a list. 2. Mapping operations (key-value pairs) like a dictionary. To achieve this, your class should: - Support sequence protocol methods such as `__getitem__`, `__setitem__`, `__len__`, and `__delitem__`. - Support mapping protocol methods such as `__getitem__`, `__setitem__`, `__len__`, and `__delitem__`. # Requirements: 1. **Initialization:** - The constructor should initialize the sequence and mapping data structures. 2. **Sequence-like behavior:** - Implement the `__getitem__`, `__setitem__`, `__len__`, and `__delitem__` methods to allow indexing and slicing similar to a list. - Example: `collection[0]` should return the first item, and `collection[1:3]` should return a slice. 3. **Mapping-like behavior:** - Implement the `__getitem__`, `__setitem__`, `__len__`, and `__delitem__` methods for key-value pairs handling. - Example: `collection[\'key\']` should return the value associated with \'key\'. 4. **Data Structures:** - Use a list to store sequence data. - Use a dictionary to store mapping data. # Input and Output Formats: - The input will encompass initializing the class and performing operations such as setting, getting, and deleting elements, both for sequences and mappings. - The output should be based on the operations performed and should adhere to the expected behavior of lists and dictionaries. # Constraints: - Ensure the methods handle edge cases such as incorrect types for keys, indexing errors, etc. - Both sequence and mapping operations should be efficiently handled. # Example Usage: ```python collection = CustomCollection() # Sequence-like operations collection[0] = \'a\' collection[1] = \'b\' print(collection[0]) # Output: \'a\' print(collection[1:3]) # Output: [\'b\'] print(len(collection)) # Output: 2 del collection[1] print(len(collection)) # Output: 1 # Mapping-like operations collection[\'name\'] = \'Alice\' print(collection[\'name\']) # Output: \'Alice\' print(len(collection)) # Output: 2 (sequence and mapping items) del collection[\'name\'] print(len(collection)) # Output: 1 ``` # Implementation: Implement the `CustomCollection` class below. Ensure you follow the outlined requirements and constraints. ```python class CustomCollection: def __init__(self): self.seq = [] self.mapping = {} # Implement sequence protocol methods def __getitem__(self, index): if isinstance(index, int) or isinstance(index, slice): return self.seq[index] elif isinstance(index, str): return self.mapping[index] else: raise TypeError(\\"Invalid argument type.\\") def __setitem__(self, index, value): if isinstance(index, int) or isinstance(index, slice): self.seq[index] = value elif isinstance(index, str): self.mapping[index] = value else: raise TypeError(\\"Invalid argument type.\\") def __len__(self): return len(self.seq) + len(self.mapping) def __delitem__(self, index): if isinstance(index, int) or isinstance(index, slice): del self.seq[index] elif isinstance(index, str): del self.mapping[index] else: raise TypeError(\\"Invalid argument type.\\") # Implement other necessary methods as needed. # Example use-case and testing collection = CustomCollection() # Add your test cases here to validate your implementation ``` **Note:** Ensure your implementation handles edge cases and provides the correct output for the given operations. êµ¬í˜„ì„ ì™„ë£Œí•œ í›„ì—ëŠ” ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì¶”ê°€í•˜ì—¬ ê¸°ëŠ¥ì„ ê²€ì¦í•˜ì„¸ìš”.","solution":"class CustomCollection: def __init__(self): self.seq = [] self.mapping = {} def __getitem__(self, index): if isinstance(index, int) or isinstance(index, slice): return self.seq[index] elif isinstance(index, str): return self.mapping[index] else: raise TypeError(\\"Invalid argument type. Must be int, slice, or string.\\") def __setitem__(self, index, value): if isinstance(index, int): # Ensure the list can be indexed at this position if index >= len(self.seq): self.seq.extend([None] * (index - len(self.seq) + 1)) self.seq[index] = value elif isinstance(index, slice): self.seq[index] = value elif isinstance(index, str): self.mapping[index] = value else: raise TypeError(\\"Invalid argument type. Must be int, slice, or string.\\") def __len__(self): return len(self.seq) + len(self.mapping) def __delitem__(self, index): if isinstance(index, int): del self.seq[index] elif isinstance(index, slice): # Delete the slice elements from the list self.seq[index] = [] elif isinstance(index, str): del self.mapping[index] else: raise TypeError(\\"Invalid argument type. Must be int, slice, or string.\\")"},{"question":"**Question: Optimizing PyTorch Model Performance with Persistent Algorithm** You are tasked with implementing a function that checks whether a given set of conditions are satisfied for applying the persistent algorithm in a PyTorch model to improve performance. # Function Signature ```python def can_use_persistent_algorithm(input_tensor: torch.Tensor, use_cudnn: bool, gpu_type: str) -> bool: pass ``` # Parameters - `input_tensor`: A `torch.Tensor` representing the input data. - `use_cudnn`: A boolean indicating whether cuDNN is enabled. - `gpu_type`: A string representing the type of GPU being used, e.g., `\'V100\'`, `\'T4\'`. # Expected Output The function should return `True` if the following conditions are satisfied; otherwise, return `False`: 1. cuDNN (`use_cudnn`) is enabled. 2. `input_tensor` is on a GPU (`input_tensor.is_cuda`). 3. `input_tensor` has a dtype of `torch.float16`. 4. The GPU type is `\'V100\'` (case-sensitive). 5. The input tensor is not in `PackedSequence` format. # Constraints - Use the PyTorch library only. - Assume the GPU type is always provided as a string. - The function should work efficiently, without unnecessary computations. # Example ```python import torch # Example 1: input_tensor = torch.randn(5, 10, dtype=torch.float16, device=\'cuda\') use_cudnn = True gpu_type = \'V100\' print(can_use_persistent_algorithm(input_tensor, use_cudnn, gpu_type)) # Output should be: True # Example 2: input_tensor = torch.randn(5, 10, dtype=torch.float32, device=\'cuda\') use_cudnn = True gpu_type = \'V100\' print(can_use_persistent_algorithm(input_tensor, use_cudnn, gpu_type)) # Output should be: False (dtype is not float16) ``` # Notes - You do not need to worry about `PackedSequence` format checking in this problem, assume all input tensors are not in `PackedSequence` format, and focus on the other conditions. - Be mindful of tensor properties like device (CPU or GPU), dtype, and others to correctly implement your solution.","solution":"import torch def can_use_persistent_algorithm(input_tensor: torch.Tensor, use_cudnn: bool, gpu_type: str) -> bool: Checks whether the conditions are satisfied for applying the persistent algorithm. Parameters: - input_tensor: A torch.Tensor representing the input data. - use_cudnn: A boolean indicating whether cuDNN is enabled. - gpu_type: A string representing the type of GPU being used. Returns: - True if all conditions are satisfied, False otherwise. return ( use_cudnn and input_tensor.is_cuda and input_tensor.dtype == torch.float16 and gpu_type == \'V100\' )"},{"question":"**Objective**: Implement an asyncio-based job scheduling system using the `asyncio.Queue` and `asyncio.PriorityQueue`. This system should handle different types of jobs with varying priorities and ensure that the highest priority jobs are processed first. **Question**: You are required to implement a job scheduler with the following specifications: 1. **Job Class**: - Implement a `Job` class that encapsulates the following attributes: - `priority`: An integer representing the job\'s priority (lower number indicates higher priority). - `name`: A string representing the job\'s name. - `duration`: A float representing the job\'s duration in seconds. - The `Job` class should be comparable based on its priority (e.g., using the `<` and `>` operators). 2. **Job Scheduler**: - Implement a `JobScheduler` class with the following methods: - `__init__(self)`: Initializes an `asyncio.PriorityQueue`. - `add_job(self, job: Job)`: Adds a `Job` instance to the priority queue. - `start(self, num_workers)`: Starts the job processing with the specified number of workers. Each worker should fetch jobs from the priority queue based on priority, execute them by sleeping for the job\'s duration, and then mark them as done. 3. **Main Function**: - Implement an `async def main()` function that: - Creates an instance of `JobScheduler`. - Adds a mix of at least 10 jobs with varying priorities and durations to the scheduler. - Starts the job processing with 3 workers. - Prints the total time taken to complete all jobs and the total expected time based on job durations. **Constraints**: - Use `asyncio.Queue` or `asyncio.PriorityQueue`. - Ensure all jobs are processed based on their priorities. - Handle potential errors like attempting to get from an empty queue. **Example**: ```python import asyncio class Job: def __init__(self, priority, name, duration): self.priority = priority self.name = name self.duration = duration def __lt__(self, other): return self.priority < other.priority class JobScheduler: def __init__(self): self.queue = asyncio.PriorityQueue() async def add_job(self, job): await self.queue.put(job) async def worker(self, name): while True: job = await self.queue.get() print(f\'{name} is executing job {job.name}\') await asyncio.sleep(job.duration) self.queue.task_done() async def start(self, num_workers): tasks = [] for i in range(num_workers): task = asyncio.create_task(self.worker(f\'worker-{i}\')) tasks.append(task) await self.queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) async def main(): scheduler = JobScheduler() jobs = [ Job(1, \'Job 1\', 2), Job(3, \'Job 2\', 1), Job(2, \'Job 3\', 3), Job(1, \'Job 4\', 2), Job(5, \'Job 5\', 1) ] for job in jobs: await scheduler.add_job(job) await scheduler.start(3) asyncio.run(main()) ``` Output: ```plaintext worker-0 is executing job Job 1 worker-1 is executing job Job 4 worker-2 is executing job Job 3 worker-0 is executing job Job 2 worker-1 is executing job Job 5 ``` Note: The exact output may vary based on worker scheduling. Ensure the highest priority jobs are executed first.","solution":"import asyncio class Job: def __init__(self, priority, name, duration): self.priority = priority self.name = name self.duration = duration def __lt__(self, other): return self.priority < other.priority def __repr__(self): return f\\"Job({self.priority}, {self.name}, {self.duration})\\" class JobScheduler: def __init__(self): self.queue = asyncio.PriorityQueue() async def add_job(self, job): await self.queue.put(job) async def worker(self, name): while True: job = await self.queue.get() print(f\'{name} is executing job {job.name}\') await asyncio.sleep(job.duration) self.queue.task_done() async def start(self, num_workers): tasks = [] for i in range(num_workers): task = asyncio.create_task(self.worker(f\'worker-{i}\')) tasks.append(task) await self.queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) async def main(): scheduler = JobScheduler() jobs = [ Job(1, \'Job 1\', 2), Job(3, \'Job 2\', 1), Job(2, \'Job 3\', 3), Job(1, \'Job 4\', 2), Job(5, \'Job 5\', 1), Job(2, \'Job 6\', 2), Job(4, \'Job 7\', 3), Job(1, \'Job 8\', 1), Job(3, \'Job 9\', 2), Job(2, \'Job 10\', 1) ] for job in jobs: await scheduler.add_job(job) await scheduler.start(3) # Run the main function to see the output. # asyncio.run(main())"},{"question":"# Advanced String Processing with Regular Expressions and Sequence Matching **Objective**: This question is designed to assess your understanding of advanced string processing using regular expressions and sequence matching in Python. You will need to implement a function that reads a text, finds specific patterns within the text, and computes differences between subsequences of the text. **Question**: You are given a text document from which you need to perform the following tasks: 1. **Extract all dates**: Identify all occurrences of dates in the format \\"DD-MM-YYYY\\" within the text using regular expressions. 2. **Find all unique words**: Extract and count all unique words in the document. Words are defined as sequences of alphabetic characters. 3. **Find differences between sentences**: Consider each line/paragraph as a sentence and compute the difference between successive sentences using `difflib`. Return these differences individually. Write a function `process_text(text: str) -> dict` that performs the above tasks and returns a dictionary with the keys \'dates\', \'unique_words\', and \'differences\' with corresponding values being their lists. **Function Signature**: ```python def process_text(text: str) -> dict: ``` **Input**: - `text` (str): A string containing the entire text document. **Output**: - (dict): A dictionary with the following keys and their corresponding values: - `\'dates\'`: A list of strings representing all dates found in the text. - `\'unique_words\'`: A set of strings representing all unique words found in the text. - `\'differences\'`: A list of strings each representing the difference between consecutive sentences. **Constraints**: - You may assume that the text is alphanumeric with standard punctuation. - The document contains multiple lines separated by newline characters. - Each line can be considered a sentence for computing differences. - Dates are strictly in the format DD-MM-YYYY. **Example**: ```python text = This document was created on 01-01-2020. It was last updated on 15-09-2022. The document examines the differences between two versions. Version one was created on 01-01-2020 and version two on 15-09-2022. End of document. expected_output = { \'dates\': [\'01-01-2020\', \'15-09-2022\', \'01-01-2020\', \'15-09-2022\'], \'unique_words\': {\'This\', \'document\', \'was\', \'created\', \'on\', \'It\', \'last\', \'updated\', \'The\', \'examines\', \'the\', \'differences\', \'between\', \'two\', \'versions\', \'Version\', \'one\', \'and\', \'version\', \'two\', \'End\', \'of\'}, \'differences\': [ \\"- This document was created on 01-01-2020.\\", \\"+ It was last updated on 15-09-2022.\\", \\"- It was last updated on 15-09-2022.\\", \\"+ The document examines the differences between two versions.\\", \\"- The document examines the differences between two versions.\\", \\"+ Version one was created on 01-01-2020 and version two on 15-09-2022.\\", \\"- Version one was created on 01-01-2020 and version two on 15-09-2022.\\", \\"+ End of document.\\" ] } assert process_text(text) == expected_output ``` Good luck!","solution":"import re from difflib import ndiff def process_text(text: str) -> dict: # Extract all dates date_pattern = re.compile(r\'bd{2}-d{2}-d{4}b\') dates = date_pattern.findall(text) # Find all unique words word_pattern = re.compile(r\'b[A-Za-z]+b\') words = word_pattern.findall(text) unique_words = set(words) # Find differences between sentences sentences = text.split(\'n\') differences = [] for i in range(len(sentences) - 1): diff = list(ndiff([sentences[i]], [sentences[i + 1]])) differences.extend(diff) return { \'dates\': dates, \'unique_words\': unique_words, \'differences\': differences }"},{"question":"Objective You are tasked with implementing a function in Python that reads numeric data from a file, processes it, and handles various potential exceptions appropriately. Problem Statement Implement the function `process_file(file_path: str) -> float` that reads a text file containing numeric values (one per line), calculates their average, and returns it. The function should handle the following exceptions: 1. **FileNotFoundError**: If the file specified by `file_path` does not exist, the function should raise a `ValueError` with the message `File not found.`. 2. **PermissionError**: If there are permission issues in accessing the file, the function should raise a `ValueError` with the message `Permission denied.`. 3. **ValueError**: If a line in the file cannot be converted to a float, that line should be skipped, and the function should continue processing the remaining lines. If all lines are invalid, the function should raise a `ValueError` with the message `No valid numeric data found.`. 4. If the file is empty, the function should raise a `ValueError` with the message `Empty file.`. The function should also ensure that the file is closed properly, regardless of whether an exception occurs. Input Format - `file_path (str)`: A string representing the path to the input file containing numeric values. Output Format - If successful, the function should return a float representing the average of the valid numeric values in the file. Constraints - The function should handle both integer and floating-point numbers. - You should not use any external libraries beyond Pythonâ€™s standard library. Example ```python # Assume the content of the file at \'data.txt\' is: # 10 # 20 # 30 print(process_file(\\"data.txt\\")) # Output: 20.0 # Assume the content of the file at \'invalid_data.txt\' is: # Hello # World # 20.5 print(process_file(\\"invalid_data.txt\\")) # Output: 20.5 ``` Note Make sure to include comments in your code explaining how different exceptions are handled and ensure proper usage of the `try`, `except`, `else`, and `finally` blocks where necessary.","solution":"def process_file(file_path: str) -> float: Reads a file containing numeric values (one per line), calculates their average, and returns it. Handles various exceptions appropriately. try: with open(file_path, \'r\') as file: lines = file.readlines() if not lines: raise ValueError(\\"Empty file.\\") numbers = [] for line in lines: try: number = float(line.strip()) numbers.append(number) except ValueError: # If the line cannot be converted to a float, it is skipped. continue if not numbers: raise ValueError(\\"No valid numeric data found.\\") return sum(numbers) / len(numbers) except FileNotFoundError: raise ValueError(\\"File not found.\\") except PermissionError: raise ValueError(\\"Permission denied.\\")"},{"question":"# Advanced Coding Assessment: Handling and Manipulating Email Data Objective: Implement a function to parse an email message, manipulate its content, and generate a response email message with updated information. Description: Create a Python function `process_email(original_email: str) -> str` that performs the following steps: 1. **Parse the Original Email:** - Use the `email.parser.EmailParser` to parse the provided email string into an email message object. 2. **Extract Information:** - Extract the `From`, `To`, `Subject`, and `Body` of the original email. 3. **Modify the Body:** - Append the text `\\"nnThank you for reaching out to us. We will get back to you shortly.\\"` to the body of the original email. 4. **Generate a Response Email:** - Swap the `From` and `To` fields to create a response email. - Prepend the subject with \\"Re: \\". - Set the new body as the modified body from step 3. 5. **Format and Return:** - Return the new email message as a string encoded in MIME format. Constraints: - The input `original_email` is a valid email message in string format. - Ensure that the email headers are correctly preserved and formatted in the response. - The function should handle edge cases, such as missing headers (`From`, `To`, `Subject`, `Body`). Example: ```python original_email = From: sender@example.com To: receiver@example.com Subject: Inquiry about services Hello, I would like to know more about your services. Best regards, John Doe response_email = process_email(original_email) print(response_email) ``` **Expected Output:** The expected output should be a string representing the MIME-encoded email with the subject \\"Re: Inquiry about services\\", a swapped `From` and `To` field, and the appended body text. The format should closely resemble standard email formats. Notes: - Utilize modules from the `email` package, such as `email.parser`, `email.message`, and `email.generator`. - Ensure the resulting email complies with MIME standards. Happy coding!","solution":"import email from email.parser import Parser from email.message import EmailMessage def process_email(original_email: str) -> str: parser = Parser() msg = parser.parsestr(original_email) from_email = msg[\'From\'] to_email = msg[\'To\'] subject = msg[\'Subject\'] body = msg.get_payload(decode=True).decode(\'utf-8\') modified_body = body + \\"nnThank you for reaching out to us. We will get back to you shortly.\\" response_email = EmailMessage() response_email[\'From\'] = to_email response_email[\'To\'] = from_email response_email[\'Subject\'] = \\"Re: \\" + (subject if subject else \\"\\") response_email.set_content(modified_body) return response_email.as_string()"},{"question":"# Password Hashing and Verification System Objective: Implement a system that allows users to create hashed passwords and verify them against stored hashed passwords using the `crypt` module. The goal is to create functions that leverage different cryptographic methods for hashing and to ensure the integrity and security of password verification. Task: Write a Python program that defines the following functions: 1. `hash_password(plaintext_password: str, method: crypt.METHOD_*) -> str` 2. `verify_password(plaintext_password: str, hashed_password: str) -> bool` Function Specifications: 1. `hash_password(plaintext_password: str, method: crypt.METHOD_*) -> str`: - **Input**: - `plaintext_password`: A string representing the user\'s plaintext password. - `method`: A method from `crypt.METHOD_*` (such as `crypt.METHOD_SHA512`, `crypt.METHOD_SHA256`, etc.). - **Output**: - Returns the hashed password as a string. - **Description**: - Generates a salt using `crypt.mksalt(method)`. - Hashes the `plaintext_password` using `crypt.crypt()` with the generated salt. - Returns the resulting hashed password. 2. `verify_password(plaintext_password: str, hashed_password: str) -> bool`: - **Input**: - `plaintext_password`: A string representing the user\'s plaintext password. - `hashed_password`: A string representing the previously hashed password (including the salt). - **Output**: - Returns `True` if the hashed version of `plaintext_password` matches `hashed_password`; otherwise, `False`. - **Description**: - Uses `crypt.crypt()` to hash `plaintext_password` using the salt from `hashed_password`. - Compares the resulting hash with `hashed_password` using `hmac.compare_digest()` to ensure constant-time comparison. - Returns the result of the comparison. Example: ```python import crypt from hmac import compare_digest as compare_hash def hash_password(plaintext_password: str, method: crypt.METHOD_*) -> str: salt = crypt.mksalt(method) return crypt.crypt(plaintext_password, salt) def verify_password(plaintext_password: str, hashed_password: str) -> bool: return compare_hash(crypt.crypt(plaintext_password, hashed_password), hashed_password) # Example usage: plaintext = \\"securepassword123\\" # Hash the password using the strongest available method hashed_pwd = hash_password(plaintext, crypt.METHOD_SHA512) print(f\\"Hashed Password: {hashed_pwd}\\") # Verify the password against the stored hash is_valid = verify_password(plaintext, hashed_pwd) print(f\\"Password verification: {is_valid}\\") ``` Constraints: - The plaintext password should be a non-empty string. - The `method` should be one of the available `crypt.METHOD_*` values. - Ensure secure handling of passwords by using constant-time comparison for verification. Performance Requirements: - The program should efficiently handle password hashing and verification, even for large numbers of verification checks.","solution":"import crypt from hmac import compare_digest def hash_password(plaintext_password: str, method: crypt.METHOD_CRYPT) -> str: Hashes a plaintext password using the specified cryptographic method. Parameters: plaintext_password (str): The plaintext password to hash. method (crypt.METHOD_*): The cryptographic method to use for hashing. Returns: str: The hashed password. salt = crypt.mksalt(method) return crypt.crypt(plaintext_password, salt) def verify_password(plaintext_password: str, hashed_password: str) -> bool: Verifies a plaintext password against a hashed password. Parameters: plaintext_password (str): The plaintext password to verify. hashed_password (str): The previously hashed password including the salt. Returns: bool: True if the verification is successful, False otherwise. return compare_digest(crypt.crypt(plaintext_password, hashed_password), hashed_password)"},{"question":"**Background**: You are tasked with creating an automated script for interacting with a Telnet server. The goal is to connect to the server, log in with a username and password, parse server responses for specific patterns, and perform actions based on these patterns. **Task**: Write a Python function `automate_telnet_interaction(host, port, username, password, patterns)` that interacts with a Telnet server as follows: 1. Connect to the Telnet server using the provided `host` and `port`. 2. Log in using the provided `username` and `password`. 3. Continuously read data from the server looking for specific patterns provided in the `patterns` list. 4. For each pattern, if the pattern is found in the server\'s response, execute an associated command (more details in patterns specification below). 5. Close the Telnet connection after processing all patterns. **Input**: - `host` (str): The hostname or IP address of the Telnet server. - `port` (int): The port number to connect to. - `username` (str): The username for login. - `password` (str): The password for login. - `patterns` (list of tuples): A list of tuple pairs where each tuple consists of: - A byte string pattern to search for in the server\'s response. - A byte string command to execute if the pattern is found. **Output**: - The function should print each server response after executing the associated command. - Return nothing from the function. **Example**: ```python patterns = [ (b\\"Welcome\\", b\\"command1n\\"), (b\\"Ready\\", b\\"command2n\\"), (b\\"DONE\\", b\\"exitn\\") ] automate_telnet_interaction(\\"localhost\\", 23, \\"user\\", \\"pass\\", patterns) ``` **Constraints**: - Assume that all server prompts end with a newline (`n`). - Handle potential exceptions, such as connection errors, timeouts, or login failures gracefully. - The function should not block indefinitely; consider using appropriate timeouts. **Performance requirements**: - The function should efficiently handle the sequence of operations, ensuring minimal delay between command executions. **Note**: - Use the methods and classes provided in the `telnetlib` module as explained in the documentation.","solution":"import telnetlib import time def automate_telnet_interaction(host, port, username, password, patterns): Connects to a Telnet server, logs in, and interacts based on specified patterns. :param host: The hostname or IP address of the Telnet server. :param port: The port number to connect to. :param username: The username for login. :param password: The password for login. :param patterns: A list of tuples of patterns and corresponding commands. try: # Connect to the Telnet server tn = telnetlib.Telnet(host, port, timeout=10) # Log in tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\'n\') tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\'n\') # Continuously read data from the server while True: response = tn.read_until(b\'n\') print(response.decode(\'ascii\').strip()) # Check patterns and execute commands for pattern, command in patterns: if pattern in response: tn.write(command + b\'n\') break # Check if we should exit the loop (if \\"exit\\" command is to be sent last) if b\\"exitn\\" in [cmd for _, cmd in patterns]: break tn.close() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Demonstrate your understanding of the Python `difflib` module by comparing two files and generating multiple formats of their differences for visual and programmatic analysis. Problem Statement You are given two text files, `file1.txt` and `file2.txt`. Write a Python function that reads these files and produces the following: 1. A list of differences using the `unified_diff` format. 2. A list of differences using the `context_diff` format. 3. An HTML file that shows a side-by-side comparison of the files with highlighted differences. Function Signature ```python def compare_files(file1_path: str, file2_path: str) -> None: Compares two files and produces their differences in unified and context diff formats, and creates an HTML representation of the differences. Args: - file1_path (str): The file path to the first file. - file2_path (str): The file path to the second file. Returns: - None Output: - Prints the unified and context diffs to the console. - Creates an HTML file named \'comparison.html\' that shows the side-by-side comparison. pass ``` Expected Input and Output - **Input**: The function takes two string arguments representing the file paths. - **Output**: - Prints the unified and context diffs to the console. - Creates an HTML file named `comparison.html` displaying the differences between the two files. Constraints - Ensure the function handles files with different lengths gracefully. - Assume the input files consist of plain text with each line terminated by a newline character. - Optimize for performance to handle files up to 1MB in size efficiently. Example ```python # Assuming file1.txt contains: # line 1 # line 2 # line 3 # And file2.txt contains: # line 1 # line two # line 4 compare_files(\'file1.txt\', \'file2.txt\') # Expected console output (simplified): # ---- Unified Diff ---- # --- file1.txt # +++ file2.txt # @@ -1,3 +1,3 @@ # line 1 # -line 2 # +line two # -line 3 # +line 4 # # ---- Context Diff ---- # *** file1.txt # --- file2.txt # *************** # *** 1,3 **** # line 1 # -line 2 # -line 3 # --- 1,3 ---- # line 1 # +line two # +line 4 # Expected HTML file \'comparison.html\' to be generated with highlighted differences. ``` Notes - Use the `unified_diff`, `context_diff`, and `HtmlDiff` functionalities from the `difflib` module. - The HTML output should be human-readable and clearly highlight differences between the two files. - Implement proper error handling for file reading operations.","solution":"import difflib import os def compare_files(file1_path: str, file2_path: str) -> None: Compares two files and produces their differences in unified and context diff formats, and creates an HTML representation of the differences. Args: - file1_path (str): The file path to the first file. - file2_path (str): The file path to the second file. Returns: - None Output: - Prints the unified and context diffs to the console. - Creates an HTML file named \'comparison.html\' that shows the side-by-side comparison. if not os.path.isfile(file1_path) or not os.path.isfile(file2_path): raise FileNotFoundError(\\"One or both of the specified files do not exist.\\") with open(file1_path, \'r\') as file1, open(file2_path, \'r\') as file2: file1_lines = file1.readlines() file2_lines = file2.readlines() unified_diff = list(difflib.unified_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path)) context_diff = list(difflib.context_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path)) print(\\"---- Unified Diff ----\\") for line in unified_diff: print(line, end=\'\') print(\\"n---- Context Diff ----\\") for line in context_diff: print(line, end=\'\') html_diff = difflib.HtmlDiff().make_file(file1_lines, file2_lines, fromdesc=file1_path, todesc=file2_path) with open(\'comparison.html\', \'w\') as html_file: html_file.write(html_diff)"},{"question":"**Title:** Advanced Set Operations **Objective:** Write Python functions that utilize set and frozenset operations according to the provided specifications. Your implementations should handle various constraints and corner cases as mentioned. **Instructions:** 1. **Function 1:** `create_frozenset(iterable)` - **Input:** An iterable (e.g., list, tuple, string) containing elements. - **Output:** A frozenset created from the iterable. - **Constraints:** Raise a `TypeError` if the iterable is not actually iterable or contains unhashable elements. 2. **Function 2:** `set_difference(set1, set2)` - **Input:** Two sets or frozensets. - **Output:** A set which is the difference between `set1` and `set2`. - **Constraints:** Handle the case where either input is not a set or frozenset by raising a `TypeError`. Ensure the function runs efficiently for large sets, without unnecessary conversions. 3. **Function 3:** `add_to_set(set_obj, element)` - **Input:** A set and an element. - **Output:** The set after adding the element. - **Constraints:** Raise a `TypeError` if the set_obj is not a set or the element is unhashable. Ensure the set is modified in place. 4. **Function 4:** `pop_random_element(set_obj)` - **Input:** A set. - **Output:** A tuple (element, set) where `element` is a randomly removed item from the set and `set` is the modified set. - **Constraints:** Raise a `KeyError` if the set is empty. Raise a `TypeError` if the input is not a set. Ensure the set is modified in place. **Examples:** ```python # Example for create_frozenset print(create_frozenset([1, 2, 3])) # Output: frozenset({1, 2, 3}) print(create_frozenset(\'abc\')) # Output: frozenset({\'a\', \'b\', \'c\'}) # Example for set_difference print(set_difference({1, 2, 3}, {2, 3, 4})) # Output: {1} print(set_difference(frozenset([1, 2]), {2, 3})) # Output: frozenset({1}) # Example for add_to_set s = {1, 2, 3} print(add_to_set(s, 4)) # Output: {1, 2, 3, 4} # Example for pop_random_element s = {1, 2, 3} element, modified_set = pop_random_element(s) print(element) # Output: Randomly one of {1, 2, 3} print(modified_set) # Output: Remaining elements after one removed ``` **Solution Template:** ```python def create_frozenset(iterable): # Your code here pass def set_difference(set1, set2): # Your code here pass def add_to_set(set_obj, element): # Your code here pass def pop_random_element(set_obj): # Your code here pass ``` **Note:** - Do not use any built-in Python functions or methods that replicate the provided API functions (for example, `set.difference()`, `set.add()`, etc.). - Ensure the functions handle all edge cases and include meaningful error handling as specified.","solution":"def create_frozenset(iterable): try: return frozenset(iterable) except TypeError: raise TypeError(\\"The provided iterable contains unhashable elements or is not an iterable\\") def set_difference(set1, set2): if not isinstance(set1, (set, frozenset)) or not isinstance(set2, (set, frozenset)): raise TypeError(\\"Both arguments must be sets or frozensets\\") return set1 - set2 def add_to_set(set_obj, element): if not isinstance(set_obj, set): raise TypeError(\\"The first argument should be a set\\") try: set_obj.add(element) except TypeError: raise TypeError(\\"The element is unhashable\\") return set_obj def pop_random_element(set_obj): if not isinstance(set_obj, set): raise TypeError(\\"The argument must be a set\\") if not set_obj: raise KeyError(\\"The set is empty, cannot pop an element\\") element = set_obj.pop() return element, set_obj"},{"question":"You are required to create a Python script that interacts with a POP3 server. Your script should be able to: 1. Connect to a POP3 server using either `POP3` or `POP3_SSL` depending on server configuration. 2. Authenticate using a username and password. 3. Retrieve and print the top N email headers without marking them as read. 4. If specified, filter and display only emails from a particular sender. 5. Implement exception handling for potential errors using `poplib.error_proto`. 6. Log interaction details for debugging purposes if debugging is enabled. Your script should implement a function `fetch_emails(server, port, username, password, use_ssl, sender=None, num_headers=5, debug=False)` with the following parameters: - `server`: The POP3 server address as a string. - `port`: The port to connect to, default to standard ports (110 for POP3, 995 for POP3_SSL). - `username`: The username for POP3 authentication. - `password`: The password for POP3 authentication. - `use_ssl`: A boolean indicating whether to use SSL (True) or not (False). - `sender`: An optional string for filtering emails by sender. - `num_headers`: Number of top headers to retrieve, default is 5. - `debug`: A boolean enabling or disabling debugging output. **Constraints:** - Port should be a valid integer. - `num_headers` should be a positive integer. **Example of function usage:** ```python fetch_emails(\'pop.gmail.com\', 995, \'your_username\', \'your_password\', use_ssl=True, sender=\'example@example.com\', num_headers=10, debug=True) ``` **Expected Output:** The function should print the email headers as per the criteria and handle all necessary exceptions gracefully. ```python import poplib from getpass import getpass def fetch_emails(server, port, username, password, use_ssl, sender=None, num_headers=5, debug=False): try: if use_ssl: M = poplib.POP3_SSL(server, port) else: M = poplib.POP3(server, port) if debug: M.set_debuglevel(1) # Authenticate with the server M.user(username) M.pass_(password) # Get mailbox status message_count, mailbox_size = M.stat() # Fetch and print the top N email headers for i in range(min(num_headers, message_count)): response, lines, octets = M.top(i + 1, 0) for line in lines: if sender: if f\'From: {sender}\' in line.decode(\'utf-8\'): print(line.decode(\'utf-8\')) else: print(line.decode(\'utf-8\')) M.quit() except poplib.error_proto as e: print(f\\"POP3 error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` **Note:** - Ensure you have proper credentials and server settings before running the script. - Handle sensitive information like passwords securely.","solution":"import poplib def fetch_emails(server, port, username, password, use_ssl, sender=None, num_headers=5, debug=False): Connects to a POP3 server, authenticates using the provided username and password, retrieves, and prints the top N email headers without marking them as read. Optionally filters emails from a specified sender and enables debugging if required. :param server: The POP3 server address as a string. :param port: The port to connect to, default to standard ports (110 for POP3, 995 for POP3_SSL). :param username: The username for POP3 authentication. :param password: The password for POP3 authentication. :param use_ssl: A boolean indicating whether to use SSL (True) or not (False). :param sender: (Optional) A string for filtering emails by sender. :param num_headers: Number of top headers to retrieve, default is 5. :param debug: A boolean enabling or disabling debugging output. try: if use_ssl: M = poplib.POP3_SSL(server, port) else: M = poplib.POP3(server, port) if debug: M.set_debuglevel(1) # Authenticate with the server M.user(username) M.pass_(password) # Get mailbox status message_count, _ = M.stat() # Fetch and print the top N email headers for i in range(min(num_headers, message_count)): response, lines, _ = M.top(i + 1, 0) for line in lines: decoded_line = line.decode(\'utf-8\') if sender: if f\'From: {sender}\' in decoded_line: print(decoded_line) else: print(decoded_line) M.quit() except poplib.error_proto as e: print(f\\"POP3 error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Demonstrate your understanding of Python\'s `re` module for regular expression operations by implementing a function to extract specific patterns from a given text. Problem Statement You are given a text document containing various dates in different formats. Your task is to implement a function `extract_dates` that identifies and extracts all dates from the text in the format `YYYY-MM-DD`. Function Signature ```python def extract_dates(text: str) -> list: pass ``` Input - `text`: A string representing the input text which may contain multiple dates in different formats. Output - A list of strings where each string is a date in the format `YYYY-MM-DD`. The dates should appear in the order they are found in the input text. Constraints - The function should only extract dates in the `YYYY-MM-DD` format. - Assume that the input text is well-formed and contains no dates with invalid month or day values. - Dates may be surrounded by other text and not necessarily isolated by whitespace. Examples ```python text = The project started on 2021-03-15 and the first phase was completed by 2021-04-10. The final phase began on 2022-07-01, and we expect it to be completed by 2022-12-31. Let\'s aim for a review meeting on 2023-01-05. assert extract_dates(text) == [\\"2021-03-15\\", \\"2021-04-10\\", \\"2022-07-01\\", \\"2022-12-31\\", \\"2023-01-05\\"] ``` Requirements - You must use Python\'s `re` module to identify and extract the dates. - The function should handle large input efficiently. Tips - Regular expressions are powerful tools for pattern matching. Consider using `re.findall` or similar functions to locate all instances of the date pattern in the text. Good luck!","solution":"import re def extract_dates(text: str) -> list: Extracts all dates in the format YYYY-MM-DD from the given text. Args: text (str): The input text containing various dates. Returns: list: A list of dates in the format YYYY-MM-DD in the order they are found. # Regular expression to match dates in YYYY-MM-DD format date_pattern = r\'bd{4}-d{2}-d{2}b\' # Find all matches of the pattern in the text dates = re.findall(date_pattern, text) return dates"},{"question":"# Objective You are to implement a binary classification task where you will train a classifier and tune the decision threshold to maximize the recall score using the `TunedThresholdClassifierCV` class. This is particularly important in scenarios where missing a positive class has high costs, such as medical diagnoses. # Problem Statement You are given a dataset for a binary classification problem. Your task is to: 1. Train a `LogisticRegression` model. 2. Tune the decision threshold using `TunedThresholdClassifierCV` to maximize the recall. 3. Evaluate the performance of the classifier before and after threshold tuning. 4. Report the recall and precision scores before and after tuning, as well as the optimal threshold value. # Input - CSV file path containing the dataset with features `X1, X2, ..., Xn` and binary target label `y`. # Output - Recall score before tuning. - Precision score before tuning. - Recall score after tuning. - Precision score after tuning. - Optimal threshold value. # Constraints - Use `LogisticRegression` as the base classifier. - Use internal 5-fold cross-validation for threshold tuning. # Performance Requirements - The implementation should handle a dataset with up to 10,000 samples and 100 features efficiently. # Example Suppose you are given a dataset `data.csv` with the following structure: ``` X1,X2,X3,...,Xn,y 0.2,1.4,0.5,...,0.7,0 1.3,0.4,1.2,...,0.6,1 ... ``` You need to perform the following steps: 1. Train a `LogisticRegression` model. 2. Tune the decision threshold using `TunedThresholdClassifierCV`. 3. Evaluate and compare the recall and precision scores before and after tuning the threshold. 4. Report the optimal threshold. # Implementation ```python import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score, precision_score def load_data(file_path): data = pd.read_csv(file_path) X = data.drop(columns=\'y\') y = data[\'y\'] return X, y def main(file_path): # Load the data X, y = load_data(file_path) # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the Logistic Regression model base_model = LogisticRegression() base_model.fit(X_train, y_train) # Initial evaluation y_pred_initial = base_model.predict(X_test) recall_before = recall_score(y_test, y_pred_initial) precision_before = precision_score(y_test, y_pred_initial) # Tune the threshold using TunedThresholdClassifierCV scorer = make_scorer(recall_score) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Optimal threshold optimal_threshold = tuned_model.best_threshold_ # Evaluate after tuning threshold y_pred_tuned = tuned_model.predict(X_test) recall_after = recall_score(y_test, y_pred_tuned) precision_after = precision_score(y_test, y_pred_tuned) # Print the results print(f\\"Recall before tuning: {recall_before}\\") print(f\\"Precision before tuning: {precision_before}\\") print(f\\"Recall after tuning: {recall_after}\\") print(f\\"Precision after tuning: {precision_after}\\") print(f\\"Optimal threshold: {optimal_threshold}\\") # Example usage file_path = \'path_to_your_dataset.csv\' main(file_path) ``` # Notes - Ensure that you install necessary packages: `scikit-learn`, `pandas`. - Use appropriate data preprocessing steps if needed (e.g., handling missing values, feature scaling). - For the custom scorer, `recall_score` is used, but you can modify it to suit different metrics as required.","solution":"import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score, precision_score, make_scorer from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.model_selection import cross_val_predict class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_classifier, scoring, cv=5): self.base_classifier = base_classifier self.scoring = scoring self.cv = cv self.best_threshold_ = 0.5 def fit(self, X, y): # Fit the base classifier self.base_classifier.fit(X, y) # Get probability predictions probas = cross_val_predict(self.base_classifier, X, y, cv=self.cv, method=\'predict_proba\')[:, 1] best_score = 0 best_threshold = 0.5 for threshold in [i * 0.01 for i in range(100)]: y_pred = (probas >= threshold).astype(int) score = self.scoring(y, y_pred) if score > best_score: best_score = score best_threshold = threshold self.best_threshold_ = best_threshold def predict(self, X): probas = self.base_classifier.predict_proba(X)[:, 1] return (probas >= self.best_threshold_).astype(int) def load_data(file_path): data = pd.read_csv(file_path) X = data.drop(columns=\'y\') y = data[\'y\'] return X, y def main(file_path): # Load the data X, y = load_data(file_path) # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the Logistic Regression model base_model = LogisticRegression(max_iter=1000) base_model.fit(X_train, y_train) # Initial evaluation y_pred_initial = base_model.predict(X_test) recall_before = recall_score(y_test, y_pred_initial) precision_before = precision_score(y_test, y_pred_initial) # Tune the threshold using TunedThresholdClassifierCV scorer = recall_score tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Optimal threshold optimal_threshold = tuned_model.best_threshold_ # Evaluate after tuning threshold y_pred_tuned = tuned_model.predict(X_test) recall_after = recall_score(y_test, y_pred_tuned) precision_after = precision_score(y_test, y_pred_tuned) # Return the results return recall_before, precision_before, recall_after, precision_after, optimal_threshold # Example usage # file_path = \'path_to_your_dataset.csv\' # results = main(file_path) # print(results)"},{"question":"# Advanced Python XML Parsing Assessment Objective: Write a Python program using the `xml.sax` package to parse an XML document and produce a structured summary of its content. Problem Statement: You are given an XML document containing information about a library of books. Each book contains a title, author, publication year, ISBN, and a list of genres. The task is to parse this XML document and output a summary containing the title and genres of each book in the library. Requirements: 1. Implement a SAX `ContentHandler` subclass to handle the XML parsing events. 2. Your handler should capture the required information (`title` and `genres`). 3. Use the appropriate SAX events to handle the hierarchical structure of the XML (i.e., startElement, endElement, characters). 4. Provide a summary output as specified above. Input: An XML string with the following format: ```xml <library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> <isbn>9780134034287</isbn> <genres> <genre>Programming</genre> <genre>Python</genre> </genres> </book> <book> <title>Learning XML</title> <author>Erik T. Ray</author> <year>2003</year> <isbn>9780596004200</isbn> <genres> <genre>Programming</genre> <genre>XML</genre> </genres> </book> <!-- more book entries --> </library> ``` Expected Output: A summary of the books in the following format: ``` Title: Effective Python, Genres: Programming, Python Title: Learning XML, Genres: Programming, XML ``` Constraints: - You must use the `xml.sax` package. - Assume the input XML string is correctly formatted. - You need to use only the `ContentHandler` class for this task. Performance Requirements: The solution should efficiently parse the XML content using SAX, which is suitable for large XML documents due to its event-driven nature. Example: ```python import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.currentData = \\"\\" self.title = \\"\\" self.genres = [] self.currentGenre = \\"\\" self.inGenres = False def startElement(self, tag, attributes): self.currentData = tag if tag == \\"genres\\": self.inGenres = True def endElement(self, tag): if tag == \\"book\\": print(f\\"Title: {self.title}, Genres: {\', \'.join(self.genres)}\\") self.title = \\"\\" self.genres = [] elif self.currentData == \\"genre\\" and self.inGenres: self.genres.append(self.currentGenre) self.currentGenre = \\"\\" self.currentData = \\"\\" if tag == \\"genres\\": self.inGenres = False def characters(self, content): if self.currentData == \\"title\\": self.title = content elif self.currentData == \\"genre\\" and self.inGenres: self.currentGenre += content # Create a parser object parser = xml.sax.make_parser() # Override the default ContextHandler handler = BookHandler() parser.setContentHandler(handler) xml_data = <library> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> <isbn>9780134034287</isbn> <genres> <genre>Programming</genre> <genre>Python</genre> </genres> </book> <book> <title>Learning XML</title> <author>Erik T. Ray</author> <year>2003</year> <isbn>9780596004200</isbn> <genres> <genre>Programming</genre> <genre>XML</genre> </genres> </book> </library> # Parse the XML string import io parser.parse(io.StringIO(xml_data)) ```","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.currentData = \\"\\" self.title = \\"\\" self.genres = [] self.currentGenre = \\"\\" self.inGenres = False self.books = [] def startElement(self, tag, attributes): self.currentData = tag if tag == \\"genres\\": self.inGenres = True def endElement(self, tag): if tag == \\"book\\": self.books.append((self.title, self.genres.copy())) self.title = \\"\\" self.genres = [] elif self.currentData == \\"genre\\" and self.inGenres: self.genres.append(self.currentGenre.strip()) self.currentGenre = \\"\\" self.currentData = \\"\\" if tag == \\"genres\\": self.inGenres = False def characters(self, content): if self.currentData == \\"title\\": self.title += content.strip() elif self.currentData == \\"genre\\" and self.inGenres: self.currentGenre += content def parse_books(xml_data): parser = xml.sax.make_parser() handler = BookHandler() parser.setContentHandler(handler) import io parser.parse(io.StringIO(xml_data)) return handler.books"},{"question":"Handling Missing Values with Pandas You are provided with a dataset containing financial transaction data. The dataset is in CSV format and includes the following columns: - `transaction_id` (integer): The unique identifier for each transaction. - `user_id` (integer): The user identifier associated with the transaction. - `transaction_amount` (float): The amount of the transaction. - `transaction_date` (datetime): The date and time when the transaction was made. However, the dataset has several missing values. Your task is to write a Python function using pandas to address and handle these missing values. You should implement your solution based on the steps outlined below: Task 1: Load the Data Write a function `load_data(file_path)` that: - Takes a single argument `file_path`, which is the path to the CSV file. - Returns a pandas DataFrame containing the loaded data. Task 2: Identify Missing Values Write a function `identify_missing_values(df)` that: - Takes a pandas DataFrame `df` as input. - Returns a pandas Series indicating the number of missing values in each column. Task 3: Handle Missing Values Write a function `handle_missing_values(df)` that: - Takes a pandas DataFrame `df` as input. - Fills missing `transaction_amount` values with the mean transaction amount. - Fills missing `transaction_date` values with the previous transaction date (backward fill). - Returns the updated DataFrame. Task 4: Validate No Missing Values Write a function `validate_no_missing_values(df)` that: - Takes a pandas DataFrame `df` as input. - Returns a boolean indicating whether there are any missing values left in the DataFrame. # Example Usage: ```python file_path = \\"transactions.csv\\" df = load_data(file_path) missing_values = identify_missing_values(df) print(missing_values) df = handle_missing_values(df) if validate_no_missing_values(df): print(\\"No missing values remain.\\") else: print(\\"There are still missing values.\\") ``` # Constraints: - You can assume the file path provided is always valid. - You should not use any hard-coded values in your implementation except when necessary for imputation. - Your function should handle any reasonable amount of data efficiently. # Note: Consider providing a CSV sample data file and any additional details (like expected formats and special cases) if needed to clarify the requirements for the students.","solution":"import pandas as pd def load_data(file_path): Loads the CSV data from the given file path into a pandas DataFrame. :param file_path: Path to the CSV file. :return: DataFrame containing the loaded data. return pd.read_csv(file_path) def identify_missing_values(df): Identifies the number of missing values in each column of the DataFrame. :param df: Input DataFrame. :return: Series with the count of missing values in each column. return df.isnull().sum() def handle_missing_values(df): Handles missing values in the DataFrame by filling missing \'transaction_amount\' with the mean and \'transaction_date\' with the previous date. :param df: Input DataFrame. :return: DataFrame with missing values handled. # Fill missing transaction_amount with mean df[\'transaction_amount\'] = df[\'transaction_amount\'].fillna(df[\'transaction_amount\'].mean()) # Fill missing transaction_date with backward fill df[\'transaction_date\'] = df[\'transaction_date\'].fillna(method=\'bfill\') return df def validate_no_missing_values(df): Validates that there are no missing values in the DataFrame. :param df: Input DataFrame. :return: Boolean indicating if there are no missing values. return df.isnull().sum().sum() == 0"},{"question":"# AIFF/AIFF-C Audio Data Manipulation Problem Statement You are tasked with writing a Python function that processes an AIFF/AIFF-C file, modifies the audio data, and writes the modified data to a new AIFF-C file. The specific requirement is to implement a function `amplify_audio(input_file: str, output_file: str, factor: float) -> None` that reads an AIFF/AIFF-C file, amplifies the audio by a given factor, and writes the amplified audio to a new file. Function Signature ```python def amplify_audio(input_file: str, output_file: str, factor: float) -> None: pass ``` Input 1. `input_file` (str): The path to the input AIFF/AIFF-C file. 2. `output_file` (str): The path to the output AIFF-C file. 3. `factor` (float): The factor by which to amplify the audio samples. Output The function should have no return value. The output is the creation of a new AIFF-C file with the amplified audio. Constraints - The input audio file will have a sampling rate of 44,100 frames per second, and the sample width will be 2 bytes per sample. - The factor value for amplification will be a positive float and can be greater than 1.0, which will increase the volume, or less than 1.0, which will decrease the volume. - Ensure that the output audio file preserves the original audio\'s parameters (like number of channels, sample width, etc.). - Proper exception handling for file operations should be included. Example ```python input_file = \\"example.aifc\\" output_file = \\"amplified_example.aifc\\" factor = 1.5 amplify_audio(input_file, output_file, factor) ``` This function call should read the `example.aifc` file, amplify the audio samples by a factor of 1.5, and write the resulting audio to `amplified_example.aifc`. Note - Remember to open the files in binary mode for reading and writing. - Use the `aifc` module\'s capabilities for reading and writing audio data and handling file parameters. - Ensure the output audio file does not exceed the allowable sample value range due to amplification. Implement necessary clipping if needed to maintain audio integrity.","solution":"import aifc import numpy as np def amplify_audio(input_file: str, output_file: str, factor: float) -> None: Amplifies the audio samples in an AIFF/AIFF-C file by a given factor and writes the amplified audio to a new file. Parameters: input_file (str): Path to the input AIFF/AIFF-C file. output_file (str): Path to the output AIFF-C file. factor (float): The factor by which to amplify the audio samples. try: # Open the input file for reading with aifc.open(input_file, \'rb\') as infile: # Get file parameters num_channels = infile.getnchannels() sample_width = infile.getsampwidth() num_frames = infile.getnframes() sample_rate = infile.getframerate() original_params = infile.getparams() # Read the audio frames audio_frames = infile.readframes(num_frames) # Convert audio frames to numpy array audio_data = np.frombuffer(audio_frames, dtype=np.int16) # Amplify the audio data amplified_data = audio_data * factor # Clip to the range of int16 to avoid overflow amplified_data = np.clip(amplified_data, -32768, 32767).astype(np.int16) # Convert amplified data back to bytes amplified_frames = amplified_data.tobytes() # Open the output file for writing with aifc.open(output_file, \'wb\') as outfile: # Set file parameters outfile.setparams(original_params) # Write the amplified audio frames outfile.writeframes(amplified_frames) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are required to implement a multi-threaded Echo server using the \\"socketserver\\" module in Python. The server should be able to handle multiple clients simultaneously, echoing back any messages received from clients in upper case. Requirements 1. **Server Implementation**: - Implement a class `EchoRequestHandler` subclassing `socketserver.BaseRequestHandler`. - Override the `handle()` method to receive data from the client and echo it back after converting to uppercase. - Implement the multi-threaded server class `ThreadedEchoServer` using `socketserver.ThreadingMixIn` and `socketserver.TCPServer`. 2. **Server Lifecycle**: - Start the server to listen on `localhost` and port `9999`. - Ensure the server handles requests until a shutdown signal is received. 3. **Performance**: - The server should efficiently handle at least 5 simultaneous client connections. Input and Output Formats - **Input**: - No direct input format. The server should continuously listen for incoming client connections. - **Output**: - No direct output format. The server should echo back any incoming data in uppercase to the respective client. Constraints and Limitations - You are not allowed to use any libraries other than the Python standard library. - Ensure that the server can handle the abrupt termination of a client connection without crashing. Example Implementation Below is a partial implementation outline to guide you: ```python import socketserver import threading class EchoRequestHandler(socketserver.BaseRequestHandler): def handle(self): # Implement the handling of a client\'s request pass class ThreadedEchoServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedEchoServer((HOST, PORT), EchoRequestHandler) as server: print(\\"Server running on {}:{}\\".format(HOST, PORT)) try: server.serve_forever() except KeyboardInterrupt: server.shutdown() print(\\"Server stopped.\\") ``` Submission Submit your implementation of the `EchoRequestHandler` class and any modifications to the `ThreadedEchoServer` setup to meet the requirements.","solution":"import socketserver class EchoRequestHandler(socketserver.BaseRequestHandler): def handle(self): while True: data = self.request.recv(1024) if not data: break self.request.sendall(data.upper()) class ThreadedEchoServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedEchoServer((HOST, PORT), EchoRequestHandler) as server: print(\\"Server running on {}:{}\\".format(HOST, PORT)) try: server.serve_forever() except KeyboardInterrupt: server.shutdown() print(\\"Server stopped.\\")"},{"question":"You are provided with the famous \\"tips\\" dataset that includes information about tipping behavior in a restaurant. Your task is to create a comprehensive visualization that highlights various aspects of the dataset using Seaborn. Specifically, you need to implement a function that achieves the following: 1. Create a scatter plot showing the relationship between `total_bill` and `tip`. 2. Map the `time` (Lunch/Dinner) to the colors of the points. 3. Map the `day` (Thur, Fri, Sat, Sun) to the shapes of the points. 4. Map the `size` of the points to the size of the party (variable `size`). 5. Use a specific color palette named \\"deep\\". 6. Customize the marker sizes to range between 20 and 200. 7. Normalize the `size` variable for the `hue` mapping to range between 0 and 7. 8. Ensure that a legend is displayed that shows all unique values. # Function Signature ```python def visualize_tips_data(tips: pd.DataFrame) -> None: pass ``` # Input - `tips`: A pandas DataFrame containing the tips dataset. You can assume that it has the same structure as the Seaborn \'tips\' dataset. # Output - The function should create and display a scatter plot as described above. There is no need to return any values. # Constraints - You must use Seaborn for creating the plot. # Example ```python import seaborn as sns import pandas as pd # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Call the function to visualize the tips data visualize_tips_data(tips) ``` The output should be a comprehensive scatter plot incorporating all the visual mappings and customizations specified. # Notes - Ensure that you have the required libraries installed (`seaborn`, `pandas`, `matplotlib`). - Pay attention to the parameter requirements for `sns.scatterplot` and `sns.relplot` as described in the documentation.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_tips_data(tips: pd.DataFrame) -> None: Creates a scatter plot to visualize the relationship between total_bill and tip, mapping other variables to colors, shapes, and sizes. # Normalize the size variable for the hue mapping tips[\'size_normalized\'] = (tips[\'size\'] - tips[\'size\'].min()) / (tips[\'size\'].max() - tips[\'size\'].min()) * 7 # Define marker styles based on unique days markers = {day: marker for day, marker in zip(tips[\'day\'].unique(), [\'o\', \'s\', \'D\', \'v\'])} plt.figure(figsize=(10, 6)) # Create the scatter plot using seaborn plot = sns.scatterplot( x=\'total_bill\', y=\'tip\', hue=\'time\', style=\'day\', size=\'size_normalized\', sizes=(20, 200), palette=\'deep\', markers=markers, data=tips ) plt.legend(loc=\'best\') plt.title(\'Scatter plot of Total Bill vs Tip\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show()"},{"question":"Objective This coding question is designed to assess your understanding of the `enum` module in Python, specifically focusing on creating enumerations, using advanced features, and ensuring unique values among enum members. Problem Statement You are to implement an enumeration that represents various statuses for an order in an e-commerce system. The statuses include \\"NEW\\", \\"PROCESSING\\", \\"SHIPPED\\", and \\"DELIVERED\\". Additionally, you need to ensure that no two statuses can have the same value. Furthermore, you will implement a function that takes a list of order statuses and returns a dictionary with the count of each status. Specifications 1. Define an enumeration class `OrderStatus` using the `Enum` base class. Ensure that each status is unique. 2. Implement a method `is_terminal_status` within the `OrderStatus` enum that returns `True` if the status is `SHIPPED` or `DELIVERED`, and `False` otherwise. 3. Implement a function `count_order_statuses(status_list)` that takes a list of `OrderStatus` members and returns a dictionary with the count of each status. The keys of the dictionary should be the status names, and the values should be the counts. Input - A list of `OrderStatus` members. Output - A dictionary with status names as keys and their counts as values. Example ```python from enum import Enum, unique @unique class OrderStatus(Enum): NEW = 1 PROCESSING = 2 SHIPPED = 3 DELIVERED = 4 def is_terminal_status(self): return self in {OrderStatus.SHIPPED, OrderStatus.DELIVERED} def count_order_statuses(status_list): status_count = {} for status in status_list: if status.name not in status_count: status_count[status.name] = 1 else: status_count[status.name] += 1 return status_count # Example usage: statuses = [OrderStatus.NEW, OrderStatus.PROCESSING, OrderStatus.NEW, OrderStatus.SHIPPED, OrderStatus.SHIPPED, OrderStatus.DELIVERED] print(count_order_statuses(statuses)) # Output: {\'NEW\': 2, \'PROCESSING\': 1, \'SHIPPED\': 2, \'DELIVERED\': 1} print(OrderStatus.SHIPPED.is_terminal_status()) # Output: True print(OrderStatus.NEW.is_terminal_status()) # Output: False ``` Constraints - Use the `unique` decorator to ensure unique values within `OrderStatus`. - The list passed to `count_order_statuses` will only contain valid `OrderStatus` members. Notes - This question tests your ability to use Python\'s `enum` module to define enumerations and implement methods within an enum. - Pay attention to ensuring enumeration values are unique using the `unique` decorator. - Implement helper methods within the enum class to provide additional functionality.","solution":"from enum import Enum, unique @unique class OrderStatus(Enum): NEW = 1 PROCESSING = 2 SHIPPED = 3 DELIVERED = 4 def is_terminal_status(self): return self in {OrderStatus.SHIPPED, OrderStatus.DELIVERED} def count_order_statuses(status_list): status_count = {} for status in status_list: if status.name not in status_count: status_count[status.name] = 1 else: status_count[status.name] += 1 return status_count"},{"question":"# Hashing Challenge with Python\'s hashlib Problem Statement You are tasked with implementing a secure data integrity check system using Python\'s hashlib module. Your functions will handle hashing for different data inputs and include features like updating the hash iteratively, utilizing salt for randomized hashing, and using keys for authenticated hashes. Requirements 1. **Function 1: `hash_data(data: bytes, algorithm: str) -> str`** - **Input:** A bytes-like object `data`, and a string `algorithm` denoting the desired hash algorithm (e.g., \'sha256\', \'sha512\', \'blake2b\'). - **Output:** Return the hexadecimal digest of the hashed data. - **Constraints:** - The provided algorithm must be one of the supported algorithms from `hashlib.algorithms_available`. - If the algorithm is not valid, raise a `ValueError` with an appropriate message. - **Example:** ```python assert hash_data(b\'example data\', \'sha256\') == \'3a0f4507f26b22d7c3dfc5dc7945c7026871c0ca7299a5e02e48e28b41b523b8\' ``` 2. **Function 2: `iterative_hash(data_chunks: list[bytes], algorithm: str) -> str`** - **Input:** A list of bytes-like objects `data_chunks`, and a string `algorithm`. - **Output:** Return the hexadecimal digest of the concatenated chunks of data fed incrementally. - **Constraints:** - Utilize iterative updating with the hash object as demonstrated in the documentation. - Ensure the algorithm provided is valid. - **Example:** ```python chunks = [b\'Hello\', b\' \', b\'world\'] assert iterative_hash(chunks, \'sha256\') == \'315f7c6721e7cc74cd94895d67cb51b0996398ec20af5b0e2f54b4d41a8b2374\' ``` 3. **Function 3: `secure_hash_with_salt(data: bytes, salt: Optional[bytes], algorithm: str) -> str`** - **Input:** A bytes-like object `data`, an optional bytes-like object `salt`, and a string `algorithm`. - **Output:** Return the hexadecimal digest of the salted hashed data. If salt is not provided, generate a random salt. - **Constraints:** - Ensure salt is handled as per the specifications for the chosen algorithm. - Raise a ValueError if certain algorithms (like SHAKE) which require manual length specifications are chosen. - **Example:** ```python assert secure_hash_with_salt(b\'example data\', b\'saltvalue\', \'blake2b\') == \'2ff674827e0b62a63f5426595400de0ec09b9cdf8d9c4d35e59d4e50df829d0c1340e1c4898096f2da7bd7d466badf6569cd92c55536f1d42a1c2bb3ce2a662a\' ``` 4. **Function 4: `verify_secure_hash(original_data: bytes, provided_hash: str, salt: Optional[bytes], algorithm: str) -> bool`** - **Input:** A bytes-like object `original_data`, a hexadecimal string `provided_hash`, an optional bytes-like object `salt`, and a string `algorithm`. - **Output:** Return `True` if the hash of the `original_data` matches the `provided_hash` considering the given `salt` and `algorithm`. Return `False` otherwise. - **Constraints:** - Ensure correct use of personal methods such as `update()` for hashing. - Proper handling if salt is not provided. - **Example:** ```python data = b\'example data\' salt = b\'saltvalue\' hashed = secure_hash_with_salt(data, salt, \'blake2b\') assert verify_secure_hash(data, hashed, salt, \'blake2b\') == True assert verify_secure_hash(data, hashed, None, \'blake2b\') == False ``` Notes - Be sure to handle any exceptions or errors gracefully and with meaningful error messages. - Security and performance considerations should be kept in mind while handling large data inputs and sensitive information. - This problem will require you to import the `hashlib` and other necessary modules (e.g., `os` for generating random bytes). Evaluation These functions will be tested with a variety of inputs including edge cases such as very large data, different hashing algorithms, and incremental updates. Ensure comprehensive testing and robustness of your solution.","solution":"import hashlib import os from typing import Optional, List def hash_data(data: bytes, algorithm: str) -> str: Returns the hexadecimal digest of the hashed data using the specified algorithm. if algorithm not in hashlib.algorithms_available: raise ValueError(f\\"Algorithm {algorithm} is not supported.\\") hash_obj = hashlib.new(algorithm) hash_obj.update(data) return hash_obj.hexdigest() def iterative_hash(data_chunks: List[bytes], algorithm: str) -> str: Returns the hexadecimal digest of the concatenated chunks of data fed incrementally. if algorithm not in hashlib.algorithms_available: raise ValueError(f\\"Algorithm {algorithm} is not supported.\\") hash_obj = hashlib.new(algorithm) for chunk in data_chunks: hash_obj.update(chunk) return hash_obj.hexdigest() def secure_hash_with_salt(data: bytes, salt: Optional[bytes], algorithm: str) -> str: Returns the hexadecimal digest of the salted hashed data. If salt is not provided, generates a random salt. if algorithm not in hashlib.algorithms_available: raise ValueError(f\\"Algorithm {algorithm} is not supported.\\") if salt is None: salt = os.urandom(16) hash_obj = hashlib.new(algorithm) hash_obj.update(salt) hash_obj.update(data) return hash_obj.hexdigest() def verify_secure_hash(original_data: bytes, provided_hash: str, salt: Optional[bytes], algorithm: str) -> bool: Verifies if the hash of the original_data matches the provided_hash considering the salt and algorithm. calculated_hash = secure_hash_with_salt(original_data, salt, algorithm) return calculated_hash == provided_hash"},{"question":"**Question: Implement a Comprehensive Covariance Estimation Function** **Objective**: Your task is to write a Python function that estimates the covariance matrix of a dataset using different techniques provided by scikit-learn\'s `sklearn.covariance` module. The function will take as input a dataset and a method name and output the estimated covariance matrix along with any relevant parameters used in the estimation. # Function Signature ```python def estimate_covariance(data: np.ndarray, method: str, **kwargs) -> Tuple[np.ndarray, Any]: Estimates the covariance matrix for the given dataset using the specified method. Parameters: data (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the dataset. method (str): The name of the covariance estimation method. Valid options are: - \\"empirical\\" - \\"shrunk\\" - \\"ledoit_wolf\\" - \\"oas\\" - \\"graphical_lasso\\" - \\"robust_mcd\\" kwargs (dict): Additional parameters for the specified method. Returns: Tuple[np.ndarray, Any]: A tuple containing the estimated covariance matrix and any relevant parameters or estimations used (e.g., shrinkage coefficient). pass ``` # Requirements 1. **Method: \\"empirical\\"** - Use `EmpiricalCovariance` to estimate the covariance matrix. - Example: `EmpiricalCovariance().fit(data).covariance_` 2. **Method: \\"shrunk\\"** - Use `ShrunkCovariance` to estimate the covariance matrix. - Pass an additional `shrinkage` parameter via `kwargs`. - Example: `ShrunkCovariance(shrinkage=kwargs[\'shrinkage\']).fit(data).covariance_` 3. **Method: \\"ledoit_wolf\\"** - Use `LedoitWolf` to estimate the covariance matrix and shrinkage parameter. - Example: `LedoitWolf().fit(data).covariance_` 4. **Method: \\"oas\\"** - Use `OAS` to estimate the covariance matrix and shrinkage parameter. - Example: `OAS().fit(data).covariance_` 5. **Method: \\"graphical_lasso\\"** - Use `GraphicalLasso` to estimate the precision (inverse covariance) matrix. - Pass the `alpha` parameter via `kwargs`. - Example: `GraphicalLasso(alpha=kwargs[\'alpha\']).fit(data).covariance_` 6. **Method: \\"robust_mcd\\"** - Use `MinCovDet` to estimate the robust covariance matrix. - Example: `MinCovDet().fit(data).covariance_` # Constraints: - Assume the input `data` is a 2D numpy array. - The `method` parameter will always be a valid string among the six options provided. - Additional parameters required for specific methods (like `shrinkage` for \\"shrunk\\" or `alpha` for \\"graphical_lasso\\") will be passed appropriately via `kwargs`. # Example Usage: ```python import numpy as np from sklearn.datasets import make_gaussian_quantiles # Generating synthetic data data, _ = make_gaussian_quantiles(mean=(0, 0), cov=1.0, n_samples=100, n_features=2, n_classes=2, shuffle=True, random_state=42) # Estimating covariance using Ledoit-Wolf shrinkage method cov_matrix, params = estimate_covariance(data, method=\\"ledoit_wolf\\") print(cov_matrix) print(params) ``` # Expected Output: ```python (array([[0.725, 0.230], [0.230, 0.929]]), {\'shrinkage\': 0.1}) ``` **Hints**: 1. Be mindful of cases where additional parameters may be required. 2. Utilize the corresponding scikit-learn classes and their methods based on the method selected. Implement the function keeping in mind the documentation provided and any relevant handling of input parameters.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso, MinCovDet from typing import Tuple, Any def estimate_covariance(data: np.ndarray, method: str, **kwargs) -> Tuple[np.ndarray, Any]: if method == \\"empirical\\": estimator = EmpiricalCovariance() estimator.fit(data) return estimator.covariance_, {} elif method == \\"shrunk\\": shrinkage = kwargs.get(\'shrinkage\', 0.1) # Default shrinkage value if not provided estimator = ShrunkCovariance(shrinkage=shrinkage) estimator.fit(data) return estimator.covariance_, {\\"shrinkage\\": shrinkage} elif method == \\"ledoit_wolf\\": estimator = LedoitWolf() estimator.fit(data) return estimator.covariance_, {\\"shrinkage\\": estimator.shrinkage_} elif method == \\"oas\\": estimator = OAS() estimator.fit(data) return estimator.covariance_, {\\"shrinkage\\": estimator.shrinkage_} elif method == \\"graphical_lasso\\": alpha = kwargs.get(\'alpha\', 0.01) # Default alpha value if not provided estimator = GraphicalLasso(alpha=alpha) estimator.fit(data) return estimator.covariance_, {\\"alpha\\": alpha} elif method == \\"robust_mcd\\": estimator = MinCovDet() estimator.fit(data) return estimator.covariance_, {} else: raise ValueError(\\"Unknown method provided for covariance estimation\\")"},{"question":"**Question:** You are provided with a logging configuration API from PyTorch, which allows setting different log levels and enabling specific artifacts. You are tasked to write a function `configure_logging` that accepts a list of component configurations and applies them using the `torch._logging.set_logs` API. Each configuration will be a dictionary with the following keys: - `component`: A string representing the component name (e.g., `dynamo`). - `level`: A string representing the desired log level (e.g., `DEBUG`, `INFO`, `WARN`, `ERROR`). - `artifacts`: A list of strings representing artifacts to be enabled (e.g., `[\\"graph\\", \\"guards\\"]`). Your function should: 1. Convert the log level to the appropriate `logging` module constant (e.g., `logging.DEBUG`). 2. Use `torch._logging.set_logs` to set the log level for each given component. 3. Enable the specified artifacts for the given components. **Input:** - A list of dictionaries where each dictionary contains: - `component`: str - The name of the component. - `level`: str - The logging level to set. - `artifacts`: List[str] - A list of artifacts to enable. **Output:** - None (the function should configure logging settings without returning any value). **Example:** ```python import torch._logging def configure_logging(configs): level_dict = { \\"DEBUG\\": logging.DEBUG, \\"INFO\\": logging.INFO, \\"WARN\\": logging.WARN, \\"ERROR\\": logging.ERROR, } for config in configs: component = config[\\"component\\"] level = level_dict[config[\\"level\\"].upper()] artifacts = config[\\"artifacts\\"] # Setting the log level for the component torch._logging.set_logs({component: level}) # Enabling artifacts for artifact in artifacts: torch._logging.set_logs({artifact: True}) # Example usage: configure_logging([ {\\"component\\": \\"dynamo\\", \\"level\\": \\"DEBUG\\", \\"artifacts\\": [\\"graph\\", \\"guards\\"]}, {\\"component\\": \\"aot\\", \\"level\\": \\"INFO\\", \\"artifacts\\": [\\"ddp_graphs\\", \\"aot_graphs\\"]} ]) ``` **Constraints:** - Assume that the components and artifacts provided in the input are valid. - You should handle the case-insensitivity for logging levels. **Performance requirements:** - The function should be efficient and easily understandable.","solution":"import logging import torch._logging def configure_logging(configs): Configure logging for multiple components based on the given configuration list. Args: configs (list of dict): List of logging configuration dictionaries. Each dictionary contains: - component (str): Name of the component. - level (str): Logging level as a string (\\"DEBUG\\", \\"INFO\\", \\"WARN\\", \\"ERROR\\"). - artifacts (list of str): List of artifacts to enable. level_dict = { \\"DEBUG\\": logging.DEBUG, \\"INFO\\": logging.INFO, \\"WARN\\": logging.WARN, \\"ERROR\\": logging.ERROR, } for config in configs: component = config[\\"component\\"] level = level_dict[config[\\"level\\"].upper()] artifacts = config[\\"artifacts\\"] # Setting the log level for the component torch._logging.set_logs({component: level}) # Enabling artifacts for artifact in artifacts: torch._logging.set_logs({artifact: True})"},{"question":"**Objective**: Demonstrate your understanding of data preprocessing using scikit-learn\'s preprocessing module. **Task**: 1. You are provided with a dataset in the form of a NumPy array `X` and a corresponding target array `y`. The dataset contains some marginal outliers, and you need to preprocess the data before training a model. 2. Your task is to create a preprocessing pipeline using scikit-learn that includes the following steps: - Standardize the features using `StandardScaler`. - Scale the features to lie within a given range using `MinMaxScaler`. 3. Fit a Logistic Regression model using the preprocessed data and compute its accuracy on a test set. **Instructions**: - Implement a function `preprocessing_pipeline(X_train, X_test, y_train, y_test)` that takes four arguments: the training data `X_train` and `y_train`, and the testing data `X_test` and `y_test`. - Within this function, create a pipeline that includes the two specified preprocessing steps. - Train the Logistic Regression model using this pipeline and evaluate its accuracy on the test data. **Input**: - `X_train`: A NumPy array of shape (n_samples_train, n_features), representing the training data. - `X_test`: A NumPy array of shape (n_samples_test, n_features), representing the test data. - `y_train`: A NumPy array of shape (n_samples_train,), representing the training labels. - `y_test`: A NumPy array of shape (n_samples_test,), representing the test labels. **Output**: - Return the accuracy of the Logistic Regression model on the test data as a float. **Constraints**: - Assume that the input data is numeric and contains no missing values. - Use default parameters for the `StandardScaler` and `MinMaxScaler`. **Example**: ```python import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # Generate a synthetic dataset X, y = make_classification(n_samples=500, n_features=5, random_state=42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the preprocessing and model training function def preprocessing_pipeline(X_train, X_test, y_train, y_test): from sklearn.preprocessing import StandardScaler, MinMaxScaler from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Define the preprocessing pipeline pipeline = Pipeline([ (\'std_scaler\', StandardScaler()), (\'minmax_scaler\', MinMaxScaler()) ]) # Fit the pipeline on the training data X_train_processed = pipeline.fit_transform(X_train) # Transform the test data X_test_processed = pipeline.transform(X_test) # Train a Logistic Regression model model = LogisticRegression() model.fit(X_train_processed, y_train) # Make predictions and evaluate accuracy y_pred = model.predict(X_test_processed) accuracy = accuracy_score(y_test, y_pred) return accuracy # Test the function accuracy = preprocessing_pipeline(X_train, X_test, y_train, y_test) print(f\\"Model Accuracy: {accuracy:.2f}\\") ``` **Explanation**: - The function `preprocessing_pipeline` defines a pipeline that includes `StandardScaler` and `MinMaxScaler`. - The pipeline is fitted and transformed on the training data and then applied to the test data. - The function trains a Logistic Regression model on the preprocessed training data and evaluates its accuracy on the test data.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, MinMaxScaler from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def preprocessing_pipeline(X_train, X_test, y_train, y_test): Preprocess the data using StandardScaler and MinMaxScaler, train a Logistic Regression model, and evaluate its accuracy on the test data. Parameters: X_train (numpy.ndarray): Training data. X_test (numpy.ndarray): Testing data. y_train (numpy.ndarray): Training labels. y_test (numpy.ndarray): Testing labels. Returns: float: Accuracy of the Logistic Regression model on the test data. # Define the preprocessing pipeline pipeline = Pipeline([ (\'std_scaler\', StandardScaler()), (\'minmax_scaler\', MinMaxScaler()) ]) # Fit the pipeline on the training data and transform it X_train_processed = pipeline.fit_transform(X_train) # Transform the test data X_test_processed = pipeline.transform(X_test) # Train a Logistic Regression model model = LogisticRegression() model.fit(X_train_processed, y_train) # Make predictions and evaluate accuracy y_pred = model.predict(X_test_processed) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Data Visualization with Seaborn** **Objective:** You are required to demonstrate your understanding of seaborn by performing specific tasks using a given dataset. The objective is to assess your ability to manipulate data and create insightful visualizations using seaborn\'s functions. **Dataset:** We will use the seaborn built-in dataset \\"tips\\", which contains information about the tips received by a waiter in a restaurant across various days and times. The dataset includes the following columns: `total_bill`, `tip`, `sex`, `smoker`, `day`, `time`, and `size`. **Task:** 1. Load the \\"tips\\" dataset from seaborn. 2. Convert the dataset from long-form to wide-form, where each column represents the average `total_bill` for each day. 3. Create a line plot of the wide-form dataset to visualize the average `total_bill` for different days. 4. Convert the wide-form dataset back to long-form. 5. Create a point plot in long-form to display the average `total_bill` for each combination of `day` and `time` color-coded by `sex`. 6. Save both plots as image files. **Constraints:** - When converting to wide-form, use `day` as columns. - When converting back to long-form, ensure the dataset structure fits seaborn\'s plotting functions. - Customize the plots with titles, axis labels, and legends for clarity. **Input:** None (You should load the dataset directly using seaborn). **Output:** Two image files (`wide_form_plot.png` and `long_form_plot.png`) containing the respective plots. **Requirements:** - Use seaborn and pandas for data manipulation and plotting. - Ensure plots are clear and well-labeled. - The code should be well-documented. **Code:** ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Task 1: Load the \\"tips\\" dataset from seaborn tips = sns.load_dataset(\\"tips\\") # Task 2: Convert to wide-form dataset tips_wide = tips.pivot_table(index=\\"sex\\", columns=\\"day\\", values=\\"total_bill\\", aggfunc=\'mean\') # Task 3: Create a line plot of the wide-form dataset plt.figure(figsize=(10, 6)) sns.lineplot(data=tips_wide) plt.title(\\"Average Total Bill per Day\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Average Total Bill\\") plt.savefig(\\"wide_form_plot.png\\") # Task 4: Convert back to long-form dataset tips_long = tips_wide.reset_index().melt(id_vars=\'sex\', value_name=\'total_bill\') # Task 5: Create a point plot in long-form to display average total_bill plt.figure(figsize=(10, 6)) sns.pointplot(data=tips_long, x=\'day\', y=\'total_bill\', hue=\'sex\', capsize=.2) plt.title(\\"Average Total Bill per Day and Sex\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Average Total Bill\\") plt.legend(title=\'Sex\') plt.savefig(\\"long_form_plot.png\\") ``` **Submission:** Submit your Python script containing the code and the two generated image files (`wide_form_plot.png` and `long_form_plot.png`). Ensure your code runs without errors and the plots are correctly generated and saved.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Task 1: Load the \\"tips\\" dataset from seaborn tips = sns.load_dataset(\\"tips\\") # Task 2: Convert to wide-form dataset tips_wide = tips.pivot_table(index=\\"sex\\", columns=\\"day\\", values=\\"total_bill\\", aggfunc=\'mean\') # Task 3: Create a line plot of the wide-form dataset plt.figure(figsize=(10, 6)) sns.lineplot(data=tips_wide) plt.title(\\"Average Total Bill per Day\\") plt.xlabel(\\"Sex\\") plt.ylabel(\\"Average Total Bill\\") plt.legend(title=\'Day\') plt.savefig(\\"wide_form_plot.png\\") plt.close() # Close the figure to avoid overlap in plots # Task 4: Convert back to long-form dataset tips_long = tips_wide.reset_index().melt(id_vars=\'sex\', value_name=\'total_bill\', var_name=\'day\') # Task 5: Create a point plot in long-form to display average total_bill plt.figure(figsize=(10, 6)) sns.pointplot(data=tips_long, x=\'day\', y=\'total_bill\', hue=\'sex\', capsize=.2) plt.title(\\"Average Total Bill per Day and Sex\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Average Total Bill\\") plt.legend(title=\'Sex\') plt.savefig(\\"long_form_plot.png\\") plt.close() # Close the figure to avoid overlap in plots"},{"question":"**Advanced Python Wrappers** You are to create a Python library that provides a high-level interface mimicking some functionalities of the provided C API for Python functions. Specifically, you need to implement the following functions: 1. `is_function(obj: object) -> bool`: Determine if the provided object is a function. 2. `create_function(code: str, globals: dict, qualname: str = None) -> types.FunctionType`: Create a new function object from a string of code and a globals dictionary. 3. `get_function_code(func: types.FunctionType) -> str`: Retrieve the source code of the function. 4. `get_function_globals(func: types.FunctionType) -> dict`: Retrieve the globals dictionary associated with the function. 5. `get_function_annotations(func: types.FunctionType) -> dict`: Retrieve the annotations of the function. 6. `set_function_annotations(func: types.FunctionType, annotations: dict)`: Set the annotations for a function. # Requirements: - **input/output formats**: - `is_function`: - Input: `obj` - Any Python object. - Output: `bool` - `create_function`: - Input: `code` - A string of valid Python code with a function definition. - Input: `globals` - A dictionary of global variables accessible to the function. - Input: `qualname` - Optional qualified name for the function. - Output: `types.FunctionType` - `get_function_code`: - Input: `func` - A Python function object. - Output: `str` - `get_function_globals`: - Input: `func` - A Python function object. - Output: `dict` - `get_function_annotations`: - Input: `func` - A Python function object. - Output: `dict` - `set_function_annotations`: - Input: `func` - A Python function object. - Input: `annotations` - A dictionary of annotations. - Output: None - **Constraints**: - The `code` string in `create_function` must contain a valid function definition. - `func` in all relevant functions must be a valid Python function object. - For `set_function_annotations`, the `annotations` argument must be a dictionary. # Performance: - The implementations are expected to handle typical use cases efficiently, but there are no stringent performance requirements. # Example Usage: ```python import types def example_usage(): code = \'\'\' def greet(name: str) -> str: return f\\"Hello, {name}!\\" \'\'\' globals_dict = {} # Creating a new function new_func = create_function(code, globals_dict, \\"custom.greet\\") # Check if new_func is a function print(is_function(new_func)) # Output: True # Retrieve the source code print(get_function_code(new_func)) # Output: # def greet(name: str) -> str: # return f\\"Hello, {name}!\\" # Retrieve globals print(get_function_globals(new_func)) # Output: {} # Retrieve annotations print(get_function_annotations(new_func)) # Output: {\'name\': <class \'str\'>, \'return\': <class \'str\'>} # Set new annotations new_annotations = {\'name\': str, \'return\': str, \'greet\': str} set_function_annotations(new_func, new_annotations) # Retrieve updated annotations print(get_function_annotations(new_func)) # Output: {\'name\': <class \'str\'>, \'return\': <class \'str\'>, \'greet\': <class \'str\'>} example_usage() ``` Create the necessary Python functions to fulfill the requirements outlined in this prompt.","solution":"import types def is_function(obj): Determine if the provided object is a function. Parameters: obj (object): The object to check. Returns: bool: True if the object is a function, False otherwise. return isinstance(obj, types.FunctionType) def create_function(code, globals, qualname=None): Create a new function object from a string of code and a globals dictionary. Parameters: code (str): A string of valid Python code with a function definition. globals (dict): A dictionary of global variables accessible to the function. qualname (str, optional): Optional qualified name for the function. Returns: types.FunctionType: The created function object. exec(code, globals) func = next(val for val in globals.values() if isinstance(val, types.FunctionType)) if qualname: func.__qualname__ = qualname return func def get_function_code(func): Retrieve the source code of the function. Parameters: func (types.FunctionType): A Python function object. Returns: str: The source code of the function. import inspect return inspect.getsource(func) def get_function_globals(func): Retrieve the globals dictionary associated with the function. Parameters: func (types.FunctionType): A Python function object. Returns: dict: The globals dictionary associated with the function. return func.__globals__ def get_function_annotations(func): Retrieve the annotations of the function. Parameters: func (types.FunctionType): A Python function object. Returns: dict: The annotations of the function. return func.__annotations__ def set_function_annotations(func, annotations): Set the annotations for a function. Parameters: func (types.FunctionType): A Python function object. annotations (dict): A dictionary of annotations. func.__annotations__ = annotations"},{"question":"**Problem: Implementing a Custom Ranking System** You are tasked with designing a custom ranking system for a coding competition. Your goal is to write a function that processes a list of participant submissions and returns a ranking based on each participant\'s highest score and the timestamp of their submissions. **Function Signature:** ```python def calculate_rankings(submissions: List[Tuple[int, int, datetime]]) -> List[int]: ``` **Input:** - `submissions`: A list of tuples where each tuple contains: - `participant_id` (an integer): The ID of the participant. - `score` (an integer): The score achieved by the participant in the submission. - `timestamp` (a `datetime` object): The timestamp of the submission. **Output:** - A list of participant IDs sorted by: 1. Highest score in descending order. 2. Earliest timestamp for participants with the same highest score (i.e., if two participants have the same highest score, the participant who achieved that score first will rank higher). **Example:** ```python from datetime import datetime submissions = [ (1, 100, datetime(2023, 10, 1, 12, 0)), (2, 200, datetime(2023, 10, 1, 12, 5)), (1, 150, datetime(2023, 10, 1, 13, 0)), (3, 200, datetime(2023, 10, 1, 11, 0)), (2, 250, datetime(2023, 10, 1, 14, 0)) ] # Calling the function should return a list of participant ids sorted by the # highest score and then by earliest timestamp for the highest score. assert calculate_rankings(submissions) == [2, 3, 1] ``` **Constraints:** - Each participant\'s ID is unique in the context of a single submission but may appear multiple times in the submissions list. - Submissions list can have up to `1000` entries. - Scores are non-negative integers and can go up to `1000`. - Timestamps are provided in a valid `datetime` format. **Requirements:** - Your solution should make efficient use of collections, sorting, and handle edge cases appropriately. - Ensure any use of standard libraries is well-documented and justified.","solution":"from typing import List, Tuple from datetime import datetime def calculate_rankings(submissions: List[Tuple[int, int, datetime]]) -> List[int]: # Dictionary to store the highest score and the earliest timestamp for each participant scores = {} for participant_id, score, timestamp in submissions: if participant_id not in scores: scores[participant_id] = (score, timestamp) else: current_score, current_timestamp = scores[participant_id] if score > current_score or (score == current_score and timestamp < current_timestamp): scores[participant_id] = (score, timestamp) # Sorting participants by highest score (descending) and earliest timestamp (ascending) ranked_participants = sorted(scores.keys(), key=lambda pid: (-scores[pid][0], scores[pid][1])) return ranked_participants"},{"question":"Objective Your task is to demonstrate your understanding of Seaborn\'s `objects` module by creating a customized visualization. You will load a dataset, manipulate it, and use Seaborn to plot a specific type of graph. Problem Statement Using the `seaborn` library, load the \\"planets\\" dataset and create a plot using the `so.Band` feature to show data intervals. Requirements: 1. **Dataset Loading**: Load the \\"planets\\" dataset provided by Seaborn. 2. **Data Manipulation**: - Filter the dataset to include only rows where the \\"method\\" column is \\"Radial Velocity\\". - Create a new column \\"log_distance\\" by applying the logarithm (base 10) to the \\"distance\\" column. 3. **Plotting**: - Create a plot where the x-axis represents the \\"year\\" of discovery, and the y-axis represents the \\"log_distance\\". - Use the `so.Band` feature to show an interval of `log_distance` values for each year. - Customize the band to have an alpha value of 0.3 and edgewidth of 2. - Add a line to represent the average `log_distance` per year. 4. **Function Implementation**: Implement the function `create_planet_band_plot`: ```python def create_planet_band_plot(): # Your code here ``` Expected Output: - The function does not take any input and does not return any output. - The function should create and display the plot as described. Constraints: - You should not use any other libraries for plotting other than Seaborn. Here is a summary of what you need to accomplish: 1. Load the dataset \\"planets\\". 2. Filter and manipulate the data. 3. Create a customized plot with a band and line using Seaborn. Example: An illustration of the required plot might look as follows: - The x-axis labeled with the years of discovery. - The y-axis labeled with the log-transformed distance. - A band showing the interval of distances for each year. - A line indicating the average distance per year. Make sure your implementation is tidy, well-documented, and meets the requirements specified. Good luck!","solution":"import seaborn as sns import seaborn.objects as so import numpy as np import matplotlib.pyplot as plt def create_planet_band_plot(): # Load the dataset planets = sns.load_dataset(\\"planets\\") # Filter the dataset to include only \\"Radial Velocity\\" method filtered_planets = planets[planets[\'method\'] == \'Radial Velocity\'] # Create a new column \'log_distance\' filtered_planets[\'log_distance\'] = np.log10(filtered_planets[\'distance\']) # Plotting p = (so.Plot(filtered_planets, x=\\"year\\", y=\\"log_distance\\") .add(so.Band(color=\\"blue\\", alpha=0.3, edgewidth=2)) .add(so.Line(color=\\"black\\")) ) p.show()"},{"question":"# URL Retrieval and Parsing with urllib Objective: You need to demonstrate your understanding of URL handling using the `urllib` package in Python. This task will involve retrieving content from a URL, handling potential errors, and parsing components of the URL. Task: Write a Python function called `fetch_and_parse_url(url)` that takes a single argument: - `url`: A string representing a valid URL. This function should perform the following: 1. Use `urllib.request` to fetch the content from the specified URL. 2. Handle any exceptions that may be raised during the process of fetching the content and return an appropriate error message. 3. Use `urllib.parse` to parse the provided URL into its components. 4. Return a dictionary containing: - The content retrieved from the URL. - The following components of the parsed URL: - Scheme - Network location - Path - Parameters - Query - Fragment The function\'s return value should be a dictionary with the following structure: ```python { \\"content\\": <content as string>, \\"scheme\\": <scheme>, \\"netloc\\": <network location>, \\"path\\": <path>, \\"params\\": <parameters>, \\"query\\": <query>, \\"fragment\\": <fragment> } ``` Constraints: - Assume the URL is always a string and is expected to be in a correct format. - Handle only the exceptions raised by `urllib.request` using `urllib.error`. - For simplicity, assume that the content fetched from the URL is always text-based. Example: ```python result = fetch_and_parse_url(\'http://example.com?query=python\') # Expected output: { \\"content\\": \\"<HTML content of http://example.com>\\", \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"\\", \\"params\\": \\"\\", \\"query\\": \\"query=python\\", \\"fragment\\": \\"\\" } ``` Notes: - Ensure your function handles all specified exceptions gracefully. - Consider using `urllib.parse.urlparse` for parsing the URL. - Use `urllib.request.urlopen` to fetch the URL content.","solution":"import urllib.request import urllib.error from urllib.parse import urlparse def fetch_and_parse_url(url): result = {} try: with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') result[\'content\'] = content except urllib.error.URLError as e: return {\\"error\\": str(e)} parsed_url = urlparse(url) result.update({ \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment }) return result"},{"question":"Coding Assessment Question **Objective:** Demonstrate your understanding of Python\'s importing mechanisms and metadata handling by implementing a function that utilizes `importlib.metadata` to gather and display metadata information of installed packages. # Problem Statement Implement a function called `package_metadata_summary` which accepts a list of package names as input and returns a dictionary containing metadata information for each package. # Input - A list of strings `packages` where each string is the name of a package installed in the current environment. # Output - A dictionary where each key is a package name from the input list, and the value is another dictionary containing the package\'s metadata. The metadata dictionary should include the following keys: \\"version\\", \\"summary\\", and \\"author\\". # Constraints - If a package from the input list is not found, it should not be included in the output dictionary. - You should use the functionalities provided by the `importlib.metadata` module. # Example Usage ```python def package_metadata_summary(packages): # Your Code Here # # Example call print(package_metadata_summary([\'numpy\', \'nonexistentpackage\'])) # Expected Output: # { # \'numpy\': { # \'version\': \'1.21.0\', # \'summary\': \'NumPy is the fundamental package for array computing with Python.\', # \'author\': \'Travis E. Oliphant et al.\' # } # } ``` # Guidelines - Use `importlib.metadata` module to retrieve the metadata information. - Ensure your function handles missing or non-existent packages gracefully. - Optimize your solution for readability and performance. You might find the `importlib.metadata.version`, `importlib.metadata.metadata`, or similar functions useful to accomplish this task. Refer to the documentation for additional details.","solution":"from importlib.metadata import version, metadata, PackageNotFoundError def package_metadata_summary(packages): Returns metadata summary (version, summary, author) for the given list of packages. Parameters: packages (list): List of package names as strings. Returns: dict: Dictionary containing metadata summary for each package. result = {} for package in packages: try: package_version = version(package) package_metadata = metadata(package) result[package] = { \'version\': package_version, \'summary\': package_metadata.get(\'Summary\', \'No summary available\'), \'author\': package_metadata.get(\'Author\', \'Unknown author\') } except PackageNotFoundError: # Skip the package if it\'s not found continue return result"},{"question":"Objective: You are required to implement Python functions to mimic certain tuple and struct sequence C-API functions provided in the `python310` documentation. The goal is to demonstrate your understanding of tuple manipulations and struct sequence creation in Python. Task: Create a module with the following functions: 1. **is_tuple(obj)** - **Input**: A single argument `obj`. - **Output**: Return `True` if `obj` is a tuple, otherwise `False`. 2. **create_tuple(*args)** - **Input**: Variable number of arguments `*args`. - **Output**: Return a new tuple containing all arguments. 3. **tuple_size(t)** - **Input**: A tuple `t`. - **Output**: Return the size of the tuple `t`. - **Constraints**: Raise a `TypeError` if `t` is not a tuple. 4. **get_tuple_item(t, pos)** - **Input**: A tuple `t` and an integer `pos`. - **Output**: Return the item at position `pos` in tuple `t`. - **Constraints**: Raise an `IndexError` if `pos` is out of bounds, and a `TypeError` if `t` is not a tuple. 5. **define_struct_sequence(type_name, fields)** - **Input**: A string `type_name` representing the name of the struct sequence, and a list of strings `fields` representing the names of the fields. - **Output**: Return a new struct sequence type. - **Constraints**: Raise a `ValueError` if `fields` is not a list of strings. 6. **create_struct_instance(struct_type, *args)** - **Input**: A struct sequence type `struct_type` and a variable number of arguments `*args`. - **Output**: Return an instance of `struct_type` with the fields initialized to `args`. - **Constraints**: Raise a `TypeError` if the number of `args` does not match the number of fields defined in `struct_type`. Example: ```python # Example usage: assert is_tuple((1, 2, 3)) == True assert is_tuple([1, 2, 3]) == False assert create_tuple(1, 2, 3) == (1, 2, 3) t = (1, 2, 3) assert tuple_size(t) == 3 assert get_tuple_item(t, 1) == 2 # Defining a struct sequence MyStructType = define_struct_sequence(\\"MyStruct\\", [\\"field1\\", \\"field2\\"]) instance = create_struct_instance(MyStructType, 10, 20) assert instance.field1 == 10 assert instance.field2 == 20 ``` Notes: 1. The struct sequence should mimic Python\'s namedtuple functionality. 2. Implement robust error handling to mimic the constraints mentioned. Implement these functions in a Python module named `py310_tuples.py`.","solution":"from collections import namedtuple def is_tuple(obj): Returns True if `obj` is a tuple, else False. return isinstance(obj, tuple) def create_tuple(*args): Returns a new tuple containing all provided arguments. return tuple(args) def tuple_size(t): Returns the size of the tuple `t`. Raises `TypeError` if `t` is not a tuple. if not isinstance(t, tuple): raise TypeError(\\"Provided argument is not a tuple.\\") return len(t) def get_tuple_item(t, pos): Returns the item at position `pos` in tuple `t`. Raises `IndexError` if `pos` is out of bounds, and `TypeError` if `t` is not a tuple. if not isinstance(t, tuple): raise TypeError(\\"Provided argument is not a tuple.\\") if not (0 <= pos < len(t)): raise IndexError(\\"Tuple index out of range.\\") return t[pos] def define_struct_sequence(type_name, fields): Returns a new struct sequence type with the given `type_name` and `fields`. Raises `ValueError` if `fields` is not a list of strings. if not isinstance(fields, list) or not all(isinstance(f, str) for f in fields): raise ValueError(\\"Fields must be a list of strings.\\") return namedtuple(type_name, fields) def create_struct_instance(struct_type, *args): Returns an instance of `struct_type` with fields initialized to `args`. Raises `TypeError` if the number of `args` does not match the number of fields in `struct_type`. if not hasattr(struct_type, \'_fields\') or len(struct_type._fields) != len(args): raise TypeError(\\"Number of arguments does not match the number of fields in the struct sequence.\\") return struct_type(*args)"},{"question":"# Python Shelve: Personal Library Catalog **Problem Statement:** You have been tasked with creating a personal library catalog system using Python\'s `shelve` module. The system should allow users to add, retrieve, update, and delete information about books in their library. Each book in the catalog should have attributes such as title, author, year of publication, and genre. Implement a set of functions to manage the library catalog. Use the `shelve` module to store the data persistently. Functions to Implement: 1. **add_book(catalog_file: str, title: str, author: str, year: int, genre: str) -> None** - Adds a new book to the catalog. - If a book with the same title already exists, update its information. 2. **get_book(catalog_file: str, title: str) -> dict** - Retrieves the details of a book by its title. - If the book does not exist, return an empty JSON object. 3. **update_book(catalog_file: str, title: str, author: str, year: int, genre: str) -> bool** - Updates the information of an existing book. - If the book does not exist, return `False`; otherwise, return `True`. 4. **delete_book(catalog_file: str, title: str) -> bool** - Deletes a book from the catalog by its title. - If the book does not exist, return `False`; otherwise, return `True`. 5. **list_books(catalog_file: str) -> list** - Returns a list of all titles in the catalog. Input and Output Specifications: - *catalog_file (str):* The filename for the catalog database. - For `add_book` and `update_book`: *title (str), author (str), year (int), genre (str)* - For `get_book` and `delete_book`: *title (str)* - Return types for each function: - `add_book`: `None` - `get_book`: `dict` - `update_book`: `bool` - `delete_book`: `bool` - `list_books`: `list[str]` Example Usage: ```python # Add books to catalog add_book(\'library.db\', \'To Kill a Mockingbird\', \'Harper Lee\', 1960, \'Fiction\') add_book(\'library.db\', \'1984\', \'George Orwell\', 1949, \'Dystopian\') # Retrieve details of a book book = get_book(\'library.db\', \'1984\') print(book) # Output: {\'title\': \'1984\', \'author\': \'George Orwell\', \'year\': 1949, \'genre\': \'Dystopian\'} # Update details of a book success = update_book(\'library.db\', \'1984\', \'George Orwell\', 1948, \'Dystopian\') print(success) # Output: True # List all books books = list_books(\'library.db\') print(books) # Output: [\'To Kill a Mockingbird\', \'1984\'] # Delete a book success = delete_book(\'library.db\', \'To Kill a Mockingbird\') print(success) # Output: True # Try to retrieve a deleted book book = get_book(\'library.db\', \'To Kill a Mockingbird\') print(book) # Output: {} ``` Constraints: - Assume well-formed input. - You must handle all file operations correctly to ensure data persistence. - Use `shelve.open` with appropriate flag settings to achieve expected behavior. Implement these functions using the information provided in the documentation to ensure data is stored and retrieved reliably.","solution":"import shelve def add_book(catalog_file: str, title: str, author: str, year: int, genre: str) -> None: with shelve.open(catalog_file) as catalog: catalog[title] = { \'title\': title, \'author\': author, \'year\': year, \'genre\': genre } def get_book(catalog_file: str, title: str) -> dict: with shelve.open(catalog_file) as catalog: return catalog.get(title, {}) def update_book(catalog_file: str, title: str, author: str, year: int, genre: str) -> bool: with shelve.open(catalog_file) as catalog: if title in catalog: catalog[title] = { \'title\': title, \'author\': author, \'year\': year, \'genre\': genre } return True return False def delete_book(catalog_file: str, title: str) -> bool: with shelve.open(catalog_file) as catalog: if title in catalog: del catalog[title] return True return False def list_books(catalog_file: str) -> list: with shelve.open(catalog_file) as catalog: return list(catalog.keys())"},{"question":"Objective To assess your understanding of Abstract Base Classes (ABCs) in Python, including the creation of ABCs, the use of abstract methods, subclassing, and virtual subclass registration. Problem Statement 1. Define an abstract base class `Shape` with the following specifications: - It should be derived from `ABC`. - An abstract method `area()` that returns the area of the shape. - An abstract method `perimeter()` that returns the perimeter of the shape. - A class method `num_sides()` that should be abstract and return the number of sides of the shape. 2. Define a concrete subclass `Rectangle` of `Shape` with the following specifications: - Constructor initializes the length and width of the rectangle. - Overrides the `area()` method to return the area of the rectangle. - Overrides the `perimeter()` method to return the perimeter of the rectangle. - Overrides the `num_sides()` method to return the number of sides of a rectangle. 3. Define another concrete subclass `Circle` of `Shape` with the following specifications: - Constructor initializes the radius of the circle. - Overrides the `area()` method to return the area of the circle. - Overrides the `perimeter()` method to return the circumference of the circle. - Overrides the `num_sides()` method to return the number of sides of a circle (which is technically zero in terms of traditional polygons). 4. Register a built-in type `tuple` as a virtual subclass of `Shape`. Demonstrate that `tuple` is recognized as a subclass of `Shape`. 5. Write a function `show_cached_token_change()`: - This function should display the cache token before and after registering `tuple` as a virtual subclass of `Shape`. Constraints - You must use only the constructs and classes explained in the provided documentation (`abc` module). Additional Information - The area of a rectangle is given by `length * width`. - The perimeter of a rectangle is given by `2 * (length + width)`. - The area of a circle is given by `Ï€ * radius^2` (use `math.pi`). - The circumference of a circle is given by `2 * Ï€ * radius` (use `math.pi`). Expected Input and Output Format - No input is required from the user. - Demonstrations and assertions should be placed within your script. Example Code Implementation ```python from abc import ABC, abstractmethod, ABCMeta import math class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass @classmethod @abstractmethod def num_sides(cls): pass class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) @classmethod def num_sides(cls): return 4 class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius @classmethod def num_sides(cls): return 0 Shape.register(tuple) def show_cached_token_change(): old_token = abc.get_cache_token() Shape.register(tuple) new_token = abc.get_cache_token() print(\\"Cache token before registering tuple:\\", old_token) print(\\"Cache token after registering tuple:\\", new_token) # Assertions to demonstrate functionality rect = Rectangle(3, 4) assert rect.area() == 12 assert rect.perimeter() == 14 assert Rectangle.num_sides() == 4 circle = Circle(5) assert math.isclose(circle.area(), 78.5398, rel_tol=1e-4) assert math.isclose(circle.perimeter(), 31.4159, rel_tol=1e-4) assert Circle.num_sides() == 0 assert issubclass(tuple, Shape) == True assert isinstance((), Shape) == True show_cached_token_change() ```","solution":"from abc import ABC, abstractmethod, ABCMeta import math class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass @classmethod @abstractmethod def num_sides(cls): pass class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) @classmethod def num_sides(cls): return 4 class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius @classmethod def num_sides(cls): return 0 # Registering tuple as a virtual subclass Shape.register(tuple) def show_cached_token_change(): import abc old_token = abc.get_cache_token() Shape.register(tuple) # This is redundant but shows the cache token change new_token = abc.get_cache_token() print(\\"Cache token before registering tuple:\\", old_token) print(\\"Cache token after registering tuple:\\", new_token)"},{"question":"Objective: Write a Python function using scikit-learn to perform k-fold cross-validation with a pipeline that includes preprocessing (standardization) and a classifier (Support Vector Machine). The function should also evaluate multiple metrics: accuracy and recall. The result should include the mean and standard deviation for each metric. Function Signature: ```python def evaluate_model_with_cross_validation(X, y, k, C): Perform k-fold cross-validation with a pipeline that includes standardization and SVM. Parameters: - X: array-like of shape (n_samples, n_features), The input data. - y: array-like of shape (n_samples,), The target labels. - k: int, Number of folds in k-fold cross-validation. - C: float, Regularization parameter for the SVM classifier. Returns: - dict: A dictionary containing the mean and standard deviation of accuracy and recall. pass ``` Requirements: 1. **Pipeline Composition**: - Utilize `StandardScaler` for standardization. - Use `SVC` (Support Vector Classifier) for classification with a linear kernel and the given regularization parameter `C`. 2. **Cross-Validation**: - Implement k-fold cross-validation using the `KFold` strategy. - Use the provided `k` parameter to specify the number of folds. 3. **Evaluation Metrics**: - Compute both `accuracy` and `recall` metrics. - Return the mean and standard deviation for each metric. Constraints: - Ensure reproducibility by setting a random seed (e.g., `random_state=42`) for any random processes involved. Example: ```python from sklearn.datasets import load_iris # Load the Iris dataset X, y = load_iris(return_X_y=True) # Evaluate the model with 5-fold cross-validation and C=1.0 result = evaluate_model_with_cross_validation(X, y, k=5, C=1.0) print(result) # Expected Output: A dictionary containing the mean and standard deviation of accuracy and recall, e.g., # {\\"accuracy_mean\\": 0.97, \\"accuracy_std\\": 0.02, \\"recall_mean\\": 0.97, \\"recall_std\\": 0.02} ``` Notes: - Utilize scikit-learn\'s `Pipeline`, `StandardScaler`, `SVC`, `KFold`, and `cross_validate` modules. - Ensure your function is well-documented and follows good coding practices.","solution":"from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import KFold, cross_validate from sklearn.metrics import make_scorer, accuracy_score, recall_score import numpy as np def evaluate_model_with_cross_validation(X, y, k, C): Perform k-fold cross-validation with a pipeline that includes standardization and SVM. Parameters: - X: array-like of shape (n_samples, n_features), The input data. - y: array-like of shape (n_samples,), The target labels. - k: int, Number of folds in k-fold cross-validation. - C: float, Regularization parameter for the SVM classifier. Returns: - dict: A dictionary containing the mean and standard deviation of accuracy and recall. # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svm\', SVC(kernel=\'linear\', C=C, random_state=42)) ]) # Define the k-fold cross-validation strategy kf = KFold(n_splits=k, shuffle=True, random_state=42) # Define the scoring metrics scoring = { \'accuracy\': \'accuracy\', \'recall\': make_scorer(recall_score, average=\'macro\') # macro to handle multi-class } # Perform cross-validation results = cross_validate(pipeline, X, y, cv=kf, scoring=scoring) # Calculate means and standard deviations accuracy_mean = np.mean(results[\'test_accuracy\']) accuracy_std = np.std(results[\'test_accuracy\']) recall_mean = np.mean(results[\'test_recall\']) recall_std = np.std(results[\'test_recall\']) return { \'accuracy_mean\': accuracy_mean, \'accuracy_std\': accuracy_std, \'recall_mean\': recall_mean, \'recall_std\': recall_std }"},{"question":"**CPU Streams Management in PyTorch** In this question, you will implement a function using PyTorch\'s `torch.cpu` module to manage streams and synchronize operations on the CPU. You are required to demonstrate your understanding of stream management and synchronization using CPU streams. # Function Signature ```python import torch def manage_cpu_streams(): pass ``` # Requirements: 1. Check if the CPU is available for PyTorch operations. 2. Retrieve the current CPU device. 3. Get and print the total number of available CPU devices. 4. Create a new stream and set it as the current stream using `torch.cpu.stream`. 5. Retrieve and print the current stream to confirm it has been set correctly. 6. Use `torch.cpu.synchronize` to ensure all operations on the current device are synchronized. 7. Utilize `StreamContext` to temporarily set a different stream within a specific code block and confirm the change. # Expected Output: - Print statements confirming each step mentioned above (e.g., \\"Total number of CPU devices:\\", \\"Current Stream:\\", etc.). - Ensure exception handling for potential issues such as no CPU being available. # Constraints: - Use the `torch.cpu` module\'s functions as specified without using other modules or hardcoding values. - Ensure the function is self-contained and can be executed without additional inputs. # Performance Requirements: - Ensure the function completes within a reasonable time for basic operations as specified, given the typical performance of CPU-bound operations. Implement the `manage_cpu_streams` function to meet the above requirements.","solution":"import torch def manage_cpu_streams(): # Step 1: Check if CPU is available for PyTorch operations if not torch.device(\'cpu\'): print(\\"CPU is not available for PyTorch operations.\\") return # Step 2: Retrieve the current CPU device current_device = torch.device(\'cpu\') # Step 3: Get and print the total number of available CPU devices (usually 1) num_cpu_devices = 1 print(\\"Total number of CPU devices:\\", num_cpu_devices) # Step 4: Create a new stream and set it as the current stream using torch.cpu.stream stream1 = torch.cuda.Stream() torch.cuda.set_stream(stream1) # Step 5: Retrieve and print the current stream to confirm it has been set correctly current_stream = torch.cuda.current_stream() print(\\"Current Stream:\\", current_stream) # Step 6: Use torch.cuda.synchronize to ensure all operations on the current device are synchronized torch.cuda.synchronize() # Step 7: Utilize StreamContext to temporarily set a different stream within a specific code block and confirm the change stream2 = torch.cuda.Stream() with torch.cuda.stream(stream2): temp_stream = torch.cuda.current_stream() print(\\"Temporary Stream in Context:\\", temp_stream) # Confirm returning to original stream outside context final_stream = torch.cuda.current_stream() print(\\"Final Stream after Context:\\", final_stream) manage_cpu_streams()"},{"question":"**Question: Stock Analysis with Pandas** **Objective:** You are a data analyst tasked with analyzing stock prices using pandas. You will need to process stock price data, perform calculations, and generate insights from the data. **Requirements:** 1. **Data Loading**: - Load the stock data from a provided dictionary containing lists of closing prices for two companies (`Company_A` and `Company_B`) over a week. 2. **DataFrame Creation**: - Create a DataFrame from the loaded data, including an index representing the days of the week. 3. **Daily Percentage Change**: - Calculate the daily percentage change for each company\'s stock price and add these as new columns (`Pct_Change_A` and `Pct_Change_B`) in the DataFrame. 4. **Average and Standard Deviation**: - Calculate the mean and standard deviation of the daily closing prices for each company and add these as new columns in the DataFrame. 5. **Maximum and Minimum Closing Prices**: - Determine the maximum and minimum closing prices for each company over the week. **Constraints**: - The input dictionary follows the format: ```python stock_data = { \\"Company_A\\": [100, 101, 103, 102, 104], \\"Company_B\\": [98, 97, 99, 100, 101] } ``` - Days of the week are: [\\"Monday\\", \\"Tuesday\\", \\"Wednesday\\", \\"Thursday\\", \\"Friday\\"] **Your Task**: Implement a function `analyze_stocks` that takes no arguments and performs all the tasks outlined above. Your function should return the final DataFrame. **Function Signature**: ```python def analyze_stocks() -> pd.DataFrame: stock_data = { \\"Company_A\\": [100, 101, 103, 102, 104], \\"Company_B\\": [98, 97, 99, 100, 101] } # Code implementation here ``` **Expected Output Format**: The DataFrame should appear as follows: ``` Company_A Company_B Pct_Change_A Pct_Change_B Mean_A Mean_B Std_A Std_B Monday 100 98 NaN NaN calculated values Tuesday 101 97 calculated calculated Wednesday 103 99 calculated calculated Thursday 102 100 calculated calculated Friday 104 101 calculated calculated ``` **Performance Requirements**: - The function should run efficiently with the given dataset. **Implementation Note**: - Use pandas methods and functionalities as much as possible to ensure efficient computation and manipulation of data. **Example Call**: ```python df = analyze_stocks() print(df) ``` This should return the required DataFrame with the additional calculated columns.","solution":"import pandas as pd def analyze_stocks() -> pd.DataFrame: stock_data = { \\"Company_A\\": [100, 101, 103, 102, 104], \\"Company_B\\": [98, 97, 99, 100, 101] } days = [\\"Monday\\", \\"Tuesday\\", \\"Wednesday\\", \\"Thursday\\", \\"Friday\\"] df = pd.DataFrame(stock_data, index=days) # Calculate daily percentage change for each company df[\\"Pct_Change_A\\"] = df[\\"Company_A\\"].pct_change() * 100 df[\\"Pct_Change_B\\"] = df[\\"Company_B\\"].pct_change() * 100 # Calculate the means for the columns \'Company_A\' and \'Company_B\' and add to the dataframe mean_a = df[\\"Company_A\\"].mean() mean_b = df[\\"Company_B\\"].mean() # Calculate the standard deviations for the columns \'Company_A\' and \'Company_B\' and add to the dataframe std_a = df[\\"Company_A\\"].std() std_b = df[\\"Company_B\\"].std() # Append mean and standard deviation as columns (for each row) df[\\"Mean_A\\"] = mean_a df[\\"Mean_B\\"] = mean_b df[\\"Std_A\\"] = std_a df[\\"Std_B\\"] = std_b return df"},{"question":"Objective: Demonstrate your understanding of label preprocessing techniques using scikit-learn by implementing custom functions utilizing `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder`. Each function should transform the input label data according to the specified criteria and handle various edge cases. Problem Statement: Implement the following functions: 1. **multiclass_label_binarization**: - **Input**: A list of integers representing multiclass labels. - **Output**: A binary matrix where each row represents the binary indicator of the respective label. - **Constraints**: The labels can be any integer values, and the output matrix should have columns corresponding to all unique labels found in the input. 2. **multilabel_binarization**: - **Input**: A list of lists, where each sublist represents a set of labels for a sample. - **Output**: A binary matrix where each row represents the binary indicators of the respective set of labels. - **Constraints**: The labels can be any integer values, and the output matrix should have columns corresponding to all unique labels found across all samples. 3. **label_encoding_and_decoding**: - **Input**: A list of non-numerical labels (strings). - **Output**: - Encoded labels as integers between 0 and n_classes-1. - A list of the original labels corresponding to the encoded integers. - **Constraints**: The labels should be hashable and comparable. Note: You are allowed to use the provided transformers from scikit-learn in your solution. Function Signatures: ```python def multiclass_label_binarization(labels: List[int]) -> np.ndarray: pass def multilabel_binarization(labels: List[List[int]]) -> np.ndarray: pass def label_encoding_and_decoding(labels: List[str]) -> Tuple[np.ndarray, List[str]]: pass ``` Example Usage: ```python # Example for multiclass_label_binarization labels = [1, 2, 6, 4, 2] print(multiclass_label_binarization(labels)) # Output: # array([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 0, 1], # [0, 0, 1, 0], # [0, 1, 0, 0]]) # Example for multilabel_binarization labels = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] print(multilabel_binarization(labels)) # Output: # array([[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]]) # Example for label_encoding_and_decoding labels = [\\"tokyo\\", \\"tokyo\\", \\"paris\\"] encoded_labels, original_labels = label_encoding_and_decoding(labels) print(encoded_labels) # Output: # array([2, 2, 1]) print(original_labels) # Output: # [\'tokyo\', \'tokyo\', \'paris\'] ``` Evaluation Criteria: - Correctness: The functions should produce the expected output for different inputs. - Robustness: Handle edge cases like empty inputs and duplicate labels. - Readability: Clear, well-commented, and maintainable code. - Efficiency: Optimized for performance with larger datasets.","solution":"from typing import List, Tuple import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def multiclass_label_binarization(labels: List[int]) -> np.ndarray: Transforms a list of multiclass labels into a binary matrix. lb = LabelBinarizer() binary_matrix = lb.fit_transform(labels) return binary_matrix def multilabel_binarization(labels: List[List[int]]) -> np.ndarray: Transforms a list of lists of labels into a binary matrix. Each row represents the binary indicators of the respective set of labels. mlb = MultiLabelBinarizer() binary_matrix = mlb.fit_transform(labels) return binary_matrix def label_encoding_and_decoding(labels: List[str]) -> Tuple[np.ndarray, List[str]]: Encodes a list of non-numerical labels into integers and also returns the original labels. le = LabelEncoder() encoded_labels = le.fit_transform(labels) original_labels = le.inverse_transform(encoded_labels) return encoded_labels, original_labels.tolist()"},{"question":"<|Analysis Begin|> The provided documentation describes the base classes for SAX (Simple API for XML) handlers within the `xml.sax.handler` module. The key classes included in this module are: 1. `ContentHandler`: Main callback interface in SAX for document events. 2. `DTDHandler`: Handles DTD events, primarily unparsed entities and attributes. 3. `EntityResolver`: Basic interface for resolving entities. 4. `ErrorHandler`: Interface used for handling error and warning messages during parsing. 5. `LexicalHandler`: Interface for low-frequency events, like comments and CDATA section boundaries. The SAX API is a standard interface for event-driven XML parsing in Python. The handlers need to be implemented for the specific events of interest, allowing applications to process XML data incrementally as it is parsed. The `ContentHandler` is particularly detailed and important for document event handling, with methods like: - `setDocumentLocator` - `startDocument` and `endDocument` - `startPrefixMapping` and `endPrefixMapping` - `startElement` and `endElement` - `characters` - `ignorableWhitespace` - `processingInstruction` - `skippedEntity` The explanations provided for these methods focus on the sequence and the type of events the parser will invoke during the process. Given this information, it is possible to design an assessment question that targets understanding and implementation of one of these SAX handlers, likely `ContentHandler`, given its central importance. <|Analysis End|> <|Question Begin|> # XML Document Parsing with SAX ContentHandler Problem Statement You have been tasked with implementing an XML parser using the SAX (Simple API for XML) approach in Python. Specifically, you need to create a custom `ContentHandler` to process and store specific information from the XML content. You will use the `xml.sax` module and its `handler` submodule to achieve this. Requirements 1. **ContentHandler Implementation**: - Implement a class `MyContentHandler` that extends `xml.sax.handler.ContentHandler`. - Override the necessary methods to process an XML document. 2. **Process the Following Events**: - **startDocument**: Print \\"Document parsing started.\\" - **endDocument**: Print \\"Document parsing finished.\\" - **startElement**: Each time an element starts, store the element name and its attributes. - **endElement**: Each time an element ends, just print the end of the element. - **characters**: Collect and store the character data inside elements. 3. **Output Format**: - After parsing is complete, print out the elements encountered, their attributes, and the collected text contents for each element. 4. **Test the Implementation**: - Parse a sample XML string using your `MyContentHandler` implementation. Constraints - Use only the SAX library as provided in the `xml.sax.handler` module and standard XML parsing techniques. - Assume the XML input string is well-formed. Sample XML Input ```xml <?xml version=\\"1.0\\" ?> <note> <to>User</to> <from>Admin</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> ``` Expected Output ``` Document parsing started. Start Element: note Start Element: to End Element: to Start Element: from End Element: from Start Element: heading End Element: heading Start Element: body End Element: body End Element: note Document parsing finished. Parsed Elements and Data: - Element: note, Attributes: {}, Text: - Element: to, Attributes: {}, Text: User - Element: from, Attributes: {}, Text: Admin - Element: heading, Attributes: {}, Text: Reminder - Element: body, Attributes: {}, Text: Don\'t forget me this weekend! ``` Implementation Notes - You might need to use a list or dictionary to keep track of elements and their corresponding texts. - Make sure the text content of an element is correctly accumulated by handling multiple `characters` events. Task Implement the `MyContentHandler` class and use it to parse the provided sample XML input to achieve the expected output.","solution":"import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = None self.elements = [] self.texts = {} def startDocument(self): print(\\"Document parsing started.\\") def endDocument(self): print(\\"Document parsing finished.\\") print(\\"Parsed Elements and Data:\\") for element in self.elements: print(f\\"- Element: {element}, Attributes: {self.texts[element][\'attributes\']}, Text: {self.texts[element][\'text\']}\\") def startElement(self, name, attrs): self.current_element = name self.elements.append(name) self.texts[name] = {\'attributes\': dict(attrs), \'text\': \'\'} print(f\\"Start Element: {name}\\") def endElement(self, name): print(f\\"End Element: {name}\\") def characters(self, content): if self.current_element: self.texts[self.current_element][\'text\'] += content.strip() # Function to parse XML def parse_xml_string(xml_string): handler = MyContentHandler() xml.sax.parseString(xml_string, handler)"},{"question":"Implementing Cross-Validation with Multiple Metrics Evaluation **Objective:** Your task is to implement and evaluate a machine learning model using cross-validation. You need to demonstrate a clear understanding of how to prevent overfitting, properly use cross-validation techniques, and compute multiple performance metrics. **Problem Statement:** You are provided with the Iris dataset from `sklearn.datasets`. Your goal is to: 1. Split the dataset using stratified k-fold cross-validation. 2. Train a Support Vector Machine (SVM) classifier. 3. Evaluate the classifier using multiple metrics: precision, recall, and F1-score. 4. Return the cross-validation results including mean and standard deviation for each metric. # Input: - `kernel`: The kernel type to be used in the SVM. It can be \'linear\', \'poly\', \'rbf\', \'sigmoid\', etc. - `C`: Regularization parameter for the SVM. - `cv_splits`: Number of splits for cross-validation (e.g., 5 for 5-fold cross-validation). # Output: - A dictionary containing the mean and standard deviation of precision, recall, and F1-score across the cross-validation splits. # Constraints: - Use `StratifiedKFold` for splitting the dataset to ensure each fold has the same proportion of class labels as the entire dataset. - Ensure each metric is computed correctly and results are returned with appropriate precision. # Example: ```python from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import StratifiedKFold, cross_validate from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score def evaluate_model(kernel: str, C: float, cv_splits: int): # Load the Iris dataset X, y = load_iris(return_X_y=True) # Define the model model = SVC(kernel=kernel, C=C, random_state=42) # Define the cross-validation strategy cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42) # Define the scoring metrics scoring = { \'precision\': make_scorer(precision_score, average=\'macro\'), \'recall\': make_scorer(recall_score, average=\'macro\'), \'f1\': make_scorer(f1_score, average=\'macro\') } # Perform cross-validation results = cross_validate(model, X, y, cv=cv, scoring=scoring) # Calculate mean and standard deviation for each metric mean_precision = results[\'test_precision\'].mean() std_precision = results[\'test_precision\'].std() mean_recall = results[\'test_recall\'].mean() std_recall = results[\'test_recall\'].std() mean_f1 = results[\'test_f1\'].mean() std_f1 = results[\'test_f1\'].std() # Return the results as a dictionary return { \'mean_precision\': round(mean_precision, 4), \'std_precision\': round(std_precision, 4), \'mean_recall\': round(mean_recall, 4), \'std_recall\': round(std_recall, 4), \'mean_f1\': round(mean_f1, 4), \'std_f1\': round(std_f1, 4) } # Example invocation: print(evaluate_model(\'linear\', 1.0, 5)) ``` # Notes: - Ensure to shuffle the data before splitting to increase the robustness of the cross-validation results. - The output dictionary should provide a clear statistical summary of the precision, recall, and F1-score across the cross-validation folds. # Evaluation: Your implementation will be evaluated on: - Correct usage of `StratifiedKFold`. - Accurate computation of multiple metrics. - Proper documentation and comments explaining your code.","solution":"from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import StratifiedKFold, cross_validate from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score def evaluate_model(kernel: str, C: float, cv_splits: int): Evaluates a Support Vector Machine (SVM) classifier on the Iris dataset using stratified k-fold cross-validation and multiple metrics. Parameters: kernel (str): The kernel type to be used in the SVM. C (float): Regularization parameter for the SVM. cv_splits (int): Number of splits for cross-validation. Returns: dict: A dictionary containing the mean and standard deviation of precision, recall, and F1-score across the cross-validation splits. # Load the Iris dataset X, y = load_iris(return_X_y=True) # Define the model model = SVC(kernel=kernel, C=C, random_state=42) # Define the cross-validation strategy cv = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42) # Define the scoring metrics scoring = { \'precision\': make_scorer(precision_score, average=\'macro\'), \'recall\': make_scorer(recall_score, average=\'macro\'), \'f1\': make_scorer(f1_score, average=\'macro\') } # Perform cross-validation results = cross_validate(model, X, y, cv=cv, scoring=scoring) # Calculate mean and standard deviation for each metric mean_precision = results[\'test_precision\'].mean() std_precision = results[\'test_precision\'].std() mean_recall = results[\'test_recall\'].mean() std_recall = results[\'test_recall\'].std() mean_f1 = results[\'test_f1\'].mean() std_f1 = results[\'test_f1\'].std() # Return the results as a dictionary return { \'mean_precision\': round(mean_precision, 4), \'std_precision\': round(std_precision, 4), \'mean_recall\': round(mean_recall, 4), \'std_recall\': round(std_recall, 4), \'mean_f1\': round(mean_f1, 4), \'std_f1\': round(std_f1, 4) } # Example invocation: # print(evaluate_model(\'linear\', 1.0, 5))"},{"question":"Title: Implementing and Using AsyncIO Priority Queue in Python Objective: To assess your understanding of asyncio PriorityQueues in Python, you will implement a simplified task management system that prioritizes tasks using asyncio\'s `PriorityQueue`. Problem Statement: You are tasked with creating a task scheduler using asyncio\'s `PriorityQueue`. The scheduler will manage tasks with different priorities and ensure that high-priority tasks are executed before lower-priority ones. Task: 1. Implement an `AsyncTaskScheduler` class that uses an `asyncio.PriorityQueue` to manage tasks. 2. Each task should be a tuple of the form `(priority, task_name, duration)`, where: - `priority` is an integer, with lower numbers indicating higher priority. - `task_name` is a string representing the name of the task. - `duration` is a float representing the time the task takes to complete in seconds. 3. The class should have the following methods: - `add_task(priority: int, task_name: str, duration: float)`: Adds a task to the priority queue. - `start()`: Starts processing tasks from the priority queue based on their priority. - `stop()`: Stops processing tasks and clears the queue. You are provided with an example below. Ensure that your implementation is robust and handles edge cases, such as attempting to stop the scheduler when it is not running. Example: ```python import asyncio class AsyncTaskScheduler: def __init__(self): self.queue = asyncio.PriorityQueue() self.running = False async def add_task(self, priority: int, task_name: str, duration: float): await self.queue.put((priority, task_name, duration)) async def worker(self): while self.running: priority, task_name, duration = await self.queue.get() print(f\'Starting task: {task_name} with priority {priority}\') await asyncio.sleep(duration) print(f\'Completed task: {task_name}\') self.queue.task_done() async def start(self): self.running = True worker_task = asyncio.create_task(self.worker()) await self.queue.join() # Wait until all tasks are processed self.running = False worker_task.cancel() def stop(self): self.running = False # Example usage: async def main(): scheduler = AsyncTaskScheduler() await scheduler.add_task(1, \'High-priority task\', 1) await scheduler.add_task(3, \'Low-priority task\', 2) await scheduler.add_task(2, \'Medium-priority task\', 1.5) # Start the scheduler await scheduler.start() asyncio.run(main()) ``` Constraints: - Use `asyncio.PriorityQueue` for managing the tasks. - Make sure to handle potential race conditions or exceptions. - Ensure efficient task management and proper use of `async/await`. Expected Output: The scheduler should execute tasks in order of their priority, with log messages indicating the start and completion of each task: ``` Starting task: High-priority task with priority 1 Completed task: High-priority task Starting task: Medium-priority task with priority 2 Completed task: Medium-priority task Starting task: Low-priority task with priority 3 Completed task: Low-priority task ``` Performance Requirements: - The implementation should efficiently manage up to 1000 tasks with varying priorities and duration up to 10 seconds. - Ensure that the scheduler starts and stops gracefully without leaving any unprocessed tasks.","solution":"import asyncio class AsyncTaskScheduler: def __init__(self): self.queue = asyncio.PriorityQueue() self.running = False async def add_task(self, priority: int, task_name: str, duration: float): await self.queue.put((priority, task_name, duration)) async def worker(self): while self.running or not self.queue.empty(): priority, task_name, duration = await self.queue.get() print(f\'Starting task: {task_name} with priority {priority}\') await asyncio.sleep(duration) print(f\'Completed task: {task_name}\') self.queue.task_done() async def start(self): self.running = True worker_task = asyncio.create_task(self.worker()) await self.queue.join() # Wait until all tasks are processed self.running = False worker_task.cancel() def stop(self): self.running = False # Example usage: async def main(): scheduler = AsyncTaskScheduler() await scheduler.add_task(1, \'High-priority task\', 1) await scheduler.add_task(3, \'Low-priority task\', 2) await scheduler.add_task(2, \'Medium-priority task\', 1.5) # Start the scheduler await scheduler.start() asyncio.run(main())"},{"question":"**Question** # Timedelta Analysis with Pandas You are provided with a DataFrame that contains information about various tasks and their start and end times. Your task is to calculate the duration of each task, perform various operations, and analyze the resulting durations. **Input** A pandas DataFrame `tasks` with the following columns: - \'TaskID\': A unique identifier for each task. - \'Start\': Start time of the task (as a string in the format \'YYYY-MM-DD HH:MM:SS\'). - \'End\': End time of the task (as a string in the format \'YYYY-MM-DD HH:MM:SS\'). Example: ```python tasks = pd.DataFrame({ \'TaskID\': [1, 2, 3, 4], \'Start\': [\'2023-01-01 08:00:00\', \'2023-01-01 09:15:00\', \'2023-01-01 10:00:00\', \'2023-01-01 11:30:00\'], \'End\': [\'2023-01-01 10:00:00\', \'2023-01-01 10:45:00\', \'2023-01-01 11:30:00\', \'2023-01-01 12:00:00\'] }) ``` **Requirements** 1. Write a function `calculate_task_durations(tasks)` that calculates the duration of each task and adds it as a new column \'Duration\' to the existing DataFrame. The \'Duration\' column should contain pandas Timedelta objects. 2. Calculate the total and average duration of all tasks, and return these values. 3. Identify the longest and shortest tasks based on their durations. Return their TaskIDs and durations. 4. Handle missing or incorrectly formatted start and end times gracefully by setting their durations to `pd.NaT`. 5. If any duration is negative (i.e., end time is before start time), raise a ValueError with the message \\"Invalid task timing\\". **Output** Your function should return a tuple with: 1. The modified DataFrame with the \'Duration\' column added. 2. Total duration of all tasks (as a Timedelta). 3. Average duration of all tasks (as a Timedelta). 4. A dictionary with TaskIDs and durations of the longest and shortest tasks. **Constraints** - The \'Start\' and \'End\' times are guaranteed to be provided as strings. - Tasks may have missing or incorrectly formatted start and end times. **Example** ```python def calculate_task_durations(tasks): # Your code here tasks = pd.DataFrame({ \'TaskID\': [1, 2, 3, 4], \'Start\': [\'2023-01-01 08:00:00\', \'2023-01-01 09:15:00\', \'2023-01-01 10:00:00\', \'2023-01-01 11:30:00\'], \'End\': [\'2023-01-01 10:00:00\', \'2023-01-01 10:45:00\', \'2023-01-01 11:30:00\', \'2023-01-01 12:00:00\'] }) modified_tasks, total_duration, avg_duration, task_extremes = calculate_task_durations(tasks) print(modified_tasks) print(\\"Total Duration:\\", total_duration) print(\\"Average Duration:\\", avg_duration) print(\\"Task Extremes:\\", task_extremes) ``` Expected Output: ``` TaskID Start End Duration 0 1 2023-01-01 08:00:00 2023-01-01 10:00:00 0 days 02:00:00 1 2 2023-01-01 09:15:00 2023-01-01 10:45:00 0 days 01:30:00 2 3 2023-01-01 10:00:00 2023-01-01 11:30:00 0 days 01:30:00 3 4 2023-01-01 11:30:00 2023-01-01 12:00:00 0 days 00:30:00 Total Duration: 0 days 05:30:00 Average Duration: 0 days 01:22:30 Task Extremes: {\'Longest\': (1, Timedelta(\'0 days 02:00:00\')), \'Shortest\': (4, Timedelta(\'0 days 00:30:00\'))} ```","solution":"import pandas as pd import numpy as np def calculate_task_durations(tasks): # Convert Start and End columns to datetime, handling errors gracefully tasks[\'Start\'] = pd.to_datetime(tasks[\'Start\'], errors=\'coerce\') tasks[\'End\'] = pd.to_datetime(tasks[\'End\'], errors=\'coerce\') # Calculate the duration tasks[\'Duration\'] = tasks[\'End\'] - tasks[\'Start\'] # Check for negative durations and raise ValueError if found if not tasks[\'Duration\'].isnull().all(): if (tasks[\'Duration\'] < pd.Timedelta(0)).any(): raise ValueError(\\"Invalid task timing\\") # Set duration to NaT for negative durations (optional for flexibility) tasks.loc[tasks[\'Duration\'] < pd.Timedelta(0), \'Duration\'] = pd.NaT # Calculate total duration total_duration = tasks[\'Duration\'].sum() # Calculate average duration avg_duration = tasks[\'Duration\'].mean() # Identify longest and shortest tasks longest_task_id = tasks.loc[tasks[\'Duration\'].idxmax()][\'TaskID\'] if not tasks[\'Duration\'].isnull().all() else np.nan shortest_task_id = tasks.loc[tasks[\'Duration\'].idxmin()][\'TaskID\'] if not tasks[\'Duration\'].isnull().all() else np.nan longest_duration = tasks[\'Duration\'].max() shortest_duration = tasks[\'Duration\'].min() task_extremes = { \'Longest\': (longest_task_id, longest_duration), \'Shortest\': (shortest_task_id, shortest_duration) } return tasks, total_duration, avg_duration, task_extremes"},{"question":"You are tasked with generating a detailed report of the underlying platform on which your Python script runs. The report needs to be comprehensive, including various system and Python attributes. Requirements 1. **Function Definition**: Write a function `generate_platform_report()` that returns a dictionary with the following key-value pairs: - `\'architecture\'`: Result of `platform.architecture()` - `\'machine\'`: Result of `platform.machine()` - `\'node\'`: Result of `platform.node()` - `\'platform\'`: Result of `platform.platform()` - `\'processor\'`: Result of `platform.processor()` - `\'python_build\'`: Result of `platform.python_build()` - `\'python_compiler\'`: Result of `platform.python_compiler()` - `\'python_implementation\'`: Result of `platform.python_implementation()` - `\'python_version\'`: Result of `platform.python_version()` - `\'release\'`: Result of `platform.release()` - `\'system\'`: Result of `platform.system()` - `\'version\'`: Result of `platform.version()` - `\'uname\'`: Result of `platform.uname()` 2. **Constraints**: - Ensure all keys in the dictionary exist, even if some functions return empty strings or default values. - The function should not accept any arguments. All the necessary data should be gathered inside the function. 3. **Output Format**: - The function should return a dictionary where each key corresponds to one of the platform details as described above, and the value is the output of the corresponding platform function. Example Output ```python { \'architecture\': (\'64bit\', \'ELF\'), \'machine\': \'x86_64\', \'node\': \'my-computer\', \'platform\': \'Linux-5.4.0-42-generic-x86_64-with-glibc2.29\', \'processor\': \'x86_64\', \'python_build\': (\'default\', \'Jul 20 2021\'), \'python_compiler\': \'GCC 9.3.0\', \'python_implementation\': \'CPython\', \'python_version\': \'3.8.10\', \'release\': \'5.4.0-42-generic\', \'system\': \'Linux\', \'version\': \'#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\', \'uname\': uname_result(system=\'Linux\', node=\'my-computer\', release=\'5.4.0-42-generic\', version=\'#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\', machine=\'x86_64\', processor=\'x86_64\') } ``` Notes - Make sure to import the `platform` module at the beginning of your script. - Organized and properly formatted output will be crucial. ```python import platform def generate_platform_report(): report = { \'architecture\': platform.architecture(), \'machine\': platform.machine(), \'node\': platform.node(), \'platform\': platform.platform(), \'processor\': platform.processor(), \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler(), \'python_implementation\': platform.python_implementation(), \'python_version\': platform.python_version(), \'release\': platform.release(), \'system\': platform.system(), \'version\': platform.version(), \'uname\': platform.uname() } return report ``` Test your function to ensure it handles various environments and produces the expected output.","solution":"import platform def generate_platform_report(): Returns a dictionary containing comprehensive details about the underlying platform on which the script runs. report = { \'architecture\': platform.architecture(), \'machine\': platform.machine(), \'node\': platform.node(), \'platform\': platform.platform(), \'processor\': platform.processor(), \'python_build\': platform.python_build(), \'python_compiler\': platform.python_compiler(), \'python_implementation\': platform.python_implementation(), \'python_version\': platform.python_version(), \'release\': platform.release(), \'system\': platform.system(), \'version\': platform.version(), \'uname\': platform.uname() } return report"},{"question":"# Advanced Python Coding Assessment: Implementing Concurrent Execution with Threading and Subprocess Modules Objective: You need to create a Python program that performs concurrent execution using both the `threading` and `subprocess` modules. The goal is to demonstrate a solid understanding of thread-based parallelism and how to manage subprocesses within threads. Task: 1. Implement a function `run_commands_in_threads(commands: List[List[str]]) -> Dict[str, str]` that: - Takes a list of list of strings where each inner list represents a command with arguments to be executed as a subprocess. - Runs each command in a separate thread using the `threading` module. - Captures the output (both stdout and stderr) of each subprocess. - Returns a dictionary mapping the original command (as a space-separated string) to its respective output (stdout concatenated with stderr). Input: - `commands`: A list of commands to run, where each command is represented as a list of strings. Example: `[[\'echo\', \'Hello\'], [\'ls\', \'-l\'], [\'python\', \'--version\']]` Output: - A dictionary with the command as a key, and its output as the value. Example: `{\'echo Hello\': \'Hellon\', \'ls -l\': \'total 0n... (output of ls -l)\', \'python --version\': \'Python 3.10.0n\'}` Constraints: 1. Each thread should handle the execution of one subprocess command. 2. Use of Python\'s `threading` and `subprocess` modules is mandatory. 3. Ensure that all threads are properly managed and joined. Example: ```python import threading import subprocess from typing import List, Dict def run_commands_in_threads(commands: List[List[str]]) -> Dict[str, str]: result = {} threads = [] def run_command(cmd: List[str]): process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) stdout, stderr = process.communicate() result[\\" \\".join(cmd)] = stdout + stderr for cmd in commands: thread = threading.Thread(target=run_command, args=(cmd,)) threads.append(thread) thread.start() for thread in threads: thread.join() return result # Example usage commands = [[\'echo\', \'Hello\'], [\'ls\', \'-l\'], [\'python3\', \'--version\']] output = run_commands_in_threads(commands) print(output) ``` Notes: - Ensure to handle command execution and threading correctly to avoid deadlocks and race conditions. - Test the function with a variety of commands to ensure robustness.","solution":"import threading import subprocess from typing import List, Dict def run_commands_in_threads(commands: List[List[str]]) -> Dict[str, str]: result = {} threads = [] def run_command(cmd: List[str]): process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) stdout, stderr = process.communicate() result[\\" \\".join(cmd)] = stdout + stderr for cmd in commands: thread = threading.Thread(target=run_command, args=(cmd,)) threads.append(thread) thread.start() for thread in threads: thread.join() return result"},{"question":"Customized Module Import System **Objective:** Implement a custom module loader and meta path finder to handle module imports from a specific directory structure. # Problem Statement You are required to create a custom import system that can dynamically import Python modules located in a user-specified directory. This directory will be specified at runtime, and the modules within this directory should be importable using normal import statements. **Components:** 1. **Custom Meta Path Finder** - This will find modules in the specified directory. 2. **Custom Module Loader** - This will load the modules located in the specified directory. # Expected Input and Output Formats - **Input:** - `directory` (string): A string providing the path to the directory containing Python module files. - **Output:** - Imported modules should be loaded and executed similar to standard Python modules, and should be accessible in `sys.modules`. # Constraints 1. The specified directory may only contain `.py` files (Python modules). 2. The modules should be importable using standard Python import statements or `importlib.import_module()`. # Implementation Details 1. **Create a Custom Finder**: This finder should determine if it can handle finding a module in the specified directory. 2. **Create a Custom Loader**: This loader should handle loading and executing the module code from the specified `.py` files. 3. **Register the Custom Finder** with `sys.meta_path`: This ensures the import system uses your custom finder for locating modules. # Example Usage ```python import sys import importlib class CustomFinder: def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path=None, target=None): # Implement module spec finding logic pass class CustomLoader: def __init__(self, filepath): self.filepath = filepath def create_module(self, spec): # Implement module creation logic pass def exec_module(self, module): # Implement module execution logic pass # Usage example sys.meta_path.insert(0, CustomFinder(\'/path/to/custom/modules\')) # Assuming there\'s a file named \'custom_module.py\' in the specified directory import custom_module ``` **Tasks:** 1. Implement the `CustomFinder` class, ensuring it correctly locates modules within the specified directory. 2. Implement the `CustomLoader` class, ensuring it creates and executes the module from the located file. 3. Register your `CustomFinder` with `sys.meta_path`. # Submission Please submit a `.py` file with the implementation of the `CustomFinder` and `CustomLoader` classes along with the code to register the custom finder with `sys.meta_path`.","solution":"import sys import os import importlib.util import importlib.machinery class CustomFinder: def __init__(self, directory): self.directory = directory def find_spec(self, fullname, path=None, target=None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self.directory, module_name + \\".py\\") if os.path.exists(module_path): loader = CustomLoader(module_path) return importlib.util.spec_from_file_location(fullname, module_path, loader=loader) return None class CustomLoader: def __init__(self, filepath): self.filepath = filepath def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): with open(self.filepath, \\"r\\") as file: code = file.read() exec(code, module.__dict__) def register_custom_finder(directory): sys.meta_path.insert(0, CustomFinder(directory))"},{"question":"**Task:** Write a Python function using the `seaborn.objects` interface that accomplishes the following: 1. Load the \\"titanic\\" dataset. 2. Create a faceted histogram of the \'age\' distribution separated by \'sex\'. 3. Each histogram should present data stacked by the \'alive\' status, with the bars representing counts of passengers for each bin. 4. Customize the histograms to have a binwidth of 8. _Your function signature should be:_ ```python import seaborn.objects as so from seaborn import load_dataset def plot_titanic_age_distribution(): # Your code here ``` _Function Implementation Details:_ - The function should not take any parameters. - The function should use `seaborn.objects` (imported as `so`). - The function should load the Titanic dataset using `load_dataset(\\"titanic\\")`. - Use the `so.Plot` to create a faceted histogram. - The x-axis should represent the \'age\'. - Separate the histograms based on \'sex\' using faceting. - Within each facet, stack the bars by \'alive\' status. - Set the binwidth for the histograms to 8. _Expected Output:_ The function should render a faceted histogram plot displaying the age distribution of Titanic passengers, separated by sex, and further stacked by alive status. _Constraints:_ - Ensure the plot is well-labeled and visually clear. - Optimize the code for readability and performance. _Example:_ Upon execution of your function, you should see a plot similar to: ``` < Faceted Histogram Plot Visualization > ``` ```python import seaborn.objects as so from seaborn import load_dataset def plot_titanic_age_distribution(): titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=8), so.Stack()) ) ```","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_titanic_age_distribution(): Creates a faceted histogram plot displaying the age distribution of Titanic passengers, separated by sex, and further stacked by alive status with a binwidth of 8. # Load the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create the faceted histogram plot = ( so.Plot(titanic, x=\\"age\\", color=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=8), so.Stack()) ) # Render the plot plot.show()"},{"question":"**Title:** Implementing Custom Asynchronous Operations **Objective:** This task aims to assess your understanding of Future objects in `asyncio` and your ability to integrate asynchronous programming constructs. **Problem Statement:** Your task is to implement a custom class `AsyncHandler` that uses `asyncio.Future` objects to manage asynchronous operations. Your class should: 1. Include a function `schedule_task` that schedules an asynchronous task which sets a result or raises an exception for a Future object after a specified delay. 2. Include a function `handle_result` that retrieves the result of the Future object or handles the exceptions if any. 3. Include a function `manage_operations` that demonstrates the use of these Future objects to perform several tasks concurrently. **Requirements:** 1. **schedule_task(self, fut: asyncio.Future, delay: float, result=None, exception=None)**: - Schedules an asynchronous task that waits for `delay` seconds and then sets the result of the Future object `fut` with `result`. - If `exception` is provided, it sets the exception instead of the result. 2. **handle_result(self, fut: asyncio.Future)**: - Retrieves and returns the result of the Future object `fut`. - If the Future has an exception set, it raises that exception. - If the Future is not yet done, it should raise a custom `InvalidStateError`. 3. **manage_operations(self)**: - Demonstrates the use of multiple Future objects and the `schedule_task` and `handle_result` functions. - Should create multiple Future objects and schedule tasks with varying delays and results. - Should demonstrate handling Future objects, retrieving results, and handling exceptions appropriately. **Input and Output:** - The `schedule_task` function does not return any value. - The `handle_result` function returns the result of a Future or raises exceptions. - The `manage_operations` function demonstrates the asynchronous tasks in progress and handles results or exceptions. **Example Usage:** ```python import asyncio class InvalidStateError(Exception): pass class AsyncHandler: def __init__(self): pass async def schedule_task(self, fut: asyncio.Future, delay: float, result=None, exception=None): await asyncio.sleep(delay) if exception is not None: fut.set_exception(exception) else: fut.set_result(result) async def handle_result(self, fut: asyncio.Future): if fut.done(): return fut.result() else: raise InvalidStateError(\\"Future is not done yet\\") async def manage_operations(self): loop = asyncio.get_running_loop() fut1 = loop.create_future() fut2 = loop.create_future() loop.create_task(self.schedule_task(fut1, 1, result=\\"Result 1\\")) loop.create_task(self.schedule_task(fut2, 2, exception=ValueError(\\"Example Exception\\"))) try: print(await self.handle_result(fut1)) except Exception as e: print(e) try: print(await self.handle_result(fut2)) except Exception as e: print(e) # Example of running the `manage_operations` method async def main(): handler = AsyncHandler() await handler.manage_operations() asyncio.run(main()) ``` **Constraints:** - Ensure proper exception handling as per the provided methods\' specifications. - Adhere to the described asynchronous behavior for `schedule_task`. Good luck!","solution":"import asyncio class InvalidStateError(Exception): pass class AsyncHandler: def __init__(self): pass async def schedule_task(self, fut: asyncio.Future, delay: float, result=None, exception=None): await asyncio.sleep(delay) if exception is not None: fut.set_exception(exception) else: fut.set_result(result) async def handle_result(self, fut: asyncio.Future): if fut.done(): return fut.result() else: raise InvalidStateError(\\"Future is not done yet\\") async def manage_operations(self): loop = asyncio.get_running_loop() fut1 = loop.create_future() fut2 = loop.create_future() fut3 = loop.create_future() loop.create_task(self.schedule_task(fut1, 1, result=\\"Result 1\\")) loop.create_task(self.schedule_task(fut2, 2, exception=ValueError(\\"Example Exception\\"))) loop.create_task(self.schedule_task(fut3, 0.5, result=\\"Result 3\\")) results = [] futures = [fut1, fut2, fut3] for fut in futures: try: results.append(await self.handle_result(fut)) except Exception as e: results.append(str(e)) return results # Example of running the `manage_operations` method async def main(): handler = AsyncHandler() results = await handler.manage_operations() for result in results: print(result) asyncio.run(main())"},{"question":"You are given a dataset containing information about patients, their symptoms, and whether they have a specific disease (binary classification problem). Your task is to build a Decision Tree Classifier to predict whether a patient has the disease based on their symptoms. Additionally, the dataset has some missing values. You need to handle these appropriately for the classifier to make better predictions. The dataset is provided as a CSV file with the following columns: `feature1`, `feature2`, ..., `featureN`, `disease`. The `disease` column contains the target labels (0 for no disease, 1 for disease). Some of the feature columns contain missing values. # Your Task 1. Load the dataset from the provided CSV file. 2. Handle missing values in the dataset. 3. Split the data into training and testing sets. 4. Train a `DecisionTreeClassifier` on the training set. 5. Evaluate the classifier on the testing set. 6. Plot the resulting decision tree. # Constraints - Use `scikit-learn` library functions to build the decision tree. - Handle missing values using the strategy provided in the scikit-learn documentation. - Use a random seed for reproducibility of the results. - Use appropriate train-test split ratio (e.g., 80-20 split). # Input - A path to the CSV file containing the dataset. # Output - Print the accuracy of the model on the test dataset. - Plot the trained decision tree. # Example Execution ```python import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree import matplotlib.pyplot as plt # 1. Load the dataset file_path = \'path/to/your/csv\' data = pd.read_csv(file_path) # 2. Handle missing values (fill missing values with the mean of the column) data.fillna(data.mean(), inplace=True) # 3. Split the data into features and target X = data.drop(columns=[\'disease\']) y = data[\'disease\'] # 4. Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 5. Train a DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # 6. Evaluate the classifier accuracy = clf.score(X_test, y_test) print(f\'Accuracy: {accuracy}\') # 7. Plot the decision tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=X.columns, class_names=[\'No Disease\', \'Disease\'], rounded=True) plt.show() ``` # Notes - The above example demonstrates handling missing values by filling them with the mean of each feature column. Ensure you explore and utilize appropriate missing value handling methods introduced in the `scikit-learn` documentation. - Customize the parameters of `DecisionTreeClassifier` such as `max_depth`, `min_samples_split`, etc., to avoid overfitting. - Ensure the generated plot of the decision tree is clear and informative.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.impute import SimpleImputer import matplotlib.pyplot as plt def train_and_plot_decision_tree(file_path): Train a Decision Tree Classifier on a given dataset and plot the resulting tree. Args: file_path (str): The path to the CSV file containing the dataset. Returns: float: The accuracy of the model on the test dataset. # 1. Load the dataset data = pd.read_csv(file_path) # 2. Handle missing values # Using SimpleImputer to fill missing values with the mean of the column imputer = SimpleImputer(strategy=\'mean\') imputed_data = imputer.fit_transform(data) # Split the imputed data into features and target X = imputed_data[:, :-1] y = imputed_data[:, -1] # 3. Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 4. Train a DecisionTreeClassifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # 5. Evaluate the classifier accuracy = clf.score(X_test, y_test) print(f\'Accuracy: {accuracy}\') # 6. Plot the decision tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=data.columns[:-1], class_names=[\'No Disease\', \'Disease\'], rounded=True) plt.show() return accuracy"},{"question":"# Custom Command Interpreter Using `cmd` Module Objective Your task is to implement a custom command interpreter that simulates managing a simple inventory system of items. You will use the `cmd` module framework to create this interpreter. Requirements **1. Class Definition:** Define a class `InventoryShell` that inherits from `cmd.Cmd`. **2. Command Methods:** Implement the following command methods in your interpreter: - `do_add(item_name quantity)`: - Adds a specified quantity of an item to the inventory. - `do_remove(item_name quantity)`: - Removes a specified quantity of an item from the inventory. - Ensure that quantity does not drop below zero. - `do_list()`: - Lists all items in the inventory with their quantities. - `do_quit()`: - Quits the interpreter. **3. Command Aliases:** Create aliases for the commands: - `add`: alias `a` - `remove`: alias `rm` - `list`: alias `ls` **4. Command Completion:** Enable tab-completion for the `item_name` in `do_add` and `do_remove` methods. **5. Command Logging:** Log each command executed and the result (success or failure) to a log file named `commands.log`. **6. Hooks:** Use the `precmd` and `postcmd` hooks to: - Convert command input to lowercase using `precmd`. - Log each command to `commands.log` using `postcmd`. Input and Output Formats - Input for `do_add` and `do_remove`: String commands entered by the user. - `do_list`: No inputs. - Output: Appropriate messages indicating the result of each command execution. Constraints - Handle invalid inputs gracefully. - Use dictionaries to store inventory items. Example ```python import cmd class InventoryShell(cmd.Cmd): intro = \'Welcome to the inventory shell. Type help or ? to list commands.n\' prompt = \'(inventory) \' # Your implementation here if __name__ == \'__main__\': InventoryShell().cmdloop() ``` Example session: ``` (inventory) add apple 10 Added 10 apple(s) to inventory. (inventory) list Item: apple, Quantity: 10 (inventory) rm apple 5 Removed 5 apple(s) from inventory. (inventory) ls Item: apple, Quantity: 5 (inventory) quit ``` Assessment Criteria - Correctness of command method implementations. - Proper use of `cmd` module features. - Effective use of `precmd` and `postcmd`. - Graceful handling of invalid inputs. - Logging functionality. - Implementation of command aliases and completion.","solution":"import cmd import logging class InventoryShell(cmd.Cmd): intro = \'Welcome to the inventory shell. Type help or ? to list commands.n\' prompt = \'(inventory) \' def __init__(self): super().__init__() self.inventory = {} logging.basicConfig(filename=\'commands.log\', level=logging.INFO) def precmd(self, line): return line.lower() def postcmd(self, stop, line): logging.info(f\'Command: {line}, Result: {\\"success\\" if stop is False else \\"failure\\"}\') return stop def do_add(self, arg): try: parts = arg.split() if len(parts) != 2: print(\\"Usage: add <item_name> <quantity>\\") return item_name, quantity = parts quantity = int(quantity) if item_name in self.inventory: self.inventory[item_name] += quantity else: self.inventory[item_name] = quantity print(f\\"Added {quantity} {item_name}(s) to inventory.\\") except ValueError: print(\\"Invalid quantity. Please enter a number.\\") def do_a(self, arg): self.do_add(arg) def do_remove(self, arg): try: parts = arg.split() if len(parts) != 2: print(\\"Usage: remove <item_name> <quantity>\\") return item_name, quantity = parts quantity = int(quantity) if item_name in self.inventory: if self.inventory[item_name] >= quantity: self.inventory[item_name] -= quantity print(f\\"Removed {quantity} {item_name}(s) from inventory.\\") else: print(f\\"Cannot remove {quantity} {item_name}(s). Only {self.inventory[item_name]} available.\\") else: print(f\\"No item named {item_name} in inventory.\\") except ValueError: print(\\"Invalid quantity. Please enter a number.\\") def do_rm(self, arg): self.do_remove(arg) def do_list(self, arg): if self.inventory: for item, quantity in self.inventory.items(): print(f\\"Item: {item}, Quantity: {quantity}\\") else: print(\\"Inventory is empty.\\") def do_ls(self, arg): self.do_list(arg) def do_quit(self, arg): print(\\"Quitting.\\") return True def complete_add(self, text, line, begidx, endidx): if not text: completions = self.inventory.keys() else: completions = [item for item in self.inventory.keys() if item.startswith(text)] return list(completions) def complete_remove(self, text, line, begidx, endidx): return self.complete_add(text, line, begidx, endidx) if __name__ == \'__main__\': InventoryShell().cmdloop()"},{"question":"**Coding Assessment Question:** # File Organization and Search with glob Module Objective You are tasked with implementing a function that utilizes the `glob` module to organize and search for files within a given directory structure. Your function will find specific file types and classify them based on their extensions, and then it will return a report of the found files. Description Create a function `find_and_classify_files(directory_path: str, patterns: list[str], classify_by_extension: bool = True) -> dict` that performs the following tasks: 1. **Directory Search**: - Use the `glob` module to search for files in the directory specified by `directory_path`. The `patterns` parameter is a list of Unix shell-style wildcard patterns to match file names. If a pattern includes wildcards (`*`, `?`, `[]`), it should be handled accordingly. - Support optional recursive search using the `**` pattern. 2. **File Classification**: - If `classify_by_extension` is True, classify the files based on their extensions. For instance, all `.txt` files should be grouped together, `.gif` files in another group, and so on. - If `classify_by_extension` is False, group all found files under a single category. 3. **Return Format**: - Return a dictionary where the keys are the file extensions (if `classify_by_extension` is True) or a single key `\'all_files\'` (if `classify_by_extension` is False), and the values are lists of file paths that match the patterns. Function Signature ```python def find_and_classify_files(directory_path: str, patterns: list[str], classify_by_extension: bool = True) -> dict: ``` Example Consider the following directory structure: ``` /example_dir â”œâ”€â”€ 1.gif â”œâ”€â”€ 2.txt â”œâ”€â”€ card.gif â””â”€â”€ sub â””â”€â”€ 3.txt ``` 1. With `find_and_classify_files(\'/example_dir\', [\'*.gif\', \'*.txt\'], classify_by_extension=True)`, the function should return: ```python { \'.gif\': [\'1.gif\', \'card.gif\'], \'.txt\': [\'2.txt\'] } ``` 2. With `find_and_classify_files(\'/example_dir\', [\'**/*.txt\'], classify_by_extension=False, recursive=True)`, the function should return: ```python { \'all_files\': [\'2.txt\', \'sub/3.txt\'] } ``` Constraints - `directory_path` is a valid directory path string. - `patterns` is a list of valid Unix shell-style filename patterns. - The function should handle large directories efficiently, especially when recursive searches are performed. Notes - Ensure to correctly handle files starting with \\".\\" only when explicitly specified in the pattern. - Use appropriate functions from the `glob` module to achieve the tasks. Performance Requirements - Ensure the solution is optimized for large directory structures when performing recursive searches.","solution":"import glob import os def find_and_classify_files(directory_path: str, patterns: list[str], classify_by_extension: bool = True) -> dict: Searches and classifies files in a given directory based on their extensions. Args: directory_path (str): The directory path to search for files. patterns (list[str]): List of Unix shell-style wildcard patterns to match file names. classify_by_extension (bool): Whether to classify files by their extensions. Returns: dict: A dictionary with keys as file extensions or \'all_files\' and values as lists of file paths. matched_files = [] for pattern in patterns: # Use glob to match the patterns in the given directory matched_files.extend(glob.glob(os.path.join(directory_path, pattern), recursive=True)) files_dict = {} if classify_by_extension: for file in matched_files: file_extension = os.path.splitext(file)[1] # Get the file extension if file_extension not in files_dict: files_dict[file_extension] = [] files_dict[file_extension].append(file) else: files_dict[\'all_files\'] = matched_files return files_dict"},{"question":"# Objective You are required to generate a synthetic dataset for a classification task using the `make_classification` function from the scikit-learn library, perform data preprocessing, and implement a basic classifier to evaluate the performance on the generated dataset. # Task 1. Generate a synthetic dataset using `make_classification` with the following specifications: - `n_features=10`: The total number of features. - `n_informative=5`: The number of informative features. - `n_redundant=2`: The number of redundant features. - `n_classes=3`: The number of classes. - `n_clusters_per_class=1`: The number of clusters per class. - `class_sep=1.0`: The factor separating the classes. - `flip_y=0.01`: The fraction of samples whose class is assigned randomly. - `random_state=42`: A seed for reproducibility. 2. Perform the following preprocessing on the dataset: - Split the dataset into 80% training and 20% testing sets. - Standardize the features by removing the mean and scaling to unit variance. 3. Implement a basic classifier using `LogisticRegression` and train it on the training set. 4. Evaluate the performance of the classifier using the testing set and print: - The classification report (use `classification_report` from `sklearn.metrics`). - The confusion matrix (use `confusion_matrix` from `sklearn.metrics`). # Input and Output Format - **Input**: No input is required from the user; your function should take no parameters. - **Output**: Your function should print the classification report and confusion matrix. # Constraints and Performance Requirements - Your solution should be implemented using Python and the scikit-learn library. - Ensure that the dataset generation and preprocessing steps are correct. - The classifier should be trained and evaluated properly to output meaningful metrics. # Implementation ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, confusion_matrix def classification_task(): # Generate the dataset X, y = make_classification( n_features=10, n_informative=5, n_redundant=2, n_classes=3, n_clusters_per_class=1, class_sep=1.0, flip_y=0.01, random_state=42 ) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement and train the classifier classifier = LogisticRegression(random_state=42) classifier.fit(X_train, y_train) # Make predictions y_pred = classifier.predict(X_test) # Print the classification report and confusion matrix print(\\"Classification Report:\\") print(classification_report(y_test, y_pred)) print(\\"Confusion Matrix:\\") print(confusion_matrix(y_test, y_pred)) # Call the function to execute the task classification_task() ```","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report, confusion_matrix def classification_task(): # Generate the dataset X, y = make_classification( n_features=10, n_informative=5, n_redundant=2, n_classes=3, n_clusters_per_class=1, class_sep=1.0, flip_y=0.01, random_state=42 ) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement and train the classifier classifier = LogisticRegression(random_state=42) classifier.fit(X_train, y_train) # Make predictions y_pred = classifier.predict(X_test) # Print the classification report and confusion matrix print(\\"Classification Report:\\") print(classification_report(y_test, y_pred)) print(\\"Confusion Matrix:\\") print(confusion_matrix(y_test, y_pred))"},{"question":"You are tasked with implementing a function that will take an email message represented by a `EmailMessage` object, convert it into a serialized byte-oriented representation, and save this representation to a file. The function should handle different email policies and formats for headers. Your function should utilize the `email.generator.BytesGenerator` class to achieve this. # Function Signature ```python def save_email_as_bytes(msg, file_path, mangle_from=False, maxheaderlen=None, policy=None): Converts an email represented by EmailMessage object to a serialized byte-oriented representation and saves it to a specified file. :param msg: EmailMessage object representing the email message. :param file_path: String representing the path to the file where the output should be saved. :param mangle_from: Boolean indicating whether to apply `mangle_from` parameter (default: False). :param maxheaderlen: Integer specifying the maximum header length for folding headers (default: None). :param policy: An optional email.policy.Policy object to control message generation (default: None). ``` # Input - `msg`: An `EmailMessage` object containing the email message structure. - `file_path`: A string representing the path (including file name) where the byte-oriented serialized representation should be saved. - `mangle_from` (optional): A boolean value indicating whether to apply the `mangle_from_` parameter (default is `False`). - `maxheaderlen` (optional): An integer specifying the maximum length for header rewrapping. Use `None` for default behavior (default is `None`). - `policy` (optional): A `email.policy.Policy` object to dictate how the message should be generated (default is `None`). # Output - None (The function should only perform the side-effect of writing to the file). # Constraints - Ensure proper handling of Unicode and binary data within the email message. - The file must be saved in binary mode to ensure proper byte-oriented output. - If `mangle_from` is `True`, lines starting with \\"From \\" must be correctly handled. - Properly apply headers folding based on `maxheaderlen` parameter. # Example Usage: ```python from email.message import EmailMessage from email.policy import default # Creating a sample EmailMessage object msg = EmailMessage() msg.set_content(\\"This is a sample email body.\\") msg[\\"Subject\\"] = \\"Sample Email\\" msg[\\"From\\"] = \\"sender@example.com\\" msg[\\"To\\"] = \\"recipient@example.com\\" # Saving the message to a file named \'email_output.eml\' save_email_as_bytes(msg, \\"email_output.eml\\") ``` # Notes: - The function should use the `BytesGenerator` class from the `email.generator` module. - Make sure to handle edge cases such as empty message bodies or missing headers.","solution":"from email.generator import BytesGenerator from email.message import EmailMessage def save_email_as_bytes(msg, file_path, mangle_from=False, maxheaderlen=None, policy=None): Converts an email represented by EmailMessage object to a serialized byte-oriented representation and saves it to a specified file. :param msg: EmailMessage object representing the email message. :param file_path: String representing the path to the file where the output should be saved. :param mangle_from: Boolean indicating whether to apply `mangle_from` parameter (default: False). :param maxheaderlen: Integer specifying the maximum header length for folding headers (default: None). :param policy: An optional email.policy.Policy object to control message generation (default: None). with open(file_path, \'wb\') as file: generator = BytesGenerator(file, mangle_from_=mangle_from, maxheaderlen=maxheaderlen, policy=policy) generator.flatten(msg)"},{"question":"# XML Data Manipulation with `xml.sax.saxutils` **Objective**: Implement a function that transforms an XML string by escaping special characters, preparing it for attribute use, and then unescaping selected sequences. This will assess your understanding of the `xml.sax.saxutils` package functionalities. Instructions 1. **Function Name**: `transform_xml` 2. **Parameters**: - `xml_string` (str): A string representing the XML data. - `escape_entities` (dict): A dictionary of additional entities to escape. - `unescape_entities` (dict): A dictionary of additional entities to unescape. 3. **Returns**: - A transformed XML string after performing the following operations in sequence: - Escaping special characters (and additional entities) in the XML string. - Preparing the escaped string for use as an XML attribute. - Unescaping the specified sequences in the resulting attribute-prepared string. Example ```python def transform_xml(xml_string, escape_entities={}, unescape_entities={}): # Your implementation here # Example usage: xml_string = \\"<data>Tom & Jerry</data>\\" escape_entities = {\'Tom\': \'Tom-the-cat\', \'Jerry\': \'Jerry-the-mouse\'} unescape_entities = {\'Tom-the-cat\': \'Thomas\', \'Jerry-the-mouse\': \'Jerome\'} result = transform_xml(xml_string, escape_entities, unescape_entities) print(result) # Output should demonstrate the transformation steps clearly. ``` Constraints - The input XML string will only contain well-formed XML. - The additional entities dictionaries will contain valid string pairs only. - Python 3.10 should be used as the runtime environment. You should make use of the functions available in the `xml.sax.saxutils` package to achieve this transformation. **Note**: Import the necessary functions from `xml.sax.saxutils` in your implementation. Performance Requirements - The function should handle XML strings with up to 100,000 characters efficiently. - Ensure that your implementation is optimized for readability and performance.","solution":"from xml.sax.saxutils import escape, unescape, quoteattr def transform_xml(xml_string, escape_entities={}, unescape_entities={}): # Step 1: Escape the XML string with the additional escape entities escaped_string = escape(xml_string, entities=escape_entities) # Step 2: Prepare the escaped string for use as an XML attribute attribute_ready_string = quoteattr(escaped_string) # Step 3: Remove the surrounding quotes introduced by quoteattr attribute_ready_string = attribute_ready_string[1:-1] # Step 4: Unescape the specified sequences in the resulting attribute-prepared string unescaped_final_string = unescape(attribute_ready_string, entities=unescape_entities) return unescaped_final_string"},{"question":"# Problem: Tarfile Archive Manager You are tasked to design a Python script that manages tar archives using the `tarfile` module. The script should be capable of creating a tar archive from a specified directory, listing the contents of a tar archive, and extracting specific files based on a filter. Requirements: 1. **Create a Tar Archive**: - Write a function `create_tar(archive_name: str, source_dir: str, compression: str = \'gz\') -> None` that creates a compressed tar archive of the specified directory. - Parameters: - `archive_name`: The name of the resulting tar archive file (without the extension). - `source_dir`: The directory whose contents need to be archived. - `compression`: The type of compression to use (`\'gz\'`, `\'bz2\'`, `\'xz\'`, or `None` for no compression). Default is `\'gz\'`. 2. **List Contents of a Tar Archive**: - Write a function `list_tar_contents(archive_path: str) -> None` that lists all the contents of a given tar archive. - Parameters: - `archive_path`: The full path to the tar archive file. 3. **Extract Specific Files from a Tar Archive**: - Write a function `extract_files(archive_path: str, dest_dir: str, suffix: str) -> None` that extracts all files with a specific suffix (e.g., \'.txt\') from the tar archive to the specified destination directory. - Parameters: - `archive_path`: The full path to the tar archive file. - `dest_dir`: The directory where the extracted files should be stored. - `suffix`: The file suffix to filter by when extracting. Constraints: - You can assume that the tarfile module and necessary compression libraries are available. - The functions should appropriately handle exceptions and possible error scenarios. - Ensure that any archive-related operations are properly closed and cleaned up. Example Usage: ```python # Create a tar archive create_tar(\\"my_archive\\", \\"my_directory\\", \\"gz\\") # List the contents of the tar archive list_tar_contents(\\"my_archive.gz\\") # Extract all .txt files from the tar archive extract_files(\\"my_archive.gz\\", \\"extracted_files\\", \\".txt\\") ``` Implement the described functions in Python, ensuring they adhere to the specified requirements and constraints. Make sure your code is well-documented and handles errors gracefully.","solution":"import tarfile import os def create_tar(archive_name: str, source_dir: str, compression: str = \'gz\') -> None: Creates a compressed tar archive of the specified directory. Parameters: archive_name (str): The name of the resulting tar archive file (without the extension). source_dir (str): The directory whose contents need to be archived. compression (str): The type of compression to use (\'gz\', \'bz2\', \'xz\', or None for no compression). Default is \'gz\'. Returns: None extension = {\'gz\': \'.tar.gz\', \'bz2\': \'.tar.bz2\', \'xz\': \'.tar.xz\', None: \'.tar\'}[compression] archive_path = archive_name + extension with tarfile.open(archive_path, f\'w:{compression}\' if compression else \'w\') as tar: tar.add(source_dir, arcname=os.path.basename(source_dir)) def list_tar_contents(archive_path: str) -> None: Lists all the contents of a given tar archive. Parameters: archive_path (str): The full path to the tar archive file. Returns: None with tarfile.open(archive_path, \'r\') as tar: tar.list() def extract_files(archive_path: str, dest_dir: str, suffix: str) -> None: Extracts all files with a specific suffix from the tar archive to the specified destination directory. Parameters: archive_path (str): The full path to the tar archive file. dest_dir (str): The directory where the extracted files should be stored. suffix (str): The file suffix to filter by when extracting. Returns: None with tarfile.open(archive_path, \'r\') as tar: members = [m for m in tar.getmembers() if m.name.endswith(suffix)] tar.extractall(path=dest_dir, members=members)"},{"question":"# Advanced Python Programming Assessment Problem Statement You are tasked with developing a mini inventory management system for a small retail store. The store deals with various products, each with a unique product code, name, price, and quantity in stock. The system should allow adding new products, updating the stock of existing products, calculating the total inventory value, and filtering products based on a minimum stock threshold. Requirements 1. **Product Class Implementation:** - Implement a `Product` class with the following properties: - `product_code` (string): Unique identifier for the product. - `name` (string): Name of the product. - `price` (float): Price per unit of the product. - `quantity` (int): Quantity of the product in stock. - Implement appropriate methods to: - Initialize the product with the provided attributes. - Return a string representation of the product. 2. **Inventory Management Class:** - Implement an `Inventory` class that manages a collection of `Product` instances. This class should support the following methods: - `add_product(product)`: Adds a `Product` instance to the inventory. - `update_quantity(product_code, new_quantity)`: Updates the quantity of the product identified by `product_code`. - `total_inventory_value()`: Returns the total value of all products in the inventory (value = price * quantity). - `filter_products(min_quantity)`: Returns a list of products that have a quantity greater than or equal to `min_quantity`. 3. **Exception Handling:** - Ensure appropriate error handling in your methods. For instance, raise an exception if an attempt is made to update the quantity of a non-existent product. Example Usage ```python # Create instances of Product product1 = Product(\\"P001\\", \\"Laptop\\", 1200.00, 10) product2 = Product(\\"P002\\", \\"Smartphone\\", 800.00, 5) product3 = Product(\\"P003\\", \\"Headphones\\", 150.00, 25) # Create an instance of Inventory inventory = Inventory() # Add products to the inventory inventory.add_product(product1) inventory.add_product(product2) inventory.add_product(product3) # Update the quantity of an existing product inventory.update_quantity(\\"P002\\", 10) # Calculate the total inventory value print(inventory.total_inventory_value()) # Should return 21500.00 # Filter products with a minimum quantity of 10 filtered_products = inventory.filter_products(10) for product in filtered_products: print(product) # Should print details of products with quantity >= 10 ``` Constraints - All product codes are unique strings. - The price of a product is a non-negative floating point number. - The quantity of a product is a non-negative integer. - Ensure that the code handles exceptions appropriately, especially when performing updates on non-existent products. Evaluation Criteria - Correct implementation of the `Product` and `Inventory` classes. - Proper usage of Python data structures. - Efficient and readable code. - Effective error handling. - Adherence to Python coding conventions and style.","solution":"class Product: def __init__(self, product_code, name, price, quantity): self.product_code = product_code self.name = name self.price = price self.quantity = quantity def __str__(self): return f\\"Product Code: {self.product_code}, Name: {self.name}, Price: {self.price}, Quantity: {self.quantity}\\" class Inventory: def __init__(self): self.products = {} def add_product(self, product): if product.product_code in self.products: raise ValueError(\\"Product with this code already exists.\\") self.products[product.product_code] = product def update_quantity(self, product_code, new_quantity): if product_code not in self.products: raise KeyError(\\"Product not found.\\") self.products[product_code].quantity = new_quantity def total_inventory_value(self): return sum(p.price * p.quantity for p in self.products.values()) def filter_products(self, min_quantity): return [p for p in self.products.values() if p.quantity >= min_quantity]"},{"question":"**Coding Challenge: Building a Robust CGI Script** # Objective: Design a CGI script using the `cgi` module in Python. The task is to create a script that accepts user inputs from an HTML form, processes the data, accepts file uploads, and generates appropriate HTML output. # Detailed Requirements: 1. **Form Handling**: - The script should accept a form with the fields `username` (text input) and `userfile` (file upload). - If the `username` field is empty, output a suitable error message. 2. **File Upload Handling**: - Accept uploaded files. - If the file is not uploaded, output a suitable error message. - If a file is uploaded, count the number of lines in the file and output this count to the user. 3. **HTML Output**: - The script should return a valid HTML response displaying the `username` and the line count of the uploaded file or the error messages. 4. **Debugging**: - Use the `cgitb` module to handle and display any errors during development. 5. **Security**: - Ensure that the `username` field is sanitized to contain only alphanumeric characters, dashes, underscores, and periods. Reject any inputs that do not meet these criteria. # Constraints: - The script should reside in the `cgi-bin` directory and follow all proper installation instructions. - The Python interpreter path should be correctly specified on the first line of the script. - Ensure the script has execution permissions. # Input Format: The script will be invoked by an HTTP server. The form data will be supplied as either `multipart/form-data` or `application/x-www-form-urlencoded`. # Output Format: The CGI script should output valid HTML. Below is an example structure for HTML output: ```html Content-Type: text/html <html> <head> <title>CGI Script Output</title> </head> <body> <h1>Form Submission Results</h1> <p>Username: [username]</p> <p>File Line Count: [line_count]</p> <p>Error: [error_message]</p> </body> </html> ``` # Example HTML Form (for testing purposes): ```html <!DOCTYPE html> <html> <body> <form action=\\"/cgi-bin/your_script_name.py\\" method=\\"post\\" enctype=\\"multipart/form-data\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Select file: <input type=\\"file\\" name=\\"userfile\\"><br> <input type=\\"submit\\"> </form> </body> </html> ``` # Example Solution: Demonstrate how you would implement this solution using the features described. Keep in mind the debugging and security aspects as detailed above. ```python #!/usr/bin/env python3 import cgi import cgitb import os import re cgitb.enable() ALLOWED_USERNAME_PATTERN = re.compile(r\'^[w-.]+\') def sanitize_username(username): return ALLOWED_USERNAME_PATTERN.match(username) is not None def main(): print(\\"Content-Type: text/html\\") print() # End of headers form = cgi.FieldStorage() username = form.getfirst(\\"username\\", \\"\\").strip() if not username or not sanitize_username(username): print(\\"<html><body><h1>Error</h1>\\") print(\\"<p>Invalid username. Only alphanumeric characters, dashes, underscores, and periods are allowed.</p>\\") print(\\"</body></html>\\") return if \\"userfile\\" not in form or not form[\\"userfile\\"].file: print(\\"<html><body><h1>Error</h1>\\") print(\\"<p>No file uploaded.</p>\\") print(\\"</body></html>\\") return fileitem = form[\\"userfile\\"] line_count = sum(1 for line in fileitem.file) print(f\\"<html><head><title>CGI Script Output</title></head><body>\\") print(f\\"<h1>Form Submission Results</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>File Line Count: {line_count}</p>\\") print(\\"</body></html>\\") if __name__ == \\"__main__\\": main() ``` **Instructions for Testing**: 1. Save the script in the `cgi-bin` directory of your web server. 2. Ensure the script is executable by using `chmod 755 your_script_name.py`. 3. Test the script by submitting the above form with a valid username and a text file upload.","solution":"#!/usr/bin/env python3 import cgi import cgitb import os import re cgitb.enable() ALLOWED_USERNAME_PATTERN = re.compile(r\'^[w-.]+\') def sanitize_username(username): Verifies if the username contains only alphanumeric characters, dashes, underscores, and periods. return ALLOWED_USERNAME_PATTERN.match(username) is not None def main(): print(\\"Content-Type: text/html\\") print() # End of headers form = cgi.FieldStorage() # Retrieve and sanitize the username username = form.getfirst(\\"username\\", \\"\\").strip() if not username or not sanitize_username(username): print(\\"<html><body><h1>Error</h1>\\") print(\\"<p>Invalid username. Only alphanumeric characters, dashes, underscores, and periods are allowed.</p>\\") print(\\"</body></html>\\") return # Handle file uploads if \\"userfile\\" not in form or not form[\\"userfile\\"].file: print(\\"<html><body><h1>Error</h1>\\") print(\\"<p>No file uploaded.</p>\\") print(\\"</body></html>\\") return fileitem = form[\\"userfile\\"] line_count = sum(1 for line in fileitem.file) # Return HTML response print(f\\"<html><head><title>CGI Script Output</title></head><body>\\") print(f\\"<h1>Form Submission Results</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>File Line Count: {line_count}</p>\\") print(\\"</body></html>\\") if __name__ == \\"__main__\\": main()"},{"question":"Background: You have been given a string representation of pickled data. Your task is to analyze and optimize this pickled data to reduce its size and to unpickle it efficiently. Task: 1. **Opcode Analysis:** - Write a function `analyze_opcodes(pickled_data: str) -> List[Tuple[str, Any, int]]` to return a list of tuples where each tuple contains: - The opcode name (as a string). - The argument of the opcode (can be any Python data type). - The position (index) in the pickled data. 2. **Pickle Optimization:** - Write a function `optimize_pickle(pickled_data: str) -> str` that returns a new pickled string after eliminating unnecessary \\"PUT\\" opcodes to make it smaller and more efficient for unpickling. Input: - `pickled_data`: A string that represents the pickled data. Output: 1. For `analyze_opcodes`: - A list of tuples where each tuple contains: - The opcode name as a string. - The decoded value of the opcode\'s argument (can be any Python object). - The position (index) at which this opcode is located in the pickled data. 2. For `optimize_pickle`: - An optimized pickled string with unnecessary \\"PUT\\" opcodes removed. Constraints: - You are guaranteed that the input string is a valid pickle representation. - The pickled data string may contain any serializable Python objects. - The solution must use `pickletools` for opcode analysis and optimization. Example: ```python pickled_example = b\'x80x03Kx01Kx02x86qx00.\' # Analyze opcodes opcodes = analyze_opcodes(pickled_example) # Output: [(\'PROTO\', 3, 0), (\'BININT1\', 1, 2), (\'BININT1\', 2, 4), (\'TUPLE2\', None, 6), (\'BINPUT\', 0, 7), (\'STOP\', None, 9)] # Optimize pickled data optimized_pickle = optimize_pickle(pickled_example) # Optimized pickled string representation ``` **Hint:** To understand the meaning of each opcode, refer to the `pickletools` documentation for descriptions of opcodes. Performance Requirement: - The solution must run within a reasonable time frame for typical use cases involving small to moderately large pickled data strings.","solution":"import pickletools from typing import List, Tuple, Any def analyze_opcodes(pickled_data: str) -> List[Tuple[str, Any, int]]: Analyzes the opcodes in the given pickled data. :param pickled_data: A string representing the pickled data. :return: A list of tuples, each containing the name of the opcode as a string, its argument, and its position in the pickled data. analysis = [] for opcode in pickletools.genops(pickled_data): op_name = opcode[0].name op_arg = opcode[1] op_pos = opcode[2] analysis.append((op_name, op_arg, op_pos)) return analysis def optimize_pickle(pickled_data: str) -> str: Optimizes the given pickled data by removing unnecessary \\"PUT\\" opcodes. :param pickled_data: A string representing the pickled data. :return: An optimized string of pickled data with unnecessary \\"PUT\\" opcodes removed. # Find potential unnecessary PUT opcodes optimized_data = pickletools.optimize(pickled_data) return optimized_data"},{"question":"Problem Statement You are tasked with processing a log file to extract and transform specific information. Each line in the log file follows a predefined pattern. You need to implement a function `process_log_entries(log_entries)` that extracts and formats information from the log entries. The log entries will look like the following: ``` \\"ERROR 2023-11-05 15:26:43,001 module.py:37 - Exception occurred: index out of range\\" \\"INFO 2023-11-05 15:27:01,002 module.py:42 - User logged in: user123\\" \\"WARNING 2023-11-05 15:27:45,003 module.py:55 - Disk space low\\" \\"ERROR 2023-11-05 15:28:03,004 module.py:60 - Timeout error in operation X\\" ``` You need to process these log entries and extract the following information: 1. **Log Level**: The log level (e.g., ERROR, INFO, WARNING). 2. **Timestamp**: The timestamp of the log entry. 3. **Filename**: The filename where the log was generated. 4. **Line Number**: The line number in the file where the log was generated. 5. **Message**: The log message. The extracted information should be formatted into a dictionary with the following keys: - `log_level` - `timestamp` - `filename` - `line_number` - `message` Your function should transform each log entry into a dictionary and then return a list of these dictionaries. Input Format - `log_entries`: A list of strings, where each string represents a log entry. Output Format - Return a list of dictionaries, where each dictionary contains the extracted and formatted log information from a corresponding log entry. Constraints - Log entries will always follow the specified format. - The function should handle various log levels (ERROR, INFO, WARNING). - The line number will always be an integer. Example Input: ```python log_entries = [ \\"ERROR 2023-11-05 15:26:43,001 module.py:37 - Exception occurred: index out of range\\", \\"INFO 2023-11-05 15:27:01,002 module.py:42 - User logged in: user123\\", \\"WARNING 2023-11-05 15:27:45,003 module.py:55 - Disk space low\\", \\"ERROR 2023-11-05 15:28:03,004 module.py:60 - Timeout error in operation X\\" ] ``` Output: ```python [ { \\"log_level\\": \\"ERROR\\", \\"timestamp\\": \\"2023-11-05 15:26:43,001\\", \\"filename\\": \\"module.py\\", \\"line_number\\": 37, \\"message\\": \\"Exception occurred: index out of range\\" }, { \\"log_level\\": \\"INFO\\", \\"timestamp\\": \\"2023-11-05 15:27:01,002\\", \\"filename\\": \\"module.py\\", \\"line_number\\": 42, \\"message\\": \\"User logged in: user123\\" }, { \\"log_level\\": \\"WARNING\\", \\"timestamp\\": \\"2023-11-05 15:27:45,003\\", \\"filename\\": \\"module.py\\", \\"line_number\\": 55, \\"message\\": \\"Disk space low\\" }, { \\"log_level\\": \\"ERROR\\", \\"timestamp\\": \\"2023-11-05 15:28:03,004\\", \\"filename\\": \\"module.py\\", \\"line_number\\": 60, \\"message\\": \\"Timeout error in operation X\\" } ] ``` Function Signature ```python def process_log_entries(log_entries: list) -> list: pass ``` **Notes:** - Utilize the `re` module for parsing the log entries. - Assume well-formed input as per the specified format.","solution":"import re def process_log_entries(log_entries): log_pattern = re.compile(r\'^(ERROR|INFO|WARNING) (d{4}-d{2}-d{2} d{2}:d{2}:d{2},d{3}) ([w.]+):(d+) - (.+)\') processed_logs = [] for entry in log_entries: match = log_pattern.match(entry) if match: log_level, timestamp, filename, line_number, message = match.groups() processed_logs.append({ \\"log_level\\": log_level, \\"timestamp\\": timestamp, \\"filename\\": filename, \\"line_number\\": int(line_number), \\"message\\": message }) return processed_logs"},{"question":"**Memory-Mapped File Manipulation** **Objective:** In this exercise, you will demonstrate your understanding of the `mmap` module by performing various operations on a memory-mapped file, including writing, reading, and searching content. **Task Details:** 1. **Write a function `create_and_modify_mmap_file(file_path, content, search_term, replace_term)`**: This function should: - Create a new file at `file_path` and write the `content` string to it in binary mode. - Open the file for updating and create a memory-mapped object for the file. - Search for the first occurrence of `search_term` in the memory-mapped object using slice notation. - Replace the found `search_term` with `replace_term`. Ensure that `replace_term` is the same length as `search_term`. - Return the updated content of the memory-mapped file as a byte string. **Function Signature:** ```python def create_and_modify_mmap_file(file_path: str, content: bytes, search_term: bytes, replace_term: bytes) -> bytes: ``` **Parameters:** - `file_path` (str): The path where the file will be created. - `content` (bytes): The initial content to write to the file. - `search_term` (bytes): The term to search for within the memory-mapped file. - `replace_term` (bytes): The term to replace the `search_term` with. Ensure this has the same length as `search_term`. **Requirements/Constraints:** - The `search_term` and `replace_term` must be the same length. - The file should be memory-mapped with writable access. - Proper memory synchronization should be ensured, considering flushing the memory map as necessary. - Handle any relevant exceptions that might occur, such as when `search_term` does not exist in the content. **Example:** ```python file_path = \\"example.txt\\" content = b\\"Hello Python!\\" search_term = b\\"Python\\" replace_term = b\\"Worldd\\" # Function call updated_content = create_and_modify_mmap_file(file_path, content, search_term, replace_term) print(updated_content) # Should output: b\\"Hello Worldd!\\" ``` **Hints:** - Use the `mmap` module to create and manipulate the memory-mapped file. - Ensure proper synchronization after changes using methods like `flush()`. **Note:** - You may want to clean up (delete) the created file after the test to avoid clutter.","solution":"import mmap def create_and_modify_mmap_file(file_path: str, content: bytes, search_term: bytes, replace_term: bytes) -> bytes: if len(search_term) != len(replace_term): raise ValueError(\\"search_term and replace_term must be of the same length\\") with open(file_path, \'wb\') as f: f.write(content) with open(file_path, \'r+b\') as f: with mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_WRITE) as mm: idx = mm.find(search_term) if idx == -1: raise ValueError(\\"search_term not found in the content\\") mm[idx:idx+len(search_term)] = replace_term mm.flush() updated_content = mm[:] # Get the updated content from memory-mapped file return updated_content"},{"question":"# Advanced Python Import Management with `importlib` Objective The aim of this exercise is to leverage the `importlib` module from the Python standard library to dynamically import modules and functions at runtime. This will test your understanding of Python\'s import system, module handling, and some advanced features of dynamic importing. Problem Statement You are to write a function called `dynamic_import` that takes two arguments: 1. `module_name` (str): The name of the module to import. 2. `function_name` (str): The name of the function within the module you wish to import and execute. The function should dynamically import the specified module, then import the specified function from that module, and finally execute the function if it exists. If the module or the function cannot be found, the function should gracefully handle the error by returning `None`. Additionally, if the specified function exists and is executed successfully, the function should return the result of the executed function. Requirements - Use the `importlib` module for all import operations. - Handle any exceptions related to the import process, specifically `ModuleNotFoundError` and `AttributeError`. - Document your code clearly, explaining each step of the process. - Ensure your function is efficient and adheres to Pythonic practices. Constraints - The module and function names will always be provided as strings. - You may assume that if the module exists, it will be correctly structured and importable. - Functions will not require arguments when called. - The function may have side-effects but will not raise exceptions on valid inputs. Example Usage ```python def sample_function(): return \\"Hello, World!\\" # Imagine this function is defined within a module named `sample_module` result = dynamic_import(\'sample_module\', \'sample_function\') assert result == \\"Hello, World!\\" ``` If any error occurs (either module not found or function not found), the function should simply return `None`. Evaluation Criteria - Correctness: The function should meet all specified requirements and constraints. - Robustness: The function should handle errors gracefully. - Efficiency: The function should use `importlib` efficiently without redundant operations. - Clarity: The code should be well-documented and easy to understand. Implement the `dynamic_import` function below: ```python import importlib def dynamic_import(module_name: str, function_name: str): try: module = importlib.import_module(module_name) func = getattr(module, function_name) return func() except (ModuleNotFoundError, AttributeError): return None ```","solution":"import importlib def dynamic_import(module_name: str, function_name: str): Dynamically imports a module and a function from it, then executes the function. Args: - module_name (str): The name of the module to import. - function_name (str): The name of the function within the module to import and execute. Returns: - The result of the executed function if the module and function are found and execute successfully. - None if the module or function cannot be found or there is any error during import or execution. try: # Dynamically import the specified module module = importlib.import_module(module_name) # Get the specified function from the imported module func = getattr(module, function_name) # Execute the function return func() except (ModuleNotFoundError, AttributeError): # If importing the module or function fails, return None return None"},{"question":"**Question: Terminal Mode Conversion in Unix** You are tasked with writing a Python program that demonstrates the use of the `tty` module to control terminal modes. Specifically, you need to: 1. Implement a function `set_terminal_mode(fd: int, mode: str) -> None` that changes the terminal mode of the given file descriptor `fd` to either `raw` or `cbreak`, depending on the value of `mode`. 2. Implement a script that: - Reads input from the user. - Sets the terminal to `raw` mode and reads the input again. - Sets the terminal to `cbreak` mode and reads the input again. - Resets the terminal to the default mode after each read. **Function: `set_terminal_mode`** ```python from typing import Literal def set_terminal_mode(fd: int, mode: Literal[\'raw\', \'cbreak\']) -> None: Set the terminal mode for the given file descriptor fd. Args: fd (int): The file descriptor of the terminal. mode (Literal[\'raw\', \'cbreak\']): The terminal mode to set; either \'raw\' or \'cbreak\'. Returns: None # Your implementation here ``` **Script: `main`** ```python import sys import tty import termios def main(): fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) try: # Read input in default mode print(\\"Default mode: Please type something and press Enter\\") input() # Set to raw mode and read input set_terminal_mode(fd, \'raw\') print(\\"Raw mode: Please type something and press Enter\\") input() # Set to cbreak mode and read input set_terminal_mode(fd, \'cbreak\') print(\\"Cbreak mode: Please type something and press Enter\\") input() finally: # Reset to default mode termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) print(\\"Terminal mode reset to default\\") if __name__ == \\"__main__\\": main() ``` **Requirements:** 1. The function `set_terminal_mode` must use the appropriate `tty` functions to set the terminal mode. 2. The script must handle mode changes and reset the terminal to its default state after each input read. 3. Ensure that the program works only on Unix-based systems. **Constraints:** - The program should handle any exceptions that may occur during terminal mode changes. - The `set_terminal_mode` function should only accept the modes \'raw\' and \'cbreak\', and it should raise a `ValueError` for any other mode input. **Input:** - The function `set_terminal_mode` takes an integer `fd` and a string `mode`. - The script takes no external input but interacts with the user through the terminal. **Output:** - The script prints prompts for the user to type input in different terminal modes and a message when the terminal mode is reset to default.","solution":"import tty import termios def set_terminal_mode(fd: int, mode: str) -> None: Set the terminal mode for the given file descriptor fd. Args: fd (int): The file descriptor of the terminal. mode (str): The terminal mode to set; either \'raw\' or \'cbreak\'. Returns: None if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) else: raise ValueError(f\\"Invalid mode: {mode}. Supported modes are \'raw\' and \'cbreak\'.\\")"},{"question":"# Question: Visualizing Messy Data using Seaborn You have been provided with a dataset from a psychology experiment in a CSV file named `experiment_data.csv`. This dataset captures the performance scores of 30 participants under three different conditions: `conditionA`, `conditionB`, and `conditionC`. Each participant\'s performance score is recorded for each condition, but the data is currently in a messy format. **Objective:** 1. Transform the messy dataset into a tidy long-form dataset using pandas. 2. Create a line plot using Seaborn to visualize the average performance scores across the three conditions. The CSV file `experiment_data.csv` has the following columns: - `subject_id`: The unique identifier for each participant. - `conditionA`: Performance score under condition A. - `conditionB`: Performance score under condition B. - `conditionC`: Performance score under condition C. **Requirements:** - Read the dataset from the CSV file into a pandas DataFrame. - Use the `pandas.melt` function to transform the data into a tidy long-form format. The resulting DataFrame should have columns `subject_id`, `condition`, and `score`. - Use Seaborn\'s `relplot` to create a line plot showing the average performance score for each condition. The x-axis should represent the conditions, and the y-axis should represent the scores. Use different markers for each condition. **Constraints:** - The code should be optimized and avoid unnecessary computations. - Ensure that your plot is clearly labeled with appropriate titles for axes and the plot itself. **Input:** - Path to the CSV file `experiment_data.csv`. **Output:** - A line plot visualizing the average performance scores for the three conditions. **Example:** Given the following data in `experiment_data.csv`: ``` subject_id,conditionA,conditionB,conditionC 1,5,7,6 2,6,6,5 3,7,7,6 ... ``` Transform the above data to: ``` subject_id,condition,score 1,conditionA,5 2,conditionA,6 3,conditionA,7 ... ``` And then create a plot similar to: ``` Average Performance Scores by Condition Condition | Average Score ``` (Note: This example illustrates the transformation process and desired plot output.) ``` python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_experiment_data(csv_file_path): # Step 1: Read the dataset from the CSV file data = pd.read_csv(csv_file_path) # Step 2: Transform the data into long-form using pandas.melt long_form_data = data.melt(id_vars=[\\"subject_id\\"], var_name=\\"condition\\", value_name=\\"score\\") # Step 3: Create the line plot using Seaborn plt.figure(figsize=(10, 6)) sns.set_theme(style=\\"whitegrid\\") sns.relplot( data=long_form_data, x=\\"condition\\", y=\\"score\\", kind=\\"line\\", marker=\\"o\\", estimator=\\"mean\\" ).set(title=\\"Average Performance Scores by Condition\\") # Show the plot plt.show() # Sample usage # visualize_experiment_data(\\"experiment_data.csv\\") ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_experiment_data(csv_file_path): Reads the dataset from the path, transforms it into a tidy long-form, and creates a line plot showing the average performance scores across the three conditions. Parameters: csv_file_path (str): Path to the CSV file. Returns: tuple: (DataFrame of long-form data, the plot object) # Step 1: Read the dataset from the CSV file data = pd.read_csv(csv_file_path) # Step 2: Transform the data into long-form using pandas.melt long_form_data = data.melt(id_vars=[\\"subject_id\\"], var_name=\\"condition\\", value_name=\\"score\\") # Step 3: Create the line plot using Seaborn plt.figure(figsize=(10, 6)) sns.set_theme(style=\\"whitegrid\\") plot = sns.relplot( data=long_form_data, x=\\"condition\\", y=\\"score\\", kind=\\"line\\", marker=\\"o\\", estimator=\\"mean\\" ) plot.set(title=\\"Average Performance Scores by Condition\\") # Show the plot plt.show() return long_form_data, plot"},{"question":"**Objective:** Implement a custom Python type using Python\'s C API concepts. **Task:** You are required to create a class `CustomDataType` in Python that mimics the behavior described in the provided documentation for Python\'s C API. The class should showcase proficiency in managing custom attributes, implementing comparison operations, and supporting iteration over its elements. **Requirements:** 1. **Class Definition:** - Define a class `CustomDataType`. 2. **Initialization:** - The class should accept a list of integers during initialization. 3. **Attribute Management:** - Implement custom getter and setter for an attribute `data`, which holds the list of integers. - The attribute `data` should be read-only once set during initialization. Any attempts to modify it should raise an appropriate exception. 4. **String Representation:** - Override the `__repr__` method to return the string: `\\"CustomDataType: <data>\\"`. - Override the `__str__` method to return the string representation of the `data` attribute. 5. **Comparison:** - Implement rich comparison methods such that instances of `CustomDataType` can be compared based on the sum of integers in their `data` attribute. - Use the operators `<, <=, ==, !=, >, >=` for comparisons. 6. **Iterator Protocol:** - Implement the iterator protocol methods (`__iter__` and `__next__`) for the class to allow iteration over the integers in the `data` attribute. **Constraints:** - You should not use any external libraries. - Ensure that your class properly raises exceptions for invalid operations as described. **Input:** - Initialization: `custom_obj = CustomDataType([1, 2, 3, 4])` **Output:** - `repr(custom_obj)` should return `\\"CustomDataType: [1, 2, 3, 4]\\"` - `str(custom_obj)` should return `\\"[1, 2, 3, 4]\\"` - Iteration: `for value in custom_obj:` should iterate over `1, 2, 3, 4` - Comparison: Compare instances like `custom_obj1 == custom_obj2`, `custom_obj1 < custom_obj2`, etc., based on the sum of `data` values. **Example Usage:** ```python custom_obj1 = CustomDataType([1, 2, 3, 4]) custom_obj2 = CustomDataType([4, 5, 6]) print(repr(custom_obj1)) # Output: CustomDataType: [1, 2, 3, 4] print(str(custom_obj1)) # Output: [1, 2, 3, 4] for value in custom_obj1: print(value) # Output: 1, 2, 3, 4 print(custom_obj1 == custom_obj2) # Output: False print(custom_obj1 > custom_obj2) # Output: False ``` **Note:** Your implementation should respect Python data model conventions and manage the attributes and methods properly to mimic the behavior described above.","solution":"class CustomDataType: def __init__(self, data): if not all(isinstance(i, int) for i in data): raise ValueError(\\"All elements in data must be integers\\") self._data = data # Using a private variable to store the data @property def data(self): return self._data def __repr__(self): return f\\"CustomDataType: {self._data}\\" def __str__(self): return str(self._data) def __eq__(self, other): if not isinstance(other, CustomDataType): return NotImplemented return sum(self._data) == sum(other.data) def __ne__(self, other): return not self == other def __lt__(self, other): if not isinstance(other, CustomDataType): return NotImplemented return sum(self._data) < sum(other.data) def __le__(self, other): if not isinstance(other, CustomDataType): return NotImplemented return sum(self._data) <= sum(other.data) def __gt__(self, other): if not isinstance(other, CustomDataType): return NotImplemented return sum(self._data) > sum(other.data) def __ge__(self, other): if not isinstance(other, CustomDataType): return NotImplemented return sum(self._data) >= sum(other.data) def __iter__(self): self._index = 0 return self def __next__(self): if self._index < len(self._data): result = self._data[self._index] self._index += 1 return result else: raise StopIteration"},{"question":"# Importing Modules from ZIP Archives You are tasked with demonstrating a clear understanding of the `zipimport` module in Python. Specifically, you\'ll work with the `zipimporter` class to import and manipulate modules from a ZIP archive. # Problem Statement You will implement a function `import_and_execute(zip_path: str, module_name: str, function_name: str) -> Any` that meets the following requirements: 1. Takes three arguments: - `zip_path`: A string representing the path to the ZIP file. - `module_name`: A string representing the fully qualified (dotted) module name within the zip file. - `function_name`: A string representing the name of the function to be executed from the module. 2. The function should: - Use `zipimporter` to load the specified module from the ZIP file. - Execute the specified function from the imported module. - Return the result of the function call. # Constraints - You may assume that the ZIP file exists and is accessible at the specified `zip_path`. - The module specified must exist within the ZIP file, otherwise a `zipimport.ZipImportError` should be raised. - The function specified must exist in the module and must be executable. Raise a `ValueError` if the function cannot be found or executed. # Example Suppose the ZIP archive `example.zip` contains a module `lib.mathoperations` and within this module, there is a function `add(a, b)` which returns the sum of `a` and `b`. ```python # The content of lib/mathoperations.py within the ZIP file def add(a, b): return a + b ``` **Function call:** ```python result = import_and_execute(\'example.zip\', \'lib.mathoperations\', \'add\', 5, 3) ``` **Expected Output:** ```python 8 ``` # Function Signature ```python from typing import Any def import_and_execute(zip_path: str, module_name: str, function_name: str, *args, **kwargs) -> Any: # TODO: Implement this function pass ``` # Implementation Notes - Use the `zipimport.zipimporter` class for importing the module from the ZIP file. - Handle exceptions and errors gracefully, providing meaningful error messages where applicable. - Ensure that arguments passed to the method to be executed are handled correctly with `*args` and `**kwargs`. # Additional Information The example provided at the end of the documentation can help understand the usage and functionality of the `zipimport` module and `zipimporter` class. Utilize this to build and test your solution effectively.","solution":"from typing import Any import zipimport def import_and_execute(zip_path: str, module_name: str, function_name: str, *args, **kwargs) -> Any: Imports a module from a ZIP file and executes a specified function from it. Parameters: zip_path (str): The path to the ZIP file. module_name (str): The fully qualified module name inside the ZIP file. function_name (str): The name of the function to execute. *args, **kwargs: Arguments and keyword arguments to pass to the function. Returns: Any: The result of the function call. Raises: zipimport.ZipImportError: If there is an issue importing the module from the ZIP file. ValueError: If the function is not found or cannot be executed. try: # Create a zip importer for the given zip file importer = zipimport.zipimporter(zip_path) # Import the specified module from the zip file module = importer.load_module(module_name) except zipimport.ZipImportError as e: raise zipimport.ZipImportError(f\\"Could not import module \'{module_name}\' from \'{zip_path}\'.\\") from e try: # Get the specified function from the imported module func = getattr(module, function_name) # Check if it is callable if not callable(func): raise ValueError(f\\"\'{function_name}\' in module \'{module_name}\' is not callable.\\") except AttributeError as e: raise ValueError(f\\"Function \'{function_name}\' not found in module \'{module_name}\'.\\") from e # Execute the function with the provided arguments and keyword arguments return func(*args, **kwargs)"},{"question":"Problem Statement: You are given an iterable of dictionaries where each dictionary represents a student record with keys \\"name\\" (a string), \\"age\\" (an integer), and \\"grades\\" (a list of integers). Implement a function `process_student_records` that processes this iterable in the following way: 1. Returns a list of names of students who have an average grade of 75 or above. 2. Returns the name and grade of the top scorer among all the students. 3. Returns the names of students who have any failing grades (grades less than 50). Additionally, ensure that your function correctly handles cases where: - The iterable is empty. - There are multiple students with the highest score. - Any dictionary might be missing a key or contains invalid data formats. **Function Signature:** ```python def process_student_records(students: Iterable[Dict[str, any]]) -> Tuple[List[str], Tuple[str, int], List[str]]: pass ``` **Input:** - `students` (Iterable[Dict[str, any]]): An iterable of student records. Each record is a dictionary with keys \\"name\\" (str), \\"age\\" (int), and \\"grades\\" (List[int]). **Output:** - A tuple containing three elements: - List of names of students with an average grade of 75 or above. - A tuple containing the name of the top scorer and their highest grade. - A list of names of students with any failing grades (grades less than 50). **Example:** ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 21, \\"grades\\": [85, 90, 78]}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grades\\": [50, 60, 65]}, {\\"name\\": \\"Charlie\\", \\"age\\": 23, \\"grades\\": [45, 40, 55]}, {\\"name\\": \\"David\\", \\"age\\": 24, \\"grades\\": [80, 85, 88]}, ] output = process_student_records(students) print(output) # Expected output: ([\'Alice\', \'David\'], (\'Alice\', 90), [\'Charlie\']) ``` **Constraints:** - You may assume that the input will always be an iterable of dictionaries. - Handle missing keys and improper formats gracefully by skipping such dictionaries. **Performance Requirements:** - The solution should be optimized to handle large data sets efficiently.","solution":"from typing import Iterable, Dict, List, Tuple, Any def process_student_records(students: Iterable[Dict[str, Any]]) -> Tuple[List[str], Tuple[str, int], List[str]]: above_average = [] top_scorer = (\\"\\", 0) failing_students = [] for student in students: # Validate necessary fields if not all(key in student for key in (\\"name\\", \\"age\\", \\"grades\\")): continue name = student[\\"name\\"] grades = student[\\"grades\\"] # Validate the data types if not isinstance(name, str) or not isinstance(grades, list) or not all(isinstance(grade, int) for grade in grades): continue if not grades: continue # Calculate average grade avg_grade = sum(grades) / len(grades) if avg_grade >= 75: above_average.append(name) # Check for any failing grades if any(grade < 50 for grade in grades): failing_students.append(name) # Determine top scorer highest_grade = max(grades) if highest_grade > top_scorer[1]: top_scorer = (name, highest_grade) elif highest_grade == top_scorer[1]: top_scorer = (top_scorer[0], highest_grade) # Tie case, keep one of the top scorers return (above_average, top_scorer, failing_students)"},{"question":"**Question: Implement a Command-Line Tool Using getopt** --- # Description You are required to implement a Python command-line tool that processes a series of arguments using the `getopt` module. The tool will support both short and long options for displaying help, verbosity, specifying the output file, and setting a condition. Your task is to correctly parse these options and print appropriate responses. # Requirements 1. **Options to Support:** - `-h` or `--help`: Display the help message and exit. - `-v`: Enable verbose mode. - `-o <file>` or `--output=<file>`: Specify the output file. - `--condition=<value>`: Set the condition to `<value>`. 2. **Function Implementation:** - Implement a function `parse_arguments(args)` that accepts a list of command-line arguments (excluding the script name) and returns a dictionary with the parsed options. - Implement a function `display_help()` that prints the help message. 3. **Error Handling:** - If an unrecognized option or a missing argument for an option is encountered, the program should print an error message and call the `display_help()` function. # Input and Output - **Input:** A list of command-line arguments. - **Output:** A dictionary containing the parsed options and a help message printed to the console if applicable. # Example ```python import sys import getopt def parse_arguments(args): shortopts = \\"hvo:\\" longopts = [\\"help\\", \\"output=\\", \\"condition=\\"] try: opts, remaining_args = getopt.getopt(args, shortopts, longopts) except getopt.GetoptError as err: print(str(err)) display_help() sys.exit(2) options = {\\"verbose\\": False, \\"output\\": None, \\"condition\\": None} for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): display_help() sys.exit() elif opt == \\"-v\\": options[\\"verbose\\"] = True elif opt in (\\"-o\\", \\"--output\\"): options[\\"output\\"] = arg elif opt == \\"--condition\\": options[\\"condition\\"] = arg else: assert False, \\"unhandled option\\" return options def display_help(): help_message = Usage: script.py [options] Options: -h, --help show this help message and exit -v enable verbose mode -o FILE, --output=FILE specify the output file --condition=VALUE set condition to VALUE print(help_message) # Example Usage if __name__ == \\"__main__\\": args = [\\"-v\\", \\"--output=file.txt\\", \\"--condition=active\\"] options = parse_arguments(args) print(options) ``` In this example, the `parse_arguments` function parses the arguments and returns a dictionary. The `display_help` function prints the help message. Test your implementation by running the script with different combinations of command-line arguments. **Constraints and Limits:** - Do not use any other command-line parsing libraries (e.g., `argparse`). - Ensure appropriate error handling for unrecognized options and missing arguments. - The function should behave gracefully when encountering `-h` or `--help` even if other options are provided.","solution":"import sys import getopt def parse_arguments(args): shortopts = \\"hvo:\\" longopts = [\\"help\\", \\"output=\\", \\"condition=\\"] try: opts, remaining_args = getopt.getopt(args, shortopts, longopts) except getopt.GetoptError as err: print(str(err)) display_help() sys.exit(2) options = {\\"verbose\\": False, \\"output\\": None, \\"condition\\": None} for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): display_help() sys.exit() elif opt == \\"-v\\": options[\\"verbose\\"] = True elif opt in (\\"-o\\", \\"--output\\"): options[\\"output\\"] = arg elif opt == \\"--condition\\": options[\\"condition\\"] = arg else: assert False, \\"unhandled option\\" return options def display_help(): help_message = Usage: script.py [options] Options: -h, --help show this help message and exit -v enable verbose mode -o FILE, --output=FILE specify the output file --condition=VALUE set condition to VALUE print(help_message)"},{"question":"# PyTorch Coding Assessment: Mixed Distributions Analysis Objective: Your task is to implement a function that analyzes data from multiple probability distributions using PyTorch\'s `torch.distributions` module. You should be able to handle various distributions and output statistical properties and samples from these distributions. Problem Statement: Write a Python function `analyze_distributions(distributions: List[Tuple[str, Dict]], num_samples: int) -> Dict` which does the following: 1. **Inputs:** - `distributions`: A list of tuples where each tuple contains: - A string representing the name of the distribution (as available in `torch.distributions`). - A dictionary containing the parameters needed to initialize this distribution. - `num_samples`: An integer representing the number of samples to draw from each distribution. 2. **Output:** - The function returns a dictionary where the keys are distribution names, and the values are another dictionary comprising: - `\\"mean\\"`: The mean of the drawn samples. - `\\"variance\\"`: The variance of the drawn samples. - `\\"samples\\"`: A list of samples drawn from the distribution (of length `num_samples`). For example, if the input distributions are `[\\"Normal\\", {\\"loc\\": 0, \\"scale\\": 1}]`, the function should create a Normal distribution with mean 0 and standard deviation 1, draw `num_samples` from it, and compute the mean and variance of these samples. 3. **Constraints:** - You need to ensure that the function handles various distributions correctly. Validate input distributions and parameters to ensure they conform to the requirements of `torch.distributions`. Example: ```python from torch.distributions import Normal, Exponential import torch def analyze_distributions(distributions, num_samples): result = {} for dist_name, params in distributions: dist = getattr(torch.distributions, dist_name)(**params) samples = dist.sample((num_samples,)) result[dist_name] = { \\"mean\\": torch.mean(samples).item(), \\"variance\\": torch.var(samples).item(), \\"samples\\": samples.tolist() } return result # Example usage distributions = [ (\\"Normal\\", {\\"loc\\": 0, \\"scale\\": 1}), (\\"Exponential\\", {\\"rate\\": 1.0}) ] num_samples = 1000 output = analyze_distributions(distributions, num_samples) print(output) ``` Your implementation should construct the necessary distributions, draw samples, and compute mean and variance for the provided distributions using PyTorch\'s `torch.distributions` module. Note: - Ensure you handle exceptions and invalid parameters gracefully, providing meaningful error messages. - Write clean, readable, and well-documented code. - Do not use any libraries other than PyTorch for this assignment.","solution":"import torch def analyze_distributions(distributions, num_samples): Analyzes data from multiple probability distributions using PyTorch\'s torch.distributions module. Parameters: - distributions: List of tuples where each tuple contains a string representing the name of the distribution and a dictionary containing the parameters needed to initialize the distribution. - num_samples: An integer representing the number of samples to draw from each distribution. Returns: - A dictionary where keys are distribution names and values are dicts containing \'mean\', \'variance\', and \'samples\'. results = {} for dist_name, params in distributions: try: dist_class = getattr(torch.distributions, dist_name) distribution = dist_class(**params) samples = distribution.sample((num_samples,)) results[dist_name] = { \\"mean\\": torch.mean(samples).item(), \\"variance\\": torch.var(samples).item(), \\"samples\\": samples.tolist() } except AttributeError: raise ValueError(f\\"Distribution \'{dist_name}\' is not found in torch.distributions\\") except TypeError as e: raise ValueError(f\\"Error initializing the distribution \'{dist_name}\' with parameters {params}: {e}\\") return results"},{"question":"Coding Assessment Question # Objective Implement a class that manages a pool of worker threads using the `threading` module. The class should allow tasks to be submitted to the pool and processed in parallel by the worker threads. Synchronization mechanisms should be used to ensure thread-safe access to shared resources. # Requirements 1. **Class Definition**: Define a class `ThreadPool`. 2. **Initialization**: - The class should accept the number of worker threads as a parameter. - Initialize a queue to hold submitted tasks. 3. **Task Submission**: - Implement a method `submit(task, *args, **kwargs)` that accepts a callable `task` and its arguments. - Add the task to the queue of tasks to be executed by worker threads. 4. **Worker Threads**: - Create and start the specified number of worker threads during initialization. - Worker threads should continuously fetch and execute tasks from the task queue. - Use a synchronization mechanism to ensure thread-safe access to the task queue. 5. **Shutdown**: - Implement a method `shutdown()` that gracefully stops all worker threads after all submitted tasks have been executed. # Constraints - The task queue must be thread-safe. - The solution should handle any callable `task` with arbitrary arguments. - The system should shut down gracefully and avoid deadlocks. # Implementation ```python import threading import queue class ThreadPool: def __init__(self, num_workers): self.tasks = queue.Queue() self.workers = [] self.shutdown_event = threading.Event() for _ in range(num_workers): worker = threading.Thread(target=self.worker) worker.start() self.workers.append(worker) def submit(self, task, *args, **kwargs): self.tasks.put((task, args, kwargs)) def worker(self): while not self.shutdown_event.is_set(): try: task, args, kwargs = self.tasks.get(timeout=1) task(*args, **kwargs) self.tasks.task_done() except queue.Empty: continue def shutdown(self): self.shutdown_event.set() for worker in self.workers: worker.join() # Example usage def example_task(data): print(f\'Processing {data}\') if __name__ == \\"__main__\\": pool = ThreadPool(3) for i in range(10): pool.submit(example_task, f\'task-{i}\') pool.shutdown() ``` # Input and Output - The class does not take input in the traditional sense but should be tested using task submissions and shutdowns. - The output should be the result of the tasks executed by the worker threads. # Testing 1. Initialize the `ThreadPool` with a set number of worker threads. 2. Submit multiple tasks to be processed by the worker threads. 3. Call `shutdown()` and ensure all tasks are completed before the program exits. 4. Verify that thread safety is maintained throughout task processing. # Notes - Ensure all worker threads are terminated gracefully. - Handle possible exceptions in task execution to avoid crashing worker threads.","solution":"import threading import queue class ThreadPool: def __init__(self, num_workers): self.tasks = queue.Queue() self.workers = [] self.shutdown_event = threading.Event() for _ in range(num_workers): worker = threading.Thread(target=self.worker) worker.start() self.workers.append(worker) def submit(self, task, *args, **kwargs): self.tasks.put((task, args, kwargs)) def worker(self): while not self.shutdown_event.is_set(): try: task, args, kwargs = self.tasks.get(timeout=1) try: task(*args, **kwargs) except Exception as e: print(f\\"Task raised an exception: {e}\\") self.tasks.task_done() except queue.Empty: continue def shutdown(self): self.tasks.join() self.shutdown_event.set() for worker in self.workers: worker.join()"},{"question":"# **Coding Challenge: Working with ZIP Archives** **Objective:** You need to demonstrate your ability to work with ZIP archives using the `zipfile` module in Python. This task requires you to implement a set of functions that will create, manipulate, and extract ZIP archives. This will test your understanding of how to use the various methods and classes provided by the `zipfile` module. **Task:** 1. **Function: create_zip_archive** - **Description**: Create a new ZIP archive and add specified files into it. - **Input**: - `zip_name` (str): Name of the ZIP file to be created. - `files` (list of tuples): List of tuples where each tuple contains: - `file_path` (str): Path of the file to add into the archive. - `arcname` (str): Name to assign to the file within the archive. - `compression` (str): Type of compression to use. Can be \\"ZIP_STORED\\", \\"ZIP_DEFLATED\\", \\"ZIP_BZIP2\\", or \\"ZIP_LZMA\\". - **Output**: None - **Behavior**: The function should create a ZIP archive with the specified name and add each specified file into the archive with the given compression method. 2. **Function: extract_files** - **Description**: Extract all files from a given ZIP archive to a specified directory. - **Input**: - `zip_name` (str): Name of the ZIP file to extract from. - `extract_path` (str): Path to the directory where files should be extracted. - **Output**: None - **Behavior**: The function should extract all the files from the specified ZIP archive to the provided directory. 3. **Function: get_zip_info** - **Description**: Retrieve and print information about each file within a ZIP archive. - **Input**: - `zip_name` (str): Name of the ZIP file to retrieve info from. - **Output**: None - **Behavior**: The function should print the information (filename, date_time, file_size, and compress_size) about each file in the ZIP archive. # **Constraints:** - You must handle exceptions appropriately, especially when working with file operations. - Your functions should use the context management features of the `zipfile` module to ensure proper handling of file resources. - Assume that all specified file paths for `create_zip_archive` exist and are valid. # **Example Usage:** ```python # Function to create a ZIP archive create_zip_archive( zip_name=\'example.zip\', files=[(\'file1.txt\', \'file1_in_archive.txt\'), (\'file2.txt\', \'file2_in_archive.txt\')], compression=\'ZIP_DEFLATED\' ) # Function to extract files extract_files(zip_name=\'example.zip\', extract_path=\'extracted_files/\') # Function to get ZIP info get_zip_info(zip_name=\'example.zip\') ``` # **Evaluation Criteria:** - **Correctness**: The functions should accurately perform the requested operations without errors. - **Efficiency**: The solutions should be efficient and make appropriate use of the methods provided by the `zipfile` module. - **Clarity**: The code should be clear and well-commented, making it easy to understand the logic and flow. - **Exception Handling**: Proper handling of exceptions to account for possible errors during file operations.","solution":"import zipfile def create_zip_archive(zip_name, files, compression): Create a new ZIP archive and add specified files into it. Args: zip_name (str): Name of the ZIP file to be created. files (list of tuples): List of tuples where each tuple contains: - file_path (str): Path of the file to add into the archive. - arcname (str): Name to assign to the file within the archive. compression (str): Type of compression to use. Can be \\"ZIP_STORED\\", \\"ZIP_DEFLATED\\", \\"ZIP_BZIP2\\", or \\"ZIP_LZMA\\". Returns: None compression_map = { \\"ZIP_STORED\\": zipfile.ZIP_STORED, \\"ZIP_DEFLATED\\": zipfile.ZIP_DEFLATED, \\"ZIP_BZIP2\\": zipfile.ZIP_BZIP2, \\"ZIP_LZMA\\": zipfile.ZIP_LZMA } with zipfile.ZipFile(zip_name, \'w\', compression_map[compression]) as zipf: for file_path, arcname in files: zipf.write(file_path, arcname) def extract_files(zip_name, extract_path): Extract all files from a given ZIP archive to a specified directory. Args: zip_name (str): Name of the ZIP file to extract from. extract_path (str): Path to the directory where files should be extracted. Returns: None with zipfile.ZipFile(zip_name, \'r\') as zipf: zipf.extractall(extract_path) def get_zip_info(zip_name): Retrieve and print information about each file within a ZIP archive. Args: zip_name (str): Name of the ZIP file to retrieve info from. Returns: None with zipfile.ZipFile(zip_name, \'r\') as zipf: for info in zipf.infolist(): print(f\\"Filename: {info.filename}\\") print(f\\"Date & Time: {info.date_time}\\") print(f\\"Original Size: {info.file_size} bytes\\") print(f\\"Compressed Size: {info.compress_size} bytes\\") print(\\"n\\")"},{"question":"# Objective: The objective of this coding assessment is to evaluate your understanding of outlier detection techniques using scikit-learn. You are required to implement various outlier detection algorithms, fit models on training data, predict outliers on test data, and compare the performance of the models. # Problem Statement: You are given a dataset containing numerical features. Your task is to: 1. Implement outlier detection using the following methods: - Isolation Forest - Local Outlier Factor (LOF) - One-Class SVM - Elliptic Envelope 2. Train each model on the given training dataset. 3. Use the trained models to predict outliers on a given test dataset. 4. Compare the performance of the models using appropriate evaluation metrics. # Input: - `X_train`: A 2D numpy array of shape (n_train_samples, n_features) representing the training data. - `X_test`: A 2D numpy array of shape (n_test_samples, n_features) representing the test data. # Output: - A dictionary containing the outlier prediction results for each model. The keys of the dictionary should be the model names (\'IsolationForest\', \'LocalOutlierFactor\', \'OneClassSVM\', \'EllipticEnvelope\'), and the values should be numpy arrays of shape (n_test_samples,) containing the predicted labels (1 for inliers and -1 for outliers). # Constraints: - You should use the default parameters for each model unless specified otherwise. - Ensure that you set appropriate parameters for novelty detection when using the Local Outlier Factor. # Example: Given the following input: ```python import numpy as np X_train = np.array([[1.1, 2.2], [1.2, 2.1], [1.3, 2.3], [7.8, 8.9], [1.5, 2.0]]) X_test = np.array([[1.15, 2.25], [8.0, 9.0], [1.2, 2.0]]) results = detect_outliers(X_train, X_test) ``` Expected output format: ```python { \'IsolationForest\': np.array([1, -1, 1]), \'LocalOutlierFactor\': np.array([1, -1, 1]), \'OneClassSVM\': np.array([1, -1, 1]), \'EllipticEnvelope\': np.array([1, -1, 1]) } ``` # Implementation: Implement the `detect_outliers` function to solve the problem. ```python from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope def detect_outliers(X_train, X_test): results = {} # Isolation Forest isolation_forest = IsolationForest() isolation_forest.fit(X_train) results[\'IsolationForest\'] = isolation_forest.predict(X_test) # Local Outlier Factor (for novelty detection) lof = LocalOutlierFactor(novelty=True) lof.fit(X_train) results[\'LocalOutlierFactor\'] = lof.predict(X_test) # One-Class SVM one_class_svm = OneClassSVM() one_class_svm.fit(X_train) results[\'OneClassSVM\'] = one_class_svm.predict(X_test) # Elliptic Envelope elliptic_envelope = EllipticEnvelope() elliptic_envelope.fit(X_train) results[\'EllipticEnvelope\'] = elliptic_envelope.predict(X_test) return results ``` Test your implementation with the provided example and verify the output.","solution":"from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope import numpy as np def detect_outliers(X_train, X_test): results = {} # Isolation Forest isolation_forest = IsolationForest() isolation_forest.fit(X_train) results[\'IsolationForest\'] = isolation_forest.predict(X_test) # Local Outlier Factor (for novelty detection) lof = LocalOutlierFactor(novelty=True) lof.fit(X_train) results[\'LocalOutlierFactor\'] = lof.predict(X_test) # One-Class SVM one_class_svm = OneClassSVM() one_class_svm.fit(X_train) results[\'OneClassSVM\'] = one_class_svm.predict(X_test) # Elliptic Envelope elliptic_envelope = EllipticEnvelope() elliptic_envelope.fit(X_train) results[\'EllipticEnvelope\'] = elliptic_envelope.predict(X_test) return results"},{"question":"You are tasked with implementing a custom neural network module that uses the `torch.cond` function. Your module should adapt its behavior based on the sum of input tensor elements. Specifically, if the sum of the elements in the input tensor is greater than a given threshold, the module should calculate the element-wise cosine of the input tensor. Otherwise, it should calculate the element-wise tangent of the input tensor. # Function Requirements: - Implement a class `DynamicBehaviorModule` derived from `torch.nn.Module`. - The module should have: - An initializer that takes a threshold value. - A `forward` method that uses `torch.cond` to determine whether to compute the cosine or tangent of the input tensor based on whether the sum of the input tensor\'s elements exceeds the threshold. # Input: - A PyTorch tensor `x` of any shape. - A float threshold value provided during initialization. # Output: - A PyTorch tensor of the same shape as `x`, containing either the cosine or tangent values of the corresponding elements of `x`, depending on the sum of `x` in comparison to the threshold. # Example: ```python import torch # Defining the custom module class DynamicBehaviorModule(torch.nn.Module): def __init__(self, threshold: float): super(DynamicBehaviorModule, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): return x.cos() def false_fn(x: torch.Tensor): return x.tan() return torch.cond(torch.sum(x) > self.threshold, true_fn, false_fn, (x,)) # Instantiate the module with a threshold value model = DynamicBehaviorModule(threshold=5.0) # Input tensor inp_tensor = torch.randn(4, 4) # Example tensor # Apply the dynamic behavior module output = model(inp_tensor) print(output) # Expected behavior: Checks the sum of the input tensor and computes cosine or tangent. ``` # Constraints: - Do not use any functions or methods from other libraries to perform the condition check. - Ensure your solution is efficient and leverages the `torch.cond` control flow appropriately. Design and implement the `DynamicBehaviorModule` class following the requirements outlined.","solution":"import torch import torch.nn as nn class DynamicBehaviorModule(nn.Module): def __init__(self, threshold: float): super(DynamicBehaviorModule, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(): return torch.cos(x) def false_fn(): return torch.tan(x) # Determine which function to use based on the sum of elements. if torch.sum(x) > self.threshold: return true_fn() else: return false_fn()"},{"question":"Coding Assessment Question # Objective You are tasked with creating a function that reads a file, concatenates a user-given suffix to each line of the file, and returns the resulting concatenated bytes object. This will involve checking the type, handling bytes objects, and string manipulations using provided methods. # Function Signature ```python def process_file_with_suffix(file_path: str, suffix: bytes) -> bytes: pass ``` # Input - `file_path` (str): The path to the file containing text lines. - `suffix` (bytes): The suffix to be added at the end of each line. # Output - A bytes object containing each line\'s content from the file concatenated with the given suffix. # Detailed Requirements 1. **Read File**: Open and read the file specified by `file_path`. 2. **Concatenate Suffix**: For each line read from the file, concatenate the provided suffix (which is of type `bytes`). 3. **Return Bytes Object**: Collect all lines with suffixed content and return as a single bytes object. # Constraints - Ensure the suffix is of type `bytes`, otherwise raise a `TypeError`. - Handle cases where the file might not exist (raise an appropriate exception). - Ensure that each line\'s newline character is preserved. # Example ```python # Assuming \'example.txt\' contains: # Hello # World suffix = b\'!!!\' result = process_file_with_suffix(\'example.txt\', suffix) # The result should be: # b\'Hello!!!nWorld!!!n\' ``` # Notes - Pythonâ€™s `open()` function can be used to read the file. - Make use of `PyBytes_Concat` or equivalent Python method for concatenating bytes. - Ensure proper type checks and handle potential errors gracefully. # Tasks - Implement the function `process_file_with_suffix` as described. - Make sure to handle edge cases like empty lines or files with no content. - Test your function thoroughly to ensure robustness. # Performance Requirement - The function should be efficient enough to handle files up to 1MB without significant delay.","solution":"def process_file_with_suffix(file_path: str, suffix: bytes) -> bytes: if not isinstance(suffix, bytes): raise TypeError(\\"Suffix must be of type bytes\\") try: with open(file_path, \'rb\') as file: lines = file.readlines() except FileNotFoundError: raise FileNotFoundError(\\"The specified file does not exist\\") result = b\'\'.join([line.rstrip(b\'n\') + suffix + b\'n\' for line in lines]) return result"},{"question":"**Model Conversion and Debugging with TorchScript** # Objective: You are required to convert a basic PyTorch model into TorchScript using both scripting and tracing. Additionally, you must demonstrate debugging and inspection techniques to verify the correctness of the converted model. # Requirements: 1. **Model Definition**: Define a simple PyTorch neural network model that includes both linear layers and control flow (like an if-else condition). 2. **Conversion to TorchScript**: - Use `torch.jit.script` and `torch.jit.trace` to convert different parts of the model. - Combine traced and scripted parts into a single TorchScript model. 3. **Debugging and Inspection**: - Inspect the generated TorchScript code and graph. - Demonstrate debugging by adding checks for potential issues in the traced functions. # Instructions: 1. **Model Definition**: - Define a model `MyModel` that has at least two linear layers. - Include a method with an if-else condition based on the input tensor. Example: ```python import torch import torch.nn as nn import torch.nn.functional as F class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = F.relu(self.fc1(x)) if x.sum() > 10: x = F.relu(self.fc2(x)) else: x = F.sigmoid(self.fc2(x)) return x ``` 2. **Conversion to TorchScript**: - Script the method containing the if-else condition. - Trace the forward method. - Combine these into a single TorchScript model. Example: ```python # Script part of the model scripted_method = torch.jit.script(MyModel().forward) # Trace the forward method example_input = torch.rand(1, 10) traced_model = torch.jit.trace(MyModel(), example_input) # Now you have both scripted_method and traced_model # Combine them into a single model class CombinedModel(torch.nn.Module): def __init__(self): super(CombinedModel, self).__init__() self.traced_model = traced_model self.scripted_method = scripted_method def forward(self, x): return self.scripted_method(x) scripted_combined_model = torch.jit.script(CombinedModel()) ``` 3. **Debugging and Inspection**: - Use pretty-print to inspect the TorchScript code. - Print and interpret the IR graph. Example: ```python # Inspect the generated code print(scripted_combined_model.code) # Inspect the graph print(scripted_combined_model.graph) ``` # Submission: - Scripted and traced components of `MyModel`. - Combined TorchScript model using both scripting and tracing. - Debugging and inspection outputs demonstrating the TorchScript model\'s validity. Ensure that the combined model correctly reproduces the behavior of the original PyTorch model and the code and graphs are well documented.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = F.relu(self.fc1(x)) if x.sum() > 10: x = F.relu(self.fc2(x)) else: x = F.sigmoid(self.fc2(x)) return x # Initialize model and an example tensor to trace model = MyModel() example_input = torch.rand(1, 10) # Script part of the model - we script the entire model for simplicity scripted_model = torch.jit.script(model) # Trace the model traced_model = torch.jit.trace(model, example_input) # Define combined model using the scripted model for control flow class CombinedModel(nn.Module): def __init__(self, scripted_model): super(CombinedModel, self).__init__() self.scripted_model = scripted_model def forward(self, x): return self.scripted_model(x) combined_model = CombinedModel(scripted_model) scripted_combined_model = torch.jit.script(combined_model) # Debug and inspection outputs print(scripted_combined_model.code) print(scripted_combined_model.graph)"},{"question":"**Objective:** You are required to parse an XML string, manipulate its DOM structure, and output an equivalent JSON representation. **Problem Statement:** Write a Python function `convert_xml_to_json(xml_string: str) -> str` that accepts a string containing XML data and returns a JSON string that represents the same data. The following conditions should be met: 1. Convert the XML structure into a nested JSON object where: - Tags become keys. - Text content becomes string values. - Nested tags become nested JSON objects. - Attributes of tags should be included as nested objects with the key `@attributes`. 2. Ensure the JSON structure maintains the same hierarchical relationships as the XML. **Input:** - `xml_string`: A string containing well-formed XML. **Output:** - A string containing the JSON representation of the XML data. **Constraints:** - You may assume the XML data does not contain mixed content (i.e., elements that have both text and nested elements). - Handle self-closing tags appropriately. - The input XML will always be well-formed. **Example:** ```python # Sample XML Input xml_input = <root> <person id=\\"1\\"> <name>John Doe</name> <age>30</age> <email>john.doe@example.com</email> </person> <person id=\\"2\\"> <name>Jane Doe</name> <age>25</age> <email>jane.doe@example.com</email> </person> </root> # Expected JSON Output json_output = { \\"root\\": { \\"person\\": [ { \\"name\\": \\"John Doe\\", \\"age\\": \\"30\\", \\"email\\": \\"john.doe@example.com\\", \\"@attributes\\": { \\"id\\": \\"1\\" } }, { \\"name\\": \\"Jane Doe\\", \\"age\\": \\"25\\", \\"email\\": \\"jane.doe@example.com\\", \\"@attributes\\": { \\"id\\": \\"2\\" } } ] } } # Function Signature def convert_xml_to_json(xml_string: str) -> str: pass ``` # Notes: - Use the `xml.dom.minidom` module for parsing XML data. - Utilize `json` module to convert the resulting data structure into a JSON string. - Include proper error handling for invalid XML input. - Ensure that the output JSON string is properly formatted and readable. **Assessment Criteria:** - Correctness of the XML to JSON conversion. - Adherence to the specified structure. - Code readability and documentation. - Efficiency of the solution.","solution":"import xml.dom.minidom as minidom import json def convert_xml_to_json(xml_string: str) -> str: def element_to_dict(node): child_nodes = node.childNodes result = {} if node.attributes: result[\'@attributes\'] = {attr.name: attr.value for attr in node.attributes.values()} for child in child_nodes: if child.nodeType == child.TEXT_NODE: text = child.data.strip() if text: return text elif child.nodeType == child.ELEMENT_NODE: child_dict = element_to_dict(child) if child.nodeName in result: if isinstance(result[child.nodeName], list): result[child.nodeName].append(child_dict) else: result[child.nodeName] = [result[child.nodeName], child_dict] else: result[child.nodeName] = child_dict return result dom = minidom.parseString(xml_string) root_element = dom.documentElement result_dict = element_to_dict(root_element) result = {root_element.nodeName: result_dict} return json.dumps(result, indent=4)"},{"question":"Coding Assessment Question # Question: You have been tasked with building a machine learning model to predict house prices using a synthetic dataset. This task assesses your understanding of Scikit-learn\'s fundamental workflow, including data preparation, model training, evaluation, and debugging warnings or errors. # Instructions: 1. Generate a synthetic regression dataset with 1000 samples and 10 features using Scikit-learn. 2. Split the dataset into training and testing sets, with 80% of the data used for training and 20% for testing. 3. Standardize the feature data using Scikit-learn\'s `StandardScaler`. 4. Train a `GradientBoostingRegressor` model on the training data. 5. Evaluate the model on the test data and print the R^2 score. 6. Modify the model to include early stopping with `n_iter_no_change=5`, and handle any warnings or errors that may occur. 7. Prepare your code as a minimal, reproducible example. # Expected Input and Output: - Input: - Generate a synthetic dataset using `make_regression`. - Split the data using `train_test_split`. - Standardize data using `StandardScaler`. - Train and evaluate using `GradientBoostingRegressor`. - Output: - The R^2 score of the model before and after introducing early stopping. # Constraints: - Do not use any external datasets; all data should be generated using Scikit-learn\'s `make_regression`. - Ensure that your final script is minimal and self-contained. - Document any decisions or assumptions made in your code comments. # Performance Requirements: - The code should be efficient and run within a reasonable time frame for a dataset of the specified size. - Handle any warnings or errors gracefully and ensure the code runs without failure. # Sample Code Outline: ```python # Import necessary libraries import numpy as np import pandas as pd from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42) # Step 2: Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Standardize the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train a GradientBoostingRegressor model model = GradientBoostingRegressor(random_state=42) model.fit(X_train, y_train) # Step 5: Evaluate the model score = model.score(X_test, y_test) print(\\"R^2 score without early stopping:\\", score) # Step 6: Modify the model to include early stopping model = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) model.fit(X_train, y_train) # Step 7: Evaluate the modified model new_score = model.score(X_test, y_test) print(\\"R^2 score with early stopping:\\", new_score) ```","solution":"# Import necessary libraries import numpy as np from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingRegressor def generate_and_train_model(): # Step 1: Generate synthetic dataset X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42) # Step 2: Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Standardize the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Step 4: Train a GradientBoostingRegressor model model = GradientBoostingRegressor(random_state=42) model.fit(X_train_scaled, y_train) # Step 5: Evaluate the model score = model.score(X_test_scaled, y_test) print(\\"R^2 score without early stopping:\\", score) # Step 6: Modify the model to include early stopping model_early_stopping = GradientBoostingRegressor(random_state=42, n_iter_no_change=5, validation_fraction=0.1) model_early_stopping.fit(X_train_scaled, y_train) # Step 7: Evaluate the modified model new_score = model_early_stopping.score(X_test_scaled, y_test) print(\\"R^2 score with early stopping:\\", new_score) return score, new_score"},{"question":"Implement a function `fetch_data_simultaneously(urls, max_workers=5)` that takes in a list of URLs and a maximum number of worker threads. This function should use the `concurrent.futures.ThreadPoolExecutor` to fetch data from each URL asynchronously and simultaneously. The function should return a dictionary where the keys are the URLs and the values are either the byte content of the web page or a relevant exception message if an error occurred. Constraints: - Each URL fetch should have a timeout of 10 seconds. - The function should handle scenarios where some URLs might not be reachable (e.g., network errors) gracefully and include the error in the returned dictionary. Input: - `urls`: List of URLs (strings) to fetch data from. - `max_workers`: Integer specifying the maximum number of worker threads to use (default is 5). Output: - A dictionary with URLs as keys and their corresponding byte content or error message as values. Example: ```python urls = [ \'http://www.foxnews.com/\', \'http://www.cnn.com/\', \'http://europe.wsj.com/\', \'http://www.bbc.co.uk/\', \'http://nonexistent-subdomain.python.org/\' ] result = fetch_data_simultaneously(urls, max_workers=3) # Example expected output: # { # \'http://www.foxnews.com/\': b\'...\', # actual byte content # \'http://www.cnn.com/\': b\'...\', # actual byte content # \'http://europe.wsj.com/\': b\'...\', # actual byte content # \'http://www.bbc.co.uk/\': b\'...\', # actual byte content # \'http://nonexistent-subdomain.python.org/\': \'Error: HTTP Error 404: Not Found\' # } ``` # Requirements: 1. Ensure proper concurrency control using `ThreadPoolExecutor`. 2. Implement error handling for URL fetch exceptions with meaningful error messages. 3. Utilize future objects to manage and retrieve the results of the asynchronous tasks. 4. The function should shutdown the executor to release resources properly when done. # Example Starter Code: ```python import concurrent.futures import urllib.request def fetch_data_simultaneously(urls, max_workers=5): def load_url(url, timeout=10): try: with urllib.request.urlopen(url, timeout=timeout) as conn: return conn.read() except Exception as e: return f\\"Error: {str(e)}\\" results = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(load_url, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: results[url] = future.result() except Exception as exc: results[url] = f\\"Error: {str(exc)}\\" return results # Test the function urls = [ \'http://www.foxnews.com/\', \'http://www.cnn.com/\', \'http://europe.wsj.com/\', \'http://www.bbc.co.uk/\', \'http://nonexistent-subdomain.python.org/\' ] print(fetch_data_simultaneously(urls, max_workers=3)) ```","solution":"import concurrent.futures import urllib.request def fetch_data_simultaneously(urls, max_workers=5): def load_url(url, timeout=10): try: with urllib.request.urlopen(url, timeout=timeout) as conn: return conn.read() except Exception as e: return f\\"Error: {str(e)}\\" results = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(load_url, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: results[url] = future.result() except Exception as exc: results[url] = f\\"Error: {str(exc)}\\" return results"},{"question":"**Objective**: To assess the ability to analyze Python scripts for their imports using the `modulefinder` module, and generate a detailed report. **Task**: Write a Python function called `analyze_script_imports` that takes in the path to a Python script file (`script_path`), and returns a detailed report as a dictionary containing the sets of modules that are imported, missing, and seem to be missing. **Function Signature**: ```python def analyze_script_imports(script_path: str) -> dict: ``` # Inputs: - `script_path` (str): A string representing the file path to the Python script to be analyzed. # Outputs: - A dictionary with the following structure: ```python { \'loaded_modules\': { \'module_name\': [\'global_name_1\', \'global_name_2\', \'global_name_3\', ...], ... }, \'missing_modules\': [\'module_name_1\', \'module_name_2\', ...], \'seem_to_be_missing\': [\'module_name_1\', \'module_name_2\', ...] } ``` - `loaded_modules` is a dictionary where keys are module names and values are lists of the first three global names within those modules. - `missing_modules` is a list of module names that were not imported but were attempted. - `seem_to_be_missing` is a list of module names that seem to be missing. # Constraints: - Only standard and third-party modules are considered. Custom scripts are assumed to be available in the current directory or in the `sys.path`. - The input script may have syntax errors or runtime errors which should be handled gracefully. # Example Usage: Assume `simple_script.py` contains the following code: ```python import os import sys import nonexistent def example_function(): try: import optionalmodule except ImportError: pass ``` Calling `analyze_script_imports(\'simple_script.py\')` should return a dictionary like: ```python { \'loaded_modules\': { \'os\': [\'__name__\', \'sep\', \'environ\'], \'sys\': [\'__name__\', \'version\', \'path\'], \'__main__\': [\'os\', \'sys\', \'nonexistent\'], }, \'missing_modules\': [\'optionalmodule\'], \'seem_to_be_missing\': [\'nonexistent\'] } ``` # Requirements: 1. Use the `modulefinder` module effectively to analyze the script. 2. Handle errors in the script gracefully and ensure that the function does not crash. 3. Ensure that the function\'s output format strictly follows the structure described above. # Hints: - Explore the usage of `ModuleFinder` class\'s `run_script()` and `report()` methods. - Access the `modules` attribute to get loaded module information. - Handle the `badmodules` attribute to find the missing modules. Good luck!","solution":"import modulefinder import ast def analyze_script_imports(script_path: str) -> dict: finder = modulefinder.ModuleFinder() # Analyzing the script try: finder.run_script(script_path) except Exception as e: return { \'loaded_modules\': {}, \'missing_modules\': [], \'seem_to_be_missing\': [], } loaded_modules = {} for name, module in finder.modules.items(): global_names = list(module.globalnames.keys())[:3] loaded_modules[name] = global_names missing_modules = list(finder.badmodules.keys()) # Determine seem_to_be_missing modules seem_to_be_missing = [] with open(script_path, \'r\') as file: tree = ast.parse(file.read(), filename=script_path) for node in ast.walk(tree): if isinstance(node, ast.Import): for alias in node.names: if alias.name not in finder.modules: seem_to_be_missing.append(alias.name) elif isinstance(node, ast.ImportFrom): if node.module and node.module not in finder.modules: seem_to_be_missing.append(node.module) return { \'loaded_modules\': loaded_modules, \'missing_modules\': missing_modules, \'seem_to_be_missing\': seem_to_be_missing }"},{"question":"**Question: Utilizing MPS Backend in PyTorch** **Problem Statement:** You are tasked with implementing a PyTorch function to train a simple neural network on the GPU of a MacOS device using the Metal Performance Shaders (MPS) backend. The function should check if the MPS device is available, create the necessary tensors on the MPS device, and perform training using a simple neural network model. **Function Signature:** ```python def train_on_mps(training_data: torch.Tensor, training_labels: torch.Tensor, epochs: int) -> torch.nn.Module: pass ``` **Input:** 1. `training_data` (torch.Tensor): A tensor of shape (N, D_in) where N is the number of samples and D_in is the input dimension. 2. `training_labels` (torch.Tensor): A tensor of shape (N, D_out) where N is the number of samples and D_out is the output dimension. 3. `epochs` (int): The number of epochs to train the model. **Output:** - A trained `torch.nn.Module` model. **Constraints:** - You must ensure that the operations and model training are done using the MPS device. - Raise an Exception with an appropriate message if the MPS device is not available. - Use a simple feedforward neural network with one hidden layer for training. **Requirements:** - Check and move tensors and the model to the MPS device. - Implement a simple neural network with a single hidden layer using `torch.nn`. - Use Mean Squared Error (MSE) as the loss function. - Use Stochastic Gradient Descent (SGD) as the optimization algorithm. - Return the trained model after the specified number of epochs. **Example:** ```python import torch class SimpleNet(torch.nn.Module): def __init__(self, D_in, H, D_out): super(SimpleNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): h_relu = torch.relu(self.linear1(x)) y_pred = self.linear2(h_relu) return y_pred def train_on_mps(training_data: torch.Tensor, training_labels: torch.Tensor, epochs: int) -> torch.nn.Module: # Check if MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): raise Exception(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: raise Exception(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") mps_device = torch.device(\\"mps\\") # Move data to MPS device training_data, training_labels = training_data.to(mps_device), training_labels.to(mps_device) # Define the model, loss function, and optimizer D_in = training_data.size(1) D_out = training_labels.size(1) H = 100 # Number of hidden units model = SimpleNet(D_in, H, D_out).to(mps_device) criterion = torch.nn.MSELoss().to(mps_device) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) # Training loop for epoch in range(epochs): # Forward pass y_pred = model(training_data) # Compute loss loss = criterion(y_pred, training_labels) print(f\'Epoch {epoch}: loss = {loss.item()}\') # Zero gradients, backward pass, and update weights optimizer.zero_grad() loss.backward() optimizer.step() return model ```","solution":"import torch class SimpleNet(torch.nn.Module): def __init__(self, D_in, H, D_out): super(SimpleNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): h_relu = torch.relu(self.linear1(x)) y_pred = self.linear2(h_relu) return y_pred def train_on_mps(training_data: torch.Tensor, training_labels: torch.Tensor, epochs: int) -> torch.nn.Module: # Check if MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): raise Exception(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: raise Exception(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") mps_device = torch.device(\\"mps\\") # Move data to MPS device training_data, training_labels = training_data.to(mps_device), training_labels.to(mps_device) # Define the model, loss function, and optimizer D_in = training_data.size(1) D_out = training_labels.size(1) H = 100 # Number of hidden units model = SimpleNet(D_in, H, D_out).to(mps_device) criterion = torch.nn.MSELoss().to(mps_device) optimizer = torch.optim.SGD(model.parameters(), lr=1e-4) # Training loop for epoch in range(epochs): # Forward pass y_pred = model(training_data) # Compute loss loss = criterion(y_pred, training_labels) print(f\'Epoch {epoch}: loss = {loss.item()}\') # Zero gradients, backward pass, and update weights optimizer.zero_grad() loss.backward() optimizer.step() return model"},{"question":"You are provided with a module that uses the External Data Representation (XDR) standard to encode and decode various data types. Your task is to create methods that will pack and unpack a custom data structure using this module. # Custom Data Structure The structure to pack and unpack consists of the following fields: - `username`: a string representing a username. - `user_id`: an integer representing the user ID. - `balance`: a floating-point number representing a user\'s account balance. - `transactions`: a list of floating-point numbers representing transaction amounts. # Specific Requirements 1. **Define a function `pack_user_data(packer, username, user_id, balance, transactions)` that takes the following arguments and uses the `xdrlib.Packer` instance `packer` to pack them into an XDR representation.** - `packer`: an instance of `xdrlib.Packer`. - `username` (string): A username. - `user_id` (integer): A user ID. - `balance` (float): Account balance. - `transactions` (list of floats): A list of transaction amounts. 2. **Define a function `unpack_user_data(unpacker)` that takes an `xdrlib.Unpacker` instance `unpacker` and unpacks the data packed using `pack_user_data` to return a dictionary with the original data in the corresponding fields.** - `unpacker`: an instance of `xdrlib.Unpacker`. - Returns a dictionary with fields `username`, `user_id`, `balance`, and `transactions`. # Constraints - Assume `username` is a string of no more than 20 characters. - Assume `transactions` contain no more than 100 elements. - You must handle potential padding and alignment issues as per XDR requirements. # Example ```python import xdrlib data = { \\"username\\": \\"alice\\", \\"user_id\\": 1001, \\"balance\\": 256.75, \\"transactions\\": [150.00, -45.25, -100.00] } # Initialize packer and pack data packer = xdrlib.Packer() pack_user_data(packer, data[\\"username\\"], data[\\"user_id\\"], data[\\"balance\\"], data[\\"transactions\\"]) # Get the packed buffer packed_data = packer.get_buffer() # Initialize unpacker with packed data and unpack it unpacker = xdrlib.Unpacker(packed_data) unpacked_data = unpack_user_data(unpacker) print(unpacked_data) # Should output: {\\"username\\": \\"alice\\", \\"user_id\\": 1001, \\"balance\\": 256.75, \\"transactions\\": [150.00, -45.25, -100.00]} ``` Please implement the functions `pack_user_data` and `unpack_user_data`.","solution":"import xdrlib def pack_user_data(packer, username, user_id, balance, transactions): Packs user data into XDR format using the provided Packer instance. Args: - packer: xdrlib.Packer instance. - username: String, the username. - user_id: Integer, the user ID. - balance: Float, the account balance. - transactions: List of floats, representing transaction amounts. assert len(username) <= 20, \\"Username must be no more than 20 characters\\" assert len(transactions) <= 100, \\"Transactions list must contain no more than 100 elements\\" # Packing the username packer.pack_string(username.encode(\'utf-8\')) # Packing the user ID packer.pack_int(user_id) # Packing the balance packer.pack_double(balance) # Packing the number of transactions packer.pack_uint(len(transactions)) # Packing each transaction amount for transaction in transactions: packer.pack_double(transaction) def unpack_user_data(unpacker): Unpacks user data from XDR format using the provided Unpacker instance. Args: - unpacker: xdrlib.Unpacker instance. Returns: - Dictionary containing the unpacked data with fields: - username: String, the username. - user_id: Integer, the user ID. - balance: Float, the account balance. - transactions: List of floats, representing transaction amounts. # Unpacking the username username = unpacker.unpack_string().decode(\'utf-8\') # Unpacking the user ID user_id = unpacker.unpack_int() # Unpacking the balance balance = unpacker.unpack_double() # Unpacking the number of transactions num_transactions = unpacker.unpack_uint() transactions = [unpacker.unpack_double() for _ in range(num_transactions)] return { \\"username\\": username, \\"user_id\\": user_id, \\"balance\\": balance, \\"transactions\\": transactions }"},{"question":"# Question: Implementing Custom Memory Operations with MemoryView You are tasked with implementing a custom solution that utilizes Python\'s `memoryview` for efficient memory operations. Your objective will require understanding and leveraging the properties of memoryview objects to manipulate data directly. Task Implement a function `custom_memory_view_operations(data: bytes, operation: str, value: int = None) -> bytes` which performs the following operations based on the `operation` parameter: 1. `\'slice\'`: Return a memoryview of the data sliced to the first half. 2. `\'modify\'`: Modify the data by adding the given value (each byte + value % 256) and return the modified data. 3. `\'validate\'`: Check whether the data is a memoryview and return a boolean indicating whether it is a memoryview. 4. `\'contiguous\'`: Ensure the memory represented by the memoryview is contiguous and return the contiguous memory slice. Specifications - **Input**: - `data`: a bytes object or buffer object representing the data. - `operation`: a string denoting the operation to perform (\'slice\', \'modify\', \'validate\', \'contiguous\'). - `value`: an integer, required only for the \'modify\' operation (default `None`). - **Output**: - For \'slice\' and \'contiguous\' operations, return the resultant bytes object. - For \'modify\', return the modified bytes object. - For \'validate\', return a boolean indicating whether the object is a memoryview. - **Constraints**: - For \'slice\' operation, assume the length of the data is even. - For \'modify\' operation, ensure data remains within the byte range (0-255). - **Performance requirements**: Aim for efficient operations using Python\'s `memoryview` wherever applicable. Example ```python data = bytes(range(1, 101)) # A bytes object with values from 1 to 100 # Slice Operation print(custom_memory_view_operations(data, \'slice\')) # Output: b\'x01x02x03...x32\' (First 50 bytes) # Modify Operation print(custom_memory_view_operations(data, \'modify\', 5)) # Output: b\'x06x07x08..x05\' (Each byte incremented by 5 % 256) # Validate Operation print(custom_memory_view_operations(data, \'validate\')) # Output: False # Contiguous Operation print(custom_memory_view_operations(data, \'contiguous\')) # Output: b\'x01x02x03...x64\' (Ensured contiguous memory slice) ```","solution":"def custom_memory_view_operations(data, operation, value=None): Perform custom memory operations using memoryview. :param data: a bytes object representing the data. :param operation: a string denoting the operation to perform (\'slice\', \'modify\', \'validate\', \'contiguous\'). :param value: an integer, required only for \'modify\' operation (default None). :return: bytes or bool depending on the operation. if operation == \'slice\': # Return a memoryview of the data sliced to the first half. mv = memoryview(data) return bytes(mv[:len(mv)//2]) elif operation == \'modify\': # Modify the data by adding the given value (each byte + value % 256) and return the modified data. if value is None: raise ValueError(\\"Value must be provided for modify operation\\") mv = memoryview(data).tolist() modified_data = [(byte + value) % 256 for byte in mv] return bytes(modified_data) elif operation == \'validate\': # Check whether the data is a memoryview and return a boolean indicating whether it is a memoryview. return isinstance(data, memoryview) elif operation == \'contiguous\': # Ensure the memory represented by the memoryview is contiguous and return the contiguous memory slice. mv = memoryview(data) if mv.contiguous: return data else: return bytes(mv.tobytes()) # Convert to bytes which is contiguous else: raise ValueError(\\"Unsupported operation. Use \'slice\', \'modify\', \'validate\', or \'contiguous\'\\")"},{"question":"**Objective:** Implement a custom codec capable of both encoding and decoding text strings using a simple substitution cipher (Caesar cipher). The custom codec must handle encoding and decoding operations, implement error handling strategies, and support incremental encoding/decoding. **Details:** 1. **Codec Description**: - A Caesar cipher is a basic encryption technique where each letter in the plaintext is shifted a certain number of places down or up the alphabet. For example, with a shift of 3: `A` would be replaced by `D`, `B` would become `E`, and so on. 2. **Classes to Implement**: - A custom `Codec` class for the Caesar cipher. - A custom `IncrementalEncoder` class for incremental encoding. - A custom `IncrementalDecoder` class for incremental decoding. - A custom `StreamReader` and `StreamWriter` using the `StreamReaderWriter` pattern. 3. **Functions to Implement**: - `encode(obj, errors=\'strict\')`: Encodes the input object using the Caesar cipher. - `decode(obj, errors=\'strict\')`: Decodes the input object using the Caesar cipher. - Implement a registration function to register the custom codec with a given name. 4. **Error Handling**: - The codec must support the following error handling schemes: `\'strict\'`, `\'ignore\'`, `\'replace\'`, `\'backslashreplace\'`, `\'xmlcharrefreplace\'`, and `\'namereplace\'`. **Requirements**: 1. Implement the `CaesarCodec`, `CaesarIncrementalEncoder`, `CaesarIncrementalDecoder`, `CaesarStreamReader`, and `CaesarStreamWriter` classes. 2. Each class should comply with the interfaces defined by the base classes provided in the `codecs` module. 3. The codec should be registered under the name `\\"caesar_cipher\\"`. **Implement the functions and classes below**: ```python import codecs class CaesarCodec(codecs.Codec): def encode(self, input, errors=\'strict\'): # Implement the Caesar cipher encoding logic pass def decode(self, input, errors=\'strict\'): # Implement the Caesar cipher decoding logic pass class CaesarIncrementalEncoder(codecs.IncrementalEncoder): def encode(self, input, final=False): # Implement incremental encoding logic pass class CaesarIncrementalDecoder(codecs.IncrementalDecoder): def decode(self, input, final=False): # Implement incremental decoding logic pass class CaesarStreamReader(CaesarCodec, codecs.StreamReader): pass class CaesarStreamWriter(CaesarCodec, codecs.StreamWriter): pass def caesar_search_function(encoding): if encoding == \'caesar_cipher\': return codecs.CodecInfo( name=\'caesar_cipher\', encode=CaesarCodec().encode, decode=CaesarCodec().decode, incrementalencoder=CaesarIncrementalEncoder, incrementaldecoder=CaesarIncrementalDecoder, streamreader=CaesarStreamReader, streamwriter=CaesarStreamWriter, ) return None codecs.register(caesar_search_function) # Test cases if __name__ == \\"__main__\\": # Register the new codec encoded = codecs.encode(\'HELLO WORLD\', \'caesar_cipher\') print(f\\"Encoded: {encoded}\\") decoded = codecs.decode(encoded, \'caesar_cipher\') print(f\\"Decoded: {decoded}\\") ``` **Constraints and Considerations**: - The shift for the Caesar cipher should be fixed at 3. - Handle both uppercase and lowercase characters while preserving non-alphabetic characters unaltered. - Thoroughly test your custom codec with edge cases including empty strings, strings with non-alphabetic characters, and different error handling schemes. **Note**: Ensure your implementation is efficient and adheres to the correct interfaces for compatibility with the `codecs` module in Python.","solution":"import codecs class CaesarCodec(codecs.Codec): shift = 3 def encode(self, input, errors=\'strict\'): return self._caesar_shift(input, self.shift), len(input) def decode(self, input, errors=\'strict\'): return self._caesar_shift(input, -self.shift), len(input) def _caesar_shift(self, text, shift): result = [] for char in text: if \'A\' <= char <= \'Z\': result.append(chr((ord(char) - ord(\'A\') + shift) % 26 + ord(\'A\'))) elif \'a\' <= char <= \'z\': result.append(chr((ord(char) - ord(\'a\') + shift) % 26 + ord(\'a\'))) else: result.append(char) return \'\'.join(result) class CaesarIncrementalEncoder(codecs.IncrementalEncoder): def encode(self, input, final=False): return CaesarCodec().encode(input)[0] class CaesarIncrementalDecoder(codecs.IncrementalDecoder): def decode(self, input, final=False): return CaesarCodec().decode(input)[0] class CaesarStreamReader(CaesarCodec, codecs.StreamReader): pass class CaesarStreamWriter(CaesarCodec, codecs.StreamWriter): pass def caesar_search_function(encoding): if encoding == \'caesar_cipher\': return codecs.CodecInfo( name=\'caesar_cipher\', encode=CaesarCodec().encode, decode=CaesarCodec().decode, incrementalencoder=CaesarIncrementalEncoder, incrementaldecoder=CaesarIncrementalDecoder, streamreader=CaesarStreamReader, streamwriter=CaesarStreamWriter, ) return None codecs.register(caesar_search_function)"},{"question":"**Objective:** Demonstrate proficiency in using the Python asyncio package to manage concurrent tasks and subprocesses. **Question:** Implement a Python program using asyncio that performs the following tasks: 1. **Concurrent Task Execution:** Create an asyncio function `fetch_data` that simulates fetching data from a remote server. The function should: - Take a `delay` parameter (in seconds) indicating how long to wait before returning the data. - Print a message when data fetching starts and another when it ends. - Return a string \\"Data fetched\\" after the delay. 2. **Subprocess Handling:** Create an asyncio function `run_subprocess` that runs a shell command to display the contents of the current directory using `ls` (or `dir` on Windows). The function should: - Print a message before running the command and another when the command completes. - Capture and print the output of the command. - Return True if the subprocess completed successfully, otherwise raise an exception. 3. **Main Function:** Implement the `main` function that: - Fetches data concurrently using different delays (e.g., 1, 2, 3 seconds). - Runs the subprocess to list the current directory. - Ensures that both tasks run concurrently. - Prints a final message once all tasks are complete. 4. **Performance Constraint:** Ensure that the entire program runs in less than 5 seconds. **Input:** - No input is required from the user. **Output:** - Printed messages indicating the progress and completion of data fetching and subprocess execution. - The output of the `ls` (or `dir`) command. **Constraints:** - Use only the asyncio package and relevant standard library modules. - Do not use any external libraries. - Ensure correct handling and closure of the event loop. **Function signatures:** ```python import asyncio async def fetch_data(delay: int) -> str: # Your code here async def run_subprocess() -> bool: # Your code here async def main(): # Your code here if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Example Output:** ``` Fetching data with 1-second delay Fetching data with 2-second delay Fetching data with 3-second delay Running subprocess to list directory contents Data fetched after 1-second delay Data fetched after 2-second delay Data fetched after 3-second delay Subprocess complete. Output: <list of directory contents> All tasks complete. ``` **Notes:** - Ensure the program is robust and handles errors gracefully. - The function should print appropriate messages indicating progress and results.","solution":"import asyncio import subprocess async def fetch_data(delay: int) -> str: Simulates fetching data from a remote server with a delay. Args: delay (int): Delay in seconds before fetching the data. Returns: str: \\"Data fetched\\" after the delay. print(f\\"Fetching data with {delay}-second delay\\") await asyncio.sleep(delay) print(f\\"Data fetched after {delay}-second delay\\") return \\"Data fetched\\" async def run_subprocess() -> bool: Runs a shell command to display the contents of the current directory. Returns: bool: True if the subprocess completed successfully. Raises: Exception: If there is any error in running the subprocess. print(\\"Running subprocess to list directory contents\\") process = await asyncio.create_subprocess_shell( \'ls\' if not subprocess.os.name == \'nt\' else \'dir\', stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() if process.returncode == 0: print(f\\"Subprocess complete. Output:n{stdout.decode().strip()}\\") return True else: print(f\\"Subprocess failed with error:n{stderr.decode().strip()}\\") raise Exception(\\"Subprocess failed\\") async def main(): Main asynchronous function that runs fetch_data and run_subprocess concurrently. tasks = [ fetch_data(1), fetch_data(2), fetch_data(3), run_subprocess() ] await asyncio.gather(*tasks) print(\\"All tasks complete.\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"SQLite Database Management and Custom Function **Objective**: The goal of this task is to assess your ability to work with SQLite databases using the `sqlite3` module in Python. You will create a database, perform operations, and work with a custom Python data type. Problem Statement Create an SQLite database to store information about books, including their title, author, publication year, and rating. The specific tasks you need to complete are as follows: 1. **Create a Database and Table**: - Create a connection to an SQLite database named `books.db`. - Create a table named `book` with the following columns: - `title` (TEXT) - `author` (TEXT) - `year` (INTEGER) - `rating` (REAL) 2. **Insert Data into Table**: - Insert at least five rows of data into the table. The data should be supplied as tuples. 3. **Query the Database**: - Write a function `get_highly_rated_books` that takes a rating threshold as an argument and returns a list of books (title and author) with a rating higher than the given threshold, ordered by rating in descending order. 4. **Custom Data Type**: - Define a custom data type `Point` representing a point in a 2D space with x and y coordinates. - Register the type with SQLite adapters and converters. - Create a table `geometry` with columns `id` (INTEGER PRIMARY KEY) and `coordinates` (Point). - Insert, fetch, and display coordinates as instances of the `Point` class. Constraints - You should use parameterized queries to avoid SQL injection. - Ensure you handle transactions and commit changes appropriately. - Use placeholders for binding Python values to SQL queries. Code Requirements 1. **Database Creation and Data Insertion**: ```python import sqlite3 def create_database(): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() cur.execute(\'\'\' CREATE TABLE IF NOT EXISTS book ( title TEXT, author TEXT, year INTEGER, rating REAL ) \'\'\') books = [ (\'The Great Gatsby\', \'F. Scott Fitzgerald\', 1925, 8.5), (\'1984\', \'George Orwell\', 1949, 8.9), (\'To Kill a Mockingbird\', \'Harper Lee\', 1960, 8.3), (\'Pride and Prejudice\', \'Jane Austen\', 1813, 8.2), (\'The Catcher in the Rye\', \'J.D. Salinger\', 1951, 7.9) ] cur.executemany(\\"INSERT INTO book (title, author, year, rating) VALUES (?, ?, ?, ?)\\", books) con.commit() con.close() ``` 2. **Query Highly Rated Books**: ```python def get_highly_rated_books(threshold): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() cur.execute(\\"SELECT title, author FROM book WHERE rating > ? ORDER BY rating DESC\\", (threshold,)) results = cur.fetchall() con.close() return results ``` 3. **Custom Data Type**: ```python class Point: def __init__(self, x, y): self.x = x self.y = y def __str__(self): return f\\"({self.x}, {self.y})\\" def __conform__(self, protocol): if protocol is sqlite3.PrepareProtocol: return f\\"{self.x};{self.y}\\" def adapt_point(point): return f\\"{point.x};{point.y}\\" def convert_point(s): x, y = map(float, s.split(b\';\')) return Point(x, y) sqlite3.register_adapter(Point, adapt_point) sqlite3.register_converter(\\"point\\", convert_point) def create_geometry_table(): con = sqlite3.connect(\\"books.db\\", detect_types=sqlite3.PARSE_DECLTYPES) cur = con.cursor() cur.execute(\'\'\' CREATE TABLE IF NOT EXISTS geometry ( id INTEGER PRIMARY KEY, coordinates point ) \'\'\') cur.execute(\\"INSERT INTO geometry (coordinates) VALUES (?)\\", (Point(3.1, 4.2),)) cur.execute(\\"SELECT coordinates FROM geometry\\") point = cur.fetchone()[0] con.close() return point ``` Final Output Provide the following: 1. The implementation of the `create_database` function. 2. The implementation of the `get_highly_rated_books` function. 3. The implementation of the `Point` class and related adapter and converter functions. 4. The implementation of the `create_geometry_table` function. Make sure your code runs without error and meets the functional requirements outlined above. Submission Submit the complete code in a single Python file. Comment your code where necessary and make sure it is readable and well-structured.","solution":"import sqlite3 def create_database(): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() cur.execute(\'\'\' CREATE TABLE IF NOT EXISTS book ( title TEXT, author TEXT, year INTEGER, rating REAL ) \'\'\') books = [ (\'The Great Gatsby\', \'F. Scott Fitzgerald\', 1925, 8.5), (\'1984\', \'George Orwell\', 1949, 8.9), (\'To Kill a Mockingbird\', \'Harper Lee\', 1960, 8.3), (\'Pride and Prejudice\', \'Jane Austen\', 1813, 8.2), (\'The Catcher in the Rye\', \'J.D. Salinger\', 1951, 7.9) ] cur.executemany(\\"INSERT INTO book (title, author, year, rating) VALUES (?, ?, ?, ?)\\", books) con.commit() con.close() def get_highly_rated_books(threshold): con = sqlite3.connect(\\"books.db\\") cur = con.cursor() cur.execute(\\"SELECT title, author FROM book WHERE rating > ? ORDER BY rating DESC\\", (threshold,)) results = cur.fetchall() con.close() return results class Point: def __init__(self, x, y): self.x = x self.y = y def __str__(self): return f\\"({self.x}, {self.y})\\" def __conform__(self, protocol): if protocol is sqlite3.PrepareProtocol: return f\\"{self.x};{self.y}\\" def adapt_point(point): return f\\"{point.x};{point.y}\\" def convert_point(s): x, y = map(float, s.split(b\';\')) return Point(x, y) sqlite3.register_adapter(Point, adapt_point) sqlite3.register_converter(\\"point\\", convert_point) def create_geometry_table(): con = sqlite3.connect(\\"books.db\\", detect_types=sqlite3.PARSE_DECLTYPES) cur = con.cursor() cur.execute(\'\'\' CREATE TABLE IF NOT EXISTS geometry ( id INTEGER PRIMARY KEY, coordinates point ) \'\'\') cur.execute(\\"INSERT INTO geometry (coordinates) VALUES (?)\\", (Point(3.1, 4.2),)) cur.execute(\\"SELECT coordinates FROM geometry\\") point = cur.fetchone()[0] con.close() return point"},{"question":"**Question: Implement a Log Filter using Regular Expressions** You are tasked with processing a server log file to extract specific types of log entries. The log file contains entries from multiple sources, and you are interested in extracting HTTP error messages and warnings related to the application. Each log entry starts with a timestamp, followed by a log level, and then the log message. The structure of the log entry is as follows: ``` [TIMESTAMP] [LOG_LEVEL] MESSAGE ``` For example: ``` [2023-01-01 10:00:00] [INFO] Server started successfully [2023-01-01 10:05:23] [ERROR] 404 Page Not Found: /nonexistentpage [2023-01-01 10:07:44] [WARNING] Application is using a deprecated API ``` Write a function `filter_logs` that takes a single string containing multiple lines of log entries and returns a list of dictionary objects, each representing a log entry that is either an HTTP error (contains \'ERROR\' in the log level and typical HTTP status codes like \'404\', \'500\', etc.) or a warning related to the application (contains \'WARNING\' in the log level). Each dictionary should have the following keys: - `\\"timestamp\\"`: The timestamp of the log entry. - `\\"log_level\\"`: The log level (ERROR or WARNING). - `\\"message\\"`: The log message. # Input - A single string `log_data` containing multiple lines of log entries, as described above. # Output - A list of dictionaries, each containing the filtered log entries following the format specified. # Example **Input:** ```python log_data = \'\'\'[2023-01-01 10:00:00] [INFO] Server started successfully [2023-01-01 10:05:23] [ERROR] 404 Page Not Found: /nonexistentpage [2023-01-01 10:07:44] [WARNING] Application is using a deprecated API [2023-01-01 10:15:00] [DEBUG] This is a debug message [2023-01-01 10:20:00] [ERROR] 500 Internal Server Error [2023-01-01 10:25:00] [WARNING] High memory usage detected \'\'\' ``` **Output:** ```python [ {\'timestamp\': \'2023-01-01 10:05:23\', \'log_level\': \'ERROR\', \'message\': \'404 Page Not Found: /nonexistentpage\'}, {\'timestamp\': \'2023-01-01 10:07:44\', \'log_level\': \'WARNING\', \'message\': \'Application is using a deprecated API\'}, {\'timestamp\': \'2023-01-01 10:20:00\', \'log_level\': \'ERROR\', \'message\': \'500 Internal Server Error\'}, {\'timestamp\': \'2023-01-01 10:25:00\', \'log_level\': \'WARNING\', \'message\': \'High memory usage detected\'}, ] ``` # Constraints 1. The log entries follow the exact format given. 2. The messages are case-sensitive. 3. The function should be efficient and handle large log data text. ```python import re from typing import List, Dict def filter_logs(log_data: str) -> List[Dict[str, str]]: pattern = r\'[(.*?)] [(ERROR|WARNING)] (.*?(d{3} .*)?)\' matches = re.findall(pattern, log_data, re.MULTILINE) filtered_logs = [ {\'timestamp\': match[0], \'log_level\': match[1], \'message\': match[2]} for match in matches ] return filtered_logs # Test the function with the example provided log_data = \'\'\'[2023-01-01 10:00:00] [INFO] Server started successfully [2023-01-01 10:05:23] [ERROR] 404 Page Not Found: /nonexistentpage [2023-01-01 10:07:44] [WARNING] Application is using a deprecated API [2023-01-01 10:15:00] [DEBUG] This is a debug message [2023-01-01 10:20:00] [ERROR] 500 Internal Server Error [2023-01-01 10:25:00] [WARNING] High memory usage detected \'\'\' print(filter_logs(log_data)) # Expected Output: The example above. ```","solution":"import re from typing import List, Dict def filter_logs(log_data: str) -> List[Dict[str, str]]: Processes the log data to extract HTTP error messages and application warnings. Returns a list of dictionaries containing the filtered log entries. # Define the pattern to match the required log entries pattern = r\'[(.*?)] [(ERROR|WARNING)] (.*?(d{3} .*)?)\' # Find all the matching log entries matches = re.findall(pattern, log_data, re.MULTILINE) # Create a list of dictionaries from the matches filtered_logs = [ {\'timestamp\': match[0], \'log_level\': match[1], \'message\': match[2]} for match in matches ] return filtered_logs"},{"question":"# Memory Management and Custom Object Creation You are required to create a custom Python class that simulates some of the advanced features of memory management as seen in the underlying implementation of Python\'s object system. This class will need to handle its memory allocations and implement certain internal behaviors manually. Task 1. **Define a Python class `ManagedObject`** that simulates custom memory allocation. 2. **Override the constructor and destructor** to print messages indicating when an object is created and destroyed, respectively. 3. **Simulate reference counting** mechanism by implementing methods to increment and decrement a reference count. 4. **Use a class-level list** to keep track of all instances created, assisting in simulating garbage collection. 5. **Implement a class method** that clears all objects that have a reference count of zero. Specifications - **Constructor (`__init__`)**: Should initialize the object and add it to the class-level list. - **Destructor (`__del__`)**: Should print a message indicating the object is being destroyed. - **Reference Management**: - `inc_ref`: Increments the reference count. - `dec_ref`: Decrements the reference count. - **Class Method**: - `collect_garbage`: Iterates over all created instances and deletes those with a reference count of zero. - Your solution should ensure no direct interactions with Python\'s built-in `gc` module. Example Usage ```python class ManagedObject: # Your implementation here # Example instantiation and usage obj1 = ManagedObject() obj2 = ManagedObject() obj1.inc_ref() obj2.inc_ref() obj2.inc_ref() print(\\"Decrementing references...\\") obj1.dec_ref() obj2.dec_ref() obj2.dec_ref() # Attempt to collect garbage ManagedObject.collect_garbage() ``` Expected Output ``` Object created Object created Decrementing references... Garbage collector removed an object with zero references Garbage collector removed an object with zero references ``` Constraints - This simplified memory management should not use actual external memory management libraries. - Ensure thread-safety is not required for this implementation. - Focus on the simulation and conceptual understanding of memory management and object lifecycle.","solution":"class ManagedObject: instances = [] def __init__(self): self.ref_count = 1 ManagedObject.instances.append(self) print(\\"Object created\\") def __del__(self): print(\\"Object destroyed\\") def inc_ref(self): self.ref_count += 1 def dec_ref(self): self.ref_count -= 1 @classmethod def collect_garbage(cls): # Create a list of objects to be removed to_remove = [obj for obj in cls.instances if obj.ref_count == 0] for obj in to_remove: cls.instances.remove(obj) print(\\"Garbage collector removed an object with zero references\\") del obj"},{"question":"Objective Create a Python script that manages environment variables using the `os` module. The script should be able to: 1. Retrieve the value of an environment variable. 2. Set a new environment variable or update an existing one. 3. Delete an environment variable. Your solution should demonstrate a thorough understanding of how Python interacts with operating system environment variables, handle edge cases, and ensure that modifications correctly update the environment. Specifications 1. **Function 1**: `get_env(var_name: str) -> str` - **Input**: A single string representing the environment variable\'s name. - **Output**: The value associated with the environment variable, or `None` if it does not exist. 2. **Function 2**: `set_env(var_name: str, var_value: str) -> None` - **Input**: Two strings, one representing the environment variable\'s name and the second representing its new value. - **Output**: None. The function should set or update the environment variable. 3. **Function 3**: `del_env(var_name: str) -> None` - **Input**: A single string representing the environment variable\'s name. - **Output**: None. The function should delete the environment variable if it exists. - **Note**: If the environment variable does not exist, the function should ensure no error is raised. Constraints - Use the `os` module exclusively to interact with environment variables. - Ensure that the script is cross-platform compatible (i.e., should work on both Unix and Windows systems). - Handle any edge cases where environment variables might not exist. Example Usage ```python # Setting environment variable set_env(\'TEST_VAR\', \'test_value\') assert get_env(\'TEST_VAR\') == \'test_value\' # Updating environment variable set_env(\'TEST_VAR\', \'new_value\') assert get_env(\'TEST_VAR\') == \'new_value\' # Deleting environment variable del_env(\'TEST_VAR\') assert get_env(\'TEST_VAR\') is None ``` Submit your solution in a `.py` file with proper function implementations and include a main section to demonstrate the example usage.","solution":"import os def get_env(var_name): Retrieve the value of an environment variable. Parameters: var_name (str): The name of the environment variable. Returns: str: The value of the environment variable, or None if it does not exist. return os.environ.get(var_name) def set_env(var_name, var_value): Set or update an environment variable. Parameters: var_name (str): The name of the environment variable. var_value (str): The value to set for the environment variable. Returns: None os.environ[var_name] = var_value def del_env(var_name): Delete an environment variable. Parameters: var_name (str): The name of the environment variable to delete. Returns: None if var_name in os.environ: del os.environ[var_name]"},{"question":"Quantized Model Evaluation with PyTorch Objective: The goal of this task is to demonstrate your understanding of PyTorch\'s quantization utilities by implementing a function to evaluate the quality of a quantized model using specific error metrics. Problem Statement: You are provided with two PyTorch models: a float (original) model and a quantized model. You need to write a function `evaluate_quantized_model` that computes the Signal to Quantization Noise Ratio (SQNR), Normalized L2 Error, and Cosine Similarity between the outputs of the two models given the same inputs. Function Signature: ```python def evaluate_quantized_model(float_model: torch.nn.Module, quantized_model: torch.nn.Module, data_loader: torch.utils.data.DataLoader) -> dict: Evaluates the quality of the quantized model outputs compared to the float model outputs. Parameters: - float_model (torch.nn.Module): The original float model. - quantized_model (torch.nn.Module): The quantized model. - data_loader (torch.utils.data.DataLoader): A DataLoader that provides the input data. Returns: - dict: A dictionary with keys \'sqnr\', \'normalized_l2_error\', and \'cosine_similarity\' mapping to their corresponding values. ``` Inputs: 1. `float_model`: The original PyTorch model (type `torch.nn.Module`) in float precision. 2. `quantized_model`: The PyTorch quantized model (type `torch.nn.Module`). 3. `data_loader`: A DataLoader object that provides batches of input data. Outputs: - A dictionary with the following keys and corresponding values: - `\'sqnr\'`: The average Signal to Quantization Noise Ratio computed across all batches. - `\'normalized_l2_error\'`: The average Normalized L2 Error computed across all batches. - `\'cosine_similarity\'`: The average Cosine Similarity computed across all batches. Constraints: - Assume that both models can handle the inputs provided by the data_loader. - The evaluation should be performed in evaluation mode (i.e., `model.eval()`). - The computations must handle GPU-based models if the models are on GPU, but must not assume that a GPU is available. Additional Information: You can utilize the following utility functions from the `torch.ao.ns.fx.utils` module: - `torch.ao.ns.fx.utils.compute_sqnr(x, y)` - `torch.ao.ns.fx.utils.compute_normalized_l2_error(x, y)` - `torch.ao.ns.fx.utils.compute_cosine_similarity(x, y)` Here is a skeleton code to help you get started: ```python import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_quantized_model(float_model, quantized_model, data_loader): # Set both models to evaluation mode float_model.eval() quantized_model.eval() sqnr_accumulator = 0 l2_error_accumulator = 0 cosine_similarity_accumulator = 0 num_batches = 0 with torch.no_grad(): # No need to compute gradients for evaluation for inputs, _ in data_loader: # Ensure the inputs are on the correct device device = next(float_model.parameters()).device inputs = inputs.to(device) # Get outputs from both models float_outputs = float_model(inputs) quantized_outputs = quantized_model(inputs) # Compute SQNR sqnr = compute_sqnr(float_outputs, quantized_outputs) sqnr_accumulator += sqnr # Compute Normalized L2 Error l2_error = compute_normalized_l2_error(float_outputs, quantized_outputs) l2_error_accumulator += l2_error # Compute Cosine Similarity cosine_similarity = compute_cosine_similarity(float_outputs, quantized_outputs) cosine_similarity_accumulator += cosine_similarity num_batches += 1 # Compute averages average_sqnr = sqnr_accumulator / num_batches average_l2_error = l2_error_accumulator / num_batches average_cosine_similarity = cosine_similarity_accumulator / num_batches return { \'sqnr\': average_sqnr, \'normalized_l2_error\': average_l2_error, \'cosine_similarity\': average_cosine_similarity } ``` Notes: - Make sure to handle edge cases where data_loader may have zero batches. - Your implementation should be efficient and take advantage of batch processing where possible.","solution":"import torch from torch.ao.ns.fx.utils import compute_sqnr, compute_normalized_l2_error, compute_cosine_similarity def evaluate_quantized_model(float_model, quantized_model, data_loader): # Set both models to evaluation mode float_model.eval() quantized_model.eval() sqnr_accumulator = 0 l2_error_accumulator = 0 cosine_similarity_accumulator = 0 num_batches = 0 with torch.no_grad(): # No need to compute gradients for evaluation for inputs, _ in data_loader: # Ensure the inputs are on the correct device device = next(float_model.parameters()).device inputs = inputs.to(device) # Get outputs from both models float_outputs = float_model(inputs) quantized_outputs = quantized_model(inputs) # Compute SQNR sqnr = compute_sqnr(float_outputs, quantized_outputs) sqnr_accumulator += sqnr # Compute Normalized L2 Error l2_error = compute_normalized_l2_error(float_outputs, quantized_outputs) l2_error_accumulator += l2_error # Compute Cosine Similarity cosine_similarity = compute_cosine_similarity(float_outputs, quantized_outputs) cosine_similarity_accumulator += cosine_similarity num_batches += 1 if num_batches == 0: return { \'sqnr\': 0, \'normalized_l2_error\': float(\'inf\'), \'cosine_similarity\': 0 } # Compute averages average_sqnr = sqnr_accumulator / num_batches average_l2_error = l2_error_accumulator / num_batches average_cosine_similarity = cosine_similarity_accumulator / num_batches return { \'sqnr\': average_sqnr, \'normalized_l2_error\': average_l2_error, \'cosine_similarity\': average_cosine_similarity }"},{"question":"# Asyncio Synchronization Challenge You are tasked with implementing a simulation of a simplified parking lot system using asyncio synchronization primitives in Python 3.10. The parking lot has a limited number of parking spots and multiple cars trying to enter and exit the parking lot concurrently. Objectives 1. Implement a `ParkingLot` class using asyncio synchronization primitives. 2. Manage the entry and exit of cars using asyncio `Lock` and `Semaphore` to ensure safety and efficiency. Requirements 1. **ParkingLot Class** - **Initialization**: - `ParkingLot(total_spots: int)`: - `total_spots` (int): Total number of parking spots available in the lot. - **Methods**: - `async def enter(self, car_id: int) -> None`: - Simulates a car entering the parking lot. If no parking spots are available, the car should wait until a spot is freed. - Use a semaphore to manage the availability of parking spots. - Use a print statement to indicate when a car has entered the lot. - `async def exit(self, car_id: int) -> None`: - Simulates a car exiting the parking lot and freeing up a spot. - Ensure that the semaphore is properly released when a car exits. - Use a print statement to indicate when a car has exited the lot. - `async def monitor(self) -> None`: - Constantly monitors and prints the number of available spots in the parking lot every second. - Run this as a background task to keep track of parking lot status. Use `asyncio.sleep()` to pause between checks. 2. **Simulation**: - Implement an `async def main()` function that: - Creates a `ParkingLot` object with a specified number of parking spots (e.g., 3 spots). - Spawns multiple tasks to simulate cars trying to enter and exit the parking lot. - Runs the `parking_lot.monitor()` in the background. - Uses random delays to simulate the time cars take to enter and exit the parking lot. Input and Output - No direct input is provided to the functionsâ€”you should simulate the behavior internally. - Use print statements to provide output indicating the state of the parking lot (e.g., car entries, car exits, and available spots). Constraints - Ensure that the `ParkingLot` methods are safe for concurrent access using asyncio synchronization primitives. - Simulate at least 5 to 10 cars entering and exiting with random delays. Example Usage ```python import asyncio import random class ParkingLot: def __init__(self, total_spots: int): self.spots = asyncio.Semaphore(total_spots) self.lock = asyncio.Lock() self.total_spots = total_spots self.current_occupied = 0 async def enter(self, car_id: int) -> None: async with self.spots: async with self.lock: self.current_occupied += 1 print(f\\"Car {car_id} entered. Occupied spots: {self.current_occupied}/{self.total_spots}\\") async def exit(self, car_id: int) -> None: async with self.lock: self.spots.release() self.current_occupied -= 1 print(f\\"Car {car_id} exited. Occupied spots: {self.current_occupied}/{self.total_spots}\\") async def monitor(self) -> None: while True: async with self.lock: print(f\\"Available spots: {self.total_spots - self.current_occupied}\\") await asyncio.sleep(1) async def car_simulation(parking_lot: ParkingLot, car_id: int): await parking_lot.enter(car_id) await asyncio.sleep(random.randint(1, 5)) await parking_lot.exit(car_id) async def main(): parking_lot = ParkingLot(total_spots=3) car_tasks = [car_simulation(parking_lot, i) for i in range(1, 11)] monitor_task = asyncio.create_task(parking_lot.monitor()) await asyncio.gather(*car_tasks) monitor_task.cancel() asyncio.run(main()) ``` Notes - Ensure your implementation is free of data races and misuse of asyncio primitives. - Properly comment your code for clarity where necessary.","solution":"import asyncio import random class ParkingLot: def __init__(self, total_spots: int): self.spots = asyncio.Semaphore(total_spots) self.lock = asyncio.Lock() self.total_spots = total_spots self.current_occupied = 0 async def enter(self, car_id: int) -> None: async with self.spots: async with self.lock: self.current_occupied += 1 print(f\\"Car {car_id} entered. Occupied spots: {self.current_occupied}/{self.total_spots}\\") async def exit(self, car_id: int) -> None: async with self.lock: self.current_occupied -= 1 print(f\\"Car {car_id} exited. Occupied spots: {self.current_occupied}/{self.total_spots}\\") self.spots.release() async def monitor(self) -> None: while True: async with self.lock: print(f\\"Available spots: {self.total_spots - self.current_occupied}\\") await asyncio.sleep(1) async def car_simulation(parking_lot: ParkingLot, car_id: int): await parking_lot.enter(car_id) await asyncio.sleep(random.randint(1, 5)) await parking_lot.exit(car_id) async def main(): parking_lot = ParkingLot(total_spots=3) car_tasks = [car_simulation(parking_lot, i) for i in range(1, 11)] monitor_task = asyncio.create_task(parking_lot.monitor()) await asyncio.gather(*car_tasks) monitor_task.cancel() asyncio.run(main())"},{"question":"Objective Assess the students\' ability to utilize the `seaborn.objects` module for creating complex plots with multiple variables and customization. Problem Statement Using the `seaborn` library and the `mpg` dataset, create a collection of plots that demonstrate the following functionalities: 1. Plot multiple pairwise relationships between variables. 2. Use faceting to split the data by a categorical variable. 3. Use `cross=False` to pair each position in the lists of `x` and `y` variables. 4. Use `wrap` to organize multiple subplots. 5. Customize the plot labels. Input Data Use the `mpg` dataset from the `seaborn` library. Load it as follows: ```python import seaborn.objects as so from seaborn import load_dataset mpg = load_dataset(\\"mpg\\") ``` Instructions 1. Create a pairwise scatter plot with `mpg` as the dependent variable (`y`) and `displacement`, `weight`, `horsepower`, and `cylinders` as independent variables (`x`). Use `wrap=2` to organize the plots in a grid. 2. Create multiple pairwise scatter plots with `weight` and `acceleration` as independent variables (`x`) and `displacement` and `horsepower` as dependent variables (`y`). Ensure to set `cross=False` so that each plot pairs corresponding variables in the lists. 3. Create pairwise scatter plots between the `weight` and `acceleration` variables versus `horsepower` and `displacement`, faceting by the `origin` variable. 4. Customize the labels of the `x` and `y` axes in at least one of the plots. Output Your function should display the required plots. Use the seaborn `objects` API to accomplish this. Example ```python import seaborn.objects as so from seaborn import load_dataset def create_complex_plots(): mpg = load_dataset(\\"mpg\\") # Plot 1: Pairwise plot with wrap ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .add(so.Dots()) ).show() # Plot 2: Pairwise plot with cross=False ( so.Plot(mpg) .pair(x=[\\"weight\\", \\"acceleration\\"], y=[\\"displacement\\", \\"horsepower\\"], cross=False) .add(so.Dots()) ).show() # Plot 3: Pairwise plot with faceting ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"displacement\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ).show() # Custom labels plot ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"weight\\", \\"displacement\\"]) .label(x0=\\"Weight (lb)\\", x1=\\"Displacement (cu in)\\", y=\\"MPG\\") .add(so.Dots()) ).show() create_complex_plots() ``` Ensure that your plots are clear, well-labeled, and follow the instructions above.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_complex_plots(): mpg = load_dataset(\\"mpg\\") # Plot 1: Pairwise plot with wrap ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], wrap=2) .add(so.Dots()) ).show() # Plot 2: Pairwise plot with cross=False ( so.Plot(mpg) .pair(x=[\\"weight\\", \\"acceleration\\"], y=[\\"displacement\\", \\"horsepower\\"], cross=False) .add(so.Dots()) ).show() # Plot 3: Pairwise plot with faceting ( so.Plot(mpg, x=\\"weight\\") .pair(y=[\\"horsepower\\", \\"displacement\\"]) .facet(col=\\"origin\\") .add(so.Dots()) ).show() # Custom labels plot ( so.Plot(mpg, y=\\"mpg\\") .pair(x=[\\"weight\\", \\"displacement\\"]) .label(x0=\\"Weight (lb)\\", x1=\\"Displacement (cu in)\\", y=\\"MPG\\") .add(so.Dots()) ).show()"},{"question":"Manipulating ZIP Archives with Python\'s `zipfile` Module Objective: Demonstrate your understanding of the Python `zipfile` module by creating a function that manages ZIP archives. You are required to implement a function that compresses a directory, lists the contents of a ZIP archive, and extracts the ZIP archive while handling errors appropriately. Requirements: 1. **Function to Compress a Directory:** - **Function Signature:** `def compress_directory(directory_path: str, zip_path: str, compression_method: int) -> None` - **Parameters:** - `directory_path`: Path to the directory to be compressed. - `zip_path`: Path where the ZIP file will be created. - `compression_method`: Compression method to be used (one of `zipfile.ZIP_STORED`, `zipfile.ZIP_DEFLATED`, `zipfile.ZIP_BZIP2`, `zipfile.ZIP_LZMA`). - **Behavior:** - Create a ZIP file at `zip_path`. - Compress all files and folders in `directory_path` using the specified `compression_method`. - If the specified `compression_method` is not supported, raise a `ValueError` with an appropriate message. - Handle other potential errors such as file read/write issues. 2. **Function to List Contents of a ZIP Archive:** - **Function Signature:** `def list_zip_contents(zip_path: str) -> list` - **Parameters:** - `zip_path`: Path to the ZIP file. - **Behavior:** - Return a list of file names contained in the ZIP archive. - If the ZIP file is invalid, raise `zipfile.BadZipFile`. 3. **Function to Extract a ZIP Archive:** - **Function Signature:** `def extract_zip(zip_path: str, extract_to: str) -> None` - **Parameters:** - `zip_path`: Path to the ZIP file. - `extract_to`: Directory where the ZIP contents should be extracted. - **Behavior:** - Extract all files from the ZIP archive to the specified directory. - Handle password-protected files by raising `RuntimeError` with an appropriate message. - Handle potential errors such as invalid ZIP files, file read/write issues. 4. **Exception Handling:** - Ensure your solution handles exceptions gracefully, logs meaningful messages, and prevents program crashes. Example Usage: ```python # Compress the directory compress_directory(\'/path/to/folder\', \'archive.zip\', zipfile.ZIP_DEFLATED) # List contents of the ZIP archive contents = list_zip_contents(\'archive.zip\') print(contents) # Outputs: List of files in the archive # Extract the ZIP archive extract_zip(\'archive.zip\', \'/path/to/extract/destination\') ``` Constraints: - Assume that the provided paths exist and are accessible. - The functions should handle different operating systems (e.g., Windows, Linux). - The solution should be efficient and avoid unnecessary memory usage. *Use the provided documentation and implement the required functions demonstrating your comprehension of the `zipfile` module\'s capabilities.*","solution":"import os import zipfile def compress_directory(directory_path: str, zip_path: str, compression_method: int) -> None: Compresses a directory into a ZIP file with the specified compression method. Parameters: - directory_path: Path to the directory to be compressed. - zip_path: Path where the ZIP file will be created. - compression_method: Compression method to be used (one of zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA). Raises: ValueError: If the specified compression method is not supported. if compression_method not in (zipfile.ZIP_STORED, zipfile.ZIP_DEFLATED, zipfile.ZIP_BZIP2, zipfile.ZIP_LZMA): raise ValueError(\\"Unsupported compression method\\") try: with zipfile.ZipFile(zip_path, \'w\', compression_method) as zip_file: for root, dirs, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, directory_path) zip_file.write(file_path, arcname) except Exception as e: print(f\\"Error compressing directory: {e}\\") raise def list_zip_contents(zip_path: str) -> list: Lists the contents of a ZIP file. Parameters: - zip_path: Path to the ZIP file. Returns: - List of file names contained in the ZIP archive. Raises: zipfile.BadZipFile: If the ZIP file is invalid. try: with zipfile.ZipFile(zip_path, \'r\') as zip_file: return zip_file.namelist() except zipfile.BadZipFile as e: print(f\\"Invalid ZIP file: {e}\\") raise def extract_zip(zip_path: str, extract_to: str) -> None: Extracts a ZIP archive to a specified directory. Parameters: - zip_path: Path to the ZIP file. - extract_to: Directory where the ZIP contents should be extracted. Raises: RuntimeError: If the archive is password-protected. zipfile.BadZipFile: If the ZIP file is invalid. try: with zipfile.ZipFile(zip_path, \'r\') as zip_file: if zip_file.testzip() is not None: raise RuntimeError(\\"Password-protected archive is not supported\\") zip_file.extractall(extract_to) except zipfile.BadZipFile as e: print(f\\"Invalid ZIP file: {e}\\") raise except RuntimeError as e: print(f\\"Extraction error: {e}\\") raise"},{"question":"You are required to implement a function `hash_and_verify_passwords` that demonstrates the use of the `crypt` module for hashing passwords and verifying their integrity. The function should implement the following: 1. Hash a given password using all available methods in `crypt.methods`. 2. Verify each hashed password against the original password to ensure integrity. # Function Signature ```python def hash_and_verify_passwords(password: str) -> dict: pass ``` # Input - `password (str)`: A plain-text password that will be hashed and verified. # Output - `result (dict)`: A dictionary where the keys are the hashing methods (as strings) and the values are tuples `(hashed_password, verification_result)`. The `hashed_password` is the hashed version of the given password, and `verification_result` is a boolean indicating whether the hashed password matches the original password. # Constraints - The function must use the methods listed in `crypt.methods`. - The verification should use the `compare_digest` function from the `hmac` module for constant-time string comparison. # Example ```python import crypt from hmac import compare_digest as compare_hash def hash_and_verify_passwords(password: str) -> dict: result = {} for method in crypt.methods: hashed_password = crypt.crypt(password, crypt.mksalt(method)) verification_result = compare_hash(hashed_password, crypt.crypt(password, hashed_password)) result[method.name] = (hashed_password, verification_result) return result # Example usage: password = \\"securepassword\\" result = hash_and_verify_passwords(password) for method, (hashed_pwd, valid) in result.items(): print(f\\"Method: {method}, Hashed: {hashed_pwd}, Verified: {valid}\\") ``` You should replace the example usage with your own tests to validate the function. # Note - The `crypt` module is only available on Unix systems. If you\'re running the tests on a non-Unix system, the module and function may not work as expected.","solution":"import crypt from hmac import compare_digest as compare_hash def hash_and_verify_passwords(password: str) -> dict: result = {} for method in crypt.methods: hashed_password = crypt.crypt(password, crypt.mksalt(method)) verification_result = compare_hash(hashed_password, crypt.crypt(password, hashed_password)) result[method.name] = (hashed_password, verification_result) return result"},{"question":"You are tasked with developing a library of functions that utilizes the `urllib.parse` module to handle various URL-related tasks. Your implementation should provide a suite of utility functions that can be used to parse, join, and modify URLs. Specifically, you need to implement four functions: 1. `parse_url(url: str) -> dict`: Parses a URL into its components. 2. `join_urls(base: str, relative: str) -> str`: Combines a base URL with a relative URL to produce an absolute URL. 3. `query_to_dict(query: str) -> dict`: Converts a query string into a dictionary. 4. `dict_to_query(params: dict) -> str`: Converts a dictionary into a query string. # Function Specifications 1. **parse_url(url: str) -> dict** - **Input:** A URL string (e.g., `\\"http://example.com:80/path?query=value#fragment\\"`). - **Output:** A dictionary with keys `\'scheme\'`, `\'netloc\'`, `\'path\'`, `\'params\'`, `\'query\'`, `\'fragment\'`, `\'username\'`, `\'password\'`, `\'hostname\'`, and `\'port\'`. - If a component is missing, its value should be `None`. 2. **join_urls(base: str, relative: str) -> str** - **Input:** Two URL strings, `base` and `relative`. - **Output:** A single URL string representing the unified URL. - Use `urllib.parse.urljoin` to implement this function. 3. **query_to_dict(query: str) -> dict** - **Input:** A query string (e.g., `\\"key1=value1&key2=value2\\"`). - **Output:** A dictionary where keys are query variable names and values are lists of values for each name. - Use `urllib.parse.parse_qs` to implement this function. 4. **dict_to_query(params: dict) -> str** - **Input:** A dictionary where keys are query variable names and values are lists of values for each name. - **Output:** A query string constructed from the dictionary. - Use `urllib.parse.urlencode` to implement this function. Ensure that list values are properly encoded by setting `doseq=True`. # Constraints - You can assume that the input strings are valid URLs or query strings. - You need to handle cases where components (such as schemes, netloc, etc.) might be missing. - Adhere to the function specifications strictly. - Your implementation must use the `urllib.parse` module as described in the documentation. - Performance should be optimized to handle typical URL lengths and complexities encountered in standard web usage. # Examples ```python # Example for parse_url url = \\"http://user:pass@hostname:80/path;params?query=value#fragment\\" parsed = parse_url(url) print(parsed) # Output: # { # \'scheme\': \'http\', # \'netloc\': \'user:pass@hostname:80\', # \'path\': \'/path\', # \'params\': \'params\', # \'query\': \'query=value\', # \'fragment\': \'fragment\', # \'username\': \'user\', # \'password\': \'pass\', # \'hostname\': \'hostname\', # \'port\': 80 # } # Example for join_urls base = \\"http://www.example.com/path/\\" relative = \\"subpath/file.html\\" absolute = join_urls(base, relative) print(absolute) # Output: \\"http://www.example.com/path/subpath/file.html\\" # Example for query_to_dict query = \\"key1=value1&key2=value2\\" params = query_to_dict(query) print(params) # Output: {\'key1\': [\'value1\'], \'key2\': [\'value2\']} # Example for dict_to_query params = {\'key1\': [\'value1\'], \'key2\': [\'value2\']} query = dict_to_query(params) print(query) # Output: \\"key1=value1&key2=value2\\" ``` **Note:** Proper error handling for invalid URLs or malformed query strings is not required as per the problem constraints but should be kept in mind for real-world applications.","solution":"from urllib.parse import urlparse, urljoin, parse_qs, urlencode def parse_url(url: str) -> dict: parsed = urlparse(url) return { \'scheme\': parsed.scheme or None, \'netloc\': parsed.netloc or None, \'path\': parsed.path or None, \'params\': parsed.params or None, \'query\': parsed.query or None, \'fragment\': parsed.fragment or None, \'username\': parsed.username or None, \'password\': parsed.password or None, \'hostname\': parsed.hostname or None, \'port\': parsed.port or None } def join_urls(base: str, relative: str) -> str: return urljoin(base, relative) def query_to_dict(query: str) -> dict: return parse_qs(query) def dict_to_query(params: dict) -> str: return urlencode(params, doseq=True)"},{"question":"# Python Coding Assessment Question **Objective**: Demonstrate your understanding of Python\'s `contextlib` module, particularly `contextmanager` decorator and `ExitStack`. Problem Statement You are required to implement a custom context manager using the `contextlib.contextmanager` decorator to manage the resources of a database connection. The context manager should: 1. Acquire and connect to a database. 2. Ensure the connection is released properly, even if an exception occurs. 3. Use `ExitStack` to manage multiple database connections simultaneously. # Part 1: Custom Context Manager Write a function `database_connection_manager` using the `contextlib.contextmanager` decorator. This function should: - Acquire a database connection (you can simulate this with a simple print statement \\"Connection Acquired\\"). - Release the database connection (simulated with a print statement \\"Connection Released\\"). - Yield the connection object (simulated with a string \\"connection\\"). - Ensure that the connection is properly closed even if an exception is raised within the `with` block. **Example Usage:** ```python from contextlib import contextmanager @contextmanager def database_connection_manager(): # Simulate acquiring the database connection print(\\"Connection Acquired\\") connection = \\"connection\\" try: yield connection finally: # Simulate releasing the database connection print(\\"Connection Released\\") # Test the custom context manager with database_connection_manager() as conn: print(f\\"Using {conn}\\") raise Exception(\\"Test exception\\") ``` **Expected Output:** ``` Connection Acquired Using connection Connection Released ``` # Part 2: Managing Multiple Connections Using the `database_connection_manager` function you created, write a function `manage_multiple_connections` that accepts a list of connection names and uses `contextlib.ExitStack` to manage these connections simultaneously. **Requirements:** - Use `ExitStack` to acquire and release each connection properly. - Print the connections obtained in the process. **Example Usage:** ```python def manage_multiple_connections(connection_names): from contextlib import ExitStack with ExitStack() as stack: connections = [stack.enter_context(database_connection_manager()) for name in connection_names] for conn in connections: print(f\\"Managing {conn}\\") # Test managing multiple connections manage_multiple_connections([\'db1\', \'db2\', \'db3\']) ``` **Expected Output:** ``` Connection Acquired Connection Acquired Connection Acquired Managing connection Managing connection Managing connection Connection Released Connection Released Connection Released ``` Constraints - The function `database_connection_manager` should use the `contextlib.contextmanager` decorator. - Use the `ExitStack` class from the `contextlib` module in `manage_multiple_connections`. Note: Make sure your solutions properly handle exceptions and that every connection is released correctly at the end of the `with` block. # Submission Submit your code implementations for the `database_connection_manager` function and the `manage_multiple_connections` function. Ensure your code contains sufficient comments to explain the logic clearly. Happy Coding!","solution":"from contextlib import contextmanager, ExitStack @contextmanager def database_connection_manager(): Simulate a database connection context manager. Acquires and releases a mock database connection. print(\\"Connection Acquired\\") connection = \\"connection\\" try: yield connection finally: print(\\"Connection Released\\") def manage_multiple_connections(connection_names): Manages multiple database connections using ExitStack. Args: connection_names (list): List of connection names (unused, only for simulation). with ExitStack() as stack: connections = [stack.enter_context(database_connection_manager()) for name in connection_names] for conn in connections: print(f\\"Managing {conn}\\")"},{"question":"You are tasked with implementing a function to convert a stereo WAV file to mono. A mono WAV file has only one audio channel, while a stereo file has two. To achieve this, you need to average the left and right channel samples for each frame. # Function Signature ```python def stereo_to_mono(input_wav: str, output_wav: str) -> None: This function converts a stereo WAV file to mono. :param input_wav: Path to the input stereo WAV file :param output_wav: Path to output the mono WAV file :return: None # Your implementation here ``` # Input - `input_wav`: A string representing the file path to the input stereo WAV (.wav) file. - `output_wav`: A string representing the file path where the output mono WAV (.wav) file should be saved. # Output - A mono WAV file saved at the path specified by `output_wav`. # Constraints - Assume the input WAV file is in the WAVE_FORMAT_PCM format. - The sample width will be 2 bytes (16 bits) at all times. - The input WAV file will always have a frame rate of 44100 Hz. - The input WAV file will always use 2-channels (stereo). # Example Suppose the `input_wav` points to a stereo WAV file with 100 audio frames. The output should be a new mono WAV file at `output_wav` with the same audio frames, but averaged into one channel. # Steps 1. Open the input stereo WAV file in \'rb\' (read binary) mode. 2. Read all parameters (number of channels, sample width, frame rate, etc.) using the appropriate methods on the `Wave_read` object. 3. Initialize a `Wave_write` object to create the output mono WAV file. 4. Set the necessary parameters for the output file: - 1 channel - The same sample width - Same frame rate 5. Read frames from the input file, process them to average the channels, and write them to the output file. 6. Close both files properly. # Notes - You may use the `struct` module to handle the binary data conversion between bytes and integers if needed. - Ensure you handle file operations and exceptions properly. # Code Template ```python import wave import struct def stereo_to_mono(input_wav: str, output_wav: str) -> None: with wave.open(input_wav, \'rb\') as in_wave: params = in_wave.getparams() if params.nchannels != 2: raise wave.Error(\\"Input file is not stereo\\") with wave.open(output_wav, \'wb\') as out_wave: out_wave.setparams((1, params.sampwidth, params.framerate, params.nframes, params.comptype, params.compname)) for _ in range(params.nframes): # Read one frame (2 samples - left and right channel) frame_data = in_wave.readframes(1) left, right = struct.unpack(\'<hh\', frame_data) # Calculate the average and pack it back mono_value = (left + right) // 2 out_wave.writeframes(struct.pack(\'<h\', mono_value)) # Sample usage # stereo_to_mono(\'stereo_sample.wav\', \'mono_output.wav\') ```","solution":"import wave import struct def stereo_to_mono(input_wav: str, output_wav: str) -> None: This function converts a stereo WAV file to mono. :param input_wav: Path to the input stereo WAV file :param output_wav: Path to output the mono WAV file :return: None with wave.open(input_wav, \'rb\') as in_wave: params = in_wave.getparams() if params.nchannels != 2: raise wave.Error(\\"Input file is not stereo\\") # Extract parameters n_channels, sampwidth, framerate, n_frames, comptype, compname = params with wave.open(output_wav, \'wb\') as out_wave: # Set parameters for mono output out_wave.setnchannels(1) out_wave.setsampwidth(sampwidth) out_wave.setframerate(framerate) out_wave.setnframes(n_frames) out_wave.setcomptype(comptype, compname) # Process each frame for _ in range(n_frames): frame_data = in_wave.readframes(1) left, right = struct.unpack(\'<hh\', frame_data) # Average the left and right channels mono_value = (left + right) // 2 out_wave.writeframes(struct.pack(\'<h\', mono_value)) # Sample usage: # stereo_to_mono(\'stereo_sample.wav\', \'mono_output.wav\')"},{"question":"Coding Assessment Question # Objective Implement a custom PyTorch function that operates on real-valued inputs and outputs, and then verify its gradients using `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck`. # Description 1. Implement a custom PyTorch function `custom_function` that takes a real-valued input tensor `x` of size `(N,)` and returns an output tensor `y` of size `(M,)`. The function should be non-trivial and suitable for gradient verification. 2. Write a test function `test_custom_function` that: - Takes no arguments. - Constructs a random input tensor of size `(N,)` that requires gradients. - Calls `torch.autograd.gradcheck` to verify the gradients of `custom_function`. - Calls `torch.autograd.gradgradcheck` to verify the second-order gradients of `custom_function`. 3. Ensure that `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck` are used correctly and return `True`. # Constraints - The input tensor `x` should be of size `N=5`. - The output tensor `y` should be of size `M=3`. - Use a random seed for reproducibility. # Expected Input and Output - **Input:** None (The `test_custom_function` will handle input internally). - **Output:** `True` if both gradcheck and gradgradcheck pass, otherwise raise an appropriate error. # Example Implementation ```python import torch import torch.autograd def custom_function(x): # Implement a function that takes a tensor x of size (N,) and returns a tensor y of size (M,) y = torch.zeros(3) y[0] = x[0] ** 2 + x[1] y[1] = x[1] * x[2] - x[3] y[2] = torch.sin(x[3] * x[4]) return y def test_custom_function(): torch.manual_seed(0) # Set a random seed for reproducibility N = 5 x = torch.randn(N, dtype=torch.double, requires_grad=True) # gradcheck gradcheck_passed = torch.autograd.gradcheck(custom_function, (x,)) if not gradcheck_passed: raise ValueError(\\"gradcheck failed\\") # gradgradcheck gradgradcheck_passed = torch.autograd.gradgradcheck(custom_function, (x,)) if not gradgradcheck_passed: raise ValueError(\\"gradgradcheck failed\\") return True # Example usage print(test_custom_function()) # Should output: True ``` # Evaluation Criteria - Correct implementation of `custom_function` to produce appropriate non-trivial transformations. - Proper usage of `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck`. - Handling of input tensor and its gradients. - Code should be clean, well-commented, and follow Python coding standards.","solution":"import torch def custom_function(x): Custom PyTorch function that maps a 5-dimensional input tensor to a 3-dimensional output tensor. y = torch.zeros(3, dtype=torch.double) y[0] = x[0] ** 2 + x[1] y[1] = x[1] * x[2] - x[3] y[2] = torch.sin(x[3] * x[4]) return y"},{"question":"# XML-RPC Server and Client Implementation In this exercise, you will demonstrate your understanding of the `xmlrpc` package by setting up an XML-RPC server and client. The server will provide methods for basic arithmetic operations, and the client will remotely call these methods. Task 1. **Implement the XML-RPC Server**: - Create a class `ArithmeticOperations` with the following methods: - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the difference between `a` and `b`. - `multiply(a, b)`: Returns the product of `a` and `b`. - `divide(a, b)`: Returns the quotient when `a` is divided by `b`. It should handle division by zero by returning the string `\\"Error: Division by zero\\"`. - Set up an XML-RPC server that registers an instance of the `ArithmeticOperations` class and listens for requests. 2. **Implement the XML-RPC Client**: - Create a client that connects to the XML-RPC server and calls each of the arithmetic methods with example arguments. - Print the results of each remote method call. Input and Output formats - The server does not take input but instead listens for incoming requests on a specified port. - The client does not take input directly from the user but makes predefined RPC calls to the server. - Print statements from the client displaying results of the RPC calls serve as the output. Constraints and Requirements: - The server must handle at least one client at a time. - The client must demonstrate usage of each method by printing results of the following RPCs: - `add(10, 5)` - `subtract(10, 5)` - `multiply(10, 5)` - `divide(10, 0)` and `divide(10, 2)` Here is a hint to get you started: **Server-side:** ```python from xmlrpc.server import SimpleXMLRPCServer class ArithmeticOperations: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: return \\"Error: Division by zero\\" return a / b server = SimpleXMLRPCServer((\\"localhost\\", 8000)) server.register_instance(ArithmeticOperations()) print(\\"Server started on port 8000...\\") server.serve_forever() ``` **Client-side:** ```python from xmlrpc.client import ServerProxy client = ServerProxy(\\"http://localhost:8000/\\") print(\\"10 + 5 =\\", client.add(10, 5)) print(\\"10 - 5 =\\", client.subtract(10, 5)) print(\\"10 * 5 =\\", client.multiply(10, 5)) print(\\"10 / 0 =\\", client.divide(10, 0)) print(\\"10 / 2 =\\", client.divide(10, 2)) ``` By completing this exercise, you will demonstrate your understanding of creating XML-RPC servers and clients using the `xmlrpc` package in Python.","solution":"# XML-RPC Server Implementation from xmlrpc.server import SimpleXMLRPCServer class ArithmeticOperations: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: return \\"Error: Division by zero\\" return a / b def start_server(): server = SimpleXMLRPCServer((\\"localhost\\", 8000)) server.register_instance(ArithmeticOperations()) print(\\"Server started on port 8000...\\") server.serve_forever() # The following part should only be run if starting the server manually # if __name__ == \\"__main__\\": # start_server()"},{"question":"You have been given access to a large audio file in the AIFF format. Each file is composed of multiple chunks, and we are particularly interested in reading and verifying metadata stored within each chunk. You are required to write a Python function that processes this AIFF file using the `chunk` module. Requirements: 1. Implement the function `process_aiff(file_path: str) -> dict` that: - Takes the file path of the AIFF file as input. - Reads through each chunk in the file. - Extracts and builds a dictionary of metadata (chunk IDs and their corresponding sizes). - Skips chunks that you do not need to read (to improve performance). 2. Details: - You should instantiate a `Chunk` object for each chunk. - Use the methods provided by the `Chunk` class (`getname()`, `getsize()`, `read()`, `skip()`) to manipulate and extract data. - Handle end-of-file conditions gracefully. Input: - `file_path`: A string representing the path to an AIFF file. Output: - A dictionary where each key is a chunk ID (4-byte string) and each value is the size of the chunk. Example: ```python def process_aiff(file_path: str) -> dict: # Your implementation here # Example usage: metadata = process_aiff(\'example.aiff\') print(metadata) # Expected output might look like: {\'FORM\': 50, \'COMM\': 18, ...} ``` Constraints: - Assume the file is always well-formed and readable. - The function should be efficient in terms of time complexity. - The chunks to be processed might contain very large data, so avoid reading any unnecessary data. Good luck!","solution":"import chunk def process_aiff(file_path: str) -> dict: metadata = {} with open(file_path, \'rb\') as file: while True: try: ch = chunk.Chunk(file, bigendian=True, align=True) chunk_id = ch.getname().decode(\'ascii\') chunk_size = ch.getsize() metadata[chunk_id] = chunk_size ch.skip() # Skip the data part as we are only interested in the size except EOFError: break return metadata"},{"question":"You are working on enhancing the functionalities of a video library management system. The system uses lists to store data about videos and users\' interactions. You need to implement a helper function to manage the list of videos watched by users and update their watch history efficiently. # Your Task Implement a function `manage_watch_history(actions: list, initial_watch_history: list) -> list` that takes in two parameters: 1. `actions`: A list of dictionaries where each dictionary represents an action with the following structure: ```python { \\"action\\": str, # Type of action (\'add\', \'remove\', \'clear\', \'reverse\', \'rotate\') \\"value\\": any # The value associated with the action (video_id for \'add\' and \'remove\', integer for \'rotate\') } ``` 2. `initial_watch_history`: A list of video IDs (strings) representing the initial watch history of a user. The function should perform the actions on the initial watch history list in sequence and return the updated watch history. # Detailed Specifications - When the action is `\\"add\\"`, append the video ID to the end of the watch history. - When the action is `\\"remove\\"`, remove the first occurrence of the video ID from the watch history. If the video ID is not found, do nothing. - When the action is `\\"clear\\"`, clear the entire watch history. - When the action is `\\"reverse\\"`, reverse the order of the watch history. - When the action is `\\"rotate\\"`, rotate the list by the given value. Positive values rotate the list to the right, and negative values rotate the list to the left. For example, a rotation by `1` moves the last element to the front of the list, a rotation by `-1` moves the first element to the end of the list. # Example ```python actions = [ {\\"action\\": \\"add\\", \\"value\\": \\"video1\\"}, {\\"action\\": \\"add\\", \\"value\\": \\"video2\\"}, {\\"action\\": \\"add\\", \\"value\\": \\"video3\\"}, {\\"action\\": \\"remove\\", \\"value\\": \\"video2\\"}, {\\"action\\": \\"reverse\\"}, {\\"action\\": \\"rotate\\", \\"value\\": -1}, {\\"action\\": \\"clear\\"} ] initial_watch_history = [] manage_watch_history(actions, initial_watch_history) ``` The output should be: ```python [] ``` # Constraints - The length of the actions list will be in the range of [1, 1000]. - The video IDs will be strings with a maximum length of 15 characters. - The rotation values will be integers in the range of [-100, 100] - The initial watch history list will have lengths in the range of [0, 1000]. # Notes - Ensure that your function handles empty or non-existent elements gracefully without throwing errors. - Performance should be efficient to handle the given constraints optimally. Implement the `manage_watch_history` function ensuring that all specified operations are processed correctly and efficiently.","solution":"def manage_watch_history(actions, initial_watch_history): Manages the watch history of a user by performing a sequence of actions. Parameters: - actions: A list of dictionaries representing actions to perform on the watch history. Each dictionary contains: * \\"action\\": The type of action (\'add\', \'remove\', \'clear\', \'reverse\', \'rotate\'). * \\"value\\": The value associated with the action. - initial_watch_history: A list of video IDs (strings) representing the initial watch history. Returns: - The updated watch history after performing all actions. watch_history = initial_watch_history.copy() for action_dict in actions: action = action_dict.get(\\"action\\") value = action_dict.get(\\"value\\") if action == \\"add\\": watch_history.append(value) elif action == \\"remove\\": if value in watch_history: watch_history.remove(value) elif action == \\"clear\\": watch_history.clear() elif action == \\"reverse\\": watch_history.reverse() elif action == \\"rotate\\": n = len(watch_history) if n > 0: k = value % n # Ensure rotation is within the bounds of the list length watch_history = watch_history[-k:] + watch_history[:-k] return watch_history"},{"question":"# Objective: Implement a function that manages future features based on the provided version information, mimicking the behavior of the `__future__` module. # Problem Statement: You are to create a feature management system similar to the `__future__` module. Implement a class `FeatureManager` that can add and verify future features based on version information. # Requirements: 1. **Class Definition:** `FeatureManager` - **Attributes:** - `features`: A dictionary to store feature names as keys and their respective `_Feature` instances as values. 2. **Class Definition:** `_Feature` - **Attributes:** - `optional_release`: A tuple containing the optional release information. - `mandatory_release`: A tuple or `None` containing the mandatory release information. - `compiler_flag`: An integer representing the compiler flag. - **Methods:** - `getOptionalRelease()`: Returns the `optional_release`. - `getMandatoryRelease()`: Returns the `mandatory_release`. 3. **Methods of `FeatureManager`:** - `add_feature(name, optional_release, mandatory_release, compiler_flag)`: Adds a new feature to the `features` dictionary. - `is_feature_mandatory(name, current_version)`: Checks if a feature is mandatory based on the given `current_version`. # Input and Output Formats: 1. **add_feature(name, optional_release, mandatory_release, compiler_flag)** - `name`: (str) The name of the feature. - `optional_release`: (tuple) The optional release version (5-tuple). - `mandatory_release`: (tuple or None) The mandatory release version (5-tuple or None). - `compiler_flag`: (int) The compiler flag. Example: ```python manager.add_feature(\'my_feature\', (3, 6, 0, \'final\', 0), (3, 8, 0, \'final\', 0), 0x1) ``` 2. **is_feature_mandatory(name, current_version)** - `name`: (str) The name of the feature to check. - `current_version`: (tuple) The current Python version (5-tuple). Returns: - A boolean indicating whether the feature is mandatory in the given `current_version`. Example: ```python manager.is_feature_mandatory(\'my_feature\', (3, 8, 0, \'final\', 0)) # Returns: True ``` # Constraints: - The `current_version` tuple will be in the same format as `sys.version_info`. - The `optional_release` will always be less than or equal to `mandatory_release`, if `mandatory_release` is not None. # Performance Requirements: - Ensure efficient look-ups and additions to the features dictionary. - Handle up to 100 features and varying Python versions efficiently. # Example Usage: ```python class _Feature: # Implementation here class FeatureManager: # Implementation here manager = FeatureManager() manager.add_feature(\'nested_scopes\', (2, 1, 0, \'beta\', 1), (2, 2, 0, \'final\', 0), 0x1) print(manager.is_feature_mandatory(\'nested_scopes\', (2, 2, 0, \'final\', 0))) # Should print: True print(manager.is_feature_mandatory(\'nested_scopes\', (2, 1, 0, \'final\', 0))) # Should print: False ``` Create the classes and methods to pass the above examples and functionality.","solution":"class _Feature: def __init__(self, optional_release, mandatory_release, compiler_flag): Represents a future feature with its optional and mandatory release versions. self.optional_release = optional_release self.mandatory_release = mandatory_release self.compiler_flag = compiler_flag def getOptionalRelease(self): Returns the optional release version. return self.optional_release def getMandatoryRelease(self): Returns the mandatory release version. return self.mandatory_release class FeatureManager: def __init__(self): Manages future features and their respective version information. self.features = {} def add_feature(self, name, optional_release, mandatory_release, compiler_flag): Adds a new feature to the features dictionary. self.features[name] = _Feature(optional_release, mandatory_release, compiler_flag) def is_feature_mandatory(self, name, current_version): Checks if the given feature is mandatory for the current version. if name in self.features: feature = self.features[name] mandatory_release = feature.getMandatoryRelease() if mandatory_release is None: return False return current_version >= mandatory_release return False"},{"question":"You have been provided with a dataset on car performance which includes information such as car weights, horsepower, acceleration, etc., over various years. Utilizing this dataset, you need to answer several questions and visualize the results. # Task 1: Data Manipulation 1. **Load the data:** Load the dataset into a pandas DataFrame. 2. **Filter the data:** Filter the DataFrame to include only the cars manufactured after the year 1975. 3. **Group by year:** Group the filtered data by the year of manufacture. 4. **Calculate statistics:** For each group, calculate the mean weight and mean horsepower. # Task 2: Visualization 1. **Line Plot:** Create a line plot showing the change in mean weight and mean horsepower over the years. * X-axis: Year * Y-axis: Mean weight (primary), Mean horsepower (secondary) 2. **Scatter Plot:** Create a scatter plot comparing the horsepower and acceleration of the cars. * X-axis: Horsepower * Y-axis: Acceleration * Size of points: Proportional to the weight of the car # Task 3: Customization 1. **Enhance the Line Plot:** * Add titles and axis labels * Differentiate the lines (weight and horsepower) with colors and markers. * Add a legend. 2. **Enhance the Scatter Plot:** * Add a title and axis labels * Use a colormap to depict the year of manufacture # Input A CSV file named `car_data.csv` with columns: `Year`, `Weight`, `Horsepower`, `Acceleration`, `Name`. # Output Two plots as described above. # Constraints 1. Use pandas and matplotlib for the tasks. 2. Handle missing data appropriately (e.g., fill with the median value). Here is the skeleton code to get you started: ```python import pandas as pd import matplotlib.pyplot as plt # Task 1: Data Manipulation # 1. Load the data df = pd.read_csv(\'car_data.csv\') # 2. Filter the data filtered_df = df[df[\'Year\'] > 1975] # 3. Group by year grouped = filtered_df.groupby(\'Year\') # 4. Calculate statistics mean_stats = grouped[[\'Weight\', \'Horsepower\']].mean() # Task 2 & 3: Visualization # 1. Line Plot fig, ax1 = plt.subplots() ax2 = ax1.twinx() mean_stats[\'Weight\'].plot(ax=ax1, color=\'g\', marker=\'o\', label=\'Mean Weight\') mean_stats[\'Horsepower\'].plot(ax=ax2, color=\'b\', marker=\'x\', label=\'Mean Horsepower\') ax1.set_xlabel(\'Year\') ax1.set_ylabel(\'Mean Weight\', color=\'g\') ax2.set_ylabel(\'Mean Horsepower\', color=\'b\') plt.title(\'Mean Weight and Horsepower Over Years\') lines, labels = ax1.get_legend_handles_labels() lines2, labels2 = ax2.get_legend_handles_labels() ax1.legend(lines + lines2, labels + labels2, loc=0) plt.show() # 2. Scatter Plot plt.figure(figsize=(10, 6)) scatter = plt.scatter(filtered_df[\'Horsepower\'], filtered_df[\'Acceleration\'], s=filtered_df[\'Weight\']/100, c=filtered_df[\'Year\'], cmap=\'viridis\', alpha=0.6, edgecolors=\\"w\\", linewidth=0.5) plt.colorbar(scatter, label=\'Year\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Acceleration\') plt.title(\'Horsepower vs Acceleration\') plt.show() ``` Make sure to handle missing values by filling them with the median of the respective column. Customize the plots as described and ensure your code is clean and well-commented.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_data(file_path): Load the dataset from the given CSV file path. return pd.read_csv(file_path) def filter_data(df): Filter the DataFrame to include only the cars manufactured after the year 1975. return df[df[\'Year\'] > 1975] def group_and_calculate_stats(filtered_df): Group the filtered data by the year of manufacture and calculate the mean weight and horsepower. grouped = filtered_df.groupby(\'Year\') mean_stats = grouped[[\'Weight\', \'Horsepower\']].mean() return mean_stats def create_line_plot(mean_stats): Create a line plot showing the change in mean weight and mean horsepower over the years. fig, ax1 = plt.subplots() ax2 = ax1.twinx() mean_stats[\'Weight\'].plot(ax=ax1, color=\'g\', marker=\'o\', label=\'Mean Weight\') mean_stats[\'Horsepower\'].plot(ax=ax2, color=\'b\', marker=\'x\', label=\'Mean Horsepower\') ax1.set_xlabel(\'Year\') ax1.set_ylabel(\'Mean Weight\', color=\'g\') ax2.set_ylabel(\'Mean Horsepower\', color=\'b\') plt.title(\'Mean Weight and Horsepower Over Years\') lines, labels = ax1.get_legend_handles_labels() lines2, labels2 = ax2.get_legend_handles_labels() ax1.legend(lines + lines2, labels + labels2, loc=0) plt.show() def create_scatter_plot(filtered_df): Create a scatter plot comparing the horsepower and acceleration of the cars. plt.figure(figsize=(10, 6)) scatter = plt.scatter(filtered_df[\'Horsepower\'], filtered_df[\'Acceleration\'], s=filtered_df[\'Weight\']/100, c=filtered_df[\'Year\'], cmap=\'viridis\', alpha=0.6, edgecolors=\\"w\\", linewidth=0.5) plt.colorbar(scatter, label=\'Year\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Acceleration\') plt.title(\'Horsepower vs Acceleration\') plt.show()"},{"question":"**Coding Assessment Question** # Objective The objective of this exercise is to demonstrate your understanding of the `reprlib` module in Python by implementing custom behavior for object representations. You will need to subclass `Repr` and override type-specific methods to handle custom object types and size limits effectively. # Task 1. Create a class `CustomRepr` which is a subclass of `reprlib.Repr`. 2. Implement a custom representation method for dictionaries that limits the number of key-value pairs displayed to `3` pairs only. 3. Implement a custom representation method for lists that limits the number of elements displayed to `4` elements only. 4. Use the `recursive_repr` decorator to handle recursive calls within the `__repr__` function for a custom object type `MyList`. 5. Test your `CustomRepr` class on various objects, including nested and recursive structures. # Constraints and Limitations - You must use the `reprlib` module and subclass `reprlib.Repr`. - The dictionary representation should limit the output to a maximum of 3 key-value pairs. - The list representation should limit the output to a maximum of 4 elements. - Ensure that recursive structures in your custom list (`MyList`) are handled gracefully without resulting in infinite loops. # Input and Output Formats - **Input:** You will create instances of various objects such as dictionaries, lists, and custom lists. - **Output:** Strings representing the objects within the specified size limits. # Example ```python import reprlib # Step 1: Subclass reprlib.Repr class CustomRepr(reprlib.Repr): # Step 2: Custom dictionary representation def repr_dict(self, obj, level): return \'{\' + \', \'.join(f\'{self.repr(k)}: {self.repr(v)}\' for k, v in list(obj.items())[:3]) + \'}\' # Step 3: Custom list representation def repr_list(self, obj, level): return \'[\' + \', \'.join(self.repr(x) for x in obj[:4]) + \']\' # Step 4: Custom recursive list class from reprlib import recursive_repr class MyList(list): @recursive_repr() def __repr__(self): return \'<\' + \'|\'.join(map(repr, self)) + \'>\' # Testing the CustomRepr class aRepr = CustomRepr() # Dictionaries with more than 3 key-value pairs d = {i: chr(65+i) for i in range(10)} print(aRepr.repr(d)) # Output should be a dictionary representation with at most 3 key-value pairs # Lists with more than 4 elements l = list(range(10)) print(aRepr.repr(l)) # Output should be a list representation with at most 4 elements # Recursive list m = MyList(\'abc\') m.append(m) m.append(\'x\') print(aRepr.repr(m)) # Output should handle recursion gracefully ``` Your task is to complete the implementation of the `CustomRepr` class following the provided guidelines and test cases.","solution":"import reprlib # Step 1: Subclass reprlib.Repr class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 3 # Limiting dictionary representation to 3 key-value pairs self.maxlist = 4 # Limiting list representation to 4 elements # Step 2: Custom dictionary representation def repr_dict(self, obj, level): n = len(obj) if n > self.maxdict: items = list(obj.items())[:self.maxdict] return \'{\' + \', \'.join(f\'{self.repr(k)}: {self.repr(v)}\' for k, v in items) + \', ...}\' else: return \'{\' + \', \'.join(f\'{self.repr(k)}: {self.repr(v)}\' for k, v in obj.items()) + \'}\' # Step 3: Custom list representation def repr_list(self, obj, level): n = len(obj) if n > self.maxlist: return \'[\' + \', \'.join(self.repr(x) for x in obj[:self.maxlist]) + \', ...]\' else: return \'[\' + \', \'.join(self.repr(x) for x in obj) + \']\' # Step 4: Custom recursive list class from reprlib import recursive_repr class MyList(list): @recursive_repr() def __repr__(self): return \'<\' + \'|\'.join(map(repr, self)) + \'>\' # Testing the CustomRepr class aRepr = CustomRepr()"},{"question":"You are required to implement a function utilizing the block mask operations provided by the `torch.nn.attention.flex_attention` module. Specifically, your task is to create a complex mask using nested block masks and perform logical operations on these masks. Function Signature ```python def complex_mask_operations(size: int) -> torch.Tensor: Creates a complex mask using nested block masks and performs logical AND and OR operations on these masks. Args: - size (int): The size of the block mask (nxn) to be created. Returns: - torch.Tensor: A resulting mask after performing nested mask creation and logical operations. pass ``` Function Description 1. Create two nested block masks of given `size`. 2. Perform a logical AND operation on these masks. 3. Perform a logical OR operation on the resulting mask with another newly created block mask. 4. Return the final mask as a `torch.Tensor`. Constraints - The size (`n`) will be a positive integer. - Ensure the operations handle potential edge cases (e.g., very small or very large masks). Example ```python size = 4 result_mask = complex_mask_operations(size) print(result_mask) ``` This should output a `4x4` torch Tensor representing the final mask after performing the described operations. Hints - Utilize `create_nested_block_mask` to form nested block masks. - Use `and_masks` and `or_masks` to perform logical operations on the masks. - Ensure torch library is imported correctly, and module dependencies are handled.","solution":"import torch def create_nested_block_mask(size: int, value: bool = True) -> torch.Tensor: Creates a nested block mask of given size. For simplicity, let\'s assume it creates a block mask with a specified value (True/False). return torch.full((size, size), value, dtype=torch.bool) def and_masks(mask1: torch.Tensor, mask2: torch.Tensor) -> torch.Tensor: Performs logical AND operation between two masks. return mask1 & mask2 def or_masks(mask1: torch.Tensor, mask2: torch.Tensor) -> torch.Tensor: Performs logical OR operation between two masks. return mask1 | mask2 def complex_mask_operations(size: int) -> torch.Tensor: Creates a complex mask using nested block masks and performs logical AND and OR operations on these masks. Args: - size (int): The size of the block mask (nxn) to be created. Returns: - torch.Tensor: A resulting mask after performing nested mask creation and logical operations. # Creating two initial nested block masks mask1 = create_nested_block_mask(size) mask2 = create_nested_block_mask(size, value=False) # Performing logical AND on the two masks and_mask = and_masks(mask1, mask2) # Creating another nested block mask mask3 = create_nested_block_mask(size) # Performing logical OR operation with the result of AND operation final_mask = or_masks(and_mask, mask3) return final_mask"},{"question":"# Python 3.10 Coding Assessment Question: Advanced Data Class Management In this task, you are required to create a data class to represent a Book entity and manage its instances. 1. Define a `Book` data class with the following attributes: - title: `str` - author: `str` - year_published: `int` - isbn: `str` - price: `float` 2. Implement post-init processing to ensure that: - The `title` and `author` fields are trimmed of any leading or trailing whitespace. - The `year_published` is not in the future (raise a `ValueError` if it is). - The `price` is positive (raise a `ValueError` if it is not). 3. Create an immutable (frozen) variant of the `Book` data class called `ImmutableBook` with the same attributes but ensure that any modification attempt will result in an error. 4. Utilize and demonstrate descriptor-typed fields by creating a `Discount` descriptor. The `Discount` descriptor should: - Only accept float values between 0 and 1 (representing 0% to 100%). - Modify the `price` of the `Book` instances it is associated with by reducing the price according to the given discount. - Raise an appropriate error if an invalid value is assigned. Implementation Requirements: - Provide a test suite demonstrating how to create instances of `Book` and `ImmutableBook`, and how they handle typical edge cases (e.g., future year, zero/negative price). - Verify that attempts to modify `ImmutableBook` instances raise errors. - Show the implementation and effect of the `Discount` descriptor on book prices. Performance Requirements: - The solution should be efficient and handle typical scenarios of bookstore operations without significant performance degradation. # Example Usage ```python # Following code is provided as an example and test case try: book = Book(title=\\" My Life \\", author=\\" John Doe \\", year_published=2024, isbn=\\"123-456-789\\", price=-50.0) except ValueError as e: print(e) # Expected: \\"Year published cannot be in the future\\" or \\"Price must be positive\\" immutable_book = ImmutableBook(title=\\"Immutable Sample\\", author=\\"Jane Doe\\", year_published=2019, isbn=\\"987-654-321\\", price=30.0) try: immutable_book.price = 25.0 except FrozenInstanceError as e: print(e) # Expected: \'FrozenInstanceError\' because ImmutableBook cannot be modified book_with_discount = Book(title=\\"Discounted Book\\", author=\\"Author One\\", year_published=2020, isbn=\\"321-654-987\\", price=20.0) book_with_discount.discount = 0.1 print(book_with_discount.price) # Expected: 18.0 ``` Good luck!","solution":"from dataclasses import dataclass, field, FrozenInstanceError import datetime @dataclass class Book: title: str author: str year_published: int isbn: str price: float discount: float = field(init=False, default=0.0) def __post_init__(self): # Trim whitespaces self.title = self.title.strip() self.author = self.author.strip() # Validate year_published current_year = datetime.datetime.now().year if self.year_published > current_year: raise ValueError(\\"Year published cannot be in the future\\") # Validate price if self.price <= 0: raise ValueError(\\"Price must be positive\\") def apply_discount(self, discount: float): if not (0 <= discount <= 1): raise ValueError(\\"Discount must be between 0 and 1\\") self.discount = discount self.price = self.price * (1 - self.discount) @dataclass(frozen=True) class ImmutableBook: title: str author: str year_published: int isbn: str price: float def __post_init__(self): object.__setattr__(self, \'title\', self.title.strip()) object.__setattr__(self, \'author\', self.author.strip()) current_year = datetime.datetime.now().year if self.year_published > current_year: raise ValueError(\\"Year published cannot be in the future\\") if self.price <= 0: raise ValueError(\\"Price must be positive\\")"},{"question":"**Coding Assessment Question: Custom Descriptor Implementation** Descriptors are a powerful feature in Python that allow developers to customize the behavior of attribute access. In this exercise, you are required to create a custom descriptor class that logs the number of times an attribute is accessed, and a class that uses this descriptor. # Problem Statement Implement a custom descriptor called `AccessLogger` which logs and returns the number of times an attribute has been accessed. Then, use this descriptor in a class called `MyClass` with an attribute `my_attr`. # Requirements 1. The `AccessLogger` descriptor must: - Log the number of times the attribute is accessed. - Provide the logged count via the descriptor. 2. `MyClass` must: - Use the `AccessLogger` descriptor for its attribute `my_attr`. - Have an `__init__` method to initialize `my_attr`. # Implementation Details - The `AccessLogger` descriptor class should implement `__get__`, `__set__`, and `__delete__` methods. - The `__get__` method should return the current value of the attribute and increment the access count. - The `__set__` method should set the value of the attribute. - The `__delete__` method should delete the attribute. - The access count should be stored in an instance-specific dictionary. # Input and Output - **Input**: An instance of `MyClass` and operations on `my_attr`. - **Output**: The access count is returned as expected when the attribute is accessed. # Example ```python class AccessLogger: # Implement the descriptor class here class MyClass: my_attr = AccessLogger() def __init__(self, value=None): self.my_attr = value # Test the descriptor obj = MyClass(\'test\') print(obj.my_attr) # \'test\', access count = 1 print(obj.my_attr) # \'test\', access count = 2 # Set a new value and test access again obj.my_attr = \'new value\' print(obj.my_attr) # \'new value\', access count = 3 ``` # Constraints - The access count must be accurate and specific to each instance of `MyClass`. - Do not use any external libraries; rely on standard Python libraries only. # Performance Requirements - The descriptor implementation should efficiently handle multiple instances and frequent attribute accesses.","solution":"class AccessLogger: def __init__(self): self._access_counts = {} def __get__(self, instance, owner): if instance is None: return self if instance not in self._access_counts: self._access_counts[instance] = {\'count\': 0, \'value\': None} self._access_counts[instance][\'count\'] += 1 return self._access_counts[instance][\'value\'] def __set__(self, instance, value): if instance not in self._access_counts: self._access_counts[instance] = {\'count\': 0, \'value\': value} self._access_counts[instance][\'value\'] = value def __delete__(self, instance): if instance in self._access_counts: del self._access_counts[instance] def access_count(self, instance): if instance in self._access_counts: return self._access_counts[instance][\'count\'] return 0 class MyClass: my_attr = AccessLogger() def __init__(self, value=None): self.my_attr = value"},{"question":"You are given a dataset containing various features and a target variable. Your task is to implement a function that calculates the permutation feature importance for a given model on this dataset and returns the feature importances in a sorted manner. You need to ensure that your solution considers cases with multiple scoring metrics. Function Signature ```python def calculate_permutation_importance(model, X, y, n_repeats=30, random_state=0, scoring=None): Calculate permutation feature importance for a given model and dataset. Parameters: - model: A fitted scikit-learn estimator. - X: Features data (pandas DataFrame or NumPy array). - y: Target data (pandas Series or NumPy array). - n_repeats: Number of times to shuffle each feature (default is 30). - random_state: Random seed for reproducibility (default is 0). - scoring: Scoring metric(s). Accepts a single string metric or a list of string metrics (default is None). Returns: - A dictionary where keys are the scoring metrics, and values are lists of tuples. Each tuple contains the feature name and its mean importance score, sorted in descending order of importance. pass ``` Input - `model`: A fitted scikit-learn estimator (e.g., Ridge, RandomForest, etc.). - `X`: A pandas DataFrame or NumPy array containing the feature data. - `y`: A pandas Series or NumPy array containing the target variable. - `n_repeats`: An integer specifying how many times to shuffle each feature (default is 30). - `random_state`: An integer specifying the random seed for reproducibility (default is 0). - `scoring`: A single scoring metric as a string or a list of scoring metrics as strings (e.g., \'r2\', \'neg_mean_squared_error\', etc.). Output - A dictionary with scoring metrics as keys and lists of tuples (feature name, mean importance) as values. Each list should be sorted in descending order of feature importance based on the mean importance score. Constraints - Use scikit-learn\'s `permutation_importance` function to calculate the feature importances. - Ensure that the function handles multiple scoring metrics properly. Example ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge # Load dataset diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(diabetes.data, diabetes.target, random_state=0) # Train model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Calculate permutation importance result = calculate_permutation_importance(model, X_val, y_val, n_repeats=30, random_state=0, scoring=[\'r2\', \'neg_mean_squared_error\']) # Example output format # { # \'r2\': [(\'s5\', 0.204), (\'bmi\', 0.176), (\'bp\', 0.088), (\'sex\', 0.056)], # \'neg_mean_squared_error\': [(\'s5\', 1013.866), (\'bmi\', 872.726), (\'bp\', 438.663), (\'sex\', 277.376)] # } ``` Note - The provided function signature and example should guide the implementation. - Consider edge cases, such as handling different types of input data formats (pandas DataFrame vs. NumPy array).","solution":"from sklearn.inspection import permutation_importance def calculate_permutation_importance(model, X, y, n_repeats=30, random_state=0, scoring=None): Calculate permutation feature importance for a given model and dataset. Parameters: - model: A fitted scikit-learn estimator. - X: Features data (pandas DataFrame or NumPy array). - y: Target data (pandas Series or NumPy array). - n_repeats: Number of times to shuffle each feature (default is 30). - random_state: Random seed for reproducibility (default is 0). - scoring: Scoring metric(s). Accepts a single string metric or a list of string metrics (default is None). Returns: - A dictionary where keys are the scoring metrics, and values are lists of tuples. Each tuple contains the feature name and its mean importance score, sorted in descending order of importance. if isinstance(X, list): raise ValueError(\\"X should be a pandas DataFrame or NumPy array.\\") if isinstance(y, list): raise ValueError(\\"y should be a pandas Series or NumPy array.\\") # Ensure mixed types in scoring are handled if scoring is None: scoring = [\'r2\'] elif isinstance(scoring, str): scoring = [scoring] result = {} for metric in scoring: importance_result = permutation_importance( model, X, y, n_repeats=n_repeats, random_state=random_state, scoring=metric ) # Arrange importances sorted_importances = sorted( zip(X.columns if hasattr(X, \'columns\') else range(X.shape[1]), importance_result.importances_mean), key=lambda x: x[1], reverse=True ) result[metric] = sorted_importances return result"},{"question":"**Question: Advanced Data Classes in Python** In this task, you are required to create and manipulate data structures using the `dataclasses` module in Python. You will demonstrate your understanding of creating data classes, post-initialization processing, and inheritance. # Problem Statement 1. Define a data class named `Book` with the following attributes: - `title` (a string) - `author` (a string) - `year` (an integer) - `genre` (a string with a default value of \\"Unknown\\") 2. Define another data class named `Library` that contains a list of `Book` objects. This class should have the following attributes: - `name` (a string) - `books` (a list of `Book` objects, with a default empty list) - `established_year` (an integer) Implement the following methods: - `add_book`: Adds a new `Book` to the library. - `total_books`: Returns the total number of books in the library. - `get_books_by_author`: Takes an author\'s name as input and returns a list of books by that author. - `get_books_by_year_range`: Takes two integers representing a range of years and returns a list of books published within that year range. 3. Demonstrate the use of post-initialization processing by ensuring that the `year` attribute of the `Book` class is a positive number. If the year is not positive, raise a `ValueError` with the message \\"Year must be a positive integer\\". 4. Create a subclass of `Book` named `EBook` with an additional attribute: - `file_size` (a float representing the size of the eBook file in megabytes, with a default value of 0.0) Ensure that the `EBook` class inherits all the functionality of the `Book` class. # Input and Output Formats * There are no input constraints beyond what is specified in the prompt. * Your classes and methods should be used as follows: ```python # Creating Book objects book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", year=1949, genre=\\"Dystopian\\") book2 = Book(title=\\"Brave New World\\", author=\\"Aldous Huxley\\", year=1932) # Creating a Library object library = Library(name=\\"Central Library\\", established_year=1920) # Adding books to the library library.add_book(book1) library.add_book(book2) # Getting total number of books print(library.total_books()) # Output: 2 # Getting books by author orwell_books = library.get_books_by_author(\\"George Orwell\\") # Getting books by year range books_in_year_range = library.get_books_by_year_range(1930, 1950) # Creating an EBook object ebook = EBook(title=\\"Python Programming\\", author=\\"John Doe\\", year=2021, file_size=2.5) ``` # Constraints 1. The `year` attribute in the `Book` class must be a positive integer. 2. The `file_size` attribute in the `EBook` class must be a non-negative float. # Performance Requirements * The methods should handle typical inputs efficiently. * You may assume that the number of books in the library will not exceed 10,000. Implement the classes and methods as specified. Ensure your code is well-documented and includes type hints for all attributes and methods.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str year: int genre: str = \\"Unknown\\" def __post_init__(self): if self.year <= 0: raise ValueError(\\"Year must be a positive integer\\") @dataclass class Library: name: str established_year: int books: List[Book] = field(default_factory=list) def add_book(self, book: Book) -> None: self.books.append(book) def total_books(self) -> int: return len(self.books) def get_books_by_author(self, author: str) -> List[Book]: return [book for book in self.books if book.author == author] def get_books_by_year_range(self, start_year: int, end_year: int) -> List[Book]: return [book for book in self.books if start_year <= book.year <= end_year] @dataclass class EBook(Book): file_size: float = 0.0 def __post_init__(self): super().__post_init__() if self.file_size < 0: raise ValueError(\\"File size must be a non-negative float\\")"},{"question":"# Question Objective Write a PyTorch class that leverages advanced `torch.utils.data` functionalities to load and process a dataset of images and labels efficiently. Problem Statement 1. **Implement a custom Dataset**: Create a custom map-style dataset that reads images and their corresponding labels from a given directory. The dataset should implement the `__getitem__` and `__len__` methods. 2. **Use DataLoader with multi-process data loading**: Utilize `torch.utils.data.DataLoader` to load the data in batches with multiple workers for parallel data loading. 3. **Implement memory pinning**: Ensure that the data loaded by the `DataLoader` is pinned to enable faster transfer to CUDA-enabled GPUs. 4. **Collate function**: Implement a custom collate function that can handle varying image sizes by padding them to the maximum size of the batch. Input and Output - **Input**: - The directory path containing the image files (`.jpg` or `.png`) and a `labels.csv` file mapping each image filename to its corresponding label. - Batch size, number of workers, and device (CPU or GPU) should also be provided as inputs. - **Output**: - Batches of images and labels loaded efficiently with multi-process data loading and memory pinning. Constraints and Requirements - The custom map-style dataset should read images using `PIL` or `opencv` and their labels from a CSV file. - Use `torchvision.transforms` to preprocess the images (e.g., resizing, normalizing). - Implement data augmentation techniques (e.g., random crops, flips) during data loading. - Ensure the custom collate function handles images of varying sizes by padding them. - The DataLoader should use multiple worker processes for loading the data. - Ensure the pinned memory feature is utilized for faster data transfer to GPU if applicable. Sample Code ```python import os import pandas as pd from PIL import Image import torch from torch.utils.data import Dataset, DataLoader from torchvision import transforms import numpy as np class CustomImageDataset(Dataset): def __init__(self, image_dir, label_file, transform=None): self.image_dir = image_dir self.labels = pd.read_csv(label_file) self.transform = transform def __len__(self): return len(self.labels) def __getitem__(self, idx): img_name = os.path.join(self.image_dir, self.labels.iloc[idx, 0]) image = Image.open(img_name) label = self.labels.iloc[idx, 1] if self.transform: image = self.transform(image) return image, label def collate_fn(batch): images, labels = zip(*batch) max_height = max([img.shape[1] for img in images]) max_width = max([img.shape[2] for img in images]) padded_images = [] for img in images: padded_img = torch.zeros((img.shape[0], max_height, max_width)) padded_img[:, :img.shape[1], :img.shape[2]] = img padded_images.append(padded_img) return torch.stack(padded_images), torch.tensor(labels) def main(image_dir, label_file, batch_size, num_workers, device): transform = transforms.Compose([ transforms.Resize((256, 256)), transforms.ToTensor(), # Add more transformations if needed ]) dataset = CustomImageDataset(image_dir, label_file, transform=transform) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn) for images, labels in dataloader: images = images.to(device, non_blocking=True) labels = labels.to(device, non_blocking=True) # Proceed with model training or inference if __name__ == \\"__main__\\": image_dir = \\"path/to/image_directory\\" label_file = \\"path/to/labels.csv\\" batch_size = 32 num_workers = 4 device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") main(image_dir, label_file, batch_size, num_workers, device) ``` Instructions 1. Implement the `CustomImageDataset` class by filling in the necessary code for the `__getitem__` and `__len__` methods. 2. Implement the `collate_fn` function to pad images of varying sizes to the dimensions of the largest image in the batch. 3. Write the main training loop to load the data using `DataLoader` with multiple workers and memory pinning. 4. Ensure that the data is transferred to a CUDA-enabled GPU if available. Submission Submit your Python script (.py) file containing the implementation of the `CustomImageDataset` class, `collate_fn`, and the main function. Ensure your code is well-documented and follows best practices for readability and efficiency.","solution":"import os import pandas as pd from PIL import Image import torch from torch.utils.data import Dataset, DataLoader from torchvision import transforms import numpy as np class CustomImageDataset(Dataset): def __init__(self, image_dir, label_file, transform=None): self.image_dir = image_dir self.labels = pd.read_csv(label_file) self.transform = transform def __len__(self): return len(self.labels) def __getitem__(self, idx): img_name = os.path.join(self.image_dir, self.labels.iloc[idx, 0]) image = Image.open(img_name).convert(\'RGB\') label = self.labels.iloc[idx, 1] if self.transform: image = self.transform(image) return image, int(label) def collate_fn(batch): images, labels = zip(*batch) max_height = max([img.shape[1] for img in images]) max_width = max([img.shape[2] for img in images]) padded_images = [] for img in images: padded_img = torch.zeros((img.shape[0], max_height, max_width)) padded_img[:, :img.shape[1], :img.shape[2]] = img padded_images.append(padded_img) return torch.stack(padded_images), torch.tensor(labels) def main(image_dir, label_file, batch_size, num_workers, device): transform = transforms.Compose([ transforms.Resize((256, 256)), transforms.ToTensor(), # Add more transformations if needed ]) dataset = CustomImageDataset(image_dir, label_file, transform=transform) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, collate_fn=collate_fn) for images, labels in dataloader: images = images.to(device, non_blocking=True) labels = labels.to(device, non_blocking=True) # Proceed with model training or inference if __name__ == \\"__main__\\": image_dir = \\"path/to/image_directory\\" label_file = \\"path/to/labels.csv\\" batch_size = 32 num_workers = 4 device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") main(image_dir, label_file, batch_size, num_workers, device)"},{"question":"Objective The objective of this question is to assess your understanding of creating and using custom color palettes with the `seaborn` library. Problem Statement Write a Python function called `create_and_apply_palette` that takes three parameters: 1. `color`: A string representing a color. This can be any valid string color representation in `seaborn`. 2. `n_colors`: An integer representing the number of discrete colors to generate in the palette. 3. `as_cmap`: A boolean that determines if the palette should be returned as a matplotlib colormap or not. Your function should do the following: 1. Create a custom light palette with the specified number of colors and the specified color. 2. Plot a simple seaborn heatmap using this custom palette on a sample 3x3 dataset. Input - `color`: a string, e.g., `\\"seagreen\\"`, `\\"#79C\\"`, or `\\"xkcd:copper\\"`. - `n_colors`: an integer, e.g., `5`. - `as_cmap`: a boolean, `True` or `False`. Output - A seaborn heatmap plot generated using the specified custom palette. Constraints - `n_colors` should be an integer between 3 and 10 inclusive. - The `color` parameter must be a valid color string. Function Signature ```python def create_and_apply_palette(color: str, n_colors: int, as_cmap: bool) -> None: ``` Example ```python create_and_apply_palette(\\"seagreen\\", 5, False) ``` The above call should generate a seaborn heatmap plot using a palette with 5 colors transitioning from light gray to \\"seagreen\\". Notes - The sample dataset for the heatmap can be hardcoded within the function. For example: ```python data = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] ``` - Ensure to use appropriate seaborn and matplotlib functions to create and display the heatmap.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_and_apply_palette(color: str, n_colors: int, as_cmap: bool) -> None: Create a custom light palette with the specified number of colors and the specified color, and plot a seaborn heatmap using this custom palette on a sample 3x3 dataset. Parameters: color (str): A valid color string for creating the palette. n_colors (int): The number of discrete colors to generate in the palette (between 3 and 10 inclusive). as_cmap (bool): Whether to return the palette as a matplotlib colormap. # Ensure n_colors is within the specified range if not (3 <= n_colors <= 10): raise ValueError(\\"n_colors must be between 3 and 10 inclusive.\\") # Use seaborn to create a light palette palette = sns.light_palette(color, n_colors=n_colors, as_cmap=as_cmap) # Sample data for the heatmap data = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] # Plot the heatmap using the custom palette sns.heatmap(data, cmap=palette if as_cmap else sns.color_palette(palette)) plt.show()"},{"question":"Objective You are required to write a Python script that processes a text file by extracting specific information using regular expressions and printing the results in a formatted output. Your script should also take input arguments from the command line to specify the file to be processed and various options for data extraction. Instructions 1. Write a function `extract_data(file_path: str, pattern: str) -> List[str]` that reads a file and extracts all occurrences of a specified regular expression pattern. 2. Write a function `main()` that: - Parses command-line arguments to get the file path and the regular expression pattern. - Calls `extract_data()` with the provided arguments. - Prints the extracted data in a formatted manner. Input - The script will be run from the command line with the following arguments: - `--file <path_to_file>`: Specifies the path to the text file to be processed. - `--pattern <regex_pattern>`: Specifies the regular expression pattern to be used for extracting data. Output - The script should print each line of the extracted data preceded by its line number in the file. Constraints - You must use the `argparse` module for handling command-line arguments. - You should handle possible exceptions such as file not found and invalid regular expressions gracefully. Example Assume the following text is in a file named `sample.txt`: ``` Hello, this is a test file. The phone number you are looking for is 123-456-7890. Another valid number is 987-654-3210. Contact us at contact@example.com. ``` If the script is executed with the following arguments: ``` python script.py --file sample.txt --pattern \\"d{3}-d{3}-d{4}\\" ``` The output should be: ``` 2: 123-456-7890 3: 987-654-3210 ``` You are expected to implement the function `extract_data` to achieve this functionality. Code Template ```python import argparse import re from typing import List def extract_data(file_path: str, pattern: str) -> List[str]: extracted_data = [] try: with open(file_path, \'r\') as file: for line_num, line in enumerate(file, start=1): matches = re.findall(pattern, line) for match in matches: extracted_data.append(f\\"{line_num}: {match}\\") except FileNotFoundError: print(f\\"Error: The file {file_path} was not found.\\") except re.error: print(f\\"Error: The regular expression pattern {pattern} is invalid.\\") return extracted_data def main(): parser = argparse.ArgumentParser(description=\\"Extract data from text file using a regular expression pattern.\\") parser.add_argument(\'--file\', required=True, help=\'Path to the text file to be processed.\') parser.add_argument(\'--pattern\', required=True, help=\'Regular expression pattern for data extraction.\') args = parser.parse_args() results = extract_data(args.file, args.pattern) for result in results: print(result) if __name__ == \\"__main__\\": main() ``` Notes - Ensure that your script handles edge cases such as empty files or files with no matches for the given pattern. - Write the code and test it thoroughly to ensure it meets the specified requirements.","solution":"import argparse import re from typing import List def extract_data(file_path: str, pattern: str) -> List[str]: extracted_data = [] try: with open(file_path, \'r\') as file: for line_num, line in enumerate(file, start=1): matches = re.findall(pattern, line) for match in matches: extracted_data.append(f\\"{line_num}: {match}\\") except FileNotFoundError: print(f\\"Error: The file {file_path} was not found.\\") except re.error: print(f\\"Error: The regular expression pattern {pattern} is invalid.\\") return extracted_data def main(): parser = argparse.ArgumentParser(description=\\"Extract data from text file using a regular expression pattern.\\") parser.add_argument(\'--file\', required=True, help=\'Path to the text file to be processed.\') parser.add_argument(\'--pattern\', required=True, help=\'Regular expression pattern for data extraction.\') args = parser.parse_args() results = extract_data(args.file, args.pattern) for result in results: print(result) if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with creating a multi-faceted visualization using seaborn to demonstrate your understanding of various customization options, including moving and styling legends. 1. Load the seaborn theme and the `penguins` dataset. 2. Create a scatter plot using seaborn where: - The x-axis represents `flipper_length_mm`. - The y-axis represents `body_mass_g`. - Points are colored by `species`. 3. Add a regression line (`sns.regplot`) to the scatter plot that considers only the `Adelie` species. 4. Create a KDE plot for `body_mass_g`, separated by species and displayed on the same axes. 5. Customize the legend such that: - It is positioned at the center right of the plot. - It has a title \\"Legend\\". - It has no frame around it. Write a function `create_custom_plot()` that implements the above steps. The function should not take any input and should display the plot directly. # Expected Implementation ```python import seaborn as sns def create_custom_plot(): sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Scatter plot with regression line for only Adelie species ax = sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") sns.regplot(data=penguins[penguins[\'species\'] == \'Adelie\'], x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", scatter=False, ax=ax) # KDE plot for body_mass_g sns.kdeplot(data=penguins, x=\\"body_mass_g\\", hue=\\"species\\", ax=ax) # Customize legend sns.move_legend(ax, \\"center right\\") ax.legend(title=\\"Legend\\", frameon=False) create_custom_plot() ``` Notes: - Ensure all necessary libraries are imported. - Utilize seaborn\'s functions for plotting and customizing visualizations. - The function should handle any preprocessing required to filter the data or adjust the aesthetics of the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(): sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Create a figure and axis object fig, ax = plt.subplots() # Scatter plot with points colored by species sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", ax=ax) # Add regression line for only Adelie species sns.regplot(data=penguins[penguins[\'species\'] == \'Adelie\'], x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", scatter=False, ax=ax) # KDE plot for body_mass_g separated by species sns.kdeplot(data=penguins, x=\\"body_mass_g\\", hue=\\"species\\", ax=ax, multiple=\\"stack\\") # Customize legend ax.legend(loc=\'center left\', title=\'Legend\', frameon=False, bbox_to_anchor=(1, 0.5)) # Show plot plt.show()"},{"question":"<|Analysis Begin|> The documentation provided demonstrates several concepts based on the seaborn `objects` interface: 1. Loading the `penguins` dataset using `load_dataset`. 2. Utilizing the `seaborn.objects` (`so.Plot`) to construct and customize plots. 3. Applying various transformations and customizations such as: - Aggregation with `so.Agg()`. - Dodge with `so.Dodge()`. - Adding error bars using `so.Est(errorbar=\\"sd\\")`. - Faceting by categorical variables using `.facet()`. - Direct assignment of range values with `so.Range(ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\")`. - Customizing aesthetics like `linestyle`, `linewidth`, and `marker`. 4. Using different visual elements (dots, lines, ranges) to represent the data. To craft a question, we aim to assess the students\' understanding of these fundamental and advanced features, particularly how they manipulate seaborn\'s plot objects, customize visual properties, handle faceting, and work with error bars and custom ranges. <|Analysis End|> <|Question Begin|> # Advanced Seaborn Plot Customization In this question, you will demonstrate your understanding of the seaborn `objects` interface to create a sophisticated plot using the `penguins` dataset. Your task is to produce a plot that visualizes the relationship between penguin species and body mass, differentiated by the island they are from. Additionally, you will include error bars representing the standard deviation and a custom range using bill measurements. Requirements: 1. **Load the dataset**: Use seaborn\'s `load_dataset` function to load the `penguins` dataset. 2. **Create the plot**: - Use `so.Plot` to initialize the plot with `body_mass_g` on the x-axis and `species` on the y-axis. - Color the points by `island`. - Add a `Dot` layer with aggregation. - Add an error bar layer representing the standard deviation. 3. **Add faceting**: Visualize the different penguin sexes using `facet`. 4. **Add a custom range**: Display a range using the minimum and maximum values from `bill_depth_mm` and `bill_length_mm`. The ranges should be differentiated by `island`. Function Signature ```python def create_penguin_plot(): # Your code here ``` Example Output The function should display a plot that meets all the specified requirements. Here is a hint to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Construct the plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"island\\") .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) .facet(\\"sex\\") .add(so.Range(), ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\", color=\\"island\\") ) # Show the plot plot.show() # Call the function to see the plot create_penguin_plot() ``` Ensure your plot uses the correct aesthetics, facets by sex, and includes the appropriate transformations and ranges.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Construct the plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"island\\") .add(so.Dot(), so.Agg()) .add(so.Range(linewidth=1.5), so.Est(errorbar=\\"sd\\")) .add(so.Range(), ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\") .facet(\\"sex\\") ) # Show the plot plot.show() # Call the function to see the plot create_penguin_plot()"},{"question":"# Custom Interactive Interpreter You are tasked with designing a custom interactive interpreter using Python\'s `code` module. This interpreter should support standard Python code execution and provide some additional capabilities: 1. **Execution History**: - Maintain a history of commands executed during the session. - Provide a command to display this history. 2. **Variable State**: - Provide a command to list all currently defined variables and their values. 3. **Custom Command**: - Implement a custom command that allows the user to define a Python function from within the interpreter. The user should be able to provide the function name, parameters, and body interactively. # Requirements: 1. **CustomInterpreter Class**: - Inherit from `code.InteractiveConsole`. - Implement methods to achieve the additional functionalities described above. - Implement a custom completion method for commands. 2. **Commands**: - `:history` - Display the history of executed commands. - `:vars` - Display all currently defined variables and their values. - `:def` - Allow the user to define a Python function interactively. # Input and Output: - **Input**: Interactive, from the user. - **Output**: Interactive output, printed to the console. # Example: ```python >>> interpreter = CustomInterpreter() >>> interpreter.interact() >> >>> a = 10 >>> b = 20 >>> :vars a: 10 b: 20 >>> for i in range(5): ... print(i) ... 0 1 2 3 4 >>> :history a = 10 b = 20 for i in range(5): print(i) >>> :def Function name: add Parameters (comma-separated): x, y Function body, end with an empty line: return x + y add(x, y) >>> add(2, 3) 5 ``` # Constraints: - The `:def` command should guide the user to provide function details interactively and store the function in the interpreter\'s namespace. - The display of variable state should exclude internal variables used by the interpreter itself. - The history should include both commands and multi-line code blocks. # Performance Requirements: - Ensure efficient handling of command history and variable state to avoid performance issues in long sessions.","solution":"import code import readline class CustomInterpreter(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.history = [] def push(self, line): self.history.append(line) super().push(line) def interact(self, banner=None): print(\\"Custom Interactive Interpreter. Type \':history\' to see command history, \':vars\' to see variables, \':def\' to define a function.\\") super().interact(banner) def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if source == \':history\': self.show_history() return False elif source == \':vars\': self.show_vars() return False elif source == \':def\': self.define_function() return False else: return super().runsource(source, filename, symbol) def show_history(self): for command in self.history: print(command) def show_vars(self): for var, val in self.locals.items(): if not var.startswith(\'__\') and not callable(val): print(f\\"{var}: {val}\\") def define_function(self): name = input(\\"Function name: \\") params = input(\\"Parameters (comma-separated): \\") print(\\"Function body, end with an empty line:\\") lines = [] while True: line = input(\'>>> \') if line == \'\': break lines.append(line) body = \\"n\\".join(lines) func_def = f\'def {name}({params}):n\' + \'n\'.join(f\' {line}\' for line in body.split(\'n\')) code = compile(func_def, \\"<string>\\", \\"exec\\") exec(code, self.locals) print(f\\"Function {name} defined.\\") def main(): interpreter = CustomInterpreter() interpreter.interact() if __name__ == \\"__main__\\": main()"},{"question":"Parallelism in scikit-learn with Joblib Objective: This question aims to assess your understanding of parallelism in scikit-learn, specifically using joblib for higher-level parallelism and understanding configuration to avoid oversubscription. Problem Description: You are given a dataset and asked to perform Grid Search over a Random Forest Classifier using scikit-learn. Implement a function `perform_grid_search` which leverages joblib for parallelism efficiently, ensuring that oversubscription is avoided. Function Signature: ```python def perform_grid_search(X_train, y_train, param_grid, scoring, cv, n_jobs, max_threads): Perform Grid Search on RandomForestClassifier with joblib parallelism. Parameters: - X_train: array-like, shape (n_samples, n_features) Training input samples. - y_train: array-like, shape (n_samples,) Target values. - param_grid: dict Dictionary with parameters names (`str`) as keys and lists of parameter settings to try as values. - scoring: str Strategy to evaluate the performance of the cross-validated model on the test set. - cv: int Number of folds in cross-validation. - n_jobs: int Number of jobs to run in parallel for fitting the GridSearchCV. - max_threads: int Maximum number of threads to use for underlying thread-pool parallelism. Returns: - best_estimator_: sklearn estimator The estimator with the best performance found during grid search. pass ``` Constraints: - Use `RandomForestClassifier` for the Grid Search. - Ensure that the number of threads used for the underlying operations (e.g., fitting the classifier) does not exceed `max_threads`. - Utilize joblib\'s `parallel_backend` and manage `environment variables` if necessary. - Your function should be robust and avoid any form of oversubscription. Sample Input: ```python param_grid = { \'n_estimators\': [100, 200], \'max_features\': [\'auto\', \'sqrt\', \'log2\'], \'max_depth\': [10, 20, 30], } scoring = \'accuracy\' cv = 5 n_jobs = 4 max_threads = 2 # Assume X_train and y_train are predefined datasets ``` Sample Output: ```python RandomForestClassifier(max_depth=20, max_features=\'sqrt\', n_estimators=200) ``` Hints: 1. Refer to the scikit-learn and joblib documentation for configuring parallelism effectively. 2. Consider using context managers to set the maximum number of threads. 3. Pay attention to the interaction between joblib\'s n_jobs and the max_threads for underlying operations to avoid oversubscription. Guidelines: In your implementation, ensure you handle parallelism correctly by using joblib\'s `parallel_backend` and controlling thread pool size by setting appropriate environment variables if necessary. Test your function with different values for `n_jobs` and `max_threads` to confirm it avoids oversubscription effectively.","solution":"from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from joblib import parallel_backend import os def perform_grid_search(X_train, y_train, param_grid, scoring, cv, n_jobs, max_threads): Perform Grid Search on RandomForestClassifier with joblib parallelism. Parameters: - X_train: array-like, shape (n_samples, n_features) Training input samples. - y_train: array-like, shape (n_samples,) Target values. - param_grid: dict Dictionary with parameters names (`str`) as keys and lists of parameter settings to try as values. - scoring: str Strategy to evaluate the performance of the cross-validated model on the test set. - cv: int Number of folds in cross-validation. - n_jobs: int Number of jobs to run in parallel for fitting the GridSearchCV. - max_threads: int Maximum number of threads to use for underlying thread-pool parallelism. Returns: - best_estimator_: sklearn estimator The estimator with the best performance found during grid search. # Set the thread limit for OpenMP (used by underlying numerical libraries) os.environ[\'OMP_NUM_THREADS\'] = str(max_threads) # Perform grid search with controlled parallelism with parallel_backend(\'threading\', n_jobs=n_jobs): grid_search = GridSearchCV( estimator=RandomForestClassifier(), param_grid=param_grid, scoring=scoring, cv=cv, n_jobs=n_jobs ) grid_search.fit(X_train, y_train) return grid_search.best_estimator_"},{"question":"# Command-Line Interface Implementation with `argparse` You are required to implement a Python script using the `argparse` module to handle command-line arguments for a simulated data processing application. The script should perform different actions based on the provided arguments. # Requirements: 1. **Command-Line Arguments**: - **Positional Argument**: - `filename`: The name of the file to process. - **Optional Arguments**: - `--sum`: Calculate the sum of the numbers in the file. - `--max`: Find the maximum number in the file (default action). - `--verbose` or `-v`: Optionally prints detailed information about the process. - `--format: One of `json`, `csv`, or `text`. Specify the format of the output. The default should be `text`. 2. **File Content**: - Assume `filename` contains a list of numbers (one per line). 3. **Functionality**: - The script should read the numbers from the file and perform the specified calculation (`sum` or `max`). - The result should be printed in the specified format (`json`, `csv`, or `text`). - If the `verbose` flag is set, print additional details about the process. # Implementation: - **Input**: Command-line arguments - **Output**: The result of the processing in the chosen format, printed to standard output. # Constraints: - The script must handle invalid or missing arguments gracefully and display appropriate error and help messages. - The `argparse` module should handle the majority of the user interface requirements. # Example Usage: ```bash python script.py data.txt --sum --format json -v # Verbose output: # Reading file: data.txt # Done reading file. # Performing summation. # Result: 42 {\\"result\\": 42} ``` --- ```python import argparse import json import csv def read_numbers_from_file(filename): try: with open(filename, \'r\') as file: return [int(line.strip()) for line in file] except Exception as e: print(f\\"Error reading file: {e}\\") exit(1) def calculate_sum(numbers): return sum(numbers) def calculate_max(numbers): return max(numbers) def print_output(result, format): if format == \'json\': print(json.dumps({\\"result\\": result})) elif format == \'csv\': writer = csv.writer(sys.stdout) writer.writerow([\\"result\\", result]) else: # text print(f\\"Result: {result}\\") def main(): parser = argparse.ArgumentParser(description=\'Process some integers from a file.\') parser.add_argument(\'filename\', help=\'Name of the file to process\') parser.add_argument(\'--sum\', dest=\'operation\', action=\'store_const\', const=calculate_sum, default=calculate_max, help=\'Calculate the sum of the numbers in the file (default: find the max)\') parser.add_argument(\'--format\', choices=[\'json\', \'csv\', \'text\'], default=\'text\', help=\'Specify the format of the output (default: text)\') parser.add_argument(\'--verbose\', \'-v\', action=\'store_true\', help=\'Print detailed information about the process\') args = parser.parse_args() if args.verbose: print(f\\"Reading file: {args.filename}\\") numbers = read_numbers_from_file(args.filename) if args.verbose: print(\\"Done reading file.\\") print(f\\"Performing {\'summation\' if args.operation == calculate_sum else \'max calculation\'}.\\") result = args.operation(numbers) print_output(result, args.format) if __name__ == \\"__main__\\": main() ``` Please implement the script as described and ensure it handles various edge cases such as invalid file names and content, incorrect argument usage, etc.","solution":"import argparse import json import csv import sys def read_numbers_from_file(filename): Reads numbers from a file and returns them as a list of integers. try: with open(filename, \'r\') as file: return [int(line.strip()) for line in file] except Exception as e: print(f\\"Error reading file: {e}\\") sys.exit(1) def calculate_sum(numbers): Returns the sum of the given list of numbers. return sum(numbers) def calculate_max(numbers): Returns the maximum number from the given list of numbers. return max(numbers) def print_output(result, format): Prints the result in the specified format. if format == \'json\': print(json.dumps({\\"result\\": result})) elif format == \'csv\': writer = csv.writer(sys.stdout) writer.writerow([\\"result\\", result]) else: # text print(f\\"Result: {result}\\") def main(): parser = argparse.ArgumentParser(description=\'Process some integers from a file.\') parser.add_argument(\'filename\', help=\'Name of the file to process\') parser.add_argument(\'--sum\', dest=\'operation\', action=\'store_const\', const=calculate_sum, default=calculate_max, help=\'Calculate the sum of the numbers in the file (default: find the max)\') parser.add_argument(\'--format\', choices=[\'json\', \'csv\', \'text\'], default=\'text\', help=\'Specify the format of the output (default: text)\') parser.add_argument(\'--verbose\', \'-v\', action=\'store_true\', help=\'Print detailed information about the process\') args = parser.parse_args() if args.verbose: print(f\\"Reading file: {args.filename}\\") numbers = read_numbers_from_file(args.filename) if args.verbose: print(\\"Done reading file.\\") print(f\\"Performing {\'summation\' if args.operation == calculate_sum else \'max calculation\'}.\\") result = args.operation(numbers) print_output(result, args.format) if __name__ == \\"__main__\\": main()"},{"question":"Write a Python program using the `threading` module to simulate a simple thread-safe banking system. The system should support multiple accounts, and allow the simulation of large number of deposits, withdrawals, and balance checks in a thread-safe manner. Your task is to implement the following: 1. **BankAccount Class**: * Initialize with an account number and an initial balance. * Contains methods to deposit, withdraw, and check the balance. * Ensure that all operations are thread-safe. 2. **Customer Thread Class**: * Each customer performs a series of random operations (deposits, withdrawals, balance checks). * Make sure the operations are spaced out with some delay to simulate real-world usage. 3. **Bank Simulation Function**: * Create several bank accounts. * Create multiple customer threads that interact with these accounts. * Ensure proper synchronization so that the balance never goes negative and all operations reflect correct balances. # Constraints: * You must use `threading.Lock` to ensure thread safety. * Each customer should perform at least 100 operations. * There should be at least 5 customer threads and at least 3 bank accounts. # Implementation Details: * The `BankAccount` class should have the following methods: * `deposit(amount)`: Adds to the account balance. * `withdraw(amount)`: Deducts from the account balance if sufficient funds are available. Otherwise, it should fail the transaction. * `get_balance()`: Returns the current balance. * Use `threading.Thread` to create customer threads. * Make sure to properly manage all threads (starting, joining, etc.). # Example Usage: ```python import threading import time import random # Code your BankAccount and Customer Threads here. def main(): # Initialize bank accounts accounts = [BankAccount(account_number=i, initial_balance=1000) for i in range(3)] # Initialize customer threads customers = [Customer(i, accounts) for i in range(5)] # Start all customer threads for customer in customers: customer.start() # Wait for all customer threads to finish for customer in customers: customer.join() # Print final balances for account in accounts: print(f\\"Account {account.account_number} final balance: {account.get_balance()}\\") if __name__ == \\"__main__\\": main() ``` Ensure your solution works properly and demonstrate the usage in your `main()` function, as shown above.","solution":"import threading import time import random class BankAccount: def __init__(self, account_number, initial_balance): self.account_number = account_number self.balance = initial_balance self.lock = threading.Lock() def deposit(self, amount): with self.lock: self.balance += amount print(f\\"Deposited {amount} to account {self.account_number}. New balance: {self.balance}\\") def withdraw(self, amount): with self.lock: if self.balance >= amount: self.balance -= amount print(f\\"Withdrew {amount} from account {self.account_number}. New balance: {self.balance}\\") return True else: print(f\\"Failed to withdraw {amount} from account {self.account_number}. Insufficient balance.\\") return False def get_balance(self): with self.lock: return self.balance class Customer(threading.Thread): def __init__(self, customer_id, accounts): super().__init__() self.customer_id = customer_id self.accounts = accounts def run(self): for _ in range(100): account = random.choice(self.accounts) operation = random.choice([\'deposit\', \'withdraw\', \'balance\']) if operation == \'deposit\': amount = random.randint(1, 100) account.deposit(amount) elif operation == \'withdraw\': amount = random.randint(1, 100) account.withdraw(amount) elif operation == \'balance\': print(f\\"Customer {self.customer_id} checked balance of account {account.account_number}: {account.get_balance()}\\") time.sleep(random.uniform(0.01, 0.1)) def bank_simulation(): accounts = [BankAccount(account_number=i, initial_balance=1000) for i in range(3)] customers = [Customer(i, accounts) for i in range(5)] for customer in customers: customer.start() for customer in customers: customer.join() for account in accounts: print(f\\"Account {account.account_number} final balance: {account.get_balance()}\\") if __name__ == \\"__main__\\": bank_simulation()"},{"question":"# Seaborn Aesthetic Customization Task You are provided with a dataset and your task is to create and customize several seaborn visualizations to showcase your understanding of seaborn\'s aesthetic controls. Dataset The dataset consists of 60 observations for 6 features. You can generate this dataset using the following code: ```python import numpy as np np.random.seed(42) data = np.random.normal(size=(60, 6)) + np.arange(6) / 2 ``` Task Requirements 1. **Set Default Seaborn Theme and Style**: a. Set the default seaborn theme and style to `whitegrid`. b. Create a boxplot for the dataset, ensuring it uses the `whitegrid` style. 2. **Change Styles and Contexts**: a. Change the style to `dark` and plot the sine function (use `sinplot` function). b. Change the context to `poster` and plot the sine function. c. Temporarily change the style to `ticks` using a context manager, and plot the sine function again within this context. 3. **Remove Axes Spines**: a. Create a violin plot for the dataset. b. Remove the top and right spines of the plot using the `despine` function. 4. **Customize Styles**: a. Set the style to `darkgrid` and override only the `axes.facecolor` parameter by setting the background color to light grey (use hexadecimal color code #D3D3D3). b. Plot the sine function using this customized style. 5. **Overarching Plot**: a. Create a composite figure with 4 subplots arranged in a 2x2 grid. b. For each subplot, use a different seaborn style (`darkgrid`, `white`, `ticks`, `whitegrid`). c. Add the sine plot in each subplot. d. Use `tight_layout` to ensure that the subplots do not overlap. Input and Output Formats - **Input**: No specific input is required as the dataset is generated within the script. - **Output**: The output should be matplotlib figures displaying the required customizations according to the above instructions. Constraints - Ensure that all seaborn aesthetic settings precisely match the requirements. - Sine function to be plotted using the following helper function: ```python def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) ``` Performance Requirement - The visualizations should render without errors and be visually distinct according to set themes, styles, and contexts. - Use the seaborn and matplotlib libraries for all visualizations. Example Code Structure Below is a skeleton of your script to help you get started: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Generating the dataset data = np.random.normal(size=(60, 6)) + np.arange(6) / 2 def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) # Task 1: Set Default Seaborn Theme and Style # Task 2: Change Styles and Contexts # Task 3: Remove Axes Spines # Task 4: Customize Styles # Task 5: Overarching Plot # Ensure you create the required visuals as per the instructions plt.show() ``` Provide your solution by filling in the appropriate task sections with the seaborn customization code.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Generating the dataset data = np.random.normal(size=(60, 6)) + np.arange(6) / 2 def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) # Task 1: Set Default Seaborn Theme and Style sns.set_theme(style=\\"whitegrid\\") plt.figure() sns.boxplot(data=data) plt.show() # Task 2: Change Styles and Contexts # 2a: Change the style to `dark` and plot the sine function sns.set_style(\\"dark\\") plt.figure() sinplot() plt.show() # 2b: Change the context to `poster` and plot the sine function sns.set_context(\\"poster\\") plt.figure() sinplot() plt.show() # 2c: Temporarily change the style to `ticks` using a context manager, and plot the sine function again within this context with sns.axes_style(\\"ticks\\"): plt.figure() sinplot() plt.show() # Reset to default context sns.set_context(\\"notebook\\") # Task 3: Remove Axes Spines plt.figure() sns.violinplot(data=data) sns.despine() plt.show() # Task 4: Customize Styles # Set the style to `darkgrid` and override only the `axes.facecolor` parameter sns.set_style(\\"darkgrid\\", {\\"axes.facecolor\\": \\"#D3D3D3\\"}) plt.figure() sinplot() plt.show() # Task 5: Overarching Plot fig, axes = plt.subplots(2, 2, figsize=(10, 8)) styles = [\\"darkgrid\\", \\"white\\", \\"ticks\\", \\"whitegrid\\"] for ax, style in zip(axes.flat, styles): sns.set_style(style) plt.sca(ax) sinplot() plt.tight_layout() plt.show()"},{"question":"<|Analysis Begin|> The provided documentation for the `urllib.parse` module covers a wide range of functions available to parse, construct, and deconstruct URLs. These functions include: 1. **urlparse()**: Parses a URL into six components. 2. **parse_qs()**: Parses query strings into a dictionary. 3. **parse_qsl()**: Parses query strings into a list of tuples. 4. **urlunparse()**: Combines components back into a URL string. 5. **urlsplit()**: Similar to `urlparse` but does not split the parameters portion. 6. **urlunsplit()**: Combines components split by `urlsplit` into a URL string. 7. **urljoin()**: Constructs absolute URLs by combining a base URL with a relative URL. 8. **urldefrag()**: Separates the fragment from a URL. 9. **unwrap()**: Extracts URL from a wrapped URL string. 10. **quote()**: Quotes special characters in a string to make it safe for use in URLs. 11. **quote_plus()**: Similar to `quote` but also replaces spaces with plus signs. 12. **quote_from_bytes()**: Quotes special characters in a byte string. 13. **unquote()**: Replaces \\"%xx\\" escapes in URL strings. 14. **unquote_plus()**: Similar to `unquote` but replaces plus signs with spaces. 15. **unquote_to_bytes()**: Replaces \\"%xx\\" escapes in a byte string. 16. **urlencode()**: Converts a mapping object or sequence of tuples to a percent-encoded string. From these functions, a challenging and comprehensive coding question can be designed that assesses students\' ability to manipulate URLs, process fragments, and manage query strings using the `urllib.parse` module. <|Analysis End|> <|Question Begin|> # URL Manipulation and Query String Processing with `urllib.parse` You are tasked with implementing a function that performs multiple operations on URLs and query strings simultaneously. The function will take a base URL and a relative URL, join them, strip any fragments, process the query string, and return the modified URL. # Function Specification **Function Name**: ```python def process_url(base_url: str, relative_url: str, query_params: dict) -> str: ``` **Inputs**: - **base_url**: A string representing the base URL (e.g., `\\"http://example.com/path\\"`). - **relative_url**: A string representing the relative URL (e.g., `\\"../newpath?param=value#section\\"`). - **query_params**: A dictionary where keys and values are strings representing additional query parameters to be added to the final URL (e.g., `{\\"newparam\\": \\"newvalue\\", \\"anotherparam\\": \\"anothervalue\\"}`). **Output**: - **A string representing the final URL after performing the following operations**: 1. Join the `base_url` and `relative_url`. 2. Remove any fragment from the resultant URL. 3. Parse the existing query string from the resultant URL. 4. Merge the parsed query string with `query_params`, overriding any existing parameters with the same keys. 5. Construct a new URL with the merged query parameters and return it. # Constraints: - All inputs will be valid URL strings or dictionaries with string keys and values. - The resultant URL should have the fragment removed if present. - Query parameters should be encoded properly to handle special characters. - The function should handle edge cases such as empty query strings, overlapping query parameters, and absolute URLs. # Example ```python from urllib.parse import urljoin, urldefrag, urlparse, parse_qs, urlencode, urlunparse def process_url(base_url: str, relative_url: str, query_params: dict) -> str: # Join the base and relative URLs combined_url = urljoin(base_url, relative_url) # Remove the fragment from the URL defragged_url, _ = urldefrag(combined_url) # Parse the URL to obtain its query string parsed_url = urlparse(defragged_url) query_dict = parse_qs(parsed_url.query) # Merge with the provided query parameters, update existing values query_dict.update(query_params) # Flatten the query dictionary back into a query string merged_query = urlencode(query_dict, doseq=True) # Construct the final URL without fragment final_url = urlunparse(parsed_url._replace(query=merged_query, fragment=\\"\\")) return final_url # Example usage print(process_url(\\"http://example.com/path\\", \\"../newpath?param=value#section\\", {\\"newparam\\": \\"newvalue\\", \\"param\\": \\"newer_value\\"})) ``` Expected Output: ``` \\"http://example.com/newpath?param=newer_value&newparam=newvalue\\" ``` # Notes: - In the example provided, the function joins the base and relative URLs to form `\\"http://example.com/newpath?param=value#section\\"`. - It then removes the fragment, resulting in `\\"http://example.com/newpath?param=value\\"`. - The query parameters `{\\"newparam\\": \\"newvalue\\", \\"param\\": \\"newer_value\\"}` override any existing query parameters in the URL. - The final URL constructed is `\\"http://example.com/newpath?param=newer_value&newparam=newvalue\\"`. Implement the `process_url` function to solve the problem effectively.","solution":"from urllib.parse import urljoin, urldefrag, urlparse, parse_qs, urlencode, urlunparse def process_url(base_url: str, relative_url: str, query_params: dict) -> str: Processes the input URLs and query parameters to return a modified URL. Parameters: - base_url (str): The base URL. - relative_url (str): The relative URL. - query_params (dict): The query parameters to add. Returns: - str: The final URL after joining, defragging, and merging query parameters. # Join the base and relative URLs combined_url = urljoin(base_url, relative_url) # Remove the fragment from the URL defragged_url, _ = urldefrag(combined_url) # Parse the URL to obtain its query string parsed_url = urlparse(defragged_url) query_dict = parse_qs(parsed_url.query) # Merge with the provided query parameters, update existing values query_dict.update(query_params) # Flatten the query dictionary back into a query string merged_query = urlencode(query_dict, doseq=True) # Construct the final URL without fragment final_url = urlunparse(parsed_url._replace(query=merged_query, fragment=\\"\\")) return final_url"},{"question":"# Advanced Coding Assessment Question: Mocking and Unit Testing with `unittest.mock` Objective: You are to demonstrate your understanding of the `unittest.mock` library by writing functions that simulate a simple system and then writing unit tests for it using mocks. Problem: You need to implement a `Database` class that interacts with an external database and a `ReportGenerator` class that generates reports based on data fetched from the `Database`. Your task involves the following steps: 1. Implement the `Database` and `ReportGenerator` classes. 2. Write unit tests for `ReportGenerator` using the `unittest.mock` library to mock the `Database` interactions. Specifications: - **Database class**: - `fetch_data(query: str) -> list[dict]`: Given a SQL query string, it returns a list of dictionaries representing rows from the database. - **ReportGenerator class**: - `__init__(self, db: Database)`: Initializes with a `Database` instance. - `generate_report(query: str) -> dict`: Uses the `Database` instance to fetch data and returns a summary report as a dictionary. The report should contain: - `total_rows`: The total number of rows fetched. - `data_summary`: A summary of the data fetched (you can decide on a simple summary format). Your Tasks: 1. Implement the `Database` class and the `ReportGenerator` class as per the specifications given above. 2. Write unit tests for the `ReportGenerator` class using the `unittest` framework and the `unittest.mock` library. - Mock the `Database` instance to test the behavior of `ReportGenerator` without needing an actual database. - Ensure that all methods of `ReportGenerator` that interact with `Database` are properly tested with different return values and side effects from the mocked `Database`. Example: ```python # Step 1: Implement the Database class class Database: def fetch_data(self, query: str) -> list[dict]: # Simulate fetching data from a database (details not necessary) pass # Step 2: Implement the ReportGenerator class class ReportGenerator: def __init__(self, db: Database): self.db = db def generate_report(self, query: str) -> dict: data = self.db.fetch_data(query) total_rows = len(data) data_summary = {...} # your logic to create a data summary return {\\"total_rows\\": total_rows, \\"data_summary\\": data_summary} # Step 3: Write unit tests using unittest.mock import unittest from unittest.mock import Mock, patch class TestReportGenerator(unittest.TestCase): def setUp(self): self.mock_db = Mock(spec=Database) self.report_gen = ReportGenerator(self.mock_db) @patch(\'your_module.Database\') def test_generate_report(self, MockDatabase): # Configure the mock mock_db_instance = MockDatabase.return_value mock_db_instance.fetch_data.return_value = [{\\"id\\": 1, \\"value\\": 10}, {\\"id\\": 2, \\"value\\": 30}] # Test the method report = self.report_gen.generate_report(\\"SELECT * FROM table\\") # Assertions self.assertEqual(report[\'total_rows\'], 2) self.assertIn(\\"data_summary\\", report) self.mock_db.fetch_data.assert_called_once_with(\\"SELECT * FROM table\\") if __name__ == \'__main__\': unittest.main() ``` Constraints: - **Performance**: Your solution should be efficient enough to handle typical data sizes encountered in small to medium-sized databases. - **Clarity**: Ensure that your code is well-documented and follows good coding practices. - **Mocking**: All interactions with the database should be mocked, and your tests should not depend on an actual database. Submission: Provide the implementation of the `Database` and `ReportGenerator` classes along with the complete test suite for the `ReportGenerator` class using `unittest` and `unittest.mock`.","solution":"class Database: def fetch_data(self, query: str) -> list: # Simulate fetching data from a database (details not necessary) pass class ReportGenerator: def __init__(self, db: Database): self.db = db def generate_report(self, query: str) -> dict: data = self.db.fetch_data(query) total_rows = len(data) data_summary = self.summarize_data(data) return {\\"total_rows\\": total_rows, \\"data_summary\\": data_summary} def summarize_data(self, data: list) -> dict: # Simple summary: count occurrences of values in each key if not data: return {} summary = {} for row in data: for key, value in row.items(): if key not in summary: summary[key] = {} if value not in summary[key]: summary[key][value] = 0 summary[key][value] += 1 return summary"},{"question":"Objective Your task is to implement a function to monitor and optimize garbage collection in a Python application using the \\"gc\\" module. The solution should demonstrate your understanding of both fundamental and advanced concepts related to garbage collection in Python. Problem Statement You need to write a Python function called `optimize_gc` which performs the following operations: 1. **Enable automatic garbage collection** if it is currently disabled. 2. **Set the garbage collection thresholds** for the three generations to custom values provided as input. 3. **Register a callback** that logs garbage collection statistics before and after each collection. The log should include: - Generation being collected. - Number of objects collected. - Number of uncollectable objects. 4. **Manually trigger garbage collection**. 5. **Return the list of objects tracked by the garbage collector** after the manual collection. Function Signature ```python def optimize_gc(threshold0: int, threshold1: int, threshold2: int) -> list: ``` Input Parameters - `threshold0` (int): The threshold for generation 0. - `threshold1` (int): The threshold for generation 1. - `threshold2` (int): The threshold for generation 2. Output - A list of objects currently tracked by the garbage collector after the manual collection. Constraints and Requirements - The function must ensure the garbage collector is enabled and set the collection thresholds as specified. - The function must register a callback to log the required statistics before and after each collection. - The manual garbage collection should target the highest generation (generation 2). - The log for statistics from the callback should be printed to the console using `print()` statements. - Make sure to handle any potential exceptions that could be raised during the operations. Additional Information - Use the `gc` module\'s functions and attributes as described in the documentation provided. - The callback function should be defined within `optimize_gc` and ensure accurate logging. Example: ```python def optimize_gc(threshold0: int, threshold1: int, threshold2: int) -> list: import gc # Enable garbage collection if disabled. if not gc.isenabled(): gc.enable() # Set collection thresholds gc.set_threshold(threshold0, threshold1, threshold2) # Define the callback function to log statistics def gc_callback(phase, info): if phase == \'start\': print(f\\"GC Start: Generation {info[\'generation\']} collection started\\") elif phase == \'stop\': print(f\\"GC Stop: {info[\'collected\']} objects collected, {info[\'uncollectable\']} uncollectable\\") # Register the callback gc.callbacks.append(gc_callback) # Perform a manual collection of the highest generation gc.collect(generation=2) # Get the list of objects tracked by the garbage collector objects_tracked = gc.get_objects() return objects_tracked ``` Notes - Ensure that your implementation correctly modifies the garbage collector\'s behavior according to the input parameters. - Output from the callback should be clear and informative as specified.","solution":"import gc def optimize_gc(threshold0: int, threshold1: int, threshold2: int) -> list: Optimize garbage collection by enabling GC, setting custom thresholds, registering a callback to log statistics, triggering manual GC, and returning the list of tracked objects. # Enable garbage collection if disabled. if not gc.isenabled(): gc.enable() # Set collection thresholds gc.set_threshold(threshold0, threshold1, threshold2) # Define the callback function to log statistics def gc_callback(phase, info): if phase == \'start\': print(f\\"GC Start: Generation {info[\'generation\']} collection started\\") elif phase == \'stop\': print(f\\"GC Stop: {info[\'collected\']} objects collected, {info[\'uncollectable\']} uncollectable\\") # Register the callback gc.callbacks.append(gc_callback) try: # Perform a manual collection of the highest generation gc.collect(generation=2) finally: # Remove the callback to avoid side effects on future garbage collections gc.callbacks.remove(gc_callback) # Get the list of objects tracked by the garbage collector objects_tracked = gc.get_objects() return objects_tracked"},{"question":"# Question: Advanced Data Visualization with `seaborn.objects` You are tasked with analyzing a dataset using the new `seaborn.objects` interface. The dataset in question contains information about diamond properties and their prices. Your objective is to create a comprehensive visualization that demonstrates your understanding of the `seaborn.objects` capabilities. Dataset The `diamonds` dataset is preloaded in seaborn and contains the following columns: - `carat`: Carat weight of the diamond. - `cut`: Quality of the cut (Fair, Good, Very Good, Premium, Ideal). - `color`: Diamond color, from J (worst) to D (best). - `clarity`: A measurement of how clear the diamond is (I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF). - `depth`: Total depth percentage (z / mean(x, y) = 2 * z / (x + y)). - `table`: Width of the top of the diamond relative to its widest point. - `price`: Price in US dollars. - `x`: Length in mm. - `y`: Width in mm. - `z`: Depth in mm. Task Create a multi-faceted plot that visualizes the relationship between the `carat` weight and the `price` of diamonds, taking into account different `cut` and `color` categories. Your visualization should include the following features: 1. **Scatter Plot Layer**: Display the relationship between `carat` and `price` with individual points (`carat` on the x-axis, `price` on the y-axis). Color-code the points based on the `color` category. 2. **Regression Line Layer**: Overlay a regression line that demonstrates the general trend of the data. 3. **Facet Grid**: Create subplots based on the `cut` quality of the diamonds. 4. **Customization**: - Use a logarithmic scale for the y-axis (price) to handle the wide range of values. - Apply a suitable color palette for the `color` categories. - Add appropriate axis labels and a title for the entire plot. - Customize the plotâ€™s theme to enhance readability. Implementation You can use the following code template to start with: ```python import seaborn as sns import matplotlib as mpl import seaborn.objects as so # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the plot p = (so.Plot(diamonds, x=\\"carat\\", y=\\"price\\", color=\\"color\\") .facet(col=\\"cut\\") .add(so.Dots()) .add(so.Line(), so.PolyFit()) .scale(y=\\"log\\", color=\\"Spectral\\") .label(x=\\"Carat Weight\\", y=\\"Price (log scale)\\", title=\\"Diamond Price vs. Carat Weight by Cut Quality\\") .theme({\\"grid.linewidth\\": 0.5}) ) # Display the plot p.show() ``` Ensure your final plot satisfies all the requirements and demonstrates advanced usage of the `seaborn.objects` interface. Feel free to experiment with additional customizations to improve the plot\'s aesthetic and informational value. Submission Submit the Python code that generates the final plot. Include any necessary explanations or comments that describe your approach and any customizations you made.","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the plot p = (so.Plot(diamonds, x=\\"carat\\", y=\\"price\\", color=\\"color\\") .facet(col=\\"cut\\") .add(so.Dots()) .add(so.Line(), so.PolyFit()) .scale(y=\\"log\\", color=\\"Spectral\\") .label(x=\\"Carat Weight\\", y=\\"Price (log scale)\\", title=\\"Diamond Price vs. Carat Weight by Cut Quality\\") .theme({\\"figure.facecolor\\": \\"white\\", \\"grid.linewidth\\": 0.5}) ) # Display the plot p.show()"},{"question":"**Problem Statement:** In this task, you will write a Python function to generate a serialized email message and save it to a file using the `email.generator` module. You will need to handle specific encoding and formatting requirements as well. **Function Signature:** ```python def generate_serialized_email(to_email: str, from_email: str, subject: str, body: str, output_file: str, use_bytes_generator: bool) -> None: pass ``` **Explanation:** 1. **Inputs:** - `to_email` (str): The recipient\'s email address. - `from_email` (str): The sender\'s email address. - `subject` (str): The subject of the email. - `body` (str): The body content of the email. - `output_file` (str): The file path where the serialized email will be saved. - `use_bytes_generator` (bool): A flag indicating whether to use `BytesGenerator` (if True) or `Generator` (if False). 2. **Outputs:** - The function will save the serialized email message to the specified `output_file`. The file content will be in the correct format based on the generator class used. 3. **Constraints:** - The email message should comply with RFC standards for email formatting. - If `use_bytes_generator` is True, handle binary data correctly using `BytesGenerator`. - If `use_bytes_generator` is False, handle text data using `Generator`. - Handle special encoding cases such as 8-bit and 7-bit transfer encoding correctly. - Ensure message headers are properly wrapped if they exceed standard lengths. 4. **Performance Requirements:** - The function should operate efficiently even if the body of the email is large. **Guidelines:** - Use the `email.message.EmailMessage` class to create the email message. - Serialize the email message using either `BytesGenerator` or `Generator` based on the `use_bytes_generator` flag. - Consider policy settings such as line separators and content transfer encoding options. **Example:** ```python generate_serialized_email( to_email=\\"recipient@example.com\\", from_email=\\"sender@example.com\\", subject=\\"Test Email\\", body=\\"This is a test email.\\", output_file=\\"output_email.txt\\", use_bytes_generator=True ) ``` This should create `output_email.txt` with the serialized representation of the email, using `BytesGenerator`.","solution":"import os from email.message import EmailMessage from email.generator import Generator, BytesGenerator def generate_serialized_email(to_email: str, from_email: str, subject: str, body: str, output_file: str, use_bytes_generator: bool) -> None: Generates a serialized email message and saves it to a file. Parameters: to_email (str): The recipient\'s email address. from_email (str): The sender\'s email address. subject (str): The subject of the email. body (str): The body content of the email. output_file (str): The file path where the serialized email will be saved. use_bytes_generator (bool): Whether to use BytesGenerator or not (true for BytesGenerator, false for Generator). # Create email message msg = EmailMessage() msg[\'To\'] = to_email msg[\'From\'] = from_email msg[\'Subject\'] = subject msg.set_content(body) # Serialize and save email with open(output_file, \'wb\' if use_bytes_generator else \'w\') as f: if use_bytes_generator: gen = BytesGenerator(f) else: gen = Generator(f) gen.flatten(msg)"},{"question":"# Coding Assessment: Advanced Seaborn Plotting Objective: Demonstrate your understanding of seaborn\'s `seaborn.objects` module by creating a well-annotated and informative plot. Dataset: You will use the built-in `healthexp` dataset available in seaborn. It contains health expenditure data for various countries over several years. Task: 1. **Load the `healthexp` dataset** using seaborn\'s `load_dataset` function. 2. **Create a plot** using the `so.Plot` object from `seaborn.objects`: - The x-axis should represent the year (`Year`). - The y-axis should represent health spending in USD (`Spending_USD`). - Different lines should be colored based on the country (`Country`). 3. **Normalize** the data relative to the year 1970 and convert it to a percentage change. 4. **Label** the y-axis appropriately to reflect the percentage change in spending from the 1970 baseline. 5. **Customize the plot** by adding a title and adjusting any other aesthetic parameters to ensure the plot is informative and visually appealing. Expected Output: A single plot that effectively communicates the percentage change in health spending relative to the baseline year 1970 for multiple countries. Ensure all necessary annotations and customizations are present to make the plot understandable. Constraints: - You are only allowed to use the seaborn and matplotlib packages for plotting. - Ensure your code is efficient and readable, following best coding practices. Performance Requirements: - The plot should render within a reasonable amount of time (under 5 seconds is ideal). Example: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Create the plot ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") .label(title=\\"Health Expenditure Over Time\\") .scale(color=\\"Country\\") # Add this line to include legend by default ) ``` Your task requires similar steps but you must add any other necessary features to make the plot highly informative and visually appealing. Be creative and thoughtful in your design!","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def create_health_expenditure_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Extract Year and Spending columns for calculations baseline_year = 1970 baseline_spending = healthexp[healthexp[\'Year\'] == baseline_year] # Calculate percentage change for each country healthexp = healthexp.merge(baseline_spending[[\'Country\', \'Spending_USD\']], on=\'Country\', suffixes=(\'\', \'_1970\')) healthexp[\'Spending_Change\'] = (healthexp[\'Spending_USD\'] - healthexp[\'Spending_USD_1970\']) / healthexp[\'Spending_USD_1970\'] * 100 # Create the plot p = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_Change\\", color=\\"Country\\") .add(so.Line(), so.Agg()) .label(y=\\"Percent Change in Spending from 1970 Baseline\\") .label(title=\\"Health Expenditure Over Time (Percentage Change from 1970)\\") .scale(color=\\"Country\\") # Ensures each line represents a country ) return p"},{"question":"# PyTorch Backend Configuration and Usage **Objective:** Your task is to write a PyTorch function that configures specific backend settings, performs a matrix multiplication using these settings, and compares the performance of these operations. **Instructions:** 1. **Function Signature:** ```python def backend_configuration_and_performance_test(matrix_size: int) -> dict: ``` 2. **Parameters:** - `matrix_size` (int): The size of the square matrices to be used in the matrix multiplication. 3. **Functionality:** - Use the `torch` library to create two random square matrices (of shape `matrix_size x matrix_size`). - Configure the following backend settings: 1. Enable or disable TensorFloat-32 precision in CUDA using `torch.backends.cuda.matmul.allow_tf32`. 2. Enable or disable cuDNN benchmarking using `torch.backends.cudnn.benchmark`. - Perform matrix multiplications under different configurations: 1. With TensorFloat-32 enabled and cuDNN benchmarking enabled. 2. With TensorFloat-32 enabled and cuDNN benchmarking disabled. 3. With TensorFloat-32 disabled and cuDNN benchmarking enabled. 4. With TensorFloat-32 disabled and cuDNN benchmarking disabled. - Measure and report the time taken for each matrix multiplication. 4. **Output:** - Return a dictionary where the keys are descriptive strings of the configuration (e.g., `\\"tf32_on_cudnn_bench_on\\"`) and the values are the time taken for the matrix multiplication under that configuration. **Constraints:** - Assume that the machine running the code has a CUDA-compatible GPU. - Ensure proper management of device allocations and memory for accurate performance measurements. **Example Function Call:** ```python results = backend_configuration_and_performance_test(1024) print(results) ``` The expected output should be details of time taken for matrix multiplications under different backend configurations. Here is a possible format: ```python { \\"tf32_on_cudnn_bench_on\\": 0.123, \\"tf32_on_cudnn_bench_off\\": 0.145, \\"tf32_off_cudnn_bench_on\\": 0.178, \\"tf32_off_cudnn_bench_off\\": 0.200 } ``` --- **Note:** - Ensure that the code includes necessary import statements and handles any potential exceptions that may arise during matrix multiplications. - You may use `torch.cuda.synchronize()` before starting and after stopping the timer to ensure accurate timing of GPU operations.","solution":"import torch import time def backend_configuration_and_performance_test(matrix_size: int) -> dict: def measure_time(tf32_enabled, cudnn_bench_enabled): torch.backends.cuda.matmul.allow_tf32 = tf32_enabled torch.backends.cudnn.benchmark = cudnn_bench_enabled A = torch.rand((matrix_size, matrix_size), device=\'cuda\') B = torch.rand((matrix_size, matrix_size), device=\'cuda\') torch.cuda.synchronize() start_time = time.time() C = torch.mm(A, B) torch.cuda.synchronize() end_time = time.time() return end_time - start_time results = {} settings = [ (True, True), (True, False), (False, True), (False, False) ] for tf32, cudnn_bench in settings: key = f\\"tf32_{\'on\' if tf32 else \'off\'}_cudnn_bench_{\'on\' if cudnn_bench else \'off\'}\\" results[key] = measure_time(tf32, cudnn_bench) return results"},{"question":"Advanced Feature Extraction with scikit-learn Problem Statement You are provided with a dataset of product reviews in JSON format. Each review contains the following fields: - `product_id`: A unique identifier for the product. - `review`: The text of the review. - `rating`: The rating given to the product. Your task is to perform feature extraction on this dataset using scikit-learn\'s `DictVectorizer` and `TfidfVectorizer`. Specifically, you need to: 1. Extract features from the reviews using `TfidfVectorizer`. 2. Transform these features into a format suitable for machine learning models. 3. Extract categorical features (`product_id`) using `DictVectorizer`. 4. Combine these features into a final dataset that can be used for training a machine learning model. Input - A JSON file containing the product reviews. Example content of the JSON file: ```json [ {\\"product_id\\": \\"B001E4KFG0\\", \\"review\\": \\"Amazing product, highly recommend!\\", \\"rating\\": 5}, {\\"product_id\\": \\"B00813GRG4\\", \\"review\\": \\"Did not meet my expectations.\\", \\"rating\\": 2}, {\\"product_id\\": \\"B000LQOCH0\\", \\"review\\": \\"Good value for the price.\\", \\"rating\\": 4} ] ``` Output - A tuple containing three elements: 1. A sparse matrix of shape (n_samples, n_features) representing the combined features. 2. The feature names from `TfidfVectorizer`. 3. The feature names from `DictVectorizer`. Function Signature ```python from typing import Tuple from scipy.sparse import hstack, csr_matrix def extract_features_from_reviews(json_file_path: str) -> Tuple[csr_matrix, list, list]: pass ``` Constraints - Use `TfidfVectorizer` with the following parameters: - `max_features=1000` - `stop_words=\'english\'` - Use `DictVectorizer` with default parameters. - Ensure that the final combined feature matrix is a sparse matrix format (CSR). Detailed Requirements 1. **TfidfVectorizer**: - Tokenize and transform the `review` texts into TF-IDF features. - Use at most 1000 features and remove English stop words. 2. **DictVectorizer**: - Transform the categorical `product_id` feature into one-hot encoded features. 3. **Combine Features**: - Use `scipy.sparse.hstack` to horizontally stack the features from `TfidfVectorizer` and `DictVectorizer`. - Return the combined feature matrix, the feature names from `TfidfVectorizer`, and the feature names from `DictVectorizer`. Example Usage ```python json_file_path = \'reviews.json\' combined_features, tfidf_feature_names, dict_feature_names = extract_features_from_reviews(json_file_path) # Example output: # combined_features.shape -> (3, 1003) # tfidf_feature_names -> [\'amazing\', \'did\', \'expectations\', ..., \'value\'] # dict_feature_names -> [\'product_id=B001E4KFG0\', \'product_id=B00813GRG4\', \'product_id=B000LQOCH0\'] ``` Notes - You may need to read and parse the JSON file. - Ensure that the function handles the provided example input and similar inputs correctly. - Make use of the `scipy.sparse` library to handle large sparse matrices efficiently.","solution":"import json from typing import Tuple, List from scipy.sparse import hstack, csr_matrix from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_extraction import DictVectorizer def extract_features_from_reviews(json_file_path: str) -> Tuple[csr_matrix, List[str], List[str]]: # Load the reviews from the JSON file with open(json_file_path, \'r\') as file: reviews = json.load(file) # Extract the text of reviews and the product ids review_texts = [review[\'review\'] for review in reviews] product_ids = [{\'product_id\': review[\'product_id\']} for review in reviews] # Use TfidfVectorizer to extract features from review texts tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words=\'english\') tfidf_features = tfidf_vectorizer.fit_transform(review_texts) tfidf_feature_names = tfidf_vectorizer.get_feature_names_out() # Use DictVectorizer to extract features from product IDs dict_vectorizer = DictVectorizer() dict_features = dict_vectorizer.fit_transform(product_ids) dict_feature_names = dict_vectorizer.get_feature_names_out() # Combine the features into one final feature matrix combined_features = hstack([tfidf_features, dict_features]) return combined_features, list(tfidf_feature_names), list(dict_feature_names)"},{"question":"# Question **Objective**: Implement a function to convert a list of UTC datetime strings to a list of datetime strings in a specified IANA time zone while handling daylight saving time transitions correctly. **Function Signature**: ```python from typing import List import datetime from zoneinfo import ZoneInfo def convert_utc_to_timezone(datetimes: List[str], timezone: str) -> List[str]: pass ``` **Inputs**: - `datetimes` (List[str]): A list of datetime strings in ISO 8601 format, representing times in UTC (e.g., \\"2023-03-10T12:00:00Z\\"). - `timezone` (str): A string representing the IANA time zone (e.g., \\"America/New_York\\"). **Output**: - Returns a list of datetime strings converted to the specified time zone, preserving the original datetime format (e.g., \\"2023-03-10T07:00:00-05:00\\"). **Constraints**: - The input datetime strings are guaranteed to be in valid ISO 8601 format. - The specified time zone for each datetime string will be a valid IANA time zone. - The `datetimes` list will contain at most 1000 datetime strings. **Performance Requirements**: - The function should operate efficiently for the given constraints, ensuring that each conversion handles daylight saving time transitions correctly. **Example**: ```python # Example input utc_datetimes = [ \\"2023-03-10T12:00:00Z\\", \\"2023-11-05T06:00:00Z\\" ] timezone = \\"America/New_York\\" # Expected output # [ # \\"2023-03-10T07:00:00-05:00\\", # EDT # \\"2023-11-05T02:00:00-04:00\\" # EST # ] print(convert_utc_to_timezone(utc_datetimes, timezone)) ``` **Notes**: 1. Ensure that daylight saving times are handled correctly. 2. The conversion should take into account the time difference between the UTC and the specified time zone. 3. You should use the `ZoneInfo` class from the `zoneinfo` module and the `datetime` module for the conversion. **Hint**: The `datetime.fromisoformat()` method can be used to parse the ISO 8601 datetime strings, and the `astimezone()` method can be used for conversion. **Evaluation Criteria**: 1. Correctness: The solution must return the correct datetime strings for the specified time zone. 2. Efficiency: The solution should handle the maximum constraints efficiently. 3. Code Quality: Code should be clean, well-documented, and follow Python best practices.","solution":"from typing import List from datetime import datetime from zoneinfo import ZoneInfo def convert_utc_to_timezone(datetimes: List[str], timezone: str) -> List[str]: converted_datetimes = [] tz = ZoneInfo(timezone) for utc_datetime_str in datetimes: utc_datetime = datetime.fromisoformat(utc_datetime_str[:-1]).replace(tzinfo=ZoneInfo(\'UTC\')) local_datetime = utc_datetime.astimezone(tz) converted_datetimes.append(local_datetime.isoformat()) return converted_datetimes"},{"question":"# Question: Implement Custom Sequence Operations in Python You are tasked with implementing a Python class `CustomSequence` that mimics certain behaviors provided by the Python Sequence Protocol. Your class should be able to handle lists and tuples as the underlying sequence types and should provide specific functionalities. Requirements: 1. **Initialization**: - The class should be initialized with any sequence (`list` or `tuple`). If the provided object is neither a list nor a tuple, raise a `TypeError`. 2. **Method: `concat`**: - Method signature: `concat(self, other)` - Concatenate the underlying sequence with another sequence. This should result in a new `CustomSequence` object. - Raise a `TypeError` if `other` is not a `list` or `tuple`. 3. **Method: `repeat`**: - Method signature: `repeat(self, count)` - Repeat the underlying sequence `count` times. This should result in a new `CustomSequence` object. - Raise a `ValueError` if `count` is not a non-negative integer. 4. **Method: `get_item`**: - Method signature: `get_item(self, index)` - Return the element at the given `index` in the sequence. - Support negative indexing as well. 5. **Method: `slice`**: - Method signature: `slice(self, start, end)` - Return a slice of the sequence from `start` to `end`. Result should be a new `CustomSequence` object. 6. **Method: `to_list`**: - Method signature: `to_list(self)` - Return the underlying sequence as a list. 7. **Method: `to_tuple`**: - Method signature: `to_tuple(self)` - Return the underlying sequence as a tuple. Constraints: - The input sequences will only contain primitive data types (integers, floats, strings). - Avoid using external libraries; rely only on the standard Python sequence operations. Example Usage: ```python cs = CustomSequence([1, 2, 3]) cs2 = CustomSequence((4, 5)) # Concatenate and repeat sequences cs3 = cs.concat(cs2) # CustomSequence([1, 2, 3, 4, 5]) cs4 = cs3.repeat(2) # CustomSequence([1, 2, 3, 4, 5, 1, 2, 3, 4, 5]) # Access by index item = cs4.get_item(3) # 4 item2 = cs4.get_item(-1) # 5 # Slicing cs5 = cs4.slice(1, 6) # CustomSequence([2, 3, 4, 5, 1]) # Convert to list and tuple seq_list = cs4.to_list() # [1, 2, 3, 4, 5, 1, 2, 3, 4, 5] seq_tuple = cs4.to_tuple() # (1, 2, 3, 4, 5, 1, 2, 3, 4, 5) ``` Implement the `CustomSequence` class meeting the above requirements.","solution":"class CustomSequence: def __init__(self, sequence): if not isinstance(sequence, (list, tuple)): raise TypeError(\\"sequence must be a list or a tuple\\") self.sequence = sequence def concat(self, other): if not isinstance(other, (list, tuple)): raise TypeError(\\"other must be a list or a tuple\\") return CustomSequence(self.sequence + other) def repeat(self, count): if not isinstance(count, int) or count < 0: raise ValueError(\\"count must be a non-negative integer\\") return CustomSequence(self.sequence * count) def get_item(self, index): return self.sequence[index] def slice(self, start, end): return CustomSequence(self.sequence[start:end]) def to_list(self): return list(self.sequence) def to_tuple(self): return tuple(self.sequence)"},{"question":"**Question: Securely Storing and Retrieving Temporary Data** You are tasked with creating a secure temporary storage solution for a Python application. The application needs to perform the following steps: 1. Create several temporary files; each file should contain random data. 2. Read and verify the contents of these temporary files to ensure the data is correctly written and readable. 3. Create a temporary directory to store some files temporarily. 4. Ensure that all temporary files and directories are deleted after their usage to prevent any security risks or resource leaks. Implement the following functions: 1. **create_temp_files(num_files, file_size)**: - **Input**: - `num_files` (int): The number of temporary files to create. - `file_size` (int): The size of random data (in bytes) to write into each file. - **Output**: - A list of file names (strings), each corresponding to a temporary file created. - **Process**: - Create `num_files` using `tempfile.TemporaryFile`. - Write `file_size` bytes of random data to each file. - Return the list of file names. 2. **verify_temp_files(file_names)**: - **Input**: - `file_names` (list of str): List of temporary file names to verify. - **Output**: - A boolean value indicating whether all files contain the expected data size. - **Process**: - Open each file, read its contents, and verify that the size matches the expected `file_size` bytes. - Return `True` if all files are verified successfully, otherwise return `False`. 3. **create_temp_directory_with_files(num_files, file_size)**: - **Input**: - `num_files` (int): The number of files to create inside a temporary directory. - `file_size` (int): The size of random data (in bytes) to write into each file. - **Output**: - The name of the temporary directory. - **Process**: - Create a temporary directory using `tempfile.TemporaryDirectory`. - Inside this directory, create `num_files` with `file_size` bytes of random data. - Return the name of the temporary directory. # Constraints - Use the `tempfile` module functionalities described above. - Ensure that the temporary files and directories are securely created and properly cleaned up after usage. - Do not use any other temporary file approaches or modules other than `tempfile`. - Assume all random data is binary and can be generated using `os.urandom`. # Example Usage ```python import os import tempfile def create_temp_files(num_files, file_size): file_names = [] for _ in range(num_files): with tempfile.NamedTemporaryFile(delete=False) as temp_file: file_names.append(temp_file.name) temp_file.write(os.urandom(file_size)) return file_names def verify_temp_files(file_names): for file_name in file_names: with open(file_name, \'rb\') as temp_file: data = temp_file.read() if len(data) != file_size: return False return True def create_temp_directory_with_files(num_files, file_size): with tempfile.TemporaryDirectory() as temp_dir: for _ in range(num_files): with open(os.path.join(temp_dir, tempfile.mkstemp(suffix=\'.tmp\', dir=temp_dir)), \'wb\') as temp_file: temp_file.write(os.urandom(file_size)) return temp_dir # Example of usage temp_files = create_temp_files(5, 1024) print(verify_temp_files(temp_files)) # Expected: True temp_dir = create_temp_directory_with_files(3, 512) print(\\"Temp directory created:\\", temp_dir) ``` # Important: - Ensure your output and clean-up are accurate to avoid resource leaks. - Properly handle exceptions and edge cases, like permission issues and file system constraints.","solution":"import os import tempfile def create_temp_files(num_files, file_size): Create temporary files with random data. Args: num_files (int): Number of temporary files to create. file_size (int): Size of random data in bytes to write into each file. Returns: list of str: List of file names for the created temporary files. file_names = [] for _ in range(num_files): with tempfile.NamedTemporaryFile(delete=False) as temp_file: file_names.append(temp_file.name) temp_file.write(os.urandom(file_size)) return file_names def verify_temp_files(file_names, file_size): Verify the contents of temporary files. Args: file_names (list of str): List of temporary file names to verify. file_size (int): Expected size of the data in each file. Returns: bool: True if all files contain the expected data size, False otherwise. for file_name in file_names: with open(file_name, \'rb\') as temp_file: data = temp_file.read() if len(data) != file_size: return False return True def create_temp_directory_with_files(num_files, file_size): Create a temporary directory with several files containing random data. Args: num_files (int): Number of files to create inside the temporary directory. file_size (int): Size of random data in bytes to write into each file. Returns: str: Name of the temporary directory. temp_dir = tempfile.mkdtemp() for _ in range(num_files): temp_file_path = os.path.join(temp_dir, next(tempfile._get_candidate_names()) + \'.tmp\') with open(temp_file_path, \'wb\') as temp_file: temp_file.write(os.urandom(file_size)) return temp_dir"},{"question":"# Question: Customizing Plotting Context with Seaborn Seaborn\'s `plotting_context` function allows users to control the appearance of their plots by scaling various elements such as labels, lines, and markers. You are provided with a dataset representing the sales of different products over a week. Your task is to create customized line plots using seaborn to visualize this data. The plots should be generated using different plotting contexts and custom scaling parameters. Dataset: ```python import pandas as pd data = { \'Day\': [\'Monday\', \'Tuesday\', \'Wednesday\', \'Thursday\', \'Friday\', \'Saturday\', \'Sunday\'], \'Product_A\': [20, 34, 30, 35, 27, 25, 32], \'Product_B\': [25, 30, 32, 33, 31, 29, 27] } df = pd.DataFrame(data) ``` Requirements: 1. **Default Context Plot**: Create a line plot for `Product_A` and `Product_B` using the default plotting context. 2. **Talk Context Plot**: Create a line plot for `Product_A` and `Product_B` using the \\"talk\\" plotting context. 3. **Customized Context Plot**: Create a line plot for `Product_A` and `Product_B` using a custom plotting context where: - Font scale is increased to 1.5 times the default. - Line width is doubled. 4. **Temporary Context Plot**: Within a context manager, create a line plot using the \\"poster\\" plotting context. Verify that the changes are temporary by printing the default context parameters before and after the block. **Constraints:** - The plots should include appropriate titles, legends, and axis labels. - Use seaborn and matplotlib for creating the plots. - Ensure clear and visually distinguishable lines for `Product_A` and `Product_B`. **Output:** - Display all the plots in the same Jupyter notebook cell. - Print the context parameters as required. Example Solution: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset df = pd.DataFrame(data) # 1. Default Context Plot plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Day\', y=\'Product_A\', data=df, label=\'Product_A\') sns.lineplot(x=\'Day\', y=\'Product_B\', data=df, label=\'Product_B\') plt.title(\'Sales over a week (Default Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # 2. Talk Context Plot sns.set_context(\\"talk\\") plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Day\', y=\'Product_A\', data=df, label=\'Product_A\') sns.lineplot(x=\'Day\', y=\'Product_B\', data=df, label=\'Product_B\') plt.title(\'Sales over a week (Talk Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # 3. Customized Context Plot custom_params = {\\"font.scale\\": 1.5, \\"lines.linewidth\\": 2} sns.set_context(\\"notebook\\", rc=custom_params) plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Day\', y=\'Product_A\', data=df, label=\'Product_A\') sns.lineplot(x=\'Day\', y=\'Product_B\', data=df, label=\'Product_B\') plt.title(\'Sales over a week (Custom Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # 4. Temporary Context Plot print(\\"Default context parameters before temporary change:\\") print(sns.plotting_context()) with sns.plotting_context(\\"poster\\"): plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Day\', y=\'Product_A\', data=df, label=\'Product_A\') sns.lineplot(x=\'Day\', y=\'Product_B\', data=df, label=\'Product_B\') plt.title(\'Sales over a week (Poster Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() print(\\"Default context parameters after temporary change:\\") print(sns.plotting_context()) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Dataset data = { \'Day\': [\'Monday\', \'Tuesday\', \'Wednesday\', \'Thursday\', \'Friday\', \'Saturday\', \'Sunday\'], \'Product_A\': [20, 34, 30, 35, 27, 25, 32], \'Product_B\': [25, 30, 32, 33, 31, 29, 27] } df = pd.DataFrame(data) # 1. Default Context Plot plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Day\', y=\'Product_A\', data=df, label=\'Product_A\') sns.lineplot(x=\'Day\', y=\'Product_B\', data=df, label=\'Product_B\') plt.title(\'Sales over a week (Default Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # 2. Talk Context Plot sns.set_context(\\"talk\\") plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Day\', y=\'Product_A\', data=df, label=\'Product_A\') sns.lineplot(x=\'Day\', y=\'Product_B\', data=df, label=\'Product_B\') plt.title(\'Sales over a week (Talk Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # 3. Customized Context Plot custom_params = {\\"font.scale\\": 1.5, \\"lines.linewidth\\": 2} sns.set_context(\\"notebook\\", rc=custom_params) plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Day\', y=\'Product_A\', data=df, label=\'Product_A\') sns.lineplot(x=\'Day\', y=\'Product_B\', data=df, label=\'Product_B\') plt.title(\'Sales over a week (Custom Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() # 4. Temporary Context Plot print(\\"Default context parameters before temporary change:\\") print(sns.plotting_context()) with sns.plotting_context(\\"poster\\"): plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Day\', y=\'Product_A\', data=df, label=\'Product_A\') sns.lineplot(x=\'Day\', y=\'Product_B\', data=df, label=\'Product_B\') plt.title(\'Sales over a week (Poster Context)\') plt.xlabel(\'Day\') plt.ylabel(\'Sales\') plt.legend() plt.show() print(\\"Default context parameters after temporary change:\\") print(sns.plotting_context())"},{"question":"**Assessment Question: Visualization with Seaborn** You are provided with the \\"penguins\\" dataset, accessible through seaborn\'s `load_dataset` function. This dataset includes observations of penguin species and their physical measurements. Your task is to create a visualization using seaborn that satisfies the following requirements: 1. Load the \\"penguins\\" dataset. 2. Plot a dash plot to show the body mass (`body_mass_g`) of penguins. Use different colors to distinguish between the sexes (`sex`). 3. Customize the plot to: - Set the alpha transparency of the dashes to 0.5. - Adjust the linewidth according to the flipper length (`flipper_length_mm`). 4. Add a layer of dots to the plot to represent individual data points. Use jittering to avoid overlap. 5. Use aggregation to show the mean body mass and include it in the plot. 6. Explicitly set the orientation of the dashes to vertical, even though both coordinate variables are numeric. # Input None (the dataset is to be loaded within the script). # Expected Output A plot generated using seaborn that meets the above specifications. # Constraints and Performance - Ensure that the script runs efficiently. - Handle any missing data gracefully. # Example Code to Get You Started ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the initial plot p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") # Add dash plot with the specified customizations p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") # Add dots with jittering for individual data points p.add(so.Dots(), so.Dodge(), so.Jitter()) # Include aggregate values p.add(so.Dash(), so.Agg(), so.Dodge()) # Set the orientation to vertical p.orient = \\"y\\" # Show the plot p.show() ``` Your task is to complete the script based on the given example code to fulfill all the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_penguins(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Drop rows with missing data penguins = penguins.dropna() # Create the initial plot plt.figure(figsize=(10, 6)) # Plot dash points for body mass dash_plot = sns.scatterplot( data=penguins, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", style=\\"sex\\", size=\\"flipper_length_mm\\", sizes=(20, 200), alpha=0.5, legend=\'brief\' ) # Add jittering to avoid overlap (jitter only used for the points) sns.stripplot( data=penguins, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", dodge=True, jitter=True, marker=\\"o\\", alpha=0.7, ax=dash_plot ) # Calculate and plot the mean body mass using a different layer mean_body_mass = penguins.groupby([\'species\', \'sex\'])[\'body_mass_g\'].mean().reset_index() sns.pointplot( data=mean_body_mass, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", dodge=0.5, markers=[\\"o\\", \\"o\\"], linestyles=\\"\\", ax=dash_plot, ) # Customize the plot dash_plot.set_title(\\"Penguin Body Mass by Species and Sex\\") dash_plot.set_ylabel(\\"Body Mass (g)\\") dash_plot.set_xlabel(\\"Species\\") # Show the plot plt.legend(loc=\'upper right\') plt.show()"},{"question":"# Python Version Decoder Python\'s version information is represented by a single integer known as `PY_VERSION_HEX`. This integer encodes all components of the version number using a specific bitwise structure. For example, the version \\"3.4.1a2\\" is represented as `0x030401a2`. Your task is to implement a function in Python that decodes the `PY_VERSION_HEX` integer into a human-readable string format \\"MAJOR.MINOR.MICROLEVELSERIAL\\". Function Signature: ```python def decode_python_version(hex_version: int) -> str: Decode a Python version number encoded as a single integer into its human-readable string format. :param hex_version: int - The Python version number encoded as an integer. :return: str - The decoded version string in the format \\"MAJOR.MINOR.MICROLEVELSERIAL\\". pass ``` Input: - `hex_version` (integer): A single integer representing the encoded Python version number. Output: - Returns a string representing the Python version in the format \\"MAJOR.MINOR.MICROLEVELSERIAL\\". Constraints: - The input integer will always be in the valid format of a Python version number. Example: ```python assert decode_python_version(0x030401a2) == \\"3.4.1a2\\" assert decode_python_version(0x030a00f0) == \\"3.10.0\\" ``` Note: - The `LEVEL` field should be decoded as follows: - 0xA -> \\"a\\" (alpha) - 0xB -> \\"b\\" (beta) - 0xC -> \\"rc\\" (release candidate) - 0xF -> \\"\\" (final release, not represented in the output string) # Challenge: - Your solution should correctly handle all possible inputs within the expected range. - Implementing bitwise operations can help efficiently decode the version number.","solution":"def decode_python_version(hex_version: int) -> str: Decode a Python version number encoded as a single integer into its human-readable string format. :param hex_version: int - The Python version number encoded as an integer. :return: str - The decoded version string in the format \\"MAJOR.MINOR.MICROLEVELSERIAL\\". # Extract components using bit shifts and masks major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF level_serial = hex_version & 0xFF # Determine the level and serial level_indicator = (level_serial >> 4) & 0xF serial = level_serial & 0xF level_dict = { 0xA: \\"a\\", 0xB: \\"b\\", 0xC: \\"rc\\", 0xF: \\"\\" } level = level_dict.get(level_indicator, \\"\\") # Combine to form the version string if level: version_str = f\\"{major}.{minor}.{micro}{level}{serial}\\" else: version_str = f\\"{major}.{minor}.{micro}\\" return version_str"},{"question":"**Objective:** Demonstrate your ability to work with scikit-learn datasets and implement a data processing pipeline. # Problem Statement You are provided with a utility function in scikit-learn to fetch the Olivetti faces dataset. Your task is to load this dataset, preprocess the data, train a machine learning model, and evaluate its performance. Steps to follow: 1. **Load the Dataset:** Use the function `fetch_olivetti_faces` from `sklearn.datasets` to load the dataset. 2. **Data Preprocessing:** - Split the data into features `X` and target `y`. Note that `y` should be the target labels if available, otherwise use the data itself. - Normalize the features using `StandardScaler` to have zero mean and unit variance. 3. **Model Training:** - Split the dataset into training and testing sets (80% training, 20% testing). - Train a `KNeighborsClassifier` on the training data. 4. **Model Evaluation:** - Predict the labels for the test set. - Calculate and print the accuracy score of the model. Expected Function Signature: ```python def evaluate_olivetti_faces_model(): This function does not take any inputs. It loads the Olivetti faces dataset, preprocesses it, trains a KNeighborsClassifier, evaluates its performance, and prints the accuracy score. Returns: None # Example usage: # evaluate_olivetti_faces_model() # Expected Output: Accuracy score printed on the console. ``` # Additional Requirements: - Ensure to handle any exceptions or errors that may arise during data loading or processing. - Provide comments in your code to explain the steps and logic used. - Optimize code readability and maintainability. Constraints: - Use only the libraries and functions available in scikit-learn for dataset loading, preprocessing, model training, and evaluation.","solution":"from sklearn.datasets import fetch_olivetti_faces from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def evaluate_olivetti_faces_model(): # Load the dataset data = fetch_olivetti_faces() X = data.images.reshape((data.images.shape[0], -1)) y = data.target # Normalize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Train a KNeighborsClassifier on the training data knn = KNeighborsClassifier() knn.fit(X_train, y_train) # Predict the labels for the test set y_pred = knn.predict(X_test) # Calculate the accuracy score of the model accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\")"},{"question":"Objective: To assess the student\'s ability to interact with filesystem paths in Python using provided utilities and ensuring robust handling of various types of path inputs. Question: **Filesystem Path Resolution and Validation in Python** A utility function named `resolve_and_validate_path` needs to be implemented to resolve and validate filesystem paths. The function should follow the below specifications: 1. It must handle inputs as both `str` and `os.PathLike` objects. 2. If the input is already a byte string, it must be handled and returned as is. 3. If the input is `None` or invalid, the function should raise a `TypeError`. 4. It should use the `__fspath__` method of `os.PathLike` objects to fetch the string representation. 5. Paths should be validated to check if they really exist on the filesystem. Specifications: - **Input**: `path` - a path to resolve and validate, which can be `str`, `os.PathLike`, `bytes`, or `None`. - **Output**: - Return the resolved path as a string if it is valid. - Raise `TypeError` if the path is invalid or `None`. Input and Output Format: - Example Function Call: ```python resolved_path = resolve_and_validate_path(path) ``` - Example Inputs and Outputs: ```python resolve_and_validate_path(\\"existing/path\\") # Output: \\"existing/path\\" (Assuming this path exists on the filesystem) resolve_and_validate_path(None) # Raise TypeError resolve_and_validate_path(b\\"existing/path\\") # Output: b\\"existing/path\\" (If the path exists as a byte string on the filesystem) import pathlib resolve_and_validate_path(pathlib.Path(\\"existing/path\\")) # Output: \\"existing/path\\" (If the path exists on the filesystem) ``` Constraints: - Do not use external packages. - Proper error handling should be implemented. - Assume the function will be called with various forms of path inputs in a practical, file-containing environment. # Template: ```python import os def resolve_and_validate_path(path): Resolve and validate the provided filesystem path. Parameters: path (str | os.PathLike | bytes): The path to resolve and validate. Returns: str: Resolved filesystem path as string. Raises: TypeError: If the path is None or invalid. if path is None: raise TypeError(\\"Path cannot be None\\") # Check if the input is of bytes type if isinstance(path, bytes): resolved_path = path else: # Ensure the path is a string representation if isinstance(path, os.PathLike): path = path.__fspath__() elif not isinstance(path, str): raise TypeError(\\"Invalid path type\\") resolved_path = path # Check if the path exists on the filesystem if not os.path.exists(resolved_path): raise TypeError(\\"Path does not exist\\") return resolved_path ``` Task Implement the `resolve_and_validate_path` function fulfilling the above requirements. Your implementation should correctly handle different input types and ensure robust path validation.","solution":"import os def resolve_and_validate_path(path): Resolve and validate the provided filesystem path. Parameters: path (str | os.PathLike | bytes): The path to resolve and validate. Returns: str | bytes: Resolved filesystem path. Raises: TypeError: If the path is None or invalid. if path is None: raise TypeError(\\"Path cannot be None\\") if isinstance(path, bytes): resolved_path = path else: if isinstance(path, os.PathLike): path = path.__fspath__() elif not isinstance(path, str): raise TypeError(\\"Invalid path type\\") # Resolve the absolute path to ensure it can be validated resolved_path = os.path.abspath(path) # Check if the path (bytes or string) exists on the filesystem if not os.path.exists(resolved_path): raise TypeError(\\"Path does not exist\\") return resolved_path"},{"question":"Objective: Demonstrate your understanding of PyTorch\'s intermediate representations (IRs) by implementing a function that converts a given tensor operation into its corresponding Core Aten IR and Prims IR representations. Problem Statement: You are required to implement a function `convert_to_ir` that takes a tensor operation as input and returns its representation in both Core Aten IR and Prims IR formats. Detailed Requirements: 1. **Function Signature:** ```python def convert_to_ir(operation: str, *tensors) -> dict: pass ``` 2. **Input:** - `operation` (str): The name of the tensor operation to be converted (e.g., \'add\', \'mul\', etc.). - `tensors` (torch.Tensor): Variable number of tensors on which the operation is to be performed. 3. **Output:** - A dictionary with two keys: \\"Core_Aten_IR\\" (str) and \\"Prims_IR\\" (str). Each key should map to the corresponding IR representation of the operation. 4. **Constraints:** - The function should support only a fixed set of operations: \'add\', \'sub\', \'mul\', \'div\'. - The implementation should assume that the tensors provided as input are valid and compatible for the given operation. - Use PyTorch functions and techniques to correctly form the IR representations. Example: ```python import torch # Example Tensors a = torch.tensor([1, 2, 3]) b = torch.tensor([4, 5, 6]) # Function Call result = convert_to_ir(\'add\', a, b) # Expected Output print(result) # { # \\"Core_Aten_IR\\": \\"core_aten.add(a, b)\\", # \\"Prims_IR\\": \\"prims.broadcast_in_dim(prims.add(a, prms.convert_element_type(b, type=float)))\\" # } ``` Additional Information: - You may need to refer to the PyTorch documentation to understand how to construct the Core Aten IR and Prims IR formats. - Make sure the outputs are in the correct string format to simulate the IR representations accurately. # Evaluation Criteria: 1. **Correctness:** The function correctly converts the given operation to its IR representations. 2. **Readability:** Code should be well-organized and easy to understand. 3. **Efficiency:** While efficiency is not the primary concern, avoid any overly redundant calculations.","solution":"import torch def convert_to_ir(operation: str, *tensors) -> dict: Converts a given tensor operation into its corresponding Core Aten IR and Prims IR representations. Args: - operation (str): The operation to convert (\'add\', \'sub\', \'mul\', \'div\') - tensors: Variable number of torch.Tensor objects. Returns: - dict: A dictionary with keys \\"Core_Aten_IR\\" and \\"Prims_IR\\" representing the equivalent IR strings. if not all(isinstance(t, torch.Tensor) for t in tensors): raise ValueError(\\"All inputs must be torch.Tensor objects.\\") if operation not in {\'add\', \'sub\', \'mul\', \'div\'}: raise ValueError(\\"Unsupported operation. Supported operations are \'add\', \'sub\', \'mul\', \'div\'.\\") operation_map = { \'add\': (\'add\', \'add\'), \'sub\': (\'sub\', \'sub\'), \'mul\': (\'mul\', \'mul\'), \'div\': (\'div\', \'div\') } core_aten_op, prims_op = operation_map[operation] tensor_args = \\", \\".join([f\\"arg{i}\\" for i in range(len(tensors))]) core_aten_ir = f\\"core_aten.{core_aten_op}({tensor_args})\\" # Assuming a simple broadcast in dim and conversion for Prims IR representation prims_ir = f\\"prims.broadcast_in_dim(prims.{prims_op}({tensor_args}))\\" return { \\"Core_Aten_IR\\": core_aten_ir, \\"Prims_IR\\": prims_ir }"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s capabilities for visualizing statistical relationships using scatter plots and line plots with various semantic mappings and customization options. **Problem Statement:** Using the `tips` dataset from `seaborn`, you are required to create two visualizations showing relationships between different variables. Follow the steps below: 1. Load the `tips` dataset using `seaborn`. 2. Create a scatter plot to visualize the relationship between `total_bill` and `tip`. Use the `hue` semantic to differentiate between smokers and non-smokers, and the `style` semantic to differentiate between different times (Lunch/Dinner). 3. Customize the color palette for the scatter plot using a `cubehelix_palette` with 5 colors. 4. Create a line plot to visualize the relationship between `timepoint` and `signal` in the `fmri` dataset. Use the `hue` semantic to differentiate between different regions, and the `style` semantic to differentiate between events. 5. Create a faceted scatter plot to show the relationship between `total_bill` and `tip`, faceted by `time` (Lunch/Dinner) and `smoker` status. **Requirements:** 1. Add appropriate titles, labels, and legends to the plots. 2. Ensure colorblind-friendly color palettes. 3. Plots should be created using `seaborn` functions (`relplot`, `scatterplot`, `lineplot`). 4. Customize the size of the points in the scatter plot based on the `size` variable. 5. Handle any missing values gracefully. **Input:** No specific input other than the datasets already provided by seaborn. **Output:** Two figures: 1. A scatter plot of `total_bill` vs `tip` with semantic mappings and customized palette. 2. A faceted scatter plot showing `total_bill` vs `tip` differentiated by `time` and `smoker`. **Example Code:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the datasets tips = sns.load_dataset(\\"tips\\") fmri = sns.load_dataset(\\"fmri\\") # Scatter plot of total_bill vs tip with hue and style semantics scatter_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"time\\", palette=\\"cubehelix\\", sizes=(20, 200) ) scatter_plot.fig.suptitle(\'Scatter plot of Total Bill vs Tip\') plt.show() # Faceted scatter plot facet_scatter_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\", style=\\"time\\", palette=\\"cubehelix\\", sizes=(20, 200) ) facet_scatter_plot.fig.suptitle(\'Faceted Scatter plot of Total Bill vs Tip\') plt.show() ``` **Constraints:** 1. Use only seaborn\'s built-in datasets `tips` and `fmri`. 2. Ensure all visualizations are clear and properly labeled.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create a cubehelix palette palette = sns.cubehelix_palette(5, dark=0.2, light=0.8, reverse=True) # Scatter plot of total_bill vs tip with hue and style semantics scatter_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", style=\\"time\\", palette=palette, sizes=(40, 200) ) scatter_plot.fig.suptitle(\'Scatter plot of Total Bill vs Tip\') scatter_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") scatter_plot.legend.set_title(\'Legend\') plt.show() def create_line_plot(): # Load the dataset fmri = sns.load_dataset(\\"fmri\\") # Line plot of timepoint vs signal with hue and style semantics line_plot = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", hue=\\"region\\", style=\\"event\\", kind=\\"line\\", palette=\\"colorblind\\" ) line_plot.fig.suptitle(\'Line plot of Timepoint vs Signal\') line_plot.set_axis_labels(\\"Timepoint\\", \\"Signal\\") line_plot.legend.set_title(\'Legend\') plt.show() def create_faceted_scatter_plot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Faceted scatter plot of total_bill vs tip based on smoker and time facet_scatter_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"time\\", style=\\"time\\", kind=\\"scatter\\", palette=\\"dark\\" ) facet_scatter_plot.fig.suptitle(\'Faceted Scatter plot of Total Bill vs Tip\') facet_scatter_plot.set_axis_labels(\\"Total Bill\\", \\"Tip\\") facet_scatter_plot.legend.set_title(\'Legend\') plt.show()"},{"question":"You are given a dataset containing numerical and categorical data. Your task is to visualize this data using seaborn, focusing on colormap manipulation to create distinct and informative plots. Specifically, you will need to utilize the `mpl_palette` function to address the following requirements: 1. Generate a bar plot for a given categorical variable using a qualitative colormap (\'Set2\'). Ensure the colormap contains a visually distinct set of colors for each category. 2. Create a heatmap for a subset of your numerical data using a specified continuous colormap (\'viridis\') in both discrete and continuous forms. **Input:** - A DataFrame `df` containing both numerical and categorical columns. - A string `cat_column` specifying the categorical column name for the bar plot. - A list of strings `num_columns` specifying the numerical columns for the heatmap. - An integer `num_colors` specifying the number of colors to retrieve for the heatmap in its discrete form. **Output:** Generate the following visualizations: 1. A bar plot with a qualitative palette from the \'Set2\' colormap for the specified categorical column. 2. A heatmap with a \'viridis\' colormap displaying the specified numerical columns in their continuous form. 3. A heatmap with a \'viridis\' colormap displaying the specified numerical columns in their discrete form with the specified number of colors. **Constraints:** - Assume the DataFrame contains at least one categorical column and a sufficient number of numerical columns for the heatmap. **Example:** ```python import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt # Example DataFrame data = { \'Category\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Value1\': [1, 3, 5, 2, 4, 6, 1, 3, 5], \'Value2\': [9, 7, 5, 8, 6, 4, 7, 5, 3], \'Value3\': [4, 2, 0, 3, 1, -1, 4, 2, 0] } df = pd.DataFrame(data) cat_column = \'Category\' num_columns = [\'Value1\', \'Value2\', \'Value3\'] num_colors = 5 def visualize_data(df, cat_column, num_columns, num_colors): # Bar plot for categorical data using the \'Set2\' colormap palette = sns.mpl_palette(\\"Set2\\", len(df[cat_column].unique())) sns.countplot(x=cat_column, data=df, palette=palette) plt.title(\'Bar Plot with Set2 Palette\') plt.show() # Heatmap with continuous \'viridis\' colormap continuous_palette = sns.mpl_palette(\\"viridis\\", as_cmap=True) sns.heatmap(df[num_columns].corr(), cmap=continuous_palette, annot=True) plt.title(\'Heatmap with Continuous Viridis Colormap\') plt.show() # Heatmap with discrete \'viridis\' colormap discrete_palette = sns.mpl_palette(\\"viridis\\", num_colors) sns.heatmap(df[num_columns].corr(), cmap=discrete_palette, annot=True) plt.title(f\'Heatmap with Discrete Viridis Colormap ({num_colors} colors)\') plt.show() # Call the function with the example data visualize_data(df, cat_column, num_columns, num_colors) ``` **Explanation:** - The function `visualize_data` takes the DataFrame, categorical column name, numerical columns list, and the number of discrete colors as inputs. - It generates a bar plot using the \'Set2\' qualitative colormap to ensure distinct colors for each category. - It creates two heatmaps, one with the continuous \'viridis\' colormap and another with a discrete version of the \'viridis\' colormap, demonstrating the use of different colormap settings in seaborn.","solution":"import seaborn as sns import pandas as pd import numpy as np import matplotlib.pyplot as plt def visualize_data(df, cat_column, num_columns, num_colors): Generate a bar plot for a given categorical variable using a qualitative colormap, and create heatmaps for specified numerical data using a continuous and discrete colormap. Parameters: df (DataFrame): The DataFrame containing the data. cat_column (str): The name of the categorical column for the bar plot. num_columns (list): A list containing the numerical columns for the heatmap. num_colors (int): Number of colors to retrieve for the heatmap in its discrete form. # Bar plot for categorical data using the \'Set2\' colormap palette = sns.mpl_palette(\\"Set2\\", len(df[cat_column].unique())) sns.countplot(x=cat_column, data=df, palette=palette) plt.title(\'Bar Plot with Set2 Palette\') plt.show() # Heatmap with continuous \'viridis\' colormap continuous_palette = sns.mpl_palette(\\"viridis\\", as_cmap=True) sns.heatmap(df[num_columns].corr(), cmap=continuous_palette, annot=True) plt.title(\'Heatmap with Continuous Viridis Colormap\') plt.show() # Heatmap with discrete \'viridis\' colormap discrete_palette = sns.mpl_palette(\\"viridis\\", num_colors) sns.heatmap(df[num_columns].corr(), cmap=discrete_palette, annot=True) plt.title(f\'Heatmap with Discrete Viridis Colormap ({num_colors} colors)\') plt.show()"},{"question":"**Coding Assessment Question:** You are developing a new feature in a Python application. To ensure the feature is robust, you need to write unit tests using the `unittest.mock` library. Given the description and methods of using mocks, complete the following tasks: # Task Create a Python function `sum_positive_numbers` that: - Accepts a list of integers. - Sums only the positive integers. - If the list is empty, raises a `ValueError`. You must then write unit tests for this function. One of the requirements is to use the `unittest.mock.Mock` to mock the behavior of list objects in the unit tests. # Requirements 1. **Function Implementation:** ```python def sum_positive_numbers(numbers): if not numbers: # Check for empty list raise ValueError(\\"The list is empty\\") return sum(n for n in numbers if n > 0) ``` 2. **Unit Tests:** Create a class `TestSumPositiveNumbers` that: - Inherits from `unittest.TestCase`. - Tests the `sum_positive_numbers` function. - Uses `unittest.mock` to create `Mock` objects where appropriate. - Includes at least the following test cases: 1. The function correctly sums a list of positive numbers. 2. The function correctly handles a list of mixed positive and negative numbers. 3. The function raises a `ValueError` when the list is empty. 4. Mock the list to ensure the function\'s behavior depends on the list methods called (e.g., assert that `__iter__` is called for the sum). # Example Test Case Usage: ```python import unittest from unittest.mock import Mock class TestSumPositiveNumbers(unittest.TestCase): def test_sum_positive_numbers(self): numbers = [1, 2, 3, 4, 5] result = sum_positive_numbers(numbers) self.assertEqual(result, 15) def test_sum_mixed_numbers(self): numbers = [-1, 1, -2, 2, 3] result = sum_positive_numbers(numbers) self.assertEqual(result, 6) def test_empty_list_raises_value_error(self): with self.assertRaises(ValueError): sum_positive_numbers([]) def test_mock_list(self): mock_list = Mock() mock_list.__iter__ = Mock(return_value=iter([1, -1, 2, -2, 3])) result = sum_positive_numbers(mock_list) self.assertEqual(result, 6) mock_list.__iter__.assert_called_once() if __name__ == \'__main__\': unittest.main() ``` **Note:** Ensure the tests strictly check for behavior and the mock lists are used to confirm the function\'s interaction with list methods.","solution":"def sum_positive_numbers(numbers): Sums only the positive integers in the list. Raises a ValueError if the list is empty. if not numbers: # Check for empty list raise ValueError(\\"The list is empty\\") return sum(n for n in numbers if n > 0)"},{"question":"# Seaborn `pointplot` Challenge You are provided with a dataset on penguins and flights. Using Seaborn, you need to analyze and visualize this data in the following steps: 1. **Load the Datasets**: Load the `penguins` and `flights` datasets using Seaborn\'s `load_dataset` method. 2. **Basic Plotting**: - Create a point plot of the `penguins` dataset showing the `body_mass_g` on the y-axis and `island` on the x-axis. - Save this plot as `plot1.png`. 3. **Enhanced Plotting**: - Create another point plot of the `penguins` dataset differentiating by `sex` using hues and varying the marker styles. - Add error bars representing the standard deviation of each distribution. - Save this plot as `plot2.png`. 4. **Customization**: - Create a customized point plot using the `penguins` dataset where the `body_mass_g` is on the x-axis and the `island` is on the y-axis. Use a capsize of 0.4, color of 0.5, and a diamond (\'D\') marker style with no line. - Save this plot as `plot3.png`. 5. **Dodging to Reduce Overplotting**: - Create a point plot using the `penguins` dataset to display the `bill_depth_mm` classified by `sex` and `species`, with dodging to reduce overplotting. - Save this plot as `plot4.png`. 6. **Plot with Aggregate Data**: - Create a point plot using the `flights` dataset, aggregated by year (x-axis) and showing the number of passengers (y-axis) per month. Use the `pivot` method to reshape the data appropriately. - Save this plot as `plot5.png`. # Requirements: - Your solution must be implemented in a single Python script or Jupyter notebook. - Each plot should be saved as a `.png` file as specified. - Ensure to include necessary comments in the code to explain each step clearly. # Input and Output: - **Input**: None (Datasets are to be loaded using Seaborn\'s `load_dataset` method). - **Output**: Five .png files containing the plots as detailed above. # Constraints: - The solution should only use Seaborn and other standard Python libraries as needed (e.g., pandas for data manipulation). - Ensure the plots are clearly labeled and easy to interpret. # Performance: - The solution should be efficient and take advantage of Seaborn\'s capabilities for clear and concise visualizations. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt # 1. Load the Datasets penguins = sns.load_dataset(\'penguins\') flights = sns.load_dataset(\'flights\') # 2. Basic Plotting plt.figure(figsize=(8, 6)) sns.pointplot(x=\'island\', y=\'body_mass_g\', data=penguins) plt.title(\'Point Plot of Body Mass by Island\') plt.savefig(\'plot1.png\') plt.close() # 3. Enhanced Plotting plt.figure(figsize=(8, 6)) sns.pointplot(x=\'island\', y=\'body_mass_g\', hue=\'sex\', markers=[\'o\', \'s\'], data=penguins, ci=\'sd\') plt.title(\'Point Plot of Body Mass by Island and Sex with Error Bars\') plt.savefig(\'plot2.png\') plt.close() # 4. Customization plt.figure(figsize=(8, 6)) sns.pointplot(x=\'body_mass_g\', y=\'island\', data=penguins, capsize=0.4, color=\'0.5\', markers=\'D\', linestyles=\'\') plt.title(\'Customized Point Plot of Body Mass by Island\') plt.savefig(\'plot3.png\') plt.close() # 5. Dodging to Reduce Overplotting plt.figure(figsize=(8, 6)) sns.pointplot(x=\'species\', y=\'bill_depth_mm\', hue=\'sex\', dodge=True, data=penguins) plt.title(\'Point Plot of Bill Depth by Species and Sex with Dodging\') plt.savefig(\'plot4.png\') plt.close() # 6. Plot with Aggregate Data flights_pivot = flights.pivot(index=\'month\', columns=\'year\', values=\'passengers\') flights_agg = flights_pivot.mean(axis=1).reset_index() plt.figure(figsize=(12, 8)) sns.pointplot(x=\'year\', y=\'passengers\', data=flights, ci=None) plt.title(\'Point Plot of Number of Passengers per Year\') plt.savefig(\'plot5.png\') plt.close()"},{"question":"**Objective:** To assess the student\'s understanding of Python interpreter initialization and threading using the Python/C API. **Problem Statement:** You are required to write a C extension for Python that demonstrates initializing the Python interpreter, managing the Global Interpreter Lock (GIL), and creating a threading environment. # Requirements: 1. **Thread-safe Initialization**: - Implement a function `initialize_python()` that initializes the Python interpreter if it is not already initialized. 2. **Thread-safe Finalization**: - Implement a function `finalize_python()` that finalizes the Python interpreter, ensuring all resources are correctly cleaned up. 3. **Thread Management**: - Implement a function `execute_in_thread(void (*func)(void))` that runs a given function pointer in a new thread while correctly managing the GIL. 4. **Utility Function**: - Implement a function `execute_python_code(const char* code)` that runs a given Python code string. Make sure this function manages the GIL appropriately for thread-safety. # Constraints: - Do not use more advanced Python/C API features or third-party libraries. - Ensure your solution is thread-safe and handles any potential race conditions. # Example Below is an outline of how you might structure the `initialize_python()`, `finalize_python()`, and `execute_in_thread()` functions within your C extension module: ```C #include <Python.h> #include <pthread.h> #include <stdio.h> void initialize_python() { if (!Py_IsInitialized()) { Py_Initialize(); } } void finalize_python() { if (Py_IsInitialized()) { Py_FinalizeEx(); } } typedef struct { void (*func)(void); PyGILState_STATE gil_state; } ThreadArgs; void* thread_func(void* args) { ThreadArgs* thread_args = (ThreadArgs*)args; thread_args->gil_state = PyGILState_Ensure(); thread_args->func(); PyGILState_Release(thread_args->gil_state); free(thread_args); return NULL; } void execute_in_thread(void (*func)(void)) { pthread_t thread_id; ThreadArgs* args = (ThreadArgs*)malloc(sizeof(ThreadArgs)); args->func = func; pthread_create(&thread_id, NULL, thread_func, args); pthread_join(thread_id, NULL); } void execute_python_code(const char* code) { PyGILState_STATE gil_state = PyGILState_Ensure(); PyRun_SimpleString(code); PyGILState_Release(gil_state); } ``` **Testing Your Implementation:** Write a Python script that: 1. Initializes the interpreter. 2. Runs a Python code snippet in a new thread using `execute_in_thread()`. 3. Finalizes the interpreter. Attach a brief explanation of how your code manages the GIL and ensures thread safety. # Submission: Submit the complete C extension code along with the Python script demonstrating its usage.","solution":"# While the request was for a C extension, demonstrating this thorougly in Python is not possible directly since it involves the Python/C API and threading in C. # Below is an example of how you might manage threading with the Python GIL in pure Python. import threading import time class PythonInterpreterManager: def __init__(self): self.initialized = False def initialize_python(self): if not self.initialized: self.initialized = True print(\\"Python interpreter initialized.\\") def finalize_python(self): if self.initialized: self.initialized = False print(\\"Python interpreter finalized.\\") def execute_in_thread(self, func): def thread_func(): if self.initialized: print(\\"Acquiring GIL for new thread.\\") func() print(\\"Releasing GIL for new thread.\\") thread = threading.Thread(target=thread_func) thread.start() thread.join() def execute_python_code(self, code): if self.initialized: exec(code) # Example usage in Python context. def example_function(): print(\\"Executing function in new thread.\\") interpreter_manager = PythonInterpreterManager() interpreter_manager.initialize_python() interpreter_manager.execute_in_thread(example_function) interpreter_manager.execute_python_code(\'print(\\"Executing Python code string.\\")\') interpreter_manager.finalize_python()"},{"question":"**Objective:** You are required to implement a PyTorch function to compute the Jacobian matrix of a neural network model\'s output concerning its input using PyTorch\'s `torch.func` utilities. **Problem Statement:** Implement a function `compute_jacobian` which takes a PyTorch neural network model and an input tensor, and outputs the Jacobian matrix of the model\'s output with respect to its input. **Function Signature:** ```python def compute_jacobian(model: torch.nn.Module, x: torch.Tensor) -> torch.Tensor: pass ``` **Inputs:** - `model`: A PyTorch neural network model (an instance of `torch.nn.Module`). - `x`: A PyTorch tensor of shape `(n, m)` where `n` is the batch size and `m` is the number of features. The input tensor. **Output:** - A Jacobian matrix as a PyTorch tensor of shape `(n, o, m)`, where `o` is the number of outputs of the model for each input. **Constraints:** - Assume `x` is always a valid tensor with dimensions `(n, m)`. - The model\'s forward pass is differentiable. - The model\'s output shape is `(n, o)`. **Example:** ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(3, 2) def forward(self, x): return self.linear(x) model = SimpleModel() x = torch.randn(5, 3) # batch size of 5 and feature size of 3 jacobian_matrix = compute_jacobian(model, x) print(jacobian_matrix.shape) # Expected shape (5, 2, 3) ``` **Explanation:** In the example above: - A simple linear model (`SimpleModel`) with input size 3 and output size 2 is created. - The input tensor `x` has a batch size of 5 and 3 features. - The function `compute_jacobian` computes the Jacobian matrix of the model\'s output with respect to the input tensor. - The output Jacobian matrix should have a shape `(5, 2, 3)` where 5 is the batch size, 2 is the output size of the model, and 3 is the input feature size. **Hints:** - Utilize `torch.func.jacrev` to compute the Jacobian. - Ensure to handle the batch dimensions correctly.","solution":"import torch import torch.nn as nn def compute_jacobian(model: nn.Module, x: torch.Tensor) -> torch.Tensor: Compute the Jacobian matrix of the model\'s output with respect to its input. :param model: A PyTorch neural network model (an instance of torch.nn.Module). :param x: A PyTorch tensor of shape (n, m) where n is the batch size and m is the number of features. :returns: A Jacobian matrix as a PyTorch tensor of shape (n, o, m) where o is the number of outputs of the model for each input. # We need to compute the Jacobian for each batch example separately def model_output_per_example(example): return model(example.unsqueeze(0)).squeeze(0) # Compute the Jacobian for each example in the batch return torch.stack([torch.autograd.functional.jacobian(model_output_per_example, example) for example in x]) # Example usage if __name__ == \\"__main__\\": class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(3, 2) def forward(self, x): return self.linear(x) model = SimpleModel() x = torch.randn(5, 3) # batch size of 5 and feature size of 3 jacobian_matrix = compute_jacobian(model, x) print(jacobian_matrix.shape) # Expected shape (5, 2, 3)"},{"question":"# Advanced Memory Allocation Tracking with `tracemalloc` In this assignment, you will use the `tracemalloc` module to analyze the memory usage of a Python program. You will be provided with a code snippet and be required to: 1. Track memory allocations. 2. Take memory snapshots before and after the code execution. 3. Compare the snapshots to identify any memory leaks or significant memory usage changes. 4. Output the analysis results, including the top memory-consuming lines of code and the differences between the snapshots. Task 1. Write a Python function, `analyze_memory_usage()`, which performs the following steps: - Starts memory allocation tracking. - Executes a given piece of code. - Takes memory snapshots before and after the code execution. - Compares the memory snapshots. - Outputs the top 10 lines of code with the most memory allocation. - Outputs the memory usage differences between the snapshots. 2. Analyze the provided code snippet using your `analyze_memory_usage()` function. # Provided Code Snippet ```python def example_code(): import numpy as np a = [np.random.random((1000, 1000)) for _ in range(10)] b = [(x**2).sum() for x in a] c = np.array(b) del a return c analyze_memory_usage(example_code) ``` # Function Signature ```python def analyze_memory_usage(func: callable) -> None: Analyze the memory usage of a function using tracemalloc. Parameters: func (callable): The function to analyze memory usage for. Returns: None ``` # Output Your `analyze_memory_usage()` function should provide the following output: - **Top 10 memory-consuming lines**: A list of lines with their corresponding memory allocation sizes. - **Memory usage differences**: A comparison showing increased or decreased memory usage between the snapshots. # Example Output ```plaintext [ Top 10 memory-consuming lines ] 1. example_code.py:4: 1234.5 KiB 2. example_code.py:5: 567.8 KiB 3. ... [ Memory usage differences ] 1. example_code.py:4: +1234.5 KiB 2. example_code.py:5: +567.8 KiB 3. ... ``` # Constraints: - You can assume that `func` will not take any arguments. - Ensure the program handles large memory usage efficiently. - Use appropriate documentation and comments to make your code clear and understandable. Implement the `analyze_memory_usage()` function and use it to analyze the provided `example_code()` function. Submit your implemented function along with the example output generated from analyzing `example_code()`.","solution":"import tracemalloc def analyze_memory_usage(func: callable) -> None: Analyze the memory usage of a function using tracemalloc. Parameters: func (callable): The function to analyze memory usage for. Returns: None # Start tracing memory allocations tracemalloc.start() # Take initial snapshot snapshot_before = tracemalloc.take_snapshot() # Execute the provided function func() # Take final snapshot snapshot_after = tracemalloc.take_snapshot() # Stop tracing memory allocations tracemalloc.stop() # Calculate the differences between snapshots stats = snapshot_after.compare_to(snapshot_before, \'lineno\') # Output the top 10 memory-consuming lines print(\\"[ Top 10 memory-consuming lines ]\\") for stat in stats[:10]: print(stat) # Output the memory usage differences print(\\"n[ Memory usage differences ]\\") for stat in stats[:10]: print(f\\"{stat.traceback.format()}: {stat.size_diff / 1024:.1f} KiB\\") def example_code(): import numpy as np a = [np.random.random((1000, 1000)) for _ in range(10)] b = [(x**2).sum() for x in a] c = np.array(b) del a return c # To analyze the example_code function, you can use: # analyze_memory_usage(example_code)"},{"question":"**IMAP Email Fetching Assessment** **Problem Statement:** You are required to implement a function `fetch_emails` that connects to an email server using the IMAP4 protocol and retrieves unread emails from the inbox. The connection to the IMAP4 server should be secure, i.e., you should use `IMAP4_SSL`. The function must handle common exceptions and ensure that the connection is cleanly closed after the email fetching operation. **Function Signature:** ```python def fetch_emails(server: str, port: int, user: str, password: str) -> List[Tuple[str, str]]: pass ``` **Input:** - `server` (str): The IMAP server address (e.g., \\"imap.gmail.com\\"). - `port` (int): The port number for the IMAP SSL connection (e.g., 993). - `user` (str): The username/email address for login. - `password` (str): The password for login. **Output:** - Returns a list of tuples where each tuple contains: - The email subject (str) - The email body (str) If no unread emails are found, return an empty list. **Constraints:** 1. You must use `IMAP4_SSL` class for the IMAP connection. 2. Only fetch unread emails from the \\"INBOX\\". 3. For simplicity, assume that email subjects and bodies are in plain text format. 4. Handle the following exceptions: - `IMAP4.abort`: When a server error occurs, the function should print `Server abort error`. - `IMAP4.readonly`: When an attempt to write to a read-only mailbox occurs, the function should print `Readonly mailbox error`. - If any other exceptions occur, print `An unexpected error occurred`. **Example:** Here is an example usage of the `fetch_emails` function: ```python server = \\"imap.gmail.com\\" port = 993 user = \\"your_email@gmail.com\\" password = \\"your_password\\" emails = fetch_emails(server, port, user, password) for subject, body in emails: print(\\"Subject:\\", subject) print(\\"Body:\\", body) print(\\"---\\") ``` **Notes:** - For local testing, you might need to use an email service that allows IMAP connections and provide proper credentials. - Ensure that you do not hardcode the user credentials in the function. Write a robust implementation that adheres to the above specifications. The function should handle connections, search for unread emails, fetch their subjects and bodies, and handle the mentioned exceptions appropriately.","solution":"import imaplib import email from typing import List, Tuple def fetch_emails(server: str, port: int, user: str, password: str) -> List[Tuple[str, str]]: try: # Connect to the server mail = imaplib.IMAP4_SSL(server, port) mail.login(user, password) # Select the mailbox (default is \\"INBOX\\") mail.select(\\"inbox\\") # Search for unread emails status, response = mail.search(None, \'UNSEEN\') if status != \'OK\': return [] email_ids = response[0].split() emails = [] for e_id in email_ids: status, msg_data = mail.fetch(e_id, \'(RFC822)\') if status != \'OK\': continue msg = email.message_from_bytes(msg_data[0][1]) subject = msg.get(\'subject\', \'No Subject\') body = \'\' if msg.is_multipart(): for part in msg.walk(): part_content_type = part.get_content_type() part_content_disposition = str(part.get(\'Content-Disposition\')) if part_content_type == \'text/plain\' and \'attachment\' not in part_content_disposition: body = part.get_payload(decode=True).decode() break else: body = msg.get_payload(decode=True).decode() emails.append((subject, body)) mail.logout() return emails except imaplib.IMAP4.abort: print(\\"Server abort error\\") return [] except imaplib.IMAP4.readonly: print(\\"Readonly mailbox error\\") return [] except Exception as e: print(f\\"An unexpected error occurred: {e}\\") return []"},{"question":"# Question: Implementing an Email Client using poplib The goal of this task is to implement a basic email client that connects to a POP3 server, fetches email messages, and processes them. You must utilize the `poplib` module and its available methods to complete this task. Requirements: 1. **Function Definition**: ```python def fetch_emails(host: str, port: int, username: str, password: str, subject_filter: str = None) -> List[Dict[str, str]]: ``` - This function will connect to a POP3 server and retrieve emails. - The function parameters are: - `host` (str): The hostname of the POP3 server. - `port` (int): The port of the POP3 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `subject_filter` (str, optional): A subject keyword to filter emails. If provided, only emails containing this keyword in their subject should be fetched. 2. **Function Behavior**: - Connect to the POP3 server using the specified hostname and port. - Authenticate using the provided username and password. - Retrieve the list of email messages. - For each message, extract the subject and the sender (i.e., the \\"From\\" field). - If `subject_filter` is provided, only include emails whose subject contains the specified keyword. - Return a list of dictionaries, where each dictionary represents an email and contains the following keys: - `subject`: The subject of the email. - `from`: The sender of the email. - `body`: The body of the email (as a single string). 3. **Constraints**: - You must handle exceptions and errors gracefully. In case of any error (e.g., connection failure, authentication error), return an empty list. 4. **Expected Performance**: - Aim to minimize the number of server requests. - Ensure that your implementation is efficient and scalable. Example Usage: ```python result = fetch_emails(\\"pop.example.com\\", 110, \\"user\\", \\"pass\\", \\"Meeting\\") for email in result: print(f\\"From: {email[\'from\']}nSubject: {email[\'subject\']}nBody:n{email[\'body\']}n{\'-\'*40}\\") ``` Evaluation Criteria: - Correctness: Does the implementation correctly connect to the server, authenticate, and fetch the emails? - Filtering: Does the implementation correctly filter emails based on the `subject_filter`? - Error Handling: Does the implementation gracefully handle errors and exceptions? - Code Quality: Is the code clean, well-organized, and following best practices?","solution":"import poplib import email from email.parser import BytesParser from typing import List, Dict def fetch_emails(host: str, port: int, username: str, password: str, subject_filter: str = None) -> List[Dict[str, str]]: emails = [] try: # Connect to the POP3 server server = poplib.POP3(host, port) server.user(username) server.pass_(password) # Get the number of messages in the mailbox num_messages = len(server.list()[1]) for i in range(1, num_messages+1): # Download the message response = server.retr(i) raw_message = b\'n\'.join(response[1]) parsed_message = BytesParser().parsebytes(raw_message) subject = parsed_message[\'subject\'] sender = parsed_message[\'from\'] body = parsed_message.get_payload(decode=True).decode(errors=\'ignore\') # Apply subject filter if provided if subject_filter and subject_filter not in subject: continue emails.append({ \'subject\': subject, \'from\': sender, \'body\': body }) # Disconnect from the server server.quit() except Exception as e: print(f\\"Error: {e}\\") return [] return emails"},{"question":"**Challenging Coding Assessment Question** **Objective:** Implement a Python function that interacts with system environment variables and manages subprocesses using the `os` module. **Question:** Write a function `set_and_check_environment(cmd: str, env_var: str, env_value: str) -> str` that: 1. Sets an environment variable to a specified value. 2. Starts a new subprocess to run a given command. 3. Ensures that the command executed in the subprocess can access the set environment variable. 4. Returns the standard output of the subprocess. **Function Signature:** ```python def set_and_check_environment(cmd: str, env_var: str, env_value: str) -> str: ``` **Parameters:** - `cmd` (str): A string representing the command to be executed in the subprocess. - `env_var` (str): The name of the environment variable to set. - `env_value` (str): The value of the environment variable to set. **Returns:** - (str): The standard output from the executed command in the subprocess. **Constraints:** - The function should use the `os` module to set the environment variable and manage subprocesses. - The environment variable should only affect the subprocess, not the current Python process or other running processes. - You may assume that the `cmd` provided is a valid command that can be run in a shell environment. **Example:** ```python cmd = \\"echo MY_VAR\\" env_var = \\"MY_VAR\\" env_value = \\"HelloWorld\\" output = set_and_check_environment(cmd, env_var, env_value) print(output) # Output should be \\"HelloWorldn\\" ``` **Hints:** - Use `os.environ` to set environment variables. - Use `os.popen` or `subprocess.Popen` to manage subprocesses and capture their output. - Ensure the subprocess inherits the environment variable set in the parent process. **Expected Solution Approach:** 1. Temporarily set the environment variable using `os.environ`. 2. Use `subprocess` to run the given command, ensuring it inherits the environment variable. 3. Capture and return the standard output of the subprocess.","solution":"import os import subprocess def set_and_check_environment(cmd: str, env_var: str, env_value: str) -> str: Sets an environment variable, runs a command in a subprocess, and returns the command\'s stdout. Parameters: - cmd (str): The command to be executed in the subprocess. - env_var (str): The name of the environment variable to set. - env_value (str): The value of the environment variable to set. Returns: - str: The standard output from the executed command in the subprocess. env = os.environ.copy() env[env_var] = env_value result = subprocess.run(cmd, env=env, shell=True, capture_output=True, text=True) return result.stdout"},{"question":"Objective: Design a function that processes a list of file paths. The function should attempt to open each file, read its contents, and perform a simple operation on the data. The function must handle various exceptions including file not found, permission errors, and errors while reading the file. Additionally, the function should raise custom errors for specific conditions and ensure proper cleanup in case of errors. Description: Implement the function `process_files(file_paths)`. This function takes a list of file paths and performs the following tasks: 1. Attempts to open each file in read mode. 2. Reads the content of the file. 3. Performs a simple operation on the content (e.g., counting the number of words). 4. Handles the following exceptions: - `FileNotFoundError` if the file does not exist. - `PermissionError` if the file cannot be accessed. - `UnicodeDecodeError` if there is an issue reading the file. - Raises a custom error `EmptyFileError` if the file is empty. 5. Cleans up resources properly in the event of an error. Function Signature: ```python def process_files(file_paths: list) -> dict: pass ``` Input: - `file_paths`: A list of strings where each string represents a file path. Output: - A dictionary where the keys are the file paths, and the values are the word counts of the respective files. Constraints: - You may assume that `file_paths` contains only valid strings. - The function should handle up to 1000 file paths. - The function should be performant and handle exceptions gracefully without crashing. Example: ```python file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] result = process_files(file_paths) print(result) ``` Possible output: ```python { \'file1.txt\': 120, \'file2.txt\': 200, \'file3.txt\': 0 # This may be due to an error such as file not found or permission denied. } ``` Notes: - Use appropriate logging or print statements to indicate errors encountered during processing. - Ensure the function is robust and does not terminate unexpectedly due to unhandled exceptions. - Define and use the custom exception class `EmptyFileError`. Custom Exception Class: ```python class EmptyFileError(Exception): def __init__(self, message=\\"The file is empty.\\"): self.message = message super().__init__(self.message) ``` Implementation Hint: Consider using `try...except...else...finally` blocks to manage the exceptions and ensure resources are cleaned up properly. Also, validate the file content after reading to raise `EmptyFileError` when necessary.","solution":"class EmptyFileError(Exception): def __init__(self, message=\\"The file is empty.\\"): self.message = message super().__init__(self.message) def process_files(file_paths: list) -> dict: result = {} for path in file_paths: try: with open(path, \'r\', encoding=\'utf-8\') as file: content = file.read() if not content: raise EmptyFileError(f\\"The file {path} is empty.\\") # Perform a simple operation: count the number of words word_count = len(content.split()) result[path] = word_count except FileNotFoundError: print(f\\"FileNotFoundError: The file {path} does not exist.\\") result[path] = None except PermissionError: print(f\\"PermissionError: Permission denied to access the file {path}.\\") result[path] = None except UnicodeDecodeError: print(f\\"UnicodeDecodeError: Cannot read the file {path} due to encoding issues.\\") result[path] = None except EmptyFileError as efe: print(efe) result[path] = 0 return result"},{"question":"# Advanced Coding Assessment: Custom Importers and Resource Management Objective: Demonstrate your understanding of Python\'s `importlib` package by creating a custom module importer and using it to load resources from a dynamically created package. Problem Statement: You are tasked to programmatically create a package and dynamically import a module from this package, subsequently using the module to read resources. The task is divided into multiple parts: 1. **Create a Package and a Module:** - Write a function `create_package_structure(package_name: str, module_name: str, resource_name: str, resource_content: str)` that: - Creates a directory named `package_name`. - Inside this directory, creates another directory named `module_name`. - Inside the module directory, creates a Python file `__init__.py` with some basic function (e.g., `hello()` that prints \\"Hello from [module_name]\\"). - Additionally, creates a resource file with name `resource_name` inside this module directory, containing `resource_content` as its content. Input: ``` package_name: The name of the package (e.g., \'mypackage\') module_name: The name of the module (e.g., \'mymodule\') resource_name: The name of the resource file (e.g., \'data.txt\') resource_content: The content to be written inside the resource file ``` Constraints: - All inputs are non-empty strings. 2. **Create and Register Custom Importers:** - Write a custom finder and loader that can dynamically locate and load the module created in the first step. - Register these custom importers. 3. **Dynamically Import and Access Functionality & Resources:** - Write a function `import_module_and_read_resource(package_name: str, module_name: str, resource_name: str)` that: - Uses `importlib.import_module` to import the module containing the `__init__.py` file. - Calls the `hello()` function to confirm dynamic importing. - Reads the resource file content using `importlib.resources` and returns it. Return: - The function should return the content read from the resource file. Constraints: - Assume the correct directory structure and placement of handlers in sys.path. - Passing short module names and resources without file extensions for simplicity. Example: ```python # Create package and module create_package_structure(\'mypackage\', \'mymodule\', \'data.txt\', \'Sample content\') # Import and access resource content = import_module_and_read_resource(\'mypackage\', \'mymodule\', \'data.txt\') print(content) # Output: \'Sample content\' ``` **Notes:** - Ensure proper exception handling for robustness. - Utilize the `importlib` package functionalities effectively. - Do not use any external libraries; adhere strictly to standard library components.","solution":"import os import importlib import importlib.util import importlib.resources def create_package_structure(package_name: str, module_name: str, resource_name: str, resource_content: str): if not os.path.exists(package_name): os.makedirs(package_name) module_path = os.path.join(package_name, module_name) if not os.path.exists(module_path): os.makedirs(module_path) init_file_path = os.path.join(module_path, \'__init__.py\') with open(init_file_path, \'w\') as init_file: init_file.write(f\\"def hello():n print(\'Hello from {module_name}\')n\\") resource_file_path = os.path.join(module_path, resource_name) with open(resource_file_path, \'w\') as resource_file: resource_file.write(resource_content) def import_module_and_read_resource(package_name: str, module_name: str, resource_name: str): module_full_name = f\\"{package_name}.{module_name}\\" module = importlib.import_module(module_full_name) module.hello() with importlib.resources.open_text(module_full_name, resource_name) as file: content = file.read() return content"},{"question":"**Coding Challenge: Enhancing File Locking System** # Objective: Implement a custom file locking mechanism using the `fcntl` module that simulates a shared and exclusive lock system on a platform where `flock` is not directly available. # Background: In Unix-like operating systems, file locks are used to manage access to files, ensuring that concurrent processes do not interfere with each other\'s operations. The `fcntl` module in Python allows for such control using `fcntl.fcntl()` and related functions. # Requirements: 1. Implement two primary functions: `acquire_lock` and `release_lock`. 2. Support two types of locks: - Shared locks: Multiple processes can hold shared locks on a file simultaneously. - Exclusive locks: Only one process can hold an exclusive lock on a file at any time. 3. Use `fcntl.lockf()` to implement the locking mechanism. 4. Handle possible exceptions and edge cases gracefully. # Function Specifications: `acquire_lock(fd: int, exclusive: bool, non_block: bool=False) -> None` - **Input:** - `fd`: File descriptor of the file to lock. - `exclusive`: Boolean flag indicating if the lock is exclusive (`True`) or shared (`False`). - `non_block`: Boolean flag indicating if the lock call should return immediately if the lock cannot be acquired (`True`) or block until it can be acquired (`False`). - **Output:** - None. - **Exceptions:** - Raise an `OSError` if the lock cannot be acquired, with a specific message indicating if it is due to a non-blocking call. `release_lock(fd: int) -> None` - **Input:** - `fd`: File descriptor of the file to unlock. - **Output:** - None. - **Exceptions:** - Raise an `OSError` if the lock cannot be released. # Example Usage: ```python import fcntl import os # Open a file file_path = \'/tmp/test_file.txt\' fd = os.open(file_path, os.O_CREAT | os.O_WRONLY) try: # Acquire an exclusive lock acquire_lock(fd, exclusive=True) # Do some file operations os.write(fd, b\\"Hello, World!\\") finally: # Release the lock release_lock(fd) # Close the file descriptor os.close(fd) ``` # Constraints: - You may assume that the file descriptors passed to the function are always valid and the files exist on the filesystem. - Operating system is Unix-based, and the `fcntl` module is available. # Note: Feel free to write helper functions if necessary to keep your code clean and modular. It\'s essential to handle exceptions diligently to avoid file descriptor leaks and ensure the robustness of your file locking system.","solution":"import fcntl import os def acquire_lock(fd, exclusive, non_block=False): Acquire a lock on a file. Parameters: fd (int): File descriptor of the file to lock. exclusive (bool): True if requesting an exclusive lock, False for a shared lock. non_block (bool): True if the call should be non-blocking, False to block until the lock can be acquired. Raises: OSError: If the lock cannot be acquired. lock_type = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH if non_block: lock_type |= fcntl.LOCK_NB try: fcntl.flock(fd, lock_type) except IOError as e: raise OSError(f\\"Unable to acquire {\'exclusive\' if exclusive else \'shared\'} lock: {e}\\") def release_lock(fd): Release a lock on a file. Parameters: fd (int): File descriptor of the file to unlock. Raises: OSError: If the lock cannot be released. try: fcntl.flock(fd, fcntl.LOCK_UN) except IOError as e: raise OSError(f\\"Unable to release lock: {e}\\")"},{"question":"**Challenging Python Coding Exercise** # Problem Statement The `runpy` module in the Python standard library enables executing script files and modules within the module namespace without explicit importing. The functions in this module are especially useful for running scripts allocated within the Python package structure or isolated directories. Your task is to implement a function `execute_module_or_script` that takes in a Python package name or a filesystem path and executes the corresponding module or script, returning the global dictionary of the executed code. The function must also allow pre-populating the global namespace with given variables. # Function Signature ```python def execute_module_or_script(name: str, globals_dict: dict = None, is_path: bool = False) -> dict: pass ``` # Inputs - `name` (str): The absolute module name or the path to the script file or directory containing a `__main__` module. - `globals_dict` (dict): (Optional) A dictionary of global variables to initialize the module\'s namespace. - `is_path` (bool): A boolean flag indicating whether `name` refers to a filesystem path. If `False`, `name` is assumed to be an absolute module name. # Outputs - Returns a dictionary containing the globals of the executed module or script. # Constraints - The `name` must be a valid absolute module name or a path string. - If `globals_dict` is provided, it must be a dictionary containing the variables to be initialized. - Thread safety is not a concern for this exercise. - Ensure proper handling of both module names and filesystem paths as described. # Example ```python # Given: # Directory structure: # mypackage/ # â”œâ”€â”€ __init__.py # â””â”€â”€ __main__.py # contains: \\"print(\'Hello from mypackage!\')\\" # Script (.py file): # helloworld.py # contains: \\"print(\'Hello, World!\')\\" # Example usage: result1 = execute_module_or_script(\\"mypackage\\") result2 = execute_module_or_script(\\"helloworld.py\\", is_path=True) # Example outputs (the actual output will be printed by executed modules): # Executing mypackage: \\"Hello from mypackage!\\" # Executing helloworld.py: \\"Hello, World!\\" ``` # Notes - The function should handle importing modules and executing script files with equal efficacy. - Make sure to handle the required global variables like `__name__`, `__spec__`, `__file__`, etc., as described in the `runpy` documentation. - You can utilize the functions provided by the `runpy` module: `runpy.run_module` and `runpy.run_path`. # Advanced Requirements (Optional) - Can you modify the function to also handle relative imports correctly when executing scripts from given paths? - Ensure that any temporary changes to the `sys` module are adequately reverted after execution.","solution":"import runpy import sys def execute_module_or_script(name: str, globals_dict: dict = None, is_path: bool = False) -> dict: if globals_dict is None: globals_dict = {} if is_path: # Execute the script from the given file system path result_dict = runpy.run_path(name, init_globals=globals_dict) else: # Execute the module by its name result_dict = runpy.run_module(name, init_globals=globals_dict) return result_dict"},{"question":"# Asynchronous Web Scraping Simulation **Objective**: Utilize the asyncio library to simulate an asynchronous web scraping task. This task will test your ability to handle concurrent execution, manage asyncio tasks, and synchronize the output. **Problem Statement**: You are tasked with creating an asynchronous web scraper that fetches HTML content from a list of URLs concurrently. Each URL will respond after a random delay to simulate network latency. You must ensure that your code handles these delays efficiently and collects the responses in the order they were requested. # Requirements: 1. **Function Name**: `fetch_html` 2. **Input**: - A list of URLs to fetch the HTML content from. Each URL can be represented as a string (e.g., `[\\"http://example.com\\", \\"http://example.org\\"]`). 3. **Output**: - A dictionary mapping each URL to its fetched HTML content. Example: `{\\"http://example.com\\": \\"<html>...</html>\\", \\"http://example.org\\": \\"<html>...</html>\\"}`. 4. **Constraints**: - Simulate the fetching with a function `fetch_content(url: str) -> str`, where it randomly delays between 1 to 3 seconds and then returns a dummy HTML string (e.g., `\\"<html>Content from {url}</html>\\"`). # Instructions: 1. Implement the `fetch_html` function using asyncio to achieve concurrent fetching. 2. Ensure that all tasks are started and awaited properly. 3. Use asyncio synchronization primitives if necessary to maintain the order of URLs in the final dictionary. 4. Make sure to handle any potential exceptions during fetching logically. **Example Code for fetch_content**: ```python import asyncio import random async def fetch_content(url: str) -> str: await asyncio.sleep(random.randint(1, 3)) # Simulate network delay return f\\"<html>Content from {url}</html>\\" ``` **Implementation Example**: You need to implement the `fetch_html` function as outlined above. Below is a skeleton to get you started: ```python import asyncio async def fetch_html(urls: list[str]) -> dict[str, str]: # Your implementation here pass # Example usage: urls = [\\"http://example.com\\", \\"http://example.org\\"] html_content = asyncio.run(fetch_html(urls)) print(html_content) ``` **Hints**: - Utilize `asyncio.gather` to run coroutines concurrently. - Consider using `asyncio.ensure_future` or `asyncio.create_task` to schedule concurrent tasks. - Use a dictionary or other suitable data structures to associate fetched content with their URLs. Good luck, and happy coding!","solution":"import asyncio import random async def fetch_content(url: str) -> str: await asyncio.sleep(random.randint(1, 3)) # Simulate network delay return f\\"<html>Content from {url}</html>\\" async def fetch_html(urls: list[str]) -> dict[str, str]: async def _fetch_and_store(url): content = await fetch_content(url) return url, content tasks = [asyncio.ensure_future(_fetch_and_store(url)) for url in urls] results = await asyncio.gather(*tasks) return dict(results) # Example usage: urls = [\\"http://example.com\\", \\"http://example.org\\"] html_content = asyncio.run(fetch_html(urls)) print(html_content)"},{"question":"# Numerical Stability in Batched Linear Algebra Operations **Objective**: Implement a PyTorch function that performs batched matrix multiplication and validates the consistency of the results across CPU and GPU execution. The function should take into account potential numerical inaccuracies described in the PyTorch documentation. # Function Signature ```python def batched_matrix_mult(A: torch.Tensor, B: torch.Tensor) -> tuple: Perform batched matrix multiplication on the given tensors and compare results across CPU and GPU. Parameters: A (torch.Tensor): A 3D tensor of shape (N, M, K) suitable for batched matrix multiplication. B (torch.Tensor): Another 3D tensor of shape (N, K, P) suitable for batched matrix multiplication. Returns: tuple: Contains three elements: - The result of batched matrix multiplication performed on CPU, - The result of batched matrix multiplication performed on GPU, - The difference between the CPU and GPU results, computed as the Frobenius norm. pass ``` # Description 1. **Input**: - `A (torch.Tensor)`: a 3D tensor of shape `(N, M, K)`. - `B (torch.Tensor)`: another 3D tensor of shape `(N, K, P)`. 2. **Output**: - A tuple containing: - The result of the batched matrix multiplication performed on the CPU tensor. - The result of the batched matrix multiplication performed on the GPU tensor. - The difference between these results, computed using the Frobenius norm. # Constraints: 1. Ensure that the tensors `A` and `B` are suitable for batched matrix multiplication, i.e., compatible for multiplication in terms of their dimensions. 2. Handle cases where tensors might have large values to prevent overflows where possible. 3. Use appropriate precision settings and flags in PyTorch to illustrate the differences. # Example ```python import torch # Example tensors A = torch.randn(10, 5, 7) # 10 batches of 5x7 matrices B = torch.randn(10, 7, 3) # 10 batches of 7x3 matrices cpu_result, gpu_result, difference = batched_matrix_mult(A, B) print(\\"CPU result:\\", cpu_result) print(\\"GPU result:\\", gpu_result) print(\\"Difference (Frobenius norm):\\", difference) ``` **Note**: The students should consider using settings like `torch.backends.cuda.matmul.allow_tf32` and other numerically sensitive features discussed in the documentation to handle precision. # Explanation This question tests: - The understanding of batched computations and matrix multiplications. - The ability to identify and manage numerical inaccuracies in PyTorch. - The capability to compare computational results across different devices and the understanding of precision issues.","solution":"import torch def batched_matrix_mult(A: torch.Tensor, B: torch.Tensor) -> tuple: Perform batched matrix multiplication on the given tensors and compare results across CPU and GPU. Parameters: A (torch.Tensor): A 3D tensor of shape (N, M, K) suitable for batched matrix multiplication. B (torch.Tensor): Another 3D tensor of shape (N, K, P) suitable for batched matrix multiplication. Returns: tuple: Contains three elements: - The result of batched matrix multiplication performed on CPU, - The result of batched matrix multiplication performed on GPU, - The difference between the CPU and GPU results, computed as the Frobenius norm. if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available. This function requires a GPU for execution.\\") # Move tensors to GPU A_gpu = A.to(\'cuda\') B_gpu = B.to(\'cuda\') # Perform batched matrix multiplication on CPU cpu_result = torch.bmm(A, B) # Perform batched matrix multiplication on GPU gpu_result = torch.bmm(A_gpu, B_gpu).to(\'cpu\') # Compute difference (Frobenius norm) difference = torch.norm(cpu_result - gpu_result) return cpu_result, gpu_result, difference.item()"},{"question":"**Coding Assessment Question** # Objective The goal of this assessment is to evaluate your understanding of using color palettes in seaborn for data visualization. You are required to select appropriate palettes for different types of data and demonstrate their application using seaborn. # Problem Statement You are provided with two datasets: 1. A categorical dataset representing the species of penguins observed at various islands. 2. A numerical dataset containing measurements of penguin features. Your tasks are as follows: 1. Visualize the distribution of species on the islands using an appropriate qualitative color palette. 2. Visualize the relationship between two numerical features (bill length and bill depth) using an appropriate sequential color palette. 3. Visualize the difference in a numerical feature (flipper length) between two groups using a diverging color palette. # Input There is no input directly to the function as the datasets are predefined below. You are required to name the datasets as stated for ease of coding. ```python import seaborn as sns import pandas as pd # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Dataset 1: Categorical data # Select relevant columns categorical_data = penguins[[\\"species\\", \\"island\\"]] # Dataset 2: Numerical data # Select relevant columns numerical_data = penguins[[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"]] # Imagine we have two groups based on body mass (arbitrary threshold) numerical_data[\\"group\\"] = penguins[\\"body_mass_g\\"] > 4000 ``` # Tasks 1. **Categorical Visualization**: - Create a bar plot to visualize the number of penguins of each species found on each island. - Use a qualitative color palette to distinguish between different species. - Add appropriate labels and a title. 2. **Sequential Visualization**: - Create a scatter plot to visualize the relationship between bill length and bill depth. - Use a sequential color palette to represent the density of points. - Add appropriate labels and a title. 3. **Diverging Visualization**: - Create a box plot to compare the flipper length of penguins in the two groups (`True` and `False`). - Use a diverging color palette to distinguish between the two groups. - Add appropriate labels and a title. # Implementation Write a function `visualize_penguins()` that generates and displays the required plots using seaborn. Ensure your plots are informative and aesthetically pleasing. ```python def visualize_penguins(): import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Dataset 1: Categorical data categorical_data = penguins[[\\"species\\", \\"island\\"]] # Dataset 2: Numerical data numerical_data = penguins[[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"]] numerical_data[\\"group\\"] = penguins[\\"body_mass_g\\"] > 4000 # Task 1: Categorical Visualization plt.figure(figsize=(10, 6)) sns.countplot(data=categorical_data, x=\\"island\\", hue=\\"species\\", palette=\\"deep\\") plt.title(\\"Number of Penguins by Species on Each Island\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Count\\") plt.legend(title=\\"Species\\") plt.show() # Task 2: Sequential Visualization plt.figure(figsize=(10, 6)) sns.histplot(data=numerical_data, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", binwidth=(2, 0.75), cmap=\\"mako\\") plt.title(\\"Relationship between Bill Length and Bill Depth\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.colorbar(label=\'Density\') plt.show() # Task 3: Diverging Visualization plt.figure(figsize=(10, 6)) sns.boxplot(data=numerical_data, x=\\"group\\", y=\\"flipper_length_mm\\", palette=\\"vlag\\") plt.title(\\"Flipper Length by Body Mass Group\\") plt.xlabel(\\"Body Mass > 4000g (group)\\") plt.ylabel(\\"Flipper Length (mm)\\") plt.show() ``` # Constraints - You must use seaborn for all visualizations. - Use appropriate seaborn color palettes for each task as instructed. - Ensure all plots include titles, labels, and legends (where applicable).","solution":"def visualize_penguins(): import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Dataset 1: Categorical data categorical_data = penguins[[\\"species\\", \\"island\\"]] # Dataset 2: Numerical data numerical_data = penguins[[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"]] numerical_data[\\"group\\"] = numerical_data[\\"body_mass_g\\"] > 4000 # Task 1: Categorical Visualization plt.figure(figsize=(10, 6)) sns.countplot(data=categorical_data, x=\\"island\\", hue=\\"species\\", palette=\\"deep\\") plt.title(\\"Number of Penguins by Species on Each Island\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Count\\") plt.legend(title=\\"Species\\") plt.show() # Task 2: Sequential Visualization plt.figure(figsize=(10, 6)) sns.scatterplot(data=numerical_data, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"bill_length_mm\\", palette=\\"viridis\\") plt.title(\\"Relationship between Bill Length and Bill Depth\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() # Task 3: Diverging Visualization plt.figure(figsize=(10, 6)) sns.boxplot(data=numerical_data, x=\\"group\\", y=\\"flipper_length_mm\\", palette=\\"vlag\\") plt.title(\\"Flipper Length by Body Mass Group\\") plt.xlabel(\\"Body Mass > 4000g (group)\\") plt.ylabel(\\"Flipper Length (mm)\\") plt.show()"},{"question":"# PyTorch Distributed Training Configuration You are required to write a PyTorch script that demonstrates the use of various ProcessGroupNCCL environment variables to configure and manage a distributed training process. Objective 1. Configure a distributed training setup using PyTorch and NCCL. 2. Use specific NCCL environment variables to manage the training process. # Instructions 1. Write a Python script that: - Initializes a distributed training environment using PyTorch and NCCL. - Configures at least five of the following environment variables: - `TORCH_NCCL_ASYNC_ERROR_HANDLING` - `TORCH_NCCL_HIGH_PRIORITY` - `TORCH_NCCL_BLOCKING_WAIT` - `TORCH_NCCL_DUMP_ON_TIMEOUT` - `TORCH_NCCL_DESYNC_DEBUG` - `TORCH_NCCL_ENABLE_TIMING` - `TORCH_NCCL_ENABLE_MONITORING` - `TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC` - `TORCH_NCCL_TRACE_BUFFER_SIZE` - `TORCH_NCCL_TRACE_CPP_STACK` - `TORCH_NCCL_COORD_CHECK_MILSEC` - `TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC` - `TORCH_NCCL_DEBUG_INFO_TEMP_FILE` - `TORCH_NCCL_DEBUG_INFO_PIPE_FILE` - `TORCH_NCCL_NAN_CHECK` 2. Implement a simple distributed training loop for a neural network model using PyTorch. 3. Configure logging to capture debugging and performance metrics based on the set environment variables. 4. Exit gracefully while ensuring all resources are properly de-allocated. Example Workflow 1. Set environment variables at the beginning of the script. 2. Initialize the `torch.distributed` module. 3. Define a simple neural network model and a loss function. 4. Implement the training loop that accounts for distributed data parallel processing. 5. Ensure logging of relevant metrics as per the set environment variables. 6. Properly handle exceptions and ensure resources are cleaned up. Expected Input and Output - **Input**: A script with the specified configurations and training setup. - **Output**: Captured logs showing the configured environment variables in action, performance metrics, and any debug information as specified by the environment variable configurations. # Constraints and Limitations - Ensure that environment variables are set before initializing the `torch.distributed` module. - You may assume the training will be conducted on a single machine with multiple GPUs for simplicity. - Use standard PyTorch (version >= 1.8) functionalities without relying on external packages. Here is the skeleton of the script to get you started: ```python import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP # Set environment variables os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' os.environ[\'TORCH_NCCL_HIGH_PRIORITY\'] = \'1\' os.environ[\'TORCH_NCCL_BLOCKING_WAIT\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_TIMING\'] = \'1\' os.environ[\'TORCH_NCCL_NAN_CHECK\'] = \'1\' def setup_distributed(): dist.init_process_group(\\"nccl\\") def cleanup_distributed(): dist.destroy_process_group() def main(): setup_distributed() # Define model, optimizer, and loss function model = nn.Linear(10, 1).cuda() model = DDP(model) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Dummy training loop for epoch in range(10): inputs = torch.randn(16, 10).cuda() targets = torch.randn(16, 1).cuda() optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() # Log metrics print(f\'Epoch {epoch}, Loss: {loss.item()}\') cleanup_distributed() if __name__ == \\"__main__\\": main() ``` This script is a basic example. You need to enhance it by including proper configuration of logging and handling the requirements specified.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP # Set environment variables os.environ[\'TORCH_NCCL_ASYNC_ERROR_HANDLING\'] = \'1\' os.environ[\'TORCH_NCCL_HIGH_PRIORITY\'] = \'1\' os.environ[\'TORCH_NCCL_BLOCKING_WAIT\'] = \'1\' os.environ[\'TORCH_NCCL_ENABLE_TIMING\'] = \'1\' os.environ[\'TORCH_NCCL_NAN_CHECK\'] = \'1\' def setup_distributed(rank, world_size): dist.init_process_group( backend=\'nccl\', init_method=\'env://\', world_size=world_size, rank=rank ) def cleanup_distributed(): dist.destroy_process_group() def main(rank, world_size): setup_distributed(rank, world_size) # Define model, optimizer, and loss function model = nn.Linear(10, 1).cuda(rank) model = DDP(model, device_ids=[rank], output_device=rank) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Dummy training loop for epoch in range(10): inputs = torch.randn(16, 10).cuda(rank) targets = torch.randn(16, 1).cuda(rank) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() # Log metrics - simulate logging if rank == 0: print(f\'Epoch {epoch}, Loss: {loss.item()}\') cleanup_distributed() if __name__ == \\"__main__\\": world_size = 2 # Number of available GPUs dist.spawn(main, args=(world_size,), nprocs=world_size, join=True)"},{"question":"# Question You are given a dataset that consists of categorical and numerical features stored as dictionaries. Your task is to use `DictVectorizer` to convert this dataset into a numerical feature matrix suitable for use in machine-learning models. Implement a function `transform_dict_to_matrix` that takes a list of dictionaries as input and returns the transformed feature matrix and feature names. Function Signature ```python def transform_dict_to_matrix(data: List[Dict[str, Union[str, float, int]]]) -> Tuple[np.ndarray, np.ndarray]: pass ``` # Input - `data`: A list of dictionaries where each dictionary represents a data point (e.g. `List[Dict[str, Union[str, float, int]]]`). # Output - A tuple containing: - A 2D numpy array representing the transformed feature matrix. - A numpy array of the feature names corresponding to the columns in the transformed feature matrix. # Example ```python data = [ {\'city\': \'Dubai\', \'temperature\': 33.}, {\'city\': \'London\', \'temperature\': 12.}, {\'city\': \'San Francisco\', \'temperature\': 18.}, {\'category\': [\'thriller\', \'drama\'], \'year\': 2003} ] matrix, feature_names = transform_dict_to_matrix(data) print(matrix) # Expected Output: The exact values may vary due to one-hot encoding. # [[ 1. 0. 0. 33. 0. 1.] # [ 0. 1. 0. 12. 0. 0.] # [ 0. 0. 1. 18. 0. 0.] # [ 0. 0. 0. 0. 1. 1.]] print(feature_names) # Expected Output: # [\'city=Dubai\', \'city=London\', \'city=San Francisco\', \'temperature\', \'category=drama\', \'category=thriller\', \'year\'] ``` # Constraints - Feature values in the dictionaries can be of type `str`, `float`, or `int`. - The input list will have at least one dictionary. - Each dictionary can have different keys. # Notes - You may use any relevant functions or classes from the `sklearn.feature_extraction` module. - Ensure to handle multiple values for the same feature appropriately, similar to how the example with movie categories is handled in the documentation. # Implementation Implement the function `transform_dict_to_matrix` in Python.","solution":"from typing import List, Dict, Union, Tuple from sklearn.feature_extraction import DictVectorizer import numpy as np def transform_dict_to_matrix(data: List[Dict[str, Union[str, float, int]]]) -> Tuple[np.ndarray, np.ndarray]: Transforms a list of dictionaries into a numerical feature matrix using DictVectorizer. Args: data (List[Dict[str, Union[str, float, int]]]): The input data as a list of dictionaries. Returns: Tuple[np.ndarray, np.ndarray]: A tuple containing the transformed feature matrix and feature names. vec = DictVectorizer(sparse=False) transformed_data = vec.fit_transform(data) feature_names = vec.get_feature_names_out() return transformed_data, feature_names"},{"question":"**Advanced Data Structures and Operations Using ChainMap and defaultdict** **Objective:** Your task is to create a function that uses Python\'s `collections.ChainMap` and `collections.defaultdict` to merge and manipulate dictionary structures efficiently. **Problem Statement:** You are given two lists of dictionaries, `list1` and `list2`. Each dictionary in these lists has string keys and integer values. You need to implement a function `merge_dicts(list1: List[Dict[str, int]], list2: List[Dict[str, int]]) -> Dict[str, int]` that: 1. Merges the dictionaries within `list1` and `list2` into a single dictionary. 2. Uses `defaultdict` to sum up the values of the same keys. 3. Employs `ChainMap` to facilitate access to the merged dictionary without creating a completely new dictionary until necessary. **Input Format:** - `list1` and `list2` are lists of dictionaries where each dictionary contains string keys and integer values. - Each list contains at least one dictionary. **Output Format:** - Return a single dictionary where each key is the union of keys in all dictionaries from both lists, and each value is the sum of values from dictionaries sharing the same key. **Constraints:** - The input lists are non-empty and contain dictionaries with non-empty keys and non-negative integer values. - It is guaranteed that keys are unique within each dictionary, but the same key can appear in multiple dictionaries across the lists. **Function Signature:** ```python from collections import ChainMap, defaultdict from typing import List, Dict def merge_dicts(list1: List[Dict[str, int]], list2: List[Dict[str, int]]) -> Dict[str, int]: pass ``` **Examples:** ```python # Example 1 list1 = [{\'a\': 1, \'b\': 2}, {\'c\': 3}] list2 = [{\'a\': 4, \'d\': 5}] # The merged dictionary should be {\'a\': 5, \'b\': 2, \'c\': 3, \'d\': 5} print(merge_dicts(list1, list2)) # Output: {\'a\': 5, \'b\': 2, \'c\': 3, \'d\': 5} # Example 2 list1 = [{\'x\': 10}, {\'y\': 20, \'z\': 30}] list2 = [{\'x\': 5, \'y\': 10}] # The merged dictionary should be {\'x\': 15, \'y\': 30, \'z\': 30} print(merge_dicts(list1, list2)) # Output: {\'x\': 15, \'y\': 30, \'z\': 30} ``` **Note:** - Make sure to use `ChainMap` and `defaultdict` effectively within your solution. - The function should handle the merging process efficiently, avoiding unnecessary creation of intermediate dictionaries.","solution":"from collections import ChainMap, defaultdict from typing import List, Dict def merge_dicts(list1: List[Dict[str, int]], list2: List[Dict[str, int]]) -> Dict[str, int]: combined = ChainMap(*list1, *list2) merged_dict = defaultdict(int) for key in combined: for mapping in combined.maps: if key in mapping: merged_dict[key] += mapping[key] return dict(merged_dict)"},{"question":"**Problem Statement:** You are given access to Unix-based NIS (Network Information Service) using the \\"nis\\" module in Python. Your task is to implement a function `get_user_info` that retrieves user information stored in an NIS map named \\"passwd.byname\\". This map associates usernames with their details. The function should: 1. Take a username as input and return the corresponding user details from the \\"passwd.byname\\" map. 2. If the username does not exist, return an empty dictionary. 3. If there is an issue accessing the NIS domain or the map, return an error message. **Function Signature:** ```python def get_user_info(username: str) -> dict: pass ``` **Input:** - `username`: A string representing the username to look up in the \\"passwd.byname\\" map. **Output:** - A dictionary containing user details if the username exists. - An empty dictionary if the username does not exist. - An error message if there is an issue accessing the NIS domain or the map. **Example:** Assuming the \\"passwd.byname\\" map contains the following entries: ``` {\\"alice\\": \\"alice:x:1001:1001::/home/alice:/bin/bash\\", \\"bob\\": \\"bob:x:1002:1002::/home/bob:/bin/bash\\"} ``` ```python print(get_user_info(\\"alice\\")) # Expected output: {\\"username\\": \\"alice\\", \\"passwd\\": \\"x\\", \\"uid\\": \\"1001\\", \\"gid\\": \\"1001\\", \\"gecos\\": \\"\\", \\"home_directory\\": \\"/home/alice\\", \\"shell\\": \\"/bin/bash\\"} print(get_user_info(\\"charlie\\")) # Expected output: {} print(get_user_info(\\"bob\\")) # Expected output: {\\"username\\": \\"bob\\", \\"passwd\\": \\"x\\", \\"uid\\": \\"1002\\", \\"gid\\": \\"1002\\", \\"gecos\\": \\"\\", \\"home_directory\\": \\"/home/bob\\", \\"shell\\": \\"/bin/bash\\"} ``` **Constraints:** - The function should handle any exceptions raised by the `nis` module appropriately and return an error message. - The function should only be implemented on Unix systems where the `nis` module is available. - The entries in the NIS map \\"passwd.byname\\" are strings following the format: `username:passwd:uid:gid:gecos:home_directory:shell`. **Notes:** - This question requires understanding and using the `nis` module\'s `match` function. - Proper error handling and exceptional cases should be considered. - This demonstrates advanced use of modules available in Python for Unix-based systems.","solution":"import nis def get_user_info(username: str) -> dict: try: nis_map = \\"passwd.byname\\" user_info_string = nis.match(username, nis_map) # Split the retrieved user info user_info_parts = user_info_string.decode().split(\':\') if len(user_info_parts) != 7: return \\"Error: Unexpected user info format.\\" # Create a dictionary with user details user_info = { \\"username\\": user_info_parts[0], \\"passwd\\": user_info_parts[1], \\"uid\\": user_info_parts[2], \\"gid\\": user_info_parts[3], \\"gecos\\": user_info_parts[4], \\"home_directory\\": user_info_parts[5], \\"shell\\": user_info_parts[6] } return user_info except nis.error: return \\"Error: Failed to access NIS domain or map.\\" except KeyError: return {}"},{"question":"<|Analysis Begin|> The provided documentation focuses on the `husl_palette` function of the seaborn library. This function generates a palette of colors with controllable parameters like the number of colors, lightness, saturation, and starting hue. It also allows creating a continuous colormap. Here\'s a summary of the features covered in the documentation: 1. **Basic Palette Generation**: Generating a default palette with 6 colors. 2. **Custom Number of Colors**: Adjusting the number of colors in the palette. 3. **Lightness Adjustment**: Modifying the lightness of the colors. 4. **Saturation Adjustment**: Modifying the saturation of the colors. 5. **Hue Adjustment**: Changing the starting hue for color sampling. 6. **Continuous Colormap**: Creating a continuous colormap instead of discrete colors. Based on this, a compelling question should involve generating a custom palette and using it for a plot to ensure an understanding of both palette creation and application in seaborn. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You have been provided a dataset containing the average monthly temperatures for different cities. Your task is to create a visualization using seaborn that demonstrates your understanding of the `husl_palette` function. Part 1: Data Preparation 1. Load the provided dataset (a CSV file named `city_temperatures.csv`) into a Pandas DataFrame. 2. The dataset contains three columns: - `city`: The name of the city. - `month`: The month of the year (1 through 12). - `temperature`: The average temperature for that month in degrees Celsius. Part 2: Custom Palette Creation 1. Create a custom color palette using `sns.husl_palette()` with the following specifications: - The palette should contain 12 colors. - The lightness parameter should be set to 0.65. - The saturation parameter should be set to 0.75. Part 3: Visualization 1. Use seaborn to create a line plot showing the average monthly temperatures for each city. The x-axis should represent the months, and the y-axis should represent the average temperature. Each city\'s temperature trend should be shown as a separate line on the plot. 2. Apply your custom palette to the plot. 3. Add appropriate titles and labels to the plot to make it informative and understandable. Expected Function Implementation Define a function `plot_city_temperatures()` which takes the path to the CSV file as input and produces the described plot. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_city_temperatures(file_path: str): This function reads a CSV file containing city temperatures and generates a line plot with a custom husl palette. Parameters: file_path (str): The path to the CSV file containing the data. Returns: None: This function does not return any value but displays the plot. # Part 1: Data Preparation df = pd.read_csv(file_path) # Part 2: Custom Palette Creation custom_palette = sns.husl_palette(n_colors=12, l=0.65, s=0.75) # Part 3: Visualization plt.figure(figsize=(14, 7)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'city\', palette=custom_palette) plt.title(\'Average Monthly Temperatures for Different Cities\') plt.xlabel(\'Month\') plt.ylabel(\'Average Temperature (Â°C)\') plt.legend(title=\'City\') plt.show() ``` You should verify that your function correctly loads the data and produces the specified plot with the custom color palette.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_city_temperatures(file_path: str): This function reads a CSV file containing city temperatures and generates a line plot with a custom husl palette. Parameters: file_path (str): The path to the CSV file containing the data. Returns: None: This function does not return any value but displays the plot. # Part 1: Data Preparation df = pd.read_csv(file_path) # Part 2: Custom Palette Creation custom_palette = sns.husl_palette(n_colors=12, l=0.65, s=0.75) # Part 3: Visualization plt.figure(figsize=(14, 7)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'city\', palette=custom_palette) plt.title(\'Average Monthly Temperatures for Different Cities\') plt.xlabel(\'Month\') plt.ylabel(\'Average Temperature (Â°C)\') plt.legend(title=\'City\') plt.show()"},{"question":"**Customizing Python Startup Paths** You are provided with the documentation of the Python \\"site\\" module which is responsible for site-specific configuration hooks during interpreter startup. This module modifies the `sys.path` to include directories for site-specific customizations and handles `.pth` files for additional path setups. **Task:** Implement a function `custom_startup_paths(enable_user_site: bool) -> list` that: 1. Conditionally modifies the `sys.path` based on the `enable_user_site` flag. 2. If `enable_user_site` is `True`, ensure that user site-packages are included in `sys.path`. 3. If `enable_user_site` is `False`, exclude user site-packages from `sys.path`. 4. Return the modified `sys.path`. **Constraints:** - Do not use the `site.addsitedir` directly in your function. - Assume `PYTHONUSERBASE` environment variable should be respected when determining user site paths. - Assume the necessary site-specific directories exist. - Handle exceptions gracefully and ensure consistent behavior on different platforms (Windows, Unix, macOS). **Example:** ```python def custom_startup_paths(enable_user_site: bool) -> list: # Your implementation here # Example Invocation: # custom_startup_paths(True) # might return: [\'/usr/lib/python3.9\', \'/home/user/.local/lib/python3.9/site-packages\', ...] # custom_startup_paths(False) # might return: [\'/usr/lib/python3.9\', ...] ``` **Note:** Refer to the provided documentation for details on handling paths, configurations, and the relevant flags.","solution":"import sys import site def custom_startup_paths(enable_user_site: bool) -> list: if enable_user_site: # Ensure that user site-packages are included site.ENABLE_USER_SITE = True user_site_path = site.getusersitepackages() if user_site_path not in sys.path: sys.path.append(user_site_path) else: # Ensure that user site-packages are excluded site.ENABLE_USER_SITE = False user_site_path = site.getusersitepackages() if user_site_path in sys.path: sys.path.remove(user_site_path) return sys.path"},{"question":"# XML Report Generation and Analysis You are tasked with creating a Python class that utilizes the `xml.etree.ElementTree` module to parse, modify, and create XML documents. This class will manage XML data representing a company\'s employee records. Each employee record includes an ID, name, position, salary, and list of dependents, stored similar to the following structure: ```xml <company> <employee id=\\"1\\"> <name>John Doe</name> <position>Software Engineer</position> <salary>80000</salary> <dependents> <dependent>Anna Doe</dependent> <dependent>Mark Doe</dependent> </dependents> </employee> <employee id=\\"2\\"> <name>Jane Smith</name> <position>Project Manager</position> <salary>90000</salary> <dependents/> </employee> <!-- Additional employees --> </company> ``` Your class should provide the following functionalities: 1. **Loading and Parsing XML Data**: - Method: `load_from_file(self, filename: str) -> None` - Reads an XML file containing employee records and loads it as an `ElementTree`. 2. **Adding a New Employee**: - Method: `add_employee(self, emp_id: str, name: str, position: str, salary: str, dependents: list) -> None` - Adds a new employee with the specified details to the XML document. 3. **Updating an Employee\'s Salary**: - Method: `update_salary(self, emp_id: str, new_salary: str) -> bool` - Updates the salary of an employee identified by `emp_id`. Returns True if the update was successful, False if the employee is not found. 4. **Removing Employees by Position**: - Method: `remove_employees_by_position(self, position: str) -> int` - Removes all employees with the specified position from the XML document. Returns the number of employees removed. 5. **Generating a Summary Report**: - Method: `generate_summary(self) -> str` - Generates and returns a summary report of all employees in the XML document, listing each employee\'s ID, name, position, salary, and number of dependents. 6. **Saving the XML Data to a File**: - Method: `save_to_file(self, filename: str) -> None` - Writes the current XML document back to a file. Constraints: - Assume all input values are valid strings. - The IDs for new employees are unique and not repeated. - The XML file read by `load_from_file` follows the structure provided above. **Example Usage:** ```python manager = EmployeeManager() manager.load_from_file(\'employees.xml\') # Add a new employee manager.add_employee(\'3\', \'Alice Johnson\', \'Data Scientist\', \'85000\', [\'Lucy Johnson\']) # Update salary manager.update_salary(\'1\', \'82000\') # Remove employees by position removed_count = manager.remove_employees_by_position(\'Project Manager\') print(f\'Removed {removed_count} employees\') # Generate summary summary = manager.generate_summary() print(summary) # Save changes to file manager.save_to_file(\'updated_employees.xml\') ``` Expected Behavior: - The XML document is correctly loaded, modified, and saved based on method calls. - The summary report accurately reflects the current state of the XML document after modifications.","solution":"import xml.etree.ElementTree as ET class EmployeeManager: def __init__(self): self.tree = None self.root = None def load_from_file(self, filename: str) -> None: Loads and parses the XML file. self.tree = ET.parse(filename) self.root = self.tree.getroot() def add_employee(self, emp_id: str, name: str, position: str, salary: str, dependents: list) -> None: Adds a new employee with the specified details to the XML document. emp_element = ET.Element(\\"employee\\", id=emp_id) name_element = ET.SubElement(emp_element, \\"name\\") name_element.text = name position_element = ET.SubElement(emp_element, \\"position\\") position_element.text = position salary_element = ET.SubElement(emp_element, \\"salary\\") salary_element.text = salary dependents_element = ET.SubElement(emp_element, \\"dependents\\") for dependent in dependents: dep_element = ET.SubElement(dependents_element, \\"dependent\\") dep_element.text = dependent self.root.append(emp_element) def update_salary(self, emp_id: str, new_salary: str) -> bool: Updates the salary of the employee identified by emp_id. for emp in self.root.findall(\'employee\'): if emp.get(\'id\') == emp_id: emp.find(\'salary\').text = new_salary return True return False def remove_employees_by_position(self, position: str) -> int: Removes all employees with the specified position. count = 0 for emp in self.root.findall(\'employee\'): if emp.find(\'position\').text == position: self.root.remove(emp) count += 1 return count def generate_summary(self) -> str: Generates a summary report of all employees. summary_lines = [] for emp in self.root.findall(\'employee\'): emp_id = emp.get(\'id\') name = emp.find(\'name\').text position = emp.find(\'position\').text salary = emp.find(\'salary\').text dependents = emp.find(\'dependents\').findall(\'dependent\') num_dependents = len(dependents) summary_lines.append(f\'ID: {emp_id}, Name: {name}, Position: {position}, Salary: {salary}, Dependents: {num_dependents}\') return \\"n\\".join(summary_lines) def save_to_file(self, filename: str) -> None: Writes the current XML document back to a file. self.tree.write(filename)"},{"question":"Objective Implement a set of functions that demonstrate the creation, concatenation, resizing, and retrieval of information from Python `bytearray` objects. Requirements 1. Implement the following functions: - `create_bytearray_from_obj(obj)`: - **Input**: An object `obj` that implements the buffer protocol. - **Output**: A new `bytearray` object created from `obj`. - `create_bytearray_from_string(string, length)`: - **Input**: A string `string` and its length `length`. - **Output**: A new `bytearray` object created from `string` and `length`. - `concat_bytearrays(bytearray1, bytearray2)`: - **Input**: Two `bytearray` objects. - **Output**: A new `bytearray` object that is the concatenation of `bytearray1` and `bytearray2`. - `get_bytearray_size(bytearray)`: - **Input**: A `bytearray` object. - **Output**: The size of the `bytearray` object. - `get_bytearray_as_string(bytearray)`: - **Input**: A `bytearray` object. - **Output**: The contents of the `bytearray` object as a string. - `resize_bytearray(bytearray, new_len)`: - **Input**: A `bytearray` object and a new length `new_len`. - **Output**: The resized `bytearray` object with the internal buffer size adjusted to `new_len`. 2. Implement additional functions to demonstrate your understanding of the provided macros: - `get_bytearray_size_macro(bytearray)`: - **Input**: A `bytearray` object. - **Output**: The size of the `bytearray` object using the macro `PyByteArray_GET_SIZE`. - `get_bytearray_as_string_macro(bytearray)`: - **Input**: A `bytearray` object. - **Output**: The contents of the `bytearray` object as a string using the macro `PyByteArray_AS_STRING`. Constraints - All inputs will be valid, and the functions should handle any edge cases gracefully. - Use the provided API functions and macros where appropriate. Example ```python # Example Usage obj = b\'example buffer\' byte_arr = create_bytearray_from_obj(obj) print(byte_arr) # Expected Output: bytearray(b\'example buffer\') string = \\"another example\\" byte_arr2 = create_bytearray_from_string(string, len(string)) print(byte_arr2) # Expected Output: bytearray(b\'another example\') concatenated = concat_bytearrays(byte_arr, byte_arr2) print(concatenated) # Expected Output: bytearray(b\'example bufferanother example\') size = get_bytearray_size(concatenated) print(size) # Expected Output: 31 byte_arr_string = get_bytearray_as_string(concatenated) print(byte_arr_string) # Expected Output: \'example bufferanother example\' resized = resize_bytearray(concatenated, 10) print(resized) # Expected Output: bytearray(b\'example bu\') size_macro = get_bytearray_size_macro(concatenated) print(size_macro) # Expected Output: 31 byte_arr_string_macro = get_bytearray_as_string_macro(concatenated) print(byte_arr_string_macro) # Expected Output: \'example bufferanother example\' ``` Note - The above example demonstrates how the functions should be designed and expected outputs. The actual implementation of functions will involve using the direct API functions and macros provided in the documentation.","solution":"def create_bytearray_from_obj(obj): Create a bytearray object from an object implementing the buffer protocol. :param obj: The object implementing the buffer protocol. :return: A new bytearray object created from obj. return bytearray(obj) def create_bytearray_from_string(string, length): Create a bytearray object from a string and its length. :param string: The input string. :param length: The length of the string to be converted to bytearray. :return: A new bytearray object created from the string. return bytearray(string[:length], \'utf-8\') def concat_bytearrays(bytearray1, bytearray2): Concatenate two bytearray objects. :param bytearray1: The first bytearray object. :param bytearray2: The second bytearray object. :return: A new bytearray object that is the concatenation of bytearray1 and bytearray2. return bytearray1 + bytearray2 def get_bytearray_size(bytearray_obj): Get the size of a bytearray object. :param bytearray_obj: The bytearray object. :return: The size of the bytearray object. return len(bytearray_obj) def get_bytearray_as_string(bytearray_obj): Get the contents of a bytearray object as a string. :param bytearray_obj: The bytearray object. :return: The contents of the bytearray object as a string. return bytearray_obj.decode(\'utf-8\') def resize_bytearray(bytearray_obj, new_len): Resize a bytearray object to a new length. :param bytearray_obj: The bytearray object. :param new_len: The new length. :return: The resized bytearray object. bytearray_obj = bytearray(bytearray_obj[:new_len]) return bytearray_obj def get_bytearray_size_macro(bytearray_obj): Get the size of a bytearray object using the macro `PyByteArray_GET_SIZE`. :param bytearray_obj: The bytearray object. :return: The size of the bytearray object. # In Python, it\'s equivalent to len(bytearray_obj) return len(bytearray_obj) def get_bytearray_as_string_macro(bytearray_obj): Get the contents of a bytearray object as a string using the macro `PyByteArray_AS_STRING`. :param bytearray_obj: The bytearray object. :return: The contents of the bytearray object as a string. # In Python, it\'s equivalent to bytearray_obj.decode(\'utf-8\') return bytearray_obj.decode(\'utf-8\')"},{"question":"Description: You are implementing a system for managing video game achievements. Each achievement has a name and a difficulty level. Additionally, achievements can be grouped into difficulty levels to quickly see the categories. Implement an enumeration to represent the achievements and group them by difficulty using Python\'s `enum` module. Requirements: 1. **Enum Class**: - Create an enumeration class `Achievement` using `enum.Enum`. - The achievements and their difficulties are: - `FIRST_KILL` with difficulty `EASY`. - `MASTER_EXPLORER` with difficulty `HARD`. - `GOLD_HOARDER` with difficulty `MEDIUM`. - `ALL_COLLECTIBLES` with difficulty `HARD`. - `FIRST_LEVEL_UP` with difficulty `EASY`. 2. **Group by Difficulty**: - Use an `enum` subclass `DifficultyGroup` to group achievements by their difficulty levels (`EASY`, `MEDIUM`, `HARD`). - The difficulty enumeration should use the `auto()` helper to automatically assign numeric values to each difficulty. 3. **Custom Methods**: - Implement a method `describe()` for the `Achievement` enum that returns the achievement\'s name and difficulty. - Assign each difficulty level a unique consecutive integer starting from 1 using the `auto()` feature. 4. **Ensure Uniqueness**: - Use the `@unique` decorator to ensure no duplicate values exist within the `Achievement` enum. Input and Output Example: **Code:** ```python from enum import Enum, auto, unique @unique class DifficultyGroup(Enum): EASY = auto() MEDIUM = auto() HARD = auto() @unique class Achievement(Enum): FIRST_KILL = (DifficultyGroup.EASY, \\"First Kill\\") MASTER_EXPLORER = (DifficultyGroup.HARD, \\"Master Explorer\\") GOLD_HOARDER = (DifficultyGroup.MEDIUM, \\"Gold Hoarder\\") ALL_COLLECTIBLES = (DifficultyGroup.HARD, \\"All Collectibles\\") FIRST_LEVEL_UP = (DifficultyGroup.EASY, \\"First Level Up\\") def describe(self): return f\\"Achievement: {self.value[1]}, Difficulty: {self.value[0].name}\\" # Outputs print(Achievement.FIRST_KILL.describe()) # Output: Achievement: First Kill, Difficulty: EASY print(Achievement.ALL_COLLECTIBLES.describe()) # Output: Achievement: All Collectibles, Difficulty: HARD print(DifficultyGroup.EASY.value) # Output: 1 (since auto() assigns consecutive integers starting from 1) ``` Constraints: - Ensure that no two achievements can have the same combination of name and difficulty. - Implement all the required functionalities as specified using Python\'s `enum` module only. **Performance Requirements**: - The implementation should not rely on any external libraries except the `enum` module. - Efficiency in accessing and describing achievements is critical. Hints: - Use the `@unique` decorator to prevent duplicate achievements. - Utilize the `auto()` feature in the `DifficultyGroup` for automatic value assignment. - Custom `describe()` method in `Achievement` should help in fetching formatted strings easily. Good luck!","solution":"from enum import Enum, auto, unique @unique class DifficultyGroup(Enum): EASY = auto() MEDIUM = auto() HARD = auto() @unique class Achievement(Enum): FIRST_KILL = (DifficultyGroup.EASY, \\"First Kill\\") MASTER_EXPLORER = (DifficultyGroup.HARD, \\"Master Explorer\\") GOLD_HOARDER = (DifficultyGroup.MEDIUM, \\"Gold Hoarder\\") ALL_COLLECTIBLES = (DifficultyGroup.HARD, \\"All Collectibles\\") FIRST_LEVEL_UP = (DifficultyGroup.EASY, \\"First Level Up\\") def describe(self): return f\\"Achievement: {self.value[1]}, Difficulty: {self.value[0].name}\\""},{"question":"# HTML Text Sanitizer and Reverser You are required to implement two functions that sanitize and then reverse the HTML escaping of a given text input. This will assess your understanding of string manipulation using the `html` moduleâ€™s utilities. Function 1: `sanitize_html(text: str, escape_quotes: bool = True) -> str` - **Input**: - `text` (str): A string that may contain HTML special characters (`&`, `<`, `>`) and optionally quotes (`\\"` and `\'`). - `escape_quotes` (bool): A boolean flag indicating whether to escape quotes or not (default is `True`). - **Output**: - Returns the sanitized version of the input text, where HTML special characters and quotes (if `escape_quotes` is `True`) are converted to their HTML-safe sequences. Function 2: `reverse_sanitization(sanitized_text: str) -> str` - **Input**: - `sanitized_text` (str): A string that contains HTML-safe sequences which need to be converted back to their original characters. - **Output**: - Returns the text with all HTML-safe sequences converted back to their corresponding characters. Constraints - You may assume that the input strings will not be empty and will be valid. - The functions should handle both named and numeric character references correctly. Examples ```python # Example 1: text = \'Hello & Welcome to <OpenAI> \\"GPT-3\\"\' sanitized = sanitize_html(text) print(sanitized) # Output: \'Hello &amp; Welcome to &lt;OpenAI&gt; &quot;GPT-3&quot;\' original = reverse_sanitization(sanitized) print(original) # Output: \'Hello & Welcome to <OpenAI> \\"GPT-3\\"\' # Example 2: text = \\"Itâ€™s sunny today & everyoneâ€™s happy!\\" sanitized = sanitize_html(text, escape_quotes=False) print(sanitized) # Output: \'Itâ€™s sunny today &amp; everyoneâ€™s happy!\' original = reverse_sanitization(sanitized) print(original) # Output: \\"Itâ€™s sunny today & everyoneâ€™s happy!\\" ``` Implement the functions `sanitize_html` and `reverse_sanitization` to complete this task.","solution":"import html def sanitize_html(text, escape_quotes=True): Sanitizes the text by converting HTML special characters and optionally quotes to HTML-safe sequences. return html.escape(text, quote=escape_quotes) def reverse_sanitization(sanitized_text): Reverses the sanitization by converting HTML-safe sequences back to their original characters. return html.unescape(sanitized_text)"},{"question":"Coding Assessment Question # Visualization of Airline Passengers Data You are provided with a dataset containing the number of airline passengers for each month from 1949 to 1960. Your task is to write code that manipulates this data to generate specific visualizations using seaborn. # Objective 1. Load the \\"flights\\" dataset from seaborn. 2. Convert the dataset from long-form to wide-form. 3. Create two different visualizations using seaborn: - A line plot showing the number of passengers per month for each year. - A box plot of monthly passengers over the entire period. # Instructions 1. **Load Dataset**: Use `seaborn.load_dataset` to load the \\"flights\\" dataset. 2. **Convert to Wide-Form**: - Convert the dataset to wide-form using pandas functions so that each month\'s time series is in a separate column. 3. **Visualizations**: - Create a line plot using `seaborn.relplot` to show the number of passengers per month for each year (long-form data). - Create a box plot of monthly passengers over the entire period using the wide-form data. # Function Signature ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flights_data(): # Step 1: Load the \\"flights\\" dataset flights = sns.load_dataset(\\"flights\\") # Step 2: Convert the dataset to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Step 3A: Create a line plot using the long-form data sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") # Step 3B: Create a box plot using the wide-form data sns.catplot(data=flights_wide, kind=\\"box\\") # Show plots plt.show() ``` # Input & Output - **Input**: There are no direct inputs as the dataset is loaded internally. - **Output**: The function should display the specified visualizations. # Constraints - Use `seaborn` and `pandas` for data manipulation and visualization. - Use provided documentation for any required function references and examples. **Note**: Ensure that your code is efficient and well-documented.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flights_data(): # Step 1: Load the \\"flights\\" dataset flights = sns.load_dataset(\\"flights\\") # Step 2: Convert the dataset to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Step 3A: Create a line plot using the long-form data line_plot = sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") # Step 3B: Create a box plot using the wide-form data box_plot = sns.catplot(data=flights, kind=\\"box\\", x=\\"month\\", y=\\"passengers\\") # Show plots plt.show() return flights, flights_wide, line_plot, box_plot"},{"question":"# Pandas Coding Assessment Question Objective: The goal of this assignment is to assess your ability to work with pandas for manipulating and analyzing data, with a focus on advanced indexing techniques. Problem Statement: You are provided with a dataset of customers\' transactions. Your task is to create several functions to analyze this data using various pandas indexing techniques discussed in the given documentation. **Dataset Structure**: The dataset is a DataFrame with the following columns: - `customer_id`: A unique identifier for customers. - `transaction_date`: The date of the transaction. - `amount_spent`: The amount of money spent in the transaction. - `store_id`: The identifier of the store where the transaction took place. Below is a sample of the dataset: | customer_id | transaction_date | amount_spent | store_id | |-------------|------------------|--------------|----------| | 1 | 2023-07-21 | 75.60 | 101 | | 2 | 2023-07-22 | 48.80 | 102 | | 1 | 2023-07-23 | 130.50 | 101 | | 3 | 2023-07-21 | 25.00 | 103 | | 2 | 2023-07-24 | 75.00 | 102 | Tasks: 1. **Filter transactions for a specific customer:** Write a function `filter_customer_transactions(data: pd.DataFrame, customer_id: int) -> pd.DataFrame` that returns a DataFrame containing only the transactions of the specified customer. 2. **Calculate total amount spent by customer:** Write a function `total_amount_spent(data: pd.DataFrame, customer_id: int) -> float` that returns the total amount spent by the specified customer. 3. **Filter transactions within a date range:** Write a function `filter_date_range(data: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame` that returns all transactions within the specified date range (inclusive). `start_date` and `end_date` should be strings in the format `YYYY-MM-DD`. 4. **Find the highest spending customer:** Write a function `highest_spending_customer(data: pd.DataFrame) -> int` that returns the `customer_id` of the customer who has spent the most in total. 5. **Store-based filtering with boolean indexing:** Write a function `transactions_above_threshold(data: pd.DataFrame, store_id: int, threshold: float) -> pd.DataFrame` that returns a DataFrame containing the transactions from the specified store where the `amount_spent` is above the given threshold. Constraints: - Do not use any external libraries other than pandas. - Assume the input DataFrame is pre-loaded and correctly formatted as shown above. - Performance should be considered, especially for large datasets. Example Usage: ```python data = pd.DataFrame({ \'customer_id\': [1, 2, 1, 3, 2], \'transaction_date\': [\'2023-07-21\', \'2023-07-22\', \'2023-07-23\', \'2023-07-21\', \'2023-07-24\'], \'amount_spent\': [75.60, 48.80, 130.50, 25.00, 75.00], \'store_id\': [101, 102, 101, 103, 102] }) # 1. Filtering transactions of customer with id 1 print(filter_customer_transactions(data, 1)) # 2. Total amount spent by customer with id 2 print(total_amount_spent(data, 2)) # 3. Filtering transactions between \'2023-07-21\' and \'2023-07-23\' print(filter_date_range(data, \'2023-07-21\', \'2023-07-23\')) # 4. Finding the highest spending customer print(highest_spending_customer(data)) # 5. Transactions in store \'102\' with amount spent above \'50\' print(transactions_above_threshold(data, 102, 50)) ``` Ensure to provide correct docstrings and comments where necessary.","solution":"import pandas as pd def filter_customer_transactions(data: pd.DataFrame, customer_id: int) -> pd.DataFrame: Returns a DataFrame containing only the transactions of the specified customer. Parameters: data (pd.DataFrame): The input DataFrame containing transaction data. customer_id (int): The customer_id to filter transactions for. Returns: pd.DataFrame: A DataFrame filtered by the specified customer_id. return data[data[\'customer_id\'] == customer_id] def total_amount_spent(data: pd.DataFrame, customer_id: int) -> float: Returns the total amount spent by the specified customer. Parameters: data (pd.DataFrame): The input DataFrame containing transaction data. customer_id (int): The customer_id to calculate total amount spent for. Returns: float: The total amount spent by the specified customer. return data[data[\'customer_id\'] == customer_id][\'amount_spent\'].sum() def filter_date_range(data: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame: Returns a DataFrame containing transactions within the specified date range (inclusive). Parameters: data (pd.DataFrame): The input DataFrame containing transaction data. start_date (str): The start date in \'YYYY-MM-DD\' format. end_date (str): The end date in \'YYYY-MM-DD\' format. Returns: pd.DataFrame: A DataFrame filtered by the specified date range. mask = (data[\'transaction_date\'] >= start_date) & (data[\'transaction_date\'] <= end_date) return data[mask] def highest_spending_customer(data: pd.DataFrame) -> int: Returns the customer_id of the customer who has spent the most in total. Parameters: data (pd.DataFrame): The input DataFrame containing transaction data. Returns: int: The customer_id of the highest spending customer. total_spent_per_customer = data.groupby(\'customer_id\')[\'amount_spent\'].sum() return total_spent_per_customer.idxmax() def transactions_above_threshold(data: pd.DataFrame, store_id: int, threshold: float) -> pd.DataFrame: Returns a DataFrame containing transactions from the specified store where amount_spent is above the given threshold. Parameters: data (pd.DataFrame): The input DataFrame containing transaction data. store_id (int): The store_id to filter transactions for. threshold (float): The spending threshold to filter transactions. Returns: pd.DataFrame: A DataFrame filtered by the specified store_id and amount_spent threshold. return data[(data[\'store_id\'] == store_id) & (data[\'amount_spent\'] > threshold)]"},{"question":"Objective: Implement a function that takes a list of group names and returns a dictionary mapping each group name to the total number of its members, using the `grp` module. Function Signature: ```python def get_group_member_counts(group_names: list) -> dict: pass ``` Input: - `group_names` (list): A list of strings where each string is a valid Unix group name. Output: - Returns a dictionary where each key is a group name from the input list, and the corresponding value is the total number of members in that group. Constraints: 1. If a group name does not exist, you should not include it in the output dictionary. 2. The function must use the `grp.getgrnam()` to fetch group information. 3. Handle possible exceptions raised by `grp.getgrnam()` if the group name is invalid. Example: ```python group_names = [\'admin\', \'users\', \'wheel\'] member_counts = get_group_member_counts(group_names) print(member_counts) # Output could be something like {\'admin\': 3, \'users\': 10, \'wheel\': 0} ``` Additional Information: - Make sure the function is optimized for quick lookups using the provided list of group names. - `grp.getgrnam(name)` raises a `KeyError` if the entry for the given group name cannot be found. Handle this exception appropriately to exclude non-existent groups from the result. - Consider writing additional helper functions for cleaner code, if necessary. - Test your function before submitting to ensure it handles various edge cases and invalid inputs correctly.","solution":"import grp def get_group_member_counts(group_names: list) -> dict: Returns a dictionary where each key is a group name from the input list, and the value is the total number of members in that group. If a group does not exist, it will not be included in the output dictionary. group_member_counts = {} for group in group_names: try: group_info = grp.getgrnam(group) group_member_counts[group] = len(group_info.gr_mem) except KeyError: # Group name does not exist, skip it continue return group_member_counts"},{"question":"Objective Design a function that demonstrates understanding of the Copy-on-Write behavior introduced in pandas 3.0. The function should make use of correct indexing operations and avoid pitfalls that can lead to unexpected data mutations. Description Write a function `validate_copy_on_write(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]` which demonstrates the following: 1. Create a DataFrame and derive a subset DataFrame. 2. Modify the original DataFrame and show that the subset is not affected. 3. Modify the subset DataFrame using proper indexing (loc or iloc) and show that the original DataFrame is not affected. 4. Create a view of the DataFrame, modify the original DataFrame showing the view does not change. 5. Convert a Series to a NumPy array and attempt an inplace modification which should raise an error. Input - `df`: A pandas DataFrame with at least two columns. Output - A tuple containing: - The original DataFrame after modification. - The subset DataFrame after modification. - The DataFrame view after modification. - The NumPy array generated from a Series. Example ```python import pandas as pd from typing import Tuple def validate_copy_on_write(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: # Create original DataFrame original_df = df.copy() # Create subset DataFrame subset_df = original_df.loc[:, [\\"foo\\"]] # Modify original DataFrame original_df.iloc[0, 0] = 100 # Modify subset DataFrame using proper indexing subset_df.loc[0, \\"foo\\"] = 200 # Create a view of the original DataFrame view_df = original_df.loc[:, :] # Modify original DataFrame original_df.iloc[0, 0] = 300 # Convert Series to NumPy array ser = original_df[\\"foo\\"] numpy_array = ser.to_numpy() try: numpy_array[0] = 400 except ValueError as e: print(f\'Error: {e}\') return original_df, subset_df, view_df, numpy_array # Example Test df = pd.DataFrame({\\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6]}) validate_copy_on_write(df) ``` Constraints - The input DataFrame must contain at least two columns. - Proper use of `loc` and `iloc` to avoid chained assignments. - Ensure NumPy array modifications are attempted correctly to demonstrate CoW rules.","solution":"import pandas as pd from typing import Tuple def validate_copy_on_write(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: # Create original DataFrame original_df = df.copy() # Create subset DataFrame subset_df = original_df.loc[:, [\\"foo\\"]] # Modify original DataFrame original_df.iloc[0, 0] = 100 # Modify subset DataFrame using proper indexing subset_df = subset_df.copy() # Ensure it\'s a Copy-on-Write operation subset_df.loc[0, \\"foo\\"] = 200 # Create a view of the original DataFrame view_df = original_df.loc[:, :] # Modify original DataFrame original_df.iloc[0, 0] = 300 # Convert Series to NumPy array ser = original_df[\\"foo\\"] numpy_array = ser.to_numpy() numpy_array_copy = numpy_array.copy() # Ensure it won\'t raise error because we modify the copy now numpy_array_copy[0] = 400 return original_df, subset_df, view_df, numpy_array_copy"},{"question":"**Objective**: Implement a custom dataset and dataloader using PyTorch modules, demonstrating comprehension of dataset types (map-style and iterable-style), custom collation, and multi-process data loading. # Problem Statement You are tasked with implementing a map-style dataset and a corresponding `DataLoader` with custom collation and multi-process loading features. Your dataset will contain pairs of numbers, and your goal is to create batches where each sample in the batch is a sum of a pair of numbers from the dataset. # Requirements 1. **Custom Dataset**: - Implement a map-style dataset, named `SumDataset`, which stores pairs of integers. - The dataset should take two parameters: - `pairs`: A list of tuples, where each tuple contains two integers. - `label`: A list of integers, where each integer is the sum of the corresponding pair in `pairs`. - The dataset should implement `__getitem__` and `__len__` methods. 2. **Custom Collate Function**: - Implement a custom collation function, `custom_collate_fn`, which takes a list of samples and returns a tuple of two lists: - The first list contains the sums of the pairs. - The second list contains the corresponding labels. 3. **DataLoader**: - Create a `DataLoader` for the `SumDataset` with the following configuration: - `batch_size`: 4 - `shuffle`: True - `num_workers`: 2 - `collate_fn`: `custom_collate_fn` 4. **Test the Implementation**: - Create an instance of `SumDataset` with a sample list of pairs and their corresponding sums. - Use the dataset to create an instance of the `DataLoader`. - Iterate over the dataloader and print the batches to verify the functionality. # Input 1. A list `pairs` containing tuples of integers. 2. A list `label` containing integers. # Output - Batches yielded by the `DataLoader`, where each batch is a tuple containing: - A list of the sums of the pairs in the batch. - A list of the corresponding labels. # Example ```python pairs = [(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8)] label = [3, 5, 7, 9, 11, 13, 15] # Step 1: Implement SumDataset class SumDataset(torch.utils.data.Dataset): def __init__(self, pairs, label): self.pairs = pairs self.label = label def __getitem__(self, index): return (self.pairs[index], self.label[index]) def __len__(self): return len(self.pairs) # Step 2: Implement custom_collate_fn def custom_collate_fn(batch): sums = [x[0][0] + x[0][1] for x in batch] labels = [x[1] for x in batch] return (sums, labels) # Step 3: Create DataLoader dataset = SumDataset(pairs, label) dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=custom_collate_fn) # Step 4: Iterate over DataLoader for sums, labels in dataloader: print(sums) print(labels) # Expected Output (example output, actual output may vary due to shuffle): # [sum1, sum2, sum3, sum4] # [label1, label2, label3, label4] ``` **Note**: The actual order and values of the batches may vary because `shuffle` is set to `True`.","solution":"import torch from torch.utils.data import DataLoader, Dataset class SumDataset(Dataset): def __init__(self, pairs, labels): self.pairs = pairs self.labels = labels def __getitem__(self, index): pair = self.pairs[index] label = self.labels[index] return pair, label def __len__(self): return len(self.pairs) def custom_collate_fn(batch): sums = [x[0][0] + x[0][1] for x in batch] labels = [x[1] for x in batch] return sums, labels # Example usage pairs = [(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8)] labels = [3, 5, 7, 9, 11, 13, 15] dataset = SumDataset(pairs, labels) dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=custom_collate_fn) for sums, labels in dataloader: print(sums) print(labels)"},{"question":"# Seaborn Coding Challenge Problem Statement You are tasked with visualizing the relationship between different performance metrics of NLP models on various tasks. For this exercise, you will utilize the Seaborn library\'s `objects` interface to create and customize a plot. Dataset We will use the popular `glue` dataset, which can be loaded directly using Seaborn\'s `load_dataset` function. The dataset should be transformed to have models and encoders as the index, and tasks as columns. You need to add an Average score column, which is the mean of the task scores for each model-encoder pair, rounded to one decimal place. Requirements 1. **Data Transformation**: - Load the `glue` dataset using `seaborn.load_dataset(\\"glue\\")`. - Pivot the dataset such that \\"Model\\" and \\"Encoder\\" are the indices, and the tasks are columns. The values should be the corresponding scores. - Add a new column \\"Average\\" which is the mean of the task scores for each row, rounded to one decimal place. - Sort the rows by \\"Average\\" in descending order. 2. **Plot Creation**: - Use `seaborn.objects.Plot` to create a plot where: - The x-axis represents the SST-2 task scores. - The y-axis represents the MRPC task scores. - The model names are indicated as text annotations on the plot. - The color of the points should be determined by the Encoder. - Customize the plot to: - Display text labels below the corresponding data points indicating the models. - Align text horizontally using an appropriate option. - Include any relevant customization to make the plot informative and visually appealing. Implementation Details - Write a Python function `visualize_glue_data()` that performs all the required operations and displays the final plot. - You can assume that seaborn and other necessary packages are already imported. Expected Function Signature ```python def visualize_glue_data(): # Your code goes here pass ``` Example Output On executing `visualize_glue_data()`, it should display a customized plot adhering to the specifications above. Good luck, and happy coding!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_glue_data(): # Load the dataset glue = sns.load_dataset(\\"glue\\") # Pivot the dataset glue_pivot = glue.pivot_table(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") # Add the Average score column glue_pivot[\\"Average\\"] = glue_pivot.mean(axis=1).round(1) # Sort by Average score in descending order glue_pivot = glue_pivot.sort_values(by=\\"Average\\", ascending=False) # Reset index for plotting glue_pivot = glue_pivot.reset_index() # Create the plot p = sns.objects.Plot(glue_pivot, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\").add( sns.objects.Point(), sns.objects.Label( glue_pivot[\'Model\'], align=\'right\', adjust=(\'dodge\', 0.5) ) ) # Customize the plot p.title(\'NLP Model Performance on SST-2 vs MRPC\') p.xlabel(\'SST-2 Score\') p.ylabel(\'MRPC Score\') p.show()"},{"question":"**Objective:** This question is designed to assess your understanding of seaborn\'s figure-level functions to create complex and customizable visualizations. **Task:** You are given a dataset about penguins that includes the following columns: - `species`: Species of the penguin (e.g., Adelie, Chinstrap, Gentoo) - `island`: Island where the penguin was observed - `bill_length_mm`: Length of the penguin\'s bill in millimeters - `bill_depth_mm`: Depth of the penguin\'s bill in millimeters - `flipper_length_mm`: Length of the penguin\'s flipper in millimeters - `body_mass_g`: Body mass of the penguin in grams - `sex`: Sex of the penguin (Male, Female) Your task is to write a function that performs the following: 1. **Loads** the penguin dataset using `seaborn.load_dataset(\\"penguins\\")`. 2. **Creates** a figure-level plot using seaborn that: - **Facets** the plot by the `sex` variable into separate columns. - **Plots** a different type of visualization for `body_mass_g` for each species. - **Customizes** the plots to have appropriate titles, axis labels, and a legend. - **Uses** `displot` for one species, `kdeplot` for another, and `histplot` for the last species for visualizing `body_mass_g`. - Ensures the plots are visually appealing and well-organized. **Input:** - None **Output:** - The function should directly display the figure. **Constraints:** - Ensure the visualizations are clear and distinguishable. - Use different colors or styles to distinguish between the facets. - Add axis labels and titles to make the plot understandable. **Guidelines:** - Make use of seaborn\'s figure-level functions and customization features as described in the documentation. - Consider the readability and interpretability of the plots when customizing them. **Example:** ```python def visualize_penguin_data(): import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Distributing species to different types of plots species_plot_kind = { \\"Adelie\\": \\"displot\\", \\"Chinstrap\\": \\"kdeplot\\", \\"Gentoo\\": \\"histplot\\" } # Set up the figure-level plot g = sns.FacetGrid(data=penguins, col=\\"sex\\", height=5, aspect=1.2) # Function to map proper plots to each species def map_plot(x, **kwargs): sns.plot_kind = kwargs.pop(\'plot_kind\') if plot_kind == \\"displot\\": sns.histplot(x=x, **kwargs) elif plot_kind == \\"kdeplot\\": sns.kdeplot(x=x, **kwargs) elif plot_kind == \\"histplot\\": sns.histplot(x=x, **kwargs) # Map each plot to the grid for species, plot_kind in species_plot_kind.items(): g.map_dataframe(map_plot, \\"body_mass_g\\", plot_kind=plot_kind) # Customize the plot g.set_axis_labels(\\"Body Mass (g)\\", \\"Density\\") g.add_legend(title=\\"Species\\") g.set_titles(\\"{col_name} Penguins\\") # Show plot plt.show() # Run the function visualize_penguin_data() ``` Ensure your solution follows the proper usage of figure-level functions and customization options available in seaborn.","solution":"def visualize_penguin_data(): import seaborn as sns import matplotlib.pyplot as plt # Load the penguin dataset penguins = sns.load_dataset(\\"penguins\\") # Create a figure-level plot faceted by the `sex` variable g = sns.FacetGrid(penguins, col=\\"sex\\", height=5, aspect=1.2) # Function to map the plots to each species def map_plot(data, species, plot_kind): if plot_kind == \\"displot\\": sns.histplot(data=data[data[\'species\'] == species], x=\\"body_mass_g\\", color=\'blue\', kde=True) elif plot_kind == \\"kdeplot\\": sns.kdeplot(data=data[data[\'species\'] == species], x=\\"body_mass_g\\", color=\'green\', fill=True) elif plot_kind == \\"histplot\\": sns.histplot(data=data[data[\'species\'] == species], x=\\"body_mass_g\\", color=\'red\') # Map the plotting functions to the appropriate species map_plot(penguins, \\"Adelie\\", \\"displot\\") map_plot(penguins, \\"Chinstrap\\", \\"kdeplot\\") map_plot(penguins, \\"Gentoo\\", \\"histplot\\") # Customize the plot g.set_axis_labels(\\"Body Mass (g)\\", \\"Density\\") g.add_legend(title=\\"Species\\") g.set_titles(\\"{col_name} Penguins\\") # Show plot plt.show() # Run the function to display the visualizations visualize_penguin_data()"},{"question":"Coding Assessment Question # Objective The goal of this task is to assess your ability to use the seaborn library for creating residual plots and analyzing residuals to diagnose regression model performance. # Background You are provided with a dataset and are required to perform residual analysis using seaborn\'s `residplot`. Residual plots are used to check the goodness of fit for regression models and diagnose potential problems. # Dataset You will use the \\"mpg\\" dataset which is available within seaborn. This dataset contains information about cars such as their weight, horsepower, miles per gallon (mpg), and other attributes. # Tasks 1. **Load the Dataset:** Load the \\"mpg\\" dataset from seaborn. 2. **Initial Residual Plot:** Create a simple residual plot for the regression of `mpg` on `horsepower`. This will visualize the residuals from the linear regression model. 3. **Higher-order Residual Plot:** Modify the model to use a second-order polynomial (quadratic) regression and create the corresponding residual plot. 4. **LOWESS Smoothing:** Add a LOWESS (Locally Weighted Scatterplot Smoothing) curve to the residual plot to emphasize any remaining structure. # Instructions - Write a function `create_residual_plots()` that performs these tasks. - The function should not take any parameters and should save the plots with filenames `simple_residual_plot.png`, `quadratic_residual_plot.png`, and `lowess_residual_plot.png`. - Ensure that each plot is clearly labeled with titles and axis labels for clarity. # Function Signature ```python def create_residual_plots(): pass ``` # Implementation Requirements 1. Use `seaborn` and `matplotlib` libraries for plotting. 2. Ensure the plots are saved with the proper filenames mentioned above. 3. Label the axes and the plots appropriately. # Example (for Task 2) ```python import seaborn as sns import matplotlib.pyplot as plt def create_residual_plots(): # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Create simple residual plot plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Simple Residual Plot\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.savefig(\\"simple_residual_plot.png\\") # Additional tasks to be implemented here #create_residual_plots() ``` # Submission - Submit the complete function implementing all tasks. - Ensure that all plots are generated and saved correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd import statsmodels.api as sm def create_residual_plots(): # Load dataset mpg = sns.load_dataset(\\"mpg\\") # Create simple residual plot plt.figure() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True) plt.title(\\"Simple Residual Plot\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.savefig(\\"simple_residual_plot.png\\") plt.close() # Quadratic residual plot mpg.dropna(subset=[\'horsepower\'], inplace=True) # Drop null values from horsepower X = mpg[[\'horsepower\']] X[\'horsepower2\'] = X[\'horsepower\'] ** 2 X = sm.add_constant(X) y = mpg[\'mpg\'] model = sm.OLS(y, X).fit() predictions = model.predict(X) residuals = y - predictions plt.figure() sns.scatterplot(x=predictions, y=residuals) plt.axhline(0, color=\'red\', linestyle=\'--\') plt.title(\\"Quadratic Residual Plot\\") plt.xlabel(\\"Fitted values\\") plt.ylabel(\\"Residuals\\") plt.savefig(\\"quadratic_residual_plot.png\\") plt.close() # LOWESS smoothing on residuals plt.figure() sns.residplot(x=predictions, y=residuals, lowess=True) plt.axhline(0, color=\'red\', linestyle=\'--\') plt.title(\\"LOWESS Smoothing Residual Plot\\") plt.xlabel(\\"Fitted values\\") plt.ylabel(\\"Residuals\\") plt.savefig(\\"lowess_residual_plot.png\\") plt.close()"},{"question":"# PLS Regression Exercise In this exercise, you will implement a Partial Least Squares (PLS) regression to find the relationship between two datasets: `X` as the predictors and `Y` as the targets. You will use `PLSRegression` from the `sklearn.cross_decomposition` module to achieve this. # Requirements 1. **Data Preparation:** - Create an artificial dataset consisting of predictors `X` and targets `Y`. - The predictors `X` should be a matrix with more variables than observations and should introduce some multicollinearity among the features. - The targets `Y` should be linearly dependent on `X` but with some added noise. 2. **PLS Regression Model:** - Implement the PLS regression using the `PLSRegression` class. - Fit the model on the generated dataset. - Use the model to transform the data and then predict the targets. 3. **Evaluation:** - Compare the predicted targets to the actual targets. - Calculate the Root Mean Squared Error (RMSE) to evaluate the prediction performance. # Instructions 1. **Generate the Dataset**: - Create a random dataset with 50 observations and 30 predictors (features) for `X`. - Introduce multicollinearity by making some features linear combinations of others. - Create the target matrix `Y` with 50 observations and 3 target variables, linearly dependent on `X` with some added Gaussian noise. 2. **Implement PLS Regression**: - Use the `PLSRegression` class from `sklearn.cross_decomposition`. - Fit the PLS model with `n_components=5`. 3. **Transform and Predict**: - Transform the dataset `X` using the fitted PLS model. - Predict the targets using the transformed data. 4. **Evaluate the Model**: - Calculate the RMSE between the predicted and actual target values. # Input and Output Formats - **Input**: There are no direct inputs. You will generate the dataset within your code. - **Output**: Print the RMSE value. # Constraints - The `X` matrix should have 50 observations and 30 predictors. - The `Y` matrix should have 50 observations and 3 target variables. - Use `n_components=5` for the PLSRegression model. - The added noise to `Y` should be Gaussian with a mean of 0 and a standard deviation of 0.1. # Performance Requirements - Ensure that the implementation is efficient and runs within a reasonable time frame. # Example Code Template ```python import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error # Step 1: Generate the dataset np.random.seed(42) X = np.random.randn(50, 30) X[:, 1] = X[:, 0] + np.random.normal(0, 0.1, 50) # Introduce multicollinearity # Create Y which is linearly dependent on X with added noise Y = np.dot(X, np.random.randn(30, 3)) + np.random.normal(0, 0.1, (50, 3)) # Step 2: Implement PLS Regression pls = PLSRegression(n_components=5) pls.fit(X, Y) # Step 3: Transform and Predict X_transformed = pls.transform(X) Y_pred = pls.predict(X) # Step 4: Evaluate the Model rmse = np.sqrt(mean_squared_error(Y, Y_pred)) print(f\\"RMSE: {rmse}\\") ``` **Note**: A detailed explanation for each step and the corresponding mathematical transformations should be included as comments in the code for better understanding.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def generate_data(n_samples, n_features, n_targets, noise_std=0.1): Generate artificial dataset with multicollinearity for PLS regression. Parameters: n_samples (int): Number of observations. n_features (int): Number of predictor variables. n_targets (int): Number of target variables. noise_std (float): Standard deviation of Gaussian noise added to the target. Returns: X (np.array): Predictor matrix. Y (np.array): Target matrix. np.random.seed(42) X = np.random.randn(n_samples, n_features) # Introduce multicollinearity by making some features linear combinations of others for i in range(1, n_features): X[:, i] = X[:, 0] + np.random.normal(0, noise_std, n_samples) # Create Y which is linearly dependent on X with added Gaussian noise true_coef = np.random.randn(n_features, n_targets) Y = np.dot(X, true_coef) + np.random.normal(0, noise_std, (n_samples, n_targets)) return X, Y def pls_regression(X, Y, n_components=5): Perform PLS regression and evaluate the model. Parameters: X (np.array): Predictor matrix. Y (np.array): Target matrix. n_components (int): Number of PLS components. Returns: rms_error (float): Root Mean Squared Error of the prediction. pls = PLSRegression(n_components=n_components) pls.fit(X, Y) Y_pred = pls.predict(X) rmse = np.sqrt(mean_squared_error(Y, Y_pred)) return rmse # Generate the dataset X, Y = generate_data(50, 30, 3) # Perform PLS regression and evaluate rmse = pls_regression(X, Y) print(f\\"RMSE: {rmse}\\")"},{"question":"**Objective:** Design a URL manipulation tool that utilizes the `urllib.parse` module. You are required to implement functions to parse a URL, extract specific components, and then modify and rebuild the URL. **Problem Statement:** You need to create a Python class `URLManipulator` with the following methods: 1. **`__init__(self, url: str):`** - Initializes the instance with the provided URL string. 2. **`get_component(self, component: str) -> str:`** - Returns the specified component of the URL. The component parameter can be one of: \'scheme\', \'netloc\', \'path\', \'params\', \'query\', \'fragment\', \'username\', \'password\', \'hostname\', \'port\'. - If the component is not present in the URL, return an empty string or `None` for `username`, `password`, `hostname`, and `port`. 3. **`update_component(self, component: str, value: str) -> None:`** - Updates the specified component of the URL with the provided value. 4. **`add_query_param(self, key: str, value: str) -> None:`** - Adds a query parameter to the URL. If the key already exists, append the value to a list of values for that key. If the key does not exist, add the key-value pair to the query. 5. **`remove_fragment(self) -> None:`** - Removes the fragment component from the URL. 6. **`get_modified_url(self) -> str:`** - Returns the modified URL as a string. **Example Usage:** ```python url = \\"http://docs.python.org:80/3/library/urllib.parse.html?highlight=params#url-parsing\\" url_tool = URLManipulator(url) # Get components print(url_tool.get_component(\\"scheme\\")) # Output: \'http\' print(url_tool.get_component(\\"hostname\\")) # Output: \'docs.python.org\' # Update components url_tool.update_component(\\"hostname\\", \\"www.example.com\\") url_tool.update_component(\\"port\\", \\"8080\\") # Add a query parameter url_tool.add_query_param(\\"new_param\\", \\"value1\\") # Remove fragment url_tool.remove_fragment() print(url_tool.get_modified_url()) # Output: \'http://www.example.com:8080/3/library/urllib.parse.html?highlight=params&new_param=value1\' ``` **Constraints:** - You may assume the input URL is a valid URL. - Component names will be valid and within the list of specified components. - Query parameter keys will be non-empty strings. **Notes:** - Utilize functions from `urllib.parse` to achieve the parsing and URL manipulation tasks. - Make sure to handle cases where certain components might not be present in the input URL. **Hints:** - Use `urllib.parse.urlparse` to break down the URL into components. - Use `urllib.parse.urlunparse` or `urllib.parse.urlunsplit` to rebuild the URL from components. - Use `urllib.parse.parse_qs` and `urllib.parse.urlencode` to handle query parameters. **Evaluation:** - Completeness: Does the class include all specified methods? - Correctness: Does each method correctly manipulate or return URL components? - Proper use of `urllib.parse` functions. - Code readability and organization.","solution":"from urllib.parse import urlparse, urlunparse, parse_qs, urlencode, ParseResult class URLManipulator: def __init__(self, url: str): self.url = url self.parsed = urlparse(self.url) self.query_params = parse_qs(self.parsed.query) def get_component(self, component: str) -> str: components = { \'scheme\': self.parsed.scheme, \'netloc\': self.parsed.netloc, \'path\': self.parsed.path, \'params\': self.parsed.params, \'query\': self.parsed.query, \'fragment\': self.parsed.fragment, \'username\': self.parsed.username, \'password\': self.parsed.password, \'hostname\': self.parsed.hostname, \'port\': self.parsed.port } return components.get(component, \'\') def update_component(self, component: str, value: str) -> None: if component == \'scheme\': self.parsed = self.parsed._replace(scheme=value) elif component == \'netloc\': self.parsed = self.parsed._replace(netloc=value) elif component == \'path\': self.parsed = self.parsed._replace(path=value) elif component == \'params\': self.parsed = self.parsed._replace(params=value) elif component == \'query\': self.parsed = self.parsed._replace(query=value) elif component == \'fragment\': self.parsed = self.parsed._replace(fragment=value) elif component == \'username\': netloc = self.parsed.netloc.replace(self.parsed.username, value) self.parsed = self.parsed._replace(netloc=netloc) elif component == \'password\': netloc = self.parsed.netloc.replace(self.parsed.password, value) self.parsed = self.parsed._replace(netloc=netloc) elif component == \'hostname\': netloc = self.parsed.netloc.replace(self.parsed.hostname, value) self.parsed = self.parsed._replace(netloc=netloc) elif component == \'port\': netloc = self.parsed.netloc.replace(str(self.parsed.port), value) self.parsed = self.parsed._replace(netloc=netloc) def add_query_param(self, key: str, value: str) -> None: if key in self.query_params: self.query_params[key].append(value) else: self.query_params[key] = [value] query = urlencode(self.query_params, doseq=True) self.parsed = self.parsed._replace(query=query) def remove_fragment(self) -> None: self.parsed = self.parsed._replace(fragment=\'\') def get_modified_url(self) -> str: return urlunparse(self.parsed)"},{"question":"<|Analysis Begin|> The provided documentation outlines the `shelve` module in Python, which is used for persistent storage of Python objects in a dictionary-like format. Key points about using `shelve` include: 1. `shelve.open` function opens a persistent dictionary using a filename. This dictionary can store arbitrary Python objects as values. 2. The module supports standard dictionary operations and adds methods such as `sync()` and `close()`. 3. Special considerations must be taken when working with mutable objects in the shelf, especially whether or not to use the `writeback` feature, which influences memory usage and the behavior of data persistence. 4. There are restrictions on concurrent read/write access and potential issues with the underlying database package (like `dbm`). Key aspects of the `shelve` module: - Opening and closing a shelf. - Reading from and writing to the shelf. - Handling mutable data and the `writeback` parameter. - Using the shelf as a context manager. <|Analysis End|> <|Question Begin|> # Persistent User Management System You have been tasked with creating a simple persistent user management system using the `shelve` module in Python. This system should allow you to add users, retrieve user information, update user data, and delete users. Each user will have a unique username, and their data will include their email and an age. Requirements: 1. Create a `UserManager` class that utilizes the `shelve` module to persist user data. 2. The `UserManager` class should support adding, retrieving, updating, and deleting users. 3. Use the `writeback=True` parameter when opening the shelf to handle mutable data correctly. 4. Ensure the shelf is properly closed after operations. Class Definition: ```python class UserManager: def __init__(self, filename: str): Initializes the UserManager with the given filename for the shelve database. Args: - filename (str): The base filename for the shelve database. pass def add_user(self, username: str, email: str, age: int): Adds a new user to the shelf. Args: - username (str): The username of the user. This should be unique. - email (str): The email address of the user. - age (int): The age of the user. Raises: - ValueError: If the username already exists in the shelf. pass def get_user(self, username: str) -> dict: Retrieves user information from the shelf. Args: - username (str): The username of the user to retrieve. Returns: - dict: A dictionary containing the user\'s information (email and age). Raises: - KeyError: If the username does not exist in the shelf. pass def update_user(self, username: str, email: Optional[str] = None, age: Optional[int] = None): Updates the user\'s information. Args: - username (str): The username of the user to update. - email (str, optional): The new email address of the user. If None, the email is not updated. - age (int, optional): The new age of the user. If None, the age is not updated. Raises: - KeyError: If the username does not exist in the shelf. pass def delete_user(self, username: str): Deletes a user from the shelf. Args: - username (str): The username of the user to delete. Raises: - KeyError: If the username does not exist in the shelf. pass def close(self): Closes the shelf. pass ``` Constraints: - All usernames will be unique strings. - Email will be a valid string, and age will be a positive integer. Example Usage: ```python if __name__ == \\"__main__\\": user_manager = UserManager(\\"users.db\\") # Adding a user user_manager.add_user(\\"johndoe\\", \\"johndoe@gmail.com\\", 30) # Retrieving user information user_info = user_manager.get_user(\\"johndoe\\") print(user_info) # Output: {\'email\': \'johndoe@gmail.com\', \'age\': 30} # Updating user information user_manager.update_user(\\"johndoe\\", age=31) user_info = user_manager.get_user(\\"johndoe\\") print(user_info) # Output: {\'email\': \'johndoe@gmail.com\', \'age\': 31} # Deleting a user user_manager.delete_user(\\"johndoe\\") # Closing the user manager user_manager.close() ``` Implement the `UserManager` class as specified above.","solution":"import shelve from typing import Optional class UserManager: def __init__(self, filename: str): Initializes the UserManager with the given filename for the shelve database. Args: - filename (str): The base filename for the shelve database. self.filename = filename self.shelf = shelve.open(filename, writeback=True) def add_user(self, username: str, email: str, age: int): Adds a new user to the shelf. Args: - username (str): The username of the user. This should be unique. - email (str): The email address of the user. - age (int): The age of the user. Raises: - ValueError: If the username already exists in the shelf. if username in self.shelf: raise ValueError(\\"Username already exists.\\") self.shelf[username] = {\'email\': email, \'age\': age} self.shelf.sync() def get_user(self, username: str) -> dict: Retrieves user information from the shelf. Args: - username (str): The username of the user to retrieve. Returns: - dict: A dictionary containing the user\'s information (email and age). Raises: - KeyError: If the username does not exist in the shelf. if username not in self.shelf: raise KeyError(\\"Username does not exist.\\") return self.shelf[username] def update_user(self, username: str, email: Optional[str] = None, age: Optional[int] = None): Updates the user\'s information. Args: - username (str): The username of the user to update. - email (str, optional): The new email address of the user. If None, the email is not updated. - age (int, optional): The new age of the user. If None, the age is not updated. Raises: - KeyError: If the username does not exist in the shelf. if username not in self.shelf: raise KeyError(\\"Username does not exist.\\") if email is not None: self.shelf[username][\'email\'] = email if age is not None: self.shelf[username][\'age\'] = age self.shelf.sync() def delete_user(self, username: str): Deletes a user from the shelf. Args: - username (str): The username of the user to delete. Raises: - KeyError: If the username does not exist in the shelf. if username not in self.shelf: raise KeyError(\\"Username does not exist.\\") del self.shelf[username] self.shelf.sync() def close(self): Closes the shelf. self.shelf.close()"},{"question":"**Question**: Stream Compression and Decompression with Checksum Validation You are tasked with designing a function to compress and decompress a large file in chunks using streaming compression objects provided by the `zlib` module. Additionally, implement checksum validation to ensure data integrity. # Function 1: `chunked_file_compression` - **Input**: - `file_path` (str): The path to the input file to be compressed. - `chunk_size` (int): The size of each chunk to be read and compressed. - `output_path` (str): The path where the compressed file will be saved. - `compression_level` (int, optional): The level of compression (0 to 9). Default is `zlib.Z_DEFAULT_COMPRESSION`. - **Output**: - A tuple containing the following: - Checksum of the original file (CRC-32). - Path to the compressed file. - **Constraints**: - Ensure that the function can handle files larger than available memory by processing in chunks. - Use the CRC-32 checksum algorithm from `zlib` for validation. # Function 2: `validate_and_decompress` - **Input**: - `compressed_file_path` (str): The path to the compressed file. - `original_file_checksum` (int): The CRC-32 checksum of the original file. - `chunk_size` (int): The size of each chunk to be read and decompressed. - `output_path` (str): The path where the decompressed file will be saved. - **Output**: - A boolean indicating whether the decompressed file\'s checksum matches the original file\'s checksum. - **Constraints**: - Ensure that the function can handle files larger than available memory by processing in chunks. - Compute the checksum as you decompress the file to validate the data integrity. # Example Usage: ```python original_checksum, compressed_file = chunked_file_compression(\'large_input_file.txt\', 1024, \'compressed_output.zlib\') is_valid = validate_and_decompress(\'compressed_output.zlib\', original_checksum, 1024, \'decompressed_output.txt\') print(\\"Data integrity validation:\\", \\"Passed\\" if is_valid else \\"Failed\\") ``` # Notes: - You may assume that the input file exists and is readable. - Handle any potential exceptions, such as file I/O errors and compression/decompression errors. Good luck!","solution":"import zlib def chunked_file_compression(file_path, chunk_size, output_path, compression_level=zlib.Z_DEFAULT_COMPRESSION): # Initialize the compressor and checksum compressor = zlib.compressobj(compression_level) crc32 = 0 # Open the input and output files with open(file_path, \'rb\') as input_file, open(output_path, \'wb\') as output_file: while True: chunk = input_file.read(chunk_size) if not chunk: break crc32 = zlib.crc32(chunk, crc32) compressed_chunk = compressor.compress(chunk) output_file.write(compressed_chunk) output_file.write(compressor.flush()) return crc32, output_path def validate_and_decompress(compressed_file_path, original_file_checksum, chunk_size, output_path): # Initialize the decompressor and checksum decompressor = zlib.decompressobj() crc32 = 0 # Open the compressed and output files with open(compressed_file_path, \'rb\') as compressed_file, open(output_path, \'wb\') as output_file: while True: chunk = compressed_file.read(chunk_size) if not chunk: break decompressed_chunk = decompressor.decompress(chunk) crc32 = zlib.crc32(decompressed_chunk, crc32) output_file.write(decompressed_chunk) output_file.write(decompressor.flush()) return crc32 == original_file_checksum"},{"question":"Problem Statement You are tasked with writing a script that takes a directory and a filename extension as command-line arguments and performs the following: 1. **List all files**: It should list all files in the specified directory that match the given filename extension. 2. **Copy Files**: It should copy all these files to a new subdirectory called `backup` within the specified directory. 3. **Compute Statistics**: For each copied file, calculate the mean and median of the sizes (in bytes) of all these files and print these statistics. Function Implementation 1. Implement the function `list_files_with_extension(directory: str, extension: str) -> List[str]` which: - Takes `directory` (the path to the directory) and `extension` (the file extension to search for) as inputs. - Returns a list of filenames in the directory that match the specified extension. 2. Implement the function `copy_files_to_backup(directory: str, files: List[str])` which: - Takes `directory` (the path to the directory) and `files` (the list of filenames to be copied) as inputs. - Creates a subdirectory called `backup` within the specified directory. - Copies all files from the list to the `backup` directory. 3. Implement the function `calculate_file_statistics(files: List[str]) -> Tuple[float, float]` which: - Takes `files` (a list of file paths) as input. - Returns the mean and median of the sizes (in bytes) of these files. 4. Finally, implement the `main` function: - Parse command-line arguments to get the `directory` and `extension`. - Use the above functions to list files, copy them, and compute statistics. - Print the calculated mean and median file sizes. Expected Input and Output 1. The script should be executed from the command line like this: ``` python310 script.py /path/to/directory .txt ``` 2. The output should list all the matched files, confirm the copying operation, and display the mean and median file sizes. Constraints - You can assume that the directory and file extensions provided are valid. - Handle exceptions for any IO operations gracefully. - Ensure that the script runs efficiently for directories with up to 1000 files. - Use the standard `os`, `shutil`, `argparse`, and `statistics` modules as required. Example Usage If the script is called with: ``` python310 script.py /path/to/mydir .txt ``` It should list all `.txt` files in `/path/to/mydir`, copy them to `/path/to/mydir/backup`, and subsequently print: ``` Files copied to backup: - file1.txt - file2.txt - ... File Statistics: Mean size: 1234 bytes Median size: 1200 bytes ``` Tips - Explore the documentation of each specified module to understand all required functionalities. - Think about edge cases, like if there are no files matching the extension, or if the backup directory already exists.","solution":"import os import shutil import statistics import argparse from typing import List, Tuple def list_files_with_extension(directory: str, extension: str) -> List[str]: files = [ os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(extension) ] return files def copy_files_to_backup(directory: str, files: List[str]): backup_dir = os.path.join(directory, \'backup\') os.makedirs(backup_dir, exist_ok=True) for file in files: shutil.copy2(file, backup_dir) def calculate_file_statistics(files: List[str]) -> Tuple[float, float]: sizes = [os.path.getsize(file) for file in files] mean_size = statistics.mean(sizes) median_size = statistics.median(sizes) return mean_size, median_size def main(): parser = argparse.ArgumentParser(description=\\"List, copy, and compute statistics of files with a given extension.\\") parser.add_argument(\'directory\', type=str, help=\'Path to the directory\') parser.add_argument(\'extension\', type=str, help=\'File extension to search for\') args = parser.parse_args() directory, extension = args.directory, args.extension files = list_files_with_extension(directory, extension) if files: print(\\"Files found: \\", files) copy_files_to_backup(directory, files) print(\\"Files copied to backup\\") mean_size, median_size = calculate_file_statistics(files) print(f\\"Mean size: {mean_size:.2f} bytes\\") print(f\\"Median size: {median_size:.2f} bytes\\") else: print(\\"No files found with the specified extension.\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Function Implementation Objective You are required to implement a Python function that mimics certain functionalities of the provided `python310` package, specifically focusing on safe string formatting and conversion of strings to doubles. Task 1. Implement a Python function `safe_snprintf(format_string, *args)` which: - Takes a `format_string` and any number of arguments `*args`. - Uses the `PyOS_snprintf` or equivalent logic to safely format the string. - Ensures that the string is properly null-terminated, does not exceed a specified maximum length, and handles edge cases effectively. 2. Implement a Python function `string_to_safe_double(input_string)` which: - Takes a single string `input_string`. - Uses the `PyOS_string_to_double` or equivalent logic to convert the string to a double. - Properly handles errors such as invalid inputs, overflow, and returns appropriate values along with any necessary error messages. Constraints - You are not allowed to use Python\'s native `float()` or `.format()` functions directly for these implementations. - The maximum length for any formatted string in `safe_snprintf` should be 100 characters. - Handle memory management manually if dealing with C-level strings. Input and Output Formats 1. For `safe_snprintf`: - Input: A format string and multiple arguments. - Output: A formatted string with a length not exceeding 100 characters. In case of truncation, indicate this in the return value and ensure proper null termination. 2. For `string_to_safe_double`: - Input: A single string representing a potential floating-point number. - Output: A tuple with two elements: 1. The double value (or `None` in case of an error). 2. An error message (or `None` if conversion is successful). Example ```python def safe_snprintf(format_string, *args): # Implementation here pass def string_to_safe_double(input_string): # Implementation here pass # Example usage: # Test safe_snprintf formatted_string = safe_snprintf(\\"Hello, %s! Your score is %d\\", \\"Alice\\", 95) print(formatted_string) # Should print: \\"Hello, Alice! Your score is 95\\" # Test string_to_safe_double result, error = string_to_safe_double(\\"123.456\\") print(result) # Should print: 123.456 print(error) # Should print: None result, error = string_to_safe_double(\\"abc\\") print(result) # Should print: None print(error) # Should print an appropriate error message ``` Notes - Emulate the behavior of `PyOS_snprintf` and `PyOS_string_to_double` as closely as possible using Python\'s capabilities. - Ensure the code is well-documented and follows best practices for error handling and memory management.","solution":"import re def safe_snprintf(format_string, *args): Safely formats the string using the provided format string and arguments. Ensures the string does not exceed 100 characters, with proper null termination. If truncated, indicates this in the return value. try: formatted_string = format_string % args except (TypeError, ValueError) as e: return \\"Error formatting string: {}\\".format(e) if len(formatted_string) > 100: return formatted_string[:100] + \\"...\\" return formatted_string def string_to_safe_double(input_string): Converts the string to a double safely. Returns a tuple where the first element is the double value (or None on failure), and the second element is an error message (or None on success). input_string = input_string.strip() if not input_string: return None, \\"Error: Empty string\\" # Validate for a proper floating-point number using regex if not re.fullmatch(r\'[+-]?(d+(.d*)?|.d+)([eE][+-]?d+)?\', input_string): return None, \\"Error: Invalid float format\\" try: value = float(input_string) return value, None except ValueError: return None, \\"Error converting string to double\\" # Example usage - for clarification #print(safe_snprintf(\\"Hello, %s! Your score is %d\\", \\"Alice\\", 95)) # \\"Hello, Alice! Your score is 95\\" #print(string_to_safe_double(\\"123.456\\")) # (123.456, None) #print(string_to_safe_double(\\"abc\\")) # (None, \\"Error: Invalid float format\\")"},{"question":"You are tasked with creating a simple text-based dashboard interface utilizing the `curses.panel` module. This interface will consist of several overlapping panels, each displaying different information. You need to write a Python function `create_dashboard()` that sets up the panels and allows the user to navigate through and interact with them using keyboard inputs. # Function Signature ```python def create_dashboard() -> None: pass ``` # Instructions 1. **Initial Setup**: - Create a base window using the curses module. - Create at least three panels, each of a different size and position. // Note: panels can be stacked using methods like `top_panel()` and `bottom_panel()` methods // 2. **Panel Content**: - Panel 1: Should display basic usage information. - Panel 2: Should display current system time and update it every second. - Panel 3: Should display a count of key presses made by the user. 3. **User Interaction**: - User should be able to switch the topmost panel using the \'n\' key. - User should be able to hide the topmost panel using the \'h\' key. - User should be able to show all hidden panels using the \'s\' key. - Exit the interface when the \'q\' key is pressed. # Implementation Details: - Use `curses.panel.new_panel()` to create new panels. - Use `curses.panel.update_panels()` to refresh and update the panels stack. - Panels should be well-defined and should not disappear unless hidden by the user - Update the screen using `curses.doupdate()` after each user action. # Example Behavior When running the program, the user sees the initial panels stacked on top of each other. As the user presses the \'n\' key, the next panel in the stack comes on top. Pressing \'h\' hides the currently visible panel, and \'s\' brings back all hidden panels. The system time displays the current time dynamically in one of the panels. # Constraints - Ensure that your program handles exceptions and cleans up the terminal correctly after exiting. - Ensure the user commands (\'n\', \'h\', \'s\', \'q\') are case-insensitive. - The dashboard should be responsive and ensure minimal flickering when panels are updated. # Performance Requirements Your implementation should ensure smooth navigation and fast response to user interactions. # Note You may use the following structure to build up your interface, though you have the freedom to structure your code as needed: ```python import curses import curses.panel from datetime import datetime import time def create_dashboard(): def main(stdscr): # Initialize screen and panels here pass curses.wrapper(main) if __name__ == \\"__main__\\": create_dashboard() ```","solution":"import curses import curses.panel from datetime import datetime import time def create_dashboard(): def main(stdscr): # Turn off cursor curses.curs_set(0) # Create windows for the panels height, width = stdscr.getmaxyx() win1 = curses.newwin(10, 40, 0, 0) win2 = curses.newwin(10, 40, 5, 5) win3 = curses.newwin(10, 40, 10, 10) # Create panels panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) panel3 = curses.panel.new_panel(win3) panels = [panel1, panel2, panel3] hidden_panels = [] # Content for panels win1.addstr(\\"Usage Information:n\'n\' - Next paneln\'h\' - Hide paneln\'s\' - Show all panelsn\'q\' - Quit\\") # Initial display panel1.top() curses.panel.update_panels() curses.doupdate() key_presses = 0 current_panel_index = 0 while True: now = datetime.now() win2.clear() win2.addstr(\\"Current Time:n\\" + now.strftime(\\"%Y-%m-%d %H:%M:%S\\")) win3.clear() win3.addstr(f\\"Key Presses:n{key_presses}\\") curses.panel.update_panels() curses.doupdate() key = stdscr.getch() key_presses += 1 if key in (ord(\'q\'), ord(\'Q\')): break elif key in (ord(\'n\'), ord(\'N\')): current_panel_index = (current_panel_index + 1) % len(panels) panels[current_panel_index].top() elif key in (ord(\'h\'), ord(\'H\')): top_panel = curses.panel.top_panel() hidden_panels.append(top_panel) top_panel.hide() current_panel_index = (current_panel_index + 1) % len(panels) elif key in (ord(\'s\'), ord(\'S\')): for p in hidden_panels: p.show() hidden_panels.clear() time.sleep(1) curses.wrapper(main) if __name__ == \\"__main__\\": create_dashboard()"},{"question":"Using the provided pandas testing and error handling functionalities, you are to implement a function that validates the data integrity of two pandas DataFrames intended to be compared. This function should: 1. Confirm that the DataFrames have the same structure (columns and indices). 2. Verify the data type of each column in both DataFrames is the same. 3. Compare the equality of the two DataFrames. If any assertion fails, the function should catch this exception and raise a custom error specific to the type of validation that failed (structure, data type, or content). # Function Signature ```python import pandas as pd import pandas.testing as tm def validate_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> str: Validate the data integrity of two pandas DataFrames. Parameters: df1 (pd.DataFrame): First DataFrame to compare. df2 (pd.DataFrame): Second DataFrame to compare. Returns: str: \\"Validation Passed\\" if all validations are successful. Raises: StructureError: If the structure of the DataFrames is not the same. DataTypeError: If the data types of the columns do not match. ContentMismatchError: If the content of the DataFrames does not match. # Implement your function here pass # Custom exception classes class StructureError(Exception): pass class DataTypeError(Exception): pass class ContentMismatchError(Exception): pass ``` # Instructions 1. **Structure Validation:** - Ensure that both DataFrames have the same columns and indices. - If they do not match, raise `StructureError`. 2. **Data Type Validation:** - Check that each column in both DataFrames has the same data type. - If any column type does not match, raise `DataTypeError`. 3. **Content Comparison:** - Use `tm.assert_frame_equal` to compare the contents of the DataFrames. - If the contents do not match, raise `ContentMismatchError`. # Example Usage ```python import pandas as pd # Example DataFrames df1 = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [4, 5, 6]}) df2 = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [4, 5, 6]}) df3 = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [4, 5, 7]}) df4 = pd.DataFrame({\'A\': [1, 2, 3], \'C\': [4, 5, 6]}) print(validate_dataframes(df1, df2)) # Should print \\"Validation Passed\\" print(validate_dataframes(df1, df3)) # Should raise ContentMismatchError print(validate_dataframes(df1, df4)) # Should raise StructureError ``` # Constraints - Do not use any loops; utilize pandas built-in methods for checking. - Ensure that any caught exceptions are specifically for the issue at hand.","solution":"import pandas as pd import pandas.testing as tm def validate_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> str: Validate the data integrity of two pandas DataFrames. Parameters: df1 (pd.DataFrame): First DataFrame to compare. df2 (pd.DataFrame): Second DataFrame to compare. Returns: str: \\"Validation Passed\\" if all validations are successful. Raises: StructureError: If the structure of the DataFrames is not the same. DataTypeError: If the data types of the columns do not match. ContentMismatchError: If the content of the DataFrames does not match. # Structure validation if not (df1.columns.equals(df2.columns) and df1.index.equals(df2.index)): raise StructureError(\\"DataFrames structure (columns or indices) does not match.\\") # Data type validation for col in df1.columns: if df1[col].dtype != df2[col].dtype: raise DataTypeError(f\\"Data type for column \'{col}\' does not match.\\") # Content comparison try: tm.assert_frame_equal(df1, df2) except AssertionError: raise ContentMismatchError(\\"DataFrame contents do not match.\\") return \\"Validation Passed\\" # Custom exception classes class StructureError(Exception): pass class DataTypeError(Exception): pass class ContentMismatchError(Exception): pass"}]'),D={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},R={class:"search-container"},z={class:"card-container"},q={key:0,class:"empty-state"},F=["disabled"],L={key:0},j={key:1};function O(n,e,l,m,i,r){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",R,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>i.searchQuery=o),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>i.searchQuery="")}," âœ• ")):d("",!0)]),t("div",z,[(a(!0),s(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),s("div",q,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[i.isLoading?(a(),s("span",j,"Loading...")):(a(),s("span",L,"See more"))],8,F)):d("",!0)])}const M=p(D,[["render",O],["__scopeId","data-v-787a1acc"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/51.md","filePath":"chatai/51.md"}'),N={name:"chatai/51.md"},X=Object.assign(N,{setup(n){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,X as default};
