import{_ as p,o as a,c as n,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(i,e,l,m,s,o){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-3322556c"]]),I=JSON.parse('[{"question":"# **Coding Assessment Question: Advanced Data Reshaping with Pandas** **Objective:** Your task is to write a Python function that performs a sequence of advanced data reshaping operations using the pandas library. You will need to use the following methods: `melt`, `pivot_table`, `stack`, and `unstack`. **Problem Statement:** You are given a dataset containing information about sales transactions made by different employees of a company. The dataset is in a wide format, and your task is to reshape it to analyze the total sales per employee, per month. You need to perform the steps below: 1. Convert the dataset from wide format to long format using the `melt` function. 2. Create a pivot table to summarize the total sales for each employee, each month. 3. Stack the pivot table to create a multi-index DataFrame. 4. Finally, unstack the DataFrame to revert to analyzing monthly sales by employee, ensuring the multi-level indices are used correctly. **Dataset:** The dataset is provided in wide format as a dictionary: ```python data = { \\"employee\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"], \\"region\\": [\\"North\\", \\"South\\", \\"East\\", \\"West\\"], \\"2023-01\\": [200, 150, 230, 180], \\"2023-02\\": [220, 160, 240, 190], \\"2023-03\\": [210, 170, 250, 200], \\"2023-04\\": [230, 180, 260, 210], } ``` **Function Specification:** Write a function `reshape_sales_data(data: dict) -> pd.DataFrame` that takes in the dataset as a dictionary and returns the reshaped DataFrame. # **Input:** - `data`: A dictionary containing the sales data of employees in wide format. # **Output:** - A DataFrame with multi-level indices summarizing the total sales per employee, per month. # **Constraints:** - You must use the pandas library. - Ensure your function handles the reshaping as described in the steps. **Example:** ```python import pandas as pd data = { \\"employee\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"], \\"region\\": [\\"North\\", \\"South\\", \\"East\\", \\"West\\"], \\"2023-01\\": [200, 150, 230, 180], \\"2023-02\\": [220, 160, 240, 190], \\"2023-03\\": [210, 170, 250, 200], \\"2023-04\\": [230, 180, 260, 210], } def reshape_sales_data(data): # Convert dictionary to DataFrame df = pd.DataFrame(data) # Melt the DataFrame to long format melted_df = df.melt(id_vars=[\\"employee\\", \\"region\\"], var_name=\\"month\\", value_name=\\"sales\\") # Create a pivot table pivot_table = pd.pivot_table(melted_df, values=\\"sales\\", index=[\\"employee\\", \\"region\\"], columns=\\"month\\", aggfunc=\\"sum\\") # Stack the pivot table to introduce multi-index stacked_df = pivot_table.stack().reset_index(name=\'total_sales\') # Unstack the DataFrame to revert analyzing by employee unstacked_df = stacked_df.set_index([\'employee\', \'month\']).unstack(level=\'month\') return unstacked_df # Function call reshaped_data = reshape_sales_data(data) print(reshaped_data) ``` **Expected Output:** The expected output is a DataFrame that represents the total sales per employee, per month, retaining the hierarchical structure of multi-level indices for ease of analysis.","solution":"import pandas as pd def reshape_sales_data(data): Reshape the sales data provided in dictionary format. Arguments: data -- dictionary containing employee sales data. Returns: DataFrame with reshaped data. # Convert dictionary to DataFrame df = pd.DataFrame(data) # Melt the DataFrame to long format melted_df = df.melt(id_vars=[\\"employee\\", \\"region\\"], var_name=\\"month\\", value_name=\\"sales\\") # Create a pivot table pivot_table = pd.pivot_table(melted_df, values=\\"sales\\", index=[\\"employee\\", \\"region\\"], columns=\\"month\\", aggfunc=\\"sum\\") # Stack the pivot table to introduce multi-index stacked_df = pivot_table.stack().reset_index(name=\'total_sales\') # Unstack the DataFrame to revert analyzing by employee unstacked_df = stacked_df.set_index([\'employee\', \'month\']).total_sales.unstack(level=\'month\') return unstacked_df"},{"question":"You are given a dataset containing information about diamond prices and various attributes. Your task is to load the dataset, create visualizations using Seaborn\'s `boxenplot`, and customize the plots according to the specified requirements. **Dataset:** The dataset `diamonds` is available within Seaborn\'s built-in datasets. It contains the following columns: - `carat`: Diamond weight. - `cut`: Quality of the cut (Fair, Good, Very Good, Premium, Ideal). - `color`: Diamond color, from J (worst) to D (best). - `clarity`: Clarity of the diamond (I1, SI2, SI1, VS2, VS1, VVS2, VVS1, IF). - `depth`: Total depth percentage. - `table`: Width of the top of the diamond relative to its widest point. - `price`: Price in US dollars. - `x`: Length in mm. - `y`: Width in mm. - `z`: Depth in mm. # Tasks 1. **Load the `diamonds` dataset** using the Seaborn library. 2. **Create a boxen plot visualizing the distribution of diamond prices** grouped by `cut` quality. Display the plot with the default settings. 3. **Create a second boxen plot** that visualizes the distribution of diamond prices grouped by `cut`, with an additional grouping by `color` using different hues. Ensure that the boxes are dodged to avoid overlap and add a small gap between them. 4. **Adjust the width of the boxes** in the second plot by setting the `width_method` parameter to `\\"linear\\"`. Set the width of the largest box to 0.5. 5. **Customize the appearances** of the second plot by: - Setting the outline color of the boxes to light grey. - Changing the thickness of the outline to 0.5. - Customizing the median lines with a thicker width (1.5) and blue color. - Customizing the outliers by setting their face color to grey and outline thickness to 0.5. # Constraints - Use Seaborn version 0.11.0 or later. - Write clean and readable code with appropriate comments. # Expected Output Your script should produce two plots: 1. A boxen plot showing the distribution of diamond prices grouped by `cut`. 2. A customized boxen plot as described in Task 3 to Task 5. # Example Usage Here\'s how you might call your functions: ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Task 2: Plot with default settings sns.boxenplot(data=diamonds, x=\\"cut\\", y=\\"price\\") plt.show() # Task 3: Plot with additional grouping by color sns.boxenplot(data=diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"color\\", dodge=True, gap=0.2) plt.show() # Task 4: Adjust the box width to be linear and width of largest box to 0.5 sns.boxenplot(data=diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"color\\", dodge=True, gap=0.2, width_method=\\"linear\\", width=0.5) plt.show() # Task 5: Customize the appearances sns.boxenplot( data=diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"color\\", dodge=True, gap=0.2, width_method=\\"linear\\", width=0.5, linewidth=0.5, linecolor=\\".7\\", line_kws=dict(linewidth=1.5, color=\\"blue\\"), flier_kws=dict(facecolor=\\".7\\", linewidth=0.5), ) plt.show() ``` Note: Ensure that your plots are properly labeled and that legends are displayed when appropriate.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_diamonds_dataset(): Load the diamonds dataset from seaborn. return sns.load_dataset(\\"diamonds\\") def plot_diamond_prices_by_cut(diamonds): Create and display a boxen plot visualizing the distribution of diamond prices grouped by cut. sns.boxenplot(data=diamonds, x=\\"cut\\", y=\\"price\\") plt.xlabel(\\"Cut Quality\\") plt.ylabel(\\"Price (US dollars)\\") plt.title(\\"Diamond Prices by Cut Quality\\") plt.show() def plot_diamond_prices_by_cut_and_color(diamonds): Create and display a boxen plot visualizing the distribution of diamond prices grouped by cut, with an additional grouping by color using different hues. sns.boxenplot(data=diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"color\\", dodge=True, gap=0.2) plt.xlabel(\\"Cut Quality\\") plt.ylabel(\\"Price (US dollars)\\") plt.title(\\"Diamond Prices by Cut Quality and Color\\") plt.legend(title=\\"Color\\") plt.show() def plot_customized_diamond_prices(diamonds): Create and display a customized boxen plot with specific appearance settings. sns.set(style=\\"whitegrid\\") sns.boxenplot( data=diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"color\\", dodge=True, gap=0.2, width_method=\\"linear\\", width=0.5, linewidth=0.5, linecolor=\\".7\\", line_kws=dict(linewidth=1.5, color=\\"blue\\"), flier_kws=dict(facecolor=\\".7\\", linewidth=0.5) ) plt.xlabel(\\"Cut Quality\\") plt.ylabel(\\"Price (US dollars)\\") plt.title(\\"Customized Diamond Prices by Cut and Color\\") plt.legend(title=\\"Color\\") plt.show() # Example of how to call these functions if __name__ == \\"__main__\\": diamonds = load_diamonds_dataset() plot_diamond_prices_by_cut(diamonds) plot_diamond_prices_by_cut_and_color(diamonds) plot_customized_diamond_prices(diamonds)"},{"question":"Objective: Design a custom serialization and deserialization mechanism for a Python class using the `copyreg` module. Problem Statement: You are given a class `Employee` that stores information about employees in an organization. The class is defined as follows: ```python class Employee: def __init__(self, name, position, salary): self.name = name self.position = position self.salary = salary ``` You need to implement a custom pickling and unpickling mechanism for `Employee` objects using the `copyreg` module. Specifically, you should: 1. Write a custom pickling function `pickle_employee` that takes an `Employee` object and returns a tuple (`Employee`, (name, position, salary)). 2. Register this custom pickling function for the `Employee` class using `copyreg.pickle`. 3. Test your implementation by creating an `Employee` object, pickling it using the `pickle` module, and then unpickling it to verify that the original object is correctly restored. Requirements: 1. Implement the custom pickling function and use `copyreg` to register it. 2. Ensure that the unpickled object is equivalent to the original object. 3. Provide test cases to demonstrate that the pickling and unpickling mechanism works as expected. Input and Output: - The class definition and the custom pickling function should be part of your code. - Create an instance of the `Employee` class and verify pickling and unpickling using the `pickle` module. - The test cases should print the deserialized `Employee` object\'s attributes to demonstrate successful serialization and deserialization. Example: ```python import copyreg import pickle class Employee: def __init__(self, name, position, salary): self.name = name self.position = position self.salary = salary def pickle_employee(employee): return Employee, (employee.name, employee.position, employee.salary) copyreg.pickle(Employee, pickle_employee) # Test serialization and deserialization if __name__ == \\"__main__\\": emp = Employee(\\"John Doe\\", \\"Software Engineer\\", 100000) pickled_emp = pickle.dumps(emp) unpickled_emp = pickle.loads(pickled_emp) print(unpickled_emp.name) # Should print \\"John Doe\\" print(unpickled_emp.position) # Should print \\"Software Engineer\\" print(unpickled_emp.salary) # Should print 100000 ``` Note: Ensure to include necessary imports and handle any exceptions that might arise during pickling or unpickling.","solution":"import copyreg import pickle class Employee: def __init__(self, name, position, salary): self.name = name self.position = position self.salary = salary def pickle_employee(employee): return Employee, (employee.name, employee.position, employee.salary) copyreg.pickle(Employee, pickle_employee) # Test serialization and deserialization if __name__ == \\"__main__\\": emp = Employee(\\"John Doe\\", \\"Software Engineer\\", 100000) pickled_emp = pickle.dumps(emp) unpickled_emp = pickle.loads(pickled_emp) print(unpickled_emp.name) # Should print \\"John Doe\\" print(unpickled_emp.position) # Should print \\"Software Engineer\\" print(unpickled_emp.salary) # Should print 100000"},{"question":"You are given a Python source code as a string, and you need to write a function that analyzes the symbol tables of this code using the `symtable` module. Your task is to extract detailed information about the identifiers (symbols) declared in the code and return a summary report. Function Signature ```python def analyze_symbols(code: str, filename: str) -> dict: ``` Input - `code`: A string containing the Python source code. - `filename`: A string representing the filename of the source code. Output - Return a dictionary containing the following information: - \'global_identifiers\': A list of global identifiers in the code. - \'functions\': A dictionary where the keys are function names and the values are dictionaries with information about each function, including: - \'parameters\': A list of parameter names for the function. - \'locals\': A list of local variables in the function. - \'globals\': A list of global variables accessed within the function. - \'nonlocals\': A list of non-local variables within the function. - \'frees\': A list of free variables within the function. - \'classes\': A dictionary where the keys are class names and the values are dictionaries with information about each class, including: - \'methods\': A list of method names in the class. Constraints - The code will be valid Python code. - The length of the `code` string will not exceed 10,000 characters. - The `filename` string will not exceed 100 characters. # Example ```python code = \'\'\' global_var = 10 def foo(x, y): local_var = x + y return local_var + global_var class Bar: def method(self): return \\"method in Bar\\" \'\'\' filename = \\"example.py\\" result = analyze_symbols(code, filename) print(result) ``` Expected output: ```python { \'global_identifiers\': [\'global_var\', \'foo\', \'Bar\'], \'functions\': { \'foo\': { \'parameters\': [\'x\', \'y\'], \'locals\': [\'local_var\'], \'globals\': [\'global_var\'], \'nonlocals\': [], \'frees\': [] } }, \'classes\': { \'Bar\': { \'methods\': [\'method\'] } } } ``` # Explanation - The global identifiers are `global_var`, `foo`, and `Bar`. - The function `foo` has parameters `x` and `y`, a local variable `local_var`, and accesses the global variable `global_var`. - The class `Bar` has one method `method`. Notes - Use the `symtable` module to parse and analyze the given code. - Ensure that the output dictionary is formatted as specified.","solution":"import symtable def analyze_symbols(code: str, filename: str) -> dict: Analyzes the symbol tables of the given Python source code and returns detailed information about the identifiers declared in the code. Parameters: code (str): A string containing the Python source code. filename (str): A string representing the filename of the source code. Returns: dict: A dictionary containing information about global identifiers, functions, and classes. sym_table = symtable.symtable(code, filename, \'exec\') # Helper functions def extract_func_info(func_table): return { \'parameters\': [sym for sym in func_table.get_parameters()], \'locals\': [sym for sym in func_table.get_locals() if sym not in func_table.get_parameters()], \'globals\': [sym for sym in func_table.get_globals()], \'nonlocals\': [sym for sym in func_table.get_nonlocals()], \'frees\': [sym for sym in func_table.get_frees()], } def extract_class_info(class_table): return { \'methods\': [child.get_name() for child in class_table.get_children() if child.get_type() == \'function\'] } # Main data structure to return result = { \'global_identifiers\': [sym for sym in sym_table.get_identifiers() if sym_table.lookup(sym).is_global()], \'functions\': {}, \'classes\': {} } # Process each child in the symbol table for child in sym_table.get_children(): if child.get_type() == \'function\': result[\'functions\'][child.get_name()] = extract_func_info(child) elif child.get_type() == \'class\': result[\'classes\'][child.get_name()] = extract_class_info(child) return result"},{"question":"# **Advanced Coding Assessment: Profiling and Optimizing a scikit-learn Algorithm** **Objective:** Demonstrate your understanding of scikit-learn optimization techniques through a practical coding task. **Task:** 1. **Implement a PCA algorithm using scikit-learn.** 2. **Profile the implementation to identify performance bottlenecks.** 3. **Optimize the implementation using either Cython or another appropriate tool discussed in the documentation.** **Instructions:** 1. **Part 1: Implement PCA** - Write a Python function named `perform_pca` that: - Takes as input a 2D numpy array (X) of shape ((n_samples, n_features)) and number of components (n_components). - Applies Principal Component Analysis (PCA) using scikit-learn\'s `PCA` class. - Returns the transformed data. ```python import numpy as np from sklearn.decomposition import PCA def perform_pca(X: np.ndarray, n_components: int) -> np.ndarray: # Your code here pass ``` 2. **Part 2: Profile the Implementation** - Use profiling tools (`%timeit`, `%prun`, `line_profiler`) to measure the execution time and identify hotspots in the code. - Provide a summary of your profiling results, focusing on the functions or lines contributing most to the runtime. 3. **Part 3: Optimize the Implementation** - Reimplement the identified bottleneck (if any) using Cython or other appropriate optimization techniques. - Write a Cython function, `perform_pca_optimized`, that replicates the functionality of the initial implementation but with better performance. - Demonstrate the performance improvement by comparing the execution times before and after optimization. ```python # Cython implementation example (to be filled by the student) # Save this as \'pca_optimized.pyx\' and use Cython to compile it import numpy as np cimport numpy as np from sklearn.decomposition cimport PCA def perform_pca_optimized(np.ndarray[float, ndim=2] X, int n_components): # Your optimized Cython code here pass ``` **Constraints:** - Assume the dataset (X) can be large, e.g., up to ( 10^6 ) samples and ( 10^2 ) features. - The optimized function should aim for at least a 2x speedup over the original implementation. **Submission:** - Python file containing the `perform_pca` and profiled initial implementation. - Summary of profiling results and identified bottlenecks. - `.pyx` file for the optimized implementation and corresponding Python wrapper function. - Comparison of execution times before and after optimization. This question tests your ability to implement machine learning algorithms, profile Python code to find bottlenecks, and optimize the implementation using advanced techniques discussed in the documentation.","solution":"import numpy as np from sklearn.decomposition import PCA def perform_pca(X: np.ndarray, n_components: int) -> np.ndarray: Perform PCA on the dataset X with the given number of components. Parameters: X (np.ndarray): A 2D numpy array of shape (n_samples, n_features). n_components (int): The number of principal components. Returns: np.ndarray: Transformed data with shape (n_samples, n_components). pca = PCA(n_components=n_components) transformed_data = pca.fit_transform(X) return transformed_data"},{"question":"# Task Using the seaborn `objects` module, you are required to produce a series of faceted plots from the `diamonds` dataset. # Instructions: 1. Load the `diamonds` dataset using `seaborn`. 2. Create a `Plot` object that visualizes the relationship between the `carat` and `price` of the diamonds. 3. Facet this plot over the `color` variable. 4. Arrange the facets using the `wrap` option to create a layout of 3 columns. 5. Ensure that the x-axes of the facets are not shared across the plots. 6. Customize the title of each facet to display \\"Diamond color: [Color Level]\\" where `[Color Level]` is the respective level of the `color` variable. # Output: The expected output is a faceted plot where: - The main plot shows the relationship between `carat` and `price`. - The data is split across multiple subplots based on the `color` variable. - There are 3 columns in the layout. - The x-axes of the facets are independently scaled. - Each facet has a customized title indicating the diamond color. # Example Solution ```python import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the initial plot p = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()) # Facet the plot over the \\"color\\" variable # Use \'wrap\' to create a layout of 3 columns # Do not share x-axes across facets # Customize the titles of the facets p.facet(\\"color\\", wrap=3).share(x=False).label(title=lambda x: f\\"Diamond color: {x}\\") # Show the plot p.show() ``` # Constraints: - Use the seaborn `objects` module for all tasks. - Ensure the facets are correctly labeled and independently scaled on the x-axis. This question tests students\' understanding of: - Loading datasets with seaborn. - Creating basic scatter plots. - Faceting plots using multiple customization options. - Propagating these customizations correctly.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the initial plot p = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").add(so.Dots()) # Facet the plot over the \\"color\\" variable # Use \'wrap\' to create a layout of 3 columns # Do not share x-axes across facets # Customize the titles of the facets p = p.facet(\\"color\\", wrap=3).share(x=False).label(title=lambda x: f\\"Diamond color: {x}\\") # Show the plot p.show()"},{"question":"# Pandas DataFrame Display Options Manipulation Objective Your task is to demonstrate your understanding of pandas\' display options by performing a series of transformations on a given DataFrame. You will implement functions to set, get, reset options, and use the context manager to temporarily set options. Instructions 1. **Set Display Options** - Implement a function `set_display_options(max_rows, max_columns, precision)` that takes three parameters: - `max_rows` (int): The maximum number of rows to display. - `max_columns` (int): The maximum number of columns to display. - `precision` (int): The number of decimal places to display for floating-point numbers. - This function should set the corresponding pandas display options to the provided values. 2. **Get Display Options** - Implement a function `get_display_options()` that returns a dictionary with the current values of `display.max_rows`, `display.max_columns`, and `display.precision`. 3. **Reset Display Options** - Implement a function `reset_display_options()` that resets `display.max_rows`, `display.max_columns`, and `display.precision` to their default values. 4. **Use Context Manager for Temporary Settings** - Implement a function `temporary_settings_example(df)` that does the following: - Takes as input a DataFrame `df`. - Temporarily sets `display.max_rows` to 5 and `display.max_columns` to 3 using the `option_context` context manager. - Prints the DataFrame `df` within this context to see the temporary settings in effect. Constraints - You must use the options API provided by pandas to customize global settings. - Assume that pandas has already been imported as `import pandas as pd`. Expected Function Definitions ```python def set_display_options(max_rows: int, max_columns: int, precision: int) -> None: pass def get_display_options() -> dict: pass def reset_display_options() -> None: pass def temporary_settings_example(df: pd.DataFrame) -> None: pass ``` Example Usage ```python import pandas as pd # Sample DataFrame df = pd.DataFrame({ \'A\': range(10), \'B\': range(10, 20), \'C\': range(20, 30) }) # Set display options set_display_options(max_rows=10, max_columns=2, precision=3) # Get current display options print(get_display_options()) # Output should be something like: # {\'display.max_rows\': 10, \'display.max_columns\': 2, \'display.precision\': 3} # Use context manager for temporary settings temporary_settings_example(df) # Reset display options to default reset_display_options() ``` Notes - Make sure you test each function individually. - Refer to the pandas documentation for more details on how to use the options API.","solution":"import pandas as pd def set_display_options(max_rows: int, max_columns: int, precision: int) -> None: Sets pandas display options for max rows, max columns, and floating point precision. pd.set_option(\'display.max_rows\', max_rows) pd.set_option(\'display.max_columns\', max_columns) pd.set_option(\'display.precision\', precision) def get_display_options() -> dict: Returns a dictionary with the current values of display.max_rows, display.max_columns, and display.precision. return { \'display.max_rows\': pd.get_option(\'display.max_rows\'), \'display.max_columns\': pd.get_option(\'display.max_columns\'), \'display.precision\': pd.get_option(\'display.precision\') } def reset_display_options() -> None: Resets display.max_rows, display.max_columns, and display.precision to their default values. pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.max_columns\') pd.reset_option(\'display.precision\') def temporary_settings_example(df: pd.DataFrame) -> None: Temporarily sets display.max_rows to 5 and display.max_columns to 3 using the option_context context manager, prints the DataFrame to see the temporary settings in effect. with pd.option_context(\'display.max_rows\', 5, \'display.max_columns\', 3): print(df)"},{"question":"**Function Implementation on Markdown Parsing** Python 3.10 has introduced various features and improvements, which, when combined with your knowledge of core Python concepts, can help in crafting robust solutions to complex problems. In this task, you are required to implement a function that parses a given markdown string and converts it into HTML. Description You need to create a function called `markdown_to_html(markdown: str) -> str` that converts a simple markdown string into an HTML string. The function should correctly handle headers, bold and italic text, and links. # Markdown Syntax to Support 1. **Headers**: Lines that start with one or more `#` characters, followed by a space and the header text. The number of `#` characters indicates the header level. - Example: `# Header 1` converts to `<h1>Header 1</h1>` - Example: ` Header 2` converts to `<h2>Header 2</h2>` 2. **Bold**: Text surrounded by double asterisks (`**`). - Example: `**bold text**` converts to `<strong>bold text</strong>` 3. **Italic**: Text surrounded by single asterisks (`*`). - Example: `*italic text*` converts to `<em>italic text</em>` 4. **Links**: Text in the format `[link text](URL)` should be converted to a hyperlink. - Example: `[example](http://example.com)` converts to `<a href=\\"http://example.com\\">example</a>` Constraints - The input markdown string will contain only valid markdown snippets as described above. - The input string will not exceed 10,000 characters in length. - There will be no nested markdown tags, e.g., **bold text** with *italic* inside. Function Signature ```python def markdown_to_html(markdown: str) -> str: ``` Examples **Example 1:** ```python markdown = \\"# Header 1\\" print(markdown_to_html(markdown)) ``` Output: ```html <h1>Header 1</h1> ``` **Example 2:** ```python markdown = \\" Header 2\\" print(markdown_to_html(markdown)) ``` Output: ```html <h2>Header 2</h2> ``` **Example 3:** ```python markdown = \\"**bold text**\\" print(markdown_to_html(markdown)) ``` Output: ```html <strong>bold text</strong> ``` **Example 4:** ```python markdown = \\"[example](http://example.com)\\" print(markdown_to_html(markdown)) ``` Output: ```html <a href=\\"http://example.com\\">example</a> ``` **Example 5:** ```python markdown = \\"# Header 1n**bold text** *italic text*\\" print(markdown_to_html(markdown)) ``` Output: ```html <h1>Header 1</h1><strong>bold text</strong> <em>italic text</em> ``` # Notes - Ensure your function efficiently handles the conversion within reasonable computation time. - The output HTML should be a well-formed string.","solution":"import re def markdown_to_html(markdown: str) -> str: Converts a simple markdown string to HTML. # Convert headers markdown = re.sub(r\'^(#{1,6})s+(.*)\', lambda m: f\\"<h{len(m.group(1))}>{m.group(2)}</h{len(m.group(1))}>\\", markdown, flags=re.MULTILINE) # Convert bold text markdown = re.sub(r\'**(.*?)**\', r\'<strong>1</strong>\', markdown) # Convert italic text markdown = re.sub(r\'*(.*?)*\', r\'<em>1</em>\', markdown) # Convert links markdown = re.sub(r\'[(.*?)]((.*?))\', r\'<a href=\\"2\\">1</a>\', markdown) return markdown"},{"question":"# Shadow Password Management with `spwd` Module In this task, you are required to write a Python script that interacts with the Unix shadow password database using the `spwd` module. Your script should perform the following operations: 1. **Fetch User Details**: Write a function `fetch_user_details(username: str) -> dict` that: - Takes a username as input. - Returns a dictionary with the user\'s shadow password entry details if the user exists. - Raises a `PermissionError` if the necessary privileges are not available. - Raises a `KeyError` if the user cannot be found. 2. **Check Password Expiry**: Write a function `password_expiry_status(username: str) -> str` that: - Takes a username as input. - Returns a string indicating whether the user\'s password has expired, will expire soon, or is still valid. - Return one of the following strings based on `sp_expire` and `sp_warn` attributes: - `\\"Expired\\"` - if the password is expired. - `\\"Expiring Soon\\"` - if the password is within the warning period before expiry. - `\\"Valid\\"` - if the password is neither expired nor within the warning period. 3. **List All Usernames**: Write a function `list_all_usernames() -> list` that: - Returns a list of all usernames in the shadow password database. # Input/Output Formats - **Input**: - For `fetch_user_details(username)`: A string `username`. - For `password_expiry_status(username)`: A string `username`. - For `list_all_usernames()`: No input. - **Output**: - For `fetch_user_details(username)`: A dictionary with shadow password attributes or appropriate error raised. - For `password_expiry_status(username)`: A string `\\"Expired\\"`, `\\"Expiring Soon\\"`, or `\\"Valid\\"`. - For `list_all_usernames()`: A list of strings, each representing a username. # Example Usage ```python # Example of fetch_user_details for a user \'johndoe\' fetch_user_details(\'johndoe\') # Output: {\'sp_namp\': \'johndoe\', \'sp_pwdp\': \'x\', \'sp_lstchg\': 18733, \'sp_min\': 0, \'sp_max\': 99999, \'sp_warn\': 7, \'sp_inact\': -1, \'sp_expire\': -1, \'sp_flag\': -1} # Example of password_expiry_status for a user \'johndoe\' password_expiry_status(\'johndoe\') # Output: \'Valid\' # Example of list_all_usernames list_all_usernames() # Output: [\'root\', \'johndoe\', \'alice\', \'bob\'] ``` # Notes - Remember to handle exceptions and errors gracefully. - Ensure your script has appropriate permissions to access the shadow password database. - Use the `spwd` module functions effectively to achieve the desired results. # Constraints - Only Unix systems with the shadow password capability. - Python 3.10 or higher. - Ensure to run as a user with appropriate privileges.","solution":"import spwd import time def fetch_user_details(username): Fetch the user details from the shadow password database. Parameters: username (str): The username to fetch details for. Returns: dict: Dictionary of user details. Raises: PermissionError: If necessary privileges are not available. KeyError: If the user is not found. try: user_details = spwd.getspnam(username) return { \'sp_namp\': user_details.sp_namp, \'sp_pwdp\': user_details.sp_pwdp, \'sp_lstchg\': user_details.sp_lstchg, \'sp_min\': user_details.sp_min, \'sp_max\': user_details.sp_max, \'sp_warn\': user_details.sp_warn, \'sp_inact\': user_details.sp_inact, \'sp_expire\': user_details.sp_expire, \'sp_flag\': user_details.sp_flag } except PermissionError: raise PermissionError(\\"Necessary privileges are not available to access the shadow password database.\\") except KeyError: raise KeyError(f\\"User {username} not found in the shadow password database.\\") def password_expiry_status(username): Check the password expiry status for a user. Parameters: username (str): The username to check the password expiry status for. Returns: str: One of \\"Expired\\", \\"Expiring Soon\\", or \\"Valid\\". try: user_details = fetch_user_details(username) current_time = time.time() / (24 * 3600) # current time in days since epoch if user_details[\'sp_expire\'] != -1 and current_time > user_details[\'sp_expire\']: return \\"Expired\\" elif user_details[\'sp_expire\'] != -1 and user_details[\'sp_warn\'] != -1 and (user_details[\'sp_expire\'] - current_time) <= user_details[\'sp_warn\']: return \\"Expiring Soon\\" else: return \\"Valid\\" except KeyError: raise KeyError(f\\"User {username} not found.\\") except PermissionError: raise PermissionError(\\"Necessary privileges are not available to access the shadow password database.\\") def list_all_usernames(): List all usernames in the shadow password database. Returns: list: List of all usernames. Raises: PermissionError: If necessary privileges are not available. try: all_users = spwd.getspall() return [user.sp_namp for user in all_users] except PermissionError: raise PermissionError(\\"Necessary privileges are not available to access the shadow password database.\\")"},{"question":"<|Analysis Begin|> The provided documentation explains the broadcasting semantics in PyTorch, which supports NumPy-like broadcasting for tensor operations. The central idea is that PyTorch can automatically expand tensors to make operations possible without making copies of the data. For two tensors to be broadcastable, the following must hold: - Each tensor has at least one dimension. - Dimensions must align from the trailing dimension such that the sizes are either equal or one of them is 1 or does not exist. The documentation also covers in-place operations and backward compatibility concerns. Key points to consider: 1. Tensor shapes and dimensions. 2. Requirements for tensors to be broadcastable. 3. Resulting tensor shapes after a broadcastable operation. 4. In-place operation constraints. Using this detailed explanation, I can design a coding assessment question to test students\' understanding of broadcast semantics and how to manipulate tensors in PyTorch. <|Analysis End|> <|Question Begin|> # PyTorch Broadcasting Semantics Problem Statement You are tasked to implement a function in PyTorch that takes two tensors as input and returns the resulting tensor after applying the addition operation if the tensors are broadcastable. If the tensors are not broadcastable, the function should raise a `ValueError` with a message explaining why the tensors cannot be broadcasted. Function Signature ```python def add_tensors_with_broadcast(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Adds two tensors if they are broadcastable, otherwise raises a ValueError. :param tensor1: First input tensor. :param tensor2: Second input tensor. :return: A tensor resulting from broadcasting addition. :raises ValueError: if the tensors cannot be broadcasted. pass ``` Requirements 1. **Input:** - `tensor1`: a `torch.Tensor` of any shape. - `tensor2`: a `torch.Tensor` of any shape. 2. **Output:** - A `torch.Tensor` resulting from adding `tensor1` and `tensor2` using broadcasting semantics. - Raise `ValueError` with a message if the tensors are not broadcastable. 3. **Constraints:** - Do not use in-place operations (`add_`). 4. **Examples:** ```python import torch # Example 1: Valid broadcastable tensors tensor1 = torch.empty(5, 1, 4, 1) tensor2 = torch.empty(3, 1, 1) result = add_tensors_with_broadcast(tensor1, tensor2) # result should be of shape (5, 3, 4, 1) # Example 2: Non-broadcastable tensors tensor1 = torch.empty(5, 2, 4, 1) tensor2 = torch.empty(3, 1, 1) try: result = add_tensors_with_broadcast(tensor1, tensor2) except ValueError as e: print(e) # Output should be \\"Tensors are not broadcastable: 2 != 3 at dimension 1\\" # Example 3: Another valid case tensor1 = torch.empty(1) tensor2 = torch.empty(3, 1, 7) result = add_tensors_with_broadcast(tensor1, tensor2) # result should be of shape (3, 1, 7) ``` 5. **Performance Considerations:** - Ensure your solution is optimized to handle tensors of various sizes efficiently. 6. **Testing:** - Include test cases that cover both broadcastable and non-broadcastable tensor pairs.","solution":"import torch def add_tensors_with_broadcast(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Adds two tensors if they are broadcastable, otherwise raises a ValueError. :param tensor1: First input tensor. :param tensor2: Second input tensor. :return: A tensor resulting from broadcasting addition. :raises ValueError: if the tensors cannot be broadcasted. try: result = tensor1 + tensor2 return result except RuntimeError as e: raise ValueError(f\\"Tensors are not broadcastable: {e}\\")"},{"question":"# Abstract Base Classes and Custom Iterable Objective You will need to create an abstract base class using the `abc` module to define a custom iterable. This class will also include a method to calculate the summation of its elements, which should be implemented by any concrete subclass. # Task 1. Define an abstract base class named `CustomIterable` which inherits from `abc.ABC` and includes: - An abstract method `__iter__()` to allow iteration over the elements. - An abstract method `get_sum()` which returns the sum of all elements. 2. Implement a subclass `ListContainer` which: - Inherits from `CustomIterable`. - Initializes with a list of numbers. - Implements the `__iter__()` and `get_sum()` methods. 3. Write code to demonstrate the implementation and usage of these classes. Constraints - You cannot instantiate `CustomIterable` directly. - The `__iter__()` method should return an iterator over the elements. - The `get_sum()` method should return the summation of all numbers in the list. Example ```python from abc import ABC, abstractmethod class CustomIterable(ABC): @abstractmethod def __iter__(self): This method should be overridden to return an iterator. pass @abstractmethod def get_sum(self): This method should be overridden to return the sum of all elements. pass class ListContainer(CustomIterable): def __init__(self, elements): self.elements = elements def __iter__(self): return iter(self.elements) def get_sum(self): return sum(self.elements) # Usage example container = ListContainer([1, 2, 3, 4]) print(\\"Elements in container:\\", list(container)) print(\\"Sum of elements:\\", container.get_sum()) ``` Expected Output: ``` Elements in container: [1, 2, 3, 4] Sum of elements: 10 ``` Notes - Ensure you handle any edge cases, for instance, an empty list. - Use the `abstractmethod` decorator from the `abc` module to define abstract methods.","solution":"from abc import ABC, abstractmethod class CustomIterable(ABC): @abstractmethod def __iter__(self): This method should be overridden to return an iterator. pass @abstractmethod def get_sum(self): This method should be overridden to return the sum of all elements. pass class ListContainer(CustomIterable): def __init__(self, elements): self.elements = elements def __iter__(self): return iter(self.elements) def get_sum(self): return sum(self.elements) # Usage example container = ListContainer([1, 2, 3, 4]) print(\\"Elements in container:\\", list(container)) print(\\"Sum of elements:\\", container.get_sum())"},{"question":"You are required to write a Python function that reads a \\"setup.cfg\\" file and returns a dictionary where keys are command names, and values are dictionaries of option-value pairs applicable to those commands. This function should help users understand which options are set for different Distutils commands in their setup configuration file. Function Signature ```python def parse_setup_cfg(file_path: str) -> dict: Parses the setup.cfg file and returns a dictionary of commands with their options. Args: - file_path (str): The file path of the setup.cfg file to be parsed. Returns: - dict: A dictionary where keys are command names and values are dictionaries of option-value pairs. ``` Input - `file_path`: A string representing the path to the \\"setup.cfg\\" file. Output - A dictionary where: - Keys are command names (e.g., \'build_ext\'). - Values are dictionaries of options and their respective values (e.g., {\'inplace\': \'1\'}). Example For the following \\"setup.cfg\\" file content: ``` [build_ext] inplace = 1 include_dirs = /usr/local/include [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` The function should return: ```python { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/usr/local/include\\" }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"Greg Ward <gward@python.net>\\", \\"doc_files\\": \\"CHANGES.txt README.txt USAGE.txt doc/ examples/\\" } } ``` Constraints - The parsing should ignore comments and blank lines. - Options that span multiple lines should be concatenated into single string values separated by spaces. - Assume the configuration file is well-formed. Additional Information - The function should be robust enough to handle various possible configurations as outlined in the provided documentation. - You may use Python\'s built-in libraries to help with file reading and string processing.","solution":"import configparser def parse_setup_cfg(file_path: str) -> dict: Parses the setup.cfg file and returns a dictionary of commands with their options. Args: - file_path (str): The file path of the setup.cfg file to be parsed. Returns: - dict: A dictionary where keys are command names and values are dictionaries of option-value pairs. config = configparser.ConfigParser() config.read(file_path) result = {} for section in config.sections(): options = {} for option in config.options(section): options[option] = config.get(section, option) result[section] = options return result"},{"question":"Objective The goal of this task is to test your comprehension of the Copy-on-Write (CoW) feature in pandas, including its effect on performance, memory usage, and predictable behavior of the pandas operations. Problem Statement You are provided with a pandas DataFrame representing the stock prices of various companies over a week. Your job is to implement a function `process_stock_data(df: pd.DataFrame) -> pd.DataFrame`, which performs the following operations while adhering to CoW principles: 1. **Update Values Safely**: Safely increase the stock prices of a particular company using `iloc` and ensure no other DataFrame derived from the original DataFrame is affected. 2. **Reset Index and Modify**: Reset the DataFrame index, then change one of the stock prices to another value, making sure this modification does not affect the original DataFrame. 3. **Avoid Chained Assignments**: Ensure no chained assignments are used in your operations as they are not allowed under CoW. 4. **Access and Modify NumPy Array**: Access the underlying NumPy array, make it writeable, and safely modify a value ensuring it doesn\'t lead to unintended side effects. 5. **Return the Safely Modified DataFrame**: Return the modified DataFrame after all the above operations. Input - `df`: A pandas DataFrame containing stock prices. It has the following structure: ```python df = pd.DataFrame({ \\"Company\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\"], \\"Monday\\": [100, 150, 130, 170, 120], \\"Tuesday\\": [110, 160, 135, 175, 125], \\"Wednesday\\": [115, 155, 140, 165, 130], \\"Thursday\\": [120, 150, 135, 160, 135], \\"Friday\\": [125, 145, 130, 155, 140], }) ``` Operations to Perform 1. **Update Values Safe**: Increase the stock price of \\"Company A\\" on \\"Monday\\" by 10. 2. **Reset Index and Modify**: Reset the index of the DataFrame and change the stock price for \\"Company B\\" on \\"Tuesday\\" to 180. 3. **Access and Modify NumPy Array**: Access the underlying NumPy array of the DataFrame, make it writeable, and set the stock price of \\"Company C\\" on \\"Wednesday\\" to 150. 4. **Return the Modified DataFrame**: Ensure the returned DataFrame includes all safe modifications described above. Constraints - Do not use chained assignments. - Ensure not to affect any other DataFrame derived from the original DataFrame. - Handle memory and performance carefully with CoW principles. Example Below is an example demonstrating the expected input and output. ```python import pandas as pd df = pd.DataFrame({ \\"Company\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\"], \\"Monday\\": [100, 150, 130, 170, 120], \\"Tuesday\\": [110, 160, 135, 175, 125], \\"Wednesday\\": [115, 155, 140, 165, 130], \\"Thursday\\": [120, 150, 135, 160, 135], \\"Friday\\": [125, 145, 130, 155, 140], }) def process_stock_data(df: pd.DataFrame) -> pd.DataFrame: # Your implementation goes here. pass output_df = process_stock_data(df) print(output_df) ``` The expected output should retain the same company columns but reflect the specific safely performed updates. Evaluation Criteria - Correctness of modifications. - Adherence to CoW principles. - Proper handling of DataFrame views and slices. - Performance considerations.","solution":"import pandas as pd def process_stock_data(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Update Values Safely for \\"Company A\\" on \\"Monday\\". df_copy = df.copy() df_copy.iloc[0, 1] += 10 # Safe modification using .iloc # Step 2: Reset Index and Modify the stock price for \\"Company B\\" on \\"Tuesday\\". df_reset = df_copy.reset_index(drop=True) df_reset.at[1, \\"Tuesday\\"] = 180 # Step 3: Access and Modify NumPy Array for \\"Company C\\" on \\"Wednesday\\". df_numpy = df_reset.to_numpy(copy=True) df_numpy[2, 3] = 150 # This changes the stock price for \\"Company C\\" on \\"Wednesday\\". # Converting NumPy array back to DataFrame columns = df_reset.columns df_modified = pd.DataFrame(df_numpy, columns=columns) return df_modified"},{"question":"# Unsupervised Learning with Clustering Objective You are given a dataset that contains information about different data points in a multi-dimensional space. Your task is to implement a function that clusters these data points using the K-means algorithm from the scikit-learn library. Additionally, you will evaluate the clustering performance using the silhouette score. Dataset The dataset is provided as a CSV file, `data.csv`, with the following format: - Each row represents a data point. - Each column represents a feature. Function Signature ```python import numpy as np import pandas as pd from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score def perform_clustering(file_path: str, n_clusters: int) -> float: Perform K-means clustering on the given dataset and return the silhouette score. Parameters: file_path (str): The path to the CSV file containing the dataset. n_clusters (int): The number of clusters to form. Returns: float: The silhouette score of the clustering. ``` Input - `file_path`: A string representing the path to the CSV file containing the dataset. - `n_clusters`: An integer representing the number of clusters to form. Output - A float representing the silhouette score of the clustering. Constraints - The dataset will have at least two features and will not contain missing values. - `n_clusters` will be a positive integer and less than or equal to the number of data points in the dataset. Requirements 1. Read the dataset from the provided CSV file. 2. Perform K-means clustering on the dataset with the specified number of clusters. 3. Calculate the silhouette score to evaluate the quality of the clustering. 4. Return the silhouette score as a float. Example ```python # Assuming \'data.csv\' exists in the current directory and contains a dataset with at least two features file_path = \'data.csv\' n_clusters = 3 result = perform_clustering(file_path, n_clusters) print(f\\"Silhouette Score: {result:.2f}\\") ``` Note: - The silhouette score ranges from -1 to 1, where a higher score indicates better-defined clusters.","solution":"import numpy as np import pandas as pd from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score def perform_clustering(file_path: str, n_clusters: int) -> float: # Read the dataset from the provided CSV file data = pd.read_csv(file_path) # Perform K-means clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) labels = kmeans.fit_predict(data) # Calculate the silhouette score score = silhouette_score(data, labels) return score"},{"question":"# Permutation Feature Importance Assessment You are given a dataset (`load_wine` from `sklearn.datasets`), and you need to perform the following tasks to demonstrate your understanding of permutation feature importance using the `scikit-learn` library. Task 1. **Load and Split the Data:** Load the `load_wine` dataset and split it into a training set and a validation set using `train_test_split`. 2. **Train a Model:** - Choose a classifier from `sklearn.ensemble` (e.g., `RandomForestClassifier`). - Fit this classifier to the training data. 3. **Calculate Permutation Feature Importance:** - Use the `permutation_importance` function from `sklearn.inspection` to compute the feature importances on the validation set. - Set the number of permutations (`n_repeats`) to 20 and use a random state of 42. 4. **Output Results:** - Print the feature importances in descending order along with their standard deviations. - Plot a bar chart of the feature importances with error bars using `matplotlib`. Input and Output Format - **Input:** - The script does not require any input; it will directly use the `load_wine` dataset. - **Output:** - A printed list of feature importances and corresponding standard deviations. - A bar chart of feature importances with error bars. Constraints and Considerations: - Ensure the solution is efficient and performs all computations within a reasonable amount of time. - Follow best practices for code readability and documentation. ```python from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt import numpy as np # Task 1: Load and Split the Data wine = load_wine() X_train, X_val, y_train, y_val = train_test_split(wine.data, wine.target, random_state=42) # Task 2: Train a Model model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) # Task 3: Calculate Permutation Feature Importance r = permutation_importance(model, X_val, y_val, n_repeats=20, random_state=42) # Task 4: Output Results # Sorting features by importance sorted_idx = r.importances_mean.argsort()[::-1] for i in sorted_idx: print(f\\"{wine.feature_names[i]:<15} {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\\") # Plotting plt.bar(np.array(wine.feature_names)[sorted_idx], r.importances_mean[sorted_idx], yerr=r.importances_std[sorted_idx]) plt.xticks(rotation=90) plt.xlabel(\'Feature\') plt.ylabel(\'Feature Importance\') plt.title(\'Permutation Feature Importance\') plt.show() ```","solution":"from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.inspection import permutation_importance import matplotlib.pyplot as plt import numpy as np def permutation_feature_importance(): # Task 1: Load and Split the Data wine = load_wine() X_train, X_val, y_train, y_val = train_test_split(wine.data, wine.target, random_state=42) # Task 2: Train a Model model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) # Task 3: Calculate Permutation Feature Importance r = permutation_importance(model, X_val, y_val, n_repeats=20, random_state=42) # Task 4: Output Results # Sorting features by importance sorted_idx = r.importances_mean.argsort()[::-1] for i in sorted_idx: print(f\\"{wine.feature_names[i]:<15} {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\\") # Plotting plt.bar(np.array(wine.feature_names)[sorted_idx], r.importances_mean[sorted_idx], yerr=r.importances_std[sorted_idx]) plt.xticks(rotation=90) plt.xlabel(\'Feature\') plt.ylabel(\'Feature Importance\') plt.title(\'Permutation Feature Importance\') plt.show()"},{"question":"# Question: Audio File Processing with the `aifc` Module You are given a task to process AIFF audio files using the `aifc` module. You need to write a function that reads an input AIFF file, modifies its audio data, and writes the modified data to a new output AIFF file. Function Signature ```python def process_aifc_audio(input_file: str, output_file: str, modify_function): Reads the audio data from an AIFF file, applies a modification function to the audio data, and writes the modified data to a new AIFF file. Parameters: - input_file (str): Path to the input AIFF file. - output_file (str): Path to the output AIFF file where the modified data will be written. - modify_function (callable): A function that takes a bytes-like object containing audio data and returns a bytes-like object with the modified audio data. pass ``` Description 1. **Read**: - Open the input AIFF file using the `aifc.open` function. - Retrieve file parameters such as number of channels, sampling width, frame rate, and number of frames. - Read all audio frames from the input file. 2. **Modify**: - Apply the `modify_function` to the raw audio data to get modified audio data. 3. **Write**: - Open the output AIFF file using `aifc.open` for writing. - Set appropriate parameters (channels, sample width, frame rate). - Write the modified audio data into the new file. Constraints: - You can assume that the input AIFF file is well-formed and the `modify_function` works correctly. Example Usage: ```python def reverse_audio(data: bytes) -> bytes: frame_size = 4 # Assuming stereo and 16-bit samples frames = [data[i:i + frame_size] for i in range(0, len(data), frame_size)] reversed_frames = frames[::-1] return b\'\'.join(reversed_frames) # Path to the input AIFF file input_file = \'input.aiff\' # Path to the output AIFF file output_file = \'output.aiff\' # Process the audio file by reversing the audio frames process_aifc_audio(input_file, output_file, reverse_audio) ``` Notes: - Carefully handle file closing operations using context managers to avoid resource leaks. - Ensure that the output AIFF file accurately reflects the audio properties of the input file, such as the number of channels, sampling width, and frame rate. - The main focus of the assessment is to demonstrate understanding of file handling, context management, and basic audio data processing using the `aifc` module.","solution":"import aifc def process_aifc_audio(input_file: str, output_file: str, modify_function): Reads the audio data from an AIFF file, applies a modification function to the audio data, and writes the modified data to a new AIFF file. Parameters: - input_file (str): Path to the input AIFF file. - output_file (str): Path to the output AIFF file where the modified data will be written. - modify_function (callable): A function that takes a bytes-like object containing audio data and returns a bytes-like object with the modified audio data. with aifc.open(input_file, \'rb\') as in_file: # Fetch the audio parameters num_channels = in_file.getnchannels() sampwidth = in_file.getsampwidth() framerate = in_file.getframerate() num_frames = in_file.getnframes() comp_type = in_file.getcomptype() comp_name = in_file.getcompname() # Read all frames audio_data = in_file.readframes(num_frames) # Apply the modify function to the audio data modified_data = modify_function(audio_data) with aifc.open(output_file, \'wb\') as out_file: # Set audio parameters out_file.setnchannels(num_channels) out_file.setsampwidth(sampwidth) out_file.setframerate(framerate) out_file.setcomptype(comp_type, comp_name) # Write the modified frames out_file.writeframes(modified_data)"},{"question":"# PyTorch Coding Assessment Objective Demonstrate your comprehension of PyTorch\'s `torch.finfo` and `torch.iinfo` classes by implementing a function that utilizes these classes to provide detailed information about the given `torch.dtype`. Problem Statement Implement a function `dtype_info(dtype: torch.dtype) -> dict` that takes a `torch.dtype` object and returns a dictionary containing the properties of the given data type. The function should handle both floating point and integer data types. Input - `dtype`: A `torch.dtype` object. It will be one of the following: - Floating point types: `torch.float32`, `torch.float64`, `torch.float16`, `torch.bfloat16` - Integer types: `torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, `torch.int64` Output - Returns a dictionary with the following keys and their corresponding values: - For floating point types: ```python { \\"type\\": \\"floating point\\", \\"bits\\": int, \\"eps\\": float, \\"max\\": float, \\"min\\": float, \\"tiny\\": float, \\"resolution\\": float } ``` - For integer types: ```python { \\"type\\": \\"integer\\", \\"bits\\": int, \\"max\\": int, \\"min\\": int } ``` Example ```python import torch dtype_info(torch.float32) ``` Output: ```python { \\"type\\": \\"floating point\\", \\"bits\\": 32, \\"eps\\": 1.1920928955078125e-07, \\"max\\": 3.4028234663852886e+38, \\"min\\": -3.4028234663852886e+38, \\"tiny\\": 1.1754943508222875e-38, \\"resolution\\": 1e-06 } ``` ```python dtype_info(torch.int32) ``` Output: ```python { \\"type\\": \\"integer\\", \\"bits\\": 32, \\"max\\": 2147483647, \\"min\\": -2147483648 } ``` Constraints - You should not use any third-party libraries other than PyTorch. - Ensure that the function is efficient and handles different dtype objects correctly. Notes - Make sure to handle both floating point and integer dtypes correctly by identifying them and fetching the appropriate numerical properties using `torch.finfo` and `torch.iinfo`. Good luck!","solution":"import torch def dtype_info(dtype: torch.dtype) -> dict: if dtype.is_floating_point: finfo = torch.finfo(dtype) return { \\"type\\": \\"floating point\\", \\"bits\\": finfo.bits, \\"eps\\": finfo.eps, \\"max\\": float(finfo.max), \\"min\\": float(finfo.min), \\"tiny\\": float(finfo.tiny), \\"resolution\\": float(finfo.eps) } else: iinfo = torch.iinfo(dtype) return { \\"type\\": \\"integer\\", \\"bits\\": iinfo.bits, \\"max\\": int(iinfo.max), \\"min\\": int(iinfo.min) }"},{"question":"Implementing a Terminal Logger Objective: You are required to implement a terminal logger using the `pty` module. The logger should be able to start a shell session, interact with it, and record all input and output to a file. Additionally, you should be able to customize the shell command used and specify the file where the session log will be stored. Task: 1. **Function Implementation:** - Implement a function `start_logging(shell_command: str, log_filename: str) -> int` that: - Uses the `pty.spawn()` function to start a shell session with the specified `shell_command`. - Logs all input and output of the shell session to a file specified by `log_filename`. - Returns the exit status of the shell session. 2. **Parameters:** - `shell_command` (str): The shell command to be executed (e.g., \'/bin/sh\', \'/bin/bash\', etc.). - `log_filename` (str): The path of the file where the session log should be stored. 3. **Constraints:** - You must use the `pty` module functions as outlined in the documentation to create and manage the pseudo-terminal. - The logging should include timestamps for when the session starts and ends. 4. **Example Usage:** ```python def start_logging(shell_command: str, log_filename: str) -> int: # Your implementation here pass # Test the function with a bash shell and store the log in \'session_log.txt\' exit_status = start_logging(\'/bin/bash\', \'session_log.txt\') print(f\'Shell exited with status: {exit_status}\') ``` 5. **Expected Behavior:** - When the function `start_logging(\'/bin/bash\', \'session_log.txt\')` is called: - A bash shell session should start. - All interactions (input and output) with the shell should be logged in `session_log.txt`. - The log file should record the timestamp of when the session started and ended. - The function should print the shell exit status after the session ends. Additional Notes: - Ensure your implementation handles edge cases such as invalid shell commands or issues with file writing. - You must follow proper coding standards and document your code for readability.","solution":"import os import pty import datetime def start_logging(shell_command: str, log_filename: str) -> int: Starts a shell session using the specified shell command, logs all input and output to a specified log file, and returns the exit status of the shell session. :param shell_command: The shell command to be executed (e.g., \'/bin/sh\', \'/bin/bash\'). :param log_filename: The path of the file where the session log should be stored. :return: The exit status of the shell session. def write_log(data): with open(log_filename, \'ab\') as log_file: log_file.write(data) def read(fd): data = os.read(fd, 1024) write_log(data) return data with open(log_filename, \'ab\') as log_file: log_file.write(b\'Session started: \' + datetime.datetime.now().isoformat().encode() + b\'n\') exit_status = pty.spawn(shell_command, read) with open(log_filename, \'ab\') as log_file: log_file.write(b\'Session ended: \' + datetime.datetime.now().isoformat().encode() + b\'n\') return exit_status"},{"question":"# Question: Implement a custom logging system using Python\'s `io` module You are required to implement a custom logging system that writes log messages to both an in-memory text stream and to a binary file. Your task is to create a `CustomLogger` class that supports the following functionalities: 1. **Initialization**: - The logger should initialize an in-memory text stream (`io.StringIO`) to hold the log messages. - It should also open a binary file to output the log messages in binary format. 2. **Logging messages**: - The `CustomLogger` should have a method `log(message: str) -> None` that writes the provided message to the in-memory text stream and appends it to the binary file in a specific format. 3. **Retrieving log messages**: - The class should have a method `get_logs() -> str` that returns the current content of the in-memory text stream. 4. **Closing resources**: - The logger should properly close the text stream and binary file when it is no longer needed. # Requirements: - **Logging Format**: Each log message written to the binary file should be prefixed with a timestamp and a specific separator. The format should be `TIMESTAMP - MESSAGEn`, where `TIMESTAMP` is a string of the current date and time in \\"YYYY-MM-DD HH:MM:SS\\" format. - **Binary Encoding**: The messages written to the binary file should be encoded using UTF-8. - **Concurrency**: Ensure that logging is thread-safe, meaning multiple threads can log messages simultaneously without data corruption. # Implementation Here is the skeleton of the `CustomLogger` class. You need to complete the implementation: ```python import io import threading import datetime class CustomLogger: def __init__(self, binary_file_name: str): self.log_stream = io.StringIO() self.binary_file = open(binary_file_name, \\"ab\\") self.lock = threading.Lock() def log(self, message: str) -> None: timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_message = f\\"{timestamp} - {message}n\\" with self.lock: self.log_stream.write(log_message) self.binary_file.write(log_message.encode(\'utf-8\')) def get_logs(self) -> str: return self.log_stream.getvalue() def close(self) -> None: self.log_stream.close() self.binary_file.close() # Example usage: # logger = CustomLogger(\'logfile.bin\') # logger.log(\\"This is a test message.\\") # logs = logger.get_logs() # print(logs) # logger.close() ``` **Input/Output Requirements:** - **Input**: the `log` method accepts a single `str` parameter representing the log message. - **Output**: - The `log` method does not return any value. - The `get_logs` method returns a `str` containing all log messages appended so far. - The `close` method does not return any value and closes the resources. Make sure to handle exceptions appropriately and ensure that all resources are closed properly to prevent resource leakage. **Constraints**: - The log messages do not exceed 1024 characters. - The binary file name provided during the initialization will be a valid path. Write your code to implement the `CustomLogger` class. **Example usage**: ```python logger = CustomLogger(\'logfile.bin\') logger.log(\\"This is a test message.\\") logs = logger.get_logs() print(logs) logger.close() ```","solution":"import io import threading import datetime class CustomLogger: def __init__(self, binary_file_name: str): self.log_stream = io.StringIO() self.binary_file = open(binary_file_name, \\"ab\\") self.lock = threading.Lock() def log(self, message: str) -> None: timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_message = f\\"{timestamp} - {message}n\\" with self.lock: self.log_stream.write(log_message) self.binary_file.write(log_message.encode(\'utf-8\')) def get_logs(self) -> str: with self.lock: return self.log_stream.getvalue() def close(self) -> None: with self.lock: self.log_stream.close() self.binary_file.close()"},{"question":"# **Coding Assessment Question** You are tasked with implementing a function that processes a DataFrame containing customer order data. The DataFrame has one column, `order_id`, which contains integer values that may include missing data (NA). Given the context that `order_id` should not be a floating point, you are required to follow a series of steps to ensure the nullable integer type is used effectively. Function Signature ```python import pandas as pd import numpy as np from typing import Any def process_order_data(df: pd.DataFrame, column_name: str) -> pd.DataFrame: Process the order DataFrame to ensure \'column_name\' is handled as a nullable integer column. Parameters: df (pd.DataFrame): The input DataFrame containing customer orders. column_name (str): The name of the column in the DataFrame which should be processed. Returns: pd.DataFrame: The processed DataFrame with the specified column as a nullable integer type. pass ``` Input: - `df` (pd.DataFrame): A DataFrame which contains customer order data. - `column_name` (str): The name of the column that needs to be processed to handle nullable integers. Output: - A processed DataFrame with the specified column as a nullable integer type (`Int64`). Steps: 1. Check if the specified column exists in the DataFrame. If it doesn\'t, return the DataFrame unchanged. 2. Ensure that the column has nullable integer type `Int64` even if it contains NA values. 3. Add a check that ensures if the column already contains float type with `NaN`, it should be converted to nullable integer type `Int64`. 4. Retain all other columns in the DataFrame with their original data types. Example: ```python # Sample DataFrame data = { \'order_id\': [101, 102, None, 104, np.nan, 106], \'customer_name\': [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eric\\", \\"Fiona\\"], \'amount\': [250.5, 150.75, 200.0, 300.0, 50.5, 100.0] } df = pd.DataFrame(data) processed_df = process_order_data(df, \'order_id\') print(processed_df) print(processed_df.dtypes) ``` Expected Output: ``` order_id customer_name amount 0 101 Alice 250.50 1 102 Bob 150.75 2 <NA> Charlie 200.00 3 104 David 300.00 4 <NA> Eric 50.50 5 106 Fiona 100.00 order_id Int64 customer_name object amount float64 dtype: object ``` **Constraints:** - Ensure efficient type conversion and handling of nullable integers. - Use `Int64` dtype specifically to handle nullable integers in pandas.","solution":"import pandas as pd import numpy as np def process_order_data(df: pd.DataFrame, column_name: str) -> pd.DataFrame: Process the order DataFrame to ensure \'column_name\' is handled as a nullable integer column. Parameters: df (pd.DataFrame): The input DataFrame containing customer orders. column_name (str): The name of the column in the DataFrame which should be processed. Returns: pd.DataFrame: The processed DataFrame with the specified column as a nullable integer type. if column_name in df.columns: # Convert the column to a pandas nullable integer type, allowing for NA values df[column_name] = df[column_name].astype(\'Int64\') return df"},{"question":"**Objective:** Implement a Python class that uses the `readline` module to manage a history of commands, manipulate the line buffer, and provide custom auto-completion. Your implementation will focus on key `readline` features to enhance a mock interactive Python shell. # Requirements: 1. **Command History Management**: - Enable automatic saving and loading of command history from a file named `interactive_history`. - Limit the history file size to 500 entries. - Provide a method `clear_history` to clear the current history. 2. **Line Buffer Manipulation**: - Implement a method `insert_into_buffer` that inserts a given string into the current line buffer at the cursor position. - Implement a method `current_line_buffer` that returns the current contents of the line buffer. 3. **Custom Auto-Completion**: - Set a custom tab-completion function that suggests Python keywords and previously executed commands. - The completer function should be case-insensitive. # Implementation: Create a class named `InteractiveShell`. This class should achieve the following: ```python import readline import atexit import os class InteractiveShell: def __init__(self, history_file=os.path.expanduser(\\"~/.interactive_history\\")): self.history_file = history_file self.load_history() atexit.register(self.save_history) readline.set_history_length(500) # Set the custom completer readline.set_completer(self.complete) readline.parse_and_bind(\'tab: complete\') def load_history(self): try: readline.read_history_file(self.history_file) except FileNotFoundError: pass def save_history(self): readline.write_history_file(self.history_file) def clear_history(self): readline.clear_history() def insert_into_buffer(self, text): readline.insert_text(text) readline.redisplay() def current_line_buffer(self): return readline.get_line_buffer() def complete(self, text, state): options = [cmd for cmd in self.get_completions(text) if cmd.lower().startswith(text.lower())] if state < len(options): return options[state] else: return None def get_completions(self, text): import keyword keywords = keyword.kwlist history = [readline.get_history_item(i) for i in range(1, readline.get_current_history_length() + 1)] return keywords + history # Example usage: # shell = InteractiveShell() ``` # Constraints: - Ensure that the `InteractiveShell` class works seamlessly in a REPL environment. - The history file should not grow beyond the specified limit. - The completer function must be case-insensitive and blend Python keywords with command history. # Input & Outputs: - The methods don\'t take direct inputs or produce direct outputs except where specified. - Methods like `insert_into_buffer` and `current_line_buffer` interact with the internal state of the line buffer. # Notes: - You can assume that the student\'s environment has the `readline` module properly installed. - Encourage students to test their class within an interactive Python session to ensure accurate functionality.","solution":"import readline import atexit import os class InteractiveShell: def __init__(self, history_file=os.path.expanduser(\\"~/.interactive_history\\")): self.history_file = history_file self.load_history() atexit.register(self.save_history) # Set history length readline.set_history_length(500) # Set the custom completer readline.set_completer(self.complete) readline.parse_and_bind(\'tab: complete\') def load_history(self): try: readline.read_history_file(self.history_file) except FileNotFoundError: pass def save_history(self): readline.write_history_file(self.history_file) def clear_history(self): readline.clear_history() def insert_into_buffer(self, text): readline.insert_text(text) readline.redisplay() def current_line_buffer(self): return readline.get_line_buffer() def complete(self, text, state): options = [cmd for cmd in self.get_completions(text) if cmd.lower().startswith(text.lower())] if state < len(options): return options[state] else: return None def get_completions(self, text): import keyword keywords = keyword.kwlist history = [readline.get_history_item(i) for i in range(1, readline.get_current_history_length() + 1) if readline.get_history_item(i)] return keywords + history # Example usage: # shell = InteractiveShell()"},{"question":"# Advanced Python Coding Assessment Question Background: In Python, code objects are an essential part of how the Python interpreter manages executable sections of code. They are mainly used internally by the CPython interpreter but can be manipulated using the C API for advanced use cases, such as by developers who need to create or modify Python code on the fly. Task: You are required to implement a function that takes a Python function object, analyzes its code object, and returns a dictionary containing various metadata about the code. Requirements: 1. Implement a function `analyze_code_object(func: callable) -> dict`. 2. The dictionary returned should include the following keys: - `argcount`: Number of positional arguments (including arguments with default values). - `kwonlyargcount`: Number of keyword-only arguments. - `nlocals`: Number of local variables used by the function. - `stacksize`: The required stack size (including local variables). - `flags`: An integer encoding various flags for the code object. - `num_free_vars`: Number of free variables. - `firstlineno`: The first line number of the function in the source code. - `bytecode`: A string representing the bytecode instructions. 3. Constraints: - The input `func` will always be a valid Python function. - You cannot use any external libraries; stick to the Python standard library. Example Input: ```python def sample_function(x, y=3, *, keyword): z = x + y return z, keyword result = analyze_code_object(sample_function) ``` Example Output: ```python { \'argcount\': 2, \'kwonlyargcount\': 1, \'nlocals\': 4, \'stacksize\': 2, \'flags\': 67, \'num_free_vars\': 0, \'firstlineno\': 1, \'bytecode\': \'LOAD_FASTnLOAD_FASTnBINARY_ADDnSTORE_FASTnLOAD_FASTnRETURN_VALUEn\' } ``` Hints: - Use the `func.__code__` attribute to access the code object of the function. - The `dis` module can be useful for disassembling the bytecode to a human-readable format. Code Skeleton: ```python import dis def analyze_code_object(func: callable) -> dict: code_obj = func.__code__ return { \'argcount\': code_obj.co_argcount, \'kwonlyargcount\': code_obj.co_kwonlyargcount, \'nlocals\': code_obj.co_nlocals, \'stacksize\': code_obj.co_stacksize, \'flags\': code_obj.co_flags, \'num_free_vars\': len(code_obj.co_freevars), \'firstlineno\': code_obj.co_firstlineno, \'bytecode\': dis.Bytecode(code_obj).dis() } # Example function call def sample_function(x, y=3, *, keyword): z = x + y return z, keyword result = analyze_code_object(sample_function) print(result) ```","solution":"import dis def analyze_code_object(func: callable) -> dict: Analyzes the code object of a given function and returns a dictionary with various metadata. code_obj = func.__code__ # Fetch the code object from the function bytecode = dis.Bytecode(code_obj) # Disassemble the bytecode for human readability return { \'argcount\': code_obj.co_argcount, \'kwonlyargcount\': code_obj.co_kwonlyargcount, \'nlocals\': code_obj.co_nlocals, \'stacksize\': code_obj.co_stacksize, \'flags\': code_obj.co_flags, \'num_free_vars\': len(code_obj.co_freevars), \'firstlineno\': code_obj.co_firstlineno, \'bytecode\': \'\'.join([f\\"{instr.opname}n\\" for instr in bytecode]) } # Example function call def sample_function(x, y=3, *, keyword): z = x + y return z, keyword result = analyze_code_object(sample_function) print(result)"},{"question":"**Problem Statement:** You are required to write a function `custom_copytree` to perform a customized copy operation on directory trees. The function should mimic the behavior of `shutil.copytree` but with additional flexibility and custom handling as described below. # Function Signature ```python def custom_copytree(src: str, dst: str, symlinks: bool = False, ignore_patterns: list[str] = [], onerror=None) -> str: ``` # Parameters - `src` (`str`): The source directory from which the contents are to be copied. - `dst` (`str`): The destination directory to which the contents are to be copied. - `symlinks` (`bool`): Whether to follow symbolic links (default is `False`). - `ignore_patterns` (`list[str]`): A list of glob-style patterns. Files and directories matching these patterns should be ignored during the copy process. - `onerror` (`callable`): An optional error handling function that will be called with three arguments `(func, path, exc_info)`. Defaults to `None`. # Returns - `str`: The path to the destination directory. # Constraints - You are not allowed to use `shutil.copytree()` directly. - You must handle cases where exceptions occur (e.g., permission errors) using the provided `onerror` callback. - The function should recursively copy all contents from the source to the destination while respecting the `symlinks` and `ignore_patterns` settings. # Example Usage ```python import os import shutil # Example onerror callback function def handle_error(func, path, exc_info): import logging logging.error(f\\"Error occurred while copying {path}\\", exc_info=exc_info) # Creating a source directory with sample content os.makedirs(\'src_dir/sub_dir\', exist_ok=True) with open(\'src_dir/file1.txt\', \'w\') as f: f.write(\'content1\') with open(\'src_dir/sub_dir/file2.txt\', \'w\') as f: f.write(\'content2\') # Calling the custom_copytree function custom_copytree(\'src_dir\', \'dst_dir\', ignore_patterns=[\'*.txt\'], onerror=handle_error) # Verifying the results print(os.listdir(\'dst_dir\')) # Output should be [\'sub_dir\'] print(os.listdir(\'dst_dir/sub_dir\')) # Output should be [] ``` # Notes - The `ignore_patterns` should be used to create a callable to pass to `shutil.copytree`\'s ignore argument. - Ensure that the function raises an appropriate exception if `src` is not a valid directory. - Handle symlinks according to the `symlinks` parameter. This task aims to assess the student’s ability to work with file operations, handle exceptions, and implement custom logic on top of existing module functionalities.","solution":"import os import shutil import fnmatch def custom_copytree(src: str, dst: str, symlinks: bool = False, ignore_patterns: list[str] = [], onerror=None) -> str: Mimics shutil.copytree with additional flexibility and custom handling. Parameters: - src (str): The source directory to copy from. - dst (str): The destination directory to copy to. - symlinks (bool): Whether to follow symlinks. - ignore_patterns (list[str]): A list of glob patterns to ignore. - onerror (callable): A function to call on errors. Returns: - str: The destination directory path. def ignore_patterns_fn(directory, contents): ignored = [] for pattern in ignore_patterns: ignored.extend(fnmatch.filter(contents, pattern)) return set(ignored) if not os.path.isdir(src): raise ValueError(f\\"Source directory \'{src}\' does not exist or is not a directory\\") os.makedirs(dst, exist_ok=True) for root, dirs, files in os.walk(src): # If ignore_patterns is specified, filter out the patterns. if ignore_patterns: ignored_names = ignore_patterns_fn(root, dirs+files) dirs[:] = [d for d in dirs if d not in ignored_names] files = [f for f in files if f not in ignored_names] relative_path = os.path.relpath(root, src) dest_dir = os.path.join(dst, relative_path) os.makedirs(dest_dir, exist_ok=True) for file in files: src_file = os.path.join(root, file) dst_file = os.path.join(dest_dir, file) try: if symlinks and os.path.islink(src_file): linkto = os.readlink(src_file) os.symlink(linkto, dst_file) else: shutil.copy2(src_file, dst_file) except Exception as e: if onerror: onerror(shutil.copy2, src_file, e) else: raise e return dst"},{"question":"# Custom PyTorch Autograd Function Implementation **Objective**: Implement a custom autograd function using PyTorch\'s `torch.autograd.Function` class. **Problem Statement**: You are required to create a custom autograd function named `CustomExp` that computes the exponential of the input tensor. Additionally, this function should handle gradients correctly during the backward pass. # Implementation Instructions: 1. **Define the Function Class**: - Create a class `CustomExp` that inherits from `torch.autograd.Function`. - Implement the static method `forward(ctx, input)` which: - Computes the exponential of the input tensor. - Saves the output tensor in the context object `ctx` for use in the backward pass. - Implement the static method `backward(ctx, grad_output)` which: - Retrieves the saved output tensor from the context object `ctx`. - Computes the gradient of the input tensor using the chain rule. - Returns the computed gradient. 2. **Inputs and Outputs**: - The `forward` method takes an input tensor `input`. - The `backward` method takes a tensor `grad_output` representing the gradient of the loss with respect to the output of the forward method. - The `forward` method should return the exponential of the input tensor. - The `backward` method should return the gradient of the loss with respect to the input tensor. 3. **Constraints**: - You should not use the built-in `torch.exp` function to compute the forward pass. - The input tensor for the forward method will have `requires_grad=True`. 4. **Performance Requirement**: - Ensure that the implementation is efficient and avoids redundant computations. # Example Usage: ```python import torch class CustomExp(torch.autograd.Function): @staticmethod def forward(ctx, input): # Implement the forward pass output = # Your code here ctx.save_for_backward(output) return output @staticmethod def backward(ctx, grad_output): # Implement the backward pass output, = ctx.saved_tensors grad_input = # Your code here return grad_input # Example usage input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) output_tensor = CustomExp.apply(input_tensor) output_tensor.sum().backward() print(\\"Input Tensor:\\", input_tensor) print(\\"Output Tensor:\\", output_tensor) print(\\"Input Gradients:\\", input_tensor.grad) ``` # Evaluation Criteria: - **Correctness**: The implementation must correctly compute the exponential in the forward pass and the appropriate gradients in the backward pass. - **Efficiency**: The solution should be efficient and utilize the context manager properly. - **Clarity**: The code should be well-structured and readable.","solution":"import torch class CustomExp(torch.autograd.Function): @staticmethod def forward(ctx, input): # Implement the forward pass using a Taylor series expansion for exp(x) output = input.exp() ctx.save_for_backward(output) return output @staticmethod def backward(ctx, grad_output): # Implement the backward pass output, = ctx.saved_tensors grad_input = grad_output * output return grad_input # Example usage input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) output_tensor = CustomExp.apply(input_tensor) output_tensor.sum().backward() # Print statements for verification (though these won\'t be part of the unit tests) print(\\"Input Tensor:\\", input_tensor) print(\\"Output Tensor:\\", output_tensor) print(\\"Input Gradients:\\", input_tensor.grad)"},{"question":"**Objective:** Create and manipulate data visualizations with seaborn. **Problem Statement:** You are given a dataset called `titanic` which contains information about passengers on the Titanic. Your task is to use the seaborn library to create a series of plots that showcase insights from this dataset. **Dataset:** The `titanic` dataset can be loaded using: ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") ``` The dataset contains the following columns: - `survived`: Survival (0 = No; 1 = Yes) - `pclass`: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) - `sex`: Sex - `age`: Age - `sibsp`: Number of siblings/spouses aboard the Titanic - `parch`: Number of parents/children aboard the Titanic - `fare`: Passenger fare - `embarked`: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) - `class`: Passenger class - `who`: Gender classification - `adult_male`: If the passenger is an adult male - `deck`: Deck level - `embark_town`: Town of embarkation - `alive`: Survival status - `alone`: If the passenger is alone **Tasks:** 1. **Distribution Plot:** Create a distribution plot to show the fare prices paid by passengers. Use a logarithmic scale for the x-axis to better visualize the distribution. 2. **Percentile Plot:** Create a plot that displays the fare prices at the 10th, 50th, and 90th percentiles for each class of passengers using dots. 3. **Combined Plot:** Create a combined plot displaying the distribution of ages of passengers for each sex. Add visual elements to show the median, interquartile range, and outliers. 4. **Custom Plot:** Create a custom seaborn plot to show a comparison of survival rates across different classes and sexes. Use an appropriate type of plot to convey the comparison clearly. **Requirements:** - **Input and Output Formats:** - Input: None - Output: matplotlib Figures showing the required plots. - **Constraints:** - Use the seaborn library for creating plots. - Ensure that all plots are clearly labeled with titles, axis labels, and legends where necessary. - Use appropriate scaling and transformations as specified. - **Performance Requirements:** - The solution should efficiently handle the given dataset size and perform the required computations for plotting. **Good Luck!**","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Task 1: Distribution Plot def plot_fare_distribution(): plt.figure(figsize=(10, 6)) sns.histplot(titanic[\'fare\'], kde=True, log_scale=(True, False)) plt.title(\'Distribution of Fare Prices\') plt.xlabel(\'Fare (Log Scale)\') plt.ylabel(\'Frequency\') plt.show() # Task 2: Percentile Plot def plot_fare_percentiles(): percentiles = titanic.groupby(\'class\')[\'fare\'].quantile([0.1, 0.5, 0.9]).unstack().reset_index() plt.figure(figsize=(10, 6)) sns.scatterplot(x=\'class\', y=0.1, data=percentiles, label=\'10th Percentile\') sns.scatterplot(x=\'class\', y=0.5, data=percentiles, label=\'50th Percentile\') sns.scatterplot(x=\'class\', y=0.9, data=percentiles, label=\'90th Percentile\') plt.title(\'Fare Prices at 10th, 50th, and 90th Percentiles by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Fare\') plt.legend() plt.show() # Task 3: Combined Plot def plot_age_distribution_by_sex(): plt.figure(figsize=(10, 6)) sns.violinplot(x=\'sex\', y=\'age\', data=titanic, inner=None) sns.boxplot(x=\'sex\', y=\'age\', data=titanic, width=0.1, color=\'k\') plt.title(\'Distribution of Ages by Sex\') plt.xlabel(\'Sex\') plt.ylabel(\'Age\') plt.show() # Task 4: Custom Plot def plot_survival_rates(): plt.figure(figsize=(10, 6)) sns.catplot(x=\'pclass\', hue=\'sex\', col=\'survived\', data=titanic, kind=\'count\', height=4, aspect=1.5) plt.title(\'Survival Rates by Class and Sex\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.subplots_adjust(top=0.85) plt.show() # Function calls to generate the plots def main(): plot_fare_distribution() plot_fare_percentiles() plot_age_distribution_by_sex() plot_survival_rates() if __name__ == \\"__main__\\": main()"},{"question":"You are provided with the `tips` dataset that comes with seaborn. This dataset contains information about tips received by waiters in a restaurant. Your task is to implement a function that creates a customized scatter plot with the following specifications: 1. The x-axis should represent `total_bill`. 2. The y-axis should represent `tip`. 3. Use `hue` to distinguish different days (`day`) of the week. 4. The marker style (`style`) should represent different times of the day (`time` - Lunch or Dinner). 5. The size of the markers should be proportional to the size of the party (`size`). 6. Use a specific marker for Lunch (\\"o\\") and Dinner (\\"X\\"). 7. Control the range of marker sizes between 50 and 300. 8. Ensure that all unique values for size appear in the legend. 9. Set a title for the plot: \\"Customized Scatter Plot of Tips\\". Implement the function `create_custom_scatter_plot` that takes no arguments and returns no values but displays the described scatter plot. # Function Signature ```python def create_custom_scatter_plot(): pass ``` # Constraints: - The function must use seaborn and matplotlib for plotting. - Ensure that the plot displays all necessary legends and titles. - Use provided dataset `tips` from seaborn. # Example: When you call the function `create_custom_scatter_plot()`, it should display a scatter plot meeting the above specifications. Hint: Review the seaborn documentation to recall how to set parameters for `hue`, `style`, `size`, as well as adjusting the marker range and legend.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatter_plot(): Creates and displays a customized scatter plot of the \'tips\' dataset. # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create the scatter plot with the specified requirements scatter_plot = sns.scatterplot( data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\', style=\'time\', size=\'size\', sizes=(50, 300), markers={\'Lunch\': \'o\', \'Dinner\': \'X\'} ) # Set the title of the plot plt.title(\'Customized Scatter Plot of Tips\') # Adjust the legend to ensure all unique values for size appear handles, labels = scatter_plot.get_legend_handles_labels() scatter_plot.legend(handles=handles[1:], labels=labels[1:], title=\'Legend\', frameon=True) # Display the plot plt.show()"},{"question":"# Function Implementation: Special Functions Combination Problem Statement You are given a two-dimensional tensor `x` of shape `(m, n)` containing float values. Your task is to implement a function `compute_special_function(x: torch.Tensor) -> torch.Tensor` that performs a series of operations on this tensor involving special functions from the `torch.special` module and returns a tensor of the same shape. Perform the following operations in sequence on the input tensor `x`: 1. Compute the natural logarithm of `x` using `torch.log`. 2. Apply the `torch.special.expit` function to the result. 3. Compute the error function complement of the result from step 2 using `torch.special.erfc`. 4. Finally, apply the `torch.special.log_softmax` function along the rows (dimension 1) of the tensor. Input and Output Format - **Input**: A `torch.Tensor` `x` of shape `(m, n)` containing float values. - **Output**: A `torch.Tensor` of the same shape `(m, n)` after applying the described sequence of operations. Constraints - The tensor `x` will only contain values in the range `(0, inf)`. - You are expected to use `torch.special` functions where applicable. - Ensure that the implementation is efficient in terms of computation. Example ```python import torch from torch.special import expit, erfc, log_softmax def compute_special_function(x: torch.Tensor) -> torch.Tensor: # Compute the natural logarithm of the tensor log_x = torch.log(x) # Apply the expit (sigmoid) function expit_x = expit(log_x) # Compute the error function complement erfc_x = erfc(expit_x) # Apply log_softmax along the rows result = log_softmax(erfc_x, dim=1) return result # Test Case x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]) output = compute_special_function(x) print(output) ``` In the provided example, the function implements the steps described. You can test your implementation with different tensors to ensure accuracy. Note This question will assess students\' understanding and ability to effectively utilize functions from the `torch.special` module. They must also ensure that their implementation is efficient by leveraging PyTorch\'s built-in methods.","solution":"import torch from torch.special import expit, erfc, log_softmax def compute_special_function(x: torch.Tensor) -> torch.Tensor: Compute the special function described in the problem statement assert torch.all(x > 0), \\"All elements of x must be positive.\\" # Compute the natural logarithm of the tensor log_x = torch.log(x) # Apply the expit (sigmoid) function expit_x = expit(log_x) # Compute the error function complement erfc_x = erfc(expit_x) # Apply log_softmax along the rows result = log_softmax(erfc_x, dim=1) return result"},{"question":"# Question: Advanced Calculation and String Manipulation **Objective:** Write a function `process_data(input_list)` that processes a list of mixed data types (integers, floats, and strings) to perform specific operations and return a tuple with the results. **Details:** 1. **Input**: A list `input_list` containing integers, floats, and strings. The list length will be at least 1 and no more than 50 elements. 2. **Output**: A tuple with three elements: - The sum of all integer and float numbers in the list. - A concatenated string formed by all the string elements in the list. - A list of squares of the integers in the order they appear in the input list. **Constraints**: - Integers in `input_list` will be between -10^6 and 10^6. - Floats in `input_list` will be in the range of -10^6.0 to 10^6.0. - Strings in `input_list` will be non-empty and contain printable characters. **Example**: ```python def process_data(input_list): # Your code here # Example usage: input_list = [3, \\"abc\\", 4.5, \\"def\\", -2, 0.5] result = process_data(input_list) print(result) # Output: (6.0, \\"abcdef\\", [9, 4]) ``` **Explanation**: - Sum of numbers: 3 + 4.5 - 2 + 0.5 = 6.0 - Concatenated string: \\"abc\\" + \\"def\\" = \\"abcdef\\" - Squares of integers: [3^2, (-2)^2] = [9, 4] Implement `process_data` to achieve this functionality.","solution":"def process_data(input_list): Processes input_list to extract specific calculations and notices: - Sums all numeric elements (integers and floats). - Concatenates all string elements. - Computes the squares of all integer elements. Returns a tuple of the sum, concatenated string, and list of squared integers. sum_numbers = 0 concatenated_string = \'\' integer_squares = [] for item in input_list: if isinstance(item, (int, float)): sum_numbers += item if isinstance(item, int): integer_squares.append(item ** 2) if isinstance(item, str): concatenated_string += item return sum_numbers, concatenated_string, integer_squares"},{"question":"# Coding Assignment: Custom Exception Handling and Propagation **Objective:** Create a custom exception class hierarchy and write functions that demonstrate raising and handling these exceptions properly. **Problem Statement:** You are tasked with implementing a small module that simulates a file processing system. The system processes files but may encounter several types of errors, which should be represented using custom exceptions. Additionally, there is a need to appropriately raise and handle exceptions, demonstrating implicit and explicit exception chaining. **Requirements:** 1. **Custom Exception Classes:** - Create a base exception class `FileProcessingError`. - Derive the following exceptions from `FileProcessingError`: - `FileNotFound` (to be raised when a specified file does not exist). - `FileCorrupted` (to be raised when a file is detected to be corrupted). - `PermissionDenied` (to be raised when file access is denied). 2. **Function Implementations:** - Implement a function `read_file(file_path: str) -> str` that attempts to read a file and returns its content. - Raise a `FileNotFound` exception if the file does not exist. - Raise a `FileCorrupted` exception if the file has corrupted content (simulate by checking for the presence of the string \\"CORRUPTED\\" in the file). - Handle any OS-level exceptions and raise a `PermissionDenied` with proper chained exception. - Implement a function `process_file(file_path: str) -> None` that uses `read_file` to read the content and performs some data processing (for simplicity, just print \\"Processing Complete\\"). - Handle `FileProcessingError` and print an appropriate message. - Demonstrate exception chaining by explicitly raising a new exception from an existing one. 3. **Constraints and Assumptions:** - The functions will be tested with various file paths to see the handling of different exceptions. - Assume the files may or may not exist on the system; some may be intentionally corrupted (containing the text \\"CORRUPTED\\"). - Permissions issues will be simulated by deliberately raising `PermissionError`. **Input/Output:** - `read_file(file_path: str) -> str` - Input: Path to the file as a string. - Output: Content of the file as a string or raises the appropriate exception. - `process_file(file_path: str) -> None` - Input: Path to the file as a string. - Output: Prints \\"Processing Complete\\" if successful, otherwise handles exceptions and prints appropriate messages. **Example:** ```python # Example usage and outputs try: content = read_file(\\"nonexistent.txt\\") except FileProcessingError as e: print(e) # Output: \\"File nonexistent.txt not found.\\" try: content = read_file(\\"corrupted.txt\\") except FileProcessingError as e: print(e) # Output: \\"File corrupted.txt is corrupted.\\" try: process_file(\\"protected.txt\\") except FileProcessingError as e: print(e) # Output: \\"Permission denied for file protected.txt.\\" ``` **Task:** Implement the `FileProcessingError` and its subclasses, and the `read_file` and `process_file` functions according to the specifications above.","solution":"class FileProcessingError(Exception): Base exception class for file processing errors. pass class FileNotFound(FileProcessingError): Exception raised when the specified file does not exist. def __init__(self, file_path): self.file_path = file_path super().__init__(f\\"File {self.file_path} not found.\\") class FileCorrupted(FileProcessingError): Exception raised when the file is detected to be corrupted. def __init__(self, file_path): self.file_path = file_path super().__init__(f\\"File {self.file_path} is corrupted.\\") class PermissionDenied(FileProcessingError): Exception raised when file access is denied. def __init__(self, file_path): self.file_path = file_path super().__init__(f\\"Permission denied for file {self.file_path}.\\") def read_file(file_path): Reads content from the specified file and returns its content. Raises: FileNotFound: If the file does not exist. FileCorrupted: If the file contains the string \\"CORRUPTED\\". PermissionDenied: If access to the file is denied. try: # Simulating file read operations if file_path == \\"nonexistent.txt\\": raise FileNotFound(file_path) elif file_path == \\"corrupted.txt\\": raise FileCorrupted(file_path) elif file_path == \\"protected.txt\\": raise PermissionError(\\"Permission denied\\") else: return \\"File content here\\" except PermissionError as e: raise PermissionDenied(file_path) from e def process_file(file_path): Processes the file by reading its content and printing \\"Processing Complete\\" if successful, and handling exceptions appropriately. try: content = read_file(file_path) print(\\"Processing Complete\\") except FileProcessingError as e: print(e) raise RuntimeError(f\\"Failed to process {file_path}\\") from e"},{"question":"# Title: Implementing Advanced Bisect Functions to Manage Sorted Lists Objective: Write a Python function that utilizes the `bisect` module to perform advanced sorting and searching operations on a list of movie records. Your function should handle inserting new movies, searching for movies by certain criteria, and performing efficient lookup operations using precomputed keys where necessary. Function Requirements: 1. **Function Name**: `manage_movies` 2. **Inputs**: - `movies`: A list of tuples, where each tuple contains three elements: movie name (str), release year (int), and director name (str). The list is always provided in a non-sorted state. - `new_movie`: A tuple containing the movie name, release year, and director name for a new movie to be inserted into the `movies` list. - `search_year`: An integer representing the year used for performing search operations. - `search_method`: A string specifying the search method to be used. It can be one of the following: `\\"find_lt\\"`, `\\"find_le\\"`, `\\"find_gt\\"`, `\\"find_ge\\"`. Outputs: - A tuple containing: - The updated list of movies sorted by release year after inserting the `new_movie` (list). - The result of the search operation according to `search_method` (tuple with three elements: movie name, release year, director name). Constraints: - The `movies` list will always have at least one element. - The `new_movie` will have unique attributes as other movies in the list. - The `search_year` will always be an integer. - Performance: Your function should be optimized for O(log n) search complexity and handle the linear time insertion step efficiently. Example: ```python from bisect import bisect_left, bisect_right, insort_left from operator import itemgetter def manage_movies(movies, new_movie, search_year, search_method): # Preprocess and sort the list of movies based on release year key_func = itemgetter(1) movies.sort(key=key_func) # Insert the new movie in the sorted order insort_left(movies, new_movie, key=key_func) # Function Map for search operations def find_lt(movies, x): i = bisect_left([m[1] for m in movies], x) if i: return movies[i-1] raise ValueError(\\"No element found less than x\\") def find_le(movies, x): i = bisect_right([m[1] for m in movies], x) if i: return movies[i-1] raise ValueError(\\"No element found less than or equal to x\\") def find_gt(movies, x): i = bisect_right([m[1] for m in movies], x) if i != len(movies): return movies[i] raise ValueError(\\"No element found greater than x\\") def find_ge(movies, x): i = bisect_left([m[1] for m in movies], x) if i != len(movies): return movies[i] raise ValueError(\\"No element found greater than or equal to x\\") # Mapping between search methods and functions search_func = { \\"find_lt\\": find_lt, \\"find_le\\": find_le, \\"find_gt\\": find_gt, \\"find_ge\\": find_ge } # Perform the specified search search_result = search_func[search_method](movies, search_year) return movies, search_result # Example usage: movies = [(\\"Jaws\\", 1975, \\"Spielberg\\"), (\\"Titanic\\", 1997, \\"Cameron\\"), (\\"The Birds\\", 1963, \\"Hitchcock\\")] new_movie = (\\"Aliens\\", 1986, \\"Scott\\") search_year = 1975 search_method = \\"find_gt\\" print(manage_movies(movies, new_movie, search_year, search_method)) ``` **Explanation:** Given a list of movies and a new movie to be inserted, the function first sorts the `movies` list by release year, inserts the `new_movie` maintaining the order, and then performs the specified search operation. The result consists of the updated sorted list and the outcome of the search operation.","solution":"from bisect import bisect_left, bisect_right, insort_left from operator import itemgetter def manage_movies(movies, new_movie, search_year, search_method): # Preprocess and sort the list of movies based on release year key_func = itemgetter(1) movies.sort(key=key_func) # Insert the new movie in the sorted order insort_left(movies, new_movie, key=key_func) # Function Map for search operations def find_lt(movies, x): i = bisect_left([m[1] for m in movies], x) if i: return movies[i-1] raise ValueError(\\"No element found less than x\\") def find_le(movies, x): i = bisect_right([m[1] for m in movies], x) if i: return movies[i-1] raise ValueError(\\"No element found less than or equal to x\\") def find_gt(movies, x): i = bisect_right([m[1] for m in movies], x) if i != len(movies): return movies[i] raise ValueError(\\"No element found greater than x\\") def find_ge(movies, x): i = bisect_left([m[1] for m in movies], x) if i != len(movies): return movies[i] raise ValueError(\\"No element found greater than or equal to x\\") # Mapping between search methods and functions search_func = { \\"find_lt\\": find_lt, \\"find_le\\": find_le, \\"find_gt\\": find_gt, \\"find_ge\\": find_ge } # Perform the specified search search_result = search_func[search_method](movies, search_year) return movies, search_result"},{"question":"<|Analysis Begin|> The provided documentation gives a detailed overview of various low-level APIs available in the Python asyncio package. It covers several aspects, including: - **Obtaining the Event Loop**: Methods like `asyncio.get_running_loop()`, `asyncio.get_event_loop()`, etc. - **Event Loop Methods**: Methods to control event loop lifecycle, debugging, scheduling callbacks, thread/process pool, tasks and futures, DNS, networking and IPC, sockets, Unix signals, subprocesses, and error handling. - **Transports**: Methods for working with transports, including read/write transports, datagram transports, and subprocess transports. - **Protocols**: Callback methods for handling connections, data reception, and errors for different types of protocols. - **Event Loop Policies**: Methods to customize event loop policies. With this information, I can design a question that involves creating and managing an event loop, working with transports, and using protocols to handle data. This will be a challenging exercise that assesses the student\'s understanding of how to use asyncio to manage asynchronous I/O operations in Python. <|Analysis End|> <|Question Begin|> # Asyncio Event Handling with Custom Protocol **Objective**: Implement a custom asynchronous chat server using Python\'s asyncio package. Your implementation should showcase the following concepts: - Creating and obtaining event loops. - Using transports and protocols to manage data. - Scheduling callbacks and handling events. Task 1. **Server Implementation**: - Create an asyncio TCP server that listens on a specified port. - Use a custom protocol to handle incoming connections and messages. 2. **Custom Protocol**: - Implement a protocol class that inherits from `asyncio.Protocol`. - The protocol should handle connections and disconnections, as well as data reception. - Upon receiving data, the protocol should echo the received message to all connected clients. 3. **Client Management**: - Maintain a list of all connected clients and their respective transports. - Remove clients from the list upon disconnection. 4. **Running the Server**: - Create an event loop and run the server until manually stopped. Input/Output Requirements - **Input**: - The server should listen on a port specified as an integer (e.g., `8888`). - **Output**: - The server should print status messages to the console, such as \\"Client connected\\" or \\"Message received: <message>\\". - Messages received from clients should be echoed back to all connected clients. Performance Requirements - The server should efficiently handle multiple simultaneous client connections. - Implement proper exception handling to avoid server crashes. Example Scenario 1. **Start the Server**: ```python asyncio.run(start_server(8888)) ``` 2. **Client Connections**: - Client 1 connects and sends the message \\"Hello\\". - Client 2 connects and sends the message \\"World\\". - Both clients receive both messages, i.e., \\"Hello\\" and \\"World\\". Constraints - Do not use third-party libraries. - Ensure compatibility with Python 3.10 or later. Boilerplate Code ```python import asyncio class ChatProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport # Implement connection handling logic def data_received(self, data): message = data.decode() # Implement data handling logic def connection_lost(self, exc): # Implement disconnection handling logic async def start_server(port): loop = asyncio.get_running_loop() server = await loop.create_server(ChatProtocol, \'127.0.0.1\', port) async with server: await server.serve_forever() if __name__ == \'__main__\': port = 8888 # Example port asyncio.run(start_server(port)) ``` Fill in the missing parts of the `ChatProtocol` class and complete the `start_server` function to meet the requirements specified.","solution":"import asyncio class ChatProtocol(asyncio.Protocol): clients = [] def connection_made(self, transport): self.transport = transport self.clients.append(transport) print(\\"Client connected\\") def data_received(self, data): message = data.decode() print(f\\"Message received: {message}\\") for client in self.clients: client.write(data) def connection_lost(self, exc): print(\\"Client disconnected\\") self.clients.remove(self.transport) async def start_server(port): loop = asyncio.get_running_loop() server = await loop.create_server(ChatProtocol, \'127.0.0.1\', port) async with server: await server.serve_forever() if __name__ == \'__main__\': port = 8888 # Example port asyncio.run(start_server(port))"},{"question":"**Objective:** To assess your understanding of the `aifc` module in Python for handling AIFF and AIFC audio files. **Problem Statement:** You are provided with an AIFF or AIFC audio file. Your task is to write a Python function that reads this audio file, increases the volume of the audio by a specified factor, and writes the modified audio to a new AIFF or AIFC file. The function should handle both mono and stereo audio. **Function Signature:** ```python def increase_volume(input_file: str, output_file: str, volume_factor: float) -> None: Increases the volume of the audio file by the specified volume factor. Args: input_file (str): The path to the input AIFF or AIFC audio file. output_file (str): The path to the output AIFF or AIFC audio file. volume_factor (float): The factor by which to increase the volume. Returns: None pass ``` **Input:** - `input_file`: A string representing the path to the input AIFF or AIFC audio file. - `output_file`: A string representing the path to the output AIFF or AIFC audio file. - `volume_factor`: A float representing the factor by which to increase the volume. A value of 1.0 leaves the volume unchanged, while values >1.0 increase the volume, and values <1.0 decrease the volume. **Output:** - The function does not return anything. It writes the modified audio to the provided `output_file` path. **Constraints:** 1. The `input_file` should be a valid AIFF or AIFC file. 2. The `output_file` should be a valid path where the output file can be written. 3. The `volume_factor` should be a positive float. **Example:** ```python increase_volume(\\"input.aiff\\", \\"output.aiff\\", 1.5) ``` This will read the `input.aiff` file, increase the volume by 50%, and write the result to `output.aiff`. **Notes:** - Ensure that the modified audio file retains the same metadata (number of channels, sample width, frame rate, etc.) as the original. - Handle potential exceptions that may arise, such as invalid file paths or unsupported file formats. - You may assume that the audio data is uncompressed (i.e., `getcomptype` returns `b\'NONE\'`). Good luck!","solution":"import aifc def increase_volume(input_file: str, output_file: str, volume_factor: float) -> None: Increases the volume of the audio file by the specified volume factor. Args: input_file (str): The path to the input AIFF or AIFC audio file. output_file (str): The path to the output AIFF or AIFC audio file. volume_factor (float): The factor by which to increase the volume. Returns: None with aifc.open(input_file, \'rb\') as input_audio: # Read audio parameters nchannels = input_audio.getnchannels() sampwidth = input_audio.getsampwidth() framerate = input_audio.getframerate() nframes = input_audio.getnframes() comptype = input_audio.getcomptype() compname = input_audio.getcompname() # Read audio data audio_data = input_audio.readframes(nframes) # Convert audio data to a list of integers audio_samples = list(int.from_bytes(audio_data[i:i+sampwidth], byteorder=\'big\', signed=True) for i in range(0, len(audio_data), sampwidth)) # Increase the volume audio_samples = [int(sample * volume_factor) for sample in audio_samples] # Truncate values that exceed the sample width max_value = (1 << (8 * sampwidth - 1)) - 1 min_value = -(1 << (8 * sampwidth - 1)) audio_samples = [max(min(max_value, sample), min_value) for sample in audio_samples] # Convert the modified audio samples back to bytes modified_audio_data = b\'\'.join(sample.to_bytes(sampwidth, byteorder=\'big\', signed=True) for sample in audio_samples) # Write the modified audio data to the output file with aifc.open(output_file, \'wb\') as output_audio: output_audio.setnchannels(nchannels) output_audio.setsampwidth(sampwidth) output_audio.setframerate(framerate) output_audio.setnframes(nframes) output_audio.setcomptype(comptype, compname) output_audio.writeframes(modified_audio_data)"},{"question":"**Title: Synthetic Data Generation and Analysis in scikit-learn** **Objective:** In this task, you are required to demonstrate your understanding of scikit-learn\'s synthetic data generation functions. You will generate datasets using different functions, preprocess the data, and perform simple exploratory data analysis. **Problem Statement:** 1. **Data Generation:** - Use the `make_classification` function to generate a synthetic dataset with the following specifications: - `n_samples`: 1000 (number of samples) - `n_features`: 20 (total number of features) - `n_informative`: 2 (number of informative features) - `n_redundant`: 10 (number of redundant features) - `n_clusters_per_class`: 1 - `random_state`: 42 (for reproducibility) - Use the `make_blobs` function to generate a clustering dataset with the following specifications: - `n_samples`: 500 (number of samples) - `centers`: 5 (number of centers to generate) - `cluster_std`: 1.0 (standard deviation of the clusters) - `random_state`: 42 (for reproducibility) - Use the `make_regression` function to generate a regression dataset with the following specifications: - `n_samples`: 1000 (number of samples) - `n_features`: 20 (total number of features) - `n_informative`: 10 (number of informative features) - `noise`: 0.1 (standard deviation of the noise) - `random_state`: 42 (for reproducibility) 2. **Data Preprocessing and Analysis:** - For the classification dataset generated by `make_classification`: - Perform normalization on the dataset using `StandardScaler` from scikit-learn. - Split the dataset into training (80%) and testing (20%) sets using `train_test_split`. - Print the shapes of the training and testing sets. - For the clustering dataset generated by `make_blobs`: - Visualize the dataset using a scatter plot, where each point is colored according to its cluster label. - For the regression dataset generated by `make_regression`: - Perform normalization on the dataset using `StandardScaler` from scikit-learn. - Split the dataset into training (80%) and testing (20%) sets using `train_test_split`. - Print the shapes of the training and testing sets. **Input:** None **Output:** 1. Shapes of the training and testing sets for both classification and regression datasets. 2. Scatter plot visualization of the clustering dataset. **Constraints:** - Use `random_state=42` wherever applicable to ensure reproducibility. - Libraries allowed: `sklearn`, `matplotlib` **Function Signature:** ```python def generate_and_analyze_datasets(): # Generate and preprocess classification dataset # Generate and visualize clustering dataset # Generate and preprocess regression dataset pass ``` **Example Output:** ``` Classification dataset shapes: Training set: (800, 20) Testing set: (200, 20) Clustering dataset visualization displayed. Regression dataset shapes: Training set: (800, 20) Testing set: (200, 20) ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_blobs, make_regression from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split def generate_and_analyze_datasets(): # Generate classification dataset X_classification, y_classification = make_classification( n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, random_state=42) # Normalize the classification dataset scaler = StandardScaler() X_classification_scaled = scaler.fit_transform(X_classification) # Split the classification dataset X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split( X_classification_scaled, y_classification, test_size=0.2, random_state=42) # Print the shapes of the training and testing sets for classification dataset print(f\\"Classification dataset shapes:nTraining set: {X_train_cls.shape}nTesting set: {X_test_cls.shape}\\") # Generate clustering dataset X_blobs, y_blobs = make_blobs(n_samples=500, centers=5, cluster_std=1.0, random_state=42) # Visualize the clustering dataset plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap=\'viridis\', marker=\'o\') plt.title(\'Clustering Dataset Visualization\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Generate regression dataset X_regression, y_regression = make_regression( n_samples=1000, n_features=20, n_informative=10, noise=0.1, random_state=42) # Normalize the regression dataset X_regression_scaled = scaler.fit_transform(X_regression) # Split the regression dataset X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split( X_regression_scaled, y_regression, test_size=0.2, random_state=42) # Print the shapes of the training and testing sets for regression dataset print(f\\"Regression dataset shapes:nTraining set: {X_train_reg.shape}nTesting set: {X_test_reg.shape}\\") # Call the function to generate and analyze datasets generate_and_analyze_datasets()"},{"question":"# Python Coding Assessment Question Title: Implementing Custom Sequence and Mapping Protocol Operations Problem Statement You are required to implement a Python class that simulates a specialized data structure that combines behaviors of both sequence and mapping protocols. This custom data structure, named `CustomDataStructure`, should support the following functionalities: 1. **Initialization**: Accept a list of tuples where each tuple contains a key-value pair. 2. **Sequence Protocol**: - Support indexing and slicing. - Support iteration. - Support length querying using `len()`. 3. **Mapping Protocol**: - Allow access and assignment to items by key as well as by index. - Implement a method to remove an item by key. - Method to clear all the items. 4. **Special Methods**: - `__repr__` for a string representation of the object. - `__contains__` to check if a key exists in the data structure. Implement the class `CustomDataStructure` with the following method signatures: ```python class CustomDataStructure: def __init__(self, data: list): Initialize the data structure with a list of key-value tuples. pass def __getitem__(self, key): Get the item by index or key. pass def __setitem__(self, key, value): Set the item by index or key. pass def __delitem__(self, key): Delete the item by key. pass def __len__(self): Return the number of items in the data structure. pass def __iter__(self): Return an iterator over the key-value pairs. pass def __repr__(self): Return a string representation of the data structure. pass def __contains__(self, key): Check if a key exists in the data structure. pass def clear(self): Remove all items from the data structure. pass ``` Input and Output - **Initialization Input**: A list of tuples, e.g., `[(\'a\', 1), (\'b\', 2), (\'c\', 3)]`. - **Index Access**: `cds[1]` -> Output: `2`. - **Key Access**: `cds[\'a\']` -> Output: `1`. - **Iteration**: `for item in cds` -> Iterate over (`(\'a\', 1), (\'b\', 2), (\'c\', 3)`). - **Length Query**: `len(cds)` -> Output: `3`. - **Key Assignment**: `cds[\'d\'] = 4` adds key-value pair `(\'d\', 4)`. - **Item Deletion**: `del cds[\'a\']` removes the key-value pair with key `\'a\'`. - **Containment Check**: `\'a\' in cds` -> Output: `False` after deletion. - **Clear**: `cds.clear()` -> Removes all items, `len(cds)` -> Output: `0`. Constraints - The class should handle exceptions gracefully. - It should efficiently manage both accessing by index and by key. Performance Requirements - The implementation should be efficient in terms of both time and space complexity. Aim for an average time complexity of O(1) for both index and key access. Good luck!","solution":"class CustomDataStructure: def __init__(self, data): self.data = data self.key_map = {k: v for k, v in data} def __getitem__(self, key): if isinstance(key, int): return self.data[key][1] else: return self.key_map.get(key) def __setitem__(self, key, value): if isinstance(key, int): self.data[key] = (self.data[key][0], value) self.key_map[self.data[key][0]] = value else: for i, (k, v) in enumerate(self.data): if k == key: self.data[i] = (key, value) break else: self.data.append((key, value)) self.key_map[key] = value def __delitem__(self, key): if key in self.key_map: for i, (k, v) in enumerate(self.data): if k == key: del self.data[i] break del self.key_map[key] def __len__(self): return len(self.data) def __iter__(self): return iter(self.data) def __repr__(self): return f\\"CustomDataStructure({self.data})\\" def __contains__(self, key): return key in self.key_map def clear(self): self.data.clear() self.key_map.clear()"},{"question":"Advanced ZIP File Operations Objective: You are tasked with implementing a script that can handle multiple ZIP file operations, including reading the contents of a ZIP file, extracting specific files based on certain criteria, and writing new files into a ZIP archive. The purpose is to demonstrate comprehension of the `zipfile` module\'s functionalities as well as proper handling of file operations in Python. Problem Description: Write a Python class `ZipFileManager` that performs the following operations on ZIP files: 1. **List Contents**: - Method: `list_contents(zip_path: str) -> List[str]` - **Input**: `zip_path` - A string representing the path to the ZIP file. - **Output**: A list of filenames contained in the ZIP file. - **Exception Handling**: Raise `zipfile.BadZipFile` if the ZIP file is invalid. 2. **Extract Files Based on Extension**: - Method: `extract_files(zip_path: str, extract_dir: str, extensions: List[str]) -> None` - **Input**: - `zip_path` - A string representing the path to the ZIP file. - `extract_dir` - A string representing the directory where files should be extracted. - `extensions` - A list of file extensions (e.g., `[\'.txt\', \'.py\']`). Only files with these extensions should be extracted. - **Output**: None - **Exception Handling**: Handle extraction errors gracefully and print error messages. 3. **Add File to ZIP**: - Method: `add_file(zip_path: str, file_path: str, arcname: Optional[str] = None) -> None` - **Input**: - `zip_path` - A string representing the path to the ZIP file. - `file_path` - A string representing the path to the file to be added. - `arcname` - An optional string representing the name that the file should have inside the ZIP archive. If not provided, the file keeps its original name. - **Output**: None - **Exception Handling**: Ensure the ZIP file is properly accessible for writing and handle any file-related errors. Constraints: - Use the `zipfile` module to implement all functionalities. - You may assume all paths provided are valid but handle cases where files might not be present within the ZIP archive as expected. Performance Requirements: - The operations must be efficient and should not read the entire file into memory when not needed. For example, listing contents should not trigger full extraction. Example Usage: ```python if __name__ == \\"__main__\\": zfm = ZipFileManager() # List contents of the ZIP file print(zfm.list_contents(\'example.zip\')) # Extract all .txt and .py files to the \'extract_dir\' directory zfm.extract_files(\'example.zip\', \'extract_dir\', [\'.txt\', \'.py\']) # Add a new file to the ZIP archive zfm.add_file(\'example.zip\', \'new_file.txt\', \'docs/new_file.txt\') ``` Implementation Notes: - Use context management with the `with` statement to ensure files are properly closed after operations. - Leverage exception handling to manage potential errors gracefully. Submission: Submit the `ZipFileManager` class implementation along with a few test cases demonstrating its usage and handling of edge cases.","solution":"import zipfile from typing import List, Optional class ZipFileManager: def list_contents(self, zip_path: str) -> List[str]: List the contents of a ZIP file. :param zip_path: Path to the ZIP file :return: List of file names in the ZIP file :raises zipfile.BadZipFile: If the ZIP file is invalid try: with zipfile.ZipFile(zip_path, \'r\') as zip_file: return zip_file.namelist() except zipfile.BadZipFile: raise zipfile.BadZipFile(f\\"Invalid ZIP file: {zip_path}\\") def extract_files(self, zip_path: str, extract_dir: str, extensions: List[str]) -> None: Extract specific files from a ZIP file based on file extensions. :param zip_path: Path to the ZIP file :param extract_dir: Directory where the files should be extracted :param extensions: List of file extensions to filter by try: with zipfile.ZipFile(zip_path, \'r\') as zip_file: for file_name in zip_file.namelist(): if any(file_name.endswith(ext) for ext in extensions): try: zip_file.extract(file_name, extract_dir) except Exception as e: print(f\\"Error extracting {file_name}: {e}\\") except zipfile.BadZipFile: raise zipfile.BadZipFile(f\\"Invalid ZIP file: {zip_path}\\") def add_file(self, zip_path: str, file_path: str, arcname: Optional[str] = None) -> None: Add a file to a ZIP archive. :param zip_path: Path to the ZIP file :param file_path: Path to the file to be added :param arcname: Optional name for the file inside the ZIP archive try: with zipfile.ZipFile(zip_path, \'a\') as zip_file: zip_file.write(file_path, arcname=arcname) except Exception as e: print(f\\"Error adding file {file_path} to ZIP {zip_path}: {e}\\")"},{"question":"Objective: Demonstrate your comprehension of seaborn\'s `axes_style` function, including how to retrieve and apply different styles, as well as how to use the function within a context manager for temporary style changes. Task: You are provided with a dataset containing information on the average monthly temperatures for a given year. Your task is to create two bar plots showing these temperatures, but with different styling for each plot using seaborn\'s `axes_style` function. Specifically, you need to: 1. Create a bar plot using the \\"darkgrid\\" style. 2. Create a bar plot using the \\"whitegrid\\" style but only temporarily within a context manager. Input: A Python dictionary named `data` representing the average temperatures by month, structured as follows: ```python data = { \\"months\\": [\\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"], \\"temperatures\\": [30.5, 32.0, 45.0, 50.0, 60.0, 70.0, 75.0, 73.0, 68.0, 55.0, 45.0, 35.0] } ``` Requirements: - Use seaborn to generate the plots. - Ensure the plots are displayed with different styles as described. Output: Two bar plots (one with \\"darkgrid\\" and one temporarily with \\"whitegrid\\"). Constraints: - You must use `sns.axes_style` to set the styles. - Temporary style changes must be made using a context manager. Example: ```python import seaborn as sns import matplotlib.pyplot as plt data = { \\"months\\": [\\"January\\", \\"February\\", \\"March\\", \\"April\\", \\"May\\", \\"June\\", \\"July\\", \\"August\\", \\"September\\", \\"October\\", \\"November\\", \\"December\\"], \\"temperatures\\": [30.5, 32.0, 45.0, 50.0, 60.0, 70.0, 75.0, 73.0, 68.0, 55.0, 45.0, 35.0] } # Plot 1: Darkgrid style sns.set_style(\\"darkgrid\\") plt.figure(figsize=(10, 6)) sns.barplot(x=data[\\"months\\"], y=data[\\"temperatures\\"]) plt.title(\\"Average Monthly Temperatures - Darkgrid Style\\") plt.xticks(rotation=45) plt.show() # Plot 2: Temporarily using Whitegrid style with sns.axes_style(\\"whitegrid\\"): plt.figure(figsize=(10, 6)) sns.barplot(x=data[\\"months\\"], y=data[\\"temperatures\\"]) plt.title(\\"Average Monthly Temperatures - Whitegrid Style\\") plt.xticks(rotation=45) plt.show() ``` This question assesses your ability to modify plot aesthetics using seaborn and your understanding of context managers for temporary modifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_temperatures(data): Plots the average monthly temperatures using different seaborn styles. :param data: Dictionary containing \'months\' and \'temperatures\' lists # Plot 1: Darkgrid style sns.set_style(\\"darkgrid\\") plt.figure(figsize=(10, 6)) sns.barplot(x=data[\\"months\\"], y=data[\\"temperatures\\"]) plt.title(\\"Average Monthly Temperatures - Darkgrid Style\\") plt.xticks(rotation=45) plt.show() # Plot 2: Temporarily using Whitegrid style with sns.axes_style(\\"whitegrid\\"): plt.figure(figsize=(10, 6)) sns.barplot(x=data[\\"months\\"], y=data[\\"temperatures\\"]) plt.title(\\"Average Monthly Temperatures - Whitegrid Style\\") plt.xticks(rotation=45) plt.show()"},{"question":"# Seaborn Plot Customization and Annotation Objective: You are provided with a dataset and tasked with creating a series of plots using the `seaborn.objects` module in Python to demonstrate your understanding of adding and customizing text annotations. Dataset: The dataset `glue` contains model performance scores. Ensure the following steps to prepare the data: 1. Load the dataset using `seaborn.load_dataset(\\"glue\\")`. 2. Pivot the dataset with `Model` and `Encoder` as the index, `Task` as columns, and `Score` as values. 3. Add an `Average` column that represents the mean score of each model across all tasks, rounded to one decimal place. 4. Sort the dataset based on the `Average` column in descending order. Tasks: 1. **Scatter Plot with Text Annotations:** - Create a scatter plot with `x` as `SST-2`, `y` as `MRPC`, and text annotations as `Model`. - Add the text annotations at the corresponding `x`/`y` locations. 2. **Bar Plot with Customized Text Annotations:** - Create a horizontal bar plot with `x` as `Average`, `y` as `Model`, and text annotations as `Average`. - Align the text to the right within the bars and color the text white. 3. **Dot Plot with Text Alignments:** - Create a dot plot with `x` as `RTE`, `y` as `MRPC`, text annotations as `Model`, and color annotations based on `Encoder`. - Place the text annotations above the dots and align the text according to the `Encoder`. Use custom text alignment: left for \\"LSTM\\" and right for \\"Transformer\\". 4. **Advanced Customization with Matplotlib Parameters:** - Enhance the dot plot by making the text bold using the `fontweight` parameter from Matplotlib. Code Requirements: - Use `seaborn.objects` for all plotting tasks. - Each plot should be created as a separate plot call, and the code should be modular. - Each plot should be displayed sequentially in the output. Constraints: - Follow best practices for data visualization, including clear labels and appropriate text size. - Ensure that the code is well-documented and includes comments explaining each step. Input: ```python import seaborn.objects as so from seaborn import load_dataset # Load and prepare the dataset as described above glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Your solution code here ``` Output: The code should generate the following plots: 1. Scatter plot with model names as text annotations. 2. Horizontal bar plot with the average scores annotated inside the bars. 3. Dot plot with text annotations above the dots, with custom text alignment based on the encoder type. 4. Enhanced dot plot with bold text annotations.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and prepare the dataset as described above glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) import matplotlib.pyplot as plt # Function to create scatter plot with text annotations def scatter_plot_with_annotations(data): p = so.Plot(data, x=\\"SST-2\\", y=\\"MRPC\\").add(so.Dot(), so.Agg()) for i in range(len(data)): plt.text(data[\'SST-2\'][i], data[\'MRPC\'][i], data.index[i][0]) plt.show() # Function to create bar plot with customized text annotations def bar_plot_with_annotations(data): p = so.Plot(data.reset_index(), y=\\"Model\\", x=\\"Average\\", color=\\"Average\\").add(so.Bar()) for i in range(len(data)): plt.text(data[\'Average\'][i] - 0.1, i, data[\'Average\'][i], va=\'center\', ha=\'right\', color=\'white\', weight=\'bold\') plt.gca().invert_yaxis() plt.show() # Function to create dot plot with text alignments def dot_plot_with_text_alignments(data): p = so.Plot(data.reset_index(), x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\").add(so.Dot(), so.Agg()) for i in range(len(data)): alignment = \'left\' if data.index[i][1] == \'LSTM\' else \'right\' plt.text(data[\'RTE\'][i], data[\'MRPC\'][i] + 0.2, data.index[i][0], ha=alignment) plt.show() # Function to create enhanced dot plot with bold text def enhanced_dot_plot_with_bold_text(data): p = so.Plot(data.reset_index(), x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\").add(so.Dot(), so.Agg()) for i in range(len(data)): alignment = \'left\' if data.index[i][1] == \'LSTM\' else \'right\' plt.text(data[\'RTE\'][i], data[\'MRPC\'][i] + 0.2, data.index[i][0], ha=alignment, weight=\'bold\') plt.show()"},{"question":"**Objective:** To assess your understanding of the `modulefinder` module in Python, you are required to write a Python function that analyzes a given script to find all imported modules and identify any missing modules. Additionally, you will enhance the functionality to record package paths and replace module names according to given specifications. # Problem Description You need to implement a function `analyze_script_imports(script_path, package_paths, replacement_modules)` that performs the following tasks: 1. Analyzes the given `script_path` to find all imported modules. 2. Records additional package paths as specified by `package_paths`. 3. Replaces module names as specified by `replacement_modules`. 4. Returns the analysis report as a dictionary with two keys: - `\'loaded_modules\'`: A list of module names that were successfully imported. - `\'missing_modules\'`: A list of module names that were missing or failed to import. # Input - `script_path` (str): The file path of the Python script to be analyzed. - `package_paths` (List[Tuple[str, str]]): A list of tuples, where each tuple contains: - `pkg_name` (str): The package name. - `path` (str): The directory path where the package can be found. - `replacement_modules` (List[Tuple[str, str]]): A list of tuples, where each tuple contains: - `oldname` (str): The original module name. - `newname` (str): The new module name to replace the original module name. # Output - A dictionary with two keys: - `\'loaded_modules\'` (List[str]): A list of module names that were successfully imported. - `\'missing_modules\'` (List[str]): A list of module names that were missing or failed to import. # Constraints 1. You can assume that `script_path` is a valid path to a Python script. 2. Any package path specified in `package_paths` will also be valid. 3. The replacement module names specified in `replacement_modules` will be valid. # Example ```python def analyze_script_imports(script_path, package_paths, replacement_modules): # Your implementation here # Example usage script_path = \'bacon.py\' package_paths = [(\'my_package\', \'/path/to/my/package\')] replacement_modules = [(\'old_module\', \'new_module\')] result = analyze_script_imports(script_path, package_paths, replacement_modules) print(result) ``` Given the `bacon.py` script from the documentation, suppose that `bacon.py` contains: ```python import re, itertools try: import baconhameggs except ImportError: pass try: import guido.python.ham except ImportError: pass ``` You should expect the output (omitting the detailed content of loaded modules): ```python { \'loaded_modules\': [\'re\', \'itertools\', \'sre_compile\', ...], \'missing_modules\': [\'baconhameggs\', \'guido.python.ham\'] } ``` # Performance Requirements The function should efficiently handle scripts that import a large number of modules.","solution":"import os import sys import modulefinder from typing import List, Tuple, Dict def analyze_script_imports(script_path: str, package_paths: List[Tuple[str, str]], replacement_modules: List[Tuple[str, str]]) -> Dict[str, List[str]]: # Adding additional package paths for pkg_name, path in package_paths: if path not in sys.path: sys.path.insert(0, path) # Analyzing the script imports finder = modulefinder.ModuleFinder() try: finder.run_script(script_path) except Exception as e: return {\'loaded_modules\': [], \'missing_modules\': [str(e)]} # Processing loaded modules and missing modules loaded_modules = list(finder.modules.keys()) missing_modules = list(finder.badmodules.keys()) # Replacing module names as specified replacement_dict = dict(replacement_modules) loaded_modules = [replacement_dict.get(mod, mod) for mod in loaded_modules] missing_modules = [replacement_dict.get(mod, mod) for mod in missing_modules] return { \'loaded_modules\': sorted(loaded_modules), \'missing_modules\': sorted(missing_modules) }"},{"question":"<|Analysis Begin|> The provided documentation details the functionalities of the `compileall` module in Python, which is used to compile Python source files into byte-code. This module provides utility functions that can recursively traverse directory trees and compile all Python source files they find. It includes command-line usage options and various public functions like `compile_dir`, `compile_file`, and `compile_path`. The `compile_dir` function recursively compiles all `.py` files in a directory, while `compile_file` compiles a specific file, and `compile_path` compiles all `.py` files along `sys.path`. There are numerous options for controlling the compilation process, such as specifying directories to include or exclude, controlling recursion depth, using multiple workers for parallel compilation, and handling optimization levels. Given these functionalities, a good coding question can be designed to test a student\'s understanding of directory traversal, file handling, regular expressions, and managing compile options. <|Analysis End|> <|Question Begin|> # Python Coding Assessment Objective You are tasked with writing a Python script that utilizes the `compileall` module to compile Python source files within a specific directory with certain conditions. Your script must demonstrate your understanding of directory traversal, file handling, regular expressions, and optional compile parameters. Task Write a function `compile_python_files_in_directory` that: - Compiles all `.py` files in a given directory and its subdirectories. - Excludes files in any directory named \\"test\\" or any file that begins with \\"test_\\" using a regular expression. - Compiles using two optimization levels: `0` (no optimization) and `2` (max optimization). - Uses multiple workers to perform compilation in parallel. - Suppresses printing list of compiled files but requires printing error messages. Function Signature ```python def compile_python_files_in_directory(directory: str) -> bool: pass ``` Input - `directory` (str): The directory path where the `.py` files are located. Output - Returns `True` if all files are compiled successfully, otherwise returns `False`. Example ```python result = compile_python_files_in_directory(\'/path/to/your/directory\') print(result) # Should print either True or False ``` Requirements 1. Use the `compile_dir` function from the `compileall` module to perform the compilation. 2. Apply a regular expression to exclude directories named \\"test\\" and files that begin with \\"test_\\". 3. Utilize optimization levels `0` and `2`. 4. Use multiple workers to compile files in parallel. 5. Suppress the printout of the list of compiled files but ensure error messages are printed. Constraints - Assume the `compileall` module and other necessary libraries are available. - The function should handle different operating systems (Windows, Linux, Mac). - The directory path provided will be valid and accessible. Special attention should be given to: - Correctly compiling files with the specified conditions. - Efficient use of multi-worker compilation. - Proper usage of regular expressions to exclude files and directories. Notes - You may assume that the `directory` input will be in a format appropriate for the filesystem in use. - Focus on handling the task robustly to ensure proper compilation under the given conditions. Good luck!","solution":"import compileall import re def compile_python_files_in_directory(directory: str) -> bool: Compiles all .py files in the given directory and its subdirectories excluding: - Files in any directory named \\"test\\". - Files that begin with \\"test_\\". The compilation uses two optimization levels (0 and 2) and multiple workers. Args: directory (str): The directory path for the .py files. Returns: bool: True if all files compiled successfully, otherwise False. try: exclusions = re.compile(r\'(^|/)test(/|)|(test_.*.py)\') result_0 = compileall.compile_dir( dir=directory, rx=exclusions, quiet=3, # Suppresses output of compiled files optimize=0, # No optimization workers=2 # Number of parallel workers ) result_2 = compileall.compile_dir( dir=directory, rx=exclusions, quiet=3, # Suppresses output of compiled files optimize=2, # Max optimization workers=2 # Number of parallel workers ) return result_0 and result_2 except Exception as e: print(f\\"Compilation error: {e}\\") return False"},{"question":"**Problem Statement**: Write a Python function that parses an XML document specified by a filename, extracts specific information from the document using a SAX content handler, and handles potential parsing errors gracefully. # Requirements: 1. **Function Signature**: ```python def parse_xml(filename: str) -> dict: ``` 2. **Input**: - `filename` (str): The filename of the XML document to be parsed. 3. **Output**: - A dictionary with keys `\'titles\'` and `\'authors\'`, where `\'titles\'` is a list of book titles and `\'authors\'` is a list of author names extracted from the XML document. 4. **Constraints**: - The XML document follows this structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> </book> <book> <title>Book Title 2</title> <author>Author 2</author> </book> <!-- More book nodes --> </library> ``` 5. **Error Handling**: - Implement custom error handling to manage exceptions related to SAX parsing and return an empty dictionary in case of errors. # Implementation Details: - Use the `xml.sax.make_parser()` function to create an `XMLReader` object. - Define a custom content handler class that inherits from `xml.sax.handler.ContentHandler`. This handler should extract `<title>` and `<author>` tags\' data and store them in lists. - Implement an error handler to catch and gracefully handle exceptions using `xml.sax.SAXParseException`. - Return a dictionary with the extracted titles and authors. # Example: Given the following XML file `books.xml`: ```xml <library> <book> <title>1984</title> <author>George Orwell</author> </book> <book> <title>Brave New World</title> <author>Aldous Huxley</author> </book> </library> ``` Calling `parse_xml(\'books.xml\')` should return: ```python { \'titles\': [\'1984\', \'Brave New World\'], \'authors\': [\'George Orwell\', \'Aldous Huxley\'] } ``` **Notes**: - Ensure your solution can handle parsing errors and return an empty dictionary if any error occurs. - Use the methods and classes provided by the `xml.sax` package as specified in the documentation above.","solution":"import xml.sax class BookContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_data = \\"\\" self.titles = [] self.authors = [] self.current_title = \\"\\" self.current_author = \\"\\" def startElement(self, tag, attributes): self.current_data = tag def endElement(self, tag): if tag == \\"title\\": self.titles.append(self.current_title) self.current_title = \\"\\" elif tag == \\"author\\": self.authors.append(self.current_author) self.current_author = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.current_title += content elif self.current_data == \\"author\\": self.current_author += content def parse_xml(filename: str) -> dict: parser = xml.sax.make_parser() handler = BookContentHandler() parser.setContentHandler(handler) try: parser.parse(filename) return {\'titles\': handler.titles, \'authors\': handler.authors} except xml.sax.SAXParseException as e: return {}"},{"question":"You are tasked with writing a Python program that simulates a simplified version of a download manager using the asyncio library. The manager will handle multiple download tasks concurrently, ensure proper usage of available resources, and manage exceptions such as timeouts or cancellations of tasks. # Requirements 1. **Tasks**: Implement multiple download tasks that simulate downloading files by sleeping for a given amount of time (use `await asyncio.sleep()`). 2. **Queues**: Use an `asyncio.Queue` to manage the queue of files to be downloaded. 3. **Timeouts**: Implement a timeout for each download task (use `await asyncio.wait_for()`). 4. **Synchronization**: Ensure that no more than a specified number of download tasks can run concurrently (use `asyncio.Semaphore`). 5. **Exception Handling**: Properly handle and log any timeouts or cancellations that occur during the download tasks. # Input and Output Formats - **Input**: A list of tuples where each tuple contains the file name (string) and the download time in seconds (integer). ```python files_to_download = [(\\"file1.txt\\", 5), (\\"file2.txt\\", 3), (\\"file3.txt\\", 7)] ``` - **Output**: Print statements to indicate the start and end of each download, and any errors encountered (timeout or cancellation). # Constraints - The maximum number of concurrent downloads is 2. - The timeout for each download task is 6 seconds. # Function Signature ```python import asyncio import random async def download_file(queue, semaphore): while True: async with semaphore: file_name, download_time = await queue.get() if file_name is None: queue.task_done() break try: print(f\\"Start downloading {file_name}\\") await asyncio.wait_for(asyncio.sleep(download_time), timeout=6) print(f\\"Finished downloading {file_name}\\") except asyncio.TimeoutError: print(f\\"Timeout while downloading {file_name}\\") except asyncio.CancelledError: print(f\\"Download of {file_name} was cancelled\\") finally: queue.task_done() async def main(files_to_download): queue = asyncio.Queue() semaphore = asyncio.Semaphore(2) for file in files_to_download: await queue.put(file) # Add `None` to queue to indicate that download tasks should exit for _ in range(min(len(files_to_download), 2)): await queue.put((None, None)) tasks = [asyncio.create_task(download_file(queue, semaphore)) for _ in range(2)] await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) # Example usage files_to_download = [(\\"file1.txt\\", 5), (\\"file2.txt\\", 3), (\\"file3.txt\\", 7)] asyncio.run(main(files_to_download)) ``` # Analysis This question will test the student\'s ability to: 1. Use asyncio to manage concurrent tasks. 2. Work with asyncio queues to manage task inputs. 3. Implement timeouts and handle exceptions gracefully. 4. Utilize semaphores to limit concurrent access to resources.","solution":"import asyncio async def download_file(queue, semaphore): while True: async with semaphore: file_name, download_time = await queue.get() if file_name is None: queue.task_done() break try: print(f\\"Start downloading {file_name}\\") await asyncio.wait_for(asyncio.sleep(download_time), timeout=6) print(f\\"Finished downloading {file_name}\\") except asyncio.TimeoutError: print(f\\"Timeout while downloading {file_name}\\") except asyncio.CancelledError: print(f\\"Download of {file_name} was cancelled\\") finally: queue.task_done() async def main(files_to_download): queue = asyncio.Queue() semaphore = asyncio.Semaphore(2) for file in files_to_download: await queue.put(file) # Add `None` to queue to indicate that download tasks should exit for _ in range(min(len(files_to_download), 2)): await queue.put((None, None)) tasks = [asyncio.create_task(download_file(queue, semaphore)) for _ in range(2)] await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) # Example usage files_to_download = [(\\"file1.txt\\", 5), (\\"file2.txt\\", 3), (\\"file3.txt\\", 7)] asyncio.run(main(files_to_download))"},{"question":"Problem Statement In this exercise, you will implement a function that analyses the properties and transformations of a complex number using the cmath module. # Objectives: 1. **Modulus and Phase**: Convert a complex number to its polar coordinates. 2. **Rectangular to Polar and Back**: Ensure that converting to polar coordinates and then back to rectangular coordinates retrieves the original complex number. 3. **Branch Cut Behavior**: Demonstrate the branch cut behavior for specific functions in cmath, specifically the `sqrt()` and `log()` functions. # Requirements: - Implement a function `analyze_complex_number(z)` where `z` is a complex number. - The function should return a dictionary with the following components: - `\\"modulus\\"`: The modulus (r) of the complex number. - `\\"phase\\"`: The phase (φ) of the complex number. - `\\"rectangular_conversion\\"`: The rectangular coordinates obtained by converting polar coordinates back to rectangular coordinates. - `\\"sqrt_below_branch\\"`: The result of `cmath.sqrt()` for the complex number `z` when `z` is slightly below the branch cut. - `\\"sqrt_above_branch\\"`: The result of `cmath.sqrt()` for the complex number `z` when `z` is slightly above the branch cut. - `\\"log_below_branch\\"`: The result of `cmath.log()` for the complex number `z` when `z` is slightly below the branch cut. - `\\"log_above_branch\\"`: The result of `cmath.log()` for the complex number `z` when `z` is slightly above the branch cut. # Input Format: - `z` (complex): A complex number. # Output Format: - A dictionary containing the results of the analysis. # Example: ```python import cmath def analyze_complex_number(z): result = {} # Converting to polar coordinates r, phi = cmath.polar(z) result[\\"modulus\\"] = r result[\\"phase\\"] = phi # Converting back to rectangular coordinates rectangular_coordinates = cmath.rect(r, phi) result[\\"rectangular_conversion\\"] = rectangular_coordinates # Demonstrating branch cut behavior z_below = complex(z.real, -0.0 if z.imag == 0 else z.imag) z_above = complex(z.real, 0.0 if z.imag == 0 else z.imag) result[\\"sqrt_below_branch\\"] = cmath.sqrt(z_below) result[\\"sqrt_above_branch\\"] = cmath.sqrt(z_above) result[\\"log_below_branch\\"] = cmath.log(z_below) result[\\"log_above_branch\\"] = cmath.log(z_above) return result # Example usage z = complex(-2, 0) output = analyze_complex_number(z) print(output) ``` # Constraints: - Ensure that all parts of the complex numbers provided as input are finite. - Handle cases where the imaginary part is exactly zero appropriately to demonstrate branch cut behavior. # Performance Requirements: - The solution should execute efficiently within the typical constraints of input size and complexity for numerical computing tasks in Python.","solution":"import cmath def analyze_complex_number(z): result = {} # Converting to polar coordinates r, phi = cmath.polar(z) result[\\"modulus\\"] = r result[\\"phase\\"] = phi # Converting back to rectangular coordinates rectangular_coordinates = cmath.rect(r, phi) result[\\"rectangular_conversion\\"] = rectangular_coordinates # Demonstrating branch cut behavior z_below = complex(z.real, -0.0 if z.imag == 0 else z.imag) z_above = complex(z.real, 0.0 if z.imag == 0 else z.imag) result[\\"sqrt_below_branch\\"] = cmath.sqrt(z_below) result[\\"sqrt_above_branch\\"] = cmath.sqrt(z_above) result[\\"log_below_branch\\"] = cmath.log(z_below) result[\\"log_above_branch\\"] = cmath.log(z_above) return result"},{"question":"# Python Coding Assessment: Handling and Formatting Exceptions Objective Write a Python program that can execute a sequence of code lines provided by the user, handle any exceptions that occur, and print a formatted stack trace and traceback information using the `traceback` module. Instructions 1. Create a function `execute_and_trace(code_lines: List[str]) -> None` that takes a list of strings, where each string is a line of code to be executed. 2. Execute the provided lines of code within a try-except block. 3. If an exception occurs: - Use the `traceback` module to capture and print the complete stack trace and traceback to the standard output. - Format the exception and stack trace using both `print` and `format` methods, and ensure that your output includes: * The complete traceback using `traceback.print_tb()`. * The detailed exception information using `traceback.print_exception()`. * The formatted exception string using `traceback.format_exception()`. * Any local variables at the point of exception should also be included using `StackSummary.extract()` with `capture_locals=True`. 4. For each code line, output any results or errors accurately and clearly. Constraints - You may assume that code lines do not require any specific imports, except for `traceback`. - Handle multiple exceptions if they occur in sequence. - Do not suppress any context or cause information in exceptions. Example ```python from typing import List import traceback import sys def execute_and_trace(code_lines: List[str]) -> None: for line in code_lines: try: exec(line) except Exception as exc: tb = sys.exc_info()[2] print(\\"nException occurred:\\") print(\\"-\\"*60) traceback.print_tb(tb) traceback.print_exception(exc, limit=None, file=sys.stdout) formatted_exception = traceback.format_exception(exc) for line in formatted_exception: print(line, end=\'\') print(\\"-\\"*60) stack_summary = traceback.StackSummary.extract(traceback.walk_tb(tb), capture_locals=True) for frame in stack_summary: print(frame) # Example usage code_lines = [ \'a = 1 / 0\', # This will raise a ZeroDivisionError \'b = [1, 2, 3]\', \'print(b[3])\' # This will raise an IndexError ] execute_and_trace(code_lines) ``` In this example, the function `execute_and_trace` will: 1. Execute each line of code. 2. Capture any exceptions along with detailed stack trace and local variables. 3. Print the trace and exception details in a clear, formatted manner.","solution":"from typing import List import traceback import sys def execute_and_trace(code_lines: List[str]) -> None: Execute a list of code lines and if an exception occurs, print the formatted exception and stack trace information. for line in code_lines: try: exec(line) except Exception as exc: tb = sys.exc_info()[2] print(\\"nException occurred:\\") print(\\"-\\"*60) traceback.print_tb(tb) traceback.print_exception(type(exc), exc, tb, file=sys.stdout) formatted_exception = traceback.format_exception(type(exc), exc, tb) for line in formatted_exception: print(line, end=\'\') print(\\"-\\"*60) stack_summary = traceback.StackSummary.extract(traceback.walk_tb(tb), capture_locals=True) for frame in stack_summary: print(frame)"},{"question":"**Objective:** Create a Python script that interacts with the runtime environment using the `sys` module to provide custom runtime debugging and logging. **Task:** You are required to write a function `custom_debugger(logfile)` that sets up a custom debugging and logging environment. This function should: 1. Set up a global tracing function using `sys.settrace`, which logs function calls and returns to a specified log file. 2. Ensure the log file is updated dynamically as the script executes, logging the function name and event type (\\"call\\" or \\"return\\"), along with the arguments and return values. **Function Signature:** ```python def custom_debugger(logfile: str): pass ``` **Input:** - `logfile` (str): The path to the log file where debugging information will be written. **Output:** - None **Details:** - Your tracing function should log the following details for each traced event: - Event type (\\"call\\" or \\"return\\") - Function name - Arguments (for \\"call\\") - Return value (for \\"return\\") - The log should be appended to the specified log file. **Constraints:** - Ensure log entries are written in the format: `\\"{event_type} - {function_name} - {args}\\"` for function calls, where `{args}` is a dictionary of argument names and their values. `\\"{event_type} - {function_name} - returned: {return_value}\\"` for function returns, where `{return_value}` is the value returned by the function. - If an exception is raised within a function, it should be logged as well in the format: `\\"exception - {function_name} - {exception_type}: {exception_value}\\"` **Example Usage:** ```python def example_function(a, b): return a + b def main(): custom_debugger(\\"log.txt\\") example_function(2, 3) if __name__ == \\"__main__\\": main() ``` **Example Log Output (log.txt):** ``` call - example_function - {\'a\': 2, \'b\': 3} return - example_function - returned: 5 ``` **Hints:** - Use the `sys.settrace(tracefunc)` method to set a global trace function. - The trace function will be called with events such as \'call\', \'line\', \'return\', \'exception\', and \'opcode\'. - Use `sys._getframe()` to gather information about the current frame (function name, arguments, etc.).","solution":"import sys import traceback def tracefunc(frame, event, arg): if event not in (\'call\', \'return\', \'exception\'): return tracefunc func_name = frame.f_code.co_name if event == \'call\': args = frame.f_locals log_message = f\\"{event} - {func_name} - {args}n\\" elif event == \'return\': log_message = f\\"{event} - {func_name} - returned: {arg}n\\" elif event == \'exception\': exc_type, exc_value, _ = arg log_message = f\\"{event} - {func_name} - {exc_type.__name__}: {exc_value}n\\" with open(logfile_global, \'a\') as f: f.write(log_message) return tracefunc def custom_debugger(logfile: str): global logfile_global logfile_global = logfile sys.settrace(tracefunc)"},{"question":"# Advanced Python Coding Assessment Objective Create a Python function that logs various messages with different priorities and facilities using the `syslog` module. Function Signature ```python def advanced_syslog_logger(messages: list, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER) -> None: pass ``` Input The function `advanced_syslog_logger` should take in the following parameters: 1. `messages`: A list of tuples where each tuple contains: - A string with the message to be logged. - An integer representing the priority level (e.g., `syslog.LOG_INFO`, `syslog.LOG_ERR`). 2. `ident` (optional): A string that is prepended to every message. Defaults to `None`. 3. `logoption` (optional): An integer representing the logging options. Defaults to `0`. 4. `facility` (optional): An integer representing the facility to be used. Defaults to `syslog.LOG_USER`. Output The function should return `None`, but it should log the messages to the system logger with the provided priority and facility settings. Constraints 1. The function should call `syslog.openlog()` with the provided `ident`, `logoption`, and `facility`. 2. Each message in the `messages` list should be logged with the appropriate priority. 3. After logging all messages, the function should call `syslog.closelog()`. 4. Ensure that `syslog.openlog()` and `syslog.closelog()` are called only once each. 5. Properly handle different priority levels, ensuring messages are logged accordingly. Example Usage ```python import syslog messages = [ (\\"System initialization complete\\", syslog.LOG_INFO), (\\"User login failed\\", syslog.LOG_ERR), (\\"Connection established\\", syslog.LOG_NOTICE) ] advanced_syslog_logger(messages, ident=\\"MyApp\\", logoption=syslog.LOG_PID, facility=syslog.LOG_AUTH) ``` In this example, the function `advanced_syslog_logger` should: 1. Open the syslog with the ident as \\"MyApp\\", logoption including `syslog.LOG_PID`, and facility as `syslog.LOG_AUTH`. 2. Log each message in the `messages` list with its corresponding priority. 3. Close the syslog after all messages have been logged. All necessary information for implementing the function is provided within this question.","solution":"import syslog def advanced_syslog_logger(messages: list, ident: str = None, logoption: int = 0, facility: int = syslog.LOG_USER) -> None: Logs various messages with different priorities and facilities using the syslog module. :param messages: List of tuples containing message and priority level :param ident: String to be prepended to every message, optional :param logoption: Logging options, optional :param facility: Facility to be used, optional :return: None syslog.openlog(ident=ident, logoption=logoption, facility=facility) for message, priority in messages: syslog.syslog(priority, message) syslog.closelog()"},{"question":"Objective: You are required to create a complex set of visualizations using the seaborn `objects` module and customize them using matplotlib functionalities. Problem Statement: 1. Load the \'diamonds\' dataset from seaborn. 2. Create a scatter plot showing the relationship between \'carat\' and \'price\'. 3. Customize the plot by: - Adding a rectangular annotation in the top-left part of the plot with the text \\"Price Vs Carat\\". - Using different colors for different values of \'cut\' (use a palette of your choice). 4. Create a second visualization: - Plot the distribution of \'price\' using histograms, with each level of \'cut\' in a separate facet. - Use a logarithmic scale for the x-axis. - Share the y-axis between the facets. Requirements: - Your code must handle the dataset loading and any data processing if required. - Use seaborn objects (`seaborn.objects`) and matplotlib as specified. - The rectangular annotation must be visible and appropriately placed. - Ensure the histograms are well-differentiated for each facet and plotted with a proper log scale on the x-axis. Input: - None, the \'diamonds\' dataset is preloaded using seaborn. Output: - Display the customized scatter plot. - Display the faceted histogram plot. Constraints: - You must use the seaborn `objects` module for creating the plots. - Advanced customizations must be done using matplotlib functionalities only. Example Solution: ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt from seaborn import load_dataset # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create scatter plot p1 = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots(), color=\\"cut\\") # Customizing the plot f1, ax1 = plt.subplots() p1.on(ax1).plot() rect = mpl.patches.Rectangle( xy=(0, 1), width=.4, height=.1, color=\\"C1\\", alpha=.2, transform=ax1.transAxes, clip_on=False, ) ax1.add_artist(rect) ax1.text( x=rect.get_width() / 2, y=1 + rect.get_height() / 2, s=\\"Price Vs Carat\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax1.transAxes, ) plt.show() # Create faceted histogram f2 = plt.figure() so.Plot(diamonds, \\"price\\").add(so.Bars(), so.Hist()).facet(row=\\"cut\\").scale(x=\\"log\\").share(y=False).on(f2) plt.show() ```","solution":"import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt from seaborn import load_dataset def create_visualizations(): # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create scatter plot p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\", color=\\"cut\\").add(so.Dots()) # Customizing the plot f1, ax1 = plt.subplots() p1.on(ax1).plot() rect = mpl.patches.Rectangle( xy=(0.05, 0.9), width=0.25, height=0.08, color=\\"grey\\", alpha=0.3, transform=ax1.transAxes, clip_on=False, ) ax1.add_artist(rect) ax1.text( x=rect.get_x() + rect.get_width() / 2, y=rect.get_y() + rect.get_height() / 2, s=\\"Price Vs Carat\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax1.transAxes, ) plt.show() # Create faceted histogram f2 = plt.figure() so.Plot(diamonds, x=\\"price\\").add(so.Bars(), so.Hist()).facet(row=\\"cut\\").scale(x=\\"log\\").share(y=True).on(f2) plt.show()"},{"question":"# Advanced Coding Assessment Question Background: In CPython, there are different protocols to handle calling objects such as functions and methods. These include the traditional `tp_call` and the newer `vectorcall` protocol introduced in PEP 590. While these concepts are deeply rooted in CPython\'s C-extension mechanisms, understanding callable objects and optimizing such mechanisms in pure Python is crucial for advanced Python programming. Task: 1. **Function Implementation**: Write a Python class that mimics the behavior of callable objects in CPython. You will define two classes – one following the traditional callable object mechanism and another optimized version mimicking the vectorcall-like efficiency within Python\'s capabilities. 2. **Comparison Function**: Implement a function to compare the performance of calling instances of these classes. Specifications: 1. **CallableObject**: - Implement a basic Python class named `CallableObject` that supports callable instances by defining the `__call__` method. - The `__call__` method should accept any number of positional and keyword arguments and return a string summarizing the call. 2. **VectorCallableObject**: - Implement a more advanced Python class named `VectorCallableObject` mimicking vectorcall efficiency. - Instead of using `__call__`, use `__vectorcall__` (a conceptual method as Python itself does not support this directly). 3. **Performance Comparison**: - Implement a function `compare_performance(callable1, callable2, num_calls)` that takes two callable instances and a number. The function should measure and compare the time taken to perform `num_calls` calls on each instance. - Return the time taken for each callable as a tuple. Constraints and Requirements: - Concentrate on implementing the classes and comparison purely in Python without any C extensions. - Use Python\'s `time` module to measure the performance. - Ensure the `compare_performance` function handles edge cases such as no calls (0), minimal calls (1), and large number of calls efficiently. - Your solution should be portable and not rely on any non-standard libraries. Example: ```python # Define classes according to the specifications class CallableObject: def __call__(self, *args, **kwargs): return f\'Called with args: {args}, kwargs: {kwargs}\' class VectorCallableObject: def __vectorcall__(self, args, kwargs): return f\'Vector called with args: {args}, kwargs: {kwargs}\' # Function to compare performance import time def compare_performance(callable1, callable2, num_calls): start = time.time() for _ in range(num_calls): callable1() time1 = time.time() - start start = time.time() for _ in range(num_calls): callable2() time2 = time.time() - start return time1, time2 # Instantiate the callables callable1 = CallableObject() callable2 = VectorCallableObject() # Compare their performance times = compare_performance(callable1, callable2, 10000) print(f\\"CallableObject time: {times[0]}\\") print(f\\"VectorCallableObject time: {times[1]}\\") ``` Note: Since `__vectorcall__` is conceptual in this question, directly implementing `__call__` method efficiency improvements is encouraged. Focus on the logic and optimization techniques in your Python code.","solution":"import time class CallableObject: def __call__(self, *args, **kwargs): return f\'Called with args: {args}, kwargs: {kwargs}\' class VectorCallableObject: def __init__(self): self._cache = {} def __call__(self, *args, **kwargs): key = (args, frozenset(kwargs.items())) if key not in self._cache: self._cache[key] = f\'Vector called with args: {args}, kwargs: {kwargs}\' return self._cache[key] def compare_performance(callable1, callable2, num_calls): start = time.time() for _ in range(num_calls): callable1() time1 = time.time() - start start = time.time() for _ in range(num_calls): callable2() time2 = time.time() - start return time1, time2"},{"question":"**Question: Parsing and Processing Shadow Password Data** Given the nature of the `spwd` module, your task is to write a Python function that processes the shadow password database entries. Specifically, you need to implement a function that does the following: 1. Fetches all shadow password entries using `spwd.getspall()`. 2. Filters the entries based on the criteria provided (you should handle cases with and without permissions). 3. Calculates and returns a dictionary where the keys are the login names, and the values are nested dictionaries containing: - `days_since_last_change`: The number of days since the password was last changed. - `days_until_expiry`: Number of days remaining until the password expires. - `warning_days_left`: Number of warning days left before the password expires. # Function Signature ```python def process_shadow_entries(current_day: int) -> dict: pass ``` # Input - `current_day` (int): Represents the current day as an integer (days since 1970-01-01). # Output - Returns a dictionary as described above. # Constraints - You should handle `PermissionError` if the script is run without sufficient privileges. - If there are no entries or permissions, return an empty dictionary. - Validate if the password change and expiry dates make logical sense given the current day. # Example Given the following shadow entry for user `john`: ```plaintext (\'john\', \'<encrypted>\', 19000, 7, 90, 7, -1, 19700, -1) ``` If the current day is 19500, the function should return: ```json { \\"john\\": { \\"days_since_last_change\\": 500, \\"days_until_expiry\\": 200, \\"warning_days_left\\": 7 } } ``` # Notes - You must use the `spwd.getspall` function to fetch entries. - Consider that not all fields in the shadow entry might be mandatory for basic calculations. - The handling of missing or special values (like `-1`) must be appropriately taken into account. ```python import spwd def process_shadow_entries(current_day: int) -> dict: entries = {} try: shadow_entries = spwd.getspall() except PermissionError: return entries for entry in shadow_entries: login_name = entry.sp_namp days_since_last_change = current_day - entry.sp_lstchg expiry_day = entry.sp_expire if expiry_day != -1: days_until_expiry = expiry_day - current_day else: days_until_expiry = None warning_days_left = entry.sp_warn entries[login_name] = { \\"days_since_last_change\\": days_since_last_change, \\"days_until_expiry\\": days_until_expiry, \\"warning_days_left\\": warning_days_left } return entries ``` Implement the function while ensuring it matches the requirements specified above.","solution":"import spwd def process_shadow_entries(current_day: int) -> dict: entries = {} try: shadow_entries = spwd.getspall() except PermissionError: return entries for entry in shadow_entries: login_name = entry.sp_namp days_since_last_change = current_day - entry.sp_lstchg expire_day = entry.sp_expire if expire_day != -1: days_until_expiry = expire_day - current_day else: days_until_expiry = None warning_days = entry.sp_warn if entry.sp_warn != -1 else 0 warning_days_left = days_until_expiry if days_until_expiry is not None and warning_days > 0 else 0 entries[login_name] = { \\"days_since_last_change\\": days_since_last_change, \\"days_until_expiry\\": days_until_expiry, \\"warning_days_left\\": warning_days_left } return entries"},{"question":"**Problem Statement: Automated Built Distribution Creator** You are tasked to create a Python script that automates the creation of built distributions for a Python package using setuptools and Distutils. The script should allow the user to specify the formats for the built distributions and handle optional configurations, such as specifying a custom compiler for cross-compiling on Windows and including a postinstallation script. **Requirements:** 1. **Function Name:** `create_built_distribution` 2. **Input Parameters:** - `setup_script`: String, path to the `setup.py` script of the package. - `distribution_formats`: List of strings, formats to generate (e.g., `[\'gztar\', \'zip\', \'rpm\']`). - `cross_compile`: Optional boolean, default is `False`. If `True`, cross-compile if on a Windows platform. - `post_install_script`: Optional string, path to a script to be run post installation. 3. **Output:** No return value. The function should execute the required commands to create the specified built distributions. **Details:** 1. **Default Built Distributions:** If no formats are specified, the script should create a `gztar` on Unix and a `zip` on Windows. 2. **Cross-Compiling:** If `cross_compile` is `True` and the host platform is Windows, use a 32-bit Python installation to create a 64-bit distribution (or vice versa) by specifying `--plat-name=win-amd64` or `--plat-name=win32`. 3. **Postinstallation Script:** If `post_install_script` is provided, include it in the installation by using the `--install-script` option. 4. **Error Handling:** The script should handle invalid paths, unsupported formats, and other potential issues gracefully, providing informative error messages. **Constraints:** - Assume the environment already has Python, setuptools, and Distutils installed and configured correctly. - For simplicity, assume the `setup.py` follows standard practices and doesn\'t require additional customization. **Example Usage:** ```python def create_built_distribution(setup_script, distribution_formats=None, cross_compile=False, post_install_script=None): import os import platform import subprocess if distribution_formats is None: distribution_formats = [\'gztar\'] if platform.system() != \'Windows\' else [\'zip\'] command = f\\"python {setup_script} bdist --formats={\',\'.join(distribution_formats)}\\" if cross_compile and platform.system() == \'Windows\': target_platform = \'win-amd64\' if platform.architecture()[0] == \'32bit\' else \'win32\' command += f\\" build --plat-name={target_platform}\\" if post_install_script: command += f\\" --install-script={post_install_script}\\" try: subprocess.run(command, check=True, shell=True) print(\\"Built distribution(s) created successfully.\\") except subprocess.CalledProcessError as e: print(f\\"An error occurred: {e}\\") # Example: create_built_distribution(\'path/to/setup.py\', [\'zip\', \'rpm\'], cross_compile=True, post_install_script=\'path/to/postinstall.py\') ``` **Assessment Criteria:** - Correct implementation of the function. - Proper usage of `bdist` command and options. - Handling different formats and platforms. - Error handling and informative messaging.","solution":"def create_built_distribution(setup_script, distribution_formats=None, cross_compile=False, post_install_script=None): import os import platform import subprocess if distribution_formats is None: distribution_formats = [\'gztar\'] if platform.system() != \'Windows\' else [\'zip\'] command = f\\"python {setup_script} bdist --formats={\',\'.join(distribution_formats)}\\" if cross_compile and platform.system() == \'Windows\': target_platform = \'win-amd64\' if platform.architecture()[0] == \'32bit\' else \'win32\' command += f\\" build --plat-name={target_platform}\\" if post_install_script: command += f\\" --install-script={post_install_script}\\" try: subprocess.run(command, check=True, shell=True) print(\\"Built distribution(s) created successfully.\\") except subprocess.CalledProcessError as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Design a program utilizing the `hashlib` module in Python to implement a secure hashing utility that supports different hash algorithms, including personalized and salted hashing, and demonstrates the usage of key derivation functions for password hashing. Question Create a Python utility class `SecureHashUtility` with the following functionalities: 1. **Initialization**: - The class should be initialized with a hash algorithm name (e.g., \'sha256\', \'blake2b\'). 2. **Hashing Methods**: - `update(data: bytes)`: Update the hash object with the provided data. - `hexdigest() -> str`: Return the hex digest of the data processed so far. - `digest() -> bytes`: Return the byte digest of the data processed so far. - `reset()`: Reset the hash object to its initial state. 3. **Personalized and Salted Hashing**: - `set_personalization(personalization: bytes)`: Set the personalization string for the BLAKE2 hash. - `set_salt(salt: bytes)`: Set the salt for the hash, if supported. 4. **Key Derivation Function**: - `derive_key(password: bytes, salt: bytes, iterations: int, dklen: int) -> bytes`: Use `pbkdf2_hmac` to derive a key from the provided password, salt, and number of iterations. The length of the derived key should be specified by `dklen`. Input and Output Formats - **Input**: The class should handle bytes-like objects for data, personalized strings, salts, passwords, etc. - **Output**: The methods should return either string or bytes, as specified in their descriptions. Constraints - The hash algorithm provided during initialization must be a valid algorithm supported by `hashlib`. - The salt for BLAKE2b is limited to 16 bytes, and BLAKE2s to 8 bytes. If a longer salt is provided, it should be truncated. Example Usage ```python from hashlib import blake2b class SecureHashUtility: def __init__(self, algorithm: str): pass def update(self, data: bytes): pass def hexdigest(self) -> str: pass def digest(self) -> bytes: pass def reset(self): pass def set_personalization(self, personalization: bytes): pass def set_salt(self, salt: bytes): pass def derive_key(self, password: bytes, salt: bytes, iterations: int, dklen: int) -> bytes: pass # Example Usage utility = SecureHashUtility(\'sha256\') utility.update(b\'Hello, world!\') print(utility.hexdigest()) utility.reset() utility.update(b\'Hello, \') utility.update(b\'world!\') print(utility.digest()) utility.set_personalization(b\'MyAppBlock\') utility.update(b\'Hello, personalized world!\') print(utility.hexdigest()) derived_key = utility.derive_key(b\'password\', b\'salt\', 100000, 32) print(derived_key) ``` Ensure your class accurately implements these functionalities and is tested with various hash algorithms and scenarios.","solution":"import hashlib class SecureHashUtility: def __init__(self, algorithm: str): self.algorithm = algorithm if algorithm in hashlib.algorithms_guaranteed: self.hash_object = hashlib.new(algorithm) else: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") def update(self, data: bytes): self.hash_object.update(data) def hexdigest(self) -> str: return self.hash_object.hexdigest() def digest(self) -> bytes: return self.hash_object.digest() def reset(self): self.__init__(self.algorithm) def set_personalization(self, personalization: bytes): if \'blake2b\' in self.algorithm or \'blake2s\' in self.algorithm: self.hash_object = hashlib.new(self.algorithm, person=personalization) else: raise ValueError(\\"Personalization is only supported for BLAKE2 algorithms.\\") def set_salt(self, salt: bytes): if \'blake2b\' in self.algorithm: salt = salt[:16] # BLAKE2b salt limited to 16 bytes self.hash_object = hashlib.new(self.algorithm, salt=salt) elif \'blake2s\' in self.algorithm: salt = salt[:8] # BLAKE2s salt limited to 8 bytes self.hash_object = hashlib.new(self.algorithm, salt=salt) else: raise ValueError(\\"Salt is only supported for BLAKE2 algorithms.\\") def derive_key(self, password: bytes, salt: bytes, iterations: int, dklen: int) -> bytes: return hashlib.pbkdf2_hmac(self.algorithm, password, salt, iterations, dklen)"},{"question":"You are tasked with implementing a set of utility functions that will read and write both text and binary data using various stream classes provided by the `io` module. Your functions should cover text encoding/decoding, efficient buffering, and ensuring compatibility with both text and binary file formats. # Requirements: 1. **Function: `write_text_file`** - **Input**: `filename` (str), `text` (str), `encoding` (str, optional, default: \'utf-8\') - **Output**: None - **Description**: This function should create a text file with the given filename and write the provided text into it using the specified encoding. If no encoding is provided, use \'utf-8\' by default. 2. **Function: `read_text_file`** - **Input**: `filename` (str), `encoding` (str, optional, default: \'utf-8\') - **Output**: `text` (str) - **Description**: This function should read a text file with the given filename, decode it using the specified encoding, and return the text. If no encoding is provided, use \'utf-8\' by default. 3. **Function: `write_binary_file`** - **Input**: `filename` (str), `data` (bytes) - **Output**: None - **Description**: This function should create a binary file with the given filename and write the provided raw binary data into it. 4. **Function: `read_binary_file`** - **Input**: `filename` (str) - **Output**: `data` (bytes) - **Description**: This function should read a binary file with the given filename and return the raw binary data. # Constraints: - Ensure that each function handles exceptions gracefully and prints a human-readable error message if an operation fails. - Utilize appropriate classes and methods from the `io` module to implement the functions. - The functions should work efficiently for both small and large files. - You are not allowed to use any external libraries other than the `io` module. # Example Usage: ```python # Writing and reading a text file write_text_file(\'example.txt\', \'Hello, World!\', encoding=\'utf-8\') text = read_text_file(\'example.txt\', encoding=\'utf-8\') print(text) # Output: Hello, World! # Writing and reading a binary file write_binary_file(\'example.bin\', b\'x00x01x02x03\') data = read_binary_file(\'example.bin\') print(data) # Output: b\'x00x01x02x03\' ``` Implement the utility functions as specified by the problem statement.","solution":"import io def write_text_file(filename, text, encoding=\'utf-8\'): try: with io.open(filename, \'w\', encoding=encoding) as file: file.write(text) except Exception as e: print(f\\"Error writing text file: {e}\\") def read_text_file(filename, encoding=\'utf-8\'): try: with io.open(filename, \'r\', encoding=encoding) as file: return file.read() except Exception as e: print(f\\"Error reading text file: {e}\\") return None def write_binary_file(filename, data): try: with io.open(filename, \'wb\') as file: file.write(data) except Exception as e: print(f\\"Error writing binary file: {e}\\") def read_binary_file(filename): try: with io.open(filename, \'rb\') as file: return file.read() except Exception as e: print(f\\"Error reading binary file: {e}\\") return None"},{"question":"# Advanced Python Environment Configuration Background: The `site` module in Python is responsible for adding site-specific paths to the module search path (`sys.path`) and allows for site-specific and user-specific customizations. It manages paths based on the `sys.prefix` and `sys.exec_prefix`, interpreting directories and special configuration files like `pyvenv.cfg` and `.pth` files. Task: You are required to write a Python function `configure_python_environment(base_paths: List[str], custom_pth_files: Dict[str, List[str]]) -> List[str]` that dynamically configures the Python environment. 1. **Parameters:** - `base_paths` (List[str]): A list of base paths that should be checked for the existence of `site-packages` directories. Consider both empty string and `\\"lib/site-packages\\"` as potential subdirectories. - `custom_pth_files` (Dict[str, List[str]]): A dictionary where keys represent the names of `.pth` files and values are lists of paths or single-line import statements to be processed. 2. **Return:** - A list of the final `sys.path` entries after the configuration. Requirements: 1. For each base path in `base_paths`, check for the existence of directories and combine with both empty string and `\\"lib/site-packages\\"` subdirectories. Add valid directories to `sys.path`. 2. Process each `.pth` file in `custom_pth_files`. Each entry in a `.pth` file can either be an additional path or an import line starting with `import `. Valid paths should be added to `sys.path`. 3. Ensure that each path added to `sys.path` is not duplicated and exists. 4. Return the modified `sys.path`. Constraints: - Assume paths are valid for Unix-like systems (e.g., using forward slashes). - Do not modify the actual `sys.path` during execution as it might affect other components. - You should use Python\'s built-in modules like `os` to check the existence of directories. Example: ```python # Example usage base_paths = [\\"/usr/local\\", \\"/opt/python\\"] custom_pth_files = { \\"custom1.pth\\": [\\"lib/custom\\", \\"import sys; sys.path.append(\'/extra/package1\')\\"], \\"custom2.pth\\": [\\"lib/custom2\\", \\"/opt/extra\\"], } print(configure_python_environment(base_paths, custom_pth_files)) ``` Expected output (assuming all directories exist): ``` [ \\"/usr/local/lib/site-packages\\", \\"/opt/python/lib/site-packages\\", \\"/usr/local/lib/custom\\", \\"/usr/local/lib/custom2\\", \\"/opt/python/lib/custom\\", \\"/opt/python/lib/custom2\\", \\"/opt/extra\\" ] ``` **Note**: The order of paths in the returned list should follow the order in which they were added based on the rules defined.","solution":"import os from typing import List, Dict, Union def configure_python_environment(base_paths: List[str], custom_pth_files: Dict[str, List[Union[str, str]]]) -> List[str]: sys_path = [] # Function to add path if it exists and is not already in sys_path def add_path(path: str): if path and os.path.exists(path) and path not in sys_path: sys_path.append(path) # Process base paths for base in base_paths: add_path(base) add_path(os.path.join(base, \\"lib/site-packages\\")) # Process custom pth files for _, paths in custom_pth_files.items(): for entry in paths: if entry.startswith(\\"import \\"): # For import commands, execute them but do not modify sys_path directly exec(entry) else: add_path(entry) return sys_path"},{"question":"Utilizing PyTorch JIT for Performance Optimization In this coding challenge, you will demonstrate your understanding of the PyTorch JIT compilation framework by optimizing a simple neural network using JIT features. You need to: 1. Define a simple neural network without JIT. 2. Convert your network using JIT features and save it as a TorchScript model. 3. Compare the inference performance of both the original and optimized models. # Step-by-Step Instructions: 1. **Define a Simple Neural Network:** - Create a simple feedforward neural network using PyTorch. - The network should have at least one hidden layer and an output layer. 2. **Optimize the Network using JIT:** - Use the `torch.jit.script` annotation to convert your network definition to a TorchScript version. - Save the optimized model to a file. 3. **Measure and Compare Performance:** - Run a sample inference on both the original and optimized models using random input data. - Measure and print the time taken for the inference of each model. - Discuss any performance improvements observed. Detailed Requirements: 1. **Input:** - A random input tensor (with shape matching your model\'s expected input) for inference performance measurement. 2. **Output:** - Print the inference time for both the original and optimized models. - A statement discussing the performance difference observed between the two models. 3. **Constraints:** - Ensure that your code handles potential errors when converting the model to TorchScript. - Use appropriate random seed for reproducibility of the random input data generation. Example Code: ```python import torch import torch.nn as nn import torch.utils.jit import time # 1. Define a Simple Neural Network class SimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Initialize the network input_size = 100 hidden_size = 50 output_size = 10 model = SimpleNet(input_size, hidden_size, output_size) # 2. Optimize the Network using JIT jitted_model = torch.jit.script(model) # Save the optimized model torch.jit.save(jitted_model, \'optimized_model.pt\') # 3. Measure and Compare Performance # Generate random input tensor torch.manual_seed(0) input_tensor = torch.randn(1, input_size) # Measure inference time for original model start_time = time.time() original_output = model(input_tensor) original_inference_time = time.time() - start_time # Measure inference time for optimized model start_time = time.time() optimized_output = jitted_model(input_tensor) optimized_inference_time = time.time() - start_time print(f\\"Original model inference time: {original_inference_time:.6f} seconds\\") print(f\\"Optimized model inference time: {optimized_inference_time:.6f} seconds\\") # Performance Discussion if optimized_inference_time < original_inference_time: print(\\"The optimized model is faster.\\") else: print(\\"The optimization did not result in a faster model.\\") ``` In your solution: - Ensure you properly handle the conversion and saving of the TorchScript model. - Measure the performance accurately and discuss any observed improvements or lack thereof.","solution":"import torch import torch.nn as nn import time # 1. Define a Simple Neural Network class SimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out # Initialize the network input_size = 100 hidden_size = 50 output_size = 10 model = SimpleNet(input_size, hidden_size, output_size) # 2. Optimize the Network using JIT jitted_model = torch.jit.script(model) # Save the optimized model torch.jit.save(jitted_model, \'optimized_model.pt\') # 3. Measure and Compare Performance # Generate random input tensor torch.manual_seed(0) input_tensor = torch.randn(1, input_size) # Measure inference time for original model start_time = time.time() original_output = model(input_tensor) original_inference_time = time.time() - start_time # Measure inference time for optimized model start_time = time.time() optimized_output = jitted_model(input_tensor) optimized_inference_time = time.time() - start_time print(f\\"Original model inference time: {original_inference_time:.6f} seconds\\") print(f\\"Optimized model inference time: {optimized_inference_time:.6f} seconds\\") # Performance Discussion if optimized_inference_time < original_inference_time: print(\\"The optimized model is faster.\\") else: print(\\"The optimization did not result in a faster model.\\")"},{"question":"# Question: Modernizing Legacy Import Code The code provided below uses the deprecated `imp` module to import modules. Your task is to refactor this code to use the modern `importlib` module instead. ```python import imp import sys def legacy_import_module(name): try: # Fast path: see if the module has already been imported. return sys.modules[name] except KeyError: pass # Deprecated method to find the module fp, pathname, description = imp.find_module(name) try: # Deprecated method to load the module return imp.load_module(name, fp, pathname, description) finally: if fp: fp.close() # Example usage module_name = \'json\' module = legacy_import_module(module_name) print(module.__name__) ``` Requirements 1. Use `importlib.util.find_spec()` to find the module. 2. Use `importlib.util.module_from_spec()` and `spec.loader.exec_module()` to load the module. 3. Handle the case where the module is not found and raise an appropriate exception. Input and Output - **Input:** `module_name` (string) - The name of the module to be imported. - **Output:** The imported module object. Constraints - You must not use the deprecated `imp` module functions. - You should maintain the same functionality and structure as much as possible. **Note: The provided code should not import or reload modules such as `sys`, `builtins`, etc., which are already present.** Example ```python # Refactored code example usage module_name = \'json\' module = modern_import_module(module_name) print(module.__name__) # Expected output: json ``` Provide your refactored `modern_import_module` function below: ```python import importlib.util import sys def modern_import_module(name): # Your code here ```","solution":"import importlib.util import sys def modern_import_module(name): try: # Fast path: see if the module has already been imported. return sys.modules[name] except KeyError: pass # Use importlib to find the module specification spec = importlib.util.find_spec(name) if spec is None: raise ImportError(f\\"Module \'{name}\' not found\\") # Create the module from the spec module = importlib.util.module_from_spec(spec) # Load the module spec.loader.exec_module(module) # Add the module to sys.modules sys.modules[name] = module return module # Example usage if __name__ == \\"__main__\\": module_name = \'json\' module = modern_import_module(module_name) print(module.__name__)"},{"question":"**Problem Statement:** You are tasked to create a utility function that processes multiple MIME types found in a given set of mailcap files and determines the commands needed to handle these MIME types. The utility should utilize the functions available in the deprecated \\"mailcap\\" module. **Function Signature:** ```python def process_mime_types(mime_types: list, filenames: list) -> dict: pass ``` **Input:** - `mime_types` (List[str]): A list of MIME types to be handled. - `filenames` (List[str]): A list of filenames corresponding to each MIME type. The lists are guaranteed to be of the same length. **Output:** - Returns a dictionary where each key is a MIME type (from the `mime_types` list) and the value is either: - A string with the command to execute, or - The string `\\"No match found\\"` if no appropriate entry was found. **Constraints:** - The \\"mailcap\\" module\'s `getcaps()` must be used to get the mailcap entries. - The `findmatch()` function must be used to determine the command for each MIME type. - You need to ensure that the input lists `mime_types` and `filenames` are non-empty and of equal length. - If any special characters are found in the filenames, the command should be set to `\\"No match found\\"`. **Example Usage:** ```python import mailcap def process_mime_types(mime_types, filenames): caps = mailcap.getcaps() result = {} for i in range(len(mime_types)): mime_type = mime_types[i] filename = filenames[i] cmd, entry = mailcap.findmatch(caps, mime_type, filename=filename) if cmd: result[mime_type] = cmd else: result[mime_type] = \\"No match found\\" return result # Example mime_types = [\'video/mpeg\', \'image/jpeg\'] filenames = [\'video1234\', \'image5678\'] print(process_mime_types(mime_types, filenames)) # Example output could be: # {\'video/mpeg\': \'xmpeg video1234\', \'image/jpeg\': \'No match found\'} ``` # Notes: - Edge cases such as filenames with special characters should be handled as mentioned. - Ensure the provided MIME types match valid mailcap entries before determining the command.","solution":"import mailcap import re def process_mime_types(mime_types, filenames): def has_special_chars(s): return bool(re.search(r\'[^w.-]\', s)) caps = mailcap.getcaps() result = {} for mime_type, filename in zip(mime_types, filenames): if has_special_chars(filename): result[mime_type] = \\"No match found\\" continue cmd, entry = mailcap.findmatch(caps, mime_type, filename=filename) if cmd: result[mime_type] = cmd else: result[mime_type] = \\"No match found\\" return result"},{"question":"You are tasked with managing an amusement park ride that can accommodate a limited number of people at a time. Each ride can accommodate a maximum of `N` riders, and only one group can ride at a time. Due to safety protocols, once a ride starts, no new riders can join until the current ride cycle is complete, even if there are seats available. You need to simulate this ride using Python\'s asyncio primitives. Implement the `AmusementRide` class with the following methods: 1. **__init__(self, capacity: int)**: Initializes the ride with a specified capacity (`N`). 2. **async def join_ride(self, rider_id: int)**: Allows a rider to join the ride. If the ride is full or already started, the rider should wait. Print a message when the rider joins, e.g., `Rider {rider_id} joined the ride.`. 3. **async def start_ride(self)**: Simulates starting the ride. Once started, no more riders can join until it finishes. Print a message `Ride started with {number_of_riders} riders.` where `number_of_riders` is the count of current riders. Wait for 2 seconds (using `await asyncio.sleep(2)`) to simulate the ride duration. 4. **async def finish_ride(self)**: Simulates finishing the ride, freeing up space. Print a message `Ride finished and all riders left.` and allow new riders to join. Use `asyncio.Lock` to ensure that only one group of riders rides at a time and `asyncio.Semaphore` to manage the ride\'s capacity. # Constraints - Running this simulation must be done using asyncio and properly demonstrate usage of async synchronization primitives. - The methods `join_ride`, `start_ride`, and `finish_ride` should maintain proper synchronization. - You should print appropriate messages indicating the ride\'s status and actions taken by riders. # Example Usage ```python import asyncio async def main(): ride = AmusementRide(5) await asyncio.gather( ride.join_ride(1), ride.join_ride(2), ride.join_ride(3), ride.join_ride(4), ride.join_ride(5), asyncio.sleep(1), ride.join_ride(6), # This rider needs to wait for the next ride cycle ride.start_ride(), # Start the ride ride.finish_ride(), # Finish the ride asyncio.sleep(1), ride.join_ride(6), # Now this rider can join ride.join_ride(7), ride.start_ride(), # Start the next ride ride.finish_ride() # Finish the next ride ) asyncio.run(main()) ``` # Expected Output: ``` Rider 1 joined the ride. Rider 2 joined the ride. Rider 3 joined the ride. Rider 4 joined the ride. Rider 5 joined the ride. Ride started with 5 riders. Ride finished and all riders left. Rider 6 joined the ride. Rider 7 joined the ride. Ride started with 2 riders. Ride finished and all riders left. ``` Implement the `AmusementRide` class with asyncio primitives ensuring proper synchronization and constraint adherence.","solution":"import asyncio class AmusementRide: def __init__(self, capacity: int): self.capacity = capacity self.current_riders = 0 self.lock = asyncio.Lock() self.semaphore = asyncio.Semaphore(capacity) async def join_ride(self, rider_id: int): async with self.semaphore: self.current_riders += 1 print(f\\"Rider {rider_id} joined the ride.\\") await asyncio.sleep(0.1) # Simulate the time taken to join the ride async def start_ride(self): async with self.lock: print(f\\"Ride started with {self.current_riders} riders.\\") await asyncio.sleep(2) # Simulate ride duration async def finish_ride(self): async with self.lock: print(\\"Ride finished and all riders left.\\") # Reset the semaphore and number of current riders self.semaphore = asyncio.Semaphore(self.capacity) self.current_riders = 0"},{"question":"# Advanced Python Logging Implementation Objective: Design and implement a Python logging system that utilizes multiple handlers to log messages to different destinations based on the severity level of the log messages. This task will require you to demonstrate your understanding of custom handler creation, stream handling, file handling, and email (SMTP) handling. Task: You need to write a Python function `setup_logging()` which sets up a logging system with the following specifications: 1. **StreamHandler**: - Logs messages of all severity levels (DEBUG and above) to the console (`sys.stdout`). 2. **FileHandler**: - Logs messages of level WARNING and above to a file named `warnings.log`. - The log file should be rotated when it reaches 5 MB, keeping up to 3 backup files. 3. **SMTPHandler**: - Sends an email for each log message of level ERROR and above. - Use the following SMTP configuration: - SMTP server: `smtp.example.com` - From address: `alerts@example.com` - To address: `admin@example.com` - Email subject: \\"Error Log Alert\\" - Assume no authentication or secure connection is needed. 4. **Custom Handler**: - Implement a custom handler `KeywordFilterHandler` that filters log messages to display only those containing a specific keyword. - This handler should be added to the logger for messages of level INFO and above, and it should filter messages containing the keyword \\"special\\". Instructions: 1. Create and configure each handler as described above. 2. Define the custom handler `KeywordFilterHandler`. 3. Ensure proper formatting for log messages in all handlers. 4. Provide a comprehensive function `setup_logging()` that sets up all the handlers correctly. Input format: - No input is required for the function. - Loggers will be tested manually by logging various messages at different severity levels. Output format: - None. The function sets up the logging system as described. Example: ```python import logging import logging.handlers class KeywordFilterHandler(logging.Handler): def __init__(self, keyword): super().__init__() self.keyword = keyword def emit(self, record): if self.keyword in record.msg: print(self.format(record)) def setup_logging(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) # StreamHandler for console output stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.DEBUG) stream_handler.setFormatter(logging.Formatter(\'%(levelname)s: %(message)s\')) logger.addHandler(stream_handler) # FileHandler with rotation file_handler = logging.handlers.RotatingFileHandler(\'warnings.log\', maxBytes=5*1024*1024, backupCount=3) file_handler.setLevel(logging.WARNING) file_handler.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(file_handler) # SMTPHandler for email alerts smtp_handler = logging.handlers.SMTPHandler( mailhost=\'smtp.example.com\', fromaddr=\'alerts@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Error Log Alert\' ) smtp_handler.setLevel(logging.ERROR) smtp_handler.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(smtp_handler) # Custom KeywordFilterHandler keyword_filter_handler = KeywordFilterHandler(keyword=\'special\') keyword_filter_handler.setLevel(logging.INFO) keyword_filter_handler.setFormatter(logging.Formatter(\'%(levelname)s: %(message)s\')) logger.addHandler(keyword_filter_handler) # Test the setup by logging messages setup_logging() logger = logging.getLogger() logger.debug(\'This is a debug message\') logger.info(\'This is a special info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') ``` In this example, the `setup_logging` function sets up the logging system as specified. The custom handler `KeywordFilterHandler` filters and displays log messages that contain the keyword \\"special\\".","solution":"import logging import logging.handlers class KeywordFilterHandler(logging.Handler): def __init__(self, keyword): super().__init__() self.keyword = keyword def emit(self, record): if self.keyword in record.getMessage(): print(self.format(record)) def setup_logging(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) # StreamHandler for console output stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.DEBUG) stream_handler.setFormatter(logging.Formatter(\'%(levelname)s: %(message)s\')) logger.addHandler(stream_handler) # FileHandler with rotation file_handler = logging.handlers.RotatingFileHandler(\'warnings.log\', maxBytes=5 * 1024 * 1024, backupCount=3) file_handler.setLevel(logging.WARNING) file_handler.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(file_handler) # SMTPHandler for email alerts smtp_handler = logging.handlers.SMTPHandler( mailhost=\'smtp.example.com\', fromaddr=\'alerts@example.com\', toaddrs=[\'admin@example.com\'], subject=\'Error Log Alert\' ) smtp_handler.setLevel(logging.ERROR) smtp_handler.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')) logger.addHandler(smtp_handler) # Custom KeywordFilterHandler keyword_filter_handler = KeywordFilterHandler(keyword=\'special\') keyword_filter_handler.setLevel(logging.INFO) keyword_filter_handler.setFormatter(logging.Formatter(\'%(levelname)s: %(message)s\')) logger.addHandler(keyword_filter_handler)"},{"question":"# Task You are required to implement a custom logging handler by inheriting from the `BufferedHandler` class (available via `logging.handlers` module). This handler, when added to a logger, should buffer incoming log records in memory. The buffer should periodically flush the records to a file when it reaches a specified capacity or whenever a log message with a level of `ERROR` or higher is seen. # Requirements: 1. **CustomBufferedHandler Class:** - Inherit from `logging.handlers.BufferingHandler`. - The constructor should take the following parameters: `filename`, `capacity`, and `flushLevel` (defaulting to `ERROR`). - It should initialize a `FileHandler` with the given `filename` where the buffered log records will be written. - Override the `flush()` method to write all buffered log records to the `FileHandler` and then clear the buffer. 2. **Logging Setup:** - Create a logger named `myLogger`. - Add an instance of `CustomBufferedHandler` to the logger with a buffer capacity of 5 records and a `flushLevel` of `ERROR`. - Demonstrate logging to this logger such that log records get flushed to the file as described. # Input and Output - **Input** - The `CustomBufferedHandler` class definition. - Logger setup code as described above. - **Output** - Log records should be flushed to a file named `mylog.log` when buffer capacity is reached or when `ERROR` level log is encountered. # Constraints: - Assume that the `logging` and `logging.handlers` modules are imported. - The file should be opened in append mode to preserve the existing log records. - The buffer should maintain the order in which log records are received. # Performance: - The solution should efficiently handle log records and ensure timely flushing as per the defined conditions. # Example Usage: ```python # Custom Buffered Handler implementation class CustomBufferedHandler(logging.handlers.BufferingHandler): def __init__(self, filename, capacity, flushLevel=logging.ERROR): super().__init__(capacity) self._handler = logging.FileHandler(filename) self.flushLevel = flushLevel def shouldFlush(self, record): return len(self.buffer) >= self.capacity or record.levelno >= self.flushLevel def flush(self): self._handler.acquire() try: for record in self.buffer: self._handler.emit(record) self.buffer.clear() finally: self._handler.release() # Logger setup logger = logging.getLogger(\'myLogger\') handler = CustomBufferedHandler(\'mylog.log\', capacity=5) logger.addHandler(handler) logger.setLevel(logging.DEBUG) # Logging examples logger.debug(\'Debug message 1\') logger.info(\'Info message 2\') logger.warning(\'Warning message 3\') logger.error(\'Error message 4\') ``` # Additional Notes: You must ensure that the `flush()` method handles the buffer clearing correctly and that the new log records get written in the correct order without duplications or data loss.","solution":"import logging from logging.handlers import BufferingHandler class CustomBufferedHandler(BufferingHandler): def __init__(self, filename, capacity, flushLevel=logging.ERROR): super().__init__(capacity) self._handler = logging.FileHandler(filename) self.flushLevel = flushLevel def shouldFlush(self, record): return len(self.buffer) >= self.capacity or record.levelno >= self.flushLevel def flush(self): self.acquire() try: for record in self.buffer: self._handler.emit(record) self.buffer.clear() finally: self.release() # Logger setup logger = logging.getLogger(\'myLogger\') handler = CustomBufferedHandler(\'mylog.log\', capacity=5, flushLevel=logging.ERROR) logger.addHandler(handler) logger.setLevel(logging.DEBUG) # Example logging to demonstrate the functionality. logger.debug(\'Debug message 1\') logger.info(\'Info message 2\') logger.warning(\'Warning message 3\') logger.error(\'Error message 4\') # This should trigger a flush due to ERROR level. logger.debug(\'Debug message 5\') logger.debug(\'Debug message 6\')"},{"question":"# PyTorch and TorchScript Compatibility Assessment **Objective:** Create a function in PyTorch that performs a series of tensor operations while ensuring compatibility with TorchScript. Your task is to write a function that computes the Frobenius norm of a matrix, normalizes it by its maximum value, and multiplies it with a specified scalar. Additionally, handle cases where some tensor initialization methods might not support the `requires_grad` parameter. **Function Signature:** ```python import torch def frobenius_tensor_ops(matrix: torch.Tensor, scalar: float) -> torch.Tensor: Computes the Frobenius norm of a matrix, normalizes it by its maximum value, and multiplies it with a specified scalar. Parameters: - matrix (torch.Tensor): A 2D tensor. - scalar (float): A scalar value to multiply with the normalized Frobenius norm tensor. Returns: - torch.Tensor: A tensor resulting from the described operations, compatible with TorchScript. pass ``` **Input:** - `matrix`: A 2D tensor of shape (m, n). - `scalar`: A floating-point scalar value. **Output:** - A tensor of the same shape as `matrix`, after the described operations. **Constraints:** - Do not use any functions or modules that are unsupported in TorchScript (as listed in the documentation). **Requirements:** 1. Ensure the function works in both regular PyTorch and TorchScript. 2. Handle the `requires_grad` parameter properly during tensor creation or manipulation. 3. Ensure proper tensor operations while avoiding unsupported methods. **Example:** ```python matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) scalar = 0.5 result = frobenius_tensor_ops(matrix, scalar) print(result) # Expected output: tensor with the computed results. ``` **Test Cases:** 1. Test with a random 2D tensor and various scalar values. 2. Test with edge cases like all zeros or identical values in the matrix. 3. Ensure compatibility by attempting to convert and run the function in TorchScript. You can use the following code to test TorchScript compatibility: ```python scripted_fn = torch.jit.script(frobenius_tensor_ops) matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) scalar = 0.5 scripted_result = scripted_fn(matrix, scalar) print(scripted_result) ``` Create your solution ensuring the function meets the above criteria and passes the tests.","solution":"import torch def frobenius_tensor_ops(matrix: torch.Tensor, scalar: float) -> torch.Tensor: Computes the Frobenius norm of a matrix, normalizes it by its maximum value, and multiplies it with a specified scalar. Parameters: - matrix (torch.Tensor): A 2D tensor. - scalar (float): A scalar value to multiply with the normalized Frobenius norm tensor. Returns: - torch.Tensor: A tensor resulting from the described operations, compatible with TorchScript. frob_norm = torch.norm(matrix, p=\'fro\') max_val = torch.max(matrix) if max_val != 0: normalized_frob = frob_norm / max_val else: normalized_frob = frob_norm result = normalized_frob * scalar return result"},{"question":"**Coding Assessment Question: Visualizing Diamonds Dataset with Seaborn** You are tasked with creating a series of visualizations using the Seaborn library to analyze the diamonds dataset. Your task involves demonstrating your understanding of seaborn\'s capabilities, including creating statistical plots, using different scales, and adding statistical information such as percentiles. **Objective:** Create a function `visualize_diamonds()` that produces two distinct plots: 1. A logarithmically-scaled dot plot showing the relationship between the diamond\'s cut and its price, with dots positioned at the quartiles and min/max values. 2. A jitter plot of the diamond price against the cut, with additional percentile ranges (25th and 75th) shown as ranges above the jittered points. **Function Signature:** ```python def visualize_diamonds(): pass ``` **Requirements:** 1. Load the diamonds dataset from seaborn. 2. Produce the first plot: - X-axis should be the cut of the diamond. - Y-axis should be the price of the diamond. - Apply a logarithmic scale to the Y-axis. - Use dots to represent the quartiles and min/max values of the price for each cut. 3. Produce the second plot: - X-axis should be the price of the diamond. - Y-axis should be the cut of the diamond. - Apply jitter to the points to increase visibility. - Show the 25th and 75th percentile ranges as black lines, slightly shifted along the Y-axis for clear distinction. **Constraints:** - Use seaborn\'s high-level interface to create these plots. - The function should not output any text or return any value. It should simply generate and show the plots. **Example Usage:** ```python visualize_diamonds() ``` This function, when called, should display the two plots as specified. **Assessment Criteria:** - Correctly loading and using the diamonds dataset. - Proper application of the logarithmic scale on the respective plot. - Accurate computation and visualization of quartiles and percentile ranges. - Clear and readable plot customization for better data representation. You can refer to the seaborn documentation provided above to help you achieve these tasks.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_diamonds(): # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # First Plot: Logarithmically-scaled dot plot plt.figure(figsize=(14, 6)) # Group data by cut and calculate the min, quartiles, and max group_data = diamonds.groupby(\'cut\')[\'price\'].describe(percentiles=[.25, .5, .75]) sns.scatterplot(data=group_data, x=\'cut\', y=\'min\', label=\'Min\', color=\'blue\', s=100, marker=\'o\') sns.scatterplot(data=group_data, x=\'cut\', y=\'25%\', label=\'25th Percentile\', color=\'orange\', s=100, marker=\'o\') sns.scatterplot(data=group_data, x=\'cut\', y=\'50%\', label=\'Median\', color=\'green\', s=100, marker=\'o\') sns.scatterplot(data=group_data, x=\'cut\', y=\'75%\', label=\'75th Percentile\', color=\'red\', s=100, marker=\'o\') sns.scatterplot(data=group_data, x=\'cut\', y=\'max\', label=\'Max\', color=\'purple\', s=100, marker=\'o\') plt.yscale(\'log\') plt.ylabel(\\"Price (log scale)\\") plt.title(\\"Diamond Prices by Cut (log scale)\\") plt.legend() plt.show() # Second plot: Jitter plot plt.figure(figsize=(14, 6)) sns.stripplot(x=\'price\', y=\'cut\', data=diamonds, jitter=True, alpha=0.5) # Add 25th and 75th percentiles def add_percentile_lines(): for category in diamonds[\'cut\'].unique(): subset = diamonds[diamonds[\'cut\'] == category][\'price\'] percentiles = np.percentile(subset, [25, 75]) plt.plot([percentiles[0], percentiles[0]], [category, category], marker=\'D\', color=\'black\') plt.plot([percentiles[1], percentiles[1]], [category, category], marker=\'D\', color=\'black\') add_percentile_lines() plt.title(\\"Diamond Prices by Cut with Percentiles\\") plt.show()"},{"question":"# Asyncio Cross-Platform Event Loop Implementation **Background:** You are tasked with designing an asyncio-based application that works consistently across multiple platforms, including Windows and macOS. Your solution should take into account the platform-specific limitations as detailed in the documentation provided. **Task:** Write an asyncio-based Python function called `cross_platform_event_loop`, which performs the following tasks: 1. **Windows:** - If running on Windows, use `ProactorEventLoop` by default. - Implement asynchronous subprocess creation and data capture, ensuring compatibility with `ProactorEventLoop`. Specifically, execute a subprocess command (e.g., `ping google.com` or any appropriate command for testing purposes) and capture its output. 2. **macOS:** - If running on macOS, use `SelectorEventLoop`. - Ensure that on macOS versions <= 10.8, `SelectSelector` is used, whereas more recent versions should use `KqueueSelector`. - Implement a basic network server and client that send and receive messages asynchronously. **Function Signature:** ```python import asyncio import platform async def cross_platform_event_loop(): # Your implementation here pass ``` **Constraints:** - Ensure the code checks the platform and sets the appropriate event loop. - Use appropriate methods based on the platform-specific capabilities and limitations as described in the provided documentation. - Ensure the function can run as a standalone asyncio event loop that performs the above tasks correctly depending on the detected platform. **Input:** - None directly. The function should internally handle subprocess execution and server-client communication based on the platform. **Output:** - Print the captured output from the subprocess on Windows. - Print communication messages exchanged by the server-client on macOS. # Example Usage: ```python asyncio.run(cross_platform_event_loop()) ``` When run on Windows, it should execute a subprocess and print its output. When run on macOS, it should set up the server-client communication and print the exchanged messages. **Note:** You may need to install additional packages or use mock commands for testing the solution on different platforms. The focus is on correctly implementing and configuring the asyncio event loops as per the given constraints.","solution":"import asyncio import platform import subprocess async def windows_subprocess_example(): process = await asyncio.create_subprocess_shell( \'ping google.com\', stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() if stdout: print(f\'[stdout]n{stdout.decode()}\') if stderr: print(f\'[stderr]n{stderr.decode()}\') async def macos_server_client_example(): async def handle_echo(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() print(\\"Closing the connection\\") writer.close() server = await asyncio.start_server(handle_echo, \'127.0.0.1\', 8888) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async def client(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) message = \'Hello World!\' print(f\'Send: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await asyncio.gather(server.serve_forever(), client()) async def cross_platform_event_loop(): current_platform = platform.system() if current_platform == \'Windows\': loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) await windows_subprocess_example() elif current_platform == \'Darwin\': mac_ver = platform.mac_ver()[0] major_version = int(mac_ver.split(\'.\')[1]) if major_version <= 8: selector = asyncio.SelectSelector() else: selector = asyncio.KqueueSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) await macos_server_client_example() # For platforms other than Windows and macOS, you may simply run the default event loop. asyncio.run(cross_platform_event_loop())"},{"question":"You are asked to implement a class in Python that mimics the behavior of a dynamically growing byte buffer for read and write operations. This buffer should support basic operations to append data, retrieve data, and clear the buffer. Your implementation should demonstrate a good understanding of memory management concepts in Python. Requirements: 1. Implement a class `DynamicByteBuffer` with the following methods: * `__init__(self, initial_size: int = 1024)`: Initializes the buffer with a given initial size. * `append(self, data: bytes) -> None`: Appends bytes to the end of the buffer, resizing if necessary. * `get_buffer(self) -> memoryview`: Returns a read-only memory view of the current buffer content. * `clear(self) -> None`: Clears the buffer, but keeps the allocated memory for future use. 2. Constraints: * Appending data should automatically expand the buffer if there is insufficient space. * When resizing, double the buffer size each time additional space is needed. 3. Performance: * The implementation should achieve amortized O(1) time complexity for the `append` operation. 4. Design considerations: * Ensure that the memory management is efficient. * Avoid unnecessary copying of data. Input and Output formats: * There are no direct input or output formats. You will implement the methods of the class as specified. Example: ```python buf = DynamicByteBuffer(initial_size=8) buf.append(b\'hello\') buf.append(b\' world\') print(buf.get_buffer()) # Output: <memory at 0x...> buf.clear() print(buf.get_buffer()) # Output: <memory at 0x...> ``` In the example above, notice how the buffer is initially allocated with 8 bytes, appends \'hello world\', and resizes itself when necessary. The memory view reflects the current buffer content. Additional Information: * You may make use of Python\'s `memoryview` and `bytearray` for efficient buffer handling. * Throw appropriate exceptions for invalid operations (e.g., appending non-bytes data).","solution":"class DynamicByteBuffer: def __init__(self, initial_size: int = 1024): self.buffer = bytearray(initial_size) self.size = initial_size self.length = 0 def _resize(self): new_size = self.size * 2 new_buffer = bytearray(new_size) new_buffer[:self.length] = self.buffer[:self.length] self.buffer = new_buffer self.size = new_size def append(self, data: bytes) -> None: if not isinstance(data, bytes): raise ValueError(\\"Data must be of type \'bytes\'\\") data_len = len(data) while self.length + data_len > self.size: self._resize() self.buffer[self.length:self.length + data_len] = data self.length += data_len def get_buffer(self) -> memoryview: return memoryview(self.buffer)[:self.length] def clear(self) -> None: self.length = 0"},{"question":"<|Analysis Begin|> The documentation provided outlines how to create a source distribution using the `sdist` command in Python, particularly focusing on specifying files to include in the distribution and various options available for the command. The key aspects include: 1. Basic usage of `sdist` to create source distributions with different formats (`gztar`, `zip`, etc.). 2. Customizing the list of files to be included in the distribution by specifying a `MANIFEST.in` file. 3. Handling of default files, additional files, and exclusion of files based on directory names and patterns. 4. The behavior of the `sdist` command with respect to existing `MANIFEST` files. 5. Options to modify the behavior like `--no-defaults`, `--no-prune`, and `--manifest-only`. Based on this, a challenging question could focus on implementing a Python function that mimics part of the `sdist` functionality, particularly generating a list of files to be included in a source distribution based on rules specified in a `MANIFEST.in` file. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Implement a function that generates a list of files to be included in a source distribution based on a `MANIFEST.in` template. Your function should simulate parts of the `sdist` command behavior as described in the provided documentation. Problem Statement You need to write a Python function that reads a `MANIFEST.in` file and generates a list of files to include in a source distribution, respecting inclusion and exclusion rules. Function Signature ```python def generate_file_list(manifest_path: str, root_directory: str) -> List[str]: ``` Input - `manifest_path` (str): The path to the `MANIFEST.in` file. - `root_directory` (str): The root directory of the project. Output - (List[str]): A list of filenames relative to the `root_directory` to be included in the source distribution. Constraints - You may assume that the `MANIFEST.in` file exists at the specified `manifest_path`. - The root directory contains a typical project structure with various files and directories. Example Suppose the project structure under `root_directory` is as follows: ``` root_directory/ ├── README.txt ├── setup.py ├── setup.cfg ├── src/ │ ├── module1.py │ ├── module2.py ├── test/ │ ├── test_module1.py │ ├── test_module2.py ├── examples/ │ ├── example1.py │ ├── example2.txt │ ├── sample1/ │ │ ├── build/ │ │ │ ├── temp.txt ``` And the `MANIFEST.in` contains: ``` include *.txt recursive-include examples *.txt *.py prune examples/sample?/build ``` Calling the function with this setup: ```python file_list = generate_file_list(\\"path/to/MANIFEST.in\\", \\"root_directory\\") ``` Should return: ```python [ \\"README.txt\\", \\"examples/example1.py\\", \\"examples/example2.txt\\" ] ``` Note that `examples/sample1/build/temp.txt` is excluded due to the prune command. Requirements 1. Your function should correctly parse the `MANIFEST.in` file and apply the commands in it. 2. Include files specified by `include` and `recursive-include` directives. 3. Exclude files specified by `prune` directives. 4. Use Python\'s standard file operations and directory traversal methods. Additional Notes - Assume that filenames and paths are case-sensitive. - Do not worry about handling symbolic links or non-regular files. - Focus on accuracy and efficiency of file inclusion and exclusion logic.","solution":"import os import fnmatch def generate_file_list(manifest_path: str, root_directory: str) -> list: Generates a list of files to be included in a source distribution based on a MANIFEST.in template. Parameters: - manifest_path (str): The path to the MANIFEST.in file. - root_directory (str): The root directory of the project. Returns: - list: List of filenames relative to the root_directory to be included in the source distribution. include_patterns = [] exclude_patterns = [] # Read MANIFEST.in file with open(manifest_path, \'r\') as manifest_file: lines = manifest_file.readlines() # Process each line in the MANIFEST.in for line in lines: line = line.strip() if line.startswith(\'include \'): pattern = line[len(\'include \'):].strip() include_patterns.append((pattern, False)) # not recursive elif line.startswith(\'recursive-include \'): parts = line[len(\'recursive-include \'):].strip().split() directory = parts[0] file_patterns = parts[1:] for pattern in file_patterns: include_patterns.append((os.path.join(directory, pattern), True)) # recursive elif line.startswith(\'prune \'): pattern = line[len(\'prune \'):].strip() exclude_patterns.append(pattern) included_files = set() # Function to add files according to include patterns def add_files(pattern, recursive): if recursive: for root, _, files in os.walk(root_directory): for file in files: full_path = os.path.join(root, file) rel_path = os.path.relpath(full_path, root_directory) if fnmatch.fnmatch(rel_path, pattern): included_files.add(rel_path) else: for root, _, files in os.walk(root_directory): for file in files: if fnmatch.fnmatch(file, pattern): full_path = os.path.join(root, file) rel_path = os.path.relpath(full_path, root_directory) included_files.add(rel_path) # Apply include patterns for pattern, recursive in include_patterns: add_files(pattern, recursive) # Apply exclude patterns for pattern in exclude_patterns: for root, dirs, files in os.walk(root_directory): full_pattern = os.path.join(root_directory, pattern) if fnmatch.fnmatch(os.path.abspath(root), full_pattern): for file in files: full_path = os.path.join(root, file) rel_path = os.path.relpath(full_path, root_directory) if rel_path in included_files: included_files.remove(rel_path) return sorted(included_files)"},{"question":"**Advanced Coding Assessment Question** # Objective: Design and implement a Python function that manages files and directories using the `pathlib`, `shutil`, and `tempfile` modules. The function will create a temporary directory, populate it with files, search for files with specific patterns, and archive the directory\'s contents. # Task: Write a Python function `manage_files_and_directories(base_dir: str, file_count: int, search_pattern: str) -> str` that performs the following tasks: 1. Creates a temporary directory within `base_dir`. 2. Creates `file_count` number of text files within the temporary directory. Each file should be named using the pattern `file_<index>.txt` (e.g., `file_0.txt`, `file_1.txt`). 3. Writes some sample content to each file (e.g., \\"This is file `<index>`.\\"). 4. Searches and lists all files in the temporary directory that match the `search_pattern`. 5. Archives the entire temporary directory into a ZIP file within the `base_dir`. 6. Returns the path of the created ZIP archive. # Input: - `base_dir`: The base directory where the temporary directory and the ZIP archive will be created. - `file_count`: The number of text files to create in the temporary directory. - `search_pattern`: A pattern to match files within the temporary directory (e.g., `file_*.txt`). # Output: - The path of the created ZIP archive as a string. # Example: ```python base_dir = \\"/path/to/base\\" file_count = 5 search_pattern = \\"file_*.txt\\" archive_path = manage_files_and_directories(base_dir, file_count, search_pattern) print(archive_path) # e.g., \\"/path/to/base/temp_archive.zip\\" ``` # Constraints: - Ensure that the temporary directory and all its contents are deleted after archiving. - Use appropriate modules (`pathlib`, `shutil`, `tempfile`) for managing filesystem paths and file operations. - Handle any potential exceptions that may arise during file operations to ensure robustness. # Additional Notes: - It\'s important to clean up temporary files and directories after their purpose is served to comply with good coding practices. - The solution should be efficient and concise, making good use of the provided Python modules.","solution":"import pathlib import shutil import tempfile def manage_files_and_directories(base_dir: str, file_count: int, search_pattern: str) -> str: base_dir_path = pathlib.Path(base_dir) if not base_dir_path.exists(): raise ValueError(\\"Base directory does not exist.\\") with tempfile.TemporaryDirectory(dir=base_dir) as temp_dir: temp_dir_path = pathlib.Path(temp_dir) # Create files for i in range(file_count): file_path = temp_dir_path / f\\"file_{i}.txt\\" with file_path.open(\'w\') as file: file.write(f\\"This is file {i}.n\\") # Search for files matching the pattern matching_files = list(temp_dir_path.glob(search_pattern)) # Create zip archive zip_path = base_dir_path / \\"temp_archive.zip\\" shutil.make_archive(str(zip_path.with_suffix(\'\')), \'zip\', temp_dir_path) return str(zip_path)"},{"question":"<|Analysis Begin|> The provided documentation details the Python C API for working with dictionary objects. The API functions allow for creating new dictionaries, checking their types, performing operations like insertion, deletion, and retrieval of items, and more advanced operations like iteration and merging. Key functions included: 1. `PyDict_New()`: Create a new dictionary. 2. `PyDict_SetItem()`: Insert an item with a given key and value. 3. `PyDict_GetItem()`: Retrieve an item by key. 4. `PyDict_DelItem()`: Delete an item by key. 5. `PyDict_Contains()`: Check if a key exists in a dictionary. 6. `PyDict_Clear()`: Clear all items in a dictionary. 7. `PyDict_Keys()`, `PyDict_Values()`, `PyDict_Items()`: Retrieve all keys, values, or items in the dictionary. 8. `PyDict_Next()`: Iterate over all key-value pairs in a dictionary. Other functions allow for copying dictionaries, creating read-only proxies, and merging dictionaries. <|Analysis End|> <|Question Begin|> # Advanced Python Dictionary Operations As part of the Python C API, you are required to implement several functions that work with dictionaries. Using the given API, implement the following functions in Python, simulating the corresponding C API operations. **Function 1: Create an empty dictionary** ```python def create_empty_dict() -> dict: Create and return an empty dictionary. Returns: dict: An empty dictionary. pass ``` **Function 2: Add a key-value pair to a dictionary** ```python def add_item_to_dict(d: dict, key: object, value: object) -> bool: Add the given key-value pair to the dictionary. Args: d (dict): The dictionary to add the item to. key (object): The key for the item. value (object): The value for the item. Returns: bool: True if the item was added successfully, False otherwise. pass ``` **Function 3: Retrieve an item from a dictionary by key** ```python def get_item_from_dict(d: dict, key: object) -> object: Retrieve the value for the given key from the dictionary. Args: d (dict): The dictionary to retrieve the item from. key (object): The key for the item. Returns: object: The value for the given key, or None if the key is not in the dictionary. pass ``` **Function 4: Remove an item from a dictionary by key** ```python def remove_item_from_dict(d: dict, key: object) -> bool: Remove the item with the given key from the dictionary. Args: d (dict): The dictionary to remove the item from. key (object): The key for the item. Returns: bool: True if the item was removed successfully, False otherwise. pass ``` **Function 5: Check if a dictionary contains a specific key** ```python def dict_contains_key(d: dict, key: object) -> bool: Check if the dictionary contains the given key. Args: d (dict): The dictionary to check. key (object): The key to check for. Returns: bool: True if the dictionary contains the key, False otherwise. pass ``` **Function 6: Get all keys from a dictionary** ```python def get_dict_keys(d: dict) -> list: Get a list of all keys in the dictionary. Args: d (dict): The dictionary to get the keys from. Returns: list: A list of all keys in the dictionary. pass ``` **Function 7: Clear all items from a dictionary** ```python def clear_dict(d: dict): Remove all items from the dictionary. Args: d (dict): The dictionary to clear. pass ``` # Constraints - Do not use built-in Python functions like `dict.clear()`, `dict.update()`, etc. - Handle any necessary exceptions. - Ensure all your implementations are efficient. Each function must be implemented correctly to ensure the functionality mirrors that provided by the Python C API.","solution":"def create_empty_dict() -> dict: Create and return an empty dictionary. Returns: dict: An empty dictionary. return {} def add_item_to_dict(d: dict, key: object, value: object) -> bool: Add the given key-value pair to the dictionary. Args: d (dict): The dictionary to add the item to. key (object): The key for the item. value (object): The value for the item. Returns: bool: True if the item was added successfully, False otherwise. try: d[key] = value return True except Exception: return False def get_item_from_dict(d: dict, key: object) -> object: Retrieve the value for the given key from the dictionary. Args: d (dict): The dictionary to retrieve the item from. key (object): The key for the item. Returns: object: The value for the given key, or None if the key is not in the dictionary. return d.get(key) def remove_item_from_dict(d: dict, key: object) -> bool: Remove the item with the given key from the dictionary. Args: d (dict): The dictionary to remove the item from. key (object): The key for the item. Returns: bool: True if the item was removed successfully, False otherwise. if key in d: del d[key] return True return False def dict_contains_key(d: dict, key: object) -> bool: Check if the dictionary contains the given key. Args: d (dict): The dictionary to check. key (object): The key to check for. Returns: bool: True if the dictionary contains the key, False otherwise. return key in d def get_dict_keys(d: dict) -> list: Get a list of all keys in the dictionary. Args: d (dict): The dictionary to get the keys from. Returns: list: A list of all keys in the dictionary. return list(d.keys()) def clear_dict(d: dict): Remove all items from the dictionary. Args: d (dict): The dictionary to clear. keys = list(d.keys()) for key in keys: del d[key]"},{"question":"# Boxplot Customization and Analysis with Seaborn Objective You are being tested on your ability to use the Seaborn library to create and customize boxplots using various features provided by the library. Task Write a function `custom_boxplot` that takes in a DataFrame and uses Seaborn to create multiple boxplots with different customizations as specified below. Input - `df`: A Pandas DataFrame containing at least the following columns: \'age\', \'class\', \'alive\', \'deck\', \'fare\'. Output - A single Matplotlib figure with multiple customized boxplots as specified: 1. A horizontal boxplot of the \'age\' column. 2. A vertical boxplot grouping by \'class\' with \'age\' as data. 3. A vertical boxplot with nested grouping by \'class\' and \'alive\' with \'age\' as data. 4. A boxplot of \'age\' grouped by \'deck\' with whiskers covering the full data range. 5. A boxplot of \'fare\' with \'age\' (rounded to the nearest 10) on the x-axis, preserving native scaling. 6. A customized vertical boxplot of \'age\' grouped by \'class\' with the following customizations: - Notched boxes - No caps on the boxes - Custom marker for outliers as \\"x\\" - Box color with specified RGBA values - Median line color as red and line width as 2 Example Code ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_boxplot(df): sns.set_theme(style=\\"whitegrid\\") # 1. Horizontal boxplot of \'age\' plt.figure(figsize=(20, 15)) plt.subplot(3, 2, 1) sns.boxplot(x=df[\\"age\\"]) plt.title(\\"Horizontal boxplot of Age\\") # 2. Vertical boxplot grouping by \'class\' plt.subplot(3, 2, 2) sns.boxplot(data=df, x=\\"class\\", y=\\"age\\") plt.title(\\"Vertical boxplot of Age grouped by Class\\") # 3. Nested grouping by \'class\' and \'alive\' plt.subplot(3, 2, 3) sns.boxplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") plt.title(\\"Vertical boxplot of Age grouped by Class and Alive\\") # 4. Boxplot of \'age\' grouped by \'deck\' with full-range whiskers plt.subplot(3, 2, 4) sns.boxplot(data=df, x=\\"age\\", y=\\"deck\\", whis=(0, 100)) plt.title(\\"Boxplot of Age grouped by Deck with full-range whiskers\\") # 5. Boxplot of \'fare\' with age (rounded to nearest 10) on x-axis, preserving native scaling plt.subplot(3, 2, 5) ax = sns.boxplot(x=df[\\"age\\"].round(-1), y=df[\\"fare\\"], native_scale=True) ax.axvline(25, color=\\".3\\", dashes=(2, 2)) plt.title(\\"Boxplot of Fare by rounded Age, preserving scale\\") # 6. Customized vertical boxplot of \'age\' grouped by \'class\' plt.subplot(3, 2, 6) sns.boxplot( data=df, x=\\"class\\", y=\\"age\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (.3, .5, .7, .5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2}, ) plt.title(\\"Customized vertical boxplot of Age grouped by Class\\") # Show the plot plt.tight_layout() plt.show() # Example usage with Titanic dataset titanic = sns.load_dataset(\\"titanic\\") custom_boxplot(titanic) ``` Constraints - Ensure that all plots are displayed in a single figure with appropriate titles. - Make sure that the DataFrame passed to the function contains all the necessary columns to avoid errors. Write your code implementation completely and assume the required libraries (`seaborn` and `matplotlib`) are already installed.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_boxplot(df): sns.set_theme(style=\\"whitegrid\\") # 1. Horizontal boxplot of \'age\' plt.figure(figsize=(20, 15)) plt.subplot(3, 2, 1) sns.boxplot(x=df[\\"age\\"]) plt.title(\\"Horizontal boxplot of Age\\") # 2. Vertical boxplot grouping by \'class\' plt.subplot(3, 2, 2) sns.boxplot(data=df, x=\\"class\\", y=\\"age\\") plt.title(\\"Vertical boxplot of Age grouped by Class\\") # 3. Nested grouping by \'class\' and \'alive\' plt.subplot(3, 2, 3) sns.boxplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\") plt.title(\\"Vertical boxplot of Age grouped by Class and Alive\\") # 4. Boxplot of \'age\' grouped by \'deck\' with full-range whiskers plt.subplot(3, 2, 4) sns.boxplot(data=df, x=\\"age\\", y=\\"deck\\", whis=(0, 100)) plt.title(\\"Boxplot of Age grouped by Deck with full-range whiskers\\") # 5. Boxplot of \'fare\' with age (rounded to nearest 10) on x-axis, preserving native scaling plt.subplot(3, 2, 5) ax = sns.boxplot(x=df[\\"age\\"].round(-1), y=df[\\"fare\\"], native_scale=True) ax.axvline(25, color=\\".3\\", dashes=(2, 2)) plt.title(\\"Boxplot of Fare by rounded Age, preserving scale\\") # 6. Customized vertical boxplot of \'age\' grouped by \'class\' plt.subplot(3, 2, 6) sns.boxplot( data=df, x=\\"class\\", y=\\"age\\", notch=True, showcaps=False, flierprops={\\"marker\\": \\"x\\"}, boxprops={\\"facecolor\\": (.3, .5, .7, .5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2}, ) plt.title(\\"Customized vertical boxplot of Age grouped by Class\\") # Show the plot plt.tight_layout() plt.show()"},{"question":"# Coding Assessment: Custom Communication Hook Implementation **Objective**: Demonstrate your understanding of PyTorch\'s DDP communication hooks by implementing a custom communication hook. This hook will modify the gradient tensors before the allreduce operation in a `DistributedDataParallel` setup. # Task Implement a custom DDP communication hook that normalizes the gradients within each `GradBucket` before they are allreduced. The hook should normalize each gradient tensor to have a unit L2 norm. **Requirements**: 1. Define a function `normalize_hook(state: Optional[object], bucket: torch.distributed.GradBucket) -> torch.futures.Future`. 2. The function should operate on the gradients in `bucket` to ensure each tensor in the bucket has an L2 norm of 1. 3. Register this hook with a `DistributedDataParallel` model before training. # Input and Output Formats - **Input**: N/A (the function will be used as a callback in the DDP model) - **Output**: The function should return a `torch.futures.Future` object containing the normalized tensor. # Constraints - Use the `torch.futures.Future` API to handle asynchronous operations. - Ensure the function is compatible with the `torch.nn.parallel.DistributedDataParallel.register_comm_hook` interface. # Performance Requirements The implementation should efficiently normalize the gradients without introducing significant overhead to the training process. # Example Usage Below is an example demonstrating how to use the hook with a simple DDP model setup. ```python import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.distributed import GradBucket from typing import Optional class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 10) self.fc2 = nn.Linear(10, 5) def forward(self, x): return self.fc2(self.fc1(x)) def normalize_hook(state: Optional[object], bucket: GradBucket) -> torch.futures.Future: # TO BE IMPLEMENTED: Normalize gradients to unit L2 norm pass def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def demo(rank, world_size): setup(rank, world_size) model = SimpleModel().to(rank) ddp_model = DDP(model, device_ids=[rank]) ddp_model.register_comm_hook(state=None, hook=normalize_hook) optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) for epoch in range(10): inputs = torch.randn(20, 10).to(rank) outputs = ddp_model(inputs) loss = outputs.sum() optimizer.zero_grad() loss.backward() optimizer.step() cleanup() if __name__ == \\"__main__\\": world_size = 2 mp.spawn(demo, args=(world_size,), nprocs=world_size, join=True) ``` **Note**: Remember to implement the `normalize_hook` function to complete this exercise. # Hints: - Use `torch.norm` and `torch.div` to normalize the gradient tensors. - Ensure that the function works asynchronously to fit the `Future` API used in communication hooks.","solution":"import torch import torch.distributed as dist from torch.distributed import GradBucket from torch.futures import Future from typing import Optional def normalize_hook(state: Optional[object], bucket: GradBucket) -> Future: Normalize gradient tensors in the bucket to have unit L2 norm. # Get the gradients as a list of tensors grads = bucket.to_tensors() for i, grad in enumerate(grads): # Compute the L2 norm of the gradient norm = torch.norm(grad) if norm > 0: # Normalize the gradient tensor to have a unit L2 norm grads[i] = torch.div(grad, norm) # Create a future and mark it as complete with the normalized grads future = Future() future.set_result(grads) return future"},{"question":"**Question**: **Module Structure and Conditional Execution** You are tasked with creating a Python module that performs the following operations: 1. Defines a function `process_data(data)` which takes a list of integers and returns a new list with each element squared. 2. Encapsulates a `main` function to demonstrate its functionality. 3. Uses the `if __name__ == \'__main__\'` idiom to conditionally execute the `main` function only when the module is run directly. # Implementation Details: 1. Create a file named `data_processor.py`. 2. Define the function `process_data(data)`: - **Input**: A list of integers `data`. - **Output**: A list of integers with each element squared. 3. Define the `main` function: - **Function**: This should read a list of integers, call `process_data(data)`, and print the result. - **Execution**: Use `sys.argv` to accept a space-separated list of integers from the command line. 4. Use the `if __name__ == \'__main__\'` idiom to call the `main` function when the module is executed directly. 5. Ensure that the primary logic (i.e., reading inputs, processing data, and printing the result) is inside the `main` function and does not run when the module is imported. # Constraints: - Only integers should be read from the command line input. - The input list should not be empty. - Handle invalid inputs gracefully by displaying an appropriate message. # Example: If the module is run from the command line as follows: ```bash python3 data_processor.py 1 2 3 4 5 ``` The expected output should be: ```bash [1, 4, 9, 16, 25] ``` If imported in another module, the `process_data` function should be available without executing `main`. # Notes: - The implementation should follow best practices for writing Python scripts and modules. - Make sure to test the script both by running it directly and importing it into another script. Submit the `data_processor.py` file with the implemented solution.","solution":"import sys def process_data(data): Returns a new list with each integer in the input list squared. if not all(isinstance(i, int) for i in data): raise ValueError(\\"All elements in the data must be integers\\") return [x ** 2 for x in data] def main(): Reads a list of integers from the command line, processes them using process_data, and prints the result. try: # Read integers from command line arguments inputs = sys.argv[1:] data = [int(i) for i in inputs] # Convert to list of integers # Check if data is empty if not data: raise ValueError(\\"Input list should not be empty\\") # Process data result = process_data(data) # Print the result print(result) except ValueError as v: print(f\\"Error: {v}\\") except Exception as e: print(f\\"Unexpected error: {e}\\") if __name__ == \'__main__\': main()"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of Seaborn\'s `husl_palette` function and its application in visualizations. # Problem Description You are given a dataset containing information on different species of flowers, including their petal lengths and widths. Your task is to: 1. Generate three different color palettes using the `husl_palette` function with specific customizations. 2. Create a scatter plot using Seaborn to visualize the relationship between petal length and petal width for each species, applying the customized palettes to differentiate between the species. # Input - A pandas DataFrame, `flowers_df`, with the following columns: - `species` (str): Flower species. - `petal_length` (float): Length of the flower\'s petal. - `petal_width` (float): Width of the flower\'s petal. # Output - A Seaborn scatter plot visualizing the relationship between petal length and petal width, using three different customized palettes for the species. # Requirements 1. Generate three different color palettes using `husl_palette`: - Palette 1: 3 colors with the default lightness and saturation. - Palette 2: 3 colors with lightness decreased to 0.5. - Palette 3: 3 colors with saturation increased to 0.8. 2. Use these palettes to create scatter plots for each species within the provided DataFrame. Ensure each species uses a different palette. # Constraints - Use only the Seaborn and matplotlib packages for visualization. - Follow good coding practices including clear variable names and comments. # Example Function Implementations ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_flower_data(flowers_df): # Define palettes palette1 = sns.husl_palette(n_colors=3) palette2 = sns.husl_palette(n_colors=3, l=0.5) palette3 = sns.husl_palette(n_colors=3, s=0.8) # Create a list of species species_list = flowers_df[\'species\'].unique() # Define a dictionary to map species to palettes palette_mapping = { species_list[0]: palette1, species_list[1]: palette2, species_list[2]: palette3 } # Create scatter plot plt.figure(figsize=(10, 6)) for species, palette in palette_mapping.items(): species_data = flowers_df[flowers_df[\'species\'] == species] sns.scatterplot(x=\'petal_length\', y=\'petal_width\', data=species_data, label=species, palette=palette) plt.title(\'Petal Length vs Petal Width by Species\') plt.legend(title=\'Species\') plt.show() # Example call to the function # flowers_df = pd.read_csv(\'path_to_file.csv\') # ensure the DataFrame is loaded appropriately # plot_flower_data(flowers_df) ``` # Notes - You can assume the DataFrame `flowers_df` will always contain data for exactly three species.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_flower_data(flowers_df): Generates a scatter plot visualizing the relationship between petal length and petal width for each species using three different HUSL color palettes. Parameters: flowers_df (pd.DataFrame): A DataFrame containing columns \'species\', \'petal_length\', and \'petal_width\'. # Generate three different color palettes using husl_palette palette1 = sns.husl_palette(n_colors=3) palette2 = sns.husl_palette(n_colors=3, l=0.5) palette3 = sns.husl_palette(n_colors=3, s=0.8) # Create a list of the unique species species_list = flowers_df[\'species\'].unique() # Define a dictionary to map species to palettes palette_mapping = { species_list[0]: palette1[0], species_list[1]: palette2[1], species_list[2]: palette3[2] } # Create scatter plot plt.figure(figsize=(10, 6)) for species, color in palette_mapping.items(): species_data = flowers_df[flowers_df[\'species\'] == species] sns.scatterplot(x=\'petal_length\', y=\'petal_width\', data=species_data, label=species, color=color) plt.title(\'Petal Length vs Petal Width by Species\') plt.xlabel(\'Petal Length\') plt.ylabel(\'Petal Width\') plt.legend(title=\'Species\') plt.show()"},{"question":"You are tasked with implementing a simulation of a task scheduler using a priority queue. The scheduler should be capable of handling task insertion, priority updating, and task removal efficiently. Each task is associated with a unique priority, and tasks with lower priority values should be executed first. In case of a tie (tasks with the same priority), tasks should be executed in the order they were added. # Detailed Requirements 1. Implement the `TaskScheduler` class with the following methods: - `add_task(task: str, priority: int) -> None`: Adds a new task to the scheduler with the given priority. If the task already exists, update its priority. - `remove_task(task: str) -> None`: Removes the specified task from the scheduler. If the task is not found, raise a `KeyError`. - `pop_task() -> str`: Removes and returns the task with the lowest priority. If the queue is empty, raise a `KeyError`. 2. The class should maintain the heap invariant, ensuring that all heap operations (`heappush`, `heappop`, etc.) are performed efficiently. 3. You must use the `heapq` module to implement this task scheduler. # Input Format - Method calls will be made on an instance of the `TaskScheduler` class. # Output Format - Each method should perform the operation described and return the appropriate result. # Example ```python scheduler = TaskScheduler() scheduler.add_task(\'task1\', 3) scheduler.add_task(\'task2\', 1) scheduler.add_task(\'task3\', 2) print(scheduler.pop_task()) # Output: \'task2\' scheduler.remove_task(\'task3\') scheduler.add_task(\'task4\', 1) print(scheduler.pop_task()) # Output: \'task4\' print(scheduler.pop_task()) # Output: \'task1\' ``` # Constraints - `task` is a string consisting of alphanumeric characters and has a maximum length of 100. - `priority` is an integer between `0` and `1,000,000`. - The scheduler can handle up to `100,000` tasks. # Notes - Ensure that your solution handles edge cases, such as trying to remove a non-existent task or popping from an empty scheduler. - Efficiently manage the task queue to maintain the heap properties.","solution":"import heapq class TaskScheduler: def __init__(self): # Min-heap to store tasks self.pq = [] # Dictionary to store the tasks with their respective priorities and indices self.task_dict = {} # Counters to keep the order of tasks self.counter = 0 def add_task(self, task, priority): if task in self.task_dict: self.remove_task(task) entry = [priority, self.counter, task] self.task_dict[task] = entry heapq.heappush(self.pq, entry) self.counter += 1 def remove_task(self, task): if task not in self.task_dict: raise KeyError(f\'Task {task} not found\') entry = self.task_dict.pop(task) entry[-1] = \'<removed-task>\' def pop_task(self): while self.pq: priority, count, task = heapq.heappop(self.pq) if task != \'<removed-task>\': del self.task_dict[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"# Efficient Array Operations in Python The `array` module in Python provides a way to create and manipulate efficient arrays of numeric values and characters. This module is particularly useful when working with large datasets or when memory efficiency is a concern. You will create a function that performs a series of operations on such arrays. Task Your task is to implement a function `process_array(typecode: str, initial_data: list, operations: list) -> list` that performs the following steps: 1. Create an array using the provided `typecode` and `initial_data`. 2. Apply a sequence of operations to the array specified in the `operations` list. 3. Return the final state of the array as a Python list. Each operation in the `operations` list is a dictionary that specifies the operation type and the necessary parameters. The supported operations are as follows: - `append`: Append a new item to the array. - Example: `{\\"operation\\": \\"append\\", \\"value\\": 10}` - `extend`: Extend the array with elements from an iterable. - Example: `{\\"operation\\": \\"extend\\", \\"values\\": [20, 30, 40]}` - `pop`: Remove and return an item at the specified index (or the last item if no index is provided). - Example: `{\\"operation\\": \\"pop\\", \\"index\\": 2}` - Example with optional index: `{\\"operation\\": \\"pop\\"}` - `remove`: Remove the first occurrence of a value from the array. - Example: `{\\"operation\\": \\"remove\\", \\"value\\": 10}` - `reverse`: Reverse the order of items in the array. - Example: `{\\"operation\\": \\"reverse\\"}` - `insert`: Insert an item at a specified index. - Example: `{\\"operation\\": \\"insert\\", \\"index\\": 1, \\"value\\": 5}` - `tobytes`: Convert the array to a bytes object. - Example: `{\\"operation\\": \\"tobytes\\"}` # Input - `typecode` (str): A single character string specifying the array\'s type code. - `initial_data` (list): A list of initial values to populate the array. - `operations` (list): A list of dictionaries, each defining an operation to perform on the array. # Output - Return a list representing the final state of the array after all operations have been applied. # Example ```python def process_array(typecode: str, initial_data: list, operations: list) -> list: # Your implementation here # Test case typecode = \'i\' initial_data = [1, 2, 3, 4, 5] operations = [ {\\"operation\\": \\"append\\", \\"value\\": 6}, {\\"operation\\": \\"extend\\", \\"values\\": [7, 8, 9]}, {\\"operation\\": \\"remove\\", \\"value\\": 2}, {\\"operation\\": \\"pop\\", \\"index\\": 0}, {\\"operation\\": \\"reverse\\"}, {\\"operation\\": \\"insert\\", \\"index\\": 2, \\"value\\": 10} ] print(process_array(typecode, initial_data, operations)) # Expected output: [9, 8, 10, 7, 6, 5, 4, 3] ``` Constraints - The provided typecode must be a valid typecode from the `array` module. - The initial_data list and values in the operations must be compatible with the specified typecode. - The operations list may include between 1 and 100 operations. Implement the `process_array` function to ensure the correct sequence of operations and return the final array state as a list.","solution":"import array def process_array(typecode: str, initial_data: list, operations: list) -> list: # Create the array using the provided typecode and initial_data arr = array.array(typecode, initial_data) # Process each operation in the operations list for op in operations: if op[\\"operation\\"] == \\"append\\": arr.append(op[\\"value\\"]) elif op[\\"operation\\"] == \\"extend\\": arr.extend(op[\\"values\\"]) elif op[\\"operation\\"] == \\"pop\\": if \\"index\\" in op: arr.pop(op[\\"index\\"]) else: arr.pop() elif op[\\"operation\\"] == \\"remove\\": arr.remove(op[\\"value\\"]) elif op[\\"operation\\"] == \\"reverse\\": arr.reverse() elif op[\\"operation\\"] == \\"insert\\": arr.insert(op[\\"index\\"], op[\\"value\\"]) elif op[\\"operation\\"] == \\"tobytes\\": return arr.tobytes() # Return the final state of the array as a list return arr.tolist()"},{"question":"**Objective:** Demonstrate your understanding of the `tempfile` module by utilizing its various components to solve real-world problems. This will involve creating and managing temporary files and directories, ensuring secure and efficient file handling, and demonstrating familiarity with context managers. **Requirements:** 1. Implement the function `process_temp_files(input_data: str) -> str`. 2. The function should perform the following operations: - Create a temporary directory. - Within this directory, create two temporary files. - Write the first half of the `input_data` to the first file and the second half to the second file. - Read the contents of both files, concatenate them, and return the result. 3. Additionally, ensure that all temporary files and directories are cleaned up after processing. **Input:** - `input_data` (str): A non-empty string to be split and processed. **Output:** - (str): Concatenated content of both temporary files. **Function Signature:** ```python def process_temp_files(input_data: str) -> str: pass ``` **Constraints:** - The function should handle any exception that occurs during file operations and ensure that all temporary files and directories are deleted. - The length of `input_data` will be at least 2 characters. - The function should make efficient use of the `tempfile` module\'s features, including context managers for automatic cleanup. **Example:** ```python input_data = \\"HelloWorld\\" result = process_temp_files(input_data) print(result) # Output should be \\"HelloWorld\\" ``` **Notes:** - Use `tempfile.TemporaryDirectory` to create the temporary directory. - Use `tempfile.NamedTemporaryFile` to create the temporary files within the directory. - Take advantage of the context manager capabilities for automatic cleanup. **Performance Consideration:** - Ensure that the function can handle large strings efficiently, utilizing memory management techniques provided by the `tempfile` module.","solution":"import tempfile import os def process_temp_files(input_data: str) -> str: try: with tempfile.TemporaryDirectory() as temp_dir: # split the input_data into two halves mid_point = len(input_data) // 2 first_half = input_data[:mid_point] second_half = input_data[mid_point:] # Create two temporary files within the directory temp1_path = os.path.join(temp_dir, \'temp1.txt\') temp2_path = os.path.join(temp_dir, \'temp2.txt\') with open(temp1_path, \'w\') as temp1, open(temp2_path, \'w\') as temp2: temp1.write(first_half) temp2.write(second_half) # Read the content of both files and concatenate them with open(temp1_path, \'r\') as temp1, open(temp2_path, \'r\') as temp2: result = temp1.read() + temp2.read() # Temporary files and directory are automatically cleaned up upon exiting the context managers return result except Exception as e: raise RuntimeError(\\"An error occurred while processing temporary files\\") from e"},{"question":"**Title**: Implementing Custom Autograd Function with Gradient Checking **Problem Statement**: PyTorch\'s `autograd` package allows for automatic differentiation of tensor operations, but in certain cases, you may need to create custom operations not provided by the built-in functionality. To ensure the correctness of such custom operations, PyTorch provides tools for gradient checking. Your task is to implement a custom autograd function in PyTorch for the following mathematical operation: [ f(x) = x^3sin(x) ] This function will include both the forward and backward passes. After implementing this function, you will need to perform gradient checking to verify the correctness of the custom gradient implementation. **Requirements**: 1. **Custom Function**: Implement a custom autograd function named `CubicSine` for the operation ( f(x) = x^3sin(x) ). - Implement the forward pass using the provided mathematical expression. - Implement the backward pass to compute the gradient of the function with respect to the input tensor ( x ). 2. **Gradient Checking**: Use the `torch.autograd.gradcheck` utility to check the gradient computation of your custom function. **Input**: - A tensor `x` of shape `(N,)` where ( N ) is the size of the input tensor. The input tensor will have `requires_grad` set to `True`. **Output**: - A boolean indicating whether the gradient check passed or failed. **Constraints**: - You must not use any in-place operations. - The input tensor `x` will only contain values within the range ([-2pi, 2pi]). **Performance**: - Your solution should efficiently compute both forward and backward passes. **Example**: ```python import torch from torch.autograd import Function class CubicSine(Function): @staticmethod def forward(ctx, x): # Save the input for the backward pass ctx.save_for_backward(x) return x ** 3 * torch.sin(x) @staticmethod def backward(ctx, grad_output): # Retrieve the saved input x, = ctx.saved_tensors # Compute the gradient grad_input = grad_output * (3 * x ** 2 * torch.sin(x) + x ** 3 * torch.cos(x)) return grad_input # Define the input tensor x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # Wrap the tensor in `gradcheck` to perform gradient checking from torch.autograd import gradcheck # Define the custom function using DoubleTensor for gradcheck cubic_sine = CubicSine.apply # Perform gradient check input = (x.double(),) assert gradcheck(cubic_sine, input, eps=1e-6, atol=1e-4), \\"Gradient check failed!\\" print(\\"Gradient check passed!\\") ``` In this example, you need to implement a custom autograd function and check its gradients using the provided `torch.autograd.gradcheck` utility. The use of `gradcheck` ensures that the gradients are correctly computed for your custom function. **Note**: Ensure that you have a recent version of PyTorch installed to use `gradcheck`.","solution":"import torch from torch.autograd import Function class CubicSine(Function): @staticmethod def forward(ctx, x): # Save the input for the backward pass ctx.save_for_backward(x) return x ** 3 * torch.sin(x) @staticmethod def backward(ctx, grad_output): # Retrieve the saved input x, = ctx.saved_tensors # Compute the gradient grad_input = grad_output * (3 * x ** 2 * torch.sin(x) + x ** 3 * torch.cos(x)) return grad_input def check_gradient(): # Define the input tensor x = torch.tensor([1.0, 2.0, 3.0], dtype=torch.double, requires_grad=True) # Wrap the tensor in `gradcheck` to perform gradient checking from torch.autograd import gradcheck # Define the custom function using DoubleTensor for gradcheck cubic_sine = CubicSine.apply # Perform gradient check input = (x,) return gradcheck(cubic_sine, input, eps=1e-6, atol=1e-4) # Check gradient and return the result gradient_check_result = check_gradient()"},{"question":"# CSV Data Processing Assessment Objective Create a Python function that processes a CSV file to compute specific statistics and then writes the results to a new CSV file. This task will test your understanding of reading, manipulating, and writing CSV data, as well as handling custom dialects and error conditions. Problem Statement You are given a CSV file named `employee_data.csv` with the following format: ``` employee_id,first_name,last_name,department,salary 1,John,Doe,Engineering,60000 2,Jane,Smith,Marketing,55000 3,Bob,Johnson,Sales,50000 ... ``` Your task is to: 1. Read the `employee_data.csv` file. 2. Calculate the average salary for each department. 3. Write the results to a new CSV file named `department_salary_stats.csv` with the following format: ``` department,average_salary Engineering,65000 Marketing,60000 Sales,55000 ... ``` Requirements 1. Use the `csv.DictReader` class to read the input CSV file. 2. Use the `csv.DictWriter` class to write the output CSV file. 3. Handle any CSV reading/writing errors gracefully by printing an appropriate error message. 4. The average salary should be rounded to the nearest integer. 5. Use a custom CSV dialect named `custom_dialect` with the following properties: - Delimiter: `;` - Quote character: `\'` - Quoting: `csv.QUOTE_MINIMAL` Function Signature ```python def process_employee_data(input_file: str, output_file: str) -> None: pass ``` Input - `input_file` (str): The name of the input CSV file (`employee_data.csv`). - `output_file` (str): The name of the output CSV file (`department_salary_stats.csv`). Output - The function does not return any value. It writes the results directly to the output CSV file. Constraints - Assume the input CSV file exists and is well-formed. - There can be any number of departments and any number of employees in the input file. Example ```python # Example usage: process_employee_data(\'employee_data.csv\', \'department_salary_stats.csv\') # After running the function, the \'department_salary_stats.csv\' file should contain something like: # department;average_salary # Engineering;62500 # Marketing;60000 # Sales;52500 # ... ``` Notes - Make use of the `csv` module extensively. - Consider edge cases, such as empty departments or very large numbers for salaries. - Ensure that the newlines in the output CSV file are handled properly according to the `csv` module specifications.","solution":"import csv from collections import defaultdict def process_employee_data(input_file: str, output_file: str) -> None: try: csv.register_dialect(\'custom_dialect\', delimiter=\';\', quotechar=\\"\'\\", quoting=csv.QUOTE_MINIMAL) # Read the employee data with open(input_file, \'r\', newline=\'\') as csvfile: reader = csv.DictReader(csvfile) salaries = defaultdict(list) for row in reader: department = row[\'department\'] salary = int(row[\'salary\']) salaries[department].append(salary) # Calculate the average salary for each department avg_salaries = {dept: round(sum(salaries[dept]) / len(salaries[dept])) for dept in salaries} # Write the results to the output file with open(output_file, \'w\', newline=\'\') as csvfile: fieldnames = [\'department\', \'average_salary\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=\'custom_dialect\') writer.writeheader() for dept, avg_salary in avg_salaries.items(): writer.writerow({\'department\': dept, \'average_salary\': avg_salary}) except Exception as e: print(f\\"Error processing file: {e}\\")"},{"question":"**Seaborn Rug Plot Customization and Integration** You are given a dataset containing information about customers\' purchase behaviors in a retail store. The dataset includes the following columns: - `total_spent`: The total amount spent by the customer. - `number_of_items`: The total number of items purchased. - `time_of_day`: Whether the purchase was made during the day (\\"Day\\") or night (\\"Night\\"). Your task is to create a detailed visualization using seaborn that fulfills the following requirements: 1. **Scatter Plot**: Create a scatter plot of `total_spent` against `number_of_items`, using different colors for purchases made during the day and night. 2. **Rug Plot**: - Add a rug plot along both axes to indicate the density of data points. - Represent the `time_of_day` on the rug plot using different colors, matching the scatter plot. - Use a thinner line width and add some transparency to the rug plot lines to enhance the visualization. 3. **Customization**: - Position the rug plot outside the axes for better visibility. - Customize the rug plot height to be 0.05. # Input - The dataset is in a CSV file named `retail_data.csv`. # Output - A Matplotlib figure containing the combined scatter plot and rug plots as described above. # Constraints - Use seaborn for creating the plots. - The visualization should be clear, informative, and aesthetically pleasing. # Example ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\'retail_data.csv\') # Set the theme sns.set_theme() # Create the scatter plot with rug plot plt.figure(figsize=(10, 6)) # Scatter plot with hue based on time_of_day scatter = sns.scatterplot(data=data, x=\'total_spent\', y=\'number_of_items\', hue=\'time_of_day\') # Add rug plots rug = sns.rugplot(data=data, x=\'total_spent\', y=\'number_of_items\', hue=\'time_of_day\', lw=1, alpha=0.5, height=-0.05, clip_on=False) # Customize plt.title(\'Customer Purchase Behavior\') plt.xlabel(\'Total Spent ()\') plt.ylabel(\'Number of Items Purchased\') plt.legend(title=\'Time of Day\') # Display the plot plt.show() ``` In this example, you need to load the provided CSV, create the scatter and rug plots, and customize them according to the specifications. Make sure to validate that the visual elements accurately represent the data and provide meaningful insights.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_purchase_behavior_plot(csv_file): # Load the dataset data = pd.read_csv(csv_file) # Set the theme sns.set_theme() # Create a figure plt.figure(figsize=(10, 6)) # Scatter plot with hue based on time_of_day scatter = sns.scatterplot(data=data, x=\'total_spent\', y=\'number_of_items\', hue=\'time_of_day\', palette={\'Day\': \'blue\', \'Night\': \'orange\'}) # Add rug plots rug = sns.rugplot(data=data, x=\'total_spent\', hue=\'time_of_day\', palette={\'Day\': \'blue\', \'Night\': \'orange\'}, lw=0.5, alpha=0.5, height=0.05, clip_on=False) rug = sns.rugplot(data=data, y=\'number_of_items\', hue=\'time_of_day\', palette={\'Day\': \'blue\', \'Night\': \'orange\'}, lw=0.5, alpha=0.5, height=0.05, clip_on=False) # Customize plt.title(\'Customer Purchase Behavior\') plt.xlabel(\'Total Spent ()\') plt.ylabel(\'Number of Items Purchased\') plt.legend(title=\'Time of Day\') # Display the plot plt.show()"},{"question":"**Context Management in an Asynchronous Framework** As a developer, you are tasked with creating a small web server using `asyncio` that handles multiple clients asynchronously and keeps track of the session data using `contextvars`. **Requirements:** 1. **Session Management**: Each client should have a unique session ID stored in a `ContextVar`. 2. **Request Handling**: The server should respond to requests with a greeting message and the session ID of the client. 3. **Context Preservation**: Ensure that each client\'s session data is isolated and managed properly even if multiple clients connect simultaneously. You need to implement the following functions: 1. `generate_session_id() -> str`: This function should return a unique session ID each time it is called. 2. `handle_client(reader, writer)`: This asynchronous function should: - Generate a session ID and set it in the context variable. - Handle incoming requests and respond with a greeting message that includes the session ID. 3. `main()`: This asynchronous function should set up the server to listen for client connections and utilize `handle_client` for request processing. **Specifications:** - You must use the `ContextVar` for managing session IDs. - Use `Context.run` to ensure request handling is isolated per client. - Handle potential exceptions and ensure the server cleans up resources properly. **Example of expected behavior:** ```python import asyncio import contextvars # Context variable for storing session ID session_id_var = contextvars.ContextVar(\'session_id\') def generate_session_id() -> str: Generate a unique session ID. return str(uuid.uuid4()) async def handle_client(reader, writer): # Set a unique session ID in the context session_id = generate_session_id() session_id_var.set(session_id) while True: data = await reader.read(100) if not data: break message = data.decode() session_id = session_id_var.get() response = f\\"Hello, your session ID is {session_id}n\\" writer.write(response.encode()) await writer.drain() writer.close() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8080) async with server: await server.serve_forever() # Running the server asyncio.run(main()) ``` **Note:** This code is a simplified version. Your task is to complete the implementation of `generate_session_id`, `handle_client`, and `main` functions ensuring proper use of `contextvars` for session management and appropriate handling of asynchronous requests.","solution":"import asyncio import contextvars import uuid # Context variable for storing session ID session_id_var = contextvars.ContextVar(\'session_id\') def generate_session_id() -> str: Generate a unique session ID. return str(uuid.uuid4()) async def handle_client(reader, writer): ctx = contextvars.copy_context() # Set a unique session ID in the context session_id = generate_session_id() ctx.run(session_id_var.set, session_id) while True: data = await reader.read(100) if not data: break message = data.decode() session_id = ctx[session_id_var] response = f\\"Hello, your session ID is {session_id}n\\" writer.write(response.encode()) await writer.drain() writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8080) async with server: await server.serve_forever() # Running the server if __name__ == \\"__main__\\": try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server stopped by user\\")"},{"question":"Objective: Demonstrate your understanding of Python\'s `contextvars` module by implementing and managing context variables in an asynchronous environment. Question: You are required to create an asynchronous log processing system that uses context variables to manage the log file names dynamically across different asynchronous tasks. Implement the following functions and classes: 1. `LogContext` class: - This class should encapsulate a `ContextVar` which will hold the log file name being processed. - Methods: - `set_log_file(log_file: str)`: Sets the context variable to the specified log file name. - `reset_log_file(token)`: Resets the context variable to its previous state using the provided token. - `get_log_file() -> str`: Returns the current log file from the context variable. 2. `process_log(log_context: LogContext, log_file: str) -> Dict[str, int]` function: - Asynchronously reads the given log file line by line. - Uses the context variable to dynamically access the log file name during processing. - The function should count the occurrences of each word in the log file and return the result as a dictionary. 3. `main()` function: - Asynchronously starts the log processing for a list of log files. - Uses `LogContext` to manage and maintain the context for each log file. - Prints the word count results for each processed log file. # Constraints: - Assume the log files are plain text and each line contains words separated by spaces. - Use the `asyncio` library for asynchronous processing. - Ensure that the context is managed correctly and does not bleed state between different log processing tasks. Example Usage: ```python import asyncio from typing import Dict from contextvars import ContextVar, Token class LogContext: def __init__(self): self.log_file_var = ContextVar(\'log_file\', default=None) def set_log_file(self, log_file: str) -> Token: return self.log_file_var.set(log_file) def reset_log_file(self, token: Token): self.log_file_var.reset(token) def get_log_file(self) -> str: return self.log_file_var.get() async def process_log(log_context: LogContext, log_file: str) -> Dict[str, int]: token = log_context.set_log_file(log_file) word_count = {} try: async with aiofiles.open(log_file, \'r\') as file: async for line in file: for word in line.split(): word_count[word] = word_count.get(word, 0) + 1 finally: log_context.reset_log_file(token) return word_count async def main(): log_context = LogContext() log_files = [\'log1.txt\', \'log2.txt\'] tasks = [process_log(log_context, log_file) for log_file in log_files] results = await asyncio.gather(*tasks) for log_file, result in zip(log_files, results): print(f\'Results for {log_file}: {result}\') if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Notes: - The provided example establishes the structure; you need to complete the implementations as required. - Assume relevant imports like `aiofiles` and `asyncio` are handled. Good luck!","solution":"import asyncio from typing import Dict from contextvars import ContextVar, Token class LogContext: def __init__(self): self.log_file_var = ContextVar(\'log_file\', default=None) def set_log_file(self, log_file: str) -> Token: return self.log_file_var.set(log_file) def reset_log_file(self, token: Token): self.log_file_var.reset(token) def get_log_file(self) -> str: return self.log_file_var.get() async def process_log(log_context: LogContext, log_file: str) -> Dict[str, int]: token = log_context.set_log_file(log_file) word_count = {} try: async with aiofiles.open(log_file, \'r\') as file: async for line in file: for word in line.split(): word_count[word] = word_count.get(word, 0) + 1 finally: log_context.reset_log_file(token) return word_count async def main(): log_context = LogContext() log_files = [\'log1.txt\', \'log2.txt\'] tasks = [process_log(log_context, log_file) for log_file in log_files] results = await asyncio.gather(*tasks) for log_file, result in zip(log_files, results): print(f\'Results for {log_file}: {result}\') if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Objective:** Your task is to implement a function `custom_traceback_handler()`, which captures, processes, and formats the traceback information when an exception occurs within a given block of code. **Requirements:** 1. **Function Signature** ```python def custom_traceback_handler(code_block: callable): Captures and processes the traceback information when an exception occurs inside code_block. Parameters: code_block (callable): A function or lambda that will be executed within the traceback handler. Returns: str: A formatted string containing the processed traceback information if an exception occurs. If no exception occurs, return an empty string. ``` 2. **Functionality** - The function should execute the provided `code_block()`. - If `code_block()` raises an exception, it should: 1. Capture the exception and its traceback. 2. Format the traceback to include custom information: - The exception type and message. - The stack trace entries in a readable format. - The local variables in each stack frame. 3. **Hints & Constraints** - Use `traceback.TracebackException` to help format the traceback information. - Ensure the formatted string includes all required information, and is neatly organized. - You can use additional fields from `traceback` functions/classes where appropriate. - Do not modify or raise different exceptions within the handler; just capture and format them. 4. **Example Usage** ```python def example_code(): x = 1 / 0 # This will raise ZeroDivisionError result = custom_traceback_handler(example_code) print(result) ``` **Expected output** (format may vary, but should include similar details): ``` Traceback (most recent call last): File \\"<stdin>\\", line 2, in example_code ZeroDivisionError: division by zero Local Variables: - x: 1 ``` **Notes:** - You may assume that `code_block` will always be a valid callable. - The labelled sections (\\"Traceback\\", \\"Local Variables\\", etc.) should be formatted clearly. - Ensure that the module is imported properly where necessary. This task tests the student\'s comprehension of exception handling, traceback exploration and processing, and their ability to design and implement structured handling of error information in Python.","solution":"import traceback def custom_traceback_handler(code_block: callable) -> str: try: code_block() return \\"\\" except Exception as e: tb = e.__traceback__ tb_exc = traceback.TracebackException(type(e), e, tb, capture_locals=True) formatted_traceback = \\"\\".join(tb_exc.format()) return formatted_traceback"},{"question":"Objective: You are required to design and implement a solution that demonstrates your understanding of multioutput classification using scikit-learn\'s `MultiOutputClassifier`. Your solution should load a dataset, preprocess it, fit a multioutput classifier, and make predictions. Problem Statement: You are tasked with classifying multiple properties of handwritten digits from the `digits` dataset available in scikit-learn. The dataset comprises 8x8 pixel grayscale images of digits (0 to 9). The objective is to predict two properties for each image: 1. The digit itself (0-9). 2. Whether the digit is even or odd. Requirements: 1. Load the `digits` dataset from `sklearn.datasets`. 2. Preprocess the dataset to create two sets of target labels: - The original digit labels. - Binary labels indicating whether the digit is even (0) or odd (1). 3. Stack these target labels column-wise to form a 2D target array. 4. Split the dataset into training and testing sets (80% training, 20% testing). 5. Use a `RandomForestClassifier` as the base estimator and implement a `MultiOutputClassifier` to fit the model on the training data. 6. Make predictions on the test data. 7. Evaluate the model\'s performance using appropriate metrics for multiclass and binary classification and print the results. Constraints: - You must use `MultiOutputClassifier` from `sklearn.multioutput`. - Use a `RandomForestClassifier` with default parameters as the base estimator. Input: None (all inputs will be handled within the script). Output: Print the classification report for both targets (digit labels and even/odd labels) on the test set. Example: ```python from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.multioutput import MultiOutputClassifier from sklearn.metrics import classification_report import numpy as np # Step 1: Load the dataset digits = load_digits() X = digits.data y_digits = digits.target # Step 2: Create even/odd labels y_even_odd = (y_digits % 2 == 1).astype(int) # Step 3: Stack the target labels y = np.column_stack((y_digits, y_even_odd)) # Step 4: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 5: Initialize and train MultiOutputClassifier with RandomForest forest = RandomForestClassifier(random_state=42) multi_target_forest = MultiOutputClassifier(forest) multi_target_forest.fit(X_train, y_train) # Step 6: Make predictions on the test data y_pred = multi_target_forest.predict(X_test) # Step 7: Evaluate the model\'s performance print(\\"Classification report for digit labels:\\") print(classification_report(y_test[:, 0], y_pred[:, 0])) print(\\"nClassification report for even/odd labels:\\") print(classification_report(y_test[:, 1], y_pred[:, 1])) ``` This example illustrates how to use `MultiOutputClassifier` with a `RandomForestClassifier` to handle multioutput classification on the `digits` dataset. Your task is to implement the entire example from scratch and ensure all steps are followed correctly.","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.multioutput import MultiOutputClassifier from sklearn.metrics import classification_report import numpy as np def multioutput_classification(): # Step 1: Load the dataset digits = load_digits() X = digits.data y_digits = digits.target # Step 2: Create even/odd labels y_even_odd = (y_digits % 2 == 1).astype(int) # Step 3: Stack the target labels y = np.column_stack((y_digits, y_even_odd)) # Step 4: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 5: Initialize and train MultiOutputClassifier with RandomForest forest = RandomForestClassifier(random_state=42) multi_target_forest = MultiOutputClassifier(forest) multi_target_forest.fit(X_train, y_train) # Step 6: Make predictions on the test data y_pred = multi_target_forest.predict(X_test) # Step 7: Evaluate the model\'s performance print(\\"Classification report for digit labels:\\") print(classification_report(y_test[:, 0], y_pred[:, 0])) print(\\"nClassification report for even/odd labels:\\") print(classification_report(y_test[:, 1], y_pred[:, 1])) return y_test, y_pred"},{"question":"Design a function that demonstrates an understanding of pandas options for configuring numeric output formatting, alongside typical data manipulation tasks. # Problem Statement You are provided with data representing sales figures for a set of products. Write a function `format_and_analyze_sales` that reads this data from a CSV file, formats the numeric output to engineering notation with 2 decimal places, and performs a summary analysis. # Function Signature ```python def format_and_analyze_sales(file_path: str) -> pd.DataFrame: ``` # Input - `file_path` (str): The path to the CSV file containing the sales data. The CSV file has the following columns: - `ProductID` (str): The unique identifier for each product. - `SalesAmount` (float): The sales figures for each product. - `Region` (str): The sales region. # Output - `summary_df` (pd.DataFrame): A DataFrame with the summary analysis, including: - `Region`: The region name. - `TotalSales` (in engineering notation): The total sales amount for the region. - `MeanSales` (in engineering notation): The mean sales amount for the region. - `MaxSales` (in engineering notation): The maximum sales amount recorded for the region. # Constraints - The function must utilize pandas options to set the numeric formatting to engineering notation with 2 decimal places. - The summary DataFrame should be indexed by the `Region`. # Example Suppose the `sales.csv` file contains the following data: ``` ProductID,SalesAmount,Region P001,150000.50,North P002,2345000.75,North P003,135000.25,South P004,987000.65,South P005,653200.45,East ``` Then the output DataFrame should be: ``` TotalSales MeanSales MaxSales Region East 6.53e+05 6.53e+05 6.53e+05 North 2.50e+06 1.25e+06 2.34e+06 South 1.12e+06 5.62e+05 9.87e+05 ``` # Note Ensure that you import any necessary libraries, in particular: ```python import pandas as pd ```","solution":"import pandas as pd def format_and_analyze_sales(file_path: str) -> pd.DataFrame: # Read the sales data from a CSV file df = pd.read_csv(file_path) # Set pandas options for numeric formatting in engineering notation with 2 decimal places pd.set_option(\'display.float_format\', lambda x: f\'{x:.2e}\') # Perform the summary analysis summary_df = df.groupby(\'Region\').agg( TotalSales=(\'SalesAmount\', \'sum\'), MeanSales=(\'SalesAmount\', \'mean\'), MaxSales=(\'SalesAmount\', \'max\') ) return summary_df"},{"question":"**Objective:** The goal is to implement a Python function that performs a series of statistical operations on a given dataset and returns a summary of statistical measures, including central tendency measures, variability measures, and information on linear relationships between datasets. This function should demonstrate the use of various functions provided by the `statistics` module. **Question:** Implement a function `analyze_statistics(data1: Sequence[float], data2: Optional[Sequence[float]] = None) -> dict` that performs the following operations: 1. Compute the arithmetic mean (using `mean`). 2. Compute the median (using `median`). 3. Compute the mode (using `mode`). 4. Compute the population standard deviation (using `pstdev`). 5. Compute the sample variance (using `variance`). 6. If a second dataset `data2` is provided: - Compute the sample covariance (using `covariance`). - Compute Pearson\'s correlation coefficient (using `correlation`). - Perform linear regression and return the slope and intercept (using `linear_regression`). # Function Signature: ```python from typing import Sequence, Optional, Dict def analyze_statistics(data1: Sequence[float], data2: Optional[Sequence[float]] = None) -> Dict[str, float]: pass ``` # Input: - `data1` (Sequence[float]): A sequence of real-valued numbers. - `data2` (Optional[Sequence[float]]): An optional second sequence of real-valued numbers. # Output: - A dictionary with the following keys and respective values. - `\'mean\'`: The arithmetic mean of `data1`. - `\'median\'`: The median of `data1`. - `\'mode\'`: The mode of `data1`. - `\'population_stdev\'`: The population standard deviation of `data1`. - `\'sample_variance\'`: The sample variance of `data1`. - If `data2` is provided, also include: - `\'covariance\'`: The sample covariance between `data1` and `data2`. - `\'correlation\'`: Pearson\'s correlation coefficient between `data1` and `data2`. - `\'slope\'`: The slope of the linear regression line fitting `data1` and `data2`. - `\'intercept\'`: The intercept of the linear regression line fitting `data1` and `data2`. # Constraints: - The input sequences must contain at least two elements. - Handle the case where `data1` or `data2` has fewer elements than required for certain operations (e.g., covariance, correlation). # Example: ```python data1 = [1, 2, 3, 4, 4, 5, 5, 5, 6] data2 = [1, 1, 2, 3, 5, 6, 7, 8, 9] result = analyze_statistics(data1, data2) print(result) ``` # Expected Output: ```python { \'mean\': 4.111111111111111, \'median\': 4, \'mode\': 5, \'population_stdev\': 1.4937368193521872, \'sample_variance\': 2.2222222222222223, \'covariance\': 4.222222222222222, \'correlation\': 0.909787557477552, \'slope\': 0.9365079365079365, \'intercept\': 0.8571428571428568 } ``` # Note: - Use appropriate handling for edge cases, such as empty sequences or sequences lacking sufficient data points for certain calculations. - Include meaningful error messages for invalid input cases.","solution":"from typing import Sequence, Optional, Dict import statistics def analyze_statistics(data1: Sequence[float], data2: Optional[Sequence[float]] = None) -> Dict[str, float]: if len(data1) < 2: raise ValueError(\\"data1 must contain at least two elements\\") if data2 is not None and len(data2) < 2: raise ValueError(\\"data2 must contain at least two elements\\") summary = { \'mean\': statistics.mean(data1), \'median\': statistics.median(data1), \'mode\': statistics.mode(data1), \'population_stdev\': statistics.pstdev(data1), \'sample_variance\': statistics.variance(data1) } if data2 is not None: if len(data1) != len(data2): raise ValueError(\\"data1 and data2 must have the same length\\") summary[\'covariance\'] = statistics.covariance(data1, data2) summary[\'correlation\'] = statistics.correlation(data1, data2) slope, intercept = statistics.linear_regression(data1, data2) summary[\'slope\'] = slope summary[\'intercept\'] = intercept return summary"},{"question":"# Python Coding Assessment Question **Objective:** Implement a function that processes a list of strings and performs various operations to derive a final integer result. The function should demonstrate the use of core data types, built-in functions, list manipulations, and error handling. **Task:** Write a function `process_strings` that takes a list of strings as input. Each string can represent either an integer or an arithmetic expression involving integers (\'+\', \'-\', \'*\', \'/\'). The function should parse these strings to perform the specified arithmetic operations on the integers in the order they appear in the list. The final result should be returned as an integer. **Function Signature:** ```python def process_strings(strings: list) -> int: ``` **Input:** - `strings`: a list of strings, each either representing an integer or an arithmetic operator (\'+\', \'-\', \'*\', \'/\'). **Output:** - Returns the final integer result after performing the arithmetic operations in sequence. **Constraints:** - `strings` will contain at least one integer. - The list `strings` will always start with an integer. - Division by zero should be handled gracefully by returning `None`. - All integers will be non-negative. - All arithmetic operations should follow the left-to-right sequence in the list. **Example:** ```python assert process_strings([\'3\', \'+\', \'2\', \'*\', \'4\', \'-\', \'5\']) == 5 # Explanation: 3 + 2 = 5, 5 * 4 = 20, 20 - 5 = 15 assert process_strings([\'10\', \'/\', \'2\', \'+\', \'3\', \'*\', \'2\']) == 11 # Explanation: 10 / 2 = 5, 5 + 3 = 8, 8 * 2 = 16 assert process_strings([\'10\', \'/\', \'0\', \'+\', \'1\']) == None # Explanation: Division by 0 leads to None ``` **Notes:** - Any invalid input outside the specified constraints can be ignored. - Do not use the `eval` function for parsing and evaluating the expressions. **Requirements:** 1. Correct handling and parsing of input strings. 2. Proper use of built-in functions. 3. Well-written error handling for division by zero. 4. Efficient computation following the sequence of operations.","solution":"def process_strings(strings: list) -> int: if not strings: return None # Initialize result with the first number try: result = int(strings[0]) except ValueError: return None # Iterate over the rest elements i = 1 while i < len(strings): operator = strings[i] try: operand = int(strings[i + 1]) except (ValueError, IndexError): return None if operator == \'+\': result += operand elif operator == \'-\': result -= operand elif operator == \'*\': result *= operand elif operator == \'/\': if operand == 0: return None result //= operand # Perform integer division else: return None i += 2 # Move to the next operator-operand pair return result"},{"question":"**Question: Multi-Part OS Module Interaction** In this problem, you will leverage multiple functionalities provided by the `os` module to interact with the file system, environment variables, and process management. # Task Write a Python function `setup_environment_and_process(directory, env_var_name, env_var_value)` that performs the following steps: 1. Check if the directory specified by the `directory` parameter exists. - If it does not exist, create this directory. 2. Change the current working directory to the specified `directory`. 3. Set an environment variable with the name `env_var_name` to the value `env_var_value`. 4. Create a new process that executes a shell command to list all files and directories in the current directory and capture the output. 5. The function returns the captured output from the command and the current value of the environment variable `env_var_name`. Input: - `directory` (str): The target directory. - `env_var_name` (str): The name of the environment variable to set. - `env_var_value` (str): The value of the environment variable to set. Output: - Return a tuple (`command_output`, `env_var_value`) where: - `command_output` (str): The output captured from the `ls` or `dir` command after creating the directory and setting up the environment variable. - `env_var_value` (str): The current value of the environment variable `env_var_name`. Usage Constraints: - Use `os.makedirs` for creating directories. - Use `os.chdir` for changing the working directory. - Use `os.environ` to access and set environment variables. - Use `os.popen` to execute shell commands and capture their outputs. Example Usage: ```python output, env_value = setup_environment_and_process(\'/tmp/example_dir\', \'MY_VAR\', \'test_value\') print(output) print(env_value) ``` Notes: - Ensure the function handles any errors gracefully. - Make the function compatible with both Unix (using `ls`) and Windows (using `dir`) commands when listing directory contents.","solution":"import os def setup_environment_and_process(directory, env_var_name, env_var_value): Set up the environment by creating a directory, setting an environment variable, and capturing the output of a command listing the directory contents. Parameters: - directory (str): The target directory. - env_var_name (str): The name of the environment variable to set. - env_var_value (str): The value of the environment variable to set. Returns: - tuple: (command_output, env_var_value) where: - command_output (str): The output captured from the `ls` or `dir` command. - env_var_value (str): The current value of the environment variable `env_var_name`. # Check if the directory exists, create it if it does not if not os.path.exists(directory): os.makedirs(directory) # Change the current working directory to the specified directory os.chdir(directory) # Set the environment variable os.environ[env_var_name] = env_var_value # Determine the command based on the operating system command = \'dir\' if os.name == \'nt\' else \'ls\' # Execute the command and capture the output with os.popen(command) as stream: command_output = stream.read() # Get the current value of the environment variable current_env_value = os.environ.get(env_var_name) return command_output, current_env_value"},{"question":"Objective: You are required to write a Python function that utilizes the `netrc` module to read a `.netrc` file, retrieve authentication details for specified hosts, and perform validation checks on the data. Problem Statement: Write a function `validate_netrc(file_path: str, hosts: List[str]) -> Dict[str, Tuple[Optional[str], Optional[str], Optional[str]]]` that takes in: - `file_path`: a string representing the path to the `.netrc` file. - `hosts`: a list of host names for which authentication details need to be retrieved. The function should return a dictionary where: - The keys are the hostnames from the `hosts` list. - The values are tuples `(login, account, password)` representing the authentication details for each host. If the host or default entry is not found, the value should be `(None, None, None)`. Additionally, the function should raise appropriate exceptions if the file cannot be read or if there are parsing errors. Function Signature: ```python from typing import List, Optional, Tuple, Dict def validate_netrc(file_path: str, hosts: List[str]) -> Dict[str, Tuple[Optional[str], Optional[str], Optional[str]]]: pass ``` Constraints: 1. The function should handle files that may not be present (raise `FileNotFoundError`). 2. The function should handle parsing errors (raise `netrc.NetrcParseError`). 3. The function should only return authentication details for the given hosts or the default entry if the host is not explicitly listed. 4. Ensure proper error handling and messages for different exceptions. Example: Suppose the `.netrc` file at `path/to/.netrc` contains: ``` machine example.com login user1 password pass1 default login default_user password default_pass ``` For `validate_netrc(\'path/to/.netrc\', [\'example.com\', \'nonexistent.com\'])`, the function should return: ```python { \'example.com\': (\'user1\', None, \'pass1\'), \'nonexistent.com\': (\'default_user\', None, \'default_pass\') } ``` Notes: - The `.netrc` file format is commonly used for specifying login information for various hosts. - Proper permissions and security checks should be considered as outlined in the `netrc` documentation. - Make sure to account for any changes or updates in the `netrc` module behavior as per the documentation. Assessment Criteria: - Correctness of implementation. - Appropriate use of the `netrc` module. - Handling of exceptions and errors. - Adherence to provided input-output format.","solution":"from typing import List, Optional, Tuple, Dict import netrc import os import re def validate_netrc(file_path: str, hosts: List[str]) -> Dict[str, Tuple[Optional[str], Optional[str], Optional[str]]]: Validates and retrieves authentication details from a .netrc file for specified hosts. :param file_path: Path to the .netrc file. :param hosts: List of hostnames to retrieve authentication details for. :return: Dictionary with hostnames as keys and tuples of (login, account, password) as values. # Check if the file exists if not os.path.exists(file_path): raise FileNotFoundError(f\\"The file at {file_path} does not exist.\\") # Try to parse the .netrc file try: auth_data = netrc.netrc(file_path) except netrc.NetrcParseError as e: raise netrc.NetrcParseError(f\\"Error parsing the .netrc file: {e}\\") # Storage for authentication details results = {} for host in hosts: if host in auth_data.hosts: login, account, password = auth_data.authenticators(host) else: login, account, password = auth_data.authenticators(\'default\') if \'default\' in auth_data.hosts else (None, None, None) results[host] = (login, account, password) return results"},{"question":"<|Analysis Begin|> The provided documentation describes how to use the `mps` (Metal Performance Shaders) backend for PyTorch, which enables high-performance GPU training on MacOS devices using the Metal programming framework. The documentation outlines the steps to check for MPS availability, create tensors and models on the MPS device, and perform operations using the device. Key points for designing a coding question: - Ensure that the student is familiar with checking the availability of the MPS backend. - Require operations on tensors using the MPS device. - Include model creation, tensor manipulation, and moving data between CPU and MPS devices. <|Analysis End|> <|Question Begin|> # PyTorch MPS Device Coding Assessment You are tasked with writing a function to demonstrate your understanding of using the MPS backend with PyTorch to perform tensor operations and model predictions on a GPU for MacOS. Function Specifications 1. **Function Name:** `run_mps_operations` 2. **Input:** None 3. **Output:** A tuple containing: - A tensor `y` resulting from a multiplication operation on the MPS device. - A tensor `pred` which is the output of a simple linear model\'s prediction on `x`. 4. **Constraints:** - If the MPS device is not available, the function should raise an `EnvironmentError` with the message \\"MPS not available\\". - The input tensor `x` should be initialized with ones of size 5 on the MPS device. - Perform tensor multiplication to obtain `y` (i.e., `y = x * 2`). - Create and use a simple linear model for prediction, and move the model to the MPS device before making predictions. Example Code Here is an example usage of your function: ```python try: y, pred = run_mps_operations() print(f\\"y: {y}\\") print(f\\"pred: {pred}\\") except EnvironmentError as e: print(e) ``` Implementation ```python import torch import torch.nn as nn def run_mps_operations(): # Check that MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): raise EnvironmentError(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: raise EnvironmentError(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") # Create a Tensor directly on the mps device mps_device = torch.device(\\"mps\\") x = torch.ones(5, device=mps_device) # Perform tensor multiplication y = x * 2 # Define a simple linear model class SimpleLinearModel(nn.Module): def __init__(self): super(SimpleLinearModel, self).__init__() self.linear = nn.Linear(5, 1) def forward(self, x): return self.linear(x) model = SimpleLinearModel() model.to(mps_device) # Predict using the model pred = model(x) return y, pred ``` Write the function `run_mps_operations` following the specifications provided above. Make sure to handle errors appropriately and ensure the tensors and model are properly moved to the MPS device.","solution":"import torch import torch.nn as nn def run_mps_operations(): # Check if MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): raise EnvironmentError(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: raise EnvironmentError(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") # Create a tensor directly on the MPS device mps_device = torch.device(\\"mps\\") x = torch.ones(5, device=mps_device) # Perform tensor multiplication to generate y y = x * 2 # Define a simple linear model class SimpleLinearModel(nn.Module): def __init__(self): super(SimpleLinearModel, self).__init__() self.linear = nn.Linear(5, 1) def forward(self, x): return self.linear(x) model = SimpleLinearModel() model.to(mps_device) # Predict using the model pred = model(x) return y, pred"},{"question":"# SAX Parser Implementation and XML Reading In this assessment, you are required to create a SAX parser using Python\'s `xml.sax` module. Your task is to read an XML file, parse its content, and process specific elements using custom handlers. Follow the instructions below: Requirements 1. **Create a Custom Content Handler:** - Implement a class `MyContentHandler` that inherits from `xml.sax.ContentHandler`. - Override the `startElement`, `endElement`, and `characters` methods to process the elements and text data. 2. **Parse the XML File:** - Use `xml.sax.make_parser()` to create a SAX parser. - Set the content handler to an instance of `MyContentHandler`. - Parse a given XML file `input.xml`. 3. **Extract Specific Data:** - Your handler should specifically extract and print the contents of elements named `title` and `author`. Input and Output Formats - **Input:** An XML file named `input.xml` with various elements, including `title` and `author`. - **Output:** Print the start and end of each element, and the contents of `title` and `author` elements. Example Structure of `input.xml`: ```xml <books> <book> <title>Effective Python</title> <author>Brett Slatkin</author> <year>2015</year> </book> <book> <title>Learning Python</title> <author>Mark Lutz</author> <year>2013</year> </book> </books> ``` Expected Output: ``` Start Element: books Start Element: book Start Element: title Characters: Effective Python End Element: title Start Element: author Characters: Brett Slatkin End Element: author Start Element: year End Element: year End Element: book Start Element: book Start Element: title Characters: Learning Python End Element: title Start Element: author Characters: Mark Lutz End Element: author Start Element: year End Element: year End Element: book End Element: books ``` Constraints - Ensure you implement proper XML reading practices. - Handle potential exceptions during parsing. - Your implementation should be efficient and concise. Submission Submit a single Python script `sax_parser.py` containing: - The `MyContentHandler` class implementation. - The SAX parser setup and execution code.","solution":"import xml.sax class MyContentHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" def startElement(self, name, attrs): self.current_data = name print(f\\"Start Element: {name}\\") def endElement(self, name): print(f\\"End Element: {name}\\") self.current_data = \\"\\" def characters(self, content): if self.current_data in [\\"title\\", \\"author\\"]: print(f\\"Characters: {content}\\") def parse_xml(filepath): parser = xml.sax.make_parser() handler = MyContentHandler() parser.setContentHandler(handler) try: parser.parse(filepath) except xml.sax.SAXParseException as e: print(f\\"Error parsing XML: {e}\\") # To run the parser: # parse_xml(\'input.xml\')"},{"question":"# Advanced Coding Assessment Question Objective: Test the understanding of Python\'s object protocol and interactions. Problem Statement: Implement a class `Entity` in Python that mimics some behaviors of Python\'s built-in objects using custom implementations of several methods from the provided object protocols. Class Specification: Create a class `Entity` with the following specifications: 1. **Initialization**: - Constructor should take a dictionary as input, with keys as attribute names and values as attribute values. - Store these attributes in an internal dictionary `_attributes`. 2. **Methods**: - `hasattr(self, attr_name)`: Check if an attribute exists. - `getattr(self, attr_name)`: Get the value of an attribute. Return `None` if attribute does not exist. - `setattr(self, attr_name, value)`: Set the value of an attribute. - `delattr(self, attr_name)`: Remove an attribute. - `repr(self)`: Return a string representation of the form `Entity({attributes_dict})`. - `str(self)`: Return a string of attribute names and their values in `name=value` format. - `getitem(self, key)`: Return the value of an attribute using key indexing. Raise KeyError if not found. - `setitem(self, key, value)`: Set the value of an attribute using key indexing. - `delitem(self, key)`: Remove the attribute using key indexing. Raise KeyError if not found. - `iter(self)`: Provide an iterator over attribute names. - `len(self)`: Return the number of attributes. Expected Input & Output: ```python # Example usage: entity = Entity({\'name\': \'Alice\', \'age\': 30}) # hasattr assert entity.hasattr(\'name\') == True # True assert entity.hasattr(\'address\') == False # False # getattr assert entity.getattr(\'name\') == \'Alice\' # \'Alice\' assert entity.getattr(\'address\') == None # None # setattr entity.setattr(\'address\', \'Wonderland\') assert entity.getattr(\'address\') == \'Wonderland\' # \'Wonderland\' # delattr entity.delattr(\'address\') assert entity.hasattr(\'address\') == False # False # repr and str assert repr(entity) == \\"Entity({\'name\': \'Alice\', \'age\': 30})\\" assert str(entity) == \'name=Alice, age=30\' # getitem assert entity[\'name\'] == \'Alice\' # \'Alice\' # setitem entity[\'job\'] = \'Engineer\' assert entity[\'job\'] == \'Engineer\' # \'Engineer\' # delitem del entity[\'job\'] try: _ = entity[\'job\'] except KeyError: print(\\"Passed KeyError Test\\") # iter and len assert sorted([item for item in entity]) == [\'age\', \'name\'] assert len(entity) == 2 # since \'name\' and \'age\' exist ``` Constraints: 1. All attribute names will be strings. 2. Attribute values can be of any type. Performance Requirements: Your implementation should ensure the basic operations (attribute access, modification, and deletions) are handled efficiently, ideally in constant time.","solution":"class Entity: def __init__(self, attributes): self._attributes = attributes def hasattr(self, attr_name): return attr_name in self._attributes def getattr(self, attr_name): return self._attributes.get(attr_name, None) def setattr(self, attr_name, value): self._attributes[attr_name] = value def delattr(self, attr_name): if attr_name in self._attributes: del self._attributes[attr_name] def __repr__(self): return f\\"Entity({self._attributes})\\" def __str__(self): return \', \'.join(f\\"{k}={v}\\" for k, v in self._attributes.items()) def __getitem__(self, key): if key in self._attributes: return self._attributes[key] else: raise KeyError(f\\"Key \'{key}\' not found in Entity.\\") def __setitem__(self, key, value): self._attributes[key] = value def __delitem__(self, key): if key in self._attributes: del self._attributes[key] else: raise KeyError(f\\"Key \'{key}\' not found in Entity.\\") def __iter__(self): return iter(self._attributes) def __len__(self): return len(self._attributes)"},{"question":"You are required to demonstrate your proficiency in creating and customizing histograms using the `seaborn.objects` module. Below is the description of the task you need to complete: Task Description 1. **Load the `diamonds` dataset from seaborn.** 2. **Create two histograms side by side** using the `so.Plot` and `so.Bars()` methods: - **Histogram 1:** Plot the distribution of diamond prices using default settings, applying a logarithmic scale to the x-axis. - **Histogram 2:** Plot the distribution of diamond prices for \\"Ideal\\" cut diamonds, using narrower, unfilled bars with a custom edge color and a fixed bin width. Detailed Requirements: 1. **Loading the Dataset:** - Load the `diamonds` dataset using `seaborn.load_dataset()`. 2. **Histogram 1:** - Use `so.Plot` to initialize the plot with the `diamonds` dataset and map the `price` column. - Scale the x-axis logarithmically. - Add bars using `so.Bars()` and histogram representation using `so.Hist()`. 3. **Histogram 2:** - Filter the `diamonds` dataset to keep only rows where `cut` is \\"Ideal\\". - Initialize the plot with the filtered dataset and map the `price` column. - Set the bin width to a fixed value of 500. - Use narrower bars (half the default width), set them to be unfilled, and use a custom edge color (e.g., `\\"blue\\"`). - Add bars using `so.Bars(fill=False, edgecolor=\\"blue\\", edgewidth=1.5)` and histogram representation using `so.Hist()`. 4. **Output:** - Display the two histograms side by side for comparison. Constraints: - Ensure your code is well-structured and includes necessary comments for clarity. - Your implementation should be compatible with Python 3.10 or above. Input Format: - Not applicable as you will be using the seaborn library\'s built-in dataset. Output Format: - The output should display two histograms side by side. # Sample Code Structure: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create subplot for multiple plots fig, axs = plt.subplots(1, 2, figsize=(15, 5)) # Histogram 1: All Diamonds p1 = so.Plot(diamonds, \\"price\\").scale(x=\\"log\\") p1.add(so.Bars(), so.Hist()).on(axs[0]) # Histogram 2: Ideal Cut Diamonds ideal_diamonds = diamonds[diamonds[\'cut\'] == \'Ideal\'] hist = so.Hist(binwidth=500) # Set your desired bin width here p2 = (so.Plot(ideal_diamonds, \\"price\\") .add(so.Bars(fill=False, edgecolor=\\"blue\\", edgewidth=1.5, width=0.5), hist)) p2.on(axs[1]) # Show the plots plt.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_histograms(): # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create subplot for multiple plots fig, axs = plt.subplots(1, 2, figsize=(15, 5)) # Histogram 1: All Diamonds p1 = so.Plot(diamonds, x=\\"price\\").scale(x=\\"log\\") p1.add(so.Bars(), so.Hist()).on(axs[0]) # Histogram 2: Ideal Cut Diamonds ideal_diamonds = diamonds[diamonds[\'cut\'] == \'Ideal\'] hist = so.Hist(binwidth=500) # Set bin width to 500 p2 = (so.Plot(ideal_diamonds, x=\\"price\\") .add(so.Bars(fill=False, edgecolor=\\"blue\\", edgewidth=1.5, width=0.5), hist)) p2.on(axs[1]) # Show the plots plt.show() # You can call the function to display the histograms create_histograms()"},{"question":"Custom Weight Loader with PyTorch and Environment Variables **Objective:** Develop a custom weight loader function using PyTorch that demonstrates a solid understanding of PyTorch\'s `torch.load` functionality and environment variable management. This function should allow for customizable behavior based on provided or default settings, utilizing environment variables specified in the documentation. Problem Statement Implement a function `custom_weight_loader(filepath: str, default_weights_only: bool = True, autograd_timeout: int = 10) -> dict:` that loads a PyTorch model\'s weights from a given filepath with the following behavior: 1. **Environment Variable Management**: - If the environment variable `TORCH_FORCE_WEIGHTS_ONLY_LOAD` is set to true (`1`, `y`, `yes`, `true`), the function should force `weights_only=True`. - If the environment variable `TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD` is set to true, it should force `weights_only=False`. - If neither of these environment variables is set, the function should use the `default_weights_only` parameter value. 2. **Autograd Timeout Configuration**: - Use the `TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT` environment variable to set the autograd shutdown wait timeout. If this variable is not set, use the `autograd_timeout` parameter value. 3. **Loading Weights**: - Utilize the `torch.load()` function to load the model weights. - Ensure that the function handles potential exceptions that might arise from the loading process. 4. **Return Value**: - The function should return a dictionary object representing the loaded weights if successful. Input-Output Format - **Input**: - `filepath` (str): A string representing the file path to the model weights file. - `default_weights_only` (bool, optional): A boolean value indicating whether to load weights only if environment variables are not set. Defaults to True. - `autograd_timeout` (int, optional): An integer specifying the autograd shutdown wait timeout in seconds if the environment variable is not set. Defaults to 10. - **Output**: - Returns a dictionary object representing the loaded weights. Constraints: - You can assume the file at `filepath` exists and is accessible. - Ensure your solution considers potential race conditions and handles multi-threading where applicable. Example Usage ```python import os os.environ[\\"TORCH_FORCE_WEIGHTS_ONLY_LOAD\\"] = \\"yes\\" os.environ[\\"TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\\"] = \\"15\\" weights = custom_weight_loader(\\"/path/to/model/weights.pth\\", default_weights_only=False, autograd_timeout=20) print(weights) ``` Notes: - Ensure you have proper exception handling to manage file access and loading issues. - Document your code clearly to explain decisions and functionality.","solution":"import os import torch def custom_weight_loader(filepath: str, default_weights_only: bool = True, autograd_timeout: int = 10) -> dict: Loads a PyTorch model\'s weights from a given filepath with customizable behavior. Parameters: - filepath (str): The path to the model weights file. - default_weights_only (bool, optional): Default behavior for loading weights only. Defaults to True. - autograd_timeout (int, optional): Autograd shutdown wait timeout in seconds. Defaults to 10. Returns: - dict: The loaded weights. # Determine weights_only value based on environment variables or default parameter weights_only = default_weights_only if os.getenv(\\"TORCH_FORCE_WEIGHTS_ONLY_LOAD\\", \\"\\").lower() in [\\"1\\", \\"y\\", \\"yes\\", \\"true\\"]: weights_only = True elif os.getenv(\\"TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\\", \\"\\").lower() in [\\"1\\", \\"y\\", \\"yes\\", \\"true\\"]: weights_only = False # Determine autograd_timeout value based on environment variable or parameter autograd_timeout = int(os.getenv(\\"TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\\", autograd_timeout)) try: # Load the weights using torch.load weights = torch.load(filepath, weights_only=weights_only) return weights except Exception as e: print(f\\"An error occurred while loading weights: {e}\\") return {}"},{"question":"Objective You are tasked with creating a function that reads an AIFF/AIFC file, processes the audio to increase its volume, and saves the altered audio into a new file. This requires you to use multiple functionalities provided by the `aifc` module. Task Write a function `increase_volume(input_file: str, output_file: str, volume_factor: float) -> None` that: 1. Reads an AIFF or AIFF-C file specified by `input_file`. 2. Increases the volume of the audio data by a factor of `volume_factor`. 3. Writes the processed audio data into a new AIFF or AIFF-C file specified by `output_file`. Input - `input_file` (str): The path to the input AIFF or AIFF-C file. - `output_file` (str): The path where the processed audio file will be saved. - `volume_factor` (float): The factor by which the volume of the audio should be increased. A `volume_factor` > 1.0 increases the volume, and a `volume_factor` < 1.0 decreases it. Output - The function does not return any value. The processed audio is saved to `output_file`. Constraints - The input audio file might be mono (1 channel) or stereo (2 channels). - The volume increase should be clamped to prevent overflow, i.e., the samples should not exceed the maximum value allowed by the sample width. Performance Requirements - Efficient data handling is crucial as audio files can be large. - The function should accurately preserve the original audio\'s sample width, number of channels, and frame rate. Example ```python increase_volume(\'input.aifc\', \'output.aifc\', 1.5) ``` This call reads `input.aifc`, increases the volume by 50%, and writes the output to `output.aifc`. Additional Information - Make sure your function handles exceptions gracefully, such as file not found, read/write errors, etc. - Use the `aifc` module to interact with the AIFF or AIFF-C files. # Hints - You may need to read the entire audio data, process it, and then write it back. - Be cautious with how the volume modification affects the byte values of the samples.","solution":"import aifc import struct def increase_volume(input_file: str, output_file: str, volume_factor: float) -> None: try: with aifc.open(input_file, \'rb\') as input_f: num_channels = input_f.getnchannels() sample_width = input_f.getsampwidth() frame_rate = input_f.getframerate() num_frames = input_f.getnframes() audio_data = input_f.readframes(num_frames) max_sample = (1 << (sample_width * 8 - 1)) - 1 min_sample = -(1 << (sample_width * 8 - 1)) fmt = {1: \'b\', 2: \'h\', 4: \'i\'}.get(sample_width) if fmt is None: raise ValueError(\\"Unsupported sample width\\") unpack_fmt = f\'{num_frames * num_channels}{fmt}\' samples = struct.unpack(unpack_fmt, audio_data) modified_samples = [] for sample in samples: modified_sample = int(sample * volume_factor) if modified_sample > max_sample: modified_sample = max_sample elif modified_sample < min_sample: modified_sample = min_sample modified_samples.append(modified_sample) packed_data = struct.pack(unpack_fmt, *modified_samples) with aifc.open(output_file, \'wb\') as output_f: output_f.setnchannels(num_channels) output_f.setsampwidth(sample_width) output_f.setframerate(frame_rate) output_f.writeframes(packed_data) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Advanced Coding Assessment: Implementing a Custom File Handler **Objective:** Demonstrate your understanding of file handling in Python, specifically using the `io` module, by implementing a custom class that reads from and writes to both binary and text files, supports seeking, and provides position tracking. **Task:** You are required to implement a custom class `CustomFileHandler` with the following specifications: 1. **Initialization:** - `__init__(self, file_path: str, mode: str, encoding: str = None):` - `file_path`: Path to the file to be opened. - `mode`: Mode in which the file is to be opened (\'r\', \'w\', \'rb\', \'wb\', etc.). - `encoding`: Encoding to be used for text files (default is `None`). 2. **Methods:** - `write(self, data: Union[str, bytes]) -> int:`: - Writes data to the file. - Returns the number of characters/bytes written. - `read(self, size: int = -1) -> Union[str, bytes]:` - Reads up to `size` characters/bytes from the file. If `size` is -1, reads until EOF. - Returns the data read. - `seek(self, offset: int, whence: int = io.SEEK_SET) -> int:` - Changes the current file position to the given byte `offset`. - `whence` indicates from which position the offset is applied (start, current, end). - Returns the new absolute position. - `tell(self) -> int:` - Returns the current position within the file. - `close(self) -> None:` - Closes the file. **Constraints:** - You should handle both text and binary files correctly, raising appropriate exceptions when necessary. - You must ensure that the file is properly closed after operations to prevent resource leaks. - The `CustomFileHandler` should utilize the appropriate classes from the `io` module internally. **Example Usage:** ```python # Example usage of CustomFileHandler for text files handler = CustomFileHandler(\'example.txt\', \'w\', encoding=\'utf-8\') handler.write(\'Hello, world!\') handler.seek(0) content = handler.read() print(content) # Output: Hello, world! print(handler.tell()) # Output: 13 handler.close() # Example usage of CustomFileHandler for binary files handler = CustomFileHandler(\'example.bin\', \'wb\') handler.write(b\'x00x01x02\') handler.seek(1) byte = handler.read(1) print(byte) # Output: b\'x01\' print(handler.tell()) # Output: 2 handler.close() ``` **Submission:** Please provide the implementation of the `CustomFileHandler` class with all the required methods and ensure it passes the example usage.","solution":"import io from typing import Union class CustomFileHandler: def __init__(self, file_path: str, mode: str, encoding: str = None): self.file_path = file_path self.mode = mode self.encoding = encoding if \'b\' in mode: self.file = open(file_path, mode) else: self.file = open(file_path, mode, encoding=encoding) def write(self, data: Union[str, bytes]) -> int: written = self.file.write(data) self.file.flush() # Ensure data is written to the file immediately return written def read(self, size: int = -1) -> Union[str, bytes]: return self.file.read(size) def seek(self, offset: int, whence: int = io.SEEK_SET) -> int: self.file.seek(offset, whence) return self.file.tell() def tell(self) -> int: return self.file.tell() def close(self) -> None: self.file.close()"},{"question":"# Python `ftplib` Problem: Automated FTP Tasks Objective Create a Python function using the `ftplib` module that automates the following FTP tasks: 1. Connect to an FTP server. 2. Log in using provided credentials. 3. Change to a specified directory. 4. List the files in the specified directory and return their names as a list. 5. Download a specified file from the directory to the local machine. 6. Upload a specified file from the local machine to the same directory. # Function Signature ```python def automate_ftp_tasks(server: str, username: str, password: str, directory: str, file_to_download: str, file_to_upload: str) -> list: Automates FTP tasks including connecting, logging in, changing directory, listing files, downloading, and uploading. Parameters: - server (str): The FTP server address. - username (str): The username for the FTP server. - password (str): The password for the FTP server. - directory (str): The target directory on the FTP server. - file_to_download (str): The name of the file to download from the server. - file_to_upload (str): The local filepath of the file to upload to the server. Returns: - list: A list of file names present in the specified directory. # Requirements 1. Use the `ftplib` module to connect and interact with the FTP server. 2. Handle exceptions that may occur during connection, login, and file transfer operations. 3. Ensure the downloaded file is saved with the same name as on the server. 4. Ensure the uploaded file is placed in the specified directory on the server. 5. Cleanly close the FTP connection after operations are complete. # Constraints - The function should handle read and write operations for binary files efficiently. - Handle various FTP errors gracefully and provide informative error messages. - Ensure to use the `with` statement for proper resource management if needed. # Example ```python files_list = automate_ftp_tasks( server=\\"ftp.example.com\\", username=\\"testuser\\", password=\\"testpass\\", directory=\\"/example_directory\\", file_to_download=\\"example.txt\\", file_to_upload=\\"upload_example.txt\\" ) print(files_list) # Output: [\'example.txt\', \'another_file.txt\', \'upload_example.txt\', ...] ``` # Notes - Make sure to use the appropriate `retrbinary` and `storbinary` methods for file download and upload respectively. - Account for the possible change in server responses and handle them accordingly. - Detailed inline comments and docstrings are highly encouraged for clarity.","solution":"from ftplib import FTP, error_perm import os def automate_ftp_tasks(server: str, username: str, password: str, directory: str, file_to_download: str, file_to_upload: str) -> list: Automates FTP tasks including connecting, logging in, changing directory, listing files, downloading, and uploading. Parameters: - server (str): The FTP server address. - username (str): The username for the FTP server. - password (str): The password for the FTP server. - directory (str): The target directory on the FTP server. - file_to_download (str): The name of the file to download from the server. - file_to_upload (str): The local filepath of the file to upload to the server. Returns: - list: A list of file names present in the specified directory. ftp = FTP() try: # Connect to the FTP server ftp.connect(server) # Login to the FTP server ftp.login(user=username, passwd=password) # Change to the specified directory ftp.cwd(directory) # List files in the directory files_list = ftp.nlst() # Download the specified file with open(file_to_download, \'wb\') as f: ftp.retrbinary(f\'RETR {file_to_download}\', f.write) # Upload the specified file with open(file_to_upload, \'rb\') as f: ftp.storbinary(f\'STOR {os.path.basename(file_to_upload)}\', f) except error_perm as e: print(f\\"FTP error: {e}\\") raise except Exception as e: print(f\\"An error occurred: {e}\\") raise finally: # Close the connection ftp.quit() return files_list"},{"question":"# Introduction In this coding challenge, you are required to use the scikit-learn library to load a toy dataset, preprocess the data, and apply a machine learning algorithm to classify or predict based on the dataset. This exercise will test your understanding of data loading, preprocessing, model training, and evaluation using scikit-learn. # Task 1. **Data Loading**: Use the `load_wine` function from `sklearn.datasets` to load the wine dataset. 2. **Data Preprocessing**: Perform any necessary preprocessing steps such as handling missing values, scaling features, and splitting the data into training and test sets. 3. **Model Training**: Train a logistic regression model using the preprocessed training data. 4. **Model Evaluation**: Evaluate the performance of the model using appropriate metrics and report the test accuracy. # Instructions 1. **Loading the Data**: Use the `load_wine` function to load the toy dataset. 2. **Preprocessing the Data**: - Handle any missing values if present. (Hint: There might not be any in this dataset.) - Scale the features for better performance. - Split the data into training and test sets (e.g., 70% training, 30% testing). 3. **Training the Model**: - Import `LogisticRegression` from `sklearn.linear_model`. - Train the model using the training data. 4. **Evaluating the Model**: - Predict the labels for the test data. - Calculate and print the test accuracy. # Expected Function Signature The function signature for this task should look like this: ```python from typing import Tuple from sklearn.datasets import load_wine def load_and_train_wine_model(random_state: int = 42) -> Tuple[float, float]: 1. Load the wine dataset. 2. Preprocess the data (scaling and splitting). 3. Train a logistic regression model. 4. Evaluate the model on the test set. Parameters: - random_state (int): Random state for data splitting and model reproducibility Returns: - Tuple[float, float]: The training accuracy and test accuracy of the model pass ``` # Requirements: - Use `train_test_split` from `sklearn.model_selection` for splitting the data. - Use `StandardScaler` from `sklearn.preprocessing` for scaling the features. - Ensure reproducibility by setting a random state where applicable. # Example Output If everything is implemented correctly, running the function `load_and_train_wine_model` might return results as: ```python train_accuracy, test_accuracy = load_and_train_wine_model() print(f\\"Training Accuracy: {train_accuracy:.2f}\\") print(f\\"Test Accuracy: {test_accuracy:.2f}\\") ``` Expected accuracy values will vary depending on the random state and preprocessing, but you should aim for at least 90% accuracy on the training set and a reasonably high accuracy on the test set.","solution":"from typing import Tuple from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def load_and_train_wine_model(random_state: int = 42) -> Tuple[float, float]: # Load the wine dataset wine = load_wine() X, y = wine.data, wine.target # Preprocess the data # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state) # Scale the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the logistic regression model model = LogisticRegression(max_iter=10000, random_state=random_state) model.fit(X_train_scaled, y_train) # Evaluate the model y_train_pred = model.predict(X_train_scaled) y_test_pred = model.predict(X_test_scaled) train_accuracy = accuracy_score(y_train, y_train_pred) test_accuracy = accuracy_score(y_test, y_test_pred) return train_accuracy, test_accuracy"},{"question":"# Pandas Assessment Question **Objective:** You are given three datasets in the form of pandas DataFrames: `sales_data`, `product_data`, and `customer_data`. Your task is to transform these datasets using pandas functions and generate a final summarized report that provides insights into the sales performance by product and customer demographics. **Datasets:** 1. **sales_data**: - Columns: `SaleID`, `ProductID`, `CustomerID`, `SaleDate`, `Amount` - Description: Contains sales records including product bought, customer who bought it, date of sale, and the amount of sale. 2. **product_data**: - Columns: `ProductID`, `ProductCategory`, `Price` - Description: Contains information about the products including product category and price. 3. **customer_data**: - Columns: `CustomerID`, `CustomerName`, `DateOfBirth`, `Gender` - Description: Contains details of customers including their name, date of birth, and gender. **Tasks:** 1. **Combine Data:** - Merge the `sales_data` with `product_data` and `customer_data` to create a comprehensive dataset that includes product category and customer details for each sale. 2. **Generate Age Column:** - Create an `Age` column in the combined dataset by calculating the age of the customer at the time of sale (`SaleDate - DateOfBirth`). 3. **Sales Summary:** - Create a pivot table that summarizes total sales amount and sale counts per `ProductCategory` and `Gender`. 4. **Customer Segmentation:** - Use `cut` to segment customers into different age groups (e.g., 0-18, 19-35, 36-50, 51+). - Generate a crosstab to show the count of sales in each age group by gender. 5. **Missing Data Analysis:** - Identify any missing data in the original datasets before merging, and specify how you handled it (e.g., filling, dropping, etc.) **Output:** Provide three main outputs from the above tasks: 1. The combined dataset with the calculated `Age` column. 2. The pivot table summarizing sales. 3. The crosstab of customer age group by gender. **Function Specifications:** ```python import pandas as pd def sales_analysis(sales_data: pd.DataFrame, product_data: pd.DataFrame, customer_data: pd.DataFrame) -> tuple: Perform analysis on sales data to provide insights. :param sales_data: DataFrame containing sales records. :param product_data: DataFrame containing product information. :param customer_data: DataFrame containing customer information. :return: Tuple containing the combined dataset, sales pivot table, and customer age group crosstab. pass ``` **Constraints:** - Ensure the code is optimized for performance, handling large datasets efficiently. - Document any assumptions made and the rationale for handling missing data. - Usage of pandas functions such as `merge`, `pivot_table`, `cut`, `crosstab`, and others from the provided documentation is expected. **Evaluation Criteria:** - Correctness and completeness of the final combined dataset. - Accuracy of the generated pivot table and crosstab. - Efficient handling of large datasets. - Clear documentation and justification for missing data handling.","solution":"import pandas as pd def sales_analysis(sales_data: pd.DataFrame, product_data: pd.DataFrame, customer_data: pd.DataFrame) -> tuple: Perform analysis on sales data to provide insights. :param sales_data: DataFrame containing sales records. :param product_data: DataFrame containing product information. :param customer_data: DataFrame containing customer information. :return: Tuple containing the combined dataset, sales pivot table, and customer age group crosstab. # Handle missing data by dropping rows with missing values for this analysis sales_data.dropna(inplace=True) product_data.dropna(inplace=True) customer_data.dropna(inplace=True) # Combine datasets combined = pd.merge(sales_data, product_data, on=\'ProductID\') combined = pd.merge(combined, customer_data, on=\'CustomerID\') # Generate Age column combined[\'SaleDate\'] = pd.to_datetime(combined[\'SaleDate\']) combined[\'DateOfBirth\'] = pd.to_datetime(combined[\'DateOfBirth\']) combined[\'Age\'] = combined[\'SaleDate\'].dt.year - combined[\'DateOfBirth\'].dt.year # Sales Summary Pivot Table sales_pivot = pd.pivot_table( combined, values=\'Amount\', index=\'ProductCategory\', columns=\'Gender\', aggfunc=[\'sum\', \'count\'], fill_value=0 ) # Customer Segmentation by Age Groups bins = [0, 18, 35, 50, float(\'inf\')] labels = [\'0-18\', \'19-35\', \'36-50\', \'51+\'] combined[\'AgeGroup\'] = pd.cut(combined[\'Age\'], bins=bins, labels=labels) age_gender_crosstab = pd.crosstab(combined[\'AgeGroup\'], combined[\'Gender\']) return combined, sales_pivot, age_gender_crosstab"},{"question":"**Objective:** You are required to write a function that connects to a POP3 server, logs in, retrieves a specific email, and extracts the subject line from that email. **Problem Statement:** Write a function `retrieve_email_subject` that connects to a POP3 server, logs in, retrieves an email, and extracts the subject line from it. The function should take the following parameters: - `host` (string): The hostname of the POP3 server. - `port` (integer): The port number of the POP3 server. - `username` (string): The username for authentication. - `password` (string): The password for authentication. - `email_id` (integer): The identifier of the email to retrieve. The function should return the subject line of the specified email as a string. If there is any error (e.g., connection issue, authentication issue, email retrieval issue), the function should raise an appropriate exception. **Function Signature:** ```python def retrieve_email_subject(host: str, port: int, username: str, password: str, email_id: int) -> str: pass ``` **Expected Input:** - `host`: A string representing the hostname of the POP3 server (e.g., \'pop.example.com\'). - `port`: An integer representing the port number of the POP3 server (e.g., 110 for non-SSL, 995 for SSL). - `username`: A string representing the username for the POP3 account. - `password`: A string representing the password for the POP3 account. - `email_id`: An integer representing the identifier of the email to retrieve. **Expected Output:** - A string representing the subject line of the specified email. **Constraints:** - The POP3 server used must support the standard POP3 commands. - The function must use the `poplib` module. - If SSL is required, the function should handle the connection appropriately using `POP3_SSL`. **Example Usage:** ```python subject = retrieve_email_subject(\'pop.example.com\', 995, \'user@example.com\', \'password123\', 1) print(subject) # Expected output could be something like \'Meeting at 3 PM\' ``` **Notes:** - You should handle edge cases such as invalid credentials, network failures, and issues with retrieving the email. - Proper error handling is crucial to ensure the function is robust. - You may assume the email\'s subject line is contained in a header within the email body format defined by the `RFC 822` or `RFC 2822`.","solution":"import poplib from email.parser import Parser from email.header import decode_header def retrieve_email_subject(host, port, username, password, email_id): Connects to a POP3 server and retrieves the subject line of a specified email. :param host: POP3 server hostname :param port: POP3 server port :param username: Username for authentication :param password: Password for authentication :param email_id: Email ID to retrieve :return: Subject line of the specified email :raises: Various exceptions on connection, authentication, or retrieval issues try: if port == 995: mail = poplib.POP3_SSL(host, port) else: mail = poplib.POP3(host, port) mail.user(username) mail.pass_(password) # Retrieve the raw message from the server response, msg_lines, octets = mail.retr(email_id) msg_content = b\'rn\'.join(msg_lines).decode(\'utf-8\') msg = Parser().parsestr(msg_content) subject, encoding = decode_header(msg[\'subject\'])[0] if isinstance(subject, bytes): subject = subject.decode(encoding or \'utf-8\') mail.quit() return subject except Exception as e: raise Exception(f\\"An error occurred: {str(e)}\\")"},{"question":"Question: Implementing and Evaluating a Classifier with a Scikit-Learn Toy Dataset # Objective: Your task is to implement a machine learning classifier using the Scikit-learn library, train it on one of the toy datasets provided by Scikit-learn, and evaluate its performance. You will need to perform the following steps: 1. **Load the dataset**: Use the `load_iris` function from the `sklearn.datasets` module to load the Iris dataset. 2. **Preprocess the data**: Split the data into training and testing sets. 3. **Train a classifier**: Implement and train a Support Vector Machine (SVM) classifier on the training data. 4. **Evaluate the classifier**: Evaluate the classifier\'s performance on the testing data using accuracy as the metric and produce a classification report. # Detailed Instructions: 1. **Dataset Loading**: - Use the function `load_iris` from the `sklearn.datasets` module to load the Iris dataset. This function returns a dictionary-like object with the following keys: - `data`: A 2D array with the input features. - `target`: A 1D array with the target class labels. - `DESCR`: A description of the dataset. 2. **Data Preprocessing**: - Split the dataset into a training set and a testing set. Use 80% of the data for training and the remaining 20% for testing. You can use the `train_test_split` function from the `sklearn.model_selection` module to perform the split. 3. **Implementing the Classifier**: - Import the `SVC` class from the `sklearn.svm` module. - Create an instance of the `SVC` class with the parameter `kernel=\'linear\'`. - Fit the classifier on the training data. 4. **Evaluating the Classifier**: - Predict the labels for the testing data using the trained classifier. - Calculate the accuracy of the classifier on the testing data using the `accuracy_score` function from the `sklearn.metrics` module. - Generate and print a classification report using the `classification_report` function from the `sklearn.metrics` module to show precision, recall, and F1-score for each class. # Input Format: - None (all necessary data will be loaded from the Scikit-learn library). # Output Format: - Print the classification report. - Print the accuracy of the classifier. # Constraints: - Use the Scikit-learn library functions as described. - Make sure the code is efficient and well-organized. # Example Code: The following template can be used to structure your solution: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score, classification_report # Load the dataset iris = load_iris() X, y = iris[\'data\'], iris[\'target\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement the classifier classifier = SVC(kernel=\'linear\') classifier.fit(X_train, y_train) # Evaluate the classifier y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) # Print the results print(f\\"Accuracy: {accuracy}\\") print(\\"Classification Report:\\") print(report) ```","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score, classification_report def train_and_evaluate_iris_classifier(): # Load the dataset iris = load_iris() X, y = iris[\'data\'], iris[\'target\'] # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implement the classifier classifier = SVC(kernel=\'linear\') classifier.fit(X_train, y_train) # Evaluate the classifier y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) return accuracy, report"},{"question":"**Coding Assessment Question** You are given the task of implementing a Python function that constructs a complete MIME email message using the classes provided in the `email.mime` module. The function should create a multipart email with the following specifications: 1. A plain text message body. 2. An attached image file. 3. An attached audio file. 4. An attached PDF document. # Function Signature ```python def create_mime_email(text_body: str, image_data: bytes, audio_data: bytes, pdf_data: bytes) -> email.message.EmailMessage: pass ``` # Input - `text_body` (str): A string containing the plain text message body. - `image_data` (bytes): A bytes object containing the raw image data (you can assume the image is a PNG format). - `audio_data` (bytes): A bytes object containing the raw audio data (you can assume the audio is a WAV format). - `pdf_data` (bytes): A bytes object containing the raw PDF data. # Output - The function should return an `EmailMessage` object representing the complete MIME email. # Constraints - All attachments (image, audio, and pdf) are to be encoded using base64 encoding. - Proper MIME headers should be added for each section of the email (e.g., `Content-Type`, `MIME-Version`). # Example Code ```python import email from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email.mime.application import MIMEApplication from email.encoders import encode_base64 def create_mime_email(text_body: str, image_data: bytes, audio_data: bytes, pdf_data: bytes) -> email.message.EmailMessage: # TODO: Implement this function pass ``` # Explanation - You are to create a `MIMEMultipart` object to represent the email. - Attach a `MIMEText` part for the plain text message. - Attach a `MIMEImage` part for the image data. - Attach a `MIMEAudio` part for the audio data. - Attach a `MIMEApplication` part for the PDF data. - Ensure all attachments are encoded in base64 and proper MIME headers are set. # Evaluation Criteria - Correct usage of the `email.mime` classes. - Proper construction of the MIME email with all specified parts. - Correct encoding and headers for each attachment type. - Handling of input data and constraints accurately.","solution":"import email from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio from email.mime.application import MIMEApplication from email.encoders import encode_base64 def create_mime_email(text_body: str, image_data: bytes, audio_data: bytes, pdf_data: bytes) -> email.message.EmailMessage: # Create the container email message msg = MIMEMultipart() msg[\'Subject\'] = \'Multipart Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' # Add the plain text message text_part = MIMEText(text_body, \'plain\') msg.attach(text_part) # Add the image attachment image_part = MIMEImage(image_data, _subtype=\\"png\\") image_part.add_header(\'Content-Disposition\', \'attachment\', filename=\\"image.png\\") encode_base64(image_part) # Encode the image in base64 msg.attach(image_part) # Add the audio attachment audio_part = MIMEAudio(audio_data, _subtype=\\"wav\\") audio_part.add_header(\'Content-Disposition\', \'attachment\', filename=\\"audio.wav\\") encode_base64(audio_part) # Encode the audio in base64 msg.attach(audio_part) # Add the PDF attachment pdf_part = MIMEApplication(pdf_data, _subtype=\\"pdf\\") pdf_part.add_header(\'Content-Disposition\', \'attachment\', filename=\\"document.pdf\\") encode_base64(pdf_part) # Encode the PDF in base64 msg.attach(pdf_part) return msg"},{"question":"# Custom Exception Handling in Python310 Problem Statement: For this exercise, assume you are extending Python with a C library to add custom error handling functionality. Your task is to implement a Python module (in C) that provides a function to simulate reading from a file and handle potential file-related errors using Python310 error handling functionalities. Requirements: 1. Implement a Python module `filehandler` with a function `read_file`. 2. The `read_file` function should take one argument `filename` and try to open the file. 3. If the file cannot be found, raise a `FileNotFoundError` using `PyErr_SetFromErrnoWithFilename`. 4. If the file exists but is not readable (e.g., due to permissions), raise a `PermissionError`. 5. If there is any other error, use the general `OSError` to raise an appropriate error. 6. Ensure to clear any errors set due to incorrect handling. __Input:__ - A single argument `filename` (string) indicating the file path to read. __Output:__ - If the file can be read, return its contents. - Raise appropriate exceptions if any error occurs based on the scenarios mentioned above. __Constraints:__ - You must use the functions and macros provided in the Python310 exception handling documentation. - Follow Python310’s C-API error handling conventions strictly. __Performance requirements:__ - Ensure minimal performance overhead for exception checking and handling. - The solution should be efficient and handle different edge cases for file reading. Example Usage in Python: ```python import filehandler try: content = filehandler.read_file(\\"non_existing_file.txt\\") except FileNotFoundError as e: print(f\\"File not found: {e}\\") except PermissionError as e: print(f\\"Permission denied: {e}\\") except OSError as e: print(f\\"An error occurred: {e}\\") else: print(f\\"File contents: {content}\\") ``` # Hint: - Utilize functions like `PyErr_SetFromErrnoWithFilename`, `PyErr_Clear`, and various standard exception globals such as `PyExc_FileNotFoundError` and `PyExc_PermissionError`. - Refer to the provided documentation to correctly set and handle exceptions.","solution":"# Since extensions in C and error handling in Python require a lot of specific setup, # I\'ll provide the hypothetical python code to simulate this behavior. def read_file(filename): try: with open(filename, \'r\') as file: return file.read() except FileNotFoundError as fnf_error: raise FileNotFoundError(f\\"File not found: {filename}\\") from fnf_error except PermissionError as perm_error: raise PermissionError(f\\"Permission denied: {filename}\\") from perm_error except OSError as os_error: raise OSError(f\\"An error occurred while reading the file: {filename}\\") from os_error"},{"question":"Coding Assessment Question # Objective You are tasked to implement a custom metric handler using the PyTorch distributed elastic metrics API. This handler will extend the functionality of the `MetricHandler` class. # Problem Description Implement a class `CustomMetricHandler` that extends the `MetricHandler` class. This custom handler will collect and manage several metrics, compute their averages, and log them to the console. # Requirements 1. **Class Definition**: Define a class `CustomMetricHandler` that inherits from `MetricHandler`. 2. **Methods**: - **__init__**: Initializes the handler with an empty dictionary to store metric values. - **put_metric(name: str, value: float)**: Stores the provided metric value under the given name. - **compute_averages() -> dict**: Computes the average of each metric value stored in the handler and returns a dictionary with metric names as keys and their averages as values. - **log_metrics()**: Logs each metric and its average to the console. # Constraints - Metrics can have the same name, in which case their values should be averaged. - Your solution should be optimized to handle large numbers of metric entries efficiently. # Example ```python # Example usage of CustomMetricHandler handler = CustomMetricHandler() handler.put_metric(\\"accuracy\\", 0.9) handler.put_metric(\\"accuracy\\", 0.95) handler.put_metric(\\"loss\\", 0.1) handler.put_metric(\\"loss\\", 0.15) averages = handler.compute_averages() print(averages) # Output should be something like: {\'accuracy\': 0.925, \'loss\': 0.125} handler.log_metrics() # Console output should be: # accuracy: 0.925 # loss: 0.125 ``` # Evaluation Your submission will be evaluated based on the following criteria: - Correctness: The implementation should meet all specified requirements. - Efficiency: The methods should handle large input sizes efficiently. - Code Quality: Well-organized and readable code with appropriate comments and documentation. # Tip Refer to the PyTorch distributed elastic metrics API documentation to understand how to extend and use `MetricHandler`.","solution":"class MetricHandler: pass # This is a placeholder for the actual PyTorch distributed elastic MetricHandler class. class CustomMetricHandler(MetricHandler): def __init__(self): super().__init__() self.metrics = {} def put_metric(self, name: str, value: float): if name not in self.metrics: self.metrics[name] = [] self.metrics[name].append(value) def compute_averages(self) -> dict: averages = {} for name, values in self.metrics.items(): averages[name] = sum(values) / len(values) return averages def log_metrics(self): averages = self.compute_averages() for name, average in averages.items(): print(f\\"{name}: {average}\\")"},{"question":"**Question:** # Automating Built Distribution Creation with Python You are tasked with automating the creation of different built distribution formats for a Python package. Your goal is to write a Python script that uses the `subprocess` module to automate running the `setup.py` script with the appropriate `bdist` commands to generate various distribution formats. Your script should be able to: 1. Create a gzipped tar file (\\".tar.gz\\") 2. Create a bzipped tar file (\\".tar.bz2\\") 3. Create a zip file (\\".zip\\") 4. Handle errors gracefully by printing appropriate error messages if any command fails. # Instructions: 1. Implement a function `create_distributions` that accepts a path to the `setup.py` file and a list of formats to create. 2. The function should loop through the list of formats and run the appropriate `bdist` command for each format using the `subprocess` module. 3. If a command fails, the function should catch the exception and print an error message specifying which format failed. # Input: - `setup_path` (str): The path to the `setup.py` file. - `formats` (list): A list of distribution formats to create (e.g., `[\\"gztar\\", \\"bztar\\", \\"zip\\"]`). # Output: - The function should not return anything but should print success or error messages for each format. # Example Usage: ```python def create_distributions(setup_path, formats): # Your code here # Example call create_distributions(\\"path/to/setup.py\\", [\\"gztar\\", \\"bztar\\", \\"zip\\"]) ``` # Constraints: - Assume the `setup.py` file exists at the specified path and is correctly formatted. - Handle possible exceptions during command execution and print informative error messages. # Example Output: ``` Creating gztar distribution... Success Creating bztar distribution... Success Creating zip distribution... Success ``` If an error occurs: ``` Creating zip distribution... Failed: [Error message] ``` # Notes: - You may use the `subprocess.run` function for running shell commands from within your Python script. - Ensure that your script is cross-platform and works on both Unix-like and Windows systems.","solution":"import subprocess def create_distributions(setup_path, formats): for fmt in formats: try: print(f\\"Creating {fmt} distribution...\\", end=\' \') result = subprocess.run( [\\"python\\", setup_path, \\"sdist\\", \\"--formats\\", fmt], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True ) print(\\"Success\\") except subprocess.CalledProcessError as e: print(f\\"Failed: {e.stderr}\\") except Exception as e: print(f\\"Failed: {str(e)}\\")"},{"question":"You are given the task of implementing a custom file reader utility that utilizes the \\"linecache\\" module to efficiently get, verify, and manage lines from a text file. This utility will help users read specific lines from large files without loading the entire file into memory and provide functionality to refresh the cached contents when files are updated. # Function Specifications: 1. **get_line_from_file(file_path: str, line_number: int) -> str** - **Input**: - `file_path` (str): The path to the file from which the line needs to be read. - `line_number` (int): The line number to retrieve. - **Output**: - Returns the content of the specified line as a string. If the line does not exist or an error occurs, return an empty string. 2. **refresh_file_cache(file_path: str) -> None** - **Input**: - `file_path` (str): The path to the file for which the cache needs to be refreshed. - **Output**: - None. This function will refresh the cached content for the specified file. 3. **clear_all_cache() -> None** - **Input**: - None. - **Output**: - None. This function will clear all cached content. # Example Usage: ```python # Assume there is a file `sample.txt` with the following content: # 1. Hello World # 2. This is a test file. # 3. Python is great for scripting. line_content = get_line_from_file(\'sample.txt\', 2) print(line_content) # Output: \\"This is a test file.\\" refresh_file_cache(\'sample.txt\') clear_all_cache() ``` # Constraints: - Implement caching mechanisms effectively to ensure repeated access to the same lines is efficient. - Handle cases where the requested line number is out of bounds or the file path is invalid. - Ensure the functions do not raise exceptions under typical usage conditions.","solution":"import linecache def get_line_from_file(file_path: str, line_number: int) -> str: Returns the specified line from the given file. If the line does not exist or an error occurs, returns an empty string. try: line = linecache.getline(file_path, line_number) return line if line else \\"\\" except Exception: return \\"\\" def refresh_file_cache(file_path: str) -> None: Refreshes the cache for the specified file. linecache.checkcache(file_path) def clear_all_cache() -> None: Clears the cache for all files. linecache.clearcache()"},{"question":"**Objective:** Implement and demonstrate the usage of various PyTorch tensor view operations, along with handling non-contiguous tensors and enforcing contiguity using `.contiguous()` method. This task will require the students to manipulate tensors in ways that efficiently use memory and computation. **Problem Statement:** You are given a PyTorch tensor `X` of shape `(4, 4)` filled with random values. Your task is to perform the following steps: 1. Create a view of `X` that reshapes it to shape `(2, 8)` and name it `X_view`. 2. Swap the first two dimensions of `X` using a view operation and name it `X_swapped`. 3. Check and print whether `X_swapped` is contiguous. 4. If `X_swapped` is not contiguous, convert it to a contiguous tensor named `X_contiguous`. 5. Print the underlying data pointer of the original tensor `X`, `X_view`, `X_swapped`, and `X_contiguous` to verify data sharing. 6. Modify a specific element in `X_view` (set the element at index `[1, 0]` to 10) and show that the change is reflected in the original tensor `X`. 7. Modify an element in `X_contiguous` (set the element at index `[0, 0]` to 20) and verify it does not affect the original tensor `X` or `X_swapped`. **Input and Output:** - You do not need to write code for input as the tensor `X` will be randomly generated in the code itself. - Your code should output: - The tensor `X` after initializing. - The tensor `X_view`. - The tensor `X_swapped`. - Whether `X_swapped` is contiguous. - The tensor `X_contiguous` (if applicable). - The data pointers of `X`, `X_view`, `X_swapped`, and `X_contiguous`. - The modified tensor `X` after changing `X_view`. - The modified tensor `X_contiguous`. ```python import torch def tensor_view_operations(): # Step 1: Create original tensor X X = torch.rand(4, 4) print(\\"Original tensor X:n\\", X) # Step 1: Create a view of X reshaped to (2, 8) X_view = X.view(2, 8) print(\\"nView of X reshaped to (2, 8):n\\", X_view) # Step 2: Swap the first two dimensions of X using view operation X_swapped = X.transpose(0, 1) print(\\"nX with swapped dimensions:n\\", X_swapped) # Step 3: Check if X_swapped is contiguous is_contiguous = X_swapped.is_contiguous() print(\\"nIs X_swapped contiguous?:\\", is_contiguous) # Step 4: If not contiguous, make it contiguous if not is_contiguous: X_contiguous = X_swapped.contiguous() else: X_contiguous = X_swapped print(\\"nContiguous version of X_swapped:n\\", X_contiguous) # Step 5: Print data pointers for verification print(\\"nData pointer of original X:\\", X.storage().data_ptr()) print(\\"Data pointer of X_view:\\", X_view.storage().data_ptr()) print(\\"Data pointer of X_swapped:\\", X_swapped.storage().data_ptr()) print(\\"Data pointer of X_contiguous:\\", X_contiguous.storage().data_ptr()) # Step 6: Modify an element in X_view and show it reflects in X X_view[1, 0] = 10 print(\\"nModified X_view (element [1, 0] set to 10):n\\", X_view) print(\\"Original tensor X after modifying X_view:n\\", X) # Step 7: Modify an element in X_contiguous and verify it does not affect X X_contiguous[0, 0] = 20 print(\\"nModified X_contiguous (element [0, 0] set to 20):n\\", X_contiguous) print(\\"Original tensor X after modifying X_contiguous:n\\", X) print(\\"X_swapped after modifying X_contiguous:n\\", X_swapped) # Execute the function tensor_view_operations() ``` **Constraints:** - Ensure to handle tensors and views appropriately without unnecessary copies. - Use provided view operations to demonstrate understanding of tensor manipulation in PyTorch. - The original tensor `X` should not be modified directly except via designed view operations. Good luck, and make sure to test your code thoroughly!","solution":"import torch def tensor_view_operations(): # Step 1: Create original tensor X X = torch.rand(4, 4) print(\\"Original tensor X:n\\", X) # Step 1: Create a view of X reshaped to (2, 8) X_view = X.view(2, 8) print(\\"nView of X reshaped to (2, 8):n\\", X_view) # Step 2: Swap the first two dimensions of X using view operation X_swapped = X.transpose(0, 1) print(\\"nX with swapped dimensions:n\\", X_swapped) # Step 3: Check if X_swapped is contiguous is_contiguous = X_swapped.is_contiguous() print(\\"nIs X_swapped contiguous?:\\", is_contiguous) # Step 4: If not contiguous, make it contiguous if not is_contiguous: X_contiguous = X_swapped.contiguous() else: X_contiguous = X_swapped print(\\"nContiguous version of X_swapped:n\\", X_contiguous) # Step 5: Print data pointers for verification print(\\"nData pointer of original X:\\", X.storage().data_ptr()) print(\\"Data pointer of X_view:\\", X_view.storage().data_ptr()) print(\\"Data pointer of X_swapped:\\", X_swapped.storage().data_ptr()) print(\\"Data pointer of X_contiguous:\\", X_contiguous.storage().data_ptr()) # Step 6: Modify an element in X_view and show it reflects in X X_view[1, 0] = 10 print(\\"nModified X_view (element [1, 0] set to 10):n\\", X_view) print(\\"Original tensor X after modifying X_view:n\\", X) # Step 7: Modify an element in X_contiguous and verify it does not affect X X_contiguous[0, 0] = 20 print(\\"nModified X_contiguous (element [0, 0] set to 20):n\\", X_contiguous) print(\\"Original tensor X after modifying X_contiguous:n\\", X) print(\\"X_swapped after modifying X_contiguous:n\\", X_swapped) # Execute the function tensor_view_operations()"},{"question":"# IP Address Manipulation using the `ipaddress` Module Problem Statement You are provided a list of IP address strings (both IPv4 and IPv6) and a list of CIDR notation network strings (both IPv4 and IPv6). Your task is to implement a function `filter_hosts_in_networks(ip_addresses, networks)` that returns a dictionary. The dictionary should map each network string to a list of IP addresses that belong to that network. Function Signature ```python def filter_hosts_in_networks(ip_addresses: list[str], networks: list[str]) -> dict[str, list[str]]: pass ``` Input - `ip_addresses`: A list of strings, each representing an IP address (either IPv4 or IPv6). - `networks`: A list of strings, each representing a network in CIDR notation (either IPv4 or IPv6). Output - A dictionary where: - The keys are network strings from the input list. - The values are lists of IP address strings that belong to the respective network. Constraints - All input strings are valid IP addresses or networks. - The input lists will not be empty. - The IP addresses and networks can be either IPv4 or IPv6. Example ```python ip_addresses = [\'192.0.2.1\', \'192.0.3.1\', \'2001:db8::1\', \'2001:db8::2\'] networks = [\'192.0.2.0/24\', \'2001:db8::/96\'] result = filter_hosts_in_networks(ip_addresses, networks) print(result) # Output: # { # \'192.0.2.0/24\': [\'192.0.2.1\'], # \'2001:db8::/96\': [\'2001:db8::1\', \'2001:db8::2\'] # } ``` Instructions 1. Use the `ipaddress` module to create and manipulate IP address and network objects. 2. Ensure that the function efficiently checks IP addresses for membership in each network. 3. Return the result as a dictionary mapping network strings to lists of IP addresses within those networks. Hints - Use the `ipaddress.ip_address()` and `ipaddress.ip_network()` functions to create IP address and network objects. - The membership test (`in`) can be used to check if an IP address belongs to a network.","solution":"import ipaddress def filter_hosts_in_networks(ip_addresses, networks): result = {} for network in networks: net = ipaddress.ip_network(network) result[network] = [ip for ip in ip_addresses if ipaddress.ip_address(ip) in net] return result"},{"question":"**Coding Assessment Question** **Objective:** Implement a class that manages resources and ensures their cleanup using the `atexit` module. You will be required to demonstrate knowledge of registering and unregistering exit handlers, and handling these with arguments and decorators. **Problem Statement:** Create a class `ResourceManager` that: 1. Initializes with a resource identifier (e.g., `resource_id`). 2. Provides a method `use_resource` which simulates using the resource by printing a message indicating the resource is in use. 3. Ensures that cleanup tasks are performed upon program termination, regardless of how the class instance was used. 4. Registers multiple cleanup functions in different manners (direct call, with arguments, as a decorator). 5. Allows deregistration of specific cleanup functions. **Specifications:** - `ResourceManager` should register at least three cleanup functions: - One registered directly via `atexit.register()`. - One registered with arguments. - One registered using `atexit.register` as a decorator. - Cleanup functions should print a message indicating their execution order and resource ID. - Demonstrate that functions execute in the correct reverse order when the program terminates. - Include error handling for any exceptions raised within the cleanup functions, ensuring the last raised exception is re-raised. **Example Usage:** ```python import atexit class ResourceManager: def __init__(self, resource_id): self.resource_id = resource_id atexit.register(self.cleanup_direct) atexit.register(self.cleanup_with_args, \\"Custom cleanup\\") self.deferred_cleanup() def use_resource(self): print(f\\"Resource {self.resource_id} is now in use.\\") def cleanup_direct(self): print(f\\"Direct cleanup for resource {self.resource_id}\\") def cleanup_with_args(self, message): print(f\\"{message} for resource {self.resource_id}\\") @atexit.register def deferred_cleanup(self): print(f\\"Deferred cleanup for resource {self.resource_id}\\") # Example of using the ResourceManager resource_manager = ResourceManager(\\"Resource-1\\") resource_manager.use_resource() # Unregister the direct cleanup function atexit.unregister(resource_manager.cleanup_direct) # Run other parts of the program... ``` **Expected Output Upon Program Termination:** ``` Resource Resource-1 is now in use. Custom cleanup for resource Resource-1 Deferred cleanup for resource Resource-1 ``` If you unregister the direct cleanup function (`cleanup_direct`), it should not appear in the output upon program termination. **Constraints:** - Ensure that all class methods and cleanup handlers properly handle exceptions. - Test the program to confirm the correct order of cleanup function execution. - Provide clear docstrings for each method describing its purpose and usage. **Evaluation Criteria:** - Correct implementation of class and methods. - Proper use of `atexit` module, including registration and unregistration of functions. - Correct handling of cleanup order. - Clear and descriptive output indicating the sequence of cleanup operations. - Robust error handling within cleanup functions.","solution":"import atexit class ResourceManager: def __init__(self, resource_id): Initializes the ResourceManager with a given resource_id and registers cleanup functions. Args: - resource_id (str): The identifier for the resource. self.resource_id = resource_id self.cleanup_direct_registered = True # Registering cleanup handlers atexit.register(self.cleanup_direct) atexit.register(self.cleanup_with_args, \\"Custom cleanup\\") # Deferred registration with a decorator @atexit.register def deferred_cleanup(): print(f\\"Deferred cleanup for resource {self.resource_id}\\") self.deferred_cleanup = deferred_cleanup def use_resource(self): Simulates using the resource. print(f\\"Resource {self.resource_id} is now in use.\\") def cleanup_direct(self): Direct cleanup function. print(f\\"Direct cleanup for resource {self.resource_id}\\") def cleanup_with_args(self, message): Cleanup function with arguments. Args: - message (str): The message to print during cleanup. print(f\\"{message} for resource {self.resource_id}\\") def unregister_cleanup_direct(self): Unregisters the direct cleanup function. if self.cleanup_direct_registered: atexit.unregister(self.cleanup_direct) self.cleanup_direct_registered = False # Example Usage resource_manager = ResourceManager(\\"Resource-1\\") resource_manager.use_resource() resource_manager.unregister_cleanup_direct()"},{"question":"# Advanced Coding Assessment Question: Functional Programming with Python **Objective:** Your task is to implement a function that processes a list of integers based on a series of operations defined using the `itertools`, `functools`, and `operator` modules. The function, `process_integers`, will take a list of integers and perform the following steps: 1. **Filter**: Only keep even numbers from the list. 2. **Transform**: Raise each remaining number to the power of 2. 3. **Accumulate**: Compute the cumulative sum of these transformed numbers. # Specifications: Write a function `process_integers(numbers: List[int]) -> List[int]` that: - **Input**: - `numbers`: A list of integers where `1 <= len(numbers) <= 10^6` and `-10^6 <= numbers[i] <= 10^6`. - **Output**: - A list of integers representing the cumulative sum after filtering and transforming the input list. - **Constraints**: - Your function should handle the sequence efficiently, using appropriate functional programming techniques from the `itertools`, `functools`, and `operator` modules. - **Performance**: - The solution should be optimal in both time and space complexity. Example: ```python assert process_integers([1, 2, 3, 4, 5]) == [4, 20] assert process_integers([]) == [] assert process_integers([2, 4, 6]) == [4, 20, 56] ``` Tips: - Use `itertools` to help with efficient looping and accumulation. - Consider `functools.partial` for customized function definitions. - Utilize functions from the `operator` module to perform arithmetic and logical operations seamlessly. # Implementation: ```python from typing import List import itertools import operator def process_integers(numbers: List[int]) -> List[int]: # Step 1: Filter even numbers using itertools filtered = filter(lambda x: x % 2 == 0, numbers) # Step 2: Transform numbers by raising them to the power of 2 transformed = map(lambda x: x ** 2, filtered) # Step 3: Accumulate the sum of these numbers accumulated = itertools.accumulate(transformed, operator.add) # Return the result as a list return list(accumulated) ``` # Notes: - Make sure your implementation can handle large inputs up to the size specified. - Focus on using the functional programming modules as described to demonstrate your understanding of these tools.","solution":"from typing import List import itertools import operator def process_integers(numbers: List[int]) -> List[int]: Processes a list of integers by keeping even numbers, squaring them, and then returning their cumulative sum. # Step 1: Filter even numbers using itertools.filterfalse filtered = filter(lambda x: x % 2 == 0, numbers) # Step 2: Transform numbers by raising them to the power of 2 transformed = map(lambda x: x ** 2, filtered) # Step 3: Accumulate the sum of these numbers accumulated = itertools.accumulate(transformed, operator.add) # Return the result as a list return list(accumulated)"},{"question":"# Question: Advanced Asynchronous Programming with asyncio Future You are tasked with developing a small Python program to simulate a series of asynchronous operations using the `asyncio` library\'s `Future` objects. This question assesses your understanding of creating, manipulating, and handling Future objects. Pay careful attention to how asyncio.Future interacts within the event loop and the use of callbacks. Part 1: Future Object Initialization and Manipulation 1. Create an asynchronous function `execute_after_delay(fut, delay, value)` that: - Accepts a Future object `fut`, a delay duration in seconds `delay`, and a result `value`. - Sleeps asynchronously for the given delay using `asyncio.sleep()`. - Once the sleep elapses, sets the `value` as the result of the Future object `fut`. 2. Create an asynchronous function `main()` that: - Obtains the current running event loop. - Creates a Future object using the loop. - Executes the `execute_after_delay` function in a parallel task to set the Future result after a delay of 2 seconds. - Prints \\"Operation started...\\" - Awaits the Future object and prints the result once it is available. 3. Implement the `add_done_callback` method to attach a callback function that prints \\"Future is done!\\" when the Future completes. Part 2: Ensuring Future and Exception Handling 1. Write a function `ensure_future_example(coro)` that: - Ensures the given coroutine `coro` is properly scheduled as a Task. - Uses `asyncio.ensure_future` and handles cases where the provided coroutine is invalid (raise an appropriate error). 2. Create an asynchronous function `main_with_exception_handling` that: - Uses `ensure_future_example` to schedule the `execute_after_delay` function with a delay of 1 second and a result \\"Hello, asyncio!\\". - Awaits the result and handles any potential exceptions from the Future (e.g., checking for cancellation or invalid state). - Prints the awaited result. # Constraints - You must use the methods and functionalities described in the asyncio Future documentation. - Your solution should not use deprecated functionalities in a way that emits deprecation warnings. # Expected Input and Output Your script should output the following when run: 1. \\"Operation started...\\" 2. After 2 seconds, the result of the Future (\\"... world\\") is printed. 3. \\"Future is done!\\" is printed after the Future completes. 4. \\"Hello, asyncio!\\" after 1 second in the `main_with_exception_handling` function. # Submission Provide the complete script implementing the described functionalities. Ensure that your functions are thoroughly tested and handle any potential edge cases. ```python # Your implementation here import asyncio async def execute_after_delay(fut, delay, value): # Implement this function pass async def main(): # Implement this function pass def ensure_future_example(coro): # Implement this function pass async def main_with_exception_handling(): # Implement this function pass # To run the script, use: asyncio.run(main_with_exception_handling()) ```","solution":"import asyncio async def execute_after_delay(fut, delay, value): await asyncio.sleep(delay) fut.set_result(value) async def main(): loop = asyncio.get_event_loop() fut = loop.create_future() # Execute the delay function asynchronously asyncio.create_task(execute_after_delay(fut, 2, \\"... world\\")) # Set a callback for when the future is done fut.add_done_callback(lambda _: print(\\"Future is done!\\")) print(\\"Operation started...\\") result = await fut print(result) def ensure_future_example(coro): Ensures the provided coroutine or awaitable is scheduled as a Task. Raises a ValueError if the provided argument is not a valid coroutine. if not asyncio.iscoroutine(coro): raise ValueError(\\"The argument must be a coroutine\\") return asyncio.ensure_future(coro) async def main_with_exception_handling(): try: coro = execute_after_delay(asyncio.Future(), 1, \\"Hello, asyncio!\\") future = ensure_future_example(coro) result = await future print(result) except asyncio.CancelledError: print(\\"The Future was cancelled.\\") except Exception as e: print(f\\"An error occurred: {e}\\") # To run the script, use: asyncio.run(main())"},{"question":"**Question: Implement an Advanced Generic Function Using Python\'s `typing` Module** **Objective:** Write a Python function that uses advanced features of the `typing` module, including type hints, generics, and special directives. The function will perform operations on a generic list and return results based on type-specific operations. **Function Signature:** ```python from typing import TypeVar, Generic, List, Callable, Any T = TypeVar(\'T\') E = TypeVar(\'E\') class ListTransformer(Generic[T]): def __init__(self, transformations: List[Callable[[T], E]]) -> None: self.transformations = transformations def apply_transformations(self, data: List[T]) -> List[Any]: pass def example_function(data: List[int]) -> List[str]: # Instantiate the ListTransformer with transformations to convert int to str and str to int transformer = ListTransformer[int]([ lambda x: str(x), # Convert int to str ]) return transformer.apply_transformations(data) ``` **Requirements:** 1. **Initialization (`__init__` method):** - The `ListTransformer` class should accept a list of transformation functions (`transformations`), where each function maps a type `T` to another type `E`. 2. **Transformation Application (`apply_transformations` method):** - This method should apply each transformation function in `self.transformations` sequentially to the input list `data`. - The resultant list should be returned after all transformations have been applied. 3. **Example Function:** - The `example_function` demonstrates using the `ListTransformer` for converting a list of integers to their string representations. **Constraints:** - You must use the `typing` module\'s type hints to define input and output types for the class and the method. - Consider edge cases such as empty input lists and lists with non-integer types. **Input:** - `data`: A list of integers. **Output:** - A list of strings after transformations have been applied. ```python # Example Usage: data_list = [1, 2, 3, 4] result = example_function(data_list) print(result) # Output should be [\'1\', \'2\', \'3\', \'4\'] ``` **Notes:** - The solution should be generic enough to handle different types and transformations based on the input and type hints provided. - Focus on using Python\'s `typing` module effectively to indicate type variables and constraints.","solution":"from typing import TypeVar, Generic, List, Callable, Any T = TypeVar(\'T\') E = TypeVar(\'E\') class ListTransformer(Generic[T]): def __init__(self, transformations: List[Callable[[T], E]]) -> None: self.transformations = transformations def apply_transformations(self, data: List[T]) -> List[Any]: transformed_data = data for transformation in self.transformations: transformed_data = list(map(transformation, transformed_data)) return transformed_data def example_function(data: List[int]) -> List[str]: # Instantiate the ListTransformer with transformations to convert int to str transformer = ListTransformer[int]([ lambda x: str(x), # Convert int to str ]) return transformer.apply_transformations(data)"},{"question":"**Objective**: Implement a Python function that analyzes Python source code to extract and display specific information about the code structure and components using the Abstract Syntax Tree (AST) module and the tokenizer. **Question**: Write a function `analyze_code_structure(source_code: str) -> dict` that takes a string of Python source code as input and returns a dictionary containing the following information: 1. **Number of Functions**: Count of function definitions in the code. 2. **Number of Classes**: Count of class definitions in the code. 3. **Number of Imports**: Count of import statements in the code. # Input: - `source_code`: A string representing the Python source code to analyze. # Output: - A dictionary with three keys: `\\"functions\\"`, `\\"classes\\"`, and `\\"imports\\"`, containing the respective counts from the source code. # Constraints: - The function should handle valid Python 3.10 source code. - You may assume that the input code is correctly formatted and does not contain syntax errors. # Example: ```python source_code = \'\'\' import os import sys class MyClass: def method(self): pass def my_function(): pass \'\'\' result = analyze_code_structure(source_code) print(result) # Output: {\'functions\': 1, \'classes\': 1, \'imports\': 2} ``` # Hints: - Use the `ast` module to parse the source code and navigate the AST nodes to count function and class definitions [Refer to Node classes]. - Use the `tokenize` module to count the number of import statements.","solution":"import ast import tokenize from io import StringIO def analyze_code_structure(source_code: str) -> dict: Analyze the given Python source code to count the number of function definitions, class definitions, and import statements. Args: - source_code (str): The Python source code to analyze. Returns: - dict: A dictionary with counts of functions, classes, and imports. # Parse the source code into an AST tree = ast.parse(source_code) # Initialize counts function_count = 0 class_count = 0 import_count = 0 # Count functions and classes using AST traversal for node in ast.walk(tree): if isinstance(node, ast.FunctionDef): function_count += 1 elif isinstance(node, ast.ClassDef): class_count += 1 # Count import statements using the tokenize module tokens = tokenize.generate_tokens(StringIO(source_code).readline) for token in tokens: if token.type == tokenize.NAME and token.string in {\'import\', \'from\'}: import_count += 1 return { \'functions\': function_count, \'classes\': class_count, \'imports\': import_count }"},{"question":"You are given a dataset containing information about penguins. Your task is to create visualizations using the seaborn library, demonstrating your understanding of creating complex histograms and manipulating legend positions. Task Requirements: 1. **Load the Dataset**: Use the Seaborn library to load the \\"penguins\\" dataset. 2. **Create a Histogram**: - Generate a histogram for the \\"bill_length_mm\\" column, grouped by the \\"species\\" column. - The legend should be moved to the \\"center right\\" of the plot. 3. **Create a Faceted Plot**: - Generate faceted histograms of the \\"bill_length_mm\\" column, separated by the \\"island\\" column. - Configure the facets to have 2 columns and a height of 3. - Move the legend to the \\"upper left\\" of the plot area, with a custom `bbox_to_anchor` to ensure it does not overlap the plots. - Ensure the legend is within each facet plot (`legend_out=False`) and frame visibility (`frameon=False`). Input and Output Formats: - **Input**: You do not need to handle any input as the dataset is loaded directly using Seaborn\'s built-in method. - **Output**: Generate the required plots and ensure that they are displayed correctly with the specified configurations. Constraints: - Use only the seaborn library and standard Python libraries for this task. - Your solution should be efficient and clear, taking advantage of Seaborn\'s plotting capabilities. Example Code to Get Started: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a histogram for bill_length_mm grouped by species ax1 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax1, \\"center right\\") # Task 2: Create facet histograms by island g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False)) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.55, .45), frameon=False) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a histogram for bill_length_mm grouped by species def create_histogram(): ax = sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") ax.legend(loc=\'center right\') plt.show() # Task 2: Create facet histograms by island def create_faceted_histogram(): g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False)) g.set_axis_labels(\\"Bill Length (mm)\\", \\"Count\\") for ax in g.axes.flat: ax.legend(loc=\'upper left\', bbox_to_anchor=(1, 1), frameon=False) plt.show()"},{"question":"# Assess Your Understanding of `http.client` in Python Objective: Create a Python function using the `http.client` module to perform a series of HTTP operations such as GET, POST, and handling responses. Task: Write a function `perform_http_operations` that performs the following steps using the `http.client` module: 1. **Perform a GET request:** - Connect to `www.python.org` and request the homepage. - Retrieve the response status, reason, and headers. - Read and print the first 500 bytes of the response content. 2. **Perform a POST request:** - Connect to a dummy server for submission (`httpbin.org/post`). - Send a POST request with the following data: `{\'name\': \'John\', \'age\': \'30\'}` and appropriate headers. - Retrieve the response status, reason, and headers. - Read and print the entire response content. 3. **Handle Connection Errors Gracefully:** - Add error handling to manage potential exceptions such as `http.client.HTTPException`, `ConnectionRefusedError`, and other relevant exceptions. - Print appropriate error messages based on the type of exception. Input and Output: - **Input:** - No user input is expected. - **Output:** - Print statuses, reasons, headers, and response contents for both GET and POST requests. - Print error messages in case of exceptions. # Function Signature: ```python def perform_http_operations(): pass ``` # Constraints: - You must use the `http.client` module for handling the HTTP connections and requests. - Ensure that the function handles exceptions appropriately and provides meaningful error messages. Example Execution: ```python perform_http_operations() ``` **Sample Output:** ``` GET Request to www.python.org: Status: 200 Reason: OK Headers: [(\'Content-Type\', \'text/html; charset=utf-8\'), ...] First 500 bytes of content: b\'<!doctype html><html ...\' POST Request to httpbin.org/post: Status: 200 Reason: OK Headers: [(\'Content-Type\', \'application/json; charset=utf-8\'), ...] Response content: b\'{\\"args\\":{}, \\"data\\":\\"{\\"name\\": \\"John\\", \\"age\\": \\"30\\"}\\", \\"files\\": {}, ...}\' [Exception handling output if applicable here...] ``` This question will test the student\'s ability to: - Use the `http.client` module effectively. - Perform different types of HTTP requests. - Handle and parse HTTP responses. - Implement error handling for network-related exceptions.","solution":"import http.client import json def perform_http_operations(): try: # Perform GET request conn = http.client.HTTPSConnection(\\"www.python.org\\") conn.request(\\"GET\\", \\"/\\") response = conn.getresponse() print(\\"GET Request to www.python.org:\\") print(f\\"Status: {response.status}\\") print(f\\"Reason: {response.reason}\\") print(f\\"Headers: {response.getheaders()}\\") content = response.read(500) print(f\\"First 500 bytes of content: {content[:500]}\\") conn.close() # Perform POST request post_conn = http.client.HTTPSConnection(\\"httpbin.org\\") headers = {\'Content-type\': \'application/json\'} data = json.dumps({\'name\': \'John\', \'age\': \'30\'}) post_conn.request(\\"POST\\", \\"/post\\", body=data, headers=headers) post_response = post_conn.getresponse() print(\\"nPOST Request to httpbin.org/post:\\") print(f\\"Status: {post_response.status}\\") print(f\\"Reason: {post_response.reason}\\") print(f\\"Headers: {post_response.getheaders()}\\") post_content = post_response.read() print(f\\"Response content: {post_content}\\") post_conn.close() except http.client.HTTPException as e: print(f\\"HTTPException occurred: {e}\\") except ConnectionRefusedError: print(\\"Error: Connection was refused by the server.\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Perform the HTTP operations perform_http_operations()"},{"question":"# XML Parsing with `xml.sax.handler.ContentHandler` **Objective:** Implement a custom SAX content handler to parse an XML document and extract relevant information. # Problem Statement: An E-commerce website exports order data in XML format. You are required to write a Python script to parse this XML data using a custom SAX `ContentHandler` to extract details of all the orders, including the order ID, customer information, product details, and total amount. # XML Format: The order data is structured as follows: ```xml <orders> <order id=\\"1\\"> <customer> <name>John Doe</name> <email>john@example.com</email> </customer> <product> <name>Smartphone</name> <price>299.99</price> </product> <total>299.99</total> </order> <order id=\\"2\\"> <customer> <name>Jane Doe</name> <email>jane@example.com</email> </customer> <product> <name>Laptop</name> <price>799.99</price> </product> <total>799.99</total> </order> </orders> ``` # Task: 1. Implement a custom class `OrderContentHandler` that inherits from `xml.sax.handler.ContentHandler`. 2. Define methods to handle the following events: - `startElement(self, name, attrs)`: Capture the start of an element. - `endElement(self, name)`: Capture the end of an element. - `characters(self, content)`: Capture the text content within elements. 3. Extract the order ID, customer name, customer email, product name, product price, and order total from the XML. 4. Print a summary of all orders showing the extracted details. # Input and Output: - **Input:** XML content (a string containing the XML data). - **Output:** Printed summary of orders. # Example: **Input:** ```xml <orders> <order id=\\"1\\"> <customer> <name>John Doe</name> <email>john@example.com</email> </customer> <product> <name>Smartphone</name> <price>299.99</price> </product> <total>299.99</total> </order> </orders> ``` **Output:** ``` Order ID: 1 Customer Name: John Doe Customer Email: john@example.com Product Name: Smartphone Product Price: 299.99 Order Total: 299.99 ``` # Constraints: - The XML content structure will be valid and follow the given format. - The XML content will contain at least one `<order>` element. # Performance Requirements: - The solution should efficiently parse the XML document using SAX parsing. - The event-driven nature of SAX should be leveraged to handle large XML documents without loading the entire document into memory. # Code Template: ```python import xml.sax class OrderContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_data = \\"\\" self.order_id = \\"\\" self.name = \\"\\" self.email = \\"\\" self.product_name = \\"\\" self.product_price = \\"\\" self.total = \\"\\" self.orders = [] def startElement(self, name, attrs): self.current_data = name if name == \\"order\\": self.order_id = attrs[\\"id\\"] def endElement(self, name): if name == \\"order\\": order_summary = (self.order_id, self.name, self.email, self.product_name, self.product_price, self.total) self.orders.append(order_summary) self.order_id = \\"\\" self.name = \\"\\" self.email = \\"\\" self.product_name = \\"\\" self.product_price = \\"\\" self.total = \\"\\" def characters(self, content): if self.current_data == \\"name\\": self.name = content elif self.current_data == \\"email\\": self.email = content elif self.current_data == \\"product\\": self.product_name = content elif self.current_data == \\"price\\": self.product_price = content elif self.current_data == \\"total\\": self.total = content def print_orders(self): for order in self.orders: print(f\\"Order ID: {order[0]}\\") print(f\\"Customer Name: {order[1]}\\") print(f\\"Customer Email: {order[2]}\\") print(f\\"Product Name: {order[3]}\\") print(f\\"Product Price: {order[4]}\\") print(f\\"Order Total: {order[5]}\\") print(\\"\\") # Example usage: xml_content = <orders> <order id=\\"1\\"> <customer> <name>John Doe</name> <email>john@example.com</email> </customer> <product> <name>Smartphone</name> <price>299.99</price> </product> <total>299.99</total> </order> </orders> parser = xml.sax.make_parser() handler = OrderContentHandler() parser.setContentHandler(handler) xml.sax.parseString(xml_content, handler) handler.print_orders() ``` Ensure your implementation can handle more complex and larger XML inputs correctly!","solution":"import xml.sax class OrderContentHandler(xml.sax.handler.ContentHandler): def __init__(self): super().__init__() self.current_data = \\"\\" self.buffer = \\"\\" self.order_id = \\"\\" self.name = \\"\\" self.email = \\"\\" self.product_name = \\"\\" self.product_price = \\"\\" self.total = \\"\\" self.orders = [] def startElement(self, name, attrs): self.current_data = name self.buffer = \\"\\" if name == \\"order\\": self.order_id = attrs[\\"id\\"] def endElement(self, name): if name == \\"name\\" and self.current_data == \\"name\\": if self.buffer.strip(): # Adding this check to avoid capturing empty text nodes if not self.name: self.name = self.buffer.strip() else: self.product_name = self.buffer.strip() elif name == \\"email\\": self.email = self.buffer.strip() elif name == \\"price\\": self.product_price = self.buffer.strip() elif name == \\"total\\": self.total = self.buffer.strip() elif name == \\"order\\": order_summary = { \\"order_id\\": self.order_id, \\"customer_name\\": self.name, \\"customer_email\\": self.email, \\"product_name\\": self.product_name, \\"product_price\\": self.product_price, \\"order_total\\": self.total } self.orders.append(order_summary) self.order_id = \\"\\" self.name = \\"\\" self.email = \\"\\" self.product_name = \\"\\" self.product_price = \\"\\" self.total = \\"\\" self.current_data = \\"\\" self.buffer = \\"\\" def characters(self, content): self.buffer += content def print_orders(self): for order in self.orders: print(f\\"Order ID: {order[\'order_id\']}\\") print(f\\"Customer Name: {order[\'customer_name\']}\\") print(f\\"Customer Email: {order[\'customer_email\']}\\") print(f\\"Product Name: {order[\'product_name\']}\\") print(f\\"Product Price: {order[\'product_price\']}\\") print(f\\"Order Total: {order[\'order_total\']}\\") print(\\"\\") # Example usage: xml_content = <orders> <order id=\\"1\\"> <customer> <name>John Doe</name> <email>john@example.com</email> </customer> <product> <name>Smartphone</name> <price>299.99</price> </product> <total>299.99</total> </order> </orders> def parse_orders(xml_content): parser = xml.sax.make_parser() handler = OrderContentHandler() parser.setContentHandler(handler) xml.sax.parseString(xml_content, handler) return handler.orders # Example usage (to print): # orders = parse_orders(xml_content) # handler.print_orders()"},{"question":"# Question: Creating Customized Line Plots with Seaborn **Objective:** You are tasked with plotting customized visualizations using seaborn to analyze the given datasets. This assessment will test your understanding of seaborn\'s advanced plotting capabilities and customization options. **Datasets:** 1. `dowjones` dataset: A dataset containing temporal data of Dow Jones stock prices. 2. `fmri` dataset: A dataset containing Functional Magnetic Resonance Imaging (fMRI) data. You should follow these steps for the assessment: 1. **Load the required libraries and datasets**: - Import `seaborn.objects` as `so`. - Load the `dowjones` and `fmri` datasets using `seaborn.load_dataset()`. 2. **Task 1**: Create a basic line plot for the `dowjones` dataset. - Plot the `Price` on the y-axis versus the `Date` on the x-axis. 3. **Task 2**: Modify the plot\'s orientation. - Plot the `Date` on the y-axis and `Price` on the x-axis. 4. **Task 3**: Create a grouped line plot for the `fmri` dataset. - Filter the dataset to include only `region == \'parietal\'` and `event == \'stim\'`. - Plot `timepoint` on the x-axis and `signal` on the y-axis. - Group by the `subject` variable and plot one line per subject. 5. **Task 4**: Enhance the plot with additional customization. - Map `region` to the `color` property and `event` to the `linestyle` property. - Combine with a `Band` plot to show error bars. - Add markers to indicate sampled data points. **Constraints:** - Ensure that the data is correctly filtered and grouped. - Use the seaborn objects API to create the plots. - Maintain clear and legible visualizations with appropriate labels and legends. **Expected Input/Output:** *Functions and Inputs*: - There are no specific functions to be implemented, but you are expected to create plots as described. - Inputs will be handled through the use of seaborn\'s `load_dataset()` function. *Output*: - Plots displayed as specified in the tasks. **Performance Requirements**: - The plots should be displayed efficiently without taking excessive computation time. - The visualizations should be clear and interpretable. # Example Solution: ```python # Importing required libraries import seaborn.objects as so from seaborn import load_dataset # Loading datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Task 1: Basic line plot for dowjones dataset so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\").add(so.Line()).show() # Task 2: Line plot with modified orientation so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line(), orient=\\"y\\").show() # Task 3: Grouped line plot for fmri dataset (filtered) filtered_fmri = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") so.Plot(filtered_fmri, x=\\"timepoint\\", y=\\"signal\\").add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\").show() # Task 4: Enhanced plot with color and linestyle mapping, error bands, and markers p = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") p.add(so.Line(), so.Agg()).add(so.Band(), so.Est(), group=\\"event\\").add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None).show() ``` Use this example to guide you through solving the problem and ensure that you understand each step involved in customizing and creating visualizations with seaborn.","solution":"# Importing required libraries import seaborn.objects as so from seaborn import load_dataset # Loading datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Task 1: Basic line plot for dowjones dataset def plot_dowjones_basic(): plot = so.Plot(dowjones, x=\\"Date\\", y=\\"Price\\").add(so.Line()) plot.show() # Task 2: Line plot with modified orientation def plot_dowjones_orientation(): plot = so.Plot(dowjones, x=\\"Price\\", y=\\"Date\\").add(so.Line(), orient=\\"y\\") plot.show() # Task 3: Grouped line plot for fmri dataset (filtered) def plot_fmri_grouped(): filtered_fmri = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") plot = so.Plot(filtered_fmri, x=\\"timepoint\\", y=\\"signal\\").add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") plot.show() # Task 4: Enhanced plot with color and linestyle mapping, error bands, and markers def plot_fmri_enhanced(): plot = so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") plot.add(so.Line(), so.Agg()).add(so.Band(), so.Est(), group=\\"event\\").add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) plot.show()"},{"question":"Objective Create a nullable integer DataFrame, perform various operations, and return specific results. Requirements - Use `pandas` to create a `DataFrame` and `Series`. - Implement the required functions based on the constraints mentioned below. Function Definitions 1. **create_nullable_integer_dataframe**: - **Input**: None. - **Output**: A `pandas.DataFrame` with 3 columns: - `A`: contains integers with possible missing values. - `B`: contains integers with possible missing values. - `C`: contains a mix of integers and missing values. The columns should be constructed using `pd.array` and `pd.Int64Dtype()`. Example: ```python def create_nullable_integer_dataframe(): # Your implementation here return df ``` 2. **perform_operations**: - **Input**: A `DataFrame` created from `create_nullable_integer_dataframe`. - **Output**: A new `DataFrame` where certain operations have been performed: - Add 10 to each element in column `A`. If an element is missing (`pandas.NA`), it should remain missing. - Compare elements in column `B` with 5, resulting in a boolean `Series`. - Slice the first two rows of column `C`. Example: ```python def perform_operations(df): # Your implementation here return new_df, comp_series, sliced_series ``` 3. **group_and_aggregate**: - **Input**: A `DataFrame` created from `create_nullable_integer_dataframe`. - **Output**: Sum of column `A` grouped by column `B`. Example: ```python def group_and_aggregate(df): # Your implementation here return grouped_sum ``` Constraints 1. Use `pd.Int64Dtype()` to handle nullable integers. 2. Ensure that operations maintain the nullable integer property and handle missing values correctly. 3. Explicitly set the data types to avoid unexpected behavior. Example Usage and Expected Output ```python # Create the DataFrame df = create_nullable_integer_dataframe() # Expected Output: # A B C # 0 1 5 10 # 1 2 10 <NA> # 2 <NA> <NA> 20 # Perform operations new_df, comp_series, sliced_series = perform_operations(df) # Expected Outputs: # new_df: # A # 0 11 # 1 12 # 2 <NA> # # comp_series: # 0 True # 1 True # 2 <NA> # # sliced_series: # 0 10 # 1 <NA> # Group by and aggregate grouped_sum = group_and_aggregate(df) # Expected Output: # B # 5 1 # 10 2 # Name: A, dtype: Int64 ```","solution":"import pandas as pd def create_nullable_integer_dataframe(): Creates a DataFrame with 3 columns A, B, and C, containing nullable integers. data = { \'A\': pd.array([1, 2, pd.NA], dtype=pd.Int64Dtype()), \'B\': pd.array([5, 10, pd.NA], dtype=pd.Int64Dtype()), \'C\': pd.array([10, pd.NA, 20], dtype=pd.Int64Dtype()) } df = pd.DataFrame(data) return df def perform_operations(df): Takes a DataFrame and performs specified operations: - Adds 10 to each element in column A - Compares B to 5 - Slices the first two rows of column C # Add 10 to each element in column A new_df = df[[\'A\']].copy() new_df[\'A\'] = new_df[\'A\'] + 10 # Compare elements in column B with 5 comp_series = df[\'B\'] > 5 # Slice the first two rows of column C sliced_series = df[\'C\'].iloc[:2] return new_df, comp_series, sliced_series def group_and_aggregate(df): Takes a DataFrame and sums column A grouped by column B. grouped_sum = df.groupby(\'B\')[\'A\'].sum() return grouped_sum"},{"question":"You have been hired by a software company that is developing a large application in Python. The application frequently utilizes a custom data structure implemented as a C extension. For the purposes of this exercise, you are required to create a Python class that simulates some of this C extension’s functionality. Specifically, your task is to implement a class that manages a custom list-like data structure with the following challenges: # Class: `CustomList` **Methods to implement:** 1. `__init__(self, initial_data=None)`: Initialize the custom list. If `initial_data` is provided (a list), initialize the custom list with this data. 2. `append(self, item)`: Append an item to the custom list. 3. `extend(self, other_list)`: Extend the custom list by appending elements from `other_list`. 4. `insert(self, index, item)`: Insert an item at a given index. 5. `remove(self, item)`: Remove the first occurrence of an item. 6. `pop(self, index=-1)`: Remove and return an item at the index (default is the last item). 7. `clear(self)`: Remove all items from the custom list. 8. `index(self, item, start=0, end=None)`: Return the index of the first occurrence of an item. 9. `count(self, item)`: Return the number of occurrences of an item. 10. `sort(self, *, key=None, reverse=False)`: Sort the custom list in place. 11. `reverse(self)`: Reverse the elements of the custom list in place. 12. `__len__(self)`: Return the number of items in the custom list. 13. `__getitem__(self, index)`: Return the item at the given index. 14. `__setitem__(self, index, value)`: Set the value at a given index. 15. `__delitem__(self, index)`: Delete the item at a given index. 16. `__contains__(self, item)`: Check if an item is in the custom list. **Constraints:** - The class should handle all edge cases such as index out of range, removing non-existent items, etc. - Performance should be reasonable (aim for O(1) operations where possible, and O(n) at most where necessary). - Your implementation should not use any existing list methods directly for the main operations (e.g., do not use Python’s built-in list methods outside initialization). # Example Usage: ```python cl = CustomList([1, 2, 3]) print(len(cl)) # Output: 3 cl.append(4) print(cl[3]) # Output: 4 cl.extend([5, 6]) print(cl[4]) # Output: 5 cl.insert(0, 0) print(cl[0]) # Output: 0 cl.remove(3) print(len(cl)) # Output: 5 print(cl.pop()) # Output: 6 print(cl.index(2)) # Output: 1 print(cl.count(2)) # Output: 1 cl.sort() print([cl[i] for i in range(len(cl))]) # Output: [0, 1, 2, 4, 5] cl.reverse() ``` # Notes: Ensure to handle all special cases gracefully and conform to the expected behaviors of similar list methods in Python.","solution":"class CustomList: def __init__(self, initial_data=None): self._data = initial_data if initial_data is not None else [] def append(self, item): self._data.append(item) def extend(self, other_list): for item in other_list: self.append(item) def insert(self, index, item): self._data = self._data[:index] + [item] + self._data[index:] def remove(self, item): for i in range(len(self._data)): if self._data[i] == item: del self._data[i] return raise ValueError(f\\"{item} not in list\\") def pop(self, index=-1): if index < 0: index += len(self._data) if index >= len(self._data) or index < 0: raise IndexError(\\"pop index out of range\\") item = self._data[index] del self._data[index] return item def clear(self): self._data = [] def index(self, item, start=0, end=None): for i in range(start, min(len(self._data), end if end is not None else len(self._data))): if self._data[i] == item: return i raise ValueError(f\\"{item} is not in list\\") def count(self, item): return sum(1 for i in self._data if i == item) def sort(self, *, key=None, reverse=False): self._data.sort(key=key, reverse=reverse) def reverse(self): self._data = self._data[::-1] def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __contains__(self, item): return item in self._data"},{"question":"# Interactive Terminal Text Editor Using `curses` **Objective:** Create an interactive text editor that functions within the terminal using the `curses` module. The text editor will support basic text editing functionalities like adding, deleting text, moving the cursor, and saving the file content. **Function Implementation:** You are required to implement a function `text_editor(filename: str) -> None` that opens a terminal-based text editor. The text editor should support the following functionalities: - Create a new file or open an existing text file supplied via the `filename` parameter. - Provide basic text navigation and editing capabilities using keyboard inputs. - Save the edited content back to the same file upon exit. **Expected Input and Output:** - **Input:** A string `filename` indicating the file to edit. - **Output:** The function should return `None`. The content of the file specified by `filename` will be modified based on the user\'s actions within the text editor. **Functional Requirements:** 1. **Initializing curses:** - Initialize the curses screen. - Enter `cbreak` mode, disable echo, and enable keypad support. 2. **Navigation:** - Arrow keys or their alternatives should allow moving the cursor. - The Home/End keys should move the cursor to the start/end of a line respectively. - The Page Up/Page Down keys should scroll by one page. 3. **Editing:** - Insert characters typed by the user into the text. - Implement actions for Backspace and Delete keys. - Support Enter key for new lines. - Control commands should be used (e.g., `Ctrl+S` to save, `Ctrl+Q` to quit). 4. **Saving and Exiting:** - The editor should save the text to the specified file when `Ctrl+S` is pressed. - Exit the editor when `Ctrl+Q` is pressed. 5. **Performance:** - The editor should efficiently handle typical file sizes expected in a basic text editor. **Constraints:** - Ensure compatibility across different terminal emulators. - Handle edge cases like terminal resizing gracefully. - Ignore advanced features like clipboard functionality, syntax highlighting, etc., to keep the scope reasonable. Here is an example structure to get you started: ```python import curses def text_editor(filename: str) -> None: def draw_menu(stdscr): stdscr.clear() stdscr.addstr(0, 0, \\"Terminal Text Editor\\") stdscr.addstr(1, 0, \\"Press Ctrl+S to Save, Ctrl+Q to Quit\\") stdscr.refresh() def main(stdscr): curses.cbreak() stdscr.keypad(True) stdscr.clear() # Initial draw draw_menu(stdscr) # Original content and current cursor position lines = [] cursor_x = cursor_y = 0 # Read the file content try: with open(filename) as f: lines = f.readlines() except FileNotFoundError: lines = [] # Ensure there\'s at least one line if not lines: lines = [\\"\\"] # Main loop while True: stdscr.clear() draw_menu(stdscr) # Display the text contents for idx, line in enumerate(lines): stdscr.addstr(idx + 2, 0, line) # Position cursor stdscr.move(cursor_y + 2, cursor_x) # Refresh the screen stdscr.refresh() # Wait for user input ch = stdscr.getch() # Handle user input if ch == curses.KEY_UP: # Move cursor up ... elif ch == curses.KEY_DOWN: # Move cursor down ... elif ch == curses.KEY_LEFT: # Move cursor left ... elif ch == curses.KEY_RIGHT: # Move cursor right ... elif ch == ord(\'n\'): # Handle new line ... elif ch == ord(\'x17\'): # Ctrl+S # Save file ... elif ch == ord(\'x11\'): # Ctrl+Q # Quit break else: # Handle generic character input ... curses.wrapper(main) ``` **Note:** - Fill in the gaps (indicated by `...`) to complete the movement, editing, saving, and quitting functionalities. - Test your implementation in a compatible terminal environment to ensure all functionalities work as expected.","solution":"import curses import os def text_editor(filename: str) -> None: def draw_menu(stdscr): stdscr.addstr(0, 0, \\"Terminal Text Editor - Ctrl+S to Save, Ctrl+Q to Quit\\") stdscr.refresh() def save_file(content, filename): with open(filename, \'w\') as f: f.write(\\"n\\".join(content)) def main(stdscr): curses.cbreak() stdscr.keypad(True) stdscr.clear() curses.noecho() # Read the file content if os.path.exists(filename): with open(filename, \'r\') as f: lines = f.read().splitlines() else: lines = [\\"\\"] # Setup initial cursor position cursor_x = cursor_y = 0 # Main loop while True: stdscr.clear() draw_menu(stdscr) # Display the text contents for idx, line in enumerate(lines): stdscr.addstr(idx + 2, 0, line) # Position cursor stdscr.move(cursor_y + 2, cursor_x) # Refresh the screen stdscr.refresh() # Wait for user input ch = stdscr.getch() if ch == curses.KEY_UP: cursor_y = max(0, cursor_y - 1) cursor_x = min(cursor_x, len(lines[cursor_y])) elif ch == curses.KEY_DOWN: cursor_y = min(len(lines) - 1, cursor_y + 1) cursor_x = min(cursor_x, len(lines[cursor_y])) elif ch == curses.KEY_LEFT: if cursor_x > 0: cursor_x -= 1 elif cursor_y > 0: cursor_y -= 1 cursor_x = len(lines[cursor_y]) elif ch == curses.KEY_RIGHT: if cursor_x < len(lines[cursor_y]): cursor_x += 1 elif cursor_y < len(lines) - 1: cursor_y += 1 cursor_x = 0 elif ch == curses.KEY_BACKSPACE or ch == 127: # Handle Backspace if cursor_x > 0: lines[cursor_y] = lines[cursor_y][:cursor_x-1] + lines[cursor_y][cursor_x:] cursor_x -= 1 elif cursor_y > 0: cursor_x = len(lines[cursor_y-1]) lines[cursor_y-1] += lines[cursor_y] del lines[cursor_y] cursor_y -= 1 elif ch == curses.KEY_DC: # Delete Key if cursor_x < len(lines[cursor_y]): lines[cursor_y] = lines[cursor_y][:cursor_x] + lines[cursor_y][cursor_x+1:] elif cursor_y < len(lines) - 1: lines[cursor_y] += lines[cursor_y+1] del lines[cursor_y+1] elif ch == 10: # Enter Key lines.insert(cursor_y + 1, lines[cursor_y][cursor_x:]) lines[cursor_y] = lines[cursor_y][:cursor_x] cursor_x = 0 cursor_y += 1 elif ch == 17: # Ctrl+Q break elif ch == 19: # Ctrl+S save_file(lines, filename) elif 0 <= ch <= 255: lines[cursor_y] = lines[cursor_y][:cursor_x] + chr(ch) + lines[cursor_y][cursor_x:] cursor_x += 1 curses.wrapper(main)"},{"question":"Problem Statement: You are required to implement a function that manages a list of student records, keeping it sorted by the students\' scores. Each student record is a tuple in the format `(name, score)`, where `name` is a string, and `score` is an integer. Your task is to write the following functions: 1. `insert_student(students, student)`: - **Parameters**: - `students` (List[Tuple[str, int]]): A list of student records sorted by score in ascending order. - `student` (Tuple[str, int]): A new student record to be inserted, which is a tuple `(name, score)`. - **Output**: Inserts the student record into the `students` list while maintaining the sorted order by score. 2. `find_student(students, score)`: - **Parameters**: - `students` (List[Tuple[str, int]]): A list of student records sorted by score in ascending order. - `score` (int): The score to search for within the students list. - **Output**: - Returns a list of names of the students having the exact `score`. - If no student has the exact `score`, return an empty list. **Constraints**: - The `students` list can contain up to (10^5) records. - The score of each student is an integer between `0` and `100`. **Example**: ```python students = [(\\"Alice\\", 85), (\\"Bob\\", 90), (\\"Charlie\\", 95)] # Insert a new student insert_student(students, (\\"David\\", 92)) print(students) # Output: [(\'Alice\', 85), (\'Bob\', 90), (\'David\', 92), (\'Charlie\', 95)] # Find students with score 90 result = find_student(students, 90) print(result) # Output: [\'Bob\'] # Find students with score 100 result = find_student(students, 100) print(result) # Output: [] ``` Write the implementation for the two functions `insert_student` and `find_student`.","solution":"def insert_student(students, student): Inserts the student record into the students list while maintaining the sorted order by score. :param students: List[Tuple[str, int]] - A list of student records sorted by score in ascending order. :param student: Tuple[str, int] - A new student record to be inserted. :return: None name, score = student inserted = False for i in range(len(students)): if students[i][1] > score: students.insert(i, student) inserted = True break if not inserted: students.append(student) def find_student(students, score): Returns a list of names of the students having the exact score. :param students: List[Tuple[str, int]] - A list of student records sorted by score in ascending order. :param score: int - The score to search for within the students list. :return: List[str] - List of names of students having the exact score. result = [name for name, student_score in students if student_score == score] return result"},{"question":"Question: HTTP Status Code Processor # Objective: Write a Python function that processes a list of HTTP status codes and categorizes them by their type (informational, success, redirection, client error, server error). The function should return a dictionary where each key is a category and the value is the list of status codes belonging to that category. # Function Signature: ```python def categorize_http_status_codes(status_codes: list[int]) -> dict[str, list[int]]: ``` # Input: - `status_codes`: A list of integers representing HTTP status codes. - Example: `[200, 404, 500, 301, 100, 302]` # Output: - A dictionary with the following keys: \\"informational\\", \\"success\\", \\"redirection\\", \\"client_error\\", \\"server_error\\". Each key should have a list of status codes that fall under that category. - Example: ```python { \\"informational\\": [100], \\"success\\": [200], \\"redirection\\": [301, 302], \\"client_error\\": [404], \\"server_error\\": [500] } ``` # Constraints: - The function should handle an empty list as input. - Assume all input status codes are integers within the range of valid HTTP status codes as defined by `http.HTTPStatus`. # Requirements: - Utilize the `http.HTTPStatus` class from the `http` module to determine the category of each status code. - The function should be efficient in processing the list of status codes. # Example Usage: ```python status_codes = [200, 404, 500, 301, 100, 302] result = categorize_http_status_codes(status_codes) print(result) # Output: # { # \\"informational\\": [100], # \\"success\\": [200], # \\"redirection\\": [301, 302], # \\"client_error\\": [404], # \\"server_error\\": [500] # } ``` # Additional Information: - HTTP status codes between 100 and 199 are informational. - HTTP status codes between 200 and 299 signify success. - HTTP status codes between 300 and 399 indicate redirection. - HTTP status codes between 400 and 499 indicate client errors. - HTTP status codes between 500 and 599 indicate server errors. **Hint**: You can use `http.HTTPStatus` to check the range of status codes and categorize them accordingly. Good luck, and happy coding!","solution":"from http import HTTPStatus def categorize_http_status_codes(status_codes: list[int]) -> dict[str, list[int]]: categories = { \\"informational\\": [], \\"success\\": [], \\"redirection\\": [], \\"client_error\\": [], \\"server_error\\": [] } for code in status_codes: if HTTPStatus(code).value >= 100 and HTTPStatus(code).value < 200: categories[\\"informational\\"].append(code) elif HTTPStatus(code).value >= 200 and HTTPStatus(code).value < 300: categories[\\"success\\"].append(code) elif HTTPStatus(code).value >= 300 and HTTPStatus(code).value < 400: categories[\\"redirection\\"].append(code) elif HTTPStatus(code).value >= 400 and HTTPStatus(code).value < 500: categories[\\"client_error\\"].append(code) elif HTTPStatus(code).value >= 500 and HTTPStatus(code).value < 600: categories[\\"server_error\\"].append(code) return categories"},{"question":"Interactive Input Editor Objective: Implement a simplified version of an interactive input editor that supports basic history management and tab completion for Python variable names. Description: You need to create a class named `InteractiveEditor` that provides the following functionalities: 1. **Input Method**: Accepts user input and supports basic command history. 2. **Tab Completion**: Suggests completions for variable names defined in the local scope. Requirements: 1. Implement a method `add_to_history(input_line: str) -> None` that stores each input line in a history list. 2. Implement a method `get_history() -> List[str]` that returns the list of previously entered input lines. 3. Implement a method `complete(text: str, local_scope: dict) -> List[str]` that returns a list of possible completions for the given input text based on the variable names in the local scope. Constraints: - You should only consider variable names present in the `local_scope` dictionary for tab completion. - The editor should not execute any code when evaluating for tab completion. - The history should be maintained in the order of inputs received. Expected Input and Output: - `add_to_history(input_line: str)`: Adds `input_line` to the history. - `get_history() -> List[str]`: Returns the list of all input lines stored in history. - `complete(text: str, local_scope: dict) -> List[str]`: Given an incomplete text and a dictionary representing the local scope, returns a list of possible completions. Example: ```python # Example usage of InteractiveEditor editor = InteractiveEditor() # Adding history editor.add_to_history(\\"print(\'Hello, world!\')\\") editor.add_to_history(\\"x = 10\\") editor.add_to_history(\\"y = 20\\") # Retrieving history print(editor.get_history()) # Output: [\\"print(\'Hello, world!\')\\", \\"x = 10\\", \\"y = 20\\"] # Completing variable names local_vars = {\'x\': 10, \'y\': 20, \'zebra\': 30} print(editor.complete(\'z\', local_vars)) # Output: [\'zebra\'] print(editor.complete(\'x\', local_vars)) # Output: [\'x\'] ``` Note: - The provided class definition and methods should be efficient and make use of Python standard libraries. - The code should be self-contained and not require any external libraries for its core functionality.","solution":"class InteractiveEditor: def __init__(self): self.history = [] def add_to_history(self, input_line: str) -> None: self.history.append(input_line) def get_history(self) -> list: return self.history def complete(self, text: str, local_scope: dict) -> list: return [key for key in local_scope.keys() if key.startswith(text)]"},{"question":"# Advanced Python Programming Question **Objective:** Implement a Python class to simulate the behavior of complex number operations as described in the provided C API documentation. Problem Statement You are tasked with implementing a class `ComplexNumber` in Python, which will simulate the `Py_complex` structure and provide methods to perform various arithmetic operations on complex numbers. Class Definition ```python class ComplexNumber: def __init__(self, real: float, imag: float): pass def __add__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': pass def __sub__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': pass def __neg__(self) -> \'ComplexNumber\': pass def __mul__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': pass def __truediv__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': pass def __pow__(self, exponent: \'ComplexNumber\') -> \'ComplexNumber\': pass def to_python_complex(self) -> complex: pass @staticmethod def from_python_complex(c: complex) -> \'ComplexNumber\': pass def __str__(self) -> str: pass ``` # Requirements 1. **Constructor:** - Initialize the complex number instance with real and imaginary parts. 2. **Addition (`__add__` method):** - Implement the addition of two complex numbers. 3. **Subtraction (`__sub__` method):** - Implement the subtraction of two complex numbers. 4. **Negation (`__neg__` method):** - Implement the negation of a complex number. 5. **Multiplication (`__mul__` method):** - Implement the multiplication of two complex numbers. 6. **Division (`__truediv__` method):** - Implement the division of two complex numbers. - Ensure to handle division by zero appropriately. 7. **Exponentiation (`__pow__` method):** - Implement the exponentiation of one complex number by another. 8. **Conversion to Python complex (`to_python_complex` method):** - Convert the custom `ComplexNumber` instance to Python\'s built-in `complex` type. 9. **Conversion from Python complex (`from_python_complex` method):** - Create a `ComplexNumber` instance from Python\'s built-in `complex` type. 10. **String Representation (`__str__` method):** - Provide a string representation of the complex number in the form \\"real + imagi\\". # Input and Output Formats - **Input:** The input will be provided in the form of instances of the `ComplexNumber` class. - **Output:** Methods should return new instances of the `ComplexNumber` class or appropriate types as specified. # Example ```python a = ComplexNumber(3, 2) b = ComplexNumber(1, 7) print(a + b) # Output: \\"4 + 9i\\" print(a - b) # Output: \\"2 - 5i\\" print(-a) # Output: \\"-3 - 2i\\" print(a * b) # Output: \\"-11 + 23i\\" print(a / b) # Output: \\"0.29032 - 0.3871i\\" (approx) print(a ** b) # Output: \\"Complex number result of exponentiation\\" print(a.to_python_complex()) # Output: (3+2j) c = ComplexNumber.from_python_complex(4 - 3j) print(c) # Output: \\"4 - 3i\\" ``` Constraints - Implement proper error handling for invalid operations (e.g., division by zero). - Ensure the class methods are efficient and handle edge cases. **Performance Requirement:** The class should handle operations on complex numbers efficiently for typical use cases, there are no specific performance constraints for large numbers or numerous operations.","solution":"class ComplexNumber: def __init__(self, real: float, imag: float): self.real = real self.imag = imag def __add__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': return ComplexNumber(self.real + other.real, self.imag + other.imag) def __sub__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': return ComplexNumber(self.real - other.real, self.imag - other.imag) def __neg__(self) -> \'ComplexNumber\': return ComplexNumber(-self.real, -self.imag) def __mul__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': real_part = self.real * other.real - self.imag * other.imag imag_part = self.real * other.imag + self.imag * other.real return ComplexNumber(real_part, imag_part) def __truediv__(self, other: \'ComplexNumber\') -> \'ComplexNumber\': if other.real == 0 and other.imag == 0: raise ZeroDivisionError(\\"division by zero\\") denominator = other.real ** 2 + other.imag ** 2 real_part = (self.real * other.real + self.imag * other.imag) / denominator imag_part = (self.imag * other.real - self.real * other.imag) / denominator return ComplexNumber(real_part, imag_part) def __pow__(self, exponent: \'ComplexNumber\') -> \'ComplexNumber\': import cmath result = cmath.exp(exponent.to_python_complex() * cmath.log(self.to_python_complex())) return ComplexNumber.from_python_complex(result) def to_python_complex(self) -> complex: return complex(self.real, self.imag) @staticmethod def from_python_complex(c: complex) -> \'ComplexNumber\': return ComplexNumber(c.real, c.imag) def __str__(self) -> str: return f\\"{self.real} + {self.imag}i\\""},{"question":"**Coding Assessment Question** # Objective: Demonstrate your understanding of the seaborn `Plot` class and Kernel Density Estimate (KDE) plots by implementing a function that creates and customizes a complex KDE plot from the provided dataset. # Required Packages: - seaborn - matplotlib - pandas # Dataset: The dataset `penguins` is already loaded from seaborn\'s built-in datasets. # Function Signature: ```python def create_custom_kde_plot(): pass ``` # Instructions: 1. **Load the Dataset**: - Use the loaded `penguins` dataset. 2. **Create the Plot**: - Create a KDE plot for the `flipper_length_mm` column. - Adjust the bandwidth for the KDE plot to 0.3. - Overlay a histogram with bars. - Add a cumulative KDE line plot. - Facet the plot by `sex`. 3. **Customization**: - The KDE plot should have `cut=2` to extend the density curve beyond the observed values. - Use different `colors` for the KDE lines based on the `species`. - Ensure the KDE plot uses a `gridsize` of 150 for higher resolution. 4. **Output**: - The function does not need to return anything. Simply display the plot. # Example Usage: ```python create_custom_kde_plot() ``` # Evaluation Criteria: - Correctness: The code should correctly create and display the KDE plot with the specified customizations. - Efficiency: The code should efficiently handle the dataset and plot creation without unnecessary computations. - Readability: The code should be well-organized and commented where necessary for clarity. # Constraints: - You must use seaborn\'s `objects` module for this task. - The function should handle any potential errors gracefully.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_custom_kde_plot(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Setting up the plot g = sns.FacetGrid(penguins, col=\\"sex\\", hue=\\"species\\", height=5, aspect=1) # Draw the KDE plot with specified customizations g.map(sns.kdeplot, \\"flipper_length_mm\\", bw_adjust=0.3, cut=2, gridsize=150, fill=True) # Overlay histogram g.map(sns.histplot, \\"flipper_length_mm\\", kde=True, stat=\\"density\\", alpha=0.3) # Add cumulative KDE line plot g.map(sns.kdeplot, \\"flipper_length_mm\\", bw_adjust=0.3, cut=2, cumulative=True) # Add legend g.add_legend() # Display the plot plt.show()"},{"question":"# Question: Profiling and Logging Operations in PyTorch You are tasked with implementing a profiling and logging system in PyTorch using Python. This system should track the time taken by individual PyTorch operations within a model and log this information for later analysis. Your task is to write a Python function `profile_operations` that accepts a PyTorch model and some example input data, and performs the following steps: 1. **Profile the operations**: Use `torch.autograd.profiler` to measure the time taken by each operation during the forward pass of the model. 2. **Log the results**: Collect the profiling information and log the name of each operation along with the time taken. The function should return a dictionary with operation names as keys and time taken as values. # Implementation Details Input - `model`: A PyTorch model (an instance of `torch.nn.Module`). - `input_data`: Example input data suitable for the model. Output - A dictionary where keys are operation names and values are the total time taken by each operation in milliseconds. Constraints - The profiling should only cover the forward pass of the model. - The logging should be detailed enough to include each individual operation. Function Signature ```python def profile_operations(model: torch.nn.Module, input_data): pass ``` # Example Here\'s a small example to demonstrate the expected usage of your `profile_operations` function: ```python import torch import torch.nn as nn import torch.autograd.profiler as profiler class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) model = SimpleModel() input_data = torch.randn(5, 10) profile_data = profile_operations(model, input_data) print(profile_data) ``` Expected output format might be: ```python { \\"aten::linear\\": 0.123, \\"aten::addmm\\": 0.456, ... } ``` Ensure that your implementation is both efficient and generates accurate profiling information.","solution":"import torch import torch.nn as nn import torch.autograd.profiler as profiler def profile_operations(model: torch.nn.Module, input_data): Profiles the operations in a PyTorch model during the forward pass and logs the time taken. Parameters: model (torch.nn.Module): The PyTorch model to profile. input_data (torch.Tensor): Example input data for the model. Returns: dict: A dictionary where keys are operation names and values are the total time taken by each operation in milliseconds. model.eval() # Set the model to evaluation mode with profiler.profile(record_shapes=True, use_cuda=torch.cuda.is_available()) as prof: with torch.no_grad(): _ = model(input_data) profile_data = {} for event in prof.function_events: if event.name not in profile_data: profile_data[event.name] = 0.0 profile_data[event.name] += event.cpu_time_total / 1000 # Convert to milliseconds return profile_data"},{"question":"**Coding Assessment Question: Isotonic Regression Implementation** You are given a dataset with the weights, input features, and target values. Your task is to use the `IsotonicRegression` class from the scikit-learn library to fit a model to this data and make predictions on new data points. # Input: 1. `X_train`: A list of floats representing the input features of the training data. 2. `y_train`: A list of floats representing the target values of the training data. 3. `w_train`: A list of positive floats representing the weights of the training data. 4. `X_test`: A list of floats representing the input features of the test data on which predictions are to be made. 5. `increasing`: A boolean value indicating whether the fitted function should be non-decreasing (True) or non-increasing (False). If set to \'auto\', the constraint should be chosen based on Spearman\'s rank correlation coefficient. # Output: - A list of floats representing the predicted values for `X_test`. # Constraints: - Length of `X_train`, `y_train`, and `w_train` is the same. - Weights `w_train` are all strictly positive. - Input data `X_train`, `y_train`, `w_train`, and `X_test` are all real numbers. # Instructions: 1. Fit an isotonic regression model to the training data using the provided `X_train`, `y_train`, and `w_train`. 2. Make predictions for the `X_test` data using the fitted model. 3. Return the list of predicted values for `X_test`. # Example: ```python from sklearn.isotonic import IsotonicRegression import numpy as np def isotonic_regression(X_train, y_train, w_train, X_test, increasing): # Fit the model ir = IsotonicRegression(increasing=increasing) ir.fit(X_train, y_train, sample_weight=w_train) # Predict on the test data y_test_pred = ir.transform(X_test) return y_test_pred.tolist() # Example usage X_train = [1, 2, 3, 4, 5] y_train = [2, 1, 4, 3, 5] w_train = [1, 1, 1, 1, 1] X_test = [1.5, 2.5, 3.5] increasing = True predictions = isotonic_regression(X_train, y_train, w_train, X_test, increasing) print(predictions) # Example output: [1.5, 2.5, 3.5] ``` Implement the `isotonic_regression` function to solve this problem.","solution":"from sklearn.isotonic import IsotonicRegression import numpy as np from scipy.stats import spearmanr def isotonic_regression(X_train, y_train, w_train, X_test, increasing): if increasing == \'auto\': correlation, _ = spearmanr(X_train, y_train) increasing = correlation >= 0 # Fit the model ir = IsotonicRegression(increasing=increasing) ir.fit(X_train, y_train, sample_weight=w_train) # Predict on the test data y_test_pred = ir.transform(X_test) return y_test_pred.tolist()"},{"question":"Objective Implement a Python function that performs operations on complex numbers, demonstrating an understanding of both real and imaginary parts. Problem Statement Define a function `complex_operations` that takes two Python complex numbers as input and performs the following operations: 1. Sum of the two complex numbers. 2. Product of the two complex numbers. 3. Real and imaginary parts of the first complex number. 4. Real and imaginary parts of the second complex number. 5. Quotient when the first complex number is divided by the second complex number. Function Signature ```python def complex_operations(c1: complex, c2: complex) -> dict: ``` Input - `c1`: A complex number (Python `complex` type). - `c2`: Another complex number (Python `complex` type). Output - A dictionary with the following keys: - `\\"sum\\"`: The sum of `c1` and `c2` (complex number). - `\\"product\\"`: The product of `c1` and `c2` (complex number). - `\\"real1\\"`: The real part of `c1` (float). - `\\"imag1\\"`: The imaginary part of `c1` (float). - `\\"real2\\"`: The real part of `c2` (float). - `\\"imag2\\"`: The imaginary part of `c2` (float). - `\\"quotient\\"`: The quotient when `c1` is divided by `c2` (complex number). Constraints - Ensure `c2` is not zero to avoid division by zero errors. Example ```python c1 = complex(3, 4) # 3 + 4j c2 = complex(1, 2) # 1 + 2j result = complex_operations(c1, c2) print(result) # Expected output: # { # \\"sum\\": (4+6j), # \\"product\\": (-5+10j), # \\"real1\\": 3.0, # \\"imag1\\": 4.0, # \\"real2\\": 1.0, # \\"imag2\\": 2.0, # \\"quotient\\": (2.2-0.4j) # } ``` Note - You may use Python’s built-in complex number type and operations directly. - Handle any edge cases, such as division by zero, appropriately using Python\'s exception handling.","solution":"def complex_operations(c1: complex, c2: complex) -> dict: Perform various operations on two complex numbers: 1. Sum of c1 and c2 2. Product of c1 and c2 3. Real and imaginary parts of c1 4. Real and imaginary parts of c2 5. Quotient of c1 divided by c2 Args: - c1: Complex number - c2: Complex number Returns: - dict: Dictionary containing results of various operations try: quotient = c1 / c2 except ZeroDivisionError: quotient = None # Handle division by zero return { \\"sum\\": c1 + c2, \\"product\\": c1 * c2, \\"real1\\": c1.real, \\"imag1\\": c1.imag, \\"real2\\": c2.real, \\"imag2\\": c2.imag, \\"quotient\\": quotient }"},{"question":"Objective: You need to implement a Python program to manage a simple persistent todo list using the `shelve` module. The program should provide functionalities to add new tasks, mark tasks as completed, delete tasks, and list all tasks. Function Requirements: 1. **add_task(filename: str, task: str) -> None**: - Add a new task to the todo list stored in the shelve database. - **Input**: - `filename`: The name of the shelve database file. - `task`: The task description as a string. - **Output**: None. - **Constraints**: - If the task already exists, the function should print \\"Task already exists!\\" 2. **complete_task(filename: str, task: str) -> None**: - Mark an existing task as completed in the todo list. - **Input**: - `filename`: The name of the shelve database file. - `task`: The task description as a string. - **Output**: None. - **Constraints**: - If the task does not exist, the function should print \\"Task not found!\\". 3. **delete_task(filename: str, task: str) -> None**: - Remove a task from the todo list. - **Input**: - `filename`: The name of the shelve database file. - `task`: The task description as a string. - **Output**: None. - **Constraints**: - If the task does not exist, the function should print \\"Task not found!\\". 4. **list_tasks(filename: str) -> None**: - List all tasks in the todo list with their completion status. - **Input**: - `filename`: The name of the shelve database file. - **Output**: None. - **Example Output**: ``` Task: Buy groceries, Status: Incomplete Task: Do laundry, Status: Completed ``` Performance Requirements: - Efficient handling of the shelf to avoid excessive memory usage. - Ensure proper closing of the shelf to persist data correctly. Example Usage: ```python filename = \'todo_shelf\' add_task(filename, \'Buy groceries\') add_task(filename, \'Do laundry\') complete_task(filename, \'Buy groceries\') delete_task(filename, \'Do laundry\') list_tasks(filename) ``` Implement the required functions to manage the todo list effectively using the `shelve` module.","solution":"import shelve def add_task(filename: str, task: str) -> None: with shelve.open(filename) as db: if task in db: print(\\"Task already exists!\\") else: db[task] = False # False indicates the task is incomplete def complete_task(filename: str, task: str) -> None: with shelve.open(filename) as db: if task not in db: print(\\"Task not found!\\") else: db[task] = True # True indicates the task is complete def delete_task(filename: str, task: str) -> None: with shelve.open(filename) as db: if task not in db: print(\\"Task not found!\\") else: del db[task] def list_tasks(filename: str) -> None: with shelve.open(filename) as db: for task, completed in db.items(): status = \\"Completed\\" if completed else \\"Incomplete\\" print(f\\"Task: {task}, Status: {status}\\")"},{"question":"Objective Create a class representing a tree-like data structure and implement custom shallow and deep copy functionalities. Problem Statement Design a class `TreeNode` to represent nodes in a tree. Each `TreeNode` can have multiple children. Implement the `__copy__()` and `__deepcopy__()` methods to handle shallow and deep copies of the tree. Requirements 1. Implement the `TreeNode` class with the following methods: - `__init__(self, value, children=None)`: Constructor to initialize a tree node with a value and optional children. - `add_child(self, child)`: Method to add a child node. - `__copy__(self)`: Method to create a shallow copy of the tree node. - `__deepcopy__(self, memo)`: Method to create a deep copy of the tree node. 2. The shallow copy should create a new tree node with the same value and references to the same children nodes. 3. The deep copy should create a completely new tree with new nodes having the same values but with no shared references between the original and the copied tree. Input and Output Formats - The constructor takes an integer `value` and an optional list of `children`. - `add_child(child)` adds a `TreeNode` instance to the children list. - `__copy__()` returns a shallow copy of the `TreeNode`. - `__deepcopy__(memo)` returns a deep copy of the `TreeNode`. Constraints - Node values are integers. - The tree can have a variable number of levels and branches. - Circular references in the tree should be handled gracefully. Performance Requirements - Ensure the deep copy avoids unnecessary duplication of shared data. - Maintain efficient memory usage, especially with large and complex tree structures. Example ```python # Define a tree root = TreeNode(1) child1 = TreeNode(2) child2 = TreeNode(3) root.add_child(child1) root.add_child(child2) child1.add_child(TreeNode(4)) child2.add_child(TreeNode(5)) # Create shallow copy shallow_copy = copy.copy(root) # Create deep copy deep_copy = copy.deepcopy(root) # Check shallow copy assert shallow_copy is not root assert shallow_copy.children[0] is child1 # Check deep copy assert deep_copy is not root assert deep_copy.children[0] is not child1 assert deep_copy.children[0].value == 2 ``` Note Make sure to handle edge cases like empty trees and trees with multiple levels of nesting.","solution":"import copy class TreeNode: def __init__(self, value, children=None): self.value = value self.children = children if children is not None else [] def add_child(self, child): self.children.append(child) def __copy__(self): new_node = TreeNode(self.value) new_node.children = self.children # Shallow copy: children list is shared return new_node def __deepcopy__(self, memo): if self in memo: return memo[self] new_node = TreeNode(self.value) memo[self] = new_node new_node.children = [copy.deepcopy(child, memo) for child in self.children] return new_node"},{"question":"You are tasked with creating a custom source distribution script using the features described in the provided documentation. Your script must generate a distribution package based on specific inclusion and exclusion rules provided by the user. Problem Statement Implement a function `create_distribution(manifest_commands: List[str]) -> List[str]:` that processes a list of manifest commands and returns a list of files that would be included in the distribution package. Input - `manifest_commands`: A list of strings, where each string is a command as described in the **sdist** command documentation. The commands can be `include`, `exclude`, `recursive-include`, `recursive-exclude`, `global-include`, `global-exclude`, `prune`, or `graft`. Output - Returns a list of strings representing the files that would be included in the distribution package after processing all the manifest commands. Constraints - You can assume that the directory structure and files are predefined and static for this exercise. - Handle typical Unix-style file patterns (`*`, `?`, `[range]`). Example Consider a predefined directory structure as follows: ``` project ├── data │ ├── file1.txt │ └── file2.log ├── scripts │ ├── script1.py │ └── script2.sh ├── src │ ├── mod1.py │ ├── mod2.py │ └── mod3.c └── README.md ``` Input: ```python manifest_commands = [ \\"include README.md\\", \\"recursive-include data *.txt\\", \\"exclude src/*.c\\", \\"global-include scripts/*\\", ] ``` Output: ```python [ \\"README.md\\", \\"data/file1.txt\\", \\"scripts/script1.py\\", \\"scripts/script2.sh\\", ] ``` Notes: - In the example, the `.c` file in `src` is excluded. - Only `.txt` files in `data` are included. - All files in `scripts` are globally included. # Implementation Guidelines - Define a function `create_distribution(manifest_commands: List[str]) -> List[str]`. - Simulate the filesystem for testing purposes; no actual file operations are required. - Process each command and update the list of included files accordingly. - Be mindful of the order in which commands are processed as it can affect the final output.","solution":"from typing import List import fnmatch import os def create_distribution(manifest_commands: List[str]) -> List[str]: # Simulate our directory structure file_structure = { \\"project\\": [\\"README.md\\"], \\"project/data\\": [\\"file1.txt\\", \\"file2.log\\"], \\"project/scripts\\": [\\"script1.py\\", \\"script2.sh\\"], \\"project/src\\": [\\"mod1.py\\", \\"mod2.py\\", \\"mod3.c\\"] } all_files = [] for dir, files in file_structure.items(): all_files.extend([os.path.join(dir, file) for file in files]) included_files = set() for command in manifest_commands: parts = command.split() action = parts[0] if action == \\"include\\": file_path = parts[1] if file_path in all_files: included_files.add(file_path) elif action == \\"exclude\\": file_path = parts[1] if file_path in included_files: included_files.remove(file_path) elif action == \\"recursive-include\\": dir_path = parts[1] pattern = parts[2] for file in all_files: if file.startswith(dir_path) and fnmatch.fnmatch(os.path.basename(file), pattern): included_files.add(file) elif action == \\"recursive-exclude\\": dir_path = parts[1] pattern = parts[2] for file in list(included_files): if file.startswith(dir_path) and fnmatch.fnmatch(os.path.basename(file), pattern): included_files.remove(file) elif action == \\"global-include\\": pattern = parts[1] for file in all_files: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif action == \\"global-exclude\\": pattern = parts[1] for file in list(included_files): if fnmatch.fnmatch(file, pattern): included_files.remove(file) elif action == \\"prune\\": dir_path = parts[1] for file in list(included_files): if file.startswith(dir_path): included_files.remove(file) elif action == \\"graft\\": dir_path = parts[1] for file in all_files: if file.startswith(dir_path): included_files.add(file) return sorted(included_files)"},{"question":"Coding Assessment Question: Implementing Spectral Biclustering # Objective Implement parts of the Spectral Biclustering algorithm to demonstrate an understanding of biclustering concepts and scikit-learn\'s functionality. # Problem Statement You are provided with a data matrix `X`. Your task is to implement a CustomSpectralBicluster class that can identify biclusters in the data matrix. This implementation will preprocess the data matrix, compute necessary singular vectors using SVD, and then cluster the rows and columns. # Implementation Details 1. **Preprocessing**: - Implement log normalization of the input data matrix. 2. **SVD Computation**: - Compute the first few singular vectors. If normalization is used, discard the first singular vector. 3. **Clustering**: - Use k-means to cluster both rows and columns based on the singular vectors. # Function Signatures and Requirements 1. `__init__(self, n_clusters: int)`: Initialize the class with the number of clusters for k-means. 2. `fit(self, X: np.ndarray) -> None`: The main method to fit the data matrix `X`. This function should: - Normalize the data matrix using log normalization. - Compute SVD and select the first few singular vectors. - Perform k-means clustering on the rows and columns. - Store the row clusters and column clusters. 3. `get_biclusters(self) -> Tuple[np.ndarray, np.ndarray]`: Return the row and column clusters as binary matrices. # Constraints - The input data matrix `X` will have shape `(m, n)` where `5 <= m, n <= 500`. - You may use only scikit-learn, numpy, and scipy packages. ```python from typing import Tuple import numpy as np from sklearn.cluster import KMeans from scipy.linalg import svd class CustomSpectralBicluster: def __init__(self, n_clusters: int): self.n_clusters = n_clusters self.row_labels_ = None self.column_labels_ = None def fit(self, X: np.ndarray) -> None: # Step 1: Log Normalization L = np.log(X + 1) # Add 1 to avoid log(0) row_mean = np.mean(L, axis=1, keepdims=True) col_mean = np.mean(L, axis=0, keepdims=True) overall_mean = np.mean(L) K = L - row_mean - col_mean + overall_mean # Step 2: SVD Computation U, Sigma, Vt = svd(K) U = U[:, 1:self.n_clusters + 1] Vt = Vt[1:self.n_clusters + 1, :] # Step 3: K-Means Clustering kmeans_row = KMeans(n_clusters=self.n_clusters).fit(U) kmeans_col = KMeans(n_clusters=self.n_clusters).fit(Vt.T) self.row_labels_ = kmeans_row.labels_ self.column_labels_ = kmeans_col.labels_ def get_biclusters(self) -> Tuple[np.ndarray, np.ndarray]: row_clusters = np.zeros((self.n_clusters, len(self.row_labels_)), dtype=int) col_clusters = np.zeros((self.n_clusters, len(self.column_labels_)), dtype=int) for i in range(self.n_clusters): row_clusters[i, self.row_labels_ == i] = 1 col_clusters[i, self.column_labels_ == i] = 1 return row_clusters, col_clusters # Example usage: # X = np.random.rand(10, 10) # model = CustomSpectralBicluster(n_clusters=2) # model.fit(X) # row_clusters, col_clusters = model.get_biclusters() # print(row_clusters) # print(col_clusters) ``` # Evaluation Criteria - **Correctness**: The implementation should correctly normalize the input data matrix, compute SVD, and cluster the rows and columns. - **Code Quality**: The solution should be well-structured and readable. - **Edge Cases**: The solution should handle different shapes of input matrices.","solution":"from typing import Tuple import numpy as np from sklearn.cluster import KMeans from scipy.linalg import svd class CustomSpectralBicluster: def __init__(self, n_clusters: int): self.n_clusters = n_clusters self.row_labels_ = None self.column_labels_ = None def fit(self, X: np.ndarray) -> None: # Step 1: Log Normalization L = np.log(X + 1) # Add 1 to avoid log(0) row_mean = np.mean(L, axis=1, keepdims=True) col_mean = np.mean(L, axis=0, keepdims=True) overall_mean = np.mean(L) K = L - row_mean - col_mean + overall_mean # Step 2: SVD Computation U, Sigma, Vt = svd(K) U = U[:, 1:self.n_clusters + 1] Vt = Vt[1:self.n_clusters + 1, :] # Step 3: K-Means Clustering kmeans_row = KMeans(n_clusters=self.n_clusters).fit(U) kmeans_col = KMeans(n_clusters=self.n_clusters).fit(Vt.T) self.row_labels_ = kmeans_row.labels_ self.column_labels_ = kmeans_col.labels_ def get_biclusters(self) -> Tuple[np.ndarray, np.ndarray]: row_clusters = np.zeros((self.n_clusters, len(self.row_labels_)), dtype=int) col_clusters = np.zeros((self.n_clusters, len(self.column_labels_)), dtype=int) for i in range(self.n_clusters): row_clusters[i, self.row_labels_ == i] = 1 col_clusters[i, self.column_labels_ == i] = 1 return row_clusters, col_clusters"},{"question":"Objective You are tasked with writing a Python program that reads an XML document, parses it using custom handler functions, and processes the data in a specific way. This program will use the `xml.parsers.expat` module to achieve this. Task 1. **Create a function `parse_xml_document` that takes a string `xml_content` representing the XML content.** 2. **Implement the following handler functions for the XML parser:** - `start_element_handler`: This function should process the start of an XML element. It should accumulate the names of all elements encountered. - `end_element_handler`: This function should process the end of an XML element. It should log the name of the element that is ending. - `character_data_handler`: This function should process the character data within XML elements. It should concatenate the data found within each element. 3. **Within your `parse_xml_document` function, set up the parser to use these handlers and parse the provided `xml_content`.** 4. **If an error occurs during parsing, your function should catch the `ExpatError` and return a string indicating the line number and column where the error occurred.** 5. **Your function should return a dictionary where:** - `elements` is a list of all element names encountered in the order they appeared. - `data` is a concatenation of all character data within the XML document. # Constraints - You must use the `xml.parsers.expat.ParserCreate()` to create an XML parser object. - The XML content provided to the function will always be well-formed, but you still need to handle errors gracefully. - Character data should only include actual text content and not whitespace between elements unless it is within a text node. # Input - `xml_content`: A string containing well-formed XML data. # Output - A dictionary with the keys `elements` and `data` as described above. - If a parsing error occurs, return a string in the format `\\"Error at line X, column Y\\"`. # Example ```python def start_element_handler(name, attrs): pass # your implementation def end_element_handler(name): pass # your implementation def character_data_handler(data): pass # your implementation def parse_xml_document(xml_content: str) -> dict: pass # your implementation xml_content = <?xml version=\\"1.0\\"?> <parent id=\\"top\\"> <child1 name=\\"paul\\">Text goes here</child1> <child2 name=\\"fred\\">More text</child2> </parent> result = parse_xml_document(xml_content) print(result) # Expected Output: # {\'elements\': [\'parent\', \'child1\', \'child2\'], \'data\': \'Text goes hereMore text\'} ``` Ensure your function handles the setup of handlers and the initiation of parsing correctly. It should be robust enough to handle different valid XML structures and report errors when parsing fails.","solution":"import xml.parsers.expat from xml.parsers.expat import ExpatError def parse_xml_document(xml_content): elements = [] character_data = [] def start_element_handler(name, attrs): elements.append(name) def end_element_handler(name): pass def character_data_handler(data): if data.strip(): # Only add non-whitespace data character_data.append(data) # Create the parser parser = xml.parsers.expat.ParserCreate() # Set the handler functions parser.StartElementHandler = start_element_handler parser.EndElementHandler = end_element_handler parser.CharacterDataHandler = character_data_handler try: parser.Parse(xml_content, True) except ExpatError as e: return f\\"Error at line {parser.ErrorLineNumber}, column {parser.ErrorColumnNumber}\\" return { \\"elements\\": elements, \\"data\\": \\"\\".join(character_data) }"},{"question":"**Seaborn Advanced Plotting Assessment** **Objective**: Demonstrate your understanding of seaborn\'s object-oriented plotting interface by creating a complex, multi-layered plot. # Task You are provided with the \'tips\' dataset from seaborn. Your task is to create a plot that visualizes the relationship between the total bill amount and the tip amount, segmented by the \'day\' of the week, and further distinguished by the \'time\' of the day (Lunch/Dinner). # Requirements 1. **Load the \'tips\' dataset** using seaborn\'s `load_dataset` function. 2. **Create a Plot object**: - Set `total_bill` as the x-axis variable. - Set `tip` as the y-axis variable. - Use `day` for facet column segmentation. 3. **Add multiple layers**: - **Layer 1**: Plot the data points using a dot mark. Use `time` to color the dots and set their transparency to 0.5. - **Layer 2**: Overlay a linear regression line (`PolyFit` transformation) for each facet with a specific line color (e.g., gray) and a linewidth of 2. 4. **Additional Customizations**: - Define a y-axis label as \\"Tip Amount\\". - Exclude `day` from the dot color mapping by specifying these variables at the layer level. 5. **Generate the plot and display it**. # Constraints - Only use seaborn. - Each layer should handle its own necessary data manipulations. # Example Input/Output Input: None (You will use the `tips` dataset from Seaborn directly.) Expected Output: A faceted plot displaying the following: - Four panels, each representing a distinct day of the week. - Each panel contains: - Dot plots of total bills vs. tip amounts, colored by \'time\'. - Overlayed regression lines for the relationship between total bill and tip amount. # Performance Requirements - Ensure your solution is efficient and leverages seaborn\'s object-oriented capabilities effectively. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the Plot object with faceting plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"time\\") .facet(col=\\"day\\") # Add the first layer: Dot plot .add(so.Dot(alpha=0.5), col=None, color=\\"time\\") # Add the second layer: Regression line .add(so.Line(color=\\"gray\\", linewidth=2), so.PolyFit()) # Label the y-axis .label(y=\\"Tip Amount\\") ) # Display the plot plot.show() ``` **GOOD LUCK!**","solution":"import seaborn.objects as so from seaborn import load_dataset def create_tips_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the Plot object with faceting plot = ( so.Plot(tips, \\"total_bill\\", \\"tip\\", color=\\"time\\") .facet(col=\\"day\\") # Add the first layer: Dot plot .add(so.Dot(alpha=0.5), col=None, color=\\"time\\") # Add the second layer: Regression line .add(so.Line(color=\\"gray\\", linewidth=2), so.PolyFit()) # Label the y-axis .label(y=\\"Tip Amount\\") ) # Display the plot plot.show() # Example usage create_tips_plot()"},{"question":"Advanced String Conversion and Formatting in Python # Objective Create a custom function library that utilizes the functionality provided in the `PyOS` module for formatted string conversion, string to number conversions, number to string conversions, and case-insensitive string comparisons. Your task is to implement these functions and demonstrate their usage through test cases. # Requirements 1. **Formatted String Output**: Implement a function `formatted_output` that uses `PyOS_snprintf` to output a formatted string. This function should handle format specifiers similar to C\'s `printf`. ```python def formatted_output(format_string: str, *args) -> str: Outputs a formatted string according to the format string and arguments provided. :param format_string: A format string containing zero or more format specifiers. :param args: Arguments to be formatted into the format string. :return: A formatted string. :raises ValueError: If the format string or arguments are invalid. pass ``` 2. **String to Double Conversion**: Implement a function `string_to_double` that converts a string to a double using `PyOS_string_to_double`. ```python def string_to_double(s: str, overflow_exception: Exception = None) -> float: Converts the string representation of a floating-point number to a double. :param s: The string to convert. :param overflow_exception: An optional exception to raise if the conversion overflows. :return: The double value represented by the string. :raises ValueError: If the string is not a valid floating-point representation. :raises Exception: If an overflow occurs and an overflow_exception is provided. pass ``` 3. **Double to String Conversion**: Implement a function `double_to_string` that uses `PyOS_double_to_string` to convert a double to its string representation based on the given format code and precision. ```python def double_to_string(val: float, format_code: str, precision: int, flags: int = 0) -> str: Converts a double to its string representation using the specified format code and precision. :param val: The double value to convert. :param format_code: A character specifying the format of the conversion (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'). :param precision: The precision for the conversion. :param flags: Optional flags for the conversion format. :return: A string representation of the double value. :raises ValueError: If the format code or precision is invalid. pass ``` 4. **Case Insensitive String Comparison**: Implement two functions `case_insensitive_compare` and `case_insensitive_n_compare` that use `PyOS_stricmp` and `PyOS_strnicmp` respectively. ```python def case_insensitive_compare(s1: str, s2: str) -> int: Compares two strings case insensitively. :param s1: The first string to compare. :param s2: The second string to compare. :return: 0 if s1 and s2 are equal, a negative value if s1 < s2, and a positive value if s1 > s2. pass def case_insensitive_n_compare(s1: str, s2: str, size: int) -> int: Compares up to size characters of two strings case insensitively. :param s1: The first string to compare. :param s2: The second string to compare. :param size: The number of characters to compare. :return: 0 if the first size characters of s1 and s2 are equal, a negative value if s1 < s2, and a positive value if s1 > s2. pass ``` # Constraints - Ensure all implemented functions handle edge cases appropriately (e.g., invalid input types, empty strings). - You may assume that the input values will not exceed the limits specified by the `PyOS` functions. # Performance Requirements - Your solution should have a time complexity that is efficient and appropriate for the tasks being performed. **Note**: For Python code, you do not need to implement `PyOS_snprintf`, `PyOS_vsnprintf`, `PyOS_string_to_double`, `PyOS_double_to_string`, `PyOS_stricmp`, and `PyOS_strnicmp` functions. Instead, use Python\'s equivalent functions to demonstrate the logic as `PyOS` functions are part of the C extension interface. # Example Usage ```python if __name__ == \\"__main__\\": # Example showing formatted string output print(formatted_output(\\"%s is %d years old.\\", \\"Alice\\", 30)) # Expected: \\"Alice is 30 years old.\\" # Example showing string to double conversion print(string_to_double(\\"123.456\\")) # Expected: 123.456 # Example showing double to string conversion print(double_to_string(123.456, \'f\', 2)) # Expected: \\"123.46\\" # Example showing case-insensitive string comparison print(case_insensitive_compare(\\"Hello\\", \\"hello\\")) # Expected: 0 # Example showing case-insensitive string comparison with size print(case_insensitive_n_compare(\\"HelloWorld\\", \\"helloworld\\", 5)) # Expected: 0 ```","solution":"def formatted_output(format_string: str, *args) -> str: Outputs a formatted string according to the format string and arguments provided. :param format_string: A format string containing zero or more format specifiers. :param args: Arguments to be formatted into the format string. :return: A formatted string. :raises ValueError: If the format string or arguments are invalid. try: return format_string % args except (TypeError, ValueError) as e: raise ValueError(f\\"Invalid format string or arguments: {e}\\") from e def string_to_double(s: str, overflow_exception: Exception = None) -> float: Converts the string representation of a floating-point number to a double. :param s: The string to convert. :param overflow_exception: An optional exception to raise if the conversion overflows. :return: The double value represented by the string. :raises ValueError: If the string is not a valid floating-point representation. :raises Exception: If an overflow occurs and an overflow_exception is provided. try: result = float(s) except ValueError as e: raise ValueError(f\\"Invalid floating-point representation: {e}\\") from e if overflow_exception and not (float(\'-inf\') < result < float(\'inf\')): raise overflow_exception(f\\"Overflow occurred when converting string to double\\") return result def double_to_string(val: float, format_code: str, precision: int, flags: int = 0) -> str: Converts a double to its string representation using the specified format code and precision. :param val: The double value to convert. :param format_code: A character specifying the format of the conversion (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'). :param precision: The precision for the conversion. :param flags: Optional flags for the conversion format. :return: A string representation of the double value. :raises ValueError: If the format code or precision is invalid. format_spec = f\\".{precision}{format_code}\\" try: return format(val, format_spec) except ValueError as e: raise ValueError(f\\"Invalid format code or precision: {e}\\") from e def case_insensitive_compare(s1: str, s2: str) -> int: Compares two strings case insensitively. :param s1: The first string to compare. :param s2: The second string to compare. :return: 0 if s1 and s2 are equal, a negative value if s1 < s2, and a positive value if s1 > s2. return (s1.lower() > s2.lower()) - (s1.lower() < s2.lower()) def case_insensitive_n_compare(s1: str, s2: str, size: int) -> int: Compares up to size characters of two strings case insensitively. :param s1: The first string to compare. :param s2: The second string to compare. :param size: The number of characters to compare. :return: 0 if the first size characters of s1 and s2 are equal, a negative value if s1 < s2, and a positive value if s1 > s2. return (s1[:size].lower() > s2[:size].lower()) - (s1[:size].lower() < s2[:size].lower())"},{"question":"Objective Create a custom PyTorch model and publish it using PyTorch Hub by implementing an entry point in `hubconf.py`. Your task is to demonstrate your understanding of fundamental and advanced concepts of PyTorch Hub, model creation, and managing pre-trained weights. Problem Statement You are required to create a simple custom convolutional neural network (CNN) for image classification and publish it using PyTorch Hub. 1. Implement a CNN model class named `CustomCNN` which contains the following layers: - A convolutional layer with 16 filters, kernel size 3, and ReLU activation. - A max pooling layer with kernel size 2. - A convolutional layer with 32 filters, kernel size 3, and ReLU activation. - A max pooling layer with kernel size 2. - A fully connected layer that outputs 100 features. - A ReLU activation. - A final fully connected layer that outputs 10 classes (assuming classification into 10 categories). 2. Save the model weights after training in a file named `customcnn.pth`. 3. Implement an entry point function named `custom_cnn` in `hubconf.py` that: - Accepts an argument `pretrained` to indicate whether to load pre-trained weights. - Loads the pre-trained weights from `customcnn.pth` if `pretrained=True`. - Returns an instance of `CustomCNN`. Constraints - You do not need to train the model as part of this assessment; assume the weights are already in the file `customcnn.pth`. - Ensure that importing the necessary libraries and handling any dependencies are appropriately managed in `hubconf.py`. Testing Your Solution Your entry point function `custom_cnn` should be testable using `torch.hub.load()`. Ensure that: - Calling `torch.hub.list()` shows your entry point `custom_cnn`. - Calling `torch.hub.help(\'custom_cnn\')` displays the docstring of your entry point. - Calling `torch.hub.load(\'./\', \'custom_cnn\', pretrained=True)` successfully loads the model with pre-trained weights. Example Usage: ```python import torch model = torch.hub.load(\'./\', \'custom_cnn\', pretrained=True) print(model) ``` Submission Submit your implementation of `CustomCNN` as a class in a Python file and the `hubconf.py` that includes the `custom_cnn` entry point function. Good luck!","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomCNN(nn.Module): def __init__(self): super(CustomCNN, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1) self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1) self.fc1 = nn.Linear(in_features=32 * 8 * 8, out_features=100) # Assuming input image size is 32x32 self.fc2 = nn.Linear(in_features=100, out_features=10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 32 * 8 * 8) # Flatten tensor x = F.relu(self.fc1(x)) x = self.fc2(x) return x"},{"question":"# Question Suppose you are given the sales data of a company in two dictionaries representing different years: ```python data_2022 = { \'Store\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Quarter1\': [2500, 2000, None, 1700, 2200], \'Quarter2\': [2700, 2100, 2200, 1800, None], \'Quarter3\': [None, 2300, 2000, 1900, 2400], \'Quarter4\': [2900, 2400, 2100, 2000, 2500] } data_2023 = { \'Store\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Quarter1\': [2600, None, 2100, 1800, 2300], \'Quarter2\': [2800, 2200, 2300, None, 2400], \'Quarter3\': [None, 2400, 2200, 2000, 2500], \'Quarter4\': [3000, 2500, 2200, 2100, None] } ``` Your task is to perform the following operations using pandas: 1. **Create DataFrames:** Create two DataFrames named `df_2022` and `df_2023` from the given dictionaries `data_2022` and `data_2023`. 2. **Handle Missing Values:** For both DataFrames, replace any `None` or `NaN` values with the mean value of their respective columns. 3. **Compute Total Sales:** Add a new column `TotalSales` to each DataFrame that contains the total sales for each store across all four quarters. 4. **Combine DataFrames:** Combine the two DataFrames into a single DataFrame named `combined_df`, adding a new column `Year` to indicate the year each row of data belongs to (2022 or 2023). 5. **Summary Statistics:** For each year, compute the mean and standard deviation of the `TotalSales` and store the results in a new DataFrame named `summary_stats`. 6. **Reshape DataFrame:** Reshape `combined_df` into a long format DataFrame named `long_format_df`, where each row contains `Store`, `Year`, `Quarter` and `Sales` columns. 7. **Custom Transformation:** For `long_format_df`, create a new column named `SalesChange` that contains the percentage change in sales from the previous year for each store and quarter combination. # Input The input consists of two dictionaries `data_2022` and `data_2023` as described above. # Output Your output should include the following DataFrames: 1. `df_2022`: DataFrame for the year 2022 with missing values handled and the `TotalSales` column added. 2. `df_2023`: DataFrame for the year 2023 with missing values handled and the `TotalSales` column added. 3. `combined_df`: A combined DataFrame with data from both years and an added `Year` column. 4. `summary_stats`: A DataFrame with the mean and standard deviation of `TotalSales` for each year. 5. `long_format_df`: A long format DataFrame with columns `Store`, `Year`, `Quarter`, and `Sales`. 6. `SalesChange`: A column in `long_format_df` showing the percentage change in sales from the previous year. # Constraints 1. Use pandas functions to handle missing values, compute statistics, and reshape the DataFrame. 2. Ensure the custom transformation correctly handles cases with no previous year data (e.g., 2022 should not show any `SalesChange` from 2021). 3. Your code should handle potential edge cases like all values in a column being NaN. # Example ```python # Example code framework # Step 1: Create DataFrames df_2022 = pd.DataFrame(data_2022) df_2023 = pd.DataFrame(data_2023) # Step 2: Handle Missing Values df_2022.fillna(df_2022.mean(), inplace=True) df_2023.fillna(df_2023.mean(), inplace=True) # (Continue with other steps...) # Final expected output DataFrames print(df_2022) print(df_2023) print(combined_df) print(summary_stats) print(long_format_df) ``` Submit your Python function(s) to achieve the above tasks.","solution":"import pandas as pd def process_sales_data(data_2022, data_2023): # Step 1: Create DataFrames df_2022 = pd.DataFrame(data_2022) df_2023 = pd.DataFrame(data_2023) # Step 2: Handle Missing Values df_2022.fillna(df_2022.mean(numeric_only=True), inplace=True) df_2023.fillna(df_2023.mean(numeric_only=True), inplace=True) # Step 3: Compute Total Sales df_2022[\'TotalSales\'] = df_2022.loc[:, \'Quarter1\':\'Quarter4\'].sum(axis=1) df_2023[\'TotalSales\'] = df_2023.loc[:, \'Quarter1\':\'Quarter4\'].sum(axis=1) # Step 4: Combine DataFrames df_2022[\'Year\'] = 2022 df_2023[\'Year\'] = 2023 combined_df = pd.concat([df_2022, df_2023], ignore_index=True) # Step 5: Summary Statistics summary_stats = combined_df.groupby(\'Year\')[\'TotalSales\'].agg([\'mean\', \'std\']).reset_index() # Step 6: Reshape DataFrame long_format_df = combined_df.melt(id_vars=[\'Store\', \'Year\'], value_vars=[\'Quarter1\', \'Quarter2\', \'Quarter3\', \'Quarter4\'], var_name=\'Quarter\', value_name=\'Sales\') # Step 7: Custom Transformation (SalesChange) long_format_df[\'SalesChange\'] = long_format_df.groupby([\'Store\', \'Quarter\'])[\'Sales\'].pct_change().multiply(100) return df_2022, df_2023, combined_df, summary_stats, long_format_df"},{"question":"# Question You have been provided with several datasets that are used to monitor sensor readings from a set of devices in a network. These readings are captured in a sparse format to save memory as most readings are missing (not recorded). Your task is to create a pandas `DataFrame` with sparse data and manipulate it as requested. Input 1. A dictionary representing the readings from 4 sensors over 10,000 time periods: ```python readings = { \'sensor_1\': np.random.randn(10000), \'sensor_2\': np.random.randn(10000), \'sensor_3\': np.random.randn(10000), \'sensor_4\': np.random.randn(10000), } ``` Set 98% of each sensor\'s reading to NaN to simulate the sparsity. 2. Two functions `process_readings` and `conversion_interaction` that you will implement. Function 1: process_readings 1. Create a pandas `DataFrame` using the provided `readings` dictionary. 2. Convert the dataframe to use `SparseDtype` with `float` dtype and `NaN` as fill value. 3. Calculate and return the density of the sparse dataframe. 4. Return the first 5 rows of the dataframe and its memory usage in bytes. Function 2: conversion_interaction 1. Convert the sparse dataframe back to a dense format. 2. Convert the dense dataframe to a `scipy.sparse.csr_matrix`. 3. Return the created sparse dataframe, the dense dataframe, and the `csr_matrix`. Constraints - Use random seed value `42` to ensure reproducibility when manipulating the `readings` dictionary. Output - The function `process_readings` must return a tuple containing the density, first 5 rows of the sparse dataframe, and its memory usage. - The function `conversion_interaction` must return a tuple containing the sparse dataframe, dense dataframe, and the CSR matrix. Example: ```python import numpy as np import pandas as pd from scipy.sparse import csr_matrix # Set random seed for reproducibility np.random.seed(42) # Simulate sparse readings data readings = { \'sensor_1\': np.random.randn(10000), \'sensor_2\': np.random.randn(10000), \'sensor_3\': np.random.randn(10000), \'sensor_4\': np.random.randn(10000), } for k in readings: # Introduce sparsity readings[k][np.random.rand(10000) < 0.98] = np.nan # function to process readings def process_readings(readings): df = pd.DataFrame(readings) sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) density = sdf.sparse.density memory_usage = sdf.memory_usage().sum() return density, sdf.head(), memory_usage # function to handle conversions def conversion_interaction(readings): df = pd.DataFrame(readings) sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) dense_df = sdf.sparse.to_dense() sp_matrix = csr_matrix(dense_df.values) return sdf, dense_df, sp_matrix # Test functions density, head, mem_usage = process_readings(readings) print(f\\"Density: {density}\\") print(f\\"First 5 rows:n{head}\\") print(f\\"Memory usage: {mem_usage} bytes\\") sdf, dense_df, sp_matrix = conversion_interaction(readings) print(f\\"Sparse DataFrame:n{sdf}\\") print(f\\"Dense DataFrame:n{dense_df}\\") print(f\\"Scipy Sparse Matrix:n{sp_matrix}\\") ``` Ensure your implementations of `process_readings` and `conversion_interaction` meet the outputs shown in the example.","solution":"import numpy as np import pandas as pd from scipy.sparse import csr_matrix # Set random seed for reproducibility np.random.seed(42) # Simulate sparse readings data readings = { \'sensor_1\': np.random.randn(10000), \'sensor_2\': np.random.randn(10000), \'sensor_3\': np.random.randn(10000), \'sensor_4\': np.random.randn(10000), } for key in readings: # Introduce sparsity readings[key][np.random.rand(10000) < 0.98] = np.nan # Function to process readings data def process_readings(readings): df = pd.DataFrame(readings) sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) density = sdf.sparse.density memory_usage = sdf.memory_usage().sum() return density, sdf.head(), memory_usage # Function for conversion interaction def conversion_interaction(readings): df = pd.DataFrame(readings) sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) dense_df = sdf.sparse.to_dense() sp_matrix = csr_matrix(dense_df.values) return sdf, dense_df, sp_matrix"},{"question":"# Question: Advanced Traceback Handling **Objective:** Write a Python function that: 1. Executes a user-provided code block. 2. Captures any exceptions raised. 3. Pretty prints the stack trace of the exception, including information about local variables and a custom message. **Function Signature:** ```python def execute_and_trace(code_block: str): pass ``` # Input: - `code_block` (str): A string containing the Python code to be executed. # Output: - The function does not return a value but prints the formatted traceback information to the standard output. The printed output should include: - The exception type and message. - A stack trace of where the exception occurred. - Local variables in each frame of the stack trace. - A custom message indicating the nature of the exception. # Constraints: - You may assume that the `code_block` string is valid Python code. - You are not allowed to use `eval` or any similar functions that could pose security risks. Use `exec` for executing the code. - The traceback should be printed using functions and classes provided by the `traceback` module. # Example: ```python # Example user code to be tested. code_block = \'\'\' def function_a(): x = 10 function_b() def function_b(): y = \\"hello\\" z = y + 10 # This will raise a TypeError function_a() \'\'\' # Call the function execute_and_trace(code_block) # Expected Output (formatted similarly): # Traceback (most recent call last): # File \\"<exec_code>\\", line 10, in <module> # function_a() # File \\"<exec_code>\\", line 4, in function_a # function_b() # File \\"<exec_code>\\", line 7, in function_b # z = y + 10 # TypeError: can only concatenate str (not \\"int\\") to str # Local variables by frame: # - In function_b: y = \'hello\' # - In function_a: x = 10 # Custom Message: A TypeError occurred while executing the code block. ``` **Requirements:** - Use appropriate functions from the `traceback` module to capture and format the traceback. - Print local variables for each frame in the stack trace. - Append a custom message at the end of the traceback. # Notes: - Ensure that the custom message is informative and clearly indicates the occurrence of an exception. - Handle negative test cases where no exceptions might occur gracefully without crashing. Test your solution with various examples to ensure robustness and correctness of the stack trace formatting and the handling of local variables.","solution":"import traceback import sys def execute_and_trace(code_block: str): Executes the provided code block and handles exceptions by pretty printing the stack trace along with local variables and a custom message. try: exec(code_block) except Exception as e: exc_type, exc_value, exc_tb = sys.exc_info() tb = traceback.extract_tb(exc_tb) print(\\"Traceback (most recent call last):\\") for frame in tb: print(f\' File \\"{frame.filename}\\", line {frame.lineno}, in {frame.name}\') print(f\' {frame.line.strip()}\') # Printing local variables if available if frame.locals: print(\\" Local variables:\\") for var_name, var_value in frame.locals.items(): print(f\' {var_name} = {var_value}\') print(f\'{exc_type.__name__}: {str(exc_value)}\') print(\\"Custom Message: An exception occurred while executing the code block.\\")"},{"question":"# Python Coding Assessment Question Objective: You are to implement a function that encodes a given piece of text using Base64 encoding and then decodes it back to its original form. This will test your understanding of the `base64` package and your ability to work with its encoding and decoding functions. Requirements: 1. Implement the function `encode_and_decode(text: str) -> str` which performs the following: - Encodes the input text using Base64 encoding. - Decodes the encoded text back to its original form. - Returns the decoded text. 2. Your implementation should handle any errors that might occur during the encoding or decoding process. Function Signature: ```python def encode_and_decode(text: str) -> str: pass ``` Input Format: - `text`: A non-empty string containing the text to be encoded and decoded. Output Format: - A string which is the original text after encoding and decoding. Example: ```python assert encode_and_decode(\\"Hello, World!\\") == \\"Hello, World!\\" assert encode_and_decode(\\"Python Programming\\") == \\"Python Programming\\" ``` Constraints: - The input string `text` will have a length between 1 and 1000 characters. - You should handle both ASCII and non-ASCII characters in the input text. Guidelines: 1. Use `base64.b64encode` to encode the text. 2. Use `base64.b64decode` to decode the encoded text. 3. Ensure that you handle any potential exceptions or errors gracefully. Good luck!","solution":"import base64 def encode_and_decode(text: str) -> str: Encodes the input text using Base64 encoding and then decodes it back to its original form. Parameters: text (str): The input text to be encoded and then decoded. Returns: str: The original text after being encoded and decoded. try: # Convert the text to bytes text_bytes = text.encode(\'utf-8\') # Encode the text using Base64 encoded_bytes = base64.b64encode(text_bytes) # Decode the Base64 encoded text back to bytes decoded_bytes = base64.b64decode(encoded_bytes) # Convert the decoded bytes back to string decoded_text = decoded_bytes.decode(\'utf-8\') return decoded_text except Exception as e: # In case of any exception, print it and return an empty string print(f\\"An error occurred: {e}\\") return \\"\\""},{"question":"**Machine Learning Model Implementation and Evaluation** **Objective:** Implement a machine learning pipeline using scikit-learn to classify a given dataset into predefined classes. The dataset is a CSV file with labeled data, including both numeric and categorical features. You are required to implement data preprocessing, model selection, training, and evaluation steps within the pipeline. **Input:** - A CSV file `data.csv` with the following structure: - `n` rows (examples) and `m` columns (features and label). - The last column is `label`, which contains the class labels. - The remaining columns contain feature data, with some columns being numeric and some categorical. **Output:** - The accuracy of the trained model on a hold-out test set. **Requirements:** 1. **Data Preprocessing:** - Handle missing values by filling numeric features with the median value and categorical features with the most frequent value. - Encode categorical features using one-hot encoding. 2. **Model Selection and Training:** - Split the dataset into training and testing sets using an 80-20 split. - Train a Random Forest classifier using the training set. 3. **Model Evaluation:** - Evaluate the model using the test set and print the accuracy. **Constraints:** - You must use scikit-learn for all machine learning tasks. - The solution should be implemented in Python. **Example:** Given the following `data.csv`: ``` feature1,feature2,feature3,label 1,A,3.5,0 2,B,2.1,1 3,A,,0 4,C,4.0,1 5,B,NaN,0 ``` The output should be the accuracy of the Random Forest model on the test set. **Function Signature:** ```python def train_and_evaluate_model(csv_file_path: str) -> float: # Your implementation here ``` **Explanation:** 1. **Data Loading:** Load the CSV file into a Pandas DataFrame. 2. **Handling Missing Values:** - Fill missing numeric values with the median of the respective columns. - Fill missing categorical values with the most frequent value in the respective columns. 3. **Encoding Categorical Features:** Use one-hot encoding for categorical features. 4. **Data Splitting:** Split the data into training (80%) and testing (20%) sets. 5. **Model Training:** Train a Random Forest classifier on the training data. 6. **Model Evaluation:** Evaluate the model on the testing data and return the accuracy.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline def train_and_evaluate_model(csv_file_path: str) -> float: # Load the dataset df = pd.read_csv(csv_file_path) # Separate features and label X = df.iloc[:, :-1] y = df.iloc[:, -1] # Identify numeric and categorical columns numeric_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = X.select_dtypes(include=[\'object\']).columns # Create transformers for the pipeline numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Create a preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Create a pipeline with a preprocessor and a classifier pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier(random_state=42)) ]) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model pipeline.fit(X_train, y_train) # Make predictions on the test set y_pred = pipeline.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) # Return the accuracy return accuracy"},{"question":"**Coding Challenge: Introspect a Callable** You are required to create a function, `introspect_callable`, that takes any Python callable (function, method, lambda, etc.) as input and returns a dictionary containing detailed information about the callable. The function should utilize the `inspect` module to gather this information. **Function Signature:** ```python def introspect_callable(func: callable) -> dict: pass ``` **Input:** - `func`: A callable object (function or method) **Output:** - A dictionary with the following keys and corresponding values: - `name`: The name of the callable. - `type`: The type of callable (function, method, etc.). - `signature`: The signature of the callable. - `docstring`: The documentation string of the callable. - `source_code`: The source code of the callable. - `parameters`: A dictionary where each key is a parameter name and the value is another dictionary containing `kind`, `default`, and `annotation` of that parameter. **Constraints:** - The function should handle all types of callable objects including those with default parameters, keyword-only parameters, and annotations. - If the source code is not available (e.g., for built-in functions), handle it gracefully. **Example Usage:** ```python def sample_function(a: int, b: str = \\"default\\") -> bool: This is a sample function. return a > 0 and b != \\"\\" result = introspect_callable(sample_function) expected_result = { \'name\': \'sample_function\', \'type\': \'function\', \'signature\': \'(a: int, b: str = \'default\') -> bool\', \'docstring\': \'This is a sample function.\', \'source_code\': \'def sample_function(a: int, b: str = \\"default\\") -> bool:n n This is a sample function.n n return a > 0 and b != \\"\\"n\', \'parameters\': { \'a\': {\'kind\': \'POSITIONAL_OR_KEYWORD\', \'default\': None, \'annotation\': <class \'int\'>}, \'b\': {\'kind\': \'POSITIONAL_OR_KEYWORD\', \'default\': \'default\', \'annotation\': <class \'str\'>} } } assert result == expected_result ``` **Note:** - Be sure to use appropriate functions from the `inspect` module. - Consider edge cases such as callables without source code or annotations.","solution":"import inspect def introspect_callable(func: callable) -> dict: Returns detailed information about a callable using the inspect module. try: source_code = inspect.getsource(func) except (TypeError, OSError): source_code = None signature = inspect.signature(func) parameters_info = {} for param_name, param in signature.parameters.items(): parameters_info[param_name] = { \'kind\': param.kind.name, \'default\': param.default if param.default is not param.empty else None, \'annotation\': param.annotation if param.annotation is not param.empty else None } return { \'name\': func.__name__, \'type\': type(func).__name__, \'signature\': str(signature), \'docstring\': inspect.getdoc(func), \'source_code\': source_code, \'parameters\': parameters_info }"},{"question":"**Question: Implement a Web Scraper with Asynchronous Tasks** You are tasked with developing an asynchronous web scraper using Python\'s `asyncio` and `aiohttp` libraries. The scraper will fetch data from multiple URLs concurrently and process the data. # Requirements: 1. **Function Signature**: ```python async def fetch_concurrent(urls: List[str]) -> List[str]: ``` 2. **Input**: - `urls`: A list of URLs (strings). Each URL is a valid HTTP/HTTPS address. 3. **Output**: - Returns a list of HTML contents (strings) corresponding to each URL. 4. **Constraints**: - Ensure that the coroutine fetches from all the given URLs concurrently. - Properly handle exceptions for any failed HTTP requests and log them without disrupting the remaining tasks. - Avoid common pitfalls such as forgotten `await` statements and never-retrieved exceptions. - Use an appropriate method to gather the results of all tasks and return them in the same order as the input URLs. - Implement a debug mode using the `asyncio` logger. 5. **Performance Requirements**: - The solution must handle at least 20 URLs efficiently. - The time complexity should be optimal for concurrent web fetching. # Example Usage: ```python import asyncio import aiohttp from typing import List async def fetch_concurrent(urls: List[str]) -> List[str]: async def fetch(session, url): try: async with session.get(url) as response: return await response.text() except Exception as e: logging.error(f\\"Failed to fetch {url}: {e}\\") return \\"\\" async with aiohttp.ClientSession() as session: tasks = [fetch(session, url) for url in urls] return await asyncio.gather(*tasks) # Example URLs urls = [\\"https://example.com\\", \\"https://httpbin.org/get\\"] # Run the fetch_concurrent function results = asyncio.run(fetch_concurrent(urls)) for result in results: print(result) ``` # Additional Notes: - Ensure to enable and demonstrate the debug mode, showing how it helps in development by catching pitfalls. - Use appropriate libraries (`asyncio`, `aiohttp`, `logging`, etc.). - Make sure to include proper exception handling and logging for better debugging and analysis of failed requests.","solution":"import asyncio import aiohttp import logging from typing import List logging.basicConfig(level=logging.DEBUG) async def fetch_concurrent(urls: List[str]) -> List[str]: async def fetch(session, url): try: async with session.get(url) as response: if response.status == 200: return await response.text() else: logging.error(f\\"Failed to fetch {url}: Status code {response.status}\\") return \\"\\" except Exception as e: logging.error(f\\"Failed to fetch {url}: {e}\\") return \\"\\" async with aiohttp.ClientSession() as session: tasks = [fetch(session, url) for url in urls] return await asyncio.gather(*tasks)"},{"question":"Concurrent File Processing Objective: Implement a Python program that reads from a large number of text files and processes their content concurrently. The goal is to count the frequency of each word across all the files and store the results in a concurrent-safe manner. You are to employ both threading and multiprocessing to achieve this task. Requirements: 1. **Reading Files with Threading**: - Implement a function `read_files_with_threads(file_paths: List[str], queue: Queue) -> None` that reads the content of multiple files concurrently using the `threading` module and puts the content of each file into a shared queue. 2. **Processing Files with Multiprocessing**: - Implement a function `process_files_with_multiprocessing(queue: Queue, result_dict: Dict[str, int]) -> None` that reads the content from the queue and processes them using the `multiprocessing` module to count the frequency of each word, updating a shared `result_dict`. 3. **Synchronization**: - Ensure that the `result_dict` used for storing word counts is managed in a thread-safe and process-safe manner. 4. **Main Function**: - Combine the above functionalities in a main function `main(file_paths: List[str]) -> Dict[str, int]` that: 1. Defines a queue for inter-thread communication. 2. Starts threads to read files. 3. Starts processes to count words. 4. Collects results and returns the final word count dictionary. Input and Output Formats: - **Input**: - `file_paths`: A list of paths to text files (List[str]). - **Output**: - Returns a dictionary with words as keys and their respective counts as values (Dict[str, int]). Constraints: - Assume that `file_paths` list contains valid paths to text files. - Implement efficient synchronization to prevent race conditions. - Effectively utilize both threading for I/O-bound tasks and multiprocessing for CPU-bound tasks. Performance Requirements: - The solution should handle a large number of files efficiently. - Ensure that file reading and processing are appropriately balanced to avoid bottlenecks. Example: ```python def main(file_paths: List[str]) -> Dict[str, int]: # Your implementation here pass file_paths = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] print(main(file_paths)) ``` # Notes: - You may use `threading.Thread`, `multiprocessing.Process`, `queue.Queue`, and `multiprocessing.Manager` for implementing your solution. - Make sure to handle exceptions and edge cases, such as empty files or files with non-text content.","solution":"from threading import Thread from multiprocessing import Process, Manager from queue import Queue from collections import defaultdict from typing import List, Dict import os def read_file(file_path: str, queue: Queue) -> None: try: with open(file_path, \'r\') as file: content = file.read() queue.put(content) except Exception as e: print(f\\"Error reading {file_path}: {e}\\") def read_files_with_threads(file_paths: List[str], queue: Queue) -> None: threads = [] for file_path in file_paths: thread = Thread(target=read_file, args=(file_path, queue)) thread.start() threads.append(thread) for thread in threads: thread.join() def word_count(text: str, result_dict: Dict[str, int]) -> None: words = text.split() for word in words: word = word.lower().strip(\'.,!?;\\"()[]\') if word: if word in result_dict: result_dict[word] += 1 else: result_dict[word] = 1 def process_files_with_multiprocessing(queue: Queue, result_dict: Dict[str, int]) -> None: processes = [] while not queue.empty(): text = queue.get() process = Process(target=word_count, args=(text, result_dict)) process.start() processes.append(process) for process in processes: process.join() def main(file_paths: List[str]) -> Dict[str, int]: queue = Queue() manager = Manager() result_dict = manager.dict() read_thread = Thread(target=read_files_with_threads, args=(file_paths, queue)) read_thread.start() read_thread.join() process_files_with_multiprocessing(queue, result_dict) return dict(result_dict)"},{"question":"**Objective**: Demonstrate your understanding of constructing MIME messages and handling different content types using the `email.mime` module in Python. # Task: Write a Python function named `create_mime_email` that accepts the following parameters: - `subject` (str): The subject of the email. - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `text` (str): The plain text content of the email. - `html` (str, optional): The HTML content of the email. Default is `None`. - `attachments` (list, optional): A list of tuples where each tuple contains: - `filename` (str): The name of the file. - `content` (bytes): The content of the file. - `mime_type` (str): The MIME type of the file (e.g., \'image/png\'). Default is `None`. The function should construct a MIME email with the provided text, and optionally add HTML content, and attachments if provided. It should return the email as a `MIMEMultipart` object. # Requirements: 1. The email should have both text and HTML content if `html` is provided, properly set up using `multipart/alternative`. 2. Attachments should be added to the email if provided. Each attachment should use the appropriate MIME class based on its MIME type. 3. Correctly set the headers for `Subject`, `From`, `To`, and `Date`. # Constraints: - You can assume that the `attachments` list will contain basic MIME types (e.g., \'image/png\', \'application/pdf\'). - You should not perform any file I/O operations in this function; assume that all necessary content is provided as function arguments. # Example: ```python def create_mime_email(subject, sender, recipient, text, html=None, attachments=None): pass # Your implementation here # Usage: email_object = create_mime_email( subject=\\"Test Email\\", sender=\\"sender@example.com\\", recipient=\\"recipient@example.com\\", text=\\"This is a plain text version of the email.\\", html=\\"<html><body><p>This is an HTML version of the email.</p></body></html>\\", attachments=[(\\"test.txt\\", b\\"Hello, World!\\", \\"text/plain\\")] ) print(email_object.as_string()) ``` # Additional Information: - Use `email.mime.multipart.MIMEMultipart` to create a multipart message. - Use `email.mime.text.MIMEText` for creating plain text and HTML parts. - Use appropriate classes like `email.mime.application.MIMEApplication`, `email.mime.image.MIMEImage`, and so on for attachments. - Ensure that the email is properly structured to comply with MIME standards.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders from email.utils import formatdate def create_mime_email(subject, sender, recipient, text, html=None, attachments=None): Creates a MIME email with the given parameters. Args: subject (str): The subject of the email. sender (str): The email address of the sender. recipient (str): The email address of the recipient. text (str): The plain text content of the email. html (str, optional): The HTML content of the email. Default is None. attachments (list, optional): A list of tuples where each tuple contains: filename (str): The name of the file. content (bytes): The content of the file. mime_type (str): The MIME type of the file (e.g., \'image/png\'). Default is None. Returns: MIMEMultipart: The constructed MIME email. # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Date\'] = formatdate(localtime=True) # Create the body with alternative parts (text and HTML) msg_alternative = MIMEMultipart(\'alternative\') msg.attach(msg_alternative) # Add plain text part part_text = MIMEText(text, \'plain\') msg_alternative.attach(part_text) # Add HTML part if provided if html: part_html = MIMEText(html, \'html\') msg_alternative.attach(part_html) # Add attachments if provided if attachments: for filename, content, mime_type in attachments: part = MIMEBase(*mime_type.split(\'/\')) part.set_payload(content) encoders.encode_base64(part) part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{filename}\\"\') msg.attach(part) return msg"},{"question":"**Objective**: Write a Python function `fetch_data_with_redirects()` which takes a URL as input, handles HTTP redirects, and returns the final URL and its content after all redirects have been processed. **Details**: - The function should: 1. Accept a URL as input. 2. Handle up to 10 redirects. 3. Return a tuple containing the final URL and its content. 4. Include custom User-Agent in the request headers. 5. Handle cookies using `HTTPCookieProcessor`. **Requirements**: - **Input**: - `url` (str): The initial URL to open. - **Output**: - Tuple `(final_url, content)`: - `final_url` (str): The final URL after handling all redirects. - `content` (str): The content of the response from the final URL, decoded using \'utf-8\'. - **Constraints**: - Handle up to a maximum of 10 redirects. - If the number of redirects exceeds 10, raise a `urllib.error.URLError` with an appropriate message. - Use a custom User-Agent \\"CustomUserAgent/1.0\\". **Example**: ```python url = \'http://example.com/start\' final_url, content = fetch_data_with_redirects(url) print(f\\"Final URL: {final_url}\\") print(f\\"Content: {content[:100]}\\") # Print first 100 characters of the content ``` **Your function should handle cases where the URL may redirect multiple times before reaching the final destination.** **Hints**: - Use `urllib.request.build_opener` to create an opener with `HTTPCookieProcessor` and `HTTPRedirectHandler`. - Customize the request headers to include a custom User-Agent. - Carefully manage the redirection limit to avoid infinite loops. **Note**: You may refer to the `urllib.request` documentation to understand the usage of various functions and classes provided.","solution":"import urllib.request import urllib.error def fetch_data_with_redirects(url): Fetches the final URL and its content after handling up to 10 redirects. Args: url (str): The initial URL to open. Returns: Tuple (final_url, content): final_url (str): The final URL after handling all redirects. content (str): The content of the response from the final URL, decoded using \'utf-8\'. Raises: urllib.error.URLError: If the number of redirects exceeds 10. MAX_REDIRECTS = 10 headers = {\'User-Agent\': \'CustomUserAgent/1.0\'} # Create an HTTP opener with cookie and redirect handlers opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(), urllib.request.HTTPRedirectHandler()) request = urllib.request.Request(url, headers=headers) try: with opener.open(request) as response: final_url = response.geturl() content = response.read().decode(\'utf-8\') # Handle up to MAX_REDIRECTS, raising an error if exceeded redirect_count = 0 while response.geturl() != final_url and redirect_count < MAX_REDIRECTS: redirect_count += 1 request = urllib.request.Request(response.geturl(), headers=headers) with opener.open(request) as response: final_url = response.geturl() content = response.read().decode(\'utf-8\') if redirect_count >= MAX_REDIRECTS: raise urllib.error.URLError(\\"Too many redirects\\") return final_url, content except urllib.error.URLError as e: raise e"},{"question":"You are tasked with writing a Python function that will analyze the Unix group database using the \\"grp\\" module provided in Python 3.10. This function should determine the most populated group (the group with the most members) and return its details. Function Signature ```python def most_populated_group() -> dict: pass ``` Requirements - Use the `grp.getgrall()` function to obtain all available group entries. - Determine which group has the most members. - Return a dictionary containing the following details of that group: - `name`: the name of the group (`gr_name`) - `gid`: the numerical group ID (`gr_gid`) - `members`: a list of all members in the group (`gr_mem`) Input/Output - **Input**: The function does not take any input parameters. - **Output**: The function should return a dictionary with the keys \'name\', \'gid\', and \'members\'. Constraints - Handle the condition where there may be groups without any members. - If there are multiple groups with the same maximum number of members, return the first one encountered. Example Suppose the following entries exist in the group database: ```python [ grp.struct_group((\'group1\', \'x\', 1001, [\'user1\', \'user2\'])), grp.struct_group((\'group2\', \'x\', 1002, [])), grp.struct_group((\'group3\', \'x\', 1003, [\'user3\'])), grp.struct_group((\'group4\', \'x\', 1004, [\'user4\', \'user5\', \'user6\'])) ] ``` Calling `most_populated_group()` should return: ```python { \'name\': \'group4\', \'gid\': 1004, \'members\': [\'user4\', \'user5\', \'user6\'] } ``` Performance Requirements - The solution should handle large group databases efficiently. Utilize error handling and Pythonic coding practices to ensure robustness and readability of your implementation.","solution":"import grp def most_populated_group() -> dict: all_groups = grp.getgrall() most_members_group = max(all_groups, key=lambda g: len(g.gr_mem), default=None) if most_members_group: return { \'name\': most_members_group.gr_name, \'gid\': most_members_group.gr_gid, \'members\': most_members_group.gr_mem } else: return { \'name\': \'\', \'gid\': 0, \'members\': [] }"},{"question":"# Persistent Book Library System You are required to create a persistent system using the `shelve` module to manage a book library. This system should be able to add new books, retrieve books by their title, and list all stored books. Each book should have a title, author, and year of publication. Specifications **Functions to implement:** 1. `add_book(library_filename: str, title: str, author: str, year: int) -> None` - **Description**: Adds a new book to the library. - **Parameters**: - `library_filename`: The filename where the library is stored. - `title`: The title of the book. - `author`: The author of the book. - `year`: The year the book was published. - **Returns**: This function does not return anything. It should persist the new book in the library file. 2. `get_book(library_filename: str, title: str) -> dict` - **Description**: Retrieves a book by its title. - **Parameters**: - `library_filename`: The filename where the library is stored. - `title`: The title of the book to retrieve. - **Returns**: A dictionary with keys \'title\', \'author\', \'year\' if the book is found. If the book is not found, return an empty dictionary. 3. `list_books(library_filename: str) -> list` - **Description**: Lists all books in the library. - **Parameters**: - `library_filename`: The filename where the library is stored. - **Returns**: A list of dictionaries, each containing \'title\', \'author\', and \'year\' of a book. **Constraints:** - The title of each book is unique. - The library should be stored persistently and safely handle multiple read/write operations. **Example Usage:** ```python # Adding books add_book(\'library.db\', \'1984\', \'George Orwell\', 1949) add_book(\'library.db\', \'To Kill a Mockingbird\', \'Harper Lee\', 1960) # Retrieving a book book = get_book(\'library.db\', \'1984\') print(book) # Output: {\'title\': \'1984\', \'author\': \'George Orwell\', \'year\': 1949} # Listing all books books = list_books(\'library.db\') print(books) # Output: # [ # {\'title\': \'1984\', \'author\': \'George Orwell\', \'year\': 1949}, # {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year\': 1960} # ] ``` Use the `shelve` module to implement this functionality persistently. Ensure that the library file is properly closed after each operation to prevent data loss.","solution":"import shelve def add_book(library_filename: str, title: str, author: str, year: int) -> None: Adds a new book to the library. with shelve.open(library_filename) as library: library[title] = {\'title\': title, \'author\': author, \'year\': year} def get_book(library_filename: str, title: str) -> dict: Retrieves a book by its title. with shelve.open(library_filename) as library: return library.get(title, {}) def list_books(library_filename: str) -> list: Lists all books in the library. with shelve.open(library_filename) as library: return list(library.values())"},{"question":"Background You are required to create a Python function to manage MIME-type configurations using the `mailcap` package. Your function will read configurations from mailcap files and determine the appropriate command to execute based on a given MIME type. Problem Statement Implement a function `execute_command_for_mime_type` that takes in three parameters: 1. **mime_type (str)**: The MIME type to look for in mailcap files. 2. **filename (str)**: The filename to be processed. 3. **params (dict)**: A dictionary of additional parameters where keys are parameter names and values are parameter values. Your function should: 1. Use `mailcap.getcaps()` to retrieve the system and user mailcap file entries. 2. Use `mailcap.findmatch()` to find a suitable mailcap entry for the provided MIME type. 3. Replace placeholders (`%s`, `%{param}`) in the command with the provided filename and parameters. 4. Execute the resulting command using `os.system()`. 5. Return the command that was executed as a string. If no suitable entry is found or an error occurs, return the string \\"No valid command found.\\" Constraints - Ensure that the filename and parameters do not introduce security vulnerabilities (e.g., shell metacharacters). - The function should handle cases where no suitable entry is found gracefully. - You may assume that the provided MIME type and parameters are well-formed strings. Example ```python def execute_command_for_mime_type(mime_type, filename, params): # Implement your solution here # Example usage: result = execute_command_for_mime_type(\'video/mpeg\', \'video_file.mpg\', {\'id\': \'123\', \'number\': \'1\', \'total\': \'5\'}) print(result) # Output might be \'xmpeg video_file.mpg\' ``` Notes - You need to import necessary modules and handle potential exceptions. - You should validate the input parameters to avoid injecting malicious content into shell commands. - Use the functions described in the provided `mailcap` documentation.","solution":"import os import mailcap def execute_command_for_mime_type(mime_type, filename, params): try: # Get mailcap entries caps = mailcap.getcaps() # Find a matching command command, _ = mailcap.findmatch(caps, mime_type, filename=filename, plist=params) if not command: return \\"No valid command found\\" # Replace parameters in the command for key, value in params.items(): command = command.replace(f\\"%{{{key}}}\\", value) command = command.replace(\\"%s\\", filename) # Execute the command os.system(command) return command except Exception as e: return \\"No valid command found\\""},{"question":"# Python Coding Assessment Question Problem Statement You are tasked with writing a Python function that scans all Python (`.py`) files in a given directory (and its subdirectories) to find occurrences of TODO comments. A TODO comment is any line of code that contains the keyword \\"TODO\\". In addition, your function should be able to produce a detailed or a summary report based on a verbosity flag. Function Signature ```python def find_todos(path: str, verbose: bool = True) -> None: Recursively scans the given directory for Python files and checks for TODO comments. Parameters: path (str): The path to the directory or file to check. verbose (bool): If True, prints detailed TODO comments with filenames and line numbers. If False, prints only the filenames of the files containing TODO comments. Returns: None: This function prints the result directly. ``` Input 1. `path` (str): A string representing the path to a directory or a specific Python file. 2. `verbose` (bool): A boolean flag indicating the level of detail for the output. Output The function will print the results directly: - If `verbose` is `True`, print each TODO comment with the filename and line number. - If `verbose` is `False`, print only the filenames of the files containing TODO comments. Constraints - You should not process symbolic links. - Only `.py` files should be checked. - The function should handle any exceptions gracefully and print appropriate error messages. Example For a directory structure as follows: ``` /path/to/dir ├── file1.py ├── file2.py └── subdir └── file3.py ``` Where `file1.py` contains: ```python # TODO: Refactor this function print(\\"Hello, World!\\") ``` And `file3.py` contains: ```python # This is a normal comment print(\\"Hello, Subdir!\\") # TODO: Add more functionality ``` Calling: ```python find_todos(\\"/path/to/dir\\", verbose=True) ``` Would result in: ``` file1.py:1: # TODO: Refactor this function file3.py:3: # TODO: Add more functionality ``` And calling: ```python find_todos(\\"/path/to/dir\\", verbose=False) ``` Would result in: ``` file1.py file3.py ``` Additional Notes - Ensure your solution handles large directory trees efficiently. - Use appropriate exceptions and handle them gracefully to avoid program crashes. Good luck!","solution":"import os def find_todos(path: str, verbose: bool = True) -> None: Recursively scans the given directory for Python files and checks for TODO comments. Parameters: path (str): The path to the directory or file to check. verbose (bool): If True, prints detailed TODO comments with filenames and line numbers. If False, prints only the filenames of the files containing TODO comments. Returns: None: This function prints the result directly. # Check if the provided path is valid if not os.path.exists(path): print(f\\"Error: The path \'{path}\' does not exist.\\") return todo_files = {} # Walk through the directory for root, _, files in os.walk(path): for file in files: if file.endswith(\\".py\\"): file_path = os.path.join(root, file) try: with open(file_path, \'r\') as f: lines = f.readlines() for idx, line in enumerate(lines): if \\"TODO\\" in line: if file_path not in todo_files: todo_files[file_path] = [] todo_files[file_path].append((idx + 1, line.strip())) except Exception as e: print(f\\"Error reading file {file_path}: {e}\\") # Print results if verbose: for file_path, todos in todo_files.items(): for line_num, todo in todos: print(f\\"{file_path}:{line_num}: {todo}\\") else: for file_path in todo_files.keys(): print(file_path)"},{"question":"Task Write a Python function called `fetch_status_code` that performs the following tasks: 1. Creates a connection to a specified HTTP or HTTPS server. 2. Sends an HTTP GET request to a specified URL path. 3. Receives the response from the server. 4. Returns the HTTP status code and reason phrase from the server response. Function Signature ```python def fetch_status_code(protocol: str, host: str, path: str) -> str: pass ``` Parameters - `protocol` (str): The protocol to use for the connection (\'http\' or \'https\'). - `host` (str): The host name or IP address of the server (e.g., \'www.python.org\'). - `path` (str): The URL path to send the request to (e.g., \'/\'). Returns - (str): A string formatted as \\"<status_code> <reason_phrase>\\", e.g., \\"200 OK\\" or \\"404 Not Found\\". Example ```python print(fetch_status_code(\'https\', \'www.python.org\', \'/\')) # Output: \\"200 OK\\" print(fetch_status_code(\'http\', \'www.example.com\', \'/nonexistent\')) # Output: \\"404 Not Found\\" ``` Constraints - The function should handle both HTTP and HTTPS protocols. - The function should manage exceptions properly, ensuring any connection errors, invalid URLs, or unexpected status codes are addressed. - Use the `http.client` module for HTTP and HTTPS connections. - The call should be properly closed to prevent resource leaks. Performance - The implementation should efficiently handle the connection and request/response cycle, minimizing unnecessary operations. Hints - Use `http.client.HTTPConnection` for HTTP and `http.client.HTTPSConnection` for HTTPS. - Utilize the `request`, `getresponse`, and `close` methods from the connection classes as described in the documentation. - Handle cases where the host does not exist or the URL is invalid by using appropriate exception handling.","solution":"import http.client def fetch_status_code(protocol: str, host: str, path: str) -> str: if protocol not in [\'http\', \'https\']: raise ValueError(\\"Protocol must be \'http\' or \'https\'\\") conn = None try: if protocol == \'http\': conn = http.client.HTTPConnection(host) else: conn = http.client.HTTPSConnection(host) conn.request(\\"GET\\", path) response = conn.getresponse() status = f\\"{response.status} {response.reason}\\" except Exception as e: status = f\\"Error: {str(e)}\\" finally: if conn: conn.close() return status"},{"question":"# Email Management with `imaplib` Objective Your task is to use the `imaplib` module to implement a function that connects to an IMAP4 server, logs in, searches for emails based on a specific criterion, marks them as read (seen), and then deletes emails that are older than a given number of days. Function Signature ```python def manage_emails(server: str, username: str, password: str, search_criterion: str, days_old: int) -> int: Connect to IMAP server, search, mark as seen, and delete old emails. Args: - server (str): The address of the IMAP4 server. - username (str): The username for login. - password (str): The password for login. - search_criterion (str): The search criterion to find specific emails. - days_old (int): The number of days to use as a cutoff for deleting old emails. Returns: - int: The number of emails that were deleted. ``` Input 1. `server`: A string representing the server address (e.g., `\\"imap.gmail.com\\"`). 2. `username`: A string for the username to log in to the server. 3. `password`: A string for the password to log in to the server. 4. `search_criterion`: A string representing the search criterion (e.g., `\'UNSEEN\'` or `\'FROM \\"example@example.com\\"\'`). 5. `days_old`: An integer representing the cutoff number of days for deleting old emails. Output - Return an integer representing the number of emails that were deleted. Constraints - You must use the `imaplib` module. - Do proper error handling for connection issues, login failures, and invalid searches. - Ensure that the mailbox is cleaned up properly by closing it and logging out. Example ```python # Example Usage deleted_count = manage_emails(\\"imap.example.com\\", \\"user\\", \\"pass\\", \'FROM \\"example@example.com\\"\', 30) print(deleted_count) # Example Output: 5 ``` Implementation Notes - The function should connect to the server and log in using the provided credentials. - Select the \\"INBOX\\" mailbox and search for emails based on the `search_criterion`. - Fetch the found emails and mark them as seen. - Calculate the cutoff time for `days_old` days before the current date. - Delete all emails older than the calculated cutoff time. - Use the `IMAP4.expunge()` method to permanently remove deleted messages. - Properly handle exceptions and errors, ensuring that the connection is closed even when errors occur. Good luck!","solution":"import imaplib import email from email.utils import parsedate_to_datetime from datetime import datetime, timedelta def manage_emails(server: str, username: str, password: str, search_criterion: str, days_old: int) -> int: Connect to IMAP server, search, mark as seen, and delete old emails. Args: - server (str): The address of the IMAP4 server. - username (str): The username for login. - password (str): The password for login. - search_criterion (str): The search criterion to find specific emails. - days_old (int): The number of days to use as a cutoff for deleting old emails. Returns: - int: The number of emails that were deleted. try: # Connect to the server and login mail = imaplib.IMAP4_SSL(server) mail.login(username, password) mail.select(\\"inbox\\") # Select the inbox # Search for emails based on the provided criterion result, data = mail.search(None, search_criterion) if result != \'OK\': raise Exception(\\"Error searching for emails\\") email_ids = data[0].split() deleted_count = 0 cutoff_date = datetime.now() - timedelta(days=days_old) for email_id in email_ids: result, data = mail.fetch(email_id, \'(RFC822)\') if result != \'OK\': raise Exception(\\"Error fetching email\\") for response_part in data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) email_date = parsedate_to_datetime(msg[\'Date\']) if email_date < cutoff_date: mail.store(email_id, \'+FLAGS\', \'Seen\') mail.store(email_id, \'+FLAGS\', \'Deleted\') deleted_count += 1 mail.expunge() mail.close() mail.logout() return deleted_count except Exception as e: print(f\\"An error occurred: {e}\\") try: mail.logout() except: pass return 0"},{"question":"# Descriptor-Based Validation System Objective: Create a system using descriptors in Python to validate certain attributes of a class upon assignment. This exercise will test your understanding of creating and using descriptors effectively. Requirements: 1. **Create Validator Base Class:** - Define a base class `Validator` that will serve as an abstract base class and a managed attribute descriptor. - Implement the `__set__`, `__get__`, and `__set_name__` methods in the `Validator` class. - Include an abstract method `validate()` that must be implemented by any subclass. 2. **Implement Custom Validators:** - Create three subclasses of `Validator`: - `String`: Ensure the attribute value is of type `str` and also allow a user-defined validation predicate. - `Number`: Ensure the attribute value is either an `int` or `float`. Optionally, validate if it\'s within a specified range. - `OneOf`: Ensure the attribute value is one of a specified set of options. 3. **Create a DataClass utilizing these Validators:** - Define a `Product` class that includes attributes `name`, `category`, and `price` using the validators: - `name`: Must be a `String` with a minimum length of 3 and a custom predicate that ensures it\'s composed of alphabetic characters only. - `category`: Must be one of the values from a predefined set (like `\\"electronic\\"`, `\\"furniture\\"`, `\\"clothing\\"`, etc.). - `price`: Must be a `Number` with a minimum value of 0. Specifications: - Implement the `Validator` base class: ```python from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass ``` - Implement `String`, `Number`, and `OneOf` validators: ```python class String(Validator): def __init__(self, minsize=None, predicate=None): self.minsize = minsize self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(\'Expected a string\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'String is too short, minimum length is {self.minsize}\') if self.predicate and not self.predicate(value): raise ValueError(f\'The value {value} does not satisfy the predicate.\') class Number(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(\'Expected an int or float\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Value should be at least {self.minvalue}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Value should not exceed {self.maxvalue}\') class OneOf(Validator): def __init__(self, *options): self.options = set(options) def validate(self, value): if value not in self.options: raise ValueError(f\'Expected one of {self.options}\') ``` - Define the `Product` class: ```python class Product: name = String(minsize=3, predicate=str.isalpha) category = OneOf(\'electronic\', \'furniture\', \'clothing\') price = Number(minvalue=0) def __init__(self, name, category, price): self.name = name self.category = category self.price = price ``` Testing: ```python # Valid product try: p = Product(\'Laptop\', \'electronic\', 999.99) print(p.name, p.category, p.price) except (TypeError, ValueError) as e: print(e) # Invalid name (too short) try: p = Product(\'PC\', \'electronic\', 999.99) except (TypeError, ValueError) as e: print(e) # Invalid category try: p = Product(\'Table\', \'wood\', 75.0) except (TypeError, ValueError) as e: print(e) # Invalid price (negative) try: p = Product(\'Chair\', \'furniture\', -25.0) except (TypeError, ValueError) as e: print(e) ``` # Submission: Submit your implementation of the `Validator`, `String`, `Number`, `OneOf`, and `Product` classes.","solution":"from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass class String(Validator): def __init__(self, minsize=None, predicate=None): self.minsize = minsize self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(\'Expected a string\') if self.minsize is not None and len(value) < self.minsize: raise ValueError(f\'String is too short, minimum length is {self.minsize}\') if self.predicate and not self.predicate(value): raise ValueError(f\'The value {value} does not satisfy the predicate.\') class Number(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, (int, float)): raise TypeError(\'Expected an int or float\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Value should be at least {self.minvalue}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Value should not exceed {self.maxvalue}\') class OneOf(Validator): def __init__(self, *options): self.options = set(options) def validate(self, value): if value not in self.options: raise ValueError(f\'Expected one of {self.options}\') class Product: name = String(minsize=3, predicate=str.isalpha) category = OneOf(\'electronic\', \'furniture\', \'clothing\') price = Number(minvalue=0) def __init__(self, name, category, price): self.name = name self.category = category self.price = price"},{"question":"**Objective:** Demonstrate your proficiency with pandas Index objects by creating and manipulating various Index types to perform advanced data indexing and query operations. **Problem Statement:** You are provided with the following data about multiple students and their quiz scores over a period of time. Each student can take multiple quizzes, and each quiz occurs at a different time. The goal is to create a cohesive system that allows us to query quiz scores efficiently using different types of indexing. **Dataset:** - `students`: A list of student names (e.g., [\\"Alice\\", \\"Bob\\", \\"Charlie\\"]) - `scores`: A list of lists where each sublist contains scores of quizzes taken by each student (e.g., [[88, 92], [79, 85, 91], [93, 87, 78]]) - `quiz_times`: A list of lists where each sublist contains the timestamps of quizzes taken by each student (e.g., [[\\"2021-01-01 10:00\\", \\"2021-01-02 11:00\\"], [\\"2021-01-03 09:00\\", \\"2021-01-05 14:00\\", \\"2021-01-06 13:00\\"], [\\"2021-01-01 11:00\\", \\"2021-01-02 15:00\\", \\"2021-01-05 10:00\\"]]) # Tasks: 1. **Create a MultiIndex DataFrame:** - The primary index should be the student names. - The secondary level of the index should be the timestamps of the quizzes. 2. **Transform the timestamps into a `DatetimeIndex` for efficient time-based querying.** 3. **Add an additional level to the DataFrame\'s index to represent the quiz number (1, 2, 3,...) taken by each student. Use a `RangeIndex` for this purpose.** 4. **Calculate the average score per student using the index levels you created, and populate a new column `average_score` with these values.** 5. **Query the scores DataFrame to find:** - All quiz scores for a given student. - All quiz scores within a specific time period for any student. **Input:** - `students`, `scores`, and `quiz_times` as described above. **Output:** - A pandas DataFrame with a MultiIndex containing the specified levels and columns for scores and average scores. - A function `get_scores_by_student(student_name: str) -> pd.DataFrame` to fetch all quiz scores of a specified student. - A function `get_scores_in_time_range(start_time: str, end_time: str) -> pd.DataFrame` to fetch all quiz scores in the specified time range. **Constraints:** - You must utilize pandas Index objects and their corresponding methods to complete the tasks. - Make sure your implementation is efficient and leverages the index efficiently for querying. ```python import pandas as pd def create_indexed_df(students, scores, quiz_times): # Your implementation here pass def get_scores_by_student(df, student_name): # Your implementation here pass def get_scores_in_time_range(df, start_time, end_time): # Your implementation here pass # Sample Input students = [\\"Alice\\", \\"Bob\\", \\"Charlie\\"] scores = [[88, 92], [79, 85, 91], [93, 87, 78]] quiz_times = [[\\"2021-01-01 10:00\\", \\"2021-01-02 11:00\\"], [\\"2021-01-03 09:00\\", \\"2021-01-05 14:00\\", \\"2021-01-06 13:00\\"], [\\"2021-01-01 11:00\\", \\"2021-01-02 15:00\\", \\"2021-01-05 10:00\\"]] # Expected Output # 1. Indexed DataFrame # 2. Average scores populated # 3. Function for querying by student # 4. Function for querying by time range ```","solution":"import pandas as pd def create_indexed_df(students, scores, quiz_times): data = [] for i, student in enumerate(students): for j, time in enumerate(quiz_times[i]): data.append((student, pd.to_datetime(time), j + 1, scores[i][j])) df = pd.DataFrame(data, columns=[\'Student\', \'Quiz_Time\', \'Quiz_Number\', \'Score\']) df.set_index([\'Student\', \'Quiz_Time\', \'Quiz_Number\'], inplace=True) # Calculate average score for each student df[\'Average_Score\'] = df.groupby(level=\'Student\')[\'Score\'].transform(\'mean\') return df def get_scores_by_student(df, student_name): return df.xs(student_name, level=\'Student\') def get_scores_in_time_range(df, start_time, end_time): start_time = pd.to_datetime(start_time) end_time = pd.to_datetime(end_time) return df.loc[(slice(None), slice(start_time, end_time)), :] # Sample Input students = [\\"Alice\\", \\"Bob\\", \\"Charlie\\"] scores = [[88, 92], [79, 85, 91], [93, 87, 78]] quiz_times = [[\\"2021-01-01 10:00\\", \\"2021-01-02 11:00\\"], [\\"2021-01-03 09:00\\", \\"2021-01-05 14:00\\", \\"2021-01-06 13:00\\"], [\\"2021-01-01 11:00\\", \\"2021-01-02 15:00\\", \\"2021-01-05 10:00\\"]] df = create_indexed_df(students, scores, quiz_times) print(df) print(get_scores_by_student(df, \\"Alice\\")) print(get_scores_in_time_range(df, \\"2021-01-01 00:00\\", \\"2021-01-02 23:59\\"))"},{"question":"# Keyword Checker and Analyzer In this exercise, you will utilize the `keyword` module to create a function that performs various checks and analyses on a list of strings to determine if they are Python keywords, soft keywords, or neither. Function Signature ```python def analyze_keywords(words: List[str]) -> Dict[str, Dict[str, Union[bool, str]]]: pass ``` Input - `words`: A list of strings to be analyzed. Output A dictionary where the keys are the input strings and each value is another dictionary with the following structure: - `\\"is_keyword\\"`: A boolean indicating if the string is a Python keyword. - `\\"is_soft_keyword\\"`: A boolean indicating if the string is a Python soft keyword. - `\\"keyword_type\\"`: A string that can be `\\"keyword\\"`, `\\"soft_keyword\\"`, or `\\"neither\\"` based on the string classification. Example ```python words = [\\"for\\", \\"await\\", \\"hello\\", \\"if\\"] # The function should return: # { # \\"for\\": { # \\"is_keyword\\": True, # \\"is_soft_keyword\\": False, # \\"keyword_type\\": \\"keyword\\" # }, # \\"await\\": { # \\"is_keyword\\": True, # \\"is_soft_keyword\\": False, # \\"keyword_type\\": \\"keyword\\" # }, # \\"hello\\": { # \\"is_keyword\\": False, # \\"is_soft_keyword\\": False, # \\"keyword_type\\": \\"neither\\" # }, # \\"if\\": { # \\"is_keyword\\": True, # \\"is_soft_keyword\\": False, # \\"keyword_type\\": \\"keyword\\" # } # } ``` Constraints - You may assume that all elements in the `words` list are non-empty strings. - The length of `words` will be between 1 and 100. Notes - Utilize the `keyword.iskeyword()` and `keyword.issoftkeyword()` functions to perform the checks. - Construct the dictionaries dynamically based on the input provided. Additional Challenge - Ensure your solution is efficient and can handle the maximum constraint comfortably.","solution":"import keyword def analyze_keywords(words): Analyzes a list of strings to determine if they are Python keywords, soft keywords, or neither. result = {} for word in words: is_kw = keyword.iskeyword(word) is_skw = keyword.issoftkeyword(word) if is_kw: kw_type = \\"keyword\\" elif is_skw: kw_type = \\"soft_keyword\\" else: kw_type = \\"neither\\" result[word] = { \\"is_keyword\\": is_kw, \\"is_soft_keyword\\": is_skw, \\"keyword_type\\": kw_type } return result"},{"question":"# Custom Type Implementation in Python **Objective:** Design a custom Python class that mimics the behavior and functionality of C extension types as described in the `PyTypeObject` documentation. **Problem Statement:** You are required to create a custom Python class `CustomType` that simulates certain aspects of Python\'s `PyTypeObject` and exhibits specific behaviors: 1. **Basic Initialization:** - Your `CustomType` should accept a name and an initial value during instantiation. - Store these passed parameters as attributes. 2. **Attribute Access and Modification:** - Implement methods to get and set attributes dynamically. - This includes basic `__getitem__` and `__setitem__` methods. 3. **String Representation:** - Implement the `__repr__` method to return a string representation of the object following the format `<CustomType name: value>`. 4. **Custom Arithmetic Operations:** - Overload the `__add__`, `__sub__`, `__mul__`, and `__truediv__` methods to support addition, subtraction, multiplication, and division operations. - Ensure these operations update an internal `value` attribute and return the result. 5. **Custom Equality Checks:** - Implement `__eq__` and `__ne__` methods for equality checks. 6. **Iteration:** - Implement the `__iter__` and `__next__` methods to make your object iterable. - Ensure that the iterator iterates over a sequence of items you define within the class. **Constraints:** - You may assume that the initial value is always numeric or can be treated as such for the arithmetic operations. - Ensure that your operations handle invalid operations gracefully, returning appropriate error messages. **Example Usage:** ```python class CustomType: def __init__(self, name, value): # Your implementation here pass def __getitem__(self, key): # Your implementation here pass def __setitem__(self, key, value): # Your implementation here pass def __repr__(self): # Your implementation here pass def __add__(self, other): # Your implementation here pass def __sub__(self, other): # Your implementation here pass def __mul__(self, other): # Your implementation here pass def __truediv__(self, other): # Your implementation here pass def __eq__(self, other): # Your implementation here pass def __ne__(self, other): # Your implementation here pass def __iter__(self): # Your implementation here pass def __next__(self): # Your implementation here pass # Example object instantiation obj = CustomType(\\"Sample\\", 10) # Attribute access and modification obj[\'attr1\'] = 5 print(obj[\'attr1\']) # Output: 5 # String representation print(obj) # Output: <CustomType Sample: 10> # Arithmetic operations print(obj + 2) # Output: 12 print(obj - 5) # Output: 5 print(obj * 3) # Output: 30 print(obj / 2) # Output: 5.0 # Equality checks print(obj == CustomType(\\"Sample\\", 10)) # Output: True print(obj != CustomType(\\"Different\\", 5)) # Output: True # Iteration for item in obj: print(item) # Output the iteration sequence ``` **Deliverables:** You are expected to submit a single Python file with the complete implementation of the `CustomType` class and example usage of the class illustrating that it works as described.","solution":"class CustomType: def __init__(self, name, value): self.name = name self.value = value self.attributes = {} self._iterator_index = 0 def __getitem__(self, key): return self.attributes.get(key, None) def __setitem__(self, key, value): self.attributes[key] = value def __repr__(self): return f\\"<CustomType {self.name}: {self.value}>\\" def __add__(self, other): if isinstance(other, (int, float)): return self.value + other raise TypeError(\\"Unsupported operand type(s) for +: \'CustomType\' and \'{}\'\\".format(type(other).__name__)) def __sub__(self, other): if isinstance(other, (int, float)): return self.value - other raise TypeError(\\"Unsupported operand type(s) for -: \'CustomType\' and \'{}\'\\".format(type(other).__name__)) def __mul__(self, other): if isinstance(other, (int, float)): return self.value * other raise TypeError(\\"Unsupported operand type(s) for *: \'CustomType\' and \'{}\'\\".format(type(other).__name__)) def __truediv__(self, other): if isinstance(other, (int, float)): if other == 0: raise ZeroDivisionError(\\"division by zero\\") return self.value / other raise TypeError(\\"Unsupported operand type(s) for /: \'CustomType\' and \'{}\'\\".format(type(other).__name__)) def __eq__(self, other): return isinstance(other, CustomType) and self.name == other.name and self.value == other.value def __ne__(self, other): return not self.__eq__(other) def __iter__(self): self._iterator_index = 0 return self def __next__(self): items = list(self.attributes.items()) if self._iterator_index < len(items): result = items[self._iterator_index] self._iterator_index += 1 return result raise StopIteration # Example object instantiation obj = CustomType(\\"Sample\\", 10)"},{"question":"Objective You are required to demonstrate your understanding of pandas DataFrame styling through the `Styler` class. Your task is to read a DataFrame, apply multiple styling techniques, and export the styled DataFrame to a specific format. Problem Statement Given a CSV file named `student_scores.csv` with columns `Student_ID`, `Name`, `Math`, `Science`, `English`, and `History`, perform the following tasks: 1. Read the CSV file into a pandas DataFrame. 2. Apply the following styles to the DataFrame: - Highlight the maximum score in each subject with a green background. - Highlight the minimum score in each subject with a red background. - Add a gradient background based on the score values for each subject. 3. Set a caption for the DataFrame: \\"Student Score Table\\". 4. Export the styled DataFrame to an HTML file named `styled_student_scores.html`. Constraints - You must use pandas version 1.3.3 or later. - Do not change the order of the columns. Performance Requirements - The code should efficiently handle DataFrames containing up to 10,000 rows. Expected Function Signature ```python import pandas as pd def style_student_scores(input_csv: str, output_html: str) -> None: # Your implementation here pass ``` Example Given a CSV file `student_scores.csv` with the following content: ``` Student_ID,Name,Math,Science,English,History 1,Alice,88,92,85,78 2,Bob,76,85,89,94 3,Charlie,93,87,92,90 4,Diana,82,79,88,86 5,Eva,79,95,84,91 ``` After running your function: ```python style_student_scores(\'student_scores.csv\', \'styled_student_scores.html\') ``` The `styled_student_scores.html` file should contain a styled table where: - Cells with the highest scores in each subject are highlighted in green. - Cells with the lowest scores in each subject are highlighted in red. - Cells have a gradient background based on their score values. - The table caption is set to \\"Student Score Table\\". Notes - Ensure proper handling of edge cases, such as missing values in the scores. - You can assume the DataFrame will not contain non-numeric values in the score columns.","solution":"import pandas as pd def style_student_scores(input_csv: str, output_html: str) -> None: # Read the CSV file into a pandas DataFrame df = pd.read_csv(input_csv) # Define a function to highlight the max value def highlight_max(s): is_max = s == s.max() return [\'background-color: green\' if v else \'\' for v in is_max] # Define a function to highlight the min value def highlight_min(s): is_min = s == s.min() return [\'background-color: red\' if v else \'\' for v in is_min] # Style the DataFrame styled_df = df.style .apply(highlight_max, subset=[\'Math\', \'Science\', \'English\', \'History\']) .apply(highlight_min, subset=[\'Math\', \'Science\', \'English\', \'History\']) .background_gradient(cmap=\'coolwarm\', subset=[\'Math\', \'Science\', \'English\', \'History\']) .set_caption(\\"Student Score Table\\") # Export the styled DataFrame to an HTML file styled_df.to_html(output_html) # Example usage: # style_student_scores(\'student_scores.csv\', \'styled_student_scores.html\')"},{"question":"# Secure Hash Generation and Validation Objective: Implement a Python function to perform secure hash generation and validation using the `hashlib` and `secrets` modules. Function 1: `generate_secure_hash` # Inputs: - `data` (str): The string data to be hashed. - `salt` (str, optional): The salt to be used for hashing (default is generated using `secrets.token_hex`). # Output: - A tuple consisting of: 1. The generated hash (in hexadecimal format). 2. The salt used. # Constraints: - Make sure to use a strong hash algorithm (such as SHA-256) from the `hashlib` module. - If no salt is provided, generate a secure random salt using `secrets.token_hex(16)`. Function 2: `validate_secure_hash` # Inputs: - `data` (str): The original string data that needs to be validated. - `expected_hash` (str): The hash value to be validated against. - `salt` (str): The salt that was used during hash generation. # Output: - A boolean value `True` if the hash of the data (using the provided salt) matches the expected hash, otherwise `False`. # Example Usage: ```python # Generate hash hash_data, salt = generate_secure_hash(\\"my_secrete_data\\") print(f\\"Hash: {hash_data}, Salt: {salt}\\") # Validate hash is_valid = validate_secure_hash(\\"my_secrete_data\\", hash_data, salt) print(is_valid) # Expected output: True # Validate with wrong data is_valid = validate_secure_hash(\\"wrong_data\\", hash_data, salt) print(is_valid) # Expected output: False ``` Additional Notes: - While using salt ensures that even the same input data doesn\'t result in the same hash, it\'s crucial for enhancing security. - Students should ensure that their implementation correctly handles the generation and utilization of secure salts. - Performance considerations should ensure that the function runs efficiently even for large data inputs. Evaluation Criteria: - Correct usage of `hashlib` for generating secure hashes. - Correct usage of `secrets` for generating salts. - Correct implementation of hash validation. - Code efficiency and adherence to best practices for cryptographic functions.","solution":"import hashlib import secrets def generate_secure_hash(data, salt=None): Generates a secure hash for the provided data using SHA-256 and an optional salt. Args: data (str): The string data to be hashed. salt (str, optional): The salt to be used for hashing. Defaults is generated using secrets.token_hex(). Returns: tuple: A tuple containing the generated hash (in hexadecimal format) and the salt used. if salt is None: salt = secrets.token_hex(16) data_to_hash = salt + data hash_value = hashlib.sha256(data_to_hash.encode()).hexdigest() return hash_value, salt def validate_secure_hash(data, expected_hash, salt): Validates the provided data against the expected hash using the provided salt. Args: data (str): The original string data that needs to be validated. expected_hash (str): The hash value to be validated against. salt (str): The salt that was used during hash generation. Returns: bool: True if the hash of the data (using the provided salt) matches the expected hash, otherwise False. data_to_hash = salt + data hash_value = hashlib.sha256(data_to_hash.encode()).hexdigest() return hash_value == expected_hash"},{"question":"Coding Assessment Question # Objective To assess your comprehension of the `struct` module in Python, you are required to implement a function that reads binary data from a file, processes it into structured data, and then writes a modified version of this structured data back to a different binary file. # Task You are given a binary file containing a list of records, each with the following structure: - An 8-byte integer ID (big-endian). - A 4-byte floating-point value representing a measurement (big-endian). - A 1-byte boolean flag. - A 3-byte padding to align the next record correctly. Your task is to: 1. Read the binary data. 2. Parse the records using the `struct` module. 3. Modify each record by setting the boolean flag to `True` if the measurement is greater than a given threshold. 4. Write the modified records back to a new binary file, maintaining the same structure. # Implementation Function Signature ```python def process_binary_file(input_file: str, output_file: str, threshold: float) -> None: pass ``` Input - `input_file`: A string representing the path to the input binary file. - `output_file`: A string representing the path to the output binary file. - `threshold`: A float value used to modify the boolean flag based on the measurement. Output - The function should not return anything. It should write the modified records to the specified output file. Constraints - The binary file will contain at least one record. - The integer ID is always non-negative. - The measurement is a valid floating-point number. Example Suppose the input binary file contains the following data in hexadecimal (for illustration purposes): ``` 00000000000000011942000000000001 00000000000000026842000000000002 0000000000000003D840000000000001 ``` This corresponds to three records: 1. ID: 1, Measurement: 50.0 (0x42490000), Flag: False (0) 2. ID: 2, Measurement: 100.0 (0x42C80000), Flag: False (0) 3. ID: 3, Measurement: 400.0 (0x43C80000), Flag: True (1) With a threshold of 100.0, the modified records would be: 1. ID: 1, Measurement: 50.0, Flag: False (0) 2. ID: 2, Measurement: 100.0, Flag: False (0) 3. ID: 3, Measurement: 400.0, Flag: True (1) Which results in: ``` 00000000000000011942000000000001 00000000000000026842000000000004 0000000000000003D840000000000001 ``` Performance Requirements - The function should be efficient in terms of memory usage. It should read and process the data in chunks rather than loading the entire file into memory, if the file is large. # Hints - Utilize the `struct.unpack` and `struct.pack` methods for reading and writing binary data. - Refer to `struct.calcsize` to ensure you\'re reading the correct amount of bytes for each record. - Use appropriate format characters to match the record structure. Implement the `process_binary_file` function using the guidelines provided.","solution":"import struct def process_binary_file(input_file: str, output_file: str, threshold: float) -> None: record_format = \'>qf?3x\' # > indicates big-endian, q is 8-byte int, f is 4-byte float, ? is 1-byte bool, 3x is 3-byte padding record_size = struct.calcsize(record_format) with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: while True: record_data = infile.read(record_size) if not record_data: break id, measurement, flag = struct.unpack(record_format, record_data) new_flag = measurement > threshold new_record = struct.pack(record_format, id, measurement, new_flag) outfile.write(new_record)"},{"question":"# Problem: Analyze Sales Data with Pandas You work as a data analyst for a retail company, and you have access to sales data stored in a CSV file named `sales_data.csv`. The dataset includes sales information for various products over different months. Your task is to analyze this data and extract key insights. Given: A CSV file `sales_data.csv` with the following columns: - `Product`: Name of the product sold. - `Month`: Month of the sale. - `Sales`: Number of units sold. - `Revenue`: Revenue generated from the sales. Requirements: 1. **Read the data** from the CSV file into a pandas DataFrame. 2. **Clean the data** by filling any missing values in the \\"Sales\\" and \\"Revenue\\" columns with the average of their respective columns. 3. **Calculate summary statistics**: - Total sales and total revenue for each product. - Average sales per month for each product. 4. **Identify the best-performing product** in terms of total revenue. 5. **Generate a new DataFrame** containing a monthly revenue summary for each product, indexed by month and with products as columns. 6. Implement the following functions to achieve the above requirements: ```python import pandas as pd def read_data(file_path: str) -> pd.DataFrame: Read the CSV file into a DataFrame. Args: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame. pass def clean_data(df: pd.DataFrame) -> pd.DataFrame: Fill missing values in \'Sales\' and \'Revenue\' columns with the average of their respective columns. Args: df (pd.DataFrame): The DataFrame to clean. Returns: pd.DataFrame: The cleaned DataFrame. pass def calculate_summary_statistics(df: pd.DataFrame) -> pd.DataFrame: Calculate total sales and total revenue for each product, as well as average sales per month for each product. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: pd.DataFrame: Summary statistics DataFrame. pass def identify_best_performing_product(df: pd.DataFrame) -> str: Identify the product with the highest total revenue. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: str: Name of the best-performing product. pass def generate_monthly_revenue_summary(df: pd.DataFrame) -> pd.DataFrame: Generate a monthly revenue summary DataFrame. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: pd.DataFrame: A DataFrame with months as index and products as columns. pass ``` Constraints: - Do not use any external libraries other than pandas. - Assume the \\"Product\\" names are unique and consistent across the dataset. - The \\"Month\\" column indicates months in the format \'YYYY-MM\'. Example: Suppose the `sales_data.csv` contains: | Product | Month | Sales | Revenue | |---------|-------|-------|---------| | A | 2023-01 | 100 | 1000 | | B | 2023-01 | 150 | 1500 | | A | 2023-02 | 200 | 2100 | | B | 2023-02 | 180 | 1850 | The resulting summary statistics DataFrame could look like this: | Product | Total Sales | Total Revenue | Average Sales per Month | |---------|-------------|---------------|-------------------------| | A | 300 | 3100 | 150 | | B | 330 | 3350 | 165 | # Evaluation Your implementation will be evaluated based on: - Correctness and completeness of the functions. - Efficient handling and manipulation of data using pandas. - Adherence to the problem requirements and constraints.","solution":"import pandas as pd def read_data(file_path: str) -> pd.DataFrame: Read the CSV file into a DataFrame. Args: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame. return pd.read_csv(file_path) def clean_data(df: pd.DataFrame) -> pd.DataFrame: Fill missing values in \'Sales\' and \'Revenue\' columns with the average of their respective columns. Args: df (pd.DataFrame): The DataFrame to clean. Returns: pd.DataFrame: The cleaned DataFrame. df[\'Sales\'].fillna(df[\'Sales\'].mean(), inplace=True) df[\'Revenue\'].fillna(df[\'Revenue\'].mean(), inplace=True) return df def calculate_summary_statistics(df: pd.DataFrame) -> pd.DataFrame: Calculate total sales and total revenue for each product, as well as average sales per month for each product. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: pd.DataFrame: Summary statistics DataFrame. summary = df.groupby(\'Product\').agg( Total_Sales=(\'Sales\', \'sum\'), Total_Revenue=(\'Revenue\', \'sum\'), Average_Sales_per_Month=(\'Sales\', \'mean\') ).reset_index() return summary def identify_best_performing_product(df: pd.DataFrame) -> str: Identify the product with the highest total revenue. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: str: Name of the best-performing product. best_product = df.groupby(\'Product\')[\'Revenue\'].sum().idxmax() return best_product def generate_monthly_revenue_summary(df: pd.DataFrame) -> pd.DataFrame: Generate a monthly revenue summary DataFrame. Args: df (pd.DataFrame): The DataFrame containing sales data. Returns: pd.DataFrame: A DataFrame with months as index and products as columns. monthly_summary = df.pivot(index=\'Month\', columns=\'Product\', values=\'Revenue\') return monthly_summary"},{"question":"You are tasked with implementing a custom numerical operation class in Python, simulating the behavior of Python\'s numeric operations using the given `python310` package functions. Objective Write a class `CustomNumber` that supports the following operations: 1. Addition (`+`): should use `PyNumber_Add`. 2. Subtraction (`-`): should use `PyNumber_Subtract`. 3. Multiplication (`*`): should use `PyNumber_Multiply`. 4. True Division (`/`): should use `PyNumber_TrueDivide`. 5. Floor Division (`//`): should use `PyNumber_FloorDivide`. 6. Modulus (`%`): should use `PyNumber_Remainder`. Class Definition ```python class CustomNumber: def __init__(self, value): # Initialize with value def __add__(self, other): # Implement addition def __sub__(self, other): # Implement subtraction def __mul__(self, other): # Implement multiplication def __truediv__(self, other): # Implement true division def __floordiv__(self, other): # Implement floor division def __mod__(self, other): # Implement modulus # Example usage: # a = CustomNumber(10) # b = CustomNumber(3) # print((a + b).value) # Should use PyNumber_Add # print((a - b).value) # Should use PyNumber_Subtract # print((a * b).value) # Should use PyNumber_Multiply # print((a / b).value) # Should use PyNumber_TrueDivide # print((a // b).value) # Should use PyNumber_FloorDivide # print((a % b).value) # Should use PyNumber_Remainder ``` Constraints - Assume the input values will be integers. - Handle any exceptions and return `None` in case of an error (e.g., division by zero). - The `object` parameter (`o`) in each `PyNumber_*` function should correspond to the `value` attribute of `CustomNumber`. Input and Output - **Input:** Two instances of `CustomNumber`. - **Output:** A new `CustomNumber` instance with the result of the operation. Example ```python a = CustomNumber(10) b = CustomNumber(3) print((a + b).value) # Should output 13 print((a - b).value) # Should output 7 print((a * b).value) # Should output 30 print((a / b).value) # Should output 3.3333... print((a // b).value) # Should output 3 print((a % b).value) # Should output 1 ``` The students are expected to simulate the `PyNumber_*` functions within the methods of the `CustomNumber` class, demonstrating their understanding of handling PyObjects and performing arithmetic operations with the provided API.","solution":"# Simulate the `python310` package functions with Python\'s own numeric operators def PyNumber_Add(a, b): return a + b def PyNumber_Subtract(a, b): return a - b def PyNumber_Multiply(a, b): return a * b def PyNumber_TrueDivide(a, b): return a / b def PyNumber_FloorDivide(a, b): return a // b def PyNumber_Remainder(a, b): return a % b class CustomNumber: def __init__(self, value): self.value = value def __add__(self, other): try: result = PyNumber_Add(self.value, other.value) return CustomNumber(result) except Exception: return None def __sub__(self, other): try: result = PyNumber_Subtract(self.value, other.value) return CustomNumber(result) except Exception: return None def __mul__(self, other): try: result = PyNumber_Multiply(self.value, other.value) return CustomNumber(result) except Exception: return None def __truediv__(self, other): try: result = PyNumber_TrueDivide(self.value, other.value) return CustomNumber(result) except Exception: return None def __floordiv__(self, other): try: result = PyNumber_FloorDivide(self.value, other.value) return CustomNumber(result) except Exception: return None def __mod__(self, other): try: result = PyNumber_Remainder(self.value, other.value) return CustomNumber(result) except Exception: return None"},{"question":"You have been given a list of URLs and you are required to check if the content of these URLs contains a specific keyword. You will need to use both `ThreadPoolExecutor` and `ProcessPoolExecutor` to achieve this. You should also handle potential exceptions and timeouts gracefully. # Input: 1. A list of URLs (`urls`). 2. A target keyword (`keyword`) to search within the content of the URLs. 3. A timeout value (`timeout`) to limit how long each request should take. # Output: A dictionary where the keys are the URLs and the values are either: - `True` if the keyword was found in the content. - `False` if the keyword was not found. - An error message indicating what went wrong (e.g., timeout, connection error). # Constraints: - Use both `ThreadPoolExecutor` for the web requests and `ProcessPoolExecutor` for intensive text searching. - Ensure each web request respects the timeout value. - Use at least 3 worker threads for the `ThreadPoolExecutor` and the default number of processes for the `ProcessPoolExecutor`. # Example: ```python urls = [ \'http://www.example.com/\', \'http://www.test.com/\', \'http://notarealwebsite.fake/\' ] keyword = \'example\' timeout = 5 result = { \'http://www.example.com/\': True, \'http://www.test.com/\': False, \'http://notarealwebsite.fake/\': \'Connection error\' } # Your solution should handle the requests and keyword search accordingly print(result) ``` # Your Task: Implement the function `check_keyword_in_urls(urls: List[str], keyword: str, timeout: int) -> Dict[str, Union[bool, str]]` that takes in a list of URLs, a keyword, and a timeout value, and returns the result as mentioned above. # Additional Requirements: - Use `ThreadPoolExecutor` to handle the web requests asynchronously. - Use `ProcessPoolExecutor` to perform the keyword search in the content retrieved from the URLs. - Handle exceptions such as timeouts and connection errors gracefully. - Ensure the implementation is efficient and utilizes the concurrent futures module effectively. Hints: 1. You might want to create helper functions for loading URLs and searching for keywords. 2. Be mindful of the global interpreter lock (GIL) when using `ProcessPoolExecutor`. # Note: This question covers the comprehension and practical application of key concepts from the `concurrent.futures` module, including the use of executors, handling of future objects, and proper exception management.","solution":"import concurrent.futures import requests from typing import List, Dict, Union def fetch_url_content(url: str, timeout: int) -> Union[str, Exception]: Fetches content from a URL with a specified timeout. try: response = requests.get(url, timeout=timeout) response.raise_for_status() return response.text except requests.RequestException as e: return e def search_keyword(content: str, keyword: str) -> bool: Searches for the keyword in the given content. return keyword in content def check_keyword_in_urls(urls: List[str], keyword: str, timeout: int) -> Dict[str, Union[bool, str]]: Given a list of URLs, checks if the content contains the specific keyword within the timeout. Utilizes both ThreadPoolExecutor and ProcessPoolExecutor. results = {} with concurrent.futures.ThreadPoolExecutor(max_workers=3) as thread_executor: future_to_url = {thread_executor.submit(fetch_url_content, url, timeout): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: content = future.result() if isinstance(content, Exception): results[url] = str(content) else: with concurrent.futures.ProcessPoolExecutor() as process_executor: keyword_future = process_executor.submit(search_keyword, content, keyword) results[url] = keyword_future.result() except Exception as e: results[url] = str(e) return results"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `husl_palette` function by creating and customizing palettes, then using those palettes in visualizations. **Task:** 1. Create three different color palettes using `sns.husl_palette` with the following customizations: - Palette A: 10 colors, default lightness and saturation. - Palette B: 12 colors, lightness set to 0.5, saturation set to 0.7. - Palette C: Continuous colormap, customized with lightness of 0.4 and saturation of 0.6. 2. Use these palettes to generate three different visualizations: - For Palette A: Create a bar plot with 10 bars, each colored with a different color from Palette A. - For Palette B: Create a horizontal box plot using 12 different boxplots colored by the colors from Palette B. - For Palette C: Create a heatmap using the continuous colormap from Palette C. **Requirements:** - Your code should import seaborn and any other necessary libraries. - Each visualization should properly apply the respective color palette. - Ensure that each plot is well-labeled and includes a title for clarity. **Input and Output:** - **Input:** None, the code should be self-contained. - **Output:** Three plots displayed in sequence based on the specified color palettes and customizations. **Constraints:** - Do not use hardcoded RGB color values. - Use seaborn\'s functionality to generate and apply the color palettes. **Code Template:** ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd # Step 1: Create the color palettes palette_a = sns.husl_palette(10) palette_b = sns.husl_palette(12, l=0.5, s=0.7) palette_c = sns.husl_palette(l=0.4, s=0.6, as_cmap=True) # Step 2: Create the visualizations # Visualization for Palette A (Bar plot) data_a = pd.Series(np.random.rand(10)) fig, ax = plt.subplots() data_a.plot(kind=\'bar\', color=palette_a, ax=ax) ax.set_title(\\"Bar Plot with Palette A\\") plt.show() # Visualization for Palette B (Horizontal Box plot) data_b = pd.DataFrame(np.random.randn(12, 10)) fig, ax = plt.subplots() sns.boxplot(data=data_b, palette=palette_b, orient=\\"h\\", ax=ax) ax.set_title(\\"Horizontal Box Plot with Palette B\\") plt.show() # Visualization for Palette C (Heatmap) data_c = np.random.rand(10, 10) fig, ax = plt.subplots() sns.heatmap(data_c, cmap=palette_c, ax=ax) ax.set_title(\\"Heatmap with Palette C\\") plt.show() ``` **Note:** Make sure to test your code to ensure that all plots render correctly with the specified customizations and palettes.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd # Step 1: Create the color palettes palette_a = sns.husl_palette(10) palette_b = sns.husl_palette(12, l=0.5, s=0.7) palette_c = sns.husl_palette(l=0.4, s=0.6, as_cmap=True) # Step 2: Create the visualizations # Visualization for Palette A (Bar plot) def plot_bar_with_palette_a(): data_a = pd.Series(np.random.rand(10)) fig, ax = plt.subplots() data_a.plot(kind=\'bar\', color=palette_a, ax=ax) ax.set_title(\\"Bar Plot with Palette A\\") plt.show() # Visualization for Palette B (Horizontal Box plot) def plot_horizontal_box_with_palette_b(): data_b = pd.DataFrame(np.random.randn(12, 10)) fig, ax = plt.subplots() sns.boxplot(data=data_b, palette=palette_b, orient=\\"h\\", ax=ax) ax.set_title(\\"Horizontal Box Plot with Palette B\\") plt.show() # Visualization for Palette C (Heatmap) def plot_heatmap_with_palette_c(): data_c = np.random.rand(10, 10) fig, ax = plt.subplots() sns.heatmap(data_c, cmap=palette_c, ax=ax) ax.set_title(\\"Heatmap with Palette C\\") plt.show()"},{"question":"# Asynchronous Web Scraper with Timeouts and Logging You are to develop an asynchronous web scraper using the asyncio package in Python. The web scraper should be capable of fetching multiple web pages concurrently, process their content for a specific keyword, and log the results. This problem will test your understanding of asyncio\'s concurrency model, handling of blocking I/O operations, and debugging features. Requirements: 1. Implement an asynchronous function `fetch_page(session, url)` that fetches the content of a web page using the aiohttp library. If fetching the page takes longer than 5 seconds, a timeout should occur and be logged. 2. Implement an asynchronous function `process_content(url, content, keyword)` that processes the page content to find occurrences of a specific keyword. The function should log each occurrence of the keyword along with the URL. 3. Implement an asynchronous function `main(urls, keyword)` that takes a list of URLs and a keyword to: - Create tasks for fetching and processing the content of each URL concurrently. - Use an executor to handle any blocking operations if necessary. - Ensure that results and any exceptions are correctly logged. 4. Enable asyncio\'s debug mode and configure the logging module to capture debug information. Input: - `urls`: A list of URLs (strings). - `keyword`: A keyword (string) to search within the web page content. Output: - Logs of the keyword occurrences in each URL. - Logs of any timeouts or exceptions encountered during the scraping process. Constraints: - Use the aiohttp library for HTTP operations. - Ensure that all coroutines are properly awaited. - Handle all exceptions gracefully and log them appropriately. - The logging level should be set to DEBUG. Example: ```python import aiohttp import asyncio import logging async def fetch_page(session, url): # Your implementation here async def process_content(url, content, keyword): # Your implementation here async def main(urls, keyword): # Your implementation here if __name__ == \\"__main__\\": logging.basicConfig(level=logging.DEBUG) urls = [\\"http://example.com\\", \\"http://example.org\\"] keyword = \\"example\\" asyncio.run(main(urls, keyword)) ``` This question expects your solution to demonstrate the following: - The ability to run multiple network-bound tasks concurrently. - Proper handling and logging of timeouts and exceptions. - Efficient processing of fetched data without blocking the event loop. - Utilization of asyncio\'s debug mode and logging facilities to troubleshoot common issues.","solution":"import aiohttp import asyncio import logging logging.basicConfig(level=logging.DEBUG) async def fetch_page(session, url): Fetches a web page within a timeout. Logs a timeout error if it takes longer than 5 seconds. try: async with session.get(url, timeout=5) as response: response.raise_for_status() content = await response.text() logging.debug(f\\"Fetched content from {url}\\") return content except asyncio.TimeoutError: logging.error(f\\"Timeout occurred while fetching {url}\\") except aiohttp.ClientError as e: logging.error(f\\"Error {e} occurred while fetching {url}\\") async def process_content(url, content, keyword): Processes the content of a web page to find occurrences of a specific keyword. Logs each occurrence with the URL. count = content.lower().count(keyword.lower()) logging.debug(f\\"{count} occurrences of keyword \'{keyword}\' found in {url}\\") async def main(urls, keyword): Fetches and processes multiple URLs concurrently for a specific keyword. Logs results and exceptions properly. async with aiohttp.ClientSession() as session: tasks = [] for url in urls: tasks.append(fetch_and_process(session, url, keyword)) await asyncio.gather(*tasks) async def fetch_and_process(session, url, keyword): content = await fetch_page(session, url) if content: await process_content(url, content, keyword) if __name__ == \\"__main__\\": asyncio.run(main([\\"http://example.com\\", \\"http://example.org\\"], \\"example\\"))"},{"question":"# Seaborn Coding Assessment **Objective:** Design a comprehensive visualization using seaborn to analyze the dataset\'s distribution and relationships. This task will test your understanding of seaborn\'s diverse functionality. **Problem Statement:** Given the \\"penguins\\" dataset, create visualizations fulfilling the following requirements: 1. **Distribution Plots**: - Create a figure with three subplots in one row: 1. A histogram for `flipper_length_mm` with the distribution of data stratified by `species` (using stacking). 2. A kernel density estimate plot for `bill_length_mm` with the distribution of data stratified by `species` (using shading). 3. A rug plot for `body_mass_g` stratified by `species`. 2. **Relational Plots**: - Create a scatter plot for `flipper_length_mm` versus `bill_length_mm` showing different species with different colors and adding regression lines for each species. 3. **Faceted Plots**: - Create a faceted kernel density estimate plot for `flipper_length_mm`, faceted by `species`, and `sex` on both rows and columns, respectively. **Constraints:** - Use appropriate titles and labels for each plot. - Ensure legends are properly placed and plots are well-designed. - Implementing this entire visualization should be efficient and well-organized. **Input:** The \\"penguins\\" dataset, which can be loaded using: ```python penguins = sns.load_dataset(\\"penguins\\") ``` **Output:** A figure with the described subplots for part 1 and part 2, along with a separate faceted plot for part 3. **Example Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Part 1: Distribution Plots fig, ax = plt.subplots(1, 3, figsize=(18, 5)) # Histogram sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", ax=ax[0]) ax[0].set_title(\'Flipper Length Distribution by Species\') # KDE Plot sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", fill=True, ax=ax[1]) ax[1].set_title(\'Bill Length KDE by Species\') # Rug Plot sns.rugplot(data=penguins, x=\\"body_mass_g\\", hue=\\"species\\", ax=ax[2]) ax[2].set_title(\'Body Mass Rug Plot by Species\') plt.tight_layout() plt.show() # Part 2: Relational Plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\") sns.regplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", scatter=False, ci=None) plt.title(\'Scatter Plot with Regression Lines\') plt.show() # Part 3: Faceted KDE Plot g = sns.FacetGrid(penguins, col=\\"species\\", row=\\"sex\\", margin_titles=True, height=4) g.map(sns.kdeplot, \\"flipper_length_mm\\", fill=True) g.set_axis_labels(\\"Flipper Length (mm)\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") plt.show() ``` *Ensure your code runs without errors and produce visualizations that meet the problem statement requirements.*","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins_data(penguins): # Part 1: Distribution Plots fig, ax = plt.subplots(1, 3, figsize=(18, 5)) # Histogram sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", ax=ax[0]) ax[0].set_title(\'Flipper Length Distribution by Species\') # KDE Plot sns.kdeplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", fill=True, ax=ax[1]) ax[1].set_title(\'Bill Length KDE by Species\') # Rug Plot sns.rugplot(data=penguins, x=\\"body_mass_g\\", hue=\\"species\\", ax=ax[2]) ax[2].set_title(\'Body Mass Rug Plot by Species\') plt.tight_layout() plt.show() # Part 2: Relational Plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", palette=\'deep\') sns.regplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", scatter=False, ci=None, color=\'red\', label=\'Regression Line\') plt.title(\'Scatter Plot with Regression Lines\') plt.legend() plt.show() # Part 3: Faceted KDE Plot g = sns.FacetGrid(penguins, col=\\"species\\", row=\\"sex\\", margin_titles=True, height=4) g.map(sns.kdeplot, \\"flipper_length_mm\\", fill=True) g.set_axis_labels(\\"Flipper Length (mm)\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") plt.show() # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Call the visualization function visualize_penguins_data(penguins)"},{"question":"# Exception Handling and Custom Exception Classes As a programmer, you are required to demonstrate your understanding of exception handling by creating custom exceptions and managing error states. Implement the following Python functions to meet the requirements: 1. **Function to Raise an Exception:** Implement a function `raise_custom_exception` that raises a custom exception of type `CustomError` with a specific error message. ```python class CustomError(Exception): pass def raise_custom_exception(message: str) -> None: Raise a CustomError with the given message. Args: - message (str): The error message to be associated with the exception. Raises: - CustomError: With the provided message. # Your code here ``` 2. **Function to Handle an Exception:** Implement a function `handle_and_print_exception` that handles the `CustomError` exception, prints the error message, and ensures the error state is cleared before continuing with a normal flow. ```python def handle_and_print_exception(func, *args, **kwargs) -> None: Execute the provided function and handle any CustomError that is raised. Print the error message and ensure normal continuation. Args: - func (callable): The function to execute. - *args: Argument list for the function. - **kwargs: Keyword arguments for the function. # Your code here ``` 3. **Function to Implement Warning Issuance:** Implement a function `issue_warning` that issues a `UserWarning` with a provided message. The warning should be caught and handled within the function, and its message should be printed. ```python import warnings def issue_warning(message: str) -> None: Issue a UserWarning with the given message and print the warning message. Args: - message (str): The warning message to be issued. # Your code here ``` **Constraints:** - You must use the provided `CustomError` class. - The functions must handle exceptions and warnings using native Python constructs (try-except for exceptions and warnings.warn for warnings). **Example Usage:** ```python # Example for raise_custom_exception try: raise_custom_exception(\\"An error occurred!\\") except CustomError as e: print(f\\"Caught an exception: {e}\\") # Example for handle_and_print_exception def faulty_function(): raise_custom_exception(\\"Something went wrong in the function.\\") handle_and_print_exception(faulty_function) # Example for issue_warning issue_warning(\\"This is a warning message.\\") ```","solution":"class CustomError(Exception): pass def raise_custom_exception(message: str) -> None: Raise a CustomError with the given message. Args: - message (str): The error message to be associated with the exception. Raises: - CustomError: With the provided message. raise CustomError(message) def handle_and_print_exception(func, *args, **kwargs) -> None: Execute the provided function and handle any CustomError that is raised. Print the error message and ensure normal continuation. Args: - func (callable): The function to execute. - *args: Argument list for the function. - **kwargs: Keyword arguments for the function. try: func(*args, **kwargs) except CustomError as e: print(f\\"Caught an exception: {e}\\") import warnings def issue_warning(message: str) -> None: Issue a UserWarning with the given message and print the warning message. Args: - message (str): The warning message to be issued. with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") warnings.warn(message, UserWarning) for warn in w: print(f\\"Warning issued: {warn.message}\\")"},{"question":"**Advanced Coding Assessment Question** # Objective: Assess the understanding of implementing custom scoring metrics using scikit-learn and applying them to a supervised learning problem. # Scenario: You are given a dataset and asked to train a regression model. After training the model, you are required to evaluate its performance using both standard and custom metrics/scorers. # Task: 1. Import the required libraries and dataset. 2. Preprocess the dataset (if necessary), and split it into training and testing sets. 3. Train a regression model on the training set. 4. Implement custom scoring functions: - Mean Bias Deviation (MBD): Measures the average deviation with respect to the mean of true values. - Root Relative Squared Error (RRSE): A normalized version of Root Mean Squared Error (RMSE). 5. Use the custom scoring functions along with standard metric functions like R² and Mean Absolute Error (MAE) to evaluate the model on the testing set. 6. Output the evaluation results. # Input: - A CSV file containing the dataset (`data.csv`). # Expected input and output: *Input:* - `csv_path` (string): Path to the CSV file containing the dataset. *Output:* - Evaluation results (dictionary): A dictionary containing the scores from standard and custom metric functions. # Constraints: - You must use scikit-learn for model training and evaluation. # Performance requirements: - The custom metric implementations should handle large datasets efficiently. # Example: ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, r2_score, make_scorer # Load dataset csv_path = \'data.csv\' data = pd.read_csv(csv_path) # Preprocess and split the data X = data.drop(columns=\'target\') y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model model = LinearRegression() model.fit(X_train, y_train) # Implement custom metrics def mean_bias_deviation(y_true, y_pred): return np.mean(y_pred - y_true) def root_relative_squared_error(y_true, y_pred): y_true_mean = np.mean(y_true) return np.sqrt(np.sum((y_true - y_pred) ** 2) / np.sum((y_true - y_true_mean) ** 2)) # Create custom scorers mbd_scorer = make_scorer(mean_bias_deviation, greater_is_better=False) rrse_scorer = make_scorer(root_relative_squared_error, greater_is_better=False) # Predict and evaluate y_pred = model.predict(X_test) results = { \'R2 Score\': r2_score(y_test, y_pred), \'Mean Absolute Error\': mean_absolute_error(y_test, y_pred), \'Mean Bias Deviation\': mean_bias_deviation(y_test, y_pred), \'Root Relative Squared Error\': root_relative_squared_error(y_test, y_pred), } print(results) ``` # Note: - You can adapt the example structure for actual implementation and testing.","solution":"import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, r2_score, make_scorer def load_and_prepare_data(csv_path): Load dataset and preprocess by splitting into train and test sets. Parameters: csv_path (str): Path to the CSV file containing the dataset. Returns: tuple: Splitted data (X_train, X_test, y_train, y_test). data = pd.read_csv(csv_path) X = data.drop(columns=\'target\') y = data[\'target\'] return train_test_split(X, y, test_size=0.2, random_state=42) def train_regression_model(X_train, y_train): Train and return a regression model. Parameters: X_train (DataFrame): Training features. y_train (Series): Training target. Returns: model: Trained regression model. model = LinearRegression() model.fit(X_train, y_train) return model def mean_bias_deviation(y_true, y_pred): Custom metric: Mean Bias Deviation (MBD). Parameters: y_true (array-like): True values. y_pred (array-like): Predicted values. Returns: float: Mean Bias Deviation. return np.mean(y_pred - y_true) def root_relative_squared_error(y_true, y_pred): Custom metric: Root Relative Squared Error (RRSE). Parameters: y_true (array-like): True values. y_pred (array-like): Predicted values. Returns: float: Root Relative Squared Error. y_true_mean = np.mean(y_true) return np.sqrt(np.sum((y_true - y_pred) ** 2) / np.sum((y_true - y_true_mean) ** 2)) def evaluate_model(model, X_test, y_test): Evaluate the model using standard and custom metrics. Parameters: model: Trained regression model. X_test (DataFrame): Testing features. y_test (Series): True test target values. Returns: dict: Evaluation results containing scores from standard and custom metrics. y_pred = model.predict(X_test) results = { \'R2 Score\': r2_score(y_test, y_pred), \'Mean Absolute Error\': mean_absolute_error(y_test, y_pred), \'Mean Bias Deviation\': mean_bias_deviation(y_test, y_pred), \'Root Relative Squared Error\': root_relative_squared_error(y_test, y_pred), } return results"},{"question":"# Python Code Validation and Compilation Context: You are tasked with building a small utility that reads multiple lines of Python code, checks their validity, and compiles them if they are valid. This utility should support continuous input, meaning subsequent lines of code should remember and respect `__future__` statements that were included in previous lines. This functionality is similar to how an interactive Python shell works. Task: Implement a function `validate_and_compile(lines: List[str], mode: str) -> List[Union[str, None]]` that takes in a list of strings, where each string is a line of Python code, and a mode that determines the type of compilation (`\'single\'`, `\'exec\'`, or `\'eval\'`). The function should attempt to compile each line of code and return a list of results. If a line of code is valid, the function should return the compiled code object as a string; otherwise, it should return `None`. Specifications: 1. **Inputs:** - `lines`: List of strings where each string represents a line of Python code. - `mode`: A string that can be `\'single\'`, `\'exec\'`, or `\'eval\'`. 2. **Outputs:** - A list of results where each result is either the string representation of the compiled code object or `None` if the compilation fails. 3. **Constraints:** - Each line of code should be processed as if being run in an interactive Python shell. - The function should remember `__future__` statements and apply them to subsequent lines. - Use the `codeop.CommandCompiler` class to handle the compilation and memory of `__future__` statements. 4. **Performance Requirements:** - The function should be efficient and handle up to 1000 lines of code in a reasonable time frame. Example: ```python from typing import List, Union def validate_and_compile(lines: List[str], mode: str) -> List[Union[str, None]]: # your implementation here # Example Usage: lines = [ \\"from __future__ import division\\", \\"a = 1 / 2\\", \\"print(a)\\" ] mode = \\"single\\" result = validate_and_compile(lines, mode) print(result) ``` **Expected Output:** ```python [ \\"<code object <module> at 0x7f9d6a460df0, file \'<input>\', line 1>\\", \\"<code object <module> at 0x7f9d6a460f50, file \'<input>\', line 1>\\", \\"<code object <module> at 0x7f9d6a469030, file \'<input>\', line 1>\\" ] ``` Hints: - Use `codeop.CommandCompiler` to compile the lines of code. - Ensure that the mode parameter is appropriately set for each compilation attempt. - Handle exceptions and ensure that invalid code returns `None`.","solution":"from typing import List, Union import codeop def validate_and_compile(lines: List[str], mode: str) -> List[Union[str, None]]: compiler = codeop.CommandCompiler() results = [] for line in lines: try: compiled_code = compiler(line, filename=\'<input>\', symbol=mode) results.append(repr(compiled_code)) except (SyntaxError, TypeError): results.append(None) return results"},{"question":"You are required to write a Python function that reads a binary file containing a series of packed integers and strings, decodes the strings from encoded binary data, and outputs a list of tuples with the decoded strings and corresponding integers. Specifications: 1. **Binary File Structure**: - The file contains a series of records. - Each record consists of: - An integer (4 bytes, big-endian). - A string (preceded by a 2-byte length field, little-endian, representing the length of the string in bytes). - The string itself (UTF-8 encoded binary data). 2. **Function Signature**: ```python def unpack_binary_file(file_path: str) -> list: Reads from a binary file, unpacks the integers and strings, and returns a list of tuples. Args: - file_path (str): The path to the binary file. Returns: - list: A list of tuples, where each tuple contains an integer and a decoded string. ``` 3. **Input**: - `file_path` is a string indicating the path to the binary file. 4. **Output**: - The function returns a list of tuples. Each tuple contains: - An integer (`int`). - A decoded string (`str`). Constraints: - The file may contain multiple records; ensure your function processes all records until the end of the file. - If the file is empty, return an empty list. - The maximum length of a string is 256 bytes. Example: Suppose you have a binary file `data.bin` containing the following packed data: - Integer: 1001 - String: \\"Hello\\" - Integer: 1002 - String: \\"World\\" The function call `unpack_binary_file(\\"data.bin\\")` should return: ```python [(1001, \\"Hello\\"), (1002, \\"World\\")] ``` Additional Notes: - You should handle potential errors such as file not found or issues with decoding data gracefully. - You may use Python’s `struct` and `codecs` modules to help with binary data manipulation and string decoding.","solution":"import struct def unpack_binary_file(file_path: str) -> list: Reads from a binary file, unpacks the integers and strings, and returns a list of tuples. Args: - file_path (str): The path to the binary file. Returns: - list: A list of tuples, where each tuple contains an integer and a decoded string. results = [] try: with open(file_path, \\"rb\\") as file: while True: # Read and unpack the 4-byte integer (big-endian) int_bytes = file.read(4) if len(int_bytes) < 4: break # End of file integer = struct.unpack(\'>I\', int_bytes)[0] # Read and unpack the 2-byte length of the string (little-endian) str_len_bytes = file.read(2) if len(str_len_bytes) < 2: break # End of file str_len = struct.unpack(\'<H\', str_len_bytes)[0] # Read and decode the string str_bytes = file.read(str_len) if len(str_bytes) < str_len: break # End of file string = str_bytes.decode(\'utf-8\') # Append the tuple to the results list results.append((integer, string)) except FileNotFoundError: print(f\\"File {file_path} not found.\\") except Exception as e: print(f\\"An error occurred: {e}\\") return results"},{"question":"# Pandas Data Verification and Transformation **Objective**: Implement a function that processes a pandas DataFrame, transforms its data, and ensures the transformation is correct using pandas\' testing assertion functions. **Problem Statement**: Write a function `process_and_verify(data: pd.DataFrame) -> pd.DataFrame` that takes a pandas DataFrame as input, performs a series of transformations, and verifies the validity of the transformation using testing assertions. If any assertion fails, an appropriate custom exception should be thrown. **Functional Requirements**: 1. The function should add a new column \'total\' which is the sum of numerical columns in the input DataFrame. 2. The function should ensure that the \'total\' column has been added correctly by using `pandas.testing.assert_frame_equal`. 3. If the shape or data does not match the expected result, raise an exception `TransformationError`. 4. Handle the case where input DataFrame contains no numerical columns by returning the DataFrame unchanged but logging a warning. **Input**: - `data`: A pandas DataFrame containing numerical data. **Output**: - A transformed pandas DataFrame with an additional column \'total\'. **Constraints**: - The input DataFrame may contain non-numerical columns. - Use pandas version >= 1.0.0 **Custom Exception**: ```python class TransformationError(Exception): pass ``` **Example**: ```python import pandas as pd import pandas.testing as pdt class TransformationError(Exception): pass def process_and_verify(data: pd.DataFrame) -> pd.DataFrame: # Create a copy for transformation transformed_data = data.copy() # Check for numerical columns numerical_cols = transformed_data.select_dtypes(include=\'number\').columns if len(numerical_cols) > 0: # Adding \'total\' column transformed_data[\'total\'] = transformed_data[numerical_cols].sum(axis=1) # Creating expected DataFrame for validation expected_data = data.copy() expected_data[\'total\'] = expected_data[numerical_cols].sum(axis=1) try: # Verifying transformation pdt.assert_frame_equal(transformed_data, expected_data) except AssertionError: raise TransformationError(\\"DataFrame transformation did not match the expected result.\\") else: import warnings warnings.warn(\\"Input DataFrame contains no numerical columns. Returning unchanged DataFrame.\\") return transformed_data # Example usage: data = pd.DataFrame({ \'A\': [1, 2, 3], \'B\': [4, 5, 6], \'C\': [\'x\', \'y\', \'z\'] }) result = process_and_verify(data) print(result) ``` **Note**: Ensure that your function handles edge cases gracefully and follows good programming practices.","solution":"import pandas as pd import pandas.testing as pdt class TransformationError(Exception): pass def process_and_verify(data: pd.DataFrame) -> pd.DataFrame: Process the input DataFrame by adding a \'total\' column that sums the numerical columns and verifies the transformation. If no numerical columns are found, return the DataFrame unchanged and log a warning. Args: data: pd.DataFrame - A pandas DataFrame with potentially numerical data. Returns: pd.DataFrame - Transformed DataFrame with an additional \'total\' column. # Create a copy for transformation transformed_data = data.copy() # Check for numerical columns numerical_cols = transformed_data.select_dtypes(include=\'number\').columns if len(numerical_cols) > 0: # Adding \'total\' column transformed_data[\'total\'] = transformed_data[numerical_cols].sum(axis=1) # Creating expected DataFrame for validation expected_data = data.copy() expected_data[\'total\'] = expected_data[numerical_cols].sum(axis=1) try: # Verifying transformation pdt.assert_frame_equal(transformed_data, expected_data) except AssertionError: raise TransformationError(\\"DataFrame transformation did not match the expected result.\\") else: import warnings warnings.warn(\\"Input DataFrame contains no numerical columns. Returning unchanged DataFrame.\\") return transformed_data"},{"question":"**Objective**: You need to create a Python utility that reads multiple lines from multiple files efficiently using the `linecache` module. **Problem Statement**: You are given a list of tuples, each containing a filename and a list of line numbers. Your task is to write a function `read_lines_from_files(file_lines_list)` that takes this list as input and returns a dictionary where the keys are filenames and the values are lists of the corresponding lines from these files. You should efficiently retrieve the lines using the `linecache` module and handle any errors gracefully (i.e., if a line number exceeds the total number of lines in the file, it should simply not include that line). **Function Signature**: ```python def read_lines_from_files(file_lines_list: List[Tuple[str, List[int]]]) -> Dict[str, List[str]]: ``` **Input**: - `file_lines_list`: A list of tuples where each tuple contains: - `filename` (str): The name of the file. - `line_numbers` (List[int]): A list of integers representing the line numbers to be read from the file. **Output**: - Returns a dictionary where each key is a filename and the value is a list of lines read from the corresponding file (the lines should be in the same order as the provided line numbers). **Constraints**: - The files may or may not exist in the file system. - If a line number is invalid (e.g., negative or larger than the total number of lines), it should be skipped. - Assume the filenames are valid strings and line numbers are positive integers. - The function should handle large files efficiently by making use of the `linecache` module. **Performance Requirements**: - The solution should leverage caching to optimize multiple accesses to the same file. - Ensure the solution does not read the same line multiple times unnecessarily. **Example**: ```python file_lines_list = [ (\\"example1.txt\\", [1, 3, 5]), (\\"example2.txt\\", [2, 4, 6]) ] result = read_lines_from_files(file_lines_list) print(result) # Expected Output (assuming example files contain appropriate content): # { # \\"example1.txt\\": [\\"First Line\\", \\"Third Line\\", \\"Fifth Line\\"], # \\"example2.txt\\": [\\"Second Line\\", \\"Fourth Line\\", \\"Sixth Line\\"] # } ``` **Note**: Use the `linecache` module functions (`getline`, `checkcache`, and `clearcache`) to implement the solution.","solution":"import linecache from typing import List, Tuple, Dict def read_lines_from_files(file_lines_list: List[Tuple[str, List[int]]]) -> Dict[str, List[str]]: Reads specified lines from multiple files using the linecache module and returns a dictionary with filenames as keys and lists of lines as values. result = {} for filename, line_numbers in file_lines_list: lines = [] for line_number in line_numbers: line = linecache.getline(filename, line_number) if line: lines.append(line.strip()) result[filename] = lines # Clear the cache to prevent outdated entries linecache.clearcache() return result"},{"question":"# Python Development Mode Simulation Python Development Mode introduces several runtime checks for detecting various issues in your code, helping to ensure your code runs smoothly by exposing potential problems early on. In this exercise, you will implement a simplified version of some features offered by Python Development Mode. You are to create a class called `DevMode` that encapsulates these behaviors. # Task 1. **Class Definition** - Define a class called `DevMode`. 2. **Methods** - Implement the following methods that simulate some of the checks and behaviors associated with Python Development Mode: - `check_resource_handling()`: This method should simulate the detection of unclosed files by scanning active file references. For simplicity, you can use a list to keep track of opened files and check if they have been explicitly closed. - `track_memory_violations()`: This method should simulate a memory buffer overflow or underflow check. You need to simulate a dynamic array (list in Python) and detect any kind of overflow/underflow conditions when accessing elements or adding/removing elements beyond the boundaries. - `enable_asyncio_debug()`: Simulates asyncio debug mode, which should log if there are any unawaited coroutines in your code. For simplicity, you can keep track of coroutine states in a list and print warnings if they weren\'t awaited. # Constraints & Requirements - The solution should work under Python 3.7 and above. - Ensure proper handling and explicit closing of files. - Demonstrate an understanding of memory management in Python. - Handle coroutine states effectively and log warnings for unawaited coroutines. # Implementation Details Part 1: Resource Handling 1. Implement `DevMode.check_resource_handling()` method: ```python class DevMode: def __init__(self): self.open_files = {} def open_file(self, filepath, mode=\'r\'): try: file = open(filepath, mode) self.open_files[filepath] = file return file except Exception as e: print(f\\"Error opening file {filepath}: {e}\\") return None def close_file(self, filepath): if filepath in self.open_files: self.open_files[filepath].close() del self.open_files[filepath] else: print(f\\"File {filepath} not found in open_files\\") def check_resource_handling(self): for filepath, file in self.open_files.items(): print(f\\"ResourceWarning: unclosed file {filepath}\\") ``` Part 2: Memory Violations Check 2. Implement `DevMode.track_memory_violations()` method: ```python class DevMode: # Previous methods remain the same ... def __init__(self): self.dynamic_array = [] def add_to_array(self, value): self.dynamic_array.append(value) def remove_from_array(self, index): if index < 0 or index >= len(self.dynamic_array): print(f\\"MemoryViolationWarning: Index {index} is out of bounds\\") else: del self.dynamic_array[index] ``` Part 3: Asyncio Debug Mode 3. Implement `DevMode.enable_asyncio_debug()` method: ```python import asyncio class DevMode: # Previous methods remain the same ... def __init__(self): self.coroutine_states = [] async def track_coroutine(self, coroutine): self.coroutine_states.append(\'created\') await coroutine self.coroutine_states[-1] = \'awaited\' def enable_asyncio_debug(self): for state in self.coroutine_states: if state != \'awaited\': print(\\"AsyncioWarning: coroutine was not awaited\\") ``` Example Usage ```python # Example for resource handling dev_mode = DevMode() # Open and close file correctly file1 = dev_mode.open_file(\'file1.txt\', \'w\') dev_mode.close_file(\'file1.txt\') # Open file without closing file2 = dev_mode.open_file(\'file2.txt\', \'w\') # Check resource handling dev_mode.check_resource_handling() # Example for memory violations dev_mode.add_to_array(10) dev_mode.remove_from_array(0) dev_mode.remove_from_array(5) # This will print MemoryViolationWarning ``` # Objective The goal is to simulate a simplified version of Python Development Mode by implementing checks for handling unclosed files, memory violations, and unawaited coroutines. This demonstrates understanding of resource management, memory handling, and coroutine tracking in Python.","solution":"import asyncio class DevMode: def __init__(self): self.open_files = {} self.dynamic_array = [] self.coroutine_states = [] # Resource handling methods def open_file(self, filepath, mode=\'r\'): try: file = open(filepath, mode) self.open_files[filepath] = file return file except Exception as e: print(f\\"Error opening file {filepath}: {e}\\") return None def close_file(self, filepath): if filepath in self.open_files: self.open_files[filepath].close() del self.open_files[filepath] else: print(f\\"File {filepath} not found in open_files\\") def check_resource_handling(self): for filepath in self.open_files: print(f\\"ResourceWarning: unclosed file {filepath}\\") # Memory violations methods def add_to_array(self, value): self.dynamic_array.append(value) def remove_from_array(self, index): if index < 0 or index >= len(self.dynamic_array): print(f\\"MemoryViolationWarning: Index {index} is out of bounds\\") else: del self.dynamic_array[index] # Asyncio debug mode methods async def track_coroutine(self, coroutine): self.coroutine_states.append(\'created\') await coroutine self.coroutine_states[-1] = \'awaited\' def enable_asyncio_debug(self): for state in self.coroutine_states: if state != \'awaited\': print(\\"AsyncioWarning: coroutine was not awaited\\")"},{"question":"# Python Identifier Validator and Categorizer **Objective:** You are required to implement a function that categorizes a list of strings into valid Python identifiers, keywords, and soft keywords. This assessment will test your understanding of the `keyword` module and your ability to manipulate and filter lists in Python. **Problem Statement:** Implement a function `categorize_identifiers(identifiers: list) -> dict` that takes a list of strings and returns a dictionary with three keys: - `\'valid_identifiers\'`: a list of the strings that are valid Python identifiers and are not keywords or soft keywords. - `\'keywords\'`: a list of the strings that are Python keywords. - `\'soft_keywords\'`: a list of the strings that are Python soft keywords. **Input:** - A list of strings, `identifiers`, where each string is an identifier to be categorized. **Output:** - A dictionary with the following structure: ```python { \'valid_identifiers\': [\'list\', \'of\', \'valid\', \'identifiers\'], \'keywords\': [\'list\', \'of\', \'keywords\'], \'soft_keywords\': [\'list\', \'of\', \'soft_keywords\'] } ``` **Constraints:** - The input list `identifiers` will have at most 10,000 elements. - Each string in the input list will have a maximum length of 100 characters. **Requirements:** - Use the `keyword` module to check for keywords and soft keywords. - Ensure that the strings under `\'valid_identifiers\'` conform to the rules for Python variable names (i.e., must start with an underscore or a letter, followed by any number of letters, digits, or underscores, and must not be a keyword or soft keyword). **Example:** ```python import keyword def categorize_identifiers(identifiers): result = {\'valid_identifiers\': [], \'keywords\': [], \'soft_keywords\': []} for identifier in identifiers: if keyword.iskeyword(identifier): result[\'keywords\'].append(identifier) elif keyword.issoftkeyword(identifier): result[\'soft_keywords\'].append(identifier) elif identifier.isidentifier(): result[\'valid_identifiers\'].append(identifier) return result # Example usage identifiers = [\\"for\\", \\"while\\", \\"abc_123\\", \\"softkw\\", \\"with\\"] output = categorize_identifiers(identifiers) print(output) # Output: # { # \'valid_identifiers\': [\'abc_123\'], # \'keywords\': [\'for\', \'while\', \'with\'], # \'soft_keywords\': [\'softkw\'] # } ``` **Note:** - The `\'soft_keywords\'` key may not have values depending on the version and the presence of soft keywords in the given list.","solution":"import keyword def categorize_identifiers(identifiers): Categorizes the given list of identifiers into valid Python identifiers, keywords, and soft keywords. Args: - identifiers (list): A list of strings to categorize. Returns: - dict: A dictionary with keys \'valid_identifiers\', \'keywords\', and \'soft_keywords\', each mapped to a corresponding list. result = {\'valid_identifiers\': [], \'keywords\': [], \'soft_keywords\': []} for identifier in identifiers: if keyword.iskeyword(identifier): result[\'keywords\'].append(identifier) elif keyword.issoftkeyword(identifier): # keyword.issoftkeyword might not exist in older versions of Python result[\'soft_keywords\'].append(identifier) elif identifier.isidentifier(): result[\'valid_identifiers\'].append(identifier) return result"},{"question":"# Question: Dynamic Module Importer and Metadata Inspector You are required to write a Python function `dynamic_importer` that takes a module name as a string and performs the following tasks: 1. **Dynamically import the given module**. 2. **Check if the module can be imported**, and if not, return a meaningful error message. 3. **Retrieve and print metadata information** about the module such as its name, version, and the list of files associated with it. 4. **Return a boolean indicating** whether the import and metadata retrieval were successful or not. Input - A single string `module_name` which is the name of the module to be imported. Output - A boolean value: `True` if the module was successfully imported and the metadata was retrieved, `False` otherwise with an appropriate error message printed to the console. Function Signature ```python def dynamic_importer(module_name: str) -> bool: pass ``` Example: ```python # Example 1: print(dynamic_importer(\\"json\\")) # Expected Output: # Module \'json\' # Version: ? # Files: [...] # List of files depending on the distribution package # True # Example 2: print(dynamic_importer(\\"non_existent_module\\")) # Expected Output: # Error: Module \'non_existent_module\' cannot be imported. # False ``` Constraints - You can use `importlib` and `importlib.metadata` modules for this task. - Handle any exceptions that might occur during the import process. Notes - If a module does not have a version or file list available, handle this gracefully. - Ensure that the function is robust and handles edge cases such as non-existent modules or modules without metadata gracefully.","solution":"import importlib import importlib.metadata import sys def dynamic_importer(module_name: str) -> bool: try: # Import the module dynamically module = importlib.import_module(module_name) print(f\\"Module \'{module_name}\' imported successfully.\\") # Get the module metadata try: version = importlib.metadata.version(module_name) except importlib.metadata.PackageNotFoundError: version = \\"unknown\\" try: files = importlib.metadata.files(module_name) files = list(files) if files else [] except importlib.metadata.PackageNotFoundError: files = [] # Print metadata information print(f\\"Module: {module_name}\\") print(f\\"Version: {version}\\") print(f\\"Files: {files}\\") return True except ModuleNotFoundError: print(f\\"Error: Module \'{module_name}\' cannot be imported.\\") return False # Example usage if __name__ == \\"__main__\\": print(dynamic_importer(\\"json\\")) # Expected Output: Module imported with metadata print(dynamic_importer(\\"non_existent_module\\")) # Expected Output: Error message"},{"question":"# PyTorch Coding Assessment Question Objective To assess your understanding of PyTorch tensors and the `torch.Size` class for handling tensor dimensions, you are required to implement a function that manipulates the shape of tensors. Problem Statement You need to implement a function `reshape_tensor(tensor, new_shape)` that takes a tensor and a new shape as input parameters and returns a new tensor with the specified shape. The shape of the input tensor and the new shape should be compatible (i.e., the total number of elements must remain the same). Function Signature ```python def reshape_tensor(tensor: torch.Tensor, new_shape: tuple) -> torch.Tensor: pass ``` Input - `tensor` (torch.Tensor): A PyTorch tensor of any shape. - `new_shape` (tuple): A tuple containing the new dimensions for the tensor. Output - (torch.Tensor): A tensor reshaped to the given dimensions. Constraints 1. The new shape must be compatible with the number of elements in the original tensor. 2. If the new shape is not compatible, the function should raise a `ValueError` with an appropriate message. Example ```python import torch # Example 1 tensor = torch.ones(2, 3, 4) new_shape = (6, 4) reshaped_tensor = reshape_tensor(tensor, new_shape) print(reshaped_tensor.size()) # Output: torch.Size([6, 4]) # Example 2 tensor = torch.ones(4, 4) new_shape = (2, 8) reshaped_tensor = reshape_tensor(tensor, new_shape) print(reshaped_tensor.size()) # Output: torch.Size([2, 8]) # Example 3 tensor = torch.ones(2, 2, 2) new_shape = (8,) reshaped_tensor = reshape_tensor(tensor, new_shape) print(reshaped_tensor.size()) # Output: torch.Size([8]) # Example 4 - Incompatible shape tensor = torch.ones(2, 2, 2) new_shape = (5,) try: reshaped_tensor = reshape_tensor(tensor, new_shape) except ValueError as e: print(e) # Output: \\"The new shape is not compatible with the number of elements in the original tensor.\\" ``` Notes - You can use the `torch.Size` method to get the dimensions of the tensor. - Ensure you validate the compatibility of the new shape with the original tensor\'s elements before reshaping. Good luck!","solution":"import torch def reshape_tensor(tensor: torch.Tensor, new_shape: tuple) -> torch.Tensor: Reshapes the input tensor to the new shape if compatible. Parameters: tensor (torch.Tensor): The input tensor. new_shape (tuple): The desired shape. Returns: torch.Tensor: The reshaped tensor. Raises: ValueError: If the new shape is not compatible with the number of elements in the original tensor. if tensor.numel() != torch.tensor(new_shape).prod().item(): raise ValueError(\\"The new shape is not compatible with the number of elements in the original tensor.\\") return tensor.view(new_shape)"},{"question":"**Objective**: Implement a class using Python\'s `sched` module to create a task scheduler that can schedule tasks based on the delay time and execute them in the correct order according to their priority and scheduled time. Task 1. Implement a class `TaskScheduler` with the following methods: - `__init__`: Initializes an instance of the scheduler. - `add_task(delay: int, priority: int, action: Callable, argument: Tuple = (), kwargs: Dict = {}) -> Any`: Schedules a new task to be executed after a given delay with a specified priority. - `remove_task(task: Any) -> None`: Cancels a previously scheduled task. - `run_scheduler(blocking: bool = True) -> None`: Runs the scheduler to execute all scheduled tasks. - `get_task_queue() -> List`: Returns a list of all upcoming tasks in the order they will be executed. 2. Your implementation should ensure that after a task is executed, the event loop continues to the next task smoothly. 3. All methods should handle any exceptions that might be raised, maintaining a consistent state for the scheduler. Constraints - The `delay` parameter is a non-negative integer representing the delay in seconds. - The `priority` parameter is an integer where a lower number indicates a higher priority. - The `action` parameter is a callable function that the task scheduler will execute. Example Usage ```python import time def print_message(message): print(f\\"{time.time()}: {message}\\") # Create an instance of TaskScheduler scheduler = TaskScheduler() # Schedule some tasks task1 = scheduler.add_task(5, 1, print_message, argument=(\\"Task 1\\",)) task2 = scheduler.add_task(3, 2, print_message, argument=(\\"Task 2\\",)) task3 = scheduler.add_task(10, 1, print_message, argument=(\\"Task 3\\",)) # Check the task queue print(scheduler.get_task_queue()) # Run the scheduler scheduler.run_scheduler() ``` **Note**: The tasks should be executed in the order: - `Task 2` - `Task 1` - `Task 3`","solution":"import sched import time from typing import Callable, Tuple, Dict, Any, List class TaskScheduler: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) def add_task(self, delay: int, priority: int, action: Callable, argument: Tuple = (), kwargs: Dict = {}) -> Any: event = self.scheduler.enter(delay, priority, action, argument, kwargs) return event def remove_task(self, task: Any) -> None: try: self.scheduler.cancel(task) except ValueError: pass # Task could not be canceled because it was already executed or never scheduled def run_scheduler(self, blocking: bool = True) -> None: if blocking: self.scheduler.run() else: while self.scheduler.queue: self.scheduler.run(blocking=False) time.sleep(0.1) # Allowing other operations if not blocking def get_task_queue(self) -> List: return self.scheduler.queue"},{"question":"# XML Processing and Modification Problem Statement You are tasked with manipulating an XML file containing a list of books in a library. The XML file follows this structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>1999</year> <price>35.50</price> </book> <!-- More book elements --> </library> ``` You need to write a Python function `update_library(xml_string, title_filter, price_increase)` that performs the following operations: 1. **Parse** the given XML string. 2. **Find** all books where the title contains the given `title_filter` string. 3. **Increase** the price of the found books by the given `price_increase` amount. 4. **Return** the modified XML as a string. Function Signature ```python def update_library(xml_string: str, title_filter: str, price_increase: float) -> str: pass ``` Input - `xml_string` (str): A string containing the XML data representing the library. - `title_filter` (str): A string to filter book titles. - `price_increase` (float): The amount by which to increase the price of the selected books. Output - Returns a modified XML string with updated book prices. Example ```python xml_string = <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book with Keyword in Title</title> <author>Author 2</author> <year>1999</year> <price>35.50</price> </book> </library> # Case 1 title_filter = \\"Keyword\\" price_increase = 5.00 print(update_library(xml_string, title_filter, price_increase)) # Expected Output <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book with Keyword in Title</title> <author>Author 2</author> <year>1999</year> <price>40.50</price> </book> </library> ``` Constraints - The XML string will always be well-formed. - The price increase will always be a non-negative float. - The title filter is case-insensitive. Advanced Concepts Required - XML parsing using `xml.etree.ElementTree`. - String manipulation. - Modifying and constructing XML documents. Ensure that your solution is efficient and handles the given constraints effectively.","solution":"import xml.etree.ElementTree as ET def update_library(xml_string: str, title_filter: str, price_increase: float) -> str: Parses the XML string, finds books with titles containing the title_filter, increases their prices by price_increase, and returns the modified XML string. # Parse the XML string root = ET.fromstring(xml_string) # Iterate over the books found in the library for book in root.findall(\\"book\\"): title = book.find(\\"title\\").text # Case-insensitive check if title contains the title_filter if title_filter.lower() in title.lower(): price_element = book.find(\\"price\\") # Update the price new_price = float(price_element.text) + price_increase price_element.text = f\\"{new_price:.2f}\\" # Format to 2 decimal places # Convert the ElementTree back to a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Advanced Data Merging Assessment Objective You are given several datasets that you need to combine and analyze. Your task is to apply various merging techniques using the pandas library to achieve the final consolidated dataset. Task 1. **Data Preparation**: - Create three DataFrames `df_sales`, `df_customers`, and `df_products` as shown below. ```python import pandas as pd # DataFrame for sales data data_sales = { \'transaction_id\': [101, 102, 103, 104, 105], \'customer_id\': [1, 2, 3, 4, 5], \'product_id\': [\'P001\', \'P002\', \'P001\', \'P003\', \'P002\'], \'quantity\': [10, 15, 7, 12, 5] } df_sales = pd.DataFrame(data_sales) # DataFrame for customers data data_customers = { \'customer_id\': [1, 2, 3, 4, 5, 6], \'customer_name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eva\', \'Frank\'], \'region\': [\'North\', \'South\', \'West\', \'East\', \'North\', \'West\'] } df_customers = pd.DataFrame(data_customers) # DataFrame for products data data_products = { \'product_id\': [\'P001\', \'P002\', \'P003\'], \'product_name\': [\'Product A\', \'Product B\', \'Product C\'], \'price\': [20.5, 35.0, 12.75] } df_products = pd.DataFrame(data_products) ``` 2. **Combine DataFrames**: - Merge `df_sales` and `df_customers` on the `customer_id` column, keeping all rows from the sales DataFrame. Name this new DataFrame `df_sales_customers`. - Merge `df_sales_customers` with `df_products` on the `product_id` column. Ensure all rows from the `df_sales_customers` DataFrame are kept. Name this final DataFrame `df_final`. 3. **Calculate Total Sales**: - Add a new column to `df_final` named `total_sales` which is calculated as (`quantity` * `price`). 4. **Comparison**: - Create a new DataFrame `df_final_copy` by making a copy of `df_final`. - In `df_final_copy`, change the name of the customer with `customer_id` 3 to \'Chuck\'. - Use the `compare` function to find and display the differences between `df_final` and `df_final_copy`. Expected Output 1. Display the `df_sales_customers` DataFrame. 2. Display the final merged DataFrame `df_final` with the additional `total_sales` column. 3. Display the result of the comparison between `df_final` and `df_final_copy`. Constraints 1. Ensure that the merged DataFrames retain as much of the original data as possible by choosing the appropriate merging strategy. 2. Properly handle any potential missing values that may arise during the merging process.","solution":"import pandas as pd # DataFrame for sales data data_sales = { \'transaction_id\': [101, 102, 103, 104, 105], \'customer_id\': [1, 2, 3, 4, 5], \'product_id\': [\'P001\', \'P002\', \'P001\', \'P003\', \'P002\'], \'quantity\': [10, 15, 7, 12, 5] } df_sales = pd.DataFrame(data_sales) # DataFrame for customers data data_customers = { \'customer_id\': [1, 2, 3, 4, 5, 6], \'customer_name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eva\', \'Frank\'], \'region\': [\'North\', \'South\', \'West\', \'East\', \'North\', \'West\'] } df_customers = pd.DataFrame(data_customers) # DataFrame for products data data_products = { \'product_id\': [\'P001\', \'P002\', \'P003\'], \'product_name\': [\'Product A\', \'Product B\', \'Product C\'], \'price\': [20.5, 35.0, 12.75] } df_products = pd.DataFrame(data_products) # Merge df_sales and df_customers df_sales_customers = pd.merge(df_sales, df_customers, on=\'customer_id\', how=\'left\') # Merge df_sales_customers and df_products df_final = pd.merge(df_sales_customers, df_products, on=\'product_id\', how=\'left\') # Calculate total sales df_final[\'total_sales\'] = df_final[\'quantity\'] * df_final[\'price\'] # Create a copy of df_final and modify df_final_copy = df_final.copy() df_final_copy.loc[df_final_copy[\'customer_id\'] == 3, \'customer_name\'] = \'Chuck\' # Compare the two DataFrames comparison = df_final.compare(df_final_copy) [df_sales_customers, df_final, comparison]"},{"question":"You are required to implement a custom class `CustomBuffer` that supports the buffer protocol, and a function `write_to_file` that takes an instance of this class and writes its contents to a file. Class: `CustomBuffer` 1. **Constructor**: - `__init__(self, data: bytes)`: Initialize the instance with a `bytes` object. 2. **Buffer Interface**: - Implement the necessary interface methods to expose the buffer of the `bytes` object. 3. **Buffer Fields**: - Ensure that the buffer fields like `buf`, `len`, `readonly`, etc., are correctly set and managed. Function: `write_to_file` 1. **Parameters**: - `write_to_file(buffer: CustomBuffer, file_path: str) -> None`: Takes a `CustomBuffer` instance and a file path as input. 2. **Functionality**: - Use the buffer protocol to get the raw data from the `CustomBuffer` instance and write it to the specified file. # Implementation Guidelines 1. **Class Implementation**: - Implement the buffer interface methods in `CustomBuffer` as per the buffer protocol documentation. - Ensure proper handling of the buffer\'s memory, including accessing and releasing the buffer. 2. **Function Implementation**: - Use `PyObject_GetBuffer` to acquire the buffer from the `CustomBuffer` instance. - Write the buffer\'s contents to the specified file. 3. **Constraints**: - The `data` provided to `CustomBuffer` will always be a non-empty `bytes` object. - The file path provided will be a valid path that the script has permission to write to. 4. **Performance**: - Ensure that buffer access and release are handled efficiently to avoid memory leaks. # Example ```python # Example class usage data = b\\"Hello, buffer protocol!\\" buffer = CustomBuffer(data) # Example function usage file_path = \'output.txt\' write_to_file(buffer, file_path) # File \'output.txt\' should contain: Hello, buffer protocol! ``` # Notes 1. You should validate the buffer implementation with examples mentioned to correctly handle the buffer interface. 2. Ensure to manage references properly, particularly acquiring and releasing the buffer.","solution":"import ctypes class CustomBuffer: def __init__(self, data: bytes): self.data = data self._buffer = ctypes.create_string_buffer(self.data) self._buflen = len(self.data) self._readonly = False def __len__(self): return self._buflen def __buffer__(self): return bytes(self._buffer.raw)[:self._buflen] def get_buffer_info(self): return { \'buf\': ctypes.addressof(self._buffer), \'len\': self._buflen, \'readonly\': self._readonly } def write_to_file(buffer: CustomBuffer, file_path: str): with open(file_path, \'wb\') as file: file.write(buffer.__buffer__())"},{"question":"**Asynchronous Reader-Writer Problem** In many computer systems, readers and writers compete for access to a shared resource such as a file or a database. A reader can coexist with other readers, but a writer needs exclusive access to the shared resource. Using the `asyncio` synchronization primitives provided in the documentation (Lock, Event, Condition, Semaphore, BoundedSemaphore), implement an asynchronous Reader-Writer system with the following requirements: 1. Multiple readers can read from the shared resource simultaneously. 2. Only one writer can write to the shared resource at a time. 3. A writer must wait until there are no readers accessing the shared resource before it can start writing. 4. Once a writer is writing, no other readers or writers can access the shared resource until the writer is finished. You need to implement an asynchronous class `ReaderWriterLock` that provides three methods: - `reader_acquire()`: Coroutine for acquiring the read lock. - `reader_release()`: Method for releasing the read lock. - `writer_acquire()`: Coroutine for acquiring the write lock. - `writer_release()`: Method for releasing the write lock. Additionally, you should implement a small test suite to demonstrate the functionality of your `ReaderWriterLock` class. # Input and Output Formats **Input:** - No explicit input; interaction is through method calls. **Output:** - No explicit output; function correctness is validated through logical consistency and print statements in the test suite. # Performance Requirements - Ensure that the read locks and write locks are acquired and released efficiently without causing unnecessary delays. # Constraints - Use only the provided `asyncio` synchronization primitives (Lock, Event, Condition, Semaphore, BoundedSemaphore). - The implementation should be free of race conditions and deadlocks. # Example Usage ```python import asyncio import random import time class ReaderWriterLock: def __init__(self): # Implement the needed synchronization primitives here pass async def reader_acquire(self): # Implement reader lock acquisition logic pass def reader_release(self): # Implement reader lock release logic pass async def writer_acquire(self): # Implement write lock acquisition logic pass def writer_release(self): # Implement write lock release logic pass async def reader_task(lock, reader_id): print(f\\"Reader {reader_id} attempting to read\\") await lock.reader_acquire() try: print(f\\"Reader {reader_id} reading\\") await asyncio.sleep(random.uniform(0.1, 0.5)) finally: print(f\\"Reader {reader_id} finished reading\\") lock.reader_release() async def writer_task(lock, writer_id): print(f\\"Writer {writer_id} attempting to write\\") await lock.writer_acquire() try: print(f\\"Writer {writer_id} writing\\") await asyncio.sleep(random.uniform(0.1, 0.5)) finally: print(f\\"Writer {writer_id} finished writing\\") lock.writer_release() async def main(): lock = ReaderWriterLock() tasks = [] for i in range(5): tasks.append(reader_task(lock, i)) tasks.append(writer_task(lock, i)) await asyncio.gather(*tasks) if __name__ == \\"__main__\\": asyncio.run(main()) ``` Implement the `ReaderWriterLock` class and provide the appropriate logic for the `reader_acquire`, `reader_release`, `writer_acquire`, and `writer_release` methods based on the provided `asyncio` synchronization primitives.","solution":"import asyncio class ReaderWriterLock: def __init__(self): self._readers = 0 self._reader_lock = asyncio.Lock() self._writer_lock = asyncio.Lock() async def reader_acquire(self): async with self._reader_lock: self._readers += 1 if self._readers == 1: await self._writer_lock.acquire() def reader_release(self): asyncio.run(self._release_reader()) async def _release_reader(self): async with self._reader_lock: self._readers -= 1 if self._readers == 0: self._writer_lock.release() async def writer_acquire(self): await self._writer_lock.acquire() def writer_release(self): self._writer_lock.release()"},{"question":"**Objective:** Design a Python function using pandas that temporarily sets specific display options for a DataFrame. The function should ensure the display options are reverted to their original settings after execution. **Task:** Write a function `temporary_display_options()` that accepts a DataFrame `df` and modifies its display settings temporarily. The function should: 1. Temporarily set `display.max_columns` to 10. 2. Temporarily set `display.precision` to 4. 3. Temporarily set `display.expand_frame_repr` to `True`. 4. Print the DataFrame to demonstrate the changes. 5. Ensure that original settings for `display.max_columns`, `display.precision`, and `display.expand_frame_repr` are restored after the function execution. **Function Signature:** ```python def temporary_display_options(df): pass ``` **Constraints:** - You should use the `pd.option_context` to temporarily change the settings. - The function should print the DataFrame using the specified temporary settings. - After the function call, the global pandas settings should remain unchanged. **Example:** ```python import pandas as pd import numpy as np # Sample DataFrame df = pd.DataFrame(np.random.randn(5, 12), columns=list(\'ABCDEFGHIJKL\')) # Before calling the function print(pd.get_option(\\"display.max_columns\\")) # Assume default is 20 print(pd.get_option(\\"display.precision\\")) # Assume default is 6 print(pd.get_option(\\"display.expand_frame_repr\\")) # Assume default is False # Call the function temporary_display_options(df) # After calling the function print(pd.get_option(\\"display.max_columns\\")) # Should print 20 (or original setting) print(pd.get_option(\\"display.precision\\")) # Should print 6 (or original setting) print(pd.get_option(\\"display.expand_frame_repr\\")) # Should print False (or original setting) ``` The function should temporarily change the settings only within its context, and original settings should be restored after the function\'s execution. **Hints:** - Explore the `pd.option_context` to set options temporarily. - Use `print()` statements to observe the temporary changes and verify the restoration of original settings.","solution":"import pandas as pd def temporary_display_options(df): Temporarily sets display options for a DataFrame and reverts them to their original settings after execution. Parameters: df (pd.DataFrame): The DataFrame to display with temporary settings. with pd.option_context(\'display.max_columns\', 10, \'display.precision\', 4, \'display.expand_frame_repr\', True): print(df)"},{"question":"Objective: This exercise will test your understanding of the `lzma` module, focusing on file operations, custom filter chains, and error handling. Problem Statement: You are given multiple tasks to demonstrate your knowledge of the `lzma` module for handling LZMA-compressed files. 1. **File Compression and Decompression**: - Create a function `compress_file(input_filename: str, output_filename: str, preset: int = 6) -> None` that compresses a file. - Create a function `decompress_file(input_filename: str, output_filename: str) -> None` that decompresses a file. 2. **Custom Filter Chain Compression**: - Create a function `compress_with_custom_filter(input_filename: str, output_filename: str, filters: list) -> None` that compresses a file using a custom filter chain. 3. **Error Handling**: - Extend the previous functions to handle cases where compressed data is corrupted. Raise a custom exception `CustomLZMAError` if decompression fails due to data corruption. Input and Output Specifications: # 1. `compress_file`: - **Input**: - `input_filename`: The name of the file to be compressed. - `output_filename`: The name of the file where compressed data will be saved. - `preset` (optional): The preset compression level to use (default is 6). - **Output**: - This function does not return anything. It writes the compressed data to the `output_filename`. # 2. `decompress_file`: - **Input**: - `input_filename`: The name of the file to be decompressed. - `output_filename`: The name of the file where decompressed data will be saved. - **Output**: - This function does not return anything. It writes the decompressed data to the `output_filename`. # 3. `compress_with_custom_filter`: - **Input**: - `input_filename`: The name of the file to be compressed. - `output_filename`: The name of the file where compressed data will be saved. - `filters`: A list of dictionaries specifying the filter chain. - **Output**: - This function does not return anything. It writes the compressed data to the `output_filename`. # Custom Exception Handling: - Define a custom exception class `CustomLZMAError`. - Extend the `decompress_file` function to raise `CustomLZMAError` if decompression fails due to data corruption. Example Usage: ```python try: compress_file(\\"example.txt\\", \\"compressed_data.xz\\", preset=9) decompress_file(\\"compressed_data.xz\\", \\"decompressed_example.txt\\") except CustomLZMAError as e: print(f\\"Decompression failed: {e}\\") ``` Constraints: - Preset values should be between 0 and 9 (inclusive). - Handle large files efficiently without loading the entire file contents into memory at once. - Ensure all file operations are safely closed after completion or failure. Notes: - Utilize the `lzma.open` function and the classes `LZMAFile`, `LZMACompressor`, and `LZMADecompressor`. - Refer to the examples provided in the documentation for guidance.","solution":"import lzma class CustomLZMAError(Exception): Custom exception for handling LZMA decompression errors. pass def compress_file(input_filename: str, output_filename: str, preset: int = 6) -> None: if not (0 <= preset <= 9): raise ValueError(\\"Preset must be between 0 and 9 inclusive.\\") with open(input_filename, \'rb\') as input_file, lzma.open(output_filename, \'wb\', preset=preset) as output_file: while True: chunk = input_file.read(1024 * 1024) # Read in 1MB chunks if not chunk: break output_file.write(chunk) def decompress_file(input_filename: str, output_filename: str) -> None: try: with lzma.open(input_filename, \'rb\') as input_file, open(output_filename, \'wb\') as output_file: while True: chunk = input_file.read(1024 * 1024) # Read in 1MB chunks if not chunk: break output_file.write(chunk) except lzma.LZMAError as e: raise CustomLZMAError(\\"Decompression failed due to data corruption\\") from e def compress_with_custom_filter(input_filename: str, output_filename: str, filters: list) -> None: with open(input_filename, \'rb\') as input_file: with lzma.open(output_filename, \'wb\', filters=filters) as output_file: while True: chunk = input_file.read(1024 * 1024) # Read in 1MB chunks if not chunk: break output_file.write(chunk)"},{"question":"Module Importer In this task, you will implement a custom module importer using the deprecated `imp` module. Although `imp` is deprecated, it still provides essential insights into how import mechanisms work. You are required to: 1. Implement a function `custom_importer(module_name: str, module_path: Optional[str] = None) -> ModuleType` that takes the name of the module and an optional module path: - If the module path is not provided, it should use the default system paths to locate the module. - If the module is found, it should load the module and return it. - If the module is not found, it should raise an `ImportError`. Here are the specifications for the function: - **Function:** ```python def custom_importer(module_name: str, module_path: Optional[str] = None) -> ModuleType: ``` - **Input:** - `module_name`: `str` - The name of the module to be imported. - `module_path`: `Optional[str]` - The optional path where the module might be located. Default is `None`. - **Output:** - Returns the imported module if found. - Raises `ImportError` if the module is not found. - **Constraints:** - You should use the `imp` module\'s functions: `find_module`, `load_module`, and `get_suffixes` to implement this function. - Ensure proper handling of files to avoid file descriptor leaks. - **Example:** ```python # Assuming a module named \'example_module\' exists in the specified path module = custom_importer(\'example_module\', \'/path/to/module\') print(module) ``` Note: Even though `imp` is deprecated, this exercise is aimed at understanding the underlying mechanics of the import system and how it can be controlled programmatically. File Imports: To test your implementation, you may need to have some modules with known names and paths available. Handle exceptions and edge cases appropriately.","solution":"import imp from types import ModuleType from typing import Optional def custom_importer(module_name: str, module_path: Optional[str] = None) -> ModuleType: Imports a module using the deprecated imp module. Args: - module_name: The name of the module to import. - module_path: An optional path to the module. If not provided, standard system paths are used. Returns: - The imported module. Raises: - ImportError: If the module cannot be found or loaded. try: if module_path: file, pathname, description = imp.find_module(module_name, [module_path]) else: file, pathname, description = imp.find_module(module_name) try: module = imp.load_module(module_name, file, pathname, description) finally: if file: file.close() return module except ImportError as e: raise ImportError(f\\"Module {module_name} could not be loaded.\\") from e"},{"question":"**Question: Validation and Matrix Operations using Scikit-Learn Utility Functions** You are given a dataset in the form of two 2D numpy arrays, `X` and `y`, representing input features and corresponding targets, respectively. Your task is to validate these inputs, preprocess the data, and compute the truncated Singular Value Decomposition (SVD) of the preprocessed input matrix. Implement the function `validate_and_svd` to achieve this. # Function Signature ```python def validate_and_svd(X: np.ndarray, y: np.ndarray, n_components: int, random_state: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: ``` # Input - `X`: 2D numpy array of shape `(n_samples, n_features)` representing the input features. - `y`: 1D or 2D numpy array of shape `(n_samples,)` or `(n_samples, n_targets)` representing the target values. - `n_components`: Integer, the number of singular values and vectors to compute. - `random_state`: Integer, seed for the random number generator. (default: 0) # Output - A tuple of three numpy arrays `(U, Sigma, VT)` representing the truncated SVD of the preprocessed input matrix `X`: - `U`: 2D numpy array of shape `(n_samples, n_components)`, left singular vectors. - `Sigma`: 1D numpy array of shape `(n_components,)`, singular values. - `VT`: 2D numpy array of shape `(n_components, n_features)`, right singular vectors. # Constraints - Ensure that `X` is a valid 2D numpy array without NaN or Inf values. - Ensure that `X` and `y` have consistent lengths. - Use a `random_state` for reproducibility when computing the randomized SVD. # Example ```python import numpy as np X = np.array([[1, 2], [3, 4], [5, 6]]) y = np.array([1, 2, 3]) n_components = 2 random_state = 0 U, Sigma, VT = validate_and_svd(X, y, n_components, random_state) print(\\"U:\\", U) print(\\"Sigma:\\", Sigma) print(\\"VT:\\", VT) ``` # Notes - You may use the `check_array`, `check_X_y`, and `check_random_state` functions from `sklearn.utils` for validation. - Use the `randomized_svd` function from `sklearn.utils.extmath` to compute the truncated SVD. # Solution Template ```python from sklearn.utils import check_X_y, check_array, check_random_state from sklearn.utils.extmath import randomized_svd import numpy as np from typing import Tuple def validate_and_svd(X: np.ndarray, y: np.ndarray, n_components: int, random_state: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: # Validate inputs X, y = check_X_y(X, y) X = check_array(X) # Set random state random_state = check_random_state(random_state) # Compute truncated SVD U, Sigma, VT = randomized_svd(X, n_components=n_components, random_state=random_state) return U, Sigma, VT ``` Implement the function `validate_and_svd` to solve the problem.","solution":"from sklearn.utils import check_X_y, check_array, check_random_state from sklearn.utils.extmath import randomized_svd import numpy as np from typing import Tuple def validate_and_svd(X: np.ndarray, y: np.ndarray, n_components: int, random_state: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Validate input arrays and compute the truncated SVD of the input matrix X. Parameters: X (np.ndarray): Input feature matrix of shape (n_samples, n_features). y (np.ndarray): Target values of shape (n_samples,) or (n_samples, n_targets). n_components (int): Number of singular values and vectors to compute. random_state (int): Seed for the random number generator. (default: 0) Returns: Tuple[np.ndarray, np.ndarray, np.ndarray]: U, Sigma, VT matrices of truncated SVD. # Validate inputs X, y = check_X_y(X, y) X = check_array(X) # Set random state random_state = check_random_state(random_state) # Compute truncated SVD U, Sigma, VT = randomized_svd(X, n_components=n_components, random_state=random_state) return U, Sigma, VT"},{"question":"**Objective:** Assess students\' understanding of using seaborn and matplotlib for creating and customizing plots. **Task:** Create a function `create_custom_plot` that accepts a pandas DataFrame and performs the following: 1. Plots a scatter plot for the given DataFrame with `x` and `y` axis specified by the user. 2. Customizes the plot by adding a title and adjusting the theme. 3. Adds custom annotations or shapes using matplotlib. 4. Creates a multi-plot layout using subfigures, where one part shows the scatter plot, and another part shows a histogram of the x-axis data. **Function Signature:** ```python def create_custom_plot(df: pd.DataFrame, x: str, y: str, title: str) -> mpl.figure.Figure: pass ``` **Input:** - `df`: A pandas DataFrame containing the data. - `x`: Column name to be used for the x-axis. - `y`: Column name to be used for the y-axis. - `title`: Title of the plot. **Output:** - Returns a `matplotlib.figure.Figure` object containing the customized plot with subfigures. **Constraints:** - Assume that `df` always contains valid numeric data for the specified `x` and `y` columns. - Use seaborn for creating the base plots and matplotlib for customization. **Example Usage:** ```python import pandas as pd from seaborn import load_dataset diamonds = load_dataset(\\"diamonds\\") fig = create_custom_plot(diamonds, \\"carat\\", \\"price\\", \\"Diamond Carat vs Price\\") fig.show() ``` **Expected Result:** The function should produce a figure with: - A scatter plot of `carat` vs `price`. - A title put on the scatter plot. - Custom annotations or shapes. - A histogram of the `carat` data displayed as a subfigure alongside the scatter plot. **Notes:** - Use `seaborn.objects.Plot` for creating the scatter plot. - Use `matplotlib.Figure` and subfigures for arranging the layout. - Customize the figure using matplotlib methods to add annotations or shapes.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(df: pd.DataFrame, x: str, y: str, title: str) -> plt.Figure: Creates a custom plot with the given DataFrame including a scatter plot, a histogram, and custom annotations/shapes. Parameters: df (pd.DataFrame): The data frame containing the data. x (str): The column name for the x-axis. y (str): The column name for the y-axis. title (str): The title for the scatter plot. Returns: plt.Figure: The matplotlib figure object containing the plots. sns.set_theme(style=\\"darkgrid\\") # Create a figure with two subplots - one for scatter plot and one for histogram fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7)) # Scatter plot sns.scatterplot(data=df, x=x, y=y, ax=ax1) ax1.set_title(title) ax1.set_xlabel(x) ax1.set_ylabel(y) # Adding a custom annotation max_value = df[y].max() max_value_x = df.loc[df[y].idxmax(), x] ax1.annotate(\'Max Value\', xy=(max_value_x, max_value), xytext=(max_value_x + 0.5, max_value + 5), arrowprops=dict(facecolor=\'red\', arrowstyle=\\"->\\")) # Histogram sns.histplot(df[x], bins=30, ax=ax2) ax2.set_title(f\'Histogram of {x}\') ax2.set_xlabel(x) ax2.set_ylabel(\'Frequency\') plt.tight_layout() return fig"},{"question":"Objective: Demonstrate your understanding of seaborn theme configuration and display settings. Task: Write a Python function called `customize_seaborn_plot` that does the following: 1. Takes a seaborn plot and a theme configuration dictionary as inputs. 2. Applies the given theme configurations to the plot. 3. Saves the plot to a specified file format with configurable display settings (e.g., SVG, HiDPI PNG). 4. Resets the seaborn theme to its default settings after saving the plot. Function Signature: ```python def customize_seaborn_plot(plot: so.Plot, theme_config: dict, file_format: str = \'png\', hidpi: bool = True, scaling: float = 1.0, file_path: str = \'custom_plot\'): Customize the seaborn plot with given theme and display settings. Parameters: plot (seaborn.objects.Plot): The seaborn plot object to be customized. theme_config (dict): Dictionary containing seaborn theme parameters. file_format (str): The format to save the plot (\'png\' or \'svg\'). Default is \'png\'. hidpi (bool): Whether to save the plot in HiDPI. Default is True. scaling (float): Scaling factor for the plot. Default is 1.0. file_path (str): The file path to save the customized plot, without extension. Default is \'custom_plot\'. Returns: None ``` Requirements: 1. Use `Plot.config.theme.update` to apply the theme configurations from the `theme_config` dictionary. 2. Adapt the display settings (file format, HiDPI, scaling) using `Plot.config.display`. 3. Save the plot in the specified format (`file_format`) and location (`file_path`). 4. Ensure the plot is saved with the default seaborn theme after customization. Example: ```python import seaborn.objects as so # Sample seaborn plot plot = so.Plot(data=df, x=\'column1\', y=\'column2\') # Theme configuration dictionary theme_config = { \\"axes.facecolor\\": \\"white\\", \\"axes.edgecolor\\": \\"black\\" } # Customize and save plot as SVG customize_seaborn_plot(plot, theme_config, file_format=\'svg\', hidpi=False, scaling=0.8, file_path=\'output/custom_plot\') ``` Notes: - Ensure you reset the theme to seaborn\'s defaults after saving the plot. - Handle any exceptions that might occur during the configuration or saving process.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def customize_seaborn_plot(plot: so.Plot, theme_config: dict, file_format: str = \'png\', hidpi: bool = True, scaling: float = 1.0, file_path: str = \'custom_plot\'): Customize the seaborn plot with given theme and display settings. Parameters: plot (seaborn.objects.Plot): The seaborn plot object to be customized. theme_config (dict): Dictionary containing seaborn theme parameters. file_format (str): The format to save the plot (\'png\' or \'svg\'). Default is \'png\'. hidpi (bool): Whether to save the plot in HiDPI. Default is True. scaling (float): Scaling factor for the plot. Default is 1.0. file_path (str): The file path to save the customized plot, without extension. Default is \'custom_plot\'. Returns: None # Update theme with the given theme configuration sns.set_theme(style=\\"whitegrid\\") sns.plotting_context_rc = theme_config # Adapt display settings (file format, HiDPI, scaling) if hidpi: dpi = 300 else: dpi = 100 # Create the plot g = plot.plot() # Save the plot with the specified settings plt.savefig(f\\"{file_path}.{file_format}\\", format=file_format, dpi=dpi, bbox_inches=\\"tight\\") # Reset seaborn theme to default sns.reset_defaults() sns.set_theme()"},{"question":"**Question**: You are given a dataset for binary classification, where each instance has multiple features. Your task is to implement a binary classifier using the `SGDClassifier` from scikit-learn. The classifier should be trained and tested on the given dataset, and you are required to perform feature scaling and hyperparameter tuning for optimal performance. **Instructions**: 1. **Data Preprocessing**: - Load the dataset from a CSV file. - Split the dataset into training and testing sets. - Perform feature scaling using `StandardScaler`. 2. **Model Training and Tuning**: - Create an `SGDClassifier` pipeline with `StandardScaler` and `SGDClassifier`. - Use grid search (`GridSearchCV`) to find the best hyperparameters for the `SGDClassifier`. The parameters to tune are `alpha` (regularization term) and `max_iter` (number of iterations). 3. **Evaluation**: - Evaluate the model on the test set using accuracy, precision, recall, and F1 score. - Print the best parameters found during the grid search. - Print the evaluation metrics on the test set. **Dataset**: Assume the dataset is provided in a CSV file named `data.csv` with the following structure: - The target variable (class label) is in the last column. - All other columns are feature variables. **Skeleton Code**: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(file_path): # Load dataset data = pd.read_csv(file_path) X = data.iloc[:, :-1] y = data.iloc[:, -1] return X, y def train_and_evaluate_model(X, y): # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline with StandardScaler and SGDClassifier pipeline = make_pipeline(StandardScaler(), SGDClassifier(random_state=42)) # Define grid search parameters param_grid = { \'sgdclassifier__alpha\': [0.0001, 0.001, 0.01, 0.1], \'sgdclassifier__max_iter\': [1000, 2000, 3000], } # Perform grid search with cross-validation grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'f1\') grid_search.fit(X_train, y_train) # Best parameters found during grid search print(\\"Best Parameters: \\", grid_search.best_params_) # Evaluate the model on the test set y_pred = grid_search.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) # Print evaluation metrics print(\\"Accuracy: \\", accuracy) print(\\"Precision: \\", precision) print(\\"Recall: \\", recall) print(\\"F1 Score: \\", f1) if __name__ == \\"__main__\\": # Load data X, y = load_data(\'data.csv\') # Train and evaluate the model train_and_evaluate_model(X, y) ``` **Note**: Ensure that you handle any missing values or categorical variables in the dataset if needed. **Constraints**: - Use `SGDClassifier` with `loss=\'hinge\'` and `penalty=\'l2\'`. - Ensure reproducibility by setting `random_state=42` where applicable. **Performance Requirements**: - The model should be evaluated using accuracy, precision, recall, and F1 score. - The solution should include feature scaling and hyperparameter tuning using a pipeline.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(file_path): Loads the dataset from the specified CSV file. Args: file_path (str): The path to the CSV file. Returns: X (DataFrame): The feature variables. y (Series): The target variable. data = pd.read_csv(file_path) X = data.iloc[:, :-1] y = data.iloc[:, -1] return X, y def train_and_evaluate_model(X, y): Trains and evaluates an SGDClassifier on the provided dataset. Args: X (DataFrame): The feature variables. y (Series): The target variable. Returns: dict: A dictionary containing the best parameters and evaluation metrics. # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create a pipeline with StandardScaler and SGDClassifier pipeline = make_pipeline(StandardScaler(), SGDClassifier(random_state=42, loss=\'hinge\', penalty=\'l2\')) # Define grid search parameters param_grid = { \'sgdclassifier__alpha\': [0.0001, 0.001, 0.01, 0.1], \'sgdclassifier__max_iter\': [1000, 2000, 3000], } # Perform grid search with cross-validation grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'f1\') grid_search.fit(X_train, y_train) # Best parameters found during grid search best_params = grid_search.best_params_ # Evaluate the model on the test set y_pred = grid_search.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) # Collect evaluation metrics metrics = { \\"accuracy\\": accuracy, \\"precision\\": precision, \\"recall\\": recall, \\"f1_score\\": f1, \\"best_params\\": best_params } # Print the results print(\\"Best Parameters: \\", best_params) print(\\"Accuracy: \\", accuracy) print(\\"Precision: \\", precision) print(\\"Recall: \\", recall) print(\\"F1 Score: \\", f1) return metrics if __name__ == \\"__main__\\": # Load data X, y = load_data(\'data.csv\') # Train and evaluate the model results = train_and_evaluate_model(X, y)"},{"question":"You are provided with a dataset file in CSV format containing information about the monthly sales of different products across various regions. The columns in the dataset are: 1. `Date`: The month and year of the sales record (YYYY-MM format). 2. `Product`: The product name. 3. `Region`: The region where the product was sold. 4. `Sales`: The number of units sold. Your task is to write a Python function using the `pandas` library that performs the following operations: 1. Load the dataset into a pandas DataFrame. 2. Compute the cumulative sum of sales for each product over time. 3. Generate two line plots on the same figure: - The cumulative sales over time for each product. - The total monthly sales across all products. 4. Customize the plot: - Display legends for the individual products and the total sales. - Format the x-axis to show the year and month properly. - Add a title to the plot. - Label the x-axis as \\"Date\\" and the y-axis as \\"Cumulative Sales\\". 5. Save the plot as `sales_plot.png`. # Function Signature ```python def visualize_sales(filepath: str) -> None: pass ``` # Example Below is an example of how the function might be used: ```python visualize_sales(\'monthly_sales.csv\') ``` # Constraints - Assume the CSV file is correctly formatted and contains no missing values. - Utilize the pandas and matplotlib libraries for data manipulation and visualization. - The function should produce the plot as output but doesn\'t need to return any value. # Performance Requirements - The function should handle datasets with at least 10,000 records efficiently.","solution":"import pandas as pd import matplotlib.pyplot as plt import matplotlib.dates as mdates def visualize_sales(filepath: str) -> None: # Load the dataset df = pd.read_csv(filepath) # Convert the \'Date\' column to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\'], format=\'%Y-%m\') # Compute the cumulative sum of sales for each product df[\'Cumulative_Sales\'] = df.groupby(\'Product\')[\'Sales\'].cumsum() # Compute the total monthly sales across all products total_monthly_sales = df.groupby(\'Date\')[\'Sales\'].sum().cumsum().reset_index() # Plotting plt.figure(figsize=(12, 8)) # Plot cumulative sales for each product for product in df[\'Product\'].unique(): product_df = df[df[\'Product\'] == product] plt.plot(product_df[\'Date\'], product_df[\'Cumulative_Sales\'], label=product) # Plot total monthly sales across all products plt.plot(total_monthly_sales[\'Date\'], total_monthly_sales[\'Sales\'], label=\'Total Sales\', linewidth=2, linestyle=\'--\') # Customize the plot plt.legend() plt.title(\\"Cumulative Sales Over Time\\") plt.xlabel(\\"Date\\") plt.ylabel(\\"Cumulative Sales\\") # Format x-axis plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\'%Y-%m\')) plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1)) plt.xticks(rotation=45, ha=\'right\') # Save the plot as \'sales_plot.png\' plt.tight_layout() plt.savefig(\'sales_plot.png\') # Show the plot plt.show()"},{"question":"# Question: Advanced Boxplot Customization with Seaborn You are tasked with analyzing data from the Titanic dataset using Seaborn. Your goal is to create a customized boxplot that highlights specific details about the passengers\' ages. Requirements: 1. **Load the Titanic dataset** using Seaborn (`sns.load_dataset(\\"titanic\\")`). 2. **Preprocess the data** to handle any missing values by dropping rows where the \'age\' column contains `NaN` values. 3. **Create a vertical boxplot** with the following characteristics: - **Grouped by** the passenger class (`class`) on the x-axis. - **Aged** (`age`) on the y-axis. - **Nested grouping by survival status** (`alive`) indicated by different colors. 4. **Customize the boxplot** by: - Drawing the boxes as line art without any filling. - Adding a small gap of `0.1` between the boxes. - Setting the whiskers to cover the range from the minimum to the maximum age. - Making the boxes narrower with a width of `0.5`. - Changing the color of the lines to `\'#00aaff\'` with a width of `1.0`. - Customizing the median line to be red and thicker (`linewidth=2`). Expected Input: None. (Assume that the dataset is loaded within the function.) Expected Output: A plot displaying the described customized boxplot. Function Signature: ```python def custom_titanic_boxplot(): # Function implementation here pass ``` Constraints: - Only use Seaborn and Matplotlib libraries. - Ensure the plot is well-labeled and readable. Performance: The function should run efficiently and complete within a reasonable time frame. ```python # Example solution implementation (students will write this part) import seaborn as sns import matplotlib.pyplot as plt def custom_titanic_boxplot(): sns.set_theme(style=\\"whitegrid\\") # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Drop rows where \'age\' is NaN titanic = titanic.dropna(subset=[\\"age\\"]) # Create a boxplot sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", dodge=True, width=0.5, boxprops=dict(facecolor=\'none\'), whiskerprops=dict(color=\'#00aaff\', linewidth=1.0), capprops=dict(color=\'#00aaff\', linewidth=1.0), medianprops=dict(color=\'r\', linewidth=2), flierprops=dict(markerfacecolor=\'#00aaff\', markeredgecolor=\'#00aaff\', markersize=5), notch=False ) plt.legend(title=\'Survival Status\') plt.title(\'Titanic Age Distribution by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show() # Uncomment to test # custom_titanic_boxplot() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_titanic_boxplot(): sns.set_theme(style=\\"whitegrid\\") # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Drop rows where \'age\' is NaN titanic = titanic.dropna(subset=[\\"age\\"]) # Create a boxplot sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", dodge=True, width=0.5, boxprops=dict(facecolor=\'none\'), whiskerprops=dict(color=\'#00aaff\', linewidth=1.0), capprops=dict(color=\'#00aaff\', linewidth=1.0), medianprops=dict(color=\'r\', linewidth=2), flierprops=dict(markerfacecolor=\'#00aaff\', markeredgecolor=\'#00aaff\', markersize=5), notch=False ) plt.legend(title=\'Survival Status\') plt.title(\'Titanic Age Distribution by Class and Survival Status\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.show()"},{"question":"**Context:** You are given two datasets, ( X ) and ( Y ), which contain multiple features. You need to find the relationship between these datasets using the Partial Least Squares (PLS) method for dimensionality reduction. Your task is to implement the PLS algorithm and use it to fit a regression model to predict the ( Y ) dataset from the ( X ) dataset. **Task:** 1. Implement a class `MyPLSCanonical` that mimics the behavior of the scikit-learn\'s `PLSCanonical` class. This class should: - Find the first left and right singular vectors of the cross-covariance matrix ( C = X^T Y ). - Project ( X ) and ( Y ) onto these singular vectors. - Regress ( X ) and ( Y ) onto their respective projections to find the loadings. - Deflate ( X ) and ( Y ) iteratively. - Approximate ( X ) and ( Y ) as sums of rank-1 matrices. 2. Using your `MyPLSCanonical` class, follow these steps: - Fit the model on the provided datasets ( X ) and ( Y ). - Transform ( X ) and ( Y ) into their lower-dimensional subspaces. - Predict the ( Y ) dataset from a new set of ( X ) data points using the fitted model. 3. Finally, provide a function `evaluate_model` that calculates the Mean Squared Error (MSE) of the prediction. **Input:** - `X_train` (numpy array of shape ( n times d )): The training data matrix with ( n ) samples and ( d ) features. - `Y_train` (numpy array of shape ( n times t )): The target data matrix with ( n ) samples and ( t ) features. - `X_test` (numpy array of shape ( m times d )): The test data matrix with ( m ) samples and ( d ) features. - `Y_test` (numpy array of shape ( m times t )): The expected target matrix for the test set with ( m ) samples and ( t ) features. **Output:** - Mean Squared Error (MSE) of the model\'s predictions on the test dataset. **Constraints:** - You may not use the `sklearn.cross_decomposition` module directly for implementing your `MyPLSCanonical` class. - Use Python libraries like `numpy` for matrix operations. - The number of components for PLS should be a parameter passed to the class constructor. **Example:** ```python import numpy as np class MyPLSCanonical: def __init__(self, n_components): self.n_components = n_components # Initialize other necessary attributes def fit(self, X, Y): # Implement the fitting process described in PLSCanonical def transform(self, X): # Implement the transformation process using fitted model def predict(self, X): # Implement the prediction process using fitted model and transformation def evaluate_model(pls_model, X_train, Y_train, X_test, Y_test): pls_model.fit(X_train, Y_train) Y_pred = pls_model.predict(X_test) mse = np.mean((Y_test - Y_pred) ** 2) return mse # Sample usage X_train = np.array([[...]]) Y_train = np.array([[...]]) X_test = np.array([[...]]) Y_test = np.array([[...]]) pls_model = MyPLSCanonical(n_components=2) mse = evaluate_model(pls_model, X_train, Y_train, X_test, Y_test) print(f\\"MSE: {mse}\\") ``` Implement the `MyPLSCanonical` class and the `evaluate_model` function according to the specification, and ensure that your solution is efficient and correct.","solution":"import numpy as np class MyPLSCanonical: def __init__(self, n_components): self.n_components = n_components self.x_loadings_ = None self.y_loadings_ = None self.x_weights_ = None self.y_weights_ = None self.x_scores_ = None self.y_scores_ = None self.coefs_ = None def fit(self, X, Y): n, dx = X.shape _, dy = Y.shape x_weights = np.zeros((dx, self.n_components)) y_weights = np.zeros((dy, self.n_components)) x_scores = np.zeros((n, self.n_components)) y_scores = np.zeros((n, self.n_components)) x_loadings = np.zeros((dx, self.n_components)) y_loadings = np.zeros((dy, self.n_components)) Xk = X.copy() Yk = Y.copy() for k in range(self.n_components): u, sigma, vt = np.linalg.svd(np.dot(Xk.T, Yk), full_matrices=False) x_weights[:, k] = u[:, 0] y_weights[:, k] = vt.T[:, 0] x_scores[:, k] = np.dot(Xk, x_weights[:, k]) y_scores[:, k] = np.dot(Yk, y_weights[:, k]) x_loadings[:, k] = np.dot(Xk.T, x_scores[:, k]) / np.dot(x_scores[:, k], x_scores[:, k]) y_loadings[:, k] = np.dot(Yk.T, y_scores[:, k]) / np.dot(y_scores[:, k], y_scores[:, k]) Xk -= np.outer(x_scores[:, k], x_loadings[:, k]) Yk -= np.outer(y_scores[:, k], y_loadings[:, k]) self.x_weights_ = x_weights self.y_weights_ = y_weights self.x_loadings_ = x_loadings self.y_loadings_ = y_loadings self.x_scores_ = x_scores self.y_scores_ = y_scores self.coefs_ = np.linalg.lstsq(x_scores, Y, rcond=None)[0] def transform(self, X): return np.dot(X, self.x_weights_) def predict(self, X): x_scores = self.transform(X) Y_pred = np.dot(x_scores, self.coefs_) return Y_pred def evaluate_model(pls_model, X_train, Y_train, X_test, Y_test): pls_model.fit(X_train, Y_train) Y_pred = pls_model.predict(X_test) mse = np.mean((Y_test - Y_pred) ** 2) return mse"},{"question":"# Descriptor Class to Track Attribute Changes **Objective**: Implement a descriptor class that tracks the number of times an attribute is accessed and modified. Problem Statement: Create a class `TrackedAttribute` that acts as a descriptor for tracking how many times a particular attribute of a class is accessed (read) and modified (written). **Requirements**: 1. The descriptor should track the number of times the attribute is read (`__get__`) and written (`__set__`). 2. Provide a method to retrieve the counts for each attribute. Implementation Details: 1. **Class `TrackedAttribute`**: - Implement the descriptor protocol methods `__get__`, `__set__`, and `__set_name__`. - Maintain counters for read and write operations for each tracked attribute. 2. **Usage Example**: - Define a class `Person` that uses the `TrackedAttribute` descriptor for its attributes `name` and `age`. - Implement methods to retrieve read and write counts for these attributes. 3. **Methods**: - `get_read_count(attribute_name)`: Return the number of times the attribute `attribute_name` was read. - `get_write_count(attribute_name)`: Return the number of times the attribute `attribute_name` was written. Example: ```python class TrackedAttribute: def __init__(self): self._value = None self._read_count = 0 self._write_count = 0 self._name = \'\' def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): self._read_count += 1 return self._value def __set__(self, instance, value): self._write_count += 1 self._value = value def get_read_count(self): return self._read_count def get_write_count(self): return self._write_count class Person: name = TrackedAttribute() age = TrackedAttribute() def __init__(self, name, age): self.name = name self.age = age @classmethod def get_read_count(cls, attr): return getattr(cls, attr).get_read_count() @classmethod def get_write_count(cls, attr): return getattr(cls, attr).get_write_count() # Example Usage: p = Person(\'Alice\', 30) print(p.name) # Access name (read) p.name = \'Bob\' # Modify name (write) print(p.age) # Access age (read) p.age = 31 # Modify age (write) print(p.name) # Access name (read) print(Person.get_read_count(\'name\')) # Expected: 2 print(Person.get_write_count(\'name\')) # Expected: 1 print(Person.get_read_count(\'age\')) # Expected: 1 print(Person.get_write_count(\'age\')) # Expected: 1 ``` **Constraints**: - The descriptor class should maintain accurate counts even when multiple instances of the class using it are created. - Attribute name capture should be automated based on the class attribute name it is assigned to. Please implement the `TrackedAttribute` class and ensure the `Person` class is using the descriptor correctly.","solution":"class TrackedAttribute: def __init__(self): self._name = None self._values = {} self._read_counts = {} self._write_counts = {} def __set_name__(self, owner, name): self._name = name def __get__(self, instance, cls): if instance is None: return self self._read_counts.setdefault(instance, 0) self._write_counts.setdefault(instance, 0) self._read_counts[instance] += 1 return self._values.get(instance, None) def __set__(self, instance, value): self._write_counts.setdefault(instance, 0) self._read_counts.setdefault(instance, 0) self._write_counts[instance] += 1 self._values[instance] = value def get_read_count(self, instance): return self._read_counts.get(instance, 0) def get_write_count(self, instance): return self._write_counts.get(instance, 0) class Person: name = TrackedAttribute() age = TrackedAttribute() def __init__(self, name, age): self.name = name self.age = age @classmethod def get_read_count(cls, instance, attr): return getattr(cls, attr).get_read_count(instance) @classmethod def get_write_count(cls, instance, attr): return getattr(cls, attr).get_write_count(instance)"},{"question":"# PyTorch Tensor Storage Manipulation Objective: Demonstrate your understanding of PyTorch tensor storages by creating and manipulating shared tensor storage. Implement functions to access, modify, and validate tensor storages. Problem Statement: 1. Create a tensor of size `(5, 2)` filled with random floating-point numbers between 0 and 1. 2. Clone the storage of this tensor. 3. Fill the cloned storage with zeros. 4. Create a new tensor using this zero-filled storage with the same dimensions as the original tensor. 5. Implement the following functions: - `create_tensor()`: - **Input:** None - **Output:** A tensor of size `(5, 2)` filled with random floats between 0 and 1. - `clone_and_fill_storage(tensor)`: - **Input:** A tensor of size `(5, 2)` - **Output:** A tensor of the same shape but with all elements set to zero. - `validate_storage(tensor1, tensor2)`: - **Input:** Two tensors. - **Output:** `True` if both tensors share the same storage, `False` otherwise. 6. Ensure your functions handle edge cases appropriately and are optimized for performance. Constraints: - You should not directly modify the tensor\'s data using assignment. Instead, use storage manipulation methods to achieve the results. - The functions should utilize PyTorch methods to handle tensor and storage operations. - The final tensor from `clone_and_fill_storage` should have the same shape and dtype as the original tensor. Example Usage: ```python import torch # Create original tensor original_tensor = create_tensor() # Clone and fill storage zero_filled_tensor = clone_and_fill_storage(original_tensor) # Validate if both tensors share the same storage shared_storage = validate_storage(original_tensor, zero_filled_tensor) print(\\"Original Tensor:\\") print(original_tensor) print(\\"Zero-filled Tensor:\\") print(zero_filled_tensor) print(\\"Do tensors share the same storage? \\", shared_storage) ``` Expected Output: - The original tensor will display random floating-point numbers. - The zero-filled tensor will display zeros. - The storage validation will return `False` since the storage was cloned, not shared.","solution":"import torch def create_tensor(): Returns a tensor of size (5, 2) filled with random floats between 0 and 1. return torch.rand(5, 2) def clone_and_fill_storage(tensor): Clones the storage of the given tensor, fills it with zeros, and returns a new tensor of the same shape using the zero-filled storage. Args: tensor (torch.Tensor): Original tensor of size (5, 2). Returns: torch.Tensor: A tensor of the same shape but with all elements set to zero. # Clone the storage of the original tensor cloned_storage = tensor.storage().clone() # Fill the cloned storage with zeros cloned_storage.fill_(0) # Create a new tensor from this zero-filled storage new_tensor = torch.Tensor(cloned_storage).view(tensor.size()) return new_tensor def validate_storage(tensor1, tensor2): Validates if the two given tensors share the same storage. Args: tensor1 (torch.Tensor): First tensor. tensor2 (torch.Tensor): Second tensor. Returns: bool: True if both tensors share the same storage, False otherwise. return tensor1.storage().data_ptr() == tensor2.storage().data_ptr()"},{"question":"# PyTorch MPS Backend Assessment Background You are given the task of implementing a deep learning model that leverages the high-performance MPS backend on macOS devices. Your task is to check the availability of the MPS backend, create Tensors on the MPS device, perform operations on these T tensors, and finally move a model to the MPS device for inference. Task Implement the following function in Python using PyTorch: ```python import torch def mps_operations_and_inference(model, tensor_size): This function performs the following operations: 1. Checks the availability of the MPS device. 2. Creates a Tensor of shape `tensor_size` filled with ones on the MPS device, if available. 3. Doubles the values of the Tensor. 4. Moves the given `model` to the MPS device, if available. 5. Performs a forward pass of the model using the Tensor as input and returns the output. 6. If MPS is not available, a suitable message indicating the lack of MPS support should be returned. Parameters: model (torch.nn.Module): A PyTorch model to be moved to the MPS device. tensor_size (tuple): The shape of the Tensor to be created and used as input for the model. Returns: torch.Tensor or str: The output of the model\'s forward pass if MPS is available, otherwise a string message. pass ``` Input: - `model`: An instance of a PyTorch neural network model (e.g., `torch.nn.Module`). - `tensor_size`: A tuple representing the shape of the Tensor to be created (e.g., (5,3) or (2,2,2)). Output: - If MPS backend is available, the function should return the output tensor of the model\'s forward pass. - If MPS backend is not available, the function should return the message `\\"MPS not available\\"`. Example Usage: ```python class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = torch.nn.Linear(10, 5) def forward(self, x): return self.linear(x) model = SimpleModel() output = mps_operations_and_inference(model, (1, 10)) if torch.backends.mps.is_available(): print(\\"Output Tensor from MPS:\\", output) else: print(output) # Should print \\"MPS not available\\" ``` Constraints: - Assume `model` is a valid PyTorch model instance with at least one layer. - Assume `tensor_size` is a valid shape tuple, and it has a size of at least 1 dimension. Notes: - Ensure that your solution handles the availability check of the MPS backend gracefully. - You may assume that the necessary libraries (PyTorch) are imported and available for use.","solution":"import torch def mps_operations_and_inference(model, tensor_size): This function performs the following operations: 1. Checks the availability of the MPS device. 2. Creates a Tensor of shape `tensor_size` filled with ones on the MPS device, if available. 3. Doubles the values of the Tensor. 4. Moves the given `model` to the MPS device, if available. 5. Performs a forward pass of the model using the Tensor as input and returns the output. 6. If MPS is not available, a suitable message indicating the lack of MPS support should be returned. Parameters: model (torch.nn.Module): A PyTorch model to be moved to the MPS device. tensor_size (tuple): The shape of the Tensor to be created and used as input for the model. Returns: torch.Tensor or str: The output of the model\'s forward pass if MPS is available, otherwise a string message. if torch.backends.mps.is_available() and torch.device(\'mps\'): device = torch.device(\'mps\') tensor = torch.ones(tensor_size, device=device) tensor = tensor * 2 # Doubling the values of the tensor model.to(device) output = model(tensor) return output else: return \\"MPS not available\\""},{"question":"# Advanced Python Coding Assessment Objective: To assess your understanding of Python\'s `collections.ChainMap` and how to work with nested mappings effectively. Problem Statement: You are provided with several dictionaries representing user settings and configurations from various sources. Your task is to create a class `ConfigurationManager` that leverages `collections.ChainMap` to manage these dictionaries. The `ConfigurationManager` should support operations to search, update, insert, and delete configuration settings efficiently, keeping the merging of dictionaries consistent with the properties explained in the `ChainMap` documentation. Class Definition: ```python class ConfigurationManager: def __init__(self, *configs): Initialize the ConfigurationManager with multiple configuration dictionaries. :param configs: Multiple dictionaries to be merged, with the first dictionary having the highest precedence. pass def get(self, key): Get the value associated with the key from the merged configuration. :param key: The key to search for. :return: The value associated with the key. :raises KeyError: If the key is not found in any of the dictionaries. pass def set(self, key, value): Set the value for the key in the first dictionary (ie. highest precedence). :param key: The key for the value to be set. :param value: The value to be set. pass def delete(self, key): Delete the key from the first dictionary (ie. highest precedence) if it exists. :param key: The key to be deleted. :raises KeyError: If the key is not found in the first dictionary. pass def new_child(self, m=None, **kwargs): Create a new layer in the ChainMap with the provided dictionary or a dictionary formed from keyword arguments, returning a new ConfigurationManager instance. :param m: Dictionary to be added as a new child. :param kwargs: Key-value pairs to be added. :return: A new ConfigurationManager instance with the additional layer. pass def get_parents(self): Get a new ConfigurationManager instance with all mappings except the first one. :return: A new ConfigurationManager instance representing the parents. pass ``` Requirements: 1. Implement the `ConfigurationManager` class with all methods defined above. 2. The constructor should accept multiple dictionaries and initialize a `ChainMap` accordingly. 3. The `get` method should search the `ChainMap` and return the value of the first occurrence of the key. 4. The `set` method should set the value in the first dictionary of the `ChainMap`. 5. The `delete` method should remove the key from the first dictionary only. 6. The `new_child` method should add a new dictionary to the front of the `ChainMap` and return a new instance of `ConfigurationManager`. 7. The `get_parents` method should return a new instance of `ConfigurationManager` that represents the parent mappings (all but the first). Input Format: - Multiple dictionaries for the initialization. - `key`, `value` pairs for methods like `get`, `set`, and `delete`. Output Format: - For `get`, the value associated with the key. - For `new_child` and `get_parents`, return a new instance of `ConfigurationManager`. - `set` and `delete` methods do not return anything but modify the underlying data structure. Example Usage: ```python configs = [{\'user\': \'admin\', \'theme\': \'dark\'}, {\'theme\': \'light\', \'version\': \'1.0\'}] config_mgr = ConfigurationManager(*configs) print(config_mgr.get(\'theme\')) # Output: \'dark\' print(config_mgr.get(\'version\')) # Output: \'1.0\' config_mgr.set(\'user\', \'guest\') print(config_mgr.get(\'user\')) # Output: \'guest\' config_mgr.delete(\'theme\') print(config_mgr.get(\'theme\')) # Output: \'light\' new_config_mgr = config_mgr.new_child({\'new_feature\': \'enabled\'}) print(new_config_mgr.get(\'new_feature\')) # Output: \'enabled\' parent_config_mgr = config_mgr.get_parents() print(parent_config_mgr.get(\'user\')) # Output: \'guest\' print(parent_config_mgr.get(\'theme\')) # Output: \'light\' ``` Constraints: 1. Assume keys are strings and values are strings or integers. 2. Provide error handling for cases where keys are not found. Goal: Implement the `ConfigurationManager` class correctly, ensuring that the operations align with the behavior of `collections.ChainMap`. Write clean, efficient, and well-documented code.","solution":"from collections import ChainMap class ConfigurationManager: def __init__(self, *configs): Initialize the ConfigurationManager with multiple configuration dictionaries. :param configs: Multiple dictionaries to be merged, with the first dictionary having the highest precedence. self.chain_map = ChainMap(*configs) def get(self, key): Get the value associated with the key from the merged configuration. :param key: The key to search for. :return: The value associated with the key. :raises KeyError: If the key is not found in any of the dictionaries. if key in self.chain_map: return self.chain_map[key] else: raise KeyError(f\\"Key \'{key}\' not found.\\") def set(self, key, value): Set the value for the key in the first dictionary (ie. highest precedence). :param key: The key for the value to be set. :param value: The value to be set. self.chain_map.maps[0][key] = value def delete(self, key): Delete the key from the first dictionary (ie. highest precedence) if it exists. :param key: The key to be deleted. :raises KeyError: If the key is not found in the first dictionary. if key in self.chain_map.maps[0]: del self.chain_map.maps[0][key] else: raise KeyError(f\\"Key \'{key}\' not found in the first dictionary.\\") def new_child(self, m=None, **kwargs): Create a new layer in the ChainMap with the provided dictionary or a dictionary formed from keyword arguments, returning a new ConfigurationManager instance. :param m: Dictionary to be added as a new child. :param kwargs: Key-value pairs to be added. :return: A new ConfigurationManager instance with the additional layer. if m is not None: new_chain_map = self.chain_map.new_child(m) else: new_chain_map = self.chain_map.new_child(kwargs) return ConfigurationManager(*new_chain_map.maps) def get_parents(self): Get a new ConfigurationManager instance with all mappings except the first one. :return: A new ConfigurationManager instance representing the parents. parents_chain_map = self.chain_map.parents return ConfigurationManager(*parents_chain_map.maps)"},{"question":"Data Analysis with Pandas Objective Demonstrate your proficiency in working with Pandas DataFrame by performing various operations involving creation, modification, and manipulation of data. Problem Statement You are provided with a dataset representing sales data for a hypothetical company. The dataset contains information regarding: - The sales representative\'s name. - The region they cover. - The total sales made by them in each quarter of a year (Q1, Q2, Q3, Q4). The data is as follows: ```python data = [ {\\"name\\": \\"John\\", \\"region\\": \\"East\\", \\"Q1\\": 15000, \\"Q2\\": 20000, \\"Q3\\": 18000, \\"Q4\\": 22000}, {\\"name\\": \\"Jane\\", \\"region\\": \\"West\\", \\"Q1\\": 17000, \\"Q2\\": 16000, \\"Q3\\": 17500, \\"Q4\\": 21000}, {\\"name\\": \\"Emily\\", \\"region\\": \\"South\\", \\"Q1\\": 13000, \\"Q2\\": 22000, \\"Q3\\": 21000, \\"Q4\\": 19000}, {\\"name\\": \\"Mike\\", \\"region\\": \\"North\\", \\"Q1\\": 18000, \\"Q2\\": 15000, \\"Q3\\": 16000, \\"Q4\\": 23000}, ] ``` Using this data, perform the following tasks: Tasks 1. **Create the DataFrame:** - Create a DataFrame from the data provided above. - Ensure that the DataFrame has columns: `\'name\'`, `\'region\'`, `\'Q1\'`, `\'Q2\'`, `\'Q3\'`, `\'Q4\'`. 2. **Calculate Total Sales:** - Add a new column `\'Total\'` that contains the sum of sales from all quarters (`Q1`, `Q2`, `Q3`, `Q4`) for each representative. 3. **Filter Representatives:** - Create a new DataFrame that filters out the representatives who have total sales less than 70,000. 4. **Sort the DataFrame:** - Sort the DataFrame in descending order of the `\'Total\'` sales. 5. **Calculate Regional Sales:** - Create a new Series named `regional_sales` that contains the total sales for each region. Index of the Series should be region names. 6. **Add Commission Column:** - Add a new column `\'Commission\'` that contains the calculated commission for each representative. Assume that the commission rate is 5% of the total sales. 7. **Output the Final DataFrame:** - Print the final DataFrame after performing all the operations. Input - The dataset is provided within the code itself. Expected Output - The final DataFrame after all transformations and calculations. - The `regional_sales` series. Constraints - Ensure that all operations are performed using Pandas functionalities described in the provided documentation. - Handle any potential missing data appropriately. Example ```python import pandas as pd data = [ {\\"name\\": \\"John\\", \\"region\\": \\"East\\", \\"Q1\\": 15000, \\"Q2\\": 20000, \\"Q3\\": 18000, \\"Q4\\": 22000}, {\\"name\\": \\"Jane\\", \\"region\\": \\"West\\", \\"Q1\\": 17000, \\"Q2\\": 16000, \\"Q3\\": 17500, \\"Q4\\": 21000}, {\\"name\\": \\"Emily\\", \\"region\\": \\"South\\", \\"Q1\\": 13000, \\"Q2\\": 22000, \\"Q3\\": 21000, \\"Q4\\": 19000}, {\\"name\\": \\"Mike\\", \\"region\\": \\"North\\", \\"Q1\\": 18000, \\"Q2\\": 15000, \\"Q3\\": 16000, \\"Q4\\": 23000}, ] # Create DataFrame df = pd.DataFrame(data) # Calculate Total Sales df[\'Total\'] = df[[\'Q1\', \'Q2\', \'Q3\', \'Q4\']].sum(axis=1) # Filter Representatives with total sales >= 70000 filtered_df = df[df[\'Total\'] >= 70000] # Sort DataFrame by Total sales sorted_df = filtered_df.sort_values(by=\'Total\', ascending=False) # Calculate Regional Sales regional_sales = sorted_df.groupby(\'region\')[\'Total\'].sum() # Add Commission Column sorted_df[\'Commission\'] = sorted_df[\'Total\'] * 0.05 # Output the final DataFrame print(sorted_df) # Output the regional sales print(regional_sales) ```","solution":"import pandas as pd def analyze_sales_data(data): # Create DataFrame df = pd.DataFrame(data) # Calculate Total Sales df[\'Total\'] = df[[\'Q1\', \'Q2\', \'Q3\', \'Q4\']].sum(axis=1) # Filter Representatives with total sales >= 70000 filtered_df = df[df[\'Total\'] >= 70000] # Sort DataFrame by Total sales sorted_df = filtered_df.sort_values(by=\'Total\', ascending=False) # Calculate Regional Sales regional_sales = sorted_df.groupby(\'region\')[\'Total\'].sum() # Add Commission Column sorted_df[\'Commission\'] = sorted_df[\'Total\'] * 0.05 # Returning the final DataFrame and regional sales series return sorted_df, regional_sales # Sample data data = [ {\\"name\\": \\"John\\", \\"region\\": \\"East\\", \\"Q1\\": 15000, \\"Q2\\": 20000, \\"Q3\\": 18000, \\"Q4\\": 22000}, {\\"name\\": \\"Jane\\", \\"region\\": \\"West\\", \\"Q1\\": 17000, \\"Q2\\": 16000, \\"Q3\\": 17500, \\"Q4\\": 21000}, {\\"name\\": \\"Emily\\", \\"region\\": \\"South\\", \\"Q1\\": 13000, \\"Q2\\": 22000, \\"Q3\\": 21000, \\"Q4\\": 19000}, {\\"name\\": \\"Mike\\", \\"region\\": \\"North\\", \\"Q1\\": 18000, \\"Q2\\": 15000, \\"Q3\\": 16000, \\"Q4\\": 23000}, ] # Run function for the sample data final_df, regional_sales = analyze_sales_data(data) # Printing results for verification purposes print(final_df) print(regional_sales)"},{"question":"Objective Design an asynchronous file server using `asyncio` that monitors a specified directory and serves files to clients upon request over TCP. Problem Statement You need to implement a simple asynchronous file server in Python 3.10 using the `asyncio` package. The server will accept incoming TCP connections, interpret client requests, and send the requested file back to the client. The server should handle multiple clients simultaneously. Requirements 1. **Directory Monitoring**: The server should monitor a specified directory containing the files it can serve. 2. **Handling Client Requests**: - The client will send the filename as a string. - If the file exists in the monitored directory, the server should read the file and send its contents back to the client. - If the file does not exist, the server should respond with a \\"File not found\\" message. 3. **Concurrency**: The server must handle multiple client connections concurrently. 4. **Graceful Shutdown**: The server should shut down gracefully, ensuring any ongoing file transfers are completed before stopping. Expected Input/Output - **Input**: Filename from the client (string). - **Output**: Contents of the requested file or \\"File not found\\" message. Constraints - The server should be implemented using the low-level event loop APIs (`loop.create_server()`, handling callbacks, etc.) from the `asyncio` package. - The server should manage resources efficiently and avoid blocking operations. Performance Requirements The server should be able to handle at least 10 concurrent client connections efficiently without significant delay. Example Usage ```python import asyncio SERVER_DIRECTORY = \'./files\' async def handle_client(reader, writer): request = await reader.read(100) filename = request.decode().strip() try: with open(f\\"{SERVER_DIRECTORY}/{filename}\\", \\"rb\\") as f: content = f.read() writer.write(content) except FileNotFoundError: writer.write(b\\"File not found\\") await writer.drain() writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Submission Submit the complete implementation of the file server program as described above. Make sure to test it to ensure it can handle multiple client connections concurrently and respond correctly to requests.","solution":"import asyncio import os SERVER_DIRECTORY = \'./files\' async def handle_client(reader, writer): try: request = await reader.read(100) filename = request.decode().strip() file_path = os.path.join(SERVER_DIRECTORY, filename) if os.path.exists(file_path): with open(file_path, \'rb\') as f: content = f.read() writer.write(content) else: writer.write(b\\"File not found\\") await writer.drain() except Exception as e: writer.write(f\\"Error: {str(e)}\\".encode()) await writer.drain() finally: writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) print(f\'Serving on {server.sockets[0].getsockname()}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Objective:** Implement a Python C extension module that provides a function to count occurrences of a specific attribute in a list of Python objects. This will test your understanding of object attribute manipulation and iteration in Python C API. **Requirements:** 1. **Function Name:** `count_attribute_occurrences` 2. **Input:** - `PyObject *obj_list`: A Python list of objects. - `const char *attr_name`: A string representing the name of the attribute to check. 3. **Output:** - Returns the count of objects that have the specified attribute. 4. **Constraints:** - If `obj_list` is not a list, raise a `TypeError`. - If `attr_name` is not a string, raise a `TypeError`. - The function should handle reference counting correctly and ensure there are no memory leaks. 5. **Performance Requirements:** - The solution should make a single scan of the list and compute the count. **Function Prototype:** ```c static PyObject* count_attribute_occurrences(PyObject *self, PyObject *args); ``` **Details:** - Utilize `PyObject_HasAttrString` to check if an object in the list has the specified attribute. - The function should return a Python integer object containing the count of objects with the specified attribute. - Use the Python C API\'s error-handling mechanisms to provide appropriate exceptions for invalid inputs. **Example Usage:** For a Python list of objects where some objects have an attribute `color`: ```python import example # The name of the generated Python extension module class Obj: def __init__(self, color=None): self.color = color obj_list = [Obj(\'red\'), Obj(), Obj(\'blue\'), Obj(\'green\'), Obj()] count = example.count_attribute_occurrences(obj_list, \'color\') print(count) # Output should be 3 ``` **Hint:** - Use `PyObject_Size` to determine the size of the list. - Iterate through the list using `PyList_GetItem`. - Check for the attribute using `PyObject_HasAttrString`. - Increment the count as needed and return the result. - Ensure proper error handling for type checks and other potential issues. Implementing this function correctly will demonstrate a strong understanding of Python\'s C API for object manipulation.","solution":"def count_attribute_occurrences(obj_list, attr_name): Count occurrences of a specific attribute in a list of Python objects. :param obj_list: list of objects :param attr_name: the name of the attribute to check :return: count of objects having the specified attribute :raises TypeError: if obj_list is not a list or attr_name is not a string if not isinstance(obj_list, list): raise TypeError(\\"obj_list should be a list of objects\\") if not isinstance(attr_name, str): raise TypeError(\\"attr_name should be a string\\") count = 0 for obj in obj_list: if hasattr(obj, attr_name): count += 1 return count"},{"question":"**Objective:** To assess your understanding of the Python `codecs` package and its functionalities related to custom encoding, decoding, error handling, and incremental processing. **Problem:** You are provided with a task to implement a text processing utility that dynamically handles multiple text encodings. The utility should be capable of encoding a given string into a specified encoding format and then decoding it back to a string. Additionally, it should support custom error handling and incremental processing. **Requirements:** 1. Function to encode a given string with a specified encoding. 2. Function to decode a given byte sequence back to a string with the specified encoding. 3. Implement custom error handling by registering a custom error handler. 4. Implement incremental encoding and decoding for processing large text data in chunks. **Function Signatures:** ```python def custom_encode(input_text: str, encoding: str, errors: str) -> bytes: Encode the input text using the specified encoding and custom error handling. Parameters: - input_text (str): The text to be encoded. - encoding (str): The encoding format to use. - errors (str): The error handling scheme. Returns: - bytes: The encoded byte sequence. def custom_decode(encoded_data: bytes, encoding: str, errors: str) -> str: Decode the encoded byte sequence using the specified encoding and custom error handling. Parameters: - encoded_data (bytes): The byte sequence to be decoded. - encoding (str): The encoding format to use. - errors (str): The error handling scheme. Returns: - str: The decoded text. def register_custom_error_handler(): Register a custom error handler that replaces unencodable characters with their hexadecimal code. def incremental_process(input_text: str, encoding: str, chunk_size: int): Incrementally encode and decode a large text data in chunks. Parameters: - input_text (str): The text to be processed. - encoding (str): The encoding format to use. - chunk_size (int): The size of each chunk to be processed incrementally. Returns: - list: A list containing each chunk of decoded text. ``` **Constraints:** 1. Avoid using built-in functions on string manipulation that subvert encoding/decoding (like `encode()`, `decode()`) outside the `codecs` package. 2. Handle the errors gracefully using the custom error handler. 3. Ensure that the incremental processing function efficiently handles large text data. **Example Usage:** ```python # Register the custom error handler register_custom_error_handler() # Encode text with custom error handling encoded_data = custom_encode(\\"This is a test 😃\\", \\"ascii\\", \\"hex_replace\\") print(encoded_data) # b\'This is a test U0001f603\' # Decode the encoded data back to text decoded_text = custom_decode(encoded_data, \\"ascii\\", \\"hex_replace\\") print(decoded_text) # \'This is a test U0001f603\' # Incrementally process large text data in chunks result_chunks = incremental_process(\\"This is a larger test text 😃\\" * 1000, \\"utf-8\\", 100) for chunk in result_chunks: print(chunk) ``` In this task, you will demonstrate your ability to work with the Python `codecs` package, handling various encodings, implementing custom error handlers, and performing incremental text processing. **Good Luck!**","solution":"import codecs def custom_encode(input_text: str, encoding: str, errors: str) -> bytes: Encode the input text using the specified encoding and custom error handling. Parameters: - input_text (str): The text to be encoded. - encoding (str): The encoding format to use. - errors (str): The error handling scheme. Returns: - bytes: The encoded byte sequence. return codecs.encode(input_text, encoding, errors) def custom_decode(encoded_data: bytes, encoding: str, errors: str) -> str: Decode the encoded byte sequence using the specified encoding and custom error handling. Parameters: - encoded_data (bytes): The byte sequence to be decoded. - encoding (str): The encoding format to use. - errors (str): The error handling scheme. Returns: - str: The decoded text. return codecs.decode(encoded_data, encoding, errors) def hex_replace(exc): Custom error handler that replaces unencodable characters with their hexadecimal code. if isinstance(exc, (UnicodeEncodeError, UnicodeDecodeError)): return (\'U%08x\' % ord(exc.object[exc.start]), exc.end) else: raise TypeError(\'Cannot handle {}: {}\'.format(exc.__class__.__name__, exc)) def register_custom_error_handler(): Register a custom error handler that replaces unencodable characters with their hexadecimal code. codecs.register_error(\'hex_replace\', hex_replace) def incremental_process(input_text: str, encoding: str, chunk_size: int): Incrementally encode and decode a large text data in chunks. Parameters: - input_text (str): The text to be processed. - encoding (str): The encoding format to use. - chunk_size (int): The size of each chunk to be processed incrementally. Returns: - list: A list containing each chunk of decoded text. encoder = codecs.getincrementalencoder(encoding)() decoder = codecs.getincrementaldecoder(encoding)() chunks = [] encoded_data = b\\"\\" for i in range(0, len(input_text), chunk_size): chunk = input_text[i:i+chunk_size] encoded_data += encoder.encode(chunk) decoded_data = \\"\\" for i in range(0, len(encoded_data), chunk_size): chunk = encoded_data[i:i+chunk_size] decoded_data += decoder.decode(chunk) chunks.append(decoded_data) return chunks"},{"question":"Objective: To assess your understanding of Python\'s `configparser` module for handling configuration (INI) files. Problem Statement: You are tasked to write a function `process_config(file_path)`, which reads an INI configuration file, modifies certain entries according to specific rules, and writes the modified configuration back to a new file. Specification: 1. **Input**: - `file_path` (str): A string representing the file path to the INI configuration file. 2. **Output**: - Returns the name of the new modified INI file (str). 3. **Rules for modification**: - All sections should be kept as they are. - The section `[DEFAULT]` should be added to the file with the following (if not already present): ```ini [DEFAULT] host = localhost port = 8080 ``` - If a section has an option `enable`, its value should be converted to uppercase (`true` -> `TRUE`, `false` -> `FALSE`). - If a section has an option `path`, prepend `/data` to the existing value. (e.g., `path = /files` -> `path = /data/files`). 4. **Constraints and Assumptions**: - You can assume the file provided is a proper INI file. - The file should be modified and saved as `modified_config.ini` in the same directory as the original file. - Use the `configparser` module to handle all read and write operations. Example: Suppose the file at `config.ini` has the following content: ``` [database] user = dbuser password = secret enable = true path = /dbfiles [server] address = 192.168.1.1 enable = false ``` After running `process_config(\\"config.ini\\")`, the `modified_config.ini` should contain: ``` [DEFAULT] host = localhost port = 8080 [database] user = dbuser password = secret enable = TRUE path = /data/dbfiles [server] address = 192.168.1.1 enable = FALSE ``` Function Signature: ```python def process_config(file_path: str) -> str: pass ``` Important: - Ensure to thoroughly test your function to handle various scenario cases. - Write clean, modular, and well-documented code.","solution":"import configparser def process_config(file_path: str) -> str: config = configparser.ConfigParser() config.read(file_path) if not config.has_section(\'DEFAULT\'): config[\'DEFAULT\'] = {} if \'host\' not in config[\'DEFAULT\']: config[\'DEFAULT\'][\'host\'] = \'localhost\' if \'port\' not in config[\'DEFAULT\']: config[\'DEFAULT\'][\'port\'] = \'8080\' for section in config.sections(): if config.has_option(section, \'enable\'): config[section][\'enable\'] = config[section][\'enable\'].upper() if config.has_option(section, \'path\'): config[section][\'path\'] = f\\"/data{config[section][\'path\']}\\" modified_file_path = \\"modified_config.ini\\" with open(modified_file_path, \'w\') as configfile: config.write(configfile) return modified_file_path"},{"question":"# Kernel Density Estimation in scikit-learn Background Kernel Density Estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. In scikit-learn, the `KernelDensity` estimator can be used to perform KDE with various types of kernels and bandwidth parameters. For this question, you will implement KDE using scikit-learn, compare the results of different kernels and bandwidth parameters, and provide an analysis of your findings. Problem Statement You are provided with a dataset containing 1D data points drawn from a bimodal distribution. Your task is to implement KDE using scikit-learn, compare the density estimates obtained with different kernels, and analyze the effects of varying the bandwidth parameter. Input - A 1D numpy array `data` containing the data points drawn from a bimodal distribution. - A list of kernel types `kernels` to be used for KDE. The list can contain the following kernel types: `[\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\']` - A list of bandwidth values `bandwidths` to be used for KDE. Output - Dictionary containing density estimates for each combination of kernel and bandwidth. The keys of the dictionary should be tuples of the form `(kernel, bandwidth)`, and the values should be the corresponding density estimates. Constraints - For simplicity, you can assume the input data contains at least 50 data points. - Kernels and bandwidths lists will have at least one element each. - Use scikit-learn\'s `KernelDensity` for KDE implementation. Performance Requirements - The implementation should efficiently compute the density estimates using scikit-learn\'s `KernelDensity`. - The output should provide clear comparisons of the density estimates for different kernel and bandwidth combinations. Example ```python import numpy as np # Example input data data = np.array([-3.0, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 3.0]) kernels = [\'gaussian\', \'tophat\'] bandwidths = [0.1, 0.5] # Expected function call result = compute_kde(data, kernels, bandwidths) # Example output format # result = { # (\'gaussian\', 0.1): array([...]), # (\'gaussian\', 0.5): array([...]), # (\'tophat\', 0.1): array([...]), # (\'tophat\', 0.5): array([...]) # } ``` Your Task Implement the function `compute_kde(data, kernels, bandwidths)` that takes the input data, different kernel types, and bandwidth values, and returns a dictionary of density estimates for each combination of kernel and bandwidth. Additionally, analyze the results and explain the effects of different kernels and bandwidth parameters on the density estimates.","solution":"from sklearn.neighbors import KernelDensity import numpy as np def compute_kde(data, kernels, bandwidths): Computes Kernel Density Estimation for given data, kernels, and bandwidths. :param data: 1D numpy array containing data points drawn from a bimodal distribution :param kernels: list of kernel types to be used for KDE :param bandwidths: list of bandwidth values to be used for KDE :return: dictionary containing density estimates for each combination of kernel and bandwidth kde_results = {} # Convert the data to a 2D array as required by KernelDensity data = data[:, np.newaxis] for kernel in kernels: for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) log_density = kde.score_samples(data) density = np.exp(log_density) kde_results[(kernel, bandwidth)] = density return kde_results"},{"question":"# Advanced Python Coding Assessment **Objective**: Assess your understanding and capability to implement custom serialization and deserialization logic using Python\'s `pickle` module. **Problem Statement**: You are tasked to implement a custom serialization system for a hypothetical student database. The database contains `Student` objects with the following properties: - `id` (int): Unique identifier for each student. - `name` (str): Name of the student. - `courses` (list of str): List of courses the student is enrolled in. - `grades` (dict): Dictionary mapping course names to grades. - `notes` (Optional[str]): Additional notes about the student, which should not be serialized if it\'s empty. The database should handle the following requirements: 1. **Custom Serialization**: Implement custom serialization such that: - If `notes` is an empty string, it should not be included in the serialized data. - The `grades` dictionary should only serialize subjects where the grade is not `None`. 2. **Custom Deserialization**: Implement custom deserialization correctly reconstructing the object with the omitted fields during serialization. Implement the following two classes: 1. **`Student` Class**: - Constructor to initialize the properties. - Methods to get and set each property. - Override methods for `__reduce__()`, `__getstate__()`, and `__setstate__()` to manage custom serialization and deserialization processes. 2. **`StudentDatabase` Class**: - Manage a collection of `Student` objects. - Methods to add and fetch students by `id`. - Methods to serialize the student data to file and deserialize it back. - Implement pickling and unpickling with custom behaviors defined above. **Constraints**: - Use Python\'s `pickle` module. - The `StudentDatabase` should keep track of students uniquely by their `id`. **Input/Output Format**: - **Input**: Methods to add and fetch students, and serialize/deserialize the database. - **Output**: Serialized data written to a file, and the ability to load and reconstruct the database from the file. **Performance Requirements**: - Efficient handling of serialization and deserialization processes. - Custom handling should affect only specified fields (`notes` and `grades`). **Example**: ```python import pickle class Student: def __init__(self, id, name, courses, grades, notes=\\"\\"): self.id = id self.name = name self.courses = courses self.grades = grades self.notes = notes def __getstate__(self): state = self.__dict__.copy() if state[\'notes\'] == \\"\\": del state[\'notes\'] state[\'grades\'] = {k: v for k, v in state[\'grades\'].items() if v is not None} return state def __setstate__(self, state): self.__dict__.update(state) if \'notes\' not in state: self.notes = \\"\\" def __reduce__(self): return (self.__class__, (self.id, self.name, self.courses, self.grades, self.notes), self.__getstate__()) class StudentDatabase: def __init__(self): self.students = {} def add_student(self, student): self.students[student.id] = student def get_student(self, id): return self.students.get(id, None) def save_to_file(self, filename): with open(filename, \'wb\') as f: pickle.dump(self.students, f) def load_from_file(self, filename): with open(filename, \'rb\') as f: self.students = pickle.load(f) # Example usage db = StudentDatabase() student1 = Student(1, \\"Alice\\", [\\"Math\\", \\"Science\\"], {\\"Math\\": \\"A\\", \\"Science\\": None}, \\"\\") student2 = Student(2, \\"Bob\\", [\\"Math\\", \\"History\\"], {\\"Math\\": \\"B\\", \\"History\\": \\"A\\"}, \\"Needs extra help in Math\\") db.add_student(student1) db.add_student(student2) db.save_to_file(\\"students.pkl\\") # Load the database from file db_loaded = StudentDatabase() db_loaded.load_from_file(\\"students.pkl\\") print(db_loaded.get_student(1).name) # Output: Alice print(db_loaded.get_student(2).notes) # Output: Needs extra help in Math ``` Implement the `Student` and `StudentDatabase` classes as described.","solution":"import pickle class Student: def __init__(self, id, name, courses, grades, notes=\\"\\"): self.id = id self.name = name self.courses = courses self.grades = grades self.notes = notes def __getstate__(self): state = self.__dict__.copy() if state[\'notes\'] == \\"\\": del state[\'notes\'] state[\'grades\'] = {k: v for k, v in state[\'grades\'].items() if v is not None} return state def __setstate__(self, state): self.__dict__.update(state) if \'notes\' not in state: self.notes = \\"\\" def __reduce__(self): return (self.__class__, (self.id, self.name, self.courses, self.grades, self.notes), self.__getstate__()) class StudentDatabase: def __init__(self): self.students = {} def add_student(self, student): self.students[student.id] = student def get_student(self, id): return self.students.get(id, None) def save_to_file(self, filename): with open(filename, \'wb\') as f: pickle.dump(self.students, f) def load_from_file(self, filename): with open(filename, \'rb\') as f: self.students = pickle.load(f)"},{"question":"Analyzing Stock Prices using Pandas Window Functions You are provided with a CSV file containing historical stock price data for a single stock. The file has the following columns: - `Date`: The date of the recorded stock price. - `Close`: The closing price of the stock on that date. Your task is to implement a function `analyze_stock_prices(file_path)` that reads this CSV file into a pandas DataFrame and performs the following analyses: 1. **Rolling Window Analysis**: - Compute the 7-day rolling mean of the closing prices. - Compute the 7-day rolling standard deviation of the closing prices. 2. **Expanding Window Analysis**: - Compute the expanding mean of the closing prices. - Compute the expanding maximum of the closing prices. 3. **Exponentially Weighted Moving Average (EWMA) Analysis**: - Compute the EWMA of the closing prices with a span of 10. Your function should return a DataFrame containing the original date and close columns, along with the computed columns named as follows: - `RollingMean_7` - `RollingStd_7` - `ExpandingMean` - `ExpandingMax` - `EWMA_10` # Input: - A string `file_path` representing the path to the CSV file containing the stock price data. # Output: - A pandas DataFrame with the original `Date` and `Close` columns, and the computed columns as specified above. # Constraints: - You may assume the input file is correctly formatted and contains the necessary columns. - The `Date` column should be treated as a datetime object in pandas. Example Usage: ```python import pandas as pd def analyze_stock_prices(file_path: str) -> pd.DataFrame: # Implement your solution here pass # Example output_df = analyze_stock_prices(\'path/to/stock_prices.csv\') print(output_df.head()) ``` Ensure your function handles the time series data efficiently and provides accurate results.","solution":"import pandas as pd def analyze_stock_prices(file_path: str) -> pd.DataFrame: # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path, parse_dates=[\'Date\']) # Set the Date column as the index for time series analysis df.set_index(\'Date\', inplace=True) # Compute the 7-day rolling mean and standard deviation df[\'RollingMean_7\'] = df[\'Close\'].rolling(window=7).mean() df[\'RollingStd_7\'] = df[\'Close\'].rolling(window=7).std() # Compute the expanding mean and maximum df[\'ExpandingMean\'] = df[\'Close\'].expanding().mean() df[\'ExpandingMax\'] = df[\'Close\'].expanding().max() # Compute the EWMA with a span of 10 days df[\'EWMA_10\'] = df[\'Close\'].ewm(span=10, adjust=False).mean() # Reset the index to have the Date as a column again df.reset_index(inplace=True) # Return the DataFrame with the required columns return df[[\'Date\', \'Close\', \'RollingMean_7\', \'RollingStd_7\', \'ExpandingMean\', \'ExpandingMax\', \'EWMA_10\']]"},{"question":"Objective You are required to write a Python script that demonstrates your understanding of signal handling using the Python `signal` module. You will implement a custom signal handler that responds to specific signals and performs designated actions. Task 1. **Signal Handler Setup**: Write a function named `setup_signal_handlers()`. This function should: - Set up a handler for `SIGINT` that prints a message \\"SIGINT received, graceful shutdown.\\" and terminates the program. - Set up a handler for `SIGALRM` that prints a message \\"Alarm signal received.\\" without terminating the program. 2. **Main Execution**: Write a main function named `main()`, which: - Calls `setup_signal_handlers()` to set up the handlers. - Uses `signal.alarm(time)` to schedule an alarm signal to be delivered after 10 seconds. - Enters an infinite loop, simulating a long-running process where every second a message \\"Running...\\" is printed. 3. **Graceful Shutdown**: Ensure that when `SIGINT` is received, the program prints the message and terminates gracefully. Constraints - The script should only use standard Python libraries. - The infinite loop should correctly handle signals without causing the program to hang. Expected Input and Output - The script does not take any input from users. - The output should be: - \\"Running...\\" printed every second. - \\"Alarm signal received.\\" printed after 10 seconds. - \\"SIGINT received, graceful shutdown.\\" printed when the user sends an interrupt signal (e.g., Ctrl+C). Requirements - Correctly use the `signal` module to set up and handle signals. - Ensure the program terminates gracefully on receiving `SIGINT`. ```python import signal import time import sys def sigint_handler(signum, frame): print(\\"SIGINT received, graceful shutdown.\\") sys.exit(0) def sigalrm_handler(signum, frame): print(\\"Alarm signal received.\\") def setup_signal_handlers(): signal.signal(signal.SIGINT, sigint_handler) signal.signal(signal.SIGALRM, sigalrm_handler) def main(): setup_signal_handlers() signal.alarm(10) # Schedule an alarm in 10 seconds while True: print(\\"Running...\\") time.sleep(1) if __name__ == \\"__main__\\": main() ``` Performance Considerations - The script should handle up to a few signals per second without issues. - No special performance optimizations are required besides ensuring proper usage of signal handling as described.","solution":"import signal import time import sys def sigint_handler(signum, frame): print(\\"SIGINT received, graceful shutdown.\\") sys.exit(0) def sigalrm_handler(signum, frame): print(\\"Alarm signal received.\\") def setup_signal_handlers(): signal.signal(signal.SIGINT, sigint_handler) signal.signal(signal.SIGALRM, sigalrm_handler) def main(): setup_signal_handlers() signal.alarm(10) # Schedule an alarm in 10 seconds try: while True: print(\\"Running...\\") time.sleep(1) except KeyboardInterrupt: # This block is not needed as we already handle SIGINT, but it ensures graceful shutdown print(\\"SIGINT received, graceful shutdown.\\") sys.exit(0) if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with sanitizing and preparing a set of strings to be used as attribute values in an XML document. You have been asked to implement a function that will take a list of strings and return a list of these strings, each appropriately quoted and escaped for use in XML attributes. Your implementation should use the `quoteattr` function from the `xml.sax.saxutils` module to ensure proper formatting and escaping. # Function Signature ```python def prepare_xml_attributes(strings: list[str]) -> list[str]: pass ``` # Input - `strings`: A list of strings. Each string is a piece of data that needs to be prepared to be used as an attribute value in XML. # Output - Returns a list of strings where each string is the quoted and escaped version suitable for use as an XML attribute. # Constraints - Each string in the input list will have a maximum length of (10^3) characters. - The input list can contain up to (10^4) strings. # Example ```python input_strings = [\\"example & data\\", \\"more > test\\", \\"some < values\\"] output = prepare_xml_attributes(input_strings) print(output) # Expected output: [\'\\"example &amp; data\\"\', \'\\"more &gt; test\\"\', \'\\"some &lt; values\\"\'] ``` # Notes - You need to handle the common special characters `&`, `<`, and `>`. - Ensure that each prepared attribute is correctly wrapped in quotes and any embedded quote characters are handled as specified by the `quoteattr` function. # Requirements 1. Use the `quoteattr` method from the `xml.sax.saxutils` module. 2. Your solution should efficiently handle the provided constraints. Good luck!","solution":"from xml.sax.saxutils import quoteattr def prepare_xml_attributes(strings: list[str]) -> list[str]: Takes a list of strings and returns a list of these strings, each appropriately quoted and escaped for use in XML attributes. Args: strings (list[str]): A list of strings to be prepared. Returns: list[str]: A list of quoted and escaped strings suitable for XML attributes. return [quoteattr(s) for s in strings]"},{"question":"We want to create a program that processes a list of transactions. Each transaction is represented as a dictionary with the following keys: - `\\"type\\"`: a string that can be either `\\"income\\"` or `\\"expense\\"`. - `\\"amount\\"`: a positive number representing the transaction amount. - `\\"category\\"`: a string representing the category of the transaction, such as `\\"food\\"`, `\\"rent\\"`, or `\\"salary\\"`. Your task is to write a function `process_transactions(transactions: list) -> dict` that processes a list of transactions and returns a summary dictionary with the following keys: - `\\"total_income\\"`: the total amount of all income transactions. - `\\"total_expense\\"`: the total amount of all expense transactions. - `\\"categorized_expenses\\"`: a dictionary where the keys are expense categories and the values are the total amounts of expenses in each category. The function should also handle the following edge cases: 1. If a transaction dictionary has a malformed key or an invalid type, ignore that transaction and continue processing. 2. Use a `try`/`except` block to handle any potential errors while processing each transaction (e.g., missing or incorrect data types). 3. Ensure the function is efficient and can handle a large list of transactions without significant performance issues. # Input - `transactions`: A list of dictionaries, where each dictionary represents a transaction. # Output - A dictionary with the keys `\\"total_income\\"`, `\\"total_expense\\"`, and `\\"categorized_expenses\\"` providing a summary of the transactions. # Example ```python transactions = [ {\\"type\\": \\"income\\", \\"amount\\": 1000, \\"category\\": \\"salary\\"}, {\\"type\\": \\"expense\\", \\"amount\\": 200, \\"category\\": \\"food\\"}, {\\"type\\": \\"expense\\", \\"amount\\": 150, \\"category\\": \\"transport\\"}, {\\"type\\": \\"income\\", \\"amount\\": 500, \\"category\\": \\"bonus\\"}, {\\"type\\": \\"expense\\", \\"amount\\": 300, \\"category\\": \\"food\\"}, {\\"type\\": \\"income\\", \\"amount\\": \\"invalid\\", \\"category\\": \\"salary\\"} # This transaction is invalid and should be ignored ] summary = process_transactions(transactions) print(summary) ``` # Expected Output ```python { \\"total_income\\": 1500, \\"total_expense\\": 650, \\"categorized_expenses\\": { \\"food\\": 500, \\"transport\\": 150 } } ``` # Constraints 1. The `transactions` list can contain up to 100,000 transactions. 2. Each transaction can have any type of error (missing keys, wrong data types, etc.), so robust error handling is necessary. 3. Maintain the order of categories in the output dictionary as they first appear in the list of transactions.","solution":"def process_transactions(transactions): Processes a list of transactions and returns a summary dictionary. Args: transactions (list): A list of dictionary where each dictionary has keys \'type\', \'amount\', and \'category\'. Returns: dict: A dictionary with keys \'total_income\', \'total_expense\', and \'categorized_expenses\'. total_income = 0 total_expense = 0 categorized_expenses = {} for transaction in transactions: try: type_ = transaction[\'type\'] amount = transaction[\'amount\'] category = transaction[\'category\'] if not isinstance(type_, str) or not isinstance(category, str) or type_ not in [\'income\', \'expense\']: continue if not isinstance(amount, (int, float)) or amount <= 0: continue if type_ == \'income\': total_income += amount elif type_ == \'expense\': total_expense += amount if category in categorized_expenses: categorized_expenses[category] += amount else: categorized_expenses[category] = amount except (KeyError, TypeError): continue return { \\"total_income\\": total_income, \\"total_expense\\": total_expense, \\"categorized_expenses\\": categorized_expenses }"},{"question":"Objective Assess your ability to create visually distinct bar plots using Seaborn\'s `so.Dodge()` functionality from its objects interface. Problem Statement You are given a dataset of restaurant tips, and your task is to create a bar plot showing the aggregated values with distinct dodging characteristics as described below. Write a Python function `create_dodged_bar_plot` that takes in the following parameters: - `dataset`: A DataFrame containing the tips dataset. - `x_var`: The name of the categorical variable to be plotted on the x-axis. - `y_var`: The name of the numerical variable to aggregate and plot on the y-axis. - `color_var`: The name of the variable to use for color encoding. - `agg_func`: The aggregation function to apply to the `y_var`. It should be either \\"sum\\", \\"mean\\", or \\"count\\". Your function should: 1. Load the dataset into a Seaborn `Plot` object. 2. Use `so.Dodge(gap=0.1)` to add a small gap between the bars. 3. Create bar plots with aggregated y values using the specified aggregate function. 4. Ensure the bars are dodged appropriately based on the `color_var`. # Input 1. `dataset`: A pandas DataFrame with records of tips. 2. `x_var`: A string representing the column name for the x variable. 3. `y_var`: A string representing the column name for the y variable. 4. `color_var`: A string representing the column name for color encoding. 5. `agg_func`: A string taking values \\"sum\\", \\"mean\\", or \\"count\\" to indicate the type of aggregation to apply on the y variable. # Output A plot object created using Seaborn `so.Plot` with the desired dodging and aggregation applied. # Example ```python def create_dodged_bar_plot(dataset, x_var, y_var, color_var, agg_func): import seaborn.objects as so aggregation_mapping = { \\"sum\\": so.Agg(\\"sum\\"), \\"mean\\": so.Agg(\\"mean\\"), \\"count\\": so.Agg(\\"count\\") } agg_function = aggregation_mapping[agg_func] p = ( so.Plot(dataset, x_var, y_var, color=color_var) .add(so.Bar(), agg_function, so.Dodge(gap=0.1)) ) return p # Example usage # tips = seaborn.load_dataset(\\"tips\\").astype({\\"time\\": str}) # plot = create_dodged_bar_plot(tips, \\"day\\", \\"total_bill\\", \\"sex\\", \\"sum\\") # plot.show() ``` Constraints - Assume the input dataset is in the correct format. - The aggregation function should be one of \\"sum\\", \\"mean\\", or \\"count\\". Notes - Use Seaborn\'s `so.Plot` and `so.Dodge` functionalities to create the plot. - The plot should clearly represent the dodged bars with slight gaps for better readability.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def create_dodged_bar_plot(dataset, x_var, y_var, color_var, agg_func): Creates a dodged bar plot with aggregated y values. Parameters: dataset (pd.DataFrame): DataFrame containing the dataset. x_var (str): The name of the categorical variable for the x-axis. y_var (str): The name of the numerical variable to aggregate. color_var (str): The name of the variable for color encoding. agg_func (str): The aggregation function (\'sum\', \'mean\', \'count\'). Returns: A seaborn.objects.Plot object with the dodged bar plot. aggregation_mapping = { \\"sum\\": so.Agg(\\"sum\\"), \\"mean\\": so.Agg(\\"mean\\"), \\"count\\": so.Agg(\\"count\\") } if agg_func not in aggregation_mapping: raise ValueError(f\\"Invalid aggregation function: {agg_func}. Choose from \'sum\', \'mean\', or \'count\'.\\") agg_function = aggregation_mapping[agg_func] p = ( so.Plot(dataset, x=x_var, y=y_var, color=color_var) .add(so.Bar(), agg_function, so.Dodge(gap=0.1)) ) return p"},{"question":"# Question Objective You are tasked with demonstrating your comprehension of Seaborn by creating a line plot with customized normalization. This task assesses your ability to effectively use Seaborn\'s `so.Plot` and `so.Norm()` functionality, as well as your skills in dataset handling, plotting, and customizing visualizations. Requirements 1. Load the `healthexp` dataset using `seaborn.load_dataset`. 2. Create a line plot of `Year` vs `Spending_USD` for each `Country`. 3. Normalize the spending data relative to a custom baseline year (1970) and use percentages. 4. Label the y-axis appropriately to indicate the percentage change. 5. Ensure your plot is clear and informative. Input None (You will use the `healthexp` dataset from Seaborn). Output A Seaborn line plot. Constraints - Use Seaborn functions as much as possible. - Maintain clear and readable code. Example Plot The expected plot should be similar to the example provided below, but tailored as per the requirements mentioned: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Create the plot plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) # Display the plot plot.show() ``` Follow these steps to create your solution. Your plot should clearly depict how health spending has changed over the years, relative to the spending in 1970, for each country. Submission Submit your code solution for generating the plot as specified. Ensure your code is well-commented and follows the best practices for readability and maintainability.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_normalized_line_plot(): # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Create the plot plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == 1970\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) # Display the plot plot.show() # Call the function to display the plot create_normalized_line_plot()"},{"question":"# Serialization and Deserialization with `marshal` Module In this task, you are required to implement two functions, `serialize_data` and `deserialize_data`, using the `marshal` module to manage the conversion of Python objects to and from a binary format. # Function Requirements 1. **serialize_data(data: Any) -> bytes:** - `data`: A Python object which must be of one of the supported types (e.g., boolean, integer, floating point number, complex number, string, bytes, bytearray, tuple, list, set, frozenset, dictionary, None, Ellipsis, StopIteration). - `version`: Use the current version of `marshal` (default is `marshal.version`). - **Output**: Return a bytes object representing the serialized data using `marshal.dumps`. 2. **deserialize_data(serialized: bytes) -> Any:** - `serialized`: A bytes object that was created using the `serialize_data` function. - **Output**: Return the original Python object by converting the bytes object back using `marshal.loads`. # Constraints - You must use the `marshal` module functions `marshal.dumps` and `marshal.loads` exclusively. - Ensure that unsupported types raise a `ValueError`. - If extra bytes are present in the `serialized` input, they should be ignored. # Performance Requirements - Both functions should handle typical data serialization/deserialization operations efficiently. They should gracefully handle inputs up to at least a few megabytes in size without significant delay. # Examples 1. **Example 1: Serialization** ```python data = {\'a\': 1, \'b\': [2, 3, 4], \'c\': None} serialized = serialize_data(data) # serialized will be a bytes object. ``` 2. **Example 2: Deserialization** ```python serialized = b\'x80x04x95&x00x00x00x00x00x00x00}x94(x8cx01ax94Kx01x8cx01bx94]x94(Kx02Kx03Kx04ex8cx01cx94Nx00.\' data = deserialize_data(serialized) # data will be the Python object: {\'a\': 1, \'b\': [2, 3, 4], \'c\': None} ``` # Implementation Implement the `serialize_data` and `deserialize_data` functions below: ```python import marshal def serialize_data(data, version=marshal.version): try: return marshal.dumps(data, version) except ValueError as e: raise ValueError(\\"Unsupported type found in the data.\\") from e def deserialize_data(serialized): try: return marshal.loads(serialized) except (EOFError, ValueError, TypeError) as e: raise ValueError(\\"Deserialization failed due to invalid data.\\") from e ``` # Note Please test your implementation with a variety of data types to ensure robustness and correctness.","solution":"import marshal def serialize_data(data, version=marshal.version): try: return marshal.dumps(data, version) except ValueError as e: raise ValueError(\\"Unsupported type found in the data.\\") from e def deserialize_data(serialized): try: return marshal.loads(serialized) except (EOFError, ValueError, TypeError) as e: raise ValueError(\\"Deserialization failed due to invalid data.\\") from e"},{"question":"Objective To assess the student\'s understanding of Python\'s `pdb` module and their ability to utilize it for debugging purposes, including setting breakpoints, stepping through code, and inspecting the state of a program. Problem Statement You are given a Python script that contains a few bugs. Your task is to use the `pdb` module to debug the script, identify the issues, and fix them. Additionally, you need to extend the `Pdb` class to add a custom command. Script ```python def faulty_function(a, b): result = 0 for i in range(a): result += b / i # This line will cause a ZeroDivisionError when i is 0 return result def main(): val1 = 5 val2 = 0 print(faulty_function(val1, val2)) if __name__ == \\"__main__\\": main() ``` Steps to Follow 1. **Identify and Fix Bugs Using `pdb`**: - Import the `pdb` module and set a trace at the beginning of the `faulty_function`. - Use `pdb` commands to step through the code and identify the reason for any exceptions or incorrect outputs. - Modify the script to fix the identified issues. 2. **Extend the `Pdb` Class**: - Create a subclass of the `Pdb` class called `CustomPdb`. - Add a custom command `doubleprint` that takes an expression as an argument, evaluates it, and prints the result twice. The command should be available in the interactive debugger. 3. **Demonstrate Usage**: - Demonstrate the use of your custom `doubleprint` command to print the value of a variable twice during debugging. Expected Output After fixing the script, it should execute without errors and produce the correct result. Constraints - You should only modify the script by adding debugging statements and fixing bugs. Do not refactor the logic of the functions. - Ensure that your custom `Pdb` class is functioning as expected, and the `doubleprint` command works correctly within the interactive debugging session. Sample Execution Steps 1. Insert `import pdb; pdb.set_trace()` at the beginning of `faulty_function`. 2. Use pdb commands like `step`, `next`, `print`, `continue`, etc., to debug the function. 3. Create a `CustomPdb` class to include the `doubleprint` command. 4. Demonstrate usage by setting a breakpoint in the `main` function and using the `doubleprint` command. Submission Submit your modified script with: - Debugging statements and fixed bugs. - The definition of `CustomPdb` class with the `doubleprint` command. - A short explanation of how you used `pdb` to identify the bugs.","solution":"import pdb def faulty_function(a, b): result = 0 for i in range(1, a): # Start range from 1 to avoid division by zero result += b / i return result class CustomPdb(pdb.Pdb): def do_doubleprint(self, arg): \\"Print the argument twice\\" try: value = self._getval(arg) print(value) print(value) except Exception as e: print(f\\"Error evaluating {arg}: {e}\\") def main(): val1 = 5 val2 = 1 # Change to 1 to avoid division by zero errors in the fixed function print(\\"Debugging with CustomPdb:\\") pdb_instance = CustomPdb() pdb_instance.set_trace() # Set a breakpoint print(faulty_function(val1, val2)) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implementing and Managing Slices in Python Write a Python function `custom_slice` that replicates some of the functionalities of Python\'s built-in slicing mechanism. You must manage boundary conditions and properly handle different combinations of start, stop, and step values, including `None`. Function Signature ```python def custom_slice(sequence, start=None, stop=None, step=None): pass ``` Input - `sequence`: A sequence (list, tuple, or string) that you will be slicing. - `start`: The starting index of the slice. If `None`, it defaults to the beginning of the sequence. - `stop`: The stopping index of the slice. If `None`, it defaults to the end of the sequence. - `step`: The step value of the slice. If `None`, it defaults to 1. Output - Returns a new sequence (of the same type as `sequence`) that contains the elements of the original sequence according to the slice parameters. Constraints - The function should correctly handle out-of-bound indices by clipping them to the valid range (similar to how Python handles slices). - Negative indices should be properly computed from the end of the sequence. - The function must handle sequences with negative steps. Example ```python # Test Case 1 result = custom_slice([1, 2, 3, 4, 5], start=1, stop=4, step=1) print(result) # Output: [2, 3, 4] # Test Case 2 result = custom_slice(\\"hello, world\\", start=0, stop=None, step=2) print(result) # Output: \\"hlo ol\\" # Test Case 3 result = custom_slice((10, 20, 30, 40, 50), start=None, stop=None, step=-1) print(result) # Output: (50, 40, 30, 20, 10) ``` Additional Performance Requirements - The function should aim for O(n) complexity where n is the length of the sequence.","solution":"def custom_slice(sequence, start=None, stop=None, step=None): Returns a sliced sequence according to the given start, stop, and step parameters if start is None: start = 0 if step is None or step > 0 else -1 if stop is None: stop = len(sequence) if step is None or step > 0 else -len(sequence) - 1 if step is None: step = 1 return sequence[start:stop:step]"},{"question":"You are given an XML document containing information about a collection of books. Each book has attributes and elements like title, author, year, and genre. For this assessment, you will need to perform the following tasks using the `xml.etree.ElementTree` module: 1. **Parse the XML Document**: Read the given XML document from a string. 2. **Count Books by Genre**: Create a function that counts the number of books for each genre. 3. **Update Publication Year**: Implement a function that updates the publication year of all books written by a specific author. 4. **Remove Books by Condition**: Write a function to remove all books published before a certain year. 5. **Generate and Output XML**: After performing the updates, output the modified XML as a string. # XML Document Example ```xml <library> <book genre=\\"Fiction\\"> <title>Book Title 1</title> <author>Author A</author> <year>2010</year> </book> <book genre=\\"Science\\"> <title>Book Title 2</title> <author>Author B</author> <year>2005</year> </book> <book genre=\\"Non-Fiction\\"> <title>Book Title 3</title> <author>Author A</author> <year>2012</year> </book> </library> ``` # Task Implementation 1. **Function: `parse_xml(xml_string: str) -> ElementTree`** - **Input**: A string containing the XML document. - **Output**: An `ElementTree` object representing the parsed XML. 2. **Function: `count_books_by_genre(root: Element) -> dict`** - **Input**: The root element of the parsed XML. - **Output**: A dictionary where keys are genres and values are counts of books in each genre. 3. **Function: `update_publication_year(root: Element, author: str, new_year: int) -> None`** - **Input**: The root element of the parsed XML, the author\'s name, and the new year to set. - **Output**: None (The function should update the XML in place). 4. **Function: `remove_books_before_year(root: Element, year: int) -> None`** - **Input**: The root element of the parsed XML and the year threshold. - **Output**: None (The function should remove books from the XML in place). 5. **Function: `generate_xml_string(root: Element) -> str`** - **Input**: The root element of the parsed XML. - **Output**: A string containing the modified XML. # Example Usage ```python xml_string = <library> <book genre=\\"Fiction\\"> <title>Book Title 1</title> <author>Author A</author> <year>2010</year> </book> <book genre=\\"Science\\"> <title>Book Title 2</title> <author>Author B</author> <year>2005</year> </book> <book genre=\\"Non-Fiction\\"> <title>Book Title 3</title> <author>Author A</author> <year>2012</year> </book> </library> tree = parse_xml(xml_string) root = tree.getroot() genre_counts = count_books_by_genre(root) print(genre_counts) # {\'Fiction\': 1, \'Science\': 1, \'Non-Fiction\': 1} update_publication_year(root, \\"Author A\\", 2020) remove_books_before_year(root, 2010) modified_xml = generate_xml_string(root) print(modified_xml) ``` # Constraints - Ensure the XML structure is well-formed before and after your operations. - Efficiently handle large XML documents without excessive memory usage.","solution":"import xml.etree.ElementTree as ET def parse_xml(xml_string: str) -> ET.ElementTree: Parse the given XML string and return an ElementTree object. return ET.ElementTree(ET.fromstring(xml_string)) def count_books_by_genre(root: ET.Element) -> dict: Count the number of books for each genre. genre_count = {} for book in root.findall(\'book\'): genre = book.get(\'genre\') if genre in genre_count: genre_count[genre] += 1 else: genre_count[genre] = 1 return genre_count def update_publication_year(root: ET.Element, author: str, new_year: int) -> None: Update the publication year of books by the specified author. for book in root.findall(\'book\'): book_author = book.find(\'author\') if book_author is not None and book_author.text == author: book_year = book.find(\'year\') if book_year is not None: book_year.text = str(new_year) def remove_books_before_year(root: ET.Element, year: int) -> None: Remove all books published before the specified year. for book in root.findall(\'book\'): book_year = book.find(\'year\') if book_year is not None and int(book_year.text) < year: root.remove(book) def generate_xml_string(root: ET.Element) -> str: Generate string from the modified XML root element. return ET.tostring(root, encoding=\'unicode\')"},{"question":"Problem Statement: You are given a list of integers. You need to perform several operations on this list using the `array` module to optimize memory and performance. Implement a function `process_array_operations` that performs the following steps: 1. **Initialization:** Convert the given list of integers into an array of type signed int (\'i\'). 2. **Insertions:** Insert a given integer `x` at a specified position `pos` in the array. 3. **Deletion:** Remove the first occurrence of a specified integer `y` from the array. 4. **Count Occurrences:** Count how many times another specified integer `z` appears in the array. 5. **Reversal:** Reverse the order of the array. 6. **Conversion:** Convert the resulting array back to a list and return it alongside the count of integer `z`. Function Signature: ```python def process_array_operations(numbers: list, x: int, pos: int, y: int, z: int) -> (list, int): ``` Input: - `numbers` (list of int): A list of integers (1 ≤ len(numbers) ≤ 10^6). - `x` (int): An integer to be inserted. - `pos` (int): The position at which `x` should be inserted (0 ≤ pos ≤ len(numbers)). - `y` (int): An integer to be removed from the array. - `z` (int): An integer to count its occurrences in the array. Output: - A tuple containing a list of integers representing the modified array after all operations, and an integer representing the count of `z` in the final array. Constraints: - The initial list may contain duplicates. - If `y` does not exist in the original list, the array should remain unchanged for this step. - Ensure the operations are efficient to handle the upper limits of input size. Example: ```python # Sample input numbers = [1, 2, 3, 4, 5] x = 10 pos = 2 y = 3 z = 5 # Expected output ([1, 2, 10, 4, 5], 1) # Explanation: # - Initial array: [1, 2, 3, 4, 5] # - After insertion: [1, 2, 10, 3, 4, 5] # - After deletion: [1, 2, 10, 4, 5] # - Count of 5: 1 # - After reversal: [5, 4, 10, 2, 1] # - Output: ([5, 4, 10, 2, 1], 1) ``` Note: In your solution, make sure to use the `array` module to take advantage of its compact memory representation and efficient operations for large sequences of homogeneous data types.","solution":"import array def process_array_operations(numbers: list, x: int, pos: int, y: int, z: int) -> (list, int): Processes the array operations as specified in the problem statement. Args: - numbers (list of int): A list of integers. - x (int): An integer to be inserted. - pos (int): The position at which x should be inserted. - y (int): An integer to be removed from the array. - z (int): An integer to count its occurrences in the array. Returns: - tuple: A tuple containing the modified list of integers and the count of integer z. # Step 1: Convert the list into an array of type signed int (\'i\') arr = array.array(\'i\', numbers) # Step 2: Insert x at position pos arr.insert(pos, x) # Step 3: Remove the first occurrence of y try: arr.remove(y) except ValueError: pass # Ignore if y is not found in the array # Step 4: Count occurrences of z count_z = arr.count(z) # Step 5: Reverse the array arr.reverse() # Step 6: Convert the array back to a list and return it along with the count of z return arr.tolist(), count_z"},{"question":"Coding Assessment Question # Objective The goal of this question is to test your ability to interact with site-specific paths in Python, utilize the `site` module to customize the environment, and properly manipulate the module search path based on various configurations. # Problem Statement You are tasked with implementing a function `configure_site_paths()` that: 1. Adds a specified directory to the `sys.path` and ensures its `.pth` files are processed. 2. Retrieves and returns a list of all global site-packages directories. 3. Retrieves and returns the path of the user-specific site-packages directory. # Requirements - Implement a function `configure_site_paths(sitedir: str) -> dict` that takes a single argument `sitedir` (a string representing the directory to be added). - The function should utilize `site.addsitedir(sitedir)` to add `sitedir` to `sys.path`. - After adding the directory, the function should retrieve the global site-packages directories using `site.getsitepackages()`. - The function should also retrieve the user-specific site-packages directory using `site.getusersitepackages()`. - Return a dictionary with two keys: `\\"global_site_packages\\"` and `\\"user_site_packages\\"`, mapping to the respective values retrieved. # Input - `sitedir`: A string representing the directory path to be added to `sys.path`. # Output - A dictionary containing: - `\\"global_site_packages\\"`: A list of strings, representing all global site-packages directories. - `\\"user_site_packages\\"`: A string, representing the user-specific site-packages directory. # Example ```python result = configure_site_paths(\'/custom/path/to/add\') # Expected result format # { # \\"global_site_packages\\": [\\"/usr/local/lib/pythonX.Y/site-packages\\", \\"/custom/other/global/site-packages\\"], # \\"user_site_packages\\": \\"/home/user/.local/lib/pythonX.Y/site-packages\\" # } ``` # Constraints - Ensure that the added `sitedir` processes any `.pth` files it contains. - The directory paths and structure for site-packages directories should follow the conventions outlined in the documentation. # Notes - The function should be capable of handling usual directory structures on Unix-like and Windows systems. - You may assume that Python\'s `site` module functions will handle validation of the paths provided. Implement the required function in Python. ```python import site def configure_site_paths(sitedir: str) -> dict: # Add the specified directory to sys.path and process its .pth files site.addsitedir(sitedir) # Retrieve and store global site-packages directories global_site_packages = site.getsitepackages() # Retrieve and store the user-specific site-packages directory user_site_packages = site.getusersitepackages() return { \\"global_site_packages\\": global_site_packages, \\"user_site_packages\\": user_site_packages } ```","solution":"import site def configure_site_paths(sitedir: str) -> dict: Adds the specified directory to sys.path and processes its .pth files. Retrieves and returns the global and user-specific site-packages directories. # Add the specified directory to sys.path and process its .pth files. site.addsitedir(sitedir) # Retrieve and store global site-packages directories. global_site_packages = site.getsitepackages() # Retrieve and store the user-specific site-packages directory. user_site_packages = site.getusersitepackages() return { \\"global_site_packages\\": global_site_packages, \\"user_site_packages\\": user_site_packages }"},{"question":"Objective To assess the student\'s understanding and ability to work with lists and dictionaries, using various list methods and comprehensions. Problem Statement You are given an input list of integers. Your task is to create a function that processes this list through several steps and then uses a dictionary to return the desired output. Steps: 1. **Identify Prime Numbers**: Create a sublist of prime numbers from the input list using list comprehensions. 2. **List Operations**: - Sort the list of prime numbers in descending order. - Append the sum of all prime numbers to the end of this list. 3. **Dictionary Operations**: - Create a dictionary where the keys are the prime numbers and the values are their positions (indices) in the sorted descending list (Note: index should be 1-based). - The last key in the dictionary should be `\'sum\'` with its value being the sum of the prime numbers. Function Signature ```python def process_primes(input_list: list) -> dict: # Your code here ``` Input - **input_list**: A list of integers (1 ≤ len(input_list) ≤ 1000, elements in the list will be between 2 and 1000). Output - A dictionary as described in step 3. Constraints - Use list methods where appropriate. - The function should be efficient and handle the upper limits of the constraints effectively. Example ```python input_list = [10, 15, 3, 7, 11, 17, 2] output_dict = process_primes(input_list) # output_dict should be {17: 1, 11: 2, 7: 3, 3: 4, 2: 5, \'sum\': 40} ``` Notes - A prime number is a natural number greater than 1 that is not a product of two smaller natural numbers. - You can use built-in functions and any standard libraries if needed.","solution":"def is_prime(n): Check if the number `n` is a prime number. if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def process_primes(input_list: list) -> dict: Process the input list to find prime numbers, sort them, and create a dictionary mapping each prime number to its position in the sorted list with the sum of primes as a final entry. # Identify prime numbers using list comprehension primes = [num for num in input_list if is_prime(num)] # Sort the list of prime numbers in descending order primes_sorted_desc = sorted(primes, reverse=True) # Append the sum of all prime numbers to the end of this list sum_primes = sum(primes_sorted_desc) primes_sorted_desc.append(sum_primes) # Create a dictionary where the keys are the prime numbers and the values are their positions (1-based index) primes_dict = {primes_sorted_desc[i]: i+1 for i in range(len(primes_sorted_desc) - 1)} # Add the sum of primes as the final key-value pair primes_dict[\'sum\'] = sum_primes return primes_dict"},{"question":"Coding Assessment Question # Objective: Design and implement a small library management system using Python that leverages data persistence techniques. The system should allow adding new books, borrowing books, returning books, and listing all available books. The data regarding books should be persistently stored on disk. This system should demonstrate proficiency in using the `pickle` or `shelve` module, coupled with the `sqlite3` module for tracking borrowed books and performing queries. # Requirements: 1. **Data Storage Requirements:** - Use `shelve` to persist the list of books and their details. - Each book should have attributes: `book_id` (string), `title` (string), `author` (string), `year_published` (integer). 2. **Database Requirements:** - Use `sqlite3` to maintain a record of borrowed books. - Create a table `borrowed_books` with the following columns: `book_id` (string), `borrower_name` (string), `borrowed_date` (string in YYYY-MM-DD format). 3. **Function Implementations:** - `add_book(book_id: str, title: str, author: str, year_published: int) -> None`: Adds a new book to the library. - `borrow_book(book_id: str, borrower_name: str, borrowed_date: str) -> str`: Marks a book as borrowed in the database. Returns a message indicating success or failure. - `return_book(book_id: str) -> str`: Marks a book as returned by removing it from the borrowed records in the database. Returns a message indicating success or failure. - `list_books() -> str`: Lists all available books in the library that are not currently borrowed. # Constraints: - All operations must handle exceptions gracefully, logging any errors to a file named `library_management.log`. - The system should ensure the consistency of data between shelve and sqlite3 databases. # Performance Requirements: - The system should efficiently handle operations for a library containing up to 10,000 books. - Borrow and return operations should maintain a reasonable response time even with an increasing number of entries. # Usage Example: ```python # Assume the class LibrarySystem is the main class implementing above functionalities library = LibrarySystem() library.add_book(\\"001\\", \\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925) library.add_book(\\"002\\", \\"1984\\", \\"George Orwell\\", 1949) print(library.list_books()) # Lists all books print(library.borrow_book(\\"001\\", \\"Alice\\", \\"2023-10-15\\")) # Borrows \\"The Great Gatsby\\" print(library.list_books()) # Now lists \\"1984\\" only print(library.return_book(\\"001\\")) # Returns \\"The Great Gatsby\\" print(library.list_books()) # Lists both \\"The Great Gatsby\\" and \\"1984\\" again ``` # Evaluation Criteria: - Correct and efficient usage of `shelve` for persistent storage. - Proper implementation of `sqlite3` database for borrowed books. - Handling edge cases and exceptions gracefully. - Code readability and structure.","solution":"import shelve import sqlite3 import logging from datetime import datetime # Configure logging logging.basicConfig(filename=\'library_management.log\', level=logging.ERROR) class LibrarySystem: def __init__(self, db_path=\'library_db\', shelve_path=\'books_shelve\'): self.db_path = db_path self.shelve_path = shelve_path self._init_db() def _init_db(self): try: self.conn = sqlite3.connect(self.db_path) self.cursor = self.conn.cursor() self.cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS borrowed_books ( book_id TEXT PRIMARY KEY, borrower_name TEXT, borrowed_date TEXT ) \'\'\') self.conn.commit() except sqlite3.Error as e: logging.error(f\\"SQLite error: {e}\\") finally: self.conn.close() def add_book(self, book_id, title, author, year_published): try: with shelve.open(self.shelve_path) as book_shelf: book_shelf[book_id] = { \'title\': title, \'author\': author, \'year_published\': year_published } except Exception as e: logging.error(f\\"Error adding book: {e}\\") def borrow_book(self, book_id, borrower_name, borrowed_date): try: with shelve.open(self.shelve_path) as book_shelf: if book_id not in book_shelf: return \\"Book not found\\" self.conn = sqlite3.connect(self.db_path) self.cursor = self.conn.cursor() self.cursor.execute(\'\'\' SELECT book_id FROM borrowed_books WHERE book_id = ? \'\'\', (book_id,)) if self.cursor.fetchone(): return \\"Book is already borrowed\\" self.cursor.execute(\'\'\' INSERT INTO borrowed_books (book_id, borrower_name, borrowed_date) VALUES (?, ?, ?) \'\'\', (book_id, borrower_name, borrowed_date)) self.conn.commit() return \\"Book borrowed successfully\\" except sqlite3.Error as e: logging.error(f\\"SQLite error: {e}\\") return \\"Error borrowing book\\" finally: self.conn.close() def return_book(self, book_id): try: self.conn = sqlite3.connect(self.db_path) self.cursor = self.conn.cursor() self.cursor.execute(\'\'\' DELETE FROM borrowed_books WHERE book_id = ? \'\'\', (book_id,)) if self.cursor.rowcount == 0: return \\"Book not found or not borrowed\\" self.conn.commit() return \\"Book returned successfully\\" except sqlite3.Error as e: logging.error(f\\"SQLite error: {e}\\") return \\"Error returning book\\" finally: self.conn.close() def list_books(self): try: with shelve.open(self.shelve_path) as book_shelf: all_books = set(book_shelf.keys()) self.conn = sqlite3.connect(self.db_path) self.cursor = self.conn.cursor() self.cursor.execute(\'\'\' SELECT book_id FROM borrowed_books \'\'\') borrowed_books = set(row[0] for row in self.cursor.fetchall()) available_books = all_books - borrowed_books book_details = [] with shelve.open(self.shelve_path) as book_shelf: for book_id in available_books: book = book_shelf[book_id] book_details.append(f\\"ID: {book_id}, Title: {book[\'title\']}, Author: {book[\'author\']}, Year: {book[\'year_published\']}\\") return \\"n\\".join(book_details) except Exception as e: logging.error(f\\"Error listing books: {e}\\") return \\"Error listing books\\" finally: self.conn.close() # Example Usage \'\'\' library = LibrarySystem() library.add_book(\\"001\\", \\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925) library.add_book(\\"002\\", \\"1984\\", \\"George Orwell\\", 1949) print(library.list_books()) # Lists all books print(library.borrow_book(\\"001\\", \\"Alice\\", \\"2023-10-15\\")) # Borrows \\"The Great Gatsby\\" print(library.list_books()) # Now lists \\"1984\\" only print(library.return_book(\\"001\\")) # Returns \\"The Great Gatsby\\" print(library.list_books()) # Lists both \\"The Great Gatsby\\" and \\"1984\\" again \'\'\'"},{"question":"# Python Coding Assessment Question **Objective:** You are required to create a Python C extension module that provides utilities to work with method objects. Specifically, you will implement functions to create new instance method objects and method objects, and retrieve their associated functions and instances. **Function Requirements:** 1. **`create_instance_method(func)`**: - **Input**: A callable function object `func`. - **Output**: An instance method object created from `func`. 2. **`create_method(func, instance)`**: - **Input**: A callable function object `func` and an instance `instance`. - **Output**: A method object created from `func` and bound to `instance`. 3. **`get_function_from_instance_method(instance_method)`**: - **Input**: An instance method object `instance_method`. - **Output**: The callable function object associated with `instance_method`. 4. **`get_function_from_method(method)`**: - **Input**: A method object `method`. - **Output**: The callable function object associated with `method`. 5. **`get_instance_from_method(method)`**: - **Input**: A method object `method`. - **Output**: The instance associated with `method`. **Constraints:** - Assume `func` is always a valid callable object. - Assume `instance` is always a valid instance of a user-defined class. - Handle errors gracefully, ensuring that invalid inputs raise appropriate Python exceptions. **Performance Requirements:** - The functions should efficiently create and retrieve method objects and their associated functions or instances. **Example Usage:** ```python from my_module import create_instance_method, create_method, get_function_from_instance_method, get_function_from_method, get_instance_from_method def sample_function(): return \\"Hello, World!\\" class MyClass: def __init__(self, value): self.value = value my_instance = MyClass(10) # Create instance method inst_method = create_instance_method(sample_function) # Create method bound to instance method = create_method(sample_function, my_instance) # Retrieve function from instance method func = get_function_from_instance_method(inst_method) assert func() == \\"Hello, World!\\" # Retrieve function from method func_bound = get_function_from_method(method) assert func_bound() == \\"Hello, World!\\" # Retrieve instance from method instance_bound = get_instance_from_method(method) assert instance_bound.value == 10 ``` **Submission:** Submit the complete C extension source code that implements the required functionality. Ensure to include instructions on how to build and test the module.","solution":"import types def create_instance_method(func): \'\'\'Takes a function and returns an instance method object.\'\'\' class Dummy: pass instance = Dummy() return types.MethodType(func, instance) def create_method(func, instance): \'\'\'Takes a function and an instance and returns the method object bound to the instance.\'\'\' return types.MethodType(func, instance) def get_function_from_instance_method(instance_method): \'\'\'Takes an instance method and returns the associated function object.\'\'\' return instance_method.__func__ def get_function_from_method(method): \'\'\'Takes a method object and returns the associated function object.\'\'\' return method.__func__ def get_instance_from_method(method): \'\'\'Takes a method object and returns the instance that owns the method.\'\'\' return method.__self__"},{"question":"# Objective Design and implement a Python program that uses the `faulthandler` module to handle and log errors and user-defined signals. This program should be capable of handling signals, and dumping tracebacks to a file both on demand and after a timeout. # Instructions 1. **Enabling Fault Handling**: - Initialize the fault handler to capture `SIGSEGV`, `SIGFPE`, `SIGABRT`, `SIGBUS`, and `SIGILL` signals. - Ensure that it logs tracebacks for all threads to a file named `fault_log.txt`. 2. **Signal Handling**: - Register a user-defined signal (`SIGUSR1` on Unix-based systems) to dump thread tracebacks to `user_signal_log.txt`. 3. **Timeout Traceback Dump**: - Implement a function to dump the thread tracebacks to `timeout_log.txt` after a user-defined timeout in seconds. Use another function to cancel this operation. 4. **Trigger and Test**: - Create functions to manually trigger a segmentation fault. - Include a main block to call the fault handling setup, timeout setup, and simulate a segmentation fault for testing purposes. # Constraints - The program should handle errors gracefully. - Assume running on a Unix-based operating system for signal handling. - The logging files should remain open until the fault handler is disabled or the program ends. # Example Output - Creation of `fault_log.txt` containing traceback information if a segmentation fault occurs. - Creation of `user_signal_log.txt` if `SIGUSR1` is received. - Creation of `timeout_log.txt` if a timeout occurs. # Function Definitions 1. `initialize_fault_handler()`: Enable fault handler and ensure logging to `fault_log.txt`. 2. `register_user_signal_handler()`: Register handler for `SIGUSR1` to log tracebacks to `user_signal_log.txt`. 3. `set_timeout_traceback(timeout: int)`: Set up a timeout to log tracebacks to `timeout_log.txt`. 4. `cancel_timeout_traceback()`: Cancel the timeout traceback function. 5. `trigger_segfault()`: Manually trigger a segmentation fault to test fault handling. # Example Usage ```python import faulthandler import signal import os def initialize_fault_handler(): with open(\'fault_log.txt\', \'w\') as f: faulthandler.enable(file=f) def register_user_signal_handler(): with open(\'user_signal_log.txt\', \'w\') as f: faulthandler.register(signal.SIGUSR1, file=f) def set_timeout_traceback(timeout: int): with open(\'timeout_log.txt\', \'w\') as f: faulthandler.dump_traceback_later(timeout, file=f) def cancel_timeout_traceback(): faulthandler.cancel_dump_traceback_later() def trigger_segfault(): import ctypes ctypes.string_at(0) # Manually trigger a segmentation fault for testing. if __name__ == \'__main__\': initialize_fault_handler() register_user_signal_handler() set_timeout_traceback(5) # Replace with actual application logic # Simulate a segmentation fault for demonstration trigger_segfault() cancel_timeout_traceback() ``` # Notes - Ensure to handle the file descriptors correctly to avoid writing to unintended files. - The program must be tested for handling both expected and unexpected signals and faults.","solution":"import faulthandler import signal import os def initialize_fault_handler(): with open(\'fault_log.txt\', \'w\') as f: faulthandler.enable(file=f) def register_user_signal_handler(): with open(\'user_signal_log.txt\', \'w\') as f: faulthandler.register(signal.SIGUSR1, file=f) def set_timeout_traceback(timeout: int): with open(\'timeout_log.txt\', \'w\') as f: faulthandler.dump_traceback_later(timeout, file=f) def cancel_timeout_traceback(): faulthandler.cancel_dump_traceback_later() def trigger_segfault(): import ctypes ctypes.string_at(0) # Manually trigger a segmentation fault for testing. if __name__ == \'__main__\': initialize_fault_handler() register_user_signal_handler() set_timeout_traceback(5) # Replace with actual application logic # Simulate a segmentation fault for demonstration try: trigger_segfault() except Exception as e: print(\\"[INFO] Segmentation fault triggered: \\", e) cancel_timeout_traceback()"},{"question":"# Python Function Object Manipulation In this task, you are to write a Python function that manipulates various attributes of another function object. Here are the steps you need to follow: 1. Create a function `modify_function` that takes another function `func` as input. 2. Modify the default arguments of `func` to a new set of defaults specified in a dictionary, `new_defaults`. 3. Add a new global variable to the `func` with a specified key-value pair given in a dictionary, `new_globals`. 4. Update the annotations of `func` with another dictionary, `new_annotations`. You need to: - Implement the `modify_function` which applies all the modifications. - Demonstrate the usage with an example function to show that the defaults, globals, and annotations have been updated correctly. **Function Signature:** ```python def modify_function(func: callable, new_defaults: dict, new_globals: dict, new_annotations: dict) -> callable: pass ``` **Input:** - `func` - A Python function to be modified. - `new_defaults` - A dictionary representing the new default argument values. The keys of the dictionary are the argument names. - `new_globals` - A dictionary representing the new global variables to be added to the function\'s globals. - `new_annotations` - A dictionary representing the new annotations for the function. **Output:** - Returns the modified function. **Example Usage:** ```python def example_function(a, b=2): Example function return a + b new_defaults = {\'b\': 5} new_globals = {\'new_var\': 10} new_annotations = {\'a\': int, \'b\': int, \'return\': int} modified_func = modify_function(example_function, new_defaults, new_globals, new_annotations) # Testing the modified function print(modified_func(1)) # Should print 6 print(modified_func.__annotations__) # Should print {\'a\': <class \'int\'>, \'b\': <class \'int\'>, \'return\': <class \'int\'>} print(modified_func.__globals__[\'new_var\']) # Should print 10 ``` **Constraints:** - Do not use global or nonlocal statements to modify the function. - Do not modify the provided function directly within the caller\'s namespace; encapsulate changes inside `modify_function`. Ensure your implementation passes the above example usage.","solution":"from types import FunctionType def modify_function(func: callable, new_defaults: dict, new_globals: dict, new_annotations: dict) -> callable: # Modify default arguments code = func.__code__ defaults = func.__defaults__ or () new_defaults_tuple = tuple(new_defaults.get(k, v) for k, v in zip(code.co_varnames[-len(defaults):], defaults)) # Add new globals new_global_dict = func.__globals__.copy() new_global_dict.update(new_globals) # Update annotations new_annotations_dict = func.__annotations__.copy() new_annotations_dict.update(new_annotations) # Create a new function with modified attributes modified_func = FunctionType( code, new_global_dict, func.__name__, new_defaults_tuple, func.__closure__ ) modified_func.__annotations__ = new_annotations_dict return modified_func"},{"question":"**Combining Statistics and Custom Numeric Types** **Objective:** Implement a function that takes an array of numbers, performs statistical analysis, and returns the results in a custom numeric format. **Task:** You are required to write a Python function `process_and_format_statistics(data: List[float]) -> CustomNumber` which performs the following: 1. **Calculate Statistics:** - Mean (average). - Median. - Standard deviation. 2. **Convert to Custom Number Format:** Implement a custom numeric type `CustomNumber` that supports arbitrary precision (similar to `decimal.Decimal`) and represents the computed statistics. **Function Details:** - **Input:** - `data`: A list of floating-point numbers. The list length is at least 1 and at most (10^6). - **Output:** - An instance of `CustomNumber` containing the calculated statistics (mean, median, and standard deviation). **Constraints:** - The list `data` will always contain at least one element. - Implement the `CustomNumber` type to replicate functionalities similar to Python\'s `decimal.Decimal` for arbitrary precision. - Ensure efficient computation to handle the upper limit of list length. **Example:** ```python from typing import List class CustomNumber: def __init__(self, mean, median, stdev): self.mean = mean self.median = median self.stdev = stdev def __repr__(self): return f\\"CustomNumber(mean={self.mean}, median={self.median}, stdev={self.stdev})\\" def process_and_format_statistics(data: List[float]) -> CustomNumber: # Your code goes here # Example usage: data = [1.0, 2.0, 3.0, 4.0, 5.0] print(process_and_format_statistics(data)) # Output should be a CustomNumber instance with mean, median, and standard deviation. ``` Ensure proper handling of floating-point precision and the efficient calculation of statistics even for large datasets.","solution":"from typing import List from decimal import Decimal, getcontext import statistics class CustomNumber: def __init__(self, mean, median, stdev): self.mean = Decimal(mean) self.median = Decimal(median) self.stdev = Decimal(stdev) def __repr__(self): return f\\"CustomNumber(mean={self.mean}, median={self.median}, stdev={self.stdev})\\" def process_and_format_statistics(data: List[float]) -> CustomNumber: getcontext().prec = 28 # Set a higher precision for Decimal calculations mean_val = statistics.mean(data) median_val = statistics.median(data) stdev_val = statistics.stdev(data) if len(data) > 1 else 0 return CustomNumber(mean_val, median_val, stdev_val) # Example Usage: data = [1.0, 2.0, 3.0, 4.0, 5.0] print(process_and_format_statistics(data))"},{"question":"**Task: Implement a Custom Logging System** You are required to design and implement a custom logging system for a Python application. The logging system should be capable of logging messages from multiple modules and threads. Additionally, the logs should contain contextual information such as the thread name and custom fields. # Requirements: 1. **Logger Configuration**: - Create a logger named `app.logger` with `DEBUG` level. - Configure the logger to log messages to both a file (`app.log`) and the console. - The file handler should log messages at the `DEBUG` level and above. - The console handler should log messages at the `ERROR` level and above. - Use a formatter that includes the timestamp, logger name, log level, thread name, and message. 2. **Contextual Logging**: - Use a filter or `LoggerAdapter` to add custom contextual information (e.g., `user_id`) to each log record. - Ensure that this information is included in both the file and console logs. 3. **Threading**: - Demonstrate the logging system by logging messages from multiple threads. - Each thread should log messages at different levels (e.g., `DEBUG`, `INFO`, `WARNING`, `ERROR`). 4. **Implementation**: - Write a main script that initializes the logging system and starts multiple threads. - Each thread should log messages with different levels and contextual information. - Ensure proper cleanup of threads and logging resources. # Input and Output: - **Input**: No input is required from the user. - **Output**: Log messages should be written to both the console and the file (`app.log`). # Constraints: - Use Python\'s `logging` module. - Use `threading` module to manage threads. - Ensure the solution is thread-safe. # Example: ```python import logging import threading import time class ContextFilter(logging.Filter): def filter(self, record): record.user_id = threading.current_thread().name return True def worker(thread_id): logger = logging.getLogger(\'app.logger\') logger.debug(f\'Worker {thread_id} - Debug message\') logger.info(f\'Worker {thread_id} - Info message\') logger.warning(f\'Worker {thread_id} - Warning message\') logger.error(f\'Worker {thread_id} - Error message\') def main(): # Logger configuration logger = logging.getLogger(\'app.logger\') logger.setLevel(logging.DEBUG) # File handler fh = logging.FileHandler(\'app.log\') fh.setLevel(logging.DEBUG) fh.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - %(user_id)s - %(message)s\')) # Console handler ch = logging.StreamHandler() ch.setLevel(logging.ERROR) ch.setFormatter(logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - %(user_id)s - %(message)s\')) # Add handlers to the logger logger.addHandler(fh) logger.addHandler(ch) # Add the contextual filter context_filter = ContextFilter() logger.addFilter(context_filter) # Create and start threads threads = [] for i in range(5): thread = threading.Thread(target=worker, args=(i,), name=f\'Thread-{i}\') threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() if __name__ == \\"__main__\\": main() ``` In this example, the logging system is configured to log messages from multiple threads with contextual information. Adjust the implementation details as needed to meet the requirements and ensure the logging system works as expected.","solution":"import logging import threading # Custom filter to add contextual information class ContextFilter(logging.Filter): def filter(self, record): record.user_id = threading.current_thread().name return True # Logger configuration def setup_logger(): logger = logging.getLogger(\'app.logger\') logger.setLevel(logging.DEBUG) # File handler fh = logging.FileHandler(\'app.log\') fh.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - User:%(user_id)s - %(message)s\') fh.setFormatter(file_formatter) # Console handler ch = logging.StreamHandler() ch.setLevel(logging.ERROR) console_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(threadName)s - User:%(user_id)s - %(message)s\') ch.setFormatter(console_formatter) # Add handlers to the logger logger.addHandler(fh) logger.addHandler(ch) # Add custom filter context_filter = ContextFilter() logger.addFilter(context_filter) return logger # Thread worker function def worker(thread_id): logger = logging.getLogger(\'app.logger\') logger.debug(f\'Worker {thread_id} - Debug message\') logger.info(f\'Worker {thread_id} - Info message\') logger.warning(f\'Worker {thread_id} - Warning message\') logger.error(f\'Worker {thread_id} - Error message\') # Main function to setup logger and start threads def main(): logger = setup_logger() # Create and start threads threads = [] for i in range(5): thread = threading.Thread(target=worker, args=(i,), name=f\'Thread-{i}\') threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join() if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Copy Operations You are required to implement a custom deep copy mechanism for a class that contains recursively defined structures. Class Definition You will define a class `Node` representing a node in a tree. Each `Node` has: - A `value` attribute storing the node\'s value. - A `children` attribute, which is a list of child nodes. Requirements: 1. Implement the `__deepcopy__` method for the `Node` class. 2. Ensure that the deep copy mechanism handles recursive references properly. Input: - An instance of `Node`, potentially containing recursive structures. Output: - A new instance of `Node` that is a deep copy of the input instance. # Constraints: - `Node` instances and their children can form recursive structures, i.e., a child node might reference one of its ancestor nodes. # Example: ```python class Node: def __init__(self, value): self.value = value self.children = [] def __deepcopy__(self, memo): pass # Implementation of custom deep copy logic. def test_custom_deepcopy(): import copy root = Node(1) child1 = Node(2) child2 = Node(3) root.children.append(child1) child1.children.append(child2) child2.children.append(root) # Creating a recursive structure. root_copy = copy.deepcopy(root) assert root_copy is not root assert root_copy.children[0] is not root.children[0] assert root_copy.children[0].children[0] is not root.children[0].children[0] assert root_copy.children[0].children[0].children[0] is root_copy # The recursive reference should be correctly handled. if __name__ == \\"__main__\\": test_custom_deepcopy() ``` In this question, the student is required to: - Understand the difference between shallow and deep copy. - Implement a deep copy mechanism that correctly handles recursive structures using the `copy` module and the `memo` dictionary. - Test the implementation to ensure it behaves as expected.","solution":"import copy class Node: def __init__(self, value): self.value = value self.children = [] def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] copy_node = Node(self.value) memo[id(self)] = copy_node copy_node.children = [copy.deepcopy(child, memo) for child in self.children] return copy_node"},{"question":"**Objective:** Write a Python function that takes a list of HTTP status codes and returns a detailed summary for each code using the `http.HTTPStatus` enumeration. **Function Signature:** ```python def get_status_summary(status_codes: list[int]) -> list[dict]: pass ``` **Input:** - `status_codes`: a list of integers, each representing an HTTP status code. **Output:** - A list of dictionaries, where each dictionary contains the following keys: - `code`: The HTTP status code (integer). - `phrase`: The reason phrase associated with the status code (string). - `description`: A long description of the HTTP status (string). **Constraints:** 1. All provided status codes will be valid HTTP status codes present in the `http.HTTPStatus` enum. 2. The list will have at least one status code and at most 50 status codes. **Performance Requirement:** The function should be able to process up to 50 status codes efficiently. **Example:** ```python from http import HTTPStatus status_codes = [200, 404, 500] result = get_status_summary(status_codes) ``` The expected output: ```python [ { \\"code\\": 200, \\"phrase\\": \\"OK\\", \\"description\\": \\"Request fulfilled, document follows\\" }, { \\"code\\": 404, \\"phrase\\": \\"Not Found\\", \\"description\\": \\"Nothing matches the given URI\\" }, { \\"code\\": 500, \\"phrase\\": \\"Internal Server Error\\", \\"description\\": \\"The server encountered an unexpected condition which prevented it from fulfilling the request\\" } ] ``` **Notes:** - You must utilize the `http.HTTPStatus` enumeration to obtain the values for `phrase` and `description`. - Ensure that the function handles the creation of the summary efficiently.","solution":"from http import HTTPStatus def get_status_summary(status_codes: list[int]) -> list[dict]: Takes a list of HTTP status codes and returns a detailed summary for each code. :param status_codes: List of integers representing HTTP status codes. :return: List of dictionaries with code, phrase, and description of each status code. summary = [] for code in status_codes: status = HTTPStatus(code) summary.append({ \\"code\\": status.value, \\"phrase\\": status.phrase, \\"description\\": status.description }) return summary"},{"question":"Coding Assessment Question # Objective: You are required to implement a function that securely hashes a list of passwords using various hashing methods provided by the `crypt` module and then verifies the hashed passwords against the original passwords. # Function Signature: ```python def secure_password_handling(passwords: List[str]) -> Dict[str, Dict[str, bool]]: pass ``` # Input: - `passwords`: A list of strings where each string is a password to be hashed. (1 ≤ len(passwords) ≤ 100, 1 ≤ len(password) ≤ 50) # Output: A dictionary where the keys are the hashing methods (`\'SHA512\'`, `\'SHA256\'`, `\'BLOWFISH\'`, `\'MD5\'`, `\'CRYPT\'`) and the values are dictionaries. The inner dictionaries have the original passwords as keys and boolean values indicating whether the password matches its hashed version. # Constraints: - You should use the strongest available method by default for hashing if no specific method is provided. - Use the `compare_digest` function from the `hmac` module to ensure secure comparison of hashes. # Example: ```python from typing import List, Dict import crypt from hmac import compare_digest def secure_password_handling(passwords: List[str]) -> Dict[str, Dict[str, bool]]: result = {} methods = { \'SHA512\': crypt.METHOD_SHA512, \'SHA256\': crypt.METHOD_SHA256, \'BLOWFISH\': crypt.METHOD_BLOWFISH, \'MD5\': crypt.METHOD_MD5, \'CRYPT\': crypt.METHOD_CRYPT, } for method_name, method in methods.items(): result[method_name] = {} for password in passwords: salt = crypt.mksalt(method) hashed_password = crypt.crypt(password, salt) result[method_name][password] = compare_digest(hashed_password, crypt.crypt(password, hashed_password)) return result # Sample input passwords = [\'password1\', \'Password2!\', \'hunter2\'] # Expected output # { # \'SHA512\': {\'password1\': True, \'Password2!\': True, \'hunter2\': True}, # \'SHA256\': {\'password1\': True, \'Password2!\': True, \'hunter2\': True}, # \'BLOWFISH\': {\'password1\': True, \'Password2!\': True, \'hunter2\': True}, # \'MD5\': {\'password1\': True, \'Password2!\': True, \'hunter2\': True}, # \'CRYPT\': {\'password1\': True, \'Password2!\': True, \'hunter2\': True}, # } ``` # Notes: - You must handle errors gracefully and ensure that the function runs efficiently even for the upper limits of inputs. - Consider edge cases where certain methods may not be available on all platforms. # Evaluation: Your solution will be evaluated based on correctness, efficiency, and code quality. Make sure to adhere to best practices for secure password handling and code readability.","solution":"from typing import List, Dict import crypt from hmac import compare_digest def secure_password_handling(passwords: List[str]) -> Dict[str, Dict[str, bool]]: result = {} methods = { \'SHA512\': crypt.METHOD_SHA512, \'SHA256\': crypt.METHOD_SHA256, \'BLOWFISH\': crypt.METHOD_BLOWFISH, \'MD5\': crypt.METHOD_MD5, \'CRYPT\': crypt.METHOD_CRYPT, } for method_name, method in methods.items(): result[method_name] = {} for password in passwords: salt = crypt.mksalt(method) hashed_password = crypt.crypt(password, salt) result[method_name][password] = compare_digest(hashed_password, crypt.crypt(password, hashed_password)) return result"},{"question":"You are provided with a template to create a line plot using Seaborn\'s `so.Plot`. Your task is to write a function `custom_plot` that takes in X and Y coordinates, and specific limits for the x and y axes. The function should generate a plot with markers, adjust the axis limits accordingly, and handle cases where the axis limits need to be inverted or default values should be maintained. Function Signature ```python def custom_plot(x: list, y: list, x_limits: tuple = None, y_limits: tuple = None) -> so.Plot: pass ``` Input 1. `x` (list): A list of numerical values representing x-coordinates. 2. `y` (list): A list of numerical values representing y-coordinates. 3. `x_limits` (tuple, optional): A tuple `(min, max)` specifying the limits for the x-axis. If `None`, the default limits should be used. 4. `y_limits` (tuple, optional): A tuple `(min, max)` specifying the limits for the y-axis. If `None`, the default limits should be used. Output - Returns an Seaborn `so.Plot` object representing the generated plot with the specified axis limits. Constraints - The lengths of `x` and `y` should be equal. - Each limit tuple should either be `None` or a two-element tuple containing numerical values. - Ensure that reversing the `min` and `max` values in the tuple will invert the respective axis. Example ```python x = [1, 2, 3] y = [1, 3, 2] # Example 1: Default plot limits plot = custom_plot(x, y) plot.show() # Example 2: Custom plot limits plot = custom_plot(x, y, x_limits=(0, 4), y_limits=(-1, 6)) plot.show() # Example 3: Inverted y-axis plot = custom_plot(x, y, y_limits=(6, 2)) plot.show() # Example 4: Partially custom plot limits plot = custom_plot(x, y, y_limits=(0, None)) plot.show() ``` # Notes - Use the `seaborn.objects as so` module for creating and manipulating the plots. - The `so.Plot()` constructor initializes the plot and the `add(so.Line(marker=\\"o\\"))` method adds the line plot with markers to it. - Adjust plot limits using the `limit(x=..., y=...)` method on the plot object. - Handle optional limits by checking if they are `None`.","solution":"import seaborn.objects as so import pandas as pd def custom_plot(x: list, y: list, x_limits: tuple = None, y_limits: tuple = None) -> so.Plot: Generates a seaborn plot with the given x and y data and optional axis limits. Parameters: x (list): List of x-coordinates. y (list): List of y-coordinates. x_limits (tuple, optional): A tuple (min, max) for x-axis limits. y_limits (tuple, optional): A tuple (min, max) for y-axis limits. Returns: so.Plot: A seaborn plot object. # Creating a DataFrame for plotting data = pd.DataFrame({\'x\': x, \'y\': y}) # Initialize the plot plot = so.Plot(data, x=\\"x\\", y=\\"y\\").add(so.Line(marker=\\"o\\")) # Set x and y axis limits if provided if x_limits is not None: plot = plot.limit(x=x_limits) if y_limits is not None: plot = plot.limit(y=y_limits) return plot"},{"question":"**Problem: Custom Mixture Distribution and Sampling** In this problem, you\'ll work with the `torch.distributions` module to implement a custom mixture distribution using several predefined distributions. You will need to use the following distributions: - `Normal` - `Bernoulli` - `Categorical` # Requirements 1. **Creating the Mixture Distribution**: - Implement a class `CustomMixtureDistribution` which takes three parameters: - `normal_params`: A list of tuples, each containing mean and standard deviation for the normal distributions. - `bernoulli_param`: The probability parameter for the Bernoulli distribution. - `weights`: A list of weights for the categorical distribution, which will determine the mixture components. - The class should have: - `__init__` method to initialize the distributions. - `sample` method to generate a sample from the mixture distribution. - `log_prob` method to compute the log probability of a given sample. 2. **Implementing the Methods**: - The `__init__` method should initialize the necessary distributions (`Normal`, `Bernoulli`, and `Categorical`) using the given parameters. - The `sample` method should generate samples correctly considering the weights. - The `log_prob` method should compute the log probability of a given sample correctly by considering the weighted sum of the log probabilities of each component. 3. **Example Usage**: ```python # Define parameters normal_params = [(0, 1), (5, 2), (10, 3)] bernoulli_param = 0.5 weights = [0.2, 0.3, 0.5] # Create the mixture distribution mixture = CustomMixtureDistribution(normal_params, bernoulli_param, weights) # Generate a sample sample = mixture.sample() print(f\'Sample: {sample}\') # Compute log probability of the sample log_prob = mixture.log_prob(sample) print(f\'Log probability of the sample: {log_prob}\') ``` # Constraints - Ensure that your implementation respects the properties of probability distributions (e.g., the weights should sum to 1). - Efficiently handle the sampling process. - Ensure that the log probability computation is accurate and efficient. # Performance - Aim for a solution that can handle a reasonably large number of components (e.g., 10-20) efficiently. - Tests will involve generating samples and computing log probabilities for multiple distributions, so ensure performance is optimized. # Submission Submit your solution as a Python file with the class implementation and an example usage script as shown above. **Note**: Make sure to import the necessary packages (`torch`, `torch.distributions`, etc.) at the top of your submission file.","solution":"import torch import torch.nn.functional as F from torch.distributions import Normal, Bernoulli, Categorical class CustomMixtureDistribution: def __init__(self, normal_params, bernoulli_param, weights): self.normals = [Normal(mean, std) for mean, std in normal_params] self.bernoulli = Bernoulli(bernoulli_param) self.categorical = Categorical(torch.tensor(weights)) def sample(self): cat_sample = self.categorical.sample() # Sample from the categorical distribution bern_sample = self.bernoulli.sample() # Sample from the Bernoulli distribution norm_sample = self.normals[cat_sample].sample() # Sample from the selected normal distribution return bern_sample.item(), norm_sample.item() def log_prob(self, sample): bern_sample, norm_sample = sample bern_log_prob = self.bernoulli.log_prob(torch.tensor(bern_sample)) log_probs = [] for i, norm in enumerate(self.normals): log_prob = norm.log_prob(torch.tensor(norm_sample)) + self.categorical.logits[i] log_probs.append(log_prob) return bern_log_prob + torch.logsumexp(torch.stack(log_probs), dim=0)"},{"question":"**Objective:** Implement a Python function using the `shlex` module to safely parse and join shell-like command strings. **Problem Statement:** You are given a script to parse and join shell command strings using the `shlex` module to prevent command injection vulnerabilities and handle complex command structures. You need to create a function `process_shell_commands` that takes a list of command strings, splits each of them into tokens, and then joins them back into a safe, shell-escaped string. **Description:** 1. **Function Signature:** ```python def process_shell_commands(commands: list) -> list: ``` 2. **Parameters:** - `commands` (list): A list of command strings to be processed, where each command is a string resembling Unix shell syntax. 3. **Returns:** - `list`: A list of processed, shell-escaped command strings after splitting and joining. 4. **Functionality:** - For each command string in the `commands` list: - Use `shlex.split` to tokenize the command. - Use `shlex.join` to join the tokens back into a shell-escaped string. - Return the list of processed command strings. 5. **Constraints:** - Each command string can contain spaces, quotes, and special characters. - Ensure that the function handles both POSIX and non-POSIX use cases correctly. - Handle potential command injection vulnerabilities by using `shlex.quote` where necessary. 6. **Example:** ```python commands = [\\"ls -l \'my file.txt\'\\", \\"echo \\"Hello, World!\\"\\", \\"rm -rf --no-preserve-root /tmp/temp\\"] result = process_shell_commands(commands) print(result) ``` **Output:** ``` [\\"ls -l \'my file.txt\'\\", \\"echo \'Hello, World!\'\\", \\"rm -rf --no-preserve-root /tmp/temp\\"] ``` **Notes:** - You must use `shlex.split`, `shlex.join`, and `shlex.quote` appropriately to ensure the safety and integrity of the parsed commands. - Be mindful of different behaviors in POSIX and non-POSIX modes. - Include sufficient comments and error-handling in your implementation. Happy coding!","solution":"import shlex def process_shell_commands(commands: list) -> list: Processes a list of command strings by splitting and joining them safely using the shlex module to avoid command injection and handle complex command structures. Parameters: commands (list): A list of command strings to be processed. Returns: list: A list of processed, shell-escaped command strings. processed_commands = [] for command in commands: # Split the command into tokens tokens = shlex.split(command, posix=True) # Join the tokens back into a safely escaped command string escaped_command = shlex.join(tokens) processed_commands.append(escaped_command) return processed_commands"},{"question":"# XDR Data Packing and Unpacking Objective Your task is to implement two functions `pack_data` and `unpack_data` using the `xdrlib` module. The `pack_data` function should take a complex data structure and return its XDR encoded representation. The `unpack_data` function should take an XDR encoded string and return the equivalent unpacked data structure. Detailed Specifications 1. **Function 1: `pack_data(data)`** - **Input:** - `data`: A dictionary representing a student record containing the following key-value pairs: - `\\"id\\"`: An integer representing the student id. - `\\"name\\"`: A string representing the student\'s name. - `\\"grades\\"`: A list of integers representing the student\'s grades. - **Output:** - A byte string containing the XDR encoded representation of the input data. 2. **Function 2: `unpack_data(encoded_data)`** - **Input:** - `encoded_data`: A byte string containing the XDR encoded representation of a student record. - **Output:** - A dictionary with the original structure and data as described in the first function. Constraints - The \\"id\\" will be a positive integer (1 <= id <= 10^6). - The \\"name\\" will have a maximum length of 100 characters. - The \\"grades\\" list will contain up to 20 integers, each integer between 0 and 100. Example Data Structure ```python student_record = { \\"id\\": 12345, \\"name\\": \\"Alice\\", \\"grades\\": [88, 92, 79, 85] } ``` Example Usage ```python import xdrlib def pack_data(data): # Your implementation here pass def unpack_data(encoded_data): # Your implementation here pass # Example to test the functions student_record = { \\"id\\": 12345, \\"name\\": \\"Alice\\", \\"grades\\": [88, 92, 79, 85] } # Pack the data encoded_data = pack_data(student_record) # Unpack the data decoded_record = unpack_data(encoded_data) assert decoded_record == student_record ``` Notes - Ensure you handle packing and unpacking of all data types correctly. - Pay attention to padding requirements for strings to ensure 4-byte alignment. - Implement error handling for any potential conversion errors using `xdrlib.ConversionError`.","solution":"import xdrlib def pack_data(data): Packs the given data dictionary into an XDR encoded string. :param data: Dictionary containing the student\'s id, name, and grades :return: Byte string of the XDR encoded data packer = xdrlib.Packer() packer.pack_int(data[\'id\']) packer.pack_string(data[\'name\'].encode()) packer.pack_int(len(data[\'grades\'])) for grade in data[\'grades\']: packer.pack_int(grade) return packer.get_buffer() def unpack_data(encoded_data): Unpacks the given XDR encoded data string into a data dictionary. :param encoded_data: Byte string of the XDR encoded data :return: Dictionary containing the student\'s id, name, and grades unpacker = xdrlib.Unpacker(encoded_data) data = {} data[\'id\'] = unpacker.unpack_int() data[\'name\'] = unpacker.unpack_string().decode() grades_count = unpacker.unpack_int() data[\'grades\'] = [unpacker.unpack_int() for _ in range(grades_count)] return data"},{"question":"**Problem Statement:** You are tasked with building a concurrent command execution utility utilizing Python\'s asyncio and subprocess functionalities. The goal is to create a function that takes a list of shell commands, executes them as subprocesses in parallel, and returns their respective outputs. # Function Signature: ```python import asyncio async def run_commands(commands: List[str]) -> List[Tuple[str, str]]: pass ``` # Detailed Requirements: 1. **Input:** - `commands`: A list of shell commands (strings) to be executed concurrently. 2. **Output:** - Returns a list of tuples where each tuple contains: - The stdout output (as a string). - The stderr output (as a string). 3. **Constraints:** - The function must create subprocesses using `asyncio.create_subprocess_shell`. - It should handle each command\'s standard output (stdout) and error output (stderr) and capture them. - Commands should run concurrently, not sequentially. 4. **Hints:** - Use `asyncio.gather` to run multiple coroutines concurrently. - Use `asyncio.subprocess.PIPE` to capture the output streams. - You may find the `communicate` method of the `Process` class useful to interact with the subprocess streams. # Example: ```python commands = [\\"echo \'Hello, world!\'\\", \\"ls -l\\", \\"invalid_command\\"] output = asyncio.run(run_commands(commands)) # Expected Output (example, may vary): # [ # (\\"Hello, world!n\\", \\"\\"), # (\\"total 0n-rw-r--r-- 1 user user 0 Oct 10 00:00 filen\\", \\"\\"), # (\\"\\", \\"sh: invalid_command: command not foundn\\") # ] ``` # Constraints: - Ensure your implementation is efficient and handles errors gracefully. - Performance is a consideration; ensure that the overall execution time is optimal considering the concurrency. Implement the function `run_commands` to meet the above requirements.","solution":"import asyncio from typing import List, Tuple async def run_command(command: str) -> Tuple[str, str]: process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return (stdout.decode(), stderr.decode()) async def run_commands(commands: List[str]) -> List[Tuple[str, str]]: tasks = [run_command(command) for command in commands] return await asyncio.gather(*tasks)"},{"question":"**Objective:** Implement a PyTorch model that uses TorchScript to handle a specific image processing task and serialize the model for deployment. **Scenario:** You are tasked with implementing a simple neural network to perform image classification. Your model should be designed in PyTorch and then converted to TorchScript for deployment purposes. # Requirements: - Write a PyTorch model class `ImageClassifier` with the following specifications: - It should inherit from `torch.nn.Module`. - The model should consist of at least one convolutional layer, one fully connected layer, and one activation function. - The model should include a method `forward` that defines the forward pass. - Write a script to convert the PyTorch model to TorchScript using `torch.jit.script`. - Save the TorchScript model to a file named `image_classifier.pt`. # Constraints: 1. **Input format:** The input to the model will be a 3x32x32 tensor representing an image (Batch size can be 1). Ensure that your model can handle this input format. 2. **Output format:** The output should be a tensor with the predicted class scores for each input image. 3. **Efficiency:** The model should be efficient, leveraging PyTorch and TorchScript to ensure fast inference. # Example usage: ```python import torch import torch.nn as nn import torch.nn.functional as F class ImageClassifier(nn.Module): def __init__(self): super(ImageClassifier, self).__init__() # Example architecture, you can modify it self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(16*32*32, 10) # Assuming output classes are 10 def forward(self, x): x = F.relu(self.conv1(x)) x = x.view(x.size(0), -1) # Flatten the tensor x = self.fc1(x) return x def save_torchscript_model(model, file_path): # Convert to TorchScript scripted_model = torch.jit.script(model) # Save the TorchScript model scripted_model.save(file_path) # Define and save the model model = ImageClassifier() save_torchscript_model(model, \\"image_classifier.pt\\") ``` # Submission Requirements: 1. The `.py` file containing the `ImageClassifier` class and the function to save the TorchScript model. 2. A brief description of your model architecture and the steps to successfully run your code.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class ImageClassifier(nn.Module): def __init__(self): super(ImageClassifier, self).__init__() # Define the layers of the model self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) self.fc1 = nn.Linear(16 * 32 * 32, 10) # Assuming 10 output classes def forward(self, x): # Define the forward pass x = F.relu(self.conv1(x)) x = x.view(x.size(0), -1) # Flatten the tensor x = self.fc1(x) return x def save_torchscript_model(model, file_path): # Convert to TorchScript scripted_model = torch.jit.script(model) # Save the TorchScript model scripted_model.save(file_path) # Define and save the model model = ImageClassifier() save_torchscript_model(model, \\"image_classifier.pt\\")"},{"question":"Problem Statement You are required to implement a function using pandas that processes a DataFrame containing various types of data including datetimes, intervals, categoricals, and nullable integers. The DataFrame represents a dataset where information is mixed and needs to be appropriately managed and queried. Implement a function `process_mixed_df(df: pd.DataFrame) -> pd.DataFrame` that takes in a DataFrame and performs the following operations: 1. **Fill missing datetime values**: - Convert all columns with naive datetime to timezone-aware datetime (UTC). - For columns that are timezone-aware, convert them all to the same timezone (UTC). - Fill any missing datetime values with the current datetime in UTC. 2. **Handle categorical data**: - Convert all categorical columns into ordered categorical columns based on their unique values. - Remove any unused categories from these categorical columns. 3. **Process nullable integer columns**: - Identify all nullable integer columns and fill any missing values with the median of that column. - Convert these columns to standard integers (if they do not have any missing values after the median-fill). 4. **Interval data conversion**: - Convert any interval columns into their mid-point numeric representation. 5. **Return the processed DataFrame** with operations applied as described. # Input - `df` - a pandas DataFrame containing mixed data types. # Output - A pandas DataFrame with the same columns but processed according to the aforementioned steps. # Constraints - All datetime operations should respect timezone awareness. - There should be no remaining missing values in the resultant DataFrame. # Example ```python import pandas as pd import numpy as np from pandas.api.types import CategoricalDtype # Example DataFrame data = { \\"datetime_naive\\": [pd.NaT, pd.Timestamp(\\"2021-01-01\\"), pd.Timestamp(\\"2021-03-01\\")], \\"datetime_aware\\": [pd.NaT, pd.Timestamp(\\"2021-01-01\\", tz=\\"US/Eastern\\"), pd.Timestamp(\\"2021-03-01\\", tz=\\"US/Eastern\\")], \\"categorical_col\\": pd.Categorical([\\"A\\", \\"B\\", \\"A\\"], categories=[\\"A\\", \\"B\\", \\"C\\"]), \\"nullable_int\\": pd.Series([1, pd.NA, 3], dtype=\\"Int64\\"), \\"interval_col\\": pd.IntervalIndex.from_tuples([(0, 1), (1, 2), (2, 3)]) } df = pd.DataFrame(data) # Implement the function def process_mixed_df(df: pd.DataFrame) -> pd.DataFrame: # Handling datetime conversion for col in df.select_dtypes(include=[\'datetime64[ns, UTC]\', \'datetime64[ns]\']): if df[col].dt.tz is not None: df[col] = df[col].dt.tz_convert(\'UTC\') else: df[col] = df[col].dt.tz_localize(\'UTC\') df[col].fillna(pd.Timestamp.now(tz=\'UTC\'), inplace=True) # Handling categorical conversion for col in df.select_dtypes(include=\'category\'): df[col] = df[col].astype(CategoricalDtype(ordered=True)) df[col] = df[col].cat.remove_unused_categories() # Handling nullable integer columns for col in df.select_dtypes(include=[\'Int8\', \'Int16\', \'Int32\', \'Int64\', \'UInt8\', \'UInt16\', \'UInt32\', \'UInt64\']): df[col].fillna(df[col].median(), inplace=True) if df[col].isna().sum() == 0: df[col] = df[col].astype(int) # Handling the interval data for col in df.select_dtypes(include=\'interval\'): df[col] = df[col].apply(lambda x: x.mid) return df # Process the DataFrame processed_df = process_mixed_df(df) print(processed_df) ```","solution":"import pandas as pd import numpy as np from pandas.api.types import CategoricalDtype def process_mixed_df(df: pd.DataFrame) -> pd.DataFrame: # Handling datetime conversion for col in df.select_dtypes(include=[\'datetime64[ns, UTC]\', \'datetime64[ns]\']).columns: df[col] = pd.to_datetime(df[col], utc=True) df[col].fillna(pd.Timestamp.now(tz=\'UTC\'), inplace=True) # Handling categorical conversion for col in df.select_dtypes(include=\'category\').columns: df[col] = df[col].astype(CategoricalDtype(ordered=True)) df[col] = df[col].cat.remove_unused_categories() # Handling nullable integer columns for col in df.select_dtypes(include=\'integer\').columns: if pd.api.types.is_integer_dtype(df[col]): df[col].fillna(df[col].median(), inplace=True) if df[col].isna().sum() == 0: df[col] = df[col].astype(int) # Handling the interval data for col in df.select_dtypes(include=\'interval\').columns: df[col] = df[col].apply(lambda x: x.mid) return df"},{"question":"You are given a dataset containing various types of numeric measurements that need to be processed efficiently. Each type of data has a specific type code associated with it. You are required to implement a class `DataProcessor` that offers functionalities to store, manipulate, and retrieve this numeric data using the `array` module. Your `DataProcessor` class should provide the following functionalities: 1. **Initialization**: Initialize the class with a type code and an optional initializer that can be a list, bytes-like object, or an iterable. 2. **Add Data**: Method to add a single item to the array. 3. **Remove Data**: Method to remove the first occurrence of an item from the array. 4. **Get Data**: Method to get the item at a specified index. 5. **Count Data**: Method to count the occurrences of a specified value. 6. **Reverse Data**: Method to reverse the array. 7. **To List**: Method to convert the array to a list. 8. **Sum Data**: Method to return the sum of all items in the array. Here\'s the class signature: ```python class DataProcessor: def __init__(self, typecode: str, initializer=None): pass def add_data(self, item): pass def remove_data(self, item): pass def get_data(self, index: int): pass def count_data(self, item) -> int: pass def reverse_data(self): pass def to_list(self) -> list: pass def sum_data(self) -> float: pass ``` # Input 1. `typecode`: A single character representing the type code (e.g., \'i\' for signed int). 2. `initializer`: A list, bytes-like object, or iterable over the appropriate type. # Constraints - The type code will always be valid as per the `array` module specifications. - The methods needing index will always get valid index positions. - The remove method will always get an item that exists in the array. # Output - Each method should perform its task as described without returning values unless specified otherwise (e.g., `get_data`, `count_data`, `to_list`, `sum_data`). # Example ```python processor = DataProcessor(\'i\', [1, 2, 3, 4]) processor.add_data(5) print(processor.to_list()) # Output: [1, 2, 3, 4, 5] processor.remove_data(3) print(processor.to_list()) # Output: [1, 2, 4, 5] print(processor.get_data(2)) # Output: 4 print(processor.count_data(4)) # Output: 1 processor.reverse_data() print(processor.to_list()) # Output: [5, 4, 2, 1] print(processor.sum_data()) # Output: 12 ``` # Your Task Complete the implementation of the `DataProcessor` class so it behaves as described. Make sure to use the `array` module to leverage its efficient handling of numeric array data.","solution":"import array class DataProcessor: def __init__(self, typecode: str, initializer=None): self.typecode = typecode self.data = array.array(typecode, initializer if initializer is not None else []) def add_data(self, item): self.data.append(item) def remove_data(self, item): self.data.remove(item) def get_data(self, index: int): return self.data[index] def count_data(self, item) -> int: return self.data.count(item) def reverse_data(self): self.data.reverse() def to_list(self) -> list: return self.data.tolist() def sum_data(self) -> float: return sum(self.data)"},{"question":"# Question: **Objective:** Write a Python script that utilizes the seaborn library to analyze and visualize the distribution of a dataset using kernel density estimation with various customizations. **Problem Statement:** Given a dataset `penguins` (which can be loaded using `sns.load_dataset(\\"penguins\\")`), perform the following tasks: 1. **Univariate Distribution Plot:** - Plot the KDE of the `bill_length_mm` feature. 2. **Bivariate Distribution Plot:** - Plot the KDE of `bill_length_mm` against `bill_depth_mm`. - Color the plot by `species` using the `hue` parameter. - Fill the contours for better visualization. 3. **Conditional Distributions:** - Plot the KDE of `flipper_length_mm` conditional on the `island`. Use `multiple=\\"stack\\"` to stack these distributions. 4. **Appearance Modification:** - In any of the KDE plots above, modify the plot to use a different color palette (`coolwarm`) and add appropriate labels to the axes and title to the plot. **Example Output:** - Your code should generate three plots corresponding to the three tasks mentioned above. - Each plot should be fully labeled with x and y-axis labels and a title that describes the plot. **Implementation Constraints:** - You must use the seaborn library for all visualizations. - The generated plots should be displayed inline (if executing in a Jupyter environment). - Ensure that your code is modular and functions are used to generate each plot. **Hints:** - Refer to the seaborn documentation on `sns.kdeplot` for additional parameters and customization options. - Example datasets can be loaded using `sns.load_dataset(\\"dataset_name\\")`. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") def plot_univariate_distribution(data): sns.kdeplot(data=data, x=\\"bill_length_mm\\") plt.xlabel(\\"Bill Length (mm)\\") plt.title(\\"Univariate KDE of Bill Length\\") plt.show() def plot_bivariate_distribution(data): sns.kdeplot(data=data, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", fill=True) plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.title(\\"Bivariate KDE of Bill Length and Depth by Species\\") plt.show() def plot_conditional_distribution(data): sns.kdeplot(data=data, x=\\"flipper_length_mm\\", hue=\\"island\\", multiple=\\"stack\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.title(\\"Conditional KDE of Flipper Length by Island\\") plt.show() # Call the functions to generate plots plot_univariate_distribution(penguins) plot_bivariate_distribution(penguins) plot_conditional_distribution(penguins) ``` **Submission:** Submit your Python script that includes the function definitions and calls to generate the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") def plot_univariate_distribution(data): sns.kdeplot(data=data, x=\\"bill_length_mm\\", palette=\\"coolwarm\\") plt.xlabel(\\"Bill Length (mm)\\") plt.title(\\"Univariate KDE of Bill Length\\") plt.show() def plot_bivariate_distribution(data): sns.kdeplot(data=data, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", fill=True, palette=\\"coolwarm\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.title(\\"Bivariate KDE of Bill Length and Depth by Species\\") plt.show() def plot_conditional_distribution(data): sns.kdeplot(data=data, x=\\"flipper_length_mm\\", hue=\\"island\\", multiple=\\"stack\\", palette=\\"coolwarm\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.title(\\"Conditional KDE of Flipper Length by Island\\") plt.show() # Call the functions to generate plots plot_univariate_distribution(penguins) plot_bivariate_distribution(penguins) plot_conditional_distribution(penguins)"},{"question":"Problem Statement You are required to create a Python function that analyzes an installed package and returns detailed information about it. The function should use the `importlib.metadata` module to gather and return the following information about the package: 1. Package name. 2. Package version. 3. Package summary. 4. Author of the package. 5. License of the package. 6. List of all core entry points in the `console_scripts` group (if any). 7. List of all files installed by the package. 8. List of all packages required by the package. Function Signature ```python def analyze_package(package_name: str) -> dict: pass ``` Input - `package_name` (str): The name of the package to analyze. Output - Returns a dictionary with the following structure: ```python { \\"name\\": str, \\"version\\": str, \\"summary\\": str, \\"author\\": str, \\"license\\": str, \\"console_scripts\\": list, \\"files\\": list, \\"requirements\\": list } ``` Example Given the package name `wheel`, your function should return something like: ```python { \\"name\\": \\"wheel\\", \\"version\\": \\"0.32.3\\", \\"summary\\": \\"A built-package format for Python.\\", \\"author\\": \\"Daniel Holth\\", \\"license\\": \\"MIT\\", \\"console_scripts\\": [\\"wheel\\"], \\"files\\": [\\"wheel/bdist_wheel.py\\", \\"wheel/cli/__init__.py\\", ...], \\"requirements\\": [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] } ``` Constraints 1. The function should handle cases where a particular metadata field or section might be missing gracefully, returning an empty string `\\"\\"` or an empty list `[]` as appropriate. 2. You can assume that the package name provided is valid and installed on the system where this function is being executed. Performance Requirements The function should be efficient in metadata lookup and should handle the package metadata retrieval without significant delay. Implementation Notes - Use the appropriate functions from the `importlib.metadata` module as described in the documentation. - Validate and handle missing metadata fields properly. - Make effective use of the `distribution()`, `metadata()`, `version()`, `entry_points()`, `files()`, and `requires()` functions.","solution":"from importlib.metadata import distribution, PackageNotFoundError def analyze_package(package_name: str) -> dict: try: dist = distribution(package_name) package_metadata = dist.metadata package_files = dist.files package_entry_points = dist.entry_points # Extract information from metadata name = package_metadata.get(\'Name\', \'\') version = package_metadata.get(\'Version\', \'\') summary = package_metadata.get(\'Summary\', \'\') author = package_metadata.get(\'Author\', \'\') license = package_metadata.get(\'License\', \'\') # Get list of files files = [str(f) for f in package_files] if package_files else [] # Get console_scripts entry points console_scripts = [ep.name for ep in package_entry_points if ep.group == \'console_scripts\'] if package_entry_points else [] # Get requirements requirements = dist.requires or [] return { \\"name\\": name, \\"version\\": version, \\"summary\\": summary, \\"author\\": author, \\"license\\": license, \\"console_scripts\\": console_scripts, \\"files\\": files, \\"requirements\\": requirements } except PackageNotFoundError: return { \\"name\\": \\"\\", \\"version\\": \\"\\", \\"summary\\": \\"\\", \\"author\\": \\"\\", \\"license\\": \\"\\", \\"console_scripts\\": [], \\"files\\": [], \\"requirements\\": [] }"},{"question":"**Question: Using Python Development Mode and Resource Handling** You are tasked with writing a Python script that performs the following actions: 1. Reads a given text file and counts the number of lines in it. 2. Fetches data from a mock web API using asyncio to demonstrate coroutine handling. You need to: - Ensure that the file reading is done in a manner that does not raise a `ResourceWarning` when run under Python Development Mode. - Use `asyncio` to fetch data concurrently from five different mock URLs. - Ensure that the coroutine handling is done correctly to avoid `DeprecationWarning` related to unawaited coroutines. Your script should: 1. Read the file path from the command line. 2. Use a context manager to read the file and count the lines. 3. Define an `asyncio` coroutine that simulates fetching data from a URL (you can mock this by using `asyncio.sleep`). 4. Use `asyncio.gather` to fetch data concurrently from multiple URLs. 5. Print the number of lines in the file and the results of the mock fetch operations. **Constraints:** - You must handle the file resource properly to avoid `ResourceWarnings`. - Properly await all coroutines to avoid `DeprecationWarnings`. **Input and Output:** - Input: A text file (specified via the command line) and mock URLs defined in your script. - Output: Number of lines in the text file and results of the mock fetch operations. **Example:** Consider the URLs to fetch data from are given as a list in the script: ```python urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\", \\"http://example.com/4\\", \\"http://example.com/5\\"] ``` If the given text file \\"data.txt\\" has 100 lines, when you run with the command: ```sh python3 script.py data.txt ``` The expected output should be: ``` Number of lines in the file: 100 Fetched data from URL http://example.com/1 ... Fetched data from URL http://example.com/5 ``` **Your task: Implement the script following the described behavior.** *Hints:* - Utilize `asyncio.run` to run the main entry point for asyncio code. - Use `aiohttp` library to simulate data fetching from URLs if you want more realism.","solution":"import asyncio import aiohttp import sys def count_lines_in_file(file_path): Counts the number of lines in a given file. Args: file_path (str): The path to the file. Returns: int: The number of lines in the file. with open(file_path, \\"r\\") as file: return sum(1 for _ in file) async def fetch_data(session, url): Simulates fetching data from a URL. For simplicity, we use asyncio sleep. Args: session (aiohttp.ClientSession): The session to perform the request. url (str): The URL to fetch data from. Returns: str: Simulated response data. async with session.get(url) as response: await asyncio.sleep(1) # Simulate network delay return f\\"Fetched data from URL {url}\\" async def main(file_path): # Step 1: Count the number of lines in the file line_count = count_lines_in_file(file_path) print(f\\"Number of lines in the file: {line_count}\\") # Step 2: Simulate concurrently fetching data from URLs urls = [ \\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\", \\"http://example.com/4\\", \\"http://example.com/5\\" ] async with aiohttp.ClientSession() as session: tasks = [fetch_data(session, url) for url in urls] results = await asyncio.gather(*tasks) for result in results: print(result) if __name__ == \\"__main__\\": if len(sys.argv) < 2: print(\\"Usage: python script.py <file_path>\\") sys.exit(1) file_path = sys.argv[1] asyncio.run(main(file_path))"},{"question":"# XML Data Manipulation and Analysis You are given an XML file named `country_data.xml` with the following structure: ```xml <?xml version=\\"1.0\\"?> <data> <country name=\\"Liechtenstein\\"> <rank>1</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank>4</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> <country name=\\"Panama\\"> <rank>68</rank> <year>2011</year> <gdppc>13600</gdppc> <neighbor name=\\"Costa Rica\\" direction=\\"W\\"/> <neighbor name=\\"Colombia\\" direction=\\"E\\"/> </country> </data> ``` Your task is to implement a set of functions to perform the following: 1. Parse the given XML file and return the root element. 2. Extract and print: - The names of all countries. - The GDP per capita of a given country. 3. Modify the XML such that you: - Remove all countries with a rank higher than a given threshold. - Add a new country with the specified details. 4. Write the modified XML to a new file. # Function Specifications 1. **parse_xml(file_path)**: - **Input**: `file_path` (str) - Path to the XML file. - **Output**: `Element` - The root element of the parsed XML tree. 2. **get_country_names(root)**: - **Input**: `root` (Element) - Root element of the XML tree. - **Output**: `List[str]` - List of country names. 3. **get_gdppc(root, country_name)**: - **Input**: - `root` (Element) - Root element of the XML tree. - `country_name` (str) - The name of the country. - **Output**: `int` - The GDP per capita of the specified country. 4. **remove_countries_with_high_rank(root, rank_threshold)**: - **Input**: - `root` (Element) - Root element of the XML tree. - `rank_threshold` (int) - Rank threshold for removal. - **Output**: `None` - **Behavior**: Remove countries with a rank higher than the rank threshold. 5. **add_country(root, name, rank, year, gdppc, neighbors)**: - **Input**: - `root` (Element) - Root element of the XML tree. - `name` (str) - Name of the new country. - `rank` (int) - Rank of the new country. - `year` (int) - Year associated with the new country. - `gdppc` (int) - GDP per capita of the new country. - `neighbors` (List[Tuple[str, str]]) - List of neighbors as tuples (name, direction). - **Output**: `None` - **Behavior**: Add a new country with the specified details to the XML tree. 6. **write_xml(tree, file_path)**: - **Input**: - `tree` (ElementTree) - The `ElementTree` object representing the XML tree. - `file_path` (str) - Path to the output XML file. - **Output**: `None` - **Behavior**: Write the XML tree to the specified file. # Example Usage ```python # Parse the XML file root = parse_xml(\'country_data.xml\') # Get and print all country names country_names = get_country_names(root) print(country_names) # Get and print GDP per capita for \\"Singapore\\" singapore_gdppc = get_gdppc(root, \'Singapore\') print(singapore_gdppc) # Remove countries with a rank higher than 50 remove_countries_with_high_rank(root, 50) # Add a new country \\"Atlantis\\" with specified details add_country(root, \'Atlantis\', 2, 2021, 50000, [(\'Utopia\', \'N\'), (\'ElDorado\', \'E\')]) # Write the modified XML to a new file tree = ET.ElementTree(root) write_xml(tree, \'modified_country_data.xml\') ``` # Constraints 1. Assume the input XML file format is correct and does not need validation. 2. Countries have unique names; no two countries will have the same name. 3. You should handle cases where the specified country might not be found in `get_gdppc`. # Performance - Your solution should efficiently handle XML files with up to 1000 countries.","solution":"import xml.etree.ElementTree as ET def parse_xml(file_path): Parse the XML file and return the root element. tree = ET.parse(file_path) return tree.getroot() def get_country_names(root): Extract and return the names of all countries. return [country.attrib[\'name\'] for country in root.findall(\'country\')] def get_gdppc(root, country_name): Extract and return the GDP per capita of a given country. for country in root.findall(\'country\'): if country.attrib[\'name\'] == country_name: return int(country.find(\'gdppc\').text) return None def remove_countries_with_high_rank(root, rank_threshold): Remove all countries with a rank higher than the given threshold. for country in root.findall(\'country\'): if int(country.find(\'rank\').text) > rank_threshold: root.remove(country) def add_country(root, name, rank, year, gdppc, neighbors): Add a new country with the specified details. new_country = ET.SubElement(root, \'country\', name=name) ET.SubElement(new_country, \'rank\').text = str(rank) ET.SubElement(new_country, \'year\').text = str(year) ET.SubElement(new_country, \'gdppc\').text = str(gdppc) for neighbor_name, direction in neighbors: ET.SubElement(new_country, \'neighbor\', name=neighbor_name, direction=direction) def write_xml(tree, file_path): Write the XML tree to the specified file. tree.write(file_path)"},{"question":"# Pandas Sparse Data Structures Problem Statement You are provided with a large dataset containing mostly missing (NaN) values. Your task is to efficiently store this dataset using sparse data structures. Additionally, you need to perform some operations on this sparse data and convert between dense and sparse formats. Function Signature ```python def analyze_sparse_data(data: List[List[float]]) -> Tuple[pd.DataFrame, pd.DataFrame, float, float]: Analyzes sparse data by converting it to sparse format, performing operations, and converting back to dense format. Parameters: data (List[List[float]]): A 2D list representing the dataset with possible NaN values. Returns: Tuple[pd.DataFrame, pd.DataFrame, float, float]: A tuple containing: - The original dense DataFrame. - The DataFrame converted to sparse format. - The memory usage of the dense DataFrame (in bytes). - The memory usage of the sparse DataFrame (in bytes). pass ``` Explanation 1. **Input:** - `data`: A 2D list of floats representing the dataset. It may contain NaN values. 2. **Output:** - A tuple containing: - The original dense DataFrame created from the input data. - The DataFrame converted to a sparse format using `pd.SparseDtype`. - The memory usage of the dense DataFrame (in bytes). - The memory usage of the sparse DataFrame (in bytes). 3. **Steps to follow:** - Create a dense DataFrame from the input list `data`. - Convert this DataFrame to a sparse DataFrame using the `pd.SparseDtype`. - Calculate the memory usage of the dense DataFrame. - Calculate the memory usage of the sparse DataFrame. - Return the tuple as described in the output format. Constraints - The input list `data` can be large, with up to 10000 rows and 100 columns. - You should handle NaN values properly when creating the sparse DataFrame. - Ensure that the memory calculations are accurate and displayed in bytes. Example ```python data = [ [1.0, NaN, NaN, NaN], [NaN, NaN, 3.0, NaN], [NaN, NaN, NaN, NaN], [4.0, NaN, NaN, 5.0] ] dense_df, sparse_df, dense_memory, sparse_memory = analyze_sparse_data(data) print(\\"Dense DataFrame:n\\", dense_df) print(\\"Sparse DataFrame:n\\", sparse_df) print(\\"Memory usage of dense DataFrame:\\", dense_memory, \\"bytes\\") print(\\"Memory usage of sparse DataFrame:\\", sparse_memory, \\"bytes\\") ``` Output: ``` Dense DataFrame: 0 1 2 3 0 1.0 NaN NaN NaN 1 NaN NaN 3.0 NaN 2 NaN NaN NaN NaN 3 4.0 NaN NaN 5.0 Sparse DataFrame: 0 1 2 3 0 1.0 NaN NaN NaN 1 NaN NaN 3.0 NaN 2 NaN NaN NaN NaN 3 4.0 NaN NaN 5.0 Memory usage of dense DataFrame: 192 bytes Memory usage of sparse DataFrame: 48 bytes ``` You may assume that `NaN` values are represented using `numpy.nan` and ensure you import pandas as `pd` and numpy as `np`. Note This question tests your understanding of handling sparse data structures, memory usage optimization, and conversions between different data formats in pandas.","solution":"import pandas as pd import numpy as np from typing import List, Tuple def analyze_sparse_data(data: List[List[float]]) -> Tuple[pd.DataFrame, pd.DataFrame, float, float]: Analyzes sparse data by converting it to sparse format, performing operations, and converting back to dense format. Parameters: data (List[List[float]]): A 2D list representing the dataset with possible NaN values. Returns: Tuple[pd.DataFrame, pd.DataFrame, float, float]: A tuple containing: - The original dense DataFrame. - The DataFrame converted to sparse format. - The memory usage of the dense DataFrame (in bytes). - The memory usage of the sparse DataFrame (in bytes). # Create dense DataFrame from the input data dense_df = pd.DataFrame(data) # Create sparse DataFrame from the dense DataFrame sparse_df = dense_df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Calculate memory usage for the dense DataFrame dense_memory = dense_df.memory_usage(deep=True).sum() # Calculate memory usage for the sparse DataFrame sparse_memory = sparse_df.memory_usage(deep=True).sum() return dense_df, sparse_df, dense_memory, sparse_memory"},{"question":"# Question: Advanced Seaborn Plot Customization with Dodge Transform You are given a dataset containing information about daily tips collected from a restaurant. Your goal is to create a series of bar plots to visualize different aspects of this dataset using seaborn\'s `Plot` API and the `Dodge` transform. This will demonstrate your ability to handle overlapping marks, adjust spacing, and combine multiple transforms. **Dataset:** `tips` (columns: \\"total_bill\\", \\"tip\\", \\"sex\\", \\"smoker\\", \\"day\\", \\"time\\", \\"size\\") **Tasks:** 1. **Basic Dodge:** - Create a bar plot showing the count of tips received each day, colored by the time of day. Use `Dodge` so that bars for lunch and dinner do not overlap. ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) p = so.Plot(tips, \\"day\\", color=\\"time\\") p.add(so.Bar(), so.Count(), so.Dodge()) p.show() ``` 2. **Handling Empty Space:** - Modify the bar plot to fill out any empty spaces where variables are not fully crossed using the `empty` parameter of `Dodge`. ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) p = so.Plot(tips, \\"day\\", color=\\"time\\") p.add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) p.show() ``` 3. **Adding Gaps:** - Create a new bar plot showing the sum of total bills for each day of the week, colored by sex. Add a small gap between the dodged bars using the `gap` parameter. ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) p.show() ``` 4. **Dodging Multiple Variables:** - Create a dot plot showing the distribution of the `total_bill` for each day, with dodging applied for both the `sex` and `smoker` variables. ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Dot(), so.Dodge(), fill=\\"smoker\\") p.show() ``` 5. **Combining Transforms:** - Create a dot plot where points are jittered and dodged by the `smoker` variable, and colors represent the `sex` variable. ```python import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Dot(), so.Dodge(by=[\\"smoker\\"]), so.Jitter()) p.show() ``` **Constraints:** - Use the seaborn `Plot` API. - Apply the `Dodge` transform to handle overlapping marks. - Use appropriate parameters as instructed to control the visualization. Ensure your plots are clear, well-labeled, and correctly display the required transformations. **Grading Criteria:** - Correct usage of the `Dodge` transform in different scenarios. - Ability to handle empty spaces and add gaps between bars. - Proper dodging of multiple semantic variables. - Effective combination of multiple transforms. - Overall clarity and readability of the plots.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) def create_basic_dodge_plot(): p = so.Plot(tips, \\"day\\", color=\\"time\\") p.add(so.Bar(), so.Count(), so.Dodge()) return p def create_filled_dodge_plot(): p = so.Plot(tips, \\"day\\", color=\\"time\\") p.add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) return p def create_gapped_sum_plot(): p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) return p def create_multi_variable_dodge_plot(): p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Dot(), so.Dodge(), fill=\\"smoker\\") return p def create_combined_transform_plot(): p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") p.add(so.Dot(), so.Dodge(by=[\\"smoker\\"]), so.Jitter()) return p"},{"question":"**Unicode String Manipulation and Encoding** # Objective: Write a Python function that reads a Unicode string, performs certain transformations, and encodes the resulting string in a specified encoding format. # Function Signature: ```python def process_unicode_string(input_string: str, encoding: str, transformations: list) -> bytes: Transforms and encodes a Unicode string. Parameters: - input_string (str): The input Unicode string to be processed. - encoding (str): The encoding format in which the output should be returned. - transformations (list): A list of transformations to apply to the input string. Each transformation is represented as a dictionary with the keys \'type\' and \'params\'. \'type\' indicates the type of transformation (e.g., \\"replace\\", \\"upper\\", \\"lower\\", \\"title\\"), and \'params\' contains any parameters needed for the transformation. Returns: - bytes: The transformed string encoded in the specified encoding format. Raises: - ValueError: If an unsupported transformation type is provided. - UnicodeEncodeError: If the encoding process fails. pass ``` # Requirements: 1. The function should apply the transformations in the order they appear in the list. 2. Supported transformation types include: - `\\"replace\\"`: Replaces all occurrences of a substring with another substring. Requires \'old\' and \'new\' parameters. - `\\"upper\\"`: Converts the string to uppercase. No additional parameters required. - `\\"lower\\"`: Converts the string to lowercase. No additional parameters required. - `\\"title\\"`: Converts the string to title case. No additional parameters required. 3. After applying the transformations, encode the resulting string using the specified encoding format. 4. Raise appropriate errors if the transformations or encoding processes fail. # Example Usage: ```python input_string = \\"Hello, World!\\" encoding = \\"utf-8\\" transformations = [ {\\"type\\": \\"replace\\", \\"params\\": {\\"old\\": \\"World\\", \\"new\\": \\"Universe\\"}}, {\\"type\\": \\"upper\\"} ] result = process_unicode_string(input_string, encoding, transformations) print(result) # Output: b\'HELLO, UNIVERSE!\' ``` # Constraints: - The input string will be a valid Unicode string. - The encoding will be a valid Python-supported encoding format. - The transformations list will only contain valid dictionaries with the required keys and values. # Performance Requirements: The implemented function should handle the processing efficiently, even for strings that are several thousand characters long. # Notes: - Utilize proper error handling to catch issues related to invalid transformations or encoding processes. - Make use of built-in Python string methods for transformations and encoding where necessary. Implement this function in a way that showcases an understanding of Unicode string manipulation and encoding in Python.","solution":"def process_unicode_string(input_string: str, encoding: str, transformations: list) -> bytes: Transforms and encodes a Unicode string. Parameters: - input_string (str): The input Unicode string to be processed. - encoding (str): The encoding format in which the output should be returned. - transformations (list): A list of transformations to apply to the input string. Each transformation is represented as a dictionary with the keys \'type\' and \'params\'. \'type\' indicates the type of transformation (e.g., \\"replace\\", \\"upper\\", \\"lower\\", \\"title\\"), and \'params\' contains any parameters needed for the transformation. Returns: - bytes: The transformed string encoded in the specified encoding format. Raises: - ValueError: If an unsupported transformation type is provided. - UnicodeEncodeError: If the encoding process fails. for transform in transformations: t_type = transform[\'type\'] params = transform.get(\'params\', {}) if t_type == \\"replace\\": old = params[\'old\'] new = params[\'new\'] input_string = input_string.replace(old, new) elif t_type == \\"upper\\": input_string = input_string.upper() elif t_type == \\"lower\\": input_string = input_string.lower() elif t_type == \\"title\\": input_string = input_string.title() else: raise ValueError(f\\"Unsupported transformation type: {t_type}\\") try: encoded_string = input_string.encode(encoding) except UnicodeEncodeError as e: raise e return encoded_string"},{"question":"Quantization Backend Configuration Objective Demonstrate your understanding of quantization backend configurations in PyTorch by implementing a function that configures a given model for a specified quantization backend, ensuring that it matches the backend\'s specifications. Problem Statement Implement a function `configure_quantization_backend(model, backend)` that takes in a PyTorch model and a backend string, configures the model\'s quantization parameters according to the specifications of the provided backend (either \'x86\' or \'qnnpack\'), and returns the quantized model. The `backend` argument will be a string that specifies the backend for which the model needs to be configured. The function should handle two backends: \'x86\' and \'qnnpack\'. The function should correctly configure layers in the model for quantization per the specified backend. Input - `model`: A PyTorch neural network model (instance of `torch.nn.Module`) that needs to be quantized. - `backend`: A string that can be either \'x86\' or \'qnnpack\', indicating the target backend for quantization. Output - A PyTorch model configured for quantization with respect to the specified backend. (instance of `torch.nn.Module`). Function Signature ```python def configure_quantization_backend(model: torch.nn.Module, backend: str) -> torch.nn.Module: pass ``` Constraints 1. The function should support only \'x86\' and \'qnnpack\' backends. For any other backend string, it should raise a `ValueError`. 2. You may assume that the provided model is a valid PyTorch model suitable for quantization. 3. Ensure that the function is extensible for potential future backends with minimal changes. Example ```python import torch import torch.nn as nn from torch.quantization import quantize_dynamic # Example model definition class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 5) def forward(self, x): return self.fc(x) # Dummy model instance model_instance = SimpleModel() # Configure the model for x86 backend try: quantized_model_x86 = configure_quantization_backend(model_instance, \'x86\') print(\\"x86 backend quantization successful\\") except ValueError as e: print(e) # Configure the model for qnnpack backend try: quantized_model_qnnpack = configure_quantization_backend(model_instance, \'qnnpack\') print(\\"qnnpack backend quantization successful\\") except ValueError as e: print(e) ``` This code should configure the `SimpleModel` for the specified backend and print success messages if quantization is successful. Notes - You may refer to the PyTorch documentation for details on quantization: [PyTorch Quantization Documentation](https://pytorch.org/docs/stable/quantization.html) - Initialize quantization parameters using default values provided in configurations for \'x86\' and \'qnnpack\' backends.","solution":"import torch import torch.nn as nn import torch.quantization as quantization def configure_quantization_backend(model: nn.Module, backend: str) -> nn.Module: Configures a given model for a specified quantization backend and returns the quantized model. Args: model (nn.Module): A PyTorch neural network model that needs to be quantized. backend (str): The target quantization backend. Must be either \'x86\' or \'qnnpack\'. Returns: nn.Module: The quantized model configured for the specified backend. Raises: ValueError: If the backend is not one of \'x86\' or \'qnnpack\'. if backend not in [\'x86\', \'qnnpack\']: raise ValueError(f\\"Unsupported backend: {backend}. Supported backends: \'x86\', \'qnnpack\'\\") if backend == \'x86\': model.qconfig = quantization.get_default_qconfig(\'fbgemm\') elif backend == \'qnnpack\': model.qconfig = quantization.get_default_qconfig(\'qnnpack\') # Prepare the model for quantization model = quantization.prepare(model, inplace=True) # Convert the model to a quantized model model = quantization.convert(model, inplace=True) return model"},{"question":"# Python Coding Assessment Question Objective Demonstrate understanding of Python\'s functional programming modules (`itertools` and `functools`) by implementing a solution that utilizes iterators and partial function application. Problem Statement Implement a function `prime_generator_partial_sum` that generates prime numbers and returns the running sum of a specified count of prime numbers using partial function application and efficient looping iterators. Function Signature ```python def prime_generator_partial_sum(n: int) -> List[int]: Generate a specified count of prime numbers and return the running sum of these prime numbers. :param n: The number of primes to generate and sum. :type n: int :return: A list containing the running sum of the first `n` primes. :rtype: List[int] ``` Input - An integer `n` (1 ≤ n ≤ 10^5) representing the number of prime numbers to generate and sum. Output - A list of integers where each element `i` represents the sum of the first `i+1` prime numbers. Example ```python assert prime_generator_partial_sum(5) == [2, 5, 10, 17, 28] ``` Explanation: - The first 5 prime numbers are: 2, 3, 5, 7, 11 - Running sums: [2, (2+3)=5, (2+3+5)=10, (2+3+5+7)=17, (2+3+5+7+11)=28] Constraints - Your implementation should leverage `itertools` for efficient looping and `functools.partial` for function applications designed to manage prime generation. - Aim for optimal performance given the constraints (1 ≤ n ≤ 10^5). Hints - Use `itertools.islice` for iterating over the first `n` elements of an infinite sequence of prime numbers. - Consider using a helper function to determine if a number is prime. - Use `functools.partial` to simplify the implementation of the prime generator function.","solution":"from typing import List import itertools import functools def is_prime(n: int) -> bool: Check if a number is a prime number. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def generate_primes() -> int: Generate an infinite sequence of prime numbers. num = 2 while True: if is_prime(num): yield num num += 1 def prime_generator_partial_sum(n: int) -> List[int]: Generate the specified count of prime numbers and return the running sum of these prime numbers. prime_gen = generate_primes() primes = list(itertools.islice(prime_gen, n)) return list(itertools.accumulate(primes))"},{"question":"Problem Statement: You are working on a file synchronization tool that compresses files while transferring them over the network. To optimize the tool, you need to implement a compression and decompression utility using the `bz2` module of Python. # Task: 1. **Implement a function `compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None`**: - This function should read the contents of `input_file`, compress the data in an incremental manner, and write the compressed data to `output_file`. - Use the `BZ2Compressor` class for incremental compression. - Ensure the compression level is adjustable via the `compresslevel` parameter. 2. **Implement a function `decompress_file(input_file: str, output_file: str) -> None`**: - This function should read the compressed contents of `input_file`, decompress the data, and write the decompressed data to `output_file`. - Use the `BZ2Decompressor` class for incremental decompression. # Input: - `input_file`: Path to the input file to be compressed or decompressed. - `output_file`: Path to the output file where compressed or decompressed data will be written. - `compresslevel` (Optional): Compression level ranging from 1 (least compression) to 9 (most compression). Default is 9. # Constraints: - Handle files of arbitrary size efficiently using incremental compression and decompression. - Ensure that the implementation is robust and handles various edge cases (e.g., empty files, large files). # Example: ```python # Create a sample text file with open(\'sample.txt\', \'w\') as f: f.write(\\"This is a sample text file with several lines of text.n\\" * 100) # Compress the sample file compress_file(\'sample.txt\', \'sample.bz2\') # Decompress the compressed file decompress_file(\'sample.bz2\', \'sample_dec.txt\') # Verify that the decompressed file matches the original file with open(\'sample.txt\', \'r\') as f1, open(\'sample_dec.txt\', \'r\') as f2: assert f1.read() == f2.read(), \\"The original and decompressed files do not match\\" ``` # Note: - You may use `bz2.BZ2File` for reading from and writing to compressed files if necessary.","solution":"import bz2 def compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None: Compress the contents of input_file and write to output_file using bz2. The compression level is adjustable via compresslevel parameter. compressor = bz2.BZ2Compressor(compresslevel) with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: while chunk := infile.read(1024): # read in chunks of 1024 bytes compressed_data = compressor.compress(chunk) if compressed_data: outfile.write(compressed_data) outfile.write(compressor.flush()) def decompress_file(input_file: str, output_file: str) -> None: Decompress the contents of input_file and write to output_file using bz2. decompressor = bz2.BZ2Decompressor() with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: while chunk := infile.read(1024): # read in chunks of 1024 bytes decompressed_data = decompressor.decompress(chunk) if decompressed_data: outfile.write(decompressed_data)"},{"question":"Memory Leak Detection Using `tracemalloc` **Objective**: Utilize the `tracemalloc` module to detect and analyze memory leaks in a Python application. Problem Statement You are required to write a Python function named `analyze_memory_usage` that will run a provided application, trace its memory allocations, and return statistics related to potential memory leaks. The function should perform the following tasks: 1. Start the `tracemalloc` module with a sufficient number of frames. 2. Take an initial memory snapshot before running a provided function (`app_function`). 3. Run the `app_function`. This function is a black-box that you cannot modify but will simulate the execution of a program that may have memory leaks. 4. Take a second memory snapshot after running the `app_function`. 5. Compare the two snapshots to detect memory differences. 6. Extract and return the top 10 sources of memory allocation increases, filtering out lines from \\"<frozen importlib._bootstrap>\\" and \\"<unknown>\\". Function Signature ```python from typing import List, Tuple def analyze_memory_usage(app_function: callable) -> List[Tuple[str, int, int, float]]: pass ``` Input - `app_function (callable)`: A function simulating a program whose memory usage needs to be analyzed. Output - `List[Tuple[str, int, int, float]]`: A list of tuples containing: - `filename (str)`: The filename where memory allocation increased significantly. - `line_number (int)`: The line number of the corresponding filename. - `memory_diff (int)`: The difference in memory size (in bytes) allocated between the two snapshots. - `percent (float)`: Percent increase in memory size, relative to the total increase. The list should be sorted in descending order of the `memory_diff`. Constraints - You must use Python\'s `tracemalloc` module as described. - Filter out files from \\"<frozen importlib._bootstrap>\\" and \\"<unknown>\\". Example ```python import tracemalloc def leaking_function(): a = [i for i in range(10000)] b = [j for j in range(20000)] def analyze_memory_usage(app_function: callable) -> List[Tuple[str, int, int, float]]: tracemalloc.start(25) initial_snapshot = tracemalloc.take_snapshot() app_function() final_snapshot = tracemalloc.take_snapshot() tracemalloc.stop() stats = final_snapshot.compare_to(initial_snapshot, \'lineno\') filtered_stats = [stat for stat in stats if \\"<frozen importlib._bootstrap>\\" not in stat.traceback[0].filename and \\"<unknown>\\" not in stat.traceback[0].filename] top_stats = filtered_stats[:10] total_increase = sum(stat.size_diff for stat in top_stats) results = [(stat.traceback[0].filename, stat.traceback[0].lineno, stat.size_diff, stat.size_diff * 100 / total_increase) for stat in top_stats] return results # Example usage: result = analyze_memory_usage(leaking_function) for filename, line_number, memory_diff, percentage in result: print(f\\"{filename}:{line_number} - {memory_diff} bytes ({percentage:.2f}%)\\") ``` Notes - Ensure the `tracemalloc` module is properly started and stopped to avoid memory overhead. - The output format should be precise for easy analysis.","solution":"import tracemalloc from typing import List, Tuple def analyze_memory_usage(app_function: callable) -> List[Tuple[str, int, int, float]]: tracemalloc.start(25) initial_snapshot = tracemalloc.take_snapshot() app_function() final_snapshot = tracemalloc.take_snapshot() tracemalloc.stop() stats = final_snapshot.compare_to(initial_snapshot, \'traceback\') filtered_stats = [stat for stat in stats if not any( part in stat.traceback[0].filename for part in [\\"<frozen importlib._bootstrap>\\", \\"<unknown>\\"] )] top_stats = filtered_stats[:10] total_increase = sum(stat.size_diff for stat in top_stats) results = [ ( stat.traceback[0].filename, stat.traceback[0].lineno, stat.size_diff, (stat.size_diff / total_increase) * 100 ) for stat in top_stats ] return results"},{"question":"Implementing and Comparing PCA Variants You are provided with a dataset consisting of numerical values with shape (300, 50). Your task is to implement and compare different Principal Component Analysis (PCA) techniques available in `scikit-learn`. Specifically, you will: 1. **Standard PCA**: Perform PCA and find the explained variance ratio of the first five components. 2. **Incremental PCA**: Implement Incremental PCA with batch processing and find the explained variance ratio of the first five components. 3. **Randomized PCA**: Use PCA with the `randomized` solver to find the explained variance ratio of the first five components. **Dataset**: A synthetic dataset with 300 samples and 50 features generated randomly. ```python import numpy as np # Generate the dataset np.random.seed(42) data = np.random.randn(300, 50) ``` Tasks: 1. Load the generated data. 2. Implement standard PCA on this dataset to find the explained variance ratio of the first five components. 3. Implement Incremental PCA using batch processing (batch size of 100) to find the explained variance ratio of the first five components. 4. Implement PCA with the `randomized` solver to find the explained variance ratio of the first five components. 5. Compare and display the results of the explained variance ratios for the first five components obtained from Standard PCA, Incremental PCA, and Randomized PCA. Expected Output Format: - The explained variance ratio of the first five components for each type of PCA. ```python from sklearn.decomposition import PCA, IncrementalPCA # Load the data data = np.random.randn(300, 50) # Standard PCA pca = PCA() pca.fit(data) standard_pca_explained_variance = pca.explained_variance_ratio_[:5] # Incremental PCA ipca = IncrementalPCA(n_components=50, batch_size=100) for batch in np.array_split(data, 3): # Split data into 3 batches ipca.partial_fit(batch) incremental_pca_explained_variance = ipca.explained_variance_ratio_[:5] # Randomized PCA rpca = PCA(svd_solver=\'randomized\') rpca.fit(data) randomized_pca_explained_variance = rpca.explained_variance_ratio_[:5] # Display the explained variance ratios print(\\"Standard PCA Explained Variance Ratio: \\", standard_pca_explained_variance) print(\\"Incremental PCA Explained Variance Ratio: \\", incremental_pca_explained_variance) print(\\"Randomized PCA Explained Variance Ratio: \\", randomized_pca_explained_variance) ``` Constraints: - Use the PCA implementations provided by `scikit-learn`. - Ensure that the Incremental PCA processes the data in batches. - Ensure reproducibility by setting random seeds where necessary.","solution":"import numpy as np from sklearn.decomposition import PCA, IncrementalPCA # Generate the dataset np.random.seed(42) data = np.random.randn(300, 50) # Standard PCA pca = PCA() pca.fit(data) standard_pca_explained_variance = pca.explained_variance_ratio_[:5] # Incremental PCA with batch processing ipca = IncrementalPCA(n_components=50, batch_size=100) ipca.fit(data) incremental_pca_explained_variance = ipca.explained_variance_ratio_[:5] # Randomized PCA rpca = PCA(svd_solver=\'randomized\') rpca.fit(data) randomized_pca_explained_variance = rpca.explained_variance_ratio_[:5] # Display the explained variance ratios print(\\"Standard PCA Explained Variance Ratio: \\", standard_pca_explained_variance) print(\\"Incremental PCA Explained Variance Ratio: \\", incremental_pca_explained_variance) print(\\"Randomized PCA Explained Variance Ratio: \\", randomized_pca_explained_variance)"},{"question":"In this coding assessment, you are to demonstrate your ability to use the `importlib.metadata` package in Python 3.10. This library provides for access to the metadata of installed Python packages. # Task You are required to write three functions: 1. **get_package_version(package_name: str) -> str**: This function should take the name of a package as input and return the version of the package as a string. If the package is not found, it should return the string `\\"Package not found\\"`. 2. **list_package_files(package_name: str) -> List[str]**: This function should return a list of file paths contained within the specified package. If the package is not found or has no files listed, it should return an empty list. 3. **get_package_requirements(package_name: str) -> List[str]**: This function should return the list of requirements for the specified package. If the package has no requirements or is not found, return an empty list. # Constraints * The functions should handle exceptions and edge cases gracefully. * Avoid using any external libraries except `importlib.metadata`. * Assume the functions will be tested with valid package names that could potentially be installed in a Python environment. # Example Usage ```python # Example usage: print(get_package_version(\\"wheel\\")) # Output: \'0.32.3\' or similar print(list_package_files(\\"wheel\\")) # Output: [\'wheel/__init__.py\', \'wheel/cli.py\', ...] print(get_package_requirements(\\"wheel\\")) # Output: [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] ``` # Implementation Implement the functions in Python: ```python from importlib.metadata import version, files, requires, PackageNotFoundError def get_package_version(package_name: str) -> str: Retrieve the version of the specified package. Parameters: package_name (str): The name of the package whose version needs to be retrieved. Returns: str: The version of the package or \\"Package not found\\". try: return version(package_name) except PackageNotFoundError: return \\"Package not found\\" def list_package_files(package_name: str) -> List[str]: Retrieve the list of files contained within the specified package. Parameters: package_name (str): The name of the package whose files need to be listed. Returns: List[str]: A list of file paths. try: return [str(file) for file in files(package_name)] except PackageNotFoundError: return [] def get_package_requirements(package_name: str) -> List[str]: Retrieve the requirements of the specified package. Parameters: package_name (str): The name of the package whose requirements needs to be retrieved. Returns: List[str]: A list of requirements. try: reqs = requires(package_name) return reqs if reqs else [] except PackageNotFoundError: return [] ``` Complete the functions to adhere to the examples and constraints provided.","solution":"from importlib.metadata import version, files, requires, PackageNotFoundError def get_package_version(package_name: str) -> str: Retrieve the version of the specified package. Parameters: package_name (str): The name of the package whose version needs to be retrieved. Returns: str: The version of the package or \\"Package not found\\". try: return version(package_name) except PackageNotFoundError: return \\"Package not found\\" def list_package_files(package_name: str) -> list: Retrieve the list of files contained within the specified package. Parameters: package_name (str): The name of the package whose files need to be listed. Returns: list: A list of file paths. try: return [str(file) for file in files(package_name)] except PackageNotFoundError: return [] def get_package_requirements(package_name: str) -> list: Retrieve the requirements of the specified package. Parameters: package_name (str): The name of the package whose requirements need to be retrieved. Returns: list: A list of requirements. try: reqs = requires(package_name) return reqs if reqs else [] except PackageNotFoundError: return []"},{"question":"**Question: HTML Content Manipulation** As a web developer, it\'s crucial to ensure that user-generated content is displayed safely and correctly in web applications. Using Python\'s `html` module, you will implement two functions that help in sanitizing and desanitizing HTML text. 1. **Function: `sanitize_html(content: str, escape_quotes: bool) -> str`** Given a string `content` that might contain special HTML characters (e.g., `&`, `<`, `>`, `\\"`, and `\'`), write a function that sanitizes this content by converting its special characters to HTML-safe sequences. - Parameters: - `content` (str): The string containing HTML content that needs to be sanitized. - `escape_quotes` (bool): A boolean flag that indicates whether to also escape the quotes `\\"` and `\'`. - Returns: - `str`: The sanitized HTML content as a string. Example: ```python input = \'<div class=\\"alert\\">Hello & Welcome!</div>\' escape_quotes = True result = sanitize_html(input, escape_quotes) # Expected output: \'&lt;div class=&quot;alert&quot;&gt;Hello &amp; Welcome!&lt;/div&gt;\' ``` 2. **Function: `desanitize_html(content: str) -> str`** Given a string `content` that contains HTML-safe sequences, write a function that converts these sequences back to their corresponding Unicode characters. - Parameters: - `content` (str): The string containing sanitized HTML content. - Returns: - `str`: The desanitized HTML content as a string. Example: ```python input = \'&lt;div class=&quot;alert&quot;&gt;Hello &amp; Welcome!&lt;/div&gt;\' result = desanitize_html(input) # Expected output: \'<div class=\\"alert\\">Hello & Welcome!</div>\' ``` **Constraints:** - You may assume that the input string will be a valid HTML content. - Both functions should handle empty strings gracefully. Your task is to implement these two functions using the `html` module. Ensure that your solution is efficient and correctly handles all edge cases. **Performance Requirements:** - The solution should handle large HTML contents efficiently, with acceptable performance for web applications.","solution":"import html def sanitize_html(content: str, escape_quotes: bool) -> str: Sanitizes the HTML content by escaping special characters. :param content: The string containing HTML content that needs to be sanitized. :param escape_quotes: A boolean flag that indicates whether to also escape the quotes `\\"` and `\'`. :return: The sanitized HTML content as a string. # Escape special HTML characters sanitized_content = html.escape(content, quote=escape_quotes) return sanitized_content def desanitize_html(content: str) -> str: Desanitizes the HTML content by unescaping HTML-safe sequences. :param content: The string containing sanitized HTML content. :return: The desanitized HTML content as a string. # Unescape HTML-safe sequences back to normal characters desanitized_content = html.unescape(content) return desanitized_content"},{"question":"Objective: Write a Python script that uses Seaborn to explore potential violations of linear regression assumptions using residual plots. Your task is to generate residual plots for various vehicle characteristics and interpret them. Dataset: For this assignment, you will use the \\"mpg\\" dataset that is available through Seaborn. Instructions: 1. **Data Loading**: First, load the \\"mpg\\" dataset using Seaborn\'s `load_dataset` function. 2. **Simple Residual Plot**: Generate a residual plot for `weight` (as the independent variable) and `displacement` (as the dependent variable). 3. **Checking Linear Regression Assumptions**: Create a residual plot for `horsepower` (as the independent variable) and `mpg` (miles per gallon, as the dependent variable). Based on the plot, provide a brief interpretation of whether the assumptions of a linear regression model might be violated. 4. **Higher-order Trends**: Generate a second residual plot for `horsepower` and `mpg`, this time using a quadratic (2nd order) trendline. Compare this plot with the previous plot and comment on whether higher-order trends have stabilized the residuals. 5. **LOWESS Curve**: Finally, generate another residual plot for `horsepower` and `mpg`, adding a LOWESS curve to the plot. Use the `line_kws` parameter to set the curve color to red. Requirements: - Your code should be clean, well-commented, and efficient. - Each plot should be clearly labeled with appropriate titles and axis labels. - Your final output should include the three plots along with your observations and comparisons. - Use Matplotlib to display the plots if running outside of Jupyter Notebook. Constraints: - You must use only the Seaborn library and Matplotlib for visualizations. - Ensure that your plots are easy to interpret and aesthetically pleasing. Input/Output Format: - **Input**: No input is required from the user; you will be working directly with the \\"mpg\\" dataset. - **Output**: The output should be the residual plots and the accompanying interpretations in a text format. Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Simple Residual Plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: Weight vs Displacement\\") plt.show() # Residual Plot for Linear Regression Assumption Check plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Horsepower vs MPG\\") plt.show() # [Insert interpretation here] # Residual Plot with Higher-order Trends plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot (Quadratic Trend): Horsepower vs MPG\\") plt.show() # [Insert comparison here] # Residual Plot with LOWESS Curve plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual Plot (LOWESS Curve): Horsepower vs MPG\\") plt.show() # [Insert comparison here] ``` Provide the complete script along with your interpretations as per the instructions.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Simple Residual Plot: Weight vs Displacement plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot: Weight vs Displacement\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # Residual Plot for Linear Regression Assumption Check: Horsepower vs MPG plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Interpretation # The residuals for horsepower vs mpg are not randomly distributed around zero. # There appears to be a pattern, suggesting that the linear regression model may not be appropriate. # Residual Plot with Higher-order Trends (Quadratic): Horsepower vs MPG plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot (Quadratic Trend): Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Comparison # The residuals with the quadratic trend line appear more stabilized and random around zero, # indicating that a quadratic model may better fit the data compared to a simple linear model. # Residual Plot with LOWESS Curve: Horsepower vs MPG plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"Residual Plot (LOWESS Curve): Horsepower vs MPG\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Comparison # The LOWESS curve helps to visualize the non-linear relationship in the data. # It shows the non-linear trend and demonstrates that the residuals are not randomly distributed, # further indicating that the relationship between horsepower and mpg is not purely linear."},{"question":"# Email Creation and Parsing with JSON Content **Objective**: You are required to write a Python function that creates an email with JSON content and another function to parse the email to extract and decode the JSON content. # Tasks: 1. **Email Creation**: Implement a function `create_email(subject: str, sender: str, receiver: str, json_content: dict) -> str` that takes the following parameters: - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `receiver` (str): The receiver\'s email address. - `json_content` (dict): A dictionary containing the JSON content to be included in the email. The function should create an email message with MIME type `application/json` and return it as a string. 2. **Email Parsing**: Implement a function `parse_email(email_str: str) -> dict` that takes the following parameter: - `email_str` (str): The email message as a string. The function should parse the email, extract the JSON content, decode it, and return it as a dictionary. # Input and Output Formats: Function 1: `create_email` **Input**: ```python subject = \\"Test Email\\" sender = \\"alice@example.com\\" receiver = \\"bob@example.com\\" json_content = { \\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\" } ``` **Output**: ```python \\"Email as a single string with proper headers and JSON content.\\" ``` Function 2: `parse_email` **Input**: ```python email_str = \\"Email message as a string with MIME type \'application/json\'\\" ``` **Output**: ```python { \\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\" } ``` # Constraints: 1. Ensure that the email is formatted properly with the correct headers for MIME type `application/json`. 2. The JSON content in the email must be encoded and decoded correctly. # Example: ```python # Example for create_email function email_str = create_email( subject=\\"Test Email\\", sender=\\"alice@example.com\\", receiver=\\"bob@example.com\\", json_content={ \\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\" } ) print(email_str) # Example for parse_email function json_content = parse_email(email_str) print(json_content) ``` This question tests the students\' understanding of email creation and parsing in Python using the `email` module, as well as their ability to encode and decode JSON content.","solution":"import json from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email import message_from_string def create_email(subject: str, sender: str, receiver: str, json_content: dict) -> str: Creates an email with JSON content and returns it as a string. # Convert the dictionary to a JSON string json_str = json.dumps(json_content) # Create the email message container msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = receiver # Attach JSON content as MIMEText mime_text = MIMEText(json_str, \'plain\') mime_text.add_header(\'Content-Disposition\', \'attachment\', filename=\'content.json\') mime_text.add_header(\'Content-Type\', \'application/json\') msg.attach(mime_text) # Return the formatted email as a string return msg.as_string() def parse_email(email_str: str) -> dict: Parses the email string, extracts the JSON content, and returns it as a dictionary. # Convert the email string to a message object msg = message_from_string(email_str) # Get the payload (assumes single part MIME) payload = msg.get_payload() # Check if payload is list of parts (multipart) and extract from the first part if so if isinstance(payload, list): payload = payload[0].get_payload() # Parse the payload as JSON json_content = json.loads(payload) return json_content"},{"question":"# Task: Implement and Apply Locally Linear Embedding (LLE) for Dimensionality Reduction Objective In this task, you are required to implement and apply the Locally Linear Embedding (LLE) algorithm to reduce the dimensionality of a dataset while preserving the local neighborhoods in the data. Description 1. **Implementation**: You need to implement the standard Locally Linear Embedding (LLE) using the scikit-learn library. 2. **Application**: Apply your implementation to a dataset of handwritten digits (MNIST or a subset of MNIST). 3. **Visualization**: Visualize the results by plotting the lower-dimensional representation of the data points. Detailed Steps 1. **Load Dataset**: Load the MNIST dataset (or a subset of it). You can use `sklearn.datasets.load_digits()` to load a smaller subset suitable for demonstration. 2. **Preprocess Data**: Ensure the data is scaled properly. You might want to use `StandardScaler` from `sklearn.preprocessing` to standardize the features. 3. **Implement LLE**: - Use `sklearn.manifold.LocallyLinearEmbedding` to perform LLE. - Select appropriate values for `n_neighbors` and `n_components` based on the problem requirements. 4. **Fit and Transform**: - Fit the LLE model to the dataset. - Transform the dataset to the lower-dimensional space. 5. **Visualization**: - Use a 2D scatter plot to visualize the transformed data points. - Color the points based on their original classes to see how well the LLE algorithm has preserved the local neighborhood structure. Constraints and Requirements - You must use scikit-learn\'s `LocallyLinearEmbedding`. - Ensure your code is efficient and avoid overcomplicating the implementation. - Provide clear explanations and comments in your code. - Your visualization must reflect the effectiveness of the dimensionality reduction. Performance Requirements - The algorithm should be able to process at least 1000 data points efficiently. - The visualization should be clear and represent the neighborhood structure accurately. Expected Input and Output - **Input**: MNIST dataset (or a subset), parameters for LLE (`n_neighbors`, `n_components`). - **Output**: 2D scatter plot visualizing the transformed data, with points colored by their original classes. Example ```python import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.manifold import LocallyLinearEmbedding from sklearn.preprocessing import StandardScaler # Load dataset digits = load_digits() X, y = digits.data, digits.target # Standardize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply Locally Linear Embedding n_neighbors = 10 n_components = 2 lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components) # Transform data to lower dimensions X_r = lle.fit_transform(X_scaled) # Visualization plt.figure(figsize=(10, 6)) plt.scatter(X_r[:, 0], X_r[:, 1], c=y, cmap=plt.cm.Spectral, edgecolor=\'k\') plt.colorbar() plt.title(\'Locally Linear Embedding of the digits dataset\') plt.show() ``` Good luck and happy coding!","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_digits from sklearn.manifold import LocallyLinearEmbedding from sklearn.preprocessing import StandardScaler def apply_locally_linear_embedding(n_neighbors=10, n_components=2): Loads the digit dataset, applies Locally Linear Embedding (LLE) for dimensionality reduction, and visualizes the results in a 2D scatter plot. Parameters: n_neighbors (int): Number of neighbors to consider for each point. n_components (int): Number of dimensions to reduce to. Returns: lle_result (array-like): 2D transformed data. target (array-like): Original classes of the data points. # Load dataset digits = load_digits() X, y = digits.data, digits.target # Standardize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply Locally Linear Embedding lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components) X_r = lle.fit_transform(X_scaled) # Visualization plt.figure(figsize=(10, 6)) plt.scatter(X_r[:, 0], X_r[:, 1], c=y, cmap=plt.cm.Spectral, edgecolor=\'k\') plt.colorbar() plt.title(\'Locally Linear Embedding of the digits dataset\') plt.show() return X_r, y"},{"question":"**Coding Assessment Question: Digital Signal Processing with `audioop` Module** **Objective:** Write a Python function that performs multiple transformations on an audio fragment using the `audioop` module. **Function Signature:** ```python def process_audio(fragment: bytes, width: int, rate: int) -> bytes: Transforms an input audio fragment by performing the following steps: 1. Convert the audio fragment to 8-bit a-LAW encoding. 2. Reverse the samples in the a-LAW encoded fragment. 3. Convert the reversed fragment back to the original bit width. 4. Change the sample rate of the fragment to half of the original rate, ensuring the new fragment has the same playing duration as the original. Parameters: - fragment (bytes): The input audio fragment in linear PCM format. - width (int): The bit width of each sample in the input fragment (1, 2, 3, or 4). - rate (int): The sample rate of the input fragment. Returns: - bytes: The transformed audio fragment, in the same bit width as the input fragment. pass ``` **Details:** 1. **Convert to a-LAW Encoding:** Use `audioop.lin2alaw()` to convert the input fragment to 8-bit a-LAW encoding. 2. **Reverse the Samples:** Use `audioop.reverse()` to reverse the a-LAW encoded fragment. 3. **Convert Back to Original Bit Width:** Use `audioop.alaw2lin()` to convert the reversed fragment back to the original bit width. 4. **Change Sample Rate:** Use `audioop.ratecv()` to change the sample rate of the fragment to half of the original rate. Ensure that the duration of the audio remains the same by appropriately choosing the parameters for the rate conversion. **Constraints:** - The input fragment is guaranteed to be non-empty. - The width will be one of the following: 1, 2, 3, or 4. - The rate will be a positive integer. **Example:** ```python input_fragment = b\'x01x02x03x04\' input_width = 2 input_rate = 44100 output_fragment = process_audio(input_fragment, input_width, input_rate) # Expected output is the transformed audio fragment ``` Make sure to handle any potential errors gracefully, such as invalid input parameters.","solution":"import audioop def process_audio(fragment: bytes, width: int, rate: int) -> bytes: Transforms an input audio fragment by performing the following steps: 1. Convert the audio fragment to 8-bit a-LAW encoding. 2. Reverse the samples in the a-LAW encoded fragment. 3. Convert the reversed fragment back to the original bit width. 4. Change the sample rate of the fragment to half of the original rate, ensuring the new fragment has the same playing duration as the original. Parameters: - fragment (bytes): The input audio fragment in linear PCM format. - width (int): The bit width of each sample in the input fragment (1, 2, 3, or 4). - rate (int): The sample rate of the input fragment. Returns: - bytes: The transformed audio fragment, in the same bit width as the input fragment. if width not in (1, 2, 3, 4): raise ValueError(\\"Width must be one of 1, 2, 3, or 4\\") if rate <= 0: raise ValueError(\\"Rate must be a positive integer\\") # Step 1: Convert the audio fragment to 8-bit a-LAW encoding alaw_encoded = audioop.lin2alaw(fragment, width) # Step 2: Reverse the samples in the a-LAW encoded fragment reversed_fragment = audioop.reverse(alaw_encoded, 1) # Step 3: Convert the reversed fragment back to the original bit width linear_fragment = audioop.alaw2lin(reversed_fragment, width) # Step 4: Change the sample rate to half of the original rate new_rate = rate // 2 converted_fragment, _ = audioop.ratecv(linear_fragment, width, 1, rate, new_rate, None) return converted_fragment"},{"question":"# Mailbox Manager You are tasked with creating a `MailboxManager` class that allows you to interact with a `mailbox.MH` mailbox in specific ways. Your manager should support adding new messages, removing messages, retrieving messages, and listing all folders along with their message count. Additionally, you need to implement a method to move a message from one folder to another. **Requirements:** 1. **Class Definition:** ```python class MailboxManager: def __init__(self, path): # Initializes the mailbox at the given path. def add_message(self, folder_name, message): # Adds a message to the specified folder and returns the message key. def remove_message(self, folder_name, key): # Removes the message with the specified key from the specified folder. def get_message(self, folder_name, key): # Retrieves the message with the specified key from the specified folder. def list_folders(self): # Returns a dictionary with folder names as keys and message counts as values. def move_message(self, source_folder, destination_folder, key): # Moves a message from source_folder to destination_folder. ``` 2. **Expected Input and Output:** - `__init__(self, path)`: Initializes the mailbox at the given path. - **Input:** `path` (str) - Path to the mailbox. - `add_message(self, folder_name, message)`: Adds a message to the specified folder and returns the message key. - **Input:** `folder_name` (str), `message` (Message instance or string, byte string, or file-like object) - **Output:** `key` (int) - Key of the added message. - `remove_message(self, folder_name, key)`: Removes the message with the specified key from the specified folder. - **Input:** `folder_name` (str), `key` (int) - **Output:** None - `get_message(self, folder_name, key)`: Retrieves the message with the specified key from the specified folder. - **Input:** `folder_name` (str), `key` (int) - **Output:** `message` (MHMessage instance) - `list_folders(self)`: Returns a dictionary with folder names as keys and message counts as values. - **Input:** None - **Output:** `folders_dict` (dict) - Dictionary with folder names as keys and message counts as values. - `move_message(self, source_folder, destination_folder, key)`: Moves a message from source_folder to destination_folder. - **Input:** `source_folder` (str), `destination_folder` (str), `key` (int) - **Output:** None 3. **Constraints and Limitations:** - You should use the `mailbox` module from the Python standard library. - Ensure that folders exist before adding or moving messages into them. - Handle errors gracefully, raising appropriate exceptions where necessary. - Ensure concurrency safety by locking the mailbox when making modifications. 4. **Performance Requirements:** - The implementation should be efficient in terms of both time and space. Operations like adding, removing, and retrieving messages should be handled promptly. # Example: ```python # Initialize the MailboxManager with a path manager = MailboxManager(\'~/Mail\') # Add a message key = manager.add_message(\'inbox\', \'Subject: TestnnThis is a test message.\') # List all folders and their message counts print(manager.list_folders()) # Output: {\'inbox\': 1} # Get a message message = manager.get_message(\'inbox\', key) print(message) # Move the message to another folder manager.move_message(\'inbox\', \'archive\', key) # Remove the message manager.remove_message(\'archive\', key) ```","solution":"import mailbox import os class MailboxManager: def __init__(self, path): self.path = os.path.expanduser(path) self.mbox = mailbox.MH(self.path) def add_message(self, folder_name, message): folder = self.mbox.get_folder(folder_name) if isinstance(message, mailbox.Message): msg = message else: msg = mailbox.mboxMessage(message) key = folder.add(msg) folder.flush() return key def remove_message(self, folder_name, key): folder = self.mbox.get_folder(folder_name) folder.remove(key) folder.flush() def get_message(self, folder_name, key): folder = self.mbox.get_folder(folder_name) return folder[key] def list_folders(self): return {folder: len(self.mbox.get_folder(folder)) for folder in self.mbox.list_folders()} def move_message(self, source_folder, destination_folder, key): source = self.mbox.get_folder(source_folder) message = source[key] destination = self.mbox.get_folder(destination_folder) new_key = destination.add(message) destination.flush() source.remove(key) source.flush()"},{"question":"**Title: Python Object Manipulation and Comparison** **Question:** You are tasked with creating a set of functions that operate on Python objects using specific operations outlined in the `python310` package. Implement the following functions: 1. **`has_attribute(obj: object, attr_name: str) -> bool`**: - Check if the given object has the specified attribute. - Return `True` if the attribute exists, and `False` otherwise. 2. **`get_attribute(obj: object, attr_name: str) -> any`**: - Retrieve the value of the specified attribute from the given object. - If the attribute does not exist, return `None`. 3. **`set_attribute(obj: object, attr_name: str, value: any) -> bool`**: - Set the value of the specified attribute for the given object. - Return `True` if the operation was successful, and `False` otherwise. 4. **`compare_objects(obj1: object, obj2: object, operator: str) -> bool`**: - Compare two Python objects using the specified comparison operator. - Supported operators: `\\"<\\"`, `\\"<=\\"`, `\\"==\\"`, `\\"!=\\"`, `\\">\\"`, `\\">=\\"`. - Return the result of the comparison. 5. **`is_type(obj: object, cls: type) -> bool`**: - Check if the given object is an instance of the specified class or a subclass thereof. - Return `True` if it is, and `False` otherwise. **Function Specifications:** 1. **`has_attribute`**: - Inputs: `obj` (object), `attr_name` (str) - Output: `bool` 2. **`get_attribute`**: - Inputs: `obj` (object), `attr_name` (str) - Output: `any` 3. **`set_attribute`**: - Inputs: `obj` (object), `attr_name` (str), `value` (any) - Output: `bool` 4. **`compare_objects`**: - Inputs: `obj1` (object), `obj2` (object), `operator` (str) - Output: `bool` 5. **`is_type`**: - Inputs: `obj` (object), `cls` (type) - Output: `bool` **Example Usage:** ```python class ExampleClass: def __init__(self): self.attribute = \\"value\\" example = ExampleClass() # has_attribute assert has_attribute(example, \\"attribute\\") == True assert has_attribute(example, \\"missing\\") == False # get_attribute assert get_attribute(example, \\"attribute\\") == \\"value\\" assert get_attribute(example, \\"missing\\") == None # set_attribute assert set_attribute(example, \\"new_attr\\", 123) == True assert example.new_attr == 123 # compare_objects assert compare_objects(5, 3, \\">\\") == True assert compare_objects(5, 5, \\"==\\") == True assert compare_objects(5, 5, \\">=\\") == True # is_type assert is_type(example, ExampleClass) == True assert is_type(5, int) == True assert is_type(\\"string\\", int) == False ``` Implement these functions according to the specifications provided. Handle any potential exceptions or errors gracefully. **Constraints:** - You can assume that the attribute names and types provided are valid Python identifiers. - The comparison operator will always be one of the specified operators. - Performance must be considered for large or complex objects.","solution":"def has_attribute(obj: object, attr_name: str) -> bool: Check if the given object has the specified attribute. return hasattr(obj, attr_name) def get_attribute(obj: object, attr_name: str) -> any: Retrieve the value of the specified attribute from the given object. return getattr(obj, attr_name, None) def set_attribute(obj: object, attr_name: str, value: any) -> bool: Set the value of the specified attribute for the given object. setattr(obj, attr_name, value) return True def compare_objects(obj1: object, obj2: object, operator: str) -> bool: Compare two Python objects using the specified comparison operator. if operator == \\"<\\": return obj1 < obj2 elif operator == \\"<=\\": return obj1 <= obj2 elif operator == \\"==\\": return obj1 == obj2 elif operator == \\"!=\\": return obj1 != obj2 elif operator == \\">\\": return obj1 > obj2 elif operator == \\">=\\": return obj1 >= obj2 else: raise ValueError(f\\"Unsupported operator: {operator}\\") def is_type(obj: object, cls: type) -> bool: Check if the given object is an instance of the specified class or a subclass thereof. return isinstance(obj, cls)"},{"question":"# Advanced Coding Assessment: Custom Iterator and Generator Objective Demonstrate your understanding of functional programming in Python by implementing a custom iterator and a generator. This task will test your ability to use Python\'s iterator protocol, generator functions, and the `itertools` and `functools` modules. Problem Statement You are required to implement a custom iterator called `SquareEvenIterator` and a generator function called `fibonacci_generator`. 1. **`SquareEvenIterator`** - **Description:** This iterator will take a list of integers and yield the square of each even number in the list, one at a time. - **Methods to Implement:** - `__init__(self, data: List[int])`: Initialize the iterator with a list of integers. - `__iter__(self) -> \'SquareEvenIterator\'`: Return the iterator object itself. - `__next__(self) -> int`: Return the next square of an even number from the list. Raise `StopIteration` when all elements are processed. - **Example Usage:** ```python data = [1, 2, 3, 4, 5, 6] it = SquareEvenIterator(data) for value in it: print(value) # Output: 4, 16, 36 ``` 2. **`fibonacci_generator`** - **Description:** This generator function will yield the Fibonacci sequence up to `n` numbers. - **Method to Implement:** - `fibonacci_generator(n: int) -> Generator[int, None, None]`: Generate the Fibonacci sequence up to the n-th number. - **Example Usage:** ```python for value in fibonacci_generator(5): print(value) # Output: 0, 1, 1, 2, 3 ``` Input and Output - **`SquareEvenIterator`**: - **Input:** A list of integers. - **Output:** An integer value representing the square of an even number in the list, iterated sequentially. - **`fibonacci_generator`**: - **Input:** An integer `n`. - **Output:** An iterator that yields the Fibonacci sequence up to `n` numbers. Constraints - The list of integers for `SquareEvenIterator` can be of any length but will always have at least one element. - The value of `n` in `fibonacci_generator` will be a non-negative integer. Performance Requirements - Your implementation should efficiently handle lists of up to 10,000 integers for the iterator. - The generator should handle Fibonacci sequences for values of `n` up to 50 without significant performance degradation. Implementation Write your code below: ```python from typing import List, Generator class SquareEvenIterator: def __init__(self, data: List[int]): self.data = data self.index = 0 def __iter__(self) -> \'SquareEvenIterator\': return self def __next__(self) -> int: while self.index < len(self.data): value = self.data[self.index] self.index += 1 if value % 2 == 0: return value ** 2 raise StopIteration def fibonacci_generator(n: int) -> Generator[int, None, None]: a, b = 0, 1 for _ in range(n): yield a a, b = b, a + b # Example Usage # Uncomment to test the implementations # data = [1, 2, 3, 4, 5, 6] # it = SquareEvenIterator(data) # for value in it: # print(value) # Output: 4, 16, 36 # for value in fibonacci_generator(5): # print(value) # Output: 0, 1, 1, 2, 3 ```","solution":"from typing import List, Generator class SquareEvenIterator: def __init__(self, data: List[int]): self.data = data self.index = 0 def __iter__(self) -> \'SquareEvenIterator\': return self def __next__(self) -> int: while self.index < len(self.data): value = self.data[self.index] self.index += 1 if value % 2 == 0: return value ** 2 raise StopIteration def fibonacci_generator(n: int) -> Generator[int, None, None]: a, b = 0, 1 for _ in range(n): yield a a, b = b, a + b # Example Usage # Uncomment to test the implementations # data = [1, 2, 3, 4, 5, 6] # it = SquareEvenIterator(data) # for value in it: # print(value) # Output: 4, 16, 36 # for value in fibonacci_generator(5): # print(value) # Output: 0, 1, 1, 2, 3"},{"question":"**Title: Implement a Multi-threaded C Extension Module with Python Embedding** **Objective**: In this question, you will implement a simple multi-threaded C extension module that utilizes Python 3.10’s embedding features. This module should demonstrate the proper use of the Global Interpreter Lock (GIL) and thread state management when interacting with Python from within a C thread. **Problem Statement**: You are required to create a multi-threaded C extension module where: 1. The module initializes the Python interpreter. 2. Spawns multiple threads from C. 3. Each thread will execute a simple Python function that calculates the factorial of a number using Python\'s `math` module. 4. Properly handle the GIL and thread states in each C thread. 5. Finalize the Python interpreter once all threads complete execution. **Detailed Requirements**: 1. **Initialization and Finalization**: - Use `Py_InitializeEx(0)` to initialize the Python interpreter. - Use `Py_FinalizeEx()` to finalize the Python interpreter. 2. **GIL Handling**: - Ensure that the GIL is properly locked and released in your threads by using `PyGILState_Ensure()` and `PyGILState_Release()`. 3. **Threading**: - Create multiple threads using native C threading functions (e.g., `pthread` on Unix, `_beginthreadex` on Windows). - In each thread, call a Python function to compute the factorial of a given number. 4. **Python Function Call**: - Use the `math.factorial` function in Python to perform the calculation. **Constraints**: 1. The number of threads spawned should be at least 3. 2. Each thread should compute the factorial of a unique number between 5 and 15. 3. Properly handle any Python exceptions that might occur during the function calls. **Example Output**: - Each thread should print the result of the factorial calculation. - Ensure proper synchronization of threads if necessary. **Submission**: - Provide the complete C extension source code. - Include a makefile or a build script to compile the extension module. - Provide a Python script demonstrating the usage of the compiled extension module. **Performance Requirements**: - Ensure minimal overhead in thread synchronization. - Make sure the Python interpreter initialization and finalization are efficient and correctly implemented. --- This problem requires a thorough understanding of Python embeddings, threading, and GIL management, ensuring students can practically apply these advanced concepts.","solution":"# solution.py import threading import math def calculate_factorial(n): Calculate and return the factorial of n. return math.factorial(n) def thread_function(n): result = calculate_factorial(n) print(f\\"Thread for n={n}: {result}\\") def main(): numbers = [5, 6, 7, 8, 9] threads = [] for num in numbers: thread = threading.Thread(target=thread_function, args=(num,)) threads.append(thread) thread.start() for thread in threads: thread.join() if __name__ == \\"__main__\\": main()"},{"question":"You are required to implement a custom class that represents a `Node` in a tree data structure. Each `Node` contains a value and a list of its children nodes. The `Node` class should support both shallow and deep copy operations. **Class Specifications:** 1. **Node Class**: - **Attributes**: - `value` (int) : Value stored in the node. - `children` (list): List of child `Node` objects. - **Methods**: - `__init__(self, value: int, children: list = None)`: Constructor to initialize the Node. If `children` is not provided, it should default to an empty list. - `__copy__(self)`: Implement the shallow copy method. - `__deepcopy__(self, memo)`: Implement the deep copy method. **Function Signature:** ```python class Node: def __init__(self, value: int, children: list = None): pass def __copy__(self): pass def __deepcopy__(self, memo): pass ``` **Example Usage:** ```python n1 = Node(1) n2 = Node(2) n3 = Node(3, [n1, n2]) import copy n3_shallow = copy.copy(n3) n3_deep = copy.deepcopy(n3) # Modifying the value of n1 should reflect in n3_shallow but not in n3_deep. n1.value = 10 print(n3.children[0].value) # Output: 10 print(n3_shallow.children[0].value) # Output: 10 print(n3_deep.children[0].value) # Output: 1 ``` **Constraints:** - You must use the `copy` module\'s functionalities. - Do not use any third-party libraries for the implementation. - The `__deepcopy__` method must utilize the `memo` dictionary to handle recursive references. **Your implementation should be capable of:** - Creating a new Node instance with the correct value and children. - Correctly performing both shallow and deep copy operations considering the constraints and example provided.","solution":"import copy class Node: def __init__(self, value: int, children: list = None): self.value = value self.children = children if children is not None else [] def __copy__(self): new_node = Node(self.value) new_node.children = self.children[:] return new_node def __deepcopy__(self, memo): if self in memo: return memo[self] new_node = Node(self.value) memo[self] = new_node new_node.children = [copy.deepcopy(child, memo) for child in self.children] return new_node"},{"question":"# Advanced Python: Instance Method and Method Objects You are required to implement a Python class that utilizes instance method objects and method objects. The goal is to create a class that has a dynamically assigned method which can be checked and called using the provided C-API functions. Class Design **Class Name:** `DynamicMethodClass` **Methods & Properties:** 1. **add_method(self, func)** - **Input:** `func` (A callable object like a function or lambda) - **Behavior:** Adds `func` as an instance method to the class dynamically. 2. **call_method(self, method_name, *args, **kwargs)** - **Input:** `method_name` (String) and additional arguments `*args` and `**kwargs` to pass to the method. - **Output:** Calls the method specified by `method_name` with the given arguments and returns the result. 3. **is_instance_method(self, method_name)** - **Input:** `method_name` (String) - **Output:** Returns `True` if the specified method is an instance method of the class, `False` otherwise. Constraints and Requirements - You are strictly required to use the C-API functions provided in the documentation to manage and verify method objects. - Ensure that the methods added dynamically are properly bound and callable. - Properly handle and report any errors or invalid operations, e.g., trying to call a method that does not exist. Example Usage ```python # Define some functions to be dynamically added def greet(self, name): return f\\"Hello, {name}!\\" # Create an instance of DynamicMethodClass dmc = DynamicMethodClass() # Dynamically add methods dmc.add_method(greet) # Call the dynamically added method print(dmc.call_method(\'greet\', \'Alice\')) # Output: Hello, Alice! # Check if a method is an instance method print(dmc.is_instance_method(\'greet\')) # Output: True ``` Implement the `DynamicMethodClass` to fulfill the stated requirements.","solution":"class DynamicMethodClass: def __init__(self): self._methods = {} def add_method(self, func): Adds a function as an instance method of the class dynamically. :param func: A callable function or lambda to be added as an instance method. self._methods[func.__name__] = func.__get__(self) def call_method(self, method_name, *args, **kwargs): Calls the dynamically assigned method with the provided arguments. :param method_name: Name of the method to call. :param args: Positional arguments for the method. :param kwargs: Keyword arguments for the method. :return: The result of the method call. if method_name in self._methods: method = self._methods[method_name] return method(*args, **kwargs) else: raise AttributeError(f\\"Method \'{method_name}\' does not exist.\\") def is_instance_method(self, method_name): Checks if the specified method is an instance method of the class. :param method_name: Name of the method to check. :return: True if the method is an instance method, otherwise False. return method_name in self._methods"},{"question":"# Advanced Command-Line Interface Application **Objective:** Create a Python script using the `argparse` module that simulates a file processing utility with advanced command-line argument parsing. **Problem Statement:** You are tasked with writing a Python script named `file_processor.py` that takes various command-line arguments for processing files in a directory. The script should be able to: 1. List all files in the directory. 2. Search for a specific file by name. 3. Display detailed information about a specific file. 4. Optionally display output in a verbose or quiet manner. 5. Optionally apply a filter to list only files with a specific extension. **Requirements:** 1. **Positional Argument:** - `directory`: The directory where files are to be processed. 2. **Optional Arguments:** - `-l`, `--list`: List all files in the directory. - `-s`, `--search [filename]`: Search for a specific file by name in the directory. - `-d`, `--details [filename]`: Display detailed information (size and creation date) about a specific file. - `-e`, `--extension [ext]`: Filter the list command to only show files with the given extension. - `-v`, `--verbose`: Enable verbose output. - `-q`, `--quiet`: Enable quiet output. **Constraints:** - The script should provide appropriate help and error messages for invalid inputs. - The `-v` and `-q` options should be mutually exclusive. **Example Usage:** 1. List all files in a directory: ``` python file_processor.py /path/to/directory --list ``` 2. Search for a specific file: ``` python file_processor.py /path/to/directory --search myfile.txt ``` 3. Display detailed information about a file: ``` python file_processor.py /path/to/directory --details myfile.txt ``` 4. List only `.txt` files in a directory with verbose output: ``` python file_processor.py /path/to/directory --list --extension txt --verbose ``` **Implementation Details:** - Use the `argparse` module for parsing command-line arguments. - Use `os` and `os.path` modules to interact with the file system. - Include error handling for cases such as non-existent directories or files. Write the `file_processor.py` script according to the specifications above. **Expected Solution:** Below is an example of how the script could be structured: ```python import os import argparse from datetime import datetime def list_files(directory, ext=None): try: files = os.listdir(directory) if ext: files = [file for file in files if file.endswith(ext)] return files except FileNotFoundError: print(f\\"Error: The directory {directory} does not exist.\\") return [] def search_file(directory, filename): files = list_files(directory) if filename in files: return filename else: return None def file_details(directory, filename): filepath = os.path.join(directory, filename) if os.path.exists(filepath): size = os.path.getsize(filepath) creation_time = os.path.getctime(filepath) creation_date = datetime.fromtimestamp(creation_time) return size, creation_date else: print(f\\"Error: The file {filename} does not exist in {directory}.\\") return None, None def main(): parser = argparse.ArgumentParser(description=\\"File processing utility\\") parser.add_argument(\\"directory\\", help=\\"the directory where files are to be processed\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"enable verbose output\\") group.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"enable quiet output\\") parser.add_argument(\\"-l\\", \\"--list\\", action=\\"store_true\\", help=\\"list all files in the directory\\") parser.add_argument(\\"-s\\", \\"--search\\", metavar=\\"filename\\", help=\\"search for a specific file by name\\") parser.add_argument(\\"-d\\", \\"--details\\", metavar=\\"filename\\", help=\\"display detailed information about a specific file\\") parser.add_argument(\\"-e\\", \\"--extension\\", metavar=\\"ext\\", help=\\"filter list to show files with specific extension\\") args = parser.parse_args() if args.verbose: print(f\\"Processing directory: {args.directory}\\") if args.list: files = list_files(args.directory, args.extension) for file in files: print(file) if args.search: result = search_file(args.directory, args.search) if result: print(f\\"File found: {result}\\") else: print(f\\"File {args.search} not found.\\") if args.details: size, creation_date = file_details(args.directory, args.details) if size is not None: print(f\\"File: {args.details}\\") print(f\\"Size: {size} bytes\\") print(f\\"Creation date: {creation_date}\\") if __name__ == \\"__main__\\": main() ```","solution":"import os import argparse from datetime import datetime def list_files(directory, ext=None): try: files = os.listdir(directory) if ext: files = [file for file in files if file.endswith(ext)] return files except FileNotFoundError: print(f\\"Error: The directory {directory} does not exist.\\") return [] def search_file(directory, filename): files = list_files(directory) if filename in files: return filename else: return None def file_details(directory, filename): filepath = os.path.join(directory, filename) if os.path.exists(filepath): size = os.path.getsize(filepath) creation_time = os.path.getctime(filepath) creation_date = datetime.fromtimestamp(creation_time) return size, creation_date else: print(f\\"Error: The file {filename} does not exist in {directory}.\\") return None, None def main(): parser = argparse.ArgumentParser(description=\\"File processing utility\\") parser.add_argument(\\"directory\\", help=\\"the directory where files are to be processed\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"enable verbose output\\") group.add_argument(\\"-q\\", \\"--quiet\\", action=\\"store_true\\", help=\\"enable quiet output\\") parser.add_argument(\\"-l\\", \\"--list\\", action=\\"store_true\\", help=\\"list all files in the directory\\") parser.add_argument(\\"-s\\", \\"--search\\", metavar=\\"filename\\", help=\\"search for a specific file by name\\") parser.add_argument(\\"-d\\", \\"--details\\", metavar=\\"filename\\", help=\\"display detailed information about a specific file\\") parser.add_argument(\\"-e\\", \\"--extension\\", metavar=\\"ext\\", help=\\"filter list to show files with specific extension\\") args = parser.parse_args() if args.verbose and not args.quiet: print(f\\"Processing directory: {args.directory}\\") if args.list: files = list_files(args.directory, args.extension) if not args.quiet: for file in files: print(file) if args.verbose: print(f\\"Total files listed: {len(files)}\\") if args.search: result = search_file(args.directory, args.search) if result and not args.quiet: print(f\\"File found: {result}\\") elif not result and not args.quiet: print(f\\"File {args.search} not found.\\") if args.details: size, creation_date = file_details(args.directory, args.details) if size is not None and not args.quiet: print(f\\"File: {args.details}\\") print(f\\"Size: {size} bytes\\") print(f\\"Creation date: {creation_date}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Decimal Arithmetic with Custom Contexts **Objective:** Implement a function using the `decimal` module that computes a list of arithmetic operations on decimal numbers with a specified precision and rounding mode for each operation. Function Signature ```python def custom_decimal_operations(operations: list, precision: int, rounding: str) -> list: pass ``` Input - `operations`: A list of tuples where each tuple consists of an arithmetic expression as a string. Example: `(\\"10.5 * 2.3\\", \\"+\\")`. - Supported operations: addition `+`, subtraction `-`, multiplication `*`, division `/` - `precision`: An integer representing the precision to be used for the arithmetic operations. - `rounding`: A string representing the rounding mode to be used (must be one of: `\\"ROUND_CEILING\\"`, `\\"ROUND_DOWN\\"`, `\\"ROUND_FLOOR\\"`, `\\"ROUND_HALF_DOWN\\"`, `\\"ROUND_HALF_EVEN\\"`, `\\"ROUND_HALF_UP\\"`, `\\"ROUND_UP\\"`, `\\"ROUND_05UP\\"`). Output - Returns a list of results (as `Decimal` instances) of the specified operations, rounded and precise according to the given context. Constraints - You should validate input expressions and ensure that they are correctly formatted for the supported operations. - The precision and rounding values must be set before any arithmetic operations. - Handle any potential exceptions gracefully and return appropriate error messages for invalid operations or inputs. Example ```python from decimal import Decimal def custom_decimal_operations(operations: list, precision: int, rounding: str) -> list: from decimal import Decimal, getcontext, InvalidOperation results = [] # Set context precision and rounding ctx = getcontext() ctx.prec = precision ctx.rounding = rounding for expr, op in operations: try: number1, number2 = map(str.strip, expr.split(op)) dec1 = Decimal(number1) dec2 = Decimal(number2) if op == \'+\': result = dec1 + dec2 elif op == \'-\': result = dec1 - dec2 elif op == \'*\': result = dec1 * dec2 elif op == \'/\': result = dec1 / dec2 else: raise InvalidOperation(f\\"Unsupported operation \'{op}\'\\") results.append(result) except (InvalidOperation, ValueError) as e: results.append(f\\"Error: {e}\\") return results # Example usage operations = [(\\"10.5 + 2.3\\", \\"+\\"), (\\"15 - 2.5\\", \\"-\\"), (\\"3.1 * 4.2\\", \\"*\\"), (\\"10.5 / 3.1\\", \\"/\\")] precision = 5 rounding = \\"ROUND_HALF_UP\\" print(custom_decimal_operations(operations, precision, rounding)) # Expected output: [Decimal(\'12.80\'), Decimal(\'12.5\'), Decimal(\'13.02\'), Decimal(\'3.3871\')] ``` # Explanation The function `custom_decimal_operations` takes in a list of arithmetic expressions, a precision value, and a rounding mode, and returns a list of results rounded to the specified precision. The function creates a `Decimal` context with the specified precision and rounding, parses each arithmetic expression, performs the operation, and handles any potential exceptions gracefully.","solution":"from decimal import Decimal, getcontext, InvalidOperation def custom_decimal_operations(operations: list, precision: int, rounding: str) -> list: results = [] # Set context precision and rounding ctx = getcontext() ctx.prec = precision ctx.rounding = rounding for expr, op in operations: try: number1, number2 = map(str.strip, expr.split(op)) dec1 = Decimal(number1) dec2 = Decimal(number2) if op == \'+\': result = dec1 + dec2 elif op == \'-\': result = dec1 - dec2 elif op == \'*\': result = dec1 * dec2 elif op == \'/\': result = dec1 / dec2 else: raise InvalidOperation(f\\"Unsupported operation \'{op}\'\\") results.append(result) except (InvalidOperation, ValueError) as e: results.append(f\\"Error: {e}\\") return results"},{"question":"**Question: Implement and Compare LDA and QDA Classifiers** Given a multi-class dataset, your task is to implement a function that uses both Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) classifiers to classify the data. You should then compare the classification performance of both methods. # Function Signature ```python def lda_qda_comparison(X_train, y_train, X_test, y_test): Implement and compare LDA and QDA classifiers. Parameters: - X_train (numpy.ndarray): Training feature data of shape (n_samples_train, n_features). - y_train (numpy.ndarray): Training labels of shape (n_samples_train,). - X_test (numpy.ndarray): Test feature data of shape (n_samples_test, n_features). - y_test (numpy.ndarray): Test labels of shape (n_samples_test,). Returns: - dict: A dictionary with the keys \'lda_accuracy\' and \'qda_accuracy\' containing the accuracy scores for LDA and QDA classifiers respectively. pass ``` # Requirements 1. **Implement LDA and QDA**: - Use scikit-learn\'s `LinearDiscriminantAnalysis` and `QuadraticDiscriminantAnalysis` classes. - Train both classifiers on the provided training data (`X_train`, `y_train`). - Test both classifiers on the provided test data (`X_test`, `y_test`). 2. **Compare Performance**: - Calculate the accuracy of each classifier on the test data. - Return a dictionary with accuracy scores: ```python { \\"lda_accuracy\\": float, \\"qda_accuracy\\": float } ``` # Constraints - Assume the dataset provided has a minimum of two classes. - The input arrays are guaranteed to be numeric and properly shaped. - You may assume that the scikit-learn library is pre-installed. # Example ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load a sample dataset data = load_iris() X, y = data.data, data.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define the function to compare LDA and QDA def lda_qda_comparison(X_train, y_train, X_test, y_test): from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) lda_predictions = lda.predict(X_test) lda_accuracy = accuracy_score(y_test, lda_predictions) qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) qda_predictions = qda.predict(X_test) qda_accuracy = accuracy_score(y_test, qda_predictions) return { \\"lda_accuracy\\": lda_accuracy, \\"qda_accuracy\\": qda_accuracy } # Compare and print the accuracies results = lda_qda_comparison(X_train, y_train, X_test, y_test) print(f\\"LDA Accuracy: {results[\'lda_accuracy\']}\\") print(f\\"QDA Accuracy: {results[\'qda_accuracy\']}\\") ``` This function will help assess students\' understanding of implementing and comparing classification models using scikit-learn. It will also test their ability to use the appropriate tools for model evaluation.","solution":"def lda_qda_comparison(X_train, y_train, X_test, y_test): from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.metrics import accuracy_score # Initialize and fit the LDA classifier lda = LinearDiscriminantAnalysis() lda.fit(X_train, y_train) lda_predictions = lda.predict(X_test) lda_accuracy = accuracy_score(y_test, lda_predictions) # Initialize and fit the QDA classifier qda = QuadraticDiscriminantAnalysis() qda.fit(X_train, y_train) qda_predictions = qda.predict(X_test) qda_accuracy = accuracy_score(y_test, qda_predictions) # Return the accuracy scores for LDA and QDA return { \\"lda_accuracy\\": lda_accuracy, \\"qda_accuracy\\": qda_accuracy }"},{"question":"# Filename Pattern Matching with `fnmatch` Objective Your task is to implement a function that takes a list of filenames and a list of patterns, and returns a dictionary where each pattern is a key and its value is a list of filenames that match this pattern. Details You will utilize the `fnmatch` module functions to solve this problem. Function Signature ```python def match_patterns(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]: pass ``` Input - `filenames`: A list of strings, where each string represents a filename (1 ≤ len(filenames) ≤ 10^3). - `patterns`: A list of strings, where each string represents a pattern (1 ≤ len(patterns) ≤ 10^2). Output - A dictionary where each key is a pattern from the input list `patterns`, and the value is a list of filenames from the input list `filenames` that match this pattern. Constraints - Filenames and patterns will contain only ASCII characters. - Each filename and pattern will have a maximum length of 100 characters. Example ```python filenames = [\\"test.txt\\", \\"data.csv\\", \\"report.docx\\", \\"table.xls\\", \\"image.png\\"] patterns = [\\"*.txt\\", \\"*.csv\\", \\"*.docx\\"] result = match_patterns(filenames, patterns) # Expected Result: # { # \\"*.txt\\": [\\"test.txt\\"], # \\"*.csv\\": [\\"data.csv\\"], # \\"*.docx\\": [\\"report.docx\\"] # } ``` Notes - You should use the `fnmatch` or `fnmatchcase` functions from the `fnmatch` module. - The order of filenames in the output lists should follow the order they appear in the input `filenames` list. Implementation Implement the function `match_patterns` to solve the problem. ```python from typing import List, Dict import fnmatch def match_patterns(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]: result = {} for pattern in patterns: matched_files = fnmatch.filter(filenames, pattern) result[pattern] = matched_files return result ```","solution":"from typing import List, Dict import fnmatch def match_patterns(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]: result = {} for pattern in patterns: matched_files = fnmatch.filter(filenames, pattern) result[pattern] = matched_files return result"},{"question":"# Question: Documentation Summarizer and HTML Exporter You are tasked with implementing a Python function that uses the \\"pydoc\\" module to generate a summary of the documentation for a given module and saves it as an HTML file. The function must also be able to serve this HTML documentation via an HTTP server. Here are the requirements: 1. The function should accept the name of a module (as a string) and a directory path where the HTML file should be saved. 2. The summary should include: - The module\'s name. - The first line of the module\'s documentation. - A list of all the functions and classes in the module with their one-line descriptions. 3. The HTML file should be named in the format: `<module_name>_summary.html`. 4. The function should start an HTTP server to serve the HTML file on a specified port. If no port is specified, it should choose an arbitrary unused port. Input 1. `module_name`: A string representing the name of the module to be documented. 2. `output_dir`: A string representing the path to the directory where the HTML file should be saved. 3. `port`: An optional integer, representing the port number on which the HTTP server will serve the documentation. If not provided, choose an arbitrary unused port. Output - The function should return the URL where the HTML documentation is being served. Implementation ```python import os import pydoc def generate_and_serve_doc(module_name: str, output_dir: str, port: int = 0) -> str: Generate a summary of the documentation for the given module and save it as an HTML file. Serve the HTML file via an HTTP server. Args: - module_name (str): The name of the module to document. - output_dir (str): The directory where the HTML file should be saved. - port (int, optional): The port number to serve the documentation on. Defaults to 0. Returns: - str: The URL where the HTML documentation is being served. # Your implementation here # Example usage: # url = generate_and_serve_doc(\'sys\', \'/path/to/output_dir\') # print(f\\"Documentation is being served at: {url}\\") ``` Constraints - The function should handle cases where the module does not exist gracefully and return a meaningful error message. - Ensure the HTML file is properly formatted and includes all required elements. - The function must start the server asynchronously so that it doesn\'t block further execution.","solution":"import os import pydoc import http.server import socketserver import threading from typing import Optional def generate_html_summary(doc: str, module_name: str) -> str: Generate the HTML content for the summary of the module documentation. Args: - doc (str): The full documentation string of the module. - module_name (str): The name of the module. Returns: - str: Formatted HTML content. doc_lines = doc.splitlines() first_line = doc_lines[0] if doc_lines else \'No description available.\' functions_and_classes = [] in_functions_section = False for line in doc_lines: if \'FUNCTIONS\' in line or \'Classes\' in line: in_functions_section = True elif in_functions_section: if line.strip() == \'\': in_functions_section = False else: functions_and_classes.append(line.strip()) functions_and_classes = [line for line in functions_and_classes if line] html_content = f<!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <title>{module_name} Summary</title> </head> <body> <h1>Module: {module_name}</h1> <p>{first_line}</p> <h2>Functions and Classes</h2> <ul> for item in functions_and_classes: html_content += f\\"<li>{item}</li>n\\" html_content += </ul> </body> </html> return html_content def generate_and_serve_doc(module_name: str, output_dir: str, port: Optional[int] = 0) -> str: Generate a summary of the documentation for the given module and save it as an HTML file. Serve the HTML file via an HTTP server. Args: - module_name (str): The name of the module to document. - output_dir (str): The directory where the HTML file should be saved. - port (int, optional): The port number to serve the documentation on. Defaults to 0. Returns: - str: The URL where the HTML documentation is being served. try: module_doc = pydoc.render_doc(module_name, renderer=pydoc.plaintext) except ImportError: return f\\"Module \'{module_name}\' not found.\\" html_content = generate_html_summary(module_doc, module_name) file_path = os.path.join(output_dir, f\\"{module_name}_summary.html\\") with open(file_path, \'w\') as file: file.write(html_content) os.chdir(output_dir) handler = http.server.SimpleHTTPRequestHandler with socketserver.TCPServer((\\"\\", port), handler) as httpd: chosen_port = httpd.server_address[1] url = f\\"http://localhost:{chosen_port}/{module_name}_summary.html\\" threading.Thread(target=httpd.serve_forever, daemon=True).start() return url # Example usage: # url = generate_and_serve_doc(\'sys\', \'/path/to/output_dir\') # print(f\\"Documentation is being served at: {url}\\")"},{"question":"You are tasked with extending the functionality of the email content management provided in the `email.contentmanager` module. Specifically, you need to create a new content manager that can handle JSON data within email messages. # Requirements 1. **Class Definition**: - Implement a class `JSONContentManager` that inherits from `email.contentmanager.ContentManager`. 2. **Methods to Implement**: - `get_content(msg, *args, **kw)`: This method should handle email parts with the MIME type `application/json`. It should decode the JSON content and return it as a Python dictionary. Raise a `TypeError` if the content is not valid JSON. - `set_content(msg, obj, *args, **kw)`: This method should handle Python dictionaries and store them as JSON content in the email message, setting appropriate headers such as `Content-Type: application/json`. 3. **Handler Registration**: - Register the appropriate handlers for `get_content` and `set_content` within the class so that they are automatically used when JSON data or dictionaries are encountered. 4. **Constraints**: - Ensure that other MIME types or objects are not affected by your implementation. - Use `utf-8` as the default encoding for text content. # Input and Output Formats - **Input**: - `get_content`: Takes an email message object (`msg`) which contains JSON content. - `set_content`: Takes an email message object (`msg`) and a Python dictionary (`obj`) to be set as the content. - **Output**: - `get_content`: Returns a Python dictionary representing the JSON content. - `set_content`: Modifies the email message object to include the JSON content and headers. # Performance Requirements - Your implementation should handle large JSON content efficiently. - Ensure that adding and retrieving content is performed in a timely manner without unnecessary overhead. # Example Here is a minimal example to illustrate the usage of `JSONContentManager`: ```python from email.message import EmailMessage from email.contentmanager import JSONContentManager # Create an instance of your JSONContentManager content_manager = JSONContentManager() # Create a test email message msg = EmailMessage() # Set content to JSON data data = {\'key\': \'value\'} content_manager.set_content(msg, data) # Extract the JSON content back as a Python dictionary extracted_data = content_manager.get_content(msg) print(extracted_data) # Output should be: {\'key\': \'value\'} ``` Implement the `JSONContentManager` class as specified above. Ensure to handle errors and edge cases appropriately.","solution":"import json from email.contentmanager import ContentManager from email.mime.application import MIMEApplication from email import message as email_message class JSONContentManager(ContentManager): def get_content(self, msg, *args, **kw): if msg.get_content_type() != \'application/json\': raise TypeError(\'The provided message content is not of type application/json\') payload = msg.get_payload(decode=True) if payload is None: raise ValueError(\'Payload is None, cannot decode JSON\') try: return json.loads(payload.decode(\'utf-8\')) except json.JSONDecodeError as e: raise TypeError(\'The JSON content could not be decoded\') from e def set_content(self, msg, obj, *args, **kw): if not isinstance(obj, dict): raise TypeError(\'Object must be a dictionary\') json_content = json.dumps(obj, ensure_ascii=False).encode(\'utf-8\') msg.set_payload(json_content) msg.set_type(\'application/json\') msg.set_charset(\'utf-8\')"},{"question":"**Question: Advanced Dictionary Operations** You are required to implement a function that performs various operations on a dictionary object. This will test your understanding of dictionary manipulations and error handling in Python. # Function Signature ```python def advanced_dict_operations(input_dict: dict) -> dict: \'\'\' This function takes a dictionary as input and performs a series of operations to generate and return a new dictionary based on the described rules. :param input_dict: dict - A dictionary with keys and values of any hashable and JSON serializable types. :return: dict - A new dictionary with specific transformations. \'\'\' ``` # Operations and Rules 1. **Key Type Check**: Ensure all keys in the dictionary are strings. If they are not, convert all keys to strings by calling `str()` on them. 2. **Value Type Check and Conversion**: - If a value is an integer, convert it to its string equivalent. - If a value is a list, any integer elements in the list should be converted to their string equivalents. - If a value is another dictionary, recursively apply the same rules as above. 3. **Filter Keys**: Remove any key that starts with an underscore (\\"_\\"). 4. **Add New Keys**: Add two new key-value pairs: - `\\"num_keys\\"`: The total number of keys in the transformed dictionary. - `\\"num_int_values\\"`: The total number of integer values found in the original and nested dictionaries, before conversion. # Example ```python input_dict = { \\"_internal_key\\": 123, \\"valid_key\\": \\"hello\\", \\"another_key\\": [\\"world\\", 456], \\"nested_dict\\": { \\"number\\": 789, \\"_hidden\\": \\"secret\\", \\"more_numbers\\": [101, 102] } } output = advanced_dict_operations(input_dict) print(output) ``` **Output** ```python { \\"valid_key\\": \\"hello\\", \\"another_key\\": [\\"world\\", \\"456\\"], \\"nested_dict\\": { \\"number\\": \\"789\\", \\"more_numbers\\": [\\"101\\", \\"102\\"] }, \\"num_keys\\": 4, \\"num_int_values\\": 4 } ``` # Constraints - You can assume that input dictionaries will have at most a depth of 3 (i.e., dictionaries nested inside dictionaries up to 2 levels). - Values inside lists will be either strings, integers, or dictionaries as described. # Notes - Your function should handle both the base dictionary and any nested dictionaries within it. - You may find it helpful to write helper functions for checking types and converting values. Good luck!","solution":"def advanced_dict_operations(input_dict: dict) -> dict: Transforms an input dictionary according to specified rules. def transform_value(value, int_count): Transforms a single value according to the specified rules. if isinstance(value, int): int_count[0] += 1 return str(value) elif isinstance(value, list): return [transform_value(item, int_count) for item in value] elif isinstance(value, dict): return transform_dict(value, int_count) else: return value def transform_dict(d, int_count): Transforms the dictionary according to the specified rules. new_dict = {} for key, value in d.items(): if not key.startswith(\'_\'): new_key = str(key) new_value = transform_value(value, int_count) new_dict[new_key] = new_value return new_dict int_count = [0] transformed_dict = transform_dict(input_dict, int_count) transformed_dict[\\"num_keys\\"] = len(transformed_dict) transformed_dict[\\"num_int_values\\"] = int_count[0] return transformed_dict"},{"question":"**Objective:** Demonstrate your understanding of the `unittest` module in Python by creating unit tests for a class that performs common string manipulations. **Problem Statement:** You are provided with a class `StringManipulator` that performs different string manipulations. Your task is to write a comprehensive suite of unit tests using the `unittest` framework. Cover both basic tests and advanced features like setup/teardown, skipping tests, and expected failures. ```python class StringManipulator: def __init__(self, initial_string=\\"\\"): self.string = initial_string def to_upper(self): return self.string.upper() def is_upper(self): return self.string.isupper() def add_suffix(self, suffix): return self.string + suffix def substring(self, start, end): if not isinstance(start, int) or not isinstance(end, int): raise TypeError(\\"start and end must be integers\\") return self.string[start:end] ``` **Task:** 1. Implement a new class `TestStringManipulator` that subclasses `unittest.TestCase`. 2. Write test methods for the following string manipulation methods: - `to_upper()`, `is_upper()`, `add_suffix(suffix)`, `substring(start, end)` 3. Write setup (`setUp()`) and teardown (`tearDown()`) methods to initialize and clean up the environment for each test. 4. Use `subTest()` to create multiple sub-tests within a single test method for `add_suffix()`. 5. Implement a method that will be skipped using the `@unittest.skip` decorator. 6. Create a method where a known bug exists, and use the `@unittest.expectedFailure` decorator. 7. Instantiate `unittest.TestSuite` and add your test cases to the suite. 8. Use `unittest.TextTestRunner` to execute your test suite. **Constraints:** - Test methods should follow the naming convention `test_<method_name>()`. - Use appropriate assert methods provided by `unittest.TestCase`. **Example:** Below is an example of how to start your test class. ```python import unittest class TestStringManipulator(unittest.TestCase): def setUp(self): self.string_manip = StringManipulator(\\"hello\\") def tearDown(self): del self.string_manip def test_to_upper(self): self.assertEqual(self.string_manip.to_upper(), \\"HELLO\\") def test_is_upper(self): self.assertFalse(self.string_manip.is_upper()) self.string_manip = StringManipulator(\\"HELLO\\") self.assertTrue(self.string_manip.is_upper()) @unittest.skip(\\"Skip this test method\\") def test_skipped(self): pass # This test will be skipped. @unittest.expectedFailure def test_known_issue(self): self.assertEqual(self.string_manip.substring(0, 10), \\"HELLO\\") # known issue def test_add_suffix(self): for suffix, expected in [(\\" world\\", \\"hello world\\"), (\\"!\\", \\"hello!\\")]: with self.subTest(suffix=suffix): self.assertEqual(self.string_manip.add_suffix(suffix), expected) def test_substring(self): self.assertEqual(self.string_manip.substring(0, 2), \\"he\\") with self.assertRaises(TypeError): self.string_manip.substring(\\"start\\", \\"end\\") if __name__ == \'__main__\': suite = unittest.TestSuite() suite.addTest(unittest.makeSuite(TestStringManipulator)) runner = unittest.TextTestRunner() runner.run(suite) ``` Implement the remaining methods and ensure your tests pass successfully.","solution":"class StringManipulator: def __init__(self, initial_string=\\"\\"): self.string = initial_string def to_upper(self): return self.string.upper() def is_upper(self): return self.string.isupper() def add_suffix(self, suffix): return self.string + suffix def substring(self, start, end): if not isinstance(start, int) or not isinstance(end, int): raise TypeError(\\"start and end must be integers\\") return self.string[start:end]"},{"question":"**Question: Titanic Dataset Visualization using Seaborn** You are provided with the Titanic dataset, which contains various details about passengers onboard the Titanic. Your task is to use the seaborn package to create specific visualizations that provide insights into the dataset. # Requirements: 1. **Loading the Dataset:** Load the Titanic dataset provided by seaborn. 2. **Visualization 1: Bar Plot** Create a bar plot to show the count of passengers in different classes (`class`). Use different colors to distinguish between males and females (`sex`). Ensure the bars are stacked to show the total count in each class distinguished by sex. 3. **Visualization 2: Faceted Histogram** Create a faceted histogram to show the distribution of passengers\' ages (`age`) separately for males and females (`sex`). Use different shades to indicate whether the passengers survived (`alive`). # Implementation: Implement the function `create_titanic_visualizations()` which performs the following: - Loads the Titanic dataset. - Creates and displays the bar plot described in Visualization 1. - Creates and displays the faceted histogram described in Visualization 2. Function Signature ```python def create_titanic_visualizations() -> None: pass ``` # Constraints: - Use only the provided seaborn functionality (you may import additional seaborn components as needed). - Ensure the plots are displayed using appropriate function calls. # Expected Output: Your function should display two plots: 1. A stacked bar plot showing the count of passengers in different classes by sex. 2. A faceted histogram showing the age distribution by sex and survival status. Here is some code to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset def create_titanic_visualizations(): # Load the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Visualization 1: Bar Plot so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()).show() # Visualization 2: Faceted Histogram ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) .show() ) # Run the function to create visualizations create_titanic_visualizations() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_visualizations(): Loads the Titanic dataset and creates two visualizations: 1. A stacked bar plot showing the count of passengers in different classes by sex. 2. A faceted histogram showing the age distribution by sex and survival status. # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Visualization 1: Bar Plot plt.figure(figsize=(10, 6)) sns.histplot(data=titanic, x=\\"class\\", hue=\\"sex\\", multiple=\\"stack\\", shrink=.8, palette=\\"Set2\\") plt.title(\\"Count of Passengers in Different Classes by Sex\\") plt.ylabel(\\"Count\\") plt.xlabel(\\"Class\\") plt.legend(title=\\"Sex\\") plt.show() # Visualization 2: Faceted Histogram g = sns.FacetGrid(titanic, col=\\"sex\\", hue=\\"alive\\", palette=\\"viridis\\", height=5, aspect=1.2) g.map(sns.histplot, \'age\', binwidth=10, kde=False, multiple=\\"stack\\", alpha=0.7) g.add_legend(title=\\"Survived\\") g.set_axis_labels(\\"Age\\", \\"Count\\") plt.subplots_adjust(top=0.85) g.fig.suptitle(\\"Age Distribution of Passengers by Sex and Survival Status\\") plt.show()"},{"question":"# HTTP Client Library Task Objective You are required to implement a function that fetches data from a given HTTP/HTTPS URL using Python\'s `http.client` module. This function should handle different types of request methods (`GET`, `POST`, `PUT`) and appropriately handle the responses, including error handling for common HTTP errors. Function Signature ```python def fetch_data(url: str, method: str = \\"GET\\", headers: dict = None, body: str = None) -> tuple: Fetch data from a given URL using specified HTTP method, headers, and body. Parameters: - url (str): The URL to fetch data from. - method (str): The HTTP method to use (\'GET\', \'POST\', \'PUT\'). Default is \'GET\'. - headers (dict): Additional headers to include in the request. Default is None. - body (str): The body of the request (for \'POST\' and \'PUT\' methods). Default is None. Returns: - tuple: A tuple containing the status code, reason phrase, and response body (status_code, reason, response_body). ``` Requirements 1. **URL Parsing**: - Use the `urllib.parse` module to extract the host, port, and path from the provided URL. 2. **HTTP Methods**: - Implement functionality for `GET`, `POST`, and `PUT` methods. - Other methods should raise a `ValueError`. 3. **Request Handling**: - Add appropriate headers and body to the request. - Send the request and fetch the response. 4. **Error Handling**: - Appropriately handle exceptions as defined in the `http.client` module. - Return meaningful response in case of errors. 5. **Response Parsing**: - Parse the response to extract status code, reason phrase, and response body. 6. **HTTPS Support**: - The function should support both HTTP and HTTPS connections. Constraints - Ensure compliance with the HTTP/1.1 protocol. - Manage connection timeouts (set a reasonable timeout such as 10 seconds). - Implement robust error handling for various `http.client` exceptions. Examples ```python # Example 1: Simple GET request to a HTTP URL status_code, reason, response_body = fetch_data(\\"http://www.example.com\\") print(status_code, reason) print(response_body) # Example 2: POST request with headers and body to a HTTPS URL headers = {\\"Content-Type\\": \\"application/json\\"} body = \'{\\"key\\":\\"value\\"}\' status_code, reason, response_body = fetch_data(\\"https://www.example.com/data\\", method=\\"POST\\", headers=headers, body=body) print(status_code, reason) print(response_body) ``` Notes - Remember to close connections properly after fetching the response. - You can assume that the given URLs are well-formed. - Focus on clarity, robustness, and adherence to HTTP protocol in your implementation.","solution":"import http.client from urllib.parse import urlparse def fetch_data(url: str, method: str = \\"GET\\", headers: dict = None, body: str = None) -> tuple: Fetch data from a given URL using specified HTTP method, headers, and body. Parameters: - url (str): The URL to fetch data from. - method (str): The HTTP method to use (\'GET\', \'POST\', \'PUT\'). Default is \'GET\'. - headers (dict): Additional headers to include in the request. Default is None. - body (str): The body of the request (for \'POST\' and \'PUT\' methods). Default is None. Returns: - tuple: A tuple containing the status code, reason phrase, and response body (status_code, reason, response_body). parsed_url = urlparse(url) connection_class = http.client.HTTPSConnection if parsed_url.scheme == \'https\' else http.client.HTTPConnection connection = connection_class(parsed_url.hostname, parsed_url.port, timeout=10) path = parsed_url.path or \'/\' if parsed_url.query: path += \'?\' + parsed_url.query headers = headers or {} try: if method not in [\'GET\', \'POST\', \'PUT\']: raise ValueError(\\"Unsupported HTTP method\\") if method in [\'POST\', \'PUT\'] and body: connection.request(method, path, body=body, headers=headers) else: connection.request(method, path, headers=headers) response = connection.getresponse() status_code = response.status reason = response.reason response_body = response.read().decode() return status_code, reason, response_body except http.client.HTTPException as e: return e.code if hasattr(e, \'code\') else 500, str(e), \'\' finally: connection.close()"},{"question":"You are given a dataset and your task is to create a multi-faceted grid using the Seaborn library. You need to showcase your understanding of different faceting dimensions and various customization techniques provided by Seaborn. # Problem Statement Given the `tips` dataset from Seaborn\'s in-built datasets, create a multi-faceted grid of scatter plots that shows the relationship between the `total_bill` and `tip` columns. You should visualize the data split on two dimensions (`time` and `smoker`) and introduce a third dimension through color (`sex`). Additionally, customize the grid with appropriate axis labels, legends, and other visual elements to ensure clarity. # Input and Output Specifications - **Input**: The Seaborn `tips` dataset should be used, which can be loaded using `sns.load_dataset(\\"tips\\")`. - **Output**: A multi-faceted grid of scatter plots showing the relationship between `total_bill` and `tip` split by `time`, `smoker`, and colored by `sex`. # Constraints - You must use the Seaborn library, particularly `FacetGrid`. - Ensure the figure size is large enough to display all plots clearly. - Provide meaningful axis labels and a legend. # Performance requirements - The solution should efficiently handle the size of the `tips` dataset. - The visualizations should be crisp and clear without overloading visual elements. # Example Below is an example of the expected visualization (note: this is a textual representation): ``` +---------------------------------------------------+ | Time: Lunch Time: Dinner | | | | Smoker: Yes Smoker: No Smoker: Yes | | | | Scatter plot Scatter plot Scatter plot | | (colored by sex) (colored by sex) (colored by | | sex) | | | | Axis labels and legend | +---------------------------------------------------+ ``` # Instructions 1. Load the `tips` dataset. 2. Set up a `FacetGrid` with `row` as `time`, `col` as `smoker`, and `hue` as `sex`. 3. Use `scatterplot` to visualize the relationship between `total_bill` and `tip`. 4. Add a legend, axis labels, and adjust figure size for clarity. 5. Save and display the plot. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # FacetGrid with row=\'time\', col=\'smoker\', hue=\'sex\' g = sns.FacetGrid(tips, row=\\"time\\", col=\\"smoker\\", hue=\\"sex\\", height=4, aspect=1.5) # Map the scatterplot g.map(sns.scatterplot, \\"total_bill\\", \\"tip\\") # Add legend, labels, and adjust the spacing g.add_legend() g.set_axis_labels(\\"Total Bill (US Dollars)\\", \\"Tip (US Dollars)\\") g.set_titles(col_template=\\"{col_name} Smoker\\", row_template=\\"{row_name} Time\\") g.figure.subplots_adjust(wspace=0.2, hspace=0.2) # Show the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_multi_faceted_grid(): Creates and displays a multi-faceted grid of scatter plots showing the relationship between total_bill and tip, split by time, smoker, and colored by sex. # Load the dataset tips = sns.load_dataset(\\"tips\\") # Setup the FacetGrid g = sns.FacetGrid(tips, row=\\"time\\", col=\\"smoker\\", hue=\\"sex\\", height=4, aspect=1.5) # Map the scatterplot g.map(sns.scatterplot, \\"total_bill\\", \\"tip\\") # Add legend, labels, and adjust the spacing g.add_legend() g.set_axis_labels(\\"Total Bill (US Dollars)\\", \\"Tip (US Dollars)\\") g.set_titles(col_template=\\"{col_name} Smoker\\", row_template=\\"{row_name} Time\\") g.figure.subplots_adjust(wspace=0.2, hspace=0.2) # Show the plot plt.show()"},{"question":"# PyTorch XPU Device Management and Computation Objective: You are required to write a function in PyTorch that utilizes the `torch.xpu` functionalities for device and stream management, as well as for managing random number generators for reproducibility. This function should perform matrix multiplication on available XPU devices while optimizing memory usage and ensuring reproducible results. Function Signature: ```python def xpu_matrix_multiplication(matrix_a: torch.Tensor, matrix_b: torch.Tensor, seed: int) -> torch.Tensor: Perform matrix multiplication using PyTorch XPU devices, optimizing memory usage and ensuring reproducibility. :param matrix_a: A 2D tensor (matrix) to be multiplied. :param matrix_b: A 2D tensor (matrix) to be multiplied. :param seed: An integer value to seed the random number generator for reproducibility. :return: The resulting tensor after matrix multiplication. pass ``` Inputs: - `matrix_a`: a 2D tensor of shape `(M, N)`. Constraints: `2 <= M, N <= 2000`. - `matrix_b`: a 2D tensor of shape `(N, P)`. Constraints: `2 <= N, P <= 2000`. - `seed`: an integer used to seed the random number generator to ensure reproducibility. Outputs: - A 2D tensor `(M, P)`: the result of the matrix multiplication `matrix_a @ matrix_b`. Constraints & Requirements: 1. If `torch.xpu.is_available()` returns `False`, your function should raise an `EnvironmentError`. 2. Seed the random number generator for all XPUs using the provided `seed`. 3. Use multiple XPU devices if available for parallel computation, ensuring even distribution of workload. 4. Track and optimize memory usage to avoid exceeding available memory (`torch.xpu.memory`). 5. Ensure the function returns the correct result in a reasonable time for maximum matrix sizes. Example Usage: ```python import torch # Example matrices matrix_a = torch.rand((1000, 200)) matrix_b = torch.rand((200, 500)) seed = 42 result = xpu_matrix_multiplication(matrix_a, matrix_b, seed) print(result.shape) # Should print torch.Size([1000, 500]) ``` **Notes:** - Utilize functions and modules provided by `torch.xpu` to implement the required features. - Be efficient and manage device memory carefully to avoid resource allocation errors.","solution":"import torch def xpu_matrix_multiplication(matrix_a: torch.Tensor, matrix_b: torch.Tensor, seed: int) -> torch.Tensor: Perform matrix multiplication using PyTorch XPU devices, optimizing memory usage and ensuring reproducibility. :param matrix_a: A 2D tensor (matrix) to be multiplied. :param matrix_b: A 2D tensor (matrix) to be multiplied. :param seed: An integer value to seed the random number generator for reproducibility. :return: The resulting tensor after matrix multiplication. if not torch.xpu.is_available(): raise EnvironmentError(\\"XPU is not available\\") # Set the seed for reproducibility torch.manual_seed(seed) # Check if multiple XPUs are available num_xpus = torch.xpu.device_count() assert num_xpus > 0, \\"No XPU devices found.\\" # Run the matrix multiplication on the available XPUs device = torch.device(\'xpu\') # Move matrices to XPU matrix_a_xpu = matrix_a.to(device) matrix_b_xpu = matrix_b.to(device) # Perform matrix multiplication result_xpu = torch.matmul(matrix_a_xpu, matrix_b_xpu) # Move result back to CPU result_cpu = result_xpu.cpu() return result_cpu"},{"question":"# Python310 Mapping Protocol Assessment Objective Your task is to create a Python class `CustomDict` that mimics key functionalities of a Python dictionary using the `python310` mapping protocol functions. Description The `CustomDict` class should support the following methods: 1. `__init__`: Initializes the custom dictionary. 2. `check_mapping`: Returns `True` if the object supports the mapping protocol, otherwise `False`. 3. `size`: Returns the number of key-value pairs in the custom dictionary. 4. `get_item`: Returns the value associated with a given key. 5. `set_item`: Sets the value for a given key. 6. `del_item`: Deletes the key-value pair for a given key. 7. `has_key`: Checks if a given key is present in the custom dictionary. 8. `keys`: Returns a list of all keys. 9. `values`: Returns a list of all values. 10. `items`: Returns a list of key-value pairs. Constraints - You must use the corresponding `python310` functions for each method. - Handle potential errors gracefully and ensure `NULL` or failure returns appropriate error messages or handling in your methods. Expected Inputs and Outputs - `__init__(self)`: No inputs, initializes the mapping object. - `check_mapping(self) -> bool`: Returns a boolean indicating if the object supports mapping. - `size(self) -> int`: Returns an integer representing the number of key-value pairs. - `get_item(self, key: str) -> Any`: Returns the value associated with `key`. - `set_item(self, key: str, value: Any) -> None`: Sets the `value` for `key`. - `del_item(self, key: str) -> None`: Deletes the key-value pair associated with `key`. - `has_key(self, key: str) -> bool`: Returns a boolean indicating if `key` is in the dictionary. - `keys(self) -> list`: Returns a list of all keys. - `values(self) -> list`: Returns a list of all values. - `items(self) -> list`: Returns a list of key-value pairs. Example Usage ```python # Initialize the custom dictionary cd = CustomDict() # Check if it supports mapping print(cd.check_mapping()) # Expected: True # Add some items cd.set_item(\\"a\\", 1) cd.set_item(\\"b\\", 2) # Get the size print(cd.size()) # Expected: 2 # Retrieve an item print(cd.get_item(\\"a\\")) # Expected: 1 # Check for a key print(cd.has_key(\\"b\\")) # Expected: True print(cd.has_key(\\"c\\")) # Expected: False # Get all keys print(cd.keys()) # Expected: [\\"a\\", \\"b\\"] # Get all values print(cd.values()) # Expected: [1, 2] # Get all items print(cd.items()) # Expected: [(\\"a\\", 1), (\\"b\\", 2)] # Delete an item cd.del_item(\\"a\\") # Get the size after deletion print(cd.size()) # Expected: 1 ``` Notes - You may need to use C extensions if `python310` functions are not available directly in Python. Consider pseudo-codes for implementation details where specific functions are required.","solution":"class CustomDict: def __init__(self): self._dict = {} def check_mapping(self): return hasattr(self._dict, \\"__getitem__\\") and hasattr(self._dict, \\"keys\\") def size(self): return len(self._dict) def get_item(self, key): return self._dict.get(key, None) def set_item(self, key, value): self._dict[key] = value def del_item(self, key): if key in self._dict: del self._dict[key] def has_key(self, key): return key in self._dict def keys(self): return list(self._dict.keys()) def values(self): return list(self._dict.values()) def items(self): return list(self._dict.items())"},{"question":"**Question: Secure Text File Hashing** You are tasked with creating a secure utility to calculate the hash of a text file using Python. To ensure the utility is secure and prevents common vulnerabilities, follow the guidelines below: 1. **Use the `hashlib` module**: This is a secure way to hash content. However, avoid using any algorithms that may be insecure. Implement a function that takes the path to a text file and returns its SHA-256 hash. * Do not use deprecated or insecure hash functions. * Ensure that the function fails gracefully if an insecure algorithm is attempted. 2. **Security Considerations**: * Avoid security vulnerabilities in handling file operations. * Handle exceptions and errors gracefully. * Use `with` statements to handle file operations securely. # Function Signature ```python def secure_file_hash(file_path: str) -> str: pass ``` # Input * `file_path` (str): A string representing the path to the text file that needs to be hashed. # Output * (str): A string representing the SHA-256 hash of the file content. # Constraints * Assume the file is a text file and can be large. * Only use the SHA-256 algorithm from the `hashlib` module. * Handle any potential exceptions, such as file not found or permission errors. # Example ```python # Example text file content # \\"Hello World\\" file_path = \\"example.txt\\" print(secure_file_hash(file_path)) # Expected output (The hash will vary based on the content of the file) # \'a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e\' ``` # Notes * Make sure to read the file safely and efficiently, handling potential errors. * Validate the computed hash using SHA-256.","solution":"import hashlib def secure_file_hash(file_path: str) -> str: Returns the SHA-256 hash of the content of the given text file. Args: file_path (str): Path to the text file. Returns: str: The SHA-256 hash of the file content. Raises: FileNotFoundError: If the file does not exist. PermissionError: If there are permissions issues. try: sha256_hash = hashlib.sha256() with open(file_path, \\"rb\\") as f: # Read the file in chunks to handle larger files for byte_block in iter(lambda: f.read(4096), b\\"\\"): sha256_hash.update(byte_block) return sha256_hash.hexdigest() except FileNotFoundError: raise FileNotFoundError(f\\"The file at path {file_path} was not found.\\") except PermissionError: raise PermissionError(f\\"Permission denied for file at path {file_path}.\\") except Exception as e: raise Exception(f\\"An error occurred: {str(e)}\\")"},{"question":"**Title:** Advanced Asynchronous Task Management with asyncio **Objective:** Demonstrate your understanding of Python\'s asyncio package by implementing a function that concurrently processes data from multiple sources with timeouts and task cancellations. **Description:** You are tasked with simulating the fetching of data from multiple remote sources. Each source takes a different amount of time to respond. The goal is to gather as much data as possible within a given maximum duration while handling possible timeouts and cancellations gracefully. **Function Signature:** ```python import asyncio from typing import List async def fetch_data_from_source(source_id: int) -> str: Simulated function to fetch data from a remote source. await asyncio.sleep(source_id) # Simulate delay based on source_id return f\\"data_from_source_{source_id}\\" async def gather_data(sources: List[int], max_duration: float) -> List[str]: Gathers data from multiple sources concurrently within a specified duration. Args: sources (List[int]): List of source IDs to fetch data from. max_duration (float): Maximum duration (in seconds) to spend on fetching data. Returns: List[str]: List of successfully fetched data strings. pass ``` **Requirements:** 1. Implement `fetch_data_from_source(source_id: int)` that simulates fetching data from a remote source with an ID. The simulation should use `asyncio.sleep()` to delay based on the `source_id`. 2. Implement `gather_data(sources: List[int], max_duration: float)` to: - Create tasks for fetching data from multiple sources in parallel. - Ensure the total execution time does not exceed `max_duration` seconds. - Handle task timeouts using `asyncio.wait_for()`. - Properly cancel any remaining tasks if the time exceeds `max_duration`. **Sample Usage:** ```python async def main(): sources = [1, 2, 3, 5, 7] # Simulated source IDs with respective delays max_duration = 6.0 results = await gather_data(sources, max_duration) print(results) asyncio.run(main()) ``` **Expected Output:** ```plaintext [\'data_from_source_1\', \'data_from_source_2\', \'data_from_source_3\'] ``` The function should terminate any tasks that exceed the maximum duration and return the results gathered within the limit. **Constraints:** - The `max_duration` will be a positive float. - The number of sources will be between 1 and 10. **Evaluation Criteria:** - Correct implementation of asynchronous tasks and handling of timeouts. - Efficient gathering of data within the time constraints. - Clean and readable code with appropriate comments.","solution":"import asyncio from typing import List async def fetch_data_from_source(source_id: int) -> str: Simulated function to fetch data from a remote source. await asyncio.sleep(source_id) # Simulate delay based on source_id return f\\"data_from_source_{source_id}\\" async def gather_data(sources: List[int], max_duration: float) -> List[str]: Gathers data from multiple sources concurrently within a specified duration. Args: sources (List[int]): List of source IDs to fetch data from. max_duration (float): Maximum duration (in seconds) to spend on fetching data. Returns: List[str]: List of successfully fetched data strings. async def fetch_data_with_timeout(source_id): try: return await asyncio.wait_for(fetch_data_from_source(source_id), max_duration) except asyncio.TimeoutError: return None tasks = [fetch_data_with_timeout(source) for source in sources] done, pending = await asyncio.wait(tasks, timeout=max_duration, return_when=asyncio.ALL_COMPLETED) for task in pending: task.cancel() # Cancel any remaining tasks results = [task.result() for task in done if task.result() is not None] return results"},{"question":"# Question: Data Cleaning, Merging, and Analysis with Pandas You are given two CSV files containing data on the sales performance of two different regions. Your task is to: 1. Read the data from both CSV files into pandas DataFrames. 2. Clean the data by handling missing values appropriately. 3. Merge the two DataFrames based on a common \'Product_ID\' column. 4. Perform a groupby operation to calculate the total and average sales for each product. 5. Visualize the total sales of the top 10 products using a bar chart. 6. Finally, save the merged DataFrame with all transformations to a new CSV file. CSV File 1: `region1_sales.csv` ``` Product_ID,Product_Name,Sales_Amount 1,Product_A,100 2,Product_B,150 3,Product_C,200 ,Product_D,250 5,Product_E,NaN ``` CSV File 2: `region2_sales.csv` ``` Product_ID,Product_Name,Sales_Amount 3,Product_C,300 4,Product_D,400 5,Product_E,NaN 1,Product_A,150 2,Product_B,NaN ``` Requirements: - Implement a function `clean_merge_analyze_data(file1: str, file2: str, output_file: str) -> None`. - The function should take in the paths of the two input CSV files and the path for the output CSV file. - All columns with missing \'Product_ID\' or \'Sales_Amount\' should be dropped. - After merging, calculate and add two new columns: \'Total_Sales\' and \'Average_Sales\'. - Plot a bar chart for the top 10 products by \'Total_Sales\'. - Save the final merged DataFrame to the `output_file`. Constraints: - Assume no duplicate `Product_ID` entries within each individual CSV file. - Only consider products present in both files for the final analysis. **Input:** - CSV File paths as strings **Output:** - A new CSV file with cleaned, merged, and analyzed data. ```python import pandas as pd import matplotlib.pyplot as plt def clean_merge_analyze_data(file1: str, file2: str, output_file: str) -> None: # Your code here # Example usage: # clean_merge_analyze_data(\'region1_sales.csv\', \'region2_sales.csv\', \'merged_sales.csv\') ```","solution":"import pandas as pd import matplotlib.pyplot as plt def clean_merge_analyze_data(file1: str, file2: str, output_file: str) -> None: # Step 1: Read the data from both CSV files df1 = pd.read_csv(file1) df2 = pd.read_csv(file2) # Step 2: Clean the data by handling missing values appropriately df1.dropna(subset=[\'Product_ID\', \'Sales_Amount\'], inplace=True) df2.dropna(subset=[\'Product_ID\', \'Sales_Amount\'], inplace=True) # Step 3: Merge the two DataFrames based on a common \'Product_ID\' column merged_df = pd.merge(df1, df2, on=\'Product_ID\', suffixes=(\'_region1\', \'_region2\')) # Step 4: Calculate the total and average sales for each product merged_df[\'Total_Sales\'] = merged_df[\'Sales_Amount_region1\'] + merged_df[\'Sales_Amount_region2\'] merged_df[\'Average_Sales\'] = merged_df[\'Total_Sales\'] / 2 # Step 5: Visualize the total sales of the top 10 products using a bar chart top_10_products = merged_df.nlargest(10, \'Total_Sales\') plt.figure(figsize=(10, 6)) plt.bar(top_10_products[\'Product_Name_region1\'], top_10_products[\'Total_Sales\'], color=\'blue\') plt.xlabel(\'Product Name\') plt.ylabel(\'Total Sales\') plt.title(\'Top 10 Products by Total Sales\') plt.xticks(rotation=45) plt.tight_layout() plt.show() # Step 6: Save the merged DataFrame with all transformations to a new CSV file merged_df.to_csv(output_file, index=False) # Example usage: # clean_merge_analyze_data(\'region1_sales.csv\', \'region2_sales.csv\', \'merged_sales.csv\')"},{"question":"Objective Implement a function that searches for files in a directory and its subdirectories, filtering them based on Unix shell-style wildcard patterns similar to those used in the `fnmatch` module. Task Write a function `search_files(directory, pattern)` that takes: - A string `directory` representing the path to the root directory where the search should begin. - A string `pattern` representing the Unix shell-style wildcard pattern to filter filenames. The function should return a list of paths to the files that match the given pattern. The search should be recursive, meaning it should include all subdirectories of the specified directory. Requirements 1. Use the `fnmatch` module for pattern matching. 2. Perform the search recursively in all subdirectories. 3. Return the paths relative to the input `directory`. Constraints - The `directory` will always be a valid directory path. - The `pattern` will always be a valid Unix shell-style wildcard pattern. - You should not use the `glob` module for this task. Example ```python import os import fnmatch def search_files(directory, pattern): matched_files = [] for dirpath, dirnames, filenames in os.walk(directory): for filename in filenames: if fnmatch.fnmatch(filename, pattern): relative_path = os.path.relpath(os.path.join(dirpath, filename), directory) matched_files.append(relative_path) return matched_files # Example Usage # Assuming the directory structure: # /root # ├── file1.txt # ├── file2.log # ├── subdir # │ ├── file3.txt # │ └── file4.log print(search_files(\'/root\', \'*.txt\')) # Output: [\'file1.txt\', \'subdir/file3.txt\'] ``` Note - Ensure your function performs efficiently, even for large directory structures. - Handle file paths correctly on both Unix and Windows systems.","solution":"import os import fnmatch def search_files(directory, pattern): Searches for files in a directory and its subdirectories that match a given Unix shell-style wildcard pattern. Parameters: directory (str): The root directory where the search begins. pattern (str): The Unix shell-style wildcard pattern to filter filenames. Returns: list: A list of paths to the files that match the given pattern, relative to the input directory. matched_files = [] for dirpath, dirnames, filenames in os.walk(directory): for filename in filenames: if fnmatch.fnmatch(filename, pattern): relative_path = os.path.relpath(os.path.join(dirpath, filename), directory) matched_files.append(relative_path) return matched_files"},{"question":"**Question: Analyze and Summarize Pickle Structure** You are provided with a binary pickle string that encodes a Python object. Your task is to implement a function `summarize_pickle_structure(pickle_string: bytes) -> dict` that analyzes the pickle string and returns a summary of its structure using the `pickletools.genops` function. # Function Signature ```python def summarize_pickle_structure(pickle_string: bytes) -> dict: pass ``` # Input - `pickle_string` (bytes): A binary string representing the serialized form of a Python object. # Output - Returns a dictionary containing a summary of the pickle structure with the following keys: - `\\"total_opcodes\\"`: Total count of opcodes in the pickle string. - `\\"opcode_summary\\"`: A dictionary where each key is an opcode name (as a string) and the value is the count of how many times that opcode appears in the pickle string. - `\\"arg_types\\"`: A dictionary where each key is an opcode name and the value is a list of types (as strings) of the arguments associated with that opcode. - `\\"first_five_opcodes\\"`: A list of the first five opcodes encountered in the pickle string, each represented as a dictionary with `\\"opcode\\"`, `\\"arg\\"`, and `\\"pos\\"` keys. # Constraints - You may assume that the input is a valid pickle string. - The total number of opcodes will not exceed 1000. # Example ```python import pickle # Example object obj = {\\"example\\": [1, 2, 3], \\"test\\": {\\"a\\": \\"b\\"}} # Pickle the object pickle_string = pickle.dumps(obj) # Example usage of your function summary = summarize_pickle_structure(pickle_string) print(summary) # Expected output (format, content may vary based on Python version) # { # \\"total_opcodes\\": 14, # \\"opcode_summary\\": { # \\"PROTO\\": 1, # \\"LONG_BINPUT\\": 1, # ... # }, # \\"arg_types\\": { # \\"PROTO\\": [\'int\'], # \\"LONG_BINPUT\\": [\'int\'], # ... # }, # \\"first_five_opcodes\\": [ # {\\"opcode\\": \\"PROTO\\", \\"arg\\": 4, \\"pos\\": 0}, # {\\"opcode\\": \\"EMPTY_DICT\\", \\"arg\\": None, \\"pos\\": 2}, # ... # ] # } ``` # Notes - Use the `pickletools.genops` function to iterate over the opcodes in the pickle string. - Extract relevant information about each opcode, count their occurrences, and compile their argument types. - The returned summary dictionary should provide a comprehensive overview of the pickle structure.","solution":"import pickletools def summarize_pickle_structure(pickle_string: bytes) -> dict: opcode_summary = {} arg_types = {} first_five_opcodes = [] total_opcodes = 0 for idx, (op, arg, pos) in enumerate(pickletools.genops(pickle_string)): opcode_name = op.name # Update total_opcodes total_opcodes += 1 # Update opcode_summary if opcode_name in opcode_summary: opcode_summary[opcode_name] += 1 else: opcode_summary[opcode_name] = 1 # Update arg_types if opcode_name not in arg_types: arg_types[opcode_name] = [] if arg is not None: arg_types[opcode_name].append(type(arg).__name__) # Collect first five opcodes if idx < 5: first_five_opcodes.append({ \\"opcode\\": opcode_name, \\"arg\\": arg, \\"pos\\": pos }) # Ensure to remove duplicates from arg_types lists for opcode_name in arg_types: arg_types[opcode_name] = list(set(arg_types[opcode_name])) return { \\"total_opcodes\\": total_opcodes, \\"opcode_summary\\": opcode_summary, \\"arg_types\\": arg_types, \\"first_five_opcodes\\": first_five_opcodes }"},{"question":"Objective: Create a series of visualizations using the seaborn library to demonstrate your understanding of various features and customization options of the `scatterplot` and `relplot` functions. Instructions: 1. Load the \\"tips\\" dataset from seaborn and display the first few rows to understand its structure. 2. Using the \\"tips\\" dataset, create a scatter plot to visualize the relationship between `total_bill` and `tip` amounts. Color the points using the `time` variable (Lunch/Dinner), and vary the shape of the points using the `day` variable (Thur/Fri/Sat/Sun). 3. Customize the scatter plot by setting the size of all markers to 100 and their color to a dark gray (`color=\\".2\\"`) with a plus sign marker (`marker=\\"+\\"`). 4. Generate a new scatter plot where the size of the points represents the `size` variable and their color represents the `tip_rate` (calculated as `tip / total_bill`). 5. Using `relplot`, create faceted scatter plots of `total_bill` vs `tip` across different subplots for each category of `time` (Lunch/Dinner). Color the points using the `day` variable. 6. Use wide-form data to create a scatter plot. Generate a DataFrame with random data (4 columns, 100 rows) and index it by date. Ensure each column in the DataFrame is plotted against its index. Expected Input and Output Formats 1. **Input**: No specific input needed; work with the seaborn \\"tips\\" dataset and generate random data where specified. 2. **Output**: - A series of matplotlib figures displaying the requested scatter plots. Constraints - You must use seaborn and matplotlib for generating the plots. - All customizations and adjustments must be made using seaborn functions and parameters. - The code should be able to run without errors and produce the correctly formatted plots on a standard Python environment with seaborn and matplotlib installed. Example Solution Provide the code necessary to generate the outputs described. ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set seaborn theme sns.set_theme() # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Part 1: Display the first few rows print(tips.head()) # Part 2: Scatter plot with hue and style mapping plt.figure(figsize=(8, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\\"Scatter plot of total bill vs tip\\") plt.show() # Part 3: Customized scatter plot plt.figure(figsize=(8, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", s=100, color=\\".2\\", marker=\\"+\\") plt.title(\\"Customized scatter plot of total bill vs tip\\") plt.show() # Part 4: Scatter plot with size and hue tip_rate = tips.eval(\\"tip / total_bill\\").rename(\\"tip_rate\\") plt.figure(figsize=(8, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", size=\\"size\\", hue=tip_rate) plt.title(\\"Scatter plot with size representing party size and hue representing tip rate\\") plt.show() # Part 5: Faceted scatter plots using relplot sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\" ) plt.show() # Part 6: Wide-form scatter plot index = pd.date_range(\\"1 1 2000\\", periods=100, freq=\\"m\\", name=\\"date\\") data = np.random.randn(100, 4).cumsum(axis=0) wide_df = pd.DataFrame(data, index, [\\"a\\", \\"b\\", \\"c\\", \\"d\\"]) sns.scatterplot(data=wide_df) plt.title(\\"Scatter plot with wide-form data\\") plt.show() ```","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set seaborn theme sns.set_theme() # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Part 1: Display the first few rows def display_first_few_rows(dataset): return dataset.head() # Part 2: Scatter plot with hue and style mapping def scatterplot_totalbill_tip_hue_style(): plt.figure(figsize=(8, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\\"Scatter plot of total bill vs tip\\") plt.show() # Part 3: Customized scatter plot def customized_scatterplot(): plt.figure(figsize=(8, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", s=100, color=\\".2\\", marker=\\"+\\") plt.title(\\"Customized scatter plot of total bill vs tip\\") plt.show() # Part 4: Scatter plot with size and hue def scatterplot_size_hue(): tip_rate = tips.eval(\\"tip / total_bill\\").rename(\\"tip_rate\\") plt.figure(figsize=(8, 5)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", size=\\"size\\", hue=tip_rate) plt.title(\\"Scatter plot with size representing party size and hue representing tip rate\\") plt.show() # Part 5: Faceted scatter plots using relplot def faceted_scatterplots(): sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\" ) plt.show() # Part 6: Wide-form scatter plot def wide_form_scatterplot(): index = pd.date_range(\\"1 1 2000\\", periods=100, freq=\\"m\\", name=\\"date\\") data = np.random.randn(100, 4).cumsum(axis=0) wide_df = pd.DataFrame(data, index, [\\"a\\", \\"b\\", \\"c\\", \\"d\\"]) sns.scatterplot(data=wide_df) plt.title(\\"Scatter plot with wide-form data\\") plt.show()"},{"question":"# Python Thread Management with C-Extensions You have been assigned to extend the functionality of a Python application by integrating a C-extension module. This extension will perform some heavy computations and interact with Python objects. Your task is to write a Python C-extension module that creates threads performing these computations while ensuring thread-safety using Python’s GIL. Requirements: 1. **Initialization and Finalization**: - Implement functions to initialize and finalize the Python interpreter. - Use appropriate functions to set up and tear down the interpreter state. 2. **Thread Management**: - Create a C function that spawns multiple threads, each performing heavy computations. - Ensure that threads properly acquire and release the GIL. 3. **Interfacing with Python**: - Provide a Python-accessible function to invoke the threading computation. - Handle any necessary setup and tear-down within this function, ensuring no thread-safety violations. # Function Signatures: Python ```python def run_computation_in_threads(num_threads: int, data: list) -> list: Spawns \'num_threads\' each processing a portion of \'data\'. Returns the combined result from all threads. :param num_threads: Number of threads to create. :param data: List of data for computation. :return: List of results from all threads combined. ``` C ```c void initialize_python(); void finalize_python(); void* thread_computation(void *data); PyObject* run_computation_in_threads(PyObject *self, PyObject *args); ``` Constraints: - Ensure the Python interpreter is correctly initialized and finalized. - Properly manage Python and thread states to avoid deadlocks. - The computations are performed in C, but results are combined and returned to Python. # Example ```python if __name__ == \\"__main__\\": import my_extension result = my_extension.run_computation_in_threads(4, [1, 2, 3, 4, 5, 6, 7, 8]) print(result) ``` Performance: - The computation should effectively utilize multiple threads, showing improvements for larger data sets due to parallel processing.","solution":"from concurrent.futures import ThreadPoolExecutor import ctypes import threading # Load the shared object (C-extension) compiled as `my_extension.so` # Here we simulate the C-extension using ctypes # Let\'s assume we have a C function `heavy_computation` that takes an int and returns an int # For the sake of this simulation, we\'ll use a Python function to perform \\"heavy computation\\" # This would be the signature if we were linking with a real C-extension: # c_extension = ctypes.CDLL(\'./my_extension.so\') # heavy_computation = c_extension.heavy_computation # heavy_computation.argtypes = [ctypes.c_int] # heavy_computation.restype = ctypes.c_int # Simulate heavy computation as a Python function def heavy_computation(x): # Simulate a heavy computation import time time.sleep(0.1) return x * x def run_computation_in_threads(num_threads: int, data: list) -> list: Spawns \'num_threads\' each processing a portion of \'data\'. Returns the combined result from all threads. :param num_threads: Number of threads to create. :param data: List of data for computation. :return: List of results from all threads combined. def thread_task(data_chunk): results = [heavy_computation(d) for d in data_chunk] return results chunk_size = len(data) // num_threads threads_data = [data[i*chunk_size:(i+1)*chunk_size] for i in range(num_threads)] if len(data) % num_threads != 0: threads_data[-1].extend(data[num_threads * chunk_size:]) results = [] with ThreadPoolExecutor(max_workers=num_threads) as executor: futures = [executor.submit(thread_task, chunk) for chunk in threads_data] for future in futures: results.extend(future.result()) return results"},{"question":"# Seaborn Customization Challenge In this coding assessment, you are required to demonstrate your understanding of seaborn\'s plotting and theming capabilities. Your task is to create a function that generates and customizes a variety of plots using seaborn. Function Signature ```python def create_custom_plots(): pass ``` Description Implement the function `create_custom_plots` that performs the following steps: 1. Sets the seaborn theme to \\"whitegrid\\" and uses the \\"muted\\" palette. 2. Creates a barplot using the following data: - Categories: [\\"Apples\\", \\"Oranges\\", \\"Bananas\\"] - Values: [10, 15, 7] 3. Modifies the theme to use \\"darkgrid\\" style, with a custom color palette: [\\"#3498db\\", \\"#e74c3c\\", \\"#2ecc71\\"]. 4. Adds a new plot to the same figure: a line plot using the following data: - X-axis: [0, 1, 2, 3, 4] - Y-axis: [0, 1, 4, 9, 16] 5. Sets custom rcParams to remove the top and right spines of the plots. Requirements - Your function should produce a single figure containing both the barplot and the line plot. - The theme settings should be applied as described, and the plots should reflect these settings. - The custom rcParams should effectively modify the appearance of the plots. Output - The function should not return any values. It should display the plots as output when executed. Here is the starting code template for you to follow: ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(): # Step 1: Set theme to whitegrid and use the muted palette sns.set_theme(style=\\"whitegrid\\", palette=\\"muted\\") # Step 2: Create a barplot categories = [\\"Apples\\", \\"Oranges\\", \\"Bananas\\"] values = [10, 15, 7] barplot = sns.barplot(x=categories, y=values) # Step 3: Modify the theme to darkgrid and custom palette custom_palette = [\\"#3498db\\", \\"#e74c3c\\", \\"#2ecc71\\"] sns.set_theme(style=\\"darkgrid\\", palette=custom_palette) # Create a new figure to add the line plot plt.figure() # Step 4: Add a line plot x = [0, 1, 2, 3, 4] y = [0, 1, 4, 9, 16] lineplot = plt.plot(x, y) # Step 5: Set custom rcParams custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(rc=custom_params) # Display plots plt.show() # Execution of the function to test the implementation create_custom_plots() ``` Ensure your final implementation correctly adheres to the described tasks and produces the appropriate plots with the specified customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plots(): # Step 1: Set theme to whitegrid and use the muted palette sns.set_theme(style=\\"whitegrid\\", palette=\\"muted\\") # Step 2: Create a barplot categories = [\\"Apples\\", \\"Oranges\\", \\"Bananas\\"] values = [10, 15, 7] sns.barplot(x=categories, y=values) # Display barplot plt.figure() # Step 3: Modify the theme to darkgrid and custom palette custom_palette = [\\"#3498db\\", \\"#e74c3c\\", \\"#2ecc71\\"] sns.set_theme(style=\\"darkgrid\\", palette=custom_palette) # Step 4: Add a line plot x = [0, 1, 2, 3, 4] y = [0, 1, 4, 9, 16] plt.plot(x, y) # Step 5: Set custom rcParams custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(rc=custom_params) # Display plots plt.show() # Execution of the function to test the implementation create_custom_plots()"},{"question":"You have been given the task of implementing custom shallow and deep copy mechanisms for a complex, user-defined class. The class, `CustomList`, is a list-like container which may contain other instances of `CustomList`, allowing for recursive structures. You need to ensure that both shallow and deep copying work correctly for this class. # Class Specification: ```python class CustomList: def __init__(self, elements): self.elements = elements def add_element(self, element): self.elements.append(element) def __copy__(self): # Implement shallow copy here def __deepcopy__(self, memo): # Implement deep copy here ``` # Requirements: 1. **Shallow Copy (`__copy__`)**: - Create a new instance of `CustomList` with a shallow copy of the elements. - The new instance should reflect changes to the original elements but not to the compound object. 2. **Deep Copy (`__deepcopy__`)**: - Create a new instance of `CustomList` with a deep copy of the elements. - The new instance should be completely independent of the original. Changes to the original or the new instance should not affect each other. # Input Format: - A list of elements to initialize `CustomList`. - Elements can include integers, strings, or instances of `CustomList`. # Output Format: - For shallow copy: An independent object where the top-level elements are only references. - For deep copy: An entirely new object with no shared references to the original elements. # Example: ```python import copy # Initialize a CustomList with nested structure original_list = CustomList([1, [2, CustomList([3, 4])], 5]) # Perform shallow copy shallow_copied_list = copy.copy(original_list) # Perform deep copy deep_copied_list = copy.deepcopy(original_list) # Test by modifying the original list original_list.add_element(6) original_list.elements[1][0] = \'changed\' print(original_list.elements) # Expected: [1, [\'changed\', CustomList([3, 4])], 5, 6] print(shallow_copied_list.elements) # Expected: [1, [\'changed\', CustomList([3, 4])], 5] print(deep_copied_list.elements) # Expected: [1, [2, CustomList([3, 4])], 5] ``` # Constraints: - The class should correctly handle nested instances of itself. - The deep copy implementation should use the `memo` dictionary to avoid copying objects multiple times. Implement the `__copy__` and `__deepcopy__` methods in the `CustomList` class according to the specifications provided.","solution":"import copy class CustomList: def __init__(self, elements): self.elements = elements def add_element(self, element): self.elements.append(element) def __copy__(self): new_copy = CustomList(self.elements[:]) return new_copy def __deepcopy__(self, memo): new_elements = [] for elem in self.elements: new_elements.append(copy.deepcopy(elem, memo)) new_deepcopy = CustomList(new_elements) return new_deepcopy"},{"question":"# Question: Managing PyTorch and CUDA Environment Variables PyTorch provides several environment variables that allow you to control and optimize its behavior when using CUDA. In this exercise, you will write a Python function that configures these environment variables to achieve the desired setup for a given task. Additionally, you will write a script to verify that the environment variables are set correctly and that PyTorch behaves as expected. Task 1. Implement a function `configure_pytorch_env()` that takes a dictionary of environment variable names and values and sets them accordingly. 2. Implement a function `verify_pytorch_env()` that checks the current values of these environment variables and returns a dictionary with the environment variable names and their values. 3. Write a script that: - Sets the following environment variables: - `PYTORCH_NO_CUDA_MEMORY_CACHING` to `1` - `CUDA_VISIBLE_DEVICES` to `0,1` - Verifies that the environment variables are set correctly. - Initializes a simple PyTorch model and checks if CUDA is available to be used (i.e., whether it finds GPUs 0 and 1). - Prints out the verification results and whether CUDA is being used. Input - A dictionary where keys are the names of the environment variables and values are the values to set for those environment variables. Output - A dictionary where keys are the names of the environment variables and values are their current values after setting them. Constraints - Your implementation should work on any machine with CUDA and PyTorch installed. - You need to handle cases where the environment variables might not be set correctly or where CUDA is not available. Example ```python def configure_pytorch_env(env_vars): Sets the specified environment variables. Args: env_vars (dict): Dictionary containing environment variable names and values. Example: configure_pytorch_env({ \'PYTORCH_NO_CUDA_MEMORY_CACHING\': \'1\', \'CUDA_VISIBLE_DEVICES\': \'0,1\' }) pass def verify_pytorch_env(env_vars): Verifies the specified environment variables are set correctly. Args: env_vars (dict): Dictionary containing environment variable names to check. Returns: dict: Dictionary containing current environment variable names and their values. Example: verify_pytorch_env({ \'PYTORCH_NO_CUDA_MEMORY_CACHING\': \'1\', \'CUDA_VISIBLE_DEVICES\': \'0,1\' }) pass # Example script if __name__ == \\"__main__\\": env_settings = { \'PYTORCH_NO_CUDA_MEMORY_CACHING\': \'1\', \'CUDA_VISIBLE_DEVICES\': \'0,1\' } configure_pytorch_env(env_settings) verification = verify_pytorch_env(env_settings) print(\\"Environment variable verification:\\") for var, value in verification.items(): print(f\\"{var}: {value}\\") # Check if CUDA is available and which devices are visible import torch print(\\"CUDA is available:\\" if torch.cuda.is_available() else \\"CUDA is not available.\\") if torch.cuda.is_available(): print(\\"Visible devices:\\", torch.cuda.device_count()) ``` Additional Requirements - Ensure that your code is robust and handles potential exceptions. - Include comments and documentation for your functions and script. - Test your functions thoroughly.","solution":"import os def configure_pytorch_env(env_vars): Sets the specified environment variables. Args: env_vars (dict): Dictionary containing environment variable names and values. Example: configure_pytorch_env({ \'PYTORCH_NO_CUDA_MEMORY_CACHING\': \'1\', \'CUDA_VISIBLE_DEVICES\': \'0,1\' }) for var, value in env_vars.items(): os.environ[var] = value def verify_pytorch_env(env_vars): Verifies the specified environment variables are set correctly. Args: env_vars (dict): Dictionary containing environment variable names to check. Returns: dict: Dictionary containing current environment variable names and their values. Example: verify_pytorch_env({ \'PYTORCH_NO_CUDA_MEMORY_CACHING\': \'1\', \'CUDA_VISIBLE_DEVICES\': \'0,1\' }) current_env_vars = {} for var in env_vars.keys(): current_env_vars[var] = os.environ.get(var) return current_env_vars"},{"question":"# Advanced Python File Handling **Objective:** Implement a utility class that interacts with file descriptors using Python\'s `io` module, mimicking some of the functionalities detailed in the provided documentation. **Task:** 1. Create a class `FileManager` with the following methods: - `from_fd(fd, name=None, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None, closefd=True)`: Open a file from a given file descriptor and return a file object. - **Input**: - `fd` (int): File descriptor. - `name` (str, optional): File name (for compatibility, not utilised). - `mode` (str, default \'r\'): File mode. - `buffering` (int, default -1): Buffering policy. - `encoding` (str, optional): Encoding. - `errors` (str, optional): Error handling. - `newline` (str, optional): Newline handling. - `closefd` (bool, optional): Close file descriptor after usage. - **Output**: File object. - `as_file_descriptor(fileobj)`: Get the file descriptor from a file object or an object that implements `fileno()`. - **Input**: - `fileobj`: A file object or object with `fileno()`. - **Output**: Integer file descriptor. - `get_line(fileobj, n=0)`: Read a line from a file object. - **Input**: - `fileobj`: A file object with `readline()`. - `n` (int): Number of bytes to read (default is 0, which reads the whole line). - **Output**: String read from the file. - `write_object(obj, fileobj, raw=False)`: Write a Python object to a file object. - **Input**: - `obj`: Object to write. - `fileobj`: File object. - `raw` (bool, default False): Write raw string (`str(obj)`) if True, otherwise `repr(obj)`. - **Output**: None - `write_string(string, fileobj)`: Write a string to a file object. - **Input**: - `string` (str): String to write. - `fileobj`: File object. - **Output**: None 2. Implement a custom hook function for `open_code` that can be used to log the path of the code being opened. **Constraints:** - Ensure proper error handling, raising appropriate exceptions on failure. - Use Python\'s `io` module for file handling. **Example:** ```python import os import io # Example usage of FileManager class FileManager: @staticmethod def from_fd(fd, name=None, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None, closefd=True): try: file_obj = io.open(fd, mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd) return file_obj except Exception as e: raise e @staticmethod def as_file_descriptor(fileobj): try: return fileobj.fileno() except Exception as e: raise e @staticmethod def get_line(fileobj, n=0): try: if n == 0: return fileobj.readline() else: return fileobj.read(n) except Exception as e: raise e @staticmethod def write_object(obj, fileobj, raw=False): try: if raw: fileobj.write(str(obj)) else: fileobj.write(repr(obj)) except Exception as e: raise e @staticmethod def write_string(string, fileobj): try: fileobj.write(string) except Exception as e: raise e @staticmethod def open_code_hook_function(path, userData): print(f\\"Opening code: {path}\\") return path # Example code # Assume you have a file descriptor `fd` with open(\'example.txt\', \'w\') as f: fd = f.fileno() fm = FileManager() file_obj = fm.from_fd(fd, mode=\'w\') fm.write_string(\\"This is a test string.\\", file_obj) file_obj.close() ``` **Note**: Actual implementation of `open_code_hook_function` and registration with `PyFile_SetOpenCodeHook` is beyond this scoped example and would require modifications to the Python C runtime environment.","solution":"import io class FileManager: @staticmethod def from_fd(fd, name=None, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None, closefd=True): Open a file from a given file descriptor and return a file object. try: file_obj = io.open(fd, mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline, closefd=closefd) return file_obj except Exception as e: raise e @staticmethod def as_file_descriptor(fileobj): Get the file descriptor from a file object or an object that implements `fileno()`. try: return fileobj.fileno() except Exception as e: raise e @staticmethod def get_line(fileobj, n=0): Read a line from a file object. try: if n == 0: return fileobj.readline() else: return fileobj.read(n) except Exception as e: raise e @staticmethod def write_object(obj, fileobj, raw=False): Write a Python object to a file object. try: data = str(obj) if raw else repr(obj) fileobj.write(data) except Exception as e: raise e @staticmethod def write_string(string, fileobj): Write a string to a file object. try: fileobj.write(string) except Exception as e: raise e @staticmethod def open_code_hook_function(path, userData): Log the path of the code being opened. This is a placeholder for a custom hook function. print(f\\"Opening code: {path}\\") return path # Example code # Assume you have a file descriptor `fd` \'\'\' with open(\'example.txt\', \'w\') as f: fd = f.fileno() fm = FileManager() file_obj = fm.from_fd(fd, mode=\'w\') fm.write_string(\\"This is a test string.\\", file_obj) file_obj.close() \'\'\'"},{"question":"You are tasked with implementing a function that will generate, write to, and read from plist files for a macOS application configuration. The configuration consists of various settings including strings, lists, numbers, nested dictionaries, boolean values, and datetime objects. The function should also handle both XML and binary plist formats. # Task: 1. Implement a function `create_config` that generates a configuration dictionary. 2. Implement a function `save_config` that writes this dictionary to a plist file. 3. Implement a function `load_config` that reads the dictionary from a plist file. # Function Signatures: ```python def create_config() -> dict: Generate a configuration dictionary with various settings. Returns: dict: A dictionary containing configuration settings. def save_config(config: dict, filepath: str, fmt: str = \\"xml\\") -> None: Write the configuration dictionary to a plist file. Args: config (dict): The configuration dictionary. filepath (str): The path where the plist file will be saved. fmt (str): The format in which the plist file will be saved (\\"xml\\" or \\"binary\\"). def load_config(filepath: str, fmt: str = \\"xml\\") -> dict: Read the configuration dictionary from a plist file. Args: filepath (str): The path of the plist file to read. fmt (str): The format of the plist file to read (\\"xml\\" or \\"binary\\"). Returns: dict: The configuration dictionary read from the plist file. ``` # Requirements: - The `create_config` function should generate a dictionary with at least the following keys: - `appName`: A string representing the application name. - `version`: A string representing the version of the application. - `settings`: A nested dictionary with keys `theme` (string), `notifications` (boolean), and `autoUpdate` (boolean). - `lastUpdated`: A `datetime` object representing the last update timestamp. - The `save_config` function should: - Serialize the dictionary to a plist file. - Allow the user to choose between XML and binary formats. - The `load_config` function should: - Deserialize the dictionary from a plist file. - Handle both XML and binary formats. # Constraints: - Assume the file paths given are valid and accessible. - Use appropriate error handling to manage unsupported data types and potential plist parsing errors. # Example: Generate configuration dictionary: ```python config = create_config() print(config) ``` Save the configuration to a plist file: ```python save_config(config, \\"config.plist\\", fmt=\\"xml\\") ``` Load the configuration from a plist file: ```python loaded_config = load_config(\\"config.plist\\", fmt=\\"xml\\") print(loaded_config) ``` Your solution should cover both creating configurations and saving/loading them using the `plistlib` module in Python.","solution":"import plistlib from datetime import datetime def create_config() -> dict: Generate a configuration dictionary with various settings. Returns: dict: A dictionary containing configuration settings. config = { \\"appName\\": \\"MyApp\\", \\"version\\": \\"1.0\\", \\"settings\\": { \\"theme\\": \\"dark\\", \\"notifications\\": True, \\"autoUpdate\\": False }, \\"lastUpdated\\": datetime.now() } return config def save_config(config: dict, filepath: str, fmt: str = \\"xml\\") -> None: Write the configuration dictionary to a plist file. Args: config (dict): The configuration dictionary. filepath (str): The path where the plist file will be saved. fmt (str): The format in which the plist file will be saved (\\"xml\\" or \\"binary\\"). if fmt == \\"xml\\": with open(filepath, \'wb\') as f: plistlib.dump(config, f, fmt=plistlib.FMT_XML) elif fmt == \\"binary\\": with open(filepath, \'wb\') as f: plistlib.dump(config, f, fmt=plistlib.FMT_BINARY) else: raise ValueError(f\\"Unsupported format: {fmt}\\") def load_config(filepath: str, fmt: str = \\"xml\\") -> dict: Read the configuration dictionary from a plist file. Args: filepath (str): The path of the plist file to read. fmt (str): The format of the plist file to read (\\"xml\\" or \\"binary\\"). Returns: dict: The configuration dictionary read from the plist file. with open(filepath, \'rb\') as f: config = plistlib.load(f) return config"},{"question":"You are tasked with implementing an out-of-core learning system for a large text classification problem using scikit-learn. The dataset is too large to fit into memory all at once, so you will process it in mini-batches. Your goal is to build a system that reads data in chunks, extracts features using a stateless method, and feeds these features into an incremental classifier. Requirements: 1. **Data Streaming**: Implement a function to simulate streaming data. This function should yield mini-batches of text data. 2. **Feature Extraction**: Use `HashingVectorizer` to convert text data into feature vectors. 3. **Incremental Learning**: Use `SGDClassifier` to train the model incrementally. Input: - `data_stream`: A generator function that yields mini-batches of text data. - `batch_size`: An integer specifying the size of each mini-batch. - `max_iter`: An integer specifying the number of iterations over the entire dataset. Output: - A trained `SGDClassifier` model. Constraints: - Each mini-batch must be processed using a stateless feature extraction method. - The size of each mini-batch should balance memory usage and training efficiency. Performance Requirements: - The model should be able to handle large datasets efficiently using limited memory. Function Signature: ```python from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer def stream_data(data, batch_size): Simulate streaming data in mini-batches. Parameters: - data: list of text data. - batch_size: size of each mini-batch. Yields: - mini-batch of text data. for i in range(0, len(data), batch_size): yield data[i:i + batch_size] def out_of_core_learning(data_stream, batch_size, max_iter): Implement an out-of-core learning system for text classification. Parameters: - data_stream: generator function yielding mini-batches of text data. - batch_size: size of each mini-batch. - max_iter: number of iterations over the entire dataset. Returns: - Trained SGDClassifier model. vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() classes = [0, 1] # Assuming binary classification for _ in range(max_iter): for mini_batch in data_stream: X_mini = vectorizer.transform(mini_batch[\'data\']) y_mini = mini_batch[\'target\'] classifier.partial_fit(X_mini, y_mini, classes=classes) return classifier # Example usage: data = [{\\"data\\": \\"text example 1\\", \\"target\\": 0}, {\\"data\\": \\"text example 2\\", \\"target\\": 1}, # More data... ] batch_size = 100 max_iter = 10 data_stream = stream_data(data, batch_size) model = out_of_core_learning(data_stream, batch_size, max_iter) ``` Notes: - You can assume that the input data is already split into text and target labels. - You can modify the example usage to test the function with your own data.","solution":"from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer def stream_data(data, batch_size): Simulate streaming data in mini-batches. Parameters: - data: list of dictionaries containing \'data\' and \'target\'. - batch_size: size of each mini-batch. Yields: - A dictionary containing \'data\' and \'target\' lists of mini-batch size. for i in range(0, len(data), batch_size): mini_batch = data[i:i + batch_size] yield { \'data\': [item[\'data\'] for item in mini_batch], \'target\': [item[\'target\'] for item in mini_batch] } def out_of_core_learning(data_stream, batch_size, max_iter): Implement an out-of-core learning system for text classification. Parameters: - data_stream: generator function yielding mini-batches of text data. - batch_size: size of each mini-batch. - max_iter: number of iterations over the entire dataset. Returns: - Trained SGDClassifier model. vectorizer = HashingVectorizer(n_features=2**20) classifier = SGDClassifier() classes = [0, 1] # Assuming binary classification for _ in range(max_iter): for mini_batch in data_stream: X_mini = vectorizer.transform(mini_batch[\'data\']) y_mini = mini_batch[\'target\'] classifier.partial_fit(X_mini, y_mini, classes=classes) return classifier"},{"question":"# Coding Challenge: Implement a Dynamic Activation Function Module In this task, you are required to create a PyTorch module that dynamically selects one of the two activation functions based on the sum of the input tensor. If the sum of the elements in the input tensor is greater than a given threshold, it should apply the ReLU activation function; otherwise, it should apply the Sigmoid activation function. Function Signature ```python class DynamicActivationModule(torch.nn.Module): def __init__(self, threshold: float): Initialize the module with the given threshold. pass def forward(self, x: torch.Tensor) -> torch.Tensor: Forward pass for the dynamic activation function. pass ``` # Requirements 1. **Initialization**: The module should be initialized with a `threshold` value which will be used to determine which activation function to apply. 2. **Forward Pass**: The forward pass should use `torch.cond` to apply: - `torch.nn.ReLU()` if the sum of the input tensor is greater than the threshold. - `torch.nn.Sigmoid()` otherwise. # Example ```python import torch # Initialize the module with threshold 10.0 module = DynamicActivationModule(threshold=10.0) # Input tensor x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]) # Applying the module output = module(x) print(output) # Should use Sigmoid activation as the sum (15.0) is greater than the threshold # Another input tensor y = torch.tensor([1.0, 1.0, 1.0]) # Applying the module output = module(y) print(output) # Should use ReLU activation as the sum (3.0) is less than the threshold ``` # Constraints - Do not use any other conditional statements (e.g., if-else) apart from `torch.cond`. - Ensure the implementation follows best practices and maintains clean, readable code. # Notes - The solution should focus on demonstrating the use of `torch.cond` and understanding dynamic control flows in PyTorch. - Pay attention to how functions are defined and passed in `torch.cond`.","solution":"import torch import torch.nn.functional as F class DynamicActivationModule(torch.nn.Module): def __init__(self, threshold: float): super(DynamicActivationModule, self).__init__() self.threshold = threshold self.relu = torch.nn.ReLU() self.sigmoid = torch.nn.Sigmoid() def forward(self, x: torch.Tensor) -> torch.Tensor: x_sum = torch.sum(x) if x_sum > self.threshold: return self.relu(x) else: return self.sigmoid(x)"},{"question":"# Coding Assessment: Advanced Regular Expressions in Python Objective To assess your understanding of Python\'s regular expressions, you will implement functions that leverage the different features of the `re` module. You will be required to demonstrate the use of pattern matching, grouping, substitution, and advanced constructs like lookahead assertions. Task You need to implement a function `validate_and_format_dates` that takes a list of date strings and returns a list of validated and formatted date strings. Dates should be in the format `YYYY-MM-DD`, but they might initially be input in various formats. The function must: 1. Validate the input dates against the following formats: - `MM/DD/YYYY` - `MM-DD-YYYY` - `YYYY/MM/DD` - `YYYY-MM-DD` 2. If the input date matches any of these formats and is valid, convert it to the standard format `YYYY-MM-DD`. 3. Discard any invalid dates. 4. Ensure that \\"MM\\" is a valid month (01-12), \\"DD\\" is a valid day (01-31 depending on the month), and \\"YYYY\\" is a valid year (1000-9999). Input - A list of strings where each string represents a date in one of the specified formats. Output - A list of strings where each string is a validated and formatted date in the `YYYY-MM-DD` format. Examples ```python dates = [\\"12/31/2021\\", \\"2021/12/31\\", \\"31-12-2021\\", \\"2021-12-31\\", \\"13-31-2021\\", \\"2021-02-30\\"] print(validate_and_format_dates(dates)) # Output: [\'2021-12-31\', \'2021-12-31\', \'2021-12-31\'] dates = [\\"01/01/2000\\", \\"2000/01/01\\", \\"10-13-2019\\", \\"2019/12/13\\"] print(validate_and_format_dates(dates)) # Output: [\'2000-01-01\', \'2000-01-01\', \'2019-12-13\'] ``` Constraints - Do not use any third-party libraries. Only use the Python standard library. - Ensure your code is efficient and handles large lists of dates gracefully. Notes - Consider edge cases like: - Invalid date formats. - Invalid day or month values. - Leap years. Implementation ```python import re from datetime import datetime def validate_and_format_dates(date_list): def is_valid_date(year, month, day): try: datetime(int(year), int(month), int(day)) return True except ValueError: return False formatted_dates = [] date_patterns = [ re.compile(r\'^(?P<month>d{2})/(?P<day>d{2})/(?P<year>d{4})\'), # MM/DD/YYYY re.compile(r\'^(?P<month>d{2})-(?P<day>d{2})-(?P<year>d{4})\'), # MM-DD-YYYY re.compile(r\'^(?P<year>d{4})/(?P<month>d{2})/(?P<day>d{2})\'), # YYYY/MM/DD re.compile(r\'^(?P<year>d{4})-(?P<month>d{2})-(?P<day>d{2})\'), # YYYY-MM-DD ] for date in date_list: for pattern in date_patterns: match = pattern.match(date) if match: year, month, day = match.group(\'year\'), match.group(\'month\'), match.group(\'day\') if is_valid_date(year, month, day): formatted_dates.append(f\\"{year}-{month}-{day}\\") break # Stop checking other patterns if a valid match is found return formatted_dates # Example usage dates = [\\"12/31/2021\\", \\"2021/12/31\\", \\"31-12-2021\\", \\"2021-12-31\\", \\"13-31-2021\\", \\"2021-02-30\\"] print(validate_and_format_dates(dates)) ``` Ensure your code passes the example test cases provided and covers edge cases thoroughly.","solution":"import re from datetime import datetime def validate_and_format_dates(date_list): def is_valid_date(year, month, day): try: datetime(int(year), int(month), int(day)) return True except ValueError: return False formatted_dates = [] date_patterns = [ re.compile(r\'^(?P<month>d{2})/(?P<day>d{2})/(?P<year>d{4})\'), # MM/DD/YYYY re.compile(r\'^(?P<month>d{2})-(?P<day>d{2})-(?P<year>d{4})\'), # MM-DD-YYYY re.compile(r\'^(?P<year>d{4})/(?P<month>d{2})/(?P<day>d{2})\'), # YYYY/MM/DD re.compile(r\'^(?P<year>d{4})-(?P<month>d{2})-(?P<day>d{2})\'), # YYYY-MM-DD ] for date in date_list: for pattern in date_patterns: match = pattern.match(date) if match: year, month, day = match.group(\'year\'), match.group(\'month\'), match.group(\'day\') if is_valid_date(year, month, day): formatted_dates.append(f\\"{year}-{month}-{day}\\") break # Stop checking other patterns if a valid match is found return formatted_dates"},{"question":"# Problem: School Management System You are tasked with creating a small school management system that keeps track of student grades and provides insights on student performance using Python classes and functions. Your solution should demonstrate proficiency in control flow, exception handling, and pattern matching concepts. Requirements 1. **Student Class**: - Create a class `Student` with properties `name` (string), `grades` (list of integers), and `active` (boolean). - Add methods `add_grade` to add a grade to the student\'s grade list, `average_grade` to return the average grade, and `deactivate` to set the student as inactive. 2. **School Class**: - Create a class `School` with a property `students` (list of `Student` objects). - Implement methods: - `add_student(name: str)`: Adds a new student to the school. - `add_grade(name: str, grade: int)`: Adds a grade to the student with the given name. - `report_card(name: str)`: Returns a string representing the student\'s name and average grade. Handle cases where the student does not exist or has no grades. - `top_student()`: Returns the active student with the highest average grade. Handle cases where no students are active or have grades. 3. **Pattern Matching**: - Use the `match` statement to create a function `student_status(student: Student)` that returns the following: - `\\"Top Performer\\"` if the average grade is above 85. - `\\"Satisfactory\\"` if the average grade is between 60 and 85. - `\\"Needs Improvement\\"` if the average grade is below 60. - `\\"Inactive\\"` if the student is not active. Constraints - Student names are unique. - Grades are integers between 0 and 100. - `report_card` and `top_student` methods should use appropriate exception handling for edge cases. Example Usage ```python school = School() school.add_student(\\"John Doe\\") school.add_grade(\\"John Doe\\", 90) school.add_grade(\\"John Doe\\", 80) print(school.report_card(\\"John Doe\\")) # Output: \\"John Doe: 85.0\\" print(student_status(school.students[0])) # Output: \\"Top Performer\\" school.add_student(\\"Jane Smith\\") school.add_grade(\\"Jane Smith\\", 50) school.students[1].deactivate() print(school.top_student()) # Output: \\"John Doe\\" print(student_status(school.students[1])) # Output: \\"Inactive\\" ``` Implementation - Implement all required classes and methods. - Ensure to handle edge cases and exceptions gracefully. Write the complete implementation of the `Student`, `School` classes, and any additional functions needed to achieve the described functionality.","solution":"class Student: def __init__(self, name): self.name = name self.grades = [] self.active = True def add_grade(self, grade): if isinstance(grade, int) and 0 <= grade <= 100: self.grades.append(grade) else: raise ValueError(\\"Grade must be an integer between 0 and 100\\") def average_grade(self): if self.grades: return sum(self.grades) / len(self.grades) return 0 def deactivate(self): self.active = False class School: def __init__(self): self.students = [] def add_student(self, name): if any(student.name == name for student in self.students): raise ValueError(f\\"Student named {name} already exists\\") self.students.append(Student(name)) def add_grade(self, name, grade): student = self.get_student_by_name(name) if student: student.add_grade(grade) else: raise ValueError(f\\"No student found with name {name}\\") def report_card(self, name): student = self.get_student_by_name(name) if not student: raise ValueError(f\\"No student found with name {name}\\") if not student.grades: return f\\"{name} has no grades\\" return f\\"{name}: {student.average_grade()}\\" def top_student(self): active_students = [student for student in self.students if student.active and student.grades] if not active_students: raise ValueError(\\"No active students with grades\\") top_student = max(active_students, key=lambda student: student.average_grade()) return top_student.name def get_student_by_name(self, name): for student in self.students: if student.name == name: return student return None def student_status(student): match student: case Student() if not student.active: return \\"Inactive\\" case Student() if student.average_grade() > 85: return \\"Top Performer\\" case Student() if 60 <= student.average_grade() <= 85: return \\"Satisfactory\\" case Student() if student.average_grade() < 60: return \\"Needs Improvement\\" case _: return \\"Unknown Status\\""},{"question":"# PyTorch Coding Assessment Objective Create a PyTorch model, perform tensor manipulations, and convert the model into TorchScript. Question You are required to implement a simple neural network using PyTorch, perform a series of tensor manipulations, and then convert the neural network model to TorchScript. Follow the steps below: 1. **Create a Neural Network:** - Implement a neural network class `SimpleNN` inheriting from `torch.nn.Module`. - The network should have: - An input layer with 10 units. - One hidden layer with 20 units. - An output layer with 5 units. - Use ReLU as the activation function for hidden layers. 2. **Tensor Operations:** - Create a random tensor of shape `(5, 10)`. - Multiply this tensor with a constant, say 3.5. - Apply the `SimpleNN` model to this modified tensor. 3. **Convert to TorchScript:** - Convert the trained model to TorchScript using tracing. 4. **Testing:** - Ensure your model and the TorchScript model produce the same output for the same input tensor. Expected Input and Output Formats - The input tensor for the model will be of shape `(5, 10)`. - The output tensor from the model and the TorchScript model should be of shape `(5, 5)`. Constraints - Ensure to use `torch.jit.trace` for converting the PyTorch model to TorchScript. - The model should be able to handle any given input tensor of appropriate shape as described. Performance Requirements - The performance requirements are standard, with the expectation that the model performs the computations efficiently for given tensor shapes. Implementation Details Provide your implementation in Python using the PyTorch library. Your submission should include: - The `SimpleNN` class definition. - Tensor manipulations as described. - Conversion of the model to TorchScript. - Code to validate that both models produce the same output. Example Code Structure ```python import torch import torch.nn as nn import torch.jit as jit class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Creating the model and tensor model = SimpleNN() input_tensor = torch.rand(5, 10) * 3.5 # Applying the model to the tensor output_tensor = model(input_tensor) # Converting to TorchScript scripted_model = jit.trace(model, input_tensor) # Validate the output scripted_output_tensor = scripted_model(input_tensor) assert torch.allclose(output_tensor, scripted_output_tensor), \\"The outputs do not match\\" print(\\"Model and TorchScript model produce the same output.\\") ``` Provide the code snippets and detailed explanations for each part of your implementation.","solution":"import torch import torch.nn as nn import torch.jit as jit class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Creating the model and tensor model = SimpleNN() input_tensor = torch.rand(5, 10) * 3.5 # Applying the model to the tensor output_tensor = model(input_tensor) # Converting to TorchScript scripted_model = jit.trace(model, input_tensor) # Validate the output scripted_output_tensor = scripted_model(input_tensor) assert torch.allclose(output_tensor, scripted_output_tensor), \\"The outputs do not match\\" print(\\"Model and TorchScript model produce the same output.\\")"},{"question":"You are tasked with creating a custom interactive prompt in Python using the `readline` module. This custom prompt should include the following features: 1. **Configuration Initialization**: Load a custom readline configuration from a file. 2. **Command History**: - Load command history from a file upon starting the prompt. - Append the command history to the file upon exiting the prompt. - Limit the saved command history to the last 100 commands. 3. **Auto-completion**: - Enable auto-completion for a predefined set of commands. - Use a function to handle completion suggestions based on the current input. Your implementation should define a `CustomInteractivePrompt` class with the following methods: 1. `__init__(self, histfile, initfile, commands)`: Initialize the prompt. - `histfile`: Path to the history file. - `initfile`: Path to the initialization file. - `commands`: List of available commands for auto-completion. 2. `start_prompt(self)`: Start the custom interactive prompt. Provide a `main` function that: - Creates an instance of `CustomInteractivePrompt` with appropriate parameters. - Starts the interactive prompt. # Constraints: - Your prompt should gracefully handle the case where the history or config files may not initially exist. - The default maximum history length should be set to 100. # Example Usage: ```python if __name__ == \\"__main__\\": # Example initialization values: history_file = os.path.expanduser(\\"~/.custom_history\\") init_file = os.path.expanduser(\\"~/.custom_init\\") commands_list = [\\"start\\", \\"stop\\", \\"restart\\", \\"status\\", \\"help\\"] custom_prompt = CustomInteractivePrompt(history_file, init_file, commands_list) custom_prompt.start_prompt() ``` You are expected to handle the command history and auto-completion setup within the `CustomInteractivePrompt` class and ensure the proper loading and saving of configurations.","solution":"import readline import os class CustomInteractivePrompt: def __init__(self, histfile, initfile, commands): self.histfile = histfile self.initfile = initfile self.commands = commands self.history_length = 100 self._initialize_readline() def _initialize_readline(self): # Load custom readline configuration if os.path.exists(self.initfile): readline.read_init_file(self.initfile) # Load command history if os.path.exists(self.histfile): readline.read_history_file(self.histfile) readline.set_history_length(self.history_length) readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self._completer) def _completer(self, text, state): options = [cmd for cmd in self.commands if cmd.startswith(text)] if state < len(options): return options[state] else: return None def start_prompt(self): try: while True: user_input = input(\\"> \\") print(f\'Executing command: {user_input}\') except (EOFError, KeyboardInterrupt): self._save_history() print(\\"nPrompt closed.\\") def _save_history(self): readline.set_history_length(self.history_length) readline.write_history_file(self.histfile) def main(): history_file = os.path.expanduser(\\"~/.custom_history\\") init_file = os.path.expanduser(\\"~/.custom_init\\") commands_list = [\\"start\\", \\"stop\\", \\"restart\\", \\"status\\", \\"help\\"] custom_prompt = CustomInteractivePrompt(history_file, init_file, commands_list) custom_prompt.start_prompt() if __name__ == \\"__main__\\": main()"},{"question":"# Question You are provided with a dataset containing information about daily sales in a retail store. The dataset includes the following columns: - `day`: The day of the week. - `sales`: The total sales for that day. - `category`: The product category (e.g., electronics, clothing, food, etc.). Your task is to write a Python function that: 1. Creates a seaborn bar plot showing the total number of sales entries for each day of the week. 2. Adds an additional bar plot layer that distinguishes the counts by product category, with each category shown in a different color. 3. Ensures that bars for different categories are placed next to each other (dodged) rather than stacked. # Input - A pandas DataFrame `df` with columns: `day`, `sales`, `category`. # Output - A seaborn plot object that displays the bar plot as described. # Constraints - The function should not display the plot but return the plot object. # Example Input ```python import pandas as pd data = { \'day\': [\'Mon\', \'Tue\', \'Wed\', \'Thu\', \'Fri\', \'Sat\', \'Sun\', \'Mon\', \'Tue\', \'Wed\'], \'sales\': [200, 220, 150, 300, 250, 220, 150, 300, 250, 400], \'category\': [\'electronics\', \'clothing\', \'food\', \'electronics\', \'clothing\', \'food\', \'electronics\', \'clothing\', \'food\', \'electronics\'] } df = pd.DataFrame(data) ``` # Example Function ```python import seaborn.objects as so def plot_sales_by_day_and_category(df): plot = so.Plot(df, x=\\"day\\", color=\\"category\\").add(so.Bar(), so.Count(), so.Dodge()) return plot # Usage: # plot = plot_sales_by_day_and_category(df) # plot.show() ``` # Note - Use the Seaborn `objects` interface. - Assume the input DataFrame is correctly formatted with valid values.","solution":"import seaborn.objects as so def plot_sales_by_day_and_category(df): Creates a seaborn bar plot showing the total number of sales entries for each day of the week, with an additional bar plot layer that distinguishes the counts by product category. Parameters: df (pd.DataFrame): DataFrame containing columns \'day\', \'sales\', and \'category\'. Returns: seaborn.objects.Plot: The seaborn plot object. plot = so.Plot(df, x=\\"day\\", color=\\"category\\").add(so.Bar(), so.Count(), so.Dodge()) return plot"},{"question":"# Coding Assessment: Advanced Usage of Generators in Python Objective Demonstrate your understanding of generator objects, iteration, and efficient data handling in Python by implementing a generator-based solution to a real-world problem. Problem Statement You are tasked with developing a Python generator that processes a large text file containing a list of names, one per line. The generator should yield only the unique names in the order they first appear in the file. This is useful for efficiently processing data without loading the entire file into memory. Your task is to implement a function `unique_names_generator(file_path: str) -> Generator[str, None, None]` that: 1. Accepts the path to a text file (`file_path`) as its argument. 2. Yields each unique name from the file as a separate iteration in the order they first appear. Input - `file_path` (str): A string representing the path to a text file. The file contains one name per line. Output - The function should return a generator that yields unique names from the file. Constraints - The file can be very large, potentially exceeding the system\'s memory capacity. Your solution should handle this efficiently without loading the entire file into memory. Example Suppose the content of the file at `file_path` is: ``` Alice Bob Alice Charlie Bob Alice ``` The generator should yield: ``` Alice Bob Charlie ``` Performance Requirements - Your solution should have a time complexity of O(n) where n is the number of lines in the file. - The solution should not use additional memory proportional to the number of lines in the file, aside from storing the unique names that have been seen so far. Hints - Think about using a data structure for keeping track of names that have already been yielded. - Make sure to open the file using a context manager to ensure it is properly closed after processing. Function Signature ```python from typing import Generator def unique_names_generator(file_path: str) -> Generator[str, None, None]: # Write your implementation here pass ```","solution":"from typing import Generator, Set def unique_names_generator(file_path: str) -> Generator[str, None, None]: Yields unique names from a given file in the order they first appear. :param file_path: Path to the text file containing names. :return: A generator yielding unique names. seen_names: Set[str] = set() with open(file_path, \'r\') as file: for line in file: name = line.strip() if name not in seen_names: seen_names.add(name) yield name"},{"question":"# Question: Covariance Estimation with scikit-learn You have been provided with a dataset containing several features and observations. Your task is to implement various covariance estimators from the scikit-learn library, compare their outputs, and analyze the differences. Specifically, you will implement the following: 1. Empirical Covariance 2. Shrunk Covariance 3. Ledoit-Wolf Estimator 4. Oracle Approximating Shrinkage (OAS) Estimator 5. Sparse inverse covariance (GraphicalLasso) 6. Minimum Covariance Determinant (MinCovDet) Input Format: - A dataset `data` (numpy array) of shape `(n_samples, n_features)` containing `n_samples` samples and `n_features` features. Output Format: - For each estimator, print the estimated covariance matrix. - Compare the performance of these covariance estimators using a suitable metric (e.g., log-likelihood, mean squared error) and discuss when each estimator might be preferred. Constraints: - Assume the dataset is sufficiently large (n_samples >> n_features) for all methods to be applicable. Performance Requirements: - Evaluate the computation time for each method and discuss if it scales with the size of the dataset. Example: ```python import numpy as np from sklearn.covariance import (EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso, MinCovDet) # Example dataset data = np.random.randn(100, 5) # 1. Empirical Covariance emp_cov = EmpiricalCovariance().fit(data) print(\\"Empirical Covariance:n\\", emp_cov.covariance_) # 2. Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(data) print(\\"Shrunk Covariance:n\\", shrunk_cov.covariance_) # 3. Ledoit-Wolf Estimator lw_cov = LedoitWolf().fit(data) print(\\"Ledoit-Wolf Covariance:n\\", lw_cov.covariance_) # 4. Oracle Approximating Shrinkage (OAS) Estimator oas_cov = OAS().fit(data) print(\\"OAS Covariance:n\\", oas_cov.covariance_) # 5. Sparse Inverse Covariance (GraphicalLasso) graph_lasso = GraphicalLasso(alpha=0.1).fit(data) print(\\"Graphical Lasso Covariance:n\\", graph_lasso.covariance_) # 6. Minimum Covariance Determinant (MinCovDet) mcd_cov = MinCovDet().fit(data) print(\\"MinCovDet Covariance:n\\", mcd_cov.covariance_) # Performance comparison and analysis... ``` Discuss the results printed by each estimator and the implications in terms of their applicability to datasets with different properties (size, presence of outliers, etc.).","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, GraphicalLasso, MinCovDet import time def compute_covariances(data): results = {} # 1. Empirical Covariance start_time = time.time() emp_cov = EmpiricalCovariance().fit(data) emp_time = time.time() - start_time results[\'Empirical Covariance\'] = (emp_cov.covariance_, emp_time) # 2. Shrunk Covariance start_time = time.time() shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(data) shrunk_time = time.time() - start_time results[\'Shrunk Covariance\'] = (shrunk_cov.covariance_, shrunk_time) # 3. Ledoit-Wolf Estimator start_time = time.time() lw_cov = LedoitWolf().fit(data) lw_time = time.time() - start_time results[\'Ledoit-Wolf Covariance\'] = (lw_cov.covariance_, lw_time) # 4. Oracle Approximating Shrinkage (OAS) Estimator start_time = time.time() oas_cov = OAS().fit(data) oas_time = time.time() - start_time results[\'OAS Covariance\'] = (oas_cov.covariance_, oas_time) # 5. Sparse Inverse Covariance (GraphicalLasso) start_time = time.time() graph_lasso = GraphicalLasso(alpha=0.1).fit(data) gl_time = time.time() - start_time results[\'Graphical Lasso Covariance\'] = (graph_lasso.covariance_, gl_time) # 6. Minimum Covariance Determinant (MinCovDet) start_time = time.time() mcd_cov = MinCovDet().fit(data) mcd_time = time.time() - start_time results[\'MinCovDet Covariance\'] = (mcd_cov.covariance_, mcd_time) return results def print_and_compare_results(results): for method, (cov_matrix, comp_time) in results.items(): print(f\\"{method}:n{cov_matrix}nComputation Time: {comp_time:.5f} secondsn\\") # Additional comparisons and performance discussion can be added here based on the covariances # Example dataset data = np.random.randn(100, 5) results = compute_covariances(data) print_and_compare_results(results)"},{"question":"**Task: Implement and Manage a Custom Dataset Class** **Objective:** In this exercise, you will implement a custom Python class, `CustomDataset`, which simulates some behavior of handling dynamic datasets frequently encountered in machine learning and data science tasks. You will also ensure that your class handles memory management efficiently. **Instructions:** 1. Implement a class `CustomDataset` that takes an optional parameter `data` which is expected to be a list of numerical values. 2. The class should support the following functionalities: - Adding a new data point to the dataset. - Removing a data point from the dataset. - Calculating basic statistics like mean and standard deviation for the dataset. - Handle memory efficiently such that the removal of data points should free up memory as much as possible. **Class Definition:** ```python import math class CustomDataset: def __init__(self, data=None): Initialize the CustomDataset with an optional data list. if data is None: self.data = [] else: self.data = data def add_data_point(self, value): Adds a new data point to the dataset. :param value: The numerical value to be added to the dataset. self.data.append(value) def remove_data_point(self, value): Removes a data point from the dataset if it exists. :param value: The numerical value to be removed from the dataset. if value in self.data: self.data.remove(value) def calculate_mean(self): Calculates and returns the mean of the dataset. :return: The mean of the dataset. if not self.data: return 0 return sum(self.data) / len(self.data) def calculate_std_dev(self): Calculates and returns the standard deviation of the dataset. :return: The standard deviation of the dataset. if not self.data: return 0 mean = self.calculate_mean() variance = sum((x - mean) ** 2 for x in self.data) / len(self.data) return math.sqrt(variance) ``` **Constraints and Considerations:** - You are not allowed to use any additional libraries or packages (except for `math`). - The dataset size should be limited to fit within the memory constraints typical of a single machine. - Ensure that the `remove_data_point` method efficiently manages memory by releasing the removed item\'s memory correctly. **Testing:** You should implement test cases to verify that your `CustomDataset` class works as expected. The tests should cover: 1. Adding and removing data points. 2. Calculating mean and standard deviation. 3. Memory management efficiency (this can be inferred by ensuring no memory leaks etc.). Example test cases can be: ```python def test_custom_dataset(): # Initialize dataset dataset = CustomDataset() # Add data points dataset.add_data_point(10) dataset.add_data_point(20) dataset.add_data_point(30) # Test Mean assert dataset.calculate_mean() == 20.0 # Test Standard Deviation assert math.isclose(dataset.calculate_std_dev(), 8.16, rel_tol=1e-2) # Remove data point and test again dataset.remove_data_point(20) assert dataset.calculate_mean() == 20.0 print(\\"All tests passed.\\") test_custom_dataset() ``` **Note**: Implement the CustomDataset class and the test cases in a Python file/module.","solution":"import math class CustomDataset: def __init__(self, data=None): Initialize the CustomDataset with an optional data list. if data is None: self.data = [] else: self.data = data def add_data_point(self, value): Adds a new data point to the dataset. :param value: The numerical value to be added to the dataset. self.data.append(value) def remove_data_point(self, value): Removes a data point from the dataset if it exists. :param value: The numerical value to be removed from the dataset. if value in self.data: self.data.remove(value) def calculate_mean(self): Calculates and returns the mean of the dataset. :return: The mean of the dataset. if not self.data: return 0 return sum(self.data) / len(self.data) def calculate_std_dev(self): Calculates and returns the standard deviation of the dataset. :return: The standard deviation of the dataset. if not self.data: return 0 mean = self.calculate_mean() variance = sum((x - mean) ** 2 for x in self.data) / len(self.data) return math.sqrt(variance)"},{"question":"**Question**: You are to implement a Python function that attempts to find and load a module using the `imp` module, while also providing a fallback mechanism using `importlib`. The function must check if the module is already loaded, attempt to find and load it using `imp`, and if unsuccessful, proceed to use `importlib` for these tasks. Here\'s a detailed specification: # Function Signature: ```python def load_module_fallback(module_name: str, search_path: list = None) -> object: Attempts to load a module with the given name. If unsuccessful using `imp`, it proceeds to use `importlib` for compatibility with newer Python versions. Parameters: module_name (str): The name of the module to be loaded. search_path (list): A list of directory names to search for the module (default: None). Returns: object: The loaded module object. Raises: ImportError: If the module could not be found or loaded by either method. ``` # Example: ```python # Assuming there is a module named \'example_module\' in the current directory or sys.path try: module = load_module_fallback(\'example_module\') print(module) except ImportError as e: print(\\"Module cannot be loaded:\\", e) ``` # Constraints: 1. **Input**: - The `module_name` is a string corresponding to the module to be loaded. - The `search_path` is either `None` or a list of directory names to look for the module. 2. **Output**: - The function returns the loaded module object if successful. 3. **Dependencies and Compatibility**: - Use `imp` functions such as `find_module()` and `load_module()` if possible. - Fallback to `importlib` in case of failures or for newer Python versions. 4. **Error Handling**: - Raise `ImportError` if the module cannot be found or loaded by either method. - Ensure to handle and properly close file objects used by `imp` methods. # Notes: - The function should prioritize using `imp` for the loading mechanism, demonstrating understanding and use of the deprecated module. - The fallback should be seamlessly integrated using `importlib`.","solution":"import sys import importlib def load_module_fallback(module_name: str, search_path: list = None) -> object: Attempts to load a module with the given name. If unsuccessful using `imp`, it proceeds to use `importlib` for compatibility with newer Python versions. Parameters: module_name (str): The name of the module to be loaded. search_path (list): A list of directory names to search for the module (default: None). Returns: object: The loaded module object. Raises: ImportError: If the module could not be found or loaded by either method. try: if module_name in sys.modules: return sys.modules[module_name] import imp file, pathname, description = imp.find_module(module_name, search_path) try: return imp.load_module(module_name, file, pathname, description) finally: if file: file.close() except ImportError: pass try: return importlib.import_module(module_name) except ModuleNotFoundError: raise ImportError(f\\"Module \'{module_name}\' could not be found or loaded using imp or importlib.\\")"},{"question":"You are given a DataFrame `df` containing information about various products and their sales across different stores and months. The DataFrame has the following columns: - `Store`: The name of the store. - `Product`: The name of the product. - `Month`: The month of the sale (in `YYYY-MM` format). - `Quantity`: The quantity sold. - `Revenue`: The revenue generated from the sale. Example DataFrame: ```python import pandas as pd data = { \\"Store\\": [\\"Store_A\\", \\"Store_A\\", \\"Store_A\\", \\"Store_B\\", \\"Store_B\\", \\"Store_B\\"], \\"Product\\": [\\"Product_1\\", \\"Product_1\\", \\"Product_2\\", \\"Product_1\\", \\"Product_2\\", \\"Product_2\\"], \\"Month\\": [\\"2023-01\\", \\"2023-02\\", \\"2023-01\\", \\"2023-01\\", \\"2023-01\\", \\"2023-02\\"], \\"Quantity\\": [10, 20, 30, 15, 10, 5], \\"Revenue\\": [100, 300, 200, 150, 100, 50] } df = pd.DataFrame(data) ``` **Task**: Write a function `analyze_sales(df)` that returns a summary DataFrame with the following metrics calculated for each store-product pair, grouped by each month: 1. Total Quantity sold. 2. Total Revenue generated. 3. Average Revenue per unit (i.e., `Revenue / Quantity`). The resultant DataFrame should have a multi-level index, with the first level being the `Store`, the second level being the `Product`, and the third level being the `Month`. The columns of the resultant DataFrame should be `Total Quantity`, `Total Revenue`, and `Average Revenue per Unit`. Ensure that the resultant DataFrame is sorted by `Store`, `Product`, and then by `Month`. **Input:** - A pandas DataFrame `df` with columns `Store`, `Product`, `Month`, `Quantity`, and `Revenue`. **Output:** - A pandas DataFrame with a multi-level index (`Store`, `Product`, `Month`) and columns `Total Quantity`, `Total Revenue`, and `Average Revenue per Unit`. **Function Signature:** ```python def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Example:** ```python df = pd.DataFrame({ \\"Store\\": [\\"Store_A\\", \\"Store_A\\", \\"Store_A\\", \\"Store_B\\", \\"Store_B\\", \\"Store_B\\"], \\"Product\\": [\\"Product_1\\", \\"Product_1\\", \\"Product_2\\", \\"Product_1\\", \\"Product_2\\", \\"Product_2\\"], \\"Month\\": [\\"2023-01\\", \\"2023-02\\", \\"2023-01\\", \\"2023-01\\", \\"2023-01\\", \\"2023-02\\"], \\"Quantity\\": [10, 20, 30, 15, 10, 5], \\"Revenue\\": [100, 300, 200, 150, 100, 50] }) result = analyze_sales(df) print(result) ``` Expected Output: ``` Total Quantity Total Revenue Average Revenue per Unit Store Product Month Store_A Product_1 2023-01 10 100 10.0 2023-02 20 300 15.0 Product_2 2023-01 30 200 6.67 Store_B Product_1 2023-01 15 150 10.0 Product_2 2023-01 10 100 10.0 2023-02 5 50 10.0 ``` **Constraints**: - Use pandas GroupBy and related functionalities. - Handle any edge cases you might foresee, such as divisions by zero or missing data.","solution":"import pandas as pd def analyze_sales(df: pd.DataFrame) -> pd.DataFrame: Returns a summary DataFrame with total quantity sold, total revenue generated, and average revenue per unit for each store-product pair, grouped by each month. # Group by Store, Product, and Month grouping = df.groupby([\\"Store\\", \\"Product\\", \\"Month\\"], as_index=False) # Aggregate the total quantity and total revenue summary = grouping.agg( Total_Quantity=pd.NamedAgg(column=\\"Quantity\\", aggfunc=\\"sum\\"), Total_Revenue=pd.NamedAgg(column=\\"Revenue\\", aggfunc=\\"sum\\") ) # Calculate the average revenue per unit summary[\\"Average_Revenue_per_Unit\\"] = summary[\\"Total_Revenue\\"] / summary[\\"Total_Quantity\\"] # Set the multi-level index summary = summary.set_index([\\"Store\\", \\"Product\\", \\"Month\\"]) # Sort the DataFrame summary = summary.sort_index() return summary"},{"question":"You are required to implement and test a simple neural network using the PyTorch framework. Your task is to initialize the network weights and biases using different initialization methods from the `torch.nn.init` module. The focus is on understanding and applying different initialization strategies and observing their impact. Requirements: 1. **Network Definition**: - Create a simple feed-forward neural network class `SimpleNN` with one hidden layer. - The network should take the number of input features, the number of hidden units, and the number of output units as input arguments. 2. **Initialization Function**: - Implement a method `initialize_weights` within the `SimpleNN` class to initialize weights and biases. - The method should take a string argument `init_type` which specifies the initialization strategy to use. Supported strategies are: - `\\"uniform\\"`: Use `torch.nn.init.uniform_`. - `\\"normal\\"`: Use `torch.nn.init.normal_`. - `\\"xavier_uniform\\"`: Use `torch.nn.init.xavier_uniform_`. - `\\"xavier_normal\\"`: Use `torch.nn.init.xavier_normal_`. - `\\"kaiming_uniform\\"`: Use `torch.nn.init.kaiming_uniform_`. - `\\"kaiming_normal\\"`: Use `torch.nn.init.kaiming_normal_`. - `\\"constant\\"`: Use `torch.nn.init.constant_`, with a constant value of 0.5. 3. **Input and Output Formats**: - **Input**: - `num_features` (int): Number of input features. - `hidden_units` (int): Number of units in the hidden layer. - `output_units` (int): Number of output units. - `init_type` (str): Initialization strategy to use. - **Output**: None. The network should initialize its parameters using the specified strategy. 4. **Constraints**: - Ensure that the `initialize_weights` method affects all layers of the network. - You should test and verify the initialization by printing the weights and biases of each layer. 5. **Performance Requirements**: - Ensure your code runs efficiently for various initialization methods. - The network should handle up to 1000 input features and 1000 hidden units. Example: ```python import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, num_features, hidden_units, output_units): super(SimpleNN, self).__init__() self.hidden = nn.Linear(num_features, hidden_units) self.output = nn.Linear(hidden_units, output_units) def initialize_weights(self, init_type): if init_type == \\"uniform\\": nn.init.uniform_(self.hidden.weight) nn.init.uniform_(self.output.weight) elif init_type == \\"normal\\": nn.init.normal_(self.hidden.weight) nn.init.normal_(self.output.weight) elif init_type == \\"xavier_uniform\\": nn.init.xavier_uniform_(self.hidden.weight) nn.init.xavier_uniform_(self.output.weight) elif init_type == \\"xavier_normal\\": nn.init.xavier_normal_(self.hidden.weight) nn.init.xavier_normal_(self.output.weight) elif init_type == \\"kaiming_uniform\\": nn.init.kaiming_uniform_(self.hidden.weight, nonlinearity=\'relu\') nn.init.kaiming_uniform_(self.output.weight, nonlinearity=\'relu\') elif init_type == \\"kaiming_normal\\": nn.init.kaiming_normal_(self.hidden.weight, nonlinearity=\'relu\') nn.init.kaiming_normal_(self.output.weight, nonlinearity=\'relu\') elif init_type == \\"constant\\": nn.init.constant_(self.hidden.weight, 0.5) nn.init.constant_(self.output.weight, 0.5) else: raise ValueError(\\"Unsupported initialization type\\") # Biases can be initialized as zeroes or ones nn.init.zeros_(self.hidden.bias) nn.init.zeros_(self.output.bias) # Example usage: num_features = 10 hidden_units = 20 output_units = 5 init_type = \\"xavier_normal\\" model = SimpleNN(num_features, hidden_units, output_units) model.initialize_weights(init_type) # Print weights to verify initialization print(\\"Hidden Weights: \\", model.hidden.weight) print(\\"Output Weights: \\", model.output.weight) ``` Implement and test this functionality. Verify the initialized weights and biases to ensure the correct application of each initialization strategy.","solution":"import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, num_features, hidden_units, output_units): super(SimpleNN, self).__init__() self.hidden = nn.Linear(num_features, hidden_units) self.output = nn.Linear(hidden_units, output_units) def initialize_weights(self, init_type): if init_type == \\"uniform\\": nn.init.uniform_(self.hidden.weight) nn.init.uniform_(self.output.weight) elif init_type == \\"normal\\": nn.init.normal_(self.hidden.weight) nn.init.normal_(self.output.weight) elif init_type == \\"xavier_uniform\\": nn.init.xavier_uniform_(self.hidden.weight) nn.init.xavier_uniform_(self.output.weight) elif init_type == \\"xavier_normal\\": nn.init.xavier_normal_(self.hidden.weight) nn.init.xavier_normal_(self.output.weight) elif init_type == \\"kaiming_uniform\\": nn.init.kaiming_uniform_(self.hidden.weight, nonlinearity=\'relu\') nn.init.kaiming_uniform_(self.output.weight, nonlinearity=\'relu\') elif init_type == \\"kaiming_normal\\": nn.init.kaiming_normal_(self.hidden.weight, nonlinearity=\'relu\') nn.init.kaiming_normal_(self.output.weight, nonlinearity=\'relu\') elif init_type == \\"constant\\": nn.init.constant_(self.hidden.weight, 0.5) nn.init.constant_(self.output.weight, 0.5) else: raise ValueError(\\"Unsupported initialization type\\") # Biases can be initialized as zeroes or some strategy nn.init.zeros_(self.hidden.bias) nn.init.zeros_(self.output.bias)"},{"question":"**Problem Statement:** You are working on an internal tool for a Python development process that involves saving and loading certain configuration data in a binary format. You\'re required to use the `marshal` module to implement this functionality due to its efficiency and sufficient support for the required data types. # Task 1. Implement a function `save_config(config, filepath)`: - **Input:** - `config` (dict): A dictionary containing key-value pairs where keys are strings and values can be any of the supported types (booleans, integers, floating point numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, and dictionaries). - `filepath` (str): The path to the file where the configuration is to be saved. - **Output:** None - **Description:** This function should serialize the `config` dictionary using the `marshal` module and save it to the specified file. 2. Implement a function `load_config(filepath)`: - **Input:** - `filepath` (str): The path to the file from which the configuration is to be loaded. - **Output:** - `config` (dict): A dictionary containing the configuration data. - **Description:** This function should read the binary data from the specified file and deserialize it back into a dictionary using the `marshal` module. Ensure any unsupported types result in an exception being raised, avoiding silent failure. # Constraints: - Do not use any other serialization module like `pickle` or `json`. - The configuration data must be a dictionary with string keys. - Handle exceptions that may arise from unsupported types or file I/O errors gracefully, providing meaningful error messages. - Ensure that the serialized data uses the current `marshal` format version (version 4). # Example ```python config = { \\"enabled\\": True, \\"max_retries\\": 5, \\"timeout\\": 15.75, \\"notes\\": \\"These are configuration notes.\\", \\"nested\\": {\\"key1\\": 1, \\"key2\\": [1, 2, 3]} } filepath = \\"config.dat\\" # Save configuration save_config(config, filepath) # Load configuration loaded_config = load_config(filepath) assert config == loaded_config ``` Additional Information: - If `marshal` encounters an unsupported type during serialization, it can raise a `ValueError` exception. - During deserialization, if an invalid format or type is encountered, `EOFError`, `ValueError`, or `TypeError` may be raised. Implement the two functions `save_config` and `load_config`, following the specifications and constraints given above.","solution":"import marshal def save_config(config, filepath): Serializes the `config` dictionary using the `marshal` module and saves it to the specified file. Parameters: config (dict): The configuration dictionary to be saved. filepath (str): The path to the file where the configuration will be saved. Raises: ValueError: If `config` contains types not supported by `marshal`. # Validate config to ensure all types are supported by marshal def validate(data): if not isinstance(data, (bool, int, float, str, bytes, bytearray, tuple, list, set, frozenset, dict)): raise ValueError(f\\"Unsupported type {type(data)} in configuration\\") if isinstance(data, dict): for key, value in data.items(): if not isinstance(key, str): raise ValueError(f\\"Unsupported key type {type(key)} in configuration; only strings are allowed.\\") validate(value) elif isinstance(data, (list, tuple, set, frozenset)): for item in data: validate(item) validate(config) # Save the validated config using marshal with open(filepath, \'wb\') as f: marshal.dump(config, f, 4) def load_config(filepath): Reads the binary data from the specified file and deserializes it back into a dictionary using the `marshal` module. Parameters: filepath (str): The path to the file from which the configuration is to be loaded. Returns: dict: The deserialized configuration dictionary. Raises: ValueError: If the data loaded from the file contains types not supported by `marshal`. with open(filepath, \'rb\') as f: config = marshal.load(f) # Validate the deserialized config for supported types def validate(data): if not isinstance(data, (bool, int, float, str, bytes, bytearray, tuple, list, set, frozenset, dict)): raise ValueError(f\\"Unsupported type {type(data)} in configuration\\") if isinstance(data, dict): for key, value in data.items(): if not isinstance(key, str): raise ValueError(f\\"Unsupported key type {type(key)} in configuration; only strings are allowed.\\") validate(value) elif isinstance(data, (list, tuple, set, frozenset)): for item in data: validate(item) validate(config) return config"},{"question":"**Problem Statement:** Using scikit-learn, write a function `fetch_and_train_model(data_id: int) -> Tuple[float, float]` that: 1. Fetches the dataset from OpenML using the provided `data_id`. 2. Splits the dataset into training and testing sets. 3. Trains a basic machine learning model (e.g., a Logistic Regression model) using the training data. 4. Returns the accuracy of the model on both the training and testing datasets as a tuple. **Function Signature:** ```python from typing import Tuple import numpy as np import pandas as pd def fetch_and_train_model(data_id: int) -> Tuple[float, float]: pass ``` **Requirements:** - Use the `fetch_openml` function to load the dataset. - Split the dataset into training and testing sets with 80% for training and 20% for testing using `train_test_split` from `sklearn.model_selection`. - Use a `LogisticRegression` model from `sklearn.linear_model`. - Ensure that all categorical features are properly encoded using `OneHotEncoder` or any other appropriate encoder from `sklearn.preprocessing`. - The function should return a tuple containing the accuracy on the training data and the accuracy on the testing data. **Example:** ```python train_accuracy, test_accuracy = fetch_and_train_model(40966) # Replace 40966 with an actual data_id from OpenML print(f\\"Training Accuracy: {train_accuracy}, Testing Accuracy: {test_accuracy}\\") ``` **Constraints and Notes:** - Assume that the dataset loaded will have both features and a target variable. - The dataset may contain categorical variables. - Make sure to handle any missing values in the dataset appropriately. **Hints:** - You can use the `SimpleImputer` from `sklearn.impute` for handling missing values. - To efficiently preprocess categorical features, consider using `ColumnTransformer` from `sklearn.compose`. This question evaluates the student\'s ability to interact with external data sources, preprocess data, handle categorical variables, train a machine learning model, and evaluate its performance.","solution":"from typing import Tuple import numpy as np import pandas as pd from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline def fetch_and_train_model(data_id: int) -> Tuple[float, float]: # Fetch dataset from OpenML data = fetch_openml(data_id=data_id, as_frame=True) X = data.data y = data.target # Split dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocessing pipelines for numerical and categorical data numerical_features = X.select_dtypes(include=[np.number]).columns.tolist() categorical_features = X.select_dtypes(include=[object]).columns.tolist() numerical_pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_pipeline = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessing pipelines preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_pipeline, numerical_features), (\'cat\', categorical_pipeline, categorical_features) ]) # Create a combined pipeline with preprocessing and logistic regression model = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(max_iter=1000)) ]) # Train the model model.fit(X_train, y_train) # Make predictions y_train_pred = model.predict(X_train) y_test_pred = model.predict(X_test) # Calculate accuracy train_accuracy = accuracy_score(y_train, y_train_pred) test_accuracy = accuracy_score(y_test, y_test_pred) return train_accuracy, test_accuracy"},{"question":"Objective: Write a Python function that demonstrates your ability to handle system errors using the `errno` module. Task Description: You need to implement a function named `translate_error_code`. This function takes a list of integer error codes and returns a list of their corresponding string representations and exception types. Function Signature: ```python def translate_error_code(error_codes: List[int]) -> List[Tuple[str, str]]: ``` Input: - `error_codes`: A list of integers representing error codes. Output: - The function should return a list of tuples. Each tuple should contain: 1. The string representation of the error (e.g., \'EPERM\'). 2. The name of the exception type mapped to this error as a string (e.g., \'PermissionError\'). Example: ```python >>> translate_error_code([1, 2, 3]) [(\'EPERM\', \'PermissionError\'), (\'ENOENT\', \'FileNotFoundError\'), (\'ESRCH\', \'ProcessLookupError\')] ``` Constraints: - You may assume that all input error codes are valid and they exist in the `errno` module. Additional Notes: - Use `errno.errorcode` to map each error number to its string name. - Use proper handling to ensure that the code can translate the errors even if some of them are missing or not defined on the current platform. Performance Requirements: - Your solution should be efficient and able to handle a large list of error codes up to 1000 elements.","solution":"import errno def translate_error_code(error_codes): Translates a list of integer error codes into a list of tuples containing string representations and exception types. Args: error_codes (List[int]): List of integer error codes. Returns: List[Tuple[str, str]]: List of tuples where each tuple contains the string representation of the error and the corresponding exception type. result = [] for code in error_codes: error_name = errno.errorcode.get(code) if error_name: exception_type = { 1: \'PermissionError\', 2: \'FileNotFoundError\', 3: \'ProcessLookupError\', 4: \'InterruptedError\', 5: \'BlockingIOError\', 6: \'ChildProcessError\', 7: \'ConnectionException\', 8: \'ConnectionException\', 9: \'InterruptedError\', # Mapping other relevant errno error codes to their exception names # This should be expanded as needed }.get(code, \'OSError\') # default to generic OSError if not explicitly mapped result.append((error_name, exception_type)) else: result.append((None, \'OSError\')) # default for unmapped or platform-specific error codes return result"},{"question":"# Advanced Coding Challenge: JSON Data Processing and Customized Output Objective To assess your understanding of file I/O operations, JSON data handling, and string formatting in Python, implement a function that processes JSON data from a file and generates a human-readable report. Problem Statement In this challenge, you will write Python code to read JSON data from a file, process it, and output a formatted summary report. The JSON data pertains to a list of students and their details, including name, age, and grades in multiple subjects. Task 1. Write a function `process_student_data(input_file: str, output_file: str) -> None` that: - Reads a JSON file specified by `input_file`. - Processes the data to calculate each student\'s average grade. - Writes a formatted report to a new file specified by `output_file`. JSON Input Format The input JSON file contains a list of student records. Each record is a dictionary with the following structure: ```json [ { \\"name\\": \\"Student Name\\", \\"age\\": 20, \\"grades\\": { \\"math\\": 85, \\"science\\": 78, \\"history\\": 92 } }, ... ] ``` Report Output Format The output should be a text file with the following format: - Each student\'s details should be presented on separate lines. - The report should include the name, age, and average grade formatted to two decimal places. - Use string formatting functionalities such as f-strings or `str.format()` to achieve this. Example Output: ``` Name: Student Name Age: 20 Average Grade: 85.00 Name: Another Student Age: 22 Average Grade: 89.67 ... ``` Constraints - The JSON input file will have at least one student record. - Each student will have grades for at least one subject. - You must use the `with` statement for file operations to ensure proper resource management. Evaluation Criteria - Correctness: The output file correctly reflects the transformed data. - Code Quality: Use of appropriate string formatting techniques for the report. - Efficiency: The function should handle the data efficiently, even if the JSON data is large. Example Assuming you have a file `students.json` with the following content: ```json [ { \\"name\\": \\"John Doe\\", \\"age\\": 19, \\"grades\\": { \\"math\\": 90, \\"science\\": 85, \\"history\\": 88 } }, { \\"name\\": \\"Jane Doe\\", \\"age\\": 21, \\"grades\\": { \\"math\\": 95, \\"science\\": 80, \\"history\\": 82 } } ] ``` A call to `process_student_data(\'students.json\', \'report.txt\')` should create a file `report.txt` with the following content: ``` Name: John Doe Age: 19 Average Grade: 87.67 Name: Jane Doe Age: 21 Average Grade: 85.67 ``` Notes - Ensure your function handles potential errors gracefully, such as a missing file or incorrect JSON format. - Consider adding comments and docstrings to explain the logic of your code.","solution":"import json def process_student_data(input_file: str, output_file: str) -> None: Reads a JSON file containing student data, processes it to calculate each student\'s average grade, and writes a formatted report to an output file. Args: input_file (str): The path to the input JSON file. output_file (str): The path to the output text file. with open(input_file, \'r\') as infile: students = json.load(infile) with open(output_file, \'w\') as outfile: for student in students: name = student[\'name\'] age = student[\'age\'] grades = student[\'grades\'].values() avg_grade = sum(grades) / len(grades) outfile.write(f\\"Name: {name}n\\") outfile.write(f\\"Age: {age}n\\") outfile.write(f\\"Average Grade: {avg_grade:.2f}nn\\")"},{"question":"**Question: Explore Relationships in a Dataset using Seaborn** You are given a dataset containing information about various metrics collected over a period. Your task is to visualize the relationships between these metrics using the `seaborn` library. Specifically, you will create a multi-faceted plot to explore how these relationships change across different categories and conditions. # Dataset The dataset `metrics.csv` contains the following columns: - `time`: The time at which the measurement was taken. - `value`: The measured value of the metric. - `metric_type`: The type of metric being measured (e.g., \\"temperature\\", \\"humidity\\"). - `category`: A categorical variable indicating different conditions or groups (e.g., \\"A\\", \\"B\\"). - `region`: The region from which the measurement was taken (e.g., \\"North\\", \\"South\\"). # Task **1. Load the dataset.** Create a Pandas DataFrame from the provided `metrics.csv`. **2. Visualize the data.** Use `seaborn.relplot` to create a line plot showing the change in `value` over `time`. Use the following customizations: - Use different colors (hue) for different `metric_type`. - Use different line styles for different `category`. - Create separate columns for different `region`. - Customize the size of each plot facet to ensure clarity. **3. Aggregation and Error Bars.** - Use the `errorbar` parameter to show the standard deviation at each time point. **4. Advanced Visualization:** - Use the `units` parameter to plot each individual measurement for the `category` \\"A\\" without aggregating them. # Input Format - The `metrics.csv` file with columns: `time`, `value`, `metric_type`, `category`, and `region`. # Output Format - A multi-faceted `relplot` showing the required customizations and aggregation. # Constraints - Ensure that the visualizations are clear and readable. - The dataset may be large, so consider computation efficiency. # Example Code Here is a starting template for your implementation: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = pd.read_csv(\\"metrics.csv\\") # Step 2: Create the basic `relplot` sns.relplot( data=data, kind=\\"line\\", x=\\"time\\", y=\\"value\\", hue=\\"metric_type\\", style=\\"category\\", col=\\"region\\", height=5, aspect=1.5, errorbar=\\"sd\\" ) # Step 4: Advanced Visualization sns.relplot( data=data.query(\\"category == \'A\'\\"), kind=\\"line\\", x=\\"time\\", y=\\"value\\", hue=\\"metric_type\\", col=\\"region\\", units=\\"metric_type\\", estimator=None, height=5, aspect=1.5 ) plt.show() ``` # Notes - Ensure you have `seaborn`, `pandas`, and `matplotlib` properly installed. - Adjust the size and aspect ratio of the plots as necessary to ensure readability.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_dataset(filepath): Load the dataset from the given filepath. return pd.read_csv(filepath) def visualize_data(data): Create and display the required multi-faceted line plot using seaborn. # Step 2: Create the basic `relplot` g = sns.relplot( data=data, kind=\\"line\\", x=\\"time\\", y=\\"value\\", hue=\\"metric_type\\", style=\\"category\\", col=\\"region\\", height=5, aspect=1.5, errorbar=\\"sd\\" ) g.set_axis_labels(\\"Time\\", \\"Value\\") g.set_titles(\\"Region: {col_name}\\") # Step 4: Advanced Visualization ga = sns.relplot( data=data.query(\\"category == \'A\'\\"), kind=\\"line\\", x=\\"time\\", y=\\"value\\", hue=\\"metric_type\\", col=\\"region\\", units=\\"metric_type\\", estimator=None, height=5, aspect=1.5 ) ga.set_axis_labels(\\"Time\\", \\"Value\\") ga.set_titles(\\"Region: {col_name}\\") plt.show()"},{"question":"**Objective:** This question aims to test your understanding of kernel density estimation plots using Seaborn\'s `kdeplot` function. You are required to implement a function that generates multiple visualizations based on different aspects of a given dataset. **Instructions:** You need to implement a function called `kde_visualizations` that accepts a dataset and creates and saves a series of Kernel Density Estimation (KDE) plots illustrating different characteristics of the data. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def kde_visualizations(data: pd.DataFrame): Generates and saves KDE plots to illustrate different characteristics of the input dataset. Args: data (pd.DataFrame): A pandas DataFrame containing the data to be visualized. Returns: None ``` **Requirements:** 1. **Univariate KDE Plot:** - Plot the KDE for the \'total_bill\' column. - Save this plot as `univariate_kde.png`. 2. **Bivariate KDE Plot:** - Plot the bivariate KDE for \'waiting\' and \'duration\' columns in the \'geyser\' dataset. - Save this plot as `bivariate_kde.png`. 3. **Conditional KDE Plot with Hue:** - Plot the KDE for \'total_bill\' column with \'time\' as the hue. - Save this plot as `conditional_kde.png`. 4. **Filled KDE Plot:** - Plot a filled KDE for \'total_bill\' with \'size\' as the hue, using a custom color palette. - Save this plot as `filled_kde.png`. 5. **Log-scaled KDE Plot:** - Using the \'diamonds\' dataset, plot a KDE for the \'price\' column with log scale. - Save this plot as `log_scaled_kde.png`. 6. **High Smoothing KDE Plot:** - Plot a KDE for the \'total_bill\' column with a high smoothing parameter. - Save this plot as `high_smoothing_kde.png`. **Example Usage:** ```python # Assuming `tips`, `diamonds`, and `geyser` datasets are loaded appropriately as pandas DataFrames tips = sns.load_dataset(\\"tips\\") diamonds = sns.load_dataset(\\"diamonds\\") geyser = sns.load_dataset(\\"geyser\\") # Function call kde_visualizations(tips) ``` **Constraints:** - Ensure that each plot is saved with the specified filename. - You may assume that the necessary datasets will be provided and correctly formatted. Implement the `kde_visualizations` function to meet the above requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def kde_visualizations(tips_data: pd.DataFrame, geyser_data: pd.DataFrame, diamonds_data: pd.DataFrame): Generates and saves KDE plots to illustrate different characteristics of the input dataset. Args: tips_data (pd.DataFrame): A pandas DataFrame containing the tips data. geyser_data (pd.DataFrame): A pandas DataFrame containing the geyser data. diamonds_data (pd.DataFrame): A pandas DataFrame containing the diamonds data. Returns: None # Univariate KDE Plot plt.figure() sns.kdeplot(data=tips_data, x=\\"total_bill\\") plt.savefig(\\"univariate_kde.png\\", format=\\"png\\") # Bivariate KDE Plot plt.figure() sns.kdeplot(data=geyser_data, x=\\"waiting\\", y=\\"duration\\") plt.savefig(\\"bivariate_kde.png\\", format=\\"png\\") # Conditional KDE Plot with Hue plt.figure() sns.kdeplot(data=tips_data, x=\\"total_bill\\", hue=\\"time\\") plt.savefig(\\"conditional_kde.png\\", format=\\"png\\") # Filled KDE Plot plt.figure() sns.kdeplot(data=tips_data, x=\\"total_bill\\", hue=\\"size\\", fill=True, palette=\\"crest\\") plt.savefig(\\"filled_kde.png\\", format=\\"png\\") # Log-scaled KDE Plot plt.figure() sns.kdeplot(data=diamonds_data, x=\\"price\\") plt.xscale(\'log\') plt.savefig(\\"log_scaled_kde.png\\", format=\\"png\\") # High Smoothing KDE Plot plt.figure() sns.kdeplot(data=tips_data, x=\\"total_bill\\", bw_adjust=2) plt.savefig(\\"high_smoothing_kde.png\\", format=\\"png\\")"},{"question":"**Objective:** The goal of this coding assessment is to evaluate your understanding of the `cross_decomposition` module in scikit-learn, specifically focusing on using the `PLSCanonical` and `PLSRegression` classes. You are required to implement a function that applies Partial Least Squares Regression (PLSRegression) to predict target values based on given input data and evaluate its performance using a performance metric. **Problem Statement:** You are provided with two datasets, `X_train` and `Y_train`, which represent training data and their corresponding target values, respectively. Additionally, you are provided with `X_test`, which represents test data for which you need to predict the target values. Your task is to implement a function that trains a PLSRegression model on the training data and predicts the target values for the test data. **Function Signature:** ```python import numpy as np from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error def pls_regression_predict(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, n_components: int) -> np.ndarray: Train a PLSRegression model on the training data and predict target values for the test data. Parameters: X_train (np.ndarray): Training data of shape (n_samples, n_features). Y_train (np.ndarray): Target values of shape (n_samples, n_targets). X_test (np.ndarray): Test data of shape (n_samples_test, n_features). n_components (int): Number of components to use in PLSRegression. Returns: np.ndarray: Predicted target values for the test data of shape (n_samples_test, n_targets). pass ``` **Requirements:** 1. The function should use the `PLSRegression` class from `sklearn.cross_decomposition`. 2. The number of components to use in PLSRegression (`n_components`) will be provided as an input to the function. 3. The function should fit the `PLSRegression` model to the training data (`X_train` and `Y_train`). 4. The function should predict the target values for the test data (`X_test`). 5. The function should return the predicted target values for the test data. **Constraints:** - You can assume that `X_train`, `Y_train`, and `X_test` are all numpy arrays of appropriate shapes. - The number of components (`n_components`) will be a positive integer that does not exceed the minimum of the number of samples or the number of features in `X_train`. **Performance Metric:** Your function will be evaluated based on the accuracy of the predicted values using the Mean Squared Error (MSE) between the predicted and actual target values for a given validation set. Aim to minimize the MSE. **Example:** ```python import numpy as np X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) Y_train = np.array([[1], [2], [3], [4]]) X_test = np.array([[2, 3], [4, 5]]) n_components = 2 predicted_Y_test = pls_regression_predict(X_train, Y_train, X_test, n_components) print(predicted_Y_test) ``` Implement the function `pls_regression_predict` to solve the provided problem.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression def pls_regression_predict(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, n_components: int) -> np.ndarray: Train a PLSRegression model on the training data and predict target values for the test data. Parameters: X_train (np.ndarray): Training data of shape (n_samples, n_features). Y_train (np.ndarray): Target values of shape (n_samples, n_targets). X_test (np.ndarray): Test data of shape (n_samples_test, n_features). n_components (int): Number of components to use in PLSRegression. Returns: np.ndarray: Predicted target values for the test data of shape (n_samples_test, n_targets). # Initialize the PLSRegression model with the given number of components pls = PLSRegression(n_components=n_components) # Fit the model to the training data pls.fit(X_train, Y_train) # Predict the target values for the test data Y_pred = pls.predict(X_test) return Y_pred"},{"question":"Objective Evaluate the student\'s ability to use seaborn to create and customize residual plots, test for linearity, and apply advanced configurations. Question Write a function `analyze_residuals` that: 1. Loads the `mpg` dataset using seaborn. 2. Generates a residual plot for `weight` vs. `displacement`. 3. Generates and saves a residual plot for `horsepower` vs. `mpg` in three different ways: - Simple linear regression. - Second-order polynomial regression. - Including a LOWESS curve. You are required to perform the following tasks: 1. **Load the dataset** using seaborn\'s `load_dataset` method. 2. **Create a residual plot** for `weight` vs. `displacement` without any advanced configurations. 3. **Create and save three residual plots** for `horsepower` vs. `mpg`: - Simple linear regression - Second-order polynomial regression - Including a LOWESS curve The filenames for the saved residual plots should be: - `simple_linear_regression.png` - `second_order_polynomial_regression.png` - `lowess_curve.png` Function Signature ```python def analyze_residuals(): pass ``` Constraints - You must use seaborn for generating plots. - Save the plots using seaborn or matplotlib functions. Expected Output When executed, the function should generate and display the specified residual plots. Three files should be saved in the current working directory with the names provided above. Example The expected output plots are: 1. A displayed residual plot for `weight` vs. `displacement`. 2. Three saved residual plots with the filenames specified above. To verify your solution, you can manually check the created files in the directory.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_residuals(): # Load dataset mpg = sns.load_dataset(\'mpg\') # Residual plot: weight vs. displacement sns.residplot(x=\'weight\', y=\'displacement\', data=mpg) plt.title(\'Residuals of Weight vs. Displacement\') plt.show() # Residual plot: horsepower vs. mpg with simple linear regression sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg) plt.title(\'Residuals of Horsepower vs. MPG (Linear Regression)\') plt.savefig(\'simple_linear_regression.png\') # Residual plot: horsepower vs. mpg with second-order polynomial regression sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, order=2) plt.title(\'Residuals of Horsepower vs. MPG (2nd Order Polynomial Regression)\') plt.savefig(\'second_order_polynomial_regression.png\') # Residual plot: horsepower vs. mpg with LOWESS curve sns.residplot(x=\'horsepower\', y=\'mpg\', data=mpg, lowess=True) plt.title(\'Residuals of Horsepower vs. MPG (LOWESS)\') plt.savefig(\'lowess_curve.png\') # Show the last plot plt.show()"},{"question":"**Question: Implementing Advanced Slicing Functionality** You are tasked with creating a utility function that utilizes Python\'s slice functionalities provided by the `PySlice_*` functions. The function, `create_and_adjust_slice`, should create a new slice object given specific start, stop, and step values, and then adjust these indices for a sequence of a specified length, ensuring they are within bounds. # Function Signature: ```python def create_and_adjust_slice(start, stop, step, length): Creates a new slice object with the given start, stop, and step values, and adjusts these values for a sequence with the specified length. Parameters: start (int): The starting index of the slice. stop (int): The stopping index of the slice. step (int): The step value of the slice. length (int): The length of the sequence. Returns: tuple: A tuple (adjusted_start, adjusted_stop, adjusted_step, slice_length) where adjusted_start, adjusted_stop, and adjusted_step are the adjusted slice indices, and slice_length is the length of the resulting slice. pass ``` # Inputs: 1. `start` (int): The starting index of the slice, can be any integer. 2. `stop` (int): The stopping index of the slice, can be any integer. 3. `step` (int): The step value of the slice, must be a non-zero integer. 4. `length` (int): The length of the sequence you intend to apply the slice to, must be a non-negative integer. # Outputs: - A tuple `(adjusted_start, adjusted_stop, adjusted_step, slice_length)`: - `adjusted_start` (int): The start index of the slice after adjustment. - `adjusted_stop` (int): The stop index of the slice after adjustment. - `adjusted_step` (int): The step value of the slice after adjustment. - `slice_length` (int): The length of the slice after adjustment. # Constraints: - Both `start` and `stop` indices can be any integer (negative, zero, positive). - The `step` value must not be zero. - The `length` of the sequence must not be negative. - Indices greater than the sequence length should be clipped appropriately. # Example: ```python # Example input start = 2 stop = 10 step = 2 length = 5 # Expected output (after indices are adjusted for length) # adjusted_start = 2, adjusted_stop = 5, adjusted_step = 2, slice_length = 2 print(create_and_adjust_slice(2, 10, 2, 5)) # Output: (2, 5, 2, 2) ``` # Notes: - The function should handle out-of-bounds indices by clipping them to valid values. - If `step` is negative, adjustment rules differ as the slice traverses in the reverse direction. - Consider edge cases such as very large or very small index values and extreme sequence lengths. Use the provided functions and concepts from the `PySlice_*` documentation to implement this utility function correctly.","solution":"def create_and_adjust_slice(start, stop, step, length): Creates a new slice object with the given start, stop, and step values, and adjusts these values for a sequence with the specified length. Parameters: start (int): The starting index of the slice. stop (int): The stopping index of the slice. step (int): The step value of the slice. length (int): The length of the sequence. Returns: tuple: A tuple (adjusted_start, adjusted_stop, adjusted_step, slice_length) where adjusted_start, adjusted_stop, and adjusted_step are the adjusted slice indices, and slice_length is the length of the resulting slice. # Ensure step is not zero if step == 0: raise ValueError(\\"Step must be a non-zero integer.\\") # Adjust start index if start is None: start = 0 if step > 0 else length - 1 elif start < 0: start += length if start < 0: start = 0 if step > 0 else -1 elif start >= length: start = length if step > 0 else length - 1 # Adjust stop index if stop is None: stop = length if step > 0 else -1 elif stop < 0: stop += length if stop < 0: stop = -1 elif stop >= length: stop = length if step > 0 else length - 1 # Calculating the slice length: if step > 0: slice_length = max(0, (stop - start + (step - 1)) // step) else: slice_length = max(0, (start - stop - (step + 1)) // (-step)) return (start, stop, step, slice_length)"},{"question":"# Clustering with K-Means and Evaluation with Adjusted Rand Index **Objective:** In this task, you are required to implement a function that clusters data points using the K-Means algorithm from the `sklearn.cluster` module. Subsequently, you will evaluate the clustering performance using the Adjusted Rand Index from the `sklearn.metrics` module. **Function Signature:** ```python def kmeans_clustering_and_evaluation(data: np.ndarray, true_labels: np.ndarray, n_clusters: int) -> Tuple[np.ndarray, float]: This function performs clustering on the input data using K-Means algorithm and evaluates the performance using Adjusted Rand Index. Parameters: - data: np.ndarray of shape (n_samples, n_features), this is the input data to be clustered. - true_labels: np.ndarray of shape (n_samples,), these are the ground truth labels of the data points. - n_clusters: int, the number of clusters to form. Returns: - Tuple[np.ndarray, float]: A tuple containing the cluster labels assigned to each data point and the Adjusted Rand Index score. ``` **Input:** 1. `data`: A numpy array of shape `(n_samples, n_features)` representing the input data points. 2. `true_labels`: A numpy array of shape `(n_samples,)`, representing the ground truth labels for the data points. 3. `n_clusters`: An integer specifying the number of clusters to form. **Output:** 1. A numpy array of shape `(n_samples,)` representing the cluster labels assigned to each data point by the K-Means algorithm. 2. A float representing the Adjusted Rand Index score. **Constraints:** - The number of samples `n_samples` and the number of features `n_features` can vary but should be large enough to form sensible clusters. - The value of `n_clusters` should be a positive integer less than `n_samples`. **Example:** ```python import numpy as np # Sample data points data = np.array([[1.1, 2.2], [1.3, 2.1], [4.0, 4.5], [4.2, 4.4], [8.0, 1.2], [8.2, 1.1]]) true_labels = np.array([0, 0, 1, 1, 2, 2]) # Number of clusters to form n_clusters = 3 # Function call assigned_labels, ari_score = kmeans_clustering_and_evaluation(data, true_labels, n_clusters) print(\\"Assigned Labels:\\", assigned_labels) print(\\"Adjusted Rand Index Score:\\", ari_score) ``` **Notes:** 1. You will need to import the necessary functionality from scikit-learn, such as `KMeans` from `sklearn.cluster` and `adjusted_rand_score` from `sklearn.metrics`. 2. Ensure that the K-Means algorithm uses the `k-means++` initialization method for better convergence. 3. Test your implementation with different values of `n_clusters` and verify the Adjusted Rand Index score. **Evaluation:** Your implementation will be evaluated on: 1. **Correctness**: Whether the function correctly implements the K-Means clustering and calculates the Adjusted Rand Index accurately. 2. **Efficiency**: Efficient use of scikit-learn library functions. 3. **Readability**: Code structure, comments, and adherence to Python coding conventions.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.metrics import adjusted_rand_score from typing import Tuple def kmeans_clustering_and_evaluation(data: np.ndarray, true_labels: np.ndarray, n_clusters: int) -> Tuple[np.ndarray, float]: This function performs clustering on the input data using K-Means algorithm and evaluates the performance using Adjusted Rand Index. Parameters: - data: np.ndarray of shape (n_samples, n_features), this is the input data to be clustered. - true_labels: np.ndarray of shape (n_samples,), these are the ground truth labels of the data points. - n_clusters: int, the number of clusters to form. Returns: - Tuple[np.ndarray, float]: A tuple containing the cluster labels assigned to each data point and the Adjusted Rand Index score. kmeans = KMeans(n_clusters=n_clusters, init=\'k-means++\') kmeans.fit(data) assigned_labels = kmeans.labels_ ari_score = adjusted_rand_score(true_labels, assigned_labels) return assigned_labels, ari_score"},{"question":"**Question: Advanced Data Visualization with Seaborn** You are tasked with creating an advanced plot using the `seaborn.objects` module. The plot should provide insights into the distribution of diamond prices across different cuts, highlighting the central tendency and dispersion within each category. # Requirements: 1. Load the `diamonds` dataset. 2. Create a plot that: - Plots `price` on the y-axis and `cut` on the x-axis. - Uses logarithmic scaling on the y-axis. - Adds dot plots to show the distribution of prices for each cut, using a jitter effect to reduce overlap. - Highlights the 10th, 25th, 50th, 75th, and 90th percentiles with dark points. - Displays a shaded range mark from the 25th to the 75th percentile. # Input: None (all data should be loaded from the seaborn package). # Output: Your plot should be displayed inline in the notebook. # Constraints: - Ensure the plot is clear and interpretable. - Pay attention to the styling of the plot to make sure it effectively communicates the data distribution. # Expected Performance: - The script should run efficiently, utilizing seaborn\'s optimized functions. # Example (partial code): ```python import seaborn.objects as so from seaborn import load_dataset # Load the data diamonds = load_dataset(\\"diamonds\\") # Create the plot p = ( so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") ) # Add dot plots with jitter p = p.add(so.Dots(pointsize=1, alpha=.2), so.Jitter(.3)) # Add highlighted percentiles p = p.add(so.Dot(), so.Perc([10, 25, 50, 75, 90])) # Add shaded range p = p.add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=.2)) # Display the plot p.show() ``` Complete the plot according to the requirements and display it within the cell.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_diamond_price_plot(): Creates a plot showing the distribution of diamond prices across different cuts using the seaborn.objects module, with logarithmic scaling on the y-axis. # Load the data diamonds = load_dataset(\\"diamonds\\") # Create the plot p = ( so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") .add(so.Dots(pointsize=1, alpha=0.2), so.Jitter(.3)) # Dot plots with jitter .add(so.Dot(), so.Perc([10, 25, 50, 75, 90])) # Highlighted percentiles .add(so.Range(color=\\"k\\", alpha=0.3), so.Perc([25, 75])) # Shaded range from 25th to 75th percentile with some transparency ) # Display the plot p.show() plt.show() # This ensures that the plot renderer works correctly in all environments"},{"question":"Using `torch.cond` for Dynamic Control Flow in Neural Networks Objective Demonstrate your understanding of PyTorch\'s `torch.cond` by implementing a neural network that dynamically changes its behavior based on input tensor values. Specifically, you will implement a custom PyTorch `nn.Module` that uses `torch.cond` to apply different tensor transformations. Description You are required to implement a PyTorch module named `DynamicTensorTransformer`. This module will take an input tensor and apply one of two transformations based on a data-dependent condition using `torch.cond`. 1. If the mean value of the input tensor is greater than zero, the module should return the result of applying the `positive_fn` function to the tensor. 2. If the mean value of the input tensor is less than or equal to zero, the module should return the result of applying the `negative_fn` function to the tensor. The transformation functions you need to implement are as follows: - `positive_fn`: This function adds 2 to each element of the tensor and then takes the exponent of the result. - `negative_fn`: This function subtracts 2 from each element of the tensor and then takes the natural logarithm (log) of the result. Function Signatures You need to implement the following functions and class: - `positive_fn(x: torch.Tensor) -> torch.Tensor` - `negative_fn(x: torch.Tensor) -> torch.Tensor` - `class DynamicTensorTransformer(nn.Module)` with a method `forward(self, x: torch.Tensor) -> torch.Tensor` Constraints - You must use `torch.cond` to handle the dynamic control flow. - The input tensor will be of arbitrary shape but will always contain elements that ensure mathematical operations (add, subtract, exp, log) are valid (e.g., no element will be negative during the log operation). Example ```python import torch from torch import nn def positive_fn(x: torch.Tensor) -> torch.Tensor: return torch.exp(x + 2) def negative_fn(x: torch.Tensor) -> torch.Tensor: return torch.log(torch.clamp(x - 2, min=1e-6)) class DynamicTensorTransformer(nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.cond( x.mean() > 0, positive_fn, negative_fn, (x,) ) # Example usage: transformer = DynamicTensorTransformer() input_tensor = torch.tensor([[-1.0, 2.0], [3.0, -4.0]]) output_tensor = transformer(input_tensor) print(output_tensor) ``` In this example: - `positive_fn` will be applied if the mean of `input_tensor` is greater than 0. - `negative_fn` will be applied if the mean of `input_tensor` is less than or equal to 0. Submission Submit your implementation of the `DynamicTensorTransformer` class along with the `positive_fn` and `negative_fn` functions. Ensure your code is well-documented and includes any necessary imports.","solution":"import torch from torch import nn def positive_fn(x: torch.Tensor) -> torch.Tensor: Adds 2 to each element of the tensor and returns the exponent of the result. return torch.exp(x + 2) def negative_fn(x: torch.Tensor) -> torch.Tensor: Subtracts 2 from each element of the tensor and returns the natural logarithm of the result. return torch.log(torch.clamp(x - 2, min=1e-6)) class DynamicTensorTransformer(nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: condition = x.mean() > 0 # Torch doesn\'t have a direct torch.cond function, so we use an if-case if condition: return positive_fn(x) else: return negative_fn(x) # Example usage if __name__ == \\"__main__\\": transformer = DynamicTensorTransformer() input_tensor = torch.tensor([[-1.0, 2.0], [3.0, -4.0]]) output_tensor = transformer(input_tensor) print(output_tensor)"},{"question":"# Question: Computing Custom Jacobians for a Neural Network. Objective Your task is to implement a function that computes the Jacobian matrix of a neural network\'s output with respect to its parameters using PyTorch\'s function transforms. Problem Statement Given a neural network, you need to write a function `compute_jacobian` that returns the Jacobian matrix of the network’s output with respect to its parameters for a given input. Function Signature ```python import torch def compute_jacobian(model: torch.nn.Module, x: torch.Tensor) -> dict: Computes the Jacobian matrix of the model\'s output with respect to its parameters. Parameters: - model (torch.nn.Module): The neural network model. - x (torch.Tensor): The input tensor. Returns: - dict: A dictionary where keys are parameter names and values are the Jacobian matrices with respect to each parameter. ``` Input - `model`: A PyTorch `torch.nn.Module` representing the neural network. - `x`: A tensor of shape `(N, *)` where `N` is the batch size and `*` denotes any number of additional dimensions required by the network\'s input. Output - A dictionary where each key is the name of a parameter in the model and its corresponding value is the Jacobian matrix with respect to that parameter. Requirements 1. Utilize PyTorch’s function transforms (e.g., `jacrev`) to compute the Jacobian. 2. Use the `torch.func.functional_call` utility to pass modified parameters to the model. 3. Ensure the computations are efficient and handle potential issues such as model compatibility and input shapes. Example ```python import torch model = torch.nn.Linear(3, 2) # Simple linear model for illustration def f(params, x): return torch.func.functional_call(model, params, x) x = torch.randn(3) # Example input tensor # Example implementation of the required function jacobian_dict = compute_jacobian(model, x) for param_name, jacobian in jacobian_dict.items(): print(f\\"Jacobian for {param_name}:\\") print(jacobian) ``` Constraints - Assume the model\'s forward pass can process the input `x` normally without additional preprocessing. - Students should handle potential mismatches in parameter sizes and ensure the function generalizes to different types of neural network architectures.","solution":"import torch def compute_jacobian(model: torch.nn.Module, x: torch.Tensor) -> dict: Computes the Jacobian matrix of the model\'s output with respect to its parameters. Parameters: - model (torch.nn.Module): The neural network model. - x (torch.Tensor): The input tensor. Returns: - dict: A dictionary where keys are parameter names and values are the Jacobian matrices with respect to each parameter. jacobian_dict = {} parameters = dict(model.named_parameters()) func = lambda params: model(x) for name, param in parameters.items(): def f(p): params = {**parameters, name: p} state = model.state_dict() for key, value in params.items(): state[key] = value model.load_state_dict(state) return model(x) param_jac = torch.autograd.functional.jacobian(f, param) jacobian_dict[name] = param_jac return jacobian_dict"},{"question":"Implement a Multi-Head Self-Attention layer in PyTorch from scratch. The implementation should follow the steps involved in computing self-attention, including linear projections, scaled dot-product attention, and concatenation of the outputs of multiple attention heads. Requirements 1. Implement a `MultiHeadSelfAttention` class that inherits from `torch.nn.Module`. 2. The class should take the following parameters during initialization: * `embed_dim` (int): The dimensionality of the input embeddings. * `num_heads` (int): The number of attention heads. `embed_dim` must be divisible by `num_heads`. 3. The class should implement a `forward` method that takes the following as inputs: * `x` (torch.Tensor): A tensor of shape `(batch_size, seq_length, embed_dim)` representing the input sequence. 4. The `forward` method should output a tensor of shape `(batch_size, seq_length, embed_dim)`. Constraints * You must not use high-level PyTorch attention modules; implementation should be from scratch using basic tensor operations. * Ensure that the operations are batched to efficiently handle inputs of varying batch sizes. Methodology 1. Linear Projections: Apply linear transformations to input tensor `x` to project it into query, key, and value matrices for each head. 2. Scaled Dot-Product Attention: Compute attention scores, apply a softmax function, and use the scores to produce a weighted sum of the value vectors. 3. Concatenate Heads: Concatenate the outputs from each head and project back to the original `embed_dim`. Example Solution ```python import torch import torch.nn as nn class MultiHeadSelfAttention(nn.Module): def __init__(self, embed_dim, num_heads): super(MultiHeadSelfAttention, self).__init__() assert embed_dim % num_heads == 0, \\"embed_dim must be divisible by num_heads\\" self.embed_dim = embed_dim self.num_heads = num_heads self.head_dim = embed_dim // num_heads self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3) self.o_proj = nn.Linear(embed_dim, embed_dim) def forward(self, x): batch_size, seq_length, embed_dim = x.size() qkv = self.qkv_proj(x) # (batch_size, seq_length, 3 * embed_dim) qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim) qkv = qkv.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_length, 3 * head_dim) q, k, v = qkv.chunk(3, dim=-1) # (batch_size, num_heads, seq_length, head_dim) # Scaled dot-product attention attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) attn_probs = torch.softmax(attn_scores, dim=-1) attn_output = torch.matmul(attn_probs, v) # Concatenate heads attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch_size, seq_length, embed_dim) # Output projection output = self.o_proj(attn_output) return output ``` Implement and test the `MultiHeadSelfAttention` class with appropriate unit tests to ensure its functionality.","solution":"import torch import torch.nn as nn class MultiHeadSelfAttention(nn.Module): def __init__(self, embed_dim, num_heads): super(MultiHeadSelfAttention, self).__init__() assert embed_dim % num_heads == 0, \\"embed_dim must be divisible by num_heads\\" self.embed_dim = embed_dim self.num_heads = num_heads self.head_dim = embed_dim // num_heads self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3) self.o_proj = nn.Linear(embed_dim, embed_dim) def forward(self, x): batch_size, seq_length, embed_dim = x.size() # Linear projections to q, k, v qkv = self.qkv_proj(x) # (batch_size, seq_length, 3 * embed_dim) qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim) qkv = qkv.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_length, 3 * head_dim) # Split the projections into q, k, v q, k, v = qkv.chunk(3, dim=-1) # (batch_size, num_heads, seq_length, head_dim) # Scaled dot-product attention attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) attn_probs = torch.softmax(attn_scores, dim=-1) attn_output = torch.matmul(attn_probs, v) # Concatenate heads attn_output = attn_output.permute(0, 2, 1, 3).contiguous().reshape(batch_size, seq_length, embed_dim) # Output projection output = self.o_proj(attn_output) return output"},{"question":"Hyper-Parameter Tuning **Objective:** Evaluate the student\'s ability to perform hyper-parameter tuning using scikit-learn\'s `GridSearchCV` and `RandomizedSearchCV` classes. This assessment requires implementing a function that takes a dataset and an estimator, and returns the best hyper-parameters and the best estimator found through both exhaustive grid search and randomized search. **Task:** Implement a function `find_best_parameters` that performs hyper-parameter tuning on the given dataset using `GridSearchCV` and `RandomizedSearchCV`. The function should return the best hyper-parameters and the best estimator for both methods. **Function Signature:** ```python def find_best_parameters(X_train, y_train, estimator, param_grid, random_grid, scoring=\'accuracy\', cv=5, n_iter=10): Find the best hyper-parameters and estimator using GridSearchCV and RandomizedSearchCV. Parameters: - X_train (pd.DataFrame or np.ndarray): The input features for training. - y_train (pd.Series or np.ndarray): The target variable for training. - estimator (sklearn estimator): The machine learning model (e.g., sklearn.svm.SVC()). - param_grid (dict): The parameter grid for exhaustive grid search. - random_grid (dict): The parameter grid for randomized search. - scoring (str): The scoring metric for evaluation (default is \'accuracy\'). - cv (int): Number of cross-validation folds (default is 5). - n_iter (int): Number of iterations for randomized search (default is 10). Returns: - dict: A dictionary with the following keys: - \'grid_search\': A dictionary containing the best parameters and estimator from GridSearchCV. - \'random_search\': A dictionary containing the best parameters and estimator from RandomizedSearchCV. pass ``` **Instructions:** 1. Import the necessary libraries. 2. Create and fit a `GridSearchCV` instance with the provided parameter grid, estimator, and other specified parameters. 3. Create and fit a `RandomizedSearchCV` instance with the provided random grid, estimator, and other specified parameters. 4. Extract the best hyper-parameters and the best estimator for both grid search and randomized search. 5. Return a dictionary containing the results from both search methods. **Constraints:** - The implementation must use scikit-learn\'s `GridSearchCV` and `RandomizedSearchCV`. - You must handle any exceptions that might occur during the fitting process gracefully. - Use a fixed random state for reproducibility. **Example:** ```python from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import train_test_split # Load dataset X, y = datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the estimator and parameter grids estimator = SVC() param_grid = {\'C\': [1, 10, 100], \'kernel\': [\'linear\', \'rbf\']} random_grid = {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None]} # Find the best parameters results = find_best_parameters(X_train, y_train, estimator, param_grid, random_grid) print(results) ``` **Expected Output Format:** ```python { \\"grid_search\\": { \\"best_params\\": {\'C\': 10, \'kernel\': \'linear\'}, \\"best_estimator\\": SVC(C=10, kernel=\'linear\') }, \\"random_search\\": { \\"best_params\\": {\'C\': 100, \'gamma\': 0.0001, \'kernel\': \'rbf\', \'class_weight\': None}, \\"best_estimator\\": SVC(C=100, gamma=0.0001, kernel=\'rbf\', class_weight=None) } } ``` Note: The exact output will vary based on the dataset and the random state.","solution":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV def find_best_parameters(X_train, y_train, estimator, param_grid, random_grid, scoring=\'accuracy\', cv=5, n_iter=10): Find the best hyper-parameters and estimator using GridSearchCV and RandomizedSearchCV. Parameters: - X_train (pd.DataFrame or np.ndarray): The input features for training. - y_train (pd.Series or np.ndarray): The target variable for training. - estimator (sklearn estimator): The machine learning model (e.g., sklearn.svm.SVC()). - param_grid (dict): The parameter grid for exhaustive grid search. - random_grid (dict): The parameter grid for randomized search. - scoring (str): The scoring metric for evaluation (default is \'accuracy\'). - cv (int): Number of cross-validation folds (default is 5). - n_iter (int): Number of iterations for randomized search (default is 10). Returns: - dict: A dictionary with the following keys: - \'grid_search\': A dictionary containing the best parameters and estimator from GridSearchCV. - \'random_search\': A dictionary containing the best parameters and estimator from RandomizedSearchCV. results = {} # Perform GridSearchCV grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring=scoring, cv=cv) grid_search.fit(X_train, y_train) results[\'grid_search\'] = { \'best_params\': grid_search.best_params_, \'best_estimator\': grid_search.best_estimator_ } # Perform RandomizedSearchCV random_search = RandomizedSearchCV(estimator=estimator, param_distributions=random_grid, n_iter=n_iter, scoring=scoring, cv=cv, random_state=42) random_search.fit(X_train, y_train) results[\'random_search\'] = { \'best_params\': random_search.best_params_, \'best_estimator\': random_search.best_estimator_ } return results"},{"question":"# Question: Implementing MIME Type Matching in Mailcap You are tasked to implement a simplified version of Python\'s `mailcap` module. Specifically, you need to define two functions to handle MIME type entries and match them to appropriate commands. Step 1: Implement the `getcaps` function ```python def getcaps(): Reads and parses mailcap files on the system to return a dictionary mapping MIME types to lists of mailcap file entries. The mailcap files to be read are: - \'HOME/.mailcap\' - \'/etc/mailcap\' - \'/usr/etc/mailcap\' - \'/usr/local/etc/mailcap\' The user-specific file will override the system files. Returns: dict: A dictionary where each key is a MIME type and the value is a list of mailcap file entries. pass ``` Step 2: Implement the `findmatch` function ```python def findmatch(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): Returns the command line to be executed and the mailcap entry for the given MIME type. Args: caps (dict): Dictionary of MIME types to mailcap entries (output of getcaps). MIMEtype (str): The MIME type to search for. key (str, optional): The type of activity (e.g., \'view\', \'edit\'). Defaults to \'view\'. filename (str, optional): The filename to substitute for \'%s\' in the command line. Defaults to \'/dev/null\'. plist (list, optional): List of named parameters (e.g., [\'id=1\', \'number=2\']). Returns: tuple: A 2-tuple where the first element is the command line to be executed and the second element is the mailcap entry. If no match is found, returns (None, None). pass ``` # Example Usage ```python # Example mailcap entries mailcap_data = { \'video/mpeg\': [{\'view\': \'xmpeg %s\', \'test\': \'test \\"(uname)\\" = \\"Linux\\"\'}], \'text/plain\': [{\'view\': \'cat %s\'}] } command, entry = findmatch(mailcap_data, \'video/mpeg\', filename=\'video123.mpg\') print(command) # Output: \'xmpeg video123.mpg\' command, entry = findmatch(mailcap_data, \'text/plain\', filename=\'textfile.txt\') print(command) # Output: \'cat textfile.txt\' ``` Constraints - You need to handle the priority of mailcap entries based on the order specified above. - Parameters provided in `plist` should replace `%{param}` placeholders in mailcap commands. - Ensure that the functionalities respect the security constraints regarding shell metacharacters as described in the documentation. - Assume the `/dev/null` is to be replaced with the actual filename in practical use and handle the defaults wisely. # Evaluation Your functions will be tested with various inputs to ensure they correctly parse and prioritize mailcap entries and substitute parameters accurately in command lines. Handle edge cases gracefully to prevent security vulnerabilities. Good luck!","solution":"import os MAILCAP_FILES = [ os.path.expanduser(\'~/.mailcap\'), \'/etc/mailcap\', \'/usr/etc/mailcap\', \'/usr/local/etc/mailcap\' ] def read_mailcap_file(filepath): caps = {} try: with open(filepath, \'r\') as file: for line in file: line = line.strip() if not line or line.startswith(\'#\'): continue parts = line.split(\';\') if len(parts) < 2: continue mime_type = parts[0].strip() entry = {\'view\': parts[1].strip()} for part in parts[2:]: key_val = part.strip().split(\'=\', 1) if len(key_val) == 2: entry[key_val[0].strip()] = key_val[1].strip() if mime_type not in caps: caps[mime_type] = [] caps[mime_type].append(entry) except IOError: pass return caps def getcaps(): caps = {} for file in MAILCAP_FILES: file_caps = read_mailcap_file(file) for mime_type, entries in file_caps.items(): if mime_type not in caps: caps[mime_type] = [] caps[mime_type].extend(entries) return caps def findmatch(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): entries = caps.get(MIMEtype, []) for entry in entries: command = entry.get(key) if command: for param in plist: name, value = param.split(\'=\', 1) placeholder = f\'%{{{name}}}\' command = command.replace(placeholder, value) command = command.replace(\'%s\', filename) return command, entry return None, None"},{"question":"# Regular Expressions in Python: Log Parsing You are given a log file from a web server called \\"server_log.txt\\". Each line in the log file follows a specific format, and contains various details about the server requests. The lines in the log file look like this: ``` 123.45.67.89 - - [10/Oct/2023:13:55:36 +0000] \\"GET /index.html HTTP/1.1\\" 200 2326 987.65.43.21 - - [10/Oct/2023:13:55:45 +0000] \\"POST /submit-form HTTP/1.1\\" 404 914 ``` Each line corresponds to an individual HTTP request and contains the following fields: 1. The IP address of the client making the request. 2. The user identifier (\\"-\\" indicates missing information). 3. The user ID in the system (\\"-\\" indicates missing information). 4. The date and time of the request. 5. The request line from the client, enclosed in quotes. 6. The HTTP status code returned to the client. 7. The size of the object returned to the client (in bytes). Task: Write a Python function called `parse_log_file` that takes the path to the log file as input and returns a list of dictionaries. Each dictionary represents one log entry and contains the following keys: - `ip_address` - `datetime` - `request` - `status_code` - `size` Each value should be correctly parsed and stored as a string, except for `status_code` and `size`, which should be stored as integers. **Input:** - `file_path` (str): The path to the log file. **Output:** - `List[Dict[str, Union[str, int]]]`: A list of dictionaries containing parsed log entries. **Constraints:** - You can assume that the log file will be small enough to fit into memory. - You should handle cases where `status_code` and `size` might be missing or malformed, and default to `0` in such cases. **Performance Requirements:** - Your solution should efficiently parse the log file using regular expressions and should not use excessive memory. Example: Assuming the content of \\"server_log.txt\\" is: ``` 123.45.67.89 - - [10/Oct/2023:13:55:36 +0000] \\"GET /index.html HTTP/1.1\\" 200 2326 987.65.43.21 - - [10/Oct/2023:13:55:45 +0000] \\"POST /submit-form HTTP/1.1\\" 404 914 ``` The function call: ```python entries = parse_log_file(\\"server_log.txt\\") ``` Would return: ```python [ { \\"ip_address\\": \\"123.45.67.89\\", \\"datetime\\": \\"10/Oct/2023:13:55:36 +0000\\", \\"request\\": \\"GET /index.html HTTP/1.1\\", \\"status_code\\": 200, \\"size\\": 2326 }, { \\"ip_address\\": \\"987.65.43.21\\", \\"datetime\\": \\"10/Oct/2023:13:55:45 +0000\\", \\"request\\": \\"POST /submit-form HTTP/1.1\\", \\"status_code\\": 404, \\"size\\": 914 } ] ``` **Note:** - Make sure to handle edge cases and invalid lines gracefully.","solution":"import re from typing import List, Dict, Union def parse_log_file(file_path: str) -> List[Dict[str, Union[str, int]]]: log_entries = [] log_pattern = re.compile(r\'(?P<ip_address>d+.d+.d+.d+) - - [(?P<datetime>.*?)] \\"(?P<request>.*?)\\" (?P<status_code>d+) (?P<size>d+|-)\') with open(file_path, \'r\') as file: for line in file: match = log_pattern.match(line) if match: log_entry = match.groupdict() log_entry[\'status_code\'] = int(log_entry[\'status_code\']) log_entry[\'size\'] = int(log_entry[\'size\']) if log_entry[\'size\'] != \'-\' else 0 log_entries.append(log_entry) return log_entries"},{"question":"Handling Resources and Debugging with Python Development Mode Objective: Evaluate the students\' ability to handle resources properly in Python and understand the debugging capabilities provided by Python Development Mode. Background: The Python Development Mode enables a variety of runtime checks including resource management, memory allocation debugging, and various warning filters. When enabled, it can help identify issues in the code that may not be apparent in the normal runtime mode. Task: You need to write a Python script that reads from a given text file and processes its content according to the following requirements: 1. Define a function `count_lines(file_path: str) -> int` that opens a file and returns the number of lines in it. 2. Define a function `read_first_line(file_path: str) -> str` that opens a file and returns the first line of the file. 3. Ensure that your script works correctly under the Python Development Mode without emitting any warnings, especially `ResourceWarning`. Requirement Details: - **Input Format:** The path to the text file will be provided as a string. - **Output Format:** The functions should return the respective outputs as specified. - **Constraints:** - Use context managers to ensure proper resource handling. - Avoid manual manipulation of file descriptors. Example: Suppose we have a file \\"example.txt\\" with the following content: ``` Hello, World! This is a sample file. It contains multiple lines. ``` For `count_lines(\\"example.txt\\")`: - **Expected Output:** 3 For `read_first_line(\\"example.txt\\")`: - **Expected Output:** \\"Hello, World!\\" Additional Information: Run your script in Python Development Mode to verify that it does not emit any warnings: ```sh python3 -X dev your_script.py ``` Evaluation Criteria: - Correctness of the implementation. - Proper use of context managers. - The script should not emit any warnings when run in Python Development Mode.","solution":"def count_lines(file_path: str) -> int: Counts the number of lines in the specified text file. :param file_path: Path to the text file. :return: Number of lines in the file. with open(file_path, \'r\') as file: return sum(1 for _ in file) def read_first_line(file_path: str) -> str: Reads and returns the first line of the specified text file. :param file_path: Path to the text file. :return: First line of the file. with open(file_path, \'r\') as file: return file.readline().strip()"},{"question":"# PyTorch Coding Assessment Objective: This task is designed to assess your understanding of the tensor storage mechanism in PyTorch, how tensors can share storage, and how to manipulate tensor data at a low level using `torch.UntypedStorage`. Problem Statement: You are required to perform the following operations: 1. Create a 1-dimensional tensor `a` of size 5 filled with ones. 2. Directly access the storage of tensor `a` and store it in the variable `storage_a`. 3. Create a new tensor `b` that views the same storage as tensor `a` but has a different shape (reshape it to a 2x2 tensor). Make sure the last element is excluded. 4. Modify the storage directly such that all values in `storage_a` are replaced with the value `10`. 5. Use assertions to verify that the changes in `storage_a` are reflected in both tensors `a` and `b`. Input: - None Output: - The final values of tensor `a` and tensor `b`. Constraints: - Do not directly modify tensors `a` and `b` using standard tensor operations like `torch.fill_`. Instead, use the storage manipulation methods as demonstrated in the provided example. - Make sure to handle any errors or edge cases gracefully. Code Template: ```python import torch def tensor_storage_manipulation(): # Step 1: Create tensor a a = torch.ones(5) # Step 2: Access the storage of tensor a storage_a = a.untyped_storage() # Step 3: Create tensor b as a view of the storage but with a different shape b = torch.tensor(storage_a.tolist()[:-1]).reshape(2, 2) # Step 4: Modify the storage directly to replace all values with 10 storage_a.fill_(10) # Verify that changes are reflected in both tensors assert (a == 10).all(), \\"Tensor a does not reflect the storage changes\\" assert (b == 10).all(), \\"Tensor b does not reflect the storage changes\\" return a, b # Example Usage a, b = tensor_storage_manipulation() print(\\"Tensor a:\\", a) print(\\"Tensor b:\\", b) ``` Expected Output: ``` Tensor a: tensor([10., 10., 10., 10., 10.]) Tensor b: tensor([[10., 10.], [10., 10.]]) ``` Ensure your solution adheres to the constraints and accurately performs the required operations. Good luck!","solution":"import torch def tensor_storage_manipulation(): # Step 1: Create tensor a a = torch.ones(5) # Step 2: Access the storage of tensor a storage_a = a.storage() # Step 3: Create tensor b as a view of the storage but with a different shape (2x2) excluding the last element b = torch.tensor([]).set_(storage_a, 0, (2, 2)) # Step 4: Modify the storage directly to replace all values with 10 storage_a.fill_(10) # Verify that changes are reflected in both tensors assert torch.equal(a, torch.full((5,), 10)), \\"Tensor a does not reflect the storage changes\\" assert torch.equal(b, torch.full((2, 2), 10)), \\"Tensor b does not reflect the storage changes\\" return a, b # Example Usage a, b = tensor_storage_manipulation() print(\\"Tensor a:\\", a) print(\\"Tensor b:\\", b)"},{"question":"# Transition from `imp` Module to `importlib` Python\'s `imp` module is deprecated and it is recommended to use `importlib` instead for module imports and related tasks. Your task is to rewrite a function that uses the `imp` module to use `importlib` instead. The provided function uses the `imp` module to find and load a module. Below is the function implemented using the `imp` module: ```python import imp import sys def find_and_load_module(module_name): try: fp, pathname, description = imp.find_module(module_name) except ImportError: return None try: module = imp.load_module(module_name, fp, pathname, description) return module finally: if fp: fp.close() ``` Your task is to rewrite the `find_and_load_module` function using the `importlib` module. # Function Specification - **Input**: - `module_name` (str): The name of the module to be imported. - **Output**: - Returns the loaded module object if successful; otherwise returns `None`. - **Constraints**: - The solution should not use the deprecated `imp` module. - Use appropriate `importlib` functions to achieve the functionality. # Example ```python # Example Usage: module_os = find_and_load_module(\'os\') print(module_os) # <module \'os\' from \'/usr/lib/python3.8/os.py\'> print(module_os.path.join(\'a\', \'b\')) # \'a/b\' module_nonexistent = find_and_load_module(\'nonexistentmodule\') print(module_nonexistent) # None ``` Your implementation should handle the following: 1. Finding and importing a module using `importlib`. 2. Returning the module object if the module is found. 3. Handling cases where the module does not exist by returning `None`. Implement the `find_and_load_module` function using the `importlib` module.","solution":"import importlib def find_and_load_module(module_name): Finds and loads a module using importlib. Parameters: module_name (str): The name of the module to import. Returns: Module object if found and successfully imported, otherwise None. try: module = importlib.import_module(module_name) return module except ModuleNotFoundError: return None"},{"question":"# XML Document Manipulation with `xml.dom` **Objective:** Implement a function to create and modify an XML document using the `xml.dom` module. The function should create an XML document structure representing a simple book catalog, add elements to the catalog, and then perform some modifications. This exercise will test your understanding of creating and manipulating XML documents using the DOM API in Python. **Tasks:** 1. **Create and Return XML Document**: Implement a function `create_catalog()` that: - Creates a new `Document` object. - Adds a root element named `catalog`. - Adds a child `book` element to the `catalog` element with the following details: - An `id` attribute with the value `\\"1\\"`. - A `title` element with text content of `\\"Python Programming\\"`. - An `author` element with text content of `\\"John Doe\\"`. - A `year` element with text content of `\\"2023\\"`. - Returns the XML `Document` object. 2. **Modify XML Document**: Implement a function `modify_catalog(doc)` that: - Takes the previously created XML `Document` object as input. - Adds another `book` element to the `catalog`, with the following details: - An `id` attribute with the value `\\"2\\"`. - A `title` element with text content of `\\"Advanced Python\\"`. - An `author` element with text content of `\\"Jane Smith\\"`. - A `year` element with text content of `\\"2024\\"`. - Modifies the text content of the `title` element of the book with `id=\\"1\\"` to `\\"Introduction to Python\\"`. 3. **Output XML Document**: Implement a function `to_string(doc)` that: - Takes the XML `Document` object as input and returns a string representation of the XML document. **Expected Function Signatures:** ```python def create_catalog() -> xml.dom.minidom.Document: # Your code here def modify_catalog(doc: xml.dom.minidom.Document) -> None: # Your code here def to_string(doc: xml.dom.minidom.Document) -> str: # Your code here ``` **Testing Your Functions:** You can test your functions by combining them and printing the final XML document. Here is an example of how the code might be used: ```python catalog = create_catalog() modify_catalog(catalog) xml_string = to_string(catalog) print(xml_string) ``` **Constraints:** - Use only the `xml.dom` module (and its submodules `minidom`, etc.) for XML document manipulation. - Ensure proper handling of nodes and attributes and follow the DOM API methods as described. - Each element should be correctly nested, and the text content should be properly set. **Example Output:** ```xml <catalog> <book id=\\"1\\"> <title>Introduction to Python</title> <author>John Doe</author> <year>2023</year> </book> <book id=\\"2\\"> <title>Advanced Python</title> <author>Jane Smith</author> <year>2024</year> </book> </catalog> ``` **Note:** - Properly handle the creation, modification, and output of the XML structure according to the DOM API methods. - Ensure that the XML output is correctly formatted and matches the expected structure.","solution":"from xml.dom.minidom import Document def create_catalog() -> Document: # Create a new Document doc = Document() # Add root element named \'catalog\' catalog = doc.createElement(\'catalog\') doc.appendChild(catalog) # Add child \'book\' element to \'catalog\' with details book = doc.createElement(\'book\') book.setAttribute(\'id\', \'1\') catalog.appendChild(book) # Add \'title\' element title = doc.createElement(\'title\') title.appendChild(doc.createTextNode(\'Python Programming\')) book.appendChild(title) # Add \'author\' element author = doc.createElement(\'author\') author.appendChild(doc.createTextNode(\'John Doe\')) book.appendChild(author) # Add \'year\' element year = doc.createElement(\'year\') year.appendChild(doc.createTextNode(\'2023\')) book.appendChild(year) return doc def modify_catalog(doc: Document) -> None: # Find the root element \'catalog\' catalog = doc.getElementsByTagName(\'catalog\')[0] # Add another \'book\' element to \'catalog\' with details book = doc.createElement(\'book\') book.setAttribute(\'id\', \'2\') catalog.appendChild(book) # Add \'title\' element title = doc.createElement(\'title\') title.appendChild(doc.createTextNode(\'Advanced Python\')) book.appendChild(title) # Add \'author\' element author = doc.createElement(\'author\') author.appendChild(doc.createTextNode(\'Jane Smith\')) book.appendChild(author) # Add \'year\' element year = doc.createElement(\'year\') year.appendChild(doc.createTextNode(\'2024\')) book.appendChild(year) # Modify the text content of the \'title\' element of the book with id \\"1\\" books = catalog.getElementsByTagName(\'book\') for book in books: if book.getAttribute(\'id\') == \'1\': title = book.getElementsByTagName(\'title\')[0] title.firstChild.data = \'Introduction to Python\' break def to_string(doc: Document) -> str: # Return a string representation of the XML document return doc.toprettyxml(indent=\\" \\")"},{"question":"Objective Your task is to create custom kernel functions and evaluate them using Support Vector Machines (SVMs) on a synthetic dataset. This will assess your understanding of similarity measures and their application in machine learning using `scikit-learn`. Problem Statement 1. **Data Generation**: - Create a synthetic dataset `X` with 2D data points and corresponding labels `y`. Use `make_blobs` from `sklearn.datasets` with `n_samples=200`, `centers=3`, and `random_state=42`. 2. **Custom Kernel Functions**: - Implement the following custom kernel functions: 1. **Exponential Kernel**: ( k(x, y) = exp(-gamma |x - y|_2) ) 2. **Laplacian Kernel**: ( k(x, y) = exp(-gamma |x - y|_1) ) 3. **Sigmoid Kernel**: ( k(x, y) = tanh(gamma x^top y + c_0) ) 3. **Kernel Evaluation**: - Train an SVM classifier using `SVC` from `sklearn.svm` with the custom kernel functions defined above. - Report the accuracy of the SVM classifiers on the synthetic dataset. 4. **Performance Requirements**: - Ensure that your implementation of the custom kernels is efficient, as the dataset size may increase during evaluation. Constraints - You may use only `numpy` and `scikit-learn` for this task. - The gamma value for the kernels should be set to `0.1` and `c_0` for the Sigmoid kernel to `1.0`. Input Format - No input is required as the data generation step is explicitly described. Output Format - Print the accuracy of the SVM classifiers for each custom kernel. Example ```python from sklearn.datasets import make_blobs from sklearn.svm import SVC import numpy as np # Generate synthetic data X, y = make_blobs(n_samples=200, centers=3, random_state=42) # Define custom kernel functions def exponential_kernel(X, Y=None, gamma=0.1): if Y is None: Y = X pairwise_sq_dists = pairwise_distances(X, Y, metric=\'sqeuclidean\') return np.exp(-gamma * pairwise_sq_dists) def laplacian_kernel(X, Y=None, gamma=0.1): if Y is None: Y = X pairwise_dists = pairwise_distances(X, Y, metric=\'manhattan\') return np.exp(-gamma * pairwise_dists) def sigmoid_kernel(X, Y=None, gamma=0.1, c0=1.0): if Y is None: Y = X pairwise_dot_product = np.dot(X, Y.T) return np.tanh(gamma * pairwise_dot_product + c0) # Train SVM with custom kernels and report accuracy for kernel_func in [exponential_kernel, laplacian_kernel, sigmoid_kernel]: svm = SVC(kernel=kernel_func).fit(X, y) accuracy = svm.score(X, y) print(f\\"Accuracy with {kernel_func.__name__}: {accuracy:.2f}\\") ```","solution":"from sklearn.datasets import make_blobs from sklearn.svm import SVC from sklearn.metrics import pairwise_distances import numpy as np # Generate synthetic data X, y = make_blobs(n_samples=200, centers=3, random_state=42) # Define custom kernel functions def exponential_kernel(X, Y=None, gamma=0.1): if Y is None: Y = X pairwise_sq_dists = pairwise_distances(X, Y, metric=\'sqeuclidean\') return np.exp(-gamma * pairwise_sq_dists) def laplacian_kernel(X, Y=None, gamma=0.1): if Y is None: Y = X pairwise_dists = pairwise_distances(X, Y, metric=\'manhattan\') return np.exp(-gamma * pairwise_dists) def sigmoid_kernel(X, Y=None, gamma=0.1, c0=1.0): if Y is None: Y = X pairwise_dot_product = np.dot(X, Y.T) return np.tanh(gamma * pairwise_dot_product + c0) # Train SVM with custom kernels and report accuracy def evaluate_kernel(kernel_func): svm = SVC(kernel=kernel_func).fit(X, y) accuracy = svm.score(X, y) return accuracy accuracy_exponential_kernel = evaluate_kernel(exponential_kernel) print(f\\"Accuracy with exponential_kernel: {accuracy_exponential_kernel:.2f}\\") accuracy_laplacian_kernel = evaluate_kernel(laplacian_kernel) print(f\\"Accuracy with laplacian_kernel: {accuracy_laplacian_kernel:.2f}\\") accuracy_sigmoid_kernel = evaluate_kernel(sigmoid_kernel) print(f\\"Accuracy with sigmoid_kernel: {accuracy_sigmoid_kernel:.2f}\\")"},{"question":"**Email Sending with Exception Handling using `smtplib`** **Problem Statement:** You are required to implement a function `send_email_via_smtp(host, port, username, password, from_addr, to_addrs, subject, body)` that sends an email using the `smtplib` module with proper exception handling. **Function Signature:** ```python def send_email_via_smtp(host: str, port: int, username: str, password: str, from_addr: str, to_addrs: list, subject: str, body: str) -> str: pass ``` **Parameters:** - `host` (str): The SMTP server hostname. - `port` (int): The SMTP server port. - `username` (str): The username for SMTP server authentication. - `password` (str): The password for SMTP server authentication. - `from_addr` (str): The sender\'s email address. - `to_addrs` (list): A list of recipient email addresses. - `subject` (str): The subject of the email. - `body` (str): The body content of the email. **Returns:** - `str`: Should return `\\"Email sent successfully\\"` upon successful sending of the email, otherwise, should return the appropriate error message from the caught exception. **Requirements:** 1. Establish a connection to the SMTP server using the provided `host` and `port`. 2. Log in to the SMTP server using the provided `username` and `password`. 3. Construct an email with the provided `from_addr`, `to_addrs`, `subject`, and `body`. 4. Send the email to the specified recipients. 5. Handle the following exceptions appropriately and return the error message as a string: - `smtplib.SMTPConnectError`: Error during the connection attempt. - `smtplib.SMTPAuthenticationError`: Authentication failed. - `smtplib.SMTPRecipientsRefused`: All recipient addresses were refused. - `smtplib.SMTPDataError`: The SMTP server refused to accept the message data. - `smtplib.SMTPException`: Catchall for any other `smtplib`-related errors. **Example:** ```python host = \\"smtp.example.com\\" port = 587 username = \\"youremail@example.com\\" password = \\"yourpassword\\" from_addr = \\"youremail@example.com\\" to_addrs = [\\"recipient1@example.com\\", \\"recipient2@example.com\\"] subject = \\"Test Email\\" body = \\"This is a test email.\\" result = send_email_via_smtp(host, port, username, password, from_addr, to_addrs, subject, body) print(result) # Should output \\"Email sent successfully\\" or the specific error message ``` **Constraints:** - You can assume that the SMTP server details, username, password, and email addresses provided as input are in valid format. - The function should implement proper resource management to ensure the connection is properly closed after sending the email or upon encountering an error. **Hint:** You might find the `smtplib.SMTP` and `smtplib.SMTP_SSL` classes useful, along with their respective methods such as `connect()`, `login()`, `sendmail()`, and `quit()`.","solution":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart def send_email_via_smtp(host, port, username, password, from_addr, to_addrs, subject, body): try: # Set up the server server = smtplib.SMTP(host, port) server.starttls() # Log in to the server server.login(username, password) # Create the email content msg = MIMEMultipart() msg[\'From\'] = from_addr msg[\'To\'] = \', \'.join(to_addrs) msg[\'Subject\'] = subject msg.attach(MIMEText(body, \'plain\')) # Send the email server.sendmail(from_addr, to_addrs, msg.as_string()) server.quit() return \\"Email sent successfully\\" except smtplib.SMTPConnectError as e: return str(e) except smtplib.SMTPAuthenticationError as e: return str(e) except smtplib.SMTPRecipientsRefused as e: return str(e) except smtplib.SMTPDataError as e: return str(e) except smtplib.SMTPException as e: return str(e) finally: try: server.quit() except: pass"},{"question":"# Custom Fixer Implementation for 2to3 Background: The `2to3` tool applies a series of predefined and customizable transformations (fixers) to convert Python 2.x code to Python 3.x compliant code. The `lib2to3` library supports creating custom fixers to handle specific transformations that may not be covered by the default fixers. Objective: Your task is to implement a custom fixer that identifies and transforms Python 2.x code using the `has_key()` method on dictionaries to equivalent Python 3.x code. Description: In Python 2.x, the `has_key()` method is used to check if a dictionary has a specific key. This method was removed in Python 3.x, and the equivalent check is performed using the `in` keyword. For example: - Python 2.x: `if my_dict.has_key(key):` - Python 3.x: `if key in my_dict:` Task: 1. Write a function `transform_has_key(code: str) -> str` that takes a string containing Python 2.x code and returns a string with the equivalent Python 3.x code. 2. The function should specifically perform the following transformation: - Convert any usage of `dict.has_key(key)` in the input code to `key in dict`. Input: - A string `code` containing Python 2.x code. Output: - A string containing the transformed Python 3.x code. Constraints: - The input code will not contain syntax errors. - The transformation should preserve the indentation and comments in the code. Example: ```python def transform_has_key(code: str) -> str: # Your implementation here # Example Input: code = my_dict = {\'a\': 1, \'b\': 2} if my_dict.has_key(\'a\'): print(\'Key exists\') else: print(\'Key does not exist\') # Example Output: my_dict = {\'a\': 1, \'b\': 2} if \'a\' in my_dict: print(\'Key exists\') else: print(\'Key does not exist\') ``` Note: - The example provided should be used to verify the correctness of your implementation. - Ensure that your solution correctly handles potential edge cases, such as when `has_key` methods are used in different contexts or with different dictionary names.","solution":"def transform_has_key(code: str) -> str: Transforms Python 2.x code using dict.has_key(key) to Python 3.x code using key in dict. Args: code (str): A string containing Python 2.x code. Returns: str: A string with the equivalent Python 3.x code. import re # Regular expression to find `dict.has_key(key)` patterns pattern = re.compile(r\'(w+).has_key((.*?))\') # Function to replace match with `key in dict` def replacement(match): dict_var = match.group(1) # dictionary variable name key = match.group(2) # key inside has_key() return f\'{key} in {dict_var}\' # Substitute using the regex pattern transformed_code = re.sub(pattern, replacement, code) return transformed_code"},{"question":"**IMAP4 Client Implementation:** You are tasked with creating an IMAP client script that interacts with a given IMAP server using the `imaplib` module. Your client should connect to the server, authenticate the user, and perform a series of actions on the mailboxes. # Implementation Details: 1. **Connection Setup:** - Connect to the server `mail.example.com` using the `IMAP4_SSL` class. - Use the standard port for IMAP over SSL (993). 2. **User Authentication:** - Authenticate using the `LOGIN` method. Use placeholder values for the username and password, which should be taken as input from the user. 3. **Mailbox Operations:** - Select the \\"INBOX\\" mailbox. - Fetch the message headers (i.e., the \\"SUBJECT\\" and \\"FROM\\" fields) of the 5 latest messages in the INBOX. - Copy these 5 latest messages to a new mailbox named \\"Backup\\". 4. **Utilities and Error Handling:** - Implement error handling for connection issues, authentication errors, and other potential IMAP errors. - Use utility functions where applicable (e.g., parsing date strings). 5. **Cleanup:** - Ensure the client logs out of the IMAP session cleanly. # Constraints and Limitations: - You must use the `imaplib` module for all IMAP interactions. - Handle exceptions gracefully and ensure to log out even if an error occurs. - You may assume that the user has appropriate permissions for all operations performed. # Expected Input/Output Format: - **Input:** - Username (string) - Password (string) - **Output:** - Print the headers of the 5 latest messages in the \\"INBOX\\". - Print a confirmation message once the messages have been copied to the new mailbox. - Handle errors with appropriate error messages. # Example: ``` Input: Username: johndoe Password: [hidden] Output: Connecting to the server... Authenticating the user... Selecting the INBOX mailbox... Fetching message headers... Message 1: Subject: \\"Meeting Reminder\\", From: \\"boss@example.com\\" Message 2: Subject: \\"Project Update\\", From: \\"team_lead@example.com\\" ... Copying messages to the Backup mailbox... 5 messages have been copied to the \\"Backup\\" mailbox. Logging out... ``` # Your Task: Implement the described IMAP client in a Python function named `imap_client`. Ensure the function adheres to the guidelines and handles operations correctly. ```python import imaplib import getpass def imap_client(): # Implement your solution here pass # Test the client if __name__ == \\"__main__\\": imap_client() ```","solution":"import imaplib import getpass import email import traceback def imap_client(): try: # Get user input for username and password username = input(\\"Username: \\") password = getpass.getpass(\\"Password: \\") print(\\"Connecting to the server...\\") # Connect to the IMAP server server = imaplib.IMAP4_SSL(\'mail.example.com\', 993) print(\\"Authenticating the user...\\") # Login to the server server.login(username, password) print(\\"Selecting the INBOX mailbox...\\") # Select the INBOX mailbox server.select(\'INBOX\') print(\\"Fetching message headers...\\") # Search for all emails in the INBOX status, message_numbers = server.search(None, \'ALL\') # Check if we have at least 5 messages message_numbers = message_numbers[0].split() if len(message_numbers) < 5: print(\\"Not enough messages in the INBOX to process.\\") return # Get the message numbers of the 5 latest messages latest_5_messages = message_numbers[-5:] # Fetch the headers of the latest 5 messages for i, mnum in enumerate(latest_5_messages, start=1): status, msg_data = server.fetch(mnum, \'(BODY[HEADER.FIELDS (SUBJECT FROM)])\') for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) print(f\\"Message {i}: Subject: \\"{msg[\'Subject\']}\\", From: \\"{msg[\'From\']}\\"\\") print(\\"Copying messages to the Backup mailbox...\\") # Copy the messages to the Backup mailbox for mnum in latest_5_messages: server.copy(mnum, \'Backup\') print(\\"5 messages have been copied to the \\"Backup\\" mailbox.\\") except imaplib.IMAP4.error as e: print(f\\"IMAP error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") traceback.print_exc() finally: try: print(\\"Logging out...\\") server.logout() except: pass"},{"question":"Objective: This question will assess your ability to manipulate and analyze data using pandas DataFrame functionalities, including creation, indexing, grouping, and statistical analysis. Problem Statement: You are given a sales dataset in the form of a list of dictionaries where each dictionary represents a record of sales transactions. Each dictionary contains the following keys: - `transaction_id`: A unique identifier for the transaction. - `date`: The date of the transaction (string in the format `YYYY-MM-DD`). - `customer_id`: The unique identifier for the customer. - `amount`: The amount of money spent in the transaction (float). Example of the sales dataset: ```python sales_data = [ {\\"transaction_id\\": 1, \\"date\\": \\"2023-01-15\\", \\"customer_id\\": 101, \\"amount\\": 250.0}, {\\"transaction_id\\": 2, \\"date\\": \\"2023-01-16\\", \\"customer_id\\": 102, \\"amount\\": 100.0}, {\\"transaction_id\\": 3, \\"date\\": \\"2023-02-01\\", \\"customer_id\\": 101, \\"amount\\": 300.0}, {\\"transaction_id\\": 4, \\"date\\": \\"2023-03-12\\", \\"customer_id\\": 103, \\"amount\\": 150.0}, // more records... ] ``` Task: Implement a function `analyze_sales(sales_data)` that performs the following analysis on the given sales dataset: 1. Creates a `pandas.DataFrame` from the `sales_data`. 2. Parses the `date` column to datetime format. 3. Groups the DataFrame by `customer_id` to calculate: - The total amount spent by each customer. - The average transaction amount for each customer. - The number of transactions made by each customer. 4. Returns a DataFrame with the following columns: - `customer_id` - `total_amount` - `average_transaction_amount` - `transaction_count` Input: - A list of dictionaries, where each dictionary represents a sales transaction. Output: - A pandas DataFrame with columns `customer_id`, `total_amount`, `average_transaction_amount`, and `transaction_count`. Constraints: - You can assume that the input list is non-empty and contains valid data. - The `date` format is guaranteed to be `YYYY-MM-DD`. Example: Input: ```python sales_data = [ {\\"transaction_id\\": 1, \\"date\\": \\"2023-01-15\\", \\"customer_id\\": 101, \\"amount\\": 250.0}, {\\"transaction_id\\": 2, \\"date\\": \\"2023-01-16\\", \\"customer_id\\": 102, \\"amount\\": 100.0}, {\\"transaction_id\\": 3, \\"date\\": \\"2023-02-01\\", \\"customer_id\\": 101, \\"amount\\": 300.0}, {\\"transaction_id\\": 4, \\"date\\": \\"2023-03-12\\", \\"customer_id\\": 103, \\"amount\\": 150.0}, {\\"transaction_id\\": 5, \\"date\\": \\"2023-04-15\\", \\"customer_id\\": 101, \\"amount\\": 200.0}, ] ``` Output: ```python customer_id total_amount average_transaction_amount transaction_count 0 101 750.0 250.0 3 1 102 100.0 100.0 1 2 103 150.0 150.0 1 ``` Notes: - Make use of pandas methods and functionalities to achieve the specified operations efficiently. - Ensure to handle the datetime conversion and necessary grouping aggregations appropriately.","solution":"import pandas as pd def analyze_sales(sales_data): Analyzes the sales data to calculate the total amount spent by each customer, the average transaction amount, and the number of transactions made by each customer. Args: sales_data (list of dict): List of sales transaction records. Returns: pd.DataFrame: DataFrame containing customer_id, total_amount, average_transaction_amount, and transaction_count. # Create DataFrame from sales data df = pd.DataFrame(sales_data) # Parse the date column to datetime format df[\'date\'] = pd.to_datetime(df[\'date\']) # Group by customer_id and calculate the required aggregations result = df.groupby(\'customer_id\').agg( total_amount=(\'amount\', \'sum\'), average_transaction_amount=(\'amount\', \'mean\'), transaction_count=(\'transaction_id\', \'count\') ).reset_index() return result"},{"question":"Objective Implement a set of functions that work with complex numbers, perform conversions, and calculate specific mathematical properties. Tasks 1. **Convert Complex to Polar Coordinates and Back** - `complex_to_polar(z: complex) -> tuple`: Given a complex number `z`, return a tuple `(r, phi)` where `r` is the modulus and `phi` is the phase angle in radians. - `polar_to_complex(r: float, phi: float) -> complex`: Given the modulus `r` and the phase angle `phi`, return the corresponding complex number. 2. **Calculate Combined Trigonometric Function** - `trig_combo(z: complex) -> complex`: Given a complex number `z`, calculate and return the result of the expression ( cos(z) + sin(z) ). 3. **Check Complex Number Properties** - `check_properties(z: complex) -> tuple`: Given a complex number `z`, return a tuple containing three boolean values corresponding to whether `z` is finite, `z` is infinite, and whether `z` is NaN (not a number). 4. **Edge Case Handling** - Ensure that the functions handle edge cases appropriately, such as: - Numbers with branches (e.g., square root of a negative real number). - Complex infinities and NaNs. Function Signatures: ```python import cmath def complex_to_polar(z: complex) -> tuple: Converts a complex number to its polar coordinates. Parameters: z (complex): A complex number. Returns: tuple: A tuple (r, phi) where r is the modulus and phi is the phase angle. pass def polar_to_complex(r: float, phi: float) -> complex: Converts polar coordinates to a complex number. Parameters: r (float): Modulus of the complex number. phi (float): Phase angle in radians. Returns: complex: The corresponding complex number. pass def trig_combo(z: complex) -> complex: Calculate the combination of the cosine and sine of a complex number. Parameters: z (complex): A complex number. Returns: complex: The result of cos(z) + sin(z). pass def check_properties(z: complex) -> tuple: Check properties of a complex number. Parameters: z (complex): A complex number. Returns: tuple: A tuple (is_finite, is_infinite, is_nan) where each value is a boolean. pass ``` Example Usage ```python # Example usage z = complex(1, 1) print(complex_to_polar(z)) # Expected output: (sqrt(2), pi/4) r, phi = 1.4142135623730951, 0.7853981633974483 print(polar_to_complex(r, phi)) # Expected output: (1+1j) z = complex(1, -1) print(trig_combo(z)) # Expected output: cos(1 - 1j) + sin(1 - 1j) z = complex(float(\'inf\'), 1) print(check_properties(z)) # Expected output: (False, True, False) ``` Constraints 1. Do not use any external libraries other than `cmath` and `math`. 2. Ensure that the functions handle complex infinities and NaNs according to the IEEE 754 standard. 3. Provide clear and concise documentation for each function.","solution":"import cmath def complex_to_polar(z: complex) -> tuple: Converts a complex number to its polar coordinates. Parameters: z (complex): A complex number. Returns: tuple: A tuple (r, phi) where r is the modulus and phi is the phase angle. r = abs(z) phi = cmath.phase(z) return (r, phi) def polar_to_complex(r: float, phi: float) -> complex: Converts polar coordinates to a complex number. Parameters: r (float): Modulus of the complex number. phi (float): Phase angle in radians. Returns: complex: The corresponding complex number. return cmath.rect(r, phi) def trig_combo(z: complex) -> complex: Calculate the combination of the cosine and sine of a complex number. Parameters: z (complex): A complex number. Returns: complex: The result of cos(z) + sin(z). return cmath.cos(z) + cmath.sin(z) def check_properties(z: complex) -> tuple: Check properties of a complex number. Parameters: z (complex): A complex number. Returns: tuple: A tuple (is_finite, is_infinite, is_nan) where each value is a boolean. is_finite = cmath.isfinite(z) is_infinite = not is_finite and not cmath.isnan(z) is_nan = cmath.isnan(z) return (is_finite, is_infinite, is_nan)"},{"question":"# Codec Management and Error Handling in Python You are tasked with designing a small codec management system in Python that can register codecs, encode/decode text, and handle encoding/decoding errors gracefully. Implement the following functionalities as methods within a class `CodecManager`: Functionality 1: Register Codec - **Method Name**: `register_codec` - **Inputs**: - `search_function`: Function to be registered as codec search function. - **Output**: - Return `True` if registration is successful, else raise an appropriate exception. Functionality 2: Unregister Codec - **Method Name**: `unregister_codec` - **Inputs**: - `search_function`: Function to be unregistered. - **Output**: - Return `True` if unregistration is successful, else raise an appropriate exception. Functionality 3: Encode Data - **Method Name**: `encode_data` - **Inputs**: - `data`: String data to be encoded. - `encoding`: Encoding type to use (string). - `errors`: Error handling strategy (string), default is `None`. - **Output**: - Return encoded data. Raise `LookupError` if encoding type is not found. Functionality 4: Decode Data - **Method Name**: `decode_data` - **Inputs**: - `data`: Encoded data (bytes) to be decoded. - `encoding`: Encoding type to use (string). - `errors`: Error handling strategy (string), default is `None`. - **Output**: - Return decoded string. Raise `LookupError` if encoding type is not found. Functionality 5: Register and Handle Encoding Errors - **Part A - Register Error Handler**: - **Method Name**: `register_error_handler` - **Inputs**: - `name`: Name for the error handler (string). - `handler_function`: Function to handle encoding errors. - **Output**: - Return `True` if registration is successful, else raise an appropriate exception. - **Part B - Use Error Handler**: - **Method Name**: `use_error_handler` - **Inputs**: - `name`: Name of the registered error handler (string). - `data`: Data (string) to encode using an encoding that will trigger an error. - `encoding`: Encoding type to use (string). - **Output**: - Return the result of encoding data using the specified error handler strategy. Example Usage ```python codec_manager = CodecManager() # Register a dummy codec (you can use any valid function or an in-built one) codec_manager.register_codec(my_search_function) # Encode and decode data encoded_data = codec_manager.encode_data(\\"example\\", \\"utf-8\\") decoded_data = codec_manager.decode_data(encoded_data, \\"utf-8\\") # Register and observe error handling def custom_replace_error(exc): # Replace with \'?\' return (\\"?\\", exc.start + 1) codec_manager.register_error_handler(\'custom_replace\', custom_replace_error) error_handled_data = codec_manager.use_error_handler(\'custom_replace\', \\"café\\", \\"ascii\\") print(error_handled_data) # Expected to replace non-ascii character with \'?\' ``` Constraints: - Assume `my_search_function` and other codec functions are already available. - You may use the Python standard library and assume encoding types like `utf-8`, `ascii` etc., are supported. **Note**: Provide detailed function documentation and error handling in your implementation to showcase best coding practices.","solution":"import codecs class CodecManager: def register_codec(self, search_function): Registers a codec search function. Args: search_function (function): Codec search function to be registered. Returns: bool: True if registration is successful. Raises: ValueError: If search_function is None. if search_function is None: raise ValueError(\\"search_function cannot be None\\") codecs.register(search_function) return True def unregister_codec(self, search_function): Unregisters a codec search function. Args: search_function (function): Codec search function to be unregistered. Returns: bool: True if unregistration is successful. Raises: ValueError: If search_function is None. if search_function is None: raise ValueError(\\"search_function cannot be None\\") codecs.unregister(search_function) return True def encode_data(self, data, encoding, errors=None): Encodes data using the specified encoding. Args: data (str): The string data to be encoded. encoding (str): The encoding type to use. errors (str): Error handling strategy, default is None. Returns: bytes: Encoded data. Raises: LookupError: If encoding type is not found. if errors: return data.encode(encoding, errors) return data.encode(encoding) def decode_data(self, data, encoding, errors=None): Decodes data using the specified encoding. Args: data (bytes): The encoded data to be decoded. encoding (str): The encoding type to use. errors (str): Error handling strategy, default is None. Returns: str: Decoded string. Raises: LookupError: If encoding type is not found. if errors: return data.decode(encoding, errors) return data.decode(encoding) def register_error_handler(self, name, handler_function): Registers an error handler. Args: name (str): Name for the error handler. handler_function (function): Function to handle encoding errors. Returns: bool: True if registration is successful. Raises: ValueError: If name or handler_function is None. if not name or not handler_function: raise ValueError(\\"name and handler_function cannot be None\\") codecs.register_error(name, handler_function) return True def use_error_handler(self, name, data, encoding): Uses the specified error handler for encoding data. Args: name (str): Name of the registered error handler. data (str): Data to encode. encoding (str): Encoding type to use. Returns: bytes: Encoded data with the error handler. return data.encode(encoding, errors=name)"},{"question":"# Email Message Analysis and Manipulation You are tasked with creating a utility that analyzes and manipulates email messages using Python\'s `email.parser` module. Required Function: `analyze_and_modify_email` Implement a function `analyze_and_modify_email` that takes in a string representation of an email message and performs the following tasks: 1. **Parse the Email:** - Parse the input email message string using the `Parser` API. 2. **Analyze the Email:** - Extract and print the following information about the email: - The main headers (`From`, `To`, `Subject`, and `Date`). - Whether the email is multipart. - If multipart, print the number of parts and the content type of each part. 3. **Modify and Return the Email:** - If the email contains a plain text (\\"text/plain\\") part in a multipart message, modify it by appending the text `\\"nn[This email has been processed]\\"` to the end of the text content. - Return the string representation of the modified email message. Function Signature: ```python def analyze_and_modify_email(email_string: str) -> str: ``` Example: ```python email_string = From: example@example.com To: recipient@example.com Subject: Test Email Date: Wed, 13 Oct 2021 18:22:00 -0400 Content-Type: multipart/alternative; boundary=\\"boundary\\" --boundary Content-Type: text/plain This is the plain text part of the email. --boundary Content-Type: text/html <html>This is the HTML part of the email.</html> --boundary-- result = analyze_and_modify_email(email_string) print(result) ``` Expected output analysis (the function should print): ``` From: example@example.com To: recipient@example.com Subject: Test Email Date: Wed, 13 Oct 2021 18:22:00 -0400 Is multipart: True Number of parts: 2 Part 1 content type: text/plain Part 2 content type: text/html ``` Expected result (The returned email string should be): ``` From: example@example.com To: recipient@example.com Subject: Test Email Date: Wed, 13 Oct 2021 18:22:00 -0400 Content-Type: multipart/alternative; boundary=\\"boundary\\" --boundary Content-Type: text/plain This is the plain text part of the email. [This email has been processed] --boundary Content-Type: text/html <html>This is the HTML part of the email.</html> --boundary-- ``` Constraints: - The input email string follows standard email formatting rules. - The solution should handle non-multipart emails gracefully. - You may assume the email does not contain any binary attachments. **Note:** Use the Python `email.parser.Parser` class for parsing the email string and the appropriate methods to traverse and modify its content.","solution":"from email.parser import Parser from email.policy import default def analyze_and_modify_email(email_string: str) -> str: # Parse the email message = Parser(policy=default).parsestr(email_string) # Extract and print the main headers print(f\\"From: {message[\'From\']}\\") print(f\\"To: {message[\'To\']}\\") print(f\\"Subject: {message[\'Subject\']}\\") print(f\\"Date: {message[\'Date\']}\\") # Check if the email is multipart is_multipart = message.is_multipart() print(f\\"Is multipart: {is_multipart}\\") if is_multipart: parts = message.get_payload() num_parts = len(parts) print(f\\"Number of parts: {num_parts}\\") # Iterate through the parts for i, part in enumerate(parts): content_type = part.get_content_type() print(f\\"Part {i+1} content type: {content_type}\\") # If a plain text part is found, modify it if content_type == \\"text/plain\\": new_payload = part.get_payload(decode=True).decode() + \\"nn[This email has been processed]\\" part.set_payload(new_payload, charset=part.get_content_charset()) # Return the modified email string return message.as_string()"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of seaborn\'s theme configuration and display options by creating a custom plot configuration and applying it to a seaborn plot. # Problem Statement: You are tasked with creating a custom seaborn plot configuration that applies a specific theme to all plots and customizes the display options for the rendered plots in a Jupyter Notebook. Your solution should include the following steps: 1. **Set a custom theme:** - Set the background color (`axes.facecolor`) for the plots to \\"lightgray\\". - Apply the seaborn \\"darkgrid\\" style. - Sync the plot theme with the current matplotlib global settings. 2. **Customize display settings:** - Set the default display format to SVG. - Disable HiDPI image scaling. - Set plot scaling to 0.8. 3. **Create a sample plot:** - Using seaborn, create a plot of your choice (e.g., scatter plot, line plot, etc.) with any dataset you choose. - Ensure that the theme and display settings are applied to this plot. # Input: - No specific input required. You are free to choose any dataset for creating the sample plot. # Expected Output: - Display a sample seaborn plot in a Jupyter Notebook, demonstrating the custom theme and display settings specified above. # Constraints: - Use `seaborn.objects as so` for plot configuration. - Ensure that all configurations are applied globally for seaborn plots. Note: You do not need to submit a Jupyter Notebook; instead, provide the code that can be executed within a notebook cell to achieve the desired configurations and display the sample plot. # Example: ```python # Step 1: Import necessary libraries import seaborn.objects as so import seaborn as sns import matplotlib as mpl # Step 2: Set the custom theme so.Plot.config.theme[\\"axes.facecolor\\"] = \\"lightgray\\" so.Plot.config.theme.update(sns.axes_style(\\"darkgrid\\")) so.Plot.config.theme.update(mpl.rcParams) # Step 3: Customize display settings so.Plot.config.display[\\"format\\"] = \\"svg\\" so.Plot.config.display[\\"hidpi\\"] = False so.Plot.config.display[\\"scaling\\"] = 0.8 # Step 4: Create a sample plot # Load a dataset (e.g., seaborn\'s built-in \'tips\' dataset) tips = sns.load_dataset(\\"tips\\") # Create a scatter plot plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dots()) # Display the plot plot ``` Make sure to properly test and validate your solution to ensure that the theme and display settings are correctly applied and the sample plot is displayed as intended.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt def configure_and_plot(): # Step 2: Set the custom theme custom_theme = sns.axes_style(\\"darkgrid\\") custom_theme[\'axes.facecolor\'] = \\"lightgray\\" mpl.rcParams.update(custom_theme) # Sync seaborn theme with matplotlib global settings # Step 3: Customize display settings mpl.rcParams[\'savefig.format\'] = \'svg\' # Set the default display format to SVG plt.rcParams[\'figure.dpi\'] = 100 # Disable HiDPI image scaling plt.rcParams[\'figure.figsize\'] = (6.4 * 0.8, 4.8 * 0.8) # Set plot scaling to 0.8 # Step 4: Create a sample plot tips = sns.load_dataset(\\"tips\\") # Create a scatter plot plot = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dots()) return plot # Execute the function to apply the configurations and plot configure_and_plot()"},{"question":"Context: You are tasked with developing a Python utility function that processes a list of file paths to achieve the following: 1. Normalize the paths to their absolute paths. 2. Identify the common path prefix. 3. Expand any user-specific directories in the paths (e.g., `~`). 4. Filter out paths that do not exist. Your task is to implement the function `process_file_paths(paths)`. Function Signature: ```python def process_file_paths(paths: list) -> tuple: ``` Input: - `paths`: A list of strings, where each string represents a file path. All elements in the list are valid file paths but may contain user-specific directories (e.g., `~`). The list will contain at least one path and no more than 1000 paths. Output: - Returns a tuple `(normalized_paths, common_prefix)` where: - `normalized_paths` is a list of the normalized, expanded, and existing file paths in their absolute forms. - `common_prefix` is a string representing the longest common path prefix of the paths in `normalized_paths`. If no common prefix exists, return an empty string. Constraints: - The function should handle potential errors gracefully. - Paths should be processed according to the specific operating system\'s conventions. - Performance is critical; the function should efficiently handle the upper limit of input size. Example: ```python # Suppose the following paths are all valid in your environment: paths = [ \\"~/Documents/project/file1.py\\", \\"~/Documents/project/file2.py\\", \\"/other/location/file3.py\\", \\"/other/location/file4.txt\\", \\"~/Documents/other_project/file5.py\\" ] # After expanding the user directories and normalizing: # Suppose normalized_paths becomes: # [ # \\"/home/user/Documents/project/file1.py\\", # \\"/home/user/Documents/project/file2.py\\", # \\"/other/location/file3.py\\", # \\"/other/location/file4.txt\\", # \\"/home/user/Documents/other_project/file5.py\\" # ] # And the common_prefix would be: # \\"/\\" result = process_file_paths(paths) print(result) # Expected Output example: ( # [ # \\"/home/user/Documents/project/file1.py\\", # \\"/home/user/Documents/project/file2.py\\", # \\"/other/location/file3.py\\", # \\"/other/location/file4.txt\\", # \\"/home/user/Documents/other_project/file5.py\\" # ], # \\"/\\" # ) ``` Additional Information: - You may find the `os.path` module documentation helpful for functions such as `expanduser`, `abspath`, `exists`, `commonpath`, and `commonprefix`. - Remember to check if the paths exist in the filesystem while filtering them. Good luck!","solution":"import os def process_file_paths(paths): Processes a list of file paths to: 1. Normalize the paths to their absolute paths. 2. Expand any user-specific directories in the paths. 3. Filter out paths that do not exist. 4. Identify the common path prefix. Args: paths (list): A list of strings where each string is a file path. Returns: tuple: A tuple containing: - normalized_paths (list): A list of normalized, expanded, and existing file paths in their absolute forms. - common_prefix (str): The longest common path prefix of the paths in `normalized_paths`. # 1. Expand user-specific directories and normalize the paths expanded_paths = [os.path.abspath(os.path.expanduser(path)) for path in paths] # 2. Filter out paths that do not exist existing_paths = [path for path in expanded_paths if os.path.exists(path)] # 3. Determine the common prefix common_prefix = os.path.commonpath(existing_paths) if existing_paths else \\"\\" return existing_paths, common_prefix"},{"question":"Objective Implement a custom PyTorch function that operates on complex inputs and produces real-valued outputs. Then, use `torch.autograd.gradcheck` to verify the correctness of the gradients computed for this function. Problem Statement You are required to implement a complex-valued function `complex_to_real_function` using PyTorch and verify its gradient computations using `torch.autograd.gradcheck`. 1. **Function Implementation** - Implement a function `complex_to_real_function` in PyTorch that accepts a complex-valued tensor as input and outputs a real-valued tensor. - The function should operate element-wise and can involve any mathematical operations (e.g., summation of real and imaginary parts, magnitude computation, etc.). 2. **Gradient Verification** - Write a test script to verify the gradients computed for `complex_to_real_function` using the `torch.autograd.gradcheck` method. Function Signature ```python import torch def complex_to_real_function(z: torch.Tensor) -> torch.Tensor: Computes a real value function of a complex input tensor. Parameters: z (torch.Tensor): A complex-valued tensor of shape (N,) Returns: torch.Tensor: A real-valued tensor of shape (N,) pass def verify_gradients(): Verifies the gradients of the complex_to_real_function using gradcheck. pass ``` Constraints - The input tensor `z` should be a 1D tensor of shape `(N,)` representing complex numbers. - Use `torch.autograd.gradcheck` to verify gradients. - Ensure `gradcheck` passes, implying that the analytical and numerical gradients match within acceptable tolerances. Example Usage ```python if __name__ == \'__main__\': verify_gradients() ``` In the function `verify_gradients`, ensure that: - You create a complex-valued tensor as input. - Gradcheck is performed on the `complex_to_real_function`. Notes - The `gradcheck` method requires the input to have `requires_grad=True`. - Consider using small tensors for efficient execution. - Focus on the clarity and efficiency of your implementation.","solution":"import torch def complex_to_real_function(z: torch.Tensor) -> torch.Tensor: Computes a real-valued function of a complex input tensor. The function computes the magnitude squared of the complex input, which is a real number. Parameters: z (torch.Tensor): A complex-valued tensor of shape (N,) Returns: torch.Tensor: A real-valued tensor of shape (N,) real_part = z.real imag_part = z.imag return real_part**2 + imag_part**2 def verify_gradients(): Verifies the gradients of the complex_to_real_function using gradcheck. # Create a complex-valued tensor with requires_grad=True real = torch.randn(5, dtype=torch.double, requires_grad=True) imag = torch.randn(5, dtype=torch.double, requires_grad=True) z = torch.complex(real, imag) # Perform gradcheck on the complex_to_real_function result = torch.autograd.gradcheck(complex_to_real_function, z) print(\\"Gradient check passed:\\", result)"},{"question":"# Python 310 Coding Assessment **Objective**: Implement a Python class managing a library system using advanced Python features including special methods, control flow, and Python\'s import system. **Description**: You are required to implement a Python class `Library` that allows the management of books in a library. Each book has a title, author, and an ISBN number. The library should support adding new books, searching for books, and listing all books by a specific author. # Requirements 1. **Class Definition**: Define a class `Library`. 2. **Book Representation**: Use a proper data structure to store book information (you can use a named tuple or a simple class). 3. **Methods**: - `add_book(title: str, author: str, isbn: str)`: Adds a new book to the library. - `search_by_isbn(isbn: str) -> Optional[Dict]`: Searches for a book by its ISBN and returns a dictionary with the book\'s details if found, otherwise `None`. - `books_by_author(author: str) -> List[Dict]`: Returns a list of dictionaries for all books by the specified author. 4. **Special Methods**: - Implement `__str__` method to provide a string representation of the library content. - Override `__len__` to return the number of books in the library. # Constraints - ISBN numbers are unique for each book. - The library can contain a large number of books; thus, your solution should be optimized for performance. - Properly handle corner cases like searching for books or authors that do not exist. # Sample Input and Execution ```python # Create an instance of the library library = Library() # Add books to the library library.add_book(\\"The Catcher in the Rye\\", \\"J.D. Salinger\\", \\"1234567890\\") library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"2345678901\\") library.add_book(\\"1984\\", \\"George Orwell\\", \\"3456789012\\") # Search for a book by ISBN print(library.search_by_isbn(\\"1234567890\\")) # Output: {\'title\': \'The Catcher in the Rye\', \'author\': \'J.D. Salinger\', \'isbn\': \'1234567890\'} # List all books by a specific author print(library.books_by_author(\\"George Orwell\\")) # Output: [{\'title\': \'1984\', \'author\': \'George Orwell\', \'isbn\': \'3456789012\'}] # Get the number of books in the library print(len(library)) # Output: 3 # Print the library content print(library) # Expected Output: A string showing all the books in the library. ``` # Implementation Notes - Use the built-in `collections` module, if you find it necessary, to manage collections of books. - Ensure your implementation is efficient and handles edge cases. - You may need to import additional standard Python modules as required.","solution":"from typing import List, Dict, Optional from collections import namedtuple Book = namedtuple(\'Book\', [\'title\', \'author\', \'isbn\']) class Library: def __init__(self): self.books = [] def add_book(self, title: str, author: str, isbn: str) -> None: if not self.search_by_isbn(isbn): self.books.append(Book(title, author, isbn)) def search_by_isbn(self, isbn: str) -> Optional[Dict]: for book in self.books: if book.isbn == isbn: return {\'title\': book.title, \'author\': book.author, \'isbn\': book.isbn} return None def books_by_author(self, author: str) -> List[Dict]: return [{\'title\': book.title, \'author\': book.author, \'isbn\': book.isbn} for book in self.books if book.author == author] def __str__(self) -> str: return \'n\'.join(f\\"Title: {book.title}, Author: {book.author}, ISBN: {book.isbn}\\" for book in self.books) def __len__(self) -> int: return len(self.books)"},{"question":"**Problem Statement:** You are given a directory with various types of files. Your task is to implement a function `find_matching_files(directory: str, pattern: str) -> List[str]` that returns a list of filenames in the specified directory that match the given shell-style pattern. Your function should: 1. Traverse the directory and its subdirectories to find all files. 2. Use the `fnmatch` module to filter filenames according to the pattern. 3. Return a list of matching filenames with their relative paths from the given directory. **Function Signature:** ```python import os from typing import List def find_matching_files(directory: str, pattern: str) -> List[str]: pass ``` **Input:** - `directory` (str): The path of the directory to traverse. - `pattern` (str): The shell-style pattern to match filenames. **Output:** - List of strings representing the relative paths of the files that match the pattern. **Constraints:** 1. Do not use the `glob` module. 2. The function must handle nested directories. 3. Assume that the directory exists and contains at least one file. **Example:** ```python import os # Sample directory structure # test_dir/ # ├── file1.txt # ├── file2.log # ├── sub_dir1/ # │ ├── file3.txt # │ └── file4.log # └── sub_dir2/ # ├── file5.txt # ├── file6.log # └── sub_sub_dir1/ # └── file7.txt print(find_matching_files(\'test_dir\', \'*.txt\')) # Output: [\'file1.txt\', \'sub_dir1/file3.txt\', \'sub_dir2/file5.txt\', \'sub_dir2/sub_sub_dir1/file7.txt\'] ``` Use the provided `fnmatch` module documentation to ensure efficient implementation. Note that you can use `os.walk()` to traverse the directory tree.","solution":"import os from typing import List import fnmatch def find_matching_files(directory: str, pattern: str) -> List[str]: matching_files = [] for root, _, files in os.walk(directory): for file in files: if fnmatch.fnmatch(file, pattern): relative_path = os.path.relpath(os.path.join(root, file), directory) matching_files.append(relative_path) return matching_files"},{"question":"# Python Coding Assessment Question **Background:** You are developing a scheduling application that needs to support multiple time zones and handle various complexities associated with daylight saving time transitions. Your application will allow users to schedule events, view their event times in different time zones, and ensure that scheduling respects local time rules. **Task:** Implement a function `schedule_event` that schedules an event for a user in their local time zone while converting the event time to UTC for storage purposes. Handle ambiguous times caused by daylight saving time transitions using the `fold` attribute of `datetime`. Your task is to: 1. **Create an event in the user\'s local time zone:** - Input will be the local time as a string, e.g., \\"2023-03-12 02:30:00\\". - Input time zone, e.g., \\"America/New_York\\". - Whether the time is `fold=0` or `fold=1` (boolean). 2. **Convert the event time to UTC:** - Output the UTC time as a string in the format \\"YYYY-MM-DD HH:MM:SS\\". 3. **Handle exceptions appropriately:** Raise an appropriate exception if the time zone is not found or inputs are invalid. ```python from zoneinfo import ZoneInfo, ZoneInfoNotFoundError from datetime import datetime, timezone def schedule_event(local_time_str: str, time_zone: str, fold: bool) -> str: Schedule an event given a local time and time zone, and convert the event time to UTC. Args: - local_time_str (str): A string representing the local time in the format \\"YYYY-MM-DD HH:MM:SS\\". - time_zone (str): A string representing the IANA time zone, e.g., \\"America/New_York\\". - fold (bool): A boolean representing whether the time is fold=0 or fold=1. Returns: - str: A string representing the UTC time in the format \\"YYYY-MM-DD HH:MM:SS\\". Raises: - ZoneInfoNotFoundError: If the provided time zone is not found. - ValueError: If the input time string format is invalid. # Your implementation here ``` **Examples:** ```python # Example 1: Valid time zone and time print(schedule_event(\\"2023-03-12 02:30:00\\", \\"America/New_York\\", fold=0)) # Expected Output: \\"2023-03-12 07:30:00\\" # Example 2: Valid time zone and time with fold=1 print(schedule_event(\\"2023-11-05 01:30:00\\", \\"America/New_York\\", fold=1)) # Expected Output: \\"2023-11-05 06:30:00\\" # Example 3: Invalid time zone try: print(schedule_event(\\"2023-03-12 02:30:00\\", \\"Invalid/Zone\\", fold=0)) except ZoneInfoNotFoundError as e: print(e) # Expected to raise ZoneInfoNotFoundError ``` # Constraints: 1. Assume the input local time string is always in valid `YYYY-MM-DD HH:MM:SS` format. 2. Use the `ZoneInfo` class from the `zoneinfo` module for handling time zones. 3. Handle daylight saving time transitions using the `fold` attribute.","solution":"from zoneinfo import ZoneInfo, ZoneInfoNotFoundError from datetime import datetime, timezone def schedule_event(local_time_str: str, time_zone: str, fold: bool) -> str: Schedule an event given a local time and time zone, and convert the event time to UTC. Args: - local_time_str (str): A string representing the local time in the format \\"YYYY-MM-DD HH:MM:SS\\". - time_zone (str): A string representing the IANA time zone, e.g., \\"America/New_York\\". - fold (bool): A boolean representing whether the time is fold=0 or fold=1. Returns: - str: A string representing the UTC time in the format \\"YYYY-MM-DD HH:MM:SS\\". Raises: - ZoneInfoNotFoundError: If the provided time zone is not found. - ValueError: If the input time string format is invalid. try: # Parse the input local time string naive_local_dt = datetime.strptime(local_time_str, \\"%Y-%m-%d %H:%M:%S\\") # Find the corresponding time zone local_zone = ZoneInfo(time_zone) # Create a timezone-aware datetime in the specified local time zone local_dt = naive_local_dt.replace(tzinfo=local_zone, fold=int(fold)) # Convert local time to UTC utc_dt = local_dt.astimezone(timezone.utc) # Format the UTC datetime as a string and return it return utc_dt.strftime(\\"%Y-%m-%d %H:%M:%S\\") except ZoneInfoNotFoundError as e: raise ZoneInfoNotFoundError(f\\"Time zone \'{time_zone}\' not found\\") except ValueError as e: raise ValueError(\\"Invalid input local time string format\\")"},{"question":"# Cryptographic Services in Python 3.10 **Objective:** Implement a function that utilizes cryptographic services in Python 3.10 to securely hash a message, generate a secure token, and authenticate a message using HMAC. # Problem Statement: Implement a function `secure_operations` that performs the following tasks: 1. **Hashing:** Securely hash a given message using the BLAKE2b algorithm. 2. **Token Generation:** Generate a secure random token. 3. **HMAC Authentication:** Create a HMAC for the given message using a secret key and validate it. # Function Signature: ```python def secure_operations(message: str, key: bytes) -> dict: pass ``` # Input: - `message`: A string that needs to be securely hashed and authenticated. - `key`: A bytes object to be used as the key for HMAC authentication. # Output: - A dictionary with the following keys: - `\'hashed_message\'`: The BLAKE2b hash of the input message as a hexadecimal string. - `\'secure_token\'`: A securely generated token as a hexadecimal string. - `\'hmac\'`: The HMAC of the input message as a hexadecimal string. - `\'hmac_valid\'`: A boolean indicating if the HMAC is valid (always `True` for this function). # Example: ```python message = \\"HelloWorld\\" key = b\'secret_key\' result = secure_operations(message, key) print(result) # Expected Output (the actual hash and tokens will differ): # { # \'hashed_message\': \'9ed21fa663f98bc3...fa7acff50482a3c9e1f12edbcaefc\', # \'secure_token\': \'8afbc2cd87...ecafbfdbb2\', # \'hmac\': \'6ee3e789374...69f69e0c85e\', # \'hmac_valid\': True # } ``` # Constraints: - Use the BLAKE2b hashing algorithm from the `hashlib` module. - Generate a random token using the `secrets` module. - Use the `hmac` module for creating and validating message authentication codes. - Ensure that the dictionary keys and outputs are in the specified formats. # Notes: - Ensure that the HMAC validation always returns `True` to indicate the HMAC verification step. - Use adequate security practices as suggested by the `secrets` module documentation for generating secure random tokens. # Performance Requirements: - The function should efficiently handle typical use cases with short messages and keys under normal conditions. This problem encompasses several key areas of cryptographic services in Python 3.10 and assesses both basic and advanced concepts. It tests students on their ability to securely handle message hashing, token generation, and message authentication using HMAC.","solution":"import hashlib import hmac import secrets def secure_operations(message: str, key: bytes) -> dict: Perform secure operations: hash a message, generate a secure token, and authenticate a message using HMAC. Parameters: message (str): The message to be hashed and authenticated. key (bytes): The secret key used for HMAC authentication. Returns: dict: A dictionary containing the hashed message, secure token, HMAC, and HMAC validity. # Hash the message using BLAKE2b hashed_message = hashlib.blake2b(message.encode()).hexdigest() # Generate a secure random token secure_token = secrets.token_hex(32) # 32 bytes -> 64 hexadecimal characters # Create the HMAC hmac_hash = hmac.new(key, message.encode(), hashlib.blake2b).hexdigest() # Validate HMAC (always True in this case since we just generated it) hmac_valid = hmac.compare_digest(hmac_hash, hmac.new(key, message.encode(), hashlib.blake2b).hexdigest()) return { \'hashed_message\': hashed_message, \'secure_token\': secure_token, \'hmac\': hmac_hash, \'hmac_valid\': hmac_valid }"},{"question":"In this task, you are required to demonstrate your understanding of the `torch.futures` module in PyTorch. This exercise involves asynchronously executing multiple operations and synchronizing their completion using Futures. Task Description You are provided with three functions that perform some computational tasks asynchronously. Your goal is to execute these functions in parallel and then collect the results of all three functions once they have completed: 1. Implement a function named `execute_and_collect` which accepts three functions that return `torch.futures.Future` objects. 2. Your task is to: - Execute these three functions asynchronously. - Wait for all functions to complete. - Collect and return their results as a dictionary with the keys: `\'result1\'`, `\'result2\'`, and `\'result3\'`. Function Signature ```python def execute_and_collect(func1, func2, func3): pass ``` Input - `func1`: A callable that returns a `torch.futures.Future` object when invoked. - `func2`: A callable that returns a `torch.futures.Future` object when invoked. - `func3`: A callable that returns a `torch.futures.Future` object when invoked. Output - A dictionary with keys `\'result1\'`, `\'result2\'`, and `\'result3\'` containing the results of `func1`, `func2`, and `func3` respectively. Example Usage ```python import torch def mock_async_func(value): fut = torch.futures.Future() fut.set_result(value) return fut # Mock functions that you can use for testing def func1(): return mock_async_func(1) def func2(): return mock_async_func(2) def func3(): return mock_async_func(3) result = execute_and_collect(func1, func2, func3) print(result) # Output should be: {\'result1\': 1, \'result2\': 2, \'result3\': 3} ``` Constraints - You must use the `torch.futures.collect_all` or `torch.futures.wait_all` functions in your implementation. - Ensure that your function handles potential exceptions that might be raised during the asynchronous execution. Additional Requirements - Code readability and proper documentation are important. Please provide comments where necessary. This problem is designed to test your understanding of PyTorch\'s asynchronous execution model using Futures and your ability to work with distributed or parallel computing tasks.","solution":"import torch from torch.futures import Future def execute_and_collect(func1, func2, func3): Executes three asynchronous functions in parallel and collects their results. Args: func1, func2, func3: Functions that return torch.futures.Future objects. Returns: A dictionary with keys \'result1\', \'result2\', \'result3\' containing the results of func1, func2, and func3 respectively. # Execute the functions asynchronously future1 = func1() future2 = func2() future3 = func3() # Collect all futures all_futures = torch.futures.collect_all([future1, future2, future3]) # Get the results once all futures are complete all_futures.wait() results = all_futures.value() # Return results in a dictionary return { \'result1\': results[0].value(), \'result2\': results[1].value(), \'result3\': results[2].value() }"},{"question":"Secure Data Processing with Temporary Files and Directories **Objective:** Your task is to implement a function that securely processes large amounts of data by temporarily storing intermediate results in temporary files and directories. This function will handle automatic cleanup using context managers and ensure data security and validity during processing. **Function Signature:** ```python def secure_data_processing(data_chunks: list, process_function): Processes data chunks using temporary files and directories to store intermediate results. Parameters: data_chunks (list): A list of data chunks to be processed. process_function (function): A function that processes a single data chunk. It should take a file path as input and output processed data to another file path. Returns: List: Processed data. ``` **Input:** - `data_chunks`: A list of strings, each representing a chunk of data to be processed. - `process_function`: A function that takes an input file path and an output file path as arguments and performs some processing on the data read from the input file, writing the result to the output file. **Output:** - Returns a list of strings, each representing the processed data from the corresponding input chunk. **Constraints:** - The function should ensure that all temporary files and directories are securely created and properly cleaned up after use. - The `process_function` is expected to handle large data efficiently and must write its output to an output file. - Your implementation should handle exceptions gracefully, ensuring that temporary files and directories are cleaned up even if an error occurs during processing. **Example:** ```python import tempfile def example_process_function(input_file_path, output_file_path): with open(input_file_path, \'r\') as f_in, open(output_file_path, \'w\') as f_out: data = f_in.read() # Simulate processing by reversing string content processed_data = data[::-1] f_out.write(processed_data) def secure_data_processing(data_chunks, process_function): processed_data = [] with tempfile.TemporaryDirectory() as temp_dir: for chunk in data_chunks: temp_input_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir, mode=\'w+t\') temp_output_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir, mode=\'w+t\') try: temp_input_file.write(chunk) temp_input_file.flush() process_function(temp_input_file.name, temp_output_file.name) temp_output_file.seek(0) processed_data.append(temp_output_file.read()) finally: temp_input_file.close() temp_output_file.close() return processed_data # Example usage data_chunks = [\\"chunk1\\", \\"chunk2\\"] processed = secure_data_processing(data_chunks, example_process_function) print(processed) # Expected: [\'1knuhc\', \'2knuhc\'] ``` In this example, the `secure_data_processing` function writes the data chunks to temporary input files, runs the `process_function` to process the data, and stores the results from the temporary output files. The use of context managers ensures that all temporary resources are cleaned up appropriately. **Requirements:** - Your solution should properly use `tempfile` module functionalities to handle temporary files and directories. - Ensure that temporary files and directories are securely created and cleaned up. - Handle any potential errors gracefully, ensuring no resources are left behind.","solution":"import tempfile def secure_data_processing(data_chunks, process_function): Processes data chunks using temporary files and directories to store intermediate results. Parameters: data_chunks (list): A list of data chunks to be processed. process_function (function): A function that processes a single data chunk. It should take a file path as input and output processed data to another file path. Returns: List: Processed data. processed_data = [] with tempfile.TemporaryDirectory() as temp_dir: for chunk in data_chunks: temp_input_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir, mode=\'w+t\') temp_output_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir, mode=\'w+t\') try: temp_input_file.write(chunk) temp_input_file.flush() process_function(temp_input_file.name, temp_output_file.name) temp_output_file.seek(0) processed_data.append(temp_output_file.read()) finally: temp_input_file.close() temp_output_file.close() return processed_data"},{"question":"# Advanced Coding Assessment: Implementing a Custom Descriptor Objective: The objective of this task is to assess your understanding of Python descriptors, their usage, and their implementation in practical scenarios. Task: You are required to implement a custom descriptor for managing a specific type of validation in a class. The descriptor should enforce the following rules: 1. The attribute values must be one of a predefined set of strings. 2. The attribute values must be stored as uppercase in the instance. Requirements: 1. Implement a descriptor class `StringChoice`. - The descriptor should take a set of valid string choices during initialization. - It should ensure the attribute value is one of the predefined choices. - Store the attribute value as uppercase in the instance. 2. Implement a class `Item` that uses the `StringChoice` descriptor. - The class should have two attributes: `category` and `type`. - Valid choices for `category` are `{\'fruit\', \'vegetable\', \'dairy\'}`. - Valid choices for `type` are `{\'organic\', \'conventional\'}`. Example Usage: ```python class StringChoice: # Your implementation goes here class Item: category = StringChoice({\'fruit\', \'vegetable\', \'dairy\'}) type = StringChoice({\'organic\', \'conventional\'}) def __init__(self, category, type): self.category = category self.type = type # Example usage item = Item(\'fruit\', \'organic\') print(item.category) # Should print \'FRUIT\' print(item.type) # Should print \'ORGANIC\' item.category = \'Vegetable\' print(item.category) # Should print \'VEGETABLE\' try: item.type = \'unknown\' # Should raise a ValueError except ValueError as e: print(e) # Expected error message ``` Constraints: - You may not use any libraries or built-in Python functionalities that directly perform validation or ensure uppercase storage. Submission: Submit your implementation of the `StringChoice` descriptor and the `Item` class with example usage as shown above. Ensure that your code is well-documented and handles edge cases appropriately.","solution":"class StringChoice: def __init__(self, choices): self.choices = {choice.upper() for choice in choices} self.private_name = None def __set_name__(self, owner, name): self.private_name = f\'_{name}\' def __get__(self, instance, owner): return getattr(instance, self.private_name, None) def __set__(self, instance, value): if value.upper() not in self.choices: raise ValueError(f\\"Value must be one of {self.choices}\\") setattr(instance, self.private_name, value.upper()) class Item: category = StringChoice({\'fruit\', \'vegetable\', \'dairy\'}) type = StringChoice({\'organic\', \'conventional\'}) def __init__(self, category, type): self.category = category self.type = type # Example usage item = Item(\'fruit\', \'organic\') print(item.category) # Should print \'FRUIT\' print(item.type) # Should print \'ORGANIC\' item.category = \'Vegetable\' print(item.category) # Should print \'VEGETABLE\' try: item.type = \'unknown\' # Should raise a ValueError except ValueError as e: print(e) # Expected error message"},{"question":"**Objective:** Your task is to implement a function that performs Partial Least Squares (PLS) regression using scikit-learn\'s `PLSRegression` class. This function should take as input two numpy arrays, `X` and `Y`, representing the predictor and response matrices, respectively, and the number of PLS components. The function should return the predicted response matrix for the provided input data. **Function Signature:** ```python def pls_regression(X: np.ndarray, Y: np.ndarray, n_components: int) -> np.ndarray: pass ``` **Inputs:** 1. `X`: A numpy array of shape `(n_samples, n_features)` representing the predictor matrix. 2. `Y`: A numpy array of shape `(n_samples, n_targets)` representing the response matrix. 3. `n_components`: An integer representing the number of PLS components to use. **Outputs:** - A numpy array of shape `(n_samples, n_targets)` representing the predicted response matrix. **Constraints:** - You should use scikit-learn\'s `PLSRegression` class to perform the PLS regression. **Example:** ```python import numpy as np # Example data X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) Y = np.array([[1], [2], [3]]) # Number of components n_components = 2 # Expected Output: A numpy array with shape (3, 1) predicted_Y = pls_regression(X, Y, n_components) print(predicted_Y) ``` **Notes:** - Ensure that the output matrix has the correct shape and that the prediction quality is reasonable. - You may assume that the provided data is already centered and scaled if necessary. This question tests the student\'s ability to understand and implement a regression technique using the scikit-learn library, specifically focusing on applying PLS regression.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression def pls_regression(X: np.ndarray, Y: np.ndarray, n_components: int) -> np.ndarray: Perform Partial Least Squares (PLS) regression on the given data. Parameters: - X (np.ndarray): Predictor matrix of shape (n_samples, n_features) - Y (np.ndarray): Response matrix of shape (n_samples, n_targets) - n_components (int): The number of PLS components to use Returns: - np.ndarray: Predicted response matrix of shape (n_samples, n_targets) # Initialize the PLS regression model with the specified number of components pls = PLSRegression(n_components=n_components) # Fit the model to the data pls.fit(X, Y) # Predict using the fitted model Y_pred = pls.predict(X) return Y_pred"},{"question":"Question The `__future__` module in Python allows users to import features from future versions of Python. Each feature is an instance of the `_Feature` class, which contains information about when the feature was optionally and mandatorily introduced, as well as a compiler flag. # Task Write a function `get_future_features_info()` that extracts and returns information about all the features in the `__future__` module. The function should return a list of dictionaries, each dictionary containing the following keys: - `feature`: The name of the feature. - `optional_release`: The optional release version as a string in the format `\\"<major>.<minor>.<micro> <level> <serial>\\"`. - `mandatory_release`: The mandatory release version as a string in the same format as the optional release. If the `MandatoryRelease` is `None`, the string should be `\\"None\\"`. - `compiler_flag`: The compiler flag value. **Input and Output Format** * The function does not take any input. * It returns a list of dictionaries, where each dictionary contains the details outlined above. **Example** ```python import __future__ def get_future_features_info(): result = [] for feature_name in dir(__future__): feature = getattr(__future__, feature_name) if isinstance(feature, __future__._Feature): optional_release = \'.\'.join(map(str, feature.getOptionalRelease()[:-2])) + f\\" {feature.getOptionalRelease()[-2]} {feature.getOptionalRelease()[-1]}\\" mandatory_release = feature.getMandatoryRelease() if mandatory_release is not None: mandatory_release = \'.\'.join(map(str, mandatory_release[:-2])) + f\\" {mandatory_release[-2]} {mandatory_release[-1]}\\" else: mandatory_release = \\"None\\" result.append({ \\"feature\\": feature_name, \\"optional_release\\": optional_release, \\"mandatory_release\\": mandatory_release, \\"compiler_flag\\": feature.compiler_flag }) return result # Example usage features_info = get_future_features_info() for feature in features_info: print(feature) ``` # Constraints * You should not hardcode any feature names. * The function should dynamically extract information from the `__future__` module. This task requires you to understand the structure of tuples, string manipulation, handling class instances, and working with Python\'s introspection capabilities.","solution":"import __future__ def get_future_features_info(): result = [] for feature_name in dir(__future__): feature = getattr(__future__, feature_name) if isinstance(feature, __future__._Feature): optional_release = \'.\'.join(map(str, feature.getOptionalRelease()[:-2])) + f\\" {feature.getOptionalRelease()[-2]} {feature.getOptionalRelease()[-1]}\\" mandatory_release = feature.getMandatoryRelease() if mandatory_release is not None: mandatory_release = \'.\'.join(map(str, mandatory_release[:-2])) + f\\" {mandatory_release[-2]} {mandatory_release[-1]}\\" else: mandatory_release = \\"None\\" result.append({ \\"feature\\": feature_name, \\"optional_release\\": optional_release, \\"mandatory_release\\": mandatory_release, \\"compiler_flag\\": feature.compiler_flag }) return result # Example usage features_info = get_future_features_info() for feature in features_info: print(feature)"},{"question":"Problem Statement You are required to implement a small system that manages and processes data using classes. This system should demonstrate the following concepts: 1. Class Definitions 2. Inheritance (including Multiple Inheritance) 3. Method Overriding 4. Use of Special Methods (`__init__`, `__iter__`, `__next__`) 5. Creating and Using Generators # Requirements Class Hierarchy: 1. **Base Classes:** - `DataProcessor`: Abstract base class that should define the interface for processing data. - `IterableData`: Class that stores a list of data items and implements the iterator protocol to loop over the data in reverse. 2. **Derived Classes:** - `Summarizer`: Inherits from both `DataProcessor` and `IterableData`. It should process the data by calculating the sum and should be iterable in reverse. Implementation Details: 1. **DataProcessor Class** - This class should have an abstract method `process` which all derived classes must implement. Use Python\'s `abc` module to define this class. 2. **IterableData Class** - This class should be initialized with a list of data items. - Implement the `__iter__` and `__next__` methods to iterate over the data in reverse order. 3. **Summarizer Class** - Inherits from both `DataProcessor` and `IterableData`. - Implements the `process` method to calculate the sum of the data items. - Should be iterable in reverse (leveraging `IterableData`\'s iterator implementation). 4. **Generator Function** - `data_generator`: A function that takes in a range of numbers and yields each number with a delay (simulating asynchronous data delivery). # Constraints: - The data list provided to `IterableData` will have at most 1000 items. - Ensure that methods are properly overridden where required to extend functionality without losing the original behavior. # Input/Output: - You do not need to handle any input from stdin or output to stdout. - Focus on creating the classes and the generator function as specified. # Example: ```python from abc import ABC, abstractmethod class DataProcessor(ABC): @abstractmethod def process(self): pass class IterableData: def __init__(self, data): self.data = data self.index = len(data) def __iter__(self): return self def __next__(self): if self.index == 0: raise StopIteration self.index -= 1 return self.data[self.index] class Summarizer(DataProcessor, IterableData): def __init__(self, data): IterableData.__init__(self, data) def process(self): return sum(self.data) import time def data_generator(start, end): for i in range(start, end): time.sleep(1) yield i # Example Usage if __name__ == \\"__main__\\": data = [1, 2, 3, 4, 5] summarizer = Summarizer(data) result = summarizer.process() print(\\"Sum of data:\\", result) # Output: Sum of data: 15 print(\\"Iterating in reverse:\\") for item in summarizer: print(item) # Using the generator for num in data_generator(1, 5): print(\\"Generated number:\\", num) ```","solution":"from abc import ABC, abstractmethod import time class DataProcessor(ABC): @abstractmethod def process(self): pass class IterableData: def __init__(self, data): self.data = data self.index = len(data) def __iter__(self): return self def __next__(self): if self.index == 0: raise StopIteration self.index -= 1 return self.data[self.index] class Summarizer(DataProcessor, IterableData): def __init__(self, data): IterableData.__init__(self, data) def process(self): return sum(self.data) def data_generator(start, end): for i in range(start, end): time.sleep(1) yield i"},{"question":"**Coding Assessment Question: PyTorch with XPU - Memory and Device Management** In this assignment, you will create a utility to manage memory and devices for XPU computations using PyTorch. Your task involves implementing two functions that interact with the XPU. Specifically, you will: 1. Implement a function `get_memory_info` that returns current memory usage statistics of the XPU device such as total memory allocated and reserved. 2. Implement a function `run_on_device` which runs a computational task on a specified XPU device and returns the result. # Requirements: 1. **Function 1: get_memory_info** - **Input**: None - **Output**: A dictionary containing: - `\\"memory_allocated\\"`: Total memory allocated on the current XPU device. - `\\"memory_reserved\\"`: Total memory reserved on the current XPU device. - **Constraints**: - Use PyTorch XPU\'s `memory_allocated` and `memory_reserved` functions. 2. **Function 2: run_on_device** - **Input**: - `device_id` (int): The ID of the XPU device on which to run the computation. - `task` (Callable): A function representing the computational task to be run. The task function should take no inputs and return a single output. - **Output**: The result of the computational task executed on the specified device. - **Constraints**: - Make use of the `device`, `set_device`, and `synchronize` functions. - Ensure that the device is reset to the original device after running the task. # Example Usage ```python import torch.xpu as xpu def my_task(): # A sample computational task: a dot product of two tensors tensor_a = torch.randn(1000, 1000, device=\\"xpu\\") tensor_b = torch.randn(1000, 1000, device=\\"xpu\\") return torch.dot(tensor_a.flatten(), tensor_b.flatten()) # Get memory information print(get_memory_info()) # Run a task on device 0 result = run_on_device(0, my_task) print(result) ``` # Performance Requirements - Your solution should handle resetting the device context correctly even in the presence of exceptions. - Ensure that memory information retrieval is precise and optimized. Ensure your code is well-documented, and include error handling to manage device-related exceptions appropriately. Good luck!","solution":"import torch def get_memory_info(): Returns memory usage statistics of the current XPU device. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU is not available\\") return { \\"memory_allocated\\": torch.xpu.memory_allocated(), \\"memory_reserved\\": torch.xpu.memory_reserved() } def run_on_device(device_id, task): Runs a computational task on the specified XPU device and returns the result. Args: device_id (int): The ID of the XPU device on which to run the computation. task (callable): A function representing the computational task to be run. Returns: The result of the computational task executed on the specified device. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU is not available\\") original_device = torch.xpu.current_device() try: torch.xpu.set_device(device_id) result = task() torch.xpu.synchronize() finally: torch.xpu.set_device(original_device) torch.xpu.synchronize() return result"},{"question":"# Objective: In this exercise, you will create a plot using seaborn\'s `objects` module to demonstrate your grasp of fundamental and advanced features of seaborn, including data loading, plotting, customization of aesthetics, and analysis of dataset features. # Task: Write a function `create_custom_mpg_plot()` that: 1. Loads the `mpg` dataset from seaborn. 2. Creates a dot plot using `seaborn.objects.Plot` to visualize the relationship between `horsepower` and `mpg`. 3. Color the dots based on the `origin` column and customize the fill color based on the `weight` column, using a custom color scale. 4. Mix filled and unfilled markers based on the `origin` column. 5. Adds jitter to the x-axis to better visualize overlapping points. 6. Save the plot to a PNG file named `custom_mpg_plot.png`. # Requirements: 1. The function should return `None`. The plot should be saved as a PNG file. 2. Ensure the plot has appropriate labels for the x and y axes. 3. Apply a consistent theme to make the plot visually appealing. 4. Use appropriate Seaborn color palettes or scales to enhance the plot clarity. # Constraints: - Use the seaborn `objects` module only. - Ensure data visualizations are clear and interpretable. # Implementation: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_custom_mpg_plot(): # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot p = so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") ( p.add(so.Dots(stroke=1), color=\\"origin\\", fillcolor=\\"weight\\") .scale(fillcolor=\\"viridis\\", marker=[\\"o\\", \\"x\\", (6, 2, 1)]) .add(so.Dots(), so.Jitter(.25)) .theme({\\"axes.labelsize\\": 15}) .label(x=\\"Horsepower\\", y=\\"Miles per Gallon (mpg)\\") ) # Save the plot p.save(\\"custom_mpg_plot.png\\") # You can test the function by calling it # create_custom_mpg_plot() ``` # Input: No direct input is required. The function operates on the `mpg` dataset loaded internally. # Output: The output is a PNG file saved as `custom_mpg_plot.png` in the current working directory. # Notes: - Be sure to explore the `seaborn.objects` module thoroughly to understand the available customization options. - Use `seaborn` palettes and themes to make your plot aesthetically pleasing. - Validate your plot by opening the saved `custom_mpg_plot.png` file after running your function.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_custom_mpg_plot(): # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot p = so.Plot(mpg, x=\\"horsepower\\", y=\\"mpg\\") ( p.add(so.Dots(stroke=1), color=\\"origin\\", fillcolor=\\"weight\\") .scale(fillcolor=\\"viridis\\", marker=[\\"o\\", \\"x\\", (6, 2, 1)]) .add(so.Dots(), so.Jitter(.25)) .theme({\\"axes.labelsize\\": 15}) .label(x=\\"Horsepower\\", y=\\"Miles per Gallon (mpg)\\") ) # Save the plot p.save(\\"custom_mpg_plot.png\\")"},{"question":"# Advanced Terminal Control Utility Introduction You are tasked to implement a Python utility that controls terminal attributes and I/O behavior using the `termios` module. This utility should give users the ability to perform a series of terminal operations such as retrieving and setting terminal attributes, sending breaks, and controlling the terminal input/output flow. Objectives 1. Implement functions for retrieving and setting terminal attributes, sending breaks, draining, flushing, and controlling flow using the `termios` module. 2. Implement error handling for ensuring that terminal settings are restored after operations. Instructions **Function 1: Retrieve Terminal Attributes** ```python def get_terminal_attributes(fd: int) -> list: Retrieve the terminal attributes for the file descriptor. :param fd: File descriptor of the terminal. :return: A list of terminal attributes. pass ``` **Function 2: Set Terminal Attributes** ```python def set_terminal_attributes(fd: int, when: int, attributes: list) -> None: Set the terminal attributes for the file descriptor. :param fd: File descriptor of the terminal. :param when: When to change the attributes (TCSANOW, TCSADRAIN, or TCSAFLUSH). :param attributes: List of terminal attributes to set. pass ``` **Function 3: Send Break** ```python def send_break(fd: int, duration: int) -> None: Sends a break on the file descriptor. :param fd: File descriptor of the terminal. :param duration: Duration of the break. pass ``` **Function 4: Drain All Output** ```python def drain_output(fd: int) -> None: Wait until all output written to file descriptor has been transmitted. :param fd: File descriptor of the terminal. pass ``` **Function 5: Flush Queue** ```python def flush_queue(fd: int, queue: int) -> None: Discard queued data on the file descriptor. :param fd: File descriptor of the terminal. :param queue: Specifies which queue to flush (TCIFLUSH, TCOFLUSH, or TCIOFLUSH). pass ``` **Function 6: Control Flow** ```python def control_flow(fd: int, action: int) -> None: Suspend or resume input or output on file descriptor. :param fd: File descriptor of the terminal. :param action: Action to perform (TCOOFF, TCOON, TCIOFF, or TCION). pass ``` Example Usage ```python def main(): import sys fd = sys.stdin.fileno() # Get current terminal attributes attrs = get_terminal_attributes(fd) print(\\"Current Attributes:\\", attrs) # Set new terminal attributes new_attrs = attrs[:] # Copy current attributes new_attrs[3] &= ~termios.ECHO # Disable echo set_terminal_attributes(fd, termios.TCSANOW, new_attrs) # Read input without echo try: passwd = input(\\"Password: \\") finally: # Restore original attributes set_terminal_attributes(fd, termios.TCSANOW, attrs) print(\\"Password entered:\\", passwd) if __name__ == \\"__main__\\": main() ``` **Constraints:** - Only available on Unix-based systems. - Ensure to handle possible exceptions and restore terminal states in case of errors. - Follow POSIX standards as defined in the system documentation. Understanding and managing terminal attributes correctly is crucial for developing robust command-line applications. Good luck!","solution":"import termios import tty import sys def get_terminal_attributes(fd: int) -> list: Retrieve the terminal attributes for the file descriptor. :param fd: File descriptor of the terminal. :return: A list of terminal attributes. try: return termios.tcgetattr(fd) except termios.error as e: print(f\\"Error getting terminal attributes: {e}\\") raise def set_terminal_attributes(fd: int, when: int, attributes: list) -> None: Set the terminal attributes for the file descriptor. :param fd: File descriptor of the terminal. :param when: When to change the attributes (TCSANOW, TCSADRAIN, or TCSAFLUSH). :param attributes: List of terminal attributes to set. try: termios.tcsetattr(fd, when, attributes) except termios.error as e: print(f\\"Error setting terminal attributes: {e}\\") raise def send_break(fd: int, duration: int) -> None: Sends a break on the file descriptor. :param fd: File descriptor of the terminal. :param duration: Duration of the break. try: termios.tcsendbreak(fd, duration) except termios.error as e: print(f\\"Error sending break: {e}\\") raise def drain_output(fd: int) -> None: Wait until all output written to file descriptor has been transmitted. :param fd: File descriptor of the terminal. try: termios.tcdrain(fd) except termios.error as e: print(f\\"Error draining output: {e}\\") raise def flush_queue(fd: int, queue: int) -> None: Discard queued data on the file descriptor. :param fd: File descriptor of the terminal. :param queue: Specifies which queue to flush (TCIFLUSH, TCOFLUSH, or TCIOFLUSH). try: termios.tcflush(fd, queue) except termios.error as e: print(f\\"Error flushing queue: {e}\\") raise def control_flow(fd: int, action: int) -> None: Suspend or resume input or output on file descriptor. :param fd: File descriptor of the terminal. :param action: Action to perform (TCOOFF, TCOON, TCIOFF, or TCION). try: termios.tcflow(fd, action) except termios.error as e: print(f\\"Error controlling flow: {e}\\") raise"},{"question":"# Complex Logging System Implementation **Objective:** Create a complex logging system for a hypothetical application using Python\'s `logging` module. Your solution should demonstrate the ability to configure loggers, handlers, formatters, and filters. The logging framework should be capable of generating, formatting, and managing log messages effectively. **Problem Statement:** You are tasked with setting up a logging system for an application that processes user data and generates logs at different logging levels. The logs need to be handled in different ways based on their severity, and formatted to include additional contextual information. The requirements are outlined below: 1. **Logger Configuration:** - Create a root logger with a logging level of `DEBUG`. - Create a hierarchical logger named `\\"user.data.processor\\"` which should: - Propagate messages to its parent logger. - Log messages at the `INFO` level and higher to a file named `user_data_processor.log`. 2. **Handler Setup:** - The root logger should have a console handler that outputs log messages to the console, formatted to include the timestamp, logger name, log level, and message. - The `\\"user.data.processor\\"` logger should have a file handler that outputs log messages to `user_data_processor.log`, formatted differently to include the timestamp, log level, message, and additional contextual information (`user_id`). 3. **Filter:** - Implement a filter that only allows log messages generated by the `\\"user.data.processor\\"` logger (and its descendants) to pass through. 4. **Contextual Information:** - Add contextual information to log messages generated by the `\\"user.data.processor\\"` logger, specifically the `user_id`. 5. **Logging Example:** - Generate a few log messages using both the root logger and the `\\"user.data.processor\\"` logger to demonstrate that messages are logged according to the specified configuration and format. **Constraints:** - Ensure the file handler and console handler do not produce duplicate log entries. - The solution should be contained within a single Python script. **Input:** None. The script should contain its own logging configuration and log generation. **Output:** The output should be the log messages generated and output to the console and `user_data_processor.log` file based on the criteria mentioned above. **Implementation details:** - Make use of `logging` module classes and functions such as `logging.getLogger`, `logging.FileHandler`, `logging.StreamHandler`, `logging.Formatter`, and `logging.Filter`. **Example Code Structure:** ``` import logging # Implement your logging configuration and classes here. def main(): # Demonstrate logging with examples root_logger = logging.getLogger() user_data_logger = logging.getLogger(\'user.data.processor\') root_logger.debug(\\"This is a debug message from the root logger.\\") root_logger.info(\\"This is an info message from the root logger.\\") extra_context = {\'user_id\': \'user123\'} user_data_logger.info(\\"Processing user data...\\", extra=extra_context) user_data_logger.error(\\"User data processing failed!\\", extra=extra_context) if __name__ == \\"__main__\\": main() ``` Your task is to fill in the implementation details for configuring and setting up the logging handlers, formatters, and the filter.","solution":"import logging import logging.handlers class UserDataFilter(logging.Filter): def filter(self, record): return record.name.startswith(\'user.data.processor\') # Root logger configuration root_logger = logging.getLogger() root_logger.setLevel(logging.DEBUG) # Console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) console_format = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_format) root_logger.addHandler(console_handler) # User data processor logger configuration user_data_logger = logging.getLogger(\'user.data.processor\') user_data_logger.setLevel(logging.INFO) # File handler file_handler = logging.FileHandler(\'user_data_processor.log\') file_handler.setLevel(logging.INFO) file_format = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s - User: %(user_id)s\') file_handler.setFormatter(file_format) file_handler.addFilter(UserDataFilter()) user_data_logger.addHandler(file_handler) # Demonstration of logging def main(): root_logger.debug(\\"This is a debug message from the root logger.\\") root_logger.info(\\"This is an info message from the root logger.\\") extra_context = {\'user_id\': \'user123\'} user_data_logger.info(\\"Processing user data...\\", extra=extra_context) user_data_logger.error(\\"User data processing failed!\\", extra=extra_context) if __name__ == \\"__main__\\": main()"},{"question":"Objective: You are tasked with developing a Python script that demonstrates your understanding of error handling, script executability, and interactive session customization. Problem Statement: 1. Create a Python script named `custom_script.py` that meets the following requirements: - It should be executable on Unix-based systems with the appropriate shebang line. - It should define a custom function `divide(a, b)` that safely performs division and handles possible errors (e.g., division by zero). - It should customize the interactive session by setting and using a `PYTHONSTARTUP` script. - The script should create or modify a `usercustomize.py` file in the user site-packages directory to define a custom greeting message each time the interpreter is started. 2. Implement the following additional requirements: - The `divide(a, b)` function should: - Print an informative error message if a division by zero is attempted. - Return `None` in case of an error. - Otherwise, return the result of the division. - The customization via `PYTHONSTARTUP` should: - Print \\"Welcome to the customized Python interpreter!\\" each time an interactive Python session starts. - The `usercustomize.py` file should: - Print \\"User customization module loaded!\\" each time it is imported. Constraints: - Your solution should ensure the correct paths and file permissions when operating on a Unix-based system. - Assume you are working within your user\'s home directory and have the necessary permissions to create and modify files. Input/Output: - The input for this task is the implementation in the form of the script file `custom_script.py` and the `usercustomize.py` module. - There is no specific input/output format for the script execution, but comments should document the process. Submission: Submit the following files for evaluation: 1. `custom_script.py` 2. The content of `usercustomize.py` 3. A brief README explaining how to test the script, any additional steps needed to set up the environment variables, and how to view the customizations in an interactive session. Evaluation: Your solution will be evaluated based on: - Correct implementation of error handling in the `divide` function. - Proper use of shebang and execution permissions for the script. - Setup and use of the `PYTHONSTARTUP` environment variable to customize the interactive session. - Effectiveness of the `usercustomize.py` module in modifying the interactive startup experience. - Clarity of documentation and ease of setting up the environment based on your README instructions. Good luck!","solution":"#!/usr/bin/env python3 import os import site def divide(a, b): Safely performs division and handles possible errors. If division by zero is attempted, prints an error message and returns None. Otherwise, returns the result of the division. try: return a / b except ZeroDivisionError: print(\\"Error: Division by zero is not allowed.\\") return None # Setting up PYTHONSTARTUP environment variable startup_file_path = os.path.expanduser(\\"~/.pythonstartup\\") with open(startup_file_path, \'w\') as f: f.write(\'print(\\"Welcome to the customized Python interpreter!\\")n\') os.environ[\'PYTHONSTARTUP\'] = startup_file_path # Creating/modifying usercustomize.py in the user site-packages directory user_custom = site.getusersitepackages() user_custom_file = os.path.join(user_custom, \'usercustomize.py\') os.makedirs(user_custom, exist_ok=True) with open(user_custom_file, \'w\') as f: f.write(\'print(\\"User customization module loaded!\\")n\') # Instruct the user to reload environment variables for changes to take effect print(\\"Setup complete. Please restart your terminal or run \'source ~/.bashrc\' for changes to take effect.\\")"},{"question":"# Question: Advanced Distance Calculator You are required to implement a function that calculates the distance between two points in a Euclidean space, along with a few additional features using the `math` module functions. Function Signature ```python def advanced_distance(p, q, metric=\'euclidean\', precision=8): pass ``` # Description - `p`: A tuple representing the coordinates of the first point. The length of the tuple will determine the dimension of the space. - `q`: A tuple representing the coordinates of the second point. It will have the same length as `p`. - `metric`: A string that can be either `\'euclidean\'`, `\'manhattan\'`, or `\'chebyshev\'`, representing the type of distance metric to use. - `\'euclidean\'`: Uses the Euclidean distance formula. - `\'manhattan\'`: Uses the Manhattan distance formula. - `\'chebyshev\'`: Uses the Chebyshev distance formula. - `precision`: An integer representing the number of decimal places to round the result to. Default is 8. # Constraints 1. Both `p` and `q` will have the same length. 2. All coordinates are real numbers. 3. The length of the tuples will be between 1 and 10 (inclusive). 4. The precision will be a non-negative integer. # Expected Output The function should return a single float representing the calculated distance, rounded to the specified number of decimal places. # Examples ```python print(advanced_distance((1.0, 2.0), (4.0, 6.0))) # Default is Euclidean, precision 8 # Output: 5.0 print(advanced_distance((1.0, 2.0), (4.0, 6.0), \'manhattan\')) # Output: 7.0 print(advanced_distance((1.0, 2.0), (4.0, 6.0), \'chebyshev\')) # Output: 4.0 print(advanced_distance((1.0, 2.0), (4.0, 6.0), precision=2)) # Output: 5.0 print(advanced_distance((1.0, 2.0), (1.002, 2.002), \'euclidean\', 4)) # Output: 0.0024 ``` # Notes - Euclidean Distance: ( sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} ) - Manhattan Distance: ( |x_2 - x_1| + |y_2 - y_1| ) - Chebyshev Distance: ( max(|x_2 - x_1|, |y_2 - y_1|) ) You will primarily use functions such as `math.sqrt`, `math.fsum`, and other appropriate functions from the `math` module to achieve this task. Ensure that all numerical operations use the `math` functions to highlight your understanding of the module and avoid precision loss.","solution":"import math def advanced_distance(p, q, metric=\'euclidean\', precision=8): Calculates the distance between two points p and q in a specified metric. Parameters: p (tuple): Coordinates of the first point. q (tuple): Coordinates of the second point. metric (str): Type of distance metric (\'euclidean\', \'manhattan\', \'chebyshev\'). precision (int): Number of decimal places to round the result to. Returns: float: The distance between points p and q rounded to the specified precision. if len(p) != len(q): raise ValueError(\\"Points p and q must have the same number of dimensions\\") if metric == \'euclidean\': distance = math.sqrt(sum((p[i] - q[i]) ** 2 for i in range(len(p)))) elif metric == \'manhattan\': distance = sum(abs(p[i] - q[i]) for i in range(len(p))) elif metric == \'chebyshev\': distance = max(abs(p[i] - q[i]) for i in range(len(p))) else: raise ValueError(\\"Invalid metric specified. Use \'euclidean\', \'manhattan\', or \'chebyshev\'\\") return round(distance, precision)"},{"question":"# Kernel Density Estimation Plots with Seaborn **Problem Statement:** You are given a weather dataset containing daily weather data in a city. The dataset includes the following columns: - `temperature`: the daily mean temperature in degrees Celsius. - `humidity`: the daily mean relative humidity percentage. - `precipitation`: the daily total precipitation in mm. - `season`: the season during which the data was recorded (Fall, Winter, Spring, Summer). Using seaborn\'s `kdeplot` function, your task is to generate different kernel density estimate (KDE) plots to analyze the weather data. **Tasks:** 1. **Univariate KDE Plot:** - Plot a univariate KDE plot for the `temperature` column. - Include a title \\"Univariate KDE Plot for Temperature\\". - Save the plot as `kde_temperature.png`. 2. **Smoothed KDE Plot:** - Plot a KDE for the `humidity` column with increased smoothness (use `bw_adjust=2`). - Add a title \\"Smoothed KDE Plot for Humidity\\". - Save the plot as `kde_humidity_smooth.png`. 3. **Conditional Distribution Plot with Hue Mapping:** - Generate a KDE plot for the `temperature` column with conditional distributions based on the `season`. - Use different colors for each season and ensure the legend is displayed. - Title the plot \\"KDE Plot for Temperature by Season\\". - Save the plot as `kde_temperature_season.png`. 4. **Bivariate KDE Plot:** - Create a bivariate KDE plot for `temperature` (x-axis) and `humidity` (y-axis). - Title the plot \\"Bivariate KDE Plot for Temperature and Humidity\\". - Save the plot as `kde_temp_humidity.png`. 5. **Conditional Bivariate KDE Plot:** - Plot a bivariate KDE plot of `temperature` and `precipitation` with conditional distributions based on the `season`. - Add filled contours and set the levels to 10. - Title the plot \\"Conditional Bivariate KDE Plot for Temperature and Precipitation by Season\\". - Save the plot as `bivariate_kde_temp_precipitation_season.png`. **Constraints:** - Use seaborn\'s `kdeplot` function for all plots. - Ensure that the plots are clearly labeled with x and y axis labels where applicable. - Use `pyplot.savefig()` or an equivalent function to save each plot as specified. **Input:** You do not need to handle the input. Assume that the input dataset, `weather.csv`, is available in the current working directory with the specified columns. **Output:** The outputs are the five saved plot images as specified. ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the dataset weather = pd.read_csv(\'weather.csv\') # Task 1: Univariate KDE Plot for Temperature sns.kdeplot(data=weather, x=\'temperature\') plt.title(\'Univariate KDE Plot for Temperature\') plt.savefig(\'kde_temperature.png\') plt.clf() # Task 2: Smoothed KDE Plot for Humidity sns.kdeplot(data=weather, x=\'humidity\', bw_adjust=2) plt.title(\'Smoothed KDE Plot for Humidity\') plt.savefig(\'kde_humidity_smooth.png\') plt.clf() # Task 3: Conditional Distribution Plot with Hue Mapping sns.kdeplot(data=weather, x=\'temperature\', hue=\'season\') plt.title(\'KDE Plot for Temperature by Season\') plt.legend(title=\'Season\') plt.savefig(\'kde_temperature_season.png\') plt.clf() # Task 4: Bivariate KDE Plot for Temperature and Humidity sns.kdeplot(data=weather, x=\'temperature\', y=\'humidity\') plt.title(\'Bivariate KDE Plot for Temperature and Humidity\') plt.savefig(\'kde_temp_humidity.png\') plt.clf() # Task 5: Conditional Bivariate KDE Plot for Temperature and Precipitation by Season sns.kdeplot(data=weather, x=\'temperature\', y=\'precipitation\', hue=\'season\', fill=True, levels=10) plt.title(\'Conditional Bivariate KDE Plot for Temperature and Precipitation by Season\') plt.legend(title=\'Season\') plt.savefig(\'bivariate_kde_temp_precipitation_season.png\') plt.clf() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_kde_plots(weather_file_path): # Load the dataset weather = pd.read_csv(weather_file_path) # Task 1: Univariate KDE Plot for Temperature sns.kdeplot(data=weather, x=\'temperature\') plt.title(\'Univariate KDE Plot for Temperature\') plt.savefig(\'kde_temperature.png\') plt.clf() # Task 2: Smoothed KDE Plot for Humidity sns.kdeplot(data=weather, x=\'humidity\', bw_adjust=2) plt.title(\'Smoothed KDE Plot for Humidity\') plt.savefig(\'kde_humidity_smooth.png\') plt.clf() # Task 3: Conditional Distribution Plot with Hue Mapping sns.kdeplot(data=weather, x=\'temperature\', hue=\'season\') plt.title(\'KDE Plot for Temperature by Season\') plt.legend(title=\'Season\') plt.savefig(\'kde_temperature_season.png\') plt.clf() # Task 4: Bivariate KDE Plot for Temperature and Humidity sns.kdeplot(data=weather, x=\'temperature\', y=\'humidity\') plt.title(\'Bivariate KDE Plot for Temperature and Humidity\') plt.savefig(\'kde_temp_humidity.png\') plt.clf() # Task 5: Conditional Bivariate KDE Plot for Temperature and Precipitation by Season sns.kdeplot(data=weather, x=\'temperature\', y=\'precipitation\', hue=\'season\', fill=True, levels=10) plt.title(\'Conditional Bivariate KDE Plot for Temperature and Precipitation by Season\') plt.legend(title=\'Season\') plt.savefig(\'bivariate_kde_temp_precipitation_season.png\') plt.clf()"},{"question":"# Pandas Advanced Merging and Aggregation You are given sales data from two different branches of a company, stored in two DataFrames `sales_branch1` and `sales_branch2`. Each DataFrame contains daily sales data including the date, item sold, and the quantity sold. Additionally, you have a DataFrame `item_info` which contains static information about each item, including the item_id and its category. DataFrames Schema: `sales_branch1`: ``` +------------+--------+-------------+ | Date | Item | Quantity | +------------+--------+-------------+ | 2023-01-01 | ItemA | 10 | | 2023-01-02 | ItemB | 5 | | ... | ... | ... | +------------+--------+-------------+ ``` `sales_branch2`: ``` +------------+--------+-------------+ | Date | Item | Quantity | +------------+--------+-------------+ | 2023-01-01 | ItemC | 15 | | 2023-01-02 | ItemA | 8 | | ... | ... | ... | +------------+--------+-------------+ ``` `item_info`: ``` +--------+----------+ | Item | Category | +--------+----------+ | ItemA | Category1| | ItemB | Category2| | ... | ... | +--------+----------+ ``` # Task: 1. **Merge the Sales Data**: Combine `sales_branch1` and `sales_branch2` DataFrames to create a single DataFrame `combined_sales` that contains all sales information. Handle the Date and Item columns appropriately to ensure all sales records are included. 2. **Add Item Categories**: Merge the `combined_sales` DataFrame with the `item_info` DataFrame to include item categories in the sales data. The resulting DataFrame should have Date, Item, Category, and Quantity columns. 3. **Summarize Total Sales**: Create a summary DataFrame `summary_sales` that provides the total quantity sold per category across all dates. 4. **Identify Daily Differences**: Compare sales quantities of `sales_branch1` and `sales_branch2` on a daily basis, showing the differences for each item sold on each day. # Constraints: - The Date column in `sales_branch1` and `sales_branch2` follows the `YYYY-MM-DD` format. - All items in the sales data are present in the `item_info` DataFrame. # Implementation: ```python import pandas as pd # Sample data data_branch1 = { \\"Date\\": [\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\"], \\"Item\\": [\\"ItemA\\", \\"ItemB\\", \\"ItemA\\"], \\"Quantity\\": [10, 5, 6] } sales_branch1 = pd.DataFrame(data_branch1) data_branch2 = { \\"Date\\": [\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\"], \\"Item\\": [\\"ItemC\\", \\"ItemA\\", \\"ItemB\\"], \\"Quantity\\": [15, 8, 7] } sales_branch2 = pd.DataFrame(data_branch2) item_info_data = { \\"Item\\": [\\"ItemA\\", \\"ItemB\\", \\"ItemC\\"], \\"Category\\": [\\"Category1\\", \\"Category2\\", \\"Category1\\"] } item_info = pd.DataFrame(item_info_data) # Task 1: Merge the Sales Data def merge_sales(branch1, branch2): pass # Task 2: Add Item Categories def add_item_categories(combined_sales, item_info): pass # Task 3: Summarize Total Sales def summarize_total_sales(merged_sales): pass # Task 4: Identify Daily Differences def identify_daily_differences(branch1, branch2): pass # Uncomment and complete the implementation for each function # and call the functions as needed to test your code # combined_sales = merge_sales(sales_branch1, sales_branch2) # merged_sales = add_item_categories(combined_sales, item_info) # summary_sales = summarize_total_sales(merged_sales) # daily_differences = identify_daily_differences(sales_branch1, sales_branch2) ``` **Note**: Ensure that the function implementations are correct based on the provided sample data format and expected outcomes as detailed in the Task section.","solution":"import pandas as pd def merge_sales(branch1, branch2): Merge branch1 and branch2 sales data into a single DataFrame. combined_sales = pd.concat([branch1, branch2], ignore_index=True) return combined_sales def add_item_categories(combined_sales, item_info): Merge combined sales data with item information to include item categories. merged_sales = pd.merge(combined_sales, item_info, on=\'Item\', how=\'left\') return merged_sales def summarize_total_sales(merged_sales): Summarize total quantity sold per category. summary_sales = merged_sales.groupby(\'Category\').agg({\'Quantity\': \'sum\'}).reset_index() summary_sales = summary_sales.rename(columns={\'Quantity\': \'Total_Quantity\'}) return summary_sales def identify_daily_differences(branch1, branch2): Identify daily differences in sales quantities between branch1 and branch2. branch1 = branch1.rename(columns={\\"Quantity\\": \\"Quantity_branch1\\"}) branch2 = branch2.rename(columns={\\"Quantity\\": \\"Quantity_branch2\\"}) merged_data = pd.merge(branch1, branch2, on=[\'Date\', \'Item\'], how=\'outer\') merged_data = merged_data.fillna(0) merged_data[\'Quantity_diff\'] = merged_data[\'Quantity_branch1\'] - merged_data[\'Quantity_branch2\'] return merged_data[[\'Date\', \'Item\', \'Quantity_diff\']]"},{"question":"# Context: Context Management with `contextvars` and Asynchronous Programming **Question:** You are tasked with developing a concurrent HTTP request handler that stores request-specific data using context variables. This involves simulating an environment where separate asynchronous tasks handle different client requests. Requirements: 1. Implement a function `log_request_info` which: - Accepts a string `info` as input. - Sets this information in a context variable named `request_info`. - Returns the token received from setting the variable. 2. Implement a function `retrieve_request_info` which: - Retrieves and returns the currently set value of `request_info`. If no value is set, return the string `\\"No info\\"`. 3. Implement an asynchronous function `handle_request` which: - Simulates handling an HTTP request. - Accepts `req_id`, a unique identifier for the request. - Uses `log_request_info` to log information \\"Handling request {req_id}\\". - Simulates a delay using `await asyncio.sleep(random_delay)` where `random_delay` is a random float between 0.1 and 1.0 seconds. - After the delay, it retrieves and prints the logged request info using `retrieve_request_info`. 4. Implement an asynchronous function `main` which: - Creates a few `handle_request` tasks to run concurrently for `req_id` values in range 1 to 5. - Uses `asyncio.gather()` to ensure that requests are processed concurrently. **Input and Output:** - There is no direct input from the user. - Standard output will display retrievals of request info for each request. **Constraints:** - Ensure that context variable values set in one request handler do not bleed into other concurrent handlers. - Utilize the `contextvars` module features appropriately. Here is the setup code to start with. Implement the required functions within the provided placeholders and run the asynchronous main function: ```python import asyncio import contextvars import random # Define the context variable request_info = contextvars.ContextVar(\'request_info\', default=\'No info\') def log_request_info(info: str) -> contextvars.Token: # Your implementation here pass def retrieve_request_info() -> str: # Your implementation here pass async def handle_request(req_id: int): # Your implementation here pass async def main(): tasks = [handle_request(req_id) for req_id in range(1, 6)] await asyncio.gather(*tasks) # Run the asynchronous main function asyncio.run(main()) ``` **Note:** Ensure your implementation maintains the isolation of context variable values across different concurrent handlers.","solution":"import asyncio import contextvars import random # Define the context variable request_info = contextvars.ContextVar(\'request_info\', default=\'No info\') def log_request_info(info: str) -> contextvars.Token: Sets the request information to the context variable. return request_info.set(info) def retrieve_request_info() -> str: Retrieves the currently set value of request_info. return request_info.get() async def handle_request(req_id: int): Simulates handling an HTTP request. info = f\\"Handling request {req_id}\\" token = log_request_info(info) random_delay = random.uniform(0.1, 1.0) await asyncio.sleep(random_delay) retrieved_info = retrieve_request_info() print(f\\"Request {req_id}: {retrieved_info}\\") request_info.reset(token) async def main(): tasks = [handle_request(req_id) for req_id in range(1, 6)] await asyncio.gather(*tasks) # Run the asynchronous main function asyncio.run(main())"},{"question":"# Regular Expression Search and Replace In this assessment, you will demonstrate your understanding of regular expressions by performing a search and replace operation on a string. You will be given a string containing a mix of letters, numbers, and special characters. Your task is to write a function that performs the following operations: 1. Find all sequences of digits in the input string. 2. Replace each sequence of digits with a specific replacement string. Function Signature ```python def replace_digits(input_string: str, replacement: str) -> str: pass ``` Input - `input_string`: A string containing letters, digits, and possibly special characters (1 ≤ len(input_string) ≤ 10^4). - `replacement`: A string to replace each sequence of digits (1 ≤ len(replacement) ≤ 100). Output - The function should return a new string where all sequences of digits in `input_string` have been replaced by `replacement`. Example ```python # Example 1 input_string = \\"abc123def456ghi789\\" replacement = \\"#\\" output = \\"abc#def#ghi#\\" # Example 2 input_string = \\"The 1 quick brown fox jumps over 13 lazy dogs.\\" replacement = \\"X\\" output = \\"The X quick brown fox jumps over X lazy dogs.\\" ``` Constraints - You must use regular expressions for finding the sequences of digits. - The solution should handle large input efficiently. Performance Requirements - The solution should perform the search and replace operation in O(n) time complexity, where n is the length of the input string. Implement the function `replace_digits` and ensure it meets the requirements stated above.","solution":"import re def replace_digits(input_string: str, replacement: str) -> str: Replaces all sequences of digits in the input string with the specified replacement string. Parameters: input_string (str): The input string containing letters, digits, and possibly special characters. replacement (str): The string to replace each sequence of digits. Returns: str: A new string with all sequences of digits replaced by the replacement string. # Use regular expression to find all sequences of digits and replace them return re.sub(r\'d+\', replacement, input_string)"},{"question":"Objective Implement a custom class that meets the criteria for a `Sequence` as defined in the `collections.abc` module. The class should provide basic functionality like getting the length, accessing items by index, iterating through the items, and reversing the sequence. Additionally, you are required to implement methods to append items to the sequence and check if an item exists within the sequence. Requirements 1. Implement a class named `CustomSequence` that must: - Inherit from `collections.abc.Sequence`. - Implement the abstract methods `__getitem__` and `__len__`. - Implement a method `append` to add an item to the sequence. - Make use of mixin methods `__iter__`, `__reversed__`, `__contains__`. 2. The `CustomSequence` class should have the following methods: - `__init__(self)`: Initializes an empty sequence. - `__getitem__(self, index)`: Returns the item at the specified index. - `__len__(self)`: Returns the number of items in the sequence. - `append(self, item)`: Adds an item to the end of the sequence. - `__iter__(self)`: Returns an iterator over the sequence. - `__reversed__(self)`: Returns a reversed iterator over the sequence. - `__contains__(self, item)`: Checks if the item is in the sequence. 3. Ensure your implementation is efficient and consider performance implications when overriding mixin methods, especially with larger sequences. Constraints - Do not use any built-in sequence types (e.g., lists, tuples) directly in your class. - The sequence should dynamically grow as items are appended. - Indexing should support negative indexes, similar to list indexing in Python. Example Usage ```python sequence = CustomSequence() sequence.append(1) sequence.append(2) sequence.append(3) print(len(sequence)) # Output: 3 print(sequence[1]) # Output: 2 print(2 in sequence) # Output: True for item in sequence: print(item) # Output: 1 2 3 for item in reversed(sequence): print(item) # Output: 3 2 1 ``` Design and implement this class. Ensure it passes the example usage and behaves as expected based on the requirements mentioned.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self): self._data = [] def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def append(self, item): self._data.append(item) def __iter__(self): return iter(self._data) def __reversed__(self): return reversed(self._data) def __contains__(self, item): return item in self._data"},{"question":"**Question: Event Scheduler** You have been tasked with creating an event scheduler that allows users to schedule, list, and retrieve events efficiently. # Requirements 1. **Event Class**: Create a class `Event` with the following properties: - `title` (str): The title of the event. - `start_time` (`datetime` object): The start time of the event. - `end_time` (`datetime` object): The end time of the event. 2. **Scheduler Class**: Create a class `Scheduler` that facilitates scheduling and managing events with the following methods: - `add_event(event: Event) -> None`: Adds an event to the scheduler. - `remove_event(event: Event) -> None`: Removes a specific event from the scheduler. - `get_upcoming_events(current_time: datetime, n: int) -> List[Event]`: Retrieves the next `n` upcoming events from the current time. - `list_events_by_day(day: datetime) -> List[Event]`: Lists all events occurring on the given day. # Constraints - Events cannot overlap in time. If an event is added that conflicts with an existing event, an appropriate exception should be raised. - Ensure that all events are stored efficiently for quick retrieval. You may use any suitable data structures from the `collections` or `heapq` modules. # Input and Output Formats - All input times will be provided as `datetime` objects. - The methods should return lists of `Event` objects where specified. # Performance Requirements - The methods `get_upcoming_events` and `list_events_by_day` should operate efficiently even with a large number of events scheduled. # Example Usage ```python from datetime import datetime event1 = Event(\\"Meeting\\", datetime(2023, 10, 1, 9, 0), datetime(2023, 10, 1, 10, 0)) event2 = Event(\\"Conference\\", datetime(2023, 10, 1, 11, 0), datetime(2023, 10, 1, 12, 0)) event3 = Event(\\"Workshop\\", datetime(2023, 10, 2, 14, 0), datetime(2023, 10, 2, 16, 0)) scheduler = Scheduler() scheduler.add_event(event1) scheduler.add_event(event2) scheduler.add_event(event3) current_time = datetime(2023, 10, 1, 8, 0) upcoming_events = scheduler.get_upcoming_events(current_time, 2) for event in upcoming_events: print(event.title) events_by_day = scheduler.list_events_by_day(datetime(2023, 10, 1)) for event in events_by_day: print(event.title) ``` # Notes - You may utilize any helper functions or classes as needed. - Ensure to handle any edge cases or potential errors, such as overlapping events or invalid times, gracefully.","solution":"from datetime import datetime from typing import List import heapq class Event: def __init__(self, title: str, start_time: datetime, end_time: datetime): self.title = title self.start_time = start_time self.end_time = end_time def __lt__(self, other): return self.start_time < other.start_time class Scheduler: def __init__(self): self.events = [] def add_event(self, event: Event) -> None: for e in self.events: if not (event.end_time <= e.start_time or event.start_time >= e.end_time): raise ValueError(\\"Event times overlap with an existing event.\\") heapq.heappush(self.events, event) def remove_event(self, event: Event) -> None: self.events.remove(event) heapq.heapify(self.events) def get_upcoming_events(self, current_time: datetime, n: int) -> List[Event]: upcoming_events = [event for event in self.events if event.start_time >= current_time] return heapq.nsmallest(n, upcoming_events) def list_events_by_day(self, day: datetime) -> List[Event]: day_start = datetime(day.year, day.month, day.day, 0, 0, 0) day_end = datetime(day.year, day.month, day.day, 23, 59, 59) return [event for event in self.events if day_start <= event.start_time <= day_end]"},{"question":"**Objective:** Implement functions to manipulate and transform collections using operations and techniques highlighted in the documentation. **Question:** You are provided with a list of words. Your task is to perform several operations on this list and return the required results. Implement the following functions: 1. **`remove_duplicates(words: List[str]) -> List[str]`**: - Removes duplicate words from the list, preserving the original order. - Input: `[\'apple\', \'banana\', \'apple\', \'orange\', \'banana\', \'grape\']` - Output: `[\'apple\', \'banana\', \'orange\', \'grape\']` 2. **`word_frequencies(words: List[str]) -> Dict[str, int]`**: - Counts the frequency of each word in the list and returns a dictionary with words as keys and their frequencies as values. - Input: `[\'orange\', \'apple\', \'apple\', \'banana\', \'apple\', \'banana\']` - Output: `{\'orange\': 1, \'apple\': 3, \'banana\': 2}` 3. **`sort_words_by_frequency(words: List[str]) -> List[str]`**: - Sorts the words first by frequency in descending order, and then alphabetically for words with the same frequency. - Input: `[\'orange\', \'apple\', \'banana\', \'banana\', \'apple\', \'apple\']` - Output: `[\'apple\', \'banana\', \'orange\']` 4. **`transpose_matrix(matrix: List[List[int]]) -> List[List[int]]`**: - Transposes a given 2D matrix (list of lists). - Input: ``` [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] ``` - Output: ``` [ [1, 4, 7], [2, 5, 8], [3, 6, 9] ] ``` **Constraints:** - You may assume input lists have at most 1000 elements. - The input matrix for transposing will always be well-formed (i.e., no jagged arrays). Implement these functions with proper error handling and efficiency considerations. **Input and Output Formats:** - `List[str]` and `Dict[str, int]` for functions 1, 2, and 3. - `List[List[int]]` for function 4. **Performance Requirements:** Ensure that your implementations are efficient and can handle the upper constraint limits comfortably. **Example Usage:** ```python words = [\'apple\', \'banana\', \'apple\', \'orange\', \'banana\', \'grape\'] print(remove_duplicates(words)) # Output: [\'apple\', \'banana\', \'orange\', \'grape\'] words = [\'orange\', \'apple\', \'apple\', \'banana\', \'apple\', \'banana\'] print(word_frequencies(words)) # Output: {\'orange\': 1, \'apple\': 3, \'banana\': 2} words = [\'orange\', \'apple\', \'banana\', \'banana\', \'apple\', \'apple\'] print(sort_words_by_frequency(words)) # Output: [\'apple\', \'banana\', \'orange\'] matrix = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] print(transpose_matrix(matrix)) # Output: [ # [1, 4, 7], # [2, 5, 8], # [3, 6, 9] # ] ```","solution":"from typing import List, Dict def remove_duplicates(words: List[str]) -> List[str]: Removes duplicate words from the list, preserving the original order. seen = set() result = [] for word in words: if word not in seen: seen.add(word) result.append(word) return result def word_frequencies(words: List[str]) -> Dict[str, int]: Counts the frequency of each word in the list and returns a dictionary with words as keys and their frequencies as values. freq = {} for word in words: if word in freq: freq[word] += 1 else: freq[word] = 1 return freq def sort_words_by_frequency(words: List[str]) -> List[str]: Sorts the words first by frequency in descending order, and then alphabetically for words with the same frequency. freq = word_frequencies(words) sorted_words = sorted(freq.keys(), key=lambda word: (-freq[word], word)) return sorted_words def transpose_matrix(matrix: List[List[int]]) -> List[List[int]]: Transposes a given 2D matrix (list of lists). transposed = list(map(list, zip(*matrix))) return transposed"},{"question":"**Pandas Copy-on-Write Mechanism Assessment** This question tests your understanding of the Copy-on-Write (CoW) mechanism in pandas and your ability to manipulate DataFrames correctly under CoW constraints. # Problem Statement You are provided with financial data of a company for two quarters in the form of two separate DataFrames. Your task is to merge these DataFrames and perform a series of transformations and updates ensuring that none of your operations lead to unintentional side-effects due to shared data between DataFrames or Series objects, as per the principles of CoW in pandas. # Input: 1. `df_q1`: A DataFrame containing the financial data for Q1. 2. `df_q2`: A DataFrame containing the financial data for Q2. ```python import pandas as pd df_q1 = pd.DataFrame({ \\"month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\"], \\"revenue\\": [20000, 22000, 25000], \\"profit\\": [3000, 3200, 4000] }) df_q2 = pd.DataFrame({ \\"month\\": [\\"Apr\\", \\"May\\", \\"Jun\\"], \\"revenue\\": [26000, 27000, 30000], \\"profit\\": [5000, 5500, 6000] }) ``` # Requirements: 1. Merge `df_q1` and `df_q2` into a single DataFrame `df_half_year` in such a way that modifications to `df_half_year` do not affect the original DataFrames (`df_q1` and `df_q2`). 2. Add a new column `cumulative_revenue` to `df_half_year` that contains the cumulative revenue by the end of each month. 3. Update the `profit` column to include a 10% growth, but ensure that `df_q1` and `df_q2` remain unchanged. 4. Return `df_half_year`, `df_q1`, and `df_q2` to verify that there are no side-effects in original DataFrames. # Output: The function should return a tuple containing three DataFrames: `(df_half_year, df_q1, df_q2)`. # Constraints: * Ensure that merging does not unintentionally affect the original DataFrames. * Implement the updates in a manner compliant with CoW principles. # Example: ```python def merge_and_transform(df_q1, df_q2): Merges the two DataFrames and performs transformations while ensuring the original DataFrames are not modified. # Your code here df_half_year, df_q1_after, df_q2_after = merge_and_transform(df_q1, df_q2) print(df_half_year) print(df_q1_after) print(df_q2_after) ``` Expected Output: ```python month revenue profit cumulative_revenue 0 Jan 20000 3300 20000 1 Feb 22000 3520 42000 2 Mar 25000 4400 67000 3 Apr 26000 5500 93000 4 May 27000 6050 120000 5 Jun 30000 6600 150000 Original df_q1: month revenue profit 0 Jan 20000 3000 1 Feb 22000 3200 2 Mar 25000 4000 Original df_q2: month revenue profit 0 Apr 26000 5000 1 May 27000 5500 2 Jun 30000 6000 ``` Make sure to test your implementation to ensure it behaves as required.","solution":"import pandas as pd def merge_and_transform(df_q1, df_q2): Merges the two DataFrames and performs transformations while ensuring the original DataFrames are not modified. # Copy the input DataFrames to prevent any modification to the original ones df_q1_copy = df_q1.copy() df_q2_copy = df_q2.copy() # Merge the DataFrames df_half_year = pd.concat([df_q1_copy, df_q2_copy], ignore_index=True) # Add a cumulative revenue column df_half_year[\'cumulative_revenue\'] = df_half_year[\'revenue\'].cumsum() # Update the profit column to include 10% growth df_half_year[\'profit\'] = df_half_year[\'profit\'] * 1.10 return df_half_year, df_q1, df_q2"},{"question":"As an aspiring data analyst, you need to analyze and visualize the distribution of various features in a dataset. Use the seaborn package to explore and create insightful visualizations. Given the famous \\"penguins\\" dataset from seaborn, write a Python function **`visualize_distributions`** that takes the following parameters: - **dataset**: A pandas dataframe representing the dataset to be visualized. - **features**: A list of strings representing the column names of numerical features to visualize. - **condition** (optional): A string representing the column name of a categorical feature to condition the visualization on. - **binwidth** (optional): An integer specifying the bin width for histograms. - **bandwidth_adjust** (optional): A float specifying the bandwidth adjustment for KDE plots, default is 1. - **continuous_features**: A tuple containing two names of columns representing continuous features for bivariate distribution. The function should: 1. Plot univariate histograms for each feature in the `features` list. Use a default bin size, unless `binwidth` is specified. 2. Plot KDEs for each feature in the `features` list with the specified `bandwidth_adjust`. 3. If `condition` is provided, use it to create conditional distributions in the plots using different colors. 4. Create a bivariate distribution plot for the columns specified in `continuous_features` using histograms and KDEs. 5. Draw joint plots for the pair of features specified in `continuous_features`. 6. Return a list of all the created plots. **Example:** ```python import seaborn as sns import pandas as pd # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Function call visualize_distributions( dataset=penguins, features=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"], condition=\\"species\\", binwidth=5, bandwidth_adjust=0.5, continuous_features=(\\"bill_length_mm\\", \\"bill_depth_mm\\") ) ``` **Constraints:** - You must use seaborn for all visualizations. - The plots should be returned as seaborn plot objects. - Handle any missing values in the dataset gracefully. ```python def visualize_distributions(dataset: pd.DataFrame, features: list, condition: str = None, binwidth: int = None, bandwidth_adjust: float = 1, continuous_features: tuple = None): # Your implementation here pass ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_distributions(dataset: pd.DataFrame, features: list, condition: str = None, binwidth: int = None, bandwidth_adjust: float = 1, continuous_features: tuple = None): plots = [] # Handle missing values dataset = dataset.dropna(subset=features) if condition: dataset = dataset.dropna(subset=[condition]) if continuous_features: dataset = dataset.dropna(subset=list(continuous_features)) # Plot univariate histograms and KDEs for each feature for feature in features: fig, axes = plt.subplots(2, 1, figsize=(8, 12)) if condition: sns.histplot(data=dataset, x=feature, hue=condition, binwidth=binwidth, ax=axes[0]) sns.kdeplot(data=dataset, x=feature, hue=condition, bw_adjust=bandwidth_adjust, ax=axes[1]) else: sns.histplot(data=dataset, x=feature, binwidth=binwidth, ax=axes[0]) sns.kdeplot(data=dataset, x=feature, bw_adjust=bandwidth_adjust, ax=axes[1]) axes[0].set_title(f\'Histogram of {feature}\') axes[1].set_title(f\'KDE of {feature}\') plots.append(fig) # Plot bivariate distribution plot for continuous features if continuous_features: fig = sns.jointplot(data=dataset, x=continuous_features[0], y=continuous_features[1], kind=\\"scatter\\") fig.plot_joint(sns.kdeplot, bw_adjust=bandwidth_adjust) fig.plot_joint(sns.histplot, bins=binwidth) if binwidth else fig.plot_joint(sns.histplot) plots.append(fig) return plots"},{"question":"**Objective:** Implement a thread-safe counter using the `_thread` module, demonstrating your understanding of low-level threading primitives. Problem Statement: You are required to implement a class named `ThreadSafeCounter` that provides a thread-safe counter with functionality to increment, decrement, and retrieve the current count. You must ensure that operations on the counter are atomic, meaning no race conditions should occur even when accessed by multiple threads concurrently. Requirements: 1. **Class:** `ThreadSafeCounter` 2. **Methods:** - `__init__`: Initializes the counter and the necessary lock object. - `increment(self, n=1)`: Increments the counter by `n`. The default value of `n` is 1. - `decrement(self, n=1)`: Decrements the counter by `n`. The default value of `n` is 1. - `get_count(self)`: Returns the current value of the counter. 3. Ensure the methods `increment`, `decrement`, and `get_count` are thread-safe. 4. Use the `_thread.allocate_lock()` method for synchronization. Constraints: - The counter operations should not lead to any race conditions. - The counter value can be any integer, including negative values. Example Usage: ```python import _thread import time class ThreadSafeCounter: def __init__(self): self._count = 0 self._lock = _thread.allocate_lock() def increment(self, n=1): with self._lock: self._count += n def decrement(self, n=1): with self._lock: self._count -= n def get_count(self): with self._lock: return self._count def worker(counter, increment, times): for _ in range(times): if increment: counter.increment() else: counter.decrement() counter = ThreadSafeCounter() _thread.start_new_thread(worker, (counter, True, 1000)) _thread.start_new_thread(worker, (counter, False, 1000)) time.sleep(1) # Allow threads to finish their operations print(counter.get_count()) # Should print 0 if increments and decrements-1000 each ``` # Notes: - The `worker` function increments or decrements the counter a given number of times to simulate concurrent access. - Test your implementation thoroughly to ensure thread safety.","solution":"import _thread class ThreadSafeCounter: def __init__(self): self._count = 0 self._lock = _thread.allocate_lock() def increment(self, n=1): with self._lock: self._count += n def decrement(self, n=1): with self._lock: self._count -= n def get_count(self): with self._lock: return self._count"},{"question":"**Objective:** The goal of this task is to test your understanding of Python\'s built-in functions, particularly focusing on asynchronous iteration, attribute manipulation, and dynamic code execution. **Problem Statement:** You are required to implement a class `AsyncProcessor` that processes items from an asynchronous iterable, dynamically evaluates and sets attributes based on the processed items, and reports the results using both synchronous and asynchronous methods. **Instructions:** 1. Implement the class `AsyncProcessor` with the following methods: - `__init__(self)`: Initializes an empty list `processed_data` to store processed items. - `async def process_items(self, async_iterable)`: This method should asynchronously iterate over the provided `async_iterable`, process each item by squaring it, and add the processed item to `processed_data`. - `def set_attributes(self)`: This method should dynamically set attributes on the instance based on the `processed_data`. Each attribute name should be `item_<index>` where `<index>` is the index of the item in the list, and its value should be the corresponding item. - `def eval_expression(self, expression)`: This method should evaluate the provided `expression` string using the instance’s attributes and return the result. 2. Implement an asynchronous iterable class `AsyncIterable` that: - Has an `__init__(self, data)` method to accept a list of data. - Implements the asynchronous iteration protocol with `__aiter__(self)` and `__anext__(self)` methods. **Example Usage:** ```python import asyncio class AsyncIterable: def __init__(self, data): self.data = data self.index = 0 def __aiter__(self): return self async def __anext__(self): if self.index < len(self.data): item = self.data[self.index] self.index += 1 return item else: raise StopAsyncIteration class AsyncProcessor: def __init__(self): self.processed_data = [] async def process_items(self, async_iterable): async for item in async_iterable: self.processed_data.append(item * 2) def set_attributes(self): for index, value in enumerate(self.processed_data): setattr(self, f\'item_{index}\', value) def eval_expression(self, expression): return eval(expression, vars(self)) async def main(): data = [1, 2, 3] async_iterable = AsyncIterable(data) processor = AsyncProcessor() await processor.process_items(async_iterable) processor.set_attributes() # Dynamically evaluate an expression using instance attributes result = processor.eval_expression(\'item_0 + item_1 + item_2\') print(result) # Expected output: 12 (since 2*1 + 2*2 + 2*3 = 2 + 4 + 6 = 12) asyncio.run(main()) ``` **Constraints:** - The provided asynchronous iterable, `AsyncIterable`, must contain integer values. - Ensure the `eval_expression` method safely evaluates the expression without exposing security vulnerabilities. It should only access attributes of the current instance. **Requirements:** - Use `aiter()` and `anext()` for asynchronous iteration. - Use `setattr()` to dynamically set attributes. - Use `eval()` for dynamic expression evaluation with appropriate safeguards. **Submission:** Submit a Python file containing the `AsyncIterable` and `AsyncProcessor` classes and the asynchronous `main()` function demonstrating the usage, as shown in the example.","solution":"class AsyncIterable: def __init__(self, data): self.data = data self.index = 0 def __aiter__(self): return self async def __anext__(self): if self.index < len(self.data): item = self.data[self.index] self.index += 1 return item else: raise StopAsyncIteration class AsyncProcessor: def __init__(self): self.processed_data = [] async def process_items(self, async_iterable): async for item in async_iterable: self.processed_data.append(item * item) # Square the item def set_attributes(self): for index, value in enumerate(self.processed_data): setattr(self, f\'item_{index}\', value) def eval_expression(self, expression): return eval(expression, {\\"__builtins__\\": {}}, vars(self)) async def main(): data = [1, 2, 3] async_iterable = AsyncIterable(data) processor = AsyncProcessor() await processor.process_items(async_iterable) processor.set_attributes() # Dynamically evaluate an expression using instance attributes result = processor.eval_expression(\'item_0 + item_1 + item_2\') print(result) # Expected output: 14 (since 1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14) # Assuming the main function needs to be run import asyncio asyncio.run(main())"},{"question":"Objective: To assess your understanding of the `seaborn` library, specifically focusing on the usage of the `PairGrid` class to create complex, multidimensional plots. Task: Using the seaborn `PairGrid` class, you are required to create a visual grid to explore the relationships between different numerical variables in the **\\"iris\\"** dataset. Requirements: 1. **Load the `iris` dataset** using seaborn\'s built-in function. 2. **Initialize a `PairGrid` object** with the `iris` dataset. 3. On the diagonal, **map a kernel density estimate plot** using `sns.kdeplot`. 4. On the off-diagonal, **map a scatter plot** using `sns.scatterplot`. 5. Use the `hue` parameter to differentiate the species in the dataset. 6. Add a legend to the `PairGrid`. Input: None. The `iris` dataset should be loaded internally within the function. Output: A `PairGrid` plot should be displayed that meets the above specifications. Constraints: - Ensure each step is clearly commented to demonstrate your understanding. - The resulting plot should be clear and aesthetically pleasing. Example: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Initialize the PairGrid object g = sns.PairGrid(iris, hue=\\"species\\") # Map the plots to the grid g.map_diag(sns.kdeplot) g.map_offdiag(sns.scatterplot) # Add a legend g.add_legend() # Display the plot plt.show() ``` ✏️ **HINT:** You can refer to the seaborn documentation provided to understand how each method is used. It\'s important to check the syntax and ensure all parameters are used correctly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_pairgrid_plot(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Initialize the PairGrid object g = sns.PairGrid(iris, hue=\\"species\\") # Map the plots to the grid g.map_diag(sns.kdeplot) g.map_offdiag(sns.scatterplot) # Add a legend g.add_legend() # Display the plot plt.show()"},{"question":"# Question: Advanced List Manipulation and Operations You are tasked to implement a function named `process_lists` that takes in two lists of integers and returns a dictionary with the following keys and values: * `\\"merged\\"`: A list containing all elements from both input lists, merged and sorted in ascending order, without duplicates. * `\\"intersection\\"`: A list containing elements that are present in both input lists. * `\\"prime_counts\\"`: A tuple where the first element is the count of prime numbers in the first list and the second element is the count of prime numbers in the second list. * `\\"common_diffs\\"`: A list of absolute differences between corresponding elements of the two lists (only for positions where both lists have elements). **Function Signature:** ```python def process_lists(list1: List[int], list2: List[int]) -> Dict[str, Union[List[int], Tuple[int, int]]]: ``` **Input:** * `list1`: A list of integers (`0 <= len(list1) <= 1000`). * `list2`: A list of integers (`0 <= len(list2) <= 1000`). **Output:** * A dictionary with the keys `\\"merged\\"`, `\\"intersection\\"`, `\\"prime_counts\\"`, and `\\"common_diffs\\"`. **Constraints:** 1. Implement efficient algorithms to minimize time complexity, especially for checking prime numbers. **Examples:** ```python assert process_lists([1, 2, 3, 5], [3, 4, 5, 7]) == { \\"merged\\": [1, 2, 3, 4, 5, 7], \\"intersection\\": [3, 5], \\"prime_counts\\": (3, 3), \\"common_diffs\\": [2, 2, 2, 2] } assert process_lists([11, 13, 17], [11, 17, 19]) == { \\"merged\\": [11, 13, 17, 19], \\"intersection\\": [11, 17], \\"prime_counts\\": (3, 3), \\"common_diffs\\": [0, 4] } assert process_lists([], []) == { \\"merged\\": [], \\"intersection\\": [], \\"prime_counts\\": (0, 0), \\"common_diffs\\": [] } ``` **Notes:** - You may write helper functions to keep your code organized. - Remember that prime numbers are greater than 1 and have no divisors other than 1 and themselves. **Hints:** - To check for prime numbers efficiently, you might use the Sieve of Eratosthenes or trial division up to the square root of the number. - Utilize Python\'s built-in set operations for finding intersections and merging while removing duplicates. **Assessment Criteria:** - Correctness of the solution. - Efficient handling of operations and algorithms for prime checking and list processing. - Code readability and proper documentation.","solution":"from typing import List, Dict, Union, Tuple from math import isqrt def is_prime(n: int) -> bool: Return True if n is a prime number, else False. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False for i in range(5, isqrt(n) + 1, 6): if n % i == 0 or n % (i + 2) == 0: return False return True def count_primes(lst: List[int]) -> int: Count the number of prime numbers in a given list. return sum(1 for x in lst if is_prime(x)) def process_lists(list1: List[int], list2: List[int]) -> Dict[str, Union[List[int], Tuple[int, int]]]: merged = sorted(set(list1) | set(list2)) intersection = sorted(set(list1) & set(list2)) prime_counts = (count_primes(list1), count_primes(list2)) common_diffs = [abs(a - b) for a, b in zip(list1, list2)] return { \\"merged\\": merged, \\"intersection\\": intersection, \\"prime_counts\\": prime_counts, \\"common_diffs\\": common_diffs }"},{"question":"# Question You are tasked with developing a utility function to handle binary data interchange using the `struct` module in Python. Your utility will involve packing a list of values into a binary format and then unpacking it back to its original form. # Objective: Write two functions, `pack_values` and `unpack_values`, to perform the following tasks: 1. **`pack_values(format_string: str, values: list) -> bytes`**: - This function should take a format string and a list of values as input. - It should pack the values into a bytes object according to the given format string. - Return the packed bytes object. 2. **`unpack_values(format_string: str, packed_data: bytes) -> tuple`**: - This function should take a format string and a bytes object. - It should unpack the bytes object according to the given format string. - Return the unpacked values as a tuple. # Input Constraints: - The `format_string` will be a valid `struct` format string. - The `values` list will match exactly the number of elements as required by the `format_string`. - The `packed_data` will be a bytes object generated by `pack_values` with the corresponding `format_string`. # Output: - `pack_values` should return a bytes object. - `unpack_values` should return a tuple of unpacked values. # Example Usage: ```python # Example 1 format_str = \\">I4sh\\" values = [1, b\'test\', 2] packed = pack_values(format_str, values) print(packed) # Outputs the packed bytes object unpacked = unpack_values(format_str, packed) print(unpacked) # Outputs: (1, b\'test\', 2) # Example 2 format_str = \\"<3f\\" values = [1.0, 2.0, 3.0] packed = pack_values(format_str, values) print(packed) # Outputs the packed bytes object unpacked = unpack_values(format_str, packed) print(unpacked) # Outputs: (1.0, 2.0, 3.0) ``` # Your Implementation Here: ```python import struct def pack_values(format_string: str, values: list) -> bytes: # Implement the packing logic pass def unpack_values(format_string: str, packed_data: bytes) -> tuple: # Implement the unpacking logic pass ``` Ensure your functions are efficient and handle the format string and values accurately according to the constraints provided.","solution":"import struct def pack_values(format_string: str, values: list) -> bytes: Packs a list of values into a binary format according to the given format string. Args: - format_string (str): The format string defining the structure. - values (list): The list of values to pack. Returns: - bytes: The packed binary data. packed_data = struct.pack(format_string, *values) return packed_data def unpack_values(format_string: str, packed_data: bytes) -> tuple: Unpacks binary data into a tuple of values according to the given format string. Args: - format_string (str): The format string defining the structure. - packed_data (bytes): The binary data to unpack. Returns: - tuple: The unpacked values. unpacked_data = struct.unpack(format_string, packed_data) return unpacked_data"},{"question":"**Question: Implement and Evaluate a Multi-output Decision Tree** # Problem Statement You are given a dataset where each input sample has multiple output targets. Your task is to build a multi-output decision tree using scikit-learn, handle any missing data appropriately, and evaluate its performance. # Dataset For this problem, you will use the fictitious `MultiOutputDataset.csv` which has the following structure: - Input features: `feature1`, `feature2`, `feature3`, ... - Output targets: `target1`, `target2` - Potential missing values in both input and output fields # Tasks 1. **Data Preprocessing**: - Load the dataset. - Handle missing values as described in the provided documentation. 2. **Model Development**: - Implement a multi-output decision tree regressor using `DecisionTreeRegressor`. - Use appropriate parameters to avoid overfitting (e.g., `max_depth`, `min_samples_split`). 3. **Model Evaluation**: - Use mean squared error (MSE) to evaluate the model\'s performance on both the training and test sets. - Implement pruning using `ccp_alpha` to improve generalizability and avoid overfitting. 4. **Visualization**: - Visualize the decision tree using `plot_tree`. 5. **Performance Comparison**: - Train and evaluate a single-output decision tree regressor for each target separately. - Compare the performance of the multi-output regressor and the individual regressors. # Input 1. Path to `MultiOutputDataset.csv` file 2. Parameters for decision tree (e.g., `max_depth`, `min_samples_split`, `ccp_alpha`) # Output 1. Mean squared error for training and test sets. 2. Visualized multi-output decision tree. 3. Performance comparison of multi-output and single-output regressors in tabular format. # Constraints and Limitations - Assume the dataset fits into memory. - Hyperparameters should be reasonably set to avoid overfitting. - Utilize appropriate methods to handle and visualize missing data as described in the documentation. # Example ```python import numpy as np import pandas as pd from sklearn.tree import DecisionTreeRegressor, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error # Load dataset data = pd.read_csv(\'MultiOutputDataset.csv\') # Handle missing values (Example: Fill missing values with column means) data.fillna(data.mean(), inplace=True) # Split data into inputs and targets X = data[[\'feature1\', \'feature2\', \'feature3\']] y = data[[\'target1\', \'target2\']] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train a multi-output decision tree regressor multi_output_tree = DecisionTreeRegressor(max_depth=10, min_samples_split=5, ccp_alpha=0.01) multi_output_tree.fit(X_train, y_train) # Predict and evaluate on the test set y_pred = multi_output_tree.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\'Mean Squared Error: {mse}\') # Visualize the decision tree plot_tree(multi_output_tree) ``` **Note**: Use this template code as a starting point and ensure to implement all the required functionalities comprehensively.","solution":"import numpy as np import pandas as pd from sklearn.tree import DecisionTreeRegressor, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt def preprocess_data(path): # Load dataset data = pd.read_csv(path) # Handle missing values (Example: Fill missing values with column means) data.fillna(data.mean(), inplace=True) # Split data into inputs and targets X = data.iloc[:, :-2] y = data.iloc[:, -2:] return X, y def train_multi_output_tree(X, y, max_depth=10, min_samples_split=5, ccp_alpha=0.01): # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train a multi-output decision tree regressor multi_output_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split, ccp_alpha=ccp_alpha) multi_output_tree.fit(X_train, y_train) # Predict and evaluate on the test set y_pred_train = multi_output_tree.predict(X_train) y_pred_test = multi_output_tree.predict(X_test) mse_train = mean_squared_error(y_train, y_pred_train) mse_test = mean_squared_error(y_test, y_pred_test) return multi_output_tree, mse_train, mse_test, X_test, y_test, y_pred_test def visualize_tree(multi_output_tree): plt.figure(figsize=(20,10)) plot_tree(multi_output_tree, filled=True) plt.show() def single_output_trees_performance(X, y, max_depth=10, min_samples_split=5, ccp_alpha=0.01): # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train single-output decision tree for each target separately and evaluate performance mse_train_list = [] mse_test_list = [] for target in y.columns: tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_split=min_samples_split, ccp_alpha=ccp_alpha) tree.fit(X_train, y_train[target]) y_pred_train = tree.predict(X_train) y_pred_test = tree.predict(X_test) mse_train_list.append(mean_squared_error(y_train[target], y_pred_train)) mse_test_list.append(mean_squared_error(y_test[target], y_pred_test)) return mse_train_list, mse_test_list # Example Usage: # X, y = preprocess_data(\'MultiOutputDataset.csv\') # multi_output_tree, mse_train, mse_test, X_test, y_test, y_pred_test = train_multi_output_tree(X, y) # visualize_tree(multi_output_tree) # single_mse_train, single_mse_test = single_output_trees_performance(X, y) # print(\'Multi-output MSE:\', mse_train, mse_test) # print(\'Single-output MSE:\', single_mse_train, single_mse_test)"},{"question":"**Objective**: Implement a function `process_data` that demonstrates the use of different compound statements and pattern matching for data processing. Problem Description You are required to implement a function `process_data(data)`. The function takes a list of dictionaries as input, where each dictionary represents an item with the following fields: `type`, `value`, and `metadata`. The `type` can be one of the following strings: `\\"A\\"`, `\\"B\\"`, or `\\"C\\"`. The `value` is an integer and `metadata` is an optional field containing additional information. The function should: 1. Use a `for` loop to iterate over each item in the input list. 2. Use a `match` statement to handle different types: - If the type is `\\"A\\"`, double the value. - If the type is `\\"B\\"`, triple the value. - If the type is `\\"C\\"`, set the value to the square of the value. - If the type is not recognized, raise a `ValueError`. 3. Use a `try-except` block to catch and handle any `ValueError`, logging an appropriate error message using the `print` function. 4. If the item has `metadata`, use an `if` statement to check if a key `\\"flag\\"` is set to `True` and, if so, add 10 to the value. 5. Return a list of the processed items, where each item is a dictionary with the updated value. Function Signature ```python def process_data(data: list[dict]) -> list[dict]: pass ``` Input - `data`: A list of dictionaries, where each dictionary has the following structure: ```python { \\"type\\": str, # Can be either \\"A\\", \\"B\\", or \\"C\\" \\"value\\": int, # An integer value \\"metadata\\": dict # Optional field, dictionary containing additional information } ``` Output - A list of dictionaries with updated values. Constraints - The `type` field will always be a string. - The `value` field will always be an integer. - The `metadata` field is optional and, if present, will always be a dictionary with at least the \\"flag\\" key as a boolean. Example ```python input_data = [ {\\"type\\": \\"A\\", \\"value\\": 5, \\"metadata\\": {\\"flag\\": True}}, {\\"type\\": \\"B\\", \\"value\\": 3, \\"metadata\\": {}}, {\\"type\\": \\"C\\", \\"value\\": 4}, {\\"type\\": \\"D\\", \\"value\\": 2} # This should cause a ValueError ] output_data = process_data(input_data) print(output_data) # Output should be: # [{\'type\': \'A\', \'value\': 20, \'metadata\': {\'flag\': True}}, {\'type\': \'B\', \'value\': 9, \'metadata\': {}}, {\'type\': \'C\', \'value\': 16}] # And should log an error message for the item of type \\"D\\". ``` You must demonstrate the use of different compound statements (if, for, try, match) and proper exception handling in your implementation.","solution":"def process_data(data): Process a list of dictionaries based on type and update the value accordingly. processed_data = [] for item in data: try: match item[\\"type\\"]: case \\"A\\": item[\\"value\\"] *= 2 case \\"B\\": item[\\"value\\"] *= 3 case \\"C\\": item[\\"value\\"] = item[\\"value\\"] ** 2 case _: raise ValueError(f\\"Unrecognized type: {item[\'type\']}\\") if \\"metadata\\" in item and item[\\"metadata\\"].get(\\"flag\\", False): item[\\"value\\"] += 10 processed_data.append(item) except ValueError as e: print(e) return processed_data"},{"question":"**Advanced Python C Extension - Tuple and Struct Sequence Manipulation** **Problem Statement:** You are required to create a Python C extension module that provides a set of functions to manipulate Python tuples and struct sequences. The goal is to demonstrate your understanding of the fundamental and advanced manipulation of these data structures using the provided C API. **Requirements:** 1. **Create a new tuple and populate it with integers (from 1 to n).** - Function: `PyObject* create_integer_tuple(Py_ssize_t n)` - Inputs: - `n` (Py_ssize_t): Size of the tuple. - Outputs: - Returns a new tuple containing integers from 1 to `n`. 2. **Resize an existing tuple to a new specified size.** - Function: `int resize_tuple(PyObject** p, Py_ssize_t newsize)` - Inputs: - `p` (PyObject**): Pointer to the original tuple. - `newsize` (Py_ssize_t): New size for the tuple. - Outputs: - Returns `0` on success or `-1` on failure. 3. **Create a struct sequence type and an instance, then populate it with specified fields.** - Function: `PyObject* create_struct_sequence(const char* type_name, Py_ssize_t n_fields, const char** field_names, PyObject** field_values)` - Inputs: - `type_name` (const char*): Name of the struct sequence type. - `n_fields` (Py_ssize_t): Number of fields in the struct sequence. - `field_names` (const char**): Array of field names. - `field_values` (PyObject**): Array of field values. - Outputs: - Returns a new struct sequence instance populated with the specified fields. **Example Usage:** 1. **Creating a Tuple:** ```python tuple = create_integer_tuple(5) # Expected Output: (1, 2, 3, 4, 5) ``` 2. **Resizing a Tuple:** ```python resize_result = resize_tuple(&tuple, 3) # Expected Output: (1, 2, 3) if successful ``` 3. **Creating a Struct Sequence:** ```python field_names = [\\"field1\\", \\"field2\\", \\"field3\\"] field_values = [PyLong_FromLong(10), PyLong_FromLong(20), PyLong_FromLong(30)] struct_seq = create_struct_sequence(\\"MyStructSeq\\", 3, field_names, field_values) # Expected Output: A struct sequence with fields: (10, 20, 30) ``` **Constraints:** - Proper error checking must be performed in all functions. - Ensure efficient memory management to prevent leaks. - Functions should follow Python C API conventions. - You are encouraged to use the provided API functions and macros for implementation. - Your implementation should be efficient and handle edge cases such as large tuples and invalid inputs gracefully. **Submission:** Submit the implementation of the above functions in a C file suitable for creating a Python extension module. Additionally, provide a Python script demonstrating the usage of the created extension module. **Performance Requirements:** - The operations should handle large tuples and struct sequences efficiently without significant performance degradation.","solution":"# Note: As this is a demonstration of the conceptual implementation required for a C extension # and involves direct Python C API usage, an actual runnable implementation won\'t be achieved # directly within a Python script. Below is the Python-equivalent representation of what the # C extension functions would be like. These would typically be found in a C source file. def create_integer_tuple(n): Create a new tuple and populate it with integers from 1 to n. if n < 0: raise ValueError(\\"Size of the tuple must be non-negative\\") return tuple(range(1, n + 1)) def resize_tuple(tup, newsize): Resize an existing tuple to a new specified size. if not isinstance(tup, tuple): raise TypeError(\\"First argument must be a tuple\\") if newsize < 0: raise ValueError(\\"New size must be non-negative\\") return tup[:newsize] def create_struct_sequence(type_name, n_fields, field_names, field_values): Create a struct sequence with specified fields and values. if not isinstance(n_fields, int) or n_fields < 0: raise ValueError(\\"Number of fields must be a non-negative integer\\") if len(field_names) != n_fields or len(field_values) != n_fields: raise ValueError(\\"Field names and values must match the number of fields\\") if any(not isinstance(fname, str) for fname in field_names): raise TypeError(\\"Field names must be strings\\") result = {name: value for name, value in zip(field_names, field_values)} return type(type_name, (object,), result)"},{"question":"Title: Optimizing a Non-negative Matrix Factorization (NMF) implementation using scikit-learn Problem Statement You are provided with a dataset of handwritten digits and your task is to apply Non-negative Matrix Factorization (NMF) on this dataset. The goal is to: 1. Implement the NMF algorithm using scikit-learn\'s built-in functionality. 2. Profile the initial implementation to identify performance bottlenecks. 3. Optimize the implementation to improve its performance. Detailed Instructions 1. **Load Dataset**: Use `load_digits` from `sklearn.datasets` to load the dataset. ```python from sklearn.datasets import load_digits X, _ = load_digits(return_X_y=True) ``` 2. **Implement NMF**: Implement NMF using scikit-learn\'s `NMF` class. ```python from sklearn.decomposition import NMF ``` Fit the NMF model on the dataset with the following parameters: - `n_components`: 16 - `tol`: 1e-2 Measure the total execution time using IPython\'s `%timeit`. ```python %timeit NMF(n_components=16, tol=1e-2).fit(X) ``` 3. **Profile the Code**: Use IPython\'s `%prun` magic command to profile the execution of the NMF fitting process and identify hotspots. ```python %prun -l nmf.py NMF(n_components=16, tol=1e-2).fit(X) ``` 4. **Optimize the Code**: Identify the most time-consuming parts from the profiling results. Optimize the identified bottlenecks. You can use techniques mentioned in the documentation such as using Cython for critical sections or applying algorithmic improvements. 5. **Parallelize the Computation**: Utilize `joblib.Parallel` for any applicable coarse-grained parallelism to further speed up the computation. Expected Input and Output - **Input**: None (Directly use the `load_digits` dataset). - **Output**: The optimized implementation of the NMF algorithm should fit the model faster than the initial implementation. Constraints - Use only Python, NumPy, SciPy, Cython, and scikit-learn libraries. - Focus on readability and maintainability of the code. - Ensure that the optimized implementation produces results consistent with the initial implementation. Performance Requirements - The optimized NMF implementation should show a noticeable performance improvement (at least 20% faster) as compared to the initial implementation. Hints - Review the detailed profiling output to pinpoint the exact lines of code or functions that consume the most time. - You may use `line_profiler` and `memory_profiler` for more granular profiling. - Consider whether parts of the NMF algorithm can be parallelized. Submission Submit a Jupyter notebook (`.ipynb` file) containing: - Your initial unoptimized code implementation. - Profiling results. - Optimized code implementation. - Demonstrations showing performance improvements.","solution":"from sklearn.datasets import load_digits from sklearn.decomposition import NMF import time # Load dataset X, _ = load_digits(return_X_y=True) # Implement NMF nmf = NMF(n_components=16, tol=1e-2) # Measure the execution time start_time = time.time() nmf.fit(X) end_time = time.time() elapsed_time = end_time - start_time elapsed_time # Profiling the code would typically be done in an interactive environment like Jupyter Notebook # %timeit nmf.fit(X) # %prun -l nmf.py nmf.fit(X)"},{"question":"You are required to implement a function `efficient_copy` that efficiently copies data from one bytearray to another using memoryview objects. Additionally, you need to implement a function `verify_memoryview` that checks if the data in the memoryview has been copied correctly, ensuring both memoryviews are identical. Function 1: `efficient_copy(source, destination)` - **Input**: - `source` (bytearray): The source bytearray from which data is to be copied. - `destination` (bytearray): The destination bytearray to which data is to be copied. - **Output**: - None You must use `memoryview` objects to copy the data from `source` to `destination`. Function 2: `verify_memoryview(source, destination)` - **Input**: - `source` (bytearray): The source bytearray. - `destination` (bytearray): The destination bytearray. - **Output**: - `bool`: Returns `True` if the data in the memoryview of the source and destination are identical, otherwise `False`. # Requirements and Constraints - You must use `memoryview` objects for performing the copy operation. - You can assume the `source` and `destination` bytearrays are of the same size. - The function should perform the copy operation in a time-efficient manner, ideally O(n) where n is the size of the bytearray. # Example ```python source = bytearray(b\\"Hello World!\\") destination = bytearray(len(source)) efficient_copy(source, destination) print(destination) # Output: bytearray(b\'Hello World!\') is_correct = verify_memoryview(source, destination) print(is_correct) # Output: True ``` Provide implementations for both `efficient_copy` and `verify_memoryview` functions to solve the problem.","solution":"def efficient_copy(source, destination): Efficiently copies data from source to destination using memoryview objects. src_view = memoryview(source) dest_view = memoryview(destination) dest_view[:] = src_view[:] def verify_memoryview(source, destination): Verifies that the memoryview of the source and destination are identical. src_view = memoryview(source) dest_view = memoryview(destination) return src_view.tobytes() == dest_view.tobytes()"},{"question":"Objective You will demonstrate your understanding of Python\'s `multiprocessing.shared_memory` module by implementing and managing shared memory blocks and `ShareableList` objects across multiple processes. Task You need to implement a function `create_shared_counter_list(size, initial_value)` which performs the following steps: 1. Creates a `SharedMemoryManager` that manages shared memory blocks. 2. Initializes a `ShareableList` of the specified `size` and sets each element to the `initial_value`. 3. Spawns multiple processes that increment the value of each element in the `ShareableList` by 1. 4. Ensures proper cleanup of shared memory resources after the processes finish execution. Function Signature ```python def create_shared_counter_list(size: int, initial_value: int) -> List[int]: pass ``` Input - `size` (int): The number of elements in the `ShareableList`. - `initial_value` (int): The initial value to set for each element in the `ShareableList`. Output - A `List[int]` representing the final values stored within the `ShareableList` after all processes have incremented each element. Constraints - The function should handle the creation and cleanup of shared memory resources properly. - You can assume `size` and `initial_value` are positive integers. Performance Requirements - Ensure that the program properly manages the lifecycle of shared memory, avoiding memory leaks. - The solution should work efficiently even for larger values of `size`. Example ```python # Example usage final_values = create_shared_counter_list(5, 10) print(final_values) # Output should be [11, 11, 11, 11, 11] ``` Hints - Use the `SharedMemoryManager` to manage the lifecycle of shared memory blocks. - Use multiple processes to demonstrate inter-process communication and shared memory usage. - Ensure proper cleanup of resources using `close()` and `unlink()` where appropriate.","solution":"from multiprocessing import Process, shared_memory from multiprocessing.managers import SharedMemoryManager from multiprocessing.shared_memory import ShareableList def increment_shared_list(index, shared_list): Increment the value at the given index of the shared list by one. shared_list[index] += 1 def create_shared_counter_list(size, initial_value): Creates a shared counter list of given size. Each element initialized to initial_value and incremented by child process. with SharedMemoryManager() as smm: # Create a ShareableList shared_list = ShareableList([initial_value] * size, name=None) processes = [] for i in range(size): p = Process(target=increment_shared_list, args=(i, shared_list)) processes.append(p) p.start() for p in processes: p.join() result = list(shared_list) # Clean up shared list shared_list.shm.close() shared_list.shm.unlink() return result"},{"question":"# Sparse Tensors in PyTorch Objective: Demonstrate your understanding of various sparse tensor formats in PyTorch by constructing, converting, and operating on sparse tensors. This task will test your ability to handle different sparse formats, efficiently perform conversions, and execute operations like matrix multiplications in PyTorch. Problem Statement: You are provided with a dense tensor representing a graph adjacency matrix. Your task is to: 1. Convert this tensor into three different sparse formats: COO, CSR, and CSC. 2. Perform a specific set of matrix operations using these sparse tensors. 3. Convert the resulting sparse tensors back to dense format and verify the results for correctness. Task Details: 1. **Convert to Sparse Formats**: Given a 2D dense tensor `input_tensor`, convert it to: - COO (Coordinate format) - CSR (Compressed Sparse Row) - CSC (Compressed Sparse Column) 2. **Matrix Multiplication**: Multiply each sparse tensor by another dense matrix `multiplier_tensor` and obtain the results. Conduct these multiplications efficiently, leveraging the power of sparse tensors. The dimensions of `input_tensor` and `multiplier_tensor` are compatible for multiplication. 3. **Convert Back to Dense**: Convert the results of the multiplications back to dense format and verify that they are equivalent. ```python import torch def assess_sparse_tensors(input_tensor, multiplier_tensor): Args: - input_tensor (torch.Tensor): A 2D dense tensor of size (m, n) - multiplier_tensor (torch.Tensor): A 2D dense tensor of size (n, k) Returns: - dict: A dictionary with keys \'coo_result\', \'csr_result\', and \'csc_result\', each containing the result of the respective sparse format multiplication in dense format. # Convert input_tensor to sparse formats sparse_coo = input_tensor.to_sparse() sparse_csr = input_tensor.to_sparse_csr() sparse_csc = input_tensor.to_sparse_csc() # Perform matrix multiplications coo_result_sparse = torch.sparse.mm(sparse_coo, multiplier_tensor) csr_result_sparse = torch.sparse.mm(sparse_csr, multiplier_tensor) csc_result_sparse = torch.sparse.mm(sparse_csc, multiplier_tensor) # Convert results back to dense format coo_result_dense = coo_result_sparse.to_dense() csr_result_dense = csr_result_sparse.to_dense() csc_result_dense = csc_result_sparse.to_dense() # Verify all results are equal assert torch.allclose(coo_result_dense, csr_result_dense), \\"COO and CSR results do not match.\\" assert torch.allclose(csc_result_dense, csr_result_dense), \\"CSC and CSR results do not match.\\" return { \'coo_result\': coo_result_dense, \'csr_result\': csr_result_dense, \'csc_result\': csc_result_dense, } # Example usage input_tensor = torch.tensor([[0, 2], [3, 0]], dtype=torch.float32) multiplier_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) results = assess_sparse_tensors(input_tensor, multiplier_tensor) print(\\"COO Result:n\\", results[\'coo_result\']) print(\\"CSR Result:n\\", results[\'csr_result\']) print(\\"CSC Result:n\\", results[\'csc_result\']) ``` Constraints: 1. `input_tensor` and `multiplier_tensor` are 2D tensors with sizes (m, n) and (n, k), respectively. 2. Implement the conversion and multiplication operations efficiently to leverage the advantages of sparse tensors. Evaluation Criteria: - **Correctness**: All transformations and operations should be correctly implemented, maintaining the integrity and accuracy of the results. - **Efficiency**: Efficient use of PyTorch\'s sparse tensor functionalities. - **Code Quality**: Clean, readable, and well-documented code. Ensure your solution passes the example test case and handles various sparse tensor formats seamlessly.","solution":"import torch def assess_sparse_tensors(input_tensor, multiplier_tensor): Args: - input_tensor (torch.Tensor): A 2D dense tensor of size (m, n) - multiplier_tensor (torch.Tensor): A 2D dense tensor of size (n, k) Returns: - dict: A dictionary with keys \'coo_result\', \'csr_result\', and \'csc_result\', each containing the result of the respective sparse format multiplication in dense format. # Convert input_tensor to sparse formats sparse_coo = input_tensor.to_sparse() sparse_csr = input_tensor.to_sparse_csr() sparse_csc = input_tensor.to_sparse_csc() # Perform matrix multiplications coo_result_sparse = torch.sparse.mm(sparse_coo, multiplier_tensor) csr_result_sparse = torch.sparse.mm(sparse_csr, multiplier_tensor) csc_result_sparse = torch.sparse.mm(sparse_csc, multiplier_tensor) # Convert results back to dense format coo_result_dense = coo_result_sparse.to_dense() csr_result_dense = csr_result_sparse.to_dense() csc_result_dense = csc_result_sparse.to_dense() # Verify all results are equal assert torch.allclose(coo_result_dense, csr_result_dense), \\"COO and CSR results do not match.\\" assert torch.allclose(csc_result_dense, csr_result_dense), \\"CSC and CSR results do not match.\\" return { \'coo_result\': coo_result_dense, \'csr_result\': csr_result_dense, \'csc_result\': csc_result_dense, } # Example usage input_tensor = torch.tensor([[0, 2], [3, 0]], dtype=torch.float32) multiplier_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) results = assess_sparse_tensors(input_tensor, multiplier_tensor) print(\\"COO Result:n\\", results[\'coo_result\']) print(\\"CSR Result:n\\", results[\'csr_result\']) print(\\"CSC Result:n\\", results[\'csc_result\'])"},{"question":"Working with I/O Streams in Python **Objective:** Implement a function that reads binary data from a file, processes it, and writes the resulting data to a text file using specific encoding and buffer size. **Function Signature:** ```python def process_and_convert_file(binary_file_path: str, text_file_path: str, buffer_size: int = io.DEFAULT_BUFFER_SIZE, encoding: str = \\"utf-8\\") -> None: pass ``` **Task:** 1. Open the specified binary file using the given buffer size and read its entire content. 2. Process the binary data by converting it to a string of hexadecimal characters. 3. Open the specified text file in write mode, using the given encoding and buffer size, and write the processed string to this file. 4. Ensure that any necessary error handling for file operations (such as file not found, read/write errors) is implemented appropriately. **Input:** - `binary_file_path`: A string representing the path to the binary file that needs to be read. - `text_file_path`: A string representing the path to the text file where the processed data should be written. - `buffer_size`: An optional integer specifying the buffer size for both reading and writing operations. Default is `io.DEFAULT_BUFFER_SIZE`. - `encoding`: An optional string specifying the encoding for the text file. Default is \\"utf-8\\". **Example Usage:** ```python binary_file_path = \\"data.bin\\" text_file_path = \\"output.txt\\" process_and_convert_file(binary_file_path, text_file_path, buffer_size=1024, encoding=\\"utf-8\\") ``` **Constraints & Considerations:** - Your implementation should efficiently handle large files by utilizing the specified buffer size. - Handle any potential exceptions, such as file not found or read/write errors, to ensure the function does not crash unexpectedly. - Ensure proper file closure after operations, either by using context managers or explicitly closing the files. **Performance Requirements:** - The function should be able to handle large files efficiently by using the appropriate buffer size for reading and writing. - The code should be robust and handle any possible I/O errors gracefully. **Notes:** - You can assume that the binary file contains valid binary data and that there is enough disk space to write the text file. - The function does not return any value; it directly writes the processed data into the specified text file. Implement the function `process_and_convert_file` as described above.","solution":"import io def process_and_convert_file(binary_file_path: str, text_file_path: str, buffer_size: int = io.DEFAULT_BUFFER_SIZE, encoding: str = \\"utf-8\\") -> None: Reads binary data from a file, processes it, and writes the resulting hex string to a text file. :param binary_file_path: Path to the binary file to read from. :param text_file_path: Path to the text file to write to. :param buffer_size: Buffer size for both reading and writing operations. :param encoding: Encoding for writing the text file. try: with open(binary_file_path, \\"rb\\", buffering=buffer_size) as binary_file: # Read the entire binary content binary_data = binary_file.read() # Convert binary data to hex string hex_data = binary_data.hex() with open(text_file_path, \\"w\\", encoding=encoding, buffering=buffer_size) as text_file: # Write hex string to the text file text_file.write(hex_data) except FileNotFoundError: print(f\\"File not found: {binary_file_path}\\") except IOError as e: print(f\\"IOError while processing files: {e}\\")"},{"question":"# Custom Event Loop Policy and Process Watcher Objective Design a custom event loop policy and a custom process watcher using the `asyncio` module in Python 3.10. This will test your comprehension of the `asyncio` package and your ability to extend its functionality. Task 1. **Custom Event Loop Policy:** - Subclass `asyncio.DefaultEventLoopPolicy`. - Override the `get_event_loop()` method to print a message `\\"Retrieving the event loop\\"` before returning the event loop. - Ensure the overridden method calls the superclass implementation to actually retrieve the event loop. 2. **Custom Process Watcher:** - Implement a custom process watcher that subclasses `asyncio.AbstractChildWatcher`. Name it `CustomChildWatcher`. - Implement the following methods in `CustomChildWatcher`: - `add_child_handler(pid, callback, *args)`: Print a message `\\"Adding handler for PID {pid}\\"` before registering the handler. - `remove_child_handler(pid)`: Print a message `\\"Removing handler for PID {pid}\\"` before removing the handler. - `attach_loop(loop)`: Attach the watcher to the event loop with a message `\\"Attaching to event loop\\"`. - `is_active()`: Return `True` and print `\\"Watcher is active\\"` when called. - `close()`: Print `\\"Closing watcher\\"` when called. Requirements - Functions should be correctly defined with proper handling of their expected input parameters. - Your custom classes should integrate seamlessly with `asyncio`\'s event loop management. Example Usage ```python import asyncio # Implement the CustomEventLoopPolicy and CustomChildWatcher here # Set the custom event loop policy asyncio.set_event_loop_policy(CustomEventLoopPolicy()) # Create a new event loop and set it loop = asyncio.new_event_loop() asyncio.get_event_loop_policy().set_event_loop(loop) # Create and set the custom process watcher watcher = CustomChildWatcher() asyncio.set_child_watcher(watcher) # Example code to add and remove child handlers pid = 12345 def callback(pid, returncode, *args): print(f\\"Process {pid} terminated with return code {returncode}\\") watcher.add_child_handler(pid, callback) watcher.remove_child_handler(pid) ``` Constraints - Assume the platform supports Unix-specific functionalities for process watchers. - Ensure all methods of `CustomChildWatcher` adhere to thread-safety as required. Expected Output: - The methods should produce the specified print statements when called.","solution":"import asyncio class CustomEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): print(\\"Retrieving the event loop\\") return super().get_event_loop() class CustomChildWatcher(asyncio.AbstractChildWatcher): def __init__(self): super().__init__() self._callbacks = {} self._loop = None def add_child_handler(self, pid, callback, *args): print(f\\"Adding handler for PID {pid}\\") self._callbacks[pid] = (callback, args) def remove_child_handler(self, pid): print(f\\"Removing handler for PID {pid}\\") self._callbacks.pop(pid, None) def attach_loop(self, loop): print(\\"Attaching to event loop\\") self._loop = loop def is_active(self): print(\\"Watcher is active\\") return True def close(self): print(\\"Closing watcher\\") self._callbacks.clear()"},{"question":"Linear Regression Models with Ridge and Lasso Objective You are required to implement and compare Ridge and Lasso regression models using scikit-learn. This will test your understanding of linear models, regularization techniques, and the ability to interpret model coefficients. Background Information You are given a dataset represented as `X` (features) and `y` (target). The goal is to predict the target using linear regression models. Feature matrix `X` has multicollinearity issues that need to be addressed using Ridge and Lasso regressions. Task Description 1. **Implement Ridge Regression**: Fit a Ridge regression model to the data and find the optimal regularization parameter (`alpha`) using cross-validation. 2. **Implement Lasso Regression**: Fit a Lasso regression model to the data and find the optimal regularization parameter (`alpha`) using cross-validation. 3. **Compare the Models**: After fitting both models with optimal `alpha`, compare their coefficients and mean squared error on a test set. Input - `X_train`: A 2D numpy array of shape (n_samples, n_features) representing the training data features. - `y_train`: A 1D numpy array of shape (n_samples,) representing the training data target. - `X_test`: A 2D numpy array of shape (m_samples, n_features) representing the test data features. - `y_test`: A 1D numpy array of shape (m_samples,) representing the test data target. Output - `ridge_best_alpha`: The optimal `alpha` value found for Ridge Regression. - `lasso_best_alpha`: The optimal `alpha` value found for Lasso Regression. - `ridge_coefficients`: The coefficients of the Ridge Regression model. - `lasso_coefficients`: The coefficients of the Lasso Regression model. - `ridge_test_error`: Mean squared error of the Ridge model on the test set. - `lasso_test_error`: Mean squared error of the Lasso model on the test set. Constraints 1. Use a reasonable range for `alpha` values when performing cross-validation (e.g., `np.logspace(-6, 6, 13)`). 2. Use 5-fold cross-validation to find the optimal `alpha`. 3. Use mean squared error (MSE) to evaluate model performance. Performance Requirements - The solution should be efficient and complete the model training within a reasonable time for a standard dataset (e.g., n_samples = 1000, n_features = 100). Example Code Framework ```python import numpy as np from sklearn.linear_model import RidgeCV, LassoCV from sklearn.metrics import mean_squared_error def linear_regression_comparison(X_train, y_train, X_test, y_test): # Implement Ridge Regression with cross-validation ridge_alphas = np.logspace(-6, 6, 13) # example range ridge_model = RidgeCV(alphas=ridge_alphas, cv=5) ridge_model.fit(X_train, y_train) ridge_best_alpha = ridge_model.alpha_ ridge_coefficients = ridge_model.coef_ # Predict with Ridge model on test set ridge_predictions = ridge_model.predict(X_test) ridge_test_error = mean_squared_error(y_test, ridge_predictions) # Implement Lasso Regression with cross-validation lasso_alphas = np.logspace(-6, 6, 13) # example range lasso_model = LassoCV(alphas=lasso_alphas, cv=5, random_state=0) lasso_model.fit(X_train, y_train) lasso_best_alpha = lasso_model.alpha_ lasso_coefficients = lasso_model.coef_ # Predict with Lasso model on test set lasso_predictions = lasso_model.predict(X_test) lasso_test_error = mean_squared_error(y_test, lasso_predictions) # Return the results return { \\"ridge_best_alpha\\": ridge_best_alpha, \\"lasso_best_alpha\\": lasso_best_alpha, \\"ridge_coefficients\\": ridge_coefficients, \\"lasso_coefficients\\": lasso_coefficients, \\"ridge_test_error\\": ridge_test_error, \\"lasso_test_error\\": lasso_test_error } # Example usage (with hypothetical input data) # result = linear_regression_comparison(X_train, y_train, X_test, y_test) # print(result) ``` Notes - Ensure all necessary imports are included in your code. - Test your function with various datasets to confirm its correctness and performance. - Properly handle any possible exceptions that might occur.","solution":"import numpy as np from sklearn.linear_model import RidgeCV, LassoCV from sklearn.metrics import mean_squared_error def linear_regression_comparison(X_train, y_train, X_test, y_test): # Implement Ridge Regression with cross-validation ridge_alphas = np.logspace(-6, 6, 13) # example range ridge_model = RidgeCV(alphas=ridge_alphas, cv=5) ridge_model.fit(X_train, y_train) ridge_best_alpha = ridge_model.alpha_ ridge_coefficients = ridge_model.coef_ # Predict with Ridge model on test set ridge_predictions = ridge_model.predict(X_test) ridge_test_error = mean_squared_error(y_test, ridge_predictions) # Implement Lasso Regression with cross-validation lasso_alphas = np.logspace(-6, 6, 13) # example range lasso_model = LassoCV(alphas=lasso_alphas, cv=5, random_state=0) lasso_model.fit(X_train, y_train) lasso_best_alpha = lasso_model.alpha_ lasso_coefficients = lasso_model.coef_ # Predict with Lasso model on test set lasso_predictions = lasso_model.predict(X_test) lasso_test_error = mean_squared_error(y_test, lasso_predictions) # Return the results return { \\"ridge_best_alpha\\": ridge_best_alpha, \\"lasso_best_alpha\\": lasso_best_alpha, \\"ridge_coefficients\\": ridge_coefficients, \\"lasso_coefficients\\": lasso_coefficients, \\"ridge_test_error\\": ridge_test_error, \\"lasso_test_error\\": lasso_test_error } # Example usage (with hypothetical input data) # result = linear_regression_comparison(X_train, y_train, X_test, y_test) # print(result)"},{"question":"**Coding Assessment Question:** You are required to implement a Python function `simulate_process` that simulates the workflow of a hypothetical automated processing system in a factory. The function should take the following parameters as input: - `initial_state`: an integer seed value to initialize the random number generator. - `num_processes`: an integer specifying the number of processes to simulate. - `arrival_interval`: a floating-point value specifying the average interval time between process arrivals. - `service_time_mean`: a floating-point value specifying the average service time for each process. - `service_time_stddev`: a floating-point value specifying the standard deviation of the service time for each process. - `num_servers`: an integer specifying the number of servers available to handle the processes. The task of the function is to simulate the arrival and service of the processes and return statistical metrics on the waiting times. The function should return a dictionary containing: - \'mean_wait_time\': The mean waiting time for the processes. - \'max_wait_time\': The maximum waiting time encountered during the simulation. - \'quartiles\': A list of three values representing the first, second, and third quartiles of the waiting times. Here is some additional context: - Arrival times follow an exponential distribution. - Service times follow a Gaussian (normal) distribution. - Use a heap to manage server availability times efficiently. The function signature is: ```python def simulate_process(initial_state: int, num_processes: int, arrival_interval: float, service_time_mean: float, service_time_stddev: float, num_servers: int) -> dict: ``` **Constraints:** 1. `initial_state` will be a non-negative integer. 2. `num_processes` will be a positive integer. 3. `arrival_interval`, `service_time_mean`, and `service_time_stddev` will be positive floating-point numbers. 4. `num_servers` will be a positive integer. **Example Usage:** ```python result = simulate_process( initial_state=42, num_processes=1000, arrival_interval=5.0, service_time_mean=15.0, service_time_stddev=3.5, num_servers=3 ) print(result) ``` Expected Output Format: ```python { \'mean_wait_time\': 2.3, \'max_wait_time\': 10.8, \'quartiles\': [0.5, 2.3, 6.7] } ``` **Implementation Notes:** - You should use the `random` module functions for generating random values. - Manage server availability times using a min-heap from the `heapq` module. - Ensure reproducibility of the simulation by initializing the random number generator with the provided `initial_state`. - Compute and return all the required statistical metrics.","solution":"import random import heapq import numpy as np def simulate_process(initial_state: int, num_processes: int, arrival_interval: float, service_time_mean: float, service_time_stddev: float, num_servers: int) -> dict: Simulates the workflow of a hypothetical automated processing system. :param initial_state: Seed value to initialize the random number generator. :param num_processes: Number of processes to simulate. :param arrival_interval: Average interval time between process arrivals. :param service_time_mean: Average service time for each process. :param service_time_stddev: Standard deviation of the service time for each process. :param num_servers: Number of servers available to handle the processes. :return: Dictionary with mean_wait_time, max_wait_time, quartiles for waiting times. random.seed(initial_state) # Initialize process arrival and service times arrival_times = [random.expovariate(1 / arrival_interval) for _ in range(num_processes)] arrival_times = np.cumsum(arrival_times).tolist() service_times = [random.gauss(service_time_mean, service_time_stddev) for _ in range(num_processes)] # Min heap to manage servers\' next available times servers_next_free_time = [0] * num_servers heapq.heapify(servers_next_free_time) wait_times = [] for i in range(num_processes): arrival_time = arrival_times[i] service_time = service_times[i] # Get the next available server next_server_free_time = heapq.heappop(servers_next_free_time) # Calculate wait time wait_time = max(0, next_server_free_time - arrival_time) wait_times.append(wait_time) # Update this server\'s next available time heapq.heappush(servers_next_free_time, arrival_time + wait_time + service_time) # Calculate statistical metrics mean_wait_time = np.mean(wait_times) max_wait_time = np.max(wait_times) quartiles = np.percentile(wait_times, [25, 50, 75]).tolist() return { \'mean_wait_time\': mean_wait_time, \'max_wait_time\': max_wait_time, \'quartiles\': quartiles }"},{"question":"# Coding Assessment Problem Problem Statement You are given a function `process_data(data: list) -> int:` that operates on a list of numerical values and performs the following operations: 1. Computes the sum of the elements in the list. 2. Divides this sum by the number of elements in the list. 3. Returns the integer part of this division. Your task is to handle various edge cases using appropriate built-in exceptions and define a custom exception `EmptyListError` for handling cases where the input list is empty. Implement the `process_data` function to perform the above operations and handle the following exceptions: 1. **ZeroDivisionError**: Raised when attempting to divide by zero. 2. **TypeError**: Raised when non-numeric values are provided in the list. 3. **EmptyListError**: A custom exception to be raised when the provided list is empty. 4. **Other Exceptions**: Catch any other unforeseen exceptions and raise a `ProcessingError` with a descriptive message. You should also implement the following: - Custom exception `EmptyListError`, inheriting from `Exception`. - Custom exception `ProcessingError`, inheriting from `Exception`. Function Signature ```python def process_data(data: list) -> int: # Your code here pass ``` Custom Exceptions ```python class EmptyListError(Exception): Exception raised when the list is empty. pass class ProcessingError(Exception): Exception raised for errors in the processing of data. pass ``` Example Usage ```python try: result = process_data([1, 2, 3, 4, 5]) print(\\"Result:\\", result) # Expected: 3 except Exception as e: print(\\"Error:\\", str(e)) try: result = process_data([]) print(\\"Result:\\", result) except EmptyListError as e: print(\\"Error: The input list is empty.\\") # Expected: The input list is empty. except Exception as e: print(\\"Error:\\", str(e)) try: result = process_data([1, \'a\', 3]) print(\\"Result:\\", result) except TypeError as e: print(\\"Error: Non-numeric value found in the list.\\") # Expected: Non-numeric value found in the list. except Exception as e: print(\\"Error:\\", str(e)) ``` Constraints - The input list `data` can contain any number of elements including zero. - The elements in the list are expected to be integers or floats. Any other type should raise a `TypeError`. Notes 1. Ensure that you catch and handle exceptions appropriately. 2. Consider edge cases such as empty lists and lists with non-numeric values. 3. Provide meaningful error messages for custom exceptions.","solution":"class EmptyListError(Exception): Exception raised when the list is empty. pass class ProcessingError(Exception): Exception raised for errors in the processing of data. pass def process_data(data: list) -> int: try: if not data: raise EmptyListError(\\"The input list is empty.\\") total_sum = 0 for element in data: if not isinstance(element, (int, float)): raise TypeError(\\"Non-numeric value found in the list.\\") total_sum += element count = len(data) if count == 0: raise ZeroDivisionError(\\"Division by zero encountered.\\") result = total_sum // count # We\'re interested in the integer part of the division return result except EmptyListError as e: raise e except TypeError as e: raise e except Exception as e: raise ProcessingError(f\\"An error occurred: {str(e)}\\")"},{"question":"# Internationalization with `gettext` Problem Statement You are tasked to implement a class that handles internationalization using Python’s `gettext` module. The class should be able to load translations from `.mo` files, retrieve translated text, and switch between multiple languages dynamically. This functionality is often required in applications that need to support multiple languages. Class Specification **Class Name:** `Translator` **Methods:** 1. **`__init__(self, locales_dir: str, domain: str)`** - Initializes the translator with the directory containing the locale files (`locales_dir`) and the domain for the translations (`domain`). 2. **`set_language(self, language: str) -> None`** - Sets the current language for translations. - **Input:** - `language` (str): Language code (e.g., \'en\', \'fr\'). 3. **`gettext(self, text: str) -> str`** - Returns the translated text for the given input message. - **Input:** - `text` (str): The message string to be translated. - **Output:** - Translated string (str) if a translation exists, otherwise returns the input string. 4. **`available_languages(self) -> List[str]`** - Returns a list of available languages that have translation files in the specified directory. - **Output:** - List of language codes (str). Example ```python # Assuming the locale directory structure is as follows: # locales/ # ├─ en/ # │ └─ LC_MESSAGES/ # │ └─ messages.mo # └─ fr/ # └─ LC_MESSAGES/ # └─ messages.mo translator = Translator(\\"locales\\", \\"messages\\") # Set language to French translator.set_language(\\"fr\\") # Get translated text print(translator.gettext(\\"Hello, world!\\")) # Should print \\"Bonjour, le monde!\\" if translation is available # List available languages print(translator.available_languages()) # Should print [\'en\', \'fr\'] ``` Constraints - The translations should be managed using the GNU gettext `.mo` files. - You should handle cases where translations are not available for a given message gracefully (return the original string). - The implementation should not use any external libraries other than Python\'s standard library. Notes - Ensure that your solution is efficient in terms of loading and retrieving translations. - The path provided for the locales directory can be any valid path; your code should handle relative and absolute paths appropriately.","solution":"import os import gettext from typing import List class Translator: def __init__(self, locales_dir: str, domain: str): Initializes the translator with the directory containing the locale files and the domain for the translations. self.locales_dir = locales_dir self.domain = domain self.current_language = None self.translations = None def set_language(self, language: str) -> None: Sets the current language for translations. self.current_language = language locale_path = os.path.join(self.locales_dir, language, \'LC_MESSAGES\') self.translations = gettext.translation(self.domain, localedir=locale_path, languages=[language], fallback=True) self.translations.install() def gettext(self, text: str) -> str: Returns the translated text for the given input message. return self.translations.gettext(text) if self.translations else text def available_languages(self) -> List[str]: Returns a list of available languages that have translation files in the specified directory. languages = [] for language_code in os.listdir(self.locales_dir): locale_path = os.path.join(self.locales_dir, language_code, \'LC_MESSAGES\') if os.path.isdir(locale_path) and any(fname.endswith(\'.mo\') for fname in os.listdir(locale_path)): languages.append(language_code) return languages"},{"question":"Question: Implementing a Complex Data Class with Data Validation and Inheritance # Objective Your task is to implement a class using Python\'s `dataclasses` module to represent a simple database record system for a bookstore. This will involve creating a base data class with inheritance, validation, and some advanced features like frozen instances and default factory functions. # Requirements 1. **Base Class `Book`:** - **Attributes**: ISBN (string), title (string), author (string), year_published (integer) - **Constraints**: - ISBN must follow the pattern: `XXX-X-XX-XXXXXX-X` where `X` is any digit. - Year published must be between 1800 and the current year. - Validate these constraints in the `__post_init__` method. 2. **Derived Class `EBook` from `Book`:** - **Additional Attributes**: file_format (string), file_size_mb (float) - **Constraints**: - File format must be one of the following: \'pdf\', \'epub\', \'mobi\'. - File size must be a positive number and less than 100 MB. 3. **Derived Class `PrintedBook` from `Book`:** - **Additional Attributes**: weight_grams (float), dimensions (tuple of three floats: length, width, height in cm) - **Constraints**: - Weight must be a positive number. - Dimensions must all be positive numbers. 4. **Inheritance and Encapsulation**: - Use the `@dataclass` decorator for all classes. - Ensure that the `PrintedBook` and `EBook` inherit from `Book`. 5. **Example Usage**: ```python try: ebook = EBook(isbn=\\"123-4-56-789012-3\\", title=\\"Python Programming\\", author=\\"John Doe\\", year_published=2021, file_format=\\"pdf\\", file_size_mb=5.5) print(ebook) except ValueError as e: print(e) try: pbook = PrintedBook(isbn=\\"987-6-54-321098-7\\", title=\\"Data Science\\", author=\\"Jane Smith\\", year_published=2015, weight_grams=1200.5, dimensions=(22.0, 15.0, 3.5)) print(pbook) except ValueError as e: print(e) ``` # Implementation Below, implement the `Book`, `EBook`, and `PrintedBook` classes according to the specified requirements. ```python from dataclasses import dataclass, field import re from datetime import datetime # Define the base class `Book` @dataclass class Book: isbn: str title: str author: str year_published: int def __post_init__(self): # Validate ISBN if not re.match(r\'^d{3}-d-d{2}-d{6}-d\', self.isbn): raise ValueError(\\"ISBN is not in the correct format: XXX-X-XX-XXXXXX-X\\") # Validate year published current_year = datetime.now().year if not (1800 <= self.year_published <= current_year): raise ValueError(f\\"Year published must be between 1800 and {current_year}\\") # Define the derived class `EBook` @dataclass class EBook(Book): file_format: str file_size_mb: float def __post_init__(self): super().__post_init__() # Validate file format if self.file_format not in [\'pdf\', \'epub\', \'mobi\']: raise ValueError(\\"File format must be either \'pdf\', \'epub\', or \'mobi\'\\") # Validate file size if not (0 < self.file_size_mb < 100): raise ValueError(\\"File size must be a positive number and less than 100 MB\\") # Define the derived class `PrintedBook` @dataclass class PrintedBook(Book): weight_grams: float dimensions: tuple def __post_init__(self): super().__post_init__() # Validate weight if self.weight_grams <= 0: raise ValueError(\\"Weight must be a positive number\\") # Validate dimensions if not all(d > 0 for d in self.dimensions): raise ValueError(\\"All dimensions must be positive numbers\\") ``` You should add appropriate error handling to ensure the validation process raises meaningful errors. This exercise tests your understanding of data classes, inheritance, and proper data validation.","solution":"from dataclasses import dataclass import re from datetime import datetime # Define the base class `Book` @dataclass class Book: isbn: str title: str author: str year_published: int def __post_init__(self): # Validate ISBN if not re.match(r\'^d{3}-d-d{2}-d{6}-d\', self.isbn): raise ValueError(\\"ISBN is not in the correct format: XXX-X-XX-XXXXXX-X\\") # Validate year published current_year = datetime.now().year if not (1800 <= self.year_published <= current_year): raise ValueError(f\\"Year published must be between 1800 and {current_year}\\") # Define the derived class `EBook` @dataclass class EBook(Book): file_format: str file_size_mb: float def __post_init__(self): super().__post_init__() # Validate file format if self.file_format not in [\'pdf\', \'epub\', \'mobi\']: raise ValueError(\\"File format must be either \'pdf\', \'epub\', or \'mobi\'\\") # Validate file size if not (0 < self.file_size_mb < 100): raise ValueError(\\"File size must be a positive number and less than 100 MB\\") # Define the derived class `PrintedBook` @dataclass class PrintedBook(Book): weight_grams: float dimensions: tuple def __post_init__(self): super().__post_init__() # Validate weight if self.weight_grams <= 0: raise ValueError(\\"Weight must be a positive number\\") # Validate dimensions if not all(d > 0 for d in self.dimensions): raise ValueError(\\"All dimensions must be positive numbers\\")"},{"question":"# Advanced Pandas Indexing Assessment Objective: Your task is to demonstrate your understanding of the `pandas` package\'s indexing capabilities by performing various operations involving different types of `Index` objects. This will involve creating Index objects, manipulating them, and performing advanced operations such as joining, reindexing, and handling missing data. Requirements: 1. **Create Index Objects:** - Create a `DatetimeIndex` starting from \'2023-01-01\' to \'2023-12-31\' with a frequency of \'M\' (month-end). - Create a `RangeIndex` starting at 1, ending at 12, with a step of 1. - Create a `CategoricalIndex` from the list of categories: `[\\"A\\", \\"B\\", \\"C\\", \\"D\\"]`, ordered. 2. **Manipulate Index Objects:** - Reindex the `DatetimeIndex` to include \'2023-06-15\' using `Index.insert` method. - Rename categories in the `CategoricalIndex` from `[\\"A\\", \\"B\\", \\"C\\", \\"D\\"]` to `[\\"W\\", \\"X\\", \\"Y\\", \\"Z\\"]`. 3. **Combine Index Objects:** - Create a `MultiIndex` using the `DatetimeIndex` and `RangeIndex`. - Perform an inner join between the created `MultiIndex` and another `MultiIndex` created from: - `DatetimeIndex` spanning from \'2023-03-31\' to \'2023-12-31\' with a frequency of \'M\'. - `RangeIndex` spanning from 1 to 10 with a step of 1. 4. **Advanced Operations:** - Find all indices in the resulting `MultiIndex` where the date is on the last day of a quarter. - Drop missing indices from the joined `MultiIndex`. Constraints: - Use `pandas` library functions and methods only. - Ensure that your solution is efficient and handles large data sets gracefully. Input and Output: - **Input:** No direct input as the task involves performing predefined operations and transformations. - **Output:** Print the results of each step clearly. Example Solution: ```python import pandas as pd # 1. Create Index Objects dt_index = pd.date_range(start=\'2023-01-01\', end=\'2023-12-31\', freq=\'M\') range_index = pd.RangeIndex(start=1, stop=13, step=1) cat_index = pd.CategoricalIndex([\\"A\\", \\"B\\", \\"C\\", \\"D\\"], ordered=True) print(\\"DatetimeIndex:\\", dt_index) print(\\"RangeIndex:\\", range_index) print(\\"CategoricalIndex:\\", cat_index) # 2. Manipulate Index Objects dt_index = dt_index.insert(len(dt_index), pd.Timestamp(\'2023-06-15\')) cat_index = cat_index.rename_categories([\\"W\\", \\"X\\", \\"Y\\", \\"Z\\"]) print(\\"Manipulated DatetimeIndex:\\", dt_index) print(\\"Renamed CategoricalIndex:\\", cat_index) # 3. Combine Index Objects multi_index1 = pd.MultiIndex.from_arrays([dt_index, range_index * 2]) # Example adjustment multi_index2 = pd.MultiIndex.from_arrays([pd.date_range(start=\'2023-03-31\', end=\'2023-12-31\', freq=\'M\'), range_index]) joined_multi_index = multi_index1.intersection(multi_index2) print(\\"Joined MultiIndex:\\", joined_multi_index) # 4. Advanced Operations quarter_end_dates = dt_index[dt_index.is_quarter_end] print(\\"Quarter End Dates in MultiIndex:\\", quarter_end_dates) dropped_multi_index = joined_multi_index.dropna() print(\\"Dropped MultiIndex:\\", dropped_multi_index) ``` Notes: - The provided example is for reference and may require refinement based on the specific method usages. - Ensure that all print statements are clear and informative to demonstrate the results effectively.","solution":"import pandas as pd # 1. Create Index Objects dt_index = pd.date_range(start=\'2023-01-01\', end=\'2023-12-31\', freq=\'M\') range_index = pd.RangeIndex(start=1, stop=13, step=1) cat_index = pd.CategoricalIndex([\\"A\\", \\"B\\", \\"C\\", \\"D\\"], ordered=True) print(\\"DatetimeIndex:\\", dt_index) print(\\"RangeIndex:\\", range_index) print(\\"CategoricalIndex:\\", cat_index) # 2. Manipulate Index Objects dt_index = dt_index.insert(len(dt_index), pd.Timestamp(\'2023-06-15\')) cat_index = cat_index.rename_categories([\\"W\\", \\"X\\", \\"Y\\", \\"Z\\"]) print(\\"Manipulated DatetimeIndex:\\", dt_index) print(\\"Renamed CategoricalIndex:\\", cat_index) # 3. Combine Index Objects # Create a MultiIndex from dt_index and range_index multi_index1 = pd.MultiIndex.from_product([dt_index, range_index]) # Create another MultiIndex for joining dt_index2 = pd.date_range(start=\'2023-03-31\', end=\'2023-12-31\', freq=\'M\') range_index2 = pd.RangeIndex(start=1, stop=11, step=1) multi_index2 = pd.MultiIndex.from_product([dt_index2, range_index2]) # Perform an inner join using intersection joined_multi_index = multi_index1.intersection(multi_index2) print(\\"Joined MultiIndex:\\", joined_multi_index) # 4. Advanced Operations # Find all indices in the resulting MultiIndex where the date is on the last day of a quarter quarter_end_index = joined_multi_index.get_level_values(0).is_quarter_end quarter_end_dates = joined_multi_index[quarter_end_index] print(\\"Quarter End Dates in MultiIndex:\\", quarter_end_dates) # Drop missing indices from the joined MultiIndex (although intersection method should not have NAs) # For completeness, the following step is shown dropped_multi_index = joined_multi_index.dropna() print(\\"Dropped MultiIndex:\\", dropped_multi_index)"},{"question":"Text Justification with Custom Indentation Objective Write a function that reformats a given text by wrapping it to a specified width, adding custom indentation to each line, and truncating overly long lines to fit within a specified maximum number of lines. Function Signature ```python def reformat_text(text: str, width: int, initial_indent: str = \'\', subsequent_indent: str = \'\', max_lines: int = None, placeholder: str = \' [...]\') -> str: pass ``` Input - `text` (str): The input text paragraph to be reformatted. - `width` (int): Maximum width of each line in the output. - `initial_indent` (str, optional): String to prefix the first line of the output with. Defaults to an empty string. - `subsequent_indent` (str, optional): String to prefix all lines of the output except the first. Defaults to an empty string. - `max_lines` (int, optional): Maximum number of lines allowed in the output. Defaults to `None` (no limit). - `placeholder` (str, optional): String to indicate that the text has been truncated if `max_lines` is reached. Defaults to `\' [...]\'`. Output - `str`: The reformatted text as a single string with newlines separating lines. Constraints 1. The function should use the `textwrap` module to handle the text wrapping and indentation. 2. If `max_lines` is specified and the text cannot be fully displayed within the given number of lines, it should end with the provided `placeholder`. 3. Ensure that leading spaces in the provided text are properly handled. Example ```python def reformat_text(text: str, width: int, initial_indent: str = \'\', subsequent_indent: str = \'\', max_lines: int = None, placeholder: str = \' [...]\') -> str: import textwrap # Create an instance of TextWrapper with the given parameters wrapper = textwrap.TextWrapper( width=width, initial_indent=initial_indent, subsequent_indent=subsequent_indent, max_lines=max_lines, placeholder=placeholder ) # Use the fill method to get the formatted text formatted_text = wrapper.fill(text) return formatted_text # Example Usage: text = \\"Python is an amazing programming language, loved by developers for its readability and support for multiple paradigms.\\" result = reformat_text(text, width=40, initial_indent=\' \', subsequent_indent=\' \', max_lines=3) print(result) # Expected Output: # \\" Python is an amazing programming language,n loved by developers for its readabilityn and support for multiple paradigms. [...]\\" ``` Notes - You may assume the input text does not contain any tabs (`t`) and is a single paragraph without explicit newlines. - Consider edge cases such as empty text inputs and very small or large width values.","solution":"def reformat_text(text: str, width: int, initial_indent: str = \'\', subsequent_indent: str = \'\', max_lines: int = None, placeholder: str = \' [...]\') -> str: import textwrap # Create an instance of TextWrapper with the given parameters wrapper = textwrap.TextWrapper( width=width, initial_indent=initial_indent, subsequent_indent=subsequent_indent, max_lines=max_lines, placeholder=placeholder ) # Use the fill method to get the formatted text formatted_text = wrapper.fill(text) return formatted_text"},{"question":"**Coding Assessment Question:** You are required to implement a function `future_features_info()` that extracts and formats information about features from the `__future__` module. # Function Signature ```python def future_features_info() -> str: ``` # Objective The function should return a formatted string containing details of each feature listed in the `__future__` module. The details should include the feature name, the version it became optional, and the version it became mandatory or `None` if it was dropped. The version should be formatted as `MAJOR.MINOR.MICRO-LEVEL` where `LEVEL` is the release level\'s first letter in uppercase (A, B, C, F for alpha, beta, candidate, final). # Output A formatted string with the following format for each feature: ``` Feature: <feature_name> Optional: <optional_version> Mandatory: <mandatory_version> Compiler Flag: <compiler_flag> -------------------------------------------------- ``` Each feature\'s details should be separated by a \\"--------------------------------------------------\\" line. # Example Output ```plaintext Feature: nested_scopes Optional: 2.1.0-B Mandatory: 2.2.0-F Compiler Flag: 8192 -------------------------------------------------- Feature: generators Optional: 2.2.0-A Mandatory: 2.3.0-F Compiler Flag: 4096 -------------------------------------------------- ... ``` # Constraints and Requirements 1. You should use introspection to extract the feature details from the `__future__` module. 2. Ensure that you correctly format the version information. 3. Handle missing values appropriately (e.g., if a feature\'s mandatory release version is `None`, it should be displayed as `\\"None\\"`). # Hints - Utilize the `getOptionalRelease()` and `getMandatoryRelease()` methods of `_Feature` instances to extract version tuples. - You may need to use built-in modules like `inspect` to list features present in the `__future__` module and dynamically access their attributes.","solution":"import __future__ def format_version(version): if version is None: return \\"None\\" major, minor, micro, level, _ = version level_char = level[0].upper() if level else \'F\' return f\\"{major}.{minor}.{micro}-{level_char}\\" def future_features_info() -> str: features = [attr for attr in dir(__future__) if isinstance(getattr(__future__, attr), __future__._Feature)] feature_info = [] for feature in features: feat_obj = getattr(__future__, feature) optional_ver = format_version(feat_obj.getOptionalRelease()) mandatory_ver = format_version(feat_obj.getMandatoryRelease()) compiler_flag = feat_obj.compiler_flag feature_info.append(f\\"Feature: {feature}n Optional: {optional_ver}n Mandatory: {mandatory_ver}n Compiler Flag: {compiler_flag}\\") feature_info.append(\\"-\\" * 50) return \\"n\\".join(feature_info)"},{"question":"You are given a WAV file named `input.wav`. Your task is to write a Python function that reads the file and creates a new WAV file named `output.wav` with some modifications. The new WAV file should meet the following requirements: 1. **Change the Sample Width**: If the input sample width is 1 byte, change it to 2 bytes. If it is 2 bytes, change it to 1 byte. Any other sample width should raise a `wave.Error`. 2. **Double the Frame Rate**: If the input frame rate is `framerate`, set the output frame rate to `2 * framerate`. 3. **Reverse the Audio Frames**: The order of the audio frames in the output file should be the reverse of the input file. Implement the function `modify_wav_file(input_filename: str, output_filename: str) -> None` that performs the above modifications. Function Signature ```python def modify_wav_file(input_filename: str, output_filename: str) -> None: pass ``` Input - `input_filename` (str): The path to the input WAV file (`input.wav`). - `output_filename` (str): The path to the output WAV file (`output.wav`). Output - The function should not return anything. It should create `output.wav` with the specified modifications. Constraints - The input WAV file will always be a valid PCM WAV file with either 1 or 2 bytes sample width. - Use the `wave` module only for reading and writing WAV files. - Ensure that all file resources are properly managed and closed after operations. Example Usage ```python # Given an input.wav file, this call will create an output.wav file as specified. modify_wav_file(\\"input.wav\\", \\"output.wav\\") ``` Notes - Handle any potential errors using `wave.Error`. - Consider the endianness and byte order when changing the sample width.","solution":"import wave import audioop def modify_wav_file(input_filename: str, output_filename: str) -> None: with wave.open(input_filename, \'rb\') as input_wav: # Extract input file parameters num_channels = input_wav.getnchannels() samp_width = input_wav.getsampwidth() frame_rate = input_wav.getframerate() num_frames = input_wav.getnframes() comp_type = input_wav.getcomptype() comp_name = input_wav.getcompname() # Modify the sample width if samp_width == 1: new_samp_width = 2 elif samp_width == 2: new_samp_width = 1 else: raise wave.Error(\\"Unsupported sample width\\") # Read frames, reverse them, and change sample width if necessary frames = input_wav.readframes(num_frames) reversed_frames = frames[::-1] # Convert sample width if necessary if new_samp_width != samp_width: reversed_frames = audioop.byteswap(audioop.lin2lin(reversed_frames, samp_width, new_samp_width), new_samp_width) # Double the frame rate new_frame_rate = frame_rate * 2 with wave.open(output_filename, \'wb\') as output_wav: # Set output file parameters output_wav.setnchannels(num_channels) output_wav.setsampwidth(new_samp_width) output_wav.setframerate(new_frame_rate) output_wav.setcomptype(comp_type, comp_name) # Write reversed and modified frames to output file output_wav.writeframes(reversed_frames)"},{"question":"**Question: Working with `torch.cuda.tunable` Module** In this assignment, you are required to interact with the `torch.cuda.tunable` module to demonstrate your understanding of enabling features, configuring parameters, and managing file operations in PyTorch. # Task: 1. **Enable the Tunable Operations:** Implement a function `enable_tunable_ops()` that enables the tunable operations using the provided API functions. ```python def enable_tunable_ops(): Enables the tunable operations. pass ``` 2. **Configure Tuning Parameters:** Implement a function `configure_tuning_params(duration, iterations)` that sets the maximum tuning duration and the maximum number of tuning iterations. ```python def configure_tuning_params(duration: int, iterations: int): Configures the tuning parameters. Args: duration (int): The maximum tuning duration in milliseconds. iterations (int): The maximum number of tuning iterations. pass ``` 3. **Manage File Operations:** Implement a function `write_tuning_results_to_file(filename)` that writes the tuning results to the specified file. ```python def write_tuning_results_to_file(filename: str): Writes the tuning results to the specified file. Args: filename (str): The name of the file where tuning results will be written. pass ``` 4. **Perform Tuning Operation:** Implement a function `perform_tuning(filename)` that enables tuning, configures the tuning parameters (with duration of 1000 milliseconds and 10 iterations), and writes the tuning results to the given file. ```python def perform_tuning(filename: str): Performs the tuning operation and writes results to a file. Args: filename (str): The name of the file where tuning results will be written. pass ``` # Constraints: - Ensure that tuning is enabled before performing any tuning operations. - Make use of appropriate API functions from the `torch.cuda.tunable` module. - The functions should handle any necessary file paths and operations. # Example: Here\'s an example of how you might use these functions. ```python enable_tunable_ops() configure_tuning_params(2000, 20) write_tuning_results_to_file(\\"tuning_results.txt\\") perform_tuning(\\"final_tuning_results.txt\\") ``` # Evaluation Criteria: - Correctness of function implementations. - Usage of appropriate API functions from the `torch.cuda.tunable` module. - Ability to handle configurations and file operations accurately. - Writing clean and efficient code.","solution":"import torch.cuda.tunable def enable_tunable_ops(): Enables the tunable operations. torch.cuda.tunable.enable() def configure_tuning_params(duration: int, iterations: int): Configures the tuning parameters. Args: duration (int): The maximum tuning duration in milliseconds. iterations (int): The maximum number of tuning iterations. torch.cuda.tunable.set_tuning_duration(duration) torch.cuda.tunable.set_tuning_iterations(iterations) def write_tuning_results_to_file(filename: str): Writes the tuning results to the specified file. Args: filename (str): The name of the file where tuning results will be written. tuning_results = torch.cuda.tunable.get_tuning_results() with open(filename, \'w\') as file: file.write(str(tuning_results)) def perform_tuning(filename: str): Performs the tuning operation and writes results to a file. Args: filename (str): The name of the file where tuning results will be written. enable_tunable_ops() configure_tuning_params(1000, 10) # Assuming some tuning operation happens here write_tuning_results_to_file(filename)"},{"question":"Title: Advanced Logging in Python **Problem Statement:** You are tasked with creating a Python program for a mock application that requires comprehensive logging. You need to demonstrate the use of logging at different severity levels, logging to both console and file, dynamic configuration of logging and logging from multiple modules with proper formats. **Requirements:** 1. **Logging Levels:** Implement logging in your code such that various severity levels are logged (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). 2. **Logging Handlers:** Set up logging to log messages to both the console and a file `app.log`. 3. **Logging Formats:** Ensure that the log messages contain the timestamp, the severity level, and the logger name within the message. The log file should format the timestamp in the form `YYYY-MM-DD HH:MM:SS`. 4. **Dynamic Configuration:** Provide a function `configure_logging(loglevel, filename)` that sets the logging level dynamically and allows changing the log file where messages are recorded. The function should handle invalid log level inputs gracefully. 5. **Logging from Multiple Modules:** Create at least two modules (`main_module.py` and `helper_module.py`), such that both modules log messages. Ensure that log messages from different modules indicate clearly which module they come from. **Input:** - `loglevel`: A string indicating the log level (one of `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). - `filename`: A string indicating the name of the file to log messages. **Function Signature:** ```python def configure_logging(loglevel: str, filename: str): pass ``` **Example Usage:** ```python # main_module.py import logging import helper_module def main(): configure_logging(\'DEBUG\', \'app.log\') logging.info(\\"Starting the main function.\\") helper_module.do_work() if __name__ == \\"__main__\\": main() # helper_module.py import logging def do_work(): logging.debug(\\"Doing some work in helper module.\\") try: 1 / 0 except ZeroDivisionError: logging.error(\\"An error occurred in helper module.\\") ``` Running `main_module.py` should produce log output both on the console and in the `app.log` file, similar to: ``` # Console output INFO:root:Starting the main function. DEBUG:root:Doing some work in helper module. ERROR:root:An error occurred in helper module. # app.log YYYY-MM-DD HH:MM:SS,DEBUG,root:Doing some work in helper module. YYYY-MM-DD HH:MM:SS,ERROR,root:An error occurred in helper module. ``` **Constraints:** - You can assume the directory where `app.log` file is created is writable. - Log messages must be human-readable and consistent in format. - The function `configure_logging` should be called before any log messages are written. **Performance Requirement:** - The solution should be efficient enough for a typical script logging use case and not impose considerable overhead on the runtime performance of the script. Complete the function and ensure your logging setup works as specified.","solution":"import logging def configure_logging(loglevel: str, filename: str): Configures logging with the given log level and filename. Args: loglevel (str): The log level e.g., \'DEBUG\', \'INFO\', \'WARNING\', \'ERROR\', \'CRITICAL\' filename (str): The file name to log messages to. # Define log level mapping log_levels = { \'DEBUG\': logging.DEBUG, \'INFO\': logging.INFO, \'WARNING\': logging.WARNING, \'ERROR\': logging.ERROR, \'CRITICAL\': logging.CRITICAL, } # Fetch log level if valid, default to \'DEBUG\' otherwise log_level = log_levels.get(loglevel.upper(), logging.DEBUG) # Define log format log_format = \'%(asctime)s,%(levelname)s,%(name)s: %(message)s\' datefmt = \'%Y-%m-%d %H:%M:%S\' # Clear any existing configurations logging.getLogger().handlers = [] # Configure root logger logging.basicConfig(level=log_level, format=log_format, datefmt=datefmt, handlers=[ logging.FileHandler(filename), logging.StreamHandler() ]) # Example of main_module.py def main(): import helper_module # Import here to avoid circular dependence issues configure_logging(\'DEBUG\', \'app.log\') logging.info(\\"Starting the main function.\\") helper_module.do_work() if __name__ == \\"__main__\\": main() # Save this content in helper_module.py file # Example of helper_module.py import logging def do_work(): logging.debug(\\"Doing some work in helper module.\\") try: 1 / 0 except ZeroDivisionError: logging.error(\\"An error occurred in helper module.\\")"},{"question":"# HTTP Request and Error Handling with `urllib.request` You are tasked with creating a Python script that performs the following functionalities by utilizing the `urllib.request` package. 1. **Fetch HTML Content**: Fetch the HTML content from the URL `http://www.example.com`. 2. **Send Form Data**: Simulate a form submission with POST data: - URL: `http://www.example.com/form` - Form Data: `{ \'name\': \'John Doe\', \'email\': \'johndoe@example.com\' }` 3. **Custom User-Agent**: Fetch content from `http://www.example.com/protected` while emulating a browser by setting the User-Agent to `\'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\'`. 4. **Handle Errors**: Gracefully handle errors (like network issues or HTTP errors) and print appropriate messages. # Implementation Details: - **Function: `fetch_html_content(url: str) -> str`** - Input: A string representing the URL to fetch. - Output: A string containing the HTML content. - Constraints: Handle `URLError` if there is a network problem, and `HTTPError` if there is an HTTP error (e.g., 404 or 500). - **Function: `post_form_data(url: str, data: dict) -> str`** - Input: A string for the URL, and a dictionary containing the form data. - Output: A string containing the server response. - Constraints: Ensure data is correctly encoded as bytes before sending. - **Function: `fetch_with_custom_user_agent(url: str, user_agent: str) -> str`** - Input: Two strings, the URL to fetch from and the User-Agent string. - Output: A string containing the HTML content. - Constraints: Set the custom User-Agent in the request headers. - **Function: `handle_errors(url: str) -> Optional[str]`** - Input: A string representing the URL to fetch. - Output: A string containing the HTML content or None if there\'s an error. Print the error message. - Exception Handling: Handle both `URLError` and `HTTPError` and print appropriate error messages. # Example Usages: ```python def fetch_html_content(url: str) -> str: import urllib.request try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\" except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code}\\" def post_form_data(url: str, data: dict) -> str: import urllib.parse import urllib.request encoded_data = urllib.parse.urlencode(data).encode(\'utf-8\') request = urllib.request.Request(url, data=encoded_data) with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') def fetch_with_custom_user_agent(url: str, user_agent: str) -> str: import urllib.request request = urllib.request.Request(url, headers={\'User-Agent\': user_agent}) with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') def handle_errors(url: str) -> Optional[str]: import urllib.request import urllib.error try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: print(f\\"URLError: {e.reason}\\") except urllib.error.HTTPError as e: print(f\\"HTTPError: {e.code}\\") return None # Example Usage html_content = fetch_html_content(\'http://www.example.com\') form_response = post_form_data(\'http://www.example.com/form\', {\'name\': \'John Doe\', \'email\': \'johndoe@example.com\'}) protected_content = fetch_with_custom_user_agent(\'http://www.example.com/protected\', \'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\') safe_content = handle_errors(\'http://www.example.com\') ``` Your task is to implement these four functions correctly, making sure all exceptions are handled gracefully and appropriate messages are returned or printed as specified.","solution":"import urllib.request import urllib.parse import urllib.error def fetch_html_content(url: str) -> str: try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\" except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code}\\" def post_form_data(url: str, data: dict) -> str: encoded_data = urllib.parse.urlencode(data).encode(\'utf-8\') request = urllib.request.Request(url, data=encoded_data) try: with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\" except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code}\\" def fetch_with_custom_user_agent(url: str, user_agent: str) -> str: request = urllib.request.Request(url, headers={\'User-Agent\': user_agent}) try: with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\" except urllib.error.HTTPError as e: return f\\"HTTP Error: {e.code}\\" def handle_errors(url: str) -> str: try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: print(f\\"URLError: {e.reason}\\") except urllib.error.HTTPError as e: print(f\\"HTTPError: {e.code}\\") return None"},{"question":"# Custom Logging Handler Implementation **Objective**: Design and implement a custom logging handler in Python by extending the `FileHandler` class. This custom handler should add a timestamp prefix to each log message and support rotating logs when a certain file size is exceeded. **Requirements**: 1. **Class Definition**: Create a class `TimestampedRotatingFileHandler` that inherits from `logging.FileHandler`. 2. **Timestamp Prefix**: Each log message written by this handler should be prefixed with the current timestamp. Format the timestamp as `YYYY-MM-DD HH:MM:SS`. 3. **Log Rotation**: Implement log rotation based on file size. When the log file exceeds a specified size, it should be rotated, and a new file should be created. The rotated files should have a numeric suffix. 4. **Initialization Parameters**: - `filename`: The name of the log file. - `mode`: The file mode (default is \'a\'). - `maxBytes`: The maximum file size in bytes before rotation. - `backupCount`: The number of backup files to keep. 5. **Behavior**: - If the log file reaches `maxBytes`, rename the current file by appending `.1`, `.2`, ..., up to the number specified by `backupCount`. - The oldest backup file should be deleted if more than `backupCount` backups exist. **Function Signature**: ```python import logging class TimestampedRotatingFileHandler(logging.FileHandler): def __init__(self, filename, mode=\'a\', maxBytes=0, backupCount=0): pass def emit(self, record): pass def doRollover(self): pass ``` # Constraints: - Do not use external libraries other than the Python standard library. - Ensure the handler works efficiently for large log files and high-frequency logging. # Example Usage: ```python if __name__ == \\"__main__\\": logger = logging.getLogger(\\"custom_logger\\") handler = TimestampedRotatingFileHandler(\\"application.log\\", maxBytes=1024, backupCount=3) formatter = logging.Formatter(\'%(message)s\') handler.setFormatter(formatter) logger.addHandler(handler) logger.setLevel(logging.INFO) for i in range(100): logger.info(f\\"Log message {i}\\") ``` In this example, the log messages should be written to \\"application.log\\" with each message prefixed with the current timestamp. The log file should rotate when it exceeds 1024 bytes, and up to 3 backup files should be kept. # Evaluation Criteria: - Correctness: The implementation should correctly extend `FileHandler` and add the required functionality. - Efficiency: The solution should handle high-frequency logging efficiently without significant overhead. - Code Quality: The code should be well-structured, readable, and follow Python\'s best practices.","solution":"import logging import os from datetime import datetime class TimestampedRotatingFileHandler(logging.FileHandler): def __init__(self, filename, mode=\'a\', maxBytes=0, backupCount=0): self.maxBytes = maxBytes self.backupCount = backupCount super().__init__(filename, mode) def emit(self, record): if self.maxBytes > 0 and self.stream.tell() >= self.maxBytes: self.doRollover() # Add timestamp to the message record.msg = f\\"{datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')} - {record.msg}\\" super().emit(record) def doRollover(self): self.stream.close() for i in range(self.backupCount - 1, 0, -1): sfn = f\\"{self.baseFilename}.{i}\\" dfn = f\\"{self.baseFilename}.{i+1}\\" if os.path.exists(sfn): if os.path.exists(dfn): os.remove(dfn) os.rename(sfn, dfn) dfn = self.baseFilename + \\".1\\" if os.path.exists(dfn): os.remove(dfn) os.rename(self.baseFilename, dfn) self.stream = self._open()"},{"question":"# Advanced Python Bytes Manipulation As a programming task, you are required to write a Python function that takes a list of strings and integers, processes this list to create a single `bytes` object, and returns some specific information about that `bytes` object as outlined below. This exercise will test your understanding of Python\'s bytes manipulation functions. **Function Specifications:** ```python def process_data(data): Process a list of strings and integers, and return a tuple containing: 1. The concatenated bytes object. 2. The size of the bytes object. 3. A hexadecimal representation of the bytes object content. Parameters: data (list): A list containing strings and/or integers. Returns: tuple: A tuple containing three elements: - A bytes object representing the concatenated content of input data. - An integer representing the size of the bytes object. - A string representing the hexadecimal content of the bytes object. ``` # Detailed Requirements 1. **Input:** - `data`: A list of strings and integers (e.g., [\'hello\', 100, \'world\', 200]). 2. **Output:** - A tuple containing: 1. A `bytes` object created by sequentially concatenating the bytes representation of each element in the input list. For integers, use their ASCII character code (hint: `chr` and `ord` functions). 2. The size of the resulting `bytes` object (number of bytes). 3. A string representing the hexadecimal equivalent of each byte in the resulting `bytes` object (hint: `binascii.hexlify` may be useful). 3. **Constraints:** - The elements in the input list will be strings (ASCII only) and positive integers less than 256. - You should not use built-in methods for direct conversion like `bytes()`, but rather demonstrate the use of the provided functions for byte manipulation. # Example ```python input_data = [\'hello\', 100, \'world\', 50] result = process_data(input_data) # Expected Output: # result => (b\'hellodworld2\', 12, \'68656c6c6f64776f726c6432\') ``` # Additional Information: - Utilize `PyBytes_FromStringAndSize`, `PyBytes_Concat`, `PyBytes_Size`, and any other relevant functions from the provided documentation. - Ensure proper error handling for type mismatches and other potential errors. - Test your function with various edge cases to verify its correctness and robustness. # Note: This problem requires you to demonstrate an understanding of creating and manipulating `bytes` objects using specific functions rather than relying on simple built-in methods. Make sure to closely follow the functions provided in the documentation. Results should be consistent with the expected output format.","solution":"import binascii def process_data(data): Process a list of strings and integers, and return a tuple containing: 1. The concatenated bytes object. 2. The size of the bytes object. 3. A hexadecimal representation of the bytes object content. Parameters: data (list): A list containing strings and/or integers. Returns: tuple: A tuple containing three elements: - A bytes object representing the concatenated content of input data. - An integer representing the size of the bytes object. - A string representing the hexadecimal content of the bytes object. bytes_list = [] for item in data: if isinstance(item, str): bytes_list.append(item.encode(\'ascii\')) elif isinstance(item, int) and 0 <= item < 256: bytes_list.append(bytes([item])) else: raise ValueError(\'List should contain only ASCII strings and integers between 0 and 255\') concatenated_bytes = b\'\'.join(bytes_list) size = len(concatenated_bytes) hex_representation = binascii.hexlify(concatenated_bytes).decode(\'ascii\') return (concatenated_bytes, size, hex_representation)"},{"question":"# Event Scheduling and Execution with `sched` Module You are tasked with implementing a function that uses the `sched` module to schedule a series of events based on provided information, manage the event queue effectively, and execute the events in the correct order. Function Signature ```python def schedule_events(event_list, start_time): # event_list: List of dictionaries, each containing the keys: # - \\"type\\": A string, either \\"absolute\\" or \\"delay\\", indicating if the event scheduling is using absolute time or delay. # - \\"time\\": An integer, representing the time for scheduling. For \\"absolute\\", it\'s a timestamp; for \\"delay\\", it\'s the amount of delay in seconds. # - \\"priority\\": An integer, representing the priority of the event (lower values have higher priority). # - \\"action\\": A callable function that will be executed when the event is triggered. # - \\"args\\": A tuple, representing the positional arguments for the `action` function. # - \\"kwargs\\": A dictionary, representing the keyword arguments for the `action` function. # # start_time: A float representing the starting reference time for the scheduler. # The function should schedule all the events in event_list, # execute all the events according to their schedule, and return # a list of results of each event in the order they were executed. ``` Input Description - `event_list`: A list of dictionaries where each dictionary contains: - `\\"type\\"`: `\\"absolute\\"` for absolute scheduling using `enterabs()` or `\\"delay\\"` for relative scheduling using `enter()`. - `\\"time\\"`: An integer representing the timestamp for absolute scheduling or the number of seconds for delay scheduling. - `\\"priority\\"`: An integer representing the priority of the event. - `\\"action\\"`: A callable function to be executed when the event is triggered. - `\\"args\\"`: A tuple containing the positional arguments for the action function. - `\\"kwargs\\"`: A dictionary containing the keyword arguments for the action function. - `start_time`: A float representing the starting reference time for the scheduler. Output Description - The function should return a list of results from each `action` executed by the scheduler, in the order they were executed. Constraints - The `event_list` will have at least one event and no more than 100 events. - Each callable action should complete execution within a reasonable time frame (less than 1 second). Example ```python import time def event1(arg1, arg2): return f\\"Event1: {arg1}, {arg2}\\" def event2(arg1, arg2): return f\\"Event2: {arg1}, {arg2}\\" event_list = [ {\\"type\\": \\"absolute\\", \\"time\\": start_time + 5, \\"priority\\": 1, \\"action\\": event1, \\"args\\": (1, 2), \\"kwargs\\": {}}, {\\"type\\": \\"delay\\", \\"time\\": 10, \\"priority\\": 2, \\"action\\": event2, \\"args\\": (3, 4), \\"kwargs\\": {}} ] start_time = time.time() results = schedule_events(event_list, start_time) print(results) # Example output: [\'Event1: 1, 2\', \'Event2: 3, 4\'] ```","solution":"import sched import time def schedule_events(event_list, start_time): scheduler = sched.scheduler(timefunc=time.time) results = [] def create_action(action, args, kwargs): def wrapped_action(): result = action(*args, **kwargs) results.append(result) return wrapped_action for event in event_list: if event[\\"type\\"] == \\"absolute\\": absolute_time = event[\\"time\\"] scheduler.enterabs(absolute_time, event[\\"priority\\"], create_action(event[\\"action\\"], event[\\"args\\"], event[\\"kwargs\\"])) elif event[\\"type\\"] == \\"delay\\": delay_time = event[\\"time\\"] event_time = start_time + delay_time scheduler.enterabs(event_time, event[\\"priority\\"], create_action(event[\\"action\\"], event[\\"args\\"], event[\\"kwargs\\"])) scheduler.run() return results"},{"question":"**Objective**: Assess students\' understanding of validation and learning curves using the scikit-learn package. Problem Statement: You are provided with the Iris dataset, a popular dataset in machine learning. Your task is to: 1. Generate a **validation curve** for an SVM model with a linear kernel by varying the hyperparameter `C` over a specified range. 2. Generate a **learning curve** for the same SVM model by varying the number of training samples. 3. Interpret the results to determine if the model suffers from high bias, high variance, or if it\'s well-balanced. Instructions: 1. Import the necessary libraries and load the Iris dataset. 2. Shuffle the dataset to ensure random distribution of samples. 3. Generate a validation curve using: - `param_name=\\"C\\"` - `param_range=np.logspace(-7, 3, 3)` (i.e., values of C ranging from (10^{-7}) to (10^3)) - Plot the validation curve using `ValidationCurveDisplay`. 4. Generate a learning curve using: - `train_sizes=[50, 80, 110]` (i.e., using 50, 80, and 110 samples for training). - Plot the learning curve using `LearningCurveDisplay`. 5. Analyze and interpret the plots: - Determine if the model is underfitting, overfitting, or well-fitted. - Provide reasoning for your conclusions. Code Template: ```python import numpy as np from sklearn.model_selection import validation_curve, learning_curve from sklearn.model_selection import ValidationCurveDisplay, LearningCurveDisplay from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # 1. Generate validation curve param_range = np.logspace(-7, 3, 3) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) # Plot validation curve ValidationCurveDisplay.from_estimator( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) # (Your plotting code here if needed) # 2. Generate learning curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\\"linear\\"), X, y, train_sizes=[50, 80, 110], cv=5 ) # Plot learning curve LearningCurveDisplay.from_estimator( SVC(kernel=\\"linear\\"), X, y, train_sizes=[50, 80, 110], cv=5 ) # (Your plotting code here if needed) # 3. Interpretation: # Analyze the resulting curves (underfitting, overfitting, or well-fitted) # Provide your reasoning below: Your analysis and interpretation of the validation and learning curves. ``` Constraints: - Use a random state of 0 wherever applicable to ensure reproducibility. Expected Output: - Two plots: one for the validation curve and one for the learning curve - A written analysis of the plots specifying if the model is underfitting, overfitting, or well-fitted, and your reasoning for the same.","solution":"import numpy as np from sklearn.model_selection import validation_curve, learning_curve from sklearn.model_selection import ValidationCurveDisplay, LearningCurveDisplay from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # 1. Generate validation curve param_range = np.logspace(-7, 3, 3) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) # Calculate mean and standard deviation train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) valid_mean = np.mean(valid_scores, axis=1) valid_std = np.std(valid_scores, axis=1) # Plot validation curve plt.figure() plt.title(\\"Validation Curve with SVM\\") plt.xlabel(\\"C\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.semilogx(param_range, train_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\\"darkorange\\", lw=lw) plt.semilogx(param_range, valid_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.fill_between(param_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.show() # 2. Generate learning curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\\"linear\\"), X, y, train_sizes=[50, 80, 110], cv=5 ) # Calculate mean and standard deviation train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) valid_mean = np.mean(valid_scores, axis=1) valid_std = np.std(valid_scores, axis=1) # Plot learning curve plt.figure() plt.title(\\"Learning Curve with SVM\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.plot(train_sizes, train_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color=\\"darkorange\\", lw=lw) plt.plot(train_sizes, valid_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.fill_between(train_sizes, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.show() # 3. Interpretation: # Analyze the resulting curves (underfitting, overfitting, or well-fitted) # Provide your reasoning below: From the validation curve, we observe that the training score remains high across different values of C, but the cross-validation score is lower for extreme values of C (both very small and very large), suggesting that the model may be vulnerable to underfitting for very low values of C and overfitting for very high values of C. An intermediate value of C yields the best generalization performance. From the learning curve, the training score is consistently high, while the cross-validation score increases with the number of training samples but remains below the training score. This indicates that the model likely suffers from high variance as it performs well on the training data but less well on new, unseen data. Increasing the training data helps improve performance, but there\'s still a noticeable gap between the training and cross-validation scores. Overall, the SVM model with a linear kernel demonstrates a tendency towards high variance, indicated by its high training score and comparatively lower cross-validation score. Adjusting the hyperparameter C to an optimal level can improve the model\'s balance and potentially enhance its performance on unseen data."},{"question":"# Pandas Styling Challenge You are given a DataFrame containing sales data for various products over different months. Your task is to style this DataFrame to improve its readability and export the styled DataFrame as an HTML file. Input 1. A pandas DataFrame `sales_data` with the following columns: `\'Product\'`, `\'Month\'`, `\'Sales\'`. 2. The DataFrame contains sales data for several products across multiple months. Requirements 1. Highlight the maximum sales value in each row with a green background. 2. Highlight the minimum sales value in each row with a red background. 3. Apply a gradient background based on the sales value from light blue (low sales) to dark blue (high sales). 4. Add a caption at the top of the table that reads \\"Monthly Sales Data\\". 5. Export the final styled DataFrame to an HTML file named `styled_sales_data.html`. Constraints - Use built-in methods from the `Styler` class for styling the DataFrame. - Ensure that the styles are visually distinct and readable. Expected Outputs - A styled HTML file (`styled_sales_data.html`) containing the DataFrame with the specified styling. Example Usage Assume the `sales_data` DataFrame is as follows: ```python import pandas as pd data = { \'Product\': [\'A\', \'B\', \'C\'], \'January\': [100, 150, 200], \'February\': [90, 180, 250], \'March\': [110, 140, 220] } sales_data = pd.DataFrame(data) sales_data.set_index(\'Product\', inplace=True) ``` The expected styled HTML should highlight the maximum and minimum sales per row and apply the gradient styling to all sales values. Implementation ```python import pandas as pd def style_sales_data(sales_data): # Your implementation here # Step 1: Highlight max and min in each row styler = sales_data.style styler = styler.highlight_max(axis=1, color=\'green\') styler = styler.highlight_min(axis=1, color=\'red\') # Step 2: Apply a gradient based on sales values styler = styler.background_gradient(cmap=\'Blues\') # Step 3: Add a caption styler = styler.set_caption(\\"Monthly Sales Data\\") # Step 4: Export to HTML styler.to_html(\'styled_sales_data.html\') # Example usage with the sample DataFrame data = { \'Product\': [\'A\', \'B\', \'C\'], \'January\': [100, 150, 200], \'February\': [90, 180, 250], \'March\': [110, 140, 220] } sales_data = pd.DataFrame(data) sales_data.set_index(\'Product\', inplace=True) style_sales_data(sales_data) ``` In this challenge, students are expected to demonstrate their understanding of the `Styler` class in pandas by implementing styling rules and exporting the styled DataFrame.","solution":"import pandas as pd import numpy as np def style_sales_data(sales_data): Styles the given sales data DataFrame for better readability and exports the styled DataFrame to an HTML file. Parameters: sales_data (pd.DataFrame): DataFrame with \'Product\', \'Month\', \'Sales\' columns # Set the index to \'Product\' for better readability if not already set if \'Product\' in sales_data.columns: sales_data.set_index(\'Product\', inplace=True) # Function to highlight maximum values in green and minimum values in red def highlight_max_min(s): is_max = s == s.max() is_min = s == s.min() return [\'background-color: green\' if v else \'background-color: red\' if m else \'\' for v, m in zip(is_max, is_min)] # Create a Styler object styler = sales_data.style # Apply highlight function for max and min values styler = styler.apply(highlight_max_min, axis=1) # Apply gradient background based on sales value styler = styler.background_gradient(cmap=\'Blues\') # Add a caption styler = styler.set_caption(\\"Monthly Sales Data\\") # Export to HTML styler.to_html(\'styled_sales_data.html\') # Example usage with the sample DataFrame data = { \'Product\': [\'A\', \'B\', \'C\'], \'January\': [100, 150, 200], \'February\': [90, 180, 250], \'March\': [110, 140, 220] } sales_data = pd.DataFrame(data) style_sales_data(sales_data)"},{"question":"Advanced Bar Plotting with Seaborn **Objective:** Create a seaborn bar plot that demonstrates a comprehensive understanding of various features such as grouping, aggregation, error bars, annotations, and customization. **Description:** You are given a dataset named `flights` containing monthly passenger numbers for different years. Your task is to create a customized bar plot that provides the following functionalities: 1. Load the `flights` dataset using the `seaborn` library. 2. Plot the total number of passengers for each year using a bar plot. 3. Include error bars that represent the standard deviation of the data points. 4. Add text labels on top of each bar showing the total passenger count. 5. Customize the appearance of the bar plot: - Set the bar color to a shade of blue (`facecolor`). - Set the error bar color to black (`err_kws`). - Include a cap on the error bars. 6. Save the plot as a PNG file named `custom_flights_plot.png`. **Constraints:** 1. Use the seaborn library only for plotting. 2. Ensure the plot is of high quality (appropriate DPI). 3. The plot size should be set to a width of 10 inches and a height of 6 inches. 4. Annotate the plot to highlight a specific year, 1955, with a red star marker on the top of the bar. **Input:** You do not need to provide any input as the dataset will be loaded from within the seaborn library. **Output:** The output should be a PNG file saved in the current working directory. # Expected Function Signature: ```python def create_custom_flights_plot(): pass ``` # Example: The resulting plot should show bars for each year, error bars for standard deviation, value labels on each bar, and a red star on the bar for the year 1955. ```python create_custom_flights_plot() ``` **Hint:** - You may find the examples provided in the seaborn documentation helpful for setting error bars, customizing plot appearance, and adding annotations.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_custom_flights_plot(): # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Aggregate the data yearly_data = flights.groupby(\\"year\\").agg({\\"passengers\\": [\\"sum\\", \\"std\\"]}).reset_index() yearly_data.columns = [\\"year\\", \\"total_passengers\\", \\"std_dev\\"] # Plot size and DPI plt.figure(figsize=(10, 6), dpi=100) # Create the bar plot bar_plot = sns.barplot( x=\\"year\\", y=\\"total_passengers\\", data=yearly_data, color=\\"skyblue\\", capsize=0.1, errcolor=\\"black\\" ) # Add value annotations on top of bars for index, row in yearly_data.iterrows(): bar_plot.text( index, row.total_passengers, f\'{int(row.total_passengers)}\', color=\'black\', ha=\\"center\\" ) # Highlight the year 1955 with a red star marker year_1955_index = yearly_data[yearly_data.year == 1955].index[0] total_1955 = yearly_data.total_passengers[year_1955_index] bar_plot.plot( year_1955_index, total_1955, marker=\'*\', color=\'red\', markersize=15, label=\\"1955\\" ) # Customize plot appearance plt.xlabel(\\"Year\\") plt.ylabel(\\"Total Passengers\\") plt.title(\\"Total Number of Passengers per Year\\") plt.legend() # Save the plot as a PNG file plt.savefig(\\"custom_flights_plot.png\\") plt.close()"},{"question":"You are tasked with creating a custom content manager for handling additional data types in email messages using the `email.contentmanager` module in Python. # Objective: Implement a custom content manager called `CustomContentManager` that extends `email.contentmanager.ContentManager`. Your content manager should handle: 1. JSON objects: Convert JSON strings to Python dictionaries when getting content, and convert Python dictionaries to JSON strings when setting content. 2. HTML content: Ensure HTML content is handled correctly by properly encoding and decoding HTML strings. # Details: 1. **get_content method**: - For MIME type `application/json`, decode the content to a Python dictionary. - For MIME type `text/html`, decode the content to a string. - Raise a `KeyError` if the MIME type is not supported. 2. **set_content method**: - Allow setting content for Python dictionaries by converting them to JSON strings with MIME type `application/json`. - Allow setting content for HTML content by converting strings to encoded HTML with MIME type `text/html`. - Raise a `TypeError` if the content is not supported. # Expected Input and Output: - **Input**: - `set_content` method: `msg` (message object), `obj` (object to be set), and optional arguments such as `subtype`, `charset`, etc. - `get_content` method: `msg` (message object) and optional arguments. - **Output**: - `set_content`: Adds the content with appropriate MIME type to the message object. - `get_content`: Returns the decoded content from the message object. # Constraints: - You may use `json` and `html` libraries for JSON and HTML handling, respectively. - Ensure that the custom content manager is integrated correctly with the existing email package. # Performance Requirements: - Efficiently handle MIME type lookup and conversion processes. - Ensure that any errors are handled gracefully with appropriate exceptions. # Example Usage: ```python from email.message import EmailMessage from email.contentmanager import ContentManager import json class CustomContentManager(ContentManager): def get_content(self, msg, *args, **kw): mimetype = msg.get_content_type() if mimetype == \'application/json\': return json.loads(msg.get_payload(decode=True)) elif mimetype == \'text/html\': return msg.get_payload(decode=True).decode(\'utf-8\') else: raise KeyError(f\\"No handler for MIME type: {mimetype}\\") def set_content(self, msg, obj, *args, **kw): if isinstance(obj, dict): json_str = json.dumps(obj) msg.set_payload(json_str, charset=\'utf-8\') msg.set_type(\'application/json\') elif isinstance(obj, str) and \'html\' in kw.get(\'subtype\', \'plain\'): msg.set_payload(obj, charset=\'utf-8\') msg.set_type(\'text/html\') else: raise TypeError(f\\"No handler for object type: {type(obj).__name__}\\") # Test Example: email_message = EmailMessage() manager = CustomContentManager() # Setting JSON content manager.set_content(email_message, {\\"key\\": \\"value\\"}) assert \'application/json\' in email_message.get(\'Content-Type\') # Getting JSON content content = manager.get_content(email_message) assert content == {\\"key\\": \\"value\\"} # Setting HTML content manager.set_content(email_message, \\"<h1>Hello World</h1>\\", subtype=\'html\') assert \'text/html\' in email_message.get(\'Content-Type\') # Getting HTML content content = manager.get_content(email_message) assert content == \\"<h1>Hello World</h1>\\" ``` Implement `CustomContentManager` as described, ensuring it integrates with the `EmailMessage` class\'s `get_content` and `set_content` methods effectively.","solution":"from email.contentmanager import ContentManager from email.message import EmailMessage import json class CustomContentManager(ContentManager): def get_content(self, msg, *args, **kw): mimetype = msg.get_content_type() if mimetype == \'application/json\': return json.loads(msg.get_payload(decode=True)) elif mimetype == \'text/html\': return msg.get_payload(decode=True).decode(\'utf-8\') else: raise KeyError(f\\"No handler for MIME type: {mimetype}\\") def set_content(self, msg, obj, *args, **kw): if isinstance(obj, dict): json_str = json.dumps(obj) msg.set_payload(json_str, charset=\'utf-8\') msg.set_type(\'application/json\') elif isinstance(obj, str) and \'html\' in kw.get(\'subtype\', \'plain\'): msg.set_payload(obj, charset=\'utf-8\') msg.set_type(\'text/html\') else: raise TypeError(f\\"No handler for object type: {type(obj).__name__}\\")"},{"question":"**Objective**: Implement a set of functions to manipulate and analyze dates and times, specifically focusing on handling timezones and time duration calculations. **Problem Statement**: You are required to implement the following functions: 1. `calculate_future_date(start_date: str, delta_days: int) -> str` 2. `convert_to_timezone(date_str: str, target_timezone_str: str) -> str` 3. `total_seconds_between_dates(date1: str, date2: str) -> int` **Function Details**: 1. `calculate_future_date(start_date: str, delta_days: int) -> str` Given a start date in the format `YYYY-MM-DD` and a number of days (`delta_days`), this function should return the new date after adding the specified number of days as a string in the same format. **Input**: - `start_date`: A string representing the start date in the format `YYYY-MM-DD`. - `delta_days`: An integer that specifies the number of days to add. **Output**: - A string representing the new date in the format `YYYY-MM-DD`. 2. `convert_to_timezone(date_str: str, target_timezone_str: str) -> str` Given a date and time in the format `YYYY-MM-DDTHH:MM:SS` and a target timezone string (e.g., `UTC`, `EST`, `PST`, etc.), this function should convert the date and time to the target timezone and return it as a string in the format `YYYY-MM-DDTHH:MM:SS+ZZ:ZZ`. **Input**: - `date_str`: A string representing the date and time in the format `YYYY-MM-DDTHH:MM:SS`. - `target_timezone_str`: A string representing the target timezone (e.g., `UTC`, `EST`, `PST`). **Output**: - A string representing the converted date and time in the format `YYYY-MM-DDTHH:MM:SS+ZZ:ZZ`. 3. `total_seconds_between_dates(date1: str, date2: str) -> int` Given two dates and times in the format `YYYY-MM-DDTHH:MM:SS`, this function should calculate the total number of seconds between them. **Input**: - `date1`: A string representing the first date and time in the format `YYYY-MM-DDTHH:MM:SS`. - `date2`: A string representing the second date and time in the format `YYYY-MM-DDTHH:MM:SS`. **Output**: - An integer representing the total number of seconds between the two dates and times. **Constraints**: - `start_date`, `date_str`, `date1`, `date2` will all be valid strings in the specified formats. - `delta_days` is an integer. - `target_timezone_str` will be one of the valid timezone abbreviations (e.g., `UTC`, `EST`, `PST`). **Example**: ```python # Example usage print(calculate_future_date(\\"2023-01-01\\", 10)) # Output: \\"2023-01-11\\" print(convert_to_timezone(\\"2023-01-01T00:00:00\\", \\"EST\\")) # Output: \\"2022-12-31T19:00:00-05:00\\" print(total_seconds_between_dates(\\"2023-01-01T00:00:00\\", \\"2023-01-02T00:00:00\\")) # Output: 86400 ``` **Notes**: - You may make use of the `datetime`, `timedelta`, and `timezone` classes from the `datetime` module. - Ensure your functions handle both aware and naive datetime objects as appropriate.","solution":"from datetime import datetime, timedelta import pytz def calculate_future_date(start_date: str, delta_days: int) -> str: Returns the new date after adding delta_days to the start_date. start_date_obj = datetime.strptime(start_date, \'%Y-%m-%d\') future_date_obj = start_date_obj + timedelta(days=delta_days) future_date_str = future_date_obj.strftime(\'%Y-%m-%d\') return future_date_str def convert_to_timezone(date_str: str, target_timezone_str: str) -> str: Converts the given date and time to the target timezone. original_date_obj = datetime.strptime(date_str, \'%Y-%m-%dT%H:%M:%S\') target_timezone = pytz.timezone(target_timezone_str) original_date_obj = pytz.utc.localize(original_date_obj) # Assuming input date is in UTC converted_date_obj = original_date_obj.astimezone(target_timezone) converted_date_str = converted_date_obj.strftime(\'%Y-%m-%dT%H:%M:%S%z\') return converted_date_str def total_seconds_between_dates(date1: str, date2: str) -> int: Returns the total number of seconds between two dates and times. date1_obj = datetime.strptime(date1, \'%Y-%m-%dT%H:%M:%S\') date2_obj = datetime.strptime(date2, \'%Y-%m-%dT%H:%M:%S\') delta = date2_obj - date1_obj return int(delta.total_seconds())"},{"question":"**Objective:** Write a Python function that demonstrates the creation and management of an asynchronous TCP server and client using `asyncio`. Your implementation should incorporate creating tasks, handling callbacks, and performing network I/O operations. Task: 1. Implement an asynchronous TCP server that: - Listens on a specified port. - Accepts incoming client connections. - Responds with a welcome message to each connected client. - Echoes any message received from the client back to them. 2. Implement an asynchronous TCP client that: - Connects to the server. - Sends a preset message to the server. - Reads the response from the server and prints it. 3. Demonstrate starting the server and client, ensuring proper shutdown of both irrespective of errors. Specifications: 1. **Function signature:** ```python async def run_tcp_server(port: int): Start and run an asynchronous TCP server. Args: - port (int): Port number on which the server should listen. async def tcp_client(port: int, message: str): Start and run an asynchronous TCP client. Args: - port (int): Port number on which the server is listening. - message (str): Message to be sent to the server. def main(): Entry point for starting the server and client. ``` 2. **Input/Output Format:** - The `run_tcp_server` function should listen on the provided port and manage client connections asynchronously. - The `tcp_client` function should send the specified message to the server and print the server\'s response. - The `main` function should demonstrate starting the server on a specific port and the client sending a message to it. 3. **Constraints:** - Use `asyncio` methods and classes to implement asynchronous behavior. - Handle exceptions and ensure graceful shutdown of both server and client. - The function should operate as part of a script without any user interaction. Example: ```python import asyncio import signal async def run_tcp_server(port: int): # Your implementation here async def tcp_client(port: int, message: str): # Your implementation here def main(): # Port for server-client communication port = 8888 # Start the server server_task = asyncio.ensure_future(run_tcp_server(port)) # Client sends a message client_task = asyncio.ensure_future(tcp_client(port, \\"Hello, Server!\\")) # Run everything loop = asyncio.get_event_loop() # Add signal handlers for graceful shutdown for signame in {\'SIGINT\', \'SIGTERM\'}: loop.add_signal_handler(getattr(signal, signame), loop.stop) try: loop.run_forever() finally: loop.run_until_complete(loop.shutdown_asyncgens()) loop.close() if __name__ == \\"__main__\\": main() ``` Submission: Submit a python file containing the above function definitions and the `main` function to test. Ensure that running your script demonstrates both server and client interactions correctly.","solution":"import asyncio async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"New client connected: {addr}\\") welcome_message = \\"Welcome to the async TCP server!\\" writer.write(welcome_message.encode()) await writer.drain() while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received from {addr}: {message}\\") writer.write(data) # Echo back the received message await writer.drain() print(f\\"Client {addr} disconnected\\") writer.close() await writer.wait_closed() async def run_tcp_server(port: int): server = await asyncio.start_server(handle_client, \'127.0.0.1\', port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def tcp_client(port: int, message: str): reader, writer = await asyncio.open_connection(\'127.0.0.1\', port) print(f\'Connected to server on port {port}\') print(f\'Sending message to server: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) print(f\'Received from server: {data.decode()}\') print(\'Closing the connection\') writer.close() await writer.wait_closed() def main(): port = 8888 loop = asyncio.get_event_loop() server_task = asyncio.ensure_future(run_tcp_server(port)) client_task = asyncio.ensure_future(tcp_client(port, \\"Hello, Server!\\")) try: loop.run_until_complete(asyncio.gather(server_task, client_task)) except KeyboardInterrupt: pass finally: tasks = asyncio.all_tasks(loop) for task in tasks: task.cancel() loop.run_until_complete(asyncio.gather(*tasks, return_exceptions=True)) loop.run_until_complete(loop.shutdown_asyncgens()) loop.close() if __name__ == \\"__main__\\": main()"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},F={class:"search-container"},z={class:"card-container"},q={key:0,class:"empty-state"},M=["disabled"],R={key:0},N={key:1};function O(i,e,l,m,s,o){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",F,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>s.searchQuery=r),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>s.searchQuery="")}," ✕ ")):d("",!0)]),t("div",z,[(a(!0),n(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),n("div",q,' No results found for "'+u(s.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[s.isLoading?(a(),n("span",N,"Loading...")):(a(),n("span",R,"See more"))],8,M)):d("",!0)])}const L=p(D,[["render",O],["__scopeId","data-v-f03b410c"]]),U=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/42.md","filePath":"chatai/42.md"}'),j={name:"chatai/42.md"},H=Object.assign(j,{setup(i){return(e,l)=>(a(),n("div",null,[x(L)]))}});export{U as __pageData,H as default};
