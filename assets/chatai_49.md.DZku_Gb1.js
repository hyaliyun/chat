import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(i,e,l,m,n,r){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-b84c1e8f"]]),z=JSON.parse('[{"question":"**Question: Double Conversion and String Formatting** Implement a Python function `convert_and_format_double` that takes a string representing a floating-point number and converts it to a float. The function should then format this float into a string using a specified format code and precision. Additionally, the function should handle potential exceptions that could arise during conversion and ensure the output is as specified. # Function Definition ```python def convert_and_format_double(s: str, format_code: str, precision: int, flags: int = 0) -> str: Convert a string representing a floating-point number to a float, and format it to a string using the given format code and precision. Parameters: s (str): The string representing the floating-point number. format_code (str): The format code to use for string conversion (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'). precision (int): The number of digits of precision desired. flags (int): Flags to modify the resulting format (defaults to 0). Returns: str: The formatted float as a string, or an appropriate error message if conversion fails. pass ``` # Input 1. `s` (str): A string representing a valid floating-point number (e.g., \'123.456\', \'1e10\'). 2. `format_code` (str): A single character format code, which must be one of `\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, `\'r\'`. 3. `precision` (int): The number of digits of precision to include in the formatted string. 4. `flags` (int): An integer representing flags for formatting (optional, default is 0). # Output A string that represents the formatted version of the converted float or an appropriate error message if the conversion fails. # Constraints - You may assume that the input string `s` is always a valid floating-point number. - The `format_code` should always be one of the accepted values. - `precision` will be an integer within a reasonable range for float precision (e.g., 0 <= precision <= 20). - If conversion fails, the function should return \\"Conversion Error\\". - If formatting fails, the function should return \\"Formatting Error\\". # Example ```python assert convert_and_format_double(\\"123.456\\", \'f\', 2) == \\"123.46\\" assert convert_and_format_double(\\"1e10\\", \'E\', 1) == \\"1.0E+10\\" assert convert_and_format_double(\\"not_a_number\\", \'g\', 3) == \\"Conversion Error\\" ``` # Notes - Use appropriate error handling mechanisms to catch and handle errors during conversion and formatting. - Consider edge cases such as extremely large values, invalid format codes, etc.","solution":"def convert_and_format_double(s: str, format_code: str, precision: int, flags: int = 0) -> str: Convert a string representing a floating-point number to a float, and format it to a string using the given format code and precision. Parameters: s (str): The string representing the floating-point number. format_code (str): The format code to use for string conversion (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'). precision (int): The number of digits of precision desired. flags (int): Flags to modify the resulting format (defaults to 0). Returns: str: The formatted float as a string, or an appropriate error message if conversion fails. try: value = float(s) except ValueError: return \\"Conversion Error\\" try: format_spec = f\\".{precision}{format_code}\\" return format(value, format_spec) except (ValueError, TypeError): return \\"Formatting Error\\""},{"question":"Problem Statement: You are required to implement a custom iterator class in Python, which replicates the functionality discussed in the documentation. 1. **SeqIterator**: This class should iterate over a sequence until an `IndexError` is thrown. 2. **CallIterator**: This class should iterate over a callable object until a sentinel value is returned. Requirements: 1. Implement a class `SeqIterator`: - **Constructor**: `__init__(self, seq: Sequence)` - **Methods**: - `__iter__(self) -> SeqIterator`: Returns the iterator object. - `__next__(self) -> Any`: Returns the next item in the sequence. Raises `StopIteration` if the sequence ends. 2. Implement a class `CallIterator`: - **Constructor**: `__init__(self, callable: Callable[[], Any], sentinel: Any)` - **Methods**: - `__iter__(self) -> CallIterator`: Returns the iterator object. - `__next__(self) -> Any`: Returns the next item from the callable. Raises `StopIteration` when the sentinel value is returned. Input and Output Format: - For `SeqIterator`: - **Input**: ```python seq = [1, 2, 3] seq_iter = SeqIterator(seq) ``` - **Output**: Calling `next(seq_iter)` repeatedly should return `1`, `2`, `3`, and then raise `StopIteration`. - For `CallIterator`: - **Input**: ```python def gen(): for i in range(4): return i sentinel = -1 call_iter = CallIterator(gen, sentinel) ``` - **Output**: Calling `next(call_iter)` repeatedly should return `0`, `1`, `2`, `3`, and then raise `StopIteration`. Constraints: - The sequence provided to `SeqIterator` can be any iterable (e.g., list, tuple, etc.). - The callable object provided to `CallIterator` must be a function with no parameters. - Ensure that your implementation handles edge cases, such as empty sequences or callable objects that immediately return the sentinel value. Performance Requirements: - Your iterators should have a time complexity of O(1) for the `__next__` method. Implement the classes as described and demonstrate their usage with example inputs and expected outputs.","solution":"from typing import Sequence, Callable, Any class SeqIterator: def __init__(self, seq: Sequence): self.seq = seq self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.seq): result = self.seq[self.index] self.index += 1 return result else: raise StopIteration class CallIterator: def __init__(self, callable: Callable[[], Any], sentinel: Any): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result # Example usage: # seq = [1, 2, 3] # seq_iter = SeqIterator(seq) # for value in seq_iter: # print(value) # # def gen(): # for i in range(4): # yield i # # sentinel = -1 # call_iter = CallIterator(gen().__next__, sentinel) # for value in call_iter: # print(value)"},{"question":"**Objective**: Demonstrate your understanding of seaborn by creating a complex, well-customized plot. **Scenario**: You are given a dataset containing two continuous variables, `X` and `Y`, and one categorical variable, `Category`. The dataset has 300 observations. Your task is to visualize the relationship between `X` and `Y` using seaborn, customizing the plot to ensure clarity and visual appeal. **Input**: A pandas DataFrame `df` with the following structure: - `X`: Continuous variable - `Y`: Continuous variable - `Category`: Categorical variable (up to 4 unique categories) **Requirements**: 1. Create a scatter plot to visualize the relationship between `X` and `Y`. 2. Use different colors for each category in `Category`. 3. Fit and plot a regression line for each category. 4. Use a dark-themed color palette for the categories. 5. Include appropriate labels for the axes and a legend. 6. Ensure the plot has a grid and customize the grid\'s appearance for better readability. **Constraints**: - Do not use Seaborn’s default settings for the plot. Customize the appearance based on the requirements above. **Output**: - Return the seaborn plot object. **Example**: Here\'s a sample function signature to get you started: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_relationship(df: pd.DataFrame): sns.set_theme(style=\\"darkgrid\\") # Define a dark-themed color palette with a sufficient number of colors palette = sns.dark_palette(\\"seagreen\\", n_colors=len(df[\\"Category\\"].unique())) # Create a scatter plot with a regression line per category sns.lmplot( data=df, x=\'X\', y=\'Y\', hue=\'Category\', palette=palette, height=7, aspect=1.2, markers=\'o\', scatter_kws={\\"s\\": 50, \\"alpha\\": 0.7}, line_kws={\\"lw\\": 2}) # Customizing the appearance of the plot plt.xlabel(\\"X-axis Label\\") plt.ylabel(\\"Y-axis Label\\") plt.title(\\"Scatter Plot of X vs. Y with Regression Lines by Category\\") plt.legend(title=\'Category\', loc=\'upper left\') plt.grid(True, linestyle=\\"--\\", linewidth=0.5, alpha=0.7) # Show plot plt.show() return plt ``` Use this template and modify it to meet all the requirements.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_relationship(df: pd.DataFrame): Creates a scatter plot to visualize the relationship between continuous variables X and Y with categories differentiated by colors. Fits and plots a regression line for each category. sns.set_theme(style=\\"darkgrid\\") # Define a dark-themed color palette with a sufficient number of colors palette = sns.color_palette(\\"dark\\", n_colors=df[\\"Category\\"].nunique()) # Create a scatter plot with a regression line per category plot = sns.lmplot( data=df, x=\'X\', y=\'Y\', hue=\'Category\', palette=palette, height=7, aspect=1.2, markers=\'o\', scatter_kws={\\"s\\": 50, \\"alpha\\": 0.7}, line_kws={\\"lw\\": 2} ) # Customizing the appearance of the plot plot.set_axis_labels(\\"X-axis\\", \\"Y-axis\\") plt.title(\\"Scatter Plot of X vs. Y with Regression Lines by Category\\") plt.legend(title=\'Category\', loc=\'upper left\') plt.grid(True, linestyle=\\"--\\", linewidth=0.5, alpha=0.7) return plot"},{"question":"# PyTorch Coding Assessment **Objective**: Demonstrate your ability to utilize PyTorch\'s `torch.func` functionality, particularly focusing on `vmap` and `grad`, while adhering to the limitations and constraints described. # Question You are to write a function `batched_cosine_similarity` that computes the cosine similarity between two batches of vectors. Implement this using PyTorch\'s `vmap` for batch processing and the `grad` function to compute the gradient of the cosine similarity with respect to the input vectors. Function Signature ```python def batched_cosine_similarity(batch1: torch.Tensor, batch2: torch.Tensor) -> torch.Tensor: Parameters: - batch1 (torch.Tensor): A tensor of shape (n, d) containing n vectors each of dimension d. - batch2 (torch.Tensor): A tensor of shape (n, d) containing n vectors each of dimension d. Returns: - torch.Tensor: A tensor of shape (n,) containing the cosine similarity for each pair of vectors. pass ``` # Instructions 1. **Inputs**: - `batch1`: A tensor of shape `(n, d)` where `n` is the number of vectors and `d` is the dimension of each vector. - `batch2`: A tensor of shape `(n, d)`. 2. **Outputs**: - Return a tensor of shape `(n,)` where each element is the cosine similarity between the corresponding vectors in `batch1` and `batch2`. 3. **Constraints**: - Use `vmap` to achieve batch processing. - Ensure your implementation does not involve assigning to global variables. - Handle the calculation purely within the function without any in-place operations that `vmap` might not support. - Cosine similarity is defined as: [ text{cosine_similarity}(x, y) = frac{x cdot y}{|x|_2 |y|_2} ] 4. **Gradient Calculation**: - After implementing `batched_cosine_similarity`, use `grad` to compute the gradient of the cosine similarity with respect to the first input `batch1`. # Example ```python import torch from torch.func import vmap, grad # Example inputs batch1 = torch.randn(5, 3) # 5 vectors of dimension 3 batch2 = torch.randn(5, 3) # Implement the function to compute batched cosine similarity def batched_cosine_similarity(batch1, batch2): def cosine_sim(x, y): return torch.dot(x, y) / (torch.norm(x) * torch.norm(y)) vmap_cosine_sim = vmap(cosine_sim, in_dims=(0, 0)) return vmap_cosine_sim(batch1, batch2) # Example usage cosine_similarities = batched_cosine_similarity(batch1, batch2) print(cosine_similarities) # Compute the gradient of the batched cosine similarity with respect to batch1 def func_for_grad(batch1): return batched_cosine_similarity(batch1, batch2).sum() grad_fn = grad(func_for_grad) batch1_grad = grad_fn(batch1) print(batch1_grad) ``` Your task is to implement the `batched_cosine_similarity` function correctly, adhering to the given constraints and requirements.","solution":"import torch from torch.func import vmap, grad def batched_cosine_similarity(batch1: torch.Tensor, batch2: torch.Tensor) -> torch.Tensor: Computes the cosine similarity between two batches of vectors using vmap for batch processing. Parameters: - batch1 (torch.Tensor): A tensor of shape (n, d) containing n vectors each of dimension d. - batch2 (torch.Tensor): A tensor of shape (n, d) containing n vectors each of dimension d. Returns: - torch.Tensor: A tensor of shape (n,) containing the cosine similarity for each pair of vectors. def cosine_sim(x, y): return torch.dot(x, y) / (torch.norm(x) * torch.norm(y)) vmap_cosine_sim = vmap(cosine_sim, in_dims=(0, 0)) return vmap_cosine_sim(batch1, batch2) # Example for gradient calculation def grad_batched_cosine_similarity(batch1: torch.Tensor, batch2: torch.Tensor) -> torch.Tensor: def func_for_grad(batch1): return batched_cosine_similarity(batch1, batch2).sum() grad_fn = grad(func_for_grad) return grad_fn(batch1) # Example usage if __name__ == \\"__main__\\": batch1 = torch.randn(5, 3) batch2 = torch.randn(5, 3) cosine_similarities = batched_cosine_similarity(batch1, batch2) print(cosine_similarities) batch1_grad = grad_batched_cosine_similarity(batch1, batch2) print(batch1_grad)"},{"question":"You are provided with a folder containing multiple sound files of different formats. Your task is to write a Python function that processes each sound file in the folder, determines its type, and returns a summary report. # Function Specification Write a function `analyze_sound_files(folder_path: str) -> dict` that takes the path to the folder as an input and returns a dictionary summarizing the analysis of the sound files. Input - `folder_path` (str): A string specifying the path to the folder containing sound files. Output - A dictionary with the following structure: ```python { \'total_files\': int, # Total number of files analyzed \'unidentified_files\': int, # Number of files that could not be identified \'file_types\': { # Counts of each file type \'aifc\': int, \'aiff\': int, \'au\': int, \'hcom\': int, \'sndr\': int, \'sndt\': int, \'voc\': int, \'wav\': int, \'8svx\': int, \'sb\': int, \'ub\': int, \'ul\': int } } ``` - The dictionary should have `total_files` as the number of files processed. - `unidentified_files` should be the count of files that could not be identified. - `file_types` should be a dictionary where the key is the file type and the value is the count of files of that type. Constraints - The function should handle any exceptions that may occur while reading files (e.g., file not found, permission errors) and continue processing the remaining files. Example ```python >>> analyze_sound_files(\'/path/to/sound/files\') { \'total_files\': 10, \'unidentified_files\': 2, \'file_types\': { \'aifc\': 1, \'aiff\': 0, \'au\': 1, \'hcom\': 0, \'sndr\': 0, \'sndt\': 0, \'voc\': 1, \'wav\': 3, \'8svx\': 1, \'sb\': 0, \'ub\': 1, \'ul\': 0 } } ``` # Additional Requirements - Use the `sndhdr` module\'s `what` or `whathdr` function to identify the file types. - Ensure the function is efficient and can handle the processing of a large number of files within a reasonable time frame. # Note - You may assume that the folder contains only sound files and no subdirectories.","solution":"import os import sndhdr def analyze_sound_files(folder_path: str) -> dict: Analyze sound files in the given folder and summarize the file types. :param folder_path: Path to the folder containing sound files. :return: A dictionary summarizing the analysis of the sound files. file_types_count = { \'aifc\': 0, \'aiff\': 0, \'au\': 0, \'hcom\': 0, \'sndr\': 0, \'sndt\': 0, \'voc\': 0, \'wav\': 0, \'8svx\': 0, \'sb\': 0, \'ub\': 0, \'ul\': 0 } total_files = 0 unidentified_files = 0 try: for filename in os.listdir(folder_path): total_files += 1 file_path = os.path.join(folder_path, filename) file_type = sndhdr.what(file_path) if file_type is None: unidentified_files += 1 else: file_type_name = file_type[0] if file_type_name in file_types_count: file_types_count[file_type_name] += 1 else: unidentified_files += 1 except Exception as e: print(f\\"An error occurred: {e}\\") return { \'total_files\': total_files, \'unidentified_files\': unidentified_files, \'file_types\': file_types_count }"},{"question":"# Question: You are working on a Python-C extension that needs to manipulate Python\'s arbitrary-sized integer objects (`PyLongObject`). Specifically, you need to create a function that performs the following tasks: 1. Converts a given C `long` number to a `PyLongObject`. 2. Converts the created `PyLongObject` back to a C `long` and checks for overflow. 3. Converts the `PyLongObject` to a C `double`. 4. Verifies if the `PyLongObject` is indeed an instance of `PyLongObject`. Implement the `manipulate_long_object` function in C, which takes a single `long` input and performs all the above tasks. Return a Python tuple containing: - The original `long` number. - The converted `PyLongObject`. - The `long` number obtained from the `PyLongObject`. - The status of overflow (0 for no overflow, 1 for overflow). - The `double` representation of the `PyLongObject`. - The check if the object is an instance of `PyLongObject` (1 for true, 0 for false). **Input:** - A single `long` number. **Output:** - A Python tuple with the following structure: ```python (original_long, pylong_object, long_from_pylong, overflow_status, double_from_pylong, is_pylong_object) ``` **Constraints:** - Handle potential errors gracefully by returning `NULL` if any of the API calls fail. **Example:** ```python >>> manipulate_long_object(1234567890) (1234567890, 1234567890 (PyLongObject), 1234567890, 0, 1234567890.0, 1) ``` **Note:** You do not need to implement the entire C-extension module. Only provide the function definition that satisfies the above requirements.","solution":"# We\'ll simulate the behavior of a Python-C extension using a Python function # since we cannot run actual C-extension code in this environment. def manipulate_long_object(original_long): Function to demonstrate manipulation of long objects similar to what would be done in a Python-C extension. try: # Step 1: Convert a given C `long` number to a `PyLongObject` pylong_object = original_long # Step 2: Convert the created `PyLongObject` back to a C `long` and check for overflow try: long_from_pylong = int(pylong_object) # Simulates PyLong_AsLong overflow_status = 0 # Assume no overflow as we\'re working within Python\'s int range except OverflowError: long_from_pylong = None overflow_status = 1 # Step 3: Convert the `PyLongObject` to a C `double` double_from_pylong = float(pylong_object) # Simulates PyLong_AsDouble # Step 4: Verify if the `PyLongObject` is indeed an instance of `PyLongObject` is_pylong_object = isinstance(pylong_object, int) # Simulates PyLong_Check return (original_long, pylong_object, long_from_pylong, overflow_status, double_from_pylong, is_pylong_object) except Exception: return None"},{"question":"**Question:** *Visualizing Distributions of Penguin Dataset with Seaborn* You are given the penguins dataset from the seaborn library. Your task is to plot various distributions using ECDF and understand different aspects of seaborn plotting. Perform the following steps: 1. Load the \'penguins\' dataset from seaborn. 2. Create an ECDF plot of the penguins dataset for the \'body_mass_g\' variable along the x-axis. 3. Create another ECDF plot for the \'body_mass_g\' variable along the y-axis. 4. Create an ECDF plot of the dataset considering the variables \'bill_length_mm\' and \'bill_depth_mm\'. 5. Draw multiple ECDF plots on the same graph for the \'bill_length_mm\' variable, separating by \'species\' using hue mapping. 6. Change the displayed statistics to count when plotting the ECDF of \'flipper_length_mm\' with hue based on \'species\'. 7. Plot the empirical complementary CDF for \'bill_length_mm\' separated by \'species\'. Write a function `visualize_penguin_data` that performs these tasks. You should use seaborn to generate these plots and make sure your plots are displayed clearly. # Input: - None (The function does not take any input, it directly loads the dataset) # Output: - None (The function should only generate and display plots without returning any values) # Example Usage: ```python def visualize_penguin_data(): # Your code here # Call the function to generate the plots visualize_penguin_data() ``` # Constraints: - Use seaborn for all the plotting. - Ensure the plots are clearly generated and displayed. - Utilize appropriate seaborn methods and parameters as necessary.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # 1. Create an ECDF plot for \'body_mass_g\' along the x-axis plt.figure(figsize=(14, 8)) plt.subplot(231) sns.ecdfplot(data=penguins, x=\'body_mass_g\') plt.title(\'ECDF of Body Mass (g)\') # 2. Create another ECDF plot for \'body_mass_g\' along the y-axis plt.subplot(232) sns.ecdfplot(data=penguins, y=\'body_mass_g\') plt.title(\'ECDF of Body Mass (g) along y-axis\') # 3. Create an ECDF plot of \'bill_length_mm\' and \'bill_depth_mm\' plt.subplot(233) sns.ecdfplot(data=penguins, x=\'bill_length_mm\', label=\\"Bill Length\\") sns.ecdfplot(data=penguins, x=\'bill_depth_mm\', label=\\"Bill Depth\\") plt.legend() plt.title(\'ECDF of Bill Length and Depth (mm)\') # 4. Draw multiple ECDF plots for \'bill_length_mm\', separating by \'species\' plt.subplot(234) sns.ecdfplot(data=penguins, x=\'bill_length_mm\', hue=\'species\') plt.title(\'ECDF of Bill Length (mm) by Species\') # 5. Change the displayed statistics to count for \'flipper_length_mm\' by \'species\' plt.subplot(235) sns.ecdfplot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', stat=\'count\') plt.title(\'ECDF Count of Flipper Length (mm) by Species\') # 6. Plot the empirical complementary CDF for \'bill_length_mm\' separated by \'species\' plt.subplot(236) sns.ecdfplot(data=penguins, x=\'bill_length_mm\', hue=\'species\', complementary=True) plt.title(\'Empirical Complementary CDF of Bill Length (mm) by Species\') plt.tight_layout() plt.show() # Call the function to generate the plots visualize_penguin_data()"},{"question":"**Coding Assessment Question: Handling Dynamic Imports in Python** **Objective:** In this assessment, you are required to implement a function that dynamically imports a list of modules, handles potential errors during the import process, and returns a dictionary with the module names as keys and their respective module objects as values. Use the functions described in the provided documentation for importing modules. **Function Signature:** ```python def dynamic_import_modules(module_names: list) -> dict: Dynamically imports the specified modules and returns a dictionary with the module names as keys and the imported modules as values. If an import fails, the module will not be present in the dictionary, and an error message will be printed. Parameters: module_names (list): A list of module names (strings) to be imported. Returns: dict: A dictionary where keys are module names and values are the imported module objects. ``` **Requirements:** 1. Import each module in the `module_names` list using `PyImport_ImportModule()`. 2. If a module import fails, catch the exception, print an error message in the format: \\"Failed to import module: `<module_name>`\\". 3. Store the successfully imported modules in a dictionary, with the module name as the key and the module object as the value. 4. Return the dictionary of imported modules. **Example:** ```python modules = [\\"os\\", \\"sys\\", \\"nonexistentmodule\\"] result = dynamic_import_modules(modules) print(result) ``` Expected Output: ``` Failed to import module: nonexistentmodule { \'os\': <module \'os\' from \'/usr/lib/python3.10/os.py\'>, \'sys\': <module \'sys\' (built-in)> } ``` **Constraints:** - The function should handle any valid module name strings and gracefully deal with invalid or non-existent modules. - The system\'s Python environment must have the standard library (at a minimum) for the imports. **Performance Requirements:** - The function should efficiently handle the import process and should not attempt to reload already imported modules.","solution":"import importlib def dynamic_import_modules(module_names: list) -> dict: Dynamically imports the specified modules and returns a dictionary with the module names as keys and the imported modules as values. If an import fails, the module will not be present in the dictionary, and an error message will be printed. Parameters: module_names (list): A list of module names (strings) to be imported. Returns: dict: A dictionary where keys are module names and values are the imported module objects. imported_modules = {} for module_name in module_names: try: module = importlib.import_module(module_name) imported_modules[module_name] = module except ImportError: print(f\\"Failed to import module: {module_name}\\") return imported_modules"},{"question":"**Title:** Implementing and Using Generators in Python **Objective:** Assess your understanding of Python generators by implementing custom generator functions and using them in various scenarios. This question will test your ability to handle lazy evaluation, memory efficiency, and writing clean, readable code. **Problem Statement:** Implement a Python generator function that yields the Fibonacci sequence up to a given number of terms. Also, create a second generator function that yields the square of each number in the sequence provided by the first generator. Finally, write a function to sum the elements provided by the squared generator. **Requirements:** 1. **Fibonacci Generator:** - Write a generator function `fibonacci_sequence(n)` that yields the Fibonacci sequence up to `n` terms. - Input: `n` (an integer, `n >= 1`) - Output: Yields Fibonacci numbers one by one. 2. **Square Generator:** - Write a generator function `square_sequence(seq_gen)` that takes a generator (like the one from `fibonacci_sequence()`) and yields the square of each number. - Input: `seq_gen` (a generator object) - Output: Yields the square of each number from the input generator. 3. **Summing Function:** - Write a function `sum_of_squares(n)` that uses the above generators to compute the sum of the squares of the first `n` Fibonacci numbers. - Input: `n` (an integer, `n >= 1`) - Output: Returns an integer, the sum of the squares of the first `n` Fibonacci numbers. **Constraints:** - You must use generators for both the Fibonacci sequence and the square sequence. Using lists or other data structures for these sequences will not be accepted. - You can assume the input will always be a positive integer. **Example:** ```python def fibonacci_sequence(n): # Your code here def square_sequence(seq_gen): # Your code here def sum_of_squares(n): # Your code here # Example Usage: print(sum_of_squares(5)) # Should output 40 (0^2 + 1^2 + 1^2 + 2^2 + 3^2 = 0 + 1 + 1 + 4 + 9 = 15) print(sum_of_squares(6)) # Should output 68 (0^2 + 1^2 + 1^2 + 2^2 + 3^2 + 5^2 = 0 + 1 + 1 + 4 + 9 + 25 = 40) ``` This problem encourages students to understand and effectively use Python generators, ensuring they grasp the concept of lazy evaluation and memory efficiency.","solution":"def fibonacci_sequence(n): Generator to return Fibonacci sequence up to n terms a, b = 0, 1 for _ in range(n): yield a a, b = b, a + b def square_sequence(seq_gen): Generator to return the square of numbers from the provided sequence generator for num in seq_gen: yield num * num def sum_of_squares(n): Returns the sum of squares of the first n Fibonacci numbers using the above generators fibonacci_gen = fibonacci_sequence(n) squared_gen = square_sequence(fibonacci_gen) return sum(squared_gen)"},{"question":"**Title:** Environment Variable and Process Management in Python **Objective:** Demonstrate your ability to manipulate environment variables, handle file operations, and manage processes using Python\'s `os` module. **Background:** In system programming, it is common to interact with environment variables, perform file operations, and manage processes. Python\'s `os` module provides a comprehensive interface to these functionalities. Your task is to write a Python script that processes a provided list of shell commands, executes them as subprocesses, and utilizes environment variables while storing the output in a designated directory. **Task:** You are required to implement the following function: ```python import os import subprocess def process_commands(commands, env_vars, output_dir): Process a list of commands by executing them in subprocesses, using specified environment variables, and saving their outputs into files in the specified output directory. Args: commands (list of str): List of shell commands to execute. env_vars (dict): Dictionary of environment variables to set. output_dir (str): Path to the directory where output files should be saved. Returns: dict: Dictionary with command as key and output file path as value. # Create the output directory if it doesn\'t exist if not os.path.exists(output_dir): os.makedirs(output_dir) # Store the results in a dictionary results = {} # Iterate through each command for i, command in enumerate(commands): # Set up the environment for the subprocess env = os.environ.copy() env.update(env_vars) # Define the output file path output_file_path = os.path.join(output_dir, f\'output_{i}.txt\') # Execute the command and write the output to the file with open(output_file_path, \'w\') as output_file: subprocess.run(command, shell=True, env=env, stdout=output_file, stderr=subprocess.STDOUT) # Store the result results[command] = output_file_path return results ``` **Constraints:** 1. The function should handle any necessary exceptions, such as missing directories or permissions issues. 2. Assume the environment variables provided do not include any special characters or reserved words. 3. The function should ensure that any command errors are logged to the same output file as the standard output. **Performance Requirements:** 1. The function should efficiently create directories and handle environment setup. 2. It should ensure that subprocesses are correctly managed with proper resource cleanup. **Example Usage:** ```python commands = [\\"echo Hello World\\", \\"env\\", \\"ls -la\\"] env_vars = {\\"TEST_ENV_VAR\\": \\"test_value\\"} output_dir = \\"./command_outputs\\" result = process_commands(commands, env_vars, output_dir) for cmd, out_file in result.items(): print(f\\"Command: {cmd} | Output file: {out_file}\\") ``` **Expected Output Directory Structure:** ``` command_outputs/ output_0.txt output_1.txt output_2.txt ``` Each file (e.g., `output_0.txt`, `output_1.txt`, `output_2.txt`) should contain the output of the respective command executed in the environment specified. **Evaluation:** - Ensure your function correctly handles the creation of directories. - Validate the usage of environment variables in each subprocess. - Verify that the output is correctly redirected to the respective files. - Consider edge cases like non-existent directories and command execution failures.","solution":"import os import subprocess def process_commands(commands, env_vars, output_dir): Process a list of commands by executing them in subprocesses, using specified environment variables, and saving their outputs into files in the specified output directory. Args: commands (list of str): List of shell commands to execute. env_vars (dict): Dictionary of environment variables to set. output_dir (str): Path to the directory where output files should be saved. Returns: dict: Dictionary with command as key and output file path as value. # Create the output directory if it doesn\'t exist if not os.path.exists(output_dir): os.makedirs(output_dir) # Store the results in a dictionary results = {} # Iterate through each command for i, command in enumerate(commands): # Set up the environment for the subprocess env = os.environ.copy() env.update(env_vars) # Define the output file path output_file_path = os.path.join(output_dir, f\'output_{i}.txt\') # Execute the command and write the output to the file with open(output_file_path, \'w\') as output_file: try: subprocess.run(command, shell=True, env=env, stdout=output_file, stderr=subprocess.STDOUT) except subprocess.CalledProcessError as e: output_file.write(f\\"Error executing command {command}: {str(e)}\\") # Store the result results[command] = output_file_path return results"},{"question":"# Complex Network Application using asyncio Event Loop Problem Statement: You are tasked with implementing a simplified network monitoring tool using the asyncio event loop functionalities. This tool will act as both a client and a server. The server part listens for incoming connections and echoes back any received messages. The client part connects to a server, sends a predefined message, waits for the echoed message, and prints it out. You are required to: 1. Create a network server using `asyncio` that listens on a specified host and port. 2. The server should echo back any message it receives from a client. 3. The server should stop accepting new connections after 5 seconds but should keep running for any active connections. 4. Create a client that connects to the server, sends a message, and waits for the response. 5. Both the server and client should properly close their connections and the event loop should be gracefully shut down. Input: - Host and port on which the server should listen. - Message to be sent by the client. Output: - Server: Log each received message and echoed message. - Client: Print the echoed message received from the server. # Constraints: - The solution should be implemented using the low-level asyncio event loop API where applicable. - The server should handle multiple clients gracefully. - Properly handle exceptions and ensure graceful shutdown of both server and client. # Example: Suppose the host is `127.0.0.1` and port is `8888`. The message to be sent by the client is `\\"Hello Asyncio\\"`. Expected sequence: 1. Server starts and listens on `127.0.0.1:8888`. 2. Client connects to `127.0.0.1:8888` and sends `\\"Hello Asyncio\\"`. 3. Server receives `\\"Hello Asyncio\\"`, logs it, and sends back the same message. 4. Client receives the echoed message and prints `\\"Hello Asyncio\\"`. 5. Server stops accepting new connections after 5 seconds but keeps running for any active connections. 6. Both client and server connections are properly closed and the event loop is stopped. Implementation Outline: - **Server Implementation**: - Create an `asyncio` server using `asyncio.start_server`. - Define a coroutine to handle client connections and echo received messages. - Use event loop methods like `loop.call_later` to stop accepting new connections after 5 seconds. - Properly close connections and shut down the server. - **Client Implementation**: - Connect to the server using `loop.create_connection`. - Send message, wait for response, and log/print the received message. - Properly handle connection closure and shutdown. ```python import asyncio async def handle_client(reader, writer): try: data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Echoing: {message}\\") writer.write(data) await writer.drain() finally: print(f\\"Close the connection with {addr}\\") writer.close() await writer.wait_closed() async def start_server(host, port): server = await asyncio.start_server(handle_client, host, port) print(f\'Serving on {host}:{port}\') async with server: await asyncio.sleep(5) server.close() await server.wait_closed() print(\'Server has stopped accepting new connections.\') async def run_client(host, port, message): reader, writer = await asyncio.open_connection(host, port) print(f\'Sending: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() async def main(): host = \'127.0.0.1\' port = 8888 message = \'Hello Asyncio\' server_task = asyncio.create_task(start_server(host, port)) await asyncio.sleep(1) # Ensuring server starts first client_task = asyncio.create_task(run_client(host, port, message)) await client_task await server_task if __name__ == \'__main__\': asyncio.run(main()) ```","solution":"import asyncio async def handle_client(reader, writer): try: data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Echoing: {message}\\") writer.write(data) await writer.drain() finally: print(f\\"Close the connection with {addr}\\") writer.close() await writer.wait_closed() async def start_server(host, port): server = await asyncio.start_server(handle_client, host, port) print(f\'Serving on {host}:{port}\') async with server: await asyncio.sleep(5) server.close() await server.wait_closed() print(\'Server has stopped accepting new connections.\') async def run_client(host, port, message): reader, writer = await asyncio.open_connection(host, port) print(f\'Sending: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() async def main(): host = \'127.0.0.1\' port = 8888 message = \'Hello Asyncio\' server_task = asyncio.create_task(start_server(host, port)) await asyncio.sleep(1) # Ensuring server starts first client_task = asyncio.create_task(run_client(host, port, message)) await client_task await server_task if __name__ == \'__main__\': asyncio.run(main())"},{"question":"# Question: Implement and Analyze Semi-Supervised Learning with Label Propagation Objective: You are required to implement a semi-supervised learning model using scikit-learn\'s `LabelPropagation` class. This task will assess your understanding of semi-supervised learning, model training, and evaluation. Problem Statement: Given a dataset, your task is to: 1. Preprocess the data to create a semi-supervised scenario. 2. Implement and train a `LabelPropagation` model. 3. Evaluate the model\'s performance on a test set. 4. Analyze the effect of different kernel methods on the model\'s performance. Dataset: You will be using the digits dataset from scikit-learn, which contains images of hand-written digits (0-9) and their corresponding labels. Steps to Follow: 1. **Data Preparation**: - Load the `digits` dataset from `sklearn.datasets`. - Randomly select 50% of the labels and set them to `-1` to simulate unlabeled data. 2. **Model Implementation**: - Initialize a `LabelPropagation` model. - Train the model using the `fit` method on the partially labeled data. 3. **Evaluation**: - Evaluate the model\'s accuracy using the original labels. - Use both `rbf` and `knn` kernels for the model and compare their performances. 4. **Output**: - Print the model accuracy for each kernel type. - Discuss the performance differences and any observations. Constraints: - You should ensure reproducibility by setting a random seed. - You are only allowed to use scikit-learn for model implementation and evaluation. Code Template: ```python import numpy as np from sklearn import datasets from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split # Step 1: Load the dataset and create semi-supervised data digits = datasets.load_digits() X, y = digits.data, digits.target # Randomly set 50% of the labels to -1 to create the semi-supervised scenario np.random.seed(42) # For reproducibility num_unlabeled = int(0.5 * len(y)) indices = np.random.choice(len(y), num_unlabeled, replace=False) y_unlabeled = np.copy(y) y_unlabeled[indices] = -1 # Step 2: Train Label Propagation models with different kernels kernels = [\'rbf\', \'knn\'] for kernel in kernels: model = LabelPropagation(kernel=kernel) model.fit(X, y_unlabeled) # Step 3: Evaluate the model\'s accuracy y_pred = model.predict(X) accuracy = accuracy_score(y, y_pred) # Output the results print(f\\"Kernel: {kernel}, Accuracy: {accuracy:.2f}\\") # Step 4: Discuss the performance differences # (Include your discussion here) ``` # Performance Requirements: - Ensure the code runs efficiently and handle the dense matrix issue mentioned in the documentation when using `rbf` kernel on larger datasets by setting appropriate constraints.","solution":"import numpy as np from sklearn import datasets from sklearn.semi_supervised import LabelPropagation from sklearn.metrics import accuracy_score def label_propagation_experiment(): # Step 1: Load the dataset and create semi-supervised data digits = datasets.load_digits() X, y = digits.data, digits.target # Randomly set 50% of the labels to -1 to create the semi-supervised scenario np.random.seed(42) # For reproducibility num_unlabeled = int(0.5 * len(y)) indices = np.random.choice(len(y), num_unlabeled, replace=False) y_unlabeled = np.copy(y) y_unlabeled[indices] = -1 # Step 2: Train Label Propagation models with different kernels results = {} kernels = [\'rbf\', \'knn\'] for kernel in kernels: model = LabelPropagation(kernel=kernel) model.fit(X, y_unlabeled) # Step 3: Evaluate the model\'s accuracy y_pred = model.predict(X) accuracy = accuracy_score(y, y_pred) # Save the results results[kernel] = accuracy return results"},{"question":"# Advanced Seaborn Palette and Visualization Task You have been provided with sales data of a company over a period of one year. You are required to analyze and visualize this data using seaborn, specifically focusing on creating custom color palettes and integrating them into your plots. Data Description: Your data is in CSV format with the following columns: - `Date`: The date of the sales record (in \'YYYY-MM-DD\' format). - `Sales`: The total sales on that date. - `Region`: The region where sales were recorded. Task: 1. **Load the Data**: Write a function to load the sales data from the given CSV file. 2. **Preprocess the Data**: Convert the `Date` column to datetime format and extract the month for further analysis. Ensure any missing or incorrect data is handled. 3. **Create Custom Color Palettes**: - Create a dark palette starting from a dark gray to \'seagreen\'. - Create another palette using a specified hex code `\'#79C\'` and increase the number of colors to 10. 4. **Visualizations**: - **Monthly Sales Trend**: - Create a line plot to show the monthly sales trend. Use the palette created with dark gray to \'seagreen\' for the line color. - **Region-wise Sales Distribution**: - Create a box plot to show the sales distribution for each region. Use the hex code palette with 10 colors for the different regions. Function Requirements: - `load_data(filepath: str) -> pd.DataFrame`: Loads the data from the given CSV file path. - `preprocess_data(df: pd.DataFrame) -> pd.DataFrame`: Processes the data by converting dates and extracting months. - `create_palettes() -> dict`: Creates the specified color palettes and returns them in a dictionary. - `plot_monthly_sales(df: pd.DataFrame, palette: list) -> None`: Plots the monthly sales trend using the provided palette. - `plot_region_sales(df: pd.DataFrame, palette: list) -> None`: Plots the region-wise sales distribution using the provided palette. Data Constraints: - You can assume the CSV file path provided is valid and the file is well-formatted. - Ensure your visualizations are clear, well-labeled, and aesthetically pleasing. Performance: - Your solution should efficiently handle datasets with up to 100,000 records. Example Input/Output: ```python # Example usage: df = load_data(\\"sales_data.csv\\") df = preprocess_data(df) palettes = create_palettes() plot_monthly_sales(df, palettes[\'dark_seagreen\']) plot_region_sales(df, palettes[\'hex_palette\']) ``` Example files: - CSV file could look like this: ``` Date,Sales,Region 2023-01-01,1000,North 2023-01-02,1500,West ... ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_data(filepath: str) -> pd.DataFrame: Load the sales data from the given CSV file. return pd.read_csv(filepath) def preprocess_data(df: pd.DataFrame) -> pd.DataFrame: Process the data by converting the Date column to datetime format and extracting the month. df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.month return df def create_palettes() -> dict: Creates the specified color palettes. dark_seagreen_palette = sns.dark_palette(\\"seagreen\\", reverse=False, n_colors=12, input=\\"dark\\") hex_palette = sns.light_palette(\\"#79C\\", n_colors=10, input=\\"dark\\") return { \'dark_seagreen\': dark_seagreen_palette, \'hex_palette\': hex_palette } def plot_monthly_sales(df: pd.DataFrame, palette: list) -> None: Plots the monthly sales trend using the provided palette. monthly_sales = df.groupby(\'Month\')[\'Sales\'].sum().reset_index() plt.figure(figsize=(10, 6)) sns.lineplot(data=monthly_sales, x=\'Month\', y=\'Sales\', palette=palette) plt.title(\\"Monthly Sales Trend\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Sales\\") plt.show() def plot_region_sales(df: pd.DataFrame, palette: list) -> None: Plots the region-wise sales distribution using the provided palette. plt.figure(figsize=(10, 6)) sns.boxplot(data=df, x=\'Region\', y=\'Sales\', palette=palette) plt.title(\\"Region-wise Sales Distribution\\") plt.xlabel(\\"Region\\") plt.ylabel(\\"Sales\\") plt.show()"},{"question":"**Question: Construct a Minimal Reproducible Classification Example** You are required to demonstrate your understanding of scikit-learn by creating a minimal, reproducible example of a classification task using synthetic data. **Task**: 1. Use the `make_classification` function from `sklearn.datasets` to generate a synthetic dataset. 2. Split the dataset into training and testing sets using `train_test_split`. 3. Preprocess the features by scaling them using `StandardScaler`. 4. Train a `GradientBoostingClassifier` model on the training set. 5. Evaluate the model on the test set and print the accuracy score. **Constraints**: - The synthetic dataset should have 1000 samples, 10 features, 2 informative features, and 2 classes. - The random state for dataset generation, splitting, and model training should be set to 42 for reproducibility. - You should include all necessary import statements and ensure the code is runnable by simple copy-pasting. **Input**: - None (the code should generate the dataset) **Output**: - Print the accuracy score of the classifier on the test set. **Example**: ```python # Your solution starts here # Step 1: Generate the dataset from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=0, n_classes=2, random_state=42) # Step 2: Split the dataset from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Step 3: Preprocess the data by scaling from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train the GradientBoostingClassifier from sklearn.ensemble import GradientBoostingClassifier classifier = GradientBoostingClassifier(random_state=42) classifier.fit(X_train, y_train) # Step 5: Evaluate the model accuracy = classifier.score(X_test, y_test) print(f\\"Accuracy: {accuracy:.2f}\\") # Your solution ends here ``` Ensure your code follows the format provided and performs accurately. This will test your ability to apply scikit-learn\'s tools in creating, preprocessing, and evaluating a machine learning model.","solution":"# Step 1: Generate the dataset from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=0, n_classes=2, random_state=42) # Step 2: Split the dataset from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Step 3: Preprocess the data by scaling from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Train the GradientBoostingClassifier from sklearn.ensemble import GradientBoostingClassifier classifier = GradientBoostingClassifier(random_state=42) classifier.fit(X_train, y_train) # Step 5: Evaluate the model accuracy = classifier.score(X_test, y_test) print(f\\"Accuracy: {accuracy:.2f}\\")"},{"question":"**Objective:** Demonstrate your understanding of seaborn, particularly the creation and usage of diverging color palettes. **Question:** You are given a dataset of correlation values between multiple features of a fictitious dataset. Your task is to create a heatmap that visualizes this correlation matrix using a custom diverging color palette. The heatmap should clearly indicate the diverging nature of correlations, with a central neutral color representing zero correlation. **Instructions:** 1. Load the necessary libraries and create a random correlation matrix. 2. Create a diverging color palette using the `sns.diverging_palette` function with the following specifications: - The palette should transition from blue (`h_neg=240`) to red (`h_pos=20`) through white. - The center color should be set to dark (`center=\\"dark\\"`). - The palette should be returned as a continuous colormap (`as_cmap=True`). - Adjust the separation around the center to 30 (`sep=30`). 3. Use the created color palette to plot a heatmap of the correlation matrix using `sns.heatmap`. 4. Add appropriate labels, title, and color bar to make the heatmap comprehensible. **Expected Input:** You do not need to take any input from the user. Generate a random correlation matrix internally. **Expected Output:** A heatmap visualizing the correlation matrix using the specified diverging color palette. **Python Code Template:** ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load necessary libraries and create a random correlation matrix np.random.seed(0) data = np.random.rand(10, 10) correlation_matrix = np.corrcoef(data, rowvar=False) # Step 2: Create a custom diverging color palette custom_palette = sns.diverging_palette(240, 20, center=\\"dark\\", as_cmap=True, sep=30) # Step 3: Plot a heatmap using the created color palette sns.heatmap(correlation_matrix, cmap=custom_palette, annot=True) # Step 4: Add labels and title plt.title(\\"Correlation Matrix Heatmap with Diverging Color Palette\\") plt.show() ``` **Constraints:** - Ensure that your heatmap clearly distinguishes between positive and negative correlations using the diverging color palette. - Annotate the heatmap with the correlation values for better readability. **Performance Requirements:** - The solution should run efficiently even for larger matrices (up to 100x100 in size).","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_correlation_heatmap(): # Step 1: Load necessary libraries and create a random correlation matrix np.random.seed(0) data = np.random.rand(10, 10) correlation_matrix = np.corrcoef(data, rowvar=False) # Step 2: Create a custom diverging color palette custom_palette = sns.diverging_palette(240, 20, center=\\"dark\\", as_cmap=True, sep=30) # Step 3: Plot a heatmap using the created color palette sns.heatmap(correlation_matrix, cmap=custom_palette, annot=True) # Step 4: Add labels and title plt.title(\\"Correlation Matrix Heatmap with Diverging Color Palette\\") plt.show()"},{"question":"**Coding Assessment Question: Mocking and Patching in Python Testing** **Objective:** To assess the student\'s ability to use the `unittest.mock` module to test a system\'s component by creating and utilizing mock objects, patching methods, and asserting calls and behaviors correctly. **Problem Statement:** Consider a simple library management system. Your task is to test the following class using the `unittest.mock` module. ```python import datetime class Book: def __init__(self, title, author): self.title = title self.author = author self.is_checked_out = False self.due_date = None def check_out(self, weeks=2): if not self.is_checked_out: self.is_checked_out = True self.due_date = datetime.date.today() + datetime.timedelta(weeks=weeks) return \\"Checked out successfully!\\" return \\"Book is already checked out!\\" def return_book(self): if self.is_checked_out: self.is_checked_out = False self.due_date = None return \\"Returned successfully!\\" return \\"Book wasn\'t checked out!\\" class Library: def __init__(self): self.catalog = [] def add_book(self, book): self.catalog.append(book) return \\"Book added to catalog!\\" def list_books(self): return [(book.title, book.author) for book in self.catalog] def find_books_by_author(self, author_name): return [book.title for book in self.catalog if book.author == author_name] ``` **Task:** Write a Python test script using `unittest` and `unittest.mock` to: 1. Test the `Book.check_out` method to ensure it correctly sets the `is_checked_out` flag and `due_date` using a mock for `datetime.date.today()`. 2. Mock the `Library.add_book` method and assert it is called correctly when adding a book. 3. Ensure that `Library.find_books_by_author` correctly filters books by the author\'s name and order the results as expected. **Input and Output:** - **Input:** - `Book` and `Library` class definitions as provided. - Test script written using `unittest` and `unittest.mock`. - **Output:** - Tests should pass successfully, verifying the correct behavior of class methods. - Should include assertions about method calls and state changes in the classes. **Constraints:** - You must use the `unittest.mock` module. - Do not change the implementation of `Book` and `Library` classes. **Performance Constraints:** - Ensure that the checks are efficient and the test suite completes in a reasonable amount of time. **Example Usage of `unittest.mock`:** ```python from unittest.mock import patch # Example usage for patching datetime.date.today() with patch(\'datetime.date\') as mock_date: mock_date.today.return_value = datetime.date(2025, 11, 23) # your test code here ``` **Note:** Write clear and concise tests. Ensure your tests cover a variety of scenarios and edge cases to fully validate the functionality.","solution":"import datetime class Book: def __init__(self, title, author): self.title = title self.author = author self.is_checked_out = False self.due_date = None def check_out(self, weeks=2): if not self.is_checked_out: self.is_checked_out = True self.due_date = datetime.date.today() + datetime.timedelta(weeks=weeks) return \\"Checked out successfully!\\" return \\"Book is already checked out!\\" def return_book(self): if self.is_checked_out: self.is_checked_out = False self.due_date = None return \\"Returned successfully!\\" return \\"Book wasn\'t checked out!\\" class Library: def __init__(self): self.catalog = [] def add_book(self, book): self.catalog.append(book) return \\"Book added to catalog!\\" def list_books(self): return [(book.title, book.author) for book in self.catalog] def find_books_by_author(self, author_name): return [book.title for book in self.catalog if book.author == author_name]"},{"question":"# Question: Implement a Function to Compress and Decompress Using Gzip You are required to implement two functions: `compress_string` and `decompress_string`. These functions will use the `gzip` module to compress and decompress string data respectively. Function 1: `compress_string` **Input:** - `data` (str): A string that needs to be compressed. - `compresslevel` (int, optional): Compression level from 0 to 9, where 0 means no compression and 9 means maximum compression. Default is 9. **Output:** - Returns a bytes object containing the compressed data. Function 2: `decompress_string` **Input:** - `data` (bytes): A bytes object that contains compressed string data in gzip format. **Output:** - Returns the decompressed string. Constraints: - The `compresslevel` for `compress_string` should be between 0 and 9 (inclusive). - The functions should handle exceptions gracefully and raise a `ValueError` with an appropriate message in cases where compression or decompression fails. Example: ```python def compress_string(data, compresslevel=9): try: import gzip return gzip.compress(data.encode(), compresslevel=compresslevel) except Exception as e: raise ValueError(f\\"Compression failed: {e}\\") def decompress_string(data): try: import gzip return gzip.decompress(data).decode() except Exception as e: raise ValueError(f\\"Decompression failed: {e}\\") # Example Usage compressed_data = compress_string(\\"Hello, World!\\") print(compressed_data) decompressed_data = decompress_string(compressed_data) print(decompressed_data) ``` Notes: - You should utilize the `gzip.compress` and `gzip.decompress` functions for compression and decompression respectively. - Use appropriate exception handling to make your functions robust.","solution":"def compress_string(data, compresslevel=9): Compresses a string using gzip with the specified compression level. Args: data (str): The string to compress. compresslevel (int, optional): Compression level (0 to 9). Defaults to 9. Returns: bytes: Compressed data. Raises: ValueError: If compression fails or if compresslevel is out of bounds. if not isinstance(data, str): raise ValueError(\\"Data must be a string.\\") if not (0 <= compresslevel <= 9): raise ValueError(\\"Compresslevel must be between 0 and 9.\\") try: import gzip return gzip.compress(data.encode(), compresslevel=compresslevel) except Exception as e: raise ValueError(f\\"Compression failed: {e}\\") def decompress_string(data): Decompresses gzip-compressed data into a string. Args: data (bytes): The compressed data. Returns: str: Decompressed string. Raises: ValueError: If decompression fails. if not isinstance(data, bytes): raise ValueError(\\"Data must be bytes.\\") try: import gzip return gzip.decompress(data).decode() except Exception as e: raise ValueError(f\\"Decompression failed: {e}\\")"},{"question":"# Python Coding Assessment Question Objective Write a program that demonstrates the use of multiple Python standard library modules to process and analyze a given directory of text files. The program should perform the following tasks: 1. List all `.txt` files in the specified directory. 2. Read each `.txt` file and count the number of occurrences of a specific word (provided as a command-line argument). 3. Output the total count of the word across all files. 4. Save the filenames and their respective word counts to a new CSV file. Requirements - Use the `os`, `glob`, `argparse`, and `csv` modules from the Python standard library. - The program should handle command-line arguments for the directory path and the word to search. - The results should be printed to the console and also saved in a CSV file named `word_counts.csv` in the specified directory. Input - The directory path containing the `.txt` files. - The word to search for within the text files. Output - Print the word count for each file to the console. - Save the results to `word_counts.csv` in the following format (comma-separated): `filename, word_count`. Constraints - Assume that the directory path and search word are valid and non-empty. - There should be at least one `.txt` file in the specified directory. - The word matching should be case-insensitive. Example Suppose there are two text files in the directory `/my_texts`: - `file1.txt` containing the text \\"Hello world\\" - `file2.txt` containing the text \\"Python world\\" And the command-line inputs are: - Directory path: `/my_texts` - Search word: `world` The program should output: ``` file1.txt: 1 file2.txt: 1 ``` And save the results in a CSV file `word_counts.csv`: ``` filename,word_count file1.txt,1 file2.txt,1 ``` Implementation ```python import os import glob import argparse import csv def count_word_in_file(file_path, word): with open(file_path, \'r\', encoding=\'utf-8\') as file: text = file.read().lower() return text.count(word.lower()) def main(directory, word): txt_files = glob.glob(os.path.join(directory, \'*.txt\')) results = [] for file_path in txt_files: word_count = count_word_in_file(file_path, word) print(f\'{os.path.basename(file_path)}: {word_count}\') results.append((os.path.basename(file_path), word_count)) csv_path = os.path.join(directory, \'word_counts.csv\') with open(csv_path, \'w\', newline=\'\', encoding=\'utf-8\') as csvfile: csv_writer = csv.writer(csvfile) csv_writer.writerow([\'filename\', \'word_count\']) csv_writer.writerows(results) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Count occurrences of a word in text files.\\") parser.add_argument(\'directory\', help=\\"Directory containing the text files\\") parser.add_argument(\'word\', help=\\"Word to search for\\") args = parser.parse_args() main(args.directory, args.word) ```","solution":"import os import glob import argparse import csv def count_word_in_file(file_path, word): Counts the number of occurrences of a specific word in a file. Args: file_path (str): Path to the text file. word (str): The word to count. Returns: int: The number of occurrences of the word in the file. with open(file_path, \'r\', encoding=\'utf-8\') as file: text = file.read().lower() return text.count(word.lower()) def main(directory, word): Main function to list all text files in a directory, count occurrences of a word in each file, and save the results to a CSV file. Args: directory (str): The directory path containing the text files. word (str): The word to search for within the text files. txt_files = glob.glob(os.path.join(directory, \'*.txt\')) results = [] for file_path in txt_files: word_count = count_word_in_file(file_path, word) print(f\'{os.path.basename(file_path)}: {word_count}\') results.append((os.path.basename(file_path), word_count)) csv_path = os.path.join(directory, \'word_counts.csv\') with open(csv_path, \'w\', newline=\'\', encoding=\'utf-8\') as csvfile: csv_writer = csv.writer(csvfile) csv_writer.writerow([\'filename\', \'word_count\']) csv_writer.writerows(results) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Count occurrences of a word in text files.\\") parser.add_argument(\'directory\', help=\\"Directory containing the text files\\") parser.add_argument(\'word\', help=\\"Word to search for\\") args = parser.parse_args() main(args.directory, args.word)"},{"question":"Objective You are going to create a complex class structure leveraging the concepts of class definition, inheritance, data encapsulation, and iterators. Instructions 1. **Class `Book`**: - A class representing a single book. - **Attributes**: - `title` (str) - `author` (str) - `pages` (int) - number of pages in the book. - **Methods**: - `__init__(self, title, author, pages)`: Initializes the book with the given title, author, and pages. - `__str__(self)`: Returns a string in the format \\"<title> by <author>\\". 2. **Class `Library`**: - A class representing a collection of books. - **Attributes**: - `books` (List[Book]) - a list to store all books. - **Methods**: - `__init__(self)`: Initializes an empty list of books. - `add_book(self, book)`: Adds a `Book` object to the collection. - `list_books(self)`: Returns a list of all books. Should return a generator that yields string descriptions of each book in the format from `__str__` method of `Book`. - `book_count(self)`: Returns the total number of books in the library. 3. **Class `DigitalLibrary` (inherits from `Library`)**: - A specialized library that can also store information about digital access. - **Attributes**: - Inherits attributes from `Library`. - `digital_access` (str) - **Methods**: - Inherits methods from `Library`. - `__init__(self, digital_access)`: Initializes the library and sets up digital access. - `__str__(self)`: Returns a string \\"Digital Access: <digital_access>\\". Implement these classes as described. Respect the principles of encapsulation and inheritance and make use of generators where necessary. Example Usage ```python # Creating a book book1 = Book(\'1984\', \'George Orwell\', 328) book2 = Book(\'The Great Gatsby\', \'F. Scott Fitzgerald\', 180) # Creating a library and adding books lib = Library() lib.add_book(book1) lib.add_book(book2) # Listing books for book in lib.list_books(): print(book) # Creating a digital library dlib = DigitalLibrary(\'Unlimited\') print(str(dlib)) # Output: Digital Access: Unlimited # Adding books and listing them dlib.add_book(book1) dlib.add_book(book2) # List books for book in dlib.list_books(): print(book) # Total number of books print(f\'Total number of books: {dlib.book_count()}) # Output: Total number of books: 2 ``` Constraints - Page counts for `Book` objects must be positive integers. - Titles and authors for `Book` objects must be non-empty strings. - `digital_access` for `DigitalLibrary` must be a non-empty string.","solution":"class Book: A class representing a single book. def __init__(self, title, author, pages): if not title or not author or not isinstance(pages, int) or pages <= 0: raise ValueError(\\"Invalid title, author or page count\\") self.title = title self.author = author self.pages = pages def __str__(self): return f\\"{self.title} by {self.author}\\" class Library: A class representing a collection of books. def __init__(self): self.books = [] def add_book(self, book): if isinstance(book, Book): self.books.append(book) else: raise TypeError(\'Only instances of Book can be added\') def list_books(self): for book in self.books: yield str(book) def book_count(self): return len(self.books) class DigitalLibrary(Library): A specialized library that can also store information about digital access. def __init__(self, digital_access): super().__init__() if not digital_access: raise ValueError(\\"Digital access string cannot be empty\\") self.digital_access = digital_access def __str__(self): return f\\"Digital Access: {self.digital_access}\\""},{"question":"**Question: Implement a Python Function Using `subprocess`** # Objective: To demonstrate your understanding of the `subprocess` module, write a Python function called `execute_command` that executes a given shell command using both `subprocess.run` and `subprocess.Popen`, captures its output and error streams, and returns them along with the command’s return code. # Function Signature: ```python def execute_command(command: str, use_run: bool) -> tuple: Executes a given shell command using subprocess. Parameters: command (str): The shell command to execute. use_run (bool): If True, use subprocess.run(), otherwise use subprocess.Popen(). Returns: tuple: A tuple containing (return code, standard output, standard error). ``` # Requirements: 1. The function should handle timeouts and properly terminate the subprocess if it exceeds the timeout limit. 2. Capture both stdout and stderr streams. 3. Appropriately handle cases where the command fails (non-zero return code), and ensure exceptions are well-managed. 4. The `use_run` parameter determines whether to employ `subprocess.run` or `subprocess.Popen` for command execution. # Constraints: - The timeout for command execution should be 10 seconds. - Assume the command string will be a valid shell command. # Examples: ```python # Example 1: result = execute_command(\\"echo Hello, World!\\", use_run=True) print(result) # Expected Output: # (0, \\"Hello, World!n\\", \\"\\") # Example 2: result = execute_command(\\"invalid_command\\", use_run=False) print(result) # Expected Output: # (non-zero return code, \\"\\", \\"sh: 1: invalid_command: not foundn\\" or equivalent error message) ``` # Notes: - You may need to use `subprocess.PIPE` for capturing outputs in `subprocess.Popen`. - Make sure the function adheres to best practices for resource management (e.g., closing pipes).","solution":"import subprocess def execute_command(command: str, use_run: bool) -> tuple: Executes a given shell command using subprocess. Parameters: command (str): The shell command to execute. use_run (bool): If True, use subprocess.run(), otherwise use subprocess.Popen(). Returns: tuple: A tuple containing (return code, standard output, standard error). timeout = 10 # seconds try: if use_run: result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=timeout) return (result.returncode, result.stdout, result.stderr) else: with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) as proc: try: stdout, stderr = proc.communicate(timeout=timeout) return (proc.returncode, stdout, stderr) except subprocess.TimeoutExpired: proc.kill() stdout, stderr = proc.communicate() return (proc.returncode, stdout, stderr) except Exception as e: return (1, \'\', str(e))"},{"question":"**Objective:** Implement a multi-threaded program that simulates a simplified version of a producer-consumer problem. The program should ensure proper synchronization between producer and consumer threads using the `threading` module. **Problem Statement:** You are required to implement a producer-consumer simulation where: - The producer thread generates a specific number of data items and places them in a shared buffer. - The consumer thread retrieves and processes the data items from the shared buffer. - The program must ensure that no data is lost and no race conditions occur. **Function Specifications:** 1. `producer(buffer, buffer_size, condition, total_items)`: - **Input:** - `buffer`: A shared list where produced items are stored. - `buffer_size`: An integer representing the maximum size of the buffer. - `condition`: A `Condition` object to synchronize access to the buffer. - `total_items`: Total number of items to produce. - **Output:** None - **Description:** This function should produce `total_items` and place them in the buffer, adhering to the buffer constraints. 2. `consumer(buffer, condition, total_items)`: - **Input:** - `buffer`: A shared list where produced items are stored. - `condition`: A `Condition` object to synchronize access to the buffer. - `total_items`: Total number of items to consume. - **Output:** None - **Description:** This function should consume `total_items` from the buffer. **Additional Requirements:** - Ensure that the producer waits if the buffer is full. - Ensure that the consumer waits if the buffer is empty. - Use a `Condition` object to manage synchronization between the producer and consumer. - Implement the producer and consumer functions as target functions of `Thread` objects. **Example:** ```python import threading import time import random def producer(buffer, buffer_size, condition, total_items): for i in range(total_items): time.sleep(random.random()) # Simulate the time taken to produce an item with condition: while len(buffer) >= buffer_size: condition.wait() buffer.append(f\'item{i}\') condition.notify_all() def consumer(buffer, condition, total_items): for i in range(total_items): time.sleep(random.random()) # Simulate the time taken to process an item with condition: while len(buffer) == 0: condition.wait() item = buffer.pop(0) print(f\'Processed {item}\') condition.notify_all() if __name__ == \\"__main__\\": buffer_size = 5 total_items = 20 buffer = [] condition = threading.Condition() producer_thread = threading.Thread(target=producer, args=(buffer, buffer_size, condition, total_items)) consumer_thread = threading.Thread(target=consumer, args=(buffer, condition, total_items)) producer_thread.start() consumer_thread.start() producer_thread.join() consumer_thread.join() ``` **Evaluation Criteria:** - Correctness: The program must correctly produce and consume the specified number of items. - Thread Safety: The program must avoid race conditions and ensure synchronization using condition variables. - Code Quality: The program should be well-organized and follow best practices for thread management and synchronization. **Constraints:** - The buffer size will always be a positive integer. - The number of items to produce and consume will always be a non-negative integer. Good luck, and ensure your implementation is robust and well-documented!","solution":"import threading import time import random def producer(buffer, buffer_size, condition, total_items): Produce \'total_items\' and place them into the shared \'buffer\', ensuring synchronization using the \'condition\' variable. for i in range(total_items): time.sleep(random.random()) # Simulate the time taken to produce an item with condition: while len(buffer) >= buffer_size: condition.wait() buffer.append(f\'item{i}\') condition.notify_all() def consumer(buffer, condition, total_items): Consume \'total_items\' from the shared \'buffer\', ensuring synchronization using the \'condition\' variable. for i in range(total_items): time.sleep(random.random()) # Simulate the time taken to process an item with condition: while len(buffer) == 0: condition.wait() item = buffer.pop(0) print(f\'Processed {item}\') condition.notify_all()"},{"question":"Objective: Create a Python script that utilizes the `logging` module to log messages of different severity levels to different destinations (console and a file). The script should demonstrate the ability to configure loggers, handlers, formatters, and filters. Requirements: 1. Create a logger named `myLogger`. 2. Set the logging level of `myLogger` to `DEBUG`. 3. Add a console handler to `myLogger` that logs messages with level `INFO` or higher to the console. 4. Add a file handler to `myLogger` that logs messages with level `DEBUG` or higher to a file named `app.log`. 5. Add a custom formatter to both handlers which includes the timestamp, logger name, log level, and message. 6. Implement a custom filter for the file handler that only logs messages with the substring \\"important\\" in them. 7. Log messages of various severities (DEBUG, INFO, WARNING, ERROR, CRITICAL) using `myLogger`, and ensure that they are logged to the appropriate destinations according to the specified configuration. Input and Output formats: - No specific inputs or outputs are required for this script. - The log messages should be printed to the console and written to `app.log` as specified. Constraints: - The script should create the `app.log` file in the current working directory. - Make sure the file handler uses an appropriate mode so it doesn’t delete previous logs each time the script runs. Additional Information: - Ensure that the script handles any exceptions that might arise from file I/O operations. Example: Here is a snippet of the type of log entry that should appear in the `app.log` file: ``` 2023-03-15 10:15:32,123 - myLogger - DEBUG - This important debug message is logged to the file. ``` # Code Template: ```python import logging # Step 1: Create a logger named \'myLogger\' logger = logging.getLogger(\'myLogger\') # Step 2: Set the logging level of \'myLogger\' to DEBUG logger.setLevel(logging.DEBUG) # Step 3: Create a console handler that logs messages with level INFO or higher console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # Step 4: Create a file handler that logs messages with level DEBUG or higher file_handler = logging.FileHandler(\'app.log\', mode=\'a\') file_handler.setLevel(logging.DEBUG) # Step 5: Create a formatter that includes timestamp, logger name, level, and message formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Step 6: Implement a custom filter for the file handler class ImportantFilter(logging.Filter): def filter(self, record): return \\"important\\" in record.getMessage() file_handler.addFilter(ImportantFilter()) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) # Step 7: Log messages of various severities logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") logger.debug(\\"This important debug message is logged to the file.\\") ``` # Note: Your task is to complete the code template to meet all the specified requirements and ensure all logged messages behave as expected.","solution":"import logging # Step 1: Create a logger named \'myLogger\' logger = logging.getLogger(\'myLogger\') # Step 2: Set the logging level of \'myLogger\' to DEBUG logger.setLevel(logging.DEBUG) # Step 3: Create a console handler that logs messages with level INFO or higher console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # Step 4: Create a file handler that logs messages with level DEBUG or higher file_handler = logging.FileHandler(\'app.log\', mode=\'a\') file_handler.setLevel(logging.DEBUG) # Step 5: Create a formatter that includes timestamp, logger name, level, and message formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) # Step 6: Implement a custom filter for the file handler class ImportantFilter(logging.Filter): def filter(self, record): return \\"important\\" in record.getMessage().lower() file_handler.addFilter(ImportantFilter()) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) # Step 7: Log messages of various severities logger.debug(\\"This is a debug message\\") logger.info(\\"This is an info message\\") logger.warning(\\"This is a warning message\\") logger.error(\\"This is an error message\\") logger.critical(\\"This is a critical message\\") logger.debug(\\"This important debug message is logged to the file.\\")"},{"question":"# Task You are given a WAV file and need to create a program that reads the file, processes the audio data, and writes the processed data to a new WAV file. The processing involves doubling the volume of the audio. Requirements: 1. **Read the WAV file**: The program should be able to open and read the audio data from a given WAV file. 2. **Process the audio data**: Double the volume of the audio by manipulating the audio frames. 3. **Write the new WAV file**: Save the processed audio data to a new WAV file. Input and Output Formats: - **Input**: A path to the input WAV file. - **Output**: A path to the output WAV file with processed audio data. Constraints: - The input WAV file is in PCM format. - The input WAV file is a valid, uncompressed WAV file. - Consider the WAV file to have a single channel (mono) for simplicity. Function Signature: ```python def process_wav_file(input_file: str, output_file: str) -> None: Reads an input WAV file, processes the audio data by doubling its volume, and writes the processed data to an output WAV file. Args: input_file (str): The path to the input WAV file. output_file (str): The path to the output WAV file. Returns: None ``` # Example: Assume you have an input WAV file `input.wav` containing some audio data. ```python process_wav_file(\'input.wav\', \'output.wav\') ``` The function will read `input.wav`, double the volume of the audio data, and save the processed data to `output.wav`. # Additional Notes: - Use the `wave` module for reading and writing WAV files. - Ensure to handle files properly, closing them after operations are complete. - Doubling the volume of audio frames involves multiplying the amplitude values by 2. - Be cautious of potential clipping when amplifying the audio.","solution":"import wave import struct def process_wav_file(input_file: str, output_file: str) -> None: # Open the input WAV file with wave.open(input_file, \'rb\') as input_wav: # Get parameters from the input file params = input_wav.getparams() # Read audio frames frames = input_wav.readframes(params.nframes) # Convert byte data to amplitude values fmt = f\\"{params.nframes}h\\" amplitude_values = struct.unpack(fmt, frames) # Double the volume by multiplying each amplitude value by 2 processed_values = [min(32767, max(-32768, int(v * 2))) for v in amplitude_values] # Convert the processed amplitude values back to byte data processed_frames = struct.pack(fmt, *processed_values) # Open the output WAV file with wave.open(output_file, \'wb\') as output_wav: # Set parameters for the output file output_wav.setparams(params) # Write the processed frames to the output file output_wav.writeframes(processed_frames)"},{"question":"Coding Assessment Question # Objective Design and implement a Python function that utilizes the `site` module to manage site-specific paths and customizations. The function should demonstrate the students\' understanding of manipulating `sys.path` and handling `sitecustomize` and `usercustomize` modules. # Problem Statement You are required to implement a function `configure_python_environment(config_path: str) -> tuple` that performs the following tasks: 1. Ensures that the directory specified by `config_path` is added to `sys.path`. 2. Processes any `.pth` files within the `config_path` directory, if they exist. 3. Attempts to import a module named `sitecustomize` from the specified directory. 4. Attempts to import a module named `usercustomize` from the specified directory. 5. Returns a tuple containing two boolean values: - The first boolean indicates whether the `sitecustomize` module was successfully imported. - The second boolean indicates whether the `usercustomize` module was successfully imported. # Constraints - The `config_path` argument is guaranteed to be a valid string representing a directory path. - Assume that the system does not have the `-S` option used, so the `site` module is automatically imported. - Any ImportError exceptions for `sitecustomize` or `usercustomize` should be handled gracefully and should not terminate the program. # Input - `config_path`: A string representing the directory path where site-specific configurations are located. # Output - A tuple of two boolean values indicating the success of importing `sitecustomize` and `usercustomize` respectively. # Example ```python config_path = \'/usr/local/lib/python3.9/site-packages\' # Assuming \'sitecustomize.py\' and \'usercustomize.py\' exist and are importable in the specified directory result = configure_python_environment(config_path) print(result) # Output might be (True, True) # If only \'sitecustomize.py\' exists result = configure_python_environment(config_path) print(result) # Output might be (True, False) # If neither file exists result = configure_python_environment(config_path) print(result) # Output might be (False, False) ``` # Implementation Requirements - Make use of the functions provided by the `site` module to manipulate `sys.path`. - Ensure that the function handles any exceptions properly, especially `ImportError`.","solution":"import sys import site import importlib.util import os def configure_python_environment(config_path: str) -> tuple: Configures the Python environment by adding the specified directory to sys.path, processing any .pth files, and attempting to import `sitecustomize` and `usercustomize` modules. Arguments: - config_path: A string representing the directory path for site-specific configurations. Returns: - A tuple of two boolean values indicating the success of importing `sitecustomize` and `usercustomize`. # Add config_path to sys.path if it\'s not already there if config_path not in sys.path: sys.path.append(config_path) # Process any .pth files within the config_path directory for item in os.listdir(config_path): if item.endswith(\'.pth\'): site.addsitedir(config_path) break # Attempt to import sitecustomize sitecustomize_imported = False try: spec = importlib.util.find_spec(\'sitecustomize\') if spec: importlib.util.module_from_spec(spec) sitecustomize_imported = True except ImportError: sitecustomize_imported = False # Attempt to import usercustomize usercustomize_imported = False try: spec = importlib.util.find_spec(\'usercustomize\') if spec: importlib.util.module_from_spec(spec) usercustomize_imported = True except ImportError: usercustomize_imported = False return (sitecustomize_imported, usercustomize_imported)"},{"question":"# Question: You are tasked with writing a Python function that verifies and retrieves login credentials from a `.netrc` file using the `netrc` module. Your function should extract the login information for a given hostname and return it in a structured format. The function signature is as follows: ```python def get_credentials(file_path: str, host: str) -> Optional[Tuple[str, str, str]]: Retrieve the login credentials for a specified host from a .netrc file. :param file_path: The path to the .netrc file. :param host: The hostname for which the credentials need to be retrieved. :return: A tuple (login, account, password) if credentials are found, None otherwise. ``` # Specifications: 1. **Input**: - `file_path` (str): The path to the `.netrc` file. Provide an absolute path. - `host` (str): The hostname for which to retrieve the credentials. 2. **Output**: - Return a tuple `(login, account, password)`. - If the specified host is not found, check if there is a `default` entry and return its credentials. - If neither the specified host nor the `default` entry is available, return `None`. 3. **Constraints**: - The `.netrc` file should exist at the specified `file_path`, otherwise a `FileNotFoundError` should be raised. - Handle any netrc parsing errors gracefully by catching `NetrcParseError` and returning `None`. - Ensure that POSIX-specific security measures are checked (e.g., permission issues). 4. **Performance**: - The function should efficiently parse the `.netrc` file without unnecessary recomputation or file reads. # Example: Consider the following `.netrc` file located at `/home/user/.netrc`: ``` machine example.com login user1 account work password pass1 default login default_user account default_account password default_pass ``` - For `get_credentials(\'/home/user/.netrc\', \'example.com\')`, the function should return: ``` (\'user1\', \'work\', \'pass1\') ``` - For `get_credentials(\'/home/user/.netrc\', \'nonexistent.com\')`, the function should return: ``` (\'default_user\', \'default_account\', \'default_pass\') ``` Write the function `get_credentials` to meet the above specifications.","solution":"import netrc from typing import Optional, Tuple def get_credentials(file_path: str, host: str) -> Optional[Tuple[str, str, str]]: Retrieve the login credentials for a specified host from a .netrc file. :param file_path: The path to the .netrc file. :param host: The hostname for which the credentials need to be retrieved. :return: A tuple (login, account, password) if credentials are found, None otherwise. try: netrc_data = netrc.netrc(file_path) credentials = netrc_data.authenticators(host) or netrc_data.authenticators(\'default\') if credentials: return credentials except FileNotFoundError: raise FileNotFoundError(f\\"The specified .netrc file at {file_path} was not found.\\") except netrc.NetrcParseError: return None except PermissionError: raise PermissionError(f\\"Insufficient permissions to read the .netrc file at {file_path}.\\") return None"},{"question":"# Question: **Context:** The \\"dataclasses\\" module in Python is used for creating classes that are mainly used to store data with less boilerplate code. One of the key functionalities of dataclasses is declaring fields, adding default values, and enforcing type hints. Alongside, the \\"contextlib\\" module provides utilities for working with the \\"with\\" statement, which ensures that clean-up code is executed. **Problem Statement:** You need to design a program that leverages both the \\"dataclasses\\" and \\"contextlib\\" modules to manage a hypothetical application scenario of library management. Here\'s the detailed description: **Step 1:** Define a `Book` data class with the following attributes: - `title` (string) - `author` (string) - `year` (integer) - `is_checked_out` (boolean, default is `False`) **Step 2:** Create a `Library` class which: - Manages a collection (list) of `Book` instances. - Has methods to add a new book, check out a book (update `is_checked_out`), return a book, and list all books. **Step 3:** To safely manage book checkouts and returns, use the context manager functionality (from `contextlib` module) to ensure that resources are properly handled. Create a context manager that: - Temporarily checks out a book when entering the context. - Automatically returns the book when exiting the context. **Implementation Requirements:** 1. Implement the `Book` data class with the appropriate fields and default values. 2. Implement the `Library` class with the following methods: - `add_book(title: str, author: str, year: int)`: Adds a new book to the collection. - `checkout_book(title: str)`: Checks out a specified book if available, otherwise raises an error. - `return_book(title: str)`: Returns a specified book if it is checked out, otherwise raises an error. - `list_books()`: Prints all books in the collection. 3. Implement a context manager function using `contextlib` to handle book checkouts and returns safely. **Example:** ```python # Example usage: library = Library() library.add_book(\\"1984\\", \\"George Orwell\\", 1949) library.add_book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) print(\\"Books in library before checkout:\\") library.list_books() # Output expected: # Title: 1984, Author: George Orwell, Year: 1949, Checked Out: False # Title: To Kill a Mockingbird, Author: Harper Lee, Year: 1960, Checked Out: False with library.checkout(\\"1984\\"): print(\\"Checked out 1984.\\") print(\\"Books in library after trying to checkout 1984:\\") library.list_books() # Output expected: # Title: 1984, Author: George Orwell, Year: 1949, Checked Out: False # Title: To Kill a Mockingbird, Author: Harper Lee, Year: 1960, Checked Out: False ``` Ensure to handle exceptions and edge cases (e.g., book not found, book already checked out). **Constraints:** - A book title is unique within the library collection. - The context manager must ensure that a book is returned even if an error occurs within the context. **Performance Requirements:** - Aim for efficient list operations given that book lookups are O(n) in terms of time complexity, based on the number of books in the library.","solution":"from dataclasses import dataclass, field from typing import List from contextlib import contextmanager @dataclass class Book: title: str author: str year: int is_checked_out: bool = False class Library: def __init__(self): self.books: List[Book] = [] def add_book(self, title: str, author: str, year: int): if any(book.title == title for book in self.books): raise ValueError(f\\"A book with the title \'{title}\' already exists.\\") new_book = Book(title, author, year) self.books.append(new_book) def checkout_book(self, title: str): for book in self.books: if book.title == title: if not book.is_checked_out: book.is_checked_out = True return book else: raise ValueError(f\\"The book \'{title}\' is already checked out.\\") raise ValueError(f\\"The book \'{title}\' is not found in the library.\\") def return_book(self, title: str): for book in self.books: if book.title == title: if book.is_checked_out: book.is_checked_out = False return else: raise ValueError(f\\"The book \'{title}\' is not checked out.\\") raise ValueError(f\\"The book \'{title}\' is not found in the library.\\") def list_books(self): for book in self.books: print(f\\"Title: {book.title}, Author: {book.author}, Year: {book.year}, Checked Out: {book.is_checked_out}\\") @contextmanager def checkout(self, title: str): book = self.checkout_book(title) try: yield book finally: self.return_book(title)"},{"question":"# Python Coding Assessment Question - Working with File Metadata **Objective**: Implement functionality to analyze file metadata using the `stat` module. **Problem Statement**: You are required to write a Python script that performs the following tasks: 1. Accepts a file path as input. 2. Uses the `os.stat()` function to retrieve the file\'s metadata. 3. Implements functions that utilize the `stat` module to: - Determine and print the file type (e.g., directory, regular file, symbolic link). - Extract and print permissions (owner, group, others) in a human-readable string format. - Print other relevant metadata like inode number, device id, number of hard links, user id of the owner, group id of the owner, size of the file, last access time, last modification time, and last metadata change time. # Implementation Details: 1. Write a function `get_file_metadata(file_path)` that: - Takes a string `file_path` as input. - Retrieves and returns the file metadata using `os.stat(file_path)`. 2. Write a function `interpret_file_metadata(metadata)` that: - Takes the metadata returned from `get_file_metadata()` as input. - Determines the file type using `stat` module functions and prints it. - Converts and prints file permissions using `stat.filemode(metadata.st_mode)`. - Prints inode number, device id, number of hard links, user id of the owner, group id of the owner, size of the file, last access time, last modification time, and last metadata change time. # Constraints: - Assume the input file path is always valid. - The script should handle normal files, directories, symbolic links, and FIFO special files. - Consider using `datetime` module for converting timestamps to human-readable format. # Example Usage: ```python import os import stat from datetime import datetime def get_file_metadata(file_path): return os.stat(file_path) def interpret_file_metadata(metadata): filetype = None if stat.S_ISDIR(metadata.st_mode): filetype = \'directory\' elif stat.S_ISCHR(metadata.st_mode): filetype = \'character special device\' elif stat.S_ISBLK(metadata.st_mode): filetype = \'block special device\' elif stat.S_ISREG(metadata.st_mode): filetype = \'regular file\' elif stat.S_ISFIFO(metadata.st_mode): filetype = \'FIFO (named pipe)\' elif stat.S_ISLNK(metadata.st_mode): filetype = \'symbolic link\' elif stat.S_ISSOCK(metadata.st_mode): filetype = \'socket\' permissions = stat.filemode(metadata.st_mode) inode = metadata.st_ino device = metadata.st_dev num_links = metadata.st_nlink uid = metadata.st_uid gid = metadata.st_gid size = metadata.st_size access_time = datetime.fromtimestamp(metadata.st_atime) modification_time = datetime.fromtimestamp(metadata.st_mtime) metadata_change_time = datetime.fromtimestamp(metadata.st_ctime) print(f\\"File Type: {filetype}\\") print(f\\"Permissions: {permissions}\\") print(f\\"Inode Number: {inode}\\") print(f\\"Device ID: {device}\\") print(f\\"Number of Hard Links: {num_links}\\") print(f\\"User ID of Owner: {uid}\\") print(f\\"Group ID of Owner: {gid}\\") print(f\\"Size: {size} bytes\\") print(f\\"Last Access Time: {access_time}\\") print(f\\"Last Modification Time: {modification_time}\\") print(f\\"Last Metadata Change Time: {metadata_change_time}\\") # Example call (do not include in final script) if __name__ == \\"__main__\\": file_path = \\"sample.txt\\" metadata = get_file_metadata(file_path) interpret_file_metadata(metadata) ``` # Notes: - You should not include the example call in your final submission. - Ensure your script is well-documented and follows PEP 8 guidelines.","solution":"import os import stat from datetime import datetime def get_file_metadata(file_path): Retrieves and returns the file metadata using `os.stat(file_path)`. Parameters: file_path (str): The path of the file. Returns: os.stat_result: The metadata of the file. return os.stat(file_path) def interpret_file_metadata(metadata): Prints out various details from the file metadata. Parameters: metadata (os.stat_result): The metadata retrieved from `os.stat()`. filetype = None if stat.S_ISDIR(metadata.st_mode): filetype = \'directory\' elif stat.S_ISCHR(metadata.st_mode): filetype = \'character special device\' elif stat.S_ISBLK(metadata.st_mode): filetype = \'block special device\' elif stat.S_ISREG(metadata.st_mode): filetype = \'regular file\' elif stat.S_ISFIFO(metadata.st_mode): filetype = \'FIFO (named pipe)\' elif stat.S_ISLNK(metadata.st_mode): filetype = \'symbolic link\' elif stat.S_ISSOCK(metadata.st_mode): filetype = \'socket\' permissions = stat.filemode(metadata.st_mode) inode = metadata.st_ino device = metadata.st_dev num_links = metadata.st_nlink uid = metadata.st_uid gid = metadata.st_gid size = metadata.st_size access_time = datetime.fromtimestamp(metadata.st_atime) modification_time = datetime.fromtimestamp(metadata.st_mtime) metadata_change_time = datetime.fromtimestamp(metadata.st_ctime) print(f\\"File Type: {filetype}\\") print(f\\"Permissions: {permissions}\\") print(f\\"Inode Number: {inode}\\") print(f\\"Device ID: {device}\\") print(f\\"Number of Hard Links: {num_links}\\") print(f\\"User ID of Owner: {uid}\\") print(f\\"Group ID of Owner: {gid}\\") print(f\\"Size: {size} bytes\\") print(f\\"Last Access Time: {access_time}\\") print(f\\"Last Modification Time: {modification_time}\\") print(f\\"Last Metadata Change Time: {metadata_change_time}\\")"},{"question":"# Complex Number Geometry Calculator Objective: Implement a function that performs a series of geometric transformations on a complex number using functions from the `cmath` module. Task: Write a function `transform_complex(z: complex, transformations: list[tuple[str, float]]) -> complex` that: 1. Takes a complex number `z` and a list of transformations. 2. Each transformation is specified as a tuple where the first element is a string representing the type of transformation (`\'rotate\'`, `\'scale\'`, or `\'stretch\'`) and the second element is a float representing the parameter for the transformation. 3. Applies each transformation in the order they are provided. - `\'rotate\'`: Rotate the complex number `z` by the given angle (in radians) counterclockwise. - `\'scale\'`: Scale the modulus of `z` by the given factor. - `\'stretch\'`: Stretch the real part of `z` by the given factor while keeping the imaginary part unchanged. For example: - Rotation by an angle (theta) (in radians) can be achieved by converting `z` to polar form, modifying the phase (phi), and converting back to rectangular form. - Scaling the modulus by a factor `k` involves multiplying the modulus (r) by `k`. - Stretching the real part involves multiplying the real part by the given factor while keeping the imaginary part unchanged. Function Signature: ```python def transform_complex(z: complex, transformations: list[tuple[str, float]]) -> complex: pass ``` Input: - `z`: A complex number. - `transformations`: A list of tuples where each tuple has a string (\'rotate\', \'scale\', \'stretch\') and a float. Output: - Returns the transformed complex number after applying all the listed transformations. Constraints: - The transformations should be applied in the order provided. - Rotation should handle angles in radians. - Scaling should always be a positive factor. - Stretching should be applied only to the real part of the complex number. Example: ```python # Example usage z = 1 + 1j transformations = [(\'rotate\', cmath.pi / 4), (\'scale\', 2), (\'stretch\', 1.5)] result = transform_complex(z, transformations) print(result) # Outputs the transformed complex number ``` Note: Utilize appropriate functions from the `cmath` module to handle conversions and operations on complex numbers.","solution":"import cmath def transform_complex(z: complex, transformations: list[tuple[str, float]]) -> complex: for transformation in transformations: operation, value = transformation if operation == \'rotate\': # Rotate the complex number by modifying the phase r, phi = cmath.polar(z) phi += value z = cmath.rect(r, phi) elif operation == \'scale\': # Scale the complex number by modifying the modulus r, phi = cmath.polar(z) r *= value z = cmath.rect(r, phi) elif operation == \'stretch\': # Stretch the real part of the complex number z = complex(z.real * value, z.imag) return z"},{"question":"**Objective**: Demonstrate your understanding of concurrent programming using the `_thread` module in Python. # Problem Statement You are tasked with creating a synchronized counter that can be safely incremented by multiple threads. Your goal is to implement a thread-safe counter using the `_thread` module\'s locking mechanisms. This counter will be incremented by multiple threads, and you need to ensure that the increment operation is atomic to avoid race conditions. # Requirements 1. Implement a class `ThreadSafeCounter` with the following methods: * `__init__(self)`: Initializes the counter to 0 and allocates a lock. * `increment(self)`: Safely increments the counter by 1. * `get_value(self)`: Returns the current value of the counter. 2. Create a function `increment_counter(counter, times)` that takes a `ThreadSafeCounter` object and an integer `times`. This function should increment the counter `times` times. 3. Create a main function to: * Instantiate a `ThreadSafeCounter`. * Create and start 5 threads, each of which will call `increment_counter(counter, 10000)` to increment the counter 10,000 times. * Wait for all threads to complete. * Print the final value of the counter. # Constraints * Use only the `_thread` module for threading and synchronization. * Each thread should increment the counter exactly 10,000 times. * There should be no race conditions; the final counter value must be 50,000. * You must handle any potential thread-safety issues without using higher-level abstractions provided by the `threading` module. # Input and Output * No input is required from the user. * The program should output the final counter value. It should be 50,000 if the implementation is correct. # Example ```python # Expected output format Final counter value: 50000 ``` # Implementation ```python import _thread class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.counter += 1 def get_value(self): return self.counter def increment_counter(counter, times): for _ in range(times): counter.increment() def main(): counter = ThreadSafeCounter() threads = [] for _ in range(5): t = _thread.start_new_thread(increment_counter, (counter, 10000)) threads.append(t) # This is a placeholder for a proper way to wait for threads to finish import time time.sleep(2) print(f\\"Final counter value: {counter.get_value()}\\") if __name__ == \\"__main__\\": main() ``` Ensure your solution meets the specifications and correctly handles synchronization issues.","solution":"import _thread import time class ThreadSafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.counter += 1 def get_value(self): return self.counter def increment_counter(counter, times): for _ in range(times): counter.increment() def main(): counter = ThreadSafeCounter() threads = [] for _ in range(5): t = _thread.start_new_thread(increment_counter, (counter, 10000)) # Wait for threads to finish (rough estimation; should use more robust approach in production code) time.sleep(2) print(f\\"Final counter value: {counter.get_value()}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Logging System Implementation You are tasked with developing a robust logging system for monitoring and debugging purposes within an application. The logging system should: 1. Utilize different log levels and direct log messages to various handlers. 2. Use custom formatters for displaying log messages with specific formats. 3. Implement filters to control which log records are processed. **Requirements:** 1. **Log Levels and Handlers**: - Create a logger named `app`. - Add a `StreamHandler` that outputs log messages to the console. - Add a `FileHandler` that outputs log messages to a file named `app.log`. 2. **Custom Formatters**: - The console output should display messages in the format: `%(asctime)s - %(name)s - %(levelname)s - %(message)s` - The file output should display messages in the format: `%(levelname)s:%(name)s:%(message)s` 3. **Log Filters**: - Implement a filter that only allows log messages from the module `data_processing`. 4. **Log Usage**: - Log at different levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) within a sample function. **Function Definition:** ```python import logging def setup_logger(): Sets up the logging system with the specified handlers, formatters, and filters. def sample_function(): A sample function that logs messages at different levels. setup_logger() sample_function() ``` **Input:** The function `setup_logger` does not take any input arguments and sets up the logging configuration. The `sample_function` will generate log messages at different levels for demonstration purposes. **Output:** There is no direct console output from these functions; however, the expected logging should show in both console and a file as configured. **Constraints:** - The logging configuration should ensure no duplicate log messages in the console. - Ensure the log file `app.log` resides in the current working directory. **Performance Requirements:** - The `setup_logger` function should efficiently initialize the logging configuration without redundant handler additions. # Notes 1. You may need to explore `logging.Filter` class specifics for implementing the module-specific filter. 2. Ensure you handle different logging levels correctly and demonstrate this in the `sample_function`. 3. Demonstrate a proper working environment by configuring the handlers and filters effectively.","solution":"import logging class ModuleFilter(logging.Filter): def filter(self, record): return \\"data_processing\\" in record.name def setup_logger(): Sets up the logging system with the specified handlers, formatters, and filters. logger = logging.getLogger(\'app\') logger.setLevel(logging.DEBUG) # Create console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) console_format = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_format) logger.addHandler(console_handler) # Create file handler file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) file_format = logging.Formatter(\'%(levelname)s:%(name)s:%(message)s\') file_handler.setFormatter(file_format) logger.addHandler(file_handler) # Add filter to the logger module_filter = ModuleFilter() logger.addFilter(module_filter) def sample_function(): A sample function that logs messages at different levels. logger = logging.getLogger(\'app.data_processing\') logger.debug(\'This is a debug message.\') logger.info(\'This is an info message.\') logger.warning(\'This is a warning message.\') logger.error(\'This is an error message.\') logger.critical(\'This is a critical message.\') setup_logger() sample_function()"},{"question":"You are provided with a dataset containing information about various species of penguins. Using the seaborn library, create a comprehensive visualization that includes the following: 1. **Distribution Plots**: - Use both `histplot` and `kdeplot` to show the distribution of `flipper_length_mm` for different species in one combined figure. Each species should be distinguished using different colors. 2. **Faceted Plot**: - Utilize the `displot` figure-level function to create a faceted plot showing the distribution of `bill_length_mm`, separated by `species` and gender (`sex`). Ensure that each facet has appropriate axis labels. 3. **Joint Plot**: - Create a `jointplot` to visualize the relationship between `flipper_length_mm` and `bill_depth_mm`, colored by `species`. # Input The input dataset can be loaded using seaborn\'s `load_dataset(\\"penguins\\")` function. # Constraints - You should use seaborn for all visualizations. - Ensure that all plots are well-labeled, including axis labels and legends. - Aspect ratios and figure sizes should be adjusted for clarity. # Output A Python script or Jupyter notebook that generates the described visualizations. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Distribution Plots plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", kde=True) plt.title(\\"Histogram and KDE of Flipper Length by Species\\") plt.show() # 2. Faceted Plot g = sns.displot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"species\\", row=\\"sex\\", height=3.5, aspect=1.0) g.set_axis_labels(\\"Bill Length (mm)\\", \\"Density\\") g.add_legend(title=\\"Species\\") plt.show() # 3. Joint Plot sns.jointplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"scatter\\") plt.show() ``` Make sure your visualizations are clear and interpretable. Include necessary explanations or comments within your script to describe each step.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") def plot_penguin_visualizations(): # 1. Distribution Plots plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", kde=True) plt.title(\\"Histogram and KDE of Flipper Length by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.legend(title=\\"Species\\") plt.show() # 2. Faceted Plot g = sns.displot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"species\\", row=\\"sex\\", height=3.5, aspect=1.0) g.set_axis_labels(\\"Bill Length (mm)\\", \\"Density\\") g.add_legend(title=\\"Species\\") plt.show() # 3. Joint Plot sns.jointplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"scatter\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.show() if __name__ == \\"__main__\\": plot_penguin_visualizations()"},{"question":"# Abstract Data Type: Media Collection You are tasked to create an abstract framework for managing a media collection that ensures unity for different types of media items. Each media item should be identifiable and possess specific characteristics depending on its type. Follow the instructions below to complete this task: 1. Create an abstract base class `MediaItem` using the `abc` module. This class should have: - An abstract method `play()` that will be responsible for the specific action of playing the media. - An abstract property `title` that returns the title of the media item. - An abstract method `__str__()` for custom string representation of the media item. 2. Based on `MediaItem`, create concrete subclasses `Book`, `Movie`, and `Song`. Each concrete class must: - Implement the `play()` method: - For `Book`, `play()` should print out `\\"Opening the book: <title>\\"` - For `Movie`, `play()` should print out `\\"Playing the movie: <title>\\"` - For `Song`, `play()` should print out `\\"Playing the song: <title>\\"` - Implement the `title` property. - Implement the `__str__()` method to return a meaningful string representation of the media item. 3. Implement virtual subclass registration for your `MediaItem` abstract base class. Create a class `Podcast` that: - Does not directly inherit from `MediaItem`. - Implements the same interface as `MediaItem`. - Registers itself as a virtual subclass of `MediaItem`. 4. Write a function `test_media_items(media_items)` that takes a list of media items (instances of `Book`, `Movie`, `Song`, and `Podcast`) and: - Plays each media item by calling its `play()` method. - Prints the string representation of each media item. Constraints: - The title of each media item should be a non-empty string. - Ensure appropriate use of the `abc` module functionalities as described. Example: ```python from abc import ABC, abstractmethod, ABCMeta # Step 1: Define abstract base class MediaItem class MediaItem(ABC): @abstractmethod def play(self): pass @property @abstractmethod def title(self): pass @abstractmethod def __str__(self): pass # Step 2: Define concrete classes Book, Movie, and Song class Book(MediaItem): def __init__(self, title): self._title = title @property def title(self): return self._title def play(self): print(f\\"Opening the book: {self.title}\\") def __str__(self): return f\'Book: {self.title}\' class Movie(MediaItem): def __init__(self, title): self._title = title @property def title(self): return self._title def play(self): print(f\\"Playing the movie: {self.title}\\") def __str__(self): return f\'Movie: {self.title}\' class Song(MediaItem): def __init__(self, title): self._title = title @property def title(self): return self._title def play(self): print(f\\"Playing the song: {self.title}\\") def __str__(self): return f\'Song: {self.title}\' # Step 3: Define virtual subclass Podcast class Podcast: def __init__(self, title): self._title = title @property def title(self): return self._title def play(self): print(f\\"Playing the podcast: {self.title}\\") def __str__(self): return f\'Podcast: {self.title}\' MediaItem.register(Podcast) # Step 4: Implement test function def test_media_items(media_items): for item in media_items: item.play() print(str(item)) # Example usage book = Book(\\"1984\\") movie = Movie(\\"Inception\\") song = Song(\\"Bohemian Rhapsody\\") podcast = Podcast(\\"Python Bytes\\") media_list = [book, movie, song, podcast] test_media_items(media_list) # Expected output: # Opening the book: 1984 # Book: 1984 # Playing the movie: Inception # Movie: Inception # Playing the song: Bohemian Rhapsody # Song: Bohemian Rhapsody # Playing the podcast: Python Bytes # Podcast: Python Bytes ```","solution":"from abc import ABC, abstractmethod # Step 1: Define abstract base class MediaItem class MediaItem(ABC): @abstractmethod def play(self): pass @property @abstractmethod def title(self): pass @abstractmethod def __str__(self): pass # Step 2: Define concrete classes Book, Movie, and Song class Book(MediaItem): def __init__(self, title): self._title = title @property def title(self): return self._title def play(self): print(f\\"Opening the book: {self.title}\\") def __str__(self): return f\'Book: {self.title}\' class Movie(MediaItem): def __init__(self, title): self._title = title @property def title(self): return self._title def play(self): print(f\\"Playing the movie: {self.title}\\") def __str__(self): return f\'Movie: {self.title}\' class Song(MediaItem): def __init__(self, title): self._title = title @property def title(self): return self._title def play(self): print(f\\"Playing the song: {self.title}\\") def __str__(self): return f\'Song: {self.title}\' # Step 3: Define virtual subclass Podcast class Podcast: def __init__(self, title): self._title = title @property def title(self): return self._title def play(self): print(f\\"Playing the podcast: {self.title}\\") def __str__(self): return f\'Podcast: {self.title}\' MediaItem.register(Podcast) # Step 4: Implement test function def test_media_items(media_items): for item in media_items: item.play() print(str(item)) # Example usage if __name__ == \\"__main__\\": book = Book(\\"1984\\") movie = Movie(\\"Inception\\") song = Song(\\"Bohemian Rhapsody\\") podcast = Podcast(\\"Python Bytes\\") media_list = [book, movie, song, podcast] test_media_items(media_list)"},{"question":"**Time Zone Management and Daylight Saving Transitions with `zoneinfo`** # Problem Statement You are required to write a Python function `schedule_meeting` that helps plan an international meeting based on a common UTC time. Given a list of participant timezones and a common UTC meeting time, the function should: 1. Convert the given UTC time to each participant’s local time using their respective timezones. 2. Handle daylight saving time transitions correctly. 3. Use an alternate timezone data path to fetch the timezones. 4. Clear the caching mechanism after processing the datetimes. **Function Signature:** ```python def schedule_meeting(utc_datetime: str, participant_timezones: list[str], tzpath: list[str]) -> dict[str, str]: ``` **Input:** - `utc_datetime`: A string representing the UTC meeting time in the format `YYYY-MM-DD HH:MM`. - `participant_timezones`: A list of strings, where each string is an IANA timezone identifier (e.g., `America/New_York`, `Europe/London`). There is at least one participant. - `tzpath`: A list of absolute paths as strings where the timezone data can be found. **Output:** - A dictionary where the keys are timezones from the input list and values are the local time for the meeting in that timezone in the format `YYYY-MM-DD HH:MM`. **Constraints:** - The function should handle and correctly apply daylight saving time transitions. - The input `utc_datetime` will always represent a valid date and time. - Provided absolute paths in `tzpath` always contain valid timezone data files. **Example:** ```python utc_datetime = \\"2023-03-15 14:30\\" participant_timezones = [\\"America/Los_Angeles\\", \\"Europe/London\\", \\"Asia/Tokyo\\"] tzpath = [\\"/usr/share/zoneinfo\\", \\"/etc/zoneinfo\\"] result = schedule_meeting(utc_datetime, participant_timezones, tzpath) print(result) ``` Output: ```python { \\"America/Los_Angeles\\": \\"2023-03-15 07:30\\", \\"Europe/London\\": \\"2023-03-15 14:30\\", \\"Asia/Tokyo\\": \\"2023-03-15 23:30\\" } ``` **Notes:** 1. The `reset_tzpath()` function must be used to set the provided `tzpath`. 2. Use the `ZoneInfo` class for timezone information. 3. After processing all conversions, clear the `ZoneInfo` cache. # Hints: - Use the `datetime` module for datetime manipulations. - Import the `ZoneInfo` class from the `zoneinfo` module. - Use the `reset_tzpath()` function to set the custom timezone data path.","solution":"from datetime import datetime from zoneinfo import ZoneInfo, reset_tzpath def schedule_meeting(utc_datetime: str, participant_timezones: list[str], tzpath: list[str]) -> dict[str, str]: Convert UTC meeting time to each participant\'s local time using their respective timezones. Args: utc_datetime (str): A string representing the UTC meeting time in the format `YYYY-MM-DD HH:MM`. participant_timezones (list[str]): A list of strings representing IANA timezones. tzpath (list[str]): A list of absolute paths as strings where timezone data can be found. Returns: dict[str, str]: A dictionary where keys are the timezones from input list and values are local time for the meeting in the format `YYYY-MM-DD HH:MM`. # Set custom timezone data path reset_tzpath(tzpath) # Convert the utc_datetime string to a datetime object utc_dt = datetime.strptime(utc_datetime, \\"%Y-%m-%d %H:%M\\").replace(tzinfo=ZoneInfo(\\"UTC\\")) local_times = {} for timezone in participant_timezones: local_dt = utc_dt.astimezone(ZoneInfo(timezone)) local_times[timezone] = local_dt.strftime(\\"%Y-%m-%d %H:%M\\") # Clear ZoneInfo cache ZoneInfo.clear_cache() return local_times"},{"question":"Objective: You are tasked with implementing a Python function that uses the `subprocess` module to run a series of shell commands and captures their output, error messages, and return codes. The function should handle timeouts and raise appropriate exceptions when errors occur. Function Signature: ```python def run_shell_commands(commands: list, timeout: int) -> list: Executes a series of shell commands using the subprocess module. Args: commands (list): A list of shell command strings to be executed. timeout (int): The maximum time (in seconds) allowed for each command to run. Returns: list: A list of dictionaries, each containing the keys \'command\', \'stdout\', \'stderr\', and \'returncode\' corresponding to each executed command. Raises: ValueError: If any command exceeds the given timeout. ``` Requirements: 1. The function should iterate over the list of command strings and run each command using the `subprocess.run()` function. 2. The timeout for each command should be enforced using the `timeout` parameter of `subprocess.run()`. 3. For each command, the function should capture the standard output (`stdout`), standard error (`stderr`), and the return code (`returncode`). 4. If a command exceeds the specified timeout, the function should raise a `ValueError` with a message indicating which command timed out. 5. The function should return a list of dictionaries, each containing: - `command` (the command string executed) - `stdout` (the captured standard output) - `stderr` (the captured standard error) - `returncode` (the return code of the command) Constraints: - The function should handle a list of up to 10 commands. - Each command should complete within the specified timeout period; otherwise, a `ValueError` should be raised. - Assume all commands are valid shell commands that can be executed directly by the shell. Example Usage: ```python commands = [\\"ls -l\\", \\"echo \'Hello, World!\'\\", \\"pwd\\"] timeout = 2 results = run_shell_commands(commands, timeout) for result in results: print(f\\"Command: {result[\'command\']}\\") print(f\\"STDOUT: {result[\'stdout\']}\\") print(f\\"STDERR: {result[\'stderr\']}\\") print(f\\"Return Code: {result[\'returncode\']}\\") ``` In this example, the `run_shell_commands` function executes three shell commands, captures their output and errors, and prints them. Implementations should ensure that all required aspects of subprocess management (spawning, capturing output, enforcing timeouts, raising exceptions) are demonstrated effectively.","solution":"import subprocess def run_shell_commands(commands: list, timeout: int) -> list: Executes a series of shell commands using the subprocess module. Args: commands (list): A list of shell command strings to be executed. timeout (int): The maximum time (in seconds) allowed for each command to run. Returns: list: A list of dictionaries, each containing the keys \'command\', \'stdout\', \'stderr\', and \'returncode\' corresponding to each executed command. Raises: ValueError: If any command exceeds the given timeout. results = [] for command in commands: try: completed_process = subprocess.run( command, shell=True, capture_output=True, text=True, timeout=timeout ) result = { \'command\': command, \'stdout\': completed_process.stdout, \'stderr\': completed_process.stderr, \'returncode\': completed_process.returncode } except subprocess.TimeoutExpired: raise ValueError(f\\"Command \'{command}\' timed out after {timeout} seconds\\") results.append(result) return results"},{"question":"**Coding Assessment Question** # Email Message Creation and Manipulation You are required to create and manipulate an email message using the `EmailMessage` class from the `email.message` module. The goal is to build a complete email message with various headers and payloads and then serialize it into a specific format. Requirements 1. **Create an EmailMessage**: - Initialize an `EmailMessage` object. - Set the subject of the email to \\"Programming Assessment\\". - Add \\"From\\" and \\"To\\" headers with appropriate email addresses. 2. **Add Text Payload**: - Add a plain text payload to the email message with the content \\"Hello, this is a test email for the coding assessment.\\" 3. **Add an Attachment**: - Add a file attachment with the filename \\"test.txt\\" and the content \\"This is a test file\\". 4. **Add HTML Alternative**: - Add an HTML alternative representation of the email body with the content \\"<html><body><p>Hello, this is a test email for the coding assessment.</p></body></html>\\" 5. **Serialize the Message:** - Serialize the complete email message as both a string and bytes. - Output both serialized forms. 6. **Query and Print Details**: - Print the `Content-Type` of each part of the multipart message. - If the email has attachments, print the filenames. Expected Input and Output The function should have no input and return no values. Instead, it should print the expected outputs directly. # Constraints - Ensure that all headers are correctly added and content is appropriately managed when setting up multipart messages. - The solution should correctly handle ASCII and non-ASCII characters in email headers or payloads. # Function Signature: ```python def create_and_manipulate_email(): pass ``` Example Output: ```text Serialized as String: \\"Content-Type: multipart/mixed; ... ... Content-Type: text/plain; charset=\\"utf-8\\" ... Content-Type: text/html; charset=\\"utf-8\\" ... Serialized as Bytes: b\'Content-Type: multipart/mixed; ... Part Content-Types: multipart/mixed text/plain multipart/alternative text/plain text/html text/plain Attachment Filenames: test.txt ``` Implement the `create_and_manipulate_email` function following the given instructions.","solution":"from email.message import EmailMessage from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart def create_and_manipulate_email(): # Create an EmailMessage object msg = MIMEMultipart(\'mixed\') # Set headers msg[\'Subject\'] = \'Programming Assessment\' msg[\'From\'] = \'from@example.com\' msg[\'To\'] = \'to@example.com\' # Create the plain text part plain_text_part = MIMEText(\'Hello, this is a test email for the coding assessment.\', \'plain\') # Create the HTML part html_part = MIMEText(\'<html><body><p>Hello, this is a test email for the coding assessment.</p></body></html>\', \'html\') # Create the attachment attachment = MIMEText(\'This is a test file.\', \'plain\') attachment.add_header(\'Content-Disposition\', \'attachment\', filename=\'test.txt\') # Create an alternative part that contains plain text and HTML alternative_part = MIMEMultipart(\'alternative\') alternative_part.attach(plain_text_part) alternative_part.attach(html_part) # Attach parts to the message msg.attach(alternative_part) msg.attach(attachment) # Serialize the message serialized_string = msg.as_string() serialized_bytes = msg.as_bytes() # Print the serialized forms print(\\"Serialized as String:\\") print(serialized_string) print() print(\\"Serialized as Bytes:\\") print(serialized_bytes) print() # Print the content-type of each part print(\\"Part Content-Types:\\") for part in msg.walk(): print(part.get_content_type()) print() # Print filenames of attachments print(\\"Attachment Filenames:\\") for part in msg.walk(): if part.get_content_disposition() == \'attachment\': print(part.get_filename())"},{"question":"# PyTorch Coding Assessment Question **Objective:** To assess your understanding of creating and manipulating tensors with shared underlying storage and evaluating the performance impact in PyTorch. **Problem Statement:** You are given two tasks to demonstrate your understanding of the PyTorch storage mechanism. 1. **Task 1: Construct a Tensor from Shared Storage** - Create a tensor with random values of shape `(4, 4)` and `dtype=torch.float32`. - Obtain its underlying storage and clone it. - Create a new tensor from the cloned storage such that it shares the same data with the original tensor. - Verify that both tensors indeed share the same storage by checking their underlying data pointers. 2. **Task 2: Manipulate Storage and Evaluate Performance Impact** - Create a tensor `A` of shape `(1000, 1000)` with random values. - Clone its storage into `B_storage`. - Create Tensor `B` from `B_storage` such that it mirrors Tensor `A`. - Set the values of Tensor `A` to zero and verify that the values in Tensor `B` have also changed. **Constraints and Requirements:** - Make use of PyTorch’s `torch` module only. - Ensure that Tensor and Storage creation are as optimized as possible. - The solution should be safe and should prioritize usage of Tensor-level methods wherever possible, per documentation guidelines. - The verification step must include checks that demonstrate the shared storage mechanism. **Input Format:** - No direct inputs; the code must set up all tensors and manipulations. **Output Format:** - Print statements showing the verification process and demonstrating that both tensors share the same storage. - Outputs must include tensor shapes, underlying storage comparisons, and values of tensors before and after manipulation. **Function Signature:** ```python import torch def storage_tensors(): # Task 1 A = torch.rand((4, 4), dtype=torch.float32) A_storage = A.untyped_storage() B = torch.empty((4, 4), dtype=torch.float32).set_(A_storage, storage_offset=0, size=A.size(), stride=A.stride()) print(\\"Task 1:\\") print(\\"Tensor A:\\") print(A) print(\\"Tensor B:\\") print(B) print(\\"A and B share same storage:\\", A.data_ptr() == B.data_ptr()) # Task 2 A = torch.rand((1000, 1000)) B_storage = A.untyped_storage().clone() B = torch.empty(A.size()).set_(B_storage, storage_offset=A.storage_offset(), size=A.size(), stride=A.stride()) A.fill_(0) print(\\"nTask 2:\\") print(\\"Tensor A after zeroing:\\") print(A) print(\\"Tensor B after A is zeroed:\\") print(B) # B should also reflect the changes in A because they share the same storage print(\\"A and B share same storage after changing A to zeros:\\", A.data_ptr() == B.data_ptr()) # Call the function storage_tensors() ``` Explain each step taken in the function through appropriate comments.","solution":"import torch def storage_tensors(): # Task 1: Construct a Tensor from Shared Storage # Create a tensor with random values of shape (4, 4) and dtype=torch.float32. A = torch.rand((4, 4), dtype=torch.float32) # Obtain its underlying storage. A_storage = A.storage() # Create a new tensor from the storage such that it shares the same data. B = torch.empty((4, 4), dtype=torch.float32).set_(A_storage, storage_offset=0, size=A.size(), stride=A.stride()) # Verify that both tensors indeed share the same storage by checking their data pointers. print(\\"Task 1:\\") print(\\"Tensor A:\\") print(A) print(\\"Tensor B:\\") print(B) print(\\"A and B share the same storage:\\", A.data_ptr() == B.data_ptr()) # Task 2: Manipulate Storage and Evaluate Performance Impact # Create tensor A of shape (1000, 1000) with random values. A = torch.rand((1000, 1000)) # Clone its storage. B_storage = A.storage().clone() # Create Tensor B from B_storage such that it mirrors Tensor A. B = torch.empty(A.size()).set_(B_storage, storage_offset=A.storage_offset(), size=A.size(), stride=A.stride()) # Set values of Tensor A to zero and verify that values in Tensor B remained the same. A.fill_(0) print(\\"nTask 2:\\") print(\\"Tensor A after zeroing:\\") print(A) print(\\"Tensor B:\\") print(B) print(\\"A and B share the same storage:\\", A.data_ptr() == B.data_ptr()) # Call the function storage_tensors()"},{"question":"Objective: To assess students\' understanding of pandas extensions by implementing and utilizing custom ExtensionArray and associated features. Task: You need to implement a custom pandas ExtensionArray to handle a simple data type and integrate it into a pandas DataFrame. The custom ExtensionArray should represent a data type of \\"HalfFloat\\", which will store floating-point numbers with only half the usual precision (i.e., rounding to the nearest 0.5). 1. **Implement the `HalfFloatArray` class**: - This class should subclass `pandas.api.extensions.ExtensionArray`. - The data within this array should be stored in a numpy array. - Implement necessary methods such as `_from_sequence`, `astype`, and `unique`. 2. **Create a `HalfFloatDtype` class**: - This class should subclass `pandas.api.extensions.ExtensionDtype`. - Implement properties like `type`, `name`, and `na_value`. 3. **Register the dtype and integrate it with pandas**: - Register the `HalfFloatArray` as a recognized extension type. 4. **Test your implementation**: - Create a pandas DataFrame that uses the new `HalfFloatArray` as one of its columns. - Perform some basic operations on this DataFrame (e.g., insert, fillna, unique). Constraints: - Your implementation must provide an interface compatible with pandas\' existing API. - You must handle missing values using `numpy.nan`. Expected Input/Output: - Input: A pandas DataFrame initialization and operations such as insert and fillna. - Output: Correct behavior compliant with pandas functionalities (e.g., handling NaN, unique values). ```python from pandas.api.extensions import ExtensionArray, ExtensionDtype, register_extension_dtype import numpy as np import pandas as pd class HalfFloatDtype(ExtensionDtype): # Define the dtype, type, name, and necessary methods class HalfFloatArray(ExtensionArray): # Define the array, necessary methods for pandas interaction # Register the dtype @register_extension_dtype class HalfFloatDtype(ExtensionDtype): # properties and methods @classmethod def construct_array_type(cls): return HalfFloatArray # Unit tests for the new data type def test_half_float_array(): # Create DataFrame and test df = pd.DataFrame({\'A\': [0.5, 1.0, 2.5, np.nan]}, dtype=\'HalfFloat\') df[\'B\'] = [3.5, np.nan, 4.0, 5.0] df[\'C\'] = df[\'A\'].fillna(0) assert df[\'A\'].unique().tolist() == [0.5, 1.0, 2.5, np.nan] test_half_float_array() ``` **Notes**: - Do not change the structure of the skeleton code provided. - Ensure compatibility with common pandas operations such as `fillna`, `unique`, and handling of missing values (`NaN`).","solution":"from pandas.api.extensions import ExtensionArray, ExtensionDtype, register_extension_dtype import numpy as np import pandas as pd class HalfFloatDtype(ExtensionDtype): name = \'HalfFloat\' type = np.float64 na_value = np.nan @classmethod def construct_array_type(cls): return HalfFloatArray class HalfFloatArray(ExtensionArray): def __init__(self, data): self._data = np.array(data, dtype=self.dtype.type) @property def dtype(self): return HalfFloatDtype() def __len__(self): return len(self._data) def __getitem__(self, item): if isinstance(item, int): return self._data[item] else: return HalfFloatArray(self._data[item]) def _from_sequence(cls, scalars, dtype=None, copy=False): return HalfFloatArray(scalars) def isna(self): return np.isnan(self._data) def fillna(self, value=None, method=None, limit=None): if value is None: value = self.dtype.na_value filled_data = np.where(self.isna(), value, self._data) return HalfFloatArray(filled_data) def unique(self): unique_data = np.unique(self._data) return HalfFloatArray(unique_data) def astype(self, dtype, copy=True): if dtype == self.dtype: return self.copy() if copy else self return np.array(self._data, dtype=dtype, copy=copy) def copy(self): return HalfFloatArray(self._data.copy()) def __repr__(self): return f\\"HalfFloatArray({self._data})\\" @register_extension_dtype class HalfFloatDtype(ExtensionDtype): name = \'HalfFloat\' type = np.float64 na_value = np.nan @classmethod def construct_array_type(cls): return HalfFloatArray # Functions to generate test data and an example DataFrame def generate_half_float_data(size): data = np.round(np.random.uniform(-10, 10, size), 1) data[data%1 != 0.5] = np.ceil(data[data%1 != 0.5]) return data def create_half_float_dataframe(): data = generate_half_float_data(10) return pd.DataFrame({\'HalfFloatColumn\': HalfFloatArray(data)})"},{"question":"# Advanced PyTorch Serialization and Deserialization **Objective:** Design a class in PyTorch that demonstrates a deep understanding of serialization and deserialization concepts, including handling tensor views and module state dictionaries. Your solution must also include testing components, saving both tensor views and custom modules, and ensuring their integrity upon reloading. # Question: **Problem Statement:** You are tasked with creating a custom neural network module in PyTorch, saving its state, and verifying its integrity upon loading. Follow the steps below to complete the assignment: 1. **Define a Custom Neural Network Module:** Create a custom neural network module named `MyCustomModule` that includes the following: - Two linear layers (`l1` and `l2`). - An activation function (`ReLU`) after the first linear layer. 2. **Creating and Saving Tensors with Views:** - Generate a tensor `data` of shape `(10, 10)` using `torch.arange`. - Create two views of this tensor: `view1` (all elements) and `view2` (every second element). - Save the original tensor and the views in a single file named `tensor_views.pt`. 3. **Save the Model State:** - Save the state dictionary of `MyCustomModule` to a file named `model_state.pt`. 4. **Testing Saved Tensors and Model:** - Load the saved tensor views from `tensor_views.pt` and verify their integrity by performing an element-wise multiplication on `view1` and checking its reflection in the original tensor. - Load the state dictionary from `model_state.pt` and ensure it can be correctly applied to a new instance of `MyCustomModule`. **Input and Output:** 1. `MyCustomModule` has no specific input or output as it’s a class definition. 2. Tensor verification returns a boolean indicating successful integrity check. 3. State dictionary loading returns a message indicating success or failure. **Constraints and Requirements:** - Ensure tensor views maintain their relational properties upon saving and loading. - Use appropriate assertions to validate the correctness of loaded tensors and module state. **Performance Requirements:** - Ensure the saved tensor file is properly optimized in size. # Implementation ```python import torch import torch.nn as nn # Step 1: Define a custom neural network module class MyCustomModule(nn.Module): def __init__(self): super(MyCustomModule, self).__init__() self.l1 = nn.Linear(10, 5) self.l2 = nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.l1(x)) x = self.l2(x) return x # Step 2: Create and save tensor views data = torch.arange(100).reshape(10, 10) view1 = data[:,:] view2 = data[:,::2] torch.save((data, view1, view2), \'tensor_views.pt\') # Step 3: Instantiate and save model state model = MyCustomModule() torch.save(model.state_dict(), \'model_state.pt\') # Load and check tensor views saved_data, saved_view1, saved_view2 = torch.load(\'tensor_views.pt\') saved_view1 *= 2 assert torch.equal(saved_data, data), \\"Tensor integrity failed.\\" print(\\"Tensor views integrity check passed.\\") # Load and check module state new_model = MyCustomModule() state_dict = torch.load(\'model_state.pt\') new_model.load_state_dict(state_dict) print(\\"Model state loaded successfully.\\") ``` This question ensures the student has a deep understanding of PyTorch\'s serialization capabilities, tensor views, and module state dictionaries. They will also need to demonstrate comprehension by saving, loading, and verifying data integrity effectively.","solution":"import torch import torch.nn as nn # Step 1: Define a custom neural network module class MyCustomModule(nn.Module): def __init__(self): super(MyCustomModule, self).__init__() self.l1 = nn.Linear(10, 5) self.l2 = nn.Linear(5, 2) def forward(self, x): x = torch.relu(self.l1(x)) x = self.l2(x) return x # Step 2: Create and save tensor views data = torch.arange(100).reshape(10, 10) view1 = data[:, :] view2 = data[:, ::2] torch.save({\'data\': data, \'view1\': view1, \'view2\': view2}, \'tensor_views.pt\') # Step 3: Instantiate and save model state model = MyCustomModule() torch.save(model.state_dict(), \'model_state.pt\')"},{"question":"You are tasked with implementing a function that processes a list of mixed data types (integers, floats, and strings) as described below. Function Signature ```python def process_list(data: list) -> (int, float, str, list): ``` Input - `data`: A list containing elements that are either integers, floats, or strings. Output A tuple containing: 1. The sum of all integers in the list. 2. The product of all floating point numbers in the list. 3. A single string that is a concatenation of all strings in the list, separated by a space (\' \'). 4. A list of all elements in the input list in reverse order. Constraints - The list `data` will contain at least one element. - Elements of the list will be either integers, floats, or strings (no other types will be present). Performance Requirements - Aim to complete the function efficiently within a single pass through the list. Example ```python data = [1, \\"hello\\", 2.5, \\"world\\", 3, 4] result = process_list(data) # Expected output: (8, 2.5, \\"hello world\\", [4, 3, \\"world\\", 2.5, \\"hello\\", 1]) ``` Explanation 1. The sum of integers is `1 + 3 + 4 = 8`. 2. The product of floats is `2.5`. 3. The concatenated string is `\\"hello world\\"`. 4. The reversed list is `[4, 3, \\"world\\", 2.5, \\"hello\\", 1]`.","solution":"def process_list(data): Processes a list containing integers, floats, and strings and returns a tuple: 1. Sum of all integers in the list. 2. Product of all floating point numbers in the list. 3. Concatenation of all strings in the list, separated by a space. 4. The list reversed. Parameters: - data: A list containing integers, floats, and strings. Returns: A tuple consisting of (sum_of_integers, product_of_floats, concatenated_string, reversed_list). sum_of_integers = 0 product_of_floats = 1.0 concatenated_string = \\"\\" has_floats = False for item in data: if isinstance(item, int): sum_of_integers += item elif isinstance(item, float): product_of_floats *= item has_floats = True elif isinstance(item, str): if concatenated_string: concatenated_string += \\" \\" + item else: concatenated_string = item if not has_floats: product_of_floats = 0.0 reversed_list = data[::-1] return (sum_of_integers, product_of_floats, concatenated_string, reversed_list)"},{"question":"**Objective:** Implement a Python function using the `resource` module to control and monitor resource usage. **Problem Statement:** You are required to write a Python function `monitor_resource_usage()` that sets resource limits on processor time and then monitors the resource usage of the current process. Your function should: 1. **Set CPU time limit:** - Set a soft limit of 5 seconds and a hard limit of 7 seconds for CPU usage (`RLIMIT_CPU`). 2. **Simulate a CPU-bound operation:** - Carry out a CPU-intensive task that lasts approximately 3 seconds (e.g., a loop that performs calculations). 3. **Retrieve resource usage:** - After the CPU-bound operation, retrieve the resource usage statistics for the current process. 4. **Return the results:** - Return the resource usage as a dictionary with keys representing the resource types (e.g., `ru_utime`, `ru_stime`, `ru_maxrss`, etc.) and values being the respective measurements. **Function Signature:** ```python def monitor_resource_usage() -> dict: pass ``` **Constraints:** - Ensure your function handles any `OSError` or `ValueError` exceptions that may arise during the setting of limits. - Ensure the CPU-intensive task lasts around 3 seconds but does not exceed the 5-second soft limit imposed. **Example Output:** ```python { \'ru_utime\': 3.0, # Example output; actual values will vary \'ru_stime\': 0.1, # Example output; actual values will vary \'ru_maxrss\': 2048, # Example output; actual values will vary # ... other fields ... } ``` **Note:** - You do not need to implement unit tests, but your function should be both functional and handle exceptions gracefully. - Make sure your solution is efficient and does not perform redundant computations.","solution":"import resource import time def monitor_resource_usage() -> dict: Sets resource limits and performs a CPU-intensive task, then retrieves and returns the resource usage statistics. Returns: dict: A dictionary containing resource usage statistics. # Set CPU time limit: 5 seconds soft, 7 seconds hard try: resource.setrlimit(resource.RLIMIT_CPU, (5, 7)) except (OSError, ValueError) as e: return {\'error\': str(e)} # Simulate a CPU-bound operation lasting around 3 seconds start_time = time.time() while time.time() - start_time < 3: _ = 2 ** 20 # Retrieve resource usage statistics usage = resource.getrusage(resource.RUSAGE_SELF) # Convert the usage to a dictionary usage_dict = { \'ru_utime\': usage.ru_utime, \'ru_stime\': usage.ru_stime, \'ru_maxrss\': usage.ru_maxrss, \'ru_ixrss\': usage.ru_ixrss, \'ru_idrss\': usage.ru_idrss, \'ru_isrss\': usage.ru_isrss, \'ru_minflt\': usage.ru_minflt, \'ru_majflt\': usage.ru_majflt, \'ru_nswap\': usage.ru_nswap, \'ru_inblock\': usage.ru_inblock, \'ru_oublock\': usage.ru_oublock, \'ru_msgsnd\': usage.ru_msgsnd, \'ru_msgrcv\': usage.ru_msgrcv, \'ru_nsignals\': usage.ru_nsignals, \'ru_nvcsw\': usage.ru_nvcsw, \'ru_nivcsw\': usage.ru_nivcsw, } return usage_dict"},{"question":"Advanced `ChainMap` Usage Overview You are required to implement a function that leverages the `collections.ChainMap` class to simulate a hierarchical configuration management system. In this system, each level of configuration overrides values from previous levels. The function should support creating new levels, updating values, and retrieving the effective configuration at any given level. Function Signature ```python def configure_hierarchy(command: str, *args, **kwargs) -> ChainMap: pass ``` Function Description 1. `configure_hierarchy(command: str, *args, **kwargs) -> ChainMap`: - **command**: - `\\"create\\"`: Create a new child configuration level. It should return a new `ChainMap` with the previous hierarchy. - `\\"update\\"`: Update the first mapping (current configuration) with key-value pairs provided in `kwargs`. - `\\"retrieve\\"`: Retrieve a value from the current configuration. If the key doesn\'t exist in the first mapping, it should search through the rest of the ChainMap layers. - `\\"effective\\"`: Retrieve the effective configuration as a single dictionary by merging all levels. - **args**: - For `\\"retrieve\\"` command, args will include `key` (e.g., `args[0]`). - **kwargs**: - For `\\"update\\"` command, kwargs will include key-value pairs to be updated in the current configuration. Example Usages ```python # Create a root configuration root_config = configure_hierarchy(\\"create\\") # Update root configuration root_config = configure_hierarchy(\\"update\\", color=\\"blue\\", user=\\"admin\\") # Create a new child configuration child_config = configure_hierarchy(\\"create\\", root_config) # Update child configuration child_config = configure_hierarchy(\\"update\\", root_config, color=\\"red\\") # Retrieve value from current (child) configuration value = configure_hierarchy(\\"retrieve\\", child_config, \\"color\\") # Outputs: \\"red\\" # Get the effective configuration from child level effective_config = configure_hierarchy(\\"effective\\", child_config) # Outputs: {\'color\': \'red\', \'user\': \'admin\'} ``` Constraints - The function should handle up to `10**3` configuration levels. - The `update` command should only update the first configuration mapping in a `ChainMap`. - The function should correctly handle the merging of configurations in the `effective` command. Notes - You may assume all keys in the configuration are unique and strings. - The values are primitive data types (int, str, float, bool). Implement the `configure_hierarchy` function that supports these operations.","solution":"from collections import ChainMap def configure_hierarchy(command: str, *args, **kwargs) -> ChainMap: if command == \\"create\\": if len(args) == 0: return ChainMap({}) else: return ChainMap({}, *args) elif command == \\"update\\": config = args[0] config.maps[0].update(kwargs) return config elif command == \\"retrieve\\": config, key = args return config.get(key) elif command == \\"effective\\": config = args[0] result = {} for mapping in reversed(config.maps): result.update(mapping) return result"},{"question":"# MIME Quoted-Printable Encoding/Decoding You are tasked with implementing a set of functions to encode and decode strings using the MIME quoted-printable encoding scheme, making use of the `quopri` module. Function 1: `encode_quoted_printable(input_str: str, quotetabs: bool = False, header: bool = False) -> str` Implement a function that takes an input string and returns the MIME quoted-printable encoded string using the `quopri.encodestring` method. **Parameters:** - `input_str` (str): The string to be encoded. - `quotetabs` (bool): Flag to control whether embedded spaces and tabs should be encoded. Defaults to `False`. - `header` (bool): Flag to control if spaces are encoded as underscores. Defaults to `False`. **Returns:** - Encoded string in MIME quoted-printable format. Function 2: `decode_quoted_printable(encoded_str: str, header: bool = False) -> str` Implement a function that takes a MIME quoted-printable encoded string and returns the decoded string using the `quopri.decodestring` method. **Parameters:** - `encoded_str` (str): The string to be decoded. - `header` (bool): Flag to control if underscores are decoded as spaces. Defaults to `False`. **Returns:** - Decoded string in its original format. # Example ```python input_str = \\"Hello World! This is a test.\\" encoded_str = encode_quoted_printable(input_str, quotetabs=True, header=False) print(encoded_str) # Possible output: \'Hello=20World!=20This=20is=20a=20test.\' decoded_str = decode_quoted_printable(encoded_str, header=False) print(decoded_str) # Output: \'Hello World! This is a test.\' ``` # Constraints - The input strings (both `input_str` and `encoded_str`) will not exceed 10^6 characters. - The functions should handle spaces, tabs, and special characters as defined in the RFC 1521 and RFC 1522 specifications. # Notes - Use the `quopri.encodestring` and `quopri.decodestring` functions for encoding and decoding respectively. - Ensure that output strings are in a form that is human-readable where applicable. Implement these functions and ensure they pass the provided example and other edge cases as required by the specifications.","solution":"import quopri def encode_quoted_printable(input_str: str, quotetabs: bool = False, header: bool = False) -> str: Encode a string using MIME quoted-printable encoding. Parameters: input_str (str): The string to be encoded. quotetabs (bool): Flag to control whether embedded spaces and tabs should be encoded. Defaults to False. header (bool): Flag to control if spaces are encoded as underscores. Defaults to False. Returns: str: Encoded string in MIME quoted-printable format. bytes_input = input_str.encode() encoded_bytes = quopri.encodestring(bytes_input, quotetabs=quotetabs, header=header) return encoded_bytes.decode() def decode_quoted_printable(encoded_str: str, header: bool = False) -> str: Decode a MIME quoted-printable encoded string. Parameters: encoded_str (str): The string to be decoded. header (bool): Flag to control if underscores are decoded as spaces. Defaults to False. Returns: str: Decoded string in its original format. bytes_input = encoded_str.encode() decoded_bytes = quopri.decodestring(bytes_input, header=header) return decoded_bytes.decode()"},{"question":"# **Coding Assessment Question** **Objective:** Write a function that categorizes and processes a list of error codes, identifying those that correspond to specific exceptions in Python. **Function Signature:** ```python def process_error_codes(error_list: list) -> dict: :param error_list: List of integer error codes to be processed. :return: Dictionary containing categorized error codes. The returned dictionary should have the following structure: { \\"PermissionError\\": [list of error codes corresponding to PermissionError], \\"FileNotFoundError\\": [list of error codes corresponding to FileNotFoundError], \\"ProcessLookupError\\": [list of error codes corresponding to ProcessLookupError], \\"InterruptedError\\": [list of error codes corresponding to InterruptedError], \\"FileExistsError\\": [list of error codes corresponding to FileExistsError], \\"NotADirectoryError\\": [list of error codes corresponding to NotADirectoryError], \\"IsADirectoryError\\": [list of error codes corresponding to IsADirectoryError], \\"BlockingIOError\\": [list of error codes corresponding to BlockingIOError], \\"ChildProcessError\\": [list of error codes corresponding to ChildProcessError], \\"BrokenPipeError\\": [list of error codes corresponding to BrokenPipeError], \\"TimeoutError\\": [list of error codes corresponding to TimeoutError], \\"ConnectionAbortedError\\": [list of error codes corresponding to ConnectionAbortedError], \\"ConnectionResetError\\": [list of error codes corresponding to ConnectionResetError], \\"ConnectionRefusedError\\": [list of error codes corresponding to ConnectionRefusedError], \\"Other\\": [list of error codes that do not match any of the above exceptions] } ``` # **Task Description:** 1. **Input Format:** - A list of integers representing error codes (e.g., `[1, 2, 3, 4]`). 2. **Output Format:** - A dictionary with keys being the exceptions and values being lists of error codes that correspond to those exceptions. 3. **Requirements:** - Use the `errno.errorcode` mapping and the provided error codes to categorize each error into the respective exceptions. - If an error code in the list does not correspond to a defined exception, categorize it under the \\"Other\\" key. - The function should correctly handle any list of error codes, even if it contains duplicates or codes not defined in the `errno` module. 4. **Constraints:** - The provided error codes will always be integers within a valid range. # **Example:** ```python # Example usage error_list = [1, 2, 3, 4, 17, 22, 28, 35, 100] # Sample error codes result = process_error_codes(error_list) print(result) # Output should be something like: # { # \\"PermissionError\\": [1, 13], # \\"FileNotFoundError\\": [2], # \\"ProcessLookupError\\": [3], # \\"ChildProcessError\\": [10], # \\"FileExistsError\\": [17], # \\"NotADirectoryError\\": [20], # \\"IsADirectoryError\\": [21], # \\"BlockingIOError\\": [11], # \\"BrokenPipeError\\": [32], # \\"TimeoutError\\": [110], # \\"ConnectionAbortedError\\": [], # \\"ConnectionResetError\\": [], # \\"ConnectionRefusedError\\": [], # \\"Other\\": [4, 22, 28, 35, 100] # } ``` # **Notes:** - The exact content of the example output is not important, as it depends on the specific error codes provided in the input list. - You may use the Python `errno` module as described in the documentation to map error codes to their exceptions.","solution":"import errno def process_error_codes(error_list): Categorizes and processes a list of error codes, identifying those that correspond to specific exceptions in Python. :param error_list: List of integer error codes to be processed. :return: Dictionary containing categorized error codes. error_mapping = { 1: \'PermissionError\', 2: \'FileNotFoundError\', 3: \'ProcessLookupError\', 4: \'InterruptedError\', 17: \'FileExistsError\', 20: \'NotADirectoryError\', 21: \'IsADirectoryError\', 11: \'BlockingIOError\', 10: \'ChildProcessError\', 32: \'BrokenPipeError\', 110: \'TimeoutError\', 103: \'ConnectionAbortedError\', 104: \'ConnectionResetError\', 111: \'ConnectionRefusedError\', } categorized_errors = { \\"PermissionError\\": [], \\"FileNotFoundError\\": [], \\"ProcessLookupError\\": [], \\"InterruptedError\\": [], \\"FileExistsError\\": [], \\"NotADirectoryError\\": [], \\"IsADirectoryError\\": [], \\"BlockingIOError\\": [], \\"ChildProcessError\\": [], \\"BrokenPipeError\\": [], \\"TimeoutError\\": [], \\"ConnectionAbortedError\\": [], \\"ConnectionResetError\\": [], \\"ConnectionRefusedError\\": [], \\"Other\\": [] } for code in error_list: error_type = error_mapping.get(code, \\"Other\\") categorized_errors[error_type].append(code) return categorized_errors"},{"question":"Implementing and Utilizing Custom Descriptors Objective: To assess the student\'s understanding and practical ability to implement and utilize custom descriptors in Python for managing and validating class attributes. Question: You are asked to create a set of custom descriptors that manage and validate class attributes, and then use these descriptors in a practical application scenario. # Part 1: Implementing Custom Descriptors 1. **Custom Descriptor `GreaterThan`:** - Create a descriptor `GreaterThan` that ensures an attribute value is greater than a specified minimum value. - Implement methods `__init__`, `__get__`, and `__set__`. 2. **Custom Descriptor `LengthInRange`:** - Create a descriptor `LengthInRange` that ensures the length of a string attribute falls within a specified range (inclusive). - Implement methods `__init__`, `__get__`, and `__set__`. 3. **How to Use These Descriptors:** - Implement these descriptors and use their instances to validate and manage attributes in a class. # Part 2: Applying Descriptors in a Class 1. Create a class `Product` that uses the following: - An attribute `price` managed by the `GreaterThan` descriptor to ensure the price is positive (greater than zero). - An attribute `product_name` managed by the `LengthInRange` descriptor to ensure the length of the product name is between 3 and 50 characters. 2. Implement the `Product` class with an initializer that sets these attributes appropriately. Input and Output: - The `GreaterThan` descriptor should raise a `ValueError` if an invalid value is assigned. - The `LengthInRange` descriptor should raise a `ValueError` if a string of invalid length is assigned. - The `Product` class should be able to set its attributes through its initializer and raise errors appropriately when invalid values are provided. Example Usage: ```python class GreaterThan: def __init__(self, min_value): self._min_value = min_value def __set_name__(self, owner, name): self._private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self._private_name) def __set__(self, obj, value): if value <= self._min_value: raise ValueError(f\\"Value must be greater than {self._min_value}\\") setattr(obj, self._private_name, value) class LengthInRange: def __init__(self, min_length, max_length): self._min_length = min_length self._max_length = max_length def __set_name__(self, owner, name): self._private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self._private_name) def __set__(self, obj, value): length = len(value) if length < self._min_length or length > self._max_length: raise ValueError(f\\"Length must be between {self._min_length} and {self._max_length}\\") setattr(obj, self._private_name, value) class Product: price = GreaterThan(0) product_name = LengthInRange(3, 50) def __init__(self, price, product_name): self.price = price self.product_name = product_name try: p1 = Product(10, \\"Example Product\\") print(p1.price) # should print 10 print(p1.product_name) # should print \\"Example Product\\" p2 = Product(-5, \\"Valid Name\\") # should raise ValueError for price except ValueError as e: print(e) try: p3 = Product(20, \\"ab\\") # should raise ValueError for product_name length except ValueError as e: print(e) ``` Constraints: - You must use Python version 3.10. - Descriptors must be implemented by defining appropriate methods (`__init__`, `__get__`, `__set__`, and optionally `__set_name__`). - The `Product` class should use these custom descriptors to manage and validate its attributes.","solution":"class GreaterThan: def __init__(self, min_value): self._min_value = min_value def __set_name__(self, owner, name): self._private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self._private_name) def __set__(self, obj, value): if value <= self._min_value: raise ValueError(f\\"Value must be greater than {self._min_value}\\") setattr(obj, self._private_name, value) class LengthInRange: def __init__(self, min_length, max_length): self._min_length = min_length self._max_length = max_length def __set_name__(self, owner, name): self._private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self._private_name) def __set__(self, obj, value): length = len(value) if length < self._min_length or length > self._max_length: raise ValueError(f\\"Length must be between {self._min_length} and {self._max_length}\\") setattr(obj, self._private_name, value) class Product: price = GreaterThan(0) product_name = LengthInRange(3, 50) def __init__(self, price, product_name): self.price = price self.product_name = product_name"},{"question":"**Title: Line Cache File Reader** **Problem Statement:** You are required to implement a function that extracts specific lines from a given text file using the `linecache` module. Your function will take a filename and a list of line numbers as inputs and will return a dictionary where the keys are the line numbers and the values are the corresponding lines from the file. If the file does not exist or a specific line number does not exist in the file, the value for that line number should be an empty string `\'\'`. **Function Signature:** ```python def extract_lines_from_file(filename: str, line_numbers: list) -> dict: pass ``` **Input:** - `filename` (str): Name of the text file to read from. - `line_numbers` (list): A list of integers representing the line numbers to retrieve. **Output:** - A dictionary with line numbers as keys and the corresponding lines from the file as values. **Constraints:** - The file may or may not exist. - The file may be large, so efficiency with respect to repeated line retrieval is required. - Line numbers are positive integers starting from 1. **Example:** ```python # Assuming \'sample.txt\' has the following content: # 1. Hello World # 2. Welcome to the linecache module. # 3. This is the third line. # 4. Fourth line is here. filename = \\"sample.txt\\" line_numbers = [1, 3, 5] output = extract_lines_from_file(filename, line_numbers) # Expected output: # { # 1: \'Hello Worldn\', # 3: \'This is the third line.n\', # 5: \'\' # } ``` **Notes:** - Use the `linecache` module to implement the function. - Ensure that the function does not raise any exceptions and handles all edge cases gracefully. - Use `linecache.clearcache()` after retrieving lines to prevent memory overflow in case of large files. **Advanced Considerations:** - Explain your approach to handle the caching and checking the validity of cache if files are modified during runtime. - Discuss how lazy caching can be utilized for non-file-based sources, if applicable.","solution":"import linecache def extract_lines_from_file(filename: str, line_numbers: list) -> dict: Extracts specific lines from a given text file. Args: - filename (str): Name of the text file to read from. - line_numbers (list): A list of integers representing the line numbers to retrieve. Returns: - dict: A dictionary with line numbers as keys and the corresponding lines from the file as values. result = {} for line_number in line_numbers: line = linecache.getline(filename, line_number) result[line_number] = line if line else \'\' linecache.clearcache() return result"},{"question":"# Asynchronous Task Processing System Objective You need to design an asynchronous task processing system using `asyncio.Queue`. This system will simulate multiple workers processing tasks retrieved from a queue. Tasks will have varied process times, and the system must efficiently distribute and manage these tasks. Requirements 1. Implement a function `async def process_task(queue: asyncio.Queue, worker_name: str) -> None` that will: - Continuously retrieve tasks from the queue using `await queue.get()` - Simulate processing time by sleeping for the number of seconds specified by the task. - Upon completion, call `task_done()` to notify that the task is finished. - Print a message indicating which worker processed the task and how long it took. 2. Implement a function `async def main()` that will: - Create an `asyncio.Queue()`. - Add 100 tasks to the queue, each task being a sleep duration (float) between 0.1 and 5.0 seconds. - Create and start 5 worker tasks using `asyncio.create_task()` to run `process_task`. - Use `queue.join()` to wait until all tasks are completed. - Cancel the worker tasks once all tasks are processed. Function Signatures ```python async def process_task(queue: asyncio.Queue, worker_name: str) -> None: pass async def main() -> None: pass if __name__ == \\"__main__\\": import asyncio asyncio.run(main()) ``` Constraints - You must use the `asyncio.Queue` for managing tasks. - Use asynchronous programming practices with `async` and `await` for non-blocking task management. - Ensure that your implementation properly handles queue states (e.g., queue empty, task completion). **Input** - No direct input will be given. The functions will generate and manage their tasks internally. **Output** - The system should print messages indicating task processing as specified. Example Output ``` worker-0 processed a task for 2.5 seconds worker-1 processed a task for 1.2 seconds worker-2 processed a task for 0.7 seconds ... All tasks are completed. ``` Implement the required functions ensuring they fulfill the described behavior and constraints.","solution":"import asyncio import random async def process_task(queue: asyncio.Queue, worker_name: str) -> None: while True: task_duration = await queue.get() await asyncio.sleep(task_duration) print(f\\"{worker_name} processed a task for {task_duration:.2f} seconds\\") queue.task_done() async def main() -> None: queue = asyncio.Queue() # Adding 100 tasks to the queue, each task being a sleep duration (float) between 0.1 and 5.0 seconds for _ in range(100): task_duration = random.uniform(0.1, 5.0) await queue.put(task_duration) # Creating and starting 5 worker tasks workers = [asyncio.create_task(process_task(queue, f\\"worker-{i}\\")) for i in range(5)] # Waiting until all tasks are completed await queue.join() # Cancel the worker tasks once all tasks are processed for worker in workers: worker.cancel() if __name__ == \\"__main__\\": import asyncio asyncio.run(main())"},{"question":"You are required to implement a custom file processing utility class `CustomFileProcessor` that leverages Python\'s I/O module to perform operations on both text and binary files. Your class should be able to handle the following functionalities: 1. **Reading text data**: Read a text file and return its contents. 2. **Writing text data**: Write given text data to a text file. 3. **Reading binary data**: Read a binary file and return its contents. 4. **Writing binary data**: Write given binary data to a binary file. 5. **Handling encodings**: Be able to read and write text files with a specified encoding, defaulting to \'UTF-8\'. 6. **Handling errors**: Implement error handling for encoding and decoding errors. Your class must support methods for each of these functionalities. Additionally, test your class by performing operations on sample text and binary files. # Class Definition ```python import io class CustomFileProcessor: def __init__(self): pass def read_text_file(self, file_path: str, encoding: str = \'utf-8\') -> str: Reads a text file with specified encoding and returns its contents. :param file_path: Path to the text file. :param encoding: Encoding to use for reading the file. :return: Contents of the text file as a string. pass def write_text_file(self, file_path: str, data: str, encoding: str = \'utf-8\') -> None: Writes given text data to a text file with specified encoding. :param file_path: Path to the text file. :param data: Text data to write to the file. :param encoding: Encoding to use for writing the file. pass def read_binary_file(self, file_path: str) -> bytes: Reads a binary file and returns its contents. :param file_path: Path to the binary file. :return: Contents of the binary file as bytes. pass def write_binary_file(self, file_path: str, data: bytes) -> None: Writes given binary data to a binary file. :param file_path: Path to the binary file. :param data: Binary data to write to the file. pass def read_text_with_error_handling(self, file_path: str, encoding: str = \'utf-8\', error_mode: str = \'strict\') -> str: Reads a text file with specified encoding and error handling mode. :param file_path: Path to the text file. :param encoding: Encoding to use for reading the file. :param error_mode: Error handling mode (\'strict\', \'ignore\', \'replace\'). :return: Contents of the text file as a string. pass def write_text_with_error_handling(self, file_path: str, data: str, encoding: str = \'utf-8\', error_mode: str = \'strict\') -> None: Writes given text data to a text file with specified encoding and error handling mode. :param file_path: Path to the text file. :param data: Text data to write to the file. :param encoding: Encoding to use for writing the file. :param error_mode: Error handling mode (\'strict\', \'ignore\', \'replace\'). pass ``` # Requirements 1. **Input/Output Formats**: The input to the methods will be file paths (as strings), data (as strings or bytes), and configurations for encoding and error modes. The output for read methods should be strings or bytes depending on the file type. 2. **Constraints**: Ensure proper handling of non-existent files, encoding issues, and other potential I/O errors. 3. **Performance**: Ensure efficient reading and writing, especially for large files. # Testing Create text and binary files with sample data and test each method of your `CustomFileProcessor` class to ensure all functionalities are working as expected. # Submission Submit your `CustomFileProcessor` class implementation along with a script demonstrating the use of each method with appropriate test cases.","solution":"import io class CustomFileProcessor: def __init__(self): pass def read_text_file(self, file_path: str, encoding: str = \'utf-8\') -> str: Reads a text file with specified encoding and returns its contents. :param file_path: Path to the text file. :param encoding: Encoding to use for reading the file. :return: Contents of the text file as a string. try: with open(file_path, \'r\', encoding=encoding) as file: return file.read() except Exception as e: print(f\\"Error reading text file: {e}\\") return \\"\\" def write_text_file(self, file_path: str, data: str, encoding: str = \'utf-8\') -> None: Writes given text data to a text file with specified encoding. :param file_path: Path to the text file. :param data: Text data to write to the file. :param encoding: Encoding to use for writing the file. try: with open(file_path, \'w\', encoding=encoding) as file: file.write(data) except Exception as e: print(f\\"Error writing text file: {e}\\") def read_binary_file(self, file_path: str) -> bytes: Reads a binary file and returns its contents. :param file_path: Path to the binary file. :return: Contents of the binary file as bytes. try: with open(file_path, \'rb\') as file: return file.read() except Exception as e: print(f\\"Error reading binary file: {e}\\") return b\\"\\" def write_binary_file(self, file_path: str, data: bytes) -> None: Writes given binary data to a binary file. :param file_path: Path to the binary file. :param data: Binary data to write to the file. try: with open(file_path, \'wb\') as file: file.write(data) except Exception as e: print(f\\"Error writing binary file: {e}\\") def read_text_with_error_handling(self, file_path: str, encoding: str = \'utf-8\', error_mode: str = \'strict\') -> str: Reads a text file with specified encoding and error handling mode. :param file_path: Path to the text file. :param encoding: Encoding to use for reading the file. :param error_mode: Error handling mode (\'strict\', \'ignore\', \'replace\'). :return: Contents of the text file as a string. try: with open(file_path, \'r\', encoding=encoding, errors=error_mode) as file: return file.read() except Exception as e: print(f\\"Error reading text file: {e}\\") return \\"\\" def write_text_with_error_handling(self, file_path: str, data: str, encoding: str = \'utf-8\', error_mode: str = \'strict\') -> None: Writes given text data to a text file with specified encoding and error handling mode. :param file_path: Path to the text file. :param data: Text data to write to the file. :param encoding: Encoding to use for writing the file. :param error_mode: Error handling mode (\'strict\', \'ignore\', \'replace\'). try: with open(file_path, \'w\', encoding=encoding, errors=error_mode) as file: file.write(data) except Exception as e: print(f\\"Error writing text file: {e}\\")"},{"question":"Objective: Demonstrate your understanding of the Python `crypt` package to securely store and verify passwords using various hashing methods. Problem Statement: Implement a system to securely hash and verify user passwords using the `crypt` module in Python. Specifically, you must: 1. **Create a function `hash_password(password, method)`**: - **Input**: `password` (a string, user\'s plain-text password), `method` (hashing method from `crypt.METHOD_*` like `crypt.METHOD_SHA512`). - **Output**: A string representing the hashed password. - **Constraints**: Ensure the generated salt is based on the method provided using `crypt.mksalt(method=method)`. - **Performance**: Use the strongest available method by default if no method is provided. 2. **Create a function `verify_password(stored_password, provided_password)`**: - **Input**: `stored_password` (a string, the hashed password including the salt), `provided_password` (a string, the password to verify). - **Output**: A boolean value `True` if the provided password matches the stored password, `False` otherwise. - **Constraints**: Use a constant-time comparison to mitigate timing attacks (`hmac.compare_digest()` is advised). 3. **Unit Tests**: Write unit tests to show the functionality of `hash_password` and `verify_password` using at least three different hashing methods (e.g., SHA-512, MD5, Blowfish). Example: ```python import crypt from hmac import compare_digest def hash_password(password, method=crypt.METHOD_SHA512): # Generate salt for given method salt = crypt.mksalt(method=method) return crypt.crypt(password, salt) def verify_password(stored_password, provided_password): return compare_digest(stored_password, crypt.crypt(provided_password, stored_password)) # Example usage hashed_password = hash_password(\'my_secure_password\') print(hashed_password) # Output: the hashed password # Verification assert verify_password(hashed_password, \'my_secure_password\') == True assert verify_password(hashed_password, \'wrong_password\') == False # Unit tests def test_password_functions(): password = \'another_secure_password\' for method in crypt.methods: hashed_password = hash_password(password, method) assert verify_password(hashed_password, password) == True assert verify_password(hashed_password, \'wrong_password\') == False test_password_functions() ``` Notes: 1. Consider edge cases such as very short and very long passwords. 2. Ensure your solution properly handles different hashing methods and rounds. 3. The problem assesses your understanding of password security, hashing, and Python’s standard library for cryptographic functions.","solution":"import crypt from hmac import compare_digest def hash_password(password, method=crypt.METHOD_SHA512): Hashes a password using the specified method. Parameters: - password (str): The plain-text password to hash. - method: The hashing method to use (e.g., `crypt.METHOD_SHA512`). Returns: - str: The hashed password. salt = crypt.mksalt(method=method) return crypt.crypt(password, salt) def verify_password(stored_password, provided_password): Verifies a password against the stored hashed password. Parameters: - stored_password (str): The hashed password including the salt. - provided_password (str): The plain-text password to verify. Returns: - bool: True if passwords match, False otherwise. return compare_digest(stored_password, crypt.crypt(provided_password, stored_password))"},{"question":"You need to implement a function called `are_broadcastable` that checks if two given tensors in PyTorch are broadcastable according to the broadcasting rules described below. # Broadcasting Rules: 1. Each tensor must have at least one dimension. 2. When iterating over the dimension sizes, starting at the trailing dimension, the dimensions sizes must either be equal, one of them is 1, or one of them does not exist. # Function Signature ```python import torch def are_broadcastable(tensor1: torch.Tensor, tensor2: torch.Tensor) -> bool: Given two tensor inputs, returns True if they are broadcastable, otherwise False. Parameters: tensor1 (torch.Tensor): The first tensor. tensor2 (torch.Tensor): The second tensor. Returns: bool: True if the tensors are broadcastable, otherwise False. ``` # Input - `tensor1` and `tensor2`: Two PyTorch tensors of any valid dimensions. Each tensor has at least one dimension. # Output - Return `True` if the tensors are broadcastable, `False` otherwise. # Examples ```python import torch # Example 1 tensor1 = torch.empty(5, 7, 3) tensor2 = torch.empty(5, 7, 3) print(are_broadcastable(tensor1, tensor2)) # Expected: True # Example 2 tensor1 = torch.empty(5, 3, 4, 1) tensor2 = torch.empty(3, 1, 1) print(are_broadcastable(tensor1, tensor2)) # Expected: True # Example 3 tensor1 = torch.empty(5, 2, 4, 1) tensor2 = torch.empty(3, 1, 1) print(are_broadcastable(tensor1, tensor2)) # Expected: False # Example 4 tensor1 = torch.empty(0,) tensor2 = torch.empty(2, 2) print(are_broadcastable(tensor1, tensor2)) # Expected: False ``` # Constraints - You can assume that the input tensors have at least one dimension. - Performance should consider the comparison of dimensions only and accommodate up to 10 dimensions max. # Notes - You may use any built-in PyTorch functions and methods. - Make sure to handle edge cases such as tensors with a single dimension. Implement the function `are_broadcastable` in Python to determine if PyTorch tensors can be broadcast together.","solution":"import torch def are_broadcastable(tensor1: torch.Tensor, tensor2: torch.Tensor) -> bool: Given two tensor inputs, returns True if they are broadcastable, otherwise False. Parameters: tensor1 (torch.Tensor): The first tensor. tensor2 (torch.Tensor): The second tensor. Returns: bool: True if the tensors are broadcastable, otherwise False. shape1 = tensor1.shape shape2 = tensor2.shape # Get the reversed shapes for easier comparison from the trailing dimensions shape1 = list(shape1[::-1]) shape2 = list(shape2[::-1]) # Compare each dimension from the trailing end for i in range(max(len(shape1), len(shape2))): dim1 = shape1[i] if i < len(shape1) else 1 dim2 = shape2[i] if i < len(shape2) else 1 if dim1 != dim2 and dim1 != 1 and dim2 != 1: return False return True"},{"question":"**Objective:** Demonstrate your understanding of Python\'s `site` module, particularly how it handles site-specific configurations and path manipulations during initialization. **Problem Statement:** You are required to implement a Python function, `configure_python_site(custom_dir)`, that simulates site-specific configurations for a Python interpreter setup. The function should: 1. Add a provided `custom_dir` to Python\'s module search path. 2. Check for a `pyvenv.cfg` file in the parent directory of the current Python executable. - If it exists, include the parent directory in the search path. 3. Process any `.pth` files in the `custom_dir` and add valid paths specified in these files to `sys.path`. 4. Report any paths that were added to `sys.path` and any lines of code executed from `.pth` files. **Input:** - `custom_dir`: A string representing the path to a custom directory to be added to the module search path. **Output:** - A dictionary with two keys: - `added_paths`: A list of paths that were added to `sys.path`. - `executed_lines`: A list of lines of code executed from `.pth` files in the `custom_dir`. **Constraints:** - The function assumes the provided `custom_dir` is valid and accessible. - Any non-existent paths or invalid entries in `.pth` files should be ignored. - The `pyvenv.cfg` file, if present, should be in the parent directory of `sys.executable`. **Example:** ```python def configure_python_site(custom_dir): # Your implementation here pass # Suppose \'custom_dir\' contains a valid path configuration file \'example.pth\' result = configure_python_site(\'/path/to/custom/dir\') print(result) # Expected Output (depending on the contents of \'example.pth\' and presence of \'pyvenv.cfg\'): { \\"added_paths\\": [\'/path/to/custom/dir\', \'/path/to/parent/of/sys/executable\'], \\"executed_lines\\": [\'import some_module\'] } ``` **Note:** - You may use relevant functions from the `site` module such as `addsitedir()` and `getuserbase()`, but do not use `site.main()` directly as it will reset the paths. - Ensure the function handles both UNIX and Windows-specific configurations appropriately.","solution":"import sys import os def configure_python_site(custom_dir): Add custom_dir to Python\'s module search path, check for pyvenv.cfg in the parent directory of the current Python executable, and process any .pth files in custom_dir. Parameters: custom_dir (str): Path to the custom directory to be added to the module search path. Returns: dict: Dictionary with added paths and executed lines from .pth files. added_paths = [] executed_lines = [] if os.path.isdir(custom_dir): # Add custom_dir to sys.path if custom_dir not in sys.path: sys.path.insert(0, custom_dir) added_paths.append(custom_dir) # Check for pyvenv.cfg file in the parent of the current Python executable python_exec_parent = os.path.dirname(sys.executable) pyvenv_cfg_path = os.path.join(python_exec_parent, \'pyvenv.cfg\') if os.path.isfile(pyvenv_cfg_path): if python_exec_parent not in sys.path: sys.path.insert(0, python_exec_parent) added_paths.append(python_exec_parent) # Process .pth files in custom_dir for item in os.listdir(custom_dir): if item.endswith(\'.pth\'): pth_file = os.path.join(custom_dir, item) with open(pth_file) as f: for line in f: clean_line = line.strip() if clean_line and not clean_line.startswith(\'#\'): if \'import\' in clean_line: exec(clean_line) executed_lines.append(clean_line) else: if os.path.isdir(clean_line): if clean_line not in sys.path: sys.path.insert(0, clean_line) added_paths.append(clean_line) return {\\"added_paths\\": added_paths, \\"executed_lines\\": executed_lines}"},{"question":"# Advanced Python Coding Assessment Question Problem Statement You are working for a statistical analysis company that is required to simulate and analyze customer behavior in a retail store. Use the Python `random` module to simulate the arrival times of customers and their purchase values. Additionally, analyze the simulated data to provide meaningful statistics. Task 1. **Simulate Customer Data**: - Simulate the arrival of `n` customers at a retail store. Assume the inter-arrival time of customers follows an exponential distribution with an average arrival rate of `λ` customers per minute (`expovariate(λ)`). - For each customer, generate a purchase value following a log-normal distribution with mean `mu` and standard deviation `sigma` (`lognormvariate(mu, sigma)`). 2. **Analyze the Data**: - Compute the total number of customers. - Compute the total sales (sum of all purchase values). - Compute the average purchase value. - Compute the standard deviation of the purchase values. - Provide a histogram of the purchase values using 10 bins. Function Signature ```python import random from typing import List, Tuple, Dict def simulate_customer_data(n: int, λ: float, mu: float, sigma: float) -> Tuple[List[float], List[float]]: Simulates the arrival times and purchase values of customers. :param n: Number of customers :param λ: Average arrival rate (customers per minute) :param mu: Mean of the log-normal distribution for purchase values :param sigma: Standard deviation of the log-normal distribution for purchase values :return: A tuple containing a list of arrival times and a list of purchase values pass def analyze_customer_data(arrival_times: List[float], purchase_values: List[float]) -> Dict[str, float]: Analyzes the customer data. :param arrival_times: List of customer arrival times :param purchase_values: List of customer purchase values :return: A dictionary containing the statistics: total_customers, total_sales, average_purchase, std_deviation pass def generate_purchase_histogram(purchase_values: List[float]) -> None: Generates and prints a histogram of purchase values. :param purchase_values: List of customer purchase values pass # Example usage n = 1000 λ = 1 / 5 # average 5 minutes between arrivals mu = 3.0 # mean of log-normal distribution sigma = 1.0 # standard deviation of log-normal distribution arrivals, purchases = simulate_customer_data(n, λ, mu, sigma) stats = analyze_customer_data(arrivals, purchases) generate_purchase_histogram(purchases) print(stats) ``` Constraints - You must use functions from the `random` module to generate random numbers. - Utilize standard library functions for any statistical calculations. Expected Output - A dictionary containing the statistics: `total_customers`, `total_sales`, `average_purchase`, and `std_deviation`. - A printed histogram of the purchase values. Performance Requirements - Your solution should handle up to `n = 100,000` customers efficiently.","solution":"import random import math import matplotlib.pyplot as plt from typing import List, Tuple, Dict def simulate_customer_data(n: int, λ: float, mu: float, sigma: float) -> Tuple[List[float], List[float]]: arrival_times = [] purchase_values = [] current_time = 0 for _ in range(n): # Simulate next arrival time based on exponential distribution inter_arrival_time = random.expovariate(λ) current_time += inter_arrival_time arrival_times.append(current_time) # Simulate purchase value based on log-normal distribution purchase_value = random.lognormvariate(mu, sigma) purchase_values.append(purchase_value) return arrival_times, purchase_values def analyze_customer_data(arrival_times: List[float], purchase_values: List[float]) -> Dict[str, float]: total_customers = len(purchase_values) total_sales = sum(purchase_values) average_purchase = total_sales / total_customers variance = sum((x - average_purchase) ** 2 for x in purchase_values) / total_customers std_deviation = math.sqrt(variance) return { \'total_customers\': total_customers, \'total_sales\': total_sales, \'average_purchase\': average_purchase, \'std_deviation\': std_deviation } def generate_purchase_histogram(purchase_values: List[float]) -> None: plt.hist(purchase_values, bins=10, edgecolor=\'black\') plt.xlabel(\'Purchase Value\') plt.ylabel(\'Frequency\') plt.title(\'Histogram of Purchase Values\') plt.show() # Example usage n = 1000 λ = 1 / 5 # average 5 minutes between arrivals mu = 3.0 # mean of log-normal distribution sigma = 1.0 # standard deviation of log-normal distribution arrivals, purchases = simulate_customer_data(n, λ, mu, sigma) stats = analyze_customer_data(arrivals, purchases) generate_purchase_histogram(purchases) print(stats)"},{"question":"**Question: Implement a Multi-Layer Window Manager Using curses.panel** # Objective: You are required to implement a multi-layer window manager using the \\"curses.panel\\" module. Your task involves creating multiple panels, manipulating their stack order, and handling their visibility based on user input. # Function Signature: ```python def window_manager(stdscr) -> None: ``` # Input: - `stdscr`: A standard screen window object provided by the curses library when initializing the curses application. # Requirements: 1. **Initialize Panels**: Create three panels with different content and colors. 2. **Panel Interaction**: Allow the user to switch the visibility of specific panels using keyboard input (e.g., hide/show panels with key presses). 3. **Stack Manipulation**: Allow the user to move panels up or down in the stack with keyboard input. 4. **Panel Movement**: Allow the user to move the selected panel to different coordinates on the screen. 5. **Update Display**: Ensure that all changes are properly reflected on the screen. # Instructions: - Use the keys \'1\', \'2\', \'3\' to toggle the visibility of the first, second, and third panels respectively. - Use the keys \'t\', \'b\' to move the currently focused panel to the top or bottom of the stack respectively. - Use the arrow keys to move the currently focused panel around the screen. - Use the \'q\' key to quit the application. # Constraints: - Ensure that panels do not move outside the screen boundaries. - Keep track of the currently focused panel and make sure only one panel can be moved at a time. # Sample Interaction: If the user presses \'1\', the first panel should toggle between being visible and hidden. If the user presses the \'UP\' arrow key, the currently focused panel should move upwards on the screen. Pressing \'t\' should move the currently focused panel to the top of the panel stack. # Performance: - The program should handle real-time interaction smoothly without noticeable delays. - Ensure that the changes made to the panels are updated correctly and efficiently. # Constraints: - You need to make sure the application does not crash and handles exceptions gracefully. # Example Code Block: ```python import curses import curses.panel def window_manager(stdscr): curses.curs_set(0) # Hide the default cursor stdscr.nodelay(0) # Make getch() non-blocking max_y, max_x = stdscr.getmaxyx() # Create windows win1 = curses.newwin(max_y//3, max_x//3, 0, 0) win2 = curses.newwin(max_y//2, max_x//2, max_y//3, max_x//3) win3 = curses.newwin(max_y//4, max_x//4, max_y//2, max_x//2) # Create panels panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) panel3 = curses.panel.new_panel(win3) # Set panel content win1.addstr(0, 0, \\"Panel 1\\") win2.addstr(0, 0, \\"Panel 2\\") win3.addstr(0, 0, \\"Panel 3\\") # Update the screen with initial content curses.panel.update_panels() stdscr.refresh() # Main loop while True: key = stdscr.getch() # Handle panel visibility toggles # Handle panel movement # Handle panel stacking # Handle quit curses.panel.update_panels() curses.doupdate() ... ``` Use the above template to develop the full function based on the detailed requirements provided.","solution":"import curses import curses.panel def window_manager(stdscr) -> None: curses.curs_set(0) # Hide the default cursor stdscr.nodelay(0) # Make getch() blocking to wait for user input max_y, max_x = stdscr.getmaxyx() # Create windows win1 = curses.newwin(max_y//3, max_x//3, 0, 0) win2 = curses.newwin(max_y//3, max_x//3, max_y//3, max_x//3) win3 = curses.newwin(max_y//3, max_x//3, 2*max_y//3, 2*max_x//3) # Create panels panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) panel3 = curses.panel.new_panel(win3) panels = [panel1, panel2, panel3] focus = 0 # Initially focused panel is the first panel. # Set panel content win1.addstr(0, 0, \\"Panel 1\\") win2.addstr(0, 0, \\"Panel 2\\") win3.addstr(0, 0, \\"Panel 3\\") # Update the screen with initial content curses.panel.update_panels() stdscr.refresh() while True: key = stdscr.getch() if key == ord(\'q\'): break if key == ord(\'1\'): toggle_visibility(panel1) elif key == ord(\'2\'): toggle_visibility(panel2) elif key == ord(\'3\'): toggle_visibility(panel3) elif key == ord(\'t\'): panels[focus].top() elif key == ord(\'b\'): panels[focus].bottom() elif key == curses.KEY_UP: move_panel(panels[focus], -1, 0, max_y, max_x) elif key == curses.KEY_DOWN: move_panel(panels[focus], 1, 0, max_y, max_x) elif key == curses.KEY_LEFT: move_panel(panels[focus], 0, -1, max_y, max_x) elif key == curses.KEY_RIGHT: move_panel(panels[focus], 0, 1, max_y, max_x) elif key == ord(\'t\'): focus = (focus + 1) % 3 # Cycle through panels curses.panel.update_panels() curses.doupdate() def toggle_visibility(panel): if panel.hidden(): panel.show() else: panel.hide() def move_panel(panel, dy, dx, max_y, max_x): y, x = panel.window().getbegyx() new_y = min(max(0, y + dy), max_y - 1) new_x = min(max(0, x + dx), max_x - 1) panel.move(new_y, new_x) # Function to run window_manager using curses.wrapper def main(): curses.wrapper(window_manager) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Assessment Question: Setup Configuration File Parsing and Application Problem Description You are tasked with automating the parsing and application of a `setup.cfg` configuration file for a Python package. The goal is to read the configuration file, extract specified options for the `build_ext` and `bdist_rpm` commands, and apply these options to the `setup.py` script execution. Requirements 1. Write a function `parse_setup_config(file_path)` that takes the path to a `setup.cfg` file and returns a dictionary where: - The keys are command names (`build_ext`, `bdist_rpm`, etc.) - The values are dictionaries of option names and values. 2. Write a function `apply_config(config_dict, command)` that takes the dictionary returned by `parse_setup_config` and a command name (`build_ext` or `bdist_rpm`) as inputs and prints the relevant command-line options that should be passed to `setup.py`. Input - `file_path`: A string representing the path to the `setup.cfg` file. - `command`: A string representing the command for which to apply the configuration (e.g., `build_ext` or `bdist_rpm`). Output - `parse_setup_config(file_path)` should return a dictionary with parsed configuration options. - `apply_config(config_dict, command)` should print the options in the format that they would be passed to the `setup.py` script. Constraints - The `setup.cfg` file follows the syntax described in the provided documentation. - Assume that the file contains sections corresponding to `build_ext` and `bdist_rpm` commands only. - Handle multiline option values correctly (e.g., lists of documentation files). Example Given the following `setup.cfg` file: ```ini [build_ext] inplace=1 include_dirs=/usr/local/include [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Calling `parse_setup_config(\'setup.cfg\')` should return: ```python { \\"build_ext\\": { \\"inplace\\": \\"1\\", \\"include_dirs\\": \\"/usr/local/include\\" }, \\"bdist_rpm\\": { \\"release\\": \\"1\\", \\"packager\\": \\"Greg Ward <gward@python.net>\\", \\"doc_files\\": \\"CHANGES.txt README.txt USAGE.txt doc/ examples/\\" } } ``` Calling `apply_config(config_dict, \'build_ext\')` should print: ``` --inplace=1 --include-dirs=/usr/local/include ``` Calling `apply_config(config_dict, \'bdist_rpm\')` should print: ``` --release=1 --packager=\\"Greg Ward <gward@python.net>\\" --doc-files=\\"CHANGES.txt README.txt USAGE.txt doc/ examples/\\" ``` # Submission Submit a single Python file (`setup_config_parser.py`) implementing the `parse_setup_config` and `apply_config` functions. Include any necessary import statements and assume that the configuration file is well-formed.","solution":"import configparser def parse_setup_config(file_path): Parses the setup.cfg file and returns a dictionary of command options. Args: file_path (str): The path to the setup.cfg file. Returns: dict: A dictionary with command options. config = configparser.ConfigParser() config.read(file_path) config_dict = {} for section in config.sections(): config_dict[section] = {key: value for key, value in config.items(section)} return config_dict def apply_config(config_dict, command): Prints the relevant command-line options for setup.py from the config dictionary. Args: config_dict (dict): The configuration dictionary. command (str): The command for which to apply the configuration (e.g., \'build_ext\' or \'bdist_rpm\'). if command in config_dict: options = config_dict[command] cmd_options = [] for key, value in options.items(): option_str = f\\"--{key.replace(\'_\', \'-\')}\\" if \' \' in value or \'=\' in value: option_str += f\'=\\"{value}\\"\' else: option_str += f\'={value}\' cmd_options.append(option_str) print(\\" \\".join(cmd_options))"},{"question":"**Objective**: Demonstrate your understanding of the `urllib.request` module for handling HTTP requests, managing headers, and dealing with errors. **Problem Statement**: You are required to create a script that performs the following tasks: 1. Fetches data from a given URL using a GET request. 2. Submits data to a given URL using a POST request. 3. Customizes the request with a specific User-Agent header. 4. Handles exceptions for network issues and invalid HTTP responses. **Requirements & Specifications**: 1. **GET Request**: - **Input**: A URL string. - **Output**: The content of the fetched URL or an error message if an exception occurs. 2. **POST Request**: - **Input**: A URL string and a dictionary containing data to be sent. - **Output**: The response content of the POST request or an error message if an exception occurs. 3. **Header Customization**: - For both GET and POST requests, use the User-Agent: `Mozilla/5.0 (Windows NT 6.1; Win64; x64)`. 4. **Exception Handling**: - Handle exceptions for network-related errors (`URLError`). - Handle HTTP errors (`HTTPError`) and print the error code. **Constraints**: - Do not use any third-party libraries. - Ensure that the `data` dictionary in the POST request is properly URL encoded before sending. **Function Signatures**: ```python def fetch_data(url: str) -> str: Fetches data from a given URL using a GET request. :param url: URL to fetch data from. :return: Content of the fetched URL or an error message if an exception occurs. pass def submit_data(url: str, data: dict) -> str: Submits data to a given URL using a POST request. :param url: URL to submit data to. :param data: Data to be submitted in dictionary format. :return: Response content of the POST request or an error message if an exception occurs. pass ``` # Example Usage ```python # Example 1: GET Request url = \'http://example.com\' print(fetch_data(url)) # Expected Output (may vary): # \\"<!doctype html>...</html>\\" or \\"URLError: <reason>\\" # Example 2: POST Request url = \'http://example.com/form\' data = {\'name\': \'John Doe\', \'email\': \'john@example.com\'} print(submit_data(url, data)) # Expected Output (may vary): # \\"<!doctype html>...</html>\\" or \\"HTTPError: <code>\\" ``` **Instructions**: 1. Implement the `fetch_data` function to make a GET request to the given URL. 2. Implement the `submit_data` function to make a POST request to the given URL with the provided data. 3. Usage of the provided User-Agent header is mandatory. 4. Implement robust error handling to catch and report network and HTTP errors.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError USER_AGENT = \'Mozilla/5.0 (Windows NT 6.1; Win64; x64)\' def fetch_data(url: str) -> str: try: req = urllib.request.Request(url, headers={\'User-Agent\': USER_AGENT}) with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return f\\"HTTPError: {e.code}\\" except URLError as e: return f\\"URLError: {e.reason}\\" def submit_data(url: str, data: dict) -> str: try: data_encoded = urllib.parse.urlencode(data).encode(\'utf-8\') req = urllib.request.Request(url, data=data_encoded, headers={\'User-Agent\': USER_AGENT}) with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return f\\"HTTPError: {e.code}\\" except URLError as e: return f\\"URLError: {e.reason}\\""},{"question":"**Objective**: Demonstrate proficiency with the seaborn library by creating a multi-faceted and composite plot. **Question**: You are given a dataset on penguins that includes various measurements such as flipper length, bill length, and species. Using seaborn, perform the following tasks: 1. **Load the Dataset**: Load the penguins dataset using `sns.load_dataset(\\"penguins\\")`. 2. **Axes-Level Plot**: Create a scatter plot to visualize the relationship between flipper length and bill length, color-coded by species. 3. **Figure-Level Plot**: Create a figure-level plot that shows the distribution of flipper length across different species. Use the `displot` function and facet by the `island` column. 4. **Composite Plot**: Use `jointplot` to show the joint distribution of flipper length and bill length, color-coded by species. Include both a scatter plot and marginal histograms. 5. **Customization**: Customize the `jointplot` by adding a regression line with a specified color and style. **Specifications**: - Input: The function should not take any input parameters as it will load the dataset internally. - Output: The function should generate and display the plots as specified. - Constraints: Use seaborn for all plotting tasks, and ensure plots are properly labeled and visually distinguishable. **Expected Output**: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Axes-Level Plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"Axes-Level Scatter Plot: Flipper vs Bill Length\\") plt.show() # Figure-Level Plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"island\\", multiple=\\"stack\\") plt.suptitle(\\"Figure-Level Distribution Plot\\", y=1.02) plt.show() # Composite Plot g = sns.jointplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", kind=\\"scatter\\") g.ax_joint.set_title(\\"Composite Joint Plot: Flipper vs Bill Length\\", pad=20) # Customization g.ax_joint.axline(xy1=(160, 30), slope=0.25, color=\\"b\\", dashes=(5, 2)) plt.show() # Call the function to display the plots visualize_penguins() ``` **Requirements**: - Demonstrate the use of `sns.scatterplot`, `sns.displot`, and `sns.jointplot` functions. - Incorporate faceted plots using the `col` parameter. - Customize the plots to include titles and regression lines.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Axes-Level Plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"Axes-Level Scatter Plot: Flipper vs Bill Length\\") plt.show() # Figure-Level Plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"island\\", multiple=\\"stack\\") plt.suptitle(\\"Figure-Level Distribution Plot\\", y=1.02) plt.show() # Composite Plot g = sns.jointplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", kind=\\"scatter\\") g.ax_joint.set_title(\\"Composite Joint Plot: Flipper vs Bill Length\\", pad=20) # Customization g.ax_joint.axline(xy1=(160, 30), slope=0.25, color=\\"b\\", dashes=(5, 2)) plt.show()"},{"question":"# Localization and Internationalization with `gettext` **Problem Description:** You are tasked with localizing a Python application to support multiple languages using the `gettext` module. The application displays different messages based on certain user actions, and these messages need to be translated into French and German. Requirements: 1. Create a directory structure for translation files: - `/locales/fr/LC_MESSAGES/messages.po` (French translations) - `/locales/de/LC_MESSAGES/messages.po` (German translations) 2. Initialize the `.po` files with the following messages: - \\"Welcome to our application.\\" - \\"You have successfully logged in.\\" - \\"Your session has expired.\\" 3. Compile the `.po` files into `.mo` files using the `msgfmt` utility (or a Python script for this process). 4. Implement a function `set_language(lang_code)` that switches between \'en\', \'fr\', and \'de\' languages. 5. Implement another function `display_messages()` that prints all three messages in the currently set language. Input and Output: - The function `set_language(lang_code)` takes a string representing the language code (e.g., \'en\', \'fr\', \'de\') as input and sets the application\'s language. - The function `display_messages()` does not take any input but prints the messages in the currently set language. Constraints: - You should create and use the `gettext.translation` method for switching languages. - Use proper error handling for missing translation files. - Assume that the translations are accurate and focus on the technical implementation. Example Usage: ```python set_language(\'en\') display_messages() # Output: # Welcome to our application. # You have successfully logged in. # Your session has expired. set_language(\'fr\') display_messages() # Output: # French translations corresponding to the above messages set_language(\'de\') display_messages() # Output: # German translations corresponding to the above messages ``` Guidelines: 1. Create the necessary `.po` and `.mo` files for French and German translations. 2. Implement the required functions in a Python script: - `set_language(lang_code)` - `display_messages()` 3. You may use the `gettext` module documentation to assist with the correct usage of functions and classes. *Note: The `.mo` files are binary files created from `.po` files, and for this exercise, you can assume the correct `.mo` files will be in place. Ensure your script works correctly by simulating the presence of these files as described in the problem.*","solution":"import gettext import os # Setting up the locales directory locales_directory = os.path.join(os.getcwd(), \'locales\') # Placeholder gettext translation variable translation = None def set_language(lang_code): Sets the language for the application. :param lang_code: Language code to switch to (\'en\', \'fr\', \'de\') global translation try: translation = gettext.translation(\'messages\', localedir=locales_directory, languages=[lang_code]) translation.install() except FileNotFoundError: gettext.install(\'messages\') # Fallback to the default language (English) def display_messages(): Prints the application messages in the current set language. print(_(\\"Welcome to our application.\\")) print(_(\\"You have successfully logged in.\\")) print(_(\\"Your session has expired.\\")) # Setup initial language to English set_language(\'en\') if __name__ == \\"__main__\\": display_messages() set_language(\'fr\') display_messages() set_language(\'de\') display_messages()"},{"question":"Objective Your task is to demonstrate your understanding of the Seaborn `Plot` class and its abilities to customize plot axis limits. You will read in a dataset, create a plot with it, and adjust the axis limits according to specific instructions. Problem Statement Given a dataset of temperature readings taken over several days, you need to visualize these readings and customize the axis limits using Seaborn\'s `Plot` class. Requirements: 1. Load the provided dataset `temperature.csv`. The dataset contains two columns: `day` (integer representing the day) and `temperature` (float representing the temperature in Celsius). 2. Create a line plot with markers showing the temperature over days. 3. Customize the axis limits as follows: - The x-axis should start from the minimum day in your data minus 1 and end at the maximum day in your data plus 1. - The y-axis should start at -10 and end at 40. - Reverse the y-axis such that it starts from 40 and ends at -10. Input: - A CSV file `temperature.csv` with the following structure: ``` day,temperature 1,15.5 2,17.8 3,18.2 ... ``` Output: - A plot displayed inline in the notebook or script output that adheres to the given specifications. Constraints: - You should use the `so.Plot` along with the `add` and `limit` methods for this task. # Function Signature ```python import seaborn.objects as so import pandas as pd def create_custom_temperature_plot(csv_file_path: str): # Your code here ``` # Example Usage ```python create_custom_temperature_plot(\'temperature.csv\') ``` Ensure your solution reads the data, processes it correctly, creates the plot, and applies the specified axis customizations.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so def create_custom_temperature_plot(csv_file_path: str): # Load the CSV file df = pd.read_csv(csv_file_path) # Create the plot p = ( so.Plot(df, x=\\"day\\", y=\\"temperature\\") .add(so.Line(marker=\'o\')) .limit(x=(df[\'day\'].min() - 1, df[\'day\'].max() + 1), y=(40, -10)) ) # Display the plot p.show()"},{"question":"**Objective:** To assess your understanding of the `sklearn.random_projection` module for dimensionality reduction and its practical application. **Problem Statement:** In this task, you are required to implement functions to reduce the dimensionality of a given high-dimensional dataset using both Gaussian and Sparse random projections, and then perform an inverse transformation to approximately reconstruct the original data from the reduced data. Finally, you will compare the reconstruction error between the two methods. **Function Implementations:** 1. **Function to Perform Gaussian Random Projection:** ```python def gaussian_random_projection(data, n_components): Reduces the dimensionality of the input data using Gaussian Random Projection. Parameters: data (numpy.ndarray): The high-dimensional input data. Shape (n_samples, n_features). n_components (int): The number of dimensions to project the data into. Returns: numpy.ndarray: The data projected into a lower-dimensional space. sklearn.random_projection.GaussianRandomProjection: The fitted GaussianRandomProjection transformer. pass # Your implementation here ``` 2. **Function to Perform Sparse Random Projection:** ```python def sparse_random_projection(data, n_components): Reduces the dimensionality of the input data using Sparse Random Projection. Parameters: data (numpy.ndarray): The high-dimensional input data. Shape (n_samples, n_features). n_components (int): The number of dimensions to project the data into. Returns: numpy.ndarray: The data projected into a lower-dimensional space. sklearn.random_projection.SparseRandomProjection: The fitted SparseRandomProjection transformer. pass # Your implementation here ``` 3. **Function to Compute Reconstruction Error:** ```python def reconstruction_error(original_data, reconstructed_data): Computes the mean squared error between the original data and the reconstructed data. Parameters: original_data (numpy.ndarray): The original high-dimensional data. reconstructed_data (numpy.ndarray): The data reconstructed from the lower-dimensional representation. Returns: float: The mean squared error between the original and reconstructed data. pass # Your implementation here ``` **Input Format:** - The dataset `data` will be a numpy array of shape `(n_samples, n_features)` where `n_samples <= 1000` and `n_features <= 10000`. - The number of components `n_components` will be an integer value where `n_components <= 500`. **Output:** - For both functions `gaussian_random_projection` and `sparse_random_projection`, the expected output is the projected data as a numpy array and the fitted transformer object. - The `reconstruction_error` function should return a float indicating the mean squared error. **Constraints:** - Ensure that the `compute_inverse_components` parameter is set to True when creating transformers. **Performance Requirements:** - The functions should efficiently handle the dataset size within reasonable time limits, leveraging the inherent computational efficiency of the random projection methods. **Example Usage:** ```python import numpy as np data = np.random.rand(100, 10000) # Gaussian Random Projection projected_data_gaussian, transformer_gaussian = gaussian_random_projection(data, 500) reconstructed_data_gaussian = transformer_gaussian.inverse_transform(projected_data_gaussian) error_gaussian = reconstruction_error(data, reconstructed_data_gaussian) # Sparse Random Projection projected_data_sparse, transformer_sparse = sparse_random_projection(data, 500) reconstructed_data_sparse = transformer_sparse.inverse_transform(projected_data_sparse) error_sparse = reconstruction_error(data, reconstructed_data_sparse) print(f\\"Reconstruction error (Gaussian): {error_gaussian}\\") print(f\\"Reconstruction error (Sparse): {error_sparse}\\") ``` **Task:** Implement the above-mentioned functions in Python.","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from sklearn.metrics import mean_squared_error def gaussian_random_projection(data, n_components): Reduces the dimensionality of the input data using Gaussian Random Projection. Parameters: data (numpy.ndarray): The high-dimensional input data. Shape (n_samples, n_features). n_components (int): The number of dimensions to project the data into. Returns: numpy.ndarray: The data projected into a lower-dimensional space. sklearn.random_projection.GaussianRandomProjection: The fitted GaussianRandomProjection transformer. transformer = GaussianRandomProjection(n_components=n_components) projected_data = transformer.fit_transform(data) return projected_data, transformer def sparse_random_projection(data, n_components): Reduces the dimensionality of the input data using Sparse Random Projection. Parameters: data (numpy.ndarray): The high-dimensional input data. Shape (n_samples, n_features). n_components (int): The number of dimensions to project the data into. Returns: numpy.ndarray: The data projected into a lower-dimensional space. sklearn.random_projection.SparseRandomProjection: The fitted SparseRandomProjection transformer. transformer = SparseRandomProjection(n_components=n_components) projected_data = transformer.fit_transform(data) return projected_data, transformer def reconstruction_error(original_data, reconstructed_data): Computes the mean squared error between the original data and the reconstructed data. Parameters: original_data (numpy.ndarray): The original high-dimensional data. reconstructed_data (numpy.ndarray): The data reconstructed from the lower-dimensional representation. Returns: float: The mean squared error between the original and reconstructed data. return mean_squared_error(original_data, reconstructed_data)"},{"question":"Objective: Write a Python function that takes a string input and performs a series of Unicode-related operations on it. The function should demonstrate your understanding of Unicode manipulation, encoding, and decoding operations. Problem Statement: Implement a function `process_unicode_string(input_string: str) -> dict` that takes a Unicode string as input and performs the following operations: 1. Convert the input string to its UTF-8 encoded byte representation. 2. Decode the UTF-8 byte sequence back to a Unicode string. 3. Convert the Unicode string to its UTF-16 encoded byte representation with a Byte Order Mark (BOM). 4. Decode the UTF-16 byte sequence back to a Unicode string. 5. Check if the original and final Unicode strings are identical. 6. Count the number of whitespace characters in the original Unicode string. 7. Replace all uppercase Latin letters in the original Unicode string with their lowercase counterparts. The function should return a dictionary with the following keys and respective values: - `\'utf8_encoded\'`: The UTF-8 encoded byte sequence of the input string. - `\'utf8_decoded\'`: The Unicode string decoded from the UTF-8 byte sequence. - `\'utf16_encoded\'`: The UTF-16 encoded byte sequence of the input string including BOM. - `\'utf16_decoded\'`: The Unicode string decoded from the UTF-16 byte sequence. - `\'identical\'`: A boolean indicating whether the original and final Unicode strings are identical. - `\'whitespace_count\'`: The number of whitespace characters in the original Unicode string. - `\'lowercase_replaced\'`: The Unicode string with all uppercase Latin letters replaced by their lowercase counterparts. Constraints: - The input string `input_string` contains valid Unicode characters. - The function should handle different types of Unicode characters, including Latin, Cyrillic, and Asian scripts. - The function should correctly manage any embedded null characters in the input string. Performance Requirements: - Your solution should efficiently handle strings of length up to 10,000 characters. Function Signature: ```python def process_unicode_string(input_string: str) -> dict: # Your implementation here ``` Example: ```python input_string = \\"Hello, World! Привет, мир! こんにちは、世界！\\" result = process_unicode_string(input_string) print(result) # Expected Output: # { # \'utf8_encoded\': b\'Hello, World! ...\', # \'utf8_decoded\': \'Hello, World! ...\', # \'utf16_encoded\': b\'xffxfeHx00ex00lx00lx00ox00,...\', # \'utf16_decoded\': \'Hello, World! ...\', # \'identical\': True, # \'whitespace_count\': 5, # \'lowercase_replaced\': \'hello, world! привет, мир! こんにちは、世界！\' # } ```","solution":"def process_unicode_string(input_string: str) -> dict: # Convert the input string to its UTF-8 encoded byte representation utf8_encoded = input_string.encode(\'utf-8\') # Decode the UTF-8 byte sequence back to a Unicode string utf8_decoded = utf8_encoded.decode(\'utf-8\') # Convert the Unicode string to its UTF-16 encoded byte representation with a Byte Order Mark (BOM) utf16_encoded = input_string.encode(\'utf-16\') # Decode the UTF-16 byte sequence back to a Unicode string utf16_decoded = utf16_encoded.decode(\'utf-16\') # Check if the original and final Unicode strings are identical identical = (input_string == utf16_decoded) # Count the number of whitespace characters in the original Unicode string whitespace_count = sum(1 for char in input_string if char.isspace()) # Replace all uppercase Latin letters in the original Unicode string with their lowercase counterparts lowercase_replaced = \'\'.join(char.lower() if char.isupper() else char for char in input_string) return { \'utf8_encoded\': utf8_encoded, \'utf8_decoded\': utf8_decoded, \'utf16_encoded\': utf16_encoded, \'utf16_decoded\': utf16_decoded, \'identical\': identical, \'whitespace_count\': whitespace_count, \'lowercase_replaced\': lowercase_replaced }"},{"question":"Objective: Demonstrate your understanding of the `sklearn.datasets` module by loading a dataset, processing it, and presenting a brief exploratory analysis. Task: 1. Load the Iris dataset using the appropriate function from `sklearn.datasets`. 2. Implement a function `load_and_analyze_iris_data` that performs the following: - Loads the Iris dataset. - Returns the dataset as a tuple `(X, y)`, where `X` is a numpy array of shape `(n_samples, n_features)` containing the feature data, and `y` is a numpy array of shape `(n_samples,)` containing the target values. - Prints the following information: - Number of samples. - Number of features. - First five rows of the feature data. - Unique target values and the number of samples for each target value. Function Signature: ```python import numpy as np from sklearn.datasets import load_iris def load_and_analyze_iris_data(): Loads the Iris dataset, performs a brief exploratory analysis, and prints the results. Returns: X (np.ndarray): The feature data of shape (n_samples, n_features). y (np.ndarray): The target values of shape (n_samples,). pass ``` Constraints: - You must use the `load_iris` function from `sklearn.datasets`. - Your code should be efficient and readable. - Do not use any additional datasets not provided by scikit-learn. Example Output: When you run the `load_and_analyze_iris_data` function, it should print: ``` Number of samples: 150 Number of features: 4 First five rows of feature data: [[5.1 3.5 1.4 0.2] [4.9 3.0 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5.0 3.6 1.4 0.2]] Unique target values and their counts: {0: 50, 1: 50, 2: 50} ``` Notes: - Make sure to test your function to ensure it produces the correct output. - Consider edge cases where the dataset might have different properties.","solution":"import numpy as np from sklearn.datasets import load_iris def load_and_analyze_iris_data(): Loads the Iris dataset, performs a brief exploratory analysis, and prints the results. Returns: X (np.ndarray): The feature data of shape (n_samples, n_features). y (np.ndarray): The target values of shape (n_samples,). # Load the Iris dataset iris = load_iris() X = iris.data y = iris.target # Basic exploratory analysis num_samples, num_features = X.shape first_five_rows = X[:5] unique_targets, counts = np.unique(y, return_counts=True) target_counts = dict(zip(unique_targets, counts)) # Print the analysis results print(f\\"Number of samples: {num_samples}\\") print(f\\"Number of features: {num_features}\\") print(\\"First five rows of feature data:\\") print(first_five_rows) print(f\\"Unique target values and their counts: {target_counts}\\") return X, y"},{"question":"Objective: In this task, you will implement a Multi-layer Perceptron (MLP) classifier using scikit-learn\'s `MLPClassifier`. Your implementation will include data preprocessing, model training, and performance evaluation. Instructions: 1. **Data Loading and Preprocessing**: - Load the provided dataset `X` (features) and `y` (labels) which are structured as follows: - `X`: A list of lists where each inner list represents feature values of a sample. - `y`: A list of integers where each integer represents the class label for the corresponding sample in `X`. - Split the data into 80% training and 20% testing sets. - Standardize the data using `StandardScaler` such that each feature has mean 0 and variance 1. 2. **Model Training**: - Initialize an `MLPClassifier` with the following parameters: - `hidden_layer_sizes=(10, 5)` - `solver=\'adam\'` - `learning_rate_init=0.01` - `max_iter=200` - `random_state=42` - Train the classifier on the training data. 3. **Model Evaluation**: - Predict the labels for the testing set. - Compute the accuracy of the model on the testing set. - Output the accuracy as a floating-point number. Function Signature: ```python def train_evaluate_mlp_classifier(X, y): Function to train and evaluate an MLP Classifier. Parameters: - X: List[List[float]] : Feature dataset where each inner list represents a sample\'s features. - y: List[int] : List of class labels corresponding to the samples in X. Returns: - float : Accuracy of the model on the testing set. # Example usage: # X = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8], [0.9, 1.0]] # y = [0, 1, 0, 1, 0] # print(train_evaluate_mlp_classifier(X, y)) ``` Constraints: - Use `train_test_split` from scikit-learn with `test_size=0.2` and `random_state=42` for splitting the data. - Use `StandardScaler` for standardizing the data. - The solution should be efficient and complete within a reasonable execution time for a typical dataset size. Example: For the given inputs: ```python X = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8], [0.9, 1.0]] y = [0, 1, 0, 1, 0] ``` The function should output: ```python 0.5 ```","solution":"from sklearn.neural_network import MLPClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score def train_evaluate_mlp_classifier(X, y): Function to train and evaluate an MLP Classifier. Parameters: - X: List[List[float]] : Feature dataset where each inner list represents a sample\'s features. - y: List[int] : List of class labels corresponding to the samples in X. Returns: - float : Accuracy of the model on the testing set. # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize the MLPClassifier clf = MLPClassifier(hidden_layer_sizes=(10, 5), solver=\'adam\', learning_rate_init=0.01, max_iter=200, random_state=42) # Train the classifier clf.fit(X_train_scaled, y_train) # Predict the labels for the testing set y_pred = clf.predict(X_test_scaled) # Compute and return the accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Question:** Implement a custom enumeration named `WeatherCondition` using the `enum` module in Python, meeting the following specifications: 1. The enumeration should have four members: `SUNNY`, `RAINY`, `SNOWY`, and `WINDY`. 2. Use the `auto` feature to automatically assign values to the enum members. 3. Implement a method `describe` in the enumeration that returns a string description for each weather condition based on the following mapping: - `SUNNY` → \\"The day is bright and sunny.\\" - `RAINY` → \\"It\'s raining. Don\'t forget your umbrella.\\" - `SNOWY` → \\"Snow is falling. Stay warm!\\" - `WINDY` → \\"It\'s windy outside. Hold onto your hat.\\" 4. Override the `__str__` method to return the name of the enumeration member in a user-friendly format (for example, `SUNNY` should be formatted as \\"Sunny\\"). 5. Ensure that your enumeration can be iterated over in the order of its definition and that it avoids any duplicate values. **Input:** You do not need to handle any input or output via standard input/output in your implementation. Just define the `WeatherCondition` class and its methods. **Output:** Your code will be tested by creating instances of `WeatherCondition` and calling the `describe` and `__str__` methods on those instances. **Example:** ```python from enum import Enum, auto class WeatherCondition(Enum): SUNNY = auto() RAINY = auto() SNOWY = auto() WINDY = auto() def describe(self): descriptions = { WeatherCondition.SUNNY: \\"The day is bright and sunny.\\", WeatherCondition.RAINY: \\"It\'s raining. Don\'t forget your umbrella.\\", WeatherCondition.SNOWY: \\"Snow is falling. Stay warm!\\", WeatherCondition.WINDY: \\"It\'s windy outside. Hold onto your hat.\\" } return descriptions[self] def __str__(self): return self.name.capitalize() ``` **Constraints:** - Your implementation should use the `Enum` and `auto` features of the `enum` module. - Ensure that the `describe` method and `__str__` method work correctly without any errors. **Performance Requirements:** - The code should efficiently handle the given descriptions and should be able to quickly return the required results for any enum member. - Avoid redundancy and ensure the absence of duplicate values in the enumeration.","solution":"from enum import Enum, auto class WeatherCondition(Enum): SUNNY = auto() RAINY = auto() SNOWY = auto() WINDY = auto() def describe(self): descriptions = { WeatherCondition.SUNNY: \\"The day is bright and sunny.\\", WeatherCondition.RAINY: \\"It\'s raining. Don\'t forget your umbrella.\\", WeatherCondition.SNOWY: \\"Snow is falling. Stay warm!\\", WeatherCondition.WINDY: \\"It\'s windy outside. Hold onto your hat.\\" } return descriptions[self] def __str__(self): return self.name.capitalize()"},{"question":"Implementing and Testing Custom Autograd Function in PyTorch Objective Design a custom PyTorch autograd function to compute the following mathematical operation: [ f(x) = sin(x) + log(exp(x) + 1) ] You need to implement both the forward and backward computations for this function using `torch.autograd.Function`. Then, you will verify the correctness of your implementation by comparing the gradients computed by your custom autograd function with those computed by PyTorch\'s automatic differentiation. Instructions 1. **Define Custom Autograd Function** - Create a class `CustomSinLogExp` that inherits from `torch.autograd.Function`. - Implement the `forward` and `backward` static methods. 2. **Forward Method**: - Takes a tensor as input. - Computes the output ( f(x) = sin(x) + log(exp(x) + 1) ). - Save any intermediary results needed for the backward pass. 3. **Backward Method**: - Takes the gradient of the output with respect to the input. - Compute the gradient of the input ( frac{df}{dx} ) using the results saved during the forward pass. - Return the gradient. 4. **Verification**: - Create a tensor `x` with `requires_grad=True`. - Compute the output using both your custom function and a standard PyTorch function. - Compare gradients computed by your custom function and PyTorch’s autograd using `torch.allclose`. Constraints - Input tensor `x` will be a 1-dimensional tensor with at least one element. - Ensure that your implementation efficiently handles tensors of large sizes. Example ```python import torch class CustomSinLogExp(torch.autograd.Function): @staticmethod def forward(ctx, x): # Save necessary tensors for backward pass exp_x = torch.exp(x) sin_x = torch.sin(x) log_exp_x_plus_1 = torch.log1p(exp_x) result = sin_x + log_exp_x_plus_1 ctx.save_for_backward(x, exp_x, sin_x, log_exp_x_plus_1) return result @staticmethod def backward(ctx, grad_output): x, exp_x, sin_x, log_exp_x_plus_1 = ctx.saved_tensors cos_x = torch.cos(x) grad_x = grad_output * (cos_x + (exp_x / (exp_x + 1))) return grad_x # Verification x = torch.randn(5, requires_grad=True) # Using custom autograd function custom_func = CustomSinLogExp.apply y_custom = custom_func(x) y_custom.backward(torch.ones_like(x)) custom_grad = x.grad.clone() # Using standard PyTorch operations x.grad.zero_() y_standard = torch.sin(x) + torch.log1p(torch.exp(x)) y_standard.backward(torch.ones_like(x)) standard_grad = x.grad # Compare gradients assert torch.allclose(custom_grad, standard_grad), \\"Gradients do not match!\\" print(\\"Gradients match. Custom autograd function is implemented correctly.\\") ``` This question assesses your understanding of how to create custom autograd functions in PyTorch, handle forward and backward computations manually, and verify the implementation correctness by comparing with PyTorch’s automatic differentiation.","solution":"import torch class CustomSinLogExp(torch.autograd.Function): @staticmethod def forward(ctx, x): # Save necessary tensors for backward pass exp_x = torch.exp(x) sin_x = torch.sin(x) log_exp_x_plus_1 = torch.log1p(exp_x) result = sin_x + log_exp_x_plus_1 ctx.save_for_backward(x, exp_x, sin_x, log_exp_x_plus_1) return result @staticmethod def backward(ctx, grad_output): x, exp_x, sin_x, log_exp_x_plus_1 = ctx.saved_tensors cos_x = torch.cos(x) grad_x = grad_output * (cos_x + (exp_x / (exp_x + 1))) return grad_x # Verification def verify_custom_function(): x = torch.randn(5, requires_grad=True) # Using custom autograd function custom_func = CustomSinLogExp.apply y_custom = custom_func(x) y_custom.backward(torch.ones_like(x)) custom_grad = x.grad.clone() # Using standard PyTorch operations x.grad.zero_() y_standard = torch.sin(x) + torch.log1p(torch.exp(x)) y_standard.backward(torch.ones_like(x)) standard_grad = x.grad # Compare gradients return torch.allclose(custom_grad, standard_grad) # Run verification as a part of solution if verify_custom_function(): print(\\"Gradients match. Custom autograd function is implemented correctly.\\") else: print(\\"Gradients do not match! Check the implementation details.\\")"},{"question":"Background: In Python, `set` and `frozenset` are used to create collections of unique elements. While `set` is mutable, allowing for elements to be added or removed, `frozenset` is immutable, meaning once created, it cannot be changed. Understanding and properly utilizing these two types is crucial for efficient and effective manipulation of unstructured data. Objective: Write a Python function `set_operations` that performs a series of set operations to demonstrate your understanding of the `set` and `frozenset` objects. Requirements: 1. Implement the function `set_operations(set_input, add_elements, remove_elements)`: - **Input**: - `set_input`: A list of integers that will be converted into an initial set. - `add_elements`: A list of integers that should be added to the set. - `remove_elements`: A list of integers that should be removed from the set if present. - **Output**: - Return a tuple containing: 1. The final `set` after performing the add and remove operations. 2. A `frozenset` which is a frozen copy of the final set. 3. A boolean indicating whether the element `42` is present in the final set (True if present, False otherwise). 2. Constraints: - The function should handle input lists that have up to 1000 integers. - The integers in the input lists will be between -1000 and 1000. 3. Performance Requirement: - Your implementation should have a time complexity of O(n) where n is the total number of operations (addition and removal). Example: ```python def set_operations(set_input, add_elements, remove_elements): # Your implementation goes here # Example usage: final_set, frozen_set, contains_42 = set_operations([1, 2, 3], [4, 5, 6], [1, 3]) print(final_set) # {2, 4, 5, 6} print(frozen_set) # frozenset({2, 4, 5, 6}) print(contains_42) # False ``` Your task is to write the body of the `set_operations` function. Make sure to handle the creation and manipulation of sets correctly, and ensure the immutability of the `frozenset`.","solution":"def set_operations(set_input, add_elements, remove_elements): Perform set operations: add elements, remove elements, and check for the presence of an element. Args: - set_input (list of int): The initial list of integers to create the set. - add_elements (list of int): The list of integers to add to the set. - remove_elements (list of int): The list of integers to remove from the set. Returns: - tuple: (final_set, frozen_set, contains_42), where - final_set is the modified set after addition and removal of elements, - frozen_set is a frozenset copy of the final_set, - contains_42 is a boolean indicating whether 42 is in the final_set. # Convert the input list to a set result_set = set(set_input) # Add elements from add_elements list result_set.update(add_elements) # Remove elements from remove_elements list result_set.difference_update(remove_elements) # Create a frozenset copy of the result_set frozen_result_set = frozenset(result_set) # Check if 42 is in the final set contains_42 = 42 in result_set return result_set, frozen_result_set, contains_42"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of PyTorch\'s `torch.multiprocessing` module by implementing a parallel processing solution that involves sharing tensors between processes. # Problem Statement You are tasked with implementing a parallel processing system using PyTorch that performs matrix multiplication in multiple processes and then aggregates the results in the main process. Your implementation should: 1. **Spawn multiple subprocesses** to perform the matrix multiplications. 2. **Share input matrices** between the main process and subprocesses efficiently. 3. **Aggregate the results** in the main process after all subprocesses have completed their tasks. 4. Handle **error propagation** and ensure subprocesses are cleaned up correctly if there are any errors. # Function Signature ```python import torch import torch.multiprocessing as mp def perform_parallel_matrix_multiplications(matrices, num_processes): Perform matrix multiplications in parallel using PyTorch\'s multiprocessing module. Args: - matrices (list of torch.Tensor): A list of 2D tensors to be multiplied. Each tensor should have the same size. - num_processes (int): The number of subprocesses to spawn. Returns: - torch.Tensor: A tensor that is the sum of the resulting matrices from all subprocesses. pass ``` # Input - `matrices`: A list of 2D torch tensors, each of shape `(N, N)`, representing the matrices to be multiplied. - `num_processes`: An integer representing the number of subprocesses to spawn. # Output - A single torch tensor that is the element-wise sum of all the resulting matrices from the subprocesses. # Constraints - Assume all matrices in the `matrices` list have the same dimensions. - The number of matrices will be at least 1 and at most 100. - The number of subprocesses will be at least 1 and at most the number of matrices. # Example ```python if __name__ == \\"__main__\\": matrices = [torch.rand(3, 3) for _ in range(4)] num_processes = 2 result = perform_parallel_matrix_multiplications(matrices, num_processes) print(result) # This should print a tensor of shape (3, 3) ``` # Additional Requirements 1. **Error Handling:** Ensure that all subprocesses are properly cleaned up if any error occurs in any subprocess. 2. **Resource Management:** Demonstrate effective use of shared memory to manage tensors between processes. # Hints - Use `torch.multiprocessing.spawn` to manage subprocess creation and lifecycle. - Utilize `torch.multiprocessing.Queue` or `shared_memory` for tensor sharing. - Remember to follow best practices for sharing CUDA tensors if necessary.","solution":"import torch import torch.multiprocessing as mp def perform_parallel_matrix_multiplications(matrices, num_processes): Perform matrix multiplications in parallel using PyTorch\'s multiprocessing module. Args: - matrices (list of torch.Tensor): A list of 2D tensors to be multiplied. Each tensor should have the same size. - num_processes (int): The number of subprocesses to spawn. Returns: - torch.Tensor: A tensor that is the sum of the resulting matrices from all subprocesses. def worker(matrices, result_queue, start_idx, end_idx): Worker function to calculate the element-wise sum of sublist of matrices. partial_sum = sum(matrices[start_idx:end_idx]) result_queue.put(partial_sum) # Multiprocessing manager to handle shared memory and queue manager = mp.Manager() result_queue = manager.Queue() processes = [] matrices_per_process = len(matrices) // num_processes for i in range(num_processes): start_idx = i * matrices_per_process # Ensure the last process takes remaining matrices end_idx = (i + 1) * matrices_per_process if i != num_processes - 1 else len(matrices) p = mp.Process(target=worker, args=(matrices, result_queue, start_idx, end_idx)) processes.append(p) p.start() for p in processes: p.join() result = torch.zeros_like(matrices[0]) while not result_queue.empty(): result += result_queue.get() return result"},{"question":"Objective: You are tasked with developing a small utility in Python that will read multiple text files from a directory, compress their contents, and then archive the compressed files into a single ZIP archive. Problem Description: Implement a function `compress_and_archive(directory_path: str, output_zip: str) -> None` that: 1. Reads all `.txt` files from the given `directory_path`. 2. Compresses the contents of each `.txt` file using the `lzma` compression algorithm. 3. Stores each compressed file with the same name as the original file but with an added `.xz` extension (e.g., `example.txt` becomes `example.txt.xz`). 4. Archives all compressed `.xz` files into a single ZIP file named as per `output_zip`. Requirements: - Use the `lzma` module for compression. - Use the `zipfile` module for archiving. - Ensure that the original directory structure is preserved within the ZIP file. - Only `.txt` files should be processed; other file types should be ignored. Input: - `directory_path` (str): The path to the directory containing `.txt` files. - `output_zip` (str): The file name for the output ZIP archive (e.g., `archive.zip`). Output: - The function should not return any value. It should create the `.xz` files and the resulting ZIP archive in the specified directory. Example: Suppose the directory `/home/user/docs` contains the following files: - `file1.txt` - `file2.txt` - `image.png` After calling `compress_and_archive(\'/home/user/docs\', \'compressed_archive.zip\')`, the following should happen: 1. The directory `/home/user/docs` will have new files `file1.txt.xz` and `file2.txt.xz`. 2. A ZIP archive named `compressed_archive.zip` will be created containing `file1.txt.xz` and `file2.txt.xz`. Notes: - Handle any potential exceptions appropriately (e.g., file read/write errors). - Ensure the function works efficiently with large numbers of files and large file sizes. Constraints: - You may assume that the `directory_path` exists and contains readable files. - The function should handle edge cases such as no `.txt` files in the directory. Function Signature: ```python def compress_and_archive(directory_path: str, output_zip: str) -> None: pass ```","solution":"import os import lzma import zipfile def compress_and_archive(directory_path: str, output_zip: str) -> None: Reads all `.txt` files from the given `directory_path`, Compresses the contents of each `.txt` file using the `lzma` compression algorithm, Archives all compressed `.xz` files into a single ZIP file named as per `output_zip`. txt_files = [f for f in os.listdir(directory_path) if f.endswith(\'.txt\')] if not txt_files: return # No .txt files to process compressed_files = [] for txt_file in txt_files: txt_file_path = os.path.join(directory_path, txt_file) compressed_file_path = txt_file_path + \'.xz\' with open(txt_file_path, \'rb\') as input_file, lzma.open(compressed_file_path, \'wb\') as compressed_file: compressed_file.write(input_file.read()) compressed_files.append(compressed_file_path) with zipfile.ZipFile(output_zip, \'w\') as zipf: for compressed_file in compressed_files: arcname = os.path.relpath(compressed_file, directory_path) zipf.write(compressed_file, arcname=arcname)"},{"question":"**Question: Implementing a Simple Python Script Executor in C** You are given a legacy C application in which you need to embed a Python interpreter. The application must be able to run a specified Python script and call a function within that script with provided integer arguments. Your task is to write a C function that initializes the Python interpreter, loads the specified Python script, and calls a specific function with integer arguments extracted from the command line. # Requirements: 1. The function should be named `execute_python_function`. 2. The function should take the following arguments: - `const char *script_name`: The name of the Python script file (e.g., \\"myscript.py\\"). - `const char *function_name`: The name of the function within the script to call (e.g., \\"multiply\\"). - `int argc`: The count of the arguments. - `char **argv`: The array of argument strings. 3. The Python script and function should be called with the arguments provided after the script and function name. 4. Ensure proper error handling and memory management. # Input: - `script_name` (string): Name of the Python script file. - `function_name` (string): Name of the function to call within the script. - `argc` (int): Number of command line arguments. - `argv` (array): Array of command line argument strings where the first two are the script name and function name, and the rest are the arguments to the function. # Output: - Print the return value of the function call if successful. - Print appropriate error messages if any step fails. # Constraints: - Assume that the Python script and function exist and are accessible. # Example Python Script: ```python def multiply(a, b): return a * b ``` # Example Usage: Command Line Input: ``` ./your_c_program multiply multiply 3 5 ``` Expected Output: ``` Result of call: 15 ``` # Implementation Guidelines: 1. Create a `main` function to test your `execute_python_function` with command line arguments. 2. Initialize the Python interpreter and execute the function within the script using the provided arguments. 3. Handle data conversion between the command line arguments and the Python function arguments. 4. Properly shut down the Python interpreter after execution. # Example Implementation: ```c #include <Python.h> #include <stdio.h> #include <stdlib.h> int execute_python_function(const char *script_name, const char *function_name, int argc, char **argv) { PyObject *pName, *pModule, *pFunc; PyObject *pArgs, *pValue; int i; if (argc < 2) { fprintf(stderr, \\"Usage: %s <pythonfile> <funcname> [args]n\\", argv[0]); return 1; } Py_Initialize(); pName = PyUnicode_DecodeFSDefault(script_name); if (pName == NULL) { PyErr_Print(); return 1; } pModule = PyImport_Import(pName); Py_DECREF(pName); if (pModule != NULL) { pFunc = PyObject_GetAttrString(pModule, function_name); if (pFunc && PyCallable_Check(pFunc)) { pArgs = PyTuple_New(argc - 2); for (i = 0; i < argc - 2; ++i) { pValue = PyLong_FromLong(atoi(argv[i + 2])); if (!pValue) { Py_DECREF(pArgs); Py_DECREF(pModule); fprintf(stderr, \\"Cannot convert argumentn\\"); return 1; } PyTuple_SetItem(pArgs, i, pValue); } pValue = PyObject_CallObject(pFunc, pArgs); Py_DECREF(pArgs); if (pValue != NULL) { printf(\\"Result of call: %ldn\\", PyLong_AsLong(pValue)); Py_DECREF(pValue); } else { PyErr_Print(); return 1; } } else { if (PyErr_Occurred()) { PyErr_Print(); } fprintf(stderr, \\"Cannot find function \\"%s\\"n\\", function_name); return 1; } Py_XDECREF(pFunc); Py_DECREF(pModule); } else { PyErr_Print(); fprintf(stderr, \\"Failed to load \\"%s\\"n\\", script_name); return 1; } if (Py_FinalizeEx() < 0) { return 1; } return 0; } int main(int argc, char *argv[]) { return execute_python_function(argv[1], argv[2], argc, argv); } ``` **Note:** Ensure you link your program with the Python library during compilation.","solution":"def execute_python_function(script_name, function_name, *args): Executes a function in a given Python script with provided arguments. Parameters: script_name (str): The name of the Python script. function_name (str): The name of the function to execute. args (list): The arguments to pass to the function. Returns: The result of the function call. import importlib.util import sys # Load the module spec = importlib.util.spec_from_file_location(\\"module.name\\", script_name) module = importlib.util.module_from_spec(spec) sys.modules[\\"module.name\\"] = module spec.loader.exec_module(module) # Get the function from the module func = getattr(module, function_name) # Call the function with the provided arguments result = func(*args) return result"},{"question":"# Python Coding Assessment Question: Advanced File Compression with `gzip` Objective Your task is to implement a function that compresses multiple files into a single gzip archive and a corresponding function to decompress the archive back into individual files. Problem Statement Implement two functions: 1. **compress_files(file_paths: List[str], output_path: str, compresslevel: int = 9) -> None** - **Input**: - `file_paths`: A list of strings where each string is the path to a file that needs to be compressed. - `output_path`: A string representing the path to the gzip archive that will be created. - `compresslevel`: An optional integer from 0 to 9 controlling the level of compression (default is 9). - **Output**: None. The function should create a gzip archive at `output_path` containing all specified files. 2. **decompress_archive(input_path: str, output_dir: str) -> None** - **Input**: - `input_path`: A string representing the path to the gzip archive to be decompressed. - `output_dir`: A string representing the directory where decompressed files will be saved. - **Output**: None. The function should decompress all files in the gzip archive to the specified directory, maintaining their original filenames. Constraints - Assume all paths provided are valid and accessible. - The directory specified by `output_dir` exists and is writable. - Handle any exceptions that might occur if a file cannot be either compressed or decompressed, and print a meaningful error message. Example Usage ```python file_paths = [\'/path/to/file1.txt\', \'/path/to/file2.txt\'] output_archive = \'/path/to/output_archive.gz\' output_directory = \'/path/to/output_directory\' # Compress multiple files into a single gzip archive compress_files(file_paths, output_archive) # Decompress the archive back into individual files decompress_archive(output_archive, output_directory) ``` Hints - You can use the `gzip.open()` method for reading and writing gzip files. - Consider using the `shutil.copyfileobj` method to handle file copying while compressing. - You might need to serialize multiple files into a single gzip archive; consider a way to package multiple files together before compression. - Ensure that decompressed files retain their original filenames after extraction.","solution":"import gzip import os from typing import List def compress_files(file_paths: List[str], output_path: str, compresslevel: int = 9) -> None: Compresses multiple files into a single gzip archive. try: with open(output_path, \'wb\') as archive: with gzip.GzipFile(fileobj=archive, mode=\'wb\', compresslevel=compresslevel) as gz: for file_path in file_paths: with open(file_path, \'rb\') as f: file_contents = f.read() gz.write(len(file_path).to_bytes(4, \'little\')) # Store length of filename gz.write(file_path.encode(\'utf-8\')) # Store filename gz.write(len(file_contents).to_bytes(4, \'little\')) # Store length of file_contents gz.write(file_contents) # Store file_contents except Exception as e: print(f\\"Error occurred: {e}\\") def decompress_archive(input_path: str, output_dir: str) -> None: Decompresses a gzip archive into individual files. try: with open(input_path, \'rb\') as archive: with gzip.GzipFile(fileobj=archive, mode=\'rb\') as gz: while True: filename_length_bytes = gz.read(4) if not filename_length_bytes: break filename_length = int.from_bytes(filename_length_bytes, \'little\') filename = gz.read(filename_length).decode(\'utf-8\') file_contents_length = int.from_bytes(gz.read(4), \'little\') file_contents = gz.read(file_contents_length) output_path = os.path.join(output_dir, os.path.basename(filename)) with open(output_path, \'wb\') as f: f.write(file_contents) except Exception as e: print(f\\"Error occurred: {e}\\")"},{"question":"# Efficient Data Handling and Algorithm Selection with PyTorch Objective: Write a function in PyTorch that checks if the given inputs satisfy the conditions to enable the persistent algorithm on a V100 GPU, and if so, proceeds with a specific computation using optimized methods. Function Signature: ```python import torch def optimize_computation(data: torch.Tensor, cudnn_enabled: bool) -> str: Checks input conditions and performs an optimized computation on V100 GPU if applicable. Parameters: - data (torch.Tensor): Input tensor to be processed. - cudnn_enabled (bool): Flag indicating if cudnn is enabled. Returns: - str: Message indicating whether the computation was optimized or not. pass ``` Input: - `data`: A PyTorch Tensor containing the input data that needs to be processed. - `cudnn_enabled`: A boolean flag indicating whether cudnn is enabled. Output: - The function should return a message: - `\\"Optimized computation performed\\"` if the conditions are met and the computation is performed using the persistent algorithm. - `\\"Standard computation performed\\"` if the conditions are not met and the standard method is used. Constraints: 1. You must check the following conditions within the function: - cudnn is enabled. - The data is on a GPU. - The data type of the input is `torch.float16`. - A V100 GPU is being used. - The input data is not in `PackedSequence` format (for simplicity, assume that PackedSequence is not used). 2. Use PyTorch functions to check these conditions and perform tensor operations as necessary. Example Usage: ```python data = torch.randn(100, 100, dtype=torch.float16).cuda() # Example data on GPU print(optimize_computation(data, cudnn_enabled=True)) # Should perform optimized computation data = torch.randn(100, 100, dtype=torch.float32).cuda() # Data not in required dtype print(optimize_computation(data, cudnn_enabled=True)) # Should perform standard computation ``` # Implementation: The student needs to implement the function using appropriate PyTorch library calls to check the conditions and perform the computations as specified. Note: Ensure that you have access to a V100 GPU and necessary libraries installed to run the examples successfully.","solution":"import torch import platform def optimize_computation(data: torch.Tensor, cudnn_enabled: bool) -> str: Checks input conditions and performs an optimized computation on V100 GPU if applicable. Parameters: - data (torch.Tensor): Input tensor to be processed. - cudnn_enabled (bool): Flag indicating if cudnn is enabled. Returns: - str: Message indicating whether the computation was optimized or not. if cudnn_enabled and torch.backends.cudnn.enabled and data.is_cuda and data.dtype == torch.float16: device_name = torch.cuda.get_device_name(data.device.index) if \\"V100\\" in device_name: # Optimized computation # We\'ll assume some placeholder optimized operations here for the example. # Actual optimized operations can vary based on use case. # For the purpose of this function, we assume the optimization is valid # with the given conditions and just return the message. return \\"Optimized computation performed\\" # If any condition fails, return standard computation message return \\"Standard computation performed\\""},{"question":"**Question:** Implement a simple chat server and client using the `socket` module in Python. The server should accept multiple client connections and broadcast messages sent by any client to all connected clients. Both the server and the clients should be able to run on separate machines but should handle basic errors and disconnections gracefully. # Server Requirements: 1. The server should listen on a specified IP and port. 2. The server should accept multiple client connections. 3. The server should broadcast any message received from a client to all connected clients. 4. The server should handle client disconnections gracefully. 5. The implementation should be efficient and not consume unnecessary resources while waiting for messages. # Client Requirements: 1. The client should connect to the server using the server\'s IP and port. 2. The client should allow users to send messages to the server. 3. The client should display any message broadcast by the server. # Input and Output Formats: - The server will not take any input from standard input but will receive messages from clients. - The client will take user messages from standard input and send them to the server. - Both server and client messages are strings. # Constraints: - You may assume that the number of clients is less than 100. - Messages are less than 1024 characters in length. # Performance Requirements: - Efficient handling of multiple client connections. - Use of non-blocking sockets for improved performance over blocking sockets. # Example: 1. **Starting the Server:** ```sh python chat_server.py 127.0.0.1 5000 ``` 2. **Starting a Client:** ```sh python chat_client.py 127.0.0.1 5000 ``` 3. **Client Sending a Message:** ``` Hello, World! ``` 4. **Client Receiving a Broadcast Message:** ``` User123: Hello, World! ``` # Hints: - Use threading or `select` module to handle multiple clients simultaneously. - Properly manage socket connections and ensure they are closed on exit. Implement both `chat_server.py` and `chat_client.py` according to the specifications above.","solution":"import socket import threading def broadcast_message(message, clients): for client in clients: try: client.sendall(message.encode()) except: client.close() clients.remove(client) def handle_client(client_socket, clients): while True: try: message = client_socket.recv(1024).decode() if not message: break broadcast_message(message, clients) except: break client_socket.close() clients.remove(client_socket) def start_server(server_ip, server_port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((server_ip, server_port)) server_socket.listen(100) clients = [] while True: client_socket, client_address = server_socket.accept() clients.append(client_socket) client_handler = threading.Thread( target=handle_client, args=(client_socket, clients)) client_handler.start() def client_program(server_ip, server_port): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((server_ip, server_port)) def receive_messages(): while True: try: message = client_socket.recv(1024).decode() if not message: break print(message) except: break receive_thread = threading.Thread(target=receive_messages) receive_thread.start() while True: try: message = input() client_socket.sendall(message.encode()) except: client_socket.close() break"},{"question":"Custom LZMA Compression and Decompression Objective: Implement a pair of functions `custom_lzma_compress` and `custom_lzma_decompress` that utilize the `lzma` module to compress and decompress data using custom filter chains and parameters. Functions Specification: 1. **Function: `custom_lzma_compress(data: bytes, preset: int, delta_dist: int) -> bytes`** - **Description**: Compress the input `data` using a custom filter chain which includes a delta filter and an LZMA2 filter. The LZMA2 filter should use the specified `preset` level, and the delta filter should use the specified `delta_dist`. - **Parameters**: - `data` (bytes): The data to be compressed. - `preset` (int): Compression preset level (0-9) OR-ed with `lzma.PRESET_EXTREME` optionally. - `delta_dist` (int): Distance value for the delta filter. - **Returns**: - Compressed data as a `bytes` object. 1. **Function: `custom_lzma_decompress(data: bytes) -> bytes`** - **Description**: Decompress the input `data`, which was compressed using the `custom_lzma_compress` function. - **Parameters**: - `data` (bytes): The compressed data to be decompressed. - **Returns**: - Decompressed data as a `bytes` object. Example Usage: ```python original_data = b\\"Example text data for compression and decompression.\\" preset_level = 6 delta_distance = 5 # Compressing the data compressed_data = custom_lzma_compress(original_data, preset_level, delta_distance) # Decompressing the data decompressed_data = custom_lzma_decompress(compressed_data) assert decompressed_data == original_data ``` Constraints: - The `preset` must be an integer between 0 and 9, optionally OR-ed with `lzma.PRESET_EXTREME`. - The `delta_dist` must be a positive integer. Notes: - Make sure to handle exceptions and edge cases appropriately. - You can use the provided examples in the documentation to guide your implementation. Performance Requirements: - Your implementation should be efficient in terms of memory usage to handle reasonably large datasets.","solution":"import lzma def custom_lzma_compress(data, preset, delta_dist): Compress the input data using a custom filter chain which includes a delta filter and an LZMA2 filter. Args: data (bytes): The data to be compressed. preset (int): Compression preset level (0-9) OR-ed with lzma.PRESET_EXTREME optionally. delta_dist (int): Distance value for the delta filter. Returns: bytes: Compressed data. filters = [ { \\"id\\": lzma.FILTER_DELTA, \\"dist\\": delta_dist }, { \\"id\\": lzma.FILTER_LZMA2, \\"preset\\": preset } ] compressor = lzma.LZMACompressor(format=lzma.FORMAT_XZ, filters=filters) compressed_data = compressor.compress(data) compressed_data += compressor.flush() return compressed_data def custom_lzma_decompress(data): Decompress the input data, which was compressed using the custom_lzma_compress function. Args: data (bytes): The compressed data to be decompressed. Returns: bytes: Decompressed data. decompressor = lzma.LZMADecompressor(format=lzma.FORMAT_XZ) decompressed_data = decompressor.decompress(data) return decompressed_data"},{"question":"**Objective**: To assess students\' understanding and practical implementation skills using the `zlib` and `gzip` modules for data compression and decompression in Python. **Problem Statement**: You need to implement a function that reads a text file, compresses its content using both `zlib` and `gzip` compression algorithms, and then writes both compressed data into two separate files. Additionally, implement decompression functions that will read these compressed files and return the original data. # Function Signatures Implement the following functions: 1. `compress_files(input_filepath: str, zlib_out_filepath: str, gzip_out_filepath: str)` 2. `decompress_zlib_file(zlib_filepath: str) -> str` 3. `decompress_gzip_file(gzip_filepath: str) -> str` # Specifications 1. **compress_files(input_filepath: str, zlib_out_filepath: str, gzip_out_filepath: str)**: - **Input**: - `input_filepath`: A string representing the path to the input text file. - `zlib_out_filepath`: A string representing the path where the zlib compressed output should be written. - `gzip_out_filepath`: A string representing the path where the gzip compressed output should be written. - **Output**: None - **Behavior**: - Read the contents of the file at `input_filepath`. - Compress the contents using `zlib` and write the compressed data to `zlib_out_filepath`. - Compress the contents using `gzip` and write the compressed data to `gzip_out_filepath`. 2. **decompress_zlib_file(zlib_filepath: str) -> str**: - **Input**: - `zlib_filepath`: A string representing the path to the zlib compressed file. - **Output**: A string representing the decompressed content of the file. - **Behavior**: Read the compressed data from `zlib_filepath`, decompress it using `zlib`, and return the original uncompressed content. 3. **decompress_gzip_file(gzip_filepath: str) -> str**: - **Input**: - `gzip_filepath`: A string representing the path to the gzip compressed file. - **Output**: A string representing the decompressed content of the file. - **Behavior**: Read the compressed data from `gzip_filepath`, decompress it using `gzip`, and return the original uncompressed content. # Constraints - Handle any IOException that may occur when reading or writing files and raise a user-friendly error message. - Assume the input file size does not exceed 10MB. - You must use the appropriate methods provided by each module for compression and decompression. # Example Usage: ```python compress_files(\'example.txt\', \'compressed_with_zlib.zlib\', \'compressed_with_gzip.gz\') original_content_zlib = decompress_zlib_file(\'compressed_with_zlib.zlib\') original_content_gzip = decompress_gzip_file(\'compressed_with_gzip.gz\') print(original_content_zlib == original_content_gzip) # Should print: True ``` # Note: Provide proper inline comments and function docstrings that explain the implementation clearly.","solution":"import zlib import gzip def compress_files(input_filepath: str, zlib_out_filepath: str, gzip_out_filepath: str): Compresses the contents of a text file using zlib and gzip algorithms and writes the compressed data to respective files. Parameters: input_filepath (str): The path to the input text file. zlib_out_filepath (str): The path where the zlib compressed output should be written. gzip_out_filepath (str): The path where the gzip compressed output should be written. Returns: None try: # Read the contents of the input file with open(input_filepath, \'r\') as input_file: data = input_file.read() # Compress the data using zlib compressed_data_zlib = zlib.compress(data.encode(\'utf-8\')) # Write the zlib compressed data to the output file with open(zlib_out_filepath, \'wb\') as zlib_output_file: zlib_output_file.write(compressed_data_zlib) # Compress the data using gzip with open(gzip_out_filepath, \'wb\') as gzip_output_file: with gzip.GzipFile(fileobj=gzip_output_file, mode=\'wb\') as gzip_file: gzip_file.write(data.encode(\'utf-8\')) except IOError as e: raise Exception(f\\"An error occurred while processing the files: {e}\\") def decompress_zlib_file(zlib_filepath: str) -> str: Decompresses the contents of a zlib compressed file and returns the original content. Parameters: zlib_filepath (str): The path to the zlib compressed file. Returns: str: The decompressed content of the file. try: # Read the zlib compressed data with open(zlib_filepath, \'rb\') as zlib_file: compressed_data_zlib = zlib_file.read() # Decompress the data using zlib decompressed_data = zlib.decompress(compressed_data_zlib) return decompressed_data.decode(\'utf-8\') except IOError as e: raise Exception(f\\"An error occurred while reading the zlib file: {e}\\") def decompress_gzip_file(gzip_filepath: str) -> str: Decompresses the contents of a gzip compressed file and returns the original content. Parameters: gzip_filepath (str): The path to the gzip compressed file. Returns: str: The decompressed content of the file. try: # Read the gzip compressed data with open(gzip_filepath, \'rb\') as gzip_input_file: with gzip.GzipFile(fileobj=gzip_input_file, mode=\'rb\') as gzip_file: decompressed_data = gzip_file.read() return decompressed_data.decode(\'utf-8\') except IOError as e: raise Exception(f\\"An error occurred while reading the gzip file: {e}\\")"},{"question":"You are tasked with creating a Python function that generates an email with a combination of text, image, and application attachments using the `email.mime` module. The function should construct a MIME email message and return it as a string. Function Signature: ```python def create_mime_email(sender: str, receiver: str, subject: str, body: str, image_data: bytes, app_data: bytes, app_filename: str) -> str: pass ``` Parameters: - `sender` (str): The email address of the sender. - `receiver` (str): The email address of the receiver. - `subject` (str): The subject of the email. - `body` (str): The plain text body of the email. - `image_data` (bytes): The raw image data to be attached to the email. - `app_data` (bytes): The raw data of the application file to be attached. - `app_filename` (str): The filename to be used for the application attachment. Return: - `str`: The complete email message as a string. Constraints: - Assume the image data can be decoded by the standard Python module `imghdr`. - Assume the application data is of MIME type `application/octet-stream`. - The generated email must include the following headers: - `From` set to `sender` - `To` set to `receiver` - `Subject` set to `subject` Example Usages: ```python sender = \'example@example.com\' receiver = \'recipient@example.com\' subject = \'Test Email\' body = \'This is a test email with an image and application attachment.\' image_data = b\'...binary image data...\' app_data = b\'...binary application data...\' app_filename = \'example.pdf\' email_message = create_mime_email(sender, receiver, subject, body, image_data, app_data, app_filename) print(email_message) ``` Instructions: 1. Create a multipart email message. 2. Add a plain text body. 3. Attach an image using the `MIMEImage` class. 4. Attach an application file using the `MIMEApplication` class. 5. Ensure the email message includes the correct headers. 6. Return the fully constructed email message as a string.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication import imghdr def create_mime_email(sender: str, receiver: str, subject: str, body: str, image_data: bytes, app_data: bytes, app_filename: str) -> str: # Create the root message msg = MIMEMultipart() msg[\'From\'] = sender msg[\'To\'] = receiver msg[\'Subject\'] = subject # Attach the plain text body msg.attach(MIMEText(body, \'plain\')) # Attach the image image_type = imghdr.what(None, image_data) if image_type: image = MIMEImage(image_data, _subtype=image_type) msg.attach(image) # Attach the application file attachment = MIMEApplication(app_data) attachment.add_header(\'Content-Disposition\', \'attachment\', filename=app_filename) msg.attach(attachment) # Return the email as a string return msg.as_string()"},{"question":"# Problem Description As a data scientist, you are asked to visualize different sets of data using varied color palettes from the seaborn library. Your task is to implement a function `create_plots_with_palettes` that: 1. Takes a list of tuples where each tuple contains: - A list of x values. - A list of y values. - The name of a seaborn color palette. - A flag indicating whether the palette should be treated as a colormap (`as_cmap`). 2. For each tuple, generates a scatter plot of the provided x and y values using the specified color palette. 3. Displays all the scatter plots. # Input A list of tuples, where each tuple contains: - `x_vals` (list of ints or floats): The x values for the scatter plot. - `y_vals` (list of ints or floats): The y values for the scatter plot. - `palette_name` (str): The name of the seaborn color palette. - `as_cmap` (bool): If `True`, treat the palette as a colormap. # Output No return value. It should display the scatter plots using the specified color palettes. # Function Signature ```python def create_plots_with_palettes(data: List[Tuple[List[Union[int, float]], List[Union[int, float]], str, bool]]) -> None: pass ``` # Example ```python data = [ ([1, 2, 3, 4], [1, 4, 9, 16], \\"husl\\", False), ([1, 2, 3, 4], [1, 4, 9, 16], \\"Spectral\\", True) ] create_plots_with_palettes(data) ``` In this example, the function will generate two scatter plots: 1. The first plot will use the \\"husl\\" palette. 2. The second plot will use the \\"Spectral\\" palette treated as a colormap. # Constraints - The lengths of `x_vals` and `y_vals` in each tuple will be the same. - The `palette_name` should be a valid seaborn color palette name. - `x_vals` and `y_vals` will contain 1 to 100 values. # Note Ensure that you handle the color palette appropriately based on whether `as_cmap` is `True` or `False` and produce visually distinct plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt from typing import List, Tuple, Union def create_plots_with_palettes(data: List[Tuple[List[Union[int, float]], List[Union[int, float]], str, bool]]) -> None: for x_vals, y_vals, palette_name, as_cmap in data: plt.figure() palette = sns.color_palette(palette_name, as_cmap=as_cmap) sns.scatterplot(x=x_vals, y=y_vals, palette=palette) plt.title(f\\"Scatter Plot using {palette_name} palette\\") plt.show()"},{"question":"# Coding Assessment Question Objective Your task is to implement a function that runs an asynchronous operation, ensuring compatibility with both Unix-based systems (such as macOS) and Windows. You will handle cases where certain asynchronous methods are supported or unsupported, adapting your approach based on the platform and event loop type. Function Signature ```python import asyncio async def run_async_operation(): Runs an asynchronous operation that initiates a connection and performs an I/O task. It must adapt to the underlying platform and event loop to ensure compatibility with Unix-based systems and Windows. Specifically, handle scenarios where certain methods are unsupported on Windows and older macOS versions. This function does not return any value but should print the status of the I/O task. pass ``` Requirements 1. **Platform Compatibility**: - On Windows: Use `ProactorEventLoop` by default. If unavailable, raise an appropriate error. Ensure subprocess operations are supported. - On Unix-based systems: Use the appropriate event loop to handle socket connections and I/O operations. - Handle older macOS versions (10.6, 10.7, 10.8) explicitly, using `SelectSelector` or `PollSelector` to support character devices. 2. **Connection and I/O**: - Initiate a connection to a dummy server. - Perform a simple I/O task, like reading from and writing to the network socket. - Print the status of the I/O task (\\"Success\\" or \\"Failure\\"). 3. **Error Handling**: - Handle and print relevant error messages when encountering unsupported methods or failed operations. Constraints - Do not use blocking calls; everything should be handled asynchronously. - Ensure the solution works efficiently within the constraints of the event loop used. Example Usage ```python # Run the async operation asyncio.run(run_async_operation()) ``` # Notes - Testing this function may require setting up an appropriate environment specific to the platform. - If developing on macOS or Unix, simulate the conditions mentioned for thorough testing. - Windows users should ensure subprocess compatibility as outlined. This question assesses your ability to handle cross-platform asynchronous programming and adapt to environmental constraints, a fundamental skill for developing robust and portable Python applications.","solution":"import asyncio import sys async def run_async_operation(): Runs an asynchronous operation that initiates a connection and performs an I/O task. It must adapt to the underlying platform and event loop to ensure compatibility with Unix-based systems and Windows. Specifically, handle scenarios where certain methods are unsupported on Windows and older macOS versions. This function does not return any value but should print the status of the I/O task. try: if sys.platform == \'win32\': # Use ProactorEventLoop on Windows try: loop = asyncio.ProactorEventLoop() asyncio.set_event_loop(loop) except AttributeError: print(\\"ProactorEventLoop is not supported on this platform.\\") return else: # Use default event loop on Unix-based systems if sys.platform == \'darwin\': # Handle older macOS versions explicitly from selectors import SelectSelector, PollSelector import platform if any(v in platform.mac_ver()[0] for v in (\'10.6\', \'10.7\', \'10.8\')): loop = asyncio.SelectorEventLoop(SelectSelector()) else: loop = asyncio.SelectorEventLoop(PollSelector()) asyncio.set_event_loop(loop) else: loop = asyncio.get_event_loop() print(\\"Initiating connection to dummy server...\\") async def io_task(): reader, writer = await asyncio.open_connection(\'example.com\', 80) message = \'GET / HTTP/1.0rnHost: example.comrnrn\' writer.write(message.encode()) await writer.drain() response = await reader.read(4096) if response: print(\\"Success\\") else: print(\\"Failure\\") writer.close() await writer.wait_closed() await io_task() except Exception as e: print(f\\"An error occurred: {e}\\") # Ensure compatibility with various loops if __name__ == \\"__main__\\": try: asyncio.run(run_async_operation()) except AttributeError: # Compatibility for older versions of Python loop = asyncio.get_event_loop() loop.run_until_complete(run_async_operation())"},{"question":"Utilizing the `pipes` Module to Manipulate Pipelines **Objective:** The objective of this question is to test your understanding of the `pipes` module in Python, specifically the `pipes.Template` class and its methods. **Problem Statement:** Design a function `process_file_with_pipeline(input_file: str, output_file: str) -> None` that performs the following steps using the `pipes.Template` class: 1. Create a new pipeline template. 2. Append a command to convert all lowercase characters in the file to uppercase (use `tr a-z A-Z` command). 3. Append a command to sort the lines in the file alphabetically (use `sort` command). 4. Open the input file, process it through the pipeline, and write the output to the output file. **Function Signature:** ```python def process_file_with_pipeline(input_file: str, output_file: str) -> None: ``` **Input:** - `input_file`: A string representing the path to the input text file. - `output_file`: A string representing the path to the output text file. **Output:** - The function does not return anything but writes the transformed and sorted content to the output file. **Example:** Given the content of `input_file` as: ``` banana Apple cherry date ``` After processing, the content of `output_file` should be: ``` APPLE BANANA CHERRY DATE ``` **Constraints:** - You may assume the input and output file paths are valid and accessible. - You should handle the file operations safely. **Notes:** - Use the `pipes.Template` class to create and manage the pipeline. - Ensure that the pipeline commands are appended correctly to perform the required transformations. # Solution Template: ```python import pipes def process_file_with_pipeline(input_file: str, output_file: str) -> None: # Create a new pipeline template pipeline = pipes.Template() # Append commands to the pipeline pipeline.append(\'tr a-z A-Z\', \'--\') pipeline.append(\'sort\', \'--\') # Open the input file and process it through the pipeline with pipeline.open(input_file, \'r\') as f_in, open(output_file, \'w\') as f_out: f_out.write(f_in.read()) ``` **Explanation:** 1. The `pipes.Template` instance is created. 2. The `tr` command is appended to convert all characters from lowercase to uppercase. 3. The `sort` command is appended to sort the lines alphabetically. 4. The input file is opened, processed through the pipeline, and the output is written to the output file.","solution":"import pipes def process_file_with_pipeline(input_file: str, output_file: str) -> None: # Create a new pipeline template pipeline = pipes.Template() # Append commands to the pipeline pipeline.append(\'tr a-z A-Z\', \'--\') pipeline.append(\'sort\', \'--\') # Open the input file and process it through the pipeline with open(input_file, \'r\') as in_file: with pipeline.open(output_file, \'w\') as out_file: # feed the input file content to the pipeline out_file.write(in_file.read())"},{"question":"Objective: Demonstrate your understanding of Seaborn by creating and customizing various histograms and bivariate plots. Problem Description: You are provided with three datasets: `penguins`, `planets`, and `tips`. These datasets are preloaded and accessible via the command `sns.load_dataset(dataset_name)`. Your task is to visualize these datasets according to the given specifications. 1. **Univariate Histogram with KDE**: - Load the `penguins` dataset. - Create a univariate histogram of `flipper_length_mm` and add a kernel density estimate (KDE) to the plot. - Customize the histogram to have a `binwidth` of 5. 2. **Grouped Histograms**: - Load the `penguins` dataset. - Create a histogram of `flipper_length_mm` grouped by `species` using the `hue` parameter. - Use the `element=\\"step\\"` parameter to draw a step plot for better comparison of overlapping distributions. - Ensure the histogram is normalized such that the sum of the heights is 1 for each species by setting the appropriate `common_norm` and `stat` parameters. 3. **Bivariate Histogram**: - Load the `planets` dataset. - Create a bivariate histogram of `year` (assigned to the x-axis) and `distance` (assigned to the y-axis). - Transform the y-axis to a logarithmic scale. - Add a color bar to indicate the density of counts. 4. **Categorical Histogram**: - Load the `tips` dataset. - Create a histogram of `day` (assigned to the x-axis) with a `shrink` factor of 0.8. - Group the histogram by `sex` using the `hue` parameter and use `multiple=\\"dodge\\"` to dodge the bars. Expected Input and Output Formats: - Input: No explicit input, use the datasets as described. - Output: Four histograms visualized based on the provided specifications. Constraints: - Use the Seaborn library for all visualizations. - Ensure plot customizations match the given requirements. - Use the provided datasets without any modifications. Performance Requirements: - The plots should be generated efficiently and should render without any runtime issues in a typical Python environment. Documentation: You may refer to the documentation provided for further details on Seaborn\'s functions and parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def univariate_histogram_with_kde(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Create a univariate histogram of flipper_length_mm with KDE sns.histplot(data=penguins, x=\'flipper_length_mm\', kde=True, binwidth=5) plt.title(\'Univariate Histogram of Flipper Length with KDE\') plt.show() def grouped_histograms(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Create a histogram of flipper_length_mm grouped by species sns.histplot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', element=\'step\', stat=\'density\', common_norm=False) plt.title(\'Histogram of Flipper Length Grouped by Species\') plt.show() def bivariate_histogram(): # Load the planets dataset planets = sns.load_dataset(\'planets\') # Create a bivariate histogram of year and distance with log scale on y-axis sns.histplot(data=planets, x=\'year\', y=\'distance\', log_scale=(False, True), cbar=True) plt.title(\'Bivariate Histogram of Year and Distance\') plt.show() def categorical_histogram(): # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create a histogram of day grouped by sex sns.histplot(data=tips, x=\'day\', hue=\'sex\', multiple=\'dodge\', shrink=0.8) plt.title(\'Histogram of Day Grouped by Sex\') plt.show()"},{"question":"# Decision Trees: Classification and Visualization **Objective**: Implement a decision tree classifier on the Iris dataset, visualize the tree, and export its structure in different formats. Description 1. Load the Iris dataset. 2. Train a `DecisionTreeClassifier` on the dataset. 3. Visualize the trained decision tree using `plot_tree`. 4. Export the tree structure in Graphviz format. 5. Export the tree structure as plain text. Requirements - Your code should: - Load the Iris dataset using `sklearn.datasets.load_iris`. - Split the dataset into features (`X`) and target (`y`). - Initialize a `DecisionTreeClassifier` model. - Train the model on the dataset. - Plot the tree using the `plot_tree` function from `sklearn.tree`. - Export the tree structure in Graphviz format using `export_graphviz`. - Export the tree structure as a plain text representation using `export_text`. Constraints - You are not allowed to use any package other than `numpy` and `scikit-learn`. Input and Output Formats Input: None (You will work with the Iris dataset from `sklearn.datasets`) Output: No direct output. The expected artifacts are: - A visual tree plot using `plot_tree`. - A Graphviz dot file representing the tree structure. - A plain text representation of the tree. Example Here is a template to get you started: ```python from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz, export_text import graphviz # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Initialize and train DecisionTreeClassifier clf = DecisionTreeClassifier() clf = clf.fit(X, y) # Plotting the tree plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True) # Exporting the tree in Graphviz format dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = graphviz.Source(dot_data) graph.render(\\"iris_tree\\") # Exporting the tree as plain text tree_text = export_text(clf, feature_names=iris[\'feature_names\']) print(tree_text) ``` Evaluation Criteria - Correct implementation of the decision tree classifier. - Accurate visualization of the decision tree. - Correct export of the tree structure in both Graphviz and plain text formats. - Clear and readable code with appropriate comments.","solution":"from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz, export_text import matplotlib.pyplot as plt # Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # Initialize and train DecisionTreeClassifier clf = DecisionTreeClassifier() clf = clf.fit(X, y) # Plotting the tree plt.figure(figsize=(12, 8)) plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True) plt.savefig(\\"iris_tree_plot.png\\", format=\\"png\\") # Saving the plot as a png file # Exporting the tree in Graphviz format dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) with open(\\"iris_tree.dot\\", \\"w\\") as f: f.write(dot_data) # Exporting the tree as plain text tree_text = export_text(clf, feature_names=iris.feature_names) with open(\\"iris_tree.txt\\", \\"w\\") as f: f.write(tree_text)"},{"question":"**Question: Dynamic Input Handler** You are required to implement a Python function `dynamic_input_handler` that can handle and evaluate Python code provided in three different forms: as a complete program, as a file input, or as an interactive input. # Function Signature ```python def dynamic_input_handler(input_type: str, input_value: str) -> None: pass ``` # Input - `input_type` (str): Specifies the type of input. It can be one of the following values: - `\\"complete_program\\"`: `input_value` contains a string representing a complete Python program. - `\\"file_input\\"`: `input_value` contains the path to a file containing Python code. - `\\"interactive_input\\"`: `input_value` contains a string representing an interactive Python command. - `input_value` (str): The actual input value as described by `input_type`. # Output The function should execute the provided input and print the result (if any). If there is any exception or error, catch it and print an error message instead, mentioning the type of the error. # Constraints - The Python code provided in `input_value` will not have malicious content. - The function should handle typical code execution time efficiently. # Examples Example 1 ```python input_type = \\"complete_program\\" input_value = \\"print(\'Hello, world!\')\\" dynamic_input_handler(input_type, input_value) ``` **Output** ``` Hello, world! ``` Example 2 ```python input_type = \\"file_input\\" input_value = \\"example.py\\" # example.py contains \'print(2 + 3)nx = 10\' dynamic_input_handler(input_type, input_value) ``` **Output** ``` 5 ``` Example 3 ```python input_type = \\"interactive_input\\" input_value = \\"a = 5nb = 7nprint(a + b)\\" dynamic_input_handler(input_type, input_value) ``` **Output** ``` 12 ``` # Implementation Notes - Use appropriate Python functions and libraries to handle file reading and code execution (`exec`, `eval`). - Ensure proper error handling to catch and report exceptions that may occur during execution. - Consider the security implications and ensure the function handles inputs safely within the given constraints.","solution":"def dynamic_input_handler(input_type: str, input_value: str) -> None: try: if input_type == \\"complete_program\\": exec(input_value) elif input_type == \\"file_input\\": with open(input_value, \\"r\\") as file: code = file.read() exec(code) elif input_type == \\"interactive_input\\": exec(input_value) else: print(f\\"Unknown input_type: {input_type}\\") except Exception as e: print(f\\"Error: {e.__class__.__name__} - {str(e)}\\")"},{"question":"Mocking and Patching with `unittest.mock` **Objective:** To assess the understanding of using the `unittest.mock` library to create and manage mocks, and to use advanced concepts such as patching, return values, side effects, and mock assertions. **Problem:** You are developing a simple application that interacts with an external service to fetch data and then processes this data. We will provide an example class that is responsible for interacting with the external service and another that processes the data. Your task is to write tests using `unittest.mock` to mock the external service interaction. **Application Code:** ```python import requests class ExternalService: def fetch_data(self, endpoint): response = requests.get(endpoint) if response.status_code == 200: return response.json() else: return None class DataProcessor: def process(self, data): # Assume some complex data processing here if data: return sum(data) return 0 class Application: def __init__(self, service: ExternalService, processor: DataProcessor): self.service = service self.processor = processor def run(self, endpoint): data = self.service.fetch_data(endpoint) result = self.processor.process(data) return result ``` **Tasks:** 1. **Create a test case `test_fetch_data_success`**: This test should check if the `fetch_data` method of the `ExternalService` class correctly fetches data when the external service returns a successful response (status code 200). 2. **Create a test case `test_fetch_data_failure`**: This test should check if the `fetch_data` method returns `None` when the external service returns a failure response (status code other than 200). 3. **Create a test case `test_application_run`**: This test should use mock objects for both `ExternalService` and `DataProcessor` classes to test the entire flow of the `Application` class. Mock the `fetch_data` method to return a predefined list of data and then assert that the `process` method is called correctly and the result is as expected. **Requirements:** - Use `unittest.mock.patch` to mock the `requests.get` method during the tests for `ExternalService`. - Mock the methods `fetch_data` and `process` for testing the `Application` class. - Verify that `fetch_data` and `process` methods are called with the correct arguments and the correct number of times using appropriate assert methods. **Constraints:** - The URL endpoint used in the tests can be a dummy one like `http://dummy.endpoint`. - Ensure that the tests are isolated and do not make real HTTP requests. **Example Test Stub:** Here\'s a start for your test cases: ```python import unittest from unittest.mock import Mock, patch from your_module import ExternalService, DataProcessor, Application class TestExternalService(unittest.TestCase): @patch(\'your_module.requests.get\') def test_fetch_data_success(self, mock_get): # Setup mock_response = Mock() mock_response.status_code = 200 mock_response.json.return_value = [1, 2, 3] mock_get.return_value = mock_response service = ExternalService() # Test result = service.fetch_data(\'http://dummy.endpoint\') # Assertion self.assertEqual(result, [1, 2, 3]) mock_get.assert_called_once_with(\'http://dummy.endpoint\') @patch(\'your_module.requests.get\') def test_fetch_data_failure(self, mock_get): # Setup mock_response = Mock() mock_response.status_code = 404 mock_get.return_value = mock_response service = ExternalService() # Test result = service.fetch_data(\'http://dummy.endpoint\') # Assertion self.assertIsNone(result) mock_get.assert_called_once_with(\'http://dummy.endpoint\') class TestApplication(unittest.TestCase): @patch(\'your_module.ExternalService.fetch_data\') @patch(\'your_module.DataProcessor.process\') def test_application_run(self, mock_process, mock_fetch_data): # Setup mock_fetch_data.return_value = [1, 2, 3] mock_process.return_value = 6 service = ExternalService() processor = DataProcessor() app = Application(service, processor) # Test result = app.run(\'http://dummy.endpoint\') # Assertions self.assertEqual(result, 6) mock_fetch_data.assert_called_once_with(\'http://dummy.endpoint\') mock_process.assert_called_once_with([1, 2, 3]) if __name__ == \'__main__\': unittest.main() ``` **Notes:** - Replace `\'your_module\'` with the correct module name where the `ExternalService`, `DataProcessor`, and `Application` classes are defined. - Ensure to follow best practices in mock usage and assertions.","solution":"import requests class ExternalService: def fetch_data(self, endpoint): response = requests.get(endpoint) if response.status_code == 200: return response.json() else: return None class DataProcessor: def process(self, data): # Assume some complex data processing here if data: return sum(data) return 0 class Application: def __init__(self, service: ExternalService, processor: DataProcessor): self.service = service self.processor = processor def run(self, endpoint): data = self.service.fetch_data(endpoint) result = self.processor.process(data) return result"},{"question":"Implement a custom serialization process using the `pickle` module. You are required to serialize and deserialize a complex object representing the state of a text file reader (`TextFileReader`) that can resume from where it left off. # Specifications: 1. **Class Definition**: Define a class `TextFileReader` with the following attributes: - `filename`: The name of the text file. - `file`: The file object in use. - `lineno`: The last read line number. - `current_line`: The content of the last read line. 2. **Methods**: - `__init__(self, filename)`: Initialize the reader with the filename and open the file. - `readline(self)`: Read the next line from the file and update `lineno` and `current_line`. 3. **Custom Serialization**: - Implement `__getstate__(self)` to return the object’s state for serialization. - Implement `__setstate__(self, state)` to restore the object’s state during deserialization. 4. **Pickling and Unpickling Process**: - Use `pickle` to serialize and deserialize an instance of `TextFileReader`. - Ensure that after deserialization, the `TextFileReader` can continue reading the next line from where it left off. # Input and Output: Input: 1. A file named `sample.txt` with multiple lines of content. 2. Script execution to demonstrate creating an instance, reading lines, pickling, and unpickling. Output: 1. Print statements to show the line read before and after unpickling. # Constraints: - Ensure the file `sample.txt` contains at least three lines for testing. - Handle exceptions gracefully if any file-related operations fail. # Example Usage: ```python # Initialization and reading lines reader = TextFileReader(\\"sample.txt\\") print(reader.readline()) # Read the first line print(reader.readline()) # Read the second line # Serialize the reader state with open(\\"reader_state.pkl\\", \\"wb\\") as f: pickle.dump(reader, f) # Deserialize the reader state with open(\\"reader_state.pkl\\", \\"rb\\") as f: new_reader = pickle.load(f) # Continue reading from where it left off print(new_reader.readline()) # Read the third line ``` # Implementation Notes: - Provide the class `TextFileReader` with the required attributes and methods. - Implement custom `__getstate__` and `__setstate__` methods for the `TextFileReader`. - Ensure that the serialized state includes all necessary information to restore the reader to its exact prior state.","solution":"import pickle class TextFileReader: def __init__(self, filename): self.filename = filename self.file = open(filename, \'r\') self.lineno = 0 self.current_line = None def readline(self): self.current_line = self.file.readline() if self.current_line: self.lineno += 1 return self.current_line def __getstate__(self): # Return the state of the object for serialization state = self.__dict__.copy() # Remove file object from the state, we\'ll handle it separately del state[\'file\'] return state def __setstate__(self, state): # Update the object\'s dictionary self.__dict__.update(state) # Reopen the file and seek to the correct position self.file = open(self.filename, \'r\') for _ in range(self.lineno): self.file.readline()"},{"question":"Coding Assessment Question # Objective Implement a function `compare_strings` that processes and compares two input strings. This function should utilize string manipulation, regular expressions, and difference computations to provide a comprehensive output describing the similarities and differences between the two strings. # Function Signature ```python def compare_strings(str1: str, str2: str) -> dict: pass ``` # Expected Input and Output 1. **Input**: - `str1`: A string containing the first text to be compared. - `str2`: A string containing the second text to be compared. 2. **Output**: - A dictionary with the following keys: - `\'common_words\'`: List of words common to both strings (case insensitive). - `\'unique_to_str1\'`: List of words unique to the first string (case insensitive). - `\'unique_to_str2\'`: List of words unique to the second string (case insensitive). - `\'differences\'`: A detailed description (diff-like output) of differences between the two strings. # Constraints - Words are sequences of alphanumeric characters and may include apostrophes. - Consider words to be case insensitive for comparison. - The function should use the modules and functionalities provided by Python\'s `string`, `re`, and `difflib` libraries. # Example ```python str1 = \\"The quick brown fox jumps over the lazy dog.\\" str2 = \\"A quick brown dog jumps over the lazy fox.\\" output = compare_strings(str1, str2) print(output) ``` # Example Output ```python { \'common_words\': [\'quick\', \'brown\', \'jumps\', \'over\', \'the\', \'lazy\'], \'unique_to_str1\': [\'thefox\'], \'unique_to_str2\': [\'a\', \'dog\'], \'differences\': [ \\"- The quick brown fox jumps over the lazy dog.\\", \\"+ A quick brown dog jumps over the lazy fox.\\" ] } ``` # Detailed Description Write the `compare_strings` function to achieve the following: 1. **Extract Words**: Use regular expressions to extract words from both strings. 2. **Normalize Case**: Convert all words to lower case for consistent comparison. 3. **Find Common Words**: Identify and list words that appear in both strings. 4. **Find Unique Words**: Identify and list words unique to each string (excluding common words). 5. **Compute Differences**: Provide a detailed diff of the two strings using the `difflib` module. # Hints - Utilize the `re.findall` method to extract words based on a suitable regular expression. - Use the `difflib.ndiff` method to produce the detailed differences between the strings.","solution":"import re import difflib def compare_strings(str1: str, str2: str) -> dict: # Extract words from both strings and convert to lowercase words1 = re.findall(r\\"bw+\'w+|bw+\\", str1.lower()) words2 = re.findall(r\\"bw+\'w+|bw+\\", str2.lower()) # Convert lists to sets for easier comparison set1 = set(words1) set2 = set(words2) # Find common words, unique words to each string common_words = list(set1 & set2) unique_to_str1 = list(set1 - set2) unique_to_str2 = list(set2 - set1) # Find differences using difflib differences = list(difflib.ndiff(str1.splitlines(), str2.splitlines())) return { \'common_words\': common_words, \'unique_to_str1\': unique_to_str1, \'unique_to_str2\': unique_to_str2, \'differences\': differences }"},{"question":"# Python Module Creation and Management using C API (Advanced) In this task, you are required to demonstrate your understanding of how Python modules are defined, created, and managed using the Python C API. Specifically, you will implement a Python extension module that supports both single-phase and multi-phase initialization. This module will expose certain functions and properties to Python code. Objectives: 1. **Module Definition**: Define a module with attributes, constants, and custom functions. 2. **Single-phase Initialization**: Implement the module creation function for single-phase initialization. 3. **Multi-phase Initialization**: Implement the module creation and execution functions for multi-phase initialization. 4. **Attribute and Function Management**: Add attributes, constants, and functions to the module. Requirements: 1. Define a module `mymodule` with the following structure: - Module name: `mymodule` - Module docstring: \\"This is my custom module.\\" - Module state size: 128 bytes - Functions provided by the module: - `spam`: A function that returns the string \\"Spam ham eggs!\\" - `add(x, y)`: A function that takes two integers and returns their sum. - Constants provided by the module: - `PI`: A constant representing the value `3.14159`. - `VERSION`: A constant string representing the module version, e.g., \\"1.0\\". 2. Create the module using single-phase initialization, ensuring it can be imported and used from Python code. 3. Create the module using multi-phase initialization, ensuring it supports sub-interpreters and multiple independent module instances. 4. Add the specified attributes, constants, and functions to the module\'s namespace. Constraints: - Use the Python C API functions as described in the provided documentation. - Ensure proper error handling and memory management in your C code. - The module should be compatible with Python 3.7+. Input and Output: - **Input**: You are not required to handle any specific input. Instead, you need to implement the module functions and initialization logic in a C file. - **Output**: The module should be importable from Python and should function as described above. Example usage: After implementing the module, the following Python code should work: ```python import mymodule print(mymodule.spam()) # Output: \\"Spam ham eggs!\\" print(mymodule.add(2, 3)) # Output: 5 print(mymodule.PI) # Output: 3.14159 print(mymodule.VERSION) # Output: \\"1.0\\" ``` Implementation Skeleton: Provide the implementation skeleton that includes: ```c #include <Python.h> // Function declarations static PyObject* mymodule_spam(PyObject* self, PyObject* args); static PyObject* mymodule_add(PyObject* self, PyObject* args); // Define functions in the module static PyMethodDef MyModuleMethods[] = { {\\"spam\\", mymodule_spam, METH_NOARGS, \\"Return a spam string.\\"}, {\\"add\\", mymodule_add, METH_VARARGS, \\"Add two numbers.\\"}, {NULL, NULL, 0, NULL} }; // Define the module static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", // Module name \\"This is my custom module.\\", // Module docstring 128, // Module state size MyModuleMethods, // Module methods NULL, // Slots for multi-phase initialization NULL, // Traverse NULL, // Clear NULL // Free }; // Function implementations static PyObject* mymodule_spam(PyObject* self, PyObject* args) { return PyUnicode_FromString(\\"Spam ham eggs!\\"); } static PyObject* mymodule_add(PyObject* self, PyObject* args) { int x, y; if (!PyArg_ParseTuple(args, \\"ii\\", &x &y)) { return NULL; } return PyLong_FromLong(x + y); } // Single-phase initialization function PyMODINIT_FUNC PyInit_mymodule(void) { PyObject *module = PyModule_Create(&mymodule); if (!module) { return NULL; } if (PyModule_AddIntConstant(module, \\"PI\\", 3.14159) < 0 || PyModule_AddStringConstant(module, \\"VERSION\\", \\"1.0\\") < 0) { Py_XDECREF(module); return NULL; } return module; } // Multi-phase initialization functions static PyObject* mymodule_create(PyObject* spec, PyModuleDef* def) { // Create the module object PyObject *module = PyModule_New(def->m_name); if (!module) { return NULL; } return module; } static int mymodule_exec(PyObject* module) { // Add constants and attributes if (PyModule_AddIntConstant(module, \\"PI\\", 3.14159) < 0 || PyModule_AddStringConstant(module, \\"VERSION\\", \\"1.0\\") < 0) { return -1; } return 0; } PyMODINIT_FUNC PyInit_mymodule(void) { static struct PyModuleDef_Slot module_slots[] = { {Py_mod_create, mymodule_create}, {Py_mod_exec, mymodule_exec}, {0, NULL} }; static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"mymodule\\", \\"This is my custom module.\\", 128, MyModuleMethods, module_slots, NULL, NULL, NULL }; return PyModuleDef_Init(&mymodule); } ``` Complete the provided skeleton and ensure it compiles and works as expected. Note: You may use tools like `gcc` or `clang` to compile the C code and test the module with an actual Python interpreter.","solution":"def spam(): Returns the string \\"Spam ham eggs!\\". return \\"Spam ham eggs!\\" def add(x, y): Returns the sum of x and y. return x + y # Constants PI = 3.14159 VERSION = \\"1.0\\""},{"question":"# Python `pkgutil` Module Coding Assessment Background The `pkgutil` module in Python provides a suite of utilities for extending and managing package import paths, iterating over modules, and retrieving module data. It can be particularly useful when dealing with complex package structures or dynamically discovering submodules and resources. Problem Statement **Task**: Write a function `find_module_data_and_resolve` that performs the following operations: 1. Given a package name, use `pkgutil.walk_packages` to find all submodules of the package. 2. For each found submodule, use `pkgutil.get_data` to retrieve the content of a specified resource file. 3. Use `pkgutil.resolve_name` to dynamically resolve and return the object specified by the resultant module names. Your function should adhere to the following signature: ```python import pkgutil def find_module_data_and_resolve(package_name: str, resource_name: str) -> dict: Finds submodules of a given package, retrieves data from a specified resource file, and resolves the object represented by the submodule names. :param package_name: Name of the package to search for submodules. :param resource_name: The resource file to retrieve from each submodule. :return: A dictionary with submodule names as keys and corresponding resolved objects as values. pass ``` Input - `package_name`: A string representing the package to search within. - `resource_name`: A string representing the resource file name to retrieve from each submodule. Output - A dictionary where the keys are submodule names and the values are the corresponding objects resolved from the submodule names. Constraints 1. Ignore submodules that do not contain the specified resource file. 2. Handle exceptions and continue processing remaining submodules if an error occurs while retrieving data or resolving names. 3. You can assume that the resource file, if present, contains text/binary data readable using Python\'s standard file operations. 4. Document and handle any assumptions you make about the structure of the package or the format of the resource files. Examples ```python # Given a package `mypackage` with submodules `sub1`, `sub2`, each containing the resource `data.txt`, # and assuming `mypackage.sub1` resolves to an object `SubModule1` and `mypackage.sub2` resolves to an object `SubModule2` result = find_module_data_and_resolve(\'mypackage\', \'data.txt\') # The result might be: # { # \'mypackage.sub1\': SubModule1, # \'mypackage.sub2\': SubModule2 # } ``` Notes - This exercise tests your understanding of package structures, iterating over modules, resource handling, and dynamic imports. - Focus on robust exception handling and efficient searching. - Ensure your code is well-documented and adheres to best coding practices.","solution":"import pkgutil def find_module_data_and_resolve(package_name: str, resource_name: str) -> dict: Finds submodules of a given package, retrieves data from a specified resource file, and resolves the object represented by the submodule names. :param package_name: Name of the package to search for submodules. :param resource_name: The resource file to retrieve from each submodule. :return: A dictionary with submodule names as keys and corresponding resolved objects as values. results = {} # Use `walk_packages` to iterate over all submodules in the given package for _, module_name, _ in pkgutil.walk_packages([package_name.replace(\'.\', \'/\')]): full_module_name = f\\"{package_name}.{module_name}\\" try: # Retrieve the data from the specified resource file data = pkgutil.get_data(package_name, f\\"{module_name}/{resource_name}\\") if data: # Only consider modules with the specified resource file resolved_object = pkgutil.resolve_name(full_module_name) results[full_module_name] = resolved_object except Exception as e: # Handle exceptions and continue processing remaining submodules continue return results"},{"question":"# PyTorch Hub Integration and Model Loading The goal of this task is to assess your understanding of how to integrate models using PyTorch Hub and how to utilize the Hub functionalities to load and explore these models. Task: 1. **Create a `hubconf.py` for a custom model**: - Define a simple neural network model (e.g., a two-layer fully connected neural network) with an entry point function. - Ensure the `hubconf.py` follows the structure shown in the documentation. 2. **Loading and Running Models with PyTorch Hub**: - Write a script that uses PyTorch Hub\'s API to list available models from your custom repository. - Load your model using `torch.hub.load()`. - Demonstrate how to inspect the model\'s methods using `dir(model)` and `help(model.method)`. Requirements: - Your custom model should be implemented using `torch.nn.Module`. - The `hubconf.py` file should correctly allow users to load the pretrained weights (if applicable). - Provide clear docstrings and help messages for the entry point function. - Demonstrate the loading and usage of the model in a separate script. Example Structure: 1. **hubconf.py**: ```python import torch.nn as nn import torch.hub class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def custom_model(pretrained=False, **kwargs): Custom Model pretrained (bool): load pretrained weights if available model = CustomModel() if pretrained: checkpoint = torch.hub.load_state_dict_from_url(\'http://example.com/custom_model.pth\') model.load_state_dict(checkpoint) return model ``` 2. **Usage Script**: ```python import torch.hub # List available models print(torch.hub.list(\'path_to_your_repo\')) # Load the custom model model = torch.hub.load(\'path_to_your_repo\', \'custom_model\', pretrained=False) # Inspect the model\'s methods print(dir(model)) help(model.forward) ``` Input: - None Output: - List of available models in the repository. - Description of the model\'s methods. Constraints: - Ensure the repository path used in `torch.hub.load()` and `torch.hub.list()` is correct and points to the location where your `hubconf.py` is stored.","solution":"import torch import torch.nn as nn import torch.hub class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def custom_model(pretrained=False, **kwargs): Custom Model pretrained (bool): load pretrained weights if available model = CustomModel() if pretrained: checkpoint = torch.hub.load_state_dict_from_url(\'http://example.com/custom_model.pth\') model.load_state_dict(checkpoint) return model"},{"question":"# Question: You are tasked with creating a series of customized color palettes using the `seaborn` library\'s `hls_palette` function. Implement a function named `create_custom_palettes` that takes the following parameters and returns a dictionary of color palettes based on specified customizations: - `num_palettes` (`int`): Number of different color palettes to create. - `colors_per_palette` (`List[int]`): A list containing the number of colors each palette should have. - `lightness` (`float`): A float value between 0 and 1 to specify the lightness of the colors in the palettes. - `saturation` (`float`): A float value between 0 and 1 to specify the saturation of the colors in the palettes. - `start_hue` (`float`): A float value between 0 and 1 to specify the start-point for hue sampling for the palettes. The function should return a dictionary where keys are palette names like \\"Palette_1\\", \\"Palette_2\\", ..., \\"Palette_n\\", and values are the generated color palettes. **Constraints**: - `num_palettes` will be a positive integer less than or equal to 10. - `colors_per_palette` will be a list of positive integers with the same length as `num_palettes`, and each value will be between 1 and 20, inclusive. - `lightness`, `saturation`, and `start_hue` will be float values between 0 and 1 (inclusive). **Example Usage**: ```python import seaborn as sns def create_custom_palettes(num_palettes, colors_per_palette, lightness, saturation, start_hue): # Implementation here # Example case palettes = create_custom_palettes( num_palettes=3, colors_per_palette=[6, 8, 4], lightness=0.7, saturation=0.9, start_hue=0.5 ) print(palettes) ``` **Expected Output**: ```python { \\"Palette_1\\": [(colors...)], \\"Palette_2\\": [(colors...)], \\"Palette_3\\": [(colors...)] } ``` Make sure your implementation uses the seaborn library and the `hls_palette` function to create the custom palettes.","solution":"import seaborn as sns def create_custom_palettes(num_palettes, colors_per_palette, lightness, saturation, start_hue): Generate a dictionary of custom color palettes using seaborn\'s hls_palette. Parameters: - num_palettes (int): Number of different color palettes to create. - colors_per_palette (List[int]): List containing the number of colors each palette should have. - lightness (float): Lightness of the colors in the palettes (0 to 1). - saturation (float): Saturation of the colors in the palettes (0 to 1). - start_hue (float): Start-point for hue sampling for the palettes (0 to 1). Returns: - dict: Dictionary with palette names as keys and color palettes as values. palettes = {} for i in range(num_palettes): palette_name = f\\"Palette_{i + 1}\\" num_colors = colors_per_palette[i] palette = sns.hls_palette(n_colors=num_colors, h=start_hue, l=lightness, s=saturation).as_hex() palettes[palette_name] = palette return palettes"},{"question":"**Question: Implementing a Secure Data Processing Pipeline** You are required to implement a secure data processing pipeline in Python that adheres to the security considerations highlighted in the documentation. The pipeline will consist of the following steps: 1. **Input Data Handling**: Accept input data in plain text format. The data will consist of multiple lines, each line containing a string that can have alphanumeric characters and special symbols. 2. **Secured Hashing**: For each line of input data, compute a secure hash using the `hashlib` module. Ensure that insecure algorithms are explicitly disabled. Use a random salt for hashing to enhance security. 3. **Data Encryption**: Encrypt the hashed data using base64 encoding to ensure that it is safely stored or transmitted. 4. **Output**: Return the original input along with its hashed and encrypted representation. **Function Signature:** ```python def secure_data_pipeline(input_data: str) -> List[Tuple[str, str]]: Processes the input data securely by hashing and then encrypting it. Args: - input_data: str : A multiline string where each line has a string to be processed. Returns: - List[Tuple[str, str]]: A list of tuples where each tuple contains the original string and its corresponding hashed and encrypted representation. pass ``` **Constraints:** - Use the `hashlib` module with a secure hashing algorithm, and ensure insecure algorithms are disabled. - Use a secure random salt. - Base64 encode the hashed data. - Ensure your implementation does not expose any security vulnerabilities as described in the documentation. **Example:** ```python input_data = password123 hello_world secure_data output = secure_data_pipeline(input_data) print(output) # Expected output (changes between runs due to random salt): # [ # (\\"password123\\", \\"c2VjdXJlX2hhc2hfdmFsdWUx\\"), # (\\"hello_world\\", \\"c2VjdXJlX2hhc2hfdmFsdWUy\\"), # (\\"secure_data\\", \\"c2VjdXJlX2hhc2hfdmFsdWUz\\") # ] ``` **Evaluation Criteria:** - Correctness: The function should correctly process each line of input data. - Security: The solution must adhere to the security best practices outlined, using the right modules and ensuring no insecure algorithms are used. - Performance: The solution should efficiently handle the input data and produce the expected output.","solution":"from typing import List, Tuple import hashlib import base64 import os def secure_data_pipeline(input_data: str) -> List[Tuple[str, str]]: Processes the input data securely by hashing and then encrypting it. Args: - input_data: str : A multiline string where each line has a string to be processed. Returns: - List[Tuple[str, str]]: A list of tuples where each tuple contains the original string and its corresponding hashed and encrypted representation. result = [] for line in input_data.split(\\"n\\"): if line.strip(): salt = os.urandom(16) # Secure random salt # Using hashlib\'s PBKDF2_HMAC which is considered secure dk = hashlib.pbkdf2_hmac(\'sha256\', line.encode(), salt, 100000) hashed = salt + dk encrypted = base64.b64encode(hashed).decode() result.append((line, encrypted)) return result"},{"question":"# Advanced Python310 Coding Assessment: Working with Cell Objects Objective This exercise is designed to test your understanding of the `PyCellObject` and its various operations. You will be required to implement a series of functions that manipulate and utilize cell objects as described. Instructions Implement the following functions using the `PyCellObject` and its associated methods as described in the documentation provided: 1. **create_cell(value)** - **Input**: A single Python object `value`. - **Output**: A new cell object containing the given `value`. - **Constraints**: The input value can be of any type. 2. **get_cell_value(cell)** - **Input**: A cell object `cell`. - **Output**: The value contained in the cell. - **Constraints**: The input must be a valid cell object created using `create_cell`. 3. **set_cell_value(cell, value)** - **Input**: A cell object `cell` and a new value `value`. - **Output**: The function should update the contents of the cell with the new value. - **Constraints**: The cell must be a valid cell object, and the value can be of any type. 4. **exchange_cell_values(cell1, cell2)** - **Input**: Two cell objects `cell1` and `cell2`. - **Output**: The function should swap the values contained in the two cells. - **Constraints**: Both inputs must be valid cell objects. Performance Requirements - The functions should perform efficiently with minimal overhead. - The operations should leverage the appropriate `PyCellObject` methods to ensure correctness and safety. Example Usage ```python # Suppose the following helper functions are available: # PyCell_New, PyCell_Get, PyCell_SET def create_cell(value): return PyCell_New(value) def get_cell_value(cell): return PyCell_Get(cell) def set_cell_value(cell, value): PyCell_SET(cell, value) def exchange_cell_values(cell1, cell2): temp_value = PyCell_Get(cell1) set_cell_value(cell1, PyCell_Get(cell2)) set_cell_value(cell2, temp_value) # Test cases cell_a = create_cell(10) cell_b = create_cell(20) print(get_cell_value(cell_a)) # Output: 10 print(get_cell_value(cell_b)) # Output: 20 set_cell_value(cell_a, 30) print(get_cell_value(cell_a)) # Output: 30 exchange_cell_values(cell_a, cell_b) print(get_cell_value(cell_a)) # Output: 20 print(get_cell_value(cell_b)) # Output: 30 ``` Notes - Ensure your implementation correctly uses the `PyCellObject` functions as described in the provided documentation. - Consider edge cases and handle any potential errors or invalid inputs gracefully.","solution":"def create_cell(value): Creates a new cell object containing the given value. class Cell: Represents a simple cell to hold a single value. def __init__(self, value): self.value = value return Cell(value) def get_cell_value(cell): Returns the value contained in the cell. return cell.value def set_cell_value(cell, value): Updates the cell with the new value. cell.value = value def exchange_cell_values(cell1, cell2): Swaps the values contained in the two cells. temp = cell1.value set_cell_value(cell1, cell2.value) set_cell_value(cell2, temp)"},{"question":"You are given an XML document representing information about several books. Each book has a title, author, year of publication, and price. You need to write a function to perform the following tasks: 1. Parse the provided XML data. 2. Modify the XML data: - Add a new book to the XML document. The new book\'s details (title, author, year, and price) should be passed as an argument to the function. - Update the price of a book given its title. - Remove books published before a given year. 3. Serialize the modified XML data back to a string. Function Signature ```python def modify_books_xml(xml_data: str, new_book: dict, update_title: str, new_price: float, remove_before_year: int) -> str: ``` # Input - `xml_data` (str): A string containing the initial XML data. - `new_book` (dict): A dictionary containing the details of the new book with keys \'title\', \'author\', \'year\', and \'price\'. - `update_title` (str): The title of the book whose price needs to be updated. - `new_price` (float): The updated price of the book with title `update_title`. - `remove_before_year` (int): The year threshold. Books published before this year should be removed. # Output - Returns a string containing the modified XML data. # Constraints - The `xml_data` is valid XML and adheres to the following structure: ```xml <library> <book> <title>Book Title</title> <author>Author Name</author> <year>Year</year> <price>Price</price> </book> <!-- More book elements --> </library> ``` - The `new_book` dictionary will always contain valid data with the correct keys. - `update_title` will match the title of one and only one book in the given XML data. # Example Input: ```python xml_data = \'\'\' <library> <book> <title>1984</title> <author>George Orwell</author> <year>1949</year> <price>9.99</price> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <price>7.99</price> </book> </library> \'\'\' new_book = { \'title\': \'Brave New World\', \'author\': \'Aldous Huxley\', \'year\': 1932, \'price\': 8.99 } update_title = \'1984\' new_price = 10.99 remove_before_year = 1940 ``` Output: ```xml <library> <book> <title>1984</title> <author>George Orwell</author> <year>1949</year> <price>10.99</price> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <price>7.99</price> </book> <book> <title>Brave New World</title> <author>Aldous Huxley</author> <year>1932</year> <price>8.99</price> </book> </library> ``` # Notes: 1. Use the `xml.etree.ElementTree` module to parse and modify the XML data. 2. Ensure the modified XML data maintains proper structure and hierarchy.","solution":"import xml.etree.ElementTree as ET def modify_books_xml(xml_data: str, new_book: dict, update_title: str, new_price: float, remove_before_year: int) -> str: # Parse the provided XML data root = ET.fromstring(xml_data) # Add a new book to the XML document new_book_element = ET.Element(\\"book\\") title_element = ET.SubElement(new_book_element, \\"title\\") title_element.text = new_book[\'title\'] author_element = ET.SubElement(new_book_element, \\"author\\") author_element.text = new_book[\'author\'] year_element = ET.SubElement(new_book_element, \\"year\\") year_element.text = str(new_book[\'year\']) price_element = ET.SubElement(new_book_element, \\"price\\") price_element.text = str(new_book[\'price\']) root.append(new_book_element) # Update the price of a book given its title for book in root.findall(\\"book\\"): title = book.find(\\"title\\").text if title == update_title: book.find(\\"price\\").text = str(new_price) # Remove books published before a given year for book in root.findall(\\"book\\"): year = int(book.find(\\"year\\").text) if year < remove_before_year: root.remove(book) # Serialize the modified XML data back to a string return ET.tostring(root, encoding=\\"unicode\\")"},{"question":"# Custom Priority Queue Implementation You work for a task management software development company. Your team is enhancing its core library to manage tasks with priorities using the heapq module. The requirements are as follows: 1. **Add a new task** or **update the priority of an existing task**. 2. **Remove a task** from the priority queue. 3. **Pop the task** with the highest priority (the smallest numerical value). The priority queue must be implemented using the heapq module to maintain the min-heap property. For tie-breaking tasks with the same priority, you should ensure that older tasks are returned before newer ones (FIFO order for tasks with the same priority). # Part 1: Task Add/Update Implement a function `add_task(pq, task, priority, entry_finder, counter)` that adds a new task to the priority queue or updates the priority of an existing task. If the task already exists in the queue, it should be removed and re-added with the new priority. # Part 2: Task Removal Implement a function `remove_task(pq, task, entry_finder)` that marks a task as removed from the priority queue. Use a placeholder for removed tasks, since the heap structure would be broken if an item were removed outright. # Part 3: Task Pop Implement a function `pop_task(pq, entry_finder)` that removes and returns the task with the highest priority (lowest numerical value). In case of ties, it should return the task that was added earlier. # Constraints - You can assume all tasks are unique strings. - Your implementation should handle the heap invariant efficiently. - Minimize the time complexity for each operation. # Input and Output Formats - `add_task(pq, task, priority, entry_finder, counter)` will not produce output but will update the priority queue. - `remove_task(pq, task, entry_finder)` will not produce output but will update the priority queue. - `pop_task(pq, entry_finder)` will return the task with the highest priority. # Example ```python pq = [] # Priority queue implemented as a heap entry_finder = {} # Mapping of tasks to entries REMOVED = \'<removed-task>\' # Placeholder for a removed task counter = itertools.count() # Unique sequence count add_task(pq, \'task1\', 1, entry_finder, counter) add_task(pq, \'task2\', 2, entry_finder, counter) add_task(pq, \'task3\', 1, entry_finder, counter) print(pop_task(pq, entry_finder)) # Outputs \'task1\' remove_task(pq, \'task3\', entry_finder) print(pop_task(pq, entry_finder)) # Outputs \'task2\' ``` # Your Task Implement the functions `add_task`, `remove_task`, and `pop_task` to meet the requirements above. Good luck!","solution":"import heapq import itertools REMOVED = \'<removed-task>\' def add_task(pq, task, priority, entry_finder, counter): Add a new task or update the priority of an existing task. if task in entry_finder: remove_task(pq, task, entry_finder) count = next(counter) entry = [priority, count, task] entry_finder[task] = entry heapq.heappush(pq, entry) def remove_task(pq, task, entry_finder): Mark an existing task as REMOVED. Raise KeyError if not found. entry = entry_finder.pop(task) entry[-1] = REMOVED def pop_task(pq, entry_finder): Remove and return the highest priority task. If there are ties, return the task that was added earliest. while pq: priority, count, task = heapq.heappop(pq) if task is not REMOVED: del entry_finder[task] return task raise KeyError(\'Pop from an empty priority queue\')"},{"question":"Objective Implement a function that efficiently maintains a sorted collection of records using the `bisect` module. You will be working with a list of student records where each record contains the student\'s name and their score. Your implementation should support efficient insertion of new records and retrieving the top `n` students with the highest scores. Specifications 1. Implement a Python class `StudentRecords` with the following methods: - `__init__(self)`: Initializes an empty list to store student records. - `add_student(self, name: str, score: int)`: Adds a new student record to the list, maintaining sorted order by score (highest score first). If two students have the same score, they should be ordered alphabetically by name (ascending). - `get_top_students(self, n: int) -> list`: Returns the top `n` students based on their scores. 2. Define the function signature as follows: ```python class StudentRecords: def __init__(self): pass def add_student(self, name: str, score: int): pass def get_top_students(self, n: int) -> list: pass ``` Input - Adding records: - The `name` parameter is a string representing the student\'s name. - The `score` parameter is an integer representing the student\'s score. - Retrieving records: - The `n` parameter is an integer representing the number of top students to retrieve. Output - The `get_top_students` method returns a list of tuples, each representing a student\'s record (`name`, `score`). Constraints - Scores are non-negative integers. - Student names are non-empty strings. - Assume the number of students added will not exceed 10^5. Example Usage ```python records = StudentRecords() records.add_student(\\"Alice\\", 85) records.add_student(\\"Bob\\", 95) records.add_student(\\"Charlie\\", 85) top_students = records.get_top_students(2) print(top_students) # Output: [(\'Bob\', 95), (\'Alice\', 85)] ``` Performance Requirements - Ensure insertion maintains O(n) complexity due to the need to move elements but leverages O(log n) complexity for search operations. - Ensure retrieval of top students should be O(n) or better to handle up to the constraint limit effectively. Use the documentation of the `bisect` module to efficiently implement these functionalities.","solution":"import bisect class StudentRecords: def __init__(self): self.records = [] def add_student(self, name: str, score: int): # Find the insertion point insertion_point = bisect.bisect_left(self.records, (-score, name)) # Insert the record keeping the list sorted self.records.insert(insertion_point, (-score, name)) def get_top_students(self, n: int) -> list: # Retrieve the top n students return [(name, -score) for score, name in self.records[:n]]"},{"question":"Partial Least Squares (PLS) Canonical Algorithm Implementation In this assignment, you are required to implement a simplified version of the PLSCanonical algorithm based on the provided scikit-learn documentation. Partial Least Squares (PLS) Canonical aims to find directions in `X` that explain the maximal variance in `Y`. Task 1. **Implement the `PLSCanonical` Class:** - Function: `fit(self, X, Y, n_components)` - Input: - `X`: A numpy array of shape `(n_samples, n_features)`. - `Y`: A numpy array of shape `(n_samples, n_targets)`. - `n_components`: An integer specifying the number of PLS components to use. - Output: None 2. **Method Details:** - Compute the cross-covariance matrix `C` of the centered matrices `X` and `Y`. - Find the first left singular vector `u_k` and right singular vector `v_k` of `C`. - Project `X` and `Y` onto `u_k` and `v_k` to obtain scores `ξ_k` and `ω_k`. - Compute the loadings `γ_k` and `δ_k`. - Deflate `X` and `Y` by subtracting the rank-1 approximations `ξ_k`γ_k^T` and `ω_k`δ_k^T`. - Repeat this process for each component up to `n_components`. 3. **Constraints:** - You should use `numpy` for matrix operations. - Ensure that the input matrices `X` and `Y` are centered before computing the cross-covariance matrix. 4. **Expected Methods:** - `fit`: Fits the model to the input data `X` and `Y`. - `transform`: Transforms new data `X` and `Y` using the learned projections. - `predict`: Predicts the target `Y` given new data `X`. Example Usage ```python import numpy as np # Example centered data X = np.array([[0.1, 0.3], [0.4, 0.5], [0.9, 0.4]]) Y = np.array([[0.7, 0.8], [0.9, 0.1], [0.3, 0.5]]) # Number of components n_components = 2 # PLSCanonical Implementation pls = PLSCanonical() pls.fit(X, Y, n_components) # Transform new data X_new = np.array([[0.2, 0.4], [0.5, 0.6]]) X_transformed, Y_transformed = pls.transform(X_new, Y) # Predict new targets Y_pred = pls.predict(X_new) print(\\"Transformed X:\\", X_transformed) print(\\"Predicted Y:\\", Y_pred) ``` Your Implementation: ```python import numpy as np class PLSCanonical: def __init__(self): self.x_weights_ = None self.y_weights_ = None self.x_loadings_ = None self.y_loadings_ = None self.x_scores_ = None self.y_scores_ = None def fit(self, X, Y, n_components): # Implementation of the fit method pass def transform(self, X, Y): # Implementation of the transform method pass def predict(self, X): # Implementation of the predict method pass ``` Complete the implementation of the `PLSCanonical` class following the described steps to fit the model, transform the data, and predict targets.","solution":"import numpy as np from numpy.linalg import svd class PLSCanonical: def __init__(self): self.x_weights_ = None self.y_weights_ = None self.x_loadings_ = None self.y_loadings_ = None self.x_scores_ = None self.y_scores_ = None def fit(self, X, Y, n_components): X = X - X.mean(axis=0) Y = Y - Y.mean(axis=0) self.x_weights_ = np.zeros((X.shape[1], n_components)) self.y_weights_ = np.zeros((Y.shape[1], n_components)) self.x_loadings_ = np.zeros((X.shape[1], n_components)) self.y_loadings_ = np.zeros((Y.shape[1], n_components)) self.x_scores_ = np.zeros((X.shape[0], n_components)) self.y_scores_ = np.zeros((Y.shape[0], n_components)) for k in range(n_components): C = np.dot(X.T, Y) U, S, Vt = svd(C, full_matrices=False) u_k = U[:, 0] v_k = Vt.T[:, 0] self.x_weights_[:, k] = u_k self.y_weights_[:, k] = v_k x_score_k = np.dot(X, u_k) y_score_k = np.dot(Y, v_k) self.x_scores_[:, k] = x_score_k self.y_scores_[:, k] = y_score_k x_loading_k = np.dot(X.T, x_score_k) / np.dot(x_score_k.T, x_score_k) y_loading_k = np.dot(Y.T, y_score_k) / np.dot(y_score_k.T, y_score_k) self.x_loadings_[:, k] = x_loading_k self.y_loadings_[:, k] = y_loading_k X -= np.outer(x_score_k, x_loading_k) Y -= np.outer(y_score_k, y_loading_k) def transform(self, X, Y): X = X - X.mean(axis=0) Y = Y - Y.mean(axis=0) X_transformed = np.dot(X, self.x_weights_) Y_transformed = np.dot(Y, self.y_weights_) return X_transformed, Y_transformed def predict(self, X): X = X - X.mean(axis=0) X_pred_scores = np.dot(X, self.x_weights_) Y_pred = np.dot(X_pred_scores, self.y_loadings_.T) return Y_pred"},{"question":"# Question: Package Metadata Analyzer using `importlib.metadata` You are tasked with implementing a set of functions to analyze the metadata of a given installed package. Using the `importlib.metadata` module, implement the following functionalities: 1. **Retrieve Package Version:** Implement a function `get_package_version(package_name: str) -> str` that takes the name of the package and returns its version as a string. If the package is not found, return \\"Package not found\\". 2. **List All Entry Points:** Implement a function `list_entry_points(package_name: str) -> dict` that takes the name of the package and returns a dictionary where keys are entry point groups and values are lists of entry point names belonging to those groups. If there are no entry points or the package is not found, return an empty dictionary. 3. **Get Package Metadata:** Implement a function `get_package_metadata(package_name: str) -> dict` that takes the name of the package and returns a dictionary containing all the metadata key-value pairs for the package. If the package is not found, return an empty dictionary. 4. **List Package Files:** Implement a function `list_package_files(package_name: str) -> list` that takes the name of the package and returns a list of all files installed by the package. Each element in the list should be the string representation of the file path. If the package is not found or there are no files, return an empty list. 5. **Get Package Requirements:** Implement a function `get_package_requirements(package_name: str) -> list` that takes the name of the package and returns a list of requirement strings, which are the dependencies needed by the package. If there are no requirements or the package is not found, return an empty list. Ensure to handle exceptions and errors appropriately, and validate the inputs where necessary. # Example Usage ```python print(get_package_version(\\"wheel\\")) # Output: \'0.32.3\' print(list_entry_points(\\"wheel\\")) # Output: {\'console_scripts\': [\'wheel\']} print(get_package_metadata(\\"wheel\\")) # Output: {\'Name\': \'wheel\', \'Version\': \'0.32.3\', ...} print(list_package_files(\\"wheel\\")) # Output: [\'wheel/util.py\', ...] print(get_package_requirements(\\"wheel\\")) # Output: [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] ``` # Constraints 1. You are required to use the `importlib.metadata` module only. 2. Assume all packages queried are installed in the environment where the code is executed. 3. Handle the cases where a package is not found gracefully. 4. Consider performance implications if the package has a large number of files or metadata entries. # Performance Requirements - The functions should be efficient and should not perform redundant metadata fetching. - Try to limit the number of times the metadata is retrieved for a single function.","solution":"import importlib.metadata from typing import List, Dict def get_package_version(package_name: str) -> str: Returns the version of the specified package. If the package is not found, returns \'Package not found\'. try: return importlib.metadata.version(package_name) except importlib.metadata.PackageNotFoundError: return \\"Package not found\\" def list_entry_points(package_name: str) -> Dict[str, List[str]]: Returns a dictionary where keys are entry point groups and values are lists of entry point names. If the package is not found or has no entry points, returns an empty dictionary. try: distribution = importlib.metadata.distribution(package_name) entry_points = distribution.entry_points entry_points_dict = {} for ep in entry_points: if ep.group not in entry_points_dict: entry_points_dict[ep.group] = [] entry_points_dict[ep.group].append(ep.name) return entry_points_dict except importlib.metadata.PackageNotFoundError: return {} def get_package_metadata(package_name: str) -> Dict[str, str]: Returns a dictionary containing all the metadata key-value pairs for the package. If the package is not found, returns an empty dictionary. try: metadata = importlib.metadata.metadata(package_name) return dict(metadata) except importlib.metadata.PackageNotFoundError: return {} def list_package_files(package_name: str) -> List[str]: Returns a list of all files installed by the package. Each element in the list is the string representation of the file path. If the package is not found or there are no files, returns an empty list. try: distribution = importlib.metadata.distribution(package_name) return [str(file) for file in distribution.files] except importlib.metadata.PackageNotFoundError: return [] def get_package_requirements(package_name: str) -> List[str]: Returns a list of requirement strings which are the dependencies needed by the package. If the package is not found or there are no requirements, returns an empty list. try: distribution = importlib.metadata.distribution(package_name) return [str(req) for req in distribution.requires or []] except importlib.metadata.PackageNotFoundError: return []"},{"question":"# Advanced Dictionary Operations In this task, you are required to implement a class `AdvancedDict` that mimics certain low-level dictionary operations provided by the C API for Python dictionaries. This class should wrap a standard Python dictionary but provide additional methods to perform various functions that you will implement. Class: `AdvancedDict` **Methods:** 1. **`__init__(self) -> None`:** - Initializes an empty dictionary. 2. **`py_dict_new(self) -> dict:`** - Creates and returns a new empty dictionary. 3. **`py_dict_set_item(self, key, value) -> None:`** - Sets `value` in the dictionary for the `key`. If `key` is not hashable, raises a `TypeError`. 4. **`py_dict_get_item(self, key):`** - Returns the value associated with `key` if it exists, or `None` otherwise. 5. **`py_dict_del_item(self, key) -> None:`** - Removes the entry with `key` from the dictionary. If `key` is not in the dictionary, raises a `KeyError`. 6. **`py_dict_clear(self) -> None:`** - Removes all items from the dictionary. 7. **`py_dict_contains(self, key) -> bool:`** - Returns `True` if the dictionary contains `key`, otherwise `False`. 8. **`py_dict_keys(self) -> list:`** - Returns a list of all the keys in the dictionary. 9. **`py_dict_values(self) -> list:`** - Returns a list of all the values in the dictionary. 10. **`py_dict_items(self) -> list:`** - Returns a list of all key-value pairs in the dictionary. 11. **`py_dict_size(self) -> int:`** - Returns the number of items in the dictionary. Example Usage: ```python adv_dict = AdvancedDict() adv_dict.py_dict_set_item(\'a\', 1) adv_dict.py_dict_set_item(\'b\', 2) print(adv_dict.py_dict_get_item(\'a\')) # should print 1 print(adv_dict.py_dict_get_item(\'c\')) # should print None print(adv_dict.py_dict_contains(\'b\')) # should print True print(adv_dict.py_dict_keys()) # should print [\'a\', \'b\'] print(adv_dict.py_dict_values()) # should print [1, 2] print(adv_dict.py_dict_items()) # should print [(\'a\', 1), (\'b\', 2)] print(adv_dict.py_dict_size()) # should print 2 adv_dict.py_dict_del_item(\'a\') print(adv_dict.py_dict_keys()) # should print [\'b\'] adv_dict.py_dict_clear() print(adv_dict.py_dict_keys()) # should print [] ``` Constraints: - All keys inserted into the dictionary must be hashable. - Performance considerations should be taken into account as these functions could be tested with a large number of dictionary operations. Implement the class `AdvancedDict` with the specified methods.","solution":"class AdvancedDict: def __init__(self) -> None: Initializes an empty dictionary. self._dict = {} def py_dict_new(self) -> dict: Creates and returns a new empty dictionary. return {} def py_dict_set_item(self, key, value) -> None: Sets value in the dictionary for the key. If key is not hashable, raises a TypeError. if not isinstance(key, (int, float, str, tuple, frozenset)): raise TypeError(\\"Key must be hashable\\") self._dict[key] = value def py_dict_get_item(self, key): Returns the value associated with key if it exists, or None otherwise. return self._dict.get(key) def py_dict_del_item(self, key) -> None: Removes the entry with key from the dictionary. If key is not in the dictionary, raises a KeyError. if key in self._dict: del self._dict[key] else: raise KeyError(f\\"Key \'{key}\' not found in dictionary\\") def py_dict_clear(self) -> None: Removes all items from the dictionary. self._dict.clear() def py_dict_contains(self, key) -> bool: Returns True if the dictionary contains key, otherwise False. return key in self._dict def py_dict_keys(self) -> list: Returns a list of all the keys in the dictionary. return list(self._dict.keys()) def py_dict_values(self) -> list: Returns a list of all the values in the dictionary. return list(self._dict.values()) def py_dict_items(self) -> list: Returns a list of all key-value pairs in the dictionary. return list(self._dict.items()) def py_dict_size(self) -> int: Returns the number of items in the dictionary. return len(self._dict)"},{"question":"# Seaborn Color Palette Assessment Objective You are required to demonstrate your understanding and ability to manipulate color palettes using the seaborn library. This will involve using the `sns.mpl_palette` function to retrieve and manage color palettes from various colormaps. Task Write functions to accomplish the following requirements using the seaborn library: 1. **Discrete Palette Function**: - Function Name: `get_discrete_palette` - Parameters: - `cmap_name` (string): The name of the colormap (e.g., `\\"viridis\\"`) from which to sample colors. - `num_colors` (int): The number of discrete colors to retrieve. - Output: - A list of `num_colors` colors sampled from the specified colormap. 2. **Continuous Palette Function**: - Function Name: `get_continuous_palette` - Parameters: - `cmap_name` (string): The name of the colormap (e.g., `\\"viridis\\"`). - Output: - A continuous colormap object usable in seaborn or matplotlib plotting functions. 3. **Qualitative Palette Function**: - Function Name: `get_qualitative_palette` - Parameters: - `cmap_name` (string): The name of the colormap (e.g., `\\"Set2\\"`). - `num_colors` (int): The number of discrete colors to retrieve. - Output: - A list of up to `num_colors` sampled from the qualitative colormap. Note that the palette will contain only distinct colors and may be shorter than the requested number. Constraints - Use the seaborn library for all palette manipulations. - Ensure that all function names and parameters are exactly as specified. - Handle possible errors gracefully, such as invalid colormap names or requesting more colors than available in a qualitative palette, by returning an appropriate message or value. Example Usage ```python # Example: Discrete palette with \\"viridis\\" and 5 colors discrete_palette = get_discrete_palette(\\"viridis\\", 5) print(discrete_palette) # Example: Continuous palette with \\"viridis\\" continuous_palette = get_continuous_palette(\\"viridis\\") print(type(continuous_palette)) # Example: Qualitative palette with \\"Set2\\" and 10 colors qualitative_palette = get_qualitative_palette(\\"Set2\\", 10) print(qualitative_palette) ``` Your solution will be evaluated based on correctness, handling of edge cases, and adherence to function requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def get_discrete_palette(cmap_name, num_colors): Returns a list of discrete colors sampled from the specified colormap. Parameters: - cmap_name (string): The name of the colormap. - num_colors (int): The number of discrete colors to retrieve. Output: - List of `num_colors` colors sampled from the specified colormap. try: colors = sns.color_palette(cmap_name, num_colors) return colors except ValueError: return f\\"Invalid colormap name or number of colors: {cmap_name}, {num_colors}\\" def get_continuous_palette(cmap_name): Returns a continuous colormap object. Parameters: - cmap_name (string): The name of the colormap. Output: - A continuous colormap object usable in seaborn or matplotlib plotting functions. try: cmap = sns.color_palette(cmap_name, as_cmap=True) return cmap except ValueError: return f\\"Invalid colormap name: {cmap_name}\\" def get_qualitative_palette(cmap_name, num_colors): Returns a list of discrete colors from a qualitative colormap. Parameters: - cmap_name (string): The name of the colormap. - num_colors (int): The number of discrete colors to retrieve. Output: - List of discrete colors from the qualitative colormap. try: colors = sns.color_palette(cmap_name, num_colors) return colors except ValueError: return f\\"Invalid colormap name or number of colors: {cmap_name}, {num_colors}\\""},{"question":"# BooleanArray Logical Operations and Indexing in Pandas Problem Statement: You are given a pandas DataFrame containing employee information along with their survey responses indicating whether they will attend the company event. The survey responses may contain `True`, `False`, and `pd.NA` values. You need to implement a function `event_attendance_report(df: pd.DataFrame) -> pd.DataFrame` that generates a report about those willing to attend, those not attending, and uncertain/undecided employees. The Boolean column for responses should be handled using `BooleanArray`. Input: - `df` (`pd.DataFrame`): A DataFrame with at least the following columns: - `employee_id`: Unique identifier for each employee. - `will_attend`: BooleanArray indicating survey responses (`True`, `False`, `pd.NA`). Output: - `report` (`pd.DataFrame`): A DataFrame with the following columns: - `willing_to_attend`: List of `employee_id`s who responded `True`. - `not_willing_to_attend`: List of `employee_id`s who responded `False`. - `uncertain`: List of `employee_id`s who have `pd.NA` in their survey response. Constraints: - The function should handle large DataFrames efficiently. - Use pandas\' `BooleanArray` for handling the boolean logic and indexing. Example: ```python import pandas as pd data = { \'employee_id\': [101, 102, 103, 104, 105], \'will_attend\': pd.array([True, False, pd.NA, True, pd.NA], dtype=\\"boolean\\") } df = pd.DataFrame(data) def event_attendance_report(df: pd.DataFrame) -> pd.DataFrame: willing_to_attend = df.loc[df[\'will_attend\'] == True, \'employee_id\'].tolist() not_willing_to_attend = df.loc[df[\'will_attend\'] == False, \'employee_id\'].tolist() uncertain = df.loc[df[\'will_attend\'].isna(), \'employee_id\'].tolist() report = pd.DataFrame({ \'willing_to_attend\': [willing_to_attend], \'not_willing_to_attend\': [not_willing_to_attend], \'uncertain\': [uncertain] }) return report print(event_attendance_report(df)) ``` Expected Output: ```python willing_to_attend not_willing_to_attend uncertain 0 [101, 104] [102] [103, 105] ``` Make sure to test your implementation with different cases and sizes of dataframes to validate the correctness and performance.","solution":"import pandas as pd def event_attendance_report(df: pd.DataFrame) -> pd.DataFrame: Generates a report about employees willing to attend, not willing to attend, and uncertain employees based on their survey responses. Parameters: df (pd.DataFrame): Employee DataFrame. Returns: pd.DataFrame: Report DataFrame with columns: willing_to_attend, not_willing_to_attend, and uncertain. willing_to_attend = df.loc[df[\'will_attend\'] == True, \'employee_id\'].tolist() not_willing_to_attend = df.loc[df[\'will_attend\'] == False, \'employee_id\'].tolist() uncertain = df.loc[df[\'will_attend\'].isna(), \'employee_id\'].tolist() report = pd.DataFrame({ \'willing_to_attend\': [willing_to_attend], \'not_willing_to_attend\': [not_willing_to_attend], \'uncertain\': [uncertain] }) return report"},{"question":"**Coding Assessment Question: Implement a Custom Event Handler to Log Events to File using `torch.monitor`** Your task is to implement a custom event handler for the `torch.monitor` module that logs events to a specified file. This custom event handler should adhere to the `torch.monitor` interface and properly log event details in a structured format to a file. # Requirements 1. Create a class `FileEventHandler` that mimics the behavior of `torch.monitor.TensorboardEventHandler`. 2. This class should: * Accept a file path as an initialization argument. * Log events received through the event handler into the specified file. * Log entries should include the timestamp, event name, and the event value in CSV format. 3. You will also implement functions to register and unregister this custom event handler. # Input Format: - Initialize your `FileEventHandler` with a file path. - Use `torch.monitor.register_event_handler` to register your event handler. - Log some sample events using `torch.monitor.log_event`. - Use `torch.monitor.unregister_event_handler` to unregister your event handler. # Output Format: - The specified file should contain the logged events in CSV format (e.g., `timestamp,event_name,event_value`). # Constraints: - The event handler should handle logging with minimal performance impact. - Use appropriate file handling to ensure data integrity and consistency. # Example: ```python import torch.monitor import time class FileEventHandler: def __init__(self, file_path): self.file_path = file_path def handle(self, event): with open(self.file_path, \'a\') as f: f.write(f\\"{time.time()},{event.name},{event.value}n\\") # Register the custom event handler file_handler = FileEventHandler(\'/tmp/events.log\') handle = torch.monitor.register_event_handler(file_handler.handle) # Log some events torch.monitor.log_event(\\"event_1\\", 123) torch.monitor.log_event(\\"event_2\\", 456) # Unregister the custom event handler torch.monitor.unregister_event_handler(handle) # Verify the file contents with open(\'/tmp/events.log\', \'r\') as f: print(f.read()) ``` Your implementation should handle these steps and produce the correct output in the specified file. # Grading Criteria: - Correct implementation of the `FileEventHandler` class. - Proper registration and deregistration of the custom event handler. - Accurate logging of events in the specified CSV format. - Code clarity and appropriate use of `torch.monitor` interfaces.","solution":"import time class FileEventHandler: def __init__(self, file_path): self.file_path = file_path def handle(self, event): with open(self.file_path, \'a\') as f: f.write(f\\"{time.time()},{event[\'name\']},{event[\'value\']}n\\") def register_event_handler(handler): # Mimic registration mechanism return handler def unregister_event_handler(handler): # Mimic unregistration mechanism pass def log_event(handler, event_name, event_value): event = {\'name\': event_name, \'value\': event_value} handler.handle(event)"},{"question":"# Async Task Runner with Timeouts Problem Statement You are given a list of tasks that need to be executed concurrently. Each task is represented as a coroutine function. However, each task comes with a maximum time limit (in seconds) within which it must complete. If a task does not complete within this time limit, it should be cancelled, and an appropriate message should be logged. You need to use the `asyncio` library to manage this concurrency and handle the timeouts. Function Signature ```python import asyncio async def async_task_runner(tasks: list[(asyncio.coroutine, int)]) -> list[list]: pass ``` Input - `tasks`: A list of tuples, each containing an asynchronous coroutine function and an integer representing the maximum time limit (in seconds) for that task. Output - A list containing the respective result or error message for each task. If a task completes successfully within its time limit, the result should be included in the list. If a task is cancelled due to timeout, the error message `\\"Task cancelled due to timeout\\"` should be included. Constraints - You may assume that each task does not require more than 3 GB of memory. - The number of tasks will not exceed 1000. - Each time limit will be a positive integer less than 1000 seconds. Example ```python import asyncio async def task_1(): await asyncio.sleep(2) return \\"Task 1 complete\\" async def task_2(): await asyncio.sleep(5) return \\"Task 2 complete\\" tasks = [(task_1(), 3), (task_2(), 4)] results = await async_task_runner(tasks) print(results) # Output: [\\"Task 1 complete\\", \\"Task cancelled due to timeout\\"] ``` Note - You may use `asyncio.wait_for` or other `asyncio` utilities to enforce the time limits. - Ensure proper logging and cancellation of tasks that exceed their time limits. - Handle any possible exceptions that the tasks might raise.","solution":"import asyncio async def async_task_runner(tasks): results = [] async def run_with_timeout(task, timeout): try: return await asyncio.wait_for(task, timeout) except asyncio.TimeoutError: return \\"Task cancelled due to timeout\\" for task, timeout in tasks: result = await run_with_timeout(task, timeout) results.append(result) return results"},{"question":"# Seaborn Data Visualization Assessment Problem Statement You are provided with a dataset containing information about annual global temperature changes along with CO2 emission records from various countries for multiple years. Your task is to visualize this data using Seaborn to identify any observable trends. The dataset is provided as two separate CSV files: `global_temperature.csv` and `co2_emissions.csv`. Requirements 1. **Load the Data:** - Load `global_temperature.csv` into a DataFrame named `temp_df`. This file contains columns `Year`, `Temperature_Change`. - Load `co2_emissions.csv` into a DataFrame named `co2_df`. This file contains columns `Year`, `Country`, `CO2_Emissions`. 2. **Data Transformation:** - Merge the two dataframes into a single DataFrame named `merged_df` based on the `Year` column. - Convert `merged_df` into a wide-form dataframe named `wide_df` where each country\'s `CO2_Emissions` are separate columns. 3. **Data Visualization:** - Create a line plot using seaborn’s `relplot` that shows the annual global temperature changes over the years (`temp_df`), with `Year` on the x-axis and `Temperature_Change` on the y-axis. - Create a line plot using seaborn’s `relplot` that shows each country’s CO2 emissions over the years using the wide-form DataFrame `wide_df`. - Create a scatter plot comparing annual temperature changes with total CO2 emissions of all countries combined for each year. Ensure to compute the total CO2 emissions by summing across all countries for each year before plotting. - Create a categorized plot using seaborn’s `catplot` that shows CO2 emissions of the top 5 emitting countries in a given year (e.g., 2000). Function Definition: Implement the function `visualize_data` which takes the paths to the CSV files as arguments and performs the required tasks. ```python import pandas as pd import seaborn as sns def visualize_data(temp_file: str, co2_file: str): # 1. Load Data temp_df = pd.read_csv(temp_file) co2_df = pd.read_csv(co2_file) # 2. Merge DataFrames merged_df = pd.merge(temp_df, co2_df, on=\'Year\') # 3. Convert merged dataframe to wide-form wide_df = merged_df.pivot(index=\'Year\', columns=\'Country\', values=\'CO2_Emissions\') # 4. Plot annual global temperature change sns.relplot(data=temp_df, x=\'Year\', y=\'Temperature_Change\', kind=\'line\') # 5. Plot CO2 emissions (wide-form) sns.relplot(data=wide_df, kind=\'line\') # 6. Create scatter plot comparing temperature change with total CO2 emissions. total_co2 = co2_df.groupby(\'Year\')[\'CO2_Emissions\'].sum().reset_index() scatter_df = pd.merge(temp_df, total_co2, on=\'Year\') sns.relplot(data=scatter_df, x=\'Temperature_Change\', y=\'CO2_Emissions\', kind=\'scatter\') # 7. Plot CO2 emissions of top 5 emitting countries in a given year using catplot. top5_countries = co2_df[co2_df[\'Year\'] == 2000].nlargest(5, \'CO2_Emissions\') sns.catplot(data=top5_countries, x=\'Country\', y=\'CO2_Emissions\', kind=\'bar\') ``` Input - `temp_file` (str): Path to `global_temperature.csv`. - `co2_file` (str): Path to `co2_emissions.csv`. Output - The function will not return anything but is expected to display multiple plots. Data Constraints - Assume the data files are clean and correctly formatted. - The year ranges in both datasets overlap sufficiently for meaningful merging and analysis. Performance - Aim for efficient data handling as the datasets might be significantly large.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_data(temp_file: str, co2_file: str): # 1. Load Data temp_df = pd.read_csv(temp_file) co2_df = pd.read_csv(co2_file) # 2. Merge DataFrames merged_df = pd.merge(temp_df, co2_df, on=\'Year\') # 3. Convert merged dataframe to wide-form wide_df = merged_df.pivot(index=\'Year\', columns=\'Country\', values=\'CO2_Emissions\') # 4. Plot annual global temperature change plt.figure() sns.relplot(data=temp_df, x=\'Year\', y=\'Temperature_Change\', kind=\'line\') plt.savefig(\'global_temperature_change.png\') # 5. Plot CO2 emissions (wide-form) plt.figure() sns.relplot(data=wide_df, kind=\'line\') plt.savefig(\'co2_emissions_wide_form.png\') # 6. Create scatter plot comparing temperature change with total CO2 emissions. total_co2 = co2_df.groupby(\'Year\')[\'CO2_Emissions\'].sum().reset_index() scatter_df = pd.merge(temp_df, total_co2, on=\'Year\') plt.figure() sns.relplot(data=scatter_df, x=\'Temperature_Change\', y=\'CO2_Emissions\', kind=\'scatter\') plt.savefig(\'temp_change_vs_total_co2.png\') # 7. Plot CO2 emissions of top 5 emitting countries in a given year using catplot. top5_countries = co2_df[co2_df[\'Year\'] == 2000].nlargest(5, \'CO2_Emissions\') plt.figure() sns.catplot(data=top5_countries, x=\'Country\', y=\'CO2_Emissions\', kind=\'bar\') plt.savefig(\'top5_emissions_2000.png\')"},{"question":"# Question **Objective:** The goal of this task is to assess your understanding of Python\'s import system and your ability to interact with and manipulate module imports in Python 3.10. **Task:** You need to simulate an import system. You are required to implement a custom import system that demonstrates an understanding of finders, loaders, and the module spec. 1. Implement a class `CustomFinder` that should act as a finder to locate modules. 2. Implement a class `CustomLoader` that should act as a loader to load the module. 3. Implement a function `custom_import(module_name: str) -> Any` that uses these classes to import a module by its name. **Details:** 1. **CustomFinder Class:** - This class should have a method `find_spec(fullname, path, target=None)` that returns a module spec if the module can be found. 2. **CustomLoader Class:** - This class should have two methods: - `create_module(spec)`: which should return a new module object if the loader can create it. - `exec_module(module)`: which should execute the code of the module in the module\'s namespace. 3. **custom_import Function:** - This function should mimic the behavior of the built-in `__import__` function by using `CustomFinder` and `CustomLoader` to locate and load the module. - The function should add its `CustomFinder` to `sys.meta_path` to be involved in the import process. **Example Usage:** ```python import sys import types import os class CustomFinder: # Your implementation here class CustomLoader: # Your implementation here def custom_import(module_name: str): # Your implementation here # Test the custom import system if __name__ == \\"__main__\\": # Add CustomFinder to sys.meta_path sys.meta_path.insert(0, CustomFinder()) # Test importing a builtin module os = custom_import(\\"os\\") print(os.path) # Should print os.path module # Test importing a custom module mymodule = custom_import(\\"mymodule\\") print(mymodule) # Should print <module \'mymodule\' from \'...\'> ``` **Constraints:** - Do not use the built-in `import` statement (other than the initial imports provided in the example). - Do not use `importlib.import_module()`. - Make sure to handle exceptions such as `ModuleNotFoundError` and `ImportError` appropriately. **Submission:** Submit your implementation for the `CustomFinder`, `CustomLoader`, and `custom_import` function in a Python script file.","solution":"import sys import types import importlib.machinery import importlib.util class CustomFinder: def find_spec(self, fullname, path, target=None): try: # Try to find the module spec using standard library mechanisms spec = importlib.util.find_spec(fullname) if spec: spec.loader = CustomLoader() return spec except Exception: return None class CustomLoader: def create_module(self, spec): # Use default module creation semantics return None def exec_module(self, module): # Execute the module using the default exec and loader try: loader = importlib.machinery.SourceFileLoader(module.__name__, spec.origin) loader.exec_module(module) except FileNotFoundError: raise ModuleNotFoundError(f\\"Module {module.__name__} not found\\") def custom_import(module_name: str): if module_name in sys.modules: return sys.modules[module_name] # Use CustomFinder to find the module for finder in sys.meta_path: spec = finder.find_spec(module_name, None) if spec is not None: module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) return module raise ModuleNotFoundError(f\\"Module {module_name} not found\\") if __name__ == \\"__main__\\": # Add CustomFinder to sys.meta_path sys.meta_path.insert(0, CustomFinder()) # Test importing a builtin module os = custom_import(\\"os\\") print(os.path) # Should print os.path module # Test importing a custom module if available in the environment try: mymodule = custom_import(\\"mymodule\\") print(mymodule) # Should print <module \'mymodule\' from \'...\'> except ModuleNotFoundError: print(\\"mymodule not found\\")"},{"question":"# Seaborn Coding Assessment Objective: You are required to demonstrate your understanding of seaborn by creating a visualization using a dataset and performing specific data transformations. This will assess your ability to manipulate data and use seaborn to generate complex plots. Question: You are given a dataset containing information about different brain networks measured at various time points. Your task is to: 1. Load the dataset using the seaborn `load_dataset` function. 2. Perform specific data transformations: - Rename the index to `timepoint`. - Stack certain levels and group by `timepoint`, `network`, and `hemi`. - Compute the mean of the groups. - Unstack the `network` level and reset the index. - Filter the dataset to include only those rows where the `timepoint` is less than 100. 3. Create a pair plot for the variables `5`, `8`, `12`, and `15` on the x-axis, and `6`, `13`, and `16` on the y-axis, using seaborn\'s `so.Plot` class. 4. Customize the plot: - Set the layout size to (8, 5). - Share the axes for both x and y among all subplots. - Add `Paths` to the plot. - Set the `color` by `hemi`, `linewidth` to 1, and `alpha` to 0.8. Input: No input in terms of function parameters but you will interact with a dataset included in seaborn. Output: Generate a seaborn plot following the steps above and display it. Here is an example structure to guide you: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") # Step 2: Rename index .stack([0, 1, 2]) # Step 2: Stack levels .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]).mean() # Step 2: Group and mean .unstack(\\"network\\").reset_index() # Step 2: Unstack and reset index .query(\\"timepoint < 100\\") # Step 2: Filter rows ) # Step 3: Create the pair plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) # Step 4: Set layout size .share(x=True, y=True) # Step 4: Share axes ) # Step 4: Customize and add paths p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Finally, display the plot p.show() ``` Ensure your solution adheres to the requirements and replicates the transformation and plotting steps accurately. Constraints: - Use Python 3.10 or newer. - Ensure seaborn and other necessary libraries are installed. Evaluation: Your solution will be evaluated based on correctness, code clarity, and adherence to the instructions.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_brain_networks_plot(): # Step 1: Load the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") # Step 2: Rename index .stack([0, 1, 2]) # Step 2: Stack levels .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]).mean() # Step 2: Group and mean .unstack(\\"network\\").reset_index() # Step 2: Unstack and reset index .query(\\"timepoint < 100\\") # Step 2: Filter rows ) # Step 3: Create the pair plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) # Step 4: Set layout size .share(x=True, y=True) # Step 4: Share axes ) # Step 4: Customize and add paths p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Finally, display the plot p.show()"},{"question":"**Question: Implementing Multilingual Support in a Python Application** You are tasked with creating a small Python application that supports multiple languages using the `gettext` module. The application should display a greeting message based on the user\'s language preference. Your task is to implement a function `display_greeting` that: 1. Accepts a language code (e.g., \'en\' for English, \'es\' for Spanish) as input. 2. Uses the `gettext` module to retrieve the appropriate translation for \\"Hello, World!\\" based on the provided language code. 3. Handles cases where the language code is not supported and defaults to English. 4. Uses message catalogs for translation, assuming the `.mo` files are properly set up for \'en\' and \'es\' languages. # Requirements: - Use the `gettext` module. - Create a function with the following signature: ```python def display_greeting(language_code: str) -> str: ``` - The `gettext` machinery should assume the presence of `locales` directory structured as: ``` locales/ en/ LC_MESSAGES/ greetings.mo es/ LC_MESSAGES/ greetings.mo ``` - The function should return the translated greeting string. - Ensure your solution accounts for unsupported languages by defaulting to English (\'en\'). # Example Usage: ```python # Assuming the .mo files are correctly compiled and placed, # The following should return the appropriate translations. print(display_greeting(\'en\')) # Output: \\"Hello, World!\\" print(display_greeting(\'es\')) # Output: \\"¡Hola, Mundo!\\" print(display_greeting(\'fr\')) # Output: \\"Hello, World!\\" (defaulting to English) ``` # Notes: - You may need to simulate the presence of the `.mo` files and translations for the sake of this implementation. - The focus should be on correct setup and usage of the `gettext` module. Good luck, and happy coding!","solution":"import gettext import os def display_greeting(language_code: str) -> str: Returns a greeting message based on the provided language code. Falls back to English if the language code is not supported. :param language_code: str, language code (\'en\', \'es\', etc.) :return: str, translated greeting message locales_dir = os.path.join(os.path.dirname(__file__), \'locales\') try: translation = gettext.translation(\'greetings\', localedir=locales_dir, languages=[language_code]) translation.install() except FileNotFoundError: translation = gettext.translation(\'greetings\', localedir=locales_dir, languages=[\'en\']) translation.install() _ = translation.gettext return _(\\"Hello, World!\\") # Ensure to mock the .mo files and translations during testing."},{"question":"# Advanced Coding Assessment: Custom Object Serialization **Objective:** Implement custom serialization logic for a complex object in Python using the `copyreg` module. **Problem Statement:** You are required to serialize and deserialize an object of a custom class `Person` that contains attributes representing a person\'s details. However, some attributes need special handling during the pickling process. Implement and register custom pickle functions for the `Person` class. **Specification:** 1. **Class Definition:** Define a class `Person` with the following attributes: - `name`: The person\'s name (string). - `age`: The person\'s age (integer). - `address`: The person\'s address (string). - `secret_key`: A sensitive piece of data (string) that should not be stored during serialization. Instead, store a fixed string `\\"REDACTED\\"`. 2. **Pickle Function:** Implement a reduction function `pickle_person` for the `Person` class that: - Omits the `secret_key` attribute or replaces it with `\\"REDACTED\\"` during serialization. - Reconstructs a `Person` object using the remaining attributes during deserialization. 3. **Registering with `copyreg`:** Use the `copyreg.pickle` method to register your custom pickling function for the `Person` class. 4. **Usage:** Demonstrate the usage of your custom pickling logic by: - Creating an instance of `Person`. - Pickling and unpickling this instance using the `pickle` module. - Verifying that the `secret_key` attribute is appropriately handled during this process. **Input and Output:** - You are not required to handle any input from the user. Just implement the necessary class and functions and show the process via code snippets. - The output should demonstrate the pickling and unpickling process, showing that the `secret_key` attribute is handled as specified. **Constraints:** - Use Python 3.10 syntax and features. - Ensure your solution is generalizable, without hardcoding specific values for the `Person` instance used in usage demonstration. **Example Implementation:** ```python import copyreg import pickle # Step 1: Define the class class Person: def __init__(self, name, age, address, secret_key): self.name = name self.age = age self.address = address self.secret_key = secret_key def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, address={self.address}, secret_key={self.secret_key})\\" # Step 2: Define the pickle function def pickle_person(person): print(\\"Pickling Person instance...\\") return Person, (person.name, person.age, person.address, \\"REDACTED\\") # Step 3: Register the pickle function copyreg.pickle(Person, pickle_person) # Step 4: Usage demonstration if __name__ == \\"__main__\\": person = Person(\\"John Doe\\", 30, \\"123 Elm St.\\", \\"s3cr3t\\") print(\\"Original:\\", person) # Serialize (pickle) the person object serialized_person = pickle.dumps(person) # Deserialize (unpickle) the person object deserialized_person = pickle.loads(serialized_person) print(\\"Deserialized:\\", deserialized_person) ``` This should provide a structured and challenging problem for students to demonstrate their understanding of custom object serialization in Python.","solution":"import copyreg import pickle class Person: def __init__(self, name, age, address, secret_key): self.name = name self.age = age self.address = address self.secret_key = secret_key def __repr__(self): return f\\"Person(name={self.name}, age={self.age}, address={self.address}, secret_key={self.secret_key})\\" def pickle_person(person): return Person, (person.name, person.age, person.address, \\"REDACTED\\") copyreg.pickle(Person, pickle_person) if __name__ == \\"__main__\\": # Usage demonstration person = Person(\\"John Doe\\", 30, \\"123 Elm St.\\", \\"s3cr3t\\") print(\\"Original:\\", person) # Serialize (pickle) the person object serialized_person = pickle.dumps(person) # Deserialize (unpickle) the person object deserialized_person = pickle.loads(serialized_person) print(\\"Deserialized:\\", deserialized_person)"},{"question":"You are tasked with utilizing the seaborn package, specifically its `objects` interface, to analyze and visualize the famous \'titanic\' dataset. This dataset contains information about the passengers who were aboard the Titanic. The dataset is available within seaborn. # Problem Statement Write a Python function `plot_titanic_data` that visualizes different aspects of the Titanic dataset using seaborn\'s `objects` interface. Your function should create a single figure consisting of: 1. A dot plot showing the average age of passengers by their class and sex. 2. A dot plot with error bars showing the survival rate by passenger class and sex. 3. A line plot showing the number of passengers who embarked at each port (\'Embarked\' column) by class. 4. Facet the plots by the \'Pclass\' column for better comparison between classes. # Requirements 1. **Input**: None (Load the \'titanic\' dataset directly within the function). 2. **Output**: A visual representation in a figure. The plots should be displayed using seaborn\'s `objects` interface. 3. Use seaborn\'s `load_dataset` method to load the \'titanic\' data. 4. Implement subplots using an appropriate layout. 5. Customize the appearance with appropriate labels and legends. # Function Signature ```python import seaborn.objects as so import seaborn as sns def plot_titanic_data(): pass # Example invocation plot_titanic_data() ``` # Constraints 1. Ensure the plots are clear and correctly labeled. 2. The function should handle the dataset efficiently, considering missing values where necessary. 3. Use faceting appropriately to make meaningful comparisons. 4. No additional packages other than seaborn, pandas, and matplotlib should be used.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def plot_titanic_data(): # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Initialize the figure and axes fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # Flatten the axes for easier plotting ax1, ax2, ax3, ax4 = axes.flatten() # Plot 1: Average age of passengers by class and sex (dot plot) sns.barplot( data=titanic, x=\\"pclass\\", y=\\"age\\", hue=\\"sex\\", ax=ax1, estimator=lambda x: sum(x)/len(x) ) ax1.set_title(\'Average Age by Passenger Class and Sex\') ax1.set_xlabel(\'Passenger Class\') ax1.set_ylabel(\'Average Age\') # Plot 2: Survival rate by passenger class and sex (dot plot with error bars) sns.pointplot( data=titanic, x=\\"pclass\\", y=\\"survived\\", hue=\\"sex\\", ax=ax2, join=False, capsize=0.1, markers=[\'o\', \'D\'], errwidth=1.5 ) ax2.set_title(\'Survival Rate by Passenger Class and Sex\') ax2.set_xlabel(\'Passenger Class\') ax2.set_ylabel(\'Survival Rate\') # Plot 3: Number of passengers who embarked at each port by class (line plot) embarked_counts = titanic.groupby([\'embarked\', \'pclass\']).size().reset_index(name=\'count\') sns.lineplot( data=embarked_counts, x=\\"embarked\\", y=\\"count\\", hue=\\"pclass\\", ax=ax3, markers=True, style=\\"pclass\\" ) ax3.set_title(\'Number of Passengers by Embarkation Port and Class\') ax3.set_xlabel(\'Embarkation Port\') ax3.set_ylabel(\'Number of Passengers\') # Hide the unused fourth subplot fig.delaxes(ax4) # Adjust layout for better spacing plt.tight_layout() plt.show() # Example invocation plot_titanic_data()"},{"question":"**Objective:** Demonstrate your understanding of the `signal` module in Python by implementing a program that involves setting custom signal handlers and using timers. You are required to: 1. Implement a custom signal handler for `SIGALRM` that will print the current timestamp each time it is triggered. 2. Set up an interval timer using `signal.setitimer()` to trigger the `SIGALRM` handler every 2 seconds. 3. Implement a custom signal handler for `SIGINT` (triggered by Ctrl+C) that will gracefully terminate the program, printing a message that the program is shutting down. 4. Ensure that the interval timer stops and the program exits cleanly when the `SIGINT` is received. **Requirements:** 1. **Function Signature**: ```python def setup_signal_handlers(): pass def run_timer(): pass ``` 2. **Input and Output**: - No input is required. - Outputs involve printing timestamps and custom shutdown messages. 3. **Constraints**: - Use the provided function signatures. - You may import modules such as `signal`, `time`, and `datetime`. 4. **Additional Considerations**: - Ensure that your program can run indefinitely until interrupted by `SIGINT`. - Handle exceptions appropriately. **Notes:** - Utilize the `signal` module documentation provided to correctly handle signals and timers. # Example Output When you run the program and do not interrupt, you should see the current timestamp printed every 2 seconds: ``` 2023-10-30 10:00:01 2023-10-30 10:00:03 2023-10-30 10:00:05 ... ``` Upon pressing Ctrl+C: ``` 2023-10-30 10:00:07 Received SIGINT. Shutting down gracefully... ``` # Implementation Implement the following functions: ```python import signal import time from datetime import datetime def timestamp_handler(signum, frame): print(datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')) def sigint_handler(signum, frame): print(\'Received SIGINT. Shutting down gracefully...\') signal.setitimer(signal.ITIMER_REAL, 0) # Disable the interval timer exit(0) def setup_signal_handlers(): signal.signal(signal.SIGALRM, timestamp_handler) signal.signal(signal.SIGINT, sigint_handler) def run_timer(): signal.setitimer(signal.ITIMER_REAL, 2, 2) # Set interval timer to trigger every 2 seconds # Keep the program running indefinitely until SIGINT is received while True: time.sleep(1) if __name__ == \'__main__\': setup_signal_handlers() run_timer() ``` **Explanation:** - `timestamp_handler`: This function is triggered every time `SIGALRM` is received, printing the current timestamp. - `sigint_handler`: This function is triggered on `SIGINT` (Ctrl+C), safely cleaning up by disabling the timer and exiting the program. - `setup_signal_handlers`: This function sets up the signal handlers for `SIGALRM` and `SIGINT`. - `run_timer`: This function sets the interval timer to trigger `SIGALRM` every 2 seconds and keeps the main program running until it is interrupted. Test your implementation thoroughly to ensure it handles the signals as expected.","solution":"import signal import time from datetime import datetime def timestamp_handler(signum, frame): print(datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')) def sigint_handler(signum, frame): print(\'Received SIGINT. Shutting down gracefully...\') signal.setitimer(signal.ITIMER_REAL, 0) # Disable the interval timer exit(0) def setup_signal_handlers(): signal.signal(signal.SIGALRM, timestamp_handler) signal.signal(signal.SIGINT, sigint_handler) def run_timer(): signal.setitimer(signal.ITIMER_REAL, 2, 2) # Set interval timer to trigger every 2 seconds # Keep the program running indefinitely until SIGINT is received while True: time.sleep(1) if __name__ == \'__main__\': setup_signal_handlers() run_timer()"},{"question":"**Context:** The \\"linecache\\" module provides a way to retrieve specific lines from a file efficiently using an internal caching mechanism. This can be particularly useful when dealing with large files or when specific lines need to be accessed repeatedly. **Task:** You are tasked with implementing a utility function that analyzes a given Python script and provides a summary of the lines that match a certain criterion. This criterion could be any of the following: 1. Lines containing a specific keyword. 2. Lines starting with a specific character (e.g., comment lines starting with `#`). 3. Lines of a specific length. You must implement the function `analyze_script(filename: str, criterion: Callable[[str], bool]) -> List[str]`. This function should: 1. Read the file specified by `filename`. 2. Use the \\"linecache\\" module to retrieve lines. 3. Apply the provided `criterion` function to each line to determine if it should be included in the result. 4. Return a list of lines that match the provided criterion. **Input:** - `filename`: A string representing the path to the Python file to be analyzed. - `criterion`: A callable that takes a single string (a line from the file) and returns `True` if the line matches the criterion, and `False` otherwise. **Output:** - A list of strings, each string being a line from the file that matches the provided criterion. **Example:** ```python def contains_keyword(keyword): return lambda line: keyword in line def main(): filename = \'example.py\' keyword = \'import\' result = analyze_script(filename, contains_keyword(keyword)) for line in result: print(line) if __name__ == \'__main__\': main() ``` Here, `contains_keyword(keyword)` is a higher-order function that returns a criterion function checking if the keyword is present in a line. **Constraints:** - Do not read the entire file into memory at once. - Utilize the \\"linecache\\" module to retrieve lines. - Ensure that the function gracefully handles errors, such as non-existent files, by returning an empty list in such cases. Implement the function `analyze_script` as described above. **Notes:** - The `linecache.getline` function can be instrumental in efficiently retrieving lines. - Consider edge cases such as empty files, files without matching lines, etc. - Performance considerations should be taken into account, especially for large files.","solution":"import linecache from typing import Callable, List def analyze_script(filename: str, criterion: Callable[[str], bool]) -> List[str]: Analyzes a given Python script and returns the lines that match the provided criterion. Parameters: - filename: Path to the Python file to be analyzed. - criterion: A function that takes a line as input and returns True if the line matches the criterion, False otherwise. Returns: - A list of lines that match the criterion. result = [] try: line_num = 1 while True: line = linecache.getline(filename, line_num) if not line: break if criterion(line): result.append(line.rstrip(\'n\')) line_num += 1 except Exception as e: return [] finally: linecache.clearcache() return result"},{"question":"# Custom PyTorch Function and Module Implementation Objective Create a custom PyTorch function and module that emulates the behavior of a parameterized squaring operation. The operation should square the input tensor and allow for scaling by a given factor. The custom module should also integrate with PyTorch\'s autograd mechanism. Task 1. **Custom Function Subclass**: - Create a subclass of `torch.autograd.Function` that implements a squaring operation, `SquareFunction`. - Implement the `forward` method to compute the squared input, scaled by a given factor. - Implement the `backward` method to compute the gradient of the input during backpropagation. 2. **Custom Module**: - Create a subclass of `torch.nn.Module` called `SquareModule`. - Implement the `__init__` method to take a scaling factor as input. - Implement the `forward` method to utilize the `SquareFunction`. 3. **Integration with PyTorch**: - Ensure tensors\' gradients are correctly computed using PyTorch’s autograd mechanism. - Validate the implementation using `torch.autograd.gradcheck`. Implementation Details 1. **SquareFunction Class** - `forward(ctx, input, factor)`: Return `input^2 * factor` - `backward(ctx, grad_output)`: Compute and return the gradient wrt the input and factor. 2. **SquareModule Class** - `__init__(self, factor)`: Initialize with a given factor. - `forward(self, input)`: Apply `SquareFunction` to the input tensor using the stored factor. Constraints and Requirements - Ensure that the custom function supports passing gradients for both input and factor during the backward pass. - The `SquareModule` should be able to chain with other PyTorch operations seamlessly. - Use `gradcheck` to test the correctness of the gradients. # Example Code ```python import torch from torch.autograd import Function import torch.nn as nn class SquareFunction(Function): @staticmethod def forward(ctx, input, factor): ctx.save_for_backward(input, torch.tensor(factor, dtype=input.dtype)) return input * input * factor @staticmethod def backward(ctx, grad_output): input, factor = ctx.saved_tensors is_factor_scalar = factor.dim() == 0 grad_input = grad_factor = None if ctx.needs_input_grad[0]: grad_input = grad_output * 2 * input * factor if ctx.needs_input_grad[1]: grad_factor = (grad_output * input**2).sum() if is_factor_scalar else grad_output * input**2 return grad_input, grad_factor class SquareModule(nn.Module): def __init__(self, factor=1.0): super(SquareModule, self).__init__() self.factor = factor def forward(self, input): return SquareFunction.apply(input, self.factor) # Example use case if __name__ == \\"__main__\\": input_tensor = torch.randn(5, dtype=torch.double, requires_grad=True) factor = 3.0 square_op = SquareModule(factor=factor) output = square_op(input_tensor) output.sum().backward() # Numerical gradient check test_input = (input_tensor,) test_factor = (torch.tensor(factor, dtype=input_tensor.dtype, requires_grad=True),) print(torch.autograd.gradcheck(lambda x: SquareFunction.apply(x, test_factor[0]), test_input)) ``` Notes - Ensure that `grad_input` and `grad_factor` are properly computed in the `backward` method. - Handle both scalar and tensor factors appropriately.","solution":"import torch from torch.autograd import Function import torch.nn as nn class SquareFunction(Function): @staticmethod def forward(ctx, input, factor): ctx.save_for_backward(input, torch.tensor(factor, dtype=input.dtype)) return input * input * factor @staticmethod def backward(ctx, grad_output): input, factor = ctx.saved_tensors is_factor_scalar = factor.dim() == 0 grad_input = grad_factor = None if ctx.needs_input_grad[0]: grad_input = grad_output * 2 * input * factor if ctx.needs_input_grad[1]: grad_factor = (grad_output * input**2).sum() if is_factor_scalar else grad_output * input**2 return grad_input, grad_factor class SquareModule(nn.Module): def __init__(self, factor=1.0): super(SquareModule, self).__init__() self.factor = factor def forward(self, input): return SquareFunction.apply(input, self.factor)"},{"question":"# Python Coding Assessment Question **Objective:** Implement a function to manage the state of a variable using cell objects. This function will demonstrate an understanding of creating and manipulating cell objects, as well as handling nested scopes. **Requirements:** 1. **Function Name:** `manage_variable_state` 2. Utilize the provided cell object functions to manage the state of a variable. 3. **Function Signature:** ```python def manage_variable_state(initial_value: Any) -> Callable: ``` 4. **Input:** - `initial_value`: The initial value to be stored in the cell object. 5. **Output:** - Returns a callable that when invoked: * With no arguments, returns the current value. * With one argument, sets a new value in the cell and returns `None`. 6. **Constraints:** - You cannot use global variables. - The solution should handle nested scopes correctly. **Example Usage:** ```python state_manager = manage_variable_state(10) assert state_manager() == 10 # Get current value state_manager(20) assert state_manager() == 20 # Get updated value ``` **Note:** You may assume the availability of the following C-API functions which should be utilized wherever necessary: - `PyCell_New(ob)` - `PyCell_Get(cell)` - `PyCell_SET(cell, value)` - `PyCell_Check(ob)` **Hint:** You may need to use `ctypes` or other advanced Python techniques to interface with the provided C-API functions.","solution":"import ctypes from typing import Any, Callable # Define the necessary ctypes PyCell_New = ctypes.pythonapi.PyCell_New PyCell_New.argtypes = [ctypes.py_object] PyCell_New.restype = ctypes.py_object PyCell_Get = ctypes.pythonapi.PyCell_Get PyCell_Get.argtypes = [ctypes.py_object] PyCell_Get.restype = ctypes.py_object PyCell_Set = ctypes.pythonapi.PyCell_Set PyCell_Set.argtypes = [ctypes.py_object, ctypes.py_object] PyCell_Set.restype = ctypes.c_int def manage_variable_state(initial_value: Any) -> Callable: cell = PyCell_New(initial_value) def state_manager(new_value=None): if new_value is None: return PyCell_Get(cell) else: PyCell_Set(cell, new_value) return state_manager"},{"question":"**Question: Implement a Custom GroupBy Iterator Using `itertools` and `functools`** In this task, your goal is to implement a custom iterator `group_by_criteria` that groups elements from a given iterable based on a specified key function. You will employ utilities from the `itertools` and `functools` modules to achieve this. The function should mimic the behavior of SQL\'s `GROUP BY` clause. You are required to write two functions: 1. **group_by_criteria(iterable, key_func)**: - **Input**: - `iterable`: An iterable of elements to be grouped. - `key_func`: A function that extracts the key for grouping from each element. - **Output**: - An iterator yielding tuples, where each tuple consists of a key and a list of elements that share that key. 2. **partial_sum(add_func)** (Optional challenge): - **Input**: - `add_func`: A function that takes two elements and returns their sum. - **Output**: - A partially-applied version of the `reduce` function from `functools`, where the summing functionality is predefined by `add_func`. # Example Usage: ```python from itertools import tee from functools import reduce # Example key function def get_length(element): return len(element) # Example add function for partial_sum def add_strings(x, y): return x + y # Sample iterable iterable = [\\"apple\\", \\"bee\\", \\"banana\\", \\"carrot\\", \\"dog\\", \\"elephant\\"] # Expected output: # [(\'apple\', \'banana\', \'carrot\', \'elephant\'), (\'bee\', \'dog\')] result = list(group_by_criteria(iterable, get_length)) # Function call to partially apply reduce with add_strings custom_sum = partial_sum(add_strings) # Use custom_sum in a reduce operation sum_result = custom_sum([\\"a\\", \\"b\\", \\"c\\", \\"d\\"]) # Outputs: \\"abcd\\" print(result) print(sum_result) ``` # Constraints: - You may not use libraries beyond `itertools` and `functools`. - Ensure code efficiency and readability. - Handle edge cases such as empty iterables appropriately. # Note You are encouraged to explore the `itertools.groupby` and `functools.partial` modules and functions to structure your solution effectively.","solution":"from itertools import groupby from functools import partial, reduce def group_by_criteria(iterable, key_func): Groups elements from the given iterable based on the specified key function. Parameters: iterable (iterable): An iterable of elements to be grouped. key_func (function): A function that extracts the key for grouping from each element. Returns: iterator: An iterator yielding tuples, where each tuple consists of a key and a list of elements that share that key. # Sort the iterable based on key_func to ensure groupby works correctly sorted_iterable = sorted(iterable, key=key_func) return ((key, list(group)) for key, group in groupby(sorted_iterable, key=key_func)) def partial_sum(add_func): Creates a partially-applied version of reduce with the summarizing functionality defined by add_func. Parameters: add_func (function): A function that takes two elements and returns their sum. Returns: function: A partially-applied reduce function where adding operation is predefined by add_func. return partial(reduce, add_func)"},{"question":"You have been tasked with implementing a Python program that sends out personalized email invitations for an event. Given a list of invitees, the program should generate unique email IDs for each invitation, parse their provided addresses to extract names and email addresses, format these addresses appropriately, and timestamp the invitations in both local time and UTC. To achieve this, you need to employ various utilities from the `email.utils` module. # Implementation Details: 1. **Function Name**: `generate_email_invitations` 2. **Input**: - `invitees` (list of tuples): A list where each element is a tuple containing the invitee\'s email address and an optional domain name (e.g., `[(\\"John Doe <john.doe@example.com>\\"), (\\"Jane <jane@example.org>\\", \\"otherdomain.com\\")]`). 3. **Output**: - `invitations` (list of dictionaries): A list of dictionaries where each dictionary represents an invitation containing the following keys and values: - `id`: A unique message ID (string) compliant with RFC 2822. - `name`: The real name extracted from the email address (string). - `email`: The email address extracted (string). - `formatted_address`: The formatted real name and email address (string) suitable for sending. - `timestamp_local`: Local timestamp (RFC 2822 string format). - `timestamp_utc`: UTC timestamp (RFC 2822 string format). # Constraints: - Ensure that each message ID is unique for each invitee. - Handle cases where the email address might not have a real name. # Example Usage: ```python invitees = [ \\"John Doe <john.doe@example.com>\\", \\"Jane <jane@example.org>\\", \\"no_realname@example.net\\" ] invitations = generate_email_invitations(invitees) for invitation in invitations: print(invitation) ``` # Expected Output: ```python [ { \\"id\\": \\"<unique-id-1>\\", \\"name\\": \\"John Doe\\", \\"email\\": \\"john.doe@example.com\\", \\"formatted_address\\": \\"John Doe <john.doe@example.com>\\", \\"timestamp_local\\": \\"Local time formatted string\\", \\"timestamp_utc\\": \\"UTC time formatted string\\" }, { \\"id\\": \\"<unique-id-2>\\", \\"name\\": \\"Jane\\", \\"email\\": \\"jane@example.org\\", \\"formatted_address\\": \\"Jane <jane@example.org>\\", \\"timestamp_local\\": \\"Local time formatted string\\", \\"timestamp_utc\\": \\"UTC time formatted string\\" }, { \\"id\\": \\"<unique-id-3>\\", \\"name\\": \\"\\", \\"email\\": \\"no_realname@example.net\\", \\"formatted_address\\": \\"no_realname@example.net\\", \\"timestamp_local\\": \\"Local time formatted string\\", \\"timestamp_utc\\": \\"UTC time formatted string\\" } ] ``` # Additional Notes: - Leverage the `email.utils.localtime()`, `email.utils.formatdate()`, `email.utils.make_msgid()`, `email.utils.parseaddr()`, and `email.utils.formataddr()` functions for this implementation. - Ensure proper error handling for any malformed email addresses. Happy coding and sending out those invitations!","solution":"import email.utils import time from datetime import datetime def generate_email_invitations(invitees): Generates email invitations for a list of invitees. Args: invitees (list of str): List of invitee email addresses with optional real names. Returns: list of dict: List of dictionaries, each containing invitation information. invitations = [] for invitee in invitees: real_name, email_address = email.utils.parseaddr(invitee) msg_id = email.utils.make_msgid() formatted_address = email.utils.formataddr((real_name, email_address)) # Get local time and UTC time in RFC 2822 format timestamp_local = email.utils.formatdate(time.time(), localtime=True) timestamp_utc = email.utils.formatdate(time.time(), localtime=False) invitation = { \\"id\\": msg_id, \\"name\\": real_name, \\"email\\": email_address, \\"formatted_address\\": formatted_address, \\"timestamp_local\\": timestamp_local, \\"timestamp_utc\\": timestamp_utc } invitations.append(invitation) return invitations"},{"question":"You are required to implement a function that wraps the `zipapp.create_archive` functionality and adds some additional validation and logging. The function should: - Create an archive from a given directory, adding a shebang line with a specified Python interpreter. - Allow optional compression of the archive. - Validate that the source directory contains a `__main__.py` file. - Log the steps of the process to a specified log file. # Function Signature ```python def create_executable_archive(source, target, interpreter, compress=False, log_file=\'creation.log\'): pass ``` # Parameters - **source** (str): The path to the source directory to be archived. - **target** (str): The path where the output archive will be written. - **interpreter** (str): The Python interpreter to be specified in the shebang line. - **compress** (bool, optional): Whether to compress the files in the archive. Default is `False`. - **log_file** (str, optional): The file path where log entries will be written. Default is `creation.log`. # Requirements 1. **Validation**: - Ensure that the `source` directory exists. - Ensure that the `source` directory contains a `__main__.py` file. - Raise appropriate exceptions if the validations fail. 2. **Logging**: - Log the start and end of the archive creation process. - Log any validation errors encountered. - The log entries should include timestamps. 3. **Archive Creation**: - Use `zipapp.create_archive` to create the archive. - Ensure the created archive includes a shebang line with the specified interpreter. - Apply compression if specified. # Example Usage ```python try: create_executable_archive(\'/path/to/source\', \'/path/to/output/myapp.pyz\', \'/usr/bin/env python3\', compress=True) except Exception as e: print(f\'Error: {e}\') ``` # Constraints - The function should handle and log any exceptions that occur during the archive creation process. - The log file must be created or appended to in the same directory as the script. # Example Log Output ``` 2023-10-01 12:00:00 - Starting archive creation 2023-10-01 12:00:00 - Validating source directory: /path/to/source 2023-10-01 12:00:00 - Source directory validation successful 2023-10-01 12:00:00 - Creating archive: /path/to/output/myapp.pyz 2023-10-01 12:00:01 - Archive creation completed successfully ```","solution":"import os import zipapp from datetime import datetime def log_message(log_file, message): timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') with open(log_file, \'a\') as f: f.write(f\\"{timestamp} - {message}n\\") def create_executable_archive(source, target, interpreter, compress=False, log_file=\'creation.log\'): Create an executable Python archive from a source directory. Parameters: - source (str): Path to the source directory. - target (str): Path where the output archive will be written. - interpreter (str): Python interpreter for the shebang line. - compress (bool, optional): Whether to compress the archive. Default is False. - log_file (str, optional): Path for the log file. Default is \'creation.log\'. Raises: - FileNotFoundError: If the source directory or __main__.py does not exist. try: log_message(log_file, \\"Starting archive creation\\") # Validate source directory log_message(log_file, f\\"Validating source directory: {source}\\") if not os.path.isdir(source): raise FileNotFoundError(f\\"Source directory \'{source}\' does not exist\\") main_file_path = os.path.join(source, \'__main__.py\') if not os.path.isfile(main_file_path): raise FileNotFoundError(f\\"The source directory \'{source}\' does not contain a \'__main__.py\' file\\") log_message(log_file, \\"Source directory validation successful\\") # Create the archive log_message(log_file, f\\"Creating archive: {target}\\") zipapp.create_archive(source, target, interpreter=interpreter, compressed=compress) log_message(log_file, \\"Archive creation completed successfully\\") except Exception as e: log_message(log_file, f\\"Error: {e}\\") raise e"},{"question":"You are given a DataFrame containing a mix of different data types, including timezone-aware datetimes, nullable integers, and categorical data. Your task is to analyze and transform this data utilizing pandas\' specialized types. # Input: A DataFrame `df` which may look like the following example: ```python import pandas as pd import numpy as np data = { \'timestamp\': pd.to_datetime([\'2023-01-01 00:00:00+00:00\', \'2023-01-02 00:00:00+00:00\', \'2023-01-03 00:00:00+00:00\']), \'nullable_int\': pd.Series([1, 2, np.nan], dtype=\'Int64\'), \'category\': pd.Categorical([\'a\', \'b\', \'a\'], categories=[\'a\', \'b\', \'c\']) } df = pd.DataFrame(data) ``` # Tasks: 1. **Filter the DataFrame to a Specific Time Range**: - Write a function `filter_by_time(df, start, end)` that takes the DataFrame `df` and two timezone-aware datetime strings `start` and `end`. - Return a DataFrame containing only the rows where the `timestamp` is between `start` and `end` (inclusive). 2. **Fill Missing Nullable Integers**: - Write a function `fill_nullable_int(df, value)` that takes the DataFrame `df` and an integer `value`. - Return the DataFrame with all missing values in the `nullable_int` column filled with the provided `value`. 3. **Add a Categorical Column**: - Write a function `add_categorical_column(df, new_col, categories)` that takes the DataFrame `df`, a string `new_col` specifying the name of the new column, and a list `categories` representing the categories for the new column. - The new categorical column should initialize with the first category from the `categories` list. # Output: Your function implementations should match these signatures: ```python def filter_by_time(df: pd.DataFrame, start: str, end: str) -> pd.DataFrame: pass def fill_nullable_int(df: pd.DataFrame, value: int) -> pd.DataFrame: pass def add_categorical_column(df: pd.DataFrame, new_col: str, categories: list) -> pd.DataFrame: pass ``` # Constraints: - `start` and `end` will be timezone-aware datetime strings (e.g., `\'2023-01-01T00:00:00Z\'`). - `categories` will contain at least one value. # Example: Given the initial DataFrame as above: 1. `filter_by_time(df, \'2023-01-01T00:00:00Z\', \'2023-01-02T00:00:00Z\')` should return: ```plaintext timestamp nullable_int category 0 2023-01-01 00:00:00+00:00 1 a 1 2023-01-02 00:00:00+00:00 2 b ``` 2. `fill_nullable_int(df, 0)` should return: ```plaintext timestamp nullable_int category 0 2023-01-01 00:00:00+00:00 1 a 1 2023-01-02 00:00:00+00:00 2 b 2 2023-01-03 00:00:00+00:00 0 a ``` 3. `add_categorical_column(df, \'new_category\', [\'x\', \'y\', \'z\'])` should return: ```plaintext timestamp nullable_int category new_category 0 2023-01-01 00:00:00+00:00 1 a x 1 2023-01-02 00:00:00+00:00 2 b x 2 2023-01-03 00:00:00+00:00 0 a x ``` Solve the task by implementing these functions. Make sure to handle potential edge cases and utilize pandas\' specialized data types where necessary.","solution":"import pandas as pd def filter_by_time(df: pd.DataFrame, start: str, end: str) -> pd.DataFrame: start_dt = pd.to_datetime(start) end_dt = pd.to_datetime(end) filtered_df = df[(df[\'timestamp\'] >= start_dt) & (df[\'timestamp\'] <= end_dt)] return filtered_df def fill_nullable_int(df: pd.DataFrame, value: int) -> pd.DataFrame: df[\'nullable_int\'] = df[\'nullable_int\'].fillna(value) return df def add_categorical_column(df: pd.DataFrame, new_col: str, categories: list) -> pd.DataFrame: df[new_col] = pd.Categorical([categories[0]] * len(df), categories=categories) return df"},{"question":"# Problem: World Events Timeline Management You are given a list of world events with their corresponding dates and times in UTC. Your task is to implement two functions that will help manage this timeline. 1. **convert_to_timezone(events, timezone_str)** - **Input:** - `events`: A list of tuples, where each tuple contains an event description (string) and a datetime object in UTC. ```python events = [ (\\"Event 1\\", datetime.datetime(2023, 3, 1, 12, 00, tzinfo=datetime.timezone.utc)), (\\"Event 2\\", datetime.datetime(2023, 6, 5, 15, 30, tzinfo=datetime.timezone.utc)), ... ] ``` - `timezone_str`: A string representing the desired timezone in the format `±HH:MM`. - **Output:** - A list of tuples, where each tuple contains the event description and the datetime object converted to the desired timezone. - ```python [ (\\"Event 1\\", datetime.datetime(2023, 3, 1, 7, 00, tzinfo=<desired timezone>)), (\\"Event 2\\", datetime.datetime(2023, 6, 5, 10, 30, tzinfo=<desired timezone>)), ... ] ``` 2. **filter_events_by_date(events, start_date, end_date)** - **Input:** - `events`: A list of tuples, where each tuple contains an event description (string) and a date object. ```python events = [ (\\"Event 1\\", datetime.date(2023, 3, 1)), (\\"Event 2\\", datetime.date(2023, 6, 5)), ... ] ``` - `start_date`: A date object representing the start of the date range. - `end_date`: A date object representing the end of the date range. - **Output:** - A list of tuples, where each tuple contains the event description and the date object of the events that fall within the specified date range. # Constraints: - The `timezone_str` will always be in the format `±HH:MM`. - `start_date` and `end_date` will be valid date objects. - `events` will have at most 10,000 entries. # Example: ```python import datetime events_utc = [ (\\"Event 1\\", datetime.datetime(2023, 3, 1, 12, 0, tzinfo=datetime.timezone.utc)), (\\"Event 2\\", datetime.datetime(2023, 6, 5, 15, 30, tzinfo=datetime.timezone.utc)) ] timezone_str = \\"+05:30\\" converted_events = convert_to_timezone(events_utc, timezone_str) # Output: [ # (\\"Event 1\\", datetime.datetime(2023, 3, 1, 17, 30, tzinfo=datetime.timezone(datetime.timedelta(hours=5, minutes=30)))), # (\\"Event 2\\", datetime.datetime(2023, 6, 5, 21, 0, tzinfo=datetime.timezone(datetime.timedelta(hours=5, minutes=30)))) # ] events_dates = [ (\\"Event 1\\", datetime.date(2023, 3, 1)), (\\"Event 2\\", datetime.date(2023, 6, 5)) ] start_date = datetime.date(2023, 3, 1) end_date = datetime.date(2023, 5, 1) filtered_events = filter_events_by_date(events_dates, start_date, end_date) # Output: [(\\"Event 1\\", datetime.date(2023, 3, 1))] ``` You need to complete the implementation of the functions `convert_to_timezone(events, timezone_str)` and `filter_events_by_date(events, start_date, end_date)`. # Function Signatures: ```python def convert_to_timezone(events, timezone_str): # Your code here def filter_events_by_date(events, start_date, end_date): # Your code here ```","solution":"import datetime def convert_to_timezone(events, timezone_str): Converts the datetime of events from UTC to the specified timezone. Args: - events: A list of tuples, where each tuple contains an event description (string) and a datetime object in UTC. - timezone_str: A string representing the desired timezone in the format ±HH:MM. Returns: - A list of tuples, where each tuple contains the event description and the datetime object converted to the desired timezone. # Extract hours and minutes from the timezone string sign = 1 if timezone_str[0] == \'+\' else -1 hours = int(timezone_str[1:3]) minutes = int(timezone_str[4:6]) # Create the timedelta for the desired timezone timezone_offset = datetime.timedelta(hours=hours * sign, minutes=minutes * sign) desired_timezone = datetime.timezone(timezone_offset) # Convert each event to the desired timezone converted_events = [ (description, event_datetime.astimezone(desired_timezone)) for description, event_datetime in events ] return converted_events def filter_events_by_date(events, start_date, end_date): Filters events to include only those that fall within the specified date range. Args: - events: A list of tuples, where each tuple contains an event description (string) and a date object. - start_date: A date object representing the start of the date range. - end_date: A date object representing the end of the date range. Returns: - A list of tuples, where each tuple contains the event description and the date object of the events that fall within the specified date range. filtered_events = [ (description, event_date) for description, event_date in events if start_date <= event_date <= end_date ] return filtered_events"},{"question":"# Objective Demonstrate your understanding of the Python import system, including dynamic imports and module reloading. Use the `importlib` module, which is recommended over the deprecated `imp` module. # Problem Statement Write a Python function `dynamic_import(module_name: str) -> Any` that dynamically imports a module by its name and returns the module object. If the module is already imported, reload it to reflect any changes made to the module\'s source code before returning the module object. Your implementation should make use of the `importlib` module. # Function Signature ```python def dynamic_import(module_name: str) -> Any: ``` # Input - `module_name` (str): The name of the module to import or reload. # Output - The function should return the module object. # Constraints - You may assume that the module name provided is a valid module and exists in the Python environment. - You may not use the deprecated `imp` module. - You should use `importlib` for the import and reload processes. # Example ```python module = dynamic_import(\'math\') print(module.sqrt(4)) # Output: 2.0 # Assume we have temporarily edited the \'example_module\' to add or change a function. module = dynamic_import(\'example_module\') print(module.some_function()) # Output reflects the changes in \'example_module\' ``` # Notes - Ensure that your function handles any necessary cleanup or resource management. - Consider edge cases where the module does not exist, and handle exceptions appropriately. - This question assesses your ability to work with the import system in Python, particularly focusing on dynamic behavior and reloading modules to reflect changes.","solution":"import importlib import sys from typing import Any def dynamic_import(module_name: str) -> Any: Dynamically imports a module by its name and returns the module object. If the module is already imported, reload it to reflect any changes. :param module_name: Name of the module to import or reload. :return: The module object. if module_name in sys.modules: # Module is already imported, reload it return importlib.reload(sys.modules[module_name]) else: # Module is not imported yet, import it return importlib.import_module(module_name)"},{"question":"**Question:** You are tasked with creating a command-line utility to compress and decompress text files using the bzip2 compression algorithm, leveraging Python\'s `bz2` module. The utility should handle both single-shot and incremental (de)compression based on the user\'s preference. # Requirements: 1. **Function 1: compress_file(input_filename: str, output_filename: str, compresslevel: int = 9)** - Reads the content from `input_filename`, compresses it using the bzip2 algorithm, and writes the compressed data to `output_filename`. - The `compresslevel` parameter controls the level of compression (integer between 1 and 9, default is 9). 2. **Function 2: decompress_file(input_filename: str, output_filename: str)** - Reads compressed data from `input_filename`, decompresses it, and writes the original content to `output_filename`. 3. **Function 3: incremental_compress(input_data: bytes, chunk_size: int = 1024, compresslevel: int = 9) -> bytes** - Performs incremental compression on `input_data` with chunks of size `chunk_size` bytes. - Returns the compressed data. 4. **Function 4: incremental_decompress(input_data: bytes, chunk_size: int = 1024) -> bytes** - Performs incremental decompression on `input_data` with chunks of size `chunk_size` bytes. - Returns the decompressed data. # Input/Output Formats: - Function 1 & 2: - Input: A string representing the filename (input and output), and an integer for the compression level (only for `compress_file` function). - Output: None. The function performs file read/write operations. - Function 3 & 4: - Input: A bytes object representing the data to be (de)compressed and an integer for the chunk size. - Output: A bytes object representing the (de)compressed data. # Constraints: - Ensure the utility can handle large files efficiently by reading and writing in chunks when necessary. - Performance is key. The incremental functions should not load the entire data into memory all at once. - The utility should handle errors gracefully, such as file not found or unsupported modes. # Example Usage: ```python # Example of using the functions for compressing and decompressing files compress_file(\'example.txt\', \'example.txt.bz2\', compresslevel=5) decompress_file(\'example.txt.bz2\', \'example_decompressed.txt\') # Example of using incremental compression and decompression data = b\\"Example data that will be compressed and decompressed incrementally.\\" compressed_data = incremental_compress(data, chunk_size=10, compresslevel=5) decompressed_data = incremental_decompress(compressed_data, chunk_size=10) assert data == decompressed_data # Should be True if (de)compression works correctly. ``` Implement these functions in Python, ensuring they meet the specified requirements and constraints.","solution":"import bz2 def compress_file(input_filename: str, output_filename: str, compresslevel: int = 9): with open(input_filename, \'rb\') as input_file: data = input_file.read() compressed_data = bz2.compress(data, compresslevel) with open(output_filename, \'wb\') as output_file: output_file.write(compressed_data) def decompress_file(input_filename: str, output_filename: str): with open(input_filename, \'rb\') as input_file: compressed_data = input_file.read() data = bz2.decompress(compressed_data) with open(output_filename, \'wb\') as output_file: output_file.write(data) def incremental_compress(input_data: bytes, chunk_size: int = 1024, compresslevel: int = 9) -> bytes: compressor = bz2.BZ2Compressor(compresslevel) compressed_data = b\'\' for i in range(0, len(input_data), chunk_size): chunk = input_data[i:i + chunk_size] compressed_data += compressor.compress(chunk) compressed_data += compressor.flush() return compressed_data def incremental_decompress(input_data: bytes, chunk_size: int = 1024) -> bytes: decompressor = bz2.BZ2Decompressor() decompressed_data = b\'\' for i in range(0, len(input_data), chunk_size): chunk = input_data[i:i + chunk_size] decompressed_data += decompressor.decompress(chunk) return decompressed_data"},{"question":"Question: Configuring Options and DataFrame Manipulation in Pandas # Objective In this task, you are required to demonstrate proficiency with pandas by solving a configuration and data manipulation problem. You will utilize pandas to configure global settings, manipulate data within DataFrames, and output specific analytical results based on these configurations and manipulations. # Instructions 1. **Configuration**: - Configure pandas to display a maximum of 10 rows when printing DataFrames. - Set an option to display a maximum of 3 columns when printing DataFrames. 2. **DataFrame Operations**: - Load the given CSV data into a pandas DataFrame. The CSV data is stored in a string format (use `pd.read_csv(io.StringIO(csv_data))`). - The CSV data contains columns for \\"Name\\", \\"Age\\", \\"Salary\\", and \\"Department\\". - Perform the following operations: - Filter out rows where the \\"Age\\" is less than 30. - Calculate the average salary for each department. - Format the salary numbers to display with a dollar sign and commas (e.g., 1,000.00). # Input 1. A string containing CSV data: ``` Name,Age,Salary,Department John Doe,28,75000,Engineering Jane Smith,34,85000,Marketing Emily Davis,29,54000,HR Michael Brown,40,120000,Engineering Nancy Clark,22,32000,HR Chris Evans,31,98000,Finance Megan Lee,27,49000,Marketing ``` 2. Use the `pandas` and `io` modules for handling data. # Output 1. Print the DataFrame showing the filtered rows. 2. Print the average salary for each department. 3. Print the DataFrame with salaries formatted as specified. # Constraints - Ensure the modifications and configurations maintain the specified limits on the display of rows and columns. - Performance should handle data efficiently even if the dataset size increases. # Example Solution ```python import pandas as pd import io # Configuration Settings pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 3) # Loading data csv_data = Name,Age,Salary,Department John Doe,28,75000,Engineering Jane Smith,34,85000,Marketing Emily Davis,29,54000,HR Michael Brown,40,120000,Engineering Nancy Clark,22,32000,HR Chris Evans,31,98000,Finance Megan Lee,27,49000,Marketing data = pd.read_csv(io.StringIO(csv_data)) # Filter rows where Age < 30 filtered_data = data[data[\'Age\'] >= 30] print(\\"Filtered DataFrame:n\\", filtered_data) # Calculate average salary per department average_salary = filtered_data.groupby(\'Department\')[\'Salary\'].mean().reset_index() print(\\"nAverage Salary per Department:n\\", average_salary) # Format salary column filtered_data[\'Salary\'] = filtered_data[\'Salary\'].apply(lambda x: f\\"{x:,.2f}\\") print(\\"nDataFrame with Formatted Salary:n\\", filtered_data) ``` # Note - Ensure your solution adheres to the specified display constraints while performing the operations. - If additional libraries are needed for specific tasks (e.g., formatting), make sure they are imported and used appropriately.","solution":"import pandas as pd import io # Configuration Settings pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 3) # Loading data csv_data = Name,Age,Salary,Department John Doe,28,75000,Engineering Jane Smith,34,85000,Marketing Emily Davis,29,54000,HR Michael Brown,40,120000,Engineering Nancy Clark,22,32000,HR Chris Evans,31,98000,Finance Megan Lee,27,49000,Marketing data = pd.read_csv(io.StringIO(csv_data)) # Filter rows where Age < 30 filtered_data = data[data[\'Age\'] >= 30] print(\\"Filtered DataFrame:n\\", filtered_data) # Calculate average salary per department average_salary = filtered_data.groupby(\'Department\')[\'Salary\'].mean().reset_index() print(\\"nAverage Salary per Department:n\\", average_salary) # Format salary column filtered_data[\'Salary\'] = filtered_data[\'Salary\'].apply(lambda x: f\\"{x:,.2f}\\") print(\\"nDataFrame with Formatted Salary:n\\", filtered_data)"},{"question":"# Command Line Configuration Parser You are tasked with creating a command-line configuration parser for a hypothetical utility application. The application should support the following command-line options: - `-h` or `--help`: Display a help message and exit. - `-v` or `--verbose`: Enable verbose mode. - `-o OUTPUT` or `--output=OUTPUT`: Specify the output file. - `--config=FILE`: Specify a configuration file. - `-d DIRECTORY` or `--directory=DIRECTORY`: Specify the directory for processing. Write a Python function `parse_arguments` that takes a list of arguments `args` (excluding the program name) and returns a dictionary with the parsed options and their corresponding values. If `-h` or `--help` is present, the function should print a help message and exit. If an invalid option is encountered or an option that requires an argument is given none, the function should print an appropriate error message and exit. Function Signature ```python def parse_arguments(args: list) -> dict: pass ``` Input - `args`: A list of command-line arguments (excluding the program name). Output - Returns a dictionary with the parsed options and their corresponding values. Constraints - The function should handle errors gracefully by printing error messages and exiting. - You may not use the `argparse` module; only use `getopt` as described. - You should imitate a Unix-like behavior where non-option arguments stop the parsing of options. Example Usage and Behavior Suppose your function is called with the following arguments: ```python args = [\\"-v\\", \\"--output=result.txt\\", \\"--config=myconfig.cfg\\", \\"-d\\", \\"/home/user/data\\"] ``` Your function should return: ```python { \\"verbose\\": True, \\"output\\": \\"result.txt\\", \\"config\\": \\"myconfig.cfg\\", \\"directory\\": \\"/home/user/data\\" } ``` If invalid options or missing required option arguments are provided, your function should print an appropriate error message and exit: ```python args = [\\"-x\\"] # Output: \\"option -x not recognized\\" args = [\\"-o\\"] # Output: \\"option -o requires argument\\" ``` Implement the function `parse_arguments` to meet the above requirements.","solution":"import getopt import sys def parse_arguments(args): Parse command line arguments. Args: args (list): List of command line arguments excluding the program name. Returns: dict: A dictionary with parsed command line options and their values. opts = { \'help\': False, \'verbose\': False, \'output\': None, \'config\': None, \'directory\': None, } try: options, remainder = getopt.getopt(args, \'hvo:d:\', [\'help\', \'verbose\', \'output=\', \'config=\', \'directory=\']) except getopt.GetoptError as err: print(str(err)) sys.exit(2) for opt, arg in options: if opt in (\'-h\', \'--help\'): opts[\'help\'] = True print(\\"Usage: utility [options]n\\") print(\\"Options:\\") print(\\" -h, --help Show help message and exit\\") print(\\" -v, --verbose Enable verbose mode\\") print(\\" -o OUTPUT, --output=OUTPUT Specify the output file\\") print(\\" --config=FILE Specify a configuration file\\") print(\\" -d DIRECTORY, --directory=DIRECTORY Specify the directory for processing\\") sys.exit() elif opt in (\'-v\', \'--verbose\'): opts[\'verbose\'] = True elif opt in (\'-o\', \'--output\'): opts[\'output\'] = arg elif opt == \'--config\': opts[\'config\'] = arg elif opt in (\'-d\', \'--directory\'): opts[\'directory\'] = arg return opts"},{"question":"Objective Write a function that uses seaborn to create and customize relational plots, demonstrating a wide range of the `relplot` functionality as described. Function Signature ```python def create_custom_relplot(data: str, x: str, y: str, **kwargs: dict) -> None: Creates and customizes a relational plot using seaborn based on the provided parameters. Parameters: data (str): The name of the dataset to load using seaborn\'s load_dataset function. x (str): The name of the column to be used for x-axis. y (str): The name of the column to be used for y-axis. kwargs (dict): Additional keyword arguments to customize the `relplot` function. Includes: - kind (str): The kind of plot to draw (\\"scatter\\" or \\"line\\"). - hue (str): Grouping variable that will produce points with different colors. - size (str): Grouping variable that will produce points with different sizes. - style (str): Grouping variable that will produce points with different markers. - col (str): Categorical variable that will produce faceted subplots arranged in columns. - row (str): Categorical variable that will produce faceted subplots arranged in rows. - palette (list): List of colors for the plot. - sizes (tuple): Minimum and maximum size for plotting points. - col_wrap (int): Wrap columns after this number of faceting. - height (float): Height of each facet. - aspect (float): Aspect ratio of each facet. Returns: None ``` Constraints - The function should use `seaborn` and `matplotlib.pyplot`, and it should load datasets from seaborn. - The function should handle and plot various relational aspects and provide clear visualizations. - If the dataset name is invalid or columns provided for `x` or `y` do not exist in the dataset, raise a `ValueError`. Example Usage ```python # Creating a scatter plot of \\"total_bill\\" vs. \\"tip\\" from the \\"tips\\" dataset, colored by \\"day\\" create_custom_relplot(\\"tips\\", \\"total_bill\\", \\"tip\\", kind=\\"scatter\\", hue=\\"day\\") # Creating a line plot of \\"timepoint\\" vs. \\"signal\\" from the \\"fmri\\" dataset, colored by \\"event\\" create_custom_relplot(\\"fmri\\", \\"timepoint\\", \\"signal\\", kind=\\"line\\", hue=\\"event\\", col=\\"region\\", height=4, aspect=.7) # Creating a complex scatter plot with additional semantic mappings and faceting create_custom_relplot(\\"tips\\", \\"total_bill\\", \\"tip\\", kind=\\"scatter\\", hue=\\"time\\", size=\\"size\\", style=\\"sex\\", col=\\"time\\", row=\\"sex\\", palette=[\\"b\\", \\"r\\"], sizes=(10, 100)) ``` Notes - Ensure to include appropriate error handling. - Customize the plot using additional seaborn settings and `FacetGrid` methods as needed for clarity and aesthetics.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_relplot(data: str, x: str, y: str, **kwargs: dict) -> None: Creates and customizes a relational plot using seaborn based on the provided parameters. Parameters: data (str): The name of the dataset to load using seaborn\'s load_dataset function. x (str): The name of the column to be used for x-axis. y (str): The name of the column to be used for y-axis. kwargs (dict): Additional keyword arguments to customize the `relplot` function. Includes: - kind (str): The kind of plot to draw (\\"scatter\\" or \\"line\\"). - hue (str): Grouping variable that will produce points with different colors. - size (str): Grouping variable that will produce points with different sizes. - style (str): Grouping variable that will produce points with different markers. - col (str): Categorical variable that will produce faceted subplots arranged in columns. - row (str): Categorical variable that will produce faceted subplots arranged in rows. - palette (list): List of colors for the plot. - sizes (tuple): Minimum and maximum size for plotting points. - col_wrap (int): Wrap columns after this number of faceting. - height (float): Height of each facet. - aspect (float): Aspect ratio of each facet. Returns: None # Load the dataset try: dataset = sns.load_dataset(data) except: raise ValueError(\\"Dataset could not be loaded. Please make sure the dataset name is valid.\\") # Check if x and y columns exist in the dataset if x not in dataset.columns or y not in dataset.columns: raise ValueError(f\\"One or both of the specified columns \'{x}\' or \'{y}\' do not exist in the dataset.\\") # Create the relational plot sns.relplot(data=dataset, x=x, y=y, **kwargs) plt.show()"},{"question":"# Advanced Seaborn Question Objective You are tasked with visualizing and comparing the distributions of a dataset using Seaborn\'s KDE plot features. You should demonstrate the ability to handle diverse scenario requirements, including but not limited to the following: kernel density estimates for univariate and bivariate data, hue mapping, weight application, and customizing plot aesthetics. Your final visualization should be embedded within a subplot grid for comprehensive data presentation. Problem Statement Write a Python function `plot_kde_distributions` that takes a dataset name (string) and visualizes the kernel density estimates using Seaborn within a 2x2 grid of subplots. The dataset to be used is specified by the `dataset_name` parameter and should be preloaded using the `sns.load_dataset` function. Your visualization must meet the following specifications: 1. **Top-Left Plot**: A univariate KDE plot for the \'total_bill\' column, with `hue` mapping based on the \'time\' variable. This plot should have a filled area under the KDE curve. 2. **Top-Right Plot**: A bivariate KDE plot for the \'total_bill\' and \'tip\' columns. Use a `contour` fill and the `viridis` color palette. 3. **Bottom-Left Plot**: A univariate KDE plot for the \'total_bill\' column with less smoothing (`bw_adjust=0.3`) and weight each point by the \'size\' column. Include log scaling for the x-axis. 4. **Bottom-Right Plot**: A cumulative KDE plot of \'total_bill\' with hue mapping by \'day\', along with aesthetic modifications such as `alpha=0.5` and linewidth set to 2. Input - `dataset_name` (str): The name of the dataset to load using `sns.load_dataset`. Expects: \'tips\', \'iris\', \'diamonds\', or \'geyser\'. Output - A 2x2 grid of KDE plots meeting the specified requirements. Constraints - The dataset should be loaded using `sns.load_dataset`. - Ensure the KDE plots handle the `total_bill` and `tip` columns for the \'tips\' dataset. If other datasets are specified, default to example columns that exist. Example Usage ```python plot_kde_distributions(\'tips\') ``` This should produce a 2x2 grid of KDE plots correctly displaying as per the above specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_kde_distributions(dataset_name): Visualizes the kernel density estimates using Seaborn within a 2x2 grid of subplots. Parameters: dataset_name (str): The name of the dataset to load using sns.load_dataset. # Load the dataset data = sns.load_dataset(dataset_name) # Create a 2x2 grid of subplots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Top-Left Plot: Univariate KDE plot for \'total_bill\' with hue based on \'time\' with filled area sns.kdeplot(data=data, x=\'total_bill\', hue=\'time\', fill=True, ax=axes[0, 0]) axes[0, 0].set_title(\'KDE with hue by time\') # Top-Right Plot: Bivariate KDE plot for \'total_bill\' and \'tip\' with contour fill and viridis palette sns.kdeplot(data=data, x=\'total_bill\', y=\'tip\', fill=True, cmap=\'viridis\', ax=axes[0, 1]) axes[0, 1].set_title(\'Bivariate KDE for total_bill and tip\') # Bottom-Left Plot: Univariate KDE plot for \'total_bill\' with less smoothing and weights by \'size\', log scaling sns.kdeplot(data=data, x=\'total_bill\', weights=\'size\', bw_adjust=0.3, ax=axes[1, 0]) axes[1, 0].set_xscale(\'log\') axes[1, 0].set_title(\'KDE with weights and log scale\') # Bottom-Right Plot: Cumulative KDE plot of \'total_bill\' with hue by \'day\', alpha=0.5, linewidth=2 sns.kdeplot(data=data, x=\'total_bill\', hue=\'day\', cumulative=True, alpha=0.5, linewidth=2, ax=axes[1, 1]) axes[1, 1].set_title(\'Cumulative KDE with hue by day\') # Adjust layout plt.tight_layout() plt.show()"},{"question":"# Advanced CSV Manipulation with Python\'s `csv` Module Problem Statement You are tasked with analyzing and processing CSV data from different sources. Your goal is to read the data from a CSV file, process it according to specific requirements, and write the processed data to a new CSV file. Requirements 1. Implement a function `analyze_and_write_csv(input_file: str, output_file: str, delimiter: str = \',\', quotechar: str = \'\\"\', has_header: bool = True)` that performs the following tasks: - Reads the CSV data from `input_file`. - If `has_header` is `True`, the first row is treated as the header. - Computes the average of numeric values in each column (ignore non-numeric data). - Adds a new row at the end of the file containing these averages. - Writes the modified data to `output_file` using the specified `delimiter` and `quotechar`. Constraints - You can assume the CSV file contains well-formatted data. - Handle potential `csv.Error` exceptions during the reading/writing process. - Numeric values should be treated as floats. - If a column contains only non-numeric values, the corresponding average should be stored as `None`. Function Signature ```python def analyze_and_write_csv(input_file: str, output_file: str, delimiter: str = \',\', quotechar: str = \'\\"\', has_header: bool = True) -> None: pass ``` Example Given `input_file.csv` with content: ``` name,age,salary Alice,30,70000 Bob,25,60000 Charlie,35,80000 ``` Running `analyze_and_write_csv(\'input_file.csv\', \'output_file.csv\')` should result in `output_file.csv` with content: ``` name,age,salary Alice,30,70000 Bob,25,60000 Charlie,35,80000 ,30.0,70000.0 ``` For a CSV without a header: Given `input_file.csv` with content: ``` Alice,30,70000 Bob,25,60000 Charlie,35,80000 ``` Running `analyze_and_write_csv(\'input_file.csv\', \'output_file.csv\', has_header=False)` should append the averages to the `output_file.csv`: ``` Alice,30,70000 Bob,25,60000 Charlie,35,80000 ,30.0,70000.0 ``` Notes - This question assesses the understanding of file I/O, exception handling, and data processing using Python\'s `csv` module. - Pay attention to handling different delimiters, quote characters, and the presence or absence of headers.","solution":"import csv from statistics import mean def analyze_and_write_csv(input_file: str, output_file: str, delimiter: str = \',\', quotechar: str = \'\\"\', has_header: bool = True) -> None: try: # Open the input file to read with open(input_file, mode=\'r\', newline=\'\') as infile: reader = csv.reader(infile, delimiter=delimiter, quotechar=quotechar) # Read the input data into a list rows = list(reader) # If there is a header, store it if has_header: header = rows[0] data_rows = rows[1:] else: header = [] data_rows = rows # Initialize a list to store column-wise values columns = [] for _ in range(len(data_rows[0])): columns.append([]) # Fill the columns with data for row in data_rows: for i, value in enumerate(row): try: columns[i].append(float(value)) except ValueError: columns[i].append(None) # Calculate the average for each column averages = [] for col in columns: numeric_values = [val for val in col if val is not None] if numeric_values: averages.append(mean(numeric_values)) else: averages.append(None) # Convert averages to strings averages = [\\"{:.1f}\\".format(avg) if avg is not None else \\"\\" for avg in averages] # Open the output file to write with open(output_file, mode=\'w\', newline=\'\') as outfile: writer = csv.writer(outfile, delimiter=delimiter, quotechar=quotechar, quoting=csv.QUOTE_MINIMAL) # Write the header if it exists if has_header: writer.writerow(header) # Write the original data writer.writerows(data_rows) # Write the averages as the final row writer.writerow(averages) except csv.Error as e: print(f\\"Error processing CSV file: {e}\\")"},{"question":"# Seaborn Heatmap Customization and Data Visualization Objective You are to create a Python function using the seaborn library to generate and customize a heatmap from a given dataset. Function Signature ```python def custom_heatmap(data: pd.DataFrame) -> plt.Axes: ``` Inputs - `data`: A pandas DataFrame where each cell contains a numerical value representing scores, and the indices and columns are set as labels of the rows and columns respectively. Requirements 1. **Data Preparation**: - Ensure the DataFrame has no missing values. If there are any, fill them using the mean value of each column. - Normalize the scores in the DataFrame to be between 0 and 1. 2. **Heatmap Customization**: - Generate a heatmap from the normalized DataFrame. - Annotate each cell with its corresponding value formatted to one decimal place. - Use a colormap of your choice. Explain your choice in comments. - Make sure to add lines between the cells for better separation. - Set the colormap norm such that it spans from 0 to 1. - Rotate the x-axis labels to 45 degrees for better readability. 3. **Further Customization**: - Add the title `\'Normalized Heatmap\'`. - Add an appropriate xlabel and ylabel. - Tweak the x-axis to show the labels at the top of the heatmap rather than the bottom. Output - Returns a `matplotlib.axes.Axes` object of the customized heatmap. Constraints - You should use matplotlib and seaborn for visualization. - Handle large datasets efficiently; assume the input DataFrame can have up to 10,000 rows and columns. Example ```python import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns # Sample DataFrame data = pd.DataFrame( np.random.rand(10, 10), index=[f\'Row{i}\' for i in range(1, 11)], columns=[f\'Col{i}\' for i in range(1, 11)] ) # Generating and displaying the heatmap ax = custom_heatmap(data) plt.show() ``` Ensure your code is well-commented and follows best practices for readability and efficiency.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns def custom_heatmap(data: pd.DataFrame) -> plt.Axes: Creates a customized heatmap from the given DataFrame. Parameters: data (pd.DataFrame): A DataFrame where each cell contains a numerical value. Returns: plt.Axes: The Axes object of the heatmap. # Ensure the DataFrame has no missing values data_filled = data.fillna(data.mean()) # Normalize the scores to be between 0 and 1 data_normalized = (data_filled - data_filled.min()) / (data_filled.max() - data_filled.min()) # Create the heatmap plt.figure(figsize=(10, 8)) # Using the \'viridis\' colormap for better perceptual uniformity heatmap = sns.heatmap(data_normalized, annot=True, fmt=\\".1f\\", cmap=\'viridis\', linewidths=0.5, linecolor=\'white\', cbar_kws={\\"shrink\\": 0.75}, vmin=0, vmax=1) # Customize the heatmap as specified heatmap.set_title(\'Normalized Heatmap\', fontsize=16) heatmap.set_xlabel(\'Columns\', fontsize=14) heatmap.set_ylabel(\'Rows\', fontsize=14) # Rotate the x-axis labels and move them to the top heatmap.xaxis.set_tick_params(rotation=45) heatmap.xaxis.set_label_position(\'top\') heatmap.xaxis.tick_top() return heatmap"},{"question":"**Question: Sorting and Filtering Student Scores** You are tasked with processing and analyzing student scores from different subjects. The scores are stored in a list of dictionaries, with each dictionary representing a student\'s scores in various subjects. Your job is to write a function `process_scores` that performs the following operations: 1. **Filter and sort scores**: For each student, filter out scores that are below 60 and sort the remaining scores in descending order. 2. **Calculate summary statistics**: Calculate the highest score, lowest score, and average score for each student after filtering. 3. **Generate processed data**: Return a new list of dictionaries where each dictionary contains the student\'s name and their processed scores along with the summary statistics. The input list of dictionaries will have the following format: ```python students = [ {\'name\': \'Alice\', \'scores\': {\'Math\': 75, \'English\': 58, \'Science\': 92}}, {\'name\': \'Bob\', \'scores\': {\'Math\': 83, \'English\': 64, \'Science\': 70}}, {\'name\': \'Charlie\', \'scores\': {\'Math\': 55, \'English\': 91, \'Science\': 80}}, ] ``` The output should be a list of dictionaries in the following format: ```python processed_students = [ {\'name\': \'Alice\', \'scores\': {\'Math\': 75, \'Science\': 92}, \'highest\': 92, \'lowest\': 75, \'average\': 83.5}, {\'name\': \'Bob\', \'scores\': {\'Math\': 83, \'English\': 64, \'Science\': 70}, \'highest\': 83, \'lowest\': 64, \'average\': 72.33}, {\'name\': \'Charlie\', \'scores\': {\'English\': 91, \'Science\': 80}, \'highest\': 91, \'lowest\': 80, \'average\': 85.5}, ] ``` # Detailed Requirements 1. **Function Signature** ```python def process_scores(students: list) -> list: ``` 2. **Input** - `students`: A list of dictionaries. Each dictionary contains: - `\'name\'`: A string representing the student\'s name. - `\'scores\'`: A dictionary of subject names (strings) and corresponding scores (integers). 3. **Output** - Return a new list of dictionaries. Each dictionary should contain: - `\'name\'`: The student\'s name (string). - `\'scores\'`: A dictionary of filtered and sorted scores (subject names as keys and scores as values). - `\'highest\'`: The highest score after filtering (integer). - `\'lowest\'`: The lowest score after filtering (integer). - `\'average\'`: The average score after filtering (float, rounded to two decimal places). 4. **Constraints** - The scores are integers between 0 and 100. - The input list is non-empty, and each student has at least one score. # Example ```python students = [ {\'name\': \'Alice\', \'scores\': {\'Math\': 75, \'English\': 58, \'Science\': 92}}, {\'name\': \'Bob\', \'scores\': {\'Math\': 83, \'English\': 64, \'Science\': 70}}, {\'name\': \'Charlie\', \'scores\': {\'Math\': 55, \'English\': 91, \'Science\': 80}}, ] processed_students = process_scores(students) print(processed_students) # Output: # [ # {\'name\': \'Alice\', \'scores\': {\'Math\': 75, \'Science\': 92}, \'highest\': 92, \'lowest\': 75, \'average\': 83.5}, # {\'name\': \'Bob\', \'scores\': {\'Math\': 83, \'English\': 64, \'Science\': 70}, \'highest\': 83, \'lowest\': 64, \'average\': 72.33}, # {\'name\': \'Charlie\', \'scores\': {\'English\': 91, \'Science\': 80}, \'highest\': 91, \'lowest\': 80, \'average\': 85.5}, # ] ``` # Note - Use list comprehensions wherever possible to make the code concise and efficient. - Ensure that the average score is rounded to two decimal places.","solution":"def process_scores(students): result = [] for student in students: name = student[\'name\'] scores = student[\'scores\'] # Filter out scores below 60 and sort the remaining scores in descending order filtered_scores = {subject: score for subject, score in scores.items() if score >= 60} if not filtered_scores: continue # Calculate summary statistics highest = max(filtered_scores.values()) lowest = min(filtered_scores.values()) average = round(sum(filtered_scores.values()) / len(filtered_scores), 2) # Append the processed dictionary to the result list result.append({ \'name\': name, \'scores\': filtered_scores, \'highest\': highest, \'lowest\': lowest, \'average\': average }) return result"},{"question":"**Question: LRU Cache for Fibonacci Sequence** Design a function using the `functools.lru_cache` decorator to compute Fibonacci numbers efficiently. # Problem Statement The Fibonacci sequence is a series of numbers where a number is the addition of the last two numbers, starting with 0, and 1. Write a function `fibonacci(n: int) -> int` where: - `n` is a non-negative integer representing the n-th number in the Fibonacci sequence. - The function returns the n-th Fibonacci number. To optimize performance, use the `functools.lru_cache` decorator to cache the results of intermediate calculations. This will avoid redundant computations and speed up the process for subsequent calls with the same arguments. # Constraints - 0 ≤ `n` ≤ 1000 # Performance Requirements - The solution should have a time complexity of O(n) due to memoization. # Input and Output - Input: A single integer `n`. - Output: An integer representing the n-th Fibonacci number. # Example ```python >>> fibonacci(10) 55 >>> fibonacci(50) 12586269025 ``` # Additional Notes - Ensure that the LRU cache does not have a maximum size limit. - Use `functools.lru_cache` with `maxsize=None`. # Implementation ```python from functools import lru_cache @lru_cache(maxsize=None) def fibonacci(n: int) -> int: if n < 2: return n return fibonacci(n-1) + fibonacci(n-2) # Testing print(fibonacci(10)) # Output: 55 print(fibonacci(50)) # Output: 12586269025 ``` Implement the function and test it with various values of `n` to ensure that the caching mechanism improves performance for larger input sizes.","solution":"from functools import lru_cache @lru_cache(maxsize=None) def fibonacci(n: int) -> int: Returns the n-th Fibonacci number. if n < 2: return n return fibonacci(n - 1) + fibonacci(n - 2) # Example usage (these lines won\'t be in the final solution; just for reference): # print(fibonacci(10)) # Output: 55 # print(fibonacci(50)) # Output: 12586269025"},{"question":"Objective Your task is to create a Python script that uses the \\"nis\\" module to perform a series of operations on NIS maps. This will help assess your ability to interface with system libraries, handle exceptions, and manipulate dictionaries. Problem Statement 1. Write a function `lookup_key_in_map` that takes three arguments: `key`, `mapname`, and `domain` (defaulting to the system\'s default NIS domain). This function should return the value associated with the key in the specified map. If the key does not exist, catch the `nis.error` exception and return `None`. 2. Write a function `get_all_keys_and_values` that takes two arguments: `mapname` and `domain` (defaulting to the system\'s default NIS domain). This function should return a dictionary containing all keys and values in the specified map. If the map does not exist, catch the `nis.error` exception and return an empty dictionary. 3. Write a function `list_maps` that takes one optional argument: `domain` (defaulting to the system\'s default NIS domain). This function should return a list of all valid NIS maps in the specified domain. 4. Write a function `get_default_nis_domain` that returns the system\'s default NIS domain. Input and Output Formats - **lookup_key_in_map(key, mapname, domain=default_domain)**: - Input: `key` (str), `mapname` (str), `domain` (str, optional) - Output: `value` (bytes or None) - **get_all_keys_and_values(mapname, domain=default_domain)**: - Input: `mapname` (str), `domain` (str, optional) - Output: `dict` where keys and values are of type `bytes` - **list_maps(domain=default_domain)**: - Input: `domain` (str, optional) - Output: `list` of strings - **get_default_nis_domain()**: - Input: None - Output: `str` Example Usage ```python try: # Assuming \'passwd\' is a map and \'guest\' is a key in that map. result = lookup_key_in_map(\'guest\', \'passwd\') print(result) # Example output: b\'guest:x:405:100:Guest:/home/guest:/bin/sh\' keys_values = get_all_keys_and_values(\'passwd\') print(keys_values) # Example output: {b\'root\': b\'root:x:0:0:root:/root:/bin/bash\', ...} maps = list_maps() print(maps) # Example output: [\'passwd\', \'group\', \'hosts\', ...] default_domain = get_default_nis_domain() print(default_domain) # Example output: \'example.com\' except nis.error as e: print(f\\"NIS error occurred: {e}\\") ``` Constraints - Ensure your functions handle exceptions gracefully and return the appropriate values as specified. - All keys and values in the maps are arbitrary byte arrays and should be handled as such. You may test your functions on an actual Unix system with NIS configured, but be aware that results will depend on the specific NIS setup of your environment.","solution":"import nis def lookup_key_in_map(key, mapname, domain=None): Returns the value associated with the key in the specified NIS map. try: return nis.match(key, mapname, domain) except nis.error: return None def get_all_keys_and_values(mapname, domain=None): Returns a dictionary containing all keys and values in the specified NIS map. try: return nis.cat(mapname, domain) except nis.error: return {} def list_maps(domain=None): Returns a list of all valid NIS maps in the specified domain. try: return nis.maps(domain) except nis.error: return [] def get_default_nis_domain(): Returns the system\'s default NIS domain. return nis.get_default_domain()"},{"question":"# **Coding Question: Custom URL Data Fetcher with Advanced Error Handling** **Objective:** Write a Python function using the `urllib` package that fetches data from a given URL and handles various scenarios such as custom headers, response codes, exceptions, and timeouts. **Function Signature:** ```python def fetch_data(url: str, headers: dict = None, timeout: int = 10) -> dict: pass ``` **Inputs:** - `url` (str): The URL to fetch data from. - `headers` (dict, optional): A dictionary of HTTP headers to send with the request. Default is `None`. - `timeout` (int, optional): The timeout in seconds for the network request. Default is `10` seconds. **Outputs:** - A dictionary containing: - `status_code` (int): The HTTP status code of the response. - `data` (str or None): The body of the response as a string, if the request is successful. Otherwise, `None`. - `error` (str or None): An error message, if an exception occurs. Otherwise, `None`. **Requirements:** 1. If the request succeeds (status code 2xx), return the `status_code` and the response body `data`. 2. If a client error (status code 4xx) or server error (status code 5xx) occurs, return the `status_code` and a suitable error message in `error`. 3. If a network-related or HTTP-related error occurs (e.g., connection issues, timeouts), handle the exception and return an appropriate error message in `error`. 4. Implement the specified timeout for the request. **Example:** ```python # Example function call results = fetch_data(\\"http://www.example.com\\", headers={\\"User-Agent\\": \\"Mozilla/5.0\\"}, timeout=5) # Possible output format if request succeeds { \\"status_code\\": 200, \\"data\\": \\"<html>...</html>\\", \\"error\\": None } # Possible output format if client error occurs { \\"status_code\\": 404, \\"data\\": None, \\"error\\": \\"Not Found\\" } # Possible output format if network-related error occurs { \\"status_code\\": None, \\"data\\": None, \\"error\\": \\"Failed to reach server: [Errno -2] Name or service not known\\" } ``` **Notes:** - For simplicity, assume the URL will always be HTTP or HTTPS. - Ensure you handle the most common HTTP status codes and network errors gracefully. **Hints:** - Utilize `urllib.request.Request` for creating the request object. - Handle HTTP errors using `urllib.error.HTTPError`. - Handle URL errors using `urllib.error.URLError`. - Use the `socket` module to set a default timeout for compatibility with `urllib`. Implement the function carefully to ensure robust error handling and compliance with the requirements.","solution":"import urllib.request import urllib.error import socket def fetch_data(url: str, headers: dict = None, timeout: int = 10) -> dict: Fetches data from a given URL with specified headers and timeout. Parameters: url (str): The URL to fetch data from. headers (dict, optional): A dictionary of HTTP headers to send with the request. Default is None. timeout (int, optional): The timeout in seconds for the network request. Default is 10 seconds. Returns: dict: A dictionary containing \'status_code\', \'data\', and \'error\'. request = urllib.request.Request(url, headers=headers or {}) try: with urllib.request.urlopen(request, timeout=timeout) as response: status_code = response.getcode() data = response.read().decode(\'utf-8\') return { \'status_code\': status_code, \'data\': data, \'error\': None } except urllib.error.HTTPError as e: return { \'status_code\': e.code, \'data\': None, \'error\': e.reason } except urllib.error.URLError as e: return { \'status_code\': None, \'data\': None, \'error\': str(e.reason) } except socket.timeout: return { \'status_code\': None, \'data\': None, \'error\': \\"Request timed out\\" }"},{"question":"# Advanced Python Compression and Archiving Task You are tasked with creating a Python function to compress multiple files into a single archive and then decompress that archive back into individual files. This task will demonstrate your understanding of data compression and archiving using the provided Python modules. Requirements 1. **Function Name**: `compress_and_decompress_files` 2. **Input**: - `files`: A list of strings, each representing the path to a file to be compressed. - `archive_path`: A string representing the path where the compressed archive will be saved. - `decompressed_dir`: A string representing the directory where decompressed files will be stored. 3. **Output**: - A list of paths to the files created during decompression. Function Signature ```python def compress_and_decompress_files(files: list, archive_path: str, decompressed_dir: str) -> list: pass ``` Constraints - You must use the `zipfile` module for compression and decompression. - Ensure that the generated archive adheres to the ZIP format. - Handle any potential exceptions that may arise during reading, writing, compressing, and decompressing files. - Make sure that the decompressed files retain the same directory structure and file names as the original files. Example Usage ```python files_to_compress = [\'file1.txt\', \'file2.txt\'] archive_file = \'compressed_files.zip\' output_directory = \'decompressed_files\' decompressed_files = compress_and_decompress_files(files_to_compress, archive_file, output_directory) print(decompressed_files) # Output might be: [\'decompressed_files/file1.txt\', \'decompressed_files/file2.txt\'] ``` # Performance Requirements - Ensure that the solution is efficient and scalable for a reasonable number of files (e.g., up to 100 files with average size of 1MB each). - Decompression should correctly handle any interruptions and provide meaningful error messages if a file cannot be decompressed. This question assesses your ability to effectively use Python\'s standard library for file handling, compression, and decompression while ensuring robustness and error handling.","solution":"import os import zipfile def compress_and_decompress_files(files: list, archive_path: str, decompressed_dir: str) -> list: # Create the archive try: with zipfile.ZipFile(archive_path, \'w\', zipfile.ZIP_DEFLATED) as archive: for file in files: archive.write(file, os.path.relpath(file)) except Exception as e: raise RuntimeError(f\\"Failed to compress files: {e}\\") # Create the decompression directory if it does not exist if not os.path.exists(decompressed_dir): os.makedirs(decompressed_dir) decompressed_files = [] # Decompress the archive try: with zipfile.ZipFile(archive_path, \'r\') as archive: archive.extractall(decompressed_dir) decompressed_files = archive.namelist() except Exception as e: raise RuntimeError(f\\"Failed to decompress files: {e}\\") # Return the paths to the decompressed files return [os.path.join(decompressed_dir, file) for file in decompressed_files]"},{"question":"Precise Financial Calculations You are developing a financial application where precision in calculations is crucial. You need to implement a function using Python’s `decimal` module to perform a series of financial transactions and ensure that all calculations are precise and follow specific rounding rules. Your task is to implement the function `process_transactions(transactions: List[Tuple[str, Decimal, Decimal]]) -> Decimal`. # Function Signature ```python from decimal import Decimal from typing import List, Tuple def process_transactions(transactions: List[Tuple[str, Decimal, Decimal]]) -> Decimal: ``` # Parameters - `transactions` (List[Tuple[str, Decimal, Decimal]]): A list of tuples, where each tuple represents a financial transaction. Each tuple contains: - `operation`: A string indicating the type of transaction. Possible values are `\\"deposit\\"`, `\\"withdraw\\"`, `\\"interest\\"`, `\\"fee\\"`. - `amount`: A `Decimal` representing the amount of the transaction. - `rate`: A `Decimal` representing the rate (only applicable for `\\"interest\\"` type transactions). # Returns - `Decimal`: The final balance after processing all transactions. # Constraints 1. You should initialize the balance to `Decimal(\'0.0\')`. 2. For `\\"deposit\\"` and `\\"withdraw\\"` transactions, simply add or subtract the `amount` from the balance. 3. For `\\"interest\\"` transactions, you should calculate the interest as `balance * rate` and add it to the balance. Ensure the result is rounded to two decimal places using `ROUND_HALF_EVEN`. 4. For `\\"fee\\"` transactions, subtract the `amount` from the balance. 5. Ensure that all intermediate calculations maintain a precision of 28 decimal places. # Example ```python from decimal import Decimal transactions = [ (\\"deposit\\", Decimal(\'100.00\'), Decimal(\'0.0\')), (\\"interest\\", Decimal(\'0.0\'), Decimal(\'0.05\')), (\\"fee\\", Decimal(\'1.00\'), Decimal(\'0.0\')), (\\"withdraw\\", Decimal(\'50.00\'), Decimal(\'0.0\')), ] result = process_transactions(transactions) print(result) # Output: Decimal(\'52.95\') ``` # Notes - Use the `decimal` module to maintain the required precision and rounding. - Make sure to handle possible issues such as invalid operations and rounding anomalies appropriately using the given context and signal handling from the `decimal` module.","solution":"from decimal import Decimal, getcontext, ROUND_HALF_EVEN from typing import List, Tuple def process_transactions(transactions: List[Tuple[str, Decimal, Decimal]]) -> Decimal: # Set the precision for Decimal calculations getcontext().prec = 28 balance = Decimal(\'0.0\') for operation, amount, rate in transactions: if operation == \'deposit\': balance += amount elif operation == \'withdraw\': balance -= amount elif operation == \'interest\': interest = balance * rate interest = interest.quantize(Decimal(\'0.01\'), rounding=ROUND_HALF_EVEN) balance += interest elif operation == \'fee\': balance -= amount else: raise ValueError(f\\"Invalid transaction type: {operation}\\") return balance"},{"question":"You are required to compute a large matrix multiplication using Python\'s `multiprocessing` module efficiently. Parallelize the computation using multiple processes and shared memory, ensuring proper synchronization to avoid race conditions. # Requirements 1. **Function Implementation**: - **Function Name**: `parallel_matrix_multiplication` - **Arguments**: - `matrix_A`: List of lists, where each sublist represents a row of matrix A (must be a square matrix). - `matrix_B`: List of lists, where each sublist represents a row of matrix B (must be a square matrix). - `num_processes`: Integer representing the number of processes to be used. - **Returns**: - The resulting matrix after multiplication (list of lists). 2. Implement the multiplication using: - `Process` class to create multiple worker processes. - Synchronize and share data between processes using `Value` and/or `Array`. - Use a `Manager` object to facilitate the return of the computed matrix chunks from each process. 3. Ensure your implementation correctly synchronizes the shared resources to prevent race conditions. # Detailed Steps 1. **Input Validation**: - Matrix should be square and non-empty. - `num_processes` should be a positive integer not exceeding the number of rows in the matrices. 2. **Matrix Multiplication Logic**: - Divide the work evenly among the available processes. - Each process should compute part of the resulting matrix and store it in a shared memory structure. - Use synchronization primitives to control access to shared resources. 3. **Combining Results**: - Combine the results from all processes into the final resultant matrix. # Example ```python from multiprocessing import Process, Manager, Array def parallel_matrix_multiplication(matrix_A, matrix_B, num_processes): pass if __name__ == \'__main__\': matrix_A = [ [1, 2, 3], [4, 5, 6], [7, 8, 9] ] matrix_B = [ [1, 0, 0], [0, 1, 0], [0, 0, 1] ] result = parallel_matrix_multiplication(matrix_A, matrix_B, 2) print(result) # Expected output: # [ # [1, 2, 3], # [4, 5, 6], # [7, 8, 9] # ] ``` # Constraints - Matrix dimensions will be between 1x1 and 100x100. - Number of processes (`num_processes`) will be between 1 and the number of rows of the matrix. # Performance Requirements - Ensure the solution is optimized to handle the upper bounds efficiently.","solution":"from multiprocessing import Process, Manager import math def worker(matrix_A, matrix_B, result, start_row, end_row): Worker function to compute the rows of the product matrix from start_row to end_row. n = len(matrix_A) for i in range(start_row, end_row): for j in range(n): result[i][j] = sum(matrix_A[i][k] * matrix_B[k][j] for k in range(n)) def parallel_matrix_multiplication(matrix_A, matrix_B, num_processes): Multiplies two square matrices in parallel using the specified number of processes. if not matrix_A or not matrix_B: raise ValueError(\\"Matrices cannot be empty\\") if len(matrix_A) != len(matrix_B): raise ValueError(\\"Matrices must be square and of the same size\\") if num_processes <= 0 or num_processes > len(matrix_A): raise ValueError(\\"Invalid number of processes\\") n = len(matrix_A) manager = Manager() result = manager.list([manager.list([0] * n) for _ in range(n)]) processes = [] rows_per_process = math.ceil(n / num_processes) for i in range(num_processes): start_row = i * rows_per_process end_row = min((i + 1) * rows_per_process, n) p = Process(target=worker, args=(matrix_A, matrix_B, result, start_row, end_row)) processes.append(p) p.start() for p in processes: p.join() return [list(row) for row in result]"},{"question":"**Title: Asyncio Concurrency and Debugging** **Objective**: To assess the student\'s understanding of asyncio\'s concurrency mechanisms, enabling debug mode, running blocking code in an executor, and handling potential pitfalls such as unawaited coroutines and unhandled exceptions. **Problem Statement**: You are tasked with developing a Python application using asyncio to simulate multiple concurrent tasks. The application should perform as follows: 1. Simulate concurrent tasks that fetch data from a URL and process it. 2. Ensure that any CPU-bound processing is offloaded to a separate thread to avoid blocking the event loop. 3. Enable debug mode to detect any potential issues and log detailed information about long-running callbacks and selector I/O operations. 4. Handle scenarios where some coroutines might not be awaited or exceptions might not be retrieved. Your task is to implement and test the following requirements: 1. **Function to Fetch and Process Data**: Write an asynchronous function `fetch_and_process_data(url: str) -> str` that simulates fetching data from a URL and processing it. The data fetching should be simulated by `asyncio.sleep`, and the processing (a CPU-bound task) should involve calculating the factorial of a given number, which is offloaded to a separate thread using `loop.run_in_executor`. 2. **Main Coroutine**: Write a main coroutine `main(urls: List[str]) -> None` that: - Schedules multiple `fetch_and_process_data` tasks concurrently for a list of URLs. - Properly handles any exceptions that occur during the execution. - Uses asyncio’s debug mode to log insights about coroutine performance. 3. **Debugging and Logging**: Ensure that the asyncio debug mode is enabled, and the implementation checks for unawaited coroutines and unhandled exceptions. Set the logging level to DEBUG. **Function Signatures**: ```python import asyncio import logging from typing import List async def fetch_and_process_data(url: str) -> str: pass async def main(urls: List[str]) -> None: pass ``` **Input and Output**: - `fetch_and_process_data(url: str) -> str`: Takes a URL as input and returns a processed result as a string. - `main(urls: List[str]) -> None`: Takes a list of URLs and schedules the `fetch_and_process_data` tasks concurrently. **Constraints**: 1. The factorial calculation should be performed asynchronously without blocking the event loop. 2. Logging must be configured to capture debug-level messages. 3. Ensure proper handling of unawaited coroutines and unhandled exceptions using asyncio’s debug mode. **Example**: ```python import asyncio import logging from typing import List async def fetch_and_process_data(url: str) -> str: # Simulate fetching data await asyncio.sleep(1) # Offload CPU-bound task to a separate thread factorial_result = await asyncio.get_event_loop().run_in_executor(None, lambda: factorial(10)) return f\\"Data from {url} processed with factorial {factorial_result}\\" async def main(urls: List[str]) -> None: logging.basicConfig(level=logging.DEBUG) asyncio.get_event_loop().set_debug(True) tasks = [asyncio.create_task(fetch_and_process_data(url)) for url in urls] for task in tasks: try: await task except Exception as e: logging.error(\\"Exception: %s\\", e) def factorial(n: int) -> int: if n == 0: return 1 else: return n * factorial(n-1) urls = [\\"http://example.com/data1\\", \\"http://example.com/data2\\"] asyncio.run(main(urls)) ``` **Performance Requirements**: - The function should handle at least 10 URLs efficiently. - The debug mode should be able to identify coroutines that are not awaited and log any long-running callbacks. **Notes**: - Students are encouraged to read the provided documentation on asyncio and its functionalities. - The solution will be evaluated based on correctness, performance, and adherence to asyncio’s best practices.","solution":"import asyncio import logging from typing import List def factorial(n: int) -> int: if n == 0: return 1 else: return n * factorial(n-1) async def fetch_and_process_data(url: str) -> str: # Simulate fetching data await asyncio.sleep(1) # Offload CPU-bound task to a separate thread loop = asyncio.get_event_loop() factorial_result = await loop.run_in_executor(None, factorial, 10) return f\\"Data from {url} processed with factorial {factorial_result}\\" async def main(urls: List[str]) -> None: logging.basicConfig(level=logging.DEBUG) asyncio.get_event_loop().set_debug(True) tasks = [asyncio.create_task(fetch_and_process_data(url)) for url in urls] for task in tasks: try: result = await task logging.info(result) except Exception as e: logging.error(\\"Exception: %s\\", e)"},{"question":"Directory Comparison and Synchronization Objective Write a Python function called `synchronize_dirs` that synchronizes two directories by ensuring that all files present in one directory are also present in the other. The function should use the `filecmp` module for comparison and handle files and directories recursively. Function Signature ```python def synchronize_dirs(dir1: str, dir2: str) -> None: Synchronizes dir2 to be the same as dir1 by copying missing and differing files from dir1 to dir2, and removing extra files from dir2. Parameters: - dir1 (str): Path to the source directory. - dir2 (str): Path to the target directory. Returns: - None ``` Requirements 1. **Input Parameters**: - `dir1`: A string representing the path to the source directory. - `dir2`: A string representing the path to the target directory. 2. **Output**: - The function does not return any value. It modifies `dir2` to match `dir1`. 3. **Constraints**: - If a file in `dir1` differs from a file with the same name in `dir2`, the file from `dir1` should overwrite the file in `dir2`. - Any file or directory that exists in `dir2` but not in `dir1` should be removed from `dir2`. - The function should handle nested directories and synchronize them recursively. 4. **Performance**: - The function should efficiently handle large directories with potentially thousands of files and nested subdirectories. 5. **Hints**: - Use the `filecmp.dircmp` class for comparing directories. - Use the `shutil` module for copying and removing files and directories. Example Usage ```python import os def synchronize_dirs(dir1: str, dir2: str) -> None: import filecmp import shutil def sync_helper(dcmp): # Copy files from dir1 to dir2 for name in dcmp.left_only: src_path = os.path.join(dcmp.left, name) dst_path = os.path.join(dcmp.right, name) if os.path.isdir(src_path): shutil.copytree(src_path, dst_path) else: shutil.copy2(src_path, dst_path) # Remove files from dir2 that are not in dir1 for name in dcmp.right_only: path = os.path.join(dcmp.right, name) if os.path.isdir(path): shutil.rmtree(path) else: os.remove(path) # Overwrite differing files from dir1 to dir2 for name in dcmp.diff_files: src_path = os.path.join(dcmp.left, name) dst_path = os.path.join(dcmp.right, name) shutil.copy2(src_path, dst_path) # Sync subdirectories for sub_dcmp in dcmp.subdirs.values(): sync_helper(sub_dcmp) dcmp = filecmp.dircmp(dir1, dir2) sync_helper(dcmp) # Given two directories \'source_dir\' and \'target_dir\', call: synchronize_dirs(\'source_dir\', \'target_dir\') ``` **Note**: The sample solution above provides a scaffold to help you get started. You are expected to write the complete function implementation independently.","solution":"import os import filecmp import shutil def synchronize_dirs(dir1: str, dir2: str) -> None: Synchronizes dir2 to be the same as dir1 by copying missing and differing files from dir1 to dir2, and removing extra files from dir2. Parameters: - dir1 (str): Path to the source directory. - dir2 (str): Path to the target directory. Returns: - None def sync_helper(dcmp): # Copy files from dir1 to dir2 for name in dcmp.left_only: src_path = os.path.join(dcmp.left, name) dst_path = os.path.join(dcmp.right, name) if os.path.isdir(src_path): shutil.copytree(src_path, dst_path) else: shutil.copy2(src_path, dst_path) # Remove files and directories from dir2 that are not in dir1 for name in dcmp.right_only: path = os.path.join(dcmp.right, name) if os.path.isdir(path): shutil.rmtree(path) else: os.remove(path) # Overwrite differing files from dir1 to dir2 for name in dcmp.diff_files: src_path = os.path.join(dcmp.left, name) dst_path = os.path.join(dcmp.right, name) shutil.copy2(src_path, dst_path) # Recursively synchronize subdirectories for sub_dcmp in dcmp.subdirs.values(): sync_helper(sub_dcmp) dcmp = filecmp.dircmp(dir1, dir2) sync_helper(dcmp)"},{"question":"**Coding Assessment Question: Advanced Configuration Parsing** **Objective:** Implement a function that reads a configuration file, applies certain transformations and customizations, and then outputs specific values based on provided keys. This will assess your understanding of advanced features in the `configparser` module. **Description:** You need to write a function `custom_config_parser` that processes a given INI configuration file and extracts values based on specified keys. The function should also handle default fallback values and customize parsing behavior using interpolation. # Function Signature ```python def custom_config_parser(config_text: str, section: str, keys: List[str], default_values: Dict[str, str]) -> Dict[str, str]: pass ``` # Parameters: - `config_text` (str): A string containing the contents of the INI configuration file. - `section` (str): The name of the section from which to read the configuration values. - `keys` (List[str]): A list of keys whose values should be extracted from the given section. - `default_values` (Dict[str, str]): A dictionary containing default values if the key does not exist in the given section. # Expected Output: - The function should return a dictionary with the keys and their corresponding values. If a key is not found in the given section, use the value from `default_values`. # Constraints and Requirements: - Assume the configuration file is always correctly formatted. - If a specified key does not exist in both the section and the `default_values`, the key should not appear in the output dictionary. - You should use interpolation to handle placeholders within the values. # Example Input: ```python config_text = [General] app_version = 1.0.0 app_name = SampleApp file_path = /usr/local/sample [Database] host = localhost port = 3306 username = user password = pass section = \\"Database\\" keys = [\\"host\\", \\"port\\", \\"username\\", \\"password\\", \\"table\\"] default_values = {\\"table\\" : \\"default_table\\", \\"port\\": \\"5432\\"} result = custom_config_parser(config_text, section, keys, default_values) print(result) ``` # Expected Output: ```python { \\"host\\": \\"localhost\\", \\"port\\": \\"3306\\", \\"username\\": \\"user\\", \\"password\\": \\"pass\\", \\"table\\": \\"default_table\\" } ``` # Guidelines: 1. Use the `configparser` module to parse the INI file. 2. Implement functionality for interpolation to handle placeholders within values. 3. Properly handle missing keys by referring to `default_values`. This exercise will test your ability to implement a real-world application of configuration file parsing and the use of advanced features in the `configparser` module.","solution":"from typing import List, Dict import configparser def custom_config_parser(config_text: str, section: str, keys: List[str], default_values: Dict[str, str]) -> Dict[str, str]: Processes a given INI configuration file and extracts values based on specified keys, using default values if keys are not present in the section. Args: config_text (str): A string containing the contents of the INI configuration file. section (str): The name of the section from which to read the configuration values. keys (List[str]): A list of keys whose values should be extracted from the given section. default_values (Dict[str, str]): A dictionary containing default values if the key does not exist in the given section. Returns: Dict[str, str]: A dictionary with the keys and their corresponding values. config = configparser.ConfigParser() config.read_string(config_text) result = {} for key in keys: if config.has_option(section, key): result[key] = config.get(section, key) elif key in default_values: result[key] = default_values[key] return result"},{"question":"**Problem Statement:** You are developing a secure file verification system. The system will generate a hash for each file and store these hashes in a repository. Later, when the files need to be verified, the system will re-compute the hash for each file and compare it with the stored hash to ensure that the file has not been altered. **Requirements:** 1. Create a function `generate_file_hash(file_path: str, algorithm: str = \'sha256\') -> str` that takes a file path and an optional hashing algorithm (default to \'sha256\') and returns the hexadecimal representation of the hash of the file\'s contents. The function should handle the following algorithms: \'sha256\', \'sha512\', and \'blake2b\'. 2. Create a function `verify_file_integrity(file_path: str, stored_hash: str, algorithm: str = \'sha256\') -> bool` that takes a file path, a stored hash, and an optional hashing algorithm (default to \'sha256\'). The function should return `True` if the file\'s current hash matches the stored hash, and `False` otherwise. **Function Signatures:** ```python def generate_file_hash(file_path: str, algorithm: str = \'sha256\') -> str: # Implement this function def verify_file_integrity(file_path: str, stored_hash: str, algorithm: str = \'sha256\') -> bool: # Implement this function ``` **Constraints:** - The file to be hashed can be large, so ensure that you read the file in chunks to avoid memory issues. - You can assume the file exists at the given `file_path` and is readable. **Example Usage:** ```python # Example file content: Hello, world! # Generate SHA-256 hash for the file file_hash = generate_file_hash(\\"example.txt\\", \\"sha256\\") print(file_hash) # Output would be the SHA-256 hash of the file content # Verify the file integrity with the stored hash is_valid = verify_file_integrity(\\"example.txt\\", file_hash, \\"sha256\\") print(is_valid) # Output would be True # Verify the file integrity with a different hash (should be False) is_valid = verify_file_integrity(\\"example.txt\\", \\"0000000000000000000000000000000000000000000000000000000000000000\\", \\"sha256\\") print(is_valid) # Output would be False ``` **Note:** - Utilize the `hashlib` library for hash computation. - Handle possible errors gracefully, such as unsupported algorithms.","solution":"import hashlib def generate_file_hash(file_path: str, algorithm: str = \'sha256\') -> str: Generate the hash of a file\'s contents using the specified algorithm. Args: - file_path (str): The path to the file to hash. - algorithm (str): The hashing algorithm to use (\'sha256\', \'sha512\', \'blake2b\'). Returns: - str: The hexadecimal representation of the file\'s hash. hash_func = getattr(hashlib, algorithm.lower(), None) if hash_func is None: raise ValueError(f\\"Unsupported hash algorithm: {algorithm}\\") file_hash = hash_func() with open(file_path, \'rb\') as file: while chunk := file.read(8192): file_hash.update(chunk) return file_hash.hexdigest() def verify_file_integrity(file_path: str, stored_hash: str, algorithm: str = \'sha256\') -> bool: Verify the integrity of a file by comparing its current hash to the stored hash. Args: - file_path (str): The path to the file to verify. - stored_hash (str): The stored hash to compare against. - algorithm (str): The hashing algorithm to use (\'sha256\', \'sha512\', \'blake2b\'). Returns: - bool: True if the file\'s current hash matches the stored hash, False otherwise. current_hash = generate_file_hash(file_path, algorithm) return current_hash == stored_hash"},{"question":"**Objective:** Write a function that takes a list of error numbers (`errno` values) and returns a dictionary where each key is the error number, and the value is a tuple containing the error name from `errno.errorcode` and its corresponding error message using `os.strerror()`. **Function Signature:** ```python def get_error_details(err_numbers: list) -> dict: pass ``` # Input: - `err_numbers` (list): A list of integers representing error numbers. # Output: - (dict): A dictionary where each key is an error number (from the input list), and the value is a tuple containing the error name (from `errno.errorcode`) and the corresponding error message using `os.strerror()`. # Constraints: 1. Each integer in `err_numbers` should correspond to a valid error number available in `errno.errorcode`. 2. The length of `err_numbers` will not exceed 1000. # Example: ```python input_err_numbers = [2, 17, 13] # Sample error numbers corresponding to ENOENT, EEXIST, and EACCES result = get_error_details(input_err_numbers) ``` Expected output: ```python { 2: (\'ENOENT\', \'No such file or directory\'), 17: (\'EEXIST\', \'File exists\'), 13: (\'EACCES\', \'Permission denied\') } ``` # Additional Notes: 1. Utilize the `errno` module\'s `errorcode` dictionary to translate error numbers into error names. 2. Use `os.strerror()` to retrieve the system\'s error message for each error number. 3. Assume that the input will only include valid error numbers known to the `errno` module. # Explanation: - This assessment checks students\' understanding of importing and using standard libraries (`errno` and `os`), dictionary comprehensions, error handling, and function implementation.","solution":"import errno import os def get_error_details(err_numbers: list) -> dict: Takes a list of error numbers and returns a dictionary where each key is an error number, and the value is a tuple containing the error name and its corresponding error message. error_details = {} for num in err_numbers: error_name = errno.errorcode.get(num, \'Unknown error\') error_message = os.strerror(num) error_details[num] = (error_name, error_message) return error_details"},{"question":"**Objective**: The objective of this question is to assess your understanding of Python\'s `atexit` module and your ability to use it in implementing cleanup functions that are executed when the Python interpreter terminates. **Problem Statement**: You are required to write a Python program that simulates a simple inventory management system. The program should: 1. Allow adding items with their quantity to the inventory. 2. Save the inventory data to a file upon normal program termination. 3. Ensure that the inventory is loaded from the file when the program starts. 4. Provide a way to gracefully handle the inventory save operation using the `atexit` module. **Detailed Requirements**: 1. **Inventory Management Functions**: - Implement a function `add_item(item: str, quantity: int) -> None` that adds an item and its quantity to the inventory. If the item already exists, increment its quantity. 2. **File Handling**: - The inventory should be stored in a file named `inventory.txt`. When the program starts, it should read the inventory data from this file (if it exists) and initialize the inventory. - Implement a function `save_inventory() -> None` that saves the current inventory to `inventory.txt`. Use the `atexit` module to register this function so that it is called when the program terminates normally. 3. **Graceful Termination**: - Utilize `atexit.register()` to ensure that `save_inventory()` is called at program termination. - Handle potential file reading and writing exceptions gracefully. 4. **Input and Output**: - Function `add_item(item: str, quantity: int) -> None`: Adds the specified item and quantity to the inventory. - Function `save_inventory() -> None`: Saves the current state of the inventory to `inventory.txt`. - Function `load_inventory() -> None`: Loads the inventory from `inventory.txt` when the program starts. **Constraints**: - Assume that the file `inventory.txt` will use a simple key-value format: each line will be of the form `item_name:quantity`. - Handle the possible `FileNotFoundError` when attempting to read `inventory.txt` on program start. **Example**: ```python # File: inventory_manager.py import atexit inventory = {} def load_inventory(): global inventory try: with open(\'inventory.txt\', \'r\') as file: for line in file: item, qty = line.strip().split(\':\') inventory[item] = int(qty) except FileNotFoundError: inventory = {} def save_inventory(): global inventory with open(\'inventory.txt\', \'w\') as file: for item, qty in inventory.items(): file.write(f\'{item}:{qty}n\') def add_item(item, quantity): global inventory if item in inventory: inventory[item] += quantity else: inventory[item] = quantity # Register the save_inventory function to be called at program exit atexit.register(save_inventory) if __name__ == \\"__main__\\": load_inventory() # Example usage add_item(\\"apple\\", 10) add_item(\\"banana\\", 5) # Add more interactions as needed ``` In this problem, you should implement the functions `add_item`, `save_inventory`, and `load_inventory` as described, and ensure that `save_inventory` is registered to run at program exit using the `atexit` module.","solution":"import atexit inventory = {} def load_inventory(): global inventory try: with open(\'inventory.txt\', \'r\') as file: for line in file: item, qty = line.strip().split(\':\') inventory[item] = int(qty) except FileNotFoundError: inventory = {} def save_inventory(): global inventory with open(\'inventory.txt\', \'w\') as file: for item, qty in inventory.items(): file.write(f\'{item}:{qty}n\') def add_item(item, quantity): global inventory if item in inventory: inventory[item] += quantity else: inventory[item] = quantity # Register the save_inventory function to be called at program exit atexit.register(save_inventory) if __name__ == \\"__main__\\": load_inventory() # Example usage add_item(\\"apple\\", 10) add_item(\\"banana\\", 5) # Add more interactions as needed"},{"question":"Contextual and Multi-Destination Logging You are tasked with developing a Python application that logs events to multiple destinations with contextual information. The application needs to maintain different log formats for console and file logging, and handle logging from multiple threads efficiently. Requirements: 1. **Logger Configuration**: - Configure a root logger that logs messages to both a console and a file. - The console logger should only display messages of level `INFO` and above. - The file logger should record all messages of level `DEBUG` and above into \'app.log\' with a file size limit of 5MB and a backup count of 3. 2. **Contextual Logging**: - Log contextual information including a session ID and user ID with every log message. 3. **Multi-threaded Logging**: - Implement logging in a multi-threaded environment where multiple threads log messages concurrently. 4. **Custom Logging Format**: - Use different log formats for console and file logging: - Console format: `%(asctime)s - %(levelname)s - %(message)s` - File format: `%(asctime)s - %(name)s - %(levelname)s - [Session ID: %(session_id)s, User ID: %(user_id)s] - %(message)s` Instructions: 1. Implement a function `setup_logger(session_id: str, user_id: str)` that configures the logger: - Parameters: - `session_id` (str): The session ID for the current logging context. - `user_id` (str): The user ID for the current logging context. - Returns: - A configured logger instance. 2. Implement a class `LogContext` to manage contextual information (session ID and user ID). 3. Implement a function `log_from_multiple_threads(logger)` which: - Spawns 3 threads. - Logs 5 messages from each thread with varying levels of severity. **Example Usage**: ```python if __name__ == \\"__main__\\": user_id = \'user_123\' session_id = \'session_789\' logger = setup_logger(session_id, user_id) log_from_multiple_threads(logger) ``` Constraints: - Ensure that the solution handles thread safety for logging. - Do not use any global variables. Evaluation Criteria: - Correct setup and configuration of the logger. - Proper implementation of contextual logging. - Correct handling and demonstration of multi-threaded logging. - Adherence to the specified log formats for console and file handling. - Code quality and readability.","solution":"import logging import threading from logging.handlers import RotatingFileHandler import time class LogContext: Manages the logging context (session ID and user ID). def __init__(self, session_id: str, user_id: str): self.session_id = session_id self.user_id = user_id def add_context_to_logger(self, logger): Adds contextual information to the logger. log_formatter = logging.Formatter( \'%(asctime)s - %(name)s - %(levelname)s - \' \'[Session ID: %(session_id)s, User ID: %(user_id)s] - %(message)s\' ) for handler in logger.handlers: if isinstance(handler, RotatingFileHandler): handler.setFormatter(log_formatter) handler.addFilter(self) def filter(self, record): Adds session ID and user ID to each log record. record.session_id = self.session_id record.user_id = self.user_id return True def setup_logger(session_id: str, user_id: str): Configures the logger instance. logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Create handlers console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) file_handler = RotatingFileHandler(\'app.log\', maxBytes=5*1024*1024, backupCount=3) file_handler.setLevel(logging.DEBUG) # Create formatters and add them to handlers console_format = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') file_format = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - \' \'[Session ID: %(session_id)s, User ID: %(user_id)s] - %(message)s\') console_handler.setFormatter(console_format) file_handler.setFormatter(file_format) # Add handlers to the logger logger.addHandler(console_handler) logger.addHandler(file_handler) # Add context to the logger context = LogContext(session_id, user_id) context.add_context_to_logger(logger) return logger def log_from_multiple_threads(logger): Logs messages from multiple threads. def thread_logging(name): for i in range(5): if i % 2 == 0: logger.info(f\\"Info log from {name}: message {i}\\") else: logger.debug(f\\"Debug log from {name}: message {i}\\") time.sleep(1) threads = [] for i in range(3): thread = threading.Thread(target=thread_logging, args=(f\\"Thread-{i}\\",)) threads.append(thread) thread.start() for thread in threads: thread.join() if __name__ == \\"__main__\\": user_id = \'user_123\' session_id = \'session_789\' logger = setup_logger(session_id, user_id) log_from_multiple_threads(logger)"},{"question":"Advanced MultiIndex Operations Objective: To assess your understanding of creating and manipulating MultiIndexes in pandas and performing advanced indexing and selection operations. Question: You are given sales data for a retail store that sells various product categories in multiple regions. The data is provided as a list of dictionaries, where each dictionary represents a sale record with the details: \'Region\', \'Category\', \'Product\', \'Quantity\', and \'Revenue\'. Here is the sales data: ```python sales_data = [ {\'Region\': \'North\', \'Category\': \'Electronics\', \'Product\': \'TV\', \'Quantity\': 10, \'Revenue\': 5000}, {\'Region\': \'North\', \'Category\': \'Electronics\', \'Product\': \'Laptop\', \'Quantity\': 5, \'Revenue\': 7000}, {\'Region\': \'North\', \'Category\': \'Home Appliances\', \'Product\': \'Refrigerator\', \'Quantity\': 2, \'Revenue\': 1500}, {\'Region\': \'South\', \'Category\': \'Electronics\', \'Product\': \'TV\', \'Quantity\': 7, \'Revenue\': 3500}, {\'Region\': \'South\', \'Category\': \'Home Appliances\', \'Product\': \'Washing Machine\', \'Quantity\': 4, \'Revenue\': 2000}, {\'Region\': \'East\', \'Category\': \'Furniture\', \'Product\': \'Sofa\', \'Quantity\': 3, \'Revenue\': 1200}, {\'Region\': \'West\', \'Category\': \'Furniture\', \'Product\': \'Chair\', \'Quantity\': 15, \'Revenue\': 2100}, {\'Region\': \'West\', \'Category\': \'Electronics\', \'Product\': \'Laptop\', \'Quantity\': 8, \'Revenue\': 11200} ] ``` **Tasks**: 1. Transform the data into a pandas DataFrame. 2. Create a MultiIndex for the DataFrame with \'Region\', \'Category\', and \'Product\' as the hierarchical index. 3. Select and display all the sales records for the \'North\' region. 4. Select and display the sales records for \'Electronics\' category in the \'West\' region. 5. Using the `swaplevel` function, swap \'Category\' and \'Product\' levels in the index and display the resulting DataFrame. 6. Compute and display the total quantity and revenue for each category in all regions. 7. Reorder levels of the MultiIndex to [\'Product\', \'Region\', \'Category\'] and display the resulting DataFrame. 8. Rename the index levels to [\'Geo\', \'Category_Type\', \'Item\'] and display the updated DataFrame. **Constraints**: - You may assume that the sales data is consistent with no missing or malformed records. - Ensure that the operations you perform are efficient and follow best practices for handling MultiIndex in pandas. Expected Output Format: For each task, provide the corresponding code and the resulting output in the form of a DataFrame. Ensure that the outputs are clearly labeled for each step. Example: Here is an example of how the output should be formatted for the first task: ```python # Task 1: Transform the data into a pandas DataFrame import pandas as pd sales_data = [ {\'Region\': \'North\', \'Category\': \'Electronics\', \'Product\': \'TV\', \'Quantity\': 10, \'Revenue\': 5000}, {\'Region\': \'North\', \'Category\': \'Electronics\', \'Product\': \'Laptop\', \'Quantity\': 5, \'Revenue\': 7000}, {\'Region\': \'North\', \'Category\': \'Home Appliances\', \'Product\': \'Refrigerator\', \'Quantity\': 2, \'Revenue\': 1500}, {\'Region\': \'South\', \'Category\': \'Electronics\', \'Product\': \'TV\', \'Quantity\': 7, \'Revenue\': 3500}, {\'Region\': \'South\', \'Category\': \'Home Appliances\', \'Product\': \'Washing Machine\', \'Quantity\': 4, \'Revenue\': 2000}, {\'Region\': \'East\', \'Category\': \'Furniture\', \'Product\': \'Sofa\', \'Quantity\': 3, \'Revenue\': 1200}, {\'Region\': \'West\', \'Category\': \'Furniture\', \'Product\': \'Chair\', \'Quantity\': 15, \'Revenue\': 2100}, {\'Region\': \'West\', \'Category\': \'Electronics\', \'Product\': \'Laptop\', \'Quantity\': 8, \'Revenue\': 11200} ] df = pd.DataFrame(sales_data) print(df) ``` Output: ``` Region Category Product Quantity Revenue 0 North Electronics TV 10 5000 1 North Electronics Laptop 5 7000 2 North Home Appliances Refrigerator 2 1500 3 South Electronics TV 7 3500 4 South Home Appliances Washing Machine 4 2000 5 East Furniture Sofa 3 1200 6 West Furniture Chair 15 2100 7 West Electronics Laptop 8 11200 ``` Please provide the code and output for the remaining tasks similarly.","solution":"import pandas as pd sales_data = [ {\'Region\': \'North\', \'Category\': \'Electronics\', \'Product\': \'TV\', \'Quantity\': 10, \'Revenue\': 5000}, {\'Region\': \'North\', \'Category\': \'Electronics\', \'Product\': \'Laptop\', \'Quantity\': 5, \'Revenue\': 7000}, {\'Region\': \'North\', \'Category\': \'Home Appliances\', \'Product\': \'Refrigerator\', \'Quantity\': 2, \'Revenue\': 1500}, {\'Region\': \'South\', \'Category\': \'Electronics\', \'Product\': \'TV\', \'Quantity\': 7, \'Revenue\': 3500}, {\'Region\': \'South\', \'Category\': \'Home Appliances\', \'Product\': \'Washing Machine\', \'Quantity\': 4, \'Revenue\': 2000}, {\'Region\': \'East\', \'Category\': \'Furniture\', \'Product\': \'Sofa\', \'Quantity\': 3, \'Revenue\': 1200}, {\'Region\': \'West\', \'Category\': \'Furniture\', \'Product\': \'Chair\', \'Quantity\': 15, \'Revenue\': 2100}, {\'Region\': \'West\', \'Category\': \'Electronics\', \'Product\': \'Laptop\', \'Quantity\': 8, \'Revenue\': 11200} ] # Task 1: Transform the data into a pandas DataFrame df = pd.DataFrame(sales_data) # Task 2: Create a MultiIndex for the DataFrame with \'Region\', \'Category\', and \'Product\' as the hierarchical index df.set_index([\'Region\', \'Category\', \'Product\'], inplace=True) # Task 3: Select and display all the sales records for the \'North\' region north_sales = df.loc[\'North\'] # Task 4: Select and display the sales records for \'Electronics\' category in the \'West\' region west_electronics_sales = df.loc[(\'West\', \'Electronics\')] # Task 5: Using the `swaplevel` function, swap \'Category\' and \'Product\' levels in the index and display the resulting DataFrame swapped_df = df.swaplevel(\'Category\', \'Product\') # Task 6: Compute and display the total quantity and revenue for each category in all regions category_totals = df.groupby(\'Category\').sum() # Task 7: Reorder levels of the MultiIndex to [\'Product\', \'Region\', \'Category\'] and display the resulting DataFrame reordered_df = df.reorder_levels([\'Product\', \'Region\', \'Category\']) # Task 8: Rename the index levels to [\'Geo\', \'Category_Type\', \'Item\'] and display the updated DataFrame renamed_levels_df = df.rename_axis(index={\'Region\': \'Geo\', \'Category\': \'Category_Type\', \'Product\': \'Item\'})"},{"question":"**Title:** Implementing a Variable-Length Sequence Aggregation using Nested Tensors in PyTorch **Objective:** Demonstrate your understanding of PyTorch\'s `torch.nested` module by creating a function to aggregate data from nested tensors. Specifically, this task will involve creating a function that efficiently sums variable-length sequences stored in a nested tensor. **Problem Statement:** You are given a dataset containing multiple variable-length sequences representing some sensor readings over time. For each sensor, you have a sequence of readings, and each sensor\'s sequence length may vary. Your task is to implement a function that takes these sequences, preprocesses them into a nested tensor, and returns the sum of each sequence in the form of a regular tensor. **Function Signature:** ```python import torch def aggregate_nested_tensor(sequence_list: List[torch.Tensor]) -> torch.Tensor: Aggregates variable-length sequences by summing each sequence using nested tensors. Args: sequence_list (List[torch.Tensor]): A list of 1D tensors, each representing a variable-length sequence of sensor readings. Returns: torch.Tensor: A 1D tensor containing the sum of each sequence. ``` **Input:** - `sequence_list`: A list of 1D PyTorch tensors. Each tensor represents a sequence of sensor readings, and different tensors can have different lengths. **Output:** - A 1D PyTorch tensor where the `i-th` element is the sum of the `i-th` sequence in `sequence_list`. **Constraints:** - Each tensor in `sequence_list` must be a 1-dimensional tensor. - The input list is non-empty, and each tensor has at least one element. **Example:** ```python import torch # Sample sequences sequences = [torch.tensor([1.0, 2.0, 3.0]), torch.tensor([4.0]), torch.tensor([5.0, 6.0])] # Result of summing the sequences result = aggregate_nested_tensor(sequences) print(result) # Output: tensor([ 6., 4., 11.]) ``` **Hints:** 1. Use the `torch.nested.nested_tensor` function to convert the input list into a nested tensor with a jagged layout. 2. Utilize PyTorch operations to aggregate the sequences in the nested tensor. **Evaluation Criteria:** - Correctness: Your function should correctly sum the sequences and return the expected output. - Understanding: Demonstrate knowledge of creating and manipulating nested tensors. - Efficiency: Use the nested tensor\'s capabilities effectively to handle the variable-length sequences. **Notes:** - You may import any additional PyTorch functions as needed. - Consider edge cases such as sequences containing a single element. Happy coding!","solution":"import torch import torch.nested def aggregate_nested_tensor(sequence_list): Aggregates variable-length sequences by summing each sequence using nested tensors. Args: sequence_list (list of torch.Tensor): A list of 1D tensors, each representing a variable-length sequence of sensor readings. Returns: torch.Tensor: A 1D tensor containing the sum of each sequence. # Convert list of tensors into a nested tensor nested_tensor = torch.nested.nested_tensor(sequence_list) # Get the sum of each sequence sum_tensor = torch.nested.to_padded_tensor(nested_tensor, padding=0).sum(dim=1) return sum_tensor"},{"question":"**Objective**: Evaluate and compare the performance of different machine learning models using the appropriate evaluation metrics provided by scikit-learn. **Description**: You are provided with a dataset of your choice (e.g., the Iris dataset, Boston Housing dataset, or any other publicly available dataset). Your task is to perform the following steps: 1. **Data Preparation**: - Load the dataset. - Split it into training and testing sets. 2. **Model Training**: - Train at least two different models (e.g., Logistic Regression, Random Forest) on the training data. 3. **Model Evaluation**: - Evaluate the models using at least three different evaluation metrics from the scikit-learn `metrics` module. For classification problems, include metrics such as accuracy, precision, recall, F1-score, and ROC-AUC. For regression problems, include metrics such as mean squared error, mean absolute error, and R² score. - Use cross-validation with a specified number of folds (e.g., 5-fold cross-validation) to get a more reliable evaluation. 4. **Comparison and Reporting**: - Compile the evaluation results of the models in a tabular format, showing the performance scores for each metric. - Write a brief report comparing the performance of the models based on the evaluation metrics and suggest which model performs the best and why. **Constraints and Requirements**: - Use the scikit-learn package for model training and evaluation. - The performance evaluation should handle class imbalance if present in the dataset. - Ensure your code is efficient and follows best practices for reproducibility (e.g., setting random seeds). **Deliverables**: - A Jupyter notebook or Python script containing the implementation of the above steps. - A summary report in markdown format within the Jupyter notebook or a separate text file. **Additional Information**: - You may refer to the scikit-learn documentation provided above for details on how to use the various metrics and scoring functions. **Example Dataset**: - If you choose the Iris dataset (classification problem): `sklearn.datasets.load_iris` - If you choose the Boston Housing dataset (regression problem): `sklearn.datasets.load_boston` **Expected Input and Output**: - **Input**: Any suitable dataset. - **Output**: A comparison table of model performance metrics and a summary report. **Performance Requirements**: - The code should run efficiently on a standard machine with reasonable execution time (e.g., less than 5 minutes for model training and evaluation). **Note**: Ensure that your code is well-documented and includes comments where necessary.","solution":"import pandas as pd from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score # Load the Iris dataset def load_and_prepare_data(): iris = load_iris() X, y = iris.data, iris.target # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) return X_train, X_test, y_train, y_test # Train a Logistic Regression model def train_logistic_regression(X_train, y_train): scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) log_reg = LogisticRegression(random_state=42) log_reg.fit(X_train_scaled, y_train) return log_reg, scaler # Train a Random Forest model def train_random_forest(X_train, y_train): rf = RandomForestClassifier(random_state=42) rf.fit(X_train, y_train) return rf # Evaluate the model using different metrics def evaluate_model(model, X_test, y_test, scaler=None): if scaler: X_test = scaler.transform(X_test) y_pred = model.predict(X_test) return { \'accuracy\': accuracy_score(y_test, y_pred), \'precision\': precision_score(y_test, y_pred, average=\'weighted\'), \'recall\': recall_score(y_test, y_pred, average=\'weighted\'), \'f1_score\': f1_score(y_test, y_pred, average=\'weighted\'), \'roc_auc\': roc_auc_score(y_test, model.predict_proba(X_test), multi_class=\'ovr\') } # Compile the evaluation results of the models def compile_results(log_reg_results, rf_results): results = pd.DataFrame({ \'Metric\': [\'Accuracy\', \'Precision\', \'Recall\', \'F1 Score\', \'ROC AUC\'], \'Logistic Regression\': [log_reg_results[\'accuracy\'], log_reg_results[\'precision\'], log_reg_results[\'recall\'], log_reg_results[\'f1_score\'], log_reg_results[\'roc_auc\']], \'Random Forest\': [rf_results[\'accuracy\'], rf_results[\'precision\'], rf_results[\'recall\'], rf_results[\'f1_score\'], rf_results[\'roc_auc\']] }) return results # Main function to execute the above steps def main(): X_train, X_test, y_train, y_test = load_and_prepare_data() log_reg, scaler = train_logistic_regression(X_train, y_train) rf = train_random_forest(X_train, y_train) log_reg_results = evaluate_model(log_reg, X_test, y_test, scaler=scaler) rf_results = evaluate_model(rf, X_test, y_test) results = compile_results(log_reg_results, rf_results) print(results) return results if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Assessment Question You are required to create a Python script that can compress and archive multiple files from a directory into a single archive file using the student\'s choice of compression algorithm. The script should support `zip`, `tar.gz`, and `tar.bz2` formats. Function Specification **Function Name**: `compress_and_archive` **Input**: * `source_directory` (str): The path to the directory containing files to be archived. * `destination_archive` (str): The path where the archive should be saved (including the file name and extension). * `compression_format` (str): The compression format to use, either `\\"zip\\"`, `\\"tar.gz\\"`, or `\\"tar.bz2\\"`. **Output**: None **Constraints**: * The function should handle cases where the specified `source_directory` does not exist. * If `destination_archive` file already exists, it should be overwritten. * Your implementation should ensure that all types of archival and compression formats specified are handled appropriately. # Example Usage Input: ```python compress_and_archive(\'/path/to/source_directory\', \'/path/to/archive.zip\', \'zip\') compress_and_archive(\'/path/to/source_directory\', \'/path/to/archive.tar.gz\', \'tar.gz\') compress_and_archive(\'/path/to/source_directory\', \'/path/to/archive.tar.bz2\', \'tar.bz2\') ``` Output: The output will be archives created at the specified destinations with the specified compression formats containing all files from the source directory. # Implementation Notes - Use `zipfile` module for creating and managing `.zip` files. - Use `tarfile` module for creating `.tar.gz` and `.tar.bz2` files. - Make sure to handle file operations securely, such as checking if the source directory exists and handling potential file I/O errors. - Consider performance implications when handling large files. # Sample Code Outline ```python import os import zipfile import tarfile def compress_and_archive(source_directory, destination_archive, compression_format): # Check if source directory exists if not os.path.isdir(source_directory): raise FileNotFoundError(f\\"The specified directory {source_directory} does not exist.\\") # Implement compression and archiving based on format if compression_format == \\"zip\\": with zipfile.ZipFile(destination_archive, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, dirs, files in os.walk(source_directory): for file in files: zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), source_directory)) elif compression_format in [\\"tar.gz\\", \\"tar.bz2\\"]: mode = \\"w:gz\\" if compression_format == \\"tar.gz\\" else \\"w:bz2\\" with tarfile.open(destination_archive, mode) as tarf: tarf.add(source_directory, arcname=os.path.basename(source_directory)) else: raise ValueError(\\"Unsupported compression format. Choose from \'zip\', \'tar.gz\', \'tar.bz2\'.\\") # You may add additional helper functions if necessary ``` Make sure to thoroughly test your implementation with various edge cases.","solution":"import os import zipfile import tarfile def compress_and_archive(source_directory, destination_archive, compression_format): Compresses and archives files from source_directory into a single archive file. Parameters: source_directory (str): The path to the directory containing files to be archived. destination_archive (str): The path where the archive should be saved (including the file name and extension). compression_format (str): The compression format to use, either \'zip\', \'tar.gz\', or \'tar.bz2\'. Raises: FileNotFoundError: If the specified source directory does not exist. ValueError: If an unsupported compression format is specified. # Check if source directory exists if not os.path.isdir(source_directory): raise FileNotFoundError(f\\"The specified directory {source_directory} does not exist.\\") # Implement compression and archiving based on format if compression_format == \\"zip\\": with zipfile.ZipFile(destination_archive, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, _, files in os.walk(source_directory): for file in files: file_path = os.path.join(root, file) zipf.write(file_path, os.path.relpath(file_path, source_directory)) elif compression_format in [\\"tar.gz\\", \\"tar.bz2\\"]: mode = \\"w:gz\\" if compression_format == \\"tar.gz\\" else \\"w:bz2\\" with tarfile.open(destination_archive, mode) as tarf: tarf.add(source_directory, arcname=os.path.basename(source_directory)) else: raise ValueError(\\"Unsupported compression format. Choose from \'zip\', \'tar.gz\', \'tar.bz2\'.\\")"},{"question":"# XML Parsing and Manipulation with `xml.etree.ElementTree` **Problem Statement:** You are given an XML document representing a catalog of books with multiple categories. Each book element has attributes like title, author, year, and genre. Some books also have sub-elements like reviews. Your task is to implement a function to parse this XML data, perform specific updates, and finally write an updated XML document to a file. The XML structure of the books catalog is as follows: ```xml <catalog> <book title=\\"Book 1\\" author=\\"Author A\\" year=\\"2001\\" genre=\\"Fiction\\"> <reviews> <review rating=\\"5\\">Excellent!</review> <review rating=\\"4\\">Very good</review> </reviews> </book> <book title=\\"Book 2\\" author=\\"Author B\\" year=\\"2005\\" genre=\\"Science\\"> <reviews> <review rating=\\"3\\">Average</review> </reviews> </book> <book title=\\"Book 3\\" author=\\"Author A\\" year=\\"2012\\" genre=\\"Science Fiction\\"> </book> </catalog> ``` **Tasks:** 1. **Implement the function `update_books(xml_data: str, output_filename: str) -> None`:** - Parse the given XML string into an `ElementTree`. - Add a new `price` attribute to each `book` element based on conditions: - Fiction books with reviews have a price of `20`. - Science books have a price of `15`. - All other books have a price of `10`. - If a `book` has a `review` with a rating of `5`, add an attribute `best_seller=\\"yes\\"` to that `book`. - Write the updated XML tree to the specified output file. **Input:** - `xml_data` (str): A string containing the XML data representing the books catalog. - `output_filename` (str): The name of the file to which to write the updated XML document. **Output:** - None **Constraints:** - Assume that the input XML is well-formed and no XML parsing errors will occur. - Use the `xml.etree.ElementTree` module for parsing and manipulating the XML data. **Example:** ```python xml_data = \'\'\' <catalog> <book title=\\"Book 1\\" author=\\"Author A\\" year=\\"2001\\" genre=\\"Fiction\\"> <reviews> <review rating=\\"5\\">Excellent!</review> <review rating=\\"4\\">Very good</review> </reviews> </book> <book title=\\"Book 2\\" author=\\"Author B\\" year=\\"2005\\" genre=\\"Science\\"> <reviews> <review rating=\\"3\\">Average</review> </reviews> </book> <book title=\\"Book 3\\" author=\\"Author A\\" year=\\"2012\\" genre=\\"Science Fiction\\"> </book> </catalog> \'\'\' update_books(xml_data, \'updated_catalog.xml\') ``` After executing the function, `updated_catalog.xml` should contain: ```xml <catalog> <book title=\\"Book 1\\" author=\\"Author A\\" year=\\"2001\\" genre=\\"Fiction\\" price=\\"20\\" best_seller=\\"yes\\"> <reviews> <review rating=\\"5\\">Excellent!</review> <review rating=\\"4\\">Very good</review> </reviews> </book> <book title=\\"Book 2\\" author=\\"Author B\\" year=\\"2005\\" genre=\\"Science\\" price=\\"15\\"> <reviews> <review rating=\\"3\\">Average</review> </reviews> </book> <book title=\\"Book 3\\" author=\\"Author A\\" year=\\"2012\\" genre=\\"Science Fiction\\" price=\\"10\\"> </book> </catalog> ``` **Notes:** - Make sure to handle different genres and conditions as specified. - Ensure the output XML file is well-formed and correctly formatted.","solution":"import xml.etree.ElementTree as ET def update_books(xml_data: str, output_filename: str) -> None: Parses the given XML string, updates the books based on specified conditions, and writes the updated XML to the specified filename. # Parse the XML string root = ET.fromstring(xml_data) # Iterate over each book element in the catalog for book in root.findall(\'book\'): genre = book.get(\'genre\') has_reviews = book.find(\'reviews\') is not None price = 10 # Default price # Determine the price based on conditions if genre == \'Fiction\' and has_reviews: price = 20 elif genre == \'Science\': price = 15 # Set the price attribute book.set(\'price\', str(price)) # Check for best seller condition if has_reviews: for review in book.find(\'reviews\').findall(\'review\'): if review.get(\'rating\') == \'5\': book.set(\'best_seller\', \'yes\') break # Write the updated XML to the specified file tree = ET.ElementTree(root) tree.write(output_filename, encoding=\'utf-8\', xml_declaration=True)"},{"question":"**Question: Using the \\"runpy\\" module** The `runpy` module in Python provides functions to execute Python modules and scripts without importing them. This question will test your understanding and ability to use the `runpy` module effectively. **Task: Execute a Python Script Using runpy** You are given a string representing the path to a Python script file. Your task is to write a function `execute_script(path: str) -> dict` that uses the `runpy` module to execute the script at the given path and returns the resulting module globals dictionary. Your function should: - Take a single parameter: `path` (a string representing the file path to the script). - Use the `runpy.run_path` function to execute the script. - Return the resulting globals dictionary from the execution of the script. **Constraints:** - The script file at the given path is guaranteed to exist and be a valid Python script. - The script may define variables, functions, and classes. It might also have side effects. - You should not use any other modules besides `runpy` and standard library modules required for handling paths (e.g., `os` or `pathlib`). **Input Format:** - A single string `path` representing the file path to the script. **Output Format:** - A dictionary representing the globals dictionary resulting from the script execution. **Example:** Suppose you have a script at the location `\\"test_script.py\\"` with the following content: ```python # test_script.py x = 10 y = 20 def add(a, b): return a + b ``` **Function Call:** ```python result = execute_script(\\"test_script.py\\") print(result) ``` **Expected Output:** ```python { \'__name__\': \'__main__\', \'__file__\': \'test_script.py\', \'__package__\': None, \'__cached__\': None, \'__loader__\': None, \'x\': 10, \'y\': 20, \'add\': <function add at 0x...> } ``` Your task is to implement the `execute_script` function. **Note:** The exact output may vary based on the Python version and environment, especially function memory addresses.","solution":"import runpy def execute_script(path: str) -> dict: Executes the Python script at the given path and returns the resulting globals dictionary. :param path: A string representing the file path to the script. :return: A dictionary representing the globals dictionary resulting from the script execution. return runpy.run_path(path)"},{"question":"**Objective:** Implement and use a custom email policy to handle certain constraints or requirements that may arise in email processing while utilizing the `email.policy` module in Python. **Question:** You are required to implement a custom email policy by extending `email.policy.EmailPolicy`. This custom policy will: 1. Enforce a maximum line length of 50 characters for the email headers. 2. Require the email message to use `utf-8` for headers, thereby disallowing any encoding deviations. 3. Ensure that when headers are manipulated programmatically, headers are refolded if the length of any line in the header exceeds the maximum line length (50 characters). After implementing this custom policy, write a function `apply_custom_policy` that: - Takes an email message as input. - Utilizes the custom policy to format the email message correctly according to the above rules. - Outputs the email message as a string with the correctly applied policy. **Specifications:** 1. **Define the Custom Policy:** Implement a class `CustomEmailPolicy` that inherits from `email.policy.EmailPolicy` and applies the following settings: - `max_line_length=50` - `utf8=True` - `refold_source=\'long\'` Example: ```python from email.policy import EmailPolicy class CustomEmailPolicy(EmailPolicy): def __init__(self): super().__init__(max_line_length=50, utf8=True, refold_source=\'long\') ``` 2. **Write the `apply_custom_policy` function:** This function should: - Accept an `EmailMessage` object as input. - Create a clone of the CustomEmailPolicy and apply it to the input message. - Serialize the modified message to a string, ensuring lines are properly folded. - Return the serialized string. Function prototype: ```python def apply_custom_policy(email_message: EmailMessage) -> str: pass ``` **Input:** A valid `EmailMessage` object. **Output:** A string representing the serialized email message with the custom policy applied. ```python def apply_custom_policy(email_message: EmailMessage) -> str: # Your implementation here pass # Example usage: from email.message import EmailMessage # Create example email message msg = EmailMessage() msg[\'Subject\'] = \'This is an example email message to test the custom policy implementation\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' msg.set_content(\'This is the body of the email.\') # Apply custom policy print(apply_custom_policy(msg)) ``` **Constraints:** - Ensure that any defects in the email message should raise an error (use `raise_on_defect=True`). Good luck!","solution":"from email.policy import EmailPolicy from email.message import EmailMessage class CustomEmailPolicy(EmailPolicy): def __init__(self): super().__init__(max_line_length=50, utf8=True, refold_source=\'long\') def apply_custom_policy(email_message: EmailMessage) -> str: custom_policy = CustomEmailPolicy() email_message.policy = custom_policy return email_message.as_string()"},{"question":"**Coding Question: Advanced File Processing with `fileinput`** **Objective:** Demonstrate your comprehension of the `fileinput` module by implementing a function that processes multiple text files in a specific way. The function must read lines from input files, transform the content based on given rules, and handle various file input scenarios. **Task:** Write a function `process_files(file_list: List[str], inplace: bool=False, backup: str=\'.bak\') -> None` that: 1. Takes a list of filenames `file_list`. 2. Reads each line from the provided files using `fileinput.input()`. 3. For each line, converts all characters to uppercase, replaces each occurrence of the word \\"foo\\" with \\"bar\\", and appends the line number to the end of the line. 4. If `inplace` is `True`, modifies the files in-place while creating a backup with the specified `backup` extension. 5. If `inplace` is `False`, prints the processed lines to the standard output. **Input:** - `file_list`: A list of strings, where each string is a filename. - `inplace`: A boolean flag indicating whether to modify the files in-place. - `backup`: An optional string specifying the extension for backup files when `inplace` is `True`. **Output:** - If `inplace` is `True`, the function modifies the files directly. - If `inplace` is `False`, the function prints the processed lines to the standard output. **Constraints:** - Assume all files are text files and can be opened in read mode. - Handle potential I/O errors gracefully. **Examples:** ```python # Example usage 1: Process files and print to standard output process_files([\'file1.txt\', \'file2.txt\'], inplace=False) # Example usage 2: Process files and modify them in-place process_files([\'file1.txt\', \'file2.txt\'], inplace=True, backup=\'.bak\') # Example content of \'file1.txt\': # Line 1: \\"this is foo\\" # Line 2: \\"another foo line\\" # After processing: # File1 (inplace=True): # Line 1: \\"THIS IS BAR1\\" # Line 2: \\"ANOTHER BAR LINE2\\" # # Standard output (inplace=False): # \\"THIS IS BAR1\\" # \\"ANOTHER BAR LINE2\\" ``` **Notes:** - Ensure your function is efficient and handles multiple files correctly. - Usage of appropriate `fileinput` functions and methods is required to fulfill the task. - Remember to make use of the hooks provided if necessary.","solution":"import fileinput from typing import List def process_files(file_list: List[str], inplace: bool=False, backup: str=\'.bak\') -> None: Processes the given list of files: - Converts all characters to uppercase - Replaces occurrences of the word \\"foo\\" with \\"bar\\" - Appends the line number to the end of each line - Optionally modifies the files in-place with a backup :param file_list: List of filenames to process :param inplace: Boolean flag to modify files in-place :param backup: Extension for backup files if inplace is True with fileinput.input(files=file_list, inplace=inplace, backup=backup) as f: for line in f: processed_line = line.upper().replace(\\"FOO\\", \\"BAR\\").strip() processed_line += str(fileinput.filelineno()) if inplace: print(processed_line, end=\'n\') else: print(processed_line)"},{"question":"# Question: Implement a Thread-Safe Bounded Buffer Using Semaphores and Locks In this task, you will implement a bounded buffer (also known as a circular buffer) which is a fixed-size container that supports adding and removing elements in a thread-safe manner. The buffer should handle concurrent access by multiple producer and consumer threads. Requirements 1. Implement a `BoundedBuffer` class with the following methods: - `__init__(self, capacity: int)`: Initializes the buffer with a given capacity. - `put(self, item: Any)`: Adds an item to the buffer. If the buffer is full, the method should block until space is available. - `get(self) -> Any`: Removes and returns an item from the buffer. If the buffer is empty, the method should block until an item is available. 2. Use a combination of semaphores and locks to manage access to the buffer and ensure thread safety. 3. Implement producer and consumer functions that will use the `BoundedBuffer` to add and remove items respectively. 4. Demonstrate the usage of your `BoundedBuffer` class by creating multiple producer and consumer threads. Each producer should add items to the buffer, and each consumer should remove items from the buffer. Constraints - The buffer should block producers if it is full and block consumers if it is empty. - Use the `threading` module to create and manage threads. - Ensure that the buffer operates correctly under concurrent access. - You may assume that the buffer will be used in a single Python interpreter session (no need for inter-process communication). Example Usage ```python import threading import time from typing import Any class BoundedBuffer: def __init__(self, capacity: int): # Initialize the buffer, semaphores, and locks def put(self, item: Any): # Add item to buffer, blocking if necessary def get(self) -> Any: # Remove and return item from buffer, blocking if necessary def producer(buffer: BoundedBuffer, items: list): for item in items: print(f\\"Producing {item}\\") buffer.put(item) time.sleep(0.5) def consumer(buffer: BoundedBuffer, count: int): for _ in range(count): item = buffer.get() print(f\\"Consuming {item}\\") time.sleep(1) if __name__ == \\"__main__\\": buffer = BoundedBuffer(capacity=5) items_to_produce = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] producer_thread = threading.Thread(target=producer, args=(buffer, items_to_produce)) consumer_thread = threading.Thread(target=consumer, args=(buffer, len(items_to_produce))) producer_thread.start() consumer_thread.start() producer_thread.join() consumer_thread.join() ``` This example demonstrates how producers and consumers can interact with the buffer concurrently. Your implementation should ensure that all interactions are thread-safe and that the buffer correctly handles cases where it is full or empty.","solution":"import threading from typing import Any class BoundedBuffer: def __init__(self, capacity: int): self.capacity = capacity self.buffer = [None] * capacity self.put_index = 0 self.get_index = 0 self.count = 0 self.mutex = threading.Lock() self.not_full = threading.Semaphore(capacity) self.not_empty = threading.Semaphore(0) def put(self, item: Any): self.not_full.acquire() self.mutex.acquire() try: self.buffer[self.put_index] = item self.put_index = (self.put_index + 1) % self.capacity self.count += 1 finally: self.mutex.release() self.not_empty.release() def get(self) -> Any: self.not_empty.acquire() self.mutex.acquire() try: item = self.buffer[self.get_index] self.get_index = (self.get_index + 1) % self.capacity self.count -= 1 finally: self.mutex.release() self.not_full.release() return item"},{"question":"# Seaborn Coding Assessment Question **Objective:** The objective is to assess the student\'s ability to utilize seaborn to create and customize a complex plot based on provided specifications. **Problem Statement:** Using seaborn, create a plot that visualizes the given dataset with the following requirements: 1. The dataset contains three continuous variables (`var1`, `var2`, `var3`) and three categories (`cat1`, `cat2`, and `cat3`). 2. Create a scatter plot with `var1` on the x-axis and `var2` on the y-axis. 3. Customize the scatter plot points based on the following properties: - Color of the points should represent different categories from `cat1`. - Size of the points should be based on the values of `var3`. - All points should have a marker style of \'X\'. - Points should have an edgecolor of `black` with an edgewidth of 1.5. - Apply a logarithmic scale to the x-axis. 4. Add a density plot on the same figure for the variable `var1` with respect to `var3`. 5. Apply a background theme `ticks` to the plot and remove the top and right spines. 6. Add appropriate labels to the axes and a title to the plot. 7. Add a legend to distinguish between the different categories of `cat1`. **Input Format:** A pandas DataFrame `df` with columns: `var1`, `var2`, `var3`, `cat1`, `cat2`, and `cat3`. **Output Format:** A customized seaborn plot based on the above specifications. **Constraints:** - Assume the DataFrame `df` is pre-loaded. - Use seaborn version that supports `seaborn.objects` module. - Performance considerations are minimal for this task. **Example Code Structure:** ```python import seaborn.objects as so import pandas as pd # Assuming df is your input DataFrame def create_custom_plot(df): plot = ( so.Plot(df, x=\'var1\', y=\'var2\') .add(so.Dots(color=\\"cat1\\"), edgecolor=\'black\', edgewidth=1.5, marker=\'X\', pointsize=\'var3\') .scale(x=so.Continuous(trans=\'log\')) .theme(axes_style(\'ticks\'), {\'axes.spines.top\': False, \'axes.spines.right\': False}) .label(x=\'Variable 1 (log scale)\', y=\'Variable 2\', title=\'Custom Seaborn Plot\') ) density_plot = ( so.Plot(df, x=\'var1\', bw_adjust=0.5) .add(so.KDE(), color=\\"gray\\") ) fig = plot + density_plot return fig ``` Call `create_custom_plot(df)` to generate the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(df): This function creates a customized seaborn plot according to the given specifications. # Initialize the figure and axes fig, ax = plt.subplots() # Create scatter plot with the specified customizations scatter = sns.scatterplot( data=df, x=\'var1\', y=\'var2\', hue=\'cat1\', size=\'var3\', markers=True, edgecolor=\'black\', linewidth=1.5, legend=\'full\', ax=ax, palette=\'viridis\', marker=\'X\' ) # Apply logarithmic scale to the x-axis ax.set_xscale(\'log\') # Create density plot for var1 with respect to var3 sns.kdeplot( data=df, x=\'var1\', hue=\'cat1\', fill=True, common_norm=False, palette=\'viridis\', alpha=0.3, ax=ax ) # Apply background theme \'ticks\' sns.set_theme(style=\\"ticks\\") # Remove top and right spines sns.despine(ax=ax, top=True, right=True) # Add labels and title ax.set_xlabel(\'Variable 1 (log scale)\') ax.set_ylabel(\'Variable 2\') ax.set_title(\'Custom Seaborn Plot\') # Add legend ax.legend() # Show plot plt.show()"},{"question":"Objective Write a Python function that generates a secure password based on specific requirements. The password must be alphanumeric, have a specified length, contain at least one lowercase character, one uppercase character, and one digit. Function Signature ```python def generate_secure_password(length: int) -> str: pass ``` Input - `length` (int): The length of the password to be generated. It must be at least 8 characters long. Output - Returns a string representing the secure password. Constraints and Requirements 1. The password must be exactly `length` characters long. 2. The password must contain at least: - One lowercase letter. - One uppercase letter. - One digit. 3. The password must be generated using the `secrets` module to ensure cryptographic security. 4. Raise a `ValueError` if the length is less than 8. Example ```python print(generate_secure_password(10)) # Sample Output: \'aB3dE5fGhY\' print(generate_secure_password(8)) # Sample Output: \'X5vT2qWz\' print(generate_secure_password(15)) # Sample Output: \'aB1cD2eF3gHijKl\' ``` Notes - Ensure the password meets the security constraints. - The generated password should be highly unpredictable. Caveats - Pay special attention to the randomness and security aspect while implementing the function. - Consider using loops or recursion to ensure the generated password always meets the required constraints.","solution":"import secrets import string def generate_secure_password(length: int) -> str: if length < 8: raise ValueError(\\"Password length must be at least 8 characters\\") char_pool = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(char_pool) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password)): return password"},{"question":"You are required to implement a Python function that interacts with Python\'s `PyLongObject` through the C API. Your task is to create a Python extension module that provides a function to convert a list of mixed types (like integers, floats, strings representing numbers) into a list of Python integer objects. Function Signature ```python def convert_to_pylong(mixed_list: List[Union[int, float, str]]) -> List[int]: Convert a list of mixed types (int, float, str) into a list of Python integer objects. Parameters: mixed_list (List[Union[int, float, str]]): A list containing integers, floats, and strings representing numbers. Returns: List[int]: A list of Python integer objects converted from the elements of mixed_list. ``` Detailed Requirements 1. **Input**: - The input to the function is a list of mixed types (integers, floats, and strings representing numbers). 2. **Output**: - The output should be a list of Python integer objects (equivalent to `PyLongObject`). 3. **Constraints**: - Strings in `mixed_list` must be valid representations of integers. - If a string cannot be converted to an integer, your function should raise a `ValueError` with an appropriate message. - The input list will not be empty. 4. **Performance**: - The function should handle typical integer and floating-point representations efficiently. - Assume reasonable limits on the size and length of the input list for practical implementations. 5. **Implementation Notes**: - You should use appropriate C API functions to handle the conversions. - Error handling should be consistent with the documentation, using `PyErr_Occurred` to disambiguate error states. - The `PyLong_FromString`, `PyLong_FromLong`, `PyLong_FromDouble`, and similar functions should be used as necessary. 6. **Example**: ```python # Example input list mixed_list = [10, \\"23\\", 45.67, \\"89\\"] # Expected output list # [10, 23, 45, 89] result = convert_to_pylong(mixed_list) print(result) # Output: [10, 23, 45, 89] ``` **Your Task**: - Write the Python function `convert_to_pylong` and the corresponding C extension module that fulfills the above requirements. - Follow best practices for error handling and edge cases as described in the documentation.","solution":"from typing import List, Union def convert_to_pylong(mixed_list: List[Union[int, float, str]]) -> List[int]: Convert a list of mixed types (int, float, str) into a list of Python integer objects. Parameters: mixed_list (List[Union[int, float, str]]): A list containing integers, floats, and strings representing numbers. Returns: List[int]: A list of Python integer objects converted from the elements of mixed_list. result = [] for item in mixed_list: if isinstance(item, int): result.append(item) elif isinstance(item, float): result.append(int(item)) elif isinstance(item, str): try: result.append(int(item)) except ValueError: raise ValueError(f\\"Invalid string for conversion to int: {item}\\") else: raise ValueError(f\\"Unsupported type in list: {type(item).__name__}\\") return result"},{"question":"Overview: You will write a Python function using the `torch.distributed.elastic.multiprocessing` module to start multiple worker processes, monitor their execution, and capture logs. Your task is to implement a distributed computation simulation using these utilities. Task: Implement a function `run_distributed_computation(num_workers: int, worker_func: Callable, *args, **kwargs) -> List[Tuple[int, str]]` that performs the following: 1. **Start Worker Processes**: Utilizes `torch.distributed.elastic.multiprocessing.start_processes` to start `num_workers` parallel processes, each executing `worker_func` with provided `args` and `kwargs`. 2. **Monitor and Collect Results**: Monitors the progress of each worker process, collects their respective logs, and then consolidates the results into a list with each individual\'s process ID and corresponding output log. 3. **Return Results**: Returns a list of tuples where each tuple contains a process ID and its associated log output. Input: - `num_workers`: An integer specifying the number of worker processes to launch. - `worker_func`: A callable function representing the worker task to be executed in each process. - `args`: Positional arguments to pass to `worker_func`. - `kwargs`: Keyword arguments to pass to `worker_func`. Output: - A list of tuples, where each tuple contains: - The process ID of the worker. - A string representing the log output generated by the worker. Constraints: - The worker function must be serializable (can be pickled) as it will be executed in different processes. - The solution must handle scenarios where processes may fail or terminate unexpectedly. Example: ```python def example_worker(rank): import time time.sleep(rank) # Simulates work by sleeping for `rank` seconds return f\\"Worker {rank} finished.\\" # Function to execute result = run_distributed_computation(num_workers=3, worker_func=example_worker) for pid, log in result: print(f\\"Process {pid}: {log}\\") # Expected Output: # Process <pid1>: Worker 0 finished. # Process <pid3>: Worker 2 finished. # Process <pid2>: Worker 1 finished. ``` # Note: - Ensure that your function handles logging properly and that the logs are captured for each worker process. - Forward any exceptions or errors to the main process to aid in debugging. Additional Information: Refer to the PyTorch [Elastic Multiprocessing Documentation](https://pytorch.org/docs/stable/elastic/multiprocessing.html) for more detailed descriptions of functions and classes involved.","solution":"import torch import torch.multiprocessing as mp from typing import Callable, List, Tuple def worker_wrapper(rank: int, worker_func: Callable, return_dict, *args, **kwargs): try: result = worker_func(rank, *args, **kwargs) return_dict[rank] = (rank, result) except Exception as e: return_dict[rank] = (rank, str(e)) def run_distributed_computation(num_workers: int, worker_func: Callable, *args, **kwargs) -> List[Tuple[int, str]]: manager = mp.Manager() return_dict = manager.dict() processes = [] for rank in range(num_workers): p = mp.Process(target=worker_wrapper, args=(rank, worker_func, return_dict, *args), kwargs=kwargs) p.start() processes.append(p) for p in processes: p.join() results = [return_dict[rank] for rank in range(num_workers)] return results"},{"question":"Objective Demonstrate your understanding of the `itertools` module by applying several of its functions to solve a non-trivial problem. Problem Statement You need to process a sequence of numbers representing daily temperatures. Your task is to find the longest subsequence of days where the temperatures strictly increase, followed by the longest subsequence where the temperatures strictly decrease. Write a function `find_longest_increasing_then_decreasing_subsequence` that takes a list of integers representing daily temperatures and returns a tuple containing two lists: 1. The longest subsequence of strictly increasing temperatures. 2. The longest subsequence of strictly decreasing temperatures. Requirements 1. Use functions from the `itertools` module where applicable. 2. The input list can be of any length, including non-positive lengths. 3. The function should handle edge cases gracefully, such as empty input lists. 4. Your solution should aim for efficiency, leveraging the power of iterators and generators. Function Signature ```python def find_longest_increasing_then_decreasing_subsequence(temperatures: list[int]) -> tuple[list[int], list[int]]: pass ``` Example ```python # Given input: list of daily temperatures temperatures = [30, 32, 35, 28, 27, 25, 26, 28, 29, 31, 30, 27] # Expected output: # Longest strictly increasing subsequence: [25, 26, 28, 29, 31] # Longest strictly decreasing subsequence: [35, 28, 27, 25] print(find_longest_increasing_then_decreasing_subsequence(temperatures)) # Output: ([25, 26, 28, 29, 31], [35, 28, 27, 25]) ``` Constraints - The input list, `temperatures`, consists of integers. - The function should correctly handle lists with duplicate temperatures, but only count strictly increasing or decreasing subsequences. - The solution must use at least two different functions from the `itertools` module. # Notes - Test your solution with various edge cases to ensure robustness. - Do not use external libraries other than `itertools`. Good luck, and happy coding!","solution":"from itertools import groupby def find_longest_increasing_then_decreasing_subsequence(temperatures): def longest_subsequence(seq, comparator): longest, current = [], [] for a, b in zip(seq, seq[1:]): if comparator(a, b): current.append(a) else: if current: current.append(a) if len(current) > len(longest): longest = current current = [] if current and len(current) > len(longest): current.append(seq[-1]) longest = current return longest increasing = longest_subsequence(temperatures, lambda x, y: x < y) decreasing = longest_subsequence(temperatures, lambda x, y: x > y) return (increasing, decreasing)"},{"question":"You are given a configuration file in the INI format which has multiple sections each containing key-value pairs. Your task is to create a function `process_config(file_path: str, updates: dict) -> None` that performs the following: 1. **Read the Configuration File**: Read the configuration file from the given `file_path` parameter. 2. **Modify Configuration**: Update the configuration file according to the provided `updates` dictionary. The `updates` dictionary will be structured as follows: - Keys represent section names. - Values represent dictionaries of key-value pairs to update within those sections. 3. **Add Missing Sections/Options**: If a section or an option within a section specified in `updates` does not exist in the original configuration file, it should be added. 4. **Save the Configuration File**: Write the updated configuration back to the same `file_path`. Function Signature ```python def process_config(file_path: str, updates: dict) -> None: ``` Input - `file_path`: A string representing the file path to the configuration file. - `updates`: A dictionary where each key is a section name and each value is a dictionary of key-value pairs to be added/updated in that section. Example Assume the following content in a file named `config.ini`: ```ini [DEFAULT] ServerAliveInterval = 45 Compression = yes [forge.example] User = hg [topsecret.server.example] Port = 50022 ForwardX11 = no ``` Given the call: ```python updates = { \\"forge.example\\": {\\"User\\": \\"git\\"}, \\"topsecret.server.example\\": {\\"Port\\": \\"50023\\", \\"NewOption\\": \\"newValue\\"}, \\"new.section\\": {\\"Key\\": \\"value\\"} } process_config(\\"config.ini\\", updates) ``` The `config.ini` should be updated to: ```ini [DEFAULT] ServerAliveInterval = 45 Compression = yes [forge.example] User = git [topsecret.server.example] Port = 50023 ForwardX11 = no NewOption = newValue [new.section] Key = value ``` Constraints - Do not remove any existing sections or options unless specified in the `updates` dictionary. - Use `configparser` to handle the configuration file. Hints - Utilize the `configparser.ConfigParser` class to read and write the configuration file. - `configparser` allows treating configuration sections and options like a dictionary.","solution":"import configparser def process_config(file_path: str, updates: dict) -> None: Processes a configuration file and updates its content as per the updates dictionary. Args: file_path (str): Path to the configuration file. updates (dict): Dictionary containing sections and key-value pairs to update in the config. config = configparser.ConfigParser() config.read(file_path) for section, changes in updates.items(): if not config.has_section(section): config.add_section(section) for key, value in changes.items(): config.set(section, key, value) with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# Python Coding Assessment Objective Your task is to demonstrate your understanding of the `trace` module in Python. You will need to trace the execution of a given Python function and produce coverage information to determine which lines of code were executed. Problem Statement You are given a Python script that contains a function `sample_function`. Your task is to use the `trace` module to trace the execution of this function and generate a coverage report. 1. **Function Definition**: You need to implement the function `generate_coverage_report` that takes the following inputs: - `func`: A function object that needs to be traced. - `coverdir`: A directory path where the coverage report should be generated. 2. **Execution and Reporting**: - Initialize a `Trace` object to trace and count the execution. - Use the `Trace` object to run the given function. - Generate a coverage report in the specified directory. Function Signature ```python def generate_coverage_report(func: callable, coverdir: str) -> None: pass ``` Input - `func` (callable): A function object that needs to be traced. - `coverdir` (str): Path to the directory where the coverage report will be generated. Output - The function does not return anything but generates a coverage report file in the specified directory. Constraints 1. The `coverdir` must be a valid directory path. 2. The coverage report file should show lines that were not executed. 3. Assume that `func` is a valid callable function. Example Here\'s an example of how to use the `generate_coverage_report` function. ```python import os def sample_function(): for i in range(5): if i % 2 == 0: print(i) # Ensure the directory exists os.makedirs(\'coverage_reports\', exist_ok=True) # Generate the coverage report generate_coverage_report(sample_function, \'coverage_reports\') # After running this, check the \'coverage_reports\' directory for the generated coverage report. ``` Notes - You do not need to implement `sample_function`; it is provided for context. - The coverage report should be human-readable and indicate which lines of `sample_function` were executed. Good luck!","solution":"import os from trace import Trace def generate_coverage_report(func: callable, coverdir: str) -> None: Traces the execution of the given function and generates a coverage report. Args: func (callable): The function to trace. coverdir (str): The directory path where the coverage report will be generated. # Ensure the directory exists os.makedirs(coverdir, exist_ok=True) # Initialize Trace object with trace=True and count=True to get a trace and count the execution tracer = Trace(trace=True, count=True) # Run the provided function with tracing tracer.runfunc(func) # Generate the coverage report in the specified directory tracer.results().write_results(show_missing=True, coverdir=coverdir)"},{"question":"# Question In this task, you are required to implement a function named `process_io_streams` that reads from a text file, processes the data in memory using a combination of text and binary I/O streams, and writes the processed data to a new file. Function Signature ```python def process_io_streams(input_file: str, output_file: str, encoding: str): pass ``` Input - `input_file` (str): The path to the input text file. This file contains text data with mixed line endings (`\'n\'`, `\'r\'`, and `\'rn\'`). - `output_file` (str): The path to the output text file. The function should write the processed content to this file. - `encoding` (str): The encoding to use when reading from and writing to the files. Output - The function does not return any value. It writes the processed text into the output file. Constraints - The input file can be large (several MBs), so the function should efficiently handle the file by reading and writing in chunks. - The function should normalize all line endings in the input file to Unix style (`\'n\'`). - The function should raise an `OSError` if any I/O error occurs while reading from or writing to the files. - The function should raise a `ValueError` if the provided encoding is not supported. - If the encoding is set to `None`, default to UTF-8. Example Usage ```python # Example text data in input_file: # \\"HellornWorldnPythonrProgramming\\" input_file = \\"input.txt\\" output_file = \\"output.txt\\" encoding = \\"utf-8\\" process_io_streams(input_file, output_file, encoding) # Content in output_file after processing: # \\"HellonWorldnPythonnProgramming\\" ``` Instructions 1. Open the `input_file` in read mode with the specified `encoding`. Ensure proper handling of mixed line endings. 2. Use an in-memory text stream (`StringIO`) to process the data. 3. Normalize all line endings to Unix style (`\'n\'`). 4. Open the `output_file` in write mode with the specified `encoding`. 5. Write the processed data from the in-memory text stream to the `output_file`. 6. Ensure all streams are properly closed after operations are complete. 7. Implement proper error handling for I/O and encoding errors. Notes - You may use the `io` module and its various classes and methods to accomplish this task. - Refer to the Python documentation if needed to understand the usage of different classes and methods in the `io` module.","solution":"import os import io def process_io_streams(input_file: str, output_file: str, encoding: str = None): # Set default encoding to \'utf-8\' if encoding is None if encoding is None: encoding = \'utf-8\' try: # Open the input file in text mode with the specified encoding with open(input_file, \'r\', encoding=encoding) as f_in: # Read the input file content with mixed line endings content = f_in.read() # Normalize line endings to Unix style (\'n\') normalized_content = content.replace(\'rn\', \'n\').replace(\'r\', \'n\') # Open the output file in text mode with the specified encoding with open(output_file, \'w\', encoding=encoding) as f_out: # Write the normalized content to the output file f_out.write(normalized_content) except OSError as e: raise OSError(f\\"An error occurred while handling files: {e}\\") except LookupError as e: raise ValueError(f\\"Unsupported encoding provided: {e}\\")"},{"question":"Title: Implementing a Configuration Loader Problem Statement You are required to implement a Python function `load_configuration` that loads a configuration from a given module file. The configuration module will contain several configuration variables that must be extracted and returned in a dictionary. The extraction process involves using various assignment statements as discussed in the documentation. Function Signature ```python def load_configuration(module_path: str) -> dict: ``` Input - `module_path` (str): The path to the Python module file containing the configuration variables. Output - `config` (dict): A dictionary containing the configuration variables as keys and their corresponding values. Constraints 1. The module file is guaranteed to exist at the given path. 2. The module file may contain comments, blank lines, import statements, and variable assignments. 3. Only consider variables that are assigned at the top module level and ignore any function or class level variables. Example Consider the following configuration module `config.py`: ```python # config.py import os API_KEY = \\"123456789\\" # API Key for accessing the service MAX_RETRIES = 5 # Maximum number of retries for requests TIMEOUT = 30.0 # Timeout for requests in seconds def some_function(): LOCAL_VARIABLE = \\"This should be ignored\\" class SomeClass: CLASS_VARIABLE = \\"This should also be ignored\\" ``` For the given module file located at `\'./config.py\'`, the function call should return: ```python { \\"API_KEY\\": \\"123456789\\", \\"MAX_RETRIES\\": 5, \\"TIMEOUT\\": 30.0 } ``` Implementation Notes - Use the `importlib` and `types` modules to dynamically load the configuration module. - Utilize the techniques discussed for assignment and import statements to accurately fetch the configuration variables. - Ensure your solution handles different types of assignments and ignores any class or function scope variables. Additional Information - You can assume that all variable assignments are simple or augmented assignments (e.g., `variable = value`, `variable += value`). - Use `assert` statements for debugging and validating intermediate states if necessary.","solution":"import importlib.util import sys def load_configuration(module_path: str) -> dict: Loads the configuration from a Python module file and returns it as a dictionary. Args: - module_path (str): The path to the Python module file containing the configuration variables. Returns: - dict: A dictionary containing the configuration variables. spec = importlib.util.spec_from_file_location(\\"config_module\\", module_path) config_module = importlib.util.module_from_spec(spec) spec.loader.exec_module(config_module) config = {} for attr in dir(config_module): if attr.isupper(): # only include variables that are uppercase (common convention for global consts) value = getattr(config_module, attr) if not callable(value) and not attr.startswith(\\"__\\"): # avoid built-in attributes and callables config[attr] = value return config"},{"question":"**Objective:** Implement a class in Python that dynamically adds attributes to another class using descriptors. This exercise will require demonstrating a clear understanding of Python descriptors and object-oriented principles. **Problem Statement:** Create a class `DynamicAttributes` that uses descriptor objects to add dynamic attributes to another class. 1. Define a `DynamicAttributes` class with the following methods: - `__init__(self, attr_name, value)`: Initializes the descriptor with the attribute name and value. - `__get__(self, obj, objtype)`: Retrieves the attribute value. - `__set__(self, obj, value)`: Sets the attribute value. - `__delete__(self, obj)`: Deletes the attribute. 2. Implement a function `add_dynamic_attribute(cls, attr_name, value)` that adds a new attribute managed by the `DynamicAttributes` descriptor to the given class `cls`. **Expected Input and Output:** - `add_dynamic_attribute(cls, attr_name, value)`: - Inputs: - `cls` : Class to which the attribute should be added. - `attr_name` : Name of the new attribute. - `value` : Initial value of the attribute. - Output: None **Constraints:** - You should not modify the target class directly. - The new attribute should behave like a normal attribute but be managed by the `DynamicAttributes` descriptor. **Performance Requirements:** - Efficiently handle adding and managing multiple dynamic attributes. **Example:** ```python class MyClass: pass # Adding a dynamic attribute `dynamic_attr` with initial value `42` add_dynamic_attribute(MyClass, \'dynamic_attr\', 42) mc = MyClass() print(mc.dynamic_attr) # Output: 42 mc.dynamic_attr = 100 print(mc.dynamic_attr) # Output: 100 del mc.dynamic_attr try: print(mc.dynamic_attr) except AttributeError: print(\\"dynamic_attr has been deleted\\") # Output: dynamic_attr has been deleted ``` The solution should involve creating the `DynamicAttributes` descriptor to dynamically manage attributes added to any class.","solution":"class DynamicAttributes: Descriptor class to dynamically manage attributes. def __init__(self, attr_name, value): self.attr_name = attr_name self.value = value def __get__(self, obj, objtype=None): return self.value def __set__(self, obj, value): self.value = value def __delete__(self, obj): del self.value def add_dynamic_attribute(cls, attr_name, value): Adds a dynamic attribute to `cls` managed by the DynamicAttributes descriptor. setattr(cls, attr_name, DynamicAttributes(attr_name, value))"},{"question":"# PyTorch and HIP Integration **Objective:** In this exercise, you will demonstrate your understanding of working with PyTorch and GPU devices with a specific focus on HIP (ROCm) integration. You will need to implement a function that performs tensor operations on the GPU and monitors the memory usage. **Problem Statement:** You are required to write a function `manage_tensors_on_gpu(data)` that satisfies the following requirements: 1. **Tensor Creation:** Creates a tensor from the given `data` on the default GPU device. 2. **Device Management:** Ensures that tensor operations are explicitly targeted to the first available GPU (e.g., `cuda:0`). 3. **Memory Monitoring:** Monitors the GPU memory usage before and after tensor operations and returns these metrics. 4. **Tensor Operations:** Performs the following tensor operations: - Creates another tensor of ones with the same shape as the input tensor on the first GPU. - Adds the two tensors. - Transfers the resultant tensor to the second GPU (if available) and performs a multiplication of the resultant tensor by 2. - Transfers the final tensor back to CPU. 5. **Output:** Returns a dictionary with the following information: - The memory usage on the first GPU before and after the tensor operations. - The final tensor after all operations. **Input Format:** - `data`: A list or numpy array of numerical values. **Output Format:** - A dictionary with keys: `memory_before`, `memory_after`, and `final_tensor`. The memory metrics should be in bytes, and the final tensor should be a PyTorch tensor on the CPU. **Constraints:** - You may assume that the input data will be large enough to notice memory usage changes but small enough to fit into GPU memory. - Handle cases where no second GPU is available gracefully. **Example:** ```python def manage_tensors_on_gpu(data): # Your implementation here data = [1.0, 2.0, 3.0, 4.0] result = manage_tensors_on_gpu(data) print(result) # Output should be something like: # { # \'memory_before\': <memory before in bytes>, # \'memory_after\': <memory after in bytes>, # \'final_tensor\': tensor([...], device=\'cpu\') # } ``` Note: Ensure you utilize the PyTorch and CUDA/HIP APIs effectively to manage the tensor operations and memory monitoring. **Performance Requirements:** - The function should be efficient in terms of memory usage and operations. - Proper exception handling should be in place to handle scenarios where GPU resources are not available. Good luck!","solution":"import torch def manage_tensors_on_gpu(data): # Ensure input is a tensor tensor = torch.tensor(data, device=\'cuda:0\') # Memory before operations memory_before = torch.cuda.memory_allocated(\'cuda:0\') # Create another tensor of ones with the same shape on GPU0 ones_tensor = torch.ones_like(tensor, device=\'cuda:0\') # Add the tensors result_tensor = tensor + ones_tensor # Memory after addition on GPU0 memory_after_add = torch.cuda.memory_allocated(\'cuda:0\') # Move result to GPU1 if available, otherwise remain on GPU0 if torch.cuda.device_count() > 1: result_tensor = result_tensor.to(\'cuda:1\') # Perform multiplication by 2 on GPU1 result_tensor = result_tensor * 2 else: # Perform multiplication by 2 on GPU0 result_tensor = result_tensor * 2 # Move the result tensor back to CPU final_tensor = result_tensor.to(\'cpu\') # Memory after all operations memory_after = torch.cuda.memory_allocated(\'cuda:0\') return { \'memory_before\': memory_before, \'memory_after\': memory_after, \'final_tensor\': final_tensor }"},{"question":"You have been tasked with creating a probabilistic model representing a simplified student grading system using PyTorch\'s `torch.distributions` module. The system models the following features: 1. **Exam Scores**: Modeled as a `Normal` distribution with a mean score of 75 and a standard deviation of 10. 2. **Assignment Scores**: Modeled as a `Beta` distribution with parameters α=2 and β=5, scaled to be out of 40 points. 3. **Project Completion**: Modeled as a `Bernoulli` distribution with a probability of 0.85. Your task is to implement a function `simulate_student_grades` that simulates the grades for `n` students and returns a dictionary containing the exam scores, assignment scores, and project completion status for each student. Function Signature ```python import torch from torch.distributions import Normal, Beta, Bernoulli def simulate_student_grades(n: int) -> dict: # your implementation here pass ``` Input - `n` (int): The number of students to simulate. Output - A dictionary with keys `exam_scores`, `assignment_scores`, and `project_completed`. Each key should have a list of length `n` containing the corresponding grades or completion status for all students. - `exam_scores` should contain float values of the exam scores. - `assignment_scores` should contain float values of the assignment scores. - `project_completed` should contain boolean values indicating whether the project was completed or not. Constraints - Ensure that exam scores and assignment scores fall within a sensible range (0 to 100 for exams, 0 to 40 for assignments). - Use the `torch.distributions` module to perform all probabilistic operations. Example ```python n = 5 result = simulate_student_grades(n) print(result) # Example output: # { # \'exam_scores\': [78.3, 65.4, 80.1, 90.5, 73.0], # \'assignment_scores\': [15.2, 5.3, 20.4, 12.7, 9.3], # \'project_completed\': [True, True, False, True, False] # } ``` Note The output values will vary since the distributions involve random sampling.","solution":"import torch from torch.distributions import Normal, Beta, Bernoulli def simulate_student_grades(n: int) -> dict: # Define the distributions exam_dist = Normal(75, 10) assignment_dist = Beta(2, 5) project_dist = Bernoulli(0.85) # Generate samples exam_scores = exam_dist.sample((n,)).clamp(0, 100).tolist() assignment_scores = (assignment_dist.sample((n,)) * 40).tolist() project_completed = project_dist.sample((n,)).bool().tolist() # Create the result dictionary result = { \'exam_scores\': exam_scores, \'assignment_scores\': assignment_scores, \'project_completed\': project_completed } return result"},{"question":"# Seaborn Assessment Question Objective: You are tasked with demonstrating your understanding of Seaborn by visualizing a dataset. Your final plot should meet specific customization and grouping requirements. Dataset: Use the built-in dataset \\"tips\\" from seaborn. Instructions: 1. **Load the Data**: - Import seaborn and load the \\"tips\\" dataset into a DataFrame named `tips`. 2. **Group Data and Plot**: - Create a point plot using the following specifications: - The x-axis should represent the day of the week. - The y-axis should represent the average total bill. - Differentiate the data points by \'time\' with different colors. - Include error bars representing the standard deviation. 3. **Customize Plot**: - Enhance accessibility by using different markers for the \'time\' variable. - Customize the error bars (e.g., cap size, color). - Make any additional customization (e.g., change the plot\'s theme, adjust the gridlines). 4. **Plot Appearance**: - Ensure the plot has an appropriate title and axis labels. - Customize the tick labels for the x-axis to show full day names (e.g., \\"Sunday\\" instead of \\"Sun\\"). - Ensure the plot is aesthetically pleasing and the error bars are distinctly visible. Constraints: - Do not use any external libraries for data manipulation or plotting other than seaborn and pandas. - The code should be efficient and avoid unnecessary steps. Expected Output: A seaborn point plot that meets all the specified criteria. The plot should clearly show the average total bill for each day of the week, differentiated by time (Lunch/Dinner), with error bars for standard deviation. # Example Code Template: ```python import seaborn as sns import pandas as pd # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create and customize the point plot sns.set_theme(style=\\"whitegrid\\") ax = sns.pointplot( data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\", capsize=0.2 ) # Customization ax.set_title(\\"Average Total Bill by Day and Time\\") ax.set_xlabel(\\"Day of the Week\\") ax.set_ylabel(\\"Average Total Bill\\") ax.set_xticklabels([\\"Thursday\\", \\"Friday\\", \\"Saturday\\", \\"Sunday\\"]) # Display the plot sns.despine() ``` Submit your code implementation that produces the required plot.","solution":"import seaborn as sns import pandas as pd # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create and customize the point plot sns.set_theme(style=\\"whitegrid\\") ax = sns.pointplot( data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\", capsize=0.2 ) # Customization ax.set_title(\\"Average Total Bill by Day and Time\\") ax.set_xlabel(\\"Day of the Week\\") ax.set_ylabel(\\"Average Total Bill\\") ax.set_xticklabels([\\"Thursday\\", \\"Friday\\", \\"Saturday\\", \\"Sunday\\"]) # Display the plot sns.despine()"},{"question":"# Question: Optimizing Parallelism in scikit-learn using Joblib and OpenMP You are given a dataset and tasked with training a `RandomForestClassifier` using scikit-learn. You need to optimize the parallelism of this training process to minimize the runtime on a machine with 8 CPUs. You must achieve this by appropriately setting the `n_jobs` parameter and environment variables. **Requirements:** 1. Set up the training to use Joblib for higher-level parallelism and OpenMP for lower-level parallelism. 2. Limit the number of threads from OpenMP to 4. 3. Use the `n_jobs` parameter of the RandomForestClassifier to spawn 4 processes in parallel. 4. Measure and print the time taken to train the classifier. **Dataset:** The dataset `X` (features) and `y` (labels) are provided as NumPy arrays. **Input:** - `X`: a NumPy array of shape (n_samples, n_features) - `y`: a NumPy array of shape (n_samples,) **Output:** - Print the time taken to train the `RandomForestClassifier`. **Constraints:** - You must use the parallel_backend context manager from the joblib library to set Joblib\'s backend. - The `n_estimators` parameter of the RandomForestClassifier should be set to 100. - Ensure reproducibility by setting a random state of 42 in the RandomForestClassifier. **Hints:** - Use the `time` module to measure the training time. - Use `os.environ` to set the environment variable for OpenMP. **Example Code Skeleton:** ```python import time import os from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from joblib import parallel_backend # Set the number of OpenMP threads os.environ[\'OMP_NUM_THREADS\'] = \'4\' def optimize_parallelism(X, y): # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create the RandomForestClassifier with n_estimators set to 100 and random_state set to 42 clf = RandomForestClassifier(n_estimators=100, random_state=42) # Measure the time taken to fit the model start_time = time.time() # Use parallel_backend to set the backend to \'loky\' with n_jobs as 4 with parallel_backend(\'loky\', n_jobs=4): clf.fit(X_train, y_train) end_time = time.time() print(f\\"Training time: {end_time - start_time:.2f} seconds\\") # Example usage # X, y = ... # assuming you have your dataset loaded here # optimize_parallelism(X, y) ``` **Task:** Implement the `optimize_parallelism` function in the code skeleton provided.","solution":"import time import os import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from joblib import parallel_backend # Set the number of OpenMP threads os.environ[\'OMP_NUM_THREADS\'] = \'4\' def optimize_parallelism(X, y): # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Create the RandomForestClassifier with n_estimators set to 100 and random_state set to 42 clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=4) # Measure the time taken to fit the model start_time = time.time() # Use parallel_backend to set the backend to \'loky\' with n_jobs as 4 with parallel_backend(\'loky\', n_jobs=4): clf.fit(X_train, y_train) end_time = time.time() print(f\\"Training time: {end_time - start_time:.2f} seconds\\") # Example usage # X = np.random.rand(1000, 20) # y = np.random.randint(2, size=1000) # optimize_parallelism(X, y)"},{"question":"**Mocking with Python: Advanced Testing Scenario** # Objective: Write a function `process_data` and a corresponding test suite using the `unittest.mock` library to test its behavior under various scenarios. # Function Specification: The function `process_data(filename)` should: 1. Open a file with the specified `filename`. 2. Read its content. 3. Process the data which involves: - Filtering out any line that starts with a \'#\'. - Splitting each line into words and transforming each word to uppercase. - Returning a dictionary with word occurrences (word as key, frequency as value). # Requirements: - You must write a test suite using `unittest` and `unittest.mock` to mock the file operations and test the behavior of the `process_data` function. - Ensure that you cover: - Opening the file. - Reading a mixture of lines including lines starting with \'#\'. - Handling an empty file. - File not found exception. # Constraints: - File operations should be mocked; do not use actual filesystem I/O. - Ensure the function handles large files efficiently. # Example: ```python # Function to implement def process_data(filename): count = {} with open(filename, \'r\') as f: for line in f: if line.startswith(\'#\'): continue for word in line.strip().split(): word = word.upper() count[word] = count.get(word, 0) + 1 return count ``` # Implementation Details: 1. Implement the `process_data` function as described. 2. Write test cases using `unittest` framework and `unittest.mock` capabilities to: - Verify the function opens the file correctly. - Mock file reading and test proper processing of data. - Ensure lines starting with \'#\' are ignored. - Verify correct word count with mixed case words. - Handle and test the scenario of the file not being found. # Sample Test Cases: ```python import unittest from unittest.mock import mock_open, patch, call, MagicMock from mymodule import process_data class TestProcessData(unittest.TestCase): @patch(\'builtins.open\', new_callable=mock_open, read_data=\'\'\'# This is a comment Hello World hello world Python Mock\'\'\') def test_process_data(self, mock_file): result = process_data(\'dummyfile.txt\') expected_result = { \'HELLO\': 2, \'WORLD\': 2, \'PYTHON\': 1, \'MOCK\': 1 } self.assertEqual(result, expected_result) mock_file.assert_called_with(\'dummyfile.txt\', \'r\') @patch(\'builtins.open\', new_callable=mock_open, read_data=\'\') def test_empty_file(self, mock_file): result = process_data(\'emptyfile.txt\') expected_result = {} self.assertEqual(result, expected_result) @patch(\'builtins.open\', side_effect=FileNotFoundError) def test_file_not_found(self, mock_open): with self.assertRaises(FileNotFoundError): process_data(\'missingfile.txt\') mock_open.assert_called_with(\'missingfile.txt\', \'r\') if __name__ == \'__main__\': unittest.main() ``` The students should provide both the `process_data` function and the corresponding test suite as part of their solution.","solution":"def process_data(filename): Reads a file and processes its data by: - Filtering out lines starting with \'#\'. - Splitting lines into words and converting to uppercase. - Counting occurrences of each word. Args: filename (str): The name of the file to process. Returns: dict: A dictionary with word as keys and their counts as values. count = {} with open(filename, \'r\') as f: for line in f: if line.startswith(\'#\'): continue for word in line.strip().split(): word = word.upper() count[word] = count.get(word, 0) + 1 return count"},{"question":"# Question: Implementing and Handling Custom Exceptions in a Banking System You are required to implement a small simulation of a banking system that handles checking account transactions. The system should be able to handle deposits, withdrawals, and transfers between accounts. Additionally, you need to manage exceptions in a robust way. Requirements: 1. **Custom Exceptions**: - Define a custom exception `InsufficientFundsError` that should be raised when an account has insufficient funds for a transaction. - Define a custom exception `AccountError` which should have a subclass `ClosedAccountError` that is raised when trying to operate on a closed account. 2. **Account Class**: - Define a class `Account` with the following attributes: - `account_id`: unique identifier for the account. - `balance`: current balance of the account. - `is_open`: boolean state indicating whether the account is open or closed. - The class should have the following methods: - `deposit(amount)`: Adds the specified amount to the account balance, if the account is open. - `withdraw(amount)`: Subtracts the specified amount from the account balance, if the account is open and sufficient funds are available. - `transfer_to(amount, target_account)`: Transfers the specified amount to the target account, ensuring both accounts are open and sufficient funds are available in the source account. - `close_account()`: Closes the account. 3. **Exception Handling**: - Implement proper error handling to manage the following scenarios: - Attempting a withdrawal or transfer with insufficient funds. - Operating on a closed account. - Use exception chaining to provide detailed error context where applicable. Input and Output Formats: - **Input**: No direct input from the user is required. The methods within the class will raise appropriate exceptions based on operations performed. - **Output**: Exception messages should indicate the error and context. Example: ```python class InsufficientFundsError(Exception): pass class AccountError(Exception): pass class ClosedAccountError(AccountError): pass class Account: def __init__(self, account_id, balance=0): self.account_id = account_id self.balance = balance self.is_open = True def deposit(self, amount): if not self.is_open: raise ClosedAccountError(\\"Cannot deposit into a closed account.\\") self.balance += amount def withdraw(self, amount): if not self.is_open: raise ClosedAccountError(\\"Cannot withdraw from a closed account.\\") if amount > self.balance: raise InsufficientFundsError(\\"Insufficient funds for withdrawal.\\") self.balance -= amount def transfer_to(self, amount, target_account): if not self.is_open or not target_account.is_open: raise ClosedAccountError(\\"One or both of the accounts are closed.\\") try: self.withdraw(amount) except InsufficientFundsError as e: raise InsufficientFundsError(\\"Transfer failed due to insufficient funds.\\") from e target_account.deposit(amount) def close_account(self): self.is_open = False # Example Usage: acc1 = Account(\'001\', 100) acc2 = Account(\'002\', 50) try: acc1.transfer_to(120, acc2) except InsufficientFundsError as e: print(e) ``` Implement the `Account` class and the custom exceptions according to the above specification. Handle exceptions carefully and use exception chaining where appropriate to provide meaningful error messages.","solution":"class InsufficientFundsError(Exception): Exception raised when an account has insufficient funds for a transaction. pass class AccountError(Exception): Base class for other account-related exceptions. pass class ClosedAccountError(AccountError): Exception raised when trying to operate on a closed account. pass class Account: def __init__(self, account_id, balance=0): self.account_id = account_id self.balance = balance self.is_open = True def deposit(self, amount): if not self.is_open: raise ClosedAccountError(\\"Cannot deposit into a closed account.\\") self.balance += amount def withdraw(self, amount): if not self.is_open: raise ClosedAccountError(\\"Cannot withdraw from a closed account.\\") if amount > self.balance: raise InsufficientFundsError(\\"Insufficient funds for withdrawal.\\") self.balance -= amount def transfer_to(self, amount, target_account): if not self.is_open: raise ClosedAccountError(\\"Cannot transfer from a closed account.\\") if not target_account.is_open: raise ClosedAccountError(\\"Cannot transfer to a closed account.\\") try: self.withdraw(amount) except InsufficientFundsError as e: raise InsufficientFundsError(\\"Transfer failed due to insufficient funds.\\") from e target_account.deposit(amount) def close_account(self): self.is_open = False"},{"question":"# PyTorch Coding Assessment: Tensor Operations with TorchScript Constraints Objective Write a function that performs several Tensor operations while adhering to TorchScript constraints. You will implement this function using PyTorch and ensure it is compatible with TorchScript. Problem Statement Implement a function `custom_tensor_operations` that takes a PyTorch tensor as input and performs a series of operations, including some that are explicitly mentioned as having constraints or divergent schemas when used with TorchScript. The function should demonstrate the correct usage and manipulation of tensors given these constraints. Inputs - `input_tensor` (torch.Tensor): A PyTorch tensor of any shape. Outputs - `output_tensor` (torch.Tensor): A tensor that results from applying a sequence of supported operations on the input_tensor. Operations to Include 1. **Normalization**: Normalize the input tensor using `torch.norm` and scale it. 2. **Padding**: Add padding to the tensor. 3. **Initialization**: Create a new tensor of the same size as the input tensor, initialized using `torch.zeros_like`. 4. **Combination**: Combine the input tensor and the initialized tensor using an element-wise operation (e.g., addition). Constraints - Ensure that the function operates well under TorchScript constraints (e.g., avoid using functions directly unsupported or having divergent schemas). - Handle necessary optional parameters explicitly (e.g., dtype, device). Function Signature ```python import torch def custom_tensor_operations(input_tensor: torch.Tensor) -> torch.Tensor: # Normalize the input tensor norm = torch.norm(input_tensor) normalized_tensor = input_tensor / (norm + 1e-10) # Add padding (e.g., pad with zeros on all sides by 1) padded_tensor = torch.nn.functional.pad(normalized_tensor, (1,1,1,1), mode=\'constant\', value=0) # Initialize a tensor using torch.zeros_like initialized_tensor = torch.zeros_like(padded_tensor, dtype=input_tensor.dtype, device=input_tensor.device) # Combine input tensor and initialized tensor using addition output_tensor = padded_tensor + initialized_tensor return output_tensor # Example usage: input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) output_tensor = custom_tensor_operations(input_tensor) print(output_tensor) ``` Evaluation Criteria - Correctness: The function performs the specified operations correctly on the input tensor. - TorchScript Compliance: The function adheres to TorchScript constraints and does not use unsupported constructs. - Code Quality: The code is well-written, clear, and follows PyTorch best practices. Test your implementation to ensure that it works as expected, and submit your function for evaluation.","solution":"import torch def custom_tensor_operations(input_tensor: torch.Tensor) -> torch.Tensor: Perform a series of operations on the input tensor, ensuring compatibility with TorchScript. Args: input_tensor (torch.Tensor): A PyTorch tensor of any shape. Returns: torch.Tensor: The tensor after applying normalization, padding, and initialization. # Normalize the input tensor norm = torch.norm(input_tensor) normalized_tensor = input_tensor / (norm + 1e-10) # Add padding (1 on all sides) padded_tensor = torch.nn.functional.pad(normalized_tensor, (1, 1, 1, 1), mode=\'constant\', value=0) # Initialize a tensor using torch.zeros_like initialized_tensor = torch.zeros_like(padded_tensor, dtype=input_tensor.dtype, device=input_tensor.device) # Combine input tensor and initialized tensor using addition output_tensor = padded_tensor + initialized_tensor return output_tensor # Example usage: # input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) # output_tensor = custom_tensor_operations(input_tensor) # print(output_tensor)"},{"question":"# Configuration File Parser You are provided with an initial configuration file, `settings.ini`, which contains the following data: ```ini [General] appname = MyApp version = 1.2.3 [User] name = Alice email = alice@example.com age = 30 [Settings] theme = light autosave = true timeout = 30 ``` Your task is to write a Python function that performs the following operations: 1. Reads the `settings.ini` file. 2. Modifies the `theme` under the `[Settings]` section to a given value. 3. Adds a new section `[Database]` with the following key-value pairs: ``` [Database] host = localhost port = 5432 ``` 4. Verifies if the `version` in the `[General]` section is at least \\"1.2.3\\". If it is not, raise a `ValueError`. 5. Writes the modified configuration back to `settings.ini`. # Function Signature ```python def modify_config(file_path: str, new_theme: str) -> None: Modify the configuration file as specified. :param file_path: The path to the configuration file (settings.ini). :param new_theme: The new theme to set under [Settings]. :raises: ValueError if the version is less than \\"1.2.3\\". ``` # Constraints - `file_path` will always be a valid path to an existing file. - The `new_theme` will be a non-empty string. - The `version` will use the format `\\"x.y.z\\"` where `x`, `y`, and `z` are integers. # Example Given the `settings.ini` file as described, and invoking `modify_config(\'settings.ini\', \'dark\')`, the file should be modified to: ```ini [General] appname = MyApp version = 1.2.3 [User] name = Alice email = alice@example.com age = 30 [Settings] theme = dark autosave = true timeout = 30 [Database] host = localhost port = 5432 ``` If the version was \\"1.2.2\\" or less, the function should raise a `ValueError`. Use the `configparser` module to implement this functionality.","solution":"import configparser def modify_config(file_path: str, new_theme: str) -> None: config = configparser.ConfigParser() config.read(file_path) # Check version required_version = [1, 2, 3] current_version = list(map(int, config.get(\'General\', \'version\').split(\'.\'))) if current_version < required_version: raise ValueError(f\\"Version {config.get(\'General\', \'version\')} is less than 1.2.3\\") # Modify theme config.set(\'Settings\', \'theme\', new_theme) # Add Database section if \'Database\' not in config.sections(): config.add_section(\'Database\') config.set(\'Database\', \'host\', \'localhost\') config.set(\'Database\', \'port\', \'5432\') # Write changes back to the file with open(file_path, \'w\') as configfile: config.write(configfile)"},{"question":"# Seaborn and Matplotlib Integration You are required to demonstrate your understanding of Seaborn\'s `Plot` class and its integration with Matplotlib. Specifically, you will: 1. Create various types of plots using Seaborn. 2. Integrate these plots into Matplotlib figures and subfigures. 3. Apply customizations to the resulting plots. Task 1. **Load the \'diamonds\' dataset** from Seaborn. 2. **Create a scatter plot** of `carat` vs. `price` using `seaborn.objects.Plot`. 3. **Create a histogram with bars** of the `price` attribute, faceted by the `cut` of the diamonds. 4. **Display these two plots** side by side using Matplotlib subfigures. 5. Add the following customizations: - **For the scatter plot**: - Add a rectangle annotation with the text \\"Scatter Plot\\" on the top-left corner of the plot. - **For the histogram**: - Log scale the x-axis. - Add a title \\"Price Distribution by Cut\\". Implementation Details 1. **Input Data**: Use Seaborn\'s `diamonds` dataset. 2. **Output**: A Matplotlib figure with two subfigures—scatter plot and histogram. 3. You may use additional packages if required, but the primary focus should be on `seaborn` and `matplotlib`. 4. Ensure the figure is clearly labeled and visually appealing. Example Code Structure ```python import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt from seaborn import load_dataset # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create plots scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) hist_plot = ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) ) # Create subfigures f = mpl.figure.Figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # Add scatter plot to the first subfigure res1 = scatter_plot.on(sf1).plot() # Add rectangle annotation ax1 = sf1.axes[0] rect = mpl.patches.Rectangle(xy=(0, 1), width=0.15, height=0.1, color=\\"C1\\", alpha=0.2, transform=ax1.transAxes, clip_on=False) ax1.add_artist(rect) ax1.text(x=rect.get_width() / 2, y=1 - rect.get_height() / 2, s=\\"Scatter Plot\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax1.transAxes) # Add histogram to the second subfigure res2 = hist_plot.on(sf2).plot() # Add title to the histogram sf2.axes[0].set_title(\\"Price Distribution by Cut\\") # Display the figure plt.show() ``` Implement the code as described and ensure that the output meets the specified requirements.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt from seaborn import load_dataset def create_plots(): # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create scatter plot of carat vs. price scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) # Create histogram of price faceted by cut hist_plot = ( so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) ) # Create subfigures fig = plt.figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") sf1, sf2 = fig.subfigures(1, 2) # Add scatter plot to the first subfigure scatter_res = scatter_plot.on(sf1).plot() # Add rectangle annotation ax1 = sf1.axes[0] rect = mpl.patches.Rectangle( xy=(0, 1), width=0.15, height=0.1, color=\\"C1\\", alpha=0.2, transform=ax1.transAxes, clip_on=False ) ax1.add_artist(rect) ax1.text( x=rect.get_width() / 2, y=1 - rect.get_height() / 2, s=\\"Scatter Plot\\", size=12, ha=\\"center\\", va=\\"center\\", transform=ax1.transAxes ) # Add histogram to the second subfigure hist_res = hist_plot.on(sf2).plot() # Add title to the histogram sf2.axes[0].set_title(\\"Price Distribution by Cut\\") # Display the figure plt.show()"},{"question":"Problem Statement: You have been tasked with developing a simple HTTP client to fetch data from a specified URL and process the response. Using the Python `http.client` module, you will need to implement a function that makes an HTTP GET request to a given URL, checks the HTTP status code, and returns the content of the response. If the status code is not 200 (OK), your function should return an appropriate error message. # Function Signature: ```python def fetch_url_content(url: str) -> tuple: pass ``` # Inputs: - `url` (str): A string representing the URL to which the HTTP GET request will be sent. # Outputs: - A tuple with: - The first element being an integer representing the HTTP status code. - The second element being the response content in the form of a string if the status code is 200, or an error message string if the status code is not 200. # Constraints: - The URL provided will always be in a valid format. - The function should handle any network-related exceptions, such as timeouts or connection errors, by returning a status code of 500 and an error message. # Examples: Example 1: ```python url = \\"http://example.com\\" result = fetch_url_content(url) print(result) # Expected Output: (200, \\"Example Domain\\") # Content based on the example domain. ``` Example 2: ```python url = \\"http://nonexistenturl.com\\" result = fetch_url_content(url) print(result) # Expected Output: (500, \\"Network-related error occurred.\\") ``` # Notes: - Use the `http.client` module for making the HTTP GET request. - Ensure that the function handles different HTTP status codes appropriately. - In case of a network-related error or exception, an error message and a status code of 500 should be returned. **Hint:** Refer to the `http.client` module documentation to understand how to make HTTP requests and handle responses.","solution":"import http.client from urllib.parse import urlparse def fetch_url_content(url: str) -> tuple: try: parsed_url = urlparse(url) conn = http.client.HTTPConnection(parsed_url.netloc) conn.request(\\"GET\\", parsed_url.path or \\"/\\") response = conn.getresponse() status_code = response.status content = response.read().decode() conn.close() if status_code == 200: return (status_code, content) else: return (status_code, f\\"Error: {response.reason}\\") except Exception as e: return (500, \\"Network-related error occurred.\\")"},{"question":"Design a function that processes an AU audio file by doubling the frame rate and saving the modified audio to a new file. **Function Signature:** ```python def process_au_file(input_file: str, output_file: str) -> None: pass ``` **Input:** - `input_file` (str): The path to the input AU file. - `output_file` (str): The path where the modified AU file will be saved. **Constraints:** - The input AU file is guaranteed to be in one of the supported encoding formats: ULAW, ALAW, or linear PCM (8, 16, 24, or 32 bits). - The input AU file contains mono or stereo audio. **Output:** - The function does not return anything. It writes the modified audio data to the specified output file. **Performance Requirements:** - The function should be efficient in reading and writing files, minimizing unnecessary operations. **Details:** 1. Open the input AU file using `sunau.open`. 2. Read the header information, including the sample rate. 3. Double the sample rate. 4. Read the audio frames. 5. Write the modified header (with the new sample rate) and audio frames to the output AU file using `sunau.open` in write mode. 6. Ensure that the output AU file has valid header information and correctly written audio data. Test your function with different AU files to ensure it behaves as expected. **Example Usage:** ```python process_au_file(\\"input.au\\", \\"output.au\\") # This should read \\"input.au\\", double its sample rate, and save the result to \\"output.au\\" ```","solution":"import sunau def process_au_file(input_file: str, output_file: str) -> None: Processes an AU audio file by doubling the frame rate and saving the modified audio to a new file. :param input_file: The path to the input AU file. :param output_file: The path where the modified AU file will be saved. with sunau.open(input_file, \'rb\') as au_in: # Reading header information n_channels = au_in.getnchannels() sampwidth = au_in.getsampwidth() framerate = au_in.getframerate() n_frames = au_in.getnframes() comp_type = au_in.getcomptype() comp_name = au_in.getcompname() # Doubling the frame rate new_framerate = framerate * 2 # Reading audio frames audio_frames = au_in.readframes(n_frames) with sunau.open(output_file, \'wb\') as au_out: # Writing header with modified frame rate au_out.setnchannels(n_channels) au_out.setsampwidth(sampwidth) au_out.setframerate(new_framerate) au_out.setcomptype(comp_type, comp_name) # Writing audio frames au_out.writeframes(audio_frames)"},{"question":"# Advanced Coding Assessment Question on Named Tensors in PyTorch Objective: Demonstrate proficiency in using Named Tensors in PyTorch by creating, manipulating, and performing operations on named tensors. Problem Statement: Write a function `process_named_tensors` that takes two named tensors as input and performs the following operations: 1. **Broadcast Addition**: Add the two tensors together. Ensure that their dimension names match. If not, raise a `ValueError`. 2. **Dimension Alignment**: Align the dimensions of the resulting tensor to a specified order, `(\'batch\', \'channel\', \'height\', \'width\')`. If any of these dimension names are missing, raise a `ValueError`. 3. **Flattening**: Flatten the dimensions `\'height\'` and `\'width\'` into a single dimension named `\'spatial\'`. 4. **Named Output**: Return the resulting tensor with its names. # Input Format: - `tensor1`: A 4-dimensional named tensor with dimensions named and in any order. - `tensor2`: A 4-dimensional named tensor with dimensions named and in any order. # Output Format: - A named tensor with dimensions `(\'batch\', \'channel\', \'spatial\')`. # Constraints: - The input tensors must both have exactly four dimensions. - All operations must maintain the named dimensions without losing any names. - The function should be able to handle cases where the input tensors have dimension names in different orders. # Example: ```python import torch def process_named_tensors(tensor1, tensor2): # Add your code here # Example usage t1 = torch.randn(2, 3, 4, 5, names=(\'N\', \'C\', \'H\', \'W\')) t2 = torch.randn(2, 3, 1, 5, names=(\'N\', \'C\', \'H\', \'W\')) result = process_named_tensors(t1, t2) print(result.names) # Output should be (\'batch\', \'channel\', \'spatial\') print(result.shape) # Output should reflect the new shape after addition and flattening ``` Additional Information: - You can use `.align_to` to align tensors to the specified order. - Use `.flatten` to combine dimensions into one. - Raise `ValueError` with an appropriate message if dimensions do not match or required dimensions are missing.","solution":"import torch def process_named_tensors(tensor1, tensor2): Processes two named tensors by performing operations: 1. Broadcast addition 2. Dimension alignment 3. Flattening of \'height\' and \'width\' Returns the processed tensor with names (\'batch\', \'channel\', \'spatial\'). required_names = (\'batch\', \'channel\', \'height\', \'width\') if set(tensor1.names) != set(tensor2.names): raise ValueError(\\"Input tensors must have the same dimension names.\\") if set(tensor1.names) != set(required_names): raise ValueError(f\\"Input tensors must have dimensions {\', \'.join(required_names)}.\\") # Broadcast addition tensor_sum = tensor1 + tensor2 # Aligning dimensions to required order tensor_sum = tensor_sum.align_to(*required_names) # Flattening height and width into spatial tensor_flat = tensor_sum.flatten(\'height\', \'width\', \'spatial\') # Renaming dimensions to match output format result = tensor_flat.rename(None) result.names = (\'batch\', \'channel\', \'spatial\') return result"},{"question":"# PyTorch Parallel Processing Assessment Objective: Implement a function using PyTorch that demonstrates the use of both synchronous and asynchronous operations, leveraging inter-op and intra-op parallelism. You will need to configure the number of threads and measure the performance impact of different settings. Problem Statement: You are given a PyTorch model and need to implement an inference function that performs matrix multiplications using both synchronous and asynchronous operations. Your task is to: 1. Implement two functions: one for synchronous matrix multiplication and another for asynchronous matrix multiplication. 2. Configure PyTorch to utilize a specific number of threads for inter-op and intra-op parallelism. 3. Measure and compare the runtime of both functions under different thread settings. Function Signatures: ```python import torch import timeit # Synchronous matrix multiplication def synchronous_mm(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: Perform synchronous matrix multiplication. Args: x: torch.Tensor - First matrix. y: torch.Tensor - Second matrix. Returns: torch.Tensor - The result of matrix multiplication. return torch.mm(x, y) # Asynchronous matrix multiplication using TorchScript @torch.jit.script def compute_mm(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: return torch.mm(x, y) def asynchronous_mm(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: Perform asynchronous matrix multiplication using torch.jit._fork and torch.jit._wait. Args: x: torch.Tensor - First matrix. y: torch.Tensor - Second matrix. Returns: torch.Tensor - The result of matrix multiplication. fut = torch.jit._fork(compute_mm, x, y) z = torch.jit._wait(fut) return z # Experiment function def experiment(x: torch.Tensor, y: torch.Tensor, inter_threads: int, intra_threads: int) -> None: Configure threads and measure runtimes for synchronous and asynchronous matrix multiplications. Args: x: torch.Tensor - First matrix. y: torch.Tensor - Second matrix. inter_threads: int - Number of inter-op threads. intra_threads: int - Number of intra-op threads. # Set the number of threads torch.set_num_interop_threads(inter_threads) torch.set_num_threads(intra_threads) # Measure runtime for synchronous_mm sync_time = timeit.timeit(lambda: synchronous_mm(x, y), number=100) print(f\\"Synchronous MM runtime with {inter_threads} inter-threads and {intra_threads} intra-threads: {sync_time:.6f} seconds\\") # Measure runtime for asynchronous_mm async_time = timeit.timeit(lambda: asynchronous_mm(x, y), number=100) print(f\\"Asynchronous MM runtime with {inter_threads} inter-threads and {intra_threads} intra-threads: {async_time:.6f} seconds\\") # Example usage if __name__ == \\"__main__\\": x = torch.randn(1024, 1024) y = torch.randn(1024, 1024) experiment(x, y, inter_threads=4, intra_threads=4) experiment(x, y, inter_threads=1, intra_threads=1) ``` Constraints: - Ensure that your function handles matrices of size 1024x1024. - Measure the performance for a range of thread settings, such as (inter_threads, intra_threads): (1, 1), (4, 4), (8, 8), etc. - Print the runtime for both synchronous and asynchronous operations under each thread setting. Expected Output: The experiment function should print the runtime for both synchronous and asynchronous matrix multiplications for each combination of thread settings. This will allow you to analyze the impact of different parallelism configurations.","solution":"import torch import timeit # Synchronous matrix multiplication def synchronous_mm(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: Perform synchronous matrix multiplication. Args: x: torch.Tensor - First matrix. y: torch.Tensor - Second matrix. Returns: torch.Tensor - The result of matrix multiplication. return torch.mm(x, y) # Asynchronous matrix multiplication using TorchScript @torch.jit.script def compute_mm(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: return torch.mm(x, y) def asynchronous_mm(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: Perform asynchronous matrix multiplication using torch.jit._fork and torch.jit._wait. Args: x: torch.Tensor - First matrix. y: torch.Tensor - Second matrix. Returns: torch.Tensor - The result of matrix multiplication. fut = torch.jit._fork(compute_mm, x, y) z = torch.jit._wait(fut) return z # Experiment function def experiment(x: torch.Tensor, y: torch.Tensor, inter_threads: int, intra_threads: int) -> None: Configure threads and measure runtimes for synchronous and asynchronous matrix multiplications. Args: x: torch.Tensor - First matrix. y: torch.Tensor - Second matrix. inter_threads: int - Number of inter-op threads. intra_threads: int - Number of intra-op threads. # Set the number of threads torch.set_num_interop_threads(inter_threads) torch.set_num_threads(intra_threads) # Measure runtime for synchronous_mm sync_time = timeit.timeit(lambda: synchronous_mm(x, y), number=100) print(f\\"Synchronous MM runtime with {inter_threads} inter-threads and {intra_threads} intra-threads: {sync_time:.6f} seconds\\") # Measure runtime for asynchronous_mm async_time = timeit.timeit(lambda: asynchronous_mm(x, y), number=100) print(f\\"Asynchronous MM runtime with {inter_threads} inter-threads and {intra_threads} intra-threads: {async_time:.6f} seconds\\") # Example usage if __name__ == \\"__main__\\": x = torch.randn(1024, 1024) y = torch.randn(1024, 1024) experiment(x, y, inter_threads=4, intra_threads=4) experiment(x, y, inter_threads=1, intra_threads=1)"},{"question":"# Question: Implementing a Persistent Storage System Objective: You are tasked with developing a persistent storage system for Python objects using both the \\"pickle\\" and \\"shelve\\" modules. This system should allow users to store any serializable Python object, retrieve it, and list all stored keys. Requirements: 1. Implement the `store_object` function which takes a key and a Python object and stores the object persistently using the specified method (\\"pickle\\" or \\"shelve\\"). 2. Implement the `retrieve_object` function which takes a key and retrieves the corresponding object. 3. Implement the `list_keys` function which lists all the keys of stored objects. Constraints: - Keys are unique strings. - The choice between \\"pickle\\" and \\"shelve\\" should be specified during instantiation of the storage system. - Only serializable objects (i.e., objects that can be pickled) will be provided. - Performance: The system should efficiently handle up to 10,000 objects. Input and Output: - `store_object(key: str, obj: Any)`: - **Input**: A string `key` and a Python object `obj`. - **Output**: None. - `retrieve_object(key: str) -> Any`: - **Input**: A string `key`. - **Output**: The Python object associated with the given `key`. - `list_keys() -> List[str]`: - **Input**: None. - **Output**: A list of all stored keys. Example: ```python # Instantiate the storage system with \\"shelve\\" method storage = PersistentStorage(\\"shelve\\") # Store an object storage.store_object(\\"user1\\", {\\"name\\": \\"Alice\\", \\"age\\": 30}) # Retrieve the object user1 = storage.retrieve_object(\\"user1\\") print(user1) # Output: {\'name\': \'Alice\', \'age\': 30} # List all keys keys = storage.list_keys() print(keys) # Output: [\'user1\'] ``` Implementation Guide: 1. Define the `PersistentStorage` class with an initializer that takes the storage method as a parameter. 2. Use the `pickle` module for object serialization and deserialization when the \\"pickle\\" method is selected. 3. Use the `shelve` module for storing key-value pairs when the \\"shelve\\" method is selected. 4. Ensure the persistence of the objects between separate runs of the program. ```python import pickle import shelve from typing import Any, List class PersistentStorage: def __init__(self, method: str): self.method = method if self.method == \\"pickle\\": self.storage_file = \\"pickle_store.pkl\\" self.data = {} try: with open(self.storage_file, \'rb\') as f: self.data = pickle.load(f) except FileNotFoundError: pass elif self.method == \\"shelve\\": self.storage_file = \'shelve_store\' self.data = shelve.open(self.storage_file) else: raise ValueError(\\"Unsupported storage method\\") def store_object(self, key: str, obj: Any) -> None: self.data[key] = obj if self.method == \\"pickle\\": with open(self.storage_file, \'wb\') as f: pickle.dump(self.data, f) elif self.method == \\"shelve\\": self.data.close() self.data = shelve.open(self.storage_file) def retrieve_object(self, key: str) -> Any: if key in self.data: return self.data[key] else: raise KeyError(f\\"Key \'{key}\' not found\\") def list_keys(self) -> List[str]: return list(self.data.keys()) ```","solution":"import pickle import shelve from typing import Any, List class PersistentStorage: def __init__(self, method: str): self.method = method if self.method not in [\\"pickle\\", \\"shelve\\"]: raise ValueError(\\"Unsupported storage method\\") self.storage_file = \\"storage\\" if self.method == \\"pickle\\": self.storage_file += \\".pkl\\" self.data = {} try: with open(self.storage_file, \'rb\') as f: self.data = pickle.load(f) except FileNotFoundError: pass elif self.method == \\"shelve\\": self.storage_file += \\"_shelve.db\\" self.data = shelve.open(self.storage_file) def store_object(self, key: str, obj: Any) -> None: self.data[key] = obj if self.method == \\"pickle\\": with open(self.storage_file, \'wb\') as f: pickle.dump(self.data, f) elif self.method == \\"shelve\\": self.data.sync() def retrieve_object(self, key: str) -> Any: if key in self.data: return self.data[key] else: raise KeyError(f\\"Key \'{key}\' not found\\") def list_keys(self) -> List[str]: return list(self.data.keys()) def __del__(self): if self.method == \\"shelve\\": self.data.close()"},{"question":"**Background**: You have been tasked with developing an automated email sender that can handle simple emails. Your implementation should use the `smtplib` module\'s functionalities, such as connecting to an SMTP server, sending an email, and managing any exceptions related to the process. **Task**: 1. Implement a function `send_simple_email` that takes the following parameters: - `smtp_server` (str): The SMTP server to connect to. - `from_addr` (str): The sender\'s email address. - `to_addrs` (list): A list of recipient email addresses. - `subject` (str): The subject of the email. - `body` (str): The body of the email. - `username` (str, optional): The username for authenticating with the SMTP server (default is None). - `password` (str, optional): The password for authenticating with the SMTP server (default is None). 2. The function should: - Establish a connection to the SMTP server. - Log in if `username` and `password` are provided. - Send an email with the specified subject and body. - Properly handle exceptions that might arise during the connection and sending process, printing appropriate error messages. **Function Signature**: ```python def send_simple_email(smtp_server: str, from_addr: str, to_addrs: list, subject: str, body: str, username: str = None, password: str = None) -> None: ``` **Constraints**: - The function should properly handle exceptions including but not limited to `smtplib.SMTPHeloError`, `smtplib.SMTPSenderRefused`, `smtplib.SMTPRecipientsRefused`, `smtplib.SMTPDataError`, and `smtplib.SMTPAuthenticationError`. - The function should print \\"Email sent successfully\\" if the email is sent without any issues. - If the SMTP server does not support the authentication mechanism or has issues with the provided credentials, it should print \\"Authentication failed\\". **Example**: ```python smtp_server = \'smtp.example.com\' from_addr = \'sender@example.com\' to_addrs = [\'recipient1@example.com\', \'recipient2@example.com\'] subject = \'Test Subject\' body = \'This is the body of the email\' username = \'user@example.com\' password = \'password123\' send_simple_email(smtp_server, from_addr, to_addrs, subject, body, username, password) ``` Expected Output: ``` Email sent successfully ``` If there is an error (e.g., invalid credentials or network issues), the function should print an appropriate error message.","solution":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart def send_simple_email(smtp_server: str, from_addr: str, to_addrs: list, subject: str, body: str, username: str = None, password: str = None) -> None: try: # Create a MIMEText object to represent the email msg = MIMEMultipart() msg[\'From\'] = from_addr msg[\'To\'] = \\", \\".join(to_addrs) msg[\'Subject\'] = subject msg.attach(MIMEText(body, \'plain\')) # Connect to the SMTP server with smtplib.SMTP(smtp_server) as server: server.starttls() # Upgrade the connection to a secure encrypted SSL/TLS connection if username and password: server.login(username, password) server.sendmail(from_addr, to_addrs, msg.as_string()) print(\\"Email sent successfully\\") except smtplib.SMTPHeloError: print(\\"The server didn\'t reply properly to the HELO greeting\\") except smtplib.SMTPSenderRefused: print(\\"The server refused the sender address\\") except smtplib.SMTPRecipientsRefused: print(\\"The server refused the recipient address(es)\\") except smtplib.SMTPDataError: print(\\"The server replied with an unexpected error code\\") except smtplib.SMTPAuthenticationError: print(\\"Authentication failed\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Seaborn Color Palette Manipulation Challenge Objective: To demonstrate your understanding of Seaborn\'s `sns.mpl_palette` function and its usage in manipulating color palettes for data visualization. Problem Statement: You are required to implement a function `create_custom_palette` that accepts a colormap name, a number of colors, and a boolean flag to determine if the palette should be continuous or discrete. The function must return the appropriate color palette as specified by the parameters. Function Signature: ```python def create_custom_palette(colormap_name: str, num_colors: int, as_cmap: bool = False) -> Union[List[str], matplotlib.colors.Colormap]: pass ``` Input: - `colormap_name` (str): The name of the Matplotlib colormap (e.g., \\"viridis\\", \\"Set2\\"). - `num_colors` (int): The number of colors to be returned from the colormap. - `as_cmap` (bool): A boolean flag indicating whether to return a continuous colormap (`True`) or discrete samples (`False`). Defaults to `False`. Output: - The function should return: - A list of HEX color codes if `as_cmap` is `False`. - A continuous colormap object if `as_cmap` is `True`. Examples: ```python # Example 1: Discrete palette from \\"viridis\\" with 5 colors print(create_custom_palette(\\"viridis\\", 5)) # Expected Output: A list of 5 HEX color codes from the \\"viridis\\" colormap # Example 2: Continuous colormap from \\"viridis\\" print(create_custom_palette(\\"viridis\\", 5, as_cmap=True)) # Expected Output: A continuous colormap object for \\"viridis\\" # Example 3: Discrete palette from \\"Set2\\" with 4 colors print(create_custom_palette(\\"Set2\\", 4)) # Expected Output: A list of 4 HEX color codes from the \\"Set2\\" colormap # Example 4: Continuous colormap from \\"Set2\\" print(create_custom_palette(\\"Set2\\", 4, as_cmap=True)) # Expected Output: A continuous colormap object for \\"Set2\\" ``` Constraints: - The colormap name must be a valid Matplotlib colormap name. - `num_colors` should be a positive integer. - If the colormap cannot provide the requested number of colors, return all available colors. Notes: - Utilize the `sns.mpl_palette` function from Seaborn to achieve the desired functionality. - Ensure proper handling of both continuous and discrete colormap retrieval as outlined in the Seaborn documentation.","solution":"import seaborn as sns import matplotlib.pyplot as plt from typing import Union, List import matplotlib.colors as mcolors def create_custom_palette(colormap_name: str, num_colors: int, as_cmap: bool = False) -> Union[List[str], mcolors.Colormap]: Create a custom palette based on a colormap name, number of colors and a flag to determine if the palette should be continuous or discrete. Parameters: - colormap_name (str): The name of the Matplotlib colormap (e.g., \\"viridis\\", \\"Set2\\"). - num_colors (int): The number of colors to be returned from the colormap. - as_cmap (bool): A boolean flag indicating whether to return a continuous colormap (`True`) or discrete samples (`False`). Returns: - List[str]: If `as_cmap` is `False`, return a list of HEX color codes. - mcolors.Colormap: If `as_cmap` is `True`, return a continuous colormap object. if as_cmap: cmap = sns.color_palette(colormap_name, as_cmap=True) return cmap else: palette = sns.color_palette(colormap_name, num_colors) hex_palette = palette.as_hex() return hex_palette"},{"question":"# Coding Challenge: Advanced Bar Plotting with Seaborn **Objective:** You are required to create an advanced bar plot using the `seaborn` library that demonstrates your ability to utilize multiple facets of the library, including handling overlapping bars and adding error bars. **Problem Statement:** Given the `penguins` dataset from the seaborn library, create a plot that displays the average body mass (`body_mass_g`) of penguins grouped by island and species. The plot should include: 1. Bars colored by species. 2. Bars dodged by island. 3. Error bars representing one standard deviation from the mean. 4. Bars with a specific edge width and style. **Instructions:** 1. Load the `penguins` dataset using the `load_dataset` function from seaborn. 2. Create a bar plot with: - X-axis representing the `island`. - Y-axis representing the average `body_mass_g`. - Color encoding for different species. - Bars dodged by island. 3. Add error bars to depict one standard deviation from the mean. 4. Customize the bar edges with a width of 2 and differentiate them by species. **Expected Function Implementation:** ```python import seaborn.objects as so from seaborn import load_dataset def plot_advanced_penguin_bar_chart(): # Load the penguins dataset penguins = load_dataset(\'penguins\') # Create the plot plot = ( so.Plot(penguins, x=\'island\', y=\'body_mass_g\', color=\'species\') .add(so.Bar(edgewidth=2), so.Agg(), so.Dodge(\'species\')) .add(so.Range(), so.Est(errorbar=\'sd\'), so.Dodge()) ) # Display the plot plot.show() # Call the function to display the plot plot_advanced_penguin_bar_chart() ``` - **Input:** None - **Output:** A plot visualizing the average body mass of penguins grouped by island and species, with error bars and customized bar edges. **Constraints:** - Ensure that the plot aesthetics are clear and informative. - Handle any potential missing or NA values in the dataset appropriately. **Performance Requirements:** - The function should run efficiently, taking into consideration the size of the dataset and the rendering time of the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_advanced_penguin_bar_chart(): # Load the penguins dataset penguins = load_dataset(\'penguins\') # Create the plot plot = ( so.Plot(penguins, x=\'island\', y=\'body_mass_g\', color=\'species\') .add(so.Bar(edgewidth=2), so.Agg(), so.Dodge(\'species\')) .add(so.Range(), so.Est(errorbar=\'sd\'), so.Dodge()) ) # Display the plot plot.show() # Call the function to display the plot plot_advanced_penguin_bar_chart()"},{"question":"# Advanced Event Scheduling and Management You are tasked with developing a small event scheduling and handling system using Python\'s `sched` module. Your solution should demonstrate a good understanding of scheduling multiple events, handling their execution based on priority, and the ability to cancel events if needed. Function Implementations 1. **schedule_events(scheduler, events)** - **Input**: - `scheduler`: An instance of `sched.scheduler`. - `events`: A list of tuples where each tuple contains the following elements in order: - A time/delay (numeric, specifying when the event should occur). - A priority number (lower number means higher priority). - An action (callable, the function to execute). - Positional arguments for the action (tuple). - Keyword arguments for the action (dictionary). - An indicator (\\"abs\\" or \\"rel\\") whether the time is absolute or relative. - **Output**: None. - **Constraints**: Events should be scheduled using `enterabs` or `enter` method based on the indicator (\\"abs\\" for `enterabs`, \\"rel\\" for `enter`). Handle the scheduling appropriately. 2. **execute_events(scheduler)** - **Input**: - `scheduler`: An instance of `sched.scheduler` with events already scheduled. - **Output**: A list of tuples representing the results of the actions executed. Each tuple contains: - The time the event was executed. - The result of the event action. - **Constraints**: Ensure that all events are executed in order and capture the result and execution time. 3. **cancel_event(scheduler, event)** - **Input**: - `scheduler`: An instance of `sched.scheduler`. - `event`: The event to cancel (as returned by `scheduler.enter` or `scheduler.enterabs`). - **Output**: None. - **Constraints**: Cancel the event if it exists in the queue. If it does not, handle the potential `ValueError` gracefully. Example ```python import sched import time s = sched.scheduler(time.time, time.sleep) def sample_action(a): return a events = [ (time.time() + 10, 1, sample_action, (\\"event1\\",), {}, \'abs\'), (5, 2, sample_action, (\\"event2\\",), {}, \'rel\'), (time.time() + 15, 1, sample_action, (\\"event3\\",), {}, \'abs\'), (10, 1, sample_action, (\\"event4\\",), {}, \'rel\'), ] # Scheduling events schedule_events(s, events) # Cancel an event (optional) event_to_cancel = s.enter(20, 1, sample_action, (\\"event5\\",)) cancel_event(s, event_to_cancel) # Executing events and capturing results results = execute_events(s) for time_executed, result in results: print(f\\"Executed at: {time_executed}, Result: {result}\\") ``` Your task is to implement the functions `schedule_events`, `execute_events`, and `cancel_event` to ensure proper scheduling, execution, and cancellation of events. Ensure that the order of execution respects both time and priority constraints.","solution":"import sched import time def schedule_events(scheduler, events): Schedule events based on the provided list of event details. Parameters: - scheduler: An instance of sched.scheduler. - events: A list of tuples containing event details. for event in events: if event[-1] == \'abs\': scheduler.enterabs(event[0], event[1], event[2], event[3], event[4]) elif event[-1] == \'rel\': scheduler.enter(event[0], event[1], event[2], event[3], event[4]) def execute_events(scheduler): Execute all scheduled events and return their results with execution times. Parameters: - scheduler: An instance of sched.scheduler with events already scheduled. Returns: - A list of tuples containing execution time and result of each event. executed_results = [] def wrapper(action, args, kwargs): result = action(*args, **kwargs) executed_results.append((time.time(), result)) # Run the scheduler while scheduler.queue: event = scheduler.queue[0] scheduler.run(blocking=False) wrapper(event.action, event.argument, event.kwargs) return executed_results def cancel_event(scheduler, event): Cancel a scheduled event if it exists in the queue. Parameters: - scheduler: An instance of sched.scheduler. - event: The event to cancel. try: scheduler.cancel(event) except ValueError: print(\\"Event not found in the queue.\\")"},{"question":"# Question: Tuple Manipulation using Python C API You are required to write a function that creates and manipulates Python tuple objects using the `PyTuple_*` functions in Python C API. Your task is to implement a function `manipulate_tuples` that performs the following steps: 1. Create a new tuple of a given size. 2. Populate the tuple with elements from a given list. 3. Get and return the size of the tuple. 4. Retrieve an element from a specified position in the tuple. 5. Set a new element at a specified position in the tuple. 6. Resize the tuple to a new size. Function Signature: ```python def manipulate_tuples(initial_size: int, elements: list, get_pos: int, set_pos: int, new_element, new_size: int) -> dict: Args: - initial_size (int): The initial size of the tuple. - elements (list): A list of elements to populate the tuple. - get_pos (int): The position of the element to retrieve from the tuple. - set_pos (int): The position at which to set a new element in the tuple. - new_element: The new element to set at the specified position. - new_size (int): The new size to which the tuple should be resized. Returns: - dict: A dictionary with the following keys and values: * \\"initial_tuple\\": The initially created tuple. * \\"size_of_initial_tuple\\": The size of the initial tuple. * \\"retrieved_element\\": The element retrieved from the specified position. * \\"modified_tuple\\": The tuple after setting the new element. * \\"resized_tuple\\": The tuple after resizing. pass ``` Constraints: - The given list `elements` will always contain enough elements to populate the initial tuple. - `get_pos` and `set_pos` will always be valid positions within the bounds of the initial tuple. - The new size for resizing the tuple will be larger than or equal to the initial size. Example: ```python result = manipulate_tuples(3, [1, 2, 3], 1, 2, 10, 5) print(result) ``` Expected output: ```python { \\"initial_tuple\\": (1, 2, 3), \\"size_of_initial_tuple\\": 3, \\"retrieved_element\\": 2, \\"modified_tuple\\": (1, 2, 10), \\"resized_tuple\\": (1, 2, 10, None, None) } ``` Notes: - You are not required to handle memory management manually; assume all necessary memory is allocated correctly. - Make use of the provided `PyTuple_*` functions to manipulate the tuples.","solution":"def manipulate_tuples(initial_size: int, elements: list, get_pos: int, set_pos: int, new_element, new_size: int) -> dict: Args: - initial_size (int): The initial size of the tuple. - elements (list): A list of elements to populate the tuple. - get_pos (int): The position of the element to retrieve from the tuple. - set_pos (int): The position at which to set a new element in the tuple. - new_element: The new element to set at the specified position. - new_size (int): The new size to which the tuple should be resized. Returns: - dict: A dictionary with the following keys and values: * \\"initial_tuple\\": The initially created tuple. * \\"size_of_initial_tuple\\": The size of the initial tuple. * \\"retrieved_element\\": The element retrieved from the specified position. * \\"modified_tuple\\": The tuple after setting the new element. * \\"resized_tuple\\": The tuple after resizing. initial_tuple = tuple(elements[:initial_size]) size_of_initial_tuple = len(initial_tuple) retrieved_element = initial_tuple[get_pos] modified_list = list(initial_tuple) modified_list[set_pos] = new_element modified_tuple = tuple(modified_list) resized_list = list(modified_tuple) + [None] * (new_size - len(modified_tuple)) resized_tuple = tuple(resized_list) return { \\"initial_tuple\\": initial_tuple, \\"size_of_initial_tuple\\": size_of_initial_tuple, \\"retrieved_element\\": retrieved_element, \\"modified_tuple\\": modified_tuple, \\"resized_tuple\\": resized_tuple }"},{"question":"# Question: Advanced Seaborn Plot Customization with Jitter You are given a dataset containing information about penguins including their species, body mass and flipper length. Utilizing the `seaborn.objects` module, create a customized jitter plot that satisfies the following conditions: 1. Create a jitter plot with species on the x-axis and body mass (`body_mass_g`) on the y-axis. 2. Apply a width of 0.3 to the jitter along the x-axis. 3. Create another jitter plot with the body mass (`body_mass_g`) on the x-axis and species on the y-axis. 4. Apply a width of 0.6 to the jitter along the y-axis. 5. Create a third plot with body mass (`body_mass_g`) rounded to the nearest 1000 on the x-axis and flipper length (`flipper_length_mm`) rounded to the nearest 10 on the y-axis. 6. Apply both x and y jitter transforms such that jitter along the x-axis is 200 units and along the y-axis is 10 units. You must use the Seaborn `objects` module to accomplish this and ensure that the plots are displayed one below the other for comparison. **Input Format:** The dataset is provided as a CSV file named `penguins.csv` with columns: `species`, `body_mass_g`, and `flipper_length_mm`. **Output Format:** A function `custom_jitter_plots` should be implemented which creates the jitter plots as specified and displays them. ```python import seaborn.objects as so import pandas as pd import matplotlib.pyplot as plt def custom_jitter_plots(): penguins = pd.read_csv(\\"penguins.csv\\") # Plot 1: species vs body_mass_g with width jitter on x-axis p1 = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.3)) ) # Plot 2: body_mass_g vs species with width jitter on y-axis p2 = ( so.Plot(penguins, \\"body_mass_g\\", \\"species\\") .add(so.Dots(), so.Jitter(width=0.6)) ) # Plot 3: body_mass_g rounded and flipper_length_mm rounded with specific jitter p3 = ( so.Plot( penguins[\\"body_mass_g\\"].round(-3), penguins[\\"flipper_length_mm\\"].round(-1) ) .add(so.Dots(), so.Jitter(x=200, y=10)) ) # Display plots one below the other plt.figure(figsize=(10, 15)) plt.subplot(3, 1, 1) p1.show() plt.subplot(3, 1, 2) p2.show() plt.subplot(3, 1, 3) p3.show() plt.tight_layout() plt.show() ``` You need to make sure all plots are displayed clearly in a single figure for easy comparison. **Constraints:** - You should use Seaborn\'s `objects` module. - Ensure the jitter parameters are correctly placed for different orientations. - The dataset will be small enough to not pose any performance issues. **Note:** You can assume the availability of the library seaborn and other required libraries in your environment.","solution":"import seaborn.objects as so import pandas as pd import matplotlib.pyplot as plt def custom_jitter_plots(): penguins = pd.read_csv(\\"penguins.csv\\") # Plot 1: species vs body_mass_g with width jitter on x-axis p1 = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter(width=0.3)) ) # Plot 2: body_mass_g vs species with width jitter on y-axis p2 = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\") .add(so.Dots(), so.Jitter(width=0.6)) ) # Plot 3: body_mass_g rounded and flipper_length_mm rounded with specific jitter p3 = ( so.Plot( penguins.assign(body_mass_g_rounded=penguins[\\"body_mass_g\\"].round(-3), flipper_length_mm_rounded=penguins[\\"flipper_length_mm\\"].round(-1)), x=\\"body_mass_g_rounded\\", y=\\"flipper_length_mm_rounded\\" ) .add(so.Dots(), so.Jitter(x=200, y=10)) ) # Display plots one below the other plt.figure(figsize=(10, 15)) plt.subplot(3, 1, 1) p1.show() plt.subplot(3, 1, 2) p2.show() plt.subplot(3, 1, 3) p3.show() plt.tight_layout() plt.show()"},{"question":"**Question:** You are tasked with analyzing the \\"diamonds\\" dataset using the seaborn library. Your objective is to create multiple customized visualizations that provide insights into the distribution of diamond prices based on various categorical variables. # Instructions: 1. **Data Loading and Preparation:** - Load the \\"diamonds\\" dataset using seaborn\'s `load_dataset` function. 2. **Visualization Tasks:** - **Task 1:** Create a basic horizontal boxen plot displaying the distribution of diamond prices. - **Parameters:** Use the `boxenplot` function and plot the `price` variable on the x-axis. - **Task 2:** Create a vertical boxen plot grouping by the `clarity` variable. - **Parameters:** Plot the `price` variable on the x-axis and `clarity` on the y-axis. - **Task 3:** Create a boxen plot that further groups the data by whether the diamond is considered large. - **Parameters:** A diamond is considered large if its `carat` is greater than 1. Use this condition to create a new Boolean column `large_diamond`. - **Requirement:** Plot the `price` variable on the x-axis, `clarity` on the y-axis, and use the `large_diamond` variable for the `hue` parameter with a small gap to separate the boxes. - **Task 4:** Customize the box widths by using a linear reduction method. - **Parameters:** Plot the `price` variable on the x-axis and `clarity` on the y-axis. Set the `width_method` parameter to \\"linear\\". - **Task 5:** Apply further customizations: - **Parameters:** Plot the `price` variable on the x-axis, `clarity` on the y-axis, set the line color to a shade of grey (`.7`), and adjust the line width to 0.5. Additionally, customize the median line and outliers\' appearances using the `line_kws` and `flier_kws` parameters, respectively. The median line should be thicker and colored light blue `#cde`, and the outliers should be in grey (`.7`) with a line width of 0.5. # Example Implementation: ```python import seaborn as sns import matplotlib.pyplot as plt # Set theme sns.set_theme(style=\\"whitegrid\\") # Load dataset diamonds = sns.load_dataset(\\"diamonds\\") # Task 1: Basic horizontal boxen plot plt.figure(figsize=(12, 6)) sns.boxenplot(x=diamonds[\\"price\\"]) plt.title(\\"Distribution of Diamond Prices\\") plt.show() # Task 2: Vertical boxen plot grouped by clarity plt.figure(figsize=(12, 6)) sns.boxenplot(data=diamonds, x=\\"price\\", y=\\"clarity\\") plt.title(\\"Diamond Prices grouped by Clarity\\") plt.show() # Task 3: Boxen plot grouped by clarity and large_diamond diamonds[\\"large_diamond\\"] = diamonds[\\"carat\\"] > 1 plt.figure(figsize=(12, 6)) sns.boxenplot(data=diamonds, x=\\"price\\", y=\\"clarity\\", hue=\\"large_diamond\\", gap=.2) plt.title(\\"Diamond Prices grouped by Clarity and Size\\") plt.show() # Task 4: Boxen plot with linear width reduction plt.figure(figsize=(12, 6)) sns.boxenplot(data=diamonds, x=\\"price\\", y=\\"clarity\\", width_method=\\"linear\\") plt.title(\\"Diamond Prices grouped by Clarity with Linear Width Reduction\\") plt.show() # Task 5: Enhanced customization plt.figure(figsize=(12, 6)) sns.boxenplot( data=diamonds, x=\\"price\\", y=\\"clarity\\", linewidth=.5, linecolor=\\".7\\", line_kws=dict(linewidth=1.5, color=\\"#cde\\"), flier_kws=dict(facecolor=\\".7\\", linewidth=.5), ) plt.title(\\"Customized Diamond Prices grouped by Clarity\\") plt.show() ``` # Submission: Submit your code in a Python script or a Jupyter notebook format (.py/.ipynb). **Note:** Ensure your visualizations have appropriate titles and axis labels.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_data(): Load the diamonds dataset using seaborn\'s `load_dataset` function. return sns.load_dataset(\\"diamonds\\") def plot_horizontal_boxen(diamonds): Create a basic horizontal boxen plot displaying the distribution of diamond prices. plt.figure(figsize=(12, 6)) sns.boxenplot(x=diamonds[\\"price\\"]) plt.title(\\"Distribution of Diamond Prices\\") plt.show() def plot_vertical_boxen_by_clarity(diamonds): Create a vertical boxen plot grouping by the clarity variable. plt.figure(figsize=(12, 6)) sns.boxenplot(data=diamonds, x=\\"price\\", y=\\"clarity\\") plt.title(\\"Diamond Prices grouped by Clarity\\") plt.show() def plot_boxen_by_clarity_and_size(diamonds): Create a boxen plot that further groups the data by whether the diamond is considered large. diamonds[\\"large_diamond\\"] = diamonds[\\"carat\\"] > 1 plt.figure(figsize=(12, 6)) sns.boxenplot(data=diamonds, x=\\"price\\", y=\\"clarity\\", hue=\\"large_diamond\\", gap=.2) plt.title(\\"Diamond Prices grouped by Clarity and Size\\") plt.show() def plot_linear_width_boxen(diamonds): Customize the box widths by using a linear reduction method. plt.figure(figsize=(12, 6)) sns.boxenplot(data=diamonds, x=\\"price\\", y=\\"clarity\\", width_method=\\"linear\\") plt.title(\\"Diamond Prices grouped by Clarity with Linear Width Reduction\\") plt.show() def plot_customized_boxen(diamonds): Create enhanced customized boxen plot. plt.figure(figsize=(12, 6)) sns.boxenplot( data=diamonds, x=\\"price\\", y=\\"clarity\\", linewidth=.5, linecolor=\\".7\\", line_kws=dict(linewidth=1.5, color=\\"#cde\\"), flier_kws=dict(facecolor=\\".7\\", linewidth=.5), ) plt.title(\\"Customized Diamond Prices grouped by Clarity\\") plt.show()"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of pandas options and settings by configuring global settings using the pandas options API. # Task You are given a dataset in CSV format named `student_scores.csv` which contains student names and their corresponding scores in various subjects. Your task is to write a function `configure_pandas_options` that performs the following: 1. Sets the float format to scientific notation with 3 decimal places using `set_eng_float_format`. 2. Limits the maximum number of columns displayed to 10. 3. Ensures that long strings in dataframe cells are fully displayed without truncation. 4. Creates and returns a DataFrame from `student_scores.csv`. # Input - `file_path` (str): A string representing the file path to the `student_scores.csv` file. # Output - A pandas DataFrame created from the `student_scores.csv` file with the specified options applied. # Example Usage ```python def configure_pandas_options(file_path: str) -> pd.DataFrame: # Implement the function here # Assuming the file is located at \\"data/student_scores.csv\\" df = configure_pandas_options(\\"data/student_scores.csv\\") # Check the DataFrame print(df) ``` # Constraints - The `student_scores.csv` file must exist in the specified path. - Use the appropriate pandas options API functions to set the options. - Ensure that your implementation does not permanently alter the default settings beyond the scope of this function. # Notes 1. You may assume that `pandas` has been imported as `pd`. 2. You should import any additional modules only if necessary. 3. Hint: Use a context manager if needed to temporarily set options.","solution":"import pandas as pd def configure_pandas_options(file_path: str) -> pd.DataFrame: Configures pandas options and reads a CSV into a DataFrame. Parameters: file_path (str): The path to the student_scores.csv file. Returns: pd.DataFrame: The DataFrame created from the CSV file. # Configure pandas options pd.set_option(\'display.float_format\', lambda x: f\'{x:.3e}\') pd.set_option(\'display.max_columns\', 10) pd.set_option(\'display.max_colwidth\', None) # Load the DataFrame from the CSV file df = pd.read_csv(file_path) # Return the DataFrame return df"},{"question":"Create a Python script named `calculator.py` with the following requirements: 1. The script should define a function `add(a, b)` that returns the sum of `a` and `b`. 2. The script should define a function `subtract(a, b)` that returns the result of `a - b`. 3. Define a `main` function within the script that performs the following tasks: - Parses command-line arguments to determine the operation (`add` or `subtract`) and the two numeric arguments. - Calls the appropriate function (`add` or `subtract`) based on the command-line arguments and prints the result. 4. Use the following command-line argument format: - `python calculator.py add 4 5` should output `9`. - `python calculator.py subtract 10 3` should output `7`. 5. The script should include the typical `if __name__ == \'__main__\':` block to ensure the `main` function is only called when the script is executed directly. 6. Ensure the script is organized such that the `add` and `subtract` functions can be imported and used in other modules without executing the main logic of the script. Example Usage ```sh python calculator.py add 4 5 9 python calculator.py subtract 10 3 7 ``` Constraints - The script should handle invalid inputs gracefully, providing suitable error messages for incorrect operation names or non-numeric arguments. - Assume the operations will only be `add` or `subtract`. # Solution Template ```python import sys def add(a: int, b: int) -> int: return a + b def subtract(a: int, b: int) -> int: return a - b def main() -> int: if len(sys.argv) != 4: print(\\"Usage: python calculator.py <operation> <num1> <num2>\\") return 1 operation = sys.argv[1] try: num1 = int(sys.argv[2]) num2 = int(sys.argv[3]) except ValueError: print(\\"Please provide two numeric arguments.\\") return 1 if operation == \'add\': print(add(num1, num2)) elif operation == \'subtract\': print(subtract(num1, num2)) else: print(\\"Invalid operation. Only \'add\' and \'subtract\' are supported.\\") return 1 return 0 if __name__ == \'__main__\': sys.exit(main()) ``` This solution template provides the basic framework and students are expected to fill in the details to handle the command-line arguments and perform error checking appropriately.","solution":"import sys def add(a: int, b: int) -> int: Returns the sum of a and b. return a + b def subtract(a: int, b: int) -> int: Returns the result of a - b. return a - b def main() -> int: if len(sys.argv) != 4: print(\\"Usage: python calculator.py <operation> <num1> <num2>\\") return 1 operation = sys.argv[1] try: num1 = int(sys.argv[2]) num2 = int(sys.argv[3]) except ValueError: print(\\"Please provide two numeric arguments.\\") return 1 if operation == \'add\': print(add(num1, num2)) elif operation == \'subtract\': print(subtract(num1, num2)) else: print(\\"Invalid operation. Only \'add\' and \'subtract\' are supported.\\") return 1 return 0 if __name__ == \'__main__\': sys.exit(main())"},{"question":"**Objective:** Implement a function that merges multiple DataFrames in different ways and addresses a specific data analysis scenario. **Scenario:** You are provided with three DataFrames representing different datasets. Each DataFrame contains shared and unique data that must be combined meaningfully for analysis. **Requirements:** 1. Merge the three DataFrames using various join types and options. 2. Address scenarios where columns contain overlapping or unique data. 3. Ensure any missing values are filled appropriately. **Function Signature:** ```python import pandas as pd def combined_datasets(df1: pd.DataFrame, df2: pd.DataFrame, df3: pd.DataFrame) -> pd.DataFrame: Performs comprehensive merging operations using three DataFrames and returns the final combined DataFrame. Parameters: - df1: A DataFrame containing primary data. - df2: A DataFrame containing secondary data with potential overlapping and unique columns. - df3: A DataFrame containing tertiary data with completely unique columns. Returns: - A DataFrame that represents the combined result of all three DataFrames with appropriate merge strategies applied. # Example DataFrames: df1 = pd.DataFrame({ \\"A\\": [\\"A1\\", \\"A2\\", \\"A3\\"], \\"B\\": [\\"B1\\", \\"B2\\", \\"B3\\"], \\"key\\": [\\"K1\\", \\"K2\\", \\"K3\\"] }) df2 = pd.DataFrame({ \\"B\\": [\\"B2\\", \\"B4\\", \\"B5\\"], \\"D\\": [\\"D2\\", \\"D4\\", \\"D5\\"], \\"key\\": [\\"K1\\", \\"K2\\", \\"K4\\"] }) df3 = pd.DataFrame({ \\"E\\": [\\"E1\\", \\"E2\\", \\"E3\\"], \\"F\\": [\\"F1\\", \\"F2\\", \\"F3\\"], \\"G\\": [\\"G1\\", \\"G2\\", \\"G3\\"] }) # Expected Output: A DataFrame that combines df1, df2, df3 meaningfully using different merging techniques. ``` **Constraints:** 1. Handle missing values in df1 and df2 explicitly. 2. Consider different types of joins while merging. 3. Preserve index from df1 where necessary. **Hint:** Refer to the `pd.concat`, `pd.merge`, and `df.combine_first` methods from the pandas documentation to accomplish the task. **Performance Requirements:** 1. Ensure the function is efficient and handles operations with large DataFrames efficiently.","solution":"import pandas as pd def combined_datasets(df1: pd.DataFrame, df2: pd.DataFrame, df3: pd.DataFrame) -> pd.DataFrame: Performs comprehensive merging operations using three DataFrames and returns the final combined DataFrame. Parameters: - df1: A DataFrame containing primary data. - df2: A DataFrame containing secondary data with potential overlapping and unique columns. - df3: A DataFrame containing tertiary data with completely unique columns. Returns: - A DataFrame that represents the combined result of all three DataFrames with appropriate merge strategies applied. # Merge df1 and df2 on the \\"key\\" column using an outer join. df1_df2_merged = pd.merge(df1, df2, on=\\"key\\", how=\\"outer\\", suffixes=(\'_df1\', \'_df2\')) # Fill missing values df1_df2_merged.fillna(value={\'B_df1\': \'\', \'B_df2\': \'\', \'A\': \'\', \'D\': \'\'}, inplace=True) # Combine with df3 using a join on default index final_merged = df1_df2_merged.join(df3) return final_merged"},{"question":"# Covariance Estimation and Comparison using scikit-learn In this task, you will be working with the `sklearn.covariance` module to estimate covariance matrices using different techniques and compare their performance on a sample dataset. Given a dataset `X` (a two-dimensional array where each row is a sample with several features), perform the following tasks: 1. **Empirical Covariance**: Calculate the empirical covariance matrix using both the `empirical_covariance` function and the `EmpiricalCovariance` class. Compare the results to ensure they match. 2. **Shrunk Covariance**: - Calculate the shrunk covariance matrix using a manually defined shrinkage coefficient `alpha=0.1` with the `shrunk_covariance` function. - Fit a `LedoitWolf` model to the data and obtain the covariance matrix. Compare it with the manually shrunk covariance. 3. **Sparse Inverse Covariance**: - Fit a `GraphicalLassoCV` model to the data to find the optimal sparsity level and obtain the precision matrix. - Convert the precision matrix back to a covariance matrix. 4. **Robust Covariance Estimation**: - Fit a `MinCovDet` model to the data to get a robust estimation of the covariance matrix, which should be less sensitive to outliers. 5. **Comparison**: Create a function that computes and prints the Mahalanobis distances of a set of test points for each of the calculated covariance matrices. Use this function to compare the performance of the different covariance matrices. # Input format: - `X`: A numpy array of shape (n_samples, n_features) representing the dataset. - `test_points`: A numpy array of shape (n_test_samples, n_features) representing the test points for Mahalanobis distance computation. # Output format: - Print the covariance matrices calculated using the different methods. - Print the Mahalanobis distances of the test points for each covariance matrix. # Constraints: - Assume `X` has more samples than features (`n_samples` > `n_features`). - Use `assume_centered=True` for all covariance calculations to ensure consistent comparison. - Performance is a key aspect; ensure that the implemented code is efficient. # Example code to implement: ```python import numpy as np from sklearn.covariance import (empirical_covariance, EmpiricalCovariance, shrunk_covariance, LedoitWolf, GraphicalLassoCV, MinCovDet) def compare_covariances(X, test_points): # 1. Empirical Covariance emp_cov_matrix_func = empirical_covariance(X, assume_centered=True) emp_cov = EmpiricalCovariance(assume_centered=True).fit(X).covariance_ assert np.allclose(emp_cov_matrix_func, emp_cov), \\"Empirical Covariance mismatch!\\" # Print Empirical Covariance print(\\"Empirical Covariance Matrix:n\\", emp_cov) # 2. Shrunk Covariance shrunk_cov_matrix = shrunk_covariance(emp_cov, shrinkage=0.1) lw_cov = LedoitWolf(assume_centered=True).fit(X).covariance_ # Print Shrunk Covariance print(\\"Shrunk Covariance Matrix (alpha=0.1):n\\", shrunk_cov_matrix) print(\\"Ledoit-Wolf Covariance Matrix:n\\", lw_cov) # 3. Sparse Inverse Covariance graphical_lasso = GraphicalLassoCV().fit(X) sparse_prec_matrix = graphical_lasso.precision_ sparse_cov_matrix = np.linalg.inv(sparse_prec_matrix) # Print Sparse Covariance print(\\"Sparse Covariance Matrix (Graphical Lasso):n\\", sparse_cov_matrix) # 4. Robust Covariance mcd = MinCovDet(assume_centered=True).fit(X) robust_cov_matrix = mcd.covariance_ # Print Robust Covariance print(\\"Robust Covariance Matrix (MinCovDet):n\\", robust_cov_matrix) # 5. Comparison on Mahalanobis distances def mahalanobis_distance(cov_matrix, points): inv_cov_matrix = np.linalg.inv(cov_matrix) mean = np.mean(X, axis=0) distances = [] for point in points: diff = point - mean dist = np.sqrt(diff.T @ inv_cov_matrix @ diff) distances.append(dist) return distances print(\\"Mahalanobis distances for test points (Empirical):\\", mahalanobis_distance(emp_cov, test_points)) print(\\"Mahalanobis distances for test points (Shrunk manual):\\", mahalanobis_distance(shrunk_cov_matrix, test_points)) print(\\"Mahalanobis distances for test points (Ledoit-Wolf):\\", mahalanobis_distance(lw_cov, test_points)) print(\\"Mahalanobis distances for test points (Sparse):\\", mahalanobis_distance(sparse_cov_matrix, test_points)) print(\\"Mahalanobis distances for test points (Robust):\\", mahalanobis_distance(robust_cov_matrix, test_points)) # Example usage X = np.random.randn(100, 5) # Sample dataset test_points = np.random.randn(10, 5) # Test points compare_covariances(X, test_points) ```","solution":"import numpy as np from sklearn.covariance import (empirical_covariance, EmpiricalCovariance, shrunk_covariance, LedoitWolf, GraphicalLassoCV, MinCovDet) def compare_covariances(X, test_points): # 1. Empirical Covariance emp_cov_matrix_func = empirical_covariance(X, assume_centered=True) emp_cov = EmpiricalCovariance(assume_centered=True).fit(X).covariance_ assert np.allclose(emp_cov_matrix_func, emp_cov), \\"Empirical Covariance mismatch!\\" print(\\"Empirical Covariance Matrix:n\\", emp_cov) # 2. Shrunk Covariance shrunk_cov_matrix = shrunk_covariance(emp_cov, shrinkage=0.1) lw_cov = LedoitWolf(assume_centered=True).fit(X).covariance_ print(\\"Shrunk Covariance Matrix (alpha=0.1):n\\", shrunk_cov_matrix) print(\\"Ledoit-Wolf Covariance Matrix:n\\", lw_cov) # 3. Sparse Inverse Covariance graphical_lasso = GraphicalLassoCV().fit(X) sparse_prec_matrix = graphical_lasso.precision_ sparse_cov_matrix = np.linalg.inv(sparse_prec_matrix) print(\\"Sparse Covariance Matrix (Graphical Lasso):n\\", sparse_cov_matrix) # 4. Robust Covariance mcd = MinCovDet(assume_centered=True).fit(X) robust_cov_matrix = mcd.covariance_ print(\\"Robust Covariance Matrix (MinCovDet):n\\", robust_cov_matrix) # 5. Comparison on Mahalanobis distances def mahalanobis_distance(cov_matrix, points): inv_cov_matrix = np.linalg.inv(cov_matrix) mean = np.mean(X, axis=0) distances = [] for point in points: diff = point - mean dist = np.sqrt(diff.T @ inv_cov_matrix @ diff) distances.append(dist) return distances print(\\"Mahalanobis distances for test points (Empirical):\\", mahalanobis_distance(emp_cov, test_points)) print(\\"Mahalanobis distances for test points (Shrunk manual):\\", mahalanobis_distance(shrunk_cov_matrix, test_points)) print(\\"Mahalanobis distances for test points (Ledoit-Wolf):\\", mahalanobis_distance(lw_cov, test_points)) print(\\"Mahalanobis distances for test points (Sparse):\\", mahalanobis_distance(sparse_cov_matrix, test_points)) print(\\"Mahalanobis distances for test points (Robust):\\", mahalanobis_distance(robust_cov_matrix, test_points))"},{"question":"Advanced ZIP File Operations # Objective The objective of this question is to evaluate your understanding of the `zipfile` module in Python and your ability to utilize its functionalities to perform various operations on ZIP files. # Problem Statement You are tasked with creating a utility class `AdvancedZipManager` that provides a comprehensive set of methods to work with ZIP files. This class should be able to: 1. Create a new ZIP file and add files to it. 2. List the contents of a given ZIP file. 3. Extract specific files or all files from a ZIP archive. 4. Verify the integrity of a ZIP file. 5. Search for files within the ZIP that match a specific pattern. 6. Handle exceptions and provide meaningful error messages. # Requirements 1. **Class Name**: `AdvancedZipManager` 2. **Methods**: - `create_zip(zip_filename: str, files: list, compression_method=zipfile.ZIP_STORED) -> None` - `list_contents(zip_filename: str) -> list` - `extract_files(zip_filename: str, extract_to: str, members: list = None) -> None` - `verify_zip(zip_filename: str) -> bool` - `search_files(zip_filename: str, pattern: str) -> list` # Method Descriptions 1. **create_zip** - Parameters: - `zip_filename` (str): The name of the ZIP file to be created. - `files` (list): A list of file paths to be added to the ZIP file. - `compression_method` (`zipfile` constant, optional): The compression method to use. Default is `zipfile.ZIP_STORED`. - Functionality: Creates a ZIP file with the given name and adds the specified files using the provided compression method. 2. **list_contents** - Parameters: - `zip_filename` (str): The name of the ZIP file to list the contents of. - Returns: - `list`: A list of file names contained in the ZIP file. 3. **extract_files** - Parameters: - `zip_filename` (str): The name of the ZIP file to extract files from. - `extract_to` (str): The directory path where files should be extracted. - `members` (list, optional): A subset of files to be extracted. If None, all files are extracted. - Functionality: Extracts specified members (or all files) from the ZIP file to the given directory. 4. **verify_zip** - Parameters: - `zip_filename` (str): The name of the ZIP file to verify. - Returns: - `bool`: Returns True if the ZIP file is valid, otherwise False. - Functionality: Reads all files in the archive and checks their integrity (CRC checks). 5. **search_files** - Parameters: - `zip_filename` (str): The name of the ZIP file to search within. - `pattern` (str): The pattern to search for among the file names. - Returns: - `list`: A list of file names that match the given pattern. # Constraints - Ensure that you handle exceptions gracefully and provide meaningful error messages. - The module should be operable on large ZIP files (greater than 4 GiB). - Provide input validation where applicable. # Performance - The methods should be optimized for performance, especially when dealing with large ZIP files. # Example Usage ```python # Example usage of AdvancedZipManager # Create a new ZIP file files_to_zip = [\'file1.txt\', \'file2.txt\', \'file3.png\'] AdvancedZipManager.create_zip(\'archive.zip\', files_to_zip, zipfile.ZIP_DEFLATED) # List contents of the ZIP file print(AdvancedZipManager.list_contents(\'archive.zip\')) # Extract all files from the ZIP file AdvancedZipManager.extract_files(\'archive.zip\', \'extracted_files/\') # Verify the integrity of the ZIP file is_valid = AdvancedZipManager.verify_zip(\'archive.zip\') print(f\'Is ZIP file valid? {is_valid}\') # Search for files with a specific pattern in the ZIP file matching_files = AdvancedZipManager.search_files(\'archive.zip\', \'*.txt\') print(f\'Matching files: {matching_files}\') ``` Implement the `AdvancedZipManager` class with the specified methods and ensure it meets the requirements. Write code with good readability and comment where necessary.","solution":"import zipfile import fnmatch class AdvancedZipManager: @staticmethod def create_zip(zip_filename: str, files: list, compression_method=zipfile.ZIP_STORED) -> None: try: with zipfile.ZipFile(zip_filename, \'w\', compression=compression_method) as zipf: for file in files: zipf.write(file, arcname=file.split(\'/\')[-1]) # Ensure the file is saved without path except Exception as e: raise Exception(f\\"Failed to create ZIP file: {str(e)}\\") @staticmethod def list_contents(zip_filename: str) -> list: try: with zipfile.ZipFile(zip_filename, \'r\') as zipf: return zipf.namelist() except Exception as e: raise Exception(f\\"Failed to list contents: {str(e)}\\") @staticmethod def extract_files(zip_filename: str, extract_to: str, members: list = None) -> None: try: with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extractall(path=extract_to, members=members) except Exception as e: raise Exception(f\\"Failed to extract files: {str(e)}\\") @staticmethod def verify_zip(zip_filename: str) -> bool: try: with zipfile.ZipFile(zip_filename, \'r\') as zipf: bad_file = zipf.testzip() return bad_file is None except Exception as e: raise Exception(f\\"Failed to verify ZIP file: {str(e)}\\") @staticmethod def search_files(zip_filename: str, pattern: str) -> list: try: with zipfile.ZipFile(zip_filename, \'r\') as zipf: return [name for name in zipf.namelist() if fnmatch.fnmatch(name, pattern)] except Exception as e: raise Exception(f\\"Failed to search files: {str(e)}\\")"},{"question":"**Question: URL Data Fetcher and Processor** You are required to implement a Python function that will fetch data from a given URL and process it according to specified rules. The function should make use of various components from the `urllib` module to ensure robust handling of different scenarios such as redirections, cookies management, and error handling. # Function Signature ```python def fetch_and_process_url(url: str) -> str: Fetches data from the given URL and processes it according to specified rules. Args: - url (str): The URL from which data needs to be fetched. Returns: - str: Processed data from the URL. Raises: - ValueError: If any issue occurs while fetching or processing the URL. ``` # Input - `url` (str): A string representing the URL to fetch data from. # Output - Returns a string after processing the fetched data. # Constraints - The function must handle HTTP and HTTPS URLs. - If a redirection occurs, the function must follow up to 3 redirects. - The function must handle cookies set by the server. - The function must appropriately handle HTTP errors and raise a `ValueError` in these cases with an informative error message. - The fetched data should be transformed to uppercase before returning it. # Example Here is how the function can be used: ```python url = \\"https://example.com/page\\" data = fetch_and_process_url(url) print(data) ``` # Implementation Notes - Use `urllib.request` to handle URL opening. - Implement `HTTPErrorProcessor` to handle HTTP errors. - Use `HTTPCookieProcessor` to handle cookies. - Manage redirections within the function. - After successfully fetching data, convert the entire content to uppercase using the `str.upper()` method. **Hint**: You may need to use `urllib.request.build_opener` and `urllib.request.install_opener` to customize how URLs are opened and data is handled.","solution":"import urllib.request import urllib.error import http.cookiejar def fetch_and_process_url(url: str) -> str: Fetches data from the given URL and processes it according to specified rules. Args: - url (str): The URL from which data needs to be fetched. Returns: - str: Processed data from the URL. Raises: - ValueError: If any issue occurs while fetching or processing the URL. cj = http.cookiejar.CookieJar() opener = urllib.request.build_opener( urllib.request.HTTPCookieProcessor(cj), urllib.request.HTTPRedirectHandler(), urllib.request.HTTPErrorProcessor() ) urllib.request.install_opener(opener) response = None try: response = urllib.request.urlopen(url) data = response.read().decode(\'utf-8\') return data.upper() except urllib.error.HTTPError as e: raise ValueError(f\\"HTTP Error: {e.code} {e.reason}\\") from e except urllib.error.URLError as e: raise ValueError(f\\"URL Error: {e.reason}\\") from e finally: if response: response.close()"},{"question":"**Question: Exploring Titanic Dataset with Seaborn** The Titanic dataset contains information about the passengers on the Titanic and whether or not they survived. You are tasked with generating a series of visualizations to explore this dataset\'s features, focusing on survival rates across different dimensions. # Objectives 1. **Load and Explore the Dataset:** - Load the Titanic dataset from seaborn\'s built-in datasets using `sns.load_dataset(\'titanic\')`. - Display the first few rows of the dataset to understand the structure and content. 2. **Create a FacetGrid:** - Initialize a `FacetGrid` with the Titanic dataset, setting `col` to `\'class\'` and `row` to `\'sex\'`. - Use a scatterplot to show the relationship between `\'age\'` and `\'fare\'`. Color the points by the `\'survived\'` status. 3. **Customize the Plot:** - Add a legend to the plot to indicate the meaning of the colors used for `\'survived\'` status. - Set the axis labels: \\"Age (years)\\" for x-axis and \\"Fare ()\\" for y-axis. - Set titles for each subplot indicating the passenger class and sex. 4. **Add Reference Lines:** - Add a horizontal reference line representing the median fare value across all passengers. - Ensure the reference line is the same across all facets. 5. **Annotate Each Facet:** - Create a custom function to annotate each subplot with the number of passengers (`N`) in that facet. - Use this function to add the annotation at the top-right corner of each subplot. 6. **Adjust Subplot Parameters:** - Adjust the height and aspect ratio of the plots to make them more readable. - Save the final plot as \\"titanic_facetgrid.png\\". # Expected Input and Output - **Input:** Code implementation with comments explaining the steps. - **Output:** A saved image file \\"titanic_facetgrid.png\\" showing the facet grid with scatter plots, customization, reference lines, and annotations. # Constraints - Use only seaborn and matplotlib libraries for visualization. - Ensure the plots are well-labeled and the grid layout is logical and clear. # Performance Requirements - The code should efficiently load the dataset and render the plots, handling reasonable dataset sizes without performance issues. ```python # Example structure (actual implementation is required) import seaborn as sns import matplotlib.pyplot as plt # 1. Load and explore the dataset titanic = sns.load_dataset(\'titanic\') print(titanic.head()) # 2. Create a FacetGrid g = sns.FacetGrid(titanic, col=\'class\', row=\'sex\') # 3. Map a scatter plot g.map_dataframe(sns.scatterplot, x=\'age\', y=\'fare\', hue=\'survived\') g.add_legend() # 4. Add reference lines g.refline(y=titanic[\'fare\'].median()) # 5. Annotate each facet def annotate(data, **kws): n = len(data) ax = plt.gca() ax.text(.05, .95, f\'N = {n}\', transform=ax.transAxes, ha=\'left\', va=\'top\', fontsize=12) g.map_dataframe(annotate) # 6. Customize axis labels, titles, and subplot parameters g.set_axis_labels(\\"Age (years)\\", \\"Fare ()\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") g.fig.tight_layout() # Save the plot g.savefig(\\"titanic_facetgrid.png\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_facet_grid(): # 1. Load and explore the dataset titanic = sns.load_dataset(\'titanic\') print(titanic.head()) # 2. Create a FacetGrid g = sns.FacetGrid(titanic, col=\'class\', row=\'sex\', height=3, aspect=1.2) # 3. Map a scatter plot g.map_dataframe(sns.scatterplot, x=\'age\', y=\'fare\', hue=\'survived\') g.add_legend() # 4. Add reference lines median_fare = titanic[\'fare\'].median() for ax in g.axes.flat: ax.axhline(median_fare, ls=\'--\', color=\'red\') # 5. Annotate each facet def annotate(data, color, **kws): n = len(data) ax = plt.gca() ax.text(.95, .95, f\'N = {n}\', transform=ax.transAxes, ha=\'right\', va=\'top\', fontsize=12, color=\'black\') g.map_dataframe(annotate) # 6. Customize axis labels, titles, and subplot parameters g.set_axis_labels(\\"Age (years)\\", \\"Fare ()\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") g.fig.tight_layout() # Save the plot g.savefig(\\"titanic_facetgrid.png\\") return \\"titanic_facetgrid.png\\""},{"question":"# Question: Implement Multiclass, Multilabel, and Multioutput Classification In this task, you will demonstrate your understanding of multi-class, multi-label, and multioutput classification issues using scikit-learn. You are required to implement a solution that does the following: 1. **Multiclass Classification**: - Train a dataset using `OneVsRestClassifier` with `LinearSVC` as the base classifier. - Evaluate the model using accuracy. 2. **Multilabel Classification**: - Create a multilabel classification problem using the scikit-learn `make_multilabel_classification` function. - Train the dataset using `OneVsRestClassifier` with `LinearSVC` as the base classifier. - Evaluate the model using `hamming_loss`. 3. **Multioutput Classification**: - Train a dataset using `MultiOutputClassifier` with `RandomForestClassifier` as the base classifier. - Evaluate the model using accuracy for each output. # Requirements: - You are required to use sklearn\'s multi-estimators such as `OneVsRestClassifier` and `MultiOutputClassifier`. - The datasets should be generated using appropriate sklearn functions. - You should provide clear documentation and adherence to performance metrics mentioned. # Input: None. All datasets need to be generated within the script using sklearn dataset generation utilities. # Output: The outputs include training logs and performance scores: - Accuracy for multiclass classification. - Hamming loss for multilabel classification. - Accuracy for each output label for multioutput classification. # Constraints: - Use `LinearSVC` from `svm` module as the estimator in `OneVsRestClassifier`. - Use `RandomForestClassifier` from `ensemble` module as the estimator in `MultiOutputClassifier`. - Set `random_state=0` wherever applicable for reproducibility. # Example Solution: ```python import numpy as np from sklearn.datasets import load_iris, make_classification, make_multilabel_classification from sklearn.svm import LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.multioutput import MultiOutputClassifier from sklearn.metrics import accuracy_score, hamming_loss from sklearn.model_selection import train_test_split # 1. Multiclass Classification X, y = load_iris(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier.fit(X_train, y_train) multiclass_acc = accuracy_score(y_test, ovr_classifier.predict(X_test)) print(\'Multiclass Classification Accuracy:\', multiclass_acc) # 2. Multilabel Classification X, y = make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) ovr_classifier_ml = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier_ml.fit(X_train, y_train) multilabel_hl = hamming_loss(y_test, ovr_classifier_ml.predict(X_test)) print(\'Multilabel Classification Hamming Loss:\', multilabel_hl) # 3. Multioutput Classification X, y1 = make_classification(n_samples=100, n_classes=3, n_informative=5, random_state=0) y2 = np.random.randint(0, 3, size=100) Y = np.vstack((y1, y2)).T X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0) multi_output_classifier = MultiOutputClassifier(RandomForestClassifier(random_state=0), n_jobs=2) multi_output_classifier.fit(X_train, Y_train) Y_pred = multi_output_classifier.predict(X_test) multioutput_acc = [accuracy_score(Y_test[:, i], Y_pred[:, i]) for i in range(Y_test.shape[1])] print(\'Multioutput Classification Accuracies:\', multioutput_acc) ``` The above implementation is a guideline. Ensure you follow best practices in coding, including necessary import statements and proper handling of randomness for reproducibility.","solution":"import numpy as np from sklearn.datasets import load_iris, make_classification, make_multilabel_classification from sklearn.svm import LinearSVC from sklearn.ensemble import RandomForestClassifier from sklearn.multiclass import OneVsRestClassifier from sklearn.multioutput import MultiOutputClassifier from sklearn.metrics import accuracy_score, hamming_loss from sklearn.model_selection import train_test_split def multiclass_classification(): # Load the Iris dataset X, y = load_iris(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # Create and train the OneVsRestClassifier with LinearSVC ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier.fit(X_train, y_train) # Predict and evaluate accuracy y_pred = ovr_classifier.predict(X_test) multiclass_acc = accuracy_score(y_test, y_pred) return multiclass_acc def multilabel_classification(): # Create a multilabel classification dataset X, y = make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # Create and train the OneVsRestClassifier with LinearSVC ovr_classifier_ml = OneVsRestClassifier(LinearSVC(random_state=0)) ovr_classifier_ml.fit(X_train, y_train) # Predict and evaluate Hamming loss y_pred_ml = ovr_classifier_ml.predict(X_test) multilabel_hl = hamming_loss(y_test, y_pred_ml) return multilabel_hl def multioutput_classification(): # Create a multioutput classification dataset X, y1 = make_classification(n_samples=100, n_classes=3, n_informative=5, random_state=0) y2 = np.random.randint(0, 3, size=100) Y = np.vstack((y1, y2)).T X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0) # Create and train the MultiOutputClassifier with RandomForestClassifier multi_output_classifier = MultiOutputClassifier(RandomForestClassifier(random_state=0), n_jobs=2) multi_output_classifier.fit(X_train, Y_train) # Predict and evaluate accuracy for each output label Y_pred = multi_output_classifier.predict(X_test) multioutput_acc = [accuracy_score(Y_test[:, i], Y_pred[:, i]) for i in range(Y_test.shape[1])] return multioutput_acc"},{"question":"You have been provided with a Python application, and you suspect it has a memory leak that is negatively affecting its performance. Using the `tracemalloc` package, your task is to: 1. Start tracing memory allocations. 2. Take an initial snapshot of the memory allocations. 3. Run the function `function_with_memory_leak()` provided to you. 4. Take a final snapshot of the memory allocations. 5. Compare the two snapshots to identify the difference in memory allocations. 6. Isolate the top 5 differences in memory allocations, and provide detailed traceback information for these differences. You must implement a function `analyze_memory_leak(function_with_memory_leak)` that performs the above tasks. The function should return a list of dictionaries, each containing information about the top 5 differences, including filename, line number, memory size difference, and the detailed traceback. # Function Signature ```python def analyze_memory_leak(function_with_memory_leak) -> list: pass ``` # Expected Output A list of dictionaries with the following structure: ```python [ { \\"filename\\": \\"example.py\\", \\"lineno\\": 12, \\"size_diff\\": 1024, \\"traceback\\": [ \\"File \'example.py\', line 10\\", \\"File \'example.py\', line 11\\", \\"File \'example.py\', line 12\\" ] }, ... ] ``` # Constraints - The function `function_with_memory_leak` will be passed into your `analyze_memory_leak` function and it will execute within this function. - The captured traceback should include up to 10 frames. - Only include the top 5 differences in your output. You can assume the `function_with_memory_leak` and any other necessary code will be provided and executed in the same runtime environment as your function. # Example Here is an example of how the function might be used: ```python def sample_function_with_memory_leak(): leak_list = [] for _ in range(100000): leak_list.append(str(_)) print(analyze_memory_leak(sample_function_with_memory_leak)) ``` Output: ```python [ { \\"filename\\": \\"/usr/lib/python3.4/tracemalloc.py\\", \\"lineno\\": 516, \\"size_diff\\": 419840, \\"traceback\\": [ \\"File \'/usr/lib/python3.4/tracemalloc.py\', line 509\\", \\"File \'/usr/lib/python3.4/tracemalloc.py\', line 512\\", \\"File \'/usr/lib/python3.4/tracemalloc.py\', line 516\\" ] }, ... ] ``` Make sure to thoroughly test your function as different `function_with_memory_leak` implementations might be provided.","solution":"import tracemalloc def analyze_memory_leak(function_with_memory_leak): Analyze memory allocations to identify potential memory leaks. Params: function_with_memory_leak (function): The function suspected of having a memory leak. Returns: List[Dict]: A list containing information about the top 5 memory allocation differences. # Start tracing memory allocations tracemalloc.start() # Take an initial snapshot snapshot_before = tracemalloc.take_snapshot() # Run the function suspected of having a memory leak function_with_memory_leak() # Take a final snapshot snapshot_after = tracemalloc.take_snapshot() # Compare the two snapshots to identify differences stats = snapshot_after.compare_to(snapshot_before, \'lineno\') # Get top 5 differences top_stats = stats[:5] # Prepare the output result = [] for stat in top_stats: traceback = stat.traceback.format(limit=10) result.append({ \\"filename\\": stat.traceback[-1].filename, \\"lineno\\": stat.traceback[-1].lineno, \\"size_diff\\": stat.size_diff, \\"traceback\\": traceback }) # Stop tracing memory allocations tracemalloc.stop() return result"},{"question":"# Memory Management in Python **Objective:** Implement and manage custom memory allocation methods in Python using both the provided API and creating custom allocators as per the given specifications and use debugging hooks to identify memory issues. **Task:** 1. **Memory Allocation Functions:** - Implement a function `allocate_and_free_memory` that: - Allocates memory using `PyMem_Malloc` for an array of integers of size `n`. - Initializes the array with values from `0` to `n-1`. - Resizes the array to 2*n using `PyMem_Realloc` and initializes the new elements with values from `n` to `2*n-1`. - Frees the memory using `PyMem_Free`. 2. **Custom Memory Allocator:** - Create a custom memory allocator `MyAllocator` for the `Mem` domain that: - Allocates memory blocks with the additional behavior of logging the operations (allocations and deallocations) to a file. - The allocator should track the number of allocations and deallocations, ensuring they match, and print a message to the log file if any mismatch is detected. - Set this custom allocator for the `Mem` domain and revert to the default allocator once operations are completed. 3. **Debugging with Hooks:** - Enable memory debugging hooks to detect memory issues. - Write code that deliberately causes an over-read and under-read scenario using allocated memory and triggers debugging messages. **Requirements:** - **Input:** - For `allocate_and_free_memory`, an integer `n` representing the size of the integer array. - **Output:** - Function `allocate_and_free_memory` should return the final resized array. - The custom allocator should create a log file `memory_log.txt` containing logs of allocations and deallocations. - The code with memory debugging should print debugging messages to the console. **Constraints and Limitations:** - Ensure the `GIL` is held when using `PyMem_Alloc`, `PyMem_Realloc`, and `PyMem_Free`. - For custom allocator implementation, mimic the allocator functions closely as specified in the documentation. - Output debugging messages must be clear and indicate the type of memory issue detected. **Hint:** - Use safe pointer functions and ensure logging is thread-safe. ```python def allocate_and_free_memory(n: int): Allocate, initialize, resize, and free memory using PyMem functions. Parameters: n: int - Size of the initial integer array. Returns: list - Final resized array with updated values. import ctypes # Your code here class MyAllocator: Custom memory allocator for the Mem domain with logging. def __init__(self, log_file: str): self.log_file = log_file # Initialization code here # Define allocate, free, realloc, and other methods def malloc(self, ctx, size): # Your code here def free(self, ctx, ptr): # Your code here # Other allocator methods def enable_debugging_hooks(): Enable memory debugging hooks and cause memory issues to detect them. # Your code here # Example usage: # allocate_and_free_memory(10) # my_allocator = MyAllocator(\\"memory_log.txt\\") # enable_debugging_hooks() ``` **Performance Requirements:** - The function should handle at least 10^6 allocations efficiently. - Custom allocator operations should have a minimal performance overhead. **Assessment Criteria:** - Correct implementation of memory allocation, resizing, and freeing. - Proper setup and usage of the custom memory allocator with logging. - Efficient and accurate detection and logging of memory issues using debugging hooks. - Code readability and adherence to best practices.","solution":"def allocate_and_free_memory(n: int): import ctypes # Allocate memory for n integers array_type = ctypes.c_int * n array = array_type() # Initialize array with values from 0 to n-1 for i in range(n): array[i] = i # Resize the array to 2*n new_array_type = ctypes.c_int * (2 * n) new_array = new_array_type() for i in range(n): new_array[i] = array[i] for i in range(n, 2 * n): new_array[i] = i # Keeping returned object in a Python list for clarity resized_list = [new_array[i] for i in range(2 * n)] # Freeing the memory allocated is handled by Python\'s garbage collector return resized_list class MyAllocator: def __init__(self, log_file: str): self.log_file = log_file self.allocation_count = 0 self.deallocation_count = 0 def malloc(self, size): import ctypes # Log the allocation with open(self.log_file, \'a\') as f: f.write(f\\"Allocated {size} bytesn\\") self.allocation_count += 1 return ctypes.create_string_buffer(size) def free(self, ptr): if ptr: # Log the deallocation with open(self.log_file, \'a\') as f: f.write(\\"Freed memoryn\\") self.deallocation_count += 1 def realloc(self, ptr, size): import ctypes new_ptr = ctypes.create_string_buffer(size) if ptr: ctypes.memmove(new_ptr, ptr, min(len(ptr), size)) # Log the reallocation with open(self.log_file, \'a\') as f: f.write(f\\"Reallocated to {size} bytesn\\") return new_ptr def check_balance(self): with open(self.log_file, \'a\') as f: if self.allocation_count != self.deallocation_count: f.write(f\\"Allocation/Deallocation count mismatch: allocs={self.allocation_count}, frees={self.deallocation_count}n\\") else: f.write(\\"All allocations and deallocations match.n\\") def enable_debugging_hooks(): # This function should ideally enable memory debugging hooks # Since we don\'t have low-level hooks, here is a simple debug simulation # Purposefully causing over-read and under-read issues import ctypes # Allocate some memory ptr = ctypes.create_string_buffer(10) try: for i in range(11): print(ptr[i]) # This will cause an over-read except IndexError: print(\\"Caught over-read error\\") try: print(ptr[-1]) # This once is okay because Python allows negative index print(ptr[-11]) # This will cause an under-read except IndexError: print(\\"Caught under-read error\\") # Example usage if __name__ == \\"__main__\\": print(allocate_and_free_memory(10)) allocator = MyAllocator(\\"memory_log.txt\\") allocator.check_balance() enable_debugging_hooks()"},{"question":"**Problem Statement** You are required to implement a custom iterator in Python along with utilizing generator expressions and built-in functions from the `itertools` module to perform specific operations on data streams. The goal is to demonstrate a deep understanding of iterators, generators, and their related functional programming techniques as described in the provided documentation. **Task Breakdown** 1. **Custom Iterator**: Implement a custom iterator `RangeSquares` that mimics the behavior of Python’s `range` function but returns squares of the numbers within the specified range. - **Initialization**: Accept `start`, `stop`, and `step` parameters. - **Iterator Protocol**: Implement the `__iter__()` and `__next__()` methods to adhere to the iterator protocol in Python. **Signature**: ```python class RangeSquares: def __init__(self, start: int, stop: int, step: int = 1): # initialize with start, stop, and step def __iter__(self): # return the iterator instance itself def __next__(self): # return the next squared value in the range, stop iteration when exceeded ``` 2. **Generators and `itertools`**: Using the custom iterator `RangeSquares`, create a generator expression and utilize itertools\' functions to perform the following: - Collect the squares within the range using a generator expression. - Filter out squares that are not multiples of 3 using `itertools.filterfalse()`. - Chain the filtered results with another range of squared integers using `itertools.chain()`. **Signature**: ```python def process_squares(start: int, stop: int, step: int, chain_start: int, chain_stop: int, chain_step: int): # use RangeSquares and related itertools functions to achieve the task # return the final list of processed squares ``` **Input Format** - `start` (int): The starting value for the range in the `RangeSquares` iterator. - `stop` (int): The stopping value for the range in the `RangeSquares` iterator. - `step` (int): The step value for the range in the `RangeSquares` iterator (default: 1). - `chain_start` (int): The starting value for the additional range to chain with the filtered results. - `chain_stop` (int): The stopping value for the additional range to chain with the filtered results. - `chain_step` (int): The step value for the additional range to chain with the filtered results (default: 1). **Output Format** - A list of integers representing the processed squared numbers after filtering and chaining operations. **Example** ```python # Example Usage rs = RangeSquares(1, 10) result = process_squares(1, 10, 1, 10, 20, 2) print(result) ``` **Expected Output** ```python [1, 4, 16, 25, 49, 64, 100, 121, 169, 196, 324] ``` **Constraints** - The input parameters will be valid integers such that `start <= stop` and `chain_start <= chain_stop`. - The step values will be positive integers. **Performance Requirements** - The iterator should efficiently handle large ranges without unnecessary memory consumption. - Utilize lazy evaluation where possible to optimize performance.","solution":"from itertools import filterfalse, chain class RangeSquares: def __init__(self, start: int, stop: int, step: int = 1): self.start = start self.stop = stop self.step = step self.current = start def __iter__(self): return self def __next__(self): if self.current >= self.stop: raise StopIteration result = self.current ** 2 self.current += self.step return result def process_squares(start: int, stop: int, step: int, chain_start: int, chain_stop: int, chain_step: int): # Create RangeSquares iterator range_squares = RangeSquares(start, stop, step) # Filter squares that are not multiples of 3 filtered_squares = filterfalse(lambda x: x % 3 == 0, (x for x in range_squares)) # Create additional range of squared values additional_range_squares = (x ** 2 for x in range(chain_start, chain_stop, chain_step)) # Chain the filtered squares with the additional range chained_squares = chain(filtered_squares, additional_range_squares) # Return the final list of processed squares return list(chained_squares)"},{"question":"# Audio Processing Challenge with `audioop` Module Objective Your task is to implement a function called `process_audio` that performs a series of audio manipulations on a given audio fragment. The function will: 1. Decode a u-LAW encoded audio fragment to linear encoding. 2. Convert the linear-encoded audio fragment to 16-bit samples. 3. Apply a bias to each sample in the audio fragment. 4. Find the sample with the maximum amplitude in the resulting fragment. 5. Output the modified audio fragment and the maximum amplitude sample. Function Signature ```python def process_audio(ulaw_fragment: bytes, bias: int) -> (bytes, int): Process a u-LAW encoded audio fragment by decoding, biasing, and finding the maximum sample. Parameters: ulaw_fragment (bytes): The input audio fragment in u-LAW encoding. bias (int): Bias to be added to each sample. Returns: Tuple[bytes, int]: A tuple containing the modified audio fragment and the maximum sample value. pass ``` Detailed Steps 1. **Decode u-LAW Encoding**: Use the `audioop.ulaw2lin` function to decode the `ulaw_fragment` to linear encoding with 16-bit samples. ```python lin_fragment = audioop.ulaw2lin(ulaw_fragment, 2) ``` 2. **Apply Bias**: Utilize the `audioop.bias` function to add the specified bias to each sample in the `lin_fragment`. ```python biased_fragment = audioop.bias(lin_fragment, 2, bias) ``` 3. **Find Maximum Sample**: Use the `audioop.max` function to find the sample with the maximum absolute amplitude in the `biased_fragment`. ```python max_sample = audioop.max(biased_fragment, 2) ``` 4. **Return Result**: The function should return a tuple with the biased fragment and the maximum amplitude sample. Constraints - The `ulaw_fragment` will always be in 8-bit u-LAW encoding format. - The `bias` value will be an integer between -32768 and 32767. - The `max_sample` should be returned as an integer. Example ```python ulaw_fragment = b\'x7f...some bytes...x80\' bias = 100 result_fragment, max_sample = process_audio(ulaw_fragment, bias) print(result_fragment) # The modified audio fragment print(max_sample) # The maximum sample value ``` **Note**: Ensure that your function gracefully handles potential exceptions that might arise during processing.","solution":"import audioop def process_audio(ulaw_fragment: bytes, bias: int) -> (bytes, int): Process a u-LAW encoded audio fragment by decoding, biasing, and finding the maximum sample. Parameters: ulaw_fragment (bytes): The input audio fragment in u-LAW encoding. bias (int): Bias to be added to each sample. Returns: Tuple[bytes, int]: A tuple containing the modified audio fragment and the maximum sample value. # Decode the u-LAW fragment to linear encoding with 16-bit samples lin_fragment = audioop.ulaw2lin(ulaw_fragment, 2) # Apply the bias to each sample in the linear-encoded audio fragment biased_fragment = audioop.bias(lin_fragment, 2, bias) # Find the sample with the maximum amplitude in the biased audio fragment max_sample = audioop.max(biased_fragment, 2) return biased_fragment, max_sample"},{"question":"**Title: Implementing a Timeout for a Long-Running Function with Signal Handling** **Objective:** Implement a function that limits the execution time of another function using the `signal` module. If the execution exceeds the specified time, an exception should be raised. This requires understanding signal handlers, setting alarms, and managing exceptions. **Problem Statement:** You are required to implement a function `run_with_timeout(func, args, timeout)` that executes a callable `func` with the provided `args` and enforces a timeout. If `func` does not complete within the specified `timeout` seconds, a `TimeoutError` exception should be raised. **Function Signature:** ```python def run_with_timeout(func, args, timeout): Executes a function with a given timeout. Parameters: func (callable): The function to execute. args (tuple): Arguments to pass to the function. timeout (int): The maximum execution time in seconds. Returns: Any: The return value of the function if it completes within the timeout. Raises: TimeoutError: If the function execution exceeds the specified timeout. ``` **Input:** - `func`: A callable function to be executed. - `args`: A tuple of arguments to be passed to the `func`. - `timeout`: An integer representing the maximum execution time in seconds. **Output:** - The return value of `func` if it completes within the specified `timeout`. **Constraints:** - The `timeout` should be a positive integer. - The function `func` should be a callable that can be executed with the given `args`. **Example:** ```python import time def long_running_function(x, y): time.sleep(x) return x + y try: result = run_with_timeout(long_running_function, (5, 10), 3) except TimeoutError: print(\\"Function execution exceeded the timeout\\") ``` **Instructions:** 1. Implement the `run_with_timeout` function. 2. Use the `signal` module to set an alarm and handle the `SIGALRM` signal. 3. Raise a `TimeoutError` if the function execution exceeds the specified timeout. 4. Ensure that any previously set alarm is canceled after the function completes. 5. Test your implementation with both functions that complete within the timeout and functions that do not. **Hints:** - Utilize the `signal` module’s `signal()` function to handle the `SIGALRM` signal. - The `time.sleep()` function can be useful to simulate long-running functions. **Performance Requirements:** - The solution should ensure that any active alarms are cleaned up, regardless of whether the function completes successfully or raises an exception.","solution":"import signal class TimeoutError(Exception): pass def _handle_timeout(signum, frame): raise TimeoutError(\\"Function execution exceeded the timeout\\") def run_with_timeout(func, args, timeout): Executes a function with a given timeout. Parameters: func (callable): The function to execute. args (tuple): Arguments to pass to the function. timeout (int): The maximum execution time in seconds. Returns: Any: The return value of the function if it completes within the timeout. Raises: TimeoutError: If the function execution exceeds the specified timeout. # Set the alarm signal handler signal.signal(signal.SIGALRM, _handle_timeout) # Set the alarm signal.alarm(timeout) try: result = func(*args) finally: # Cancel the alarm signal.alarm(0) return result"},{"question":"You are provided with a dataset containing information about different products and their sales across several regions. You are required to perform a series of operations using pandas to analyze this data. # Data The dataset contains the following columns: - `ProductID`: Unique identifier for each product. - `ProductName`: Name of the product. - `Region`: The region where the product is sold. - `Sales`: Number of units sold. # Steps 1. **Load the data**: - Read the dataset from a CSV file into a pandas DataFrame. 2. **Clean the data**: - Ensure there are no duplicate `ProductID` entries. - Handle any missing values in the `Sales` column by replacing them with the mean sales value for the respective region. 3. **Analyze the data**: - Add a new column `TotalSales` representing the total sales for each product across all regions. - Identify and print the top 3 products based on overall sales. 4. **Region-wise analysis**: - Create a summary DataFrame showing the total sales for each region. - Identify and print the region with the highest total sales. # Constraints 1. The input CSV file path should be provided as an argument to your function(s). 2. Ensure optimal performance by minimizing the number of DataFrame operations where possible. # Function Signature ```python import pandas as pd def analyze_sales_data(file_path: str) -> None: # Your code here ``` # Example Execution Assuming the data CSV file is located at `data/products_sales.csv`, running the following should perform the necessary analysis: ```python analyze_sales_data(\'data/products_sales.csv\') ``` # Expected Output 1. Top 3 products by overall sales (ProductID, ProductName, TotalSales). 2. Region-wise total sales summary. 3. Region with the highest total sales. **Note**: Your output should be clear and should not include excessive data dumps. Present the results in a concise and readable manner.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> None: # Load the data from CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Clean the data: Ensure there are no duplicate ProductID entries df = df.drop_duplicates(subset=[\'ProductID\']) # Handle missing values in the Sales column by replacing them with the mean sales value for the respective region df[\'Sales\'] = df[\'Sales\'].fillna(df.groupby(\'Region\')[\'Sales\'].transform(\'mean\')) # Add a new column TotalSales representing the total sales for each product across all regions df[\'TotalSales\'] = df.groupby(\'ProductID\')[\'Sales\'].transform(\'sum\') # Identify and print the top 3 products based on overall sales top_products = df[[\'ProductID\', \'ProductName\', \'TotalSales\']].drop_duplicates().sort_values(by=\'TotalSales\', ascending=False).head(3) print(\\"Top 3 products by overall sales:\\") print(top_products) # Create a summary DataFrame showing the total sales for each region region_summary = df.groupby(\'Region\')[\'Sales\'].sum().reset_index().rename(columns={\'Sales\': \'TotalSales\'}) print(\\"nRegion-wise total sales summary:\\") print(region_summary) # Identify and print the region with the highest total sales top_region = region_summary.sort_values(by=\'TotalSales\', ascending=False).head(1) print(\\"nRegion with the highest total sales:\\") print(top_region)"},{"question":"# Covariance Estimation using Scikit-learn In this assignment, you will analyze a dataset and compare different covariance estimation techniques provided by the `sklearn.covariance` module. You will use Empirical Covariance, Ledoit-Wolf Shrinkage, and Oracle Approximating Shrinkage to estimate the covariance matrices and visualize their differences. Task 1. **Load and preprocess the dataset**: Use any standard dataset that has at least 10 features and 100 samples. Standardize the dataset if necessary. 2. **Empirical Covariance**: - Compute the empirical covariance matrix using the `EmpiricalCovariance` class. - Fit the model to the dataset. 3. **Ledoit-Wolf Shrinkage**: - Compute the covariance matrix using the `LedoitWolf` estimator. - Fit the model to the dataset. 4. **Oracle Approximating Shrinkage (OAS)**: - Compute the covariance matrix using the `OAS` estimator. - Fit the model to the dataset. 5. **Visualization**: - Visualize the covariance matrices obtained using the above methods. You can use heatmaps to visualize the differences. 6. **Comparison**: - Compare the performance of these methods based on certain criteria such as Mean Squared Error or any other relevant metric. Input and Output - **Input**: Load a suitable dataset (such as the Iris dataset from sklearn or any other dataset with appropriate features). - **Output**: - Printed covariance matrices for each method. - Heatmap visualizations for each covariance matrix. - A summary of the differences and performance comparison. Constraints - You must use the `sklearn.covariance` module for all covariance computations. - Document your code clearly and provide explanations for each step. Example ```python import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS from sklearn import datasets from sklearn.preprocessing import StandardScaler # Load dataset data = datasets.load_iris() X = data.data # Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Compute Empirical Covariance emp_cov = EmpiricalCovariance().fit(X_scaled) empirical_cov_matrix = emp_cov.covariance_ # Compute Ledoit-Wolf Shrinkage Covariance lw_cov = LedoitWolf().fit(X_scaled) ledoit_wolf_cov_matrix = lw_cov.covariance_ # Compute Oracle Approximating Shrinkage (OAS) Covariance oas_cov = OAS().fit(X_scaled) oas_cov_matrix = oas_cov.covariance_ # Visualization plt.figure(figsize=(12, 4)) plt.subplot(1, 3, 1) sns.heatmap(empirical_cov_matrix,cmap=\'coolwarm\', annot=True) plt.title(\'Empirical Covariance\') plt.subplot(1, 3, 2) sns.heatmap(ledoit_wolf_cov_matrix, cmap=\'coolwarm\', annot=True) plt.title(\'Ledoit-Wolf Covariance\') plt.subplot(1, 3, 3) sns.heatmap(oas_cov_matrix, cmap=\'coolwarm\', annot=True) plt.title(\'OAS Covariance\') plt.tight_layout() plt.show() # Comparison print(\'MSE of Empirical Covariance:\', np.mean((empirical_cov_matrix - true_cov_matrix)**2)) print(\'MSE of Ledoit-Wolf Covariance:\', np.mean((ledoit_wolf_cov_matrix - true_cov_matrix)**2)) print(\'MSE of OAS Covariance:\', np.mean((oas_cov_matrix - true_cov_matrix)**2)) ``` Note: You might need to compute the `true_cov_matrix` if you are comparing with known values. For this exercise, assume the dataset provides you with a reliable empirical estimate.","solution":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.covariance import EmpiricalCovariance, LedoitWolf, OAS from sklearn import datasets from sklearn.preprocessing import StandardScaler def load_and_preprocess_data(): # Load dataset data = datasets.load_breast_cancer() X = data.data # Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled def compute_covariance_matrices(X): # Compute Empirical Covariance emp_cov = EmpiricalCovariance().fit(X) empirical_cov_matrix = emp_cov.covariance_ # Compute Ledoit-Wolf Shrinkage Covariance lw_cov = LedoitWolf().fit(X) ledoit_wolf_cov_matrix = lw_cov.covariance_ # Compute Oracle Approximating Shrinkage (OAS) Covariance oas_cov = OAS().fit(X) oas_cov_matrix = oas_cov.covariance_ return empirical_cov_matrix, ledoit_wolf_cov_matrix, oas_cov_matrix def visualize_covariance_matrices(empirical_cov_matrix, ledoit_wolf_cov_matrix, oas_cov_matrix): # Visualization plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) sns.heatmap(empirical_cov_matrix, cmap=\'coolwarm\') plt.title(\'Empirical Covariance\') plt.subplot(1, 3, 2) sns.heatmap(ledoit_wolf_cov_matrix, cmap=\'coolwarm\') plt.title(\'Ledoit-Wolf Covariance\') plt.subplot(1, 3, 3) sns.heatmap(oas_cov_matrix, cmap=\'coolwarm\') plt.title(\'OAS Covariance\') plt.tight_layout() plt.show() def compare_covariance_matrices(empirical_cov_matrix, ledoit_wolf_cov_matrix, oas_cov_matrix): print(\'Empirical Covariance Matrix:\') print(empirical_cov_matrix) print(\'nLedoit-Wolf Covariance Matrix:\') print(ledoit_wolf_cov_matrix) print(\'nOAS Covariance Matrix:\') print(oas_cov_matrix) X_scaled = load_and_preprocess_data() empirical_cov_matrix, ledoit_wolf_cov_matrix, oas_cov_matrix = compute_covariance_matrices(X_scaled) visualize_covariance_matrices(empirical_cov_matrix, ledoit_wolf_cov_matrix, oas_cov_matrix) compare_covariance_matrices(empirical_cov_matrix, ledoit_wolf_cov_matrix, oas_cov_matrix)"},{"question":"<|Analysis Begin|> The provided documentation explains the functionality of the `telnetlib` module, which is used to implement a Telnet client in Python. It discusses the `Telnet` class, its initialization, and the methods available for performing various Telnet operations. Key methods include: - `read_until()`: Reads data until a specified byte string is encountered or a timeout occurs. - `read_all()`: Reads all data until EOF as bytes. - `read_some()`, `read_very_eager()`, `read_eager()`, `read_lazy()`, `read_very_lazy()`, `read_sb_data()`: Methods for reading data with different blocking behaviors. - `open()`: Connects to a remote Telnet server. - `write()`: Writes data to the Telnet server. - `interact()`: Provides a basic Telnet client interaction. - `expect()`: Reads until one from a list of regular expressions matches. The documentation also provides an example script demonstrating the typical usage of `telnetlib` to log in to a remote server and execute commands. Given this analysis, a challenging coding assessment question can be created to require students to use multiple methods from the `telnetlib` class to interact with a Telnet server, handle different connection scenarios, and implement a fault-tolerant Telnet client. <|Analysis End|> <|Question Begin|> # Telnet Client Implementation You are required to implement a fault-tolerant Telnet client using the `telnetlib` module to automate the process of connecting to a Telnet server, executing commands, and reading responses. Your implementation should: 1. **Connect to the Telnet Server**: - Provide a function `connect_to_server(host: str, port: int, timeout: int) -> telnetlib.Telnet`. - This function should initialize a `Telnet` object, connect to the specified host and port, and return the connected `Telnet` object. 2. **Login to the Server**: - Provide a function `login(tn: telnetlib.Telnet, username: str, password: str) -> bool`. - This function should handle the login process, including reading and writing the login and password prompts. - It should return `True` if login is successful, else `False`. 3. **Execute Commands**: - Provide a function `execute_commands(tn: telnetlib.Telnet, commands: list[str]) -> str`. - This function should write each command to the server, handle response reading, and return the combined output of all commands as a string. 4. **Fault Tolerance**: - Your Telnet client should be able to handle unexpected disconnections and prompt re-connections automatically. - If a connection is lost, it should attempt to reconnect up to 3 times before failing. 5. **Logging**: - Provide a function `set_debug_level(tn: telnetlib.Telnet, level: int)`. - This function should set the debugging level for the Telnet object to the specified level. # Input - The input consists of host, port, timeout, username, password, list of commands, and debug level. # Output - The output should be the combined string output from executing the commands. # Constraints - Handle network errors and disconnections gracefully. - Ensure all byte strings are correctly encoded/decoded using \'ascii\' encoding. - Use proper exception handling to manage common Telnet errors. # Example ```python import telnetlib def connect_to_server(host: str, port: int, timeout: int) -> telnetlib.Telnet: tn = telnetlib.Telnet() tn.open(host, port, timeout) return tn def login(tn: telnetlib.Telnet, username: str, password: str) -> bool: tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\'n\') if password: tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\'n\') # Implement logic to check if successfully logged in return True def execute_commands(tn: telnetlib.Telnet, commands: list[str]) -> str: cmd_output = [] for command in commands: tn.write(command.encode(\'ascii\') + b\'n\') output = tn.read_until(b\\"# \\").decode(\'ascii\') cmd_output.append(output) return \'\'.join(cmd_output) def set_debug_level(tn: telnetlib.Telnet, level: int): tn.set_debuglevel(level) # Example usage: host = \\"localhost\\" port = 23 timeout = 10 username = \\"user\\" password = \\"pass\\" commands = [\\"ls\\", \\"pwd\\"] debug_level = 1 tn = connect_to_server(host, port, timeout) set_debug_level(tn, debug_level) if login(tn, username, password): output = execute_commands(tn, commands) print(output) tn.close() ``` In your implementation, ensure all methods work seamlessly to provide a robust Telnet client capable of performing the required tasks effectively.","solution":"import telnetlib import time def connect_to_server(host: str, port: int, timeout: int) -> telnetlib.Telnet: tn = telnetlib.Telnet() tn.open(host, port, timeout) return tn def login(tn: telnetlib.Telnet, username: str, password: str) -> bool: try: tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\'n\') if password: tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\'n\') # Assume a successful login by checking a prompt or welcome message index, match, text = tn.expect([b\\"# \\", b\\"Login incorrect\\"], timeout=10) return b\\"# \\" in text except Exception as e: print(f\\"Login failed: {e}\\") return False def execute_commands(tn: telnetlib.Telnet, commands: list[str]) -> str: cmd_output = [] for command in commands: tn.write(command.encode(\'ascii\') + b\'n\') output = tn.read_until(b\\"# \\").decode(\'ascii\') cmd_output.append(output) return \'\'.join(cmd_output) def set_debug_level(tn: telnetlib.Telnet, level: int): tn.set_debuglevel(level) def fault_tolerant_connection(): max_retries = 3 attempts = 0 while attempts < max_retries: try: tn = connect_to_server(host, port, timeout) set_debug_level(tn, debug_level) if login(tn, username, password): output = execute_commands(tn, commands) print(output) tn.close() return else: print(\\"Login failed.\\") tn.close() return except Exception as e: print(f\\"Connection failed ({attempts + 1}/{max_retries}): {e}\\") attempts += 1 time.sleep(2) # wait before retrying print(\\"Failed to connect after several retries.\\") # Example usage: host = \\"your_telnet_host\\" port = 23 timeout = 10 username = \\"your_username\\" password = \\"your_password\\" commands = [\\"ls\\", \\"pwd\\"] debug_level = 1 fault_tolerant_connection()"},{"question":"Coding Assessment Question # Objective Your task is to write a Python program that performs precise financial calculations using the `decimal` module, and ensures that these calculations are done in a thread-safe manner using the `threading` module. # Problem Statement You are developing a system to handle transactions for a digital wallet. Each transaction involves adding or subtracting a certain amount of money from the user\'s account balance. Multiple transactions can be processed at the same time, so your implementation needs to ensure thread safety while maintaining precise decimal arithmetic. # Specifications 1. **Transaction Class**: - Implement a `Transaction` class with: - `amount`: a `decimal.Decimal` object representing the amount of money to add or subtract. - `operation`: a string, `\\"add\\"` or `\\"subtract\\"`, indicating the type of transaction. 2. **Wallet Class**: - Implement a `Wallet` class with: - `balance`: a `decimal.Decimal` object initialized to `Decimal(\'0.00\')`. - `lock`: a `threading.Lock` object to ensure thread safety. - Methods: - `perform_transaction(transaction)`: This method takes a `Transaction` object and modifies the balance accordingly. It should use the lock to ensure thread safety. - `get_balance()`: This method returns the current balance as a `decimal.Decimal`. 3. **Multi-threaded Transaction Processing**: - Write a function `process_transactions(wallet, transactions)` that takes a `Wallet` object and a list of `Transaction` objects. It should create separate threads to perform each transaction on the wallet. # Input/Output - **Input**: A list of transactions, where each transaction is represented as a dictionary with keys `\\"amount\\"` and `\\"operation\\"`. - **Output**: The final balance in the wallet after all transactions have been processed. # Constraints - Assume that each transaction will have an amount with up to 2 decimal places. - The number of transactions will not exceed 1000. - All operations should be thread-safe. # Example ```python from decimal import Decimal import threading class Transaction: def __init__(self, amount, operation): self.amount = Decimal(amount) self.operation = operation class Wallet: def __init__(self): self.balance = Decimal(\'0.00\') self.lock = threading.Lock() def perform_transaction(self, transaction): with self.lock: if transaction.operation == \'add\': self.balance += transaction.amount elif transaction.operation == \'subtract\': self.balance -= transaction.amount def get_balance(self): with self.lock: return self.balance def process_transactions(wallet, transactions): threads = [] for trx in transactions: t = Transaction(trx[\'amount\'], trx[\'operation\']) thread = threading.Thread(target=wallet.perform_transaction, args=(t,)) threads.append(thread) thread.start() for thread in threads: thread.join() # Example usage transactions = [ {\\"amount\\": \\"10.00\\", \\"operation\\": \\"add\\"}, {\\"amount\\": \\"5.00\\", \\"operation\\": \\"subtract\\"}, {\\"amount\\": \\"20.00\\", \\"operation\\": \\"add\\"}, {\\"amount\\": \\"3.50\\", \\"operation\\": \\"subtract\\"} ] wallet = Wallet() process_transactions(wallet, transactions) print(wallet.get_balance()) # Expected output: Decimal(\'21.50\') ``` Ensure that your implementation handles precise decimal arithmetic and is thread-safe.","solution":"from decimal import Decimal import threading class Transaction: def __init__(self, amount, operation): self.amount = Decimal(amount) self.operation = operation class Wallet: def __init__(self): self.balance = Decimal(\'0.00\') self.lock = threading.Lock() def perform_transaction(self, transaction): with self.lock: if transaction.operation == \'add\': self.balance += transaction.amount elif transaction.operation == \'subtract\': self.balance -= transaction.amount def get_balance(self): with self.lock: return self.balance def process_transactions(wallet, transactions): threads = [] for trx in transactions: t = Transaction(trx[\'amount\'], trx[\'operation\']) thread = threading.Thread(target=wallet.perform_transaction, args=(t,)) threads.append(thread) thread.start() for thread in threads: thread.join()"},{"question":"Objective: Demonstrate your understanding of advanced indexing features in pandas, particularly focusing on the `MultiIndex` functionality. Problem Description: You are provided with a dataset representing sales data for a chain of stores over multiple years. The dataset contains the following columns: - `Year` (int): The year of the sales data. - `Quarter` (str): The quarter in which the sales occurred (Q1, Q2, Q3, Q4). - `Region` (str): The sales region (East, West, North, South). - `Store` (str): The name of the store. - `Sales` (float): The sales figures for the corresponding quarter. Your task is to implement the following: 1. **Create a MultiIndex DataFrame**: The hierarchical index should be composed of `Year`, `Quarter`, `Region`, and `Store`. The DataFrame should have `Sales` as its only column. 2. **Perform Partial Selection and Slicing**: Write a function to perform the following tasks: - Select the sales figures for a specific year and quarter across all regions and stores. - Select the sales data for a given region across all years and quarters. - Perform a partial slice to get sales data for a specific range of years and quarters. 3. **Reindex and Align Data**: Write a function to reindex the DataFrame to include new regions (\'North-East\', \'South-West\') and fill missing sales data with 0. Align the original DataFrame with another given DataFrame having the same index structure but with different sales figures. 4. **Advanced Indexing and Aggregation**: Write a function to perform the following tasks: - Compute the total sales for each region for all years and quarters. - Swap the levels of the index so that `Store` comes first and `Year` comes last. - Reorder the levels to place `Quarter` at the first position. Constraints: - The DataFrame will have at least one row for each combination of Year, Quarter, Region, and Store. - You cannot use loops to perform selection, slicing, reindexing, or alignment operations. Input: A pandas DataFrame `df` with the sales data in the specified format. Output: - A DataFrame after applying the MultiIndex. - A dictionary with results for each partial selection and slicing task. - A DataFrame after reindexing and aligning. - A dictionary with results for each advanced indexing and aggregation task. Functions: ```python import pandas as pd def create_multiindex_df(df): Convert the given DataFrame to a MultiIndex DataFrame. Parameters: df (pandas.DataFrame): Original DataFrame with columns Year, Quarter, Region, Store, Sales. Returns: pandas.DataFrame: DataFrame with a MultiIndex. # Your code here def perform_selection_and_slicing(df): Perform partial selection and slicing on the MultiIndex DataFrame. Parameters: df (pandas.DataFrame): MultiIndex DataFrame with the sales data. Returns: dict: Dictionary with keys \'year_quarter\', \'region\', \'range_slice\' and corresponding DataFrames. # Your code here def reindex_and_align(df, new_regions, other_df): Reindex the DataFrame to include new regions and align with another DataFrame. Parameters: df (pandas.DataFrame): MultiIndex DataFrame with the sales data. new_regions (list): List of new regions to include. other_df (pandas.DataFrame): Another DataFrame for alignment. Returns: pandas.DataFrame: Reindexed and aligned DataFrame. # Your code here def advanced_indexing_aggregation(df): Perform advanced indexing and aggregation on the MultiIndex DataFrame. Parameters: df (pandas.DataFrame): MultiIndex DataFrame with the sales data. Returns: dict: Dictionary with keys \'total_sales_region\', \'swapped_levels\', \'reordered_levels\' and corresponding DataFrames. # Your code here ``` # Example: ```python import pandas as pd data = { \'Year\': [2020, 2020, 2020, 2020, 2021, 2021, 2021, 2021], \'Quarter\': [\'Q1\', \'Q2\', \'Q3\', \'Q4\', \'Q1\', \'Q2\', \'Q3\', \'Q4\'], \'Region\': [\'East\', \'West\', \'North\', \'South\', \'East\', \'West\', \'North\', \'South\'], \'Store\': [\'StoreA\', \'StoreB\', \'StoreC\', \'StoreD\', \'StoreA\', \'StoreB\', \'StoreC\', \'StoreD\'], \'Sales\': [100.0, 150.0, 200.0, 250.0, 110.0, 160.0, 210.0, 260.0] } df = pd.DataFrame(data) # Call your functions with appropriate arguments to test your implementation ```","solution":"import pandas as pd def create_multiindex_df(df): Convert the given DataFrame to a MultiIndex DataFrame. Parameters: df (pandas.DataFrame): Original DataFrame with columns Year, Quarter, Region, Store, Sales. Returns: pandas.DataFrame: DataFrame with a MultiIndex. df.set_index([\'Year\', \'Quarter\', \'Region\', \'Store\'], inplace=True) return df def perform_selection_and_slicing(df): Perform partial selection and slicing on the MultiIndex DataFrame. Parameters: df (pandas.DataFrame): MultiIndex DataFrame with the sales data. Returns: dict: Dictionary with keys \'year_quarter\', \'region\', \'range_slice\' and corresponding DataFrames. results = {} # Select the sales figures for a specific year and quarter across all regions and stores results[\'year_quarter\'] = df.loc[2020, \'Q1\'] # Select the sales data for a given region across all years and quarters results[\'region\'] = df.xs(\'East\', level=\'Region\') # Perform a partial slice to get sales data for a specific range of years and quarters results[\'range_slice\'] = df.loc[pd.IndexSlice[2020:2021, \'Q1\':\'Q2\'], :] return results def reindex_and_align(df, new_regions, other_df): Reindex the DataFrame to include new regions and align with another DataFrame. Parameters: df (pandas.DataFrame): MultiIndex DataFrame with the sales data. new_regions (list): List of new regions to include. other_df (pandas.DataFrame): Another DataFrame for alignment. Returns: pandas.DataFrame: Reindexed and aligned DataFrame. # Reindex to include new regions new_index = pd.MultiIndex.from_product([df.index.levels[0], df.index.levels[1], df.index.levels[2].tolist() + new_regions, df.index.levels[3]], names=df.index.names) reindexed_df = df.reindex(new_index, fill_value=0) # Align with another DataFrame aligned_df, _ = reindexed_df.align(other_df, fill_value=0, join=\'outer\') return aligned_df def advanced_indexing_aggregation(df): Perform advanced indexing and aggregation on the MultiIndex DataFrame. Parameters: df (pandas.DataFrame): MultiIndex DataFrame with the sales data. Returns: dict: Dictionary with keys \'total_sales_region\', \'swapped_levels\', \'reordered_levels\' and corresponding DataFrames. results = {} # Compute the total sales for each region for all years and quarters results[\'total_sales_region\'] = df.groupby(level=\'Region\').sum() # Swap the levels of the index so that Store comes first and Year comes last results[\'swapped_levels\'] = df.swaplevel(\'Store\', \'Year\') # Reorder the levels to place Quarter at the first position results[\'reordered_levels\'] = df.reorder_levels([\'Quarter\', \'Year\', \'Region\', \'Store\']) return results"},{"question":"Objective: You are given the `penguins` dataset from seaborn. Your task is to demonstrate your understanding of seaborn by creating and customizing histograms and visualizing differences across groups. Problem Statement: 1. Load the `penguins` dataset. 2. Create a histogram of the `flipper_length_mm` variable using seaborn\'s `objects` interface. 3. Adjust the histogram to display the proportion of penguins rather than the count. 4. Create faceted histograms for `flipper_length_mm` grouped by the `island` variable, ensuring each island\'s distribution is independently normalized. 5. Create stacked histograms for `flipper_length_mm`, colored by the `sex` variable, and show them as a part-whole relationship. Input: None (the dataset is to be loaded from seaborn\'s default datasets). Expected Output: Your solution should not print anything explicitly. The result should be a series of plotted histograms as described: 1. A proportion-based histogram for `flipper_length_mm`. 2. Faceted histograms for `flipper_length_mm` normalized within each island group. 3. Stacked histograms representing a part-whole relationship, colored by `sex`. Constraints: - Utilize the seaborn `objects` interface. - Ensure plots are properly labeled with appropriate titles and axis labels. Function Signature: ```python import seaborn.objects as so from seaborn import load_dataset def plot_histograms(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # 1. Create a proportion-based histogram of \'flipper_length_mm\' p1 = so.Plot(penguins, \\"flipper_length_mm\\") p1.add(so.Bars(), so.Hist(stat=\\"proportion\\")) p1.show() # Ensure the plot is displayed # 2. Create faceted histograms grouped by \'island\', normalized within each island p2 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\") p2.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)) p2.show() # Ensure the plot is displayed # 3. Create stacked histograms, colored by \'sex\' p3 = so.Plot(penguins, \\"flipper_length_mm\\", color=\\"sex\\") p3.add(so.Bars(), so.Hist(), so.Stack()) p3.show() # Ensure the plot is displayed ``` Ensure that the histograms are correctly displayed and appropriately labeled.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_histograms(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # 1. Create a proportion-based histogram of \'flipper_length_mm\' p1 = so.Plot(penguins, \\"flipper_length_mm\\") p1.add(so.Bars(), so.Hist(stat=\\"proportion\\")) p1.show() # Ensure the plot is displayed # 2. Create faceted histograms grouped by \'island\', normalized within each island p2 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\") p2.add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)) p2.show() # Ensure the plot is displayed # 3. Create stacked histograms, colored by \'sex\' p3 = so.Plot(penguins, \\"flipper_length_mm\\", color=\\"sex\\") p3.add(so.Bars(), so.Hist(), so.Stack()) p3.show() # Ensure the plot is displayed"},{"question":"# IDLE Simple Text Editor Feature Emulation In this task, you will implement a simplified version of some text editing features that can be found in IDLE. Specifically, you will focus on the search and replace functionality. # Task You need to implement a function called `search_replace` that mimics the \'Find in Files...\' and \'Replace...\' functionality of IDLE. Function Specification ```python def search_replace(directory: str, file_extension: str, search_term: str, replacement: str) -> dict: Search for a specific term in all files with a given extension within a directory and its subdirectories, and replace it with a specified replacement term. Parameters: directory (str): The path to the directory where the search should start. file_extension (str): The file extension to filter files for searching (e.g., \\".py\\"). search_term (str): The term to search for within the files. replacement (str): The term to replace the search_term with. Returns: dict: A dictionary containing filenames as keys and the number of replacements made in each file as values. pass ``` # Input and Output Format - **Input:** - `directory`: A string representing the path of the directory where the search will be initiated. - `file_extension`: A string for filtering the files to search within (e.g., \'.py\' for Python files). - `search_term`: The string to search within these files. - `replacement`: The string that will replace every occurrence of `search_term` found in the files. - **Output:** - A dictionary, where the keys are filenames (with path) and the values are the count of replacements made in that file. # Constraints: - The function should search files recursively in the given directory and its subdirectories. - Only files with the specified extension should be considered. - The directory path will always be a valid directory. - The search and replacement strings will not be empty. # Example: ```python directory = \'/example_dir\' file_extension = \'.txt\' search_term = \'hello\' replacement = \'hi\' result = search_replace(directory, file_extension, search_term, replacement) print(result) ``` Output could be: ```python { \'/example_dir/file1.txt\': 3, \'/example_dir/sub_dir/file2.txt\': 5 } ``` This means: - In `file1.txt` located in `/example_dir`, the term \'hello\' was replaced by \'hi\' 3 times. - In `file2.txt` located in `/example_dir/sub_dir`, the term \'hello\' was replaced by \'hi\' 5 times. # Notes: - You may use the `os` module for directory and file operations. - Use appropriate error handling to manage issues such as file access permissions.","solution":"import os def search_replace(directory: str, file_extension: str, search_term: str, replacement: str) -> dict: Search for a specific term in all files with a given extension within a directory and its subdirectories, and replace it with a specified replacement term. Parameters: directory (str): The path to the directory where the search should start. file_extension (str): The file extension to filter files for searching (e.g., \\".py\\"). search_term (str): The term to search for within the files. replacement (str): The term to replace the search_term with. Returns: dict: A dictionary containing filenames as keys and the number of replacements made in each file as values. result = {} for root, _, files in os.walk(directory): for file in files: if file.endswith(file_extension): file_path = os.path.join(root, file) with open(file_path, \'r\', encoding=\'utf-8\') as f: content = f.read() replacement_count = content.count(search_term) if replacement_count > 0: new_content = content.replace(search_term, replacement) with open(file_path, \'w\', encoding=\'utf-8\') as f: f.write(new_content) result[file_path] = replacement_count return result"},{"question":"**Question:** Implement a function in Python that processes a list of student records and returns a summary based on specific criteria. You are expected to use control flow statements, functions, and proper parameter handling to solve this problem. # Problem Description You are given a list of dictionaries where each dictionary represents a student\'s record. Each student record contains the following information: - `name`: A string representing the student\'s name. - `age`: An integer representing the student\'s age. - `grade`: An integer representing the student\'s grade. - `status`: A string with values `\'active\'` or `\'inactive\'`, representing the student\'s status. Write a function named `process_student_records` that takes a list of student records and returns a dictionary with the following summary information: - `total_students`: The total number of students. - `average_age`: The average age of the students. - `grades_distribution`: A dictionary where keys are grades and values are the number of students who have that grade. - `status_count`: A dictionary containing the count of `\'active\'` and `\'inactive\'` students. # Function Signature ```python def process_student_records(students: list[dict]) -> dict: ``` # Example ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 90, \\"status\\": \\"active\\"}, {\\"name\\": \\"Bob\\", \\"age\\": 21, \\"grade\\": 85, \\"status\\": \\"inactive\\"}, {\\"name\\": \\"Charlie\\", \\"age\\": 22, \\"grade\\": 90, \\"status\\": \\"active\\"}, {\\"name\\": \\"David\\", \\"age\\": 23, \\"grade\\": 85, \\"status\\": \\"inactive\\"}, {\\"name\\": \\"Eve\\", \\"age\\": 20, \\"grade\\": 90, \\"status\\": \\"active\\"} ] print(process_student_records(students)) ``` # Expected Output ```python { \\"total_students\\": 5, \\"average_age\\": 21.2, \\"grades_distribution\\": {90: 3, 85: 2}, \\"status_count\\": {\\"active\\": 3, \\"inactive\\": 2} } ``` # Constraints - You can assume that all input data is well-formed and follows the mentioned structure. - Try to follow good coding practices, including appropriate function usage, naming conventions, and code readability. # Notes - Utilizing control flow statements such as `if`, `for`, `match`, and using functions effectively will be essential for solving this problem. - Make sure to handle different edge cases like an empty list of students.","solution":"def process_student_records(students: list[dict]) -> dict: Processes a list of student records and returns a summary. Args: students (list[dict]): List of dictionaries containing student records. Returns: dict: Summary containing total students, average age, grades distribution, and status count. total_students = len(students) if total_students == 0: return { \\"total_students\\": 0, \\"average_age\\": 0, \\"grades_distribution\\": {}, \\"status_count\\": { \\"active\\": 0, \\"inactive\\": 0 } } total_age = 0 grades_distribution = {} status_count = {\\"active\\": 0, \\"inactive\\": 0} for student in students: total_age += student[\\"age\\"] grade = student[\\"grade\\"] status = student[\\"status\\"] # Update grade distribution if grade in grades_distribution: grades_distribution[grade] += 1 else: grades_distribution[grade] = 1 # Update status count if status in status_count: status_count[status] += 1 average_age = total_age / total_students return { \\"total_students\\": total_students, \\"average_age\\": average_age, \\"grades_distribution\\": grades_distribution, \\"status_count\\": status_count }"},{"question":"# Density Estimation using Kernel Density Estimation in Scikit-learn Objective: To assess your understanding of kernel density estimation using the `KernelDensity` class from scikit-learn, you will implement a solution to estimate the density of a given dataset and visualize the results. Task: 1. **Load Data**: Given a 2D dataset, load the data into a NumPy array. 2. **Fit KDE Model**: Using the `KernelDensity` class from scikit-learn, fit a KDE model to the data with a specified kernel and bandwidth. 3. **Density Estimation**: Use the fitted KDE model to estimate the log-density at a set of given sample points. 4. **Visualization**: Visualize the original data points and overlay the estimated density. Input: - A 2D NumPy array `X` of shape (n_samples, 2) representing the coordinates of `n_samples`. - A string `kernel` specifying the kernel type to use in the KDE model. (e.g., \'gaussian\', \'tophat\', \'epanechnikov\', etc.) - A float `bandwidth` specifying the bandwidth parameter for the KDE model. - A 2D NumPy array `sample_points` of shape (m_samples, 2) representing the coordinates at which to evaluate the density estimate. Output: - A 1D NumPy array of length `m_samples` containing the log-density estimates for each of the given sample points. - A scatter plot showing the original data points and a contour plot of the estimated density. Constraints: - Bandwidth must be a positive float. - Kernel must be one of the valid kernel types supported by `KernelDensity`. Example: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Sample data X = np.array([[1, 2], [2, 3], [3, 2], [6, 7], [7, 8], [8, 7]]) # Sample points for density estimation sample_points = np.array([[2, 2], [3, 3], [6, 6]]) # Kernel and bandwidth kernel = \'gaussian\' bandwidth = 0.5 # Function to fit KDE model and estimate density def kde_density_estimate(X, kernel, bandwidth, sample_points): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) log_density = kde.score_samples(sample_points) return log_density # Estimate log-density log_density = kde_density_estimate(X, kernel, bandwidth, sample_points) print(log_density) # Visualization plt.scatter(X[:, 0], X[:, 1], label=\'Original Data\') plt.scatter(sample_points[:, 0], sample_points[:, 1], c=\'red\', label=\'Sample Points\') plt.legend() plt.title(\'KDE Density Estimation\') plt.show() ``` Implement and complete the function `kde_density_estimate` to achieve the desired log-density estimates and visualization. Ensure your implementation is robust to different kernel types and bandwidth values.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kde_density_estimate(X, kernel, bandwidth, sample_points): Estimates the log-density of the sample points using KDE. Parameters: - X: 2D NumPy array of shape (n_samples, 2) containing the original data points. - kernel: String, specifying the kernel type for KDE (e.g., \'gaussian\'). - bandwidth: Positive float, specifying the bandwidth for KDE. - sample_points: 2D NumPy array of shape (m_samples, 2) for density estimation. Returns: - log_density: 1D NumPy array of length m_samples containing the log-density estimates. # Fit the KDE model to the data kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) # Estimate the log-density at the specified sample points log_density = kde.score_samples(sample_points) # Visualization plt.scatter(X[:, 0], X[:, 1], label=\'Original Data\') plt.scatter(sample_points[:, 0], sample_points[:, 1], c=\'red\', label=\'Sample Points\') plt.title(\'KDE Density Estimation\') plt.legend() plt.show() return log_density"},{"question":"# PyTorch Named Tensors: Name Inference In this task, you will implement functions using PyTorch named tensors and apply operations that propagate and manage tensor names. You need to show understanding of name propagation rules including keeps input names, removes dimensions, unifies names from inputs, permutes dimensions, and contracts away dims. # Problem Statement Given two named tensors, `tensor1` and `tensor2`, implement three functions: 1. **propagate_unary_names**: Apply a unary operation (such as `torch.abs`) to `tensor1` and return the resulting tensor with the same names. 2. **reduce_tensor**: Perform a reduction operation (such as `torch.sum`) on `tensor1` over a specified dimension and return the resulting tensor with the appropriate dimension names removed. Ensure you can handle cases where `keepdim` is set to `True`. 3. **combine_named_tensors**: Perform a binary operation (such as `torch.add`) on `tensor1` and `tensor2` and return the resulting tensor with unified dimension names according to name inference rules. # Function Specifications 1. `propagate_unary_names` ```python def propagate_unary_names(tensor1): Apply a unary operation to tensor1 and ensure the output has the same names. Parameters: tensor1 (torch.Tensor): A named tensor with any shape. Returns: torch.Tensor: A tensor resulting from a unary operation with the same dimension names. pass ``` 2. `reduce_tensor` ```python def reduce_tensor(tensor1, dim, keepdim=False): Perform a reduction operation over a specified dimension of tensor1. Parameters: tensor1 (torch.Tensor): A named tensor with any shape. dim (str or int): The dimension name or index to reduce over. keepdim (bool): Whether to keep the reduced dimension in the output tensor. Returns: torch.Tensor: A tensor with the specified dimension reduced. pass ``` 3. `combine_named_tensors` ```python def combine_named_tensors(tensor1, tensor2): Perform a binary operation on tensor1 and tensor2 and return with unified names. Parameters: tensor1 (torch.Tensor): A named tensor with any shape. tensor2 (torch.Tensor): A named tensor with compatible dimensions and any shape. Returns: torch.Tensor: A tensor resulting from a binary operation with unified dimension names. pass ``` # Example Usage ```python import torch # Example for propagate_unary_names x = torch.randn(3, 3, names=(\'N\', \'C\')) result = propagate_unary_names(x) print(result.names) # Should print (\'N\', \'C\') # Example for reduce_tensor y = torch.randn(3, 3, names=(\'N\', \'C\')) reduced = reduce_tensor(y, \'N\') print(reduced.names) # Should print (\'C\') # Example for combine_named_tensors a = torch.randn(3, 3, names=(\'N\', \'C\')) b = torch.randn(3, 3, names=(\'C\', \'W\')) combined = combine_named_tensors(a, b) print(combined.names) # Should print combined names following the rules ``` **Constraints:** - You can assume that the inputs provided conform to PyTorch\'s dimension name rules. - Ensure that tensor operations conform to PyTorch\'s compatibility and broadcasting rules. - Handle errors gracefully if the names or dimensions do not align. # Submission Submit your implementation of the above functions. Ensure that they pass the provided example usage and other edge cases that might be tested.","solution":"import torch def propagate_unary_names(tensor1): Apply a unary operation to tensor1 and ensure the output has the same names. Parameters: tensor1 (torch.Tensor): A named tensor with any shape. Returns: torch.Tensor: A tensor resulting from a unary operation with the same dimension names. result = torch.abs(tensor1) result.names = tensor1.names return result def reduce_tensor(tensor1, dim, keepdim=False): Perform a reduction operation over a specified dimension of tensor1. Parameters: tensor1 (torch.Tensor): A named tensor with any shape. dim (str or int): The dimension name or index to reduce over. keepdim (bool): Whether to keep the reduced dimension in the output tensor. Returns: torch.Tensor: A tensor with the specified dimension reduced. if isinstance(dim, str): dim = tensor1.names.index(dim) result = torch.sum(tensor1, dim=dim, keepdim=keepdim) result.names = tensor1.names if keepdim else tuple(n for i, n in enumerate(tensor1.names) if i != dim) return result def combine_named_tensors(tensor1, tensor2): Perform a binary operation on tensor1 and tensor2 and return with unified names. Parameters: tensor1 (torch.Tensor): A named tensor with any shape. tensor2 (torch.Tensor): A named tensor with compatible dimensions and any shape. Returns: torch.Tensor: A tensor resulting from a binary operation with unified dimension names. result = torch.add(tensor1, tensor2) return result"},{"question":"SVM Classification and Custom Kernels You are tasked with utilizing scikit-learn\'s Support Vector Machine (SVM) functionality to: 1. Perform binary classification on a given dataset. 2. Implement and use a custom kernel for the SVM classifier. **Requirements:** 1. **Binary Classification:** - Load a provided dataset (you may assume it is in CSV format with features and binary target labels). - Split the dataset into training and testing sets. - Train an SVM classifier using the Radial Basis Function (RBF) kernel. - Evaluate the classifier using accuracy, precision, and recall metrics on the test set. 2. **Custom Kernel Implementation:** - Implement a custom polynomial kernel function. - Train an SVM classifier using this custom kernel function. - Evaluate the classifier using the same metrics. **Input Format:** - A CSV file (`data.csv`) with `n_samples` rows and `n_features + 1` columns. - The first `n_features` columns represent the features, and the last column represents the binary target label (0 or 1). **Output Format:** - Print the evaluation metrics (accuracy, precision, recall) for both classifiers. **Constraints:** - Use scikit-learn for implementing SVM and evaluation metrics. - The polynomial kernel function should be of degree 3. **Performance Requirements:** - The evaluation should be completed within a reasonable time frame for a dataset with up to 10,000 samples and 100 features. **Example:** ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn import svm from sklearn.metrics import accuracy_score, precision_score, recall_score # Load data data = pd.read_csv(\'data.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train SVM with RBF kernel rbf_clf = svm.SVC(kernel=\'rbf\') rbf_clf.fit(X_train, y_train) # Predict and evaluate y_pred_rbf = rbf_clf.predict(X_test) print(\\"RBF Kernel SVM:\\") print(f\\"Accuracy: {accuracy_score(y_test, y_pred_rbf)}\\") print(f\\"Precision: {precision_score(y_test, y_pred_rbf)}\\") print(f\\"Recall: {recall_score(y_test, y_pred_rbf)}\\") # Custom polynomial kernel function def custom_poly_kernel(X, Y): return (1 + X.dot(Y.T)) ** 3 # Train SVM with custom kernel custom_clf = svm.SVC(kernel=custom_poly_kernel) custom_clf.fit(X_train, y_train) # Predict and evaluate y_pred_custom = custom_clf.predict(X_test) print(\\"Custom Kernel SVM:\\") print(f\\"Accuracy: {accuracy_score(y_test, y_pred_custom)}\\") print(f\\"Precision: {precision_score(y_test, y_pred_custom)}\\") print(f\\"Recall: {recall_score(y_test, y_pred_custom)}\\") ``` **Note:** Ensure the custom polynomial kernel function adheres to the specified degree and handles the dimensions of the input matrices correctly.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn import svm from sklearn.metrics import accuracy_score, precision_score, recall_score import numpy as np # Load data def load_data(file_path): data = pd.read_csv(file_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values return X, y # Custom polynomial kernel function def custom_poly_kernel(X, Y): return (1 + np.dot(X, Y.T)) ** 3 # SVM Classification function def svm_classification(file_path): X, y = load_data(file_path) # Split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train SVM with RBF kernel rbf_clf = svm.SVC(kernel=\'rbf\') rbf_clf.fit(X_train, y_train) # Predict and evaluate for RBF kernel y_pred_rbf = rbf_clf.predict(X_test) rbf_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_rbf), \'precision\': precision_score(y_test, y_pred_rbf), \'recall\': recall_score(y_test, y_pred_rbf) } # Train SVM with custom kernel custom_clf = svm.SVC(kernel=custom_poly_kernel) custom_clf.fit(X_train, y_train) # Predict and evaluate for custom kernel y_pred_custom = custom_clf.predict(X_test) custom_metrics = { \'accuracy\': accuracy_score(y_test, y_pred_custom), \'precision\': precision_score(y_test, y_pred_custom), \'recall\': recall_score(y_test, y_pred_custom) } return rbf_metrics, custom_metrics # Example usage if __name__ == \\"__main__\\": rbf_metrics, custom_metrics = svm_classification(\'data.csv\') print(\\"RBF Kernel SVM:\\", rbf_metrics) print(\\"Custom Kernel SVM:\\", custom_metrics)"},{"question":"# Advanced File Operations with `shutil` You are tasked with creating a Python script that performs a series of advanced file operations using the `shutil` module. The script should be able to: 1. **Copy** a specified source file to a specified destination directory. 2. **Move** a specified source directory to a specified destination directory. 3. **Create an archive** of a specified directory. 4. **Delete** a specified directory. 5. **Provide disk usage** statistics of a specified path. Function Specifications 1. **copy_file(src_file, dst_dir)** - **Input:** - `src_file` (str): The path of the source file to copy. - `dst_dir` (str): The destination directory where the file should be copied. - **Output:** - None. - **Behavior:** - The function should copy the source file to the destination directory. If the destination directory does not exist, it should be created. 2. **move_directory(src_dir, dst_dir)** - **Input:** - `src_dir` (str): The path of the source directory to move. - `dst_dir` (str): The destination directory where the source directory should be moved. - **Output:** - None. - **Behavior:** - The function should move the source directory to the destination directory. If the destination already exists but is not a directory, it should be overwritten. 3. **create_archive(src_dir, archive_name, archive_format)** - **Input:** - `src_dir` (str): The directory to archive. - `archive_name` (str): The name of the resulting archive (without extension). - `archive_format` (str): The format of the archive, one of `zip`, `tar`, `gztar`, `bztar`, or `xztar`. - **Output:** - None. - **Behavior:** - The function should create an archive of the specified directory with the given name and format. 4. **delete_directory(dir_path)** - **Input:** - `dir_path` (str): The directory to delete. - **Output:** - None. - **Behavior:** - The function should delete the specified directory and all its contents. If the specified path is not a directory, it should raise an appropriate exception. 5. **get_disk_usage(path)** - **Input:** - `path` (str): The path to check disk usage statistics. - **Output:** - A dictionary with keys `total`, `used`, and `free` representing the disk usage statistics in bytes. - **Behavior:** - The function should return the disk usage statistics for the specified path. Constraints - You may assume all paths provided are valid and the necessary permissions are in place. - The created archive should be in the same directory as the script. - The functions should handle typical edge cases like nonexistent paths appropriately, raising exceptions where necessary. # Example Usage ```python copy_file(\\"/path/to/source.txt\\", \\"/path/to/destination/\\") move_directory(\\"/path/to/source_dir\\", \\"/path/to/destination_dir\\") create_archive(\\"/path/to/source_dir\\", \\"my_archive\\", \\"zip\\") delete_directory(\\"/path/to/delete_dir\\") disk_usage = get_disk_usage(\\"/path/to/check\\") print(disk_usage) ``` # Evaluation Criteria - Correct usage of the `shutil` module functions. - Proper handling of edge cases. - Clear and efficient code. - Detailed docstrings for each function.","solution":"import shutil import os def copy_file(src_file, dst_dir): Copies a source file to a destination directory. Parameters: src_file (str): The path of the source file to copy. dst_dir (str): The destination directory where the file should be copied. Returns: None if not os.path.exists(dst_dir): os.makedirs(dst_dir) shutil.copy(src_file, dst_dir) def move_directory(src_dir, dst_dir): Moves a source directory to a destination directory. Parameters: src_dir (str): The path of the source directory to move. dst_dir (str): The destination directory where the source directory should be moved. Returns: None if os.path.exists(dst_dir) and not os.path.isdir(dst_dir): os.remove(dst_dir) shutil.move(src_dir, dst_dir) def create_archive(src_dir, archive_name, archive_format): Creates an archive of a specified directory. Parameters: src_dir (str): The directory to archive. archive_name (str): The name of the resulting archive (without extension). archive_format (str): The format of the archive, one of \'zip\', \'tar\', \'gztar\', \'bztar\', or \'xztar\'. Returns: None shutil.make_archive(archive_name, archive_format, src_dir) def delete_directory(dir_path): Deletes a directory and all its contents. Parameters: dir_path (str): The directory to delete. Returns: None Raises: ValueError: If the specified path is not a directory. if not os.path.isdir(dir_path): raise ValueError(f\\"The path {dir_path} is not a directory.\\") shutil.rmtree(dir_path) def get_disk_usage(path): Provides disk usage statistics of a specified path. Parameters: path (str): The path to check disk usage statistics. Returns: dict: A dictionary with keys \'total\', \'used\', and \'free\' representing the disk usage statistics in bytes. usage = shutil.disk_usage(path) return {\'total\': usage.total, \'used\': usage.used, \'free\': usage.free}"},{"question":"**Context**: You are given the `contextvars` C API documentation. Your task is to create a Python extension using these C API functions to manage context variables within Python code. The objective is to demonstrate how to create, manage, and utilize context-local state. # Task 1. **Implement the C extension**: - Create a new context using `PyContext_New`. - Create a new context variable using `PyContextVar_New`. - Set a value to the context variable using `PyContextVar_Set`. - Retrieve the value of the context variable using `PyContextVar_Get`. - Reset the context variable to its previous state using `PyContextVar_Reset`. 2. **Python Interface**: - Provide a Python module with a wrapper class `ContextManager` encapsulating the C extension functionalities. - The `ContextManager` class should expose the following methods: - `create_context()`: Creates and returns a new context. - `create_context_var(name, default_value)`: Creates and returns a new context variable. - `set_context_var(context_var, value)`: Sets the value of a context variable. - `get_context_var(context_var, default_value)`: Gets the value of a context variable. - `reset_context_var(context_var, token)`: Resets the context variable to its previous state. # Input and Output Formats **Input**: - The user will create instances of `ContextManager`, and invoke the methods with appropriate arguments. **Output**: - Methods will return results or raise exceptions based on the success or failure of the operations. # Constraints - Ensure proper error handling and resource management. - Maintain thread-safety where applicable. - Document the usage of your `ContextManager` class. # Example Usage ```python # Example usage of the ContextManager class from context_manager import ContextManager # Create an instance of the ContextManager cm = ContextManager() # Create a new context ctx = cm.create_context() # Create a new context variable var = cm.create_context_var(\'user_id\', 0) # Set a value to the context variable token = cm.set_context_var(var, 123) # Retrieve the value of the context variable user_id = cm.get_context_var(var, 0) print(user_id) # Output: 123 # Reset the context variable cm.reset_context_var(var, token) # Retrieve the reset value of the context variable user_id_reset = cm.get_context_var(var, 0) print(user_id_reset) # Output: 0 ``` Your implementation should follow these guidelines to demonstrate your understanding and proficiency in integrating Python with C APIs.","solution":"import contextvars class ContextManager: def create_context(self): self.context = contextvars.Context() return self.context def create_context_var(self, name, default_value): context_var = contextvars.ContextVar(name, default=default_value) return context_var def set_context_var(self, context_var, value): token = context_var.set(value) return token def get_context_var(self, context_var, default_value): return context_var.get(default_value) def reset_context_var(self, context_var, token): context_var.reset(token)"},{"question":"# Calculation Challenge with math Module You are tasked with implementing a Python function that performs a series of mathematical operations using the `math` module functions. Your function will take two input numbers and perform the following operations: 1. Compute the factorial of the first number. 2. Compute the natural logarithm of the factorial result. 3. Compute the cosine of the second number. 4. Compute the Euclidean distance between the factorial result and the cosine result. # Function Signature ```python import math def complex_math_operations(a: int, b: float) -> float: pass ``` # Input - `a` (int): A non-negative integer. - `b` (float): A floating-point number. # Output - Return a float value which is the Euclidean distance between the natural logarithm of the factorial of `a` and the cosine of `b`. # Constraints - `0 <= a <= 50` - `-2 * math.pi <= b <= 2 * math.pi` # Example ```python import math def complex_math_operations(a: int, b: float) -> float: factorial_result = math.factorial(a) log_result = math.log(factorial_result) cosine_result = math.cos(b) distance = math.dist([log_result], [cosine_result]) return distance # Example Usage print(complex_math_operations(5, math.pi)) # Expected Output: Distance value ``` # Notes - Ensure exception handling where required, such as when taking logarithms and computing factorials. - Utilize relevant functions directly from the `math` module. - The Euclidean distance here is computed between two single-element lists for compatibility with `math.dist`.","solution":"import math def complex_math_operations(a: int, b: float) -> float: Computes a series of mathematical operations: 1. Factorial of the first number. 2. Natural logarithm of the factorial result. 3. Cosine of the second number. 4. Euclidean distance between the logarithm result and cosine result. Args: a (int): A non-negative integer. b (float): A floating-point number. Returns: float: Euclidean distance between the logarithm of the factorial of `a` and the cosine of `b`. # Ensure the constraints are met. assert 0 <= a <= 50, \\"a should be in the range [0, 50]\\" assert -2 * math.pi <= b <= 2 * math.pi, \\"b should be in the range [-2π, 2π]\\" # Compute the factorial of a factorial_result = math.factorial(a) # Compute the natural logarithm of the factorial result log_result = math.log(factorial_result) # Compute the cosine of b cosine_result = math.cos(b) # Compute the Euclidean distance distance = math.dist([log_result], [cosine_result]) return distance"},{"question":"# Python Coding Assessment Question **Objective**: Implement a function that demonstrates your understanding of file operations, process management, and environment variables using the `os` module. # Problem Statement **Task**: You are required to implement the following two functions using Python\'s `os` module: 1. `copy_file_with_environment(file_path, new_file_path, env_var_name, env_var_value)`: This function should copy the contents of a file to a new destination while setting a specific environment variable. 2. `list_non_hidden_files(directory)`: This function should recursively list all non-hidden files in a directory (ignoring files that start with a dot `.`). # Function Details 1. `copy_file_with_environment(file_path, new_file_path, env_var_name, env_var_value)` * **Input**: - `file_path` (str): The path to the source file to be copied. - `new_file_path` (str): The path to the destination file. - `env_var_name` (str): The environment variable name to be set. - `env_var_value` (str): The value of the environment variable to be set. * **Output**: - `None` * **Behavior**: - It should copy the file from `file_path` to `new_file_path`. - Before performing the copy operation, set the environment variable `env_var_name` to `env_var_value`. - Make sure to handle exceptions related to file operations properly. 2. `list_non_hidden_files(directory)` * **Input**: - `directory` (str): The path to the directory to be scanned. * **Output**: - It should return a list of paths of non-hidden files in the directory and its subdirectories. * **Behavior**: - Recursively traverse the directory. - Collect all files that do not start with a dot `.`. - Return the collected file paths as a list. # Constraints - You are not allowed to use any external packages or libraries outside the standard `os` module. - Ensure your functions handle common edge cases such as file not found, permission errors, etc. # Example Usage ```python # Example for copy_file_with_environment copy_file_with_environment(\'/path/to/source/file.txt\', \'/path/to/destination/file.txt\', \'TEST_ENV_VAR\', \'some_value\') # Example for list_non_hidden_files files = list_non_hidden_files(\'/path/to/directory\') print(files) ``` # Assessment Criteria - Correct use of the `os` module to manipulate files, directories, and environment variables. - Proper error handling and resource management. - Clear and efficient recursive implementation for listing files. - Adherence to the provided function signatures and expected behavior.","solution":"import os import shutil def copy_file_with_environment(file_path, new_file_path, env_var_name, env_var_value): Copies a file from file_path to new_file_path and sets an environment variable. try: # Set the environment variable os.environ[env_var_name] = env_var_value # Perform the file copy operation shutil.copy(file_path, new_file_path) except Exception as e: print(f\\"An error occurred: {e}\\") def list_non_hidden_files(directory): Recursively lists all non-hidden files in a given directory and its subdirectories. non_hidden_files = [] try: for root, _, files in os.walk(directory): for file in files: if not file.startswith(\'.\'): non_hidden_files.append(os.path.join(root, file)) except Exception as e: print(f\\"An error occurred: {e}\\") return non_hidden_files"},{"question":"# Question: Implement a Custom Scheduler with `collections.deque` You are tasked with developing a custom round-robin scheduler using Python\'s `collections.deque`. A round-robin scheduler cyclically distributes tasks to a fixed number of workers, ensuring all tasks are processed fairly. # Requirements 1. **Class Definition**: Define a class called `RoundRobinScheduler`. 2. **Initialization Parameters**: The class should accept two parameters upon initialization: - `workers`: An integer indicating the number of workers. - `tasks`: A list of tasks to be scheduled where each task is represented by a unique string. 3. **Methods to Implement**: - `add_task(self, task: str)`: Add a new task to the scheduler. - `remove_task(self, task: str)`: Remove a task from the scheduler. - `next_worker(self)`: Return the next worker ID (0 to `workers-1`) to which a task should be assigned. This should work in a round-robin fashion. - `schedule_task(self) -> tuple`: Assign the next task from the task queue to the next worker in a round-robin manner and return a tuple `(worker_id, task)`. If no tasks are available, return `None`. # Constraints - The number of workers (`workers`) is a positive integer. - The list of tasks (`tasks`) can be empty or contain several task strings. - Task strings are unique. # Performance Requirement - The implementation should ensure that adding or removing tasks and fetching the next task should be efficient, having average time complexity of O(1) for each operation. # Example Usage ```python scheduler = RoundRobinScheduler(workers=3, tasks=[\\"task1\\", \\"task2\\", \\"task3\\"]) # Initial scheduling print(scheduler.schedule_task()) # Output: (0, \\"task1\\") print(scheduler.schedule_task()) # Output: (1, \\"task2\\") print(scheduler.schedule_task()) # Output: (2, \\"task3\\") print(scheduler.schedule_task()) # Output: (0, \\"task1\\") # Adding a new task scheduler.add_task(\\"task4\\") print(scheduler.schedule_task()) # Output: (1, \\"task2\\") # Removing a task scheduler.remove_task(\\"task2\\") print(scheduler.schedule_task()) # Output: (2, \\"task3\\") # Check the remaining tasks scheduler.remove_task(\\"task1\\") scheduler.remove_task(\\"task3\\") scheduler.remove_task(\\"task4\\") print(scheduler.schedule_task()) # Output: None ``` Implement the `RoundRobinScheduler` class as described above.","solution":"from collections import deque class RoundRobinScheduler: def __init__(self, workers, tasks): self.workers = workers self.tasks = deque(tasks) self.current_worker_index = 0 def add_task(self, task): self.tasks.append(task) def remove_task(self, task): try: self.tasks.remove(task) except ValueError: pass # Task not found, do nothing def next_worker(self): worker_id = self.current_worker_index self.current_worker_index = (self.current_worker_index + 1) % self.workers return worker_id def schedule_task(self): if not self.tasks: return None task = self.tasks.popleft() worker_id = self.next_worker() self.tasks.append(task) return (worker_id, task)"},{"question":"**Question:** You are provided with a dataset in the form of a pandas DataFrame. The DataFrame consists of several columns with different data types, including integers, floats, objects, and boolean values. Additionally, the DataFrame contains some missing values (NA). Your task is to write a function `analyze_dataframe(df)` that takes the DataFrame as an input and returns a dictionary with the following statistical information: 1. `memory_usage`: A dictionary where keys are the column names and values are the memory usage (in bytes) of each column, including the index. 2. `deep_memory_usage`: The total memory usage of the DataFrame when using deep introspection. 3. `missing_values_percentage`: A dictionary where keys are the column names and values are the percentage of missing values in each column. 4. `dtypes`: A dictionary where keys are the column names and values are the data types of each column in the DataFrame. **Input:** - `df`: A pandas DataFrame containing any number of columns with different data types and some missing values. **Output:** - A dictionary with four keys: `memory_usage`, `deep_memory_usage`, `missing_values_percentage`, and `dtypes`. Each key should map to the respective information as described above. **Example:** ```python import pandas as pd import numpy as np data = { \'int_column\': [1, 2, np.nan, 4, 5], \'float_column\': [1.0, 2.0, 3.0, np.nan, 5.0], \'object_column\': [\'a\', \'b\', \'c\', np.nan, \'e\'], \'bool_column\': [True, False, np.nan, True, False] } df = pd.DataFrame(data) result = analyze_dataframe(df) print(result) ``` **Expected Output:** ``` { \'memory_usage\': {\'Index\': 128, \'int_column\': 40, \'float_column\': 40, \'object_column\': 176, \'bool_column\': 40}, \'deep_memory_usage\': 608, \'missing_values_percentage\': {\'int_column\': 20.0, \'float_column\': 20.0, \'object_column\': 20.0, \'bool_column\': 20.0}, \'dtypes\': {\'int_column\': \'float64\', \'float_column\': \'float64\', \'object_column\': \'object\', \'bool_column\': \'object\'} } ``` Note: The memory usage values in the example output may vary based on the system and pandas version. Ensure your function computes the values accurately. **Constraints:** - Handle DataFrame with any number of columns and data types. - Do not modify the input DataFrame.","solution":"import pandas as pd def analyze_dataframe(df): Analyzes the given pandas DataFrame and returns statistical information as a dictionary. Args: df (pd.DataFrame): The DataFrame to analyze. Returns: dict: A dictionary with keys \'memory_usage\', \'deep_memory_usage\', \'missing_values_percentage\', and \'dtypes\'. memory_usage = df.memory_usage(deep=True).to_dict() deep_memory_usage = df.memory_usage(index=True, deep=True).sum() missing_values_percentage = (df.isna().sum() / len(df) * 100).to_dict() dtypes = df.dtypes.apply(lambda x: x.name).to_dict() return { \'memory_usage\': memory_usage, \'deep_memory_usage\': deep_memory_usage, \'missing_values_percentage\': missing_values_percentage, \'dtypes\': dtypes }"},{"question":"# Advanced Pandas Assessment: Custom Date Offsets Objective: Implement and utilize custom date offsets in pandas to perform complex time series date manipulations. Question: You are tasked with generating a time series of business days (Monday through Friday) from January 1, 2023, to December 31, 2023, excluding specified holidays. Additionally, you need to generate a time series of business hours (8 AM to 5 PM) for the same date range. **Implement the following:** 1. **Custom Business Day Generator**: Create a function `custom_business_days` that generates a dataframe with business days excluding given holidays. - **Input Parameters**: - `start_date` (str): The start date in \'YYYY-MM-DD\' format (inclusive). - `end_date` (str): The end date in \'YYYY-MM-DD\' format (inclusive). - `holidays` (list of str): A list of holidays in \'YYYY-MM-DD\' format. - **Output**: - A Pandas DataFrame with a single column `BusinessDays` containing the dates of all valid business days in the specified range excluding the holidays. 2. **Custom Business Hour Generator**: Create a function `custom_business_hours` that generates a dataframe with business hours for the valid business days. - **Input Parameters**: - `business_days` (Pandas DataFrame): The DataFrame obtained from `custom_business_days` function. - `start_hour` (int): The starting hour for business hours (e.g., 8). - `end_hour` (int): The ending hour for business hours (e.g., 17). - **Output**: - A Pandas DataFrame with two columns: `Date` and `Hour`, representing the business date and hour slots respectively. Constraints: 1. Assume all inputs are valid and given in the correct format. 2. The functions should handle large date ranges efficiently. Example Usage: ```python start_date = \'2023-01-01\' end_date = \'2023-12-31\' holidays = [\'2023-01-01\', \'2023-12-25\', \'2023-07-04\'] # Generate business days business_days_df = custom_business_days(start_date, end_date, holidays) print(business_days_df) # Generate business hours business_hours_df = custom_business_hours(business_days_df, 8, 17) print(business_hours_df) ``` Notes: - You may use pandas\' offset classes like `CustomBusinessDay`, `BusinessHour`, and other relevant functions to achieve the desired results. - Ensure that the functions are well-documented and handle various edge cases, such as leap years.","solution":"import pandas as pd def custom_business_days(start_date, end_date, holidays): Generates a DataFrame with business days excluding given holidays. Parameters: start_date (str): The start date in \'YYYY-MM-DD\' format (inclusive). end_date (str): The end date in \'YYYY-MM-DD\' format (inclusive). holidays (list of str): A list of holidays in \'YYYY-MM-DD\' format. Returns: pd.DataFrame: DataFrame with a single column \'BusinessDays\' containing the dates of all valid business days. # Convert holidays to pandas datetime format holidays = pd.to_datetime(holidays) # Generate custom business day offset excluding holidays bday = pd.offsets.CustomBusinessDay(holidays=holidays) # Generate the business days range business_days = pd.date_range(start=start_date, end=end_date, freq=bday) # Create and return the DataFrame return pd.DataFrame({\'BusinessDays\': business_days}) def custom_business_hours(business_days_df, start_hour, end_hour): Generates a DataFrame with business hours for the valid business days. Parameters: business_days_df (pd.DataFrame): The DataFrame obtained from custom_business_days function. start_hour (int): The starting hour for business hours (e.g., 8). end_hour (int): The ending hour for business hours (e.g., 17). Returns: pd.DataFrame: DataFrame with two columns: \'Date\' and \'Hour\'. # Create a DataFrame to store the result result = [] # Iterate over each business day and create the business hours for business_day in business_days_df[\'BusinessDays\']: hours = list(range(start_hour, end_hour + 1)) for hour in hours: result.append({\'Date\': business_day, \'Hour\': hour}) # Convert the result list into a DataFrame return pd.DataFrame(result)"},{"question":"# Seaborn Plotting Challenge You are given a dataset containing information about fruit prices over time. Your task is to create a comprehensive visualization using the seaborn library. This question is designed to assess your understanding of seaborn for creating and customizing plots. Dataset You need to use the `fruit_prices` dataset, which contains the following columns: - `Date`: The date of the price record. - `Fruit`: The type of fruit. - `Price`: The price of the fruit on the given date. You can assume that the dataset is loaded as follows: ```python import seaborn as sns # Load the sample dataset fruit_prices = sns.load_dataset(\\"fruit_prices\\") ``` Tasks 1. **Line Plot with Grouping** Create a line plot to show the price trends over time for each type of fruit. Each fruit should have its own distinct color. 2. **Error Bands** Enhance the line plot by including error bands to represent the variability of prices at each time point. Use a different linestyle for each fruit type. 3. **Markers** Add markers to the plot to indicate actual data points where prices were recorded. Customize the markers to be distinguishable for each type of fruit. 4. **Orientation Change** Create a variant of the plot where the Date is on the y-axis and the Price is on the x-axis. Ensure the orientation change reflects correctly by maintaining all customizations (color, linestyle, markers). Expected Functions You need to implement the function `create_fruit_price_plot(data)` which takes in the dataset and outputs the two plots: - Horizontal line plot (Date on x-axis and Price on the y-axis). - Vertical line plot (Date on y-axis and Price on the x-axis). Constraints - Make sure the plots are clear and labeled correctly. - Use a consistent color palette for the fruits across both plots. Example Here is an example of how your function might be used: ```python # Load dataset fruit_prices = sns.load_dataset(\\"fruit_prices\\") # Generate plots create_fruit_price_plot(fruit_prices) ``` The function should generate a horizontal and a vertical line plot that satisfy the described specifications. Note: If the `fruit_prices` dataset does not exist in seaborn, create a mock dataframe with similar structure for testing.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_fruit_price_plot(data): Creates two seaborn line plots for fruit prices over time. Args: data: DataFrame containing the columns \'Date\', \'Fruit\', and \'Price\'. Returns: None # Define the color palette sns.set_palette(\\"husl\\") # Create a line plot with error bands and markers line_plot = sns.lineplot( data=data, x=\\"Date\\", y=\\"Price\\", hue=\\"Fruit\\", style=\\"Fruit\\", markers=True, dashes=False, err_style=\\"band\\" ) line_plot.set(title=\\"Fruit Prices Over Time\\") plt.legend(title=\\"Fruit\\") plt.show() # Create the same plot but with Date on the y-axis and Price on the x-axis line_plot_vertical = sns.lineplot( data=data, x=\\"Price\\", y=\\"Date\\", hue=\\"Fruit\\", style=\\"Fruit\\", markers=True, dashes=False, err_style=\\"band\\" ) line_plot_vertical.set(title=\\"Fruit Prices Over Time (Vertical Orientation)\\") plt.legend(title=\\"Fruit\\") plt.show()"},{"question":"**Objective:** Design and implement a Python class that performs different operations on numeric objects and sequence objects, incorporating type checks and ensuring memory safety as described in the provided documentation. **Problem Statement:** You are asked to implement a Python class `CustomCollection` that supports the following functionalities: 1. **Initialization**: Accepts a dictionary of numeric objects, where the keys are strings and the values are either integers or floating-point numbers. 2. **Add Element**: Adds a new element to the collection. 3. **Remove Element**: Removes an element from the collection based on the key. 4. **Find Maximum**: Returns the maximum value among all numeric objects in the collection. 5. **Get Unicode Representation**: Provides the Unicode string for all sequence objects. 6. **Convert to List**: Converts the dictionary values to a list of values. **Class Definition:** ```python class CustomCollection: def __init__(self, initial_data: dict): # Initialize the collection with a dictionary of numeric objects. pass def add_element(self, key: str, value: [int, float]): # Add a new element to the collection. pass def remove_element(self, key: str): # Remove an element from the collection based on the key. pass def find_maximum(self) -> [int, float]: # Return the maximum value among all numeric objects in the collection. pass def get_unicode_representation(self) -> str: # Return the Unicode string representation for all sequence objects. pass def convert_to_list(self) -> list: # Convert the dictionary values to a list of values. pass ``` **Constraints:** - The initial dictionary, as well as all elements added, should only contain numeric objects (integers or floating points). - If a non-numeric value is attempted to be added, raise a `TypeError`. - The `get_unicode_representation()` method should concatenate all keys and values as a single Unicode string. **Input/Output Format:** - **Initialization**: `CustomCollection({\'a\': 1, \'b\': 2.5})` - **Add Element**: `add_element(\'c\', 3)` - **Remove Element**: `remove_element(\'a\')` - **Find Maximum**: `find_maximum() -> 3` - **Get Unicode Representation**: `get_unicode_representation() -> \'b2.5c3.0\'` - **Convert to List**: `convert_to_list() -> [2.5, 3]` **Performance Requirements:** - Ensure that the methods are efficient and handle large dictionaries gracefully. - Memory usage should be optimized by avoiding unnecessary duplication of data. Implement the `CustomCollection` class with the specified functionality and constraints. Write the code in a way that adheres to best practices for type checking and memory safety, as described in the provided documentation.","solution":"class CustomCollection: def __init__(self, initial_data: dict): for value in initial_data.values(): if not isinstance(value, (int, float)): raise TypeError(\\"All values must be either int or float.\\") self.data = initial_data def add_element(self, key: str, value: [int, float]): if not isinstance(value, (int, float)): raise TypeError(\\"Value must be either int or float.\\") self.data[key] = value def remove_element(self, key: str): if key in self.data: del self.data[key] def find_maximum(self) -> [int, float]: return max(self.data.values()) def get_unicode_representation(self) -> str: unicode_str = \'\' for key, value in self.data.items(): unicode_str += key + str(value) return unicode_str def convert_to_list(self) -> list: return list(self.data.values())"},{"question":"Objective: Demonstrate your understanding of attention biases in sequence modeling using PyTorch by implementing a custom attention mechanism that uses causal biases. Problem Statement: You are required to implement a function `apply_causal_attention_bias` that applies a causal bias mechanism to a given sequence of data using provided methods (`causal_lower_right` and `causal_upper_left`). The function should modify the attention weights accordingly to ensure the model adheres to the causal constraint (i.e., each element in the sequence can only attend to the past and present elements, not future elements). Function Signature: ```python import torch def apply_causal_attention_bias(attention_weights: torch.Tensor, bias_type: str) -> torch.Tensor: Applies a specified causal bias to attention weights. Args: - attention_weights (torch.Tensor): A tensor of shape (batch_size, num_heads, seq_length, seq_length) representing attention weights. - bias_type (str): The type of causal bias to apply. It can be \'lower_right\' or \'upper_left\'. Returns: - torch.Tensor: The modified attention weights tensor with the causal bias applied. # Implement the function here ``` Input: - `attention_weights` (4D Tensor): A tensor of shape `(batch_size, num_heads, seq_length, seq_length)` representing the attention weights for a batch of sequences. - `bias_type` (String): Type of causal bias to apply. It can be \'lower_right\' or \'upper_left\'. Output: - Returns a 4D tensor with the causal bias applied, maintaining the original shape `(batch_size, num_heads, seq_length, seq_length)`. Constraints: - `attention_weights` should be a valid 4D tensor. - `bias_type` should be one of \'lower_right\' or \'upper_left\'. - Implement the causal bias adjustment using two methods `causal_lower_right` and `causal_upper_left`. Performance Requirements: - The function should efficiently handle tensors up to the size of `(64, 8, 512, 512)`. Example: ```python import torch # Example attention weights tensor attention_weights = torch.rand((4, 2, 10, 10)) bias_type = \'lower_right\' # Applying causal attention bias modified_attention_weights = apply_causal_attention_bias(attention_weights, bias_type) print(modified_attention_weights.shape) # Should print: torch.Size([4, 2, 10, 10]) ``` Notes: - You may assume that relevant methods (`causal_lower_right` and `causal_upper_left`) are already implemented and available in the `torch.nn.attention.bias` namespace. - Make sure to handle different scenarios and edge cases properly.","solution":"import torch def apply_causal_attention_bias(attention_weights: torch.Tensor, bias_type: str) -> torch.Tensor: Applies a specified causal bias to attention weights. Args: - attention_weights (torch.Tensor): A tensor of shape (batch_size, num_heads, seq_length, seq_length) representing attention weights. - bias_type (str): The type of causal bias to apply. It can be \'lower_right\' or \'upper_left\'. Returns: - torch.Tensor: The modified attention weights tensor with the causal bias applied. seq_len = attention_weights.size(-1) causal_mask = torch.tril(torch.ones((seq_len, seq_len))).to(attention_weights.device) if bias_type == \'lower_right\': attention_weights = attention_weights * causal_mask elif bias_type == \'upper_left\': attention_weights = attention_weights * causal_mask.T else: raise ValueError(\\"bias_type must be either \'lower_right\' or \'upper_left\'\\") return attention_weights"},{"question":"# Question: Asynchronous I/O Event Handling in Python with Selectors You are tasked with implementing an I/O event handling system using Python’s `selectors` module. This system should be able to manage multiple file descriptors, processing them as soon as they are ready for I/O operations, using an edge-triggered mechanism for efficiency. **Objectives:** 1. Use the `selectors` module to handle multiple I/O events. 2. Implement efficient handling of file descriptors using edge-triggered event handling. 3. Ensure proper registration, modification, and unregistration of file descriptors. **Requirements:** 1. Define a class `IOEventHandler` with the following methods: - `__init__(self)`: Initializes the selector. - `register(self, fileobj, events, data=None)`: Registers a file object (supports both file descriptors and file-like objects) for specified events (e.g., read, write). Optional data can be associated with the file object. - `modify(self, fileobj, events, data=None)`: Modifies the events and/or associated data for the registered file object. - `unregister(self, fileobj)`: Unregisters the file object. - `run(self, timeout=None)`: Starts the event loop that waits for and handles I/O events. 2. Handle at least the following events: - `selectors.EVENT_READ`: Ready for reading. - `selectors.EVENT_WRITE`: Ready for writing. 3. In the `run` method, use a loop to repeatedly call `selector.select(timeout)` and process the events. When an event is detected, invoke the corresponding callback function (if any) passed as the `data` parameter during registration or modification. **Input/Output Specification:** - `fileobj`: Can be an integer file descriptor or any object with a `fileno()` method. - `events`: A bitmask specifying the events to check for. Use the `selectors.EVENT_READ` and `selectors.EVENT_WRITE` constants. - `data`: Optional data to associate with the file object, which can be a callable to handle the event. **Performance Considerations:** - Use edge-triggered mechanisms (e.g., `EPOLLET` for `epoll`) to avoid repeated notifications. - Ensure the implementation is efficient for a large number of file descriptors. **Example Usage:** ```python import selectors import socket class IOEventHandler: def __init__(self): self.selector = selectors.DefaultSelector() def register(self, fileobj, events, data=None): self.selector.register(fileobj, events, data) def modify(self, fileobj, events, data=None): self.selector.modify(fileobj, events, data) def unregister(self, fileobj): self.selector.unregister(fileobj) def run(self, timeout=None): try: while True: events = self.selector.select(timeout) for key, mask in events: callback = key.data if callback: callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Event loop stopped.\\") # Example usage: Managing a simple server socket. def accept(sock, mask): conn, addr = sock.accept() print(\'Accepted connection from\', addr) # Further implementation for handling the connection sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.bind((\'localhost\', 12345)) sock.listen() sock.setblocking(False) event_handler = IOEventHandler() event_handler.register(sock, selectors.EVENT_READ, accept) event_handler.run() ```","solution":"import selectors import socket class IOEventHandler: def __init__(self): self.selector = selectors.DefaultSelector() def register(self, fileobj, events, data=None): self.selector.register(fileobj, events, data) def modify(self, fileobj, events, data=None): self.selector.modify(fileobj, events, data) def unregister(self, fileobj): self.selector.unregister(fileobj) def run(self, timeout=None): try: while True: events = self.selector.select(timeout) for key, mask in events: callback = key.data if callback: callback(key.fileobj, mask) except KeyboardInterrupt: print(\\"Event loop stopped.\\")"},{"question":"# PyTorch DLPack Interoperability Challenge In this task, you will write a function that demonstrates the interoperability of PyTorch tensors with another deep learning framework using DLPack. Specifically, you will: 1. Create a PyTorch tensor of specified shape and filled with random values. 2. Convert this PyTorch tensor to a DLPack tensor. 3. Mock the tensor manipulation in an imaginary deep learning framework. 4. Convert the modified DLPack tensor back to a PyTorch tensor. 5. Perform a simple operation on the resulting PyTorch tensor and return the result. # Function Signature ```python def pytorch_dlpack_interoperability(shape: tuple) -> torch.Tensor: pass ``` # Input - `shape` (tuple): A tuple specifying the shape of the PyTorch tensor to be created. Example: (2, 3). # Output - Returns a PyTorch tensor which has undergone the following transformations: 1. Creation in PyTorch. 2. Conversion to DLPack. 3. Mock manipulation in a different framework. 4. Conversion back to PyTorch. 5. A simple element-wise operation (e.g., addition of a constant). # Constraints - Use `torch.utils.dlpack.from_dlpack` and `torch.utils.dlpack.to_dlpack` for DLPack conversion. - Mock manipulation in the \\"other framework\\" can be a simple addition/subtraction of a constant value directly on the DLPack tensor. # Example ```python shape = (2, 3) result_tensor = pytorch_dlpack_interoperability(shape) print(result_tensor) # Example Output: # tensor([[1.5000, 2.5000, 3.5000], # [4.5000, 5.5000, 6.5000]]) # Assuming the manipulation adds 0.5 to each element. ``` # Instructions 1. Create a PyTorch tensor of the given shape filled with random values. 2. Convert the tensor to DLPack using `torch.utils.dlpack.to_dlpack`. 3. Simulate a manipulation in a different framework by adding a value (e.g., 0.5) to each element in the DLPack tensor. 4. Convert the modified DLPack tensor back to a PyTorch tensor using `torch.utils.dlpack.from_dlpack`. 5. Apply an additional simple operation (e.g., addition of 1.0) to the now PyTorch tensor. 6. Return the final tensor.","solution":"import torch import torch.utils.dlpack def pytorch_dlpack_interoperability(shape: tuple) -> torch.Tensor: # Step 1: Create a PyTorch tensor of specified shape filled with random values tensor = torch.randn(shape) # Step 2: Convert this PyTorch tensor to a DLPack tensor dlpack_tensor = torch.utils.dlpack.to_dlpack(tensor) # Step 3: Mock the tensor manipulation in an imaginary deep learning framework # Note: For simplicity, we\'ll directly manipulate the tensor here. # In reality, this would be done within the other framework. def mock_framework_manipulation(dlpack): # Convert DLPack back to a PyTorch tensor tensor = torch.utils.dlpack.from_dlpack(dlpack) # Simulate manipulation (e.g., add 0.5 to each element) manipulated_tensor = tensor + 0.5 # Convert back to DLPack to simulate it being passed back to PyTorch return torch.utils.dlpack.to_dlpack(manipulated_tensor) # Perform the mock manipulation modified_dlpack_tensor = mock_framework_manipulation(dlpack_tensor) # Step 4: Convert the modified DLPack tensor back to a PyTorch tensor modified_tensor = torch.utils.dlpack.from_dlpack(modified_dlpack_tensor) # Step 5: Perform a simple operation on the resulting PyTorch tensor result_tensor = modified_tensor + 1.0 # Return the resulting PyTorch tensor return result_tensor"},{"question":"# Question: Implementing Advanced Garbage Collection Handling in Python Objective: You are required to implement a custom Python class that simulates some aspects of garbage collection for container objects. This task will help you understand the concepts of cyclic garbage collection and object lifecycle management in Python using the provided guidelines. Problem Statement: Implement a class `CustomContainer` that simulates certain aspects of garbage collection for container types, as described in the provided documentation. Requirements: 1. Implement a constructor to initialize the container object, which includes: - An internal list to hold references to other objects. - A method to add items to the container. 2. Implement methods that would simulate the garbage collection tracking behaviors: - `track()`: Start tracking the object for garbage collection. - `untrack()`: Stop tracking the object for garbage collection. - `is_tracked()`: Check if the object is currently being tracked for garbage collection. 3. Implement methods to simulate object traversal and clearing: - `traverse(visitor)`: Simulate the traversal of contained objects by applying a given `visitor` function to each contained object. - `clear()`: Clear all references to contained objects. Constraints: - The internal list can store any type of objects. - Assume that objects passed to the `visitor` function in the `traverse()` method are valid. Input and Output: - `CustomContainer.add(item)`: Adds `item` to the container. - `CustomContainer.track()`: Tracks the container for garbage collection. - `CustomContainer.untrack()`: Stops tracking the container. - `CustomContainer.is_tracked() -> bool`: Returns `True` if the container is being tracked, otherwise `False`. - `CustomContainer.traverse(visitor)`: Applies `visitor` function to each contained object. - `CustomContainer.clear()`: Clears all references to contained objects. Example: ```python def example_visitor(item): print(item) container = CustomContainer() container.add(1) container.add(2) container.add(3) container.track() print(container.is_tracked()) # Output: True container.traverse(example_visitor) # Output: # 1 # 2 # 3 container.clear() print(container.is_tracked()) # Output: False ``` ```python # Your implementation here class CustomContainer: def __init__(self): self._items = [] self._tracked = False def add(self, item): self._items.append(item) def track(self): self._tracked = True def untrack(self): self._tracked = False def is_tracked(self) -> bool: return self._tracked def traverse(self, visitor): for item in self._items: visitor(item) def clear(self): self._items.clear() self.untrack() ``` # Notes: - Ensure the methods function as described, closely mimicking the behavior of garbage collection tracking in Python. - Your implementation should manage and simulate the tracking state accurately. - Test your implementation thoroughly using different scenarios and object types to ensure its robustness.","solution":"class CustomContainer: def __init__(self): self._items = [] self._tracked = False def add(self, item): self._items.append(item) def track(self): self._tracked = True def untrack(self): self._tracked = False def is_tracked(self) -> bool: return self._tracked def traverse(self, visitor): for item in self._items: visitor(item) def clear(self): self._items.clear() self.untrack()"},{"question":"**Background:** You are developing a simple web scraper that retrieves and processes data from multiple web pages. Your task is to implement a function that fetches the content of a specified web page, parses specified URLs from the page content, and returns these URLs. To achieve this, you are required to use the `urllib` module which provides functionality for opening URLs and handling HTTP responses. **Objective:** Implement a function `fetch_and_parse_urls` that fetches the HTML content of a specified URL and extracts all URLs that are referenced in `<a href=\\"...\\">` tags within the HTML content. # Function Signature ```python def fetch_and_parse_urls(url: str) -> list: Fetches the content of the specified URL and returns a list of all URLs found in <a href=\\"...\\"> tags. Args: - url (str): The URL of the web page to fetch. Returns: - list: A list of URLs found in <a href=\\"...\\"> tags. ``` # Input - `url` (str): A valid URL leading to a web page that contains HTML content. # Output - A list of strings representing the URLs found in the `<a href=\\"...\\">` tags within the fetched HTML content. # Constraints - You are only allowed to use the `urllib` module for fetching the URL content. - You may assume that the URL provided is always reachable and will respond with HTML content. - You should handle both relative and absolute URLs found in `<a href=\\"...\\">` tags. - The web page\'s HTML content is well-formed. - The function should return an empty list if no URLs are found. # Example ```python # Example usage urls = fetch_and_parse_urls(\\"https://example.com\\") print(urls) # Output might look like: # [\'https://example.com/about\', \'https://example.com/contact\', \'/help\'] ``` # Additional Notes - You should consider using `urllib.request` to open and read the URL\'s content. - Use appropriate functions and methods from `urllib.parse` to handle and normalize URLs. - Parsing the HTML content to find `<a href=\\"...\\">` tags can be done using regular expressions or an HTML parsing library, but usage of an external library for parsing is optional. Try to use built-in libraries if possible. **Performance Requirements:** The solution should be efficient enough to handle typical web page sizes. It is not expected to handle extremely large pages or massive volumes of data.","solution":"import urllib.request import re from urllib.parse import urljoin def fetch_and_parse_urls(url: str) -> list: Fetches the content of the specified URL and returns a list of all URLs found in <a href=\\"...\\"> tags. Args: - url (str): The URL of the web page to fetch. Returns: - list: A list of URLs found in <a href=\\"...\\"> tags. # Open the URL and fetch the HTML content response = urllib.request.urlopen(url) html_content = response.read().decode(\'utf-8\') # Use a regular expression to find all <a href=\\"...\\"> tags href_pattern = re.compile(r\'<as+(?:[^>]*?s+)?href=\\"([^\\"]*)\\"\') hrefs = href_pattern.findall(html_content) # Convert relative URLs to absolute URLs absolute_urls = [urljoin(url, href) for href in hrefs] return absolute_urls"},{"question":"# File Management and Archiving Task Objective: Implement a function that recursively backs up a specified directory, including its file hierarchy and all metadata, into a compressed archive. The function should handle various edge cases such as symbolic links and large files, and provide informative error messages for any issues encountered during the operation. Function Definition: ```python import shutil import os def backup_directory(source: str, destination: str) -> str: Recursively back up the specified source directory to the destination archive file. Parameters: source (str): The path to the source directory to back up. destination (str): The path to the destination archive file (without extension). Returns: str: The path to the created archive file. Raises: ValueError: If the source directory does not exist. OSError: If any file operations fail. # Your implementation here ``` Input: - `source`: A string representing the path to the directory that needs to be backed up. - `destination`: A string representing the path (excluding the extension) where the backup archive should be created. Output: - A string representing the full path to the created archive file (with appropriate extension). Constraints: - You must preserve all metadata, including permissions, last access times, and extended attributes where possible. - The function should create the archive in `gztar` format. - Handle symbolic links appropriately: if the source is a symbolic link, it should be preserved as a link. - Ensure that the specified directory exists before attempting to back it up. - Provide informative error messages for any issues encountered. Examples: ```python # Example usage source = \\"/path/to/source_directory\\" destination = \\"/path/to/backup/destination\\" archive_path = backup_directory(source, destination) print(f\\"Backup completed: {archive_path}\\") ``` You can assume Python 3.10 environment for this task.","solution":"import os import shutil def backup_directory(source: str, destination: str) -> str: Recursively back up the specified source directory to the destination archive file. Parameters: source (str): The path to the source directory to back up. destination (str): The path to the destination archive file (without extension). Returns: str: The path to the created archive file. Raises: ValueError: If the source directory does not exist. OSError: If any file operations fail. # Check if the source directory exists if not os.path.isdir(source): raise ValueError(\\"The source directory does not exist.\\") try: # Create the archive file in \'gztar\' format archive_path = shutil.make_archive(destination, \'gztar\', source) return archive_path except OSError as e: raise OSError(f\\"Failed to create archive: {e}\\")"},{"question":"Question: Frame Introspection Utility in Python310 # Background Understanding the execution context of your code can be crucial when debugging or optimizing performance. Python310 provides several functions to introspect various aspects of the Python execution frame. You are required to implement a utility function that makes use of these capabilities to gather comprehensive information about the current execution stack. # Task Write a function `frame_introspection()` that gathers and returns detailed information about the current execution frame and its environment. # Requirements 1. **Function Declaration:** ```python def frame_introspection() -> dict: ``` 2. **Expected Output:** The function should return a dictionary with the following structure: ```python { \\"builtins\\": {...}, # Dictionary of builtins in the current execution frame \\"locals\\": {...}, # Dictionary of local variables in the current execution frame \\"globals\\": {...}, # Dictionary of global variables in the current execution frame \\"current_frame\\": { \\"code\\": \\"string representation of code object\\", \\"line_number\\": current_line_number, \\"back_frame\\": { \\"code\\": \\"string representation of back frame code object\\", \\"line_number\\": back_frame_line_number } } } ``` * If there is no back frame, `back_frame` should be `None`. # Constraints - Utilize the functions `PyEval_GetBuiltins`, `PyEval_GetLocals`, `PyEval_GetGlobals`, `PyEval_GetFrame`, `PyFrame_GetBack`, `PyFrame_GetCode`, and `PyFrame_GetLineNumber` as appropriate. - Ensure that no frame, either current or back, is `NULL` in your extraction process; handle such scenarios gracefully. - The function should run efficiently and avoid redundant calls to the introspection functions. # Example Here is an example of what your output might look like: ```python { \\"builtins\\": {\\"print\\": \\"<built-in function print>\\", ...}, \\"locals\\": {\\"a\\": 10, \\"b\\": 20}, \\"globals\\": {\\"__name__\\": \\"__main__\\", ...}, \\"current_frame\\": { \\"code\\": \\"<code object <module> at 0x10d2b0ac0, file \'example.py\', line 1>\\", \\"line_number\\": 42, \\"back_frame\\": { \\"code\\": \\"<code object <module> at 0x10d2b0bc0, file \'example.py\', line 2>\\", \\"line_number\\": 41 } } } ``` # Evaluation Criteria - Correctness: The function should correctly gather all required information and format it as specified. - Efficiency: The implementation should be efficient and avoid unnecessary computations. - Robustness: The function should handle edge cases, such as non-existent back frames, gracefully. - Code Quality: The code should be well-organized, clearly commented, and follow Python best practices. Good luck!","solution":"import sys def frame_introspection() -> dict: Gathers and returns detailed information about the current execution frame and its environment. frame = sys._getframe() builtins = frame.f_builtins locals_ = frame.f_locals globals_ = frame.f_globals current_frame_info = { \\"code\\": str(frame.f_code), \\"line_number\\": frame.f_lineno, } back_frame = frame.f_back if back_frame is not None: back_frame_info = { \\"code\\": str(back_frame.f_code), \\"line_number\\": back_frame.f_lineno } else: back_frame_info = None current_frame_info[\\"back_frame\\"] = back_frame_info return { \\"builtins\\": builtins, \\"locals\\": locals_, \\"globals\\": globals_, \\"current_frame\\": current_frame_info }"},{"question":"Implement an HTTP Client for Fetching and Analyzing Web Data Objective: Design and implement an HTTP client in Python using the `http.client` module. Your client should perform the following tasks: 1. **Connect to a Web Server:** - Connect to a specified web server using either HTTP or HTTPS. - You should allow the user to specify the URL and choose between the two protocols. 2. **Send an HTTP GET Request:** - Send an HTTP GET request to a specified resource on the server. - Ensure the request is successfully sent, and handle any exceptions appropriately. 3. **Parse the HTTP Response:** - Parse and print the status code and reason phrase returned by the server. - Parse and print all headers returned by the server. 4. **Extract and Save Content:** - Read the response body and save it to a local file. - Ensure you handle large responses by reading in chunks if necessary. 5. **Handle Errors:** - Gracefully handle common HTTP exceptions such as `InvalidURL`, `NotConnected`, `RemoteDisconnected`, and `IncompleteRead`. - Print meaningful error messages for each exception. 6. **Performance Requirement:** - Ensure the client can handle timeouts and retry the request up to two times if a timeout or connection-related error occurs. Input Specifications: 1. `protocol`: Either \\"http\\" or \\"https\\". 2. `host`: The hostname of the server (e.g., \\"www.example.com\\"). 3. `port`: The port number to connect to (default for HTTP is 80 and for HTTPS is 443). 4. `resource`: The resource path on the server to request (e.g., \\"/index.html\\"). Output Specifications: - Print the status code and reason phrase. - Print all header fields. - Save the response body to a file named `response_body.txt`. Constraints: - You must use the `http.client` module for handling the HTTP connections. - Handle exceptions and ensure the program does not crash unexpectedly. Example: ```python # Sample inputs protocol = \\"https\\" host = \\"www.python.org\\" port = 443 resource = \\"/\\" # Expected output: # Status: 200 OK # Headers: # { \'Content-Type\': \'text/html; charset=utf-8\', ... } # Content saved to response_body.txt ``` Implementation: ```python import http.client import ssl def fetch_web_data(protocol, host, port, resource): try: if protocol == \'https\': context = ssl.create_default_context() conn = http.client.HTTPSConnection(host, port, context=context) else: conn = http.client.HTTPConnection(host, port) conn.request(\\"GET\\", resource) response = conn.getresponse() # Print status and headers print(f\\"Status: {response.status} {response.reason}\\") headers = response.getheaders() print(\\"Headers:\\", headers) # Save response body to file with open(\\"response_body.txt\\", \\"wb\\") as file: while chunk := response.read(200): file.write(chunk) conn.close() print(\\"Content saved to response_body.txt\\") except http.client.HTTPException as e: print(f\\"HTTP error occurred: {e}\\") except ssl.SSLError as e: print(f\\"SSL error occurred: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Sample usage fetch_web_data(\\"https\\", \\"www.python.org\\", 443, \\"/\\") ``` In this question, students are expected to demonstrate their ability to create and manage HTTP connections, handle responses, and manage errors using the `http.client` module in Python.","solution":"import http.client import ssl import socket def fetch_web_data(protocol, host, port, resource, retries=2): try: for attempt in range(retries + 1): try: if protocol == \'https\': context = ssl.create_default_context() conn = http.client.HTTPSConnection(host, port, context=context, timeout=10) else: conn = http.client.HTTPConnection(host, port, timeout=10) conn.request(\\"GET\\", resource) response = conn.getresponse() # Print status and headers print(f\\"Status: {response.status} {response.reason}\\") headers = response.getheaders() print(\\"Headers:\\", headers) # Save response body to file with open(\\"response_body.txt\\", \\"wb\\") as file: while chunk := response.read(200): file.write(chunk) conn.close() print(\\"Content saved to response_body.txt\\") return except (http.client.HTTPException, ssl.SSLError, socket.timeout) as e: print(f\\"Attempt {attempt+1} failed: {e}\\") if attempt == retries: raise finally: conn.close() except http.client.InvalidURL: print(\\"The URL provided is invalid.\\") except http.client.NotConnected: print(\\"Not connected to the server.\\") except http.client.RemoteDisconnected: print(\\"The remote end has closed the connection.\\") except http.client.IncompleteRead as e: print(f\\"Partial data received: {e.partial}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Sample usage fetch_web_data(\\"https\\", \\"www.python.org\\", 443, \\"/\\")"},{"question":"<|Analysis Begin|> Based on the provided documentation, the `torch.Size` class in PyTorch is a subclass of `tuple` that describes the size of each dimension of a tensor. It supports common sequence operations such as indexing and calculating lengths. From this, we can infer that a `torch.Size` object can be used in a similar way to a tuple, which allows us to access individual dimensions and the total number of dimensions in a tensor. Given this information, we need to design a coding question that tests the students\' ability to work with tensor sizes in PyTorch. The question should require the student to implement a function that manipulates tensor dimensions using `torch.Size`. <|Analysis End|> <|Question Begin|> # PyTorch Coding Assessment Question Task Write a function `analyze_tensor_dimensions` that takes a PyTorch tensor as input and returns a dictionary containing the following information about the tensor: 1. The size of each dimension (as a list). 2. The total number of dimensions (rank of the tensor). 3. The size of the dimension with the largest number of elements. 4. Whether the tensor is a scalar (0-dimensional tensor). Input - A PyTorch tensor `tensor`. Output - A dictionary with the following keys: - `\\"dimension_sizes\\"`: A list of integers representing the size of each dimension. - `\\"num_dimensions\\"`: An integer representing the total number of dimensions (rank). - `\\"max_dimension_size\\"`: An integer representing the size of the dimension with the largest number of elements. - `\\"is_scalar\\"`: A boolean indicating whether the tensor is a scalar. Constraints - The input tensor can have any number of dimensions (including 0-dimensional). Performance Requirements - The solution should operate efficiently on large tensors. Example ```python import torch def analyze_tensor_dimensions(tensor): # Your implementation here pass tensor1 = torch.ones(10, 20, 30) print(analyze_tensor_dimensions(tensor1)) # Expected output: # { # \\"dimension_sizes\\": [10, 20, 30], # \\"num_dimensions\\": 3, # \\"max_dimension_size\\": 30, # \\"is_scalar\\": False # } tensor2 = torch.tensor(5) print(analyze_tensor_dimensions(tensor2)) # Expected output: # { # \\"dimension_sizes\\": [], # \\"num_dimensions\\": 0, # \\"max_dimension_size\\": 0, # \\"is_scalar\\": True # } ``` Note - For a scalar tensor, the list of dimension sizes should be empty (`[]`), the number of dimensions should be `0`, the size of the largest dimension should be `0`, and the scalar indicator should be `True`. - For a non-scalar tensor, the list of dimension sizes should contain the sizes of each dimension, the number of dimensions should match the length of this list, the size of the largest dimension should be the maximum value in this list, and the scalar indicator should be `False`. Good luck!","solution":"import torch def analyze_tensor_dimensions(tensor): Analyzes the dimensions of the given PyTorch tensor. Parameters: tensor (torch.Tensor): The input tensor to analyze. Returns: dict: A dictionary with the dimension sizes, number of dimensions, max dimension size, and whether it is a scalar. size = tensor.size() dimension_sizes = list(size) num_dimensions = len(size) max_dimension_size = max(size) if size else 0 is_scalar = num_dimensions == 0 return { \\"dimension_sizes\\": dimension_sizes, \\"num_dimensions\\": num_dimensions, \\"max_dimension_size\\": max_dimension_size, \\"is_scalar\\": is_scalar }"},{"question":"Coding Assessment Question # Objective: Assess the ability to implement and evaluate multiple linear regression models using scikit-learn, particularly focusing on regularization techniques and cross-validation. # Problem Statement: You have been provided a dataset containing features `X` (a 2D numpy array) and target values `y` (a 1D numpy array). This dataset simulates a scenario where the target value is influenced by multiple features with potential multicollinearity. Your task is to: 1. Fit linear regression models using `LinearRegression`, `Ridge`, and `Lasso`. 2. Perform cross-validation to find the optimal hyperparameters for `Ridge` and `Lasso`. 3. Compare the performance of these models using mean squared error (MSE). # Input: 1. `X`: A 2D numpy array of shape (n_samples, n_features) representing the feature matrix. 2. `y`: A 1D numpy array of shape (n_samples,) representing the target values. # Implementation Requirements: 1. Use `sklearn.linear_model.LinearRegression`, `sklearn.linear_model.Ridge`, and `sklearn.linear_model.Lasso`. 2. Implement cross-validation for `Ridge` and `Lasso` using `RidgeCV` and `LassoCV` respectively, searching for optimal `alpha` values over the range [0.1, 1, 10]. 3. Calculate the mean squared error (MSE) for each model on the full dataset. # Expected Functions: 1. `fit_linear_models(X, y)` # Output: 1. A dictionary containing: - `\'linear\'`: MSE of the `LinearRegression` model. - `\'ridge\'`: MSE of the best `Ridge` model. - `\'lasso\'`: MSE of the best `Lasso` model. # Constraints: - You are required to use the models and methodology illustrated in the provided scikit-learn documentation. - Ensure that the cross-validation process uses appropriate hyperparameter settings. - The dataset provided will not have missing values. # Example: ```python import numpy as np X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]) y = np.array([0, 1, 2, 3, 4]) results = fit_linear_models(X, y) print(results) ``` Output (example, actual values may differ): ```python { \'linear\': 0.0, \'ridge\': 0.0, \'lasso\': 0.0 } ``` # Evaluation Criteria: - Correct use of scikit-learn models. - Proper implementation of cross-validation to optimize hyperparameters for `Ridge` and `Lasso`. - Accurate calculation of mean squared errors for model performance comparison.","solution":"import numpy as np from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV from sklearn.metrics import mean_squared_error def fit_linear_models(X, y): # Fit Linear Regression model lin_reg = LinearRegression().fit(X, y) y_pred_linear = lin_reg.predict(X) mse_linear = mean_squared_error(y, y_pred_linear) # Fit Ridge Regression model with cross-validation ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True).fit(X, y) y_pred_ridge = ridge_cv.predict(X) mse_ridge = mean_squared_error(y, y_pred_ridge) # Fit Lasso Regression model with cross-validation lasso_cv = LassoCV(alphas=[0.1, 1.0, 10.0], cv=5).fit(X, y) y_pred_lasso = lasso_cv.predict(X) mse_lasso = mean_squared_error(y, y_pred_lasso) return { \'linear\': mse_linear, \'ridge\': mse_ridge, \'lasso\': mse_lasso }"},{"question":"**Question: Dynamic Shape Propagation with Symbolic Shapes** *Objective:* Implement a function using PyTorch\'s experimental `symbolic_shapes` module that takes two input tensors of potentially dynamic shapes and returns their element-wise maximum. Your function should ensure that symbolic shapes are correctly handled and propagated. **Function Signature:** ```python def elementwise_max_with_symbolic_shapes(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: pass ``` # Requirements: 1. **Input and Output Formats:** - Inputs: `tensor_a` and `tensor_b` both are PyTorch Tensors, potentially with dynamic shapes. - Output: A PyTorch Tensor representing the element-wise maximum of `tensor_a` and `tensor_b`. 2. **Constraints:** - Your function should handle the tensors as dynamically shaped (i.e., their shapes may change) and ensure that the symbolic shapes are managed correctly. - If the shapes of the two input tensors are incompatible for element-wise operations, the function should raise a `ValueError`. 3. **Performance Requirements:** - The function should efficiently handle tensors with large sizes and dynamic shapes using the symbolic shapes framework. # Example: ```python import torch from torch.fx.experimental.symbolic_shapes import ShapeEnv def elementwise_max_with_symbolic_shapes(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: # Your implementation goes here pass # Example usage: tensor_a = torch.rand(2, 3) tensor_b = torch.rand(2, 3) result = elementwise_max_with_symbolic_shapes(tensor_a, tensor_b) print(result.shape) # Expected output would be a tensor of shape (2, 3) ``` # Hints: - You may find the `ShapeEnv` class useful for managing and propagating symbolic shapes. - Ensure you handle cases where input tensors may have incompatible shapes and prompt appropriate errors. **Evaluation Criteria:** - Correctness of symbolic shape handling. - Efficient handling of large tensors with dynamic shapes. - Proper error handling for shape incompatibility.","solution":"import torch from torch.fx.experimental.symbolic_shapes import ShapeEnv def elementwise_max_with_symbolic_shapes(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Returns the element-wise maximum of two tensors with potentially dynamic shapes. Ensures that symbolic shapes are correctly handled and propagated. Args: tensor_a (torch.Tensor): First input tensor. tensor_b (torch.Tensor): Second input tensor. Returns: torch.Tensor: Element-wise maximum of tensor_a and tensor_b. Raises: ValueError: If the shapes of tensor_a and tensor_b are incompatible for element-wise operations. if tensor_a.shape != tensor_b.shape: raise ValueError(\\"Input tensors must have the same shape for element-wise operations.\\") return torch.maximum(tensor_a, tensor_b)"},{"question":"**Objective:** Demonstrate your understanding of tuning decision thresholds in classifiers using `scikit-learn`. Problem Statement You are a data scientist working on a binary classification problem to predict whether a patient has a specific type of cancer. The dataset you have is highly imbalanced, with 90% of the patients not having cancer and only 10% having cancer. Your objective is to tune the decision threshold of a classifier to maximize the recall score (sensitivity), as missing a cancer diagnosis is highly undesirable. You are provided with a dataset containing features for each patient and a binary target variable where `1` indicates the presence of cancer and `0` indicates the absence of cancer. 1. Load the dataset and split it into training and testing sets. 2. Train a logistic regression classifier on the training data. 3. Tune the decision threshold of the trained classifier using `TunedThresholdClassifierCV` to maximize the recall score. 4. Evaluate the classifier\'s performance on the test set by reporting both the recall and precision scores. Expected Input and Output - **Input:** - The dataset in CSV format where the last column is the target variable. - A train-test split ratio (e.g., 0.8 for training and 0.2 for testing). - **Output:** - Recall score of the tuned classifier on the test set. - Precision score of the tuned classifier on the test set. - The optimal decision threshold found during tuning. Function Signature ```python import numpy as np import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.metrics import make_scorer, recall_score, precision_score from sklearn.model_selection import train_test_split, TunedThresholdClassifierCV def tune_classifier_decision_threshold(dataset_path: str, test_size: float = 0.2) -> tuple: Train a logistic regression classifier and tune its decision threshold to maximize recall. Args: dataset_path (str): Path to the dataset CSV file. test_size (float): Proportion of the dataset to include in the test split. Returns: tuple: Recall score, precision score, and the optimal decision threshold. # Load and split the dataset data = pd.read_csv(dataset_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) # Train the logistic regression classifier base_model = LogisticRegression(random_state=42) base_model.fit(X_train, y_train) # Define the scorer to maximize recall scorer = make_scorer(recall_score, pos_label=1) # Tune the decision threshold using cross-validation tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Get the optimal threshold optimal_threshold = tuned_model.best_threshold_ # Evaluate the performance on the test set y_pred_proba = tuned_model.predict_proba(X_test)[:, 1] y_pred = (y_pred_proba >= optimal_threshold).astype(int) recall = recall_score(y_test, y_pred) precision = precision_score(y_test, y_pred) return recall, precision, optimal_threshold ``` **Constraints and Limitations:** - The dataset should be in CSV format with no missing values. - The target variable should be binary (0 or 1). - Use `scikit-learn` version 0.24 or later. - Use a random state of 42 for reproducibility. **Performance Requirements:** - The function should run in a reasonable amount of time for datasets with up to 10,000 samples and 50 features. **Example:** ```python # Assume the dataset is stored in \'cancer_data.csv\' recall, precision, threshold = tune_classifier_decision_threshold(\'cancer_data.csv\') print(f\\"Recall: {recall}, Precision: {precision}, Optimal Threshold: {threshold}\\") ```","solution":"import numpy as np import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.metrics import make_scorer, recall_score, precision_score from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV class TunedThresholdClassifierCV: def __init__(self, base_estimator, scoring=make_scorer(recall_score, pos_label=1)): self.base_estimator = base_estimator self.scoring = scoring self.best_threshold_ = 0.5 def fit(self, X, y): self.base_estimator.fit(X, y) thresholds = np.linspace(0, 1, 100) best_score = -np.inf best_threshold = 0.5 for threshold in thresholds: y_proba = self.base_estimator.predict_proba(X)[:, 1] y_pred = (y_proba >= threshold).astype(int) score = self.scoring(self.base_estimator, X, y) if score > best_score: best_score = score best_threshold = threshold self.best_threshold_ = best_threshold def predict_proba(self, X): return self.base_estimator.predict_proba(X) def tune_classifier_decision_threshold(dataset_path: str, test_size: float = 0.2) -> tuple: # Load and split the dataset data = pd.read_csv(dataset_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42) # Train the logistic regression classifier base_model = LogisticRegression(random_state=42) base_model.fit(X_train, y_train) # Define the scorer to maximize recall scorer = make_scorer(recall_score, pos_label=1) # Tune the decision threshold using cross-validation tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer) tuned_model.fit(X_train, y_train) # Get the optimal threshold optimal_threshold = tuned_model.best_threshold_ # Evaluate the performance on the test set y_pred_proba = tuned_model.predict_proba(X_test)[:, 1] y_pred = (y_pred_proba >= optimal_threshold).astype(int) recall = recall_score(y_test, y_pred) precision = precision_score(y_test, y_pred) return recall, precision, optimal_threshold"},{"question":"# Question: Compiling Python Files with Custom Rules Background You are provided with a directory containing several Python source files and sub-directories. Your task is to compile these files into byte-code using custom rules. The \\"compileall\\" module will help you accomplish this. Task Write a Python function `custom_compile(dir_path, max_depth, exclude_pattern, optimize, force, quiet)`. This function should recursively compile all Python source files in the provided directory: - **dir_path**: The root directory where the compilation should start. - **max_depth**: The maximum depth of recursion for compiling files. If set to 0, it compiles files in the given directory without diving into subdirectories. - **exclude_pattern**: A regex pattern to exclude files/directories from compilation. Files matching this pattern should not be compiled. - **optimize**: The optimization level (integer) for the compiler. Accepts only levels 0, 1 or 2. - **force**: Boolean indicating if the compilation should be forced even if timestamps are up to date. - **quiet**: Integer value to control the verbosity of the output: 0 for full output, 1 to print only errors, and 2 to suppress all output. Constraints 1. The function should handle exceptions gracefully and print an appropriate message if the compilation process encounters an error. 2. Only compile files if the provided optimization level is within the valid range (0, 1, 2). 3. Use the `compileall.compile_dir` method to accomplish the task. 4. Use the \\"re\\" module to handle regex patterns for excluding files. Example Usage ```python import re import compileall def custom_compile(dir_path, max_depth, exclude_pattern, optimize, force, quiet): # Your implementation here # Example Directory Structure # /example_dir # ├── subdir1 # │ ├── file1.py # │ └── file2.py # ├── subdir2 # │ └── skip_this.py # └── main.py custom_compile(\'/example_dir\', 1, r\'skip_this.py\', 1, True, 1) ``` In the above example, `skip_this.py` will be excluded from compilation, and `file1.py`, `file2.py`, and `main.py` will be compiled with optimization level 1, forced even if timestamps are up-to-date, and with only errors printed. ```python import os import re import compileall def custom_compile(dir_path, max_depth, exclude_pattern, optimize, force, quiet): if optimize not in [0, 1, 2]: raise ValueError(\\"Invalid optimization level! Must be 0, 1, or 2.\\") try: rx = re.compile(exclude_pattern) result = compileall.compile_dir( dir=str(dir_path), maxlevels=max_depth, force=force, rx=rx, quiet=quiet, optimize=optimize ) if result: print(\\"Compilation completed successfully.\\") else: print(\\"Some files failed to compile.\\") except Exception as e: print(f\\"An error occurred during compilation: {e}\\") # Example usage # custom_compile(\'/example_dir\', 1, r\'skip_this.py\', 1, True, 1) ``` The function `custom_compile` should help compile files according to the specified parameters, demonstrating a good understanding of the compileall module and its practical application. Test Cases 1. Test with valid inputs and verify that files are compiled correctly. 2. Test with invalid optimization levels to ensure the function raises appropriate errors. 3. Test with different levels of verbosity to ensure correct output behavior. 4. Test with different exclude patterns to ensure files are excluded as expected. 5. Test with forced compilation to ensure files are compiled regardless of their timestamps.","solution":"import os import re import compileall def custom_compile(dir_path, max_depth, exclude_pattern, optimize, force, quiet): Compiles Python files in the directory with custom rules. Parameters: dir_path (str): The root directory where the compilation should start. max_depth (int): The maximum depth of recursion for compiling files. exclude_pattern (str): A regex pattern to exclude files/directories from compilation. optimize (int): The optimization level (0, 1, 2) for the compiler. force (bool): If to force the compilation. quiet (int): The verbosity level of the output (0, 1, 2). if optimize not in [0, 1, 2]: raise ValueError(\\"Invalid optimization level! Must be 0, 1, or 2.\\") try: rx = re.compile(exclude_pattern) result = compileall.compile_dir( dir=str(dir_path), maxlevels=max_depth, force=force, rx=rx, quiet=quiet, optimize=optimize ) if result: print(\\"Compilation completed successfully.\\") else: print(\\"Some files failed to compile.\\") except Exception as e: print(f\\"An error occurred during compilation: {e}\\")"},{"question":"# Question: Seaborn Context Customization You are provided with time series data representing monthly sales figures for two products over a year. Your task is to create a comparative line plot to visualize this data using the seaborn library. Additionally, you must customize the visual aesthetics of the plot using the `seaborn.set_context` function to meet specified requirements. Input: 1. Two lists `product_a_sales` and `product_b_sales` containing 12 integer values each, representing monthly sales figures for two different products. 2. The plot context name as a string, which can be one of [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"]. 3. An optional dictionary `custom_rc` containing any specific parameters to override within the context settings (Default is an empty dictionary). Output: 1. A line plot comparing the monthly sales of the two products. 2. The plot should be customized according to the specified context and any provided rc parameters. Constraints: - The length of `product_a_sales` and `product_b_sales` will always be 12. - All sales figures will be non-negative integers. Example: ```python product_a_sales = [120, 130, 115, 140, 155, 170, 160, 180, 190, 200, 210, 230] product_b_sales = [80, 90, 85, 75, 95, 100, 110, 120, 130, 140, 150, 160] context = \\"notebook\\" custom_rc = {\\"lines.linewidth\\": 2, \\"font.size\\": 12} create_customized_sales_plot(product_a_sales, product_b_sales, context, custom_rc) ``` Your implementation should produce a plot like this: - A line plot comparing `product_a_sales` and `product_b_sales` with months on the X-axis and sales figures on the Y-axis. - The plot context should be set to \\"notebook\\". - The line width should be 2, and the font size should be adjusted according to the provided `custom_rc`. Function Signature: ```python def create_customized_sales_plot(product_a_sales, product_b_sales, context, custom_rc={}): # Your code here ``` **Notes:** - Use appropriate Seaborn and Matplotlib functions to achieve the desired plot. - Ensure that the plot visually differentiates between the two products (e.g., different colors or line styles).","solution":"import matplotlib.pyplot as plt import seaborn as sns def create_customized_sales_plot(product_a_sales, product_b_sales, context, custom_rc={}): Create a comparative line plot for monthly sales figures of two products with customized context. Parameters: - product_a_sales: List of integers representing monthly sales figures for product A. - product_b_sales: List of integers representing monthly sales figures for product B. - context: String specifying the seaborn context for the plot. - custom_rc: Dictionary specifying any custom rc parameters to override. # Set the context sns.set_context(context, rc=custom_rc) # Create the plot plt.figure(figsize=(10, 6)) # x-axis values (months) months = range(1, 13) # Plot the sales data plt.plot(months, product_a_sales, label=\\"Product A\\", marker=\'o\', linestyle=\'-\') plt.plot(months, product_b_sales, label=\\"Product B\\", marker=\'s\', linestyle=\'--\') # Adding titles and labels plt.title(\\"Monthly Sales Comparison\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Sales\\") # Show legend plt.legend() # Show plot plt.show()"},{"question":"Comprehensive File Operations with `shutil` Objective: Write a Python script using the `shutil` module to perform a series of file operations including copying, moving, archiving, and deleting files/directories. This task will demonstrate your understanding of fundamental and advanced concepts of the `shutil` package. Requirements: 1. **Copying Files and Directories**: - Write a function `copy_items(source, destination)` to copy a file or directory from the `source` path to the `destination` path. - The function should handle both files and directories, preserving metadata for files. 2. **Moving Files and Directories**: - Write a function `move_items(source, destination)` to move a file or directory from the `source` path to the `destination` path. - The function should ensure that metadata is preserved after moving. 3. **Creating an Archive**: - Write a function `create_archive(directory_path, archive_name, format)` to create an archive from the directory specified by `directory_path`. - The archive should be saved with the name specified by `archive_name` and in the format specified by `format` (e.g., \'zip\', \'tar\', etc.). 4. **Unpacking an Archive**: - Write a function `extract_archive(archive_path, extract_dir)` to extract an archive file `archive_path` into the directory `extract_dir`. 5. **Deleting Directories**: - Write a function `remove_directory(directory_path)` to remove the directory specified by `directory_path` and all its contents. Constraints: - You should handle exceptions and edge cases, such as attempting to copy or move a non-existent file, or trying to delete a non-directory path. - Ensure that symbolic links and permissions are handled appropriately. Input and Output: - **copy_items(source, destination)**: - Input: `source` (str), `destination` (str) - Output: None (the function should copy the item from `source` to `destination`) - **move_items(source, destination)**: - Input: `source` (str), `destination` (str) - Output: None (the function should move the item from `source` to `destination`) - **create_archive(directory_path, archive_name, format)**: - Input: `directory_path` (str), `archive_name` (str), `format` (str) - Output: None (the function should create an archive from the directory and save it with the given name and format) - **extract_archive(archive_path, extract_dir)**: - Input: `archive_path` (str), `extract_dir` (str) - Output: None (the function should extract the archive to the specified directory) - **remove_directory(directory_path)**: - Input: `directory_path` (str) - Output: None (the function should remove the specified directory and its contents) Example Usage: ```python # Example usage of all the required functions copy_items(\'path/to/source/file\', \'path/to/destination/file\') move_items(\'path/to/source/file\', \'path/to/destination/file\') create_archive(\'path/to/source/directory\', \'archive_name\', \'zip\') extract_archive(\'path/to/archive.zip\', \'path/to/destination/directory\') remove_directory(\'path/to/source/directory\') ``` Implement these functions and ensure they handle the mentioned requirements and constraints effectively. Your solution should demonstrate thorough error handling and efficient use of the `shutil` module.","solution":"import shutil import os def copy_items(source, destination): Copy a file or directory from source to destination. Handles both files and directories, preserving metadata. if os.path.isdir(source): shutil.copytree(source, destination) elif os.path.isfile(source): shutil.copy2(source, destination) else: raise FileNotFoundError(f\\"The source path {source} does not exist.\\") def move_items(source, destination): Move a file or directory from source to destination. Ensures that metadata is preserved. if os.path.exists(source): shutil.move(source, destination) else: raise FileNotFoundError(f\\"The source path {source} does not exist.\\") def create_archive(directory_path, archive_name, format): Create an archive from the directory specified by directory_path. Save it with the name archive_name and in the format specified by format. if os.path.isdir(directory_path): shutil.make_archive(archive_name, format, directory_path) else: raise FileNotFoundError(f\\"The directory path {directory_path} does not exist.\\") def extract_archive(archive_path, extract_dir): Extract an archive file archive_path into the directory extract_dir. if os.path.isfile(archive_path): shutil.unpack_archive(archive_path, extract_dir) else: raise FileNotFoundError(f\\"The archive path {archive_path} does not exist.\\") def remove_directory(directory_path): Remove the directory specified by directory_path and all its contents. if os.path.isdir(directory_path): shutil.rmtree(directory_path) else: raise NotADirectoryError(f\\"The directory path {directory_path} is not a directory or does not exist.\\")"},{"question":"**Coding Assignment: Advanced Command-Line Interface with argparse** **Objective:** Create a Python script that simulates a multi-functional file processor using the `argparse` module. Follow the requirements to implement parsing command-line arguments and handle different functionalities based on sub-commands. **Requirements:** 1. **Script Name:** `file_processor.py` 2. **Functionalities:** - **copy:** Copy contents from one file to another. - **move:** Move contents from one file to another (delete source after copying). - **delete:** Delete a file. - **info:** Print information about the file (size, creation date, last modified date). 3. **Argument Specifications:** - **Global Optional Arguments:** - `--verbose` or `-v`: Print detailed logs during operations. - `--backup` or `-b`: Keep a backup of the source file during copy or move operations. - **copy sub-command Arguments:** - `source`: The source file path. - `destination`: The destination file path. - **move sub-command Arguments:** - `source`: The source file path. - `destination`: The destination file path. - **delete sub-command Arguments:** - `target`: The file to be deleted. - **info sub-command Arguments:** - `target`: The file to retrieve information from. **Input and Output:** - **Input:** Command-line inputs as specified above. - **Output:** Operation results, error messages, and logs based on verbosity option. **Constraints:** - Use only the standard library for file operations (i.e., no third-party libraries). - Handle errors gracefully and print informative messages. - Ensure file paths are valid and operations have appropriate read/write permissions. **Performance Requirements:** - The script should handle typical file I/O operations efficiently. - It should manage the backup process without significant delay. **Example Usage:** ```bash python file_processor.py copy source.txt destination.txt --verbose --backup python file_processor.py move source.txt destination.txt --backup python file_processor.py delete target.txt python file_processor.py info target.txt --verbose ``` **Implementation:** Write the Python script to meet these requirements. Ensure it performs the required operations correctly and handles all specified arguments and options appropriately using the `argparse` module. **Deliverable:** Submit the `file_processor.py` script that solves the problem as described.","solution":"import argparse import shutil import os from datetime import datetime def copy_file(source, destination, verbose=False, backup=False): if backup: shutil.copy2(source, f\\"{source}.bak\\") if verbose: print(f\\"Backup of {source} created as {source}.bak\\") shutil.copy2(source, destination) if verbose: print(f\\"Copied {source} to {destination}\\") def move_file(source, destination, verbose=False, backup=False): if backup: shutil.copy2(source, f\\"{source}.bak\\") if verbose: print(f\\"Backup of {source} created as {source}.bak\\") shutil.move(source, destination) if verbose: print(f\\"Moved {source} to {destination}\\") def delete_file(target, verbose=False): os.remove(target) if verbose: print(f\\"Deleted file {target}\\") def file_info(target, verbose=False): size = os.path.getsize(target) creation_date = datetime.fromtimestamp(os.path.getctime(target)).strftime(\'%Y-%m-%d %H:%M:%S\') modification_date = datetime.fromtimestamp(os.path.getmtime(target)).strftime(\'%Y-%m-%d %H:%M:%S\') if verbose: print(f\\"File: {target}\\") print(f\\"Size: {size} bytes\\") print(f\\"Creation Date: {creation_date}\\") print(f\\"Modification Date: {modification_date}\\") else: print(f\\"File: {target}, Size: {size} bytes, Created: {creation_date}, Modified: {modification_date}\\") def main(): parser = argparse.ArgumentParser(description=\\"File Processor\\") subparsers = parser.add_subparsers(dest=\\"command\\") # Global arguments parser.add_argument(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Print detailed logs during operations\\") parser.add_argument(\\"-b\\", \\"--backup\\", action=\\"store_true\\", help=\\"Keep a backup of the source file during copy or move operations\\") # Copy command parser_copy = subparsers.add_parser(\\"copy\\", help=\\"Copy contents from one file to another\\") parser_copy.add_argument(\\"source\\", type=str, help=\\"The source file path\\") parser_copy.add_argument(\\"destination\\", type=str, help=\\"The destination file path\\") # Move command parser_move = subparsers.add_parser(\\"move\\", help=\\"Move contents from one file to another (delete source after copying)\\") parser_move.add_argument(\\"source\\", type=str, help=\\"The source file path\\") parser_move.add_argument(\\"destination\\", type=str, help=\\"The destination file path\\") # Delete command parser_delete = subparsers.add_parser(\\"delete\\", help=\\"Delete a file\\") parser_delete.add_argument(\\"target\\", type=str, help=\\"The file to be deleted\\") # Info command parser_info = subparsers.add_parser(\\"info\\", help=\\"Print information about a file\\") parser_info.add_argument(\\"target\\", type=str, help=\\"The file to retrieve information from\\") args = parser.parse_args() if args.command == \\"copy\\": copy_file(args.source, args.destination, args.verbose, args.backup) elif args.command == \\"move\\": move_file(args.source, args.destination, args.verbose, args.backup) elif args.command == \\"delete\\": delete_file(args.target, args.verbose) elif args.command == \\"info\\": file_info(args.target, args.verbose) else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question** Objective To assess your understanding of loading datasets using scikit-learn, preprocessing data, implementing a machine learning model, and evaluating its performance. Problem Statement Write a Python script that does the following: 1. Load the Iris dataset from scikit-learn. 2. Preprocess the data: - Shuffle the dataset (optional but recommended for better training). - Split the data into training and testing sets (80% train, 20% test). - Standardize the features by scaling them to zero mean and unit variance. 3. Implement a machine learning model using Logistic Regression to classify the Iris species. 4. Evaluate the model by calculating the accuracy on the test set. 5. Print out the accuracy of the model. Detailed Requirements # Functions You should implement the following functions: 1. `load_and_preprocess_data()`: - Input: None - Output: Tuple (X_train, X_test, y_train, y_test) - Functionality: - Load the Iris dataset. - Shuffle the dataset. - Split it into training and testing sets. - Standardize the features. 2. `train_logistic_regression(X_train, y_train)`: - Input: Training features (X_train), Training labels (y_train) - Output: Trained Machine Learning Model - Functionality: - Train a Logistic Regression model on the training data. 3. `evaluate_model(model, X_test, y_test)`: - Input: Trained model, Testing features (X_test), Testing labels (y_test) - Output: Accuracy of the model - Functionality: - Predict the labels for the test set. - Calculate the accuracy of the model. # Constraints and Specifications - You must use scikit-learn for loading the dataset, preprocessing, and implementing the model. - You are advised to handle any potential preprocessing steps (e.g., scaling). - For splitting the dataset, use an 80-20 split for training and testing. Example Output ``` Accuracy of Logistic Regression model on test set: 0.95 ``` # Additional Notes - You can refer to scikit-learn\'s documentation for the implementation detail of Logistic Regression. - Make sure your code is well-documented with comments. - Ensure your script runs without errors and produces the expected result.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def load_and_preprocess_data(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Shuffle and split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_logistic_regression(X_train, y_train): # Create a Logistic Regression model and train it model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) return model def evaluate_model(model, X_test, y_test): # Predict the labels for the test set y_pred = model.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) return accuracy # Main execution if __name__ == \\"__main__\\": X_train, X_test, y_train, y_test = load_and_preprocess_data() model = train_logistic_regression(X_train, y_train) accuracy = evaluate_model(model, X_test, y_test) print(f\\"Accuracy of Logistic Regression model on test set: {accuracy:.2f}\\")"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) in `scikit-learn` by implementing a classification model, performing dimensionality reduction, experimenting with different solvers and comparing different covariance estimators. # Instructions 1. **Load the Iris Dataset**: - Use `sklearn.datasets.load_iris()` to load the Iris dataset. 2. **Linear Discriminant Analysis (LDA)**: - Perform a classification on the Iris dataset using LDA. - Experiment with all possible solvers (`svd`, `lsqr`, `eigen`) and report their classification accuracy. - Apply dimensionality reduction on the Iris dataset to 2 components using LDA and plot the reduced data points indicating their class. 3. **Quadratic Discriminant Analysis (QDA)**: - Perform a classification on the Iris dataset using QDA and report the classification accuracy. 4. **Shrinkage in LDA**: - Implement LDA using shrinkage. Try both automatic shrinkage and manually setting the `shrinkage` parameter to 0.5. - Report the classification accuracy for each approach. 5. **Comparing Covariance Estimators**: - Compare the performance of LDA with different covariance estimators: Empirical, Ledoit Wolf, and Oracle Approximating Shrinkage (OAS). - Report the classification accuracy for each approach. # Expected Output - Python functions or methods for loading data, performing LDA and QDA classifications, applying dimensionality reduction, experimenting with solvers and shrinkage, and comparing covariance estimators. - Plots for visualizing dimensionality reduction. - A summary of classification accuracies for different solvers, shrinkage methods, and covariance estimators. # Constraints - Use `scikit-learn` for all implementations. - Ensure your code is efficient: avoid redundancy and optimize for performance where possible. - Include comments and docstrings to explain your code logic. # Notes - The accuracy of each model must be computed using cross-validation with 5 folds. - Visualizations for dimensionality reduction should use `matplotlib`. # Example Function Definitions ```python def load_iris_dataset(): # Loads the Iris dataset pass def lda_classification(data, target, solver): # Perform LDA classification with a specified solver and return accuracy pass def lda_dimensionality_reduction(data, target, n_components=2): # Apply LDA dimensionality reduction and plot the results pass def qda_classification(data, target): # Perform QDA classification and return accuracy pass def lda_shrinkage_experiment(data, target, shrinkage_type): # Implement LDA with shrinkage and return accuracy pass def compare_covariance_estimators(data, target): # Compare performance of LDA with different covariance estimators and return accuracies pass # Students should implement these functions and provide additional helper functions as needed. ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis from sklearn.model_selection import cross_val_score import matplotlib.pyplot as plt def load_iris_dataset(): data = load_iris() return data.data, data.target def lda_classification(data, target, solver): lda = LinearDiscriminantAnalysis(solver=solver) accuracy = cross_val_score(lda, data, target, cv=5, scoring=\'accuracy\').mean() return accuracy def lda_dimensionality_reduction(data, target, n_components=2): lda = LinearDiscriminantAnalysis(n_components=n_components) reduced_data = lda.fit_transform(data, target) plt.figure() for color, i, target_name in zip([\'red\', \'green\', \'blue\'], [0, 1, 2], load_iris().target_names): plt.scatter(reduced_data[target == i, 0], reduced_data[target == i, 1], alpha=.8, color=color, label=target_name) plt.legend(loc=\'best\') plt.title(\'LDA of Iris dataset\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.show() def qda_classification(data, target): qda = QuadraticDiscriminantAnalysis() accuracy = cross_val_score(qda, data, target, cv=5, scoring=\'accuracy\').mean() return accuracy def lda_shrinkage_experiment(data, target, shrinkage_type=\'auto\'): lda = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=shrinkage_type) accuracy = cross_val_score(lda, data, target, cv=5, scoring=\'accuracy\').mean() return accuracy def compare_covariance_estimators(data, target): from sklearn.covariance import LedoitWolf, OAS lda = LinearDiscriminantAnalysis(solver=\'lsqr\') accuracies = { \'Empirical\': cross_val_score(lda, data, target, cv=5, scoring=\'accuracy\').mean(), } lda.set_params(covariance_estimator=LedoitWolf()) accuracies[\'Ledoit Wolf\'] = cross_val_score(lda, data, target, cv=5, scoring=\'accuracy\').mean() lda.set_params(covariance_estimator=OAS()) accuracies[\'OAS\'] = cross_val_score(lda, data, target, cv=5, scoring=\'accuracy\').mean() return accuracies"},{"question":"# Codec Management System Objective: Design and implement a Python class that manages custom codecs and their associated error handlers. This class should allow registering, unregistering, and using custom codecs for encoding and decoding operations, mimicking some of the low-level functionalities provided by the codec API in CPython. Requirements: 1. **Class Name: `CodecManager`** 2. **Methods:** - `register_codec(name: str, encode_func: Callable, decode_func: Callable) -> None`: Register a new codec. - `unregister_codec(name: str) -> None`: Unregister a codec. - `encode(text: str, encoding: str, errors: str = \'strict\') -> str`: Encode the given text using the specified encoding. Handle errors based on the error handling method. - `decode(data: str, encoding: str, errors: str = \'strict\') -> str`: Decode the given data using the specified encoding. Handle errors based on the error handling method. - `register_error_handler(name: str, handler: Callable) -> None`: Register a new error handling callback. - `lookup_error_handler(name: str) -> Callable`: Lookup an error handling callback. Constraints: - The `encode_func` and `decode_func` should be callables that take a string and return its encoded or decoded form respectively. - The error handler should be a callable that conforms to the signature expected by the codec API. - Implement basic pre-defined error handlers (`strict`, `ignore`, `replace`) within the class using corresponding methods. Example: ```python class CodecManager: def __init__(self): # Your initialization code here def register_codec(self, name: str, encode_func: Callable, decode_func: Callable) -> None: # Your code here def unregister_codec(self, name: str) -> None: # Your code here def encode(self, text: str, encoding: str, errors: str = \'strict\') -> str: # Your code here def decode(self, data: str, encoding: str, errors: str = \'strict\') -> str: # Your code here def register_error_handler(self, name: str, handler: Callable) -> None: # Your code here def lookup_error_handler(self, name: str) -> Callable: # Your code here # Create an instance of CodecManager manager = CodecManager() # Register a custom codec manager.register_codec(\'reverse\', lambda s: s[::-1], lambda s: s[::-1]) # Register a custom error handler manager.register_error_handler(\'replace\', lambda exc: (\'?\', exc.start + 1)) # Encode and decode using the custom codec encoded = manager.encode(\'hello\', \'reverse\') print(encoded) # Output: \'olleh\' decoded = manager.decode(encoded, \'reverse\') print(decoded) # Output: \'hello\' ``` In this problem, students are expected to exhibit a clear understanding of class design, proper handling of encoding and decoding functionalities, and effective error handling mechanisms as per Python\'s codec API semantics.","solution":"from typing import Callable class CodecManager: def __init__(self): self.codecs = {} self.error_handlers = { \'strict\': self._strict_error_handler, \'ignore\': self._ignore_error_handler, \'replace\': self._replace_error_handler } def register_codec(self, name: str, encode_func: Callable, decode_func: Callable) -> None: self.codecs[name] = (encode_func, decode_func) def unregister_codec(self, name: str) -> None: if name in self.codecs: del self.codecs[name] def encode(self, text: str, encoding: str, errors: str = \'strict\') -> str: if encoding not in self.codecs: raise LookupError(f\\"Codec \'{encoding}\' not found.\\") encode_func, _ = self.codecs[encoding] try: return encode_func(text) except Exception as e: return self._handle_error(errors, e) def decode(self, data: str, encoding: str, errors: str = \'strict\') -> str: if encoding not in self.codecs: raise LookupError(f\\"Codec \'{encoding}\' not found.\\") _, decode_func = self.codecs[encoding] try: return decode_func(data) except Exception as e: return self._handle_error(errors, e) def register_error_handler(self, name: str, handler: Callable) -> None: self.error_handlers[name] = handler def lookup_error_handler(self, name: str) -> Callable: if name not in self.error_handlers: raise LookupError(f\\"Error handler \'{name}\' not found.\\") return self.error_handlers[name] def _handle_error(self, errors, e): if errors not in self.error_handlers: raise LookupError(f\\"Error handler \'{errors}\' not found.\\") handler = self.error_handlers[errors] return handler(e) def _strict_error_handler(self, e): raise e def _ignore_error_handler(self, e): return \'\' def _replace_error_handler(self, e): return \'?\'"},{"question":"# Question: Implement a Directory Overview Tool using Python `stat` Module You are tasked with implementing a Python script that provides an overview of all files and directories within a specified directory. The overview should include file types, permissions, and other relevant attributes. Function Specifications **Function Name**: `directory_overview` **Parameters**: - `directory_path` (str): The path to the directory to be analyzed. **Returns**: - `None` **Behavior**: 1. Traverse the directory specified by `directory_path` and recursively visit all files and directories within. 2. For each file and directory, output the following attributes to the console: - Full path - File type: \\"Directory\\", \\"Regular File\\", \\"Symbolic Link\\", \\"Character Device\\", \\"Block Device\\", \\"FIFO\\", \\"Socket\\", or \\"Unknown\\" - Permissions: A string in the format similar to Unix `ls -l` (e.g., `-rwxr-xr--`) - Size (in bytes) - Last modified time (human-readable format) 3. Use the `stat` module functions to determine file types and permissions. 4. Handle errors gracefully, for instance by skipping over files that cannot be accessed and printing a warning message. Constraints - You should not use `os.path.isdir()`, `os.path.isfile()`, etc., directly. Instead, use the `stat` module functions such as `S_ISDIR`, `S_ISREG`, etc. Example If the directory structure is as follows: ``` /example-dir ├── file1.txt ├── file2.txt └── sub-dir └── file3.txt ``` Running `directory_overview(\\"/example-dir\\")` might output: ``` Path: /example-dir/file1.txt Type: Regular File Permissions: -rw-r--r-- Size: 1234 bytes Last Modified: 2023-10-01 12:34:56 Path: /example-dir/file2.txt Type: Regular File Permissions: -rw-r--r-- Size: 5678 bytes Last Modified: 2023-09-21 16:00:00 Path: /example-dir/sub-dir Type: Directory Permissions: drwxr-xr-x Size: 4096 bytes Last Modified: 2023-09-30 11:00:00 Path: /example-dir/sub-dir/file3.txt Type: Regular File Permissions: -rw-r--r-- Size: 910 bytes Last Modified: 2023-08-15 08:45:12 ``` You must implement the `directory_overview` function to achieve the above functionality. You are free to use additional helper functions if needed.","solution":"import os import stat import time def file_type(mode): if stat.S_ISDIR(mode): return \\"Directory\\" elif stat.S_ISREG(mode): return \\"Regular File\\" elif stat.S_ISLNK(mode): return \\"Symbolic Link\\" elif stat.S_ISCHR(mode): return \\"Character Device\\" elif stat.S_ISBLK(mode): return \\"Block Device\\" elif stat.S_ISFIFO(mode): return \\"FIFO\\" elif stat.S_ISSOCK(mode): return \\"Socket\\" else: return \\"Unknown\\" def permissions_string(mode): is_dir = \'d\' if stat.S_ISDIR(mode) else \'-\' perm_mapping = [ (stat.S_IRUSR, \'r\'), (stat.S_IWUSR, \'w\'), (stat.S_IXUSR, \'x\'), (stat.S_IRGRP, \'r\'), (stat.S_IWGRP, \'w\'), (stat.S_IXGRP, \'x\'), (stat.S_IROTH, \'r\'), (stat.S_IWOTH, \'w\'), (stat.S_IXOTH, \'x\'), ] perm_str = is_dir for perm, char in perm_mapping: perm_str += char if (mode & perm) else \'-\' return perm_str def directory_overview(directory_path): for root, dirs, files in os.walk(directory_path): for name in dirs + files: full_path = os.path.join(root, name) try: status = os.lstat(full_path) except Exception as e: print(f\\"Warning: Could not access {full_path}: {e}\\") continue mode = status.st_mode file_type_str = file_type(mode) permissions_str = permissions_string(mode) size = status.st_size last_modified_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(status.st_mtime)) print(f\\"Path: {full_path}\\") print(f\\"Type: {file_type_str}\\") print(f\\"Permissions: {permissions_str}\\") print(f\\"Size: {size} bytes\\") print(f\\"Last Modified: {last_modified_time}\\") print()"},{"question":"# Problem: Weather Data Processing Description You are provided with a list of daily weather data for a month. Each entry in the list is a tuple containing the day of the month, the high temperature, the low temperature, and the precipitation level in millimeters. Your task is to write a Python function that processes this data to find specific insights using functional programming techniques. Requirements Your function, `process_weather_data`, should accept a list of tuples where each tuple contains four elements: 1. Day of the month (integer, 1-31) 2. High temperature in Celsius (float) 3. Low temperature in Celsius (float) 4. Precipitation level in millimeters (float) Your function should return a dictionary with the following keys and corresponding values: 1. `\'average_high\'`: The average of the high temperatures. 2. `\'average_low\'`: The average of the low temperatures. 3. `\'total_precipitation\'`: The total precipitation for the month. 4. `\'max_precipitation_day\'`: The day with the highest precipitation. 5. `\'range_high\'`: The range (difference between the maximum and minimum) of the high temperatures. 6. `\'days_no_precipitation\'`: A list of days where the precipitation level was 0.0 mm. Constraints - The input list will have at least one element. - The temperatures and precipitation values will be valid floating-point numbers. Using iterators, generators, and built-in functional tools such as those from the itertools and functools modules is strongly encouraged to solve this problem. Example ```python weather_data = [ (1, 30.0, 21.0, 0.0), (2, 32.0, 22.0, 2.1), (3, 33.0, 23.0, 0.0), (4, 31.0, 21.5, 5.0), ] result = process_weather_data(weather_data) print(result) ``` Expected Output: ```python { \'average_high\': 31.5, \'average_low\': 21.875, \'total_precipitation\': 7.1, \'max_precipitation_day\': 4, \'range_high\': 3.0, \'days_no_precipitation\': [1, 3] } ``` Notes - Use `statistics.mean` to calculate the averages. - If there are multiple days with the highest precipitation, return any one of them. - Make use of itertools and functools modules effectively to achieve the desired results efficiently.","solution":"from functools import reduce import itertools from statistics import mean def process_weather_data(weather_data): Processes the weather data and returns insights. high_temps = [entry[1] for entry in weather_data] low_temps = [entry[2] for entry in weather_data] precipitations = [entry[3] for entry in weather_data] no_precipitation_days = [entry[0] for entry in weather_data if entry[3] == 0.0] max_precipitation_day = max(weather_data, key=lambda x: x[3])[0] total_precipitation = sum(precipitations) high_temp_range = max(high_temps) - min(high_temps) avg_high = mean(high_temps) avg_low = mean(low_temps) return { \'average_high\': avg_high, \'average_low\': avg_low, \'total_precipitation\': total_precipitation, \'max_precipitation_day\': max_precipitation_day, \'range_high\': high_temp_range, \'days_no_precipitation\': no_precipitation_days }"},{"question":"**Coding Challenge: Bytearray Utilities** **Objective:** Write a Python function that performs multiple operations on bytearrays. You are to implement the following function: ```python def bytearray_operations(data, operations): \'\'\' Perform a sequence of operations on a bytearray. Parameters: - data (any): An object that implements the buffer protocol (e.g., bytes, bytearray, memoryview). - operations (list): A list of operations. Each operation is represented as a tuple where: - The first element is a string indicating the operation type, which can be \\"concat\\", \\"resize\\", or \\"size\\". - The succeeding elements vary based on the operation type: - \\"concat\\": The second element is another object that implements the buffer protocol to concatenate to the bytearray. - \\"resize\\": The second element is an integer indicating the new size of the bytearray. - \\"size\\": This operation takes no additional arguments and returns the current size of the bytearray. Returns: - list: A list containing the results of the performed operations. Specifically, \\"concat\\" and \\"resize\\" operations do not produce results to append to the list, but the result of the \\"size\\" operation should be included. \'\'\' pass ``` **Instructions:** 1. The function converts the initial data into a bytearray using `PyByteArray_FromObject`. 2. Then, it processes each operation in the operations list on this bytearray: - For \\"concat\\" operations, concatenate the given data to the current bytearray using `PyByteArray_Concat`. - For \\"resize\\" operations, change the size of the bytearray using `PyByteArray_Resize`. - For \\"size\\" operations, append the size of the current bytearray to the results using `PyByteArray_Size`. 3. Return the results list, containing the sizes determined during \\"size\\" operations. **Example:** ```python data = b\'initial data\' operations = [ (\'concat\', b\' more data\'), (\'size\',), (\'resize\', 5), (\'size\',) ] # Expected result: # [25, 5] print(bytearray_operations(data, operations)) ``` **Constraints:** - The `data` can be any object that implements the buffer protocol (e.g., bytes, bytearray, memoryview). - The function should handle and return errors appropriately if invalid operations or parameters are provided. Note: Use the direct API functions and macros discussed above to manipulate and check the bytearray.","solution":"def bytearray_operations(data, operations): Perform a sequence of operations on a bytearray. Parameters: - data (any): An object that implements the buffer protocol (e.g., bytes, bytearray, memoryview). - operations (list): A list of operations. Returns: - list: A list containing the results of the performed operations. bytearr = bytearray(data) results = [] for operation in operations: if operation[0] == \\"concat\\": bytearr.extend(operation[1]) elif operation[0] == \\"resize\\": bytearr = bytearr[:operation[1]] elif operation[0] == \\"size\\": results.append(len(bytearr)) else: raise ValueError(f\\"Unknown operation: {operation[0]}\\") return results"},{"question":"# Background You\'ve decided to implement a parallel execution framework using `torch.futures.Future` objects. Your goal is to run multiple tasks asynchronously and then collect the results when all tasks are completed. You will utilize the `torch.futures` package, particularly focusing on the `Future` class and the utility functions `collect_all` and `wait_all`. # Task Implement a function `execute_and_collect` that: 1. Takes a list of functions, where each function returns a `torch.futures.Future`. 2. Executes all these functions asynchronously. 3. Collects the results of these asynchronous executions once all tasks are completed. 4. Returns a list of the results of the executed functions. Function Signature ```python def execute_and_collect(tasks: List[Callable[[], torch.futures.Future]]) -> List[Any]: pass ``` Input - `tasks`: A list of functions. Each function, when called, returns a `torch.futures.Future` object. Output - A list of results from the resolved `Future` objects. Constraints - Ensure all tasks are initiated without waiting for the previous tasks to complete. - Use the `collect_all` or `wait_all` function to aggregate the `Future` objects. Example ```python import torch import torch.futures def dummy_task(): future = torch.futures.Future() future.set_result(42) return future tasks = [dummy_task, dummy_task, dummy_task] results = execute_and_collect(tasks) print(results) # Output should be [42, 42, 42] ``` Guidance - You may utilize any imported modules from `torch.futures`. - Ensure that all `Future` objects are properly handled and their results are collected correctly. # Additional Requirement Include appropriate error handling to manage potential issues during the asynchronous execution, such as ensuring that all tasks start running concurrently.","solution":"import torch from torch.futures import Future from typing import List, Callable, Any def execute_and_collect(tasks: List[Callable[[], Future]]) -> List[Any]: Executes a list of tasks asynchronously and collects the results. Args: tasks: A list of functions, where each function returns a torch.futures.Future. Returns: A list of results from the resolved Future objects. futures = [] # Execute all tasks asynchronously for task in tasks: futures.append(task()) # Wait for all futures to complete results = torch.futures.collect_all(futures).wait() # Extract results from the completed futures result_values = [result.value() for result in results] return result_values"},{"question":"# Question: Implement and Compare SVM Classifiers for Multi-Class Classification You are provided with the digits dataset from the sklearn library. Your task is to implement and compare the performance of two SVM classifiers (`SVC` and `LinearSVC`) for multi-class classification. You need to perform the following tasks: 1. **Load the digits dataset** from the sklearn library. 2. **Preprocess the dataset**: - Split the dataset into training and testing sets. - Standardize the features by removing the mean and scaling to unit variance. 3. **Train two models**: - Train an `SVC` model with an RBF kernel. - Train a `LinearSVC` model. 4. **Evaluate the models** using: - Accuracy score on the test set. - Classification report (precision, recall, f1-score for each class). 5. **Compare and discuss the results**. Expected Input and Output - **Input:** None (Read data directly from the sklearn library). - **Output:** - Print accuracy scores for both models. - Print classification reports for both models. - Provide a brief discussion on the comparison of the two models. Constraints - Use random_state=42 for reproducibility while splitting the dataset and initializing the models. - Use standard scaling for preprocessing the features. Performance Requirements - Your implementation should be efficient and should not exceed 60 seconds of runtime. ```python import numpy as np from sklearn import datasets from sklearn.svm import SVC, LinearSVC from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, classification_report # 1. Load the digits dataset digits = datasets.load_digits() X = digits.data y = digits.target # 2. Preprocess the dataset # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 3. Train models # SVC with RBF kernel svc_rbf = SVC(kernel=\'rbf\', random_state=42) svc_rbf.fit(X_train, y_train) # LinearSVC linear_svc = LinearSVC(random_state=42) linear_svc.fit(X_train, y_train) # 4. Evaluate models # Predictions y_pred_svc_rbf = svc_rbf.predict(X_test) y_pred_linear_svc = linear_svc.predict(X_test) # Accuracy scores accuracy_svc_rbf = accuracy_score(y_test, y_pred_svc_rbf) accuracy_linear_svc = accuracy_score(y_test, y_pred_linear_svc) print(f\\"SVC with RBF kernel - Accuracy: {accuracy_svc_rbf:.2f}\\") print(f\\"LinearSVC - Accuracy: {accuracy_linear_svc:.2f}\\") # Classification reports print(\\"nClassification Report for SVC with RBF Kernel:\\") print(classification_report(y_test, y_pred_svc_rbf)) print(\\"Classification Report for LinearSVC:\\") print(classification_report(y_test, y_pred_linear_svc)) # 5. Compare and discuss results # Write a brief discussion on the comparison of SVC with RBF kernel and LinearSVC based on the accuracy scores and classification reports. ``` Discussion - Summarize the comparison between the SVC with RBF kernel and LinearSVC in terms of accuracy, precision, recall, and f1-score for each class. - Discuss any trade-offs you observed between model complexity and performance. - Provide insights on which model might be more suitable for this dataset and why.","solution":"import numpy as np from sklearn import datasets from sklearn.svm import SVC, LinearSVC from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, classification_report def load_and_preprocess_data(): # Load the digits dataset digits = datasets.load_digits() X = digits.data y = digits.target # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_svc_rbf(X_train, y_train): # SVC with RBF kernel svc_rbf = SVC(kernel=\'rbf\', random_state=42) svc_rbf.fit(X_train, y_train) return svc_rbf def train_linear_svc(X_train, y_train): # LinearSVC linear_svc = LinearSVC(random_state=42, max_iter=2000) linear_svc.fit(X_train, y_train) return linear_svc def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) return accuracy, report # Load and preprocess data X_train, X_test, y_train, y_test = load_and_preprocess_data() # Train models svc_rbf = train_svc_rbf(X_train, y_train) linear_svc = train_linear_svc(X_train, y_train) # Evaluate models accuracy_svc_rbf, report_svc_rbf = evaluate_model(svc_rbf, X_test, y_test) accuracy_linear_svc, report_linear_svc = evaluate_model(linear_svc, X_test, y_test) # Print results print(f\\"SVC with RBF kernel - Accuracy: {accuracy_svc_rbf:.2f}\\") print(f\\"LinearSVC - Accuracy: {accuracy_linear_svc:.2f}\\") print(\\"nClassification Report for SVC with RBF Kernel:\\") print(report_svc_rbf) print(\\"Classification Report for LinearSVC:\\") print(report_linear_svc) # Discussion comparison = After evaluating both models, it is observed that the SVC with RBF kernel generally has a higher accuracy and better classification metrics (precision, recall, and f1-score) compared to the LinearSVC model. This suggests that the RBF kernel is better suited for capturing the complex patterns in the digits dataset. However, it is important to note that the LinearSVC model is significantly faster to train because it solves a linear problem. The trade-off for using LinearSVC is a slight decrease in performance, which might be acceptable in scenarios where training time is a crucial factor. The SVC with RBF kernel may be more suitable for this dataset due to its improved performance metrics, but the choice of model can depend on the specific requirements regarding training time and computational resources. print(comparison)"},{"question":"You are tasked with building a network utility tool in Python that utilizes the `ipaddress` module and the `urllib` package to fetch and analyze data about a given IP address. Your function should perform the following steps: 1. **Validate the IP Address**: Ensure the provided input is a valid IPv4 or IPv6 address using the `ipaddress` module. 2. **Fetch Location Data**: Use the `urllib` package to retrieve JSON data from a public API that provides geolocation information based on the IP address. 3. **Parse and Extract Information**: Extract specific details from the JSON data, such as the country, city, latitude, and longitude. 4. **Functional Programming**: Use functional programming techniques to filter and map the data effectively. # Function Signature ```python def analyze_ip_address(ip: str) -> dict: pass ``` # Input - `ip`: A string representing the IP address (IPv4 or IPv6). # Output - A dictionary containing the following keys, if available: - `country` (str): The country where the IP address is located. - `city` (str): The city where the IP address is located. - `latitude` (float): The latitude of the IP address\'s location. - `longitude` (float): The longitude of the IP address\'s location. # Example Usage Assuming you have a public API that returns the following JSON for the IP `8.8.8.8`: ```json { \\"country\\": \\"United States\\", \\"city\\": \\"Mountain View\\", \\"latitude\\": 37.386, \\"longitude\\": -122.084 } ``` ```python result = analyze_ip_address(\\"8.8.8.8\\") print(result) ``` Output: ```python { \\"country\\": \\"United States\\", \\"city\\": \\"Mountain View\\", \\"latitude\\": 37.386, \\"longitude\\": -122.084 } ``` # Constraints - The function should raise a `ValueError` if the provided IP address is not valid. - If the API does not return one of the keys, exclude that key from the result dictionary. - Use functional programming concepts (such as `map`, `filter`, and `lambda` functions) where applicable. - Ensure your implementation avoids unnecessary network calls by using caching techniques or error handling. # API Information Use an IP geolocation API of your choice (e.g., `http://ip-api.com/json/`). # Notes - Please handle exceptions and provide meaningful error messages. - Consider edge cases such as private IP addresses or malformed JSON responses. # Performance Requirements - The implementation should efficiently handle network delays and retries. - Optimize for readability and maintainability, while adhering to Pythonic principles.","solution":"import ipaddress import urllib.request import json def analyze_ip_address(ip: str) -> dict: Analyzes the given IP address for its location data. Parameters: - ip : str : IP address (IPv4 or IPv6) Returns: - dict: Dictionary containing the country, city, latitude, and longitude of the IP address. Raises: - ValueError: If the IP address is not valid. # Step 1: Validate the IP Address try: ipaddress.ip_address(ip) except ValueError as e: raise ValueError(f\\"Invalid IP address: {ip}\\") from e # Step 2: Fetch Location Data using urllib url = f\\"http://ip-api.com/json/{ip}\\" try: with urllib.request.urlopen(url) as response: data = json.load(response) except Exception as e: raise RuntimeError(f\\"Failed to fetch data for IP address: {ip}\\") from e # Step 3: Parse and Extract Information keys_of_interest = [\'country\', \'city\', \'lat\', \'lon\'] location_data = { \'country\': data.get(\'country\'), \'city\': data.get(\'city\'), \'latitude\': data.get(\'lat\'), \'longitude\': data.get(\'lon\') } # Use functional approach to filter out None values processed_data = {k: v for k, v in location_data.items() if v is not None} return processed_data"},{"question":"# Custom Interactive Python Interpreter with History Feature Objective: Implement a custom interactive Python interpreter that can execute Python code dynamically and maintain a history of executed commands. Instructions: 1. **Create a CustomInteractiveInterpreter class** that extends `code.InteractiveInterpreter`. 2. **Implement a method `run_command` to execute a single line of Python code**. 3. **Maintain a history** of all commands executed during the session. 4. **Provide a method `get_history`** which returns all commands executed in the current session. 5. **Ensure proper handling of multi-line commands** using the `codeop` module. Function Signatures: ```python class CustomInteractiveInterpreter(code.InteractiveInterpreter): def __init__(self): Initializes a new instance of the CustomInteractiveInterpreter class. Initializes necessary attributes for storing command history. pass def run_command(self, command: str) -> None: Executes a single line of Python code. Args: command (str): The Python code to be executed. Returns: None pass def get_history(self) -> list: Retrieves the history of all commands executed in the current session. Returns: list: A list of strings, each representing a command executed. pass ``` Input and Output Formats: * The `run_command` method: * **Input:** A string representing a single line of Python code. * **Output:** None (executes the code and captures any side effects). * The `get_history` method: * **Input:** None * **Output:** A list of strings, each representing a command executed in the current session. Constraints: * The interpreter should be able to handle multiple lines of Python code correctly. * Ensure that syntax errors and other exceptions do not cause the interpreter to crash. * Performance should be kept in mind, though no strict performance constraints are applied. Example Usage: ```python interpreter = CustomInteractiveInterpreter() interpreter.run_command(\\"a = 10\\") interpreter.run_command(\\"print(a)\\") interpreter.run_command(\\"for i in range(3):n print(i)\\") history = interpreter.get_history() # Expected Output: # history should be [\'a = 10\', \'print(a)\', \'for i in range(3):n print(i)\'] ``` Good luck!","solution":"import code import codeop class CustomInteractiveInterpreter(code.InteractiveInterpreter): def __init__(self): super().__init__() self.history = [] self.buffer = [] self.more = False def run_command(self, command: str) -> None: self.history.append(command) self.buffer.append(command) source = \'n\'.join(self.buffer) try: code_object = codeop.compile_command(source) except (OverflowError, SyntaxError, ValueError): self.showsyntaxerror() self.resetbuffer() return if code_object is None: self.more = True else: self.more = False self.runcode(code_object) self.resetbuffer() def get_history(self) -> list: return self.history def resetbuffer(self): self.buffer = [] # Example usage # interpreter = CustomInteractiveInterpreter() # interpreter.run_command(\\"a = 10\\") # interpreter.run_command(\\"print(a)\\") # interpreter.run_command(\\"for i in range(3):n print(i)\\") # history = interpreter.get_history() # print(history) # [\'a = 10\', \'print(a)\', \'for i in range(3):n print(i)\']"},{"question":"# Coding Assessment Question Design a function in Python that utilizes the `chunk` module to read and extract data from chunks within a provided file. Your function should be able to read multiple chunks from the file and return their IDs and sizes in the order they appear. Additionally, implement functionality to read the data of each chunk if requested. Function Signature ```python def extract_chunk_info(file_path: str, read_data: bool = False) -> list: ``` Input - `file_path` (str): The path to the file that contains the chunks. - `read_data` (bool): Optional; If set to True, the function should read and include the chunk data in the output. Default is False. Output - Returns a list of tuples containing chunk information. Each tuple should contain: - The chunk ID (str) - The chunk size (int) - Optional: The chunk data (bytes) if `read_data` is set to True Constraints 1. Assume that the file at `file_path` exists and is in the correct chunked format. 2. Handle files that contain multiple chunks correctly. 3. Performance considerations: - The function should not load the entire file into memory if it is not necessary. Example For a file with the following chunk data structure: ``` +--------+--------+--------+--------+--------+-------+--------+-------+ | Chunk | ID1 | Size1 | Data1 | Chunk | ID2 | Data2 | ... | +--------+--------+--------+--------+--------+-------+--------+-------+ ``` Calling the function as follows: ```python print(extract_chunk_info(\\"path/to/chunked_file\\", read_data=False)) ``` Should return: ``` [(\'ID1\', Size1), (\'ID2\', Size2)] ``` Calling the function with `read_data=True`: ```python print(extract_chunk_info(\\"path/to/chunked_file\\", read_data=True)) ``` Should return: ``` [(\'ID1\', Size1, Data1), (\'ID2\', Size2, Data2)] ``` Note - You are expected to use the `chunk` module\'s `Chunk` class to handle the file reading and chunk management. - Ensure proper exception handling for end of file (EOF) conditions and other potential file read errors.","solution":"import chunk def extract_chunk_info(file_path: str, read_data: bool = False) -> list: Extracts chunk information from the provided file. Args: file_path (str): The path to the file that contains the chunks. read_data (bool): If set to True, reads and includes the chunk data in the output. Default is False. Returns: list: A list of tuples containing chunk information. Each tuple contains: - chunk ID (str) - chunk size (int) - Optional: chunk data (bytes) if read_data is set to True chunk_infos = [] try: with open(file_path, \'rb\') as f: while True: try: ck = chunk.Chunk(f, bigendian=False, align=True) chunk_id = ck.getname().decode(\'ascii\') chunk_size = ck.getsize() if read_data: chunk_data = ck.read() chunk_infos.append((chunk_id, chunk_size, chunk_data)) else: chunk_infos.append((chunk_id, chunk_size)) ck.skip() # to move to the next chunk except EOFError: break except Exception as e: print(f\\"An error occurred: {e}\\") return chunk_infos"},{"question":"**Memory-Efficient Data Handling and Chunking in Pandas** You are provided with a dataset that consists of multiple parquet files, each representing data logged by different sensors for various years. Your task is to: 1. Extract specific columns from each parquet file. 2. Convert relevant columns to more memory-efficient data types. 3. Process the data in chunks to calculate some aggregate metrics. # Input: - A directory path containing multiple parquet files, each having a format similar to: ``` ├── data/ │ ├── sensor-00.parquet │ ├── sensor-01.parquet │ └── ... ``` - Each parquet file has columns: `sensor_id`, `timestamp`, `reading_value`, `status`. # Output: - Print the total memory usage before and after optimizing data types. - Print the total count of unique `sensor_id` and `status` combinations. - Print the memory reduction fraction after optimization. # Function Signature: ```python import pandas as pd from pathlib import Path def process_sensor_data(directory: str) -> None: # Your code here # Example usage process_sensor_data(\\"data/\\") ``` # Requirements: 1. **Loading and Optimizing Data Types**: - Load only the `sensor_id`, `reading_value`, and `status` columns from each parquet file. - Convert the `status` column to `pandas.Categorical`. - Downcast `reading_value` to the smallest appropriate numeric type. 2. **Processing Data in Chunks**: - Calculate the total count of unique (sensor_id, status) combinations across all files. - Ensure your implementation does not load all data into memory at once. 3. **Memory Usage Calculation**: - Print memory usage before and after data type optimization. - Print the fraction of memory used after optimization compared to the initial usage. # Constraints: - Make sure your solution can handle a large number of parquet files, with potentially large individual file sizes. - Each parquet file should be processed one at a time to fit within memory constraints. # Example Output: ``` Initial total memory usage: X bytes Optimized total memory usage: Y bytes Memory reduction fraction: Z Count of unique (sensor_id, status) combinations: N ``` Notes: - The `data` directory should be structured as shown above. - Files should be named in a format similar to `sensor-xx.parquet`. - Your solution should be robust to handle the provided constraints efficiently.","solution":"import pandas as pd from pathlib import Path def process_sensor_data(directory: str) -> None: # Initialize metrics initial_memory_usage = 0 optimized_memory_usage = 0 unique_combinations = set() # List all parquet files in the directory data_dir = Path(directory) parquet_files = list(data_dir.glob(\\"*.parquet\\")) for file in parquet_files: # Load specific columns from the parquet file df = pd.read_parquet(file, columns=[\'sensor_id\', \'reading_value\', \'status\']) # Calculate initial memory usage initial_memory_usage += df.memory_usage(deep=True).sum() # Convert `status` to categorical df[\'status\'] = df[\'status\'].astype(\'category\') # Downcast `reading_value` df[\'reading_value\'] = pd.to_numeric(df[\'reading_value\'], downcast=\'float\') # Calculate optimized memory usage optimized_memory_usage += df.memory_usage(deep=True).sum() # Add unique (sensor_id, status) combinations to the set unique_combinations.update(df[[\'sensor_id\', \'status\']].apply(tuple, axis=1)) # Calculate memory reduction fraction memory_reduction_fraction = optimized_memory_usage / initial_memory_usage # Print results print(f\\"Initial total memory usage: {initial_memory_usage} bytes\\") print(f\\"Optimized total memory usage: {optimized_memory_usage} bytes\\") print(f\\"Memory reduction fraction: {memory_reduction_fraction}\\") print(f\\"Count of unique (sensor_id, status) combinations: {len(unique_combinations)}\\") # Example usage # process_sensor_data(\\"data/\\")"},{"question":"# Question: Custom PyTorch Autograd Function Implementation You are provided with a task to implement a custom autograd function in PyTorch. The objective is to create a custom autograd function that computes the forward and backward passes for the function ( f(x) = x^3 + sin(x) ). Requirements: 1. **Custom Function Class**: Implement a custom autograd function called `CustomFunction` by extending the `torch.autograd.Function` class. 2. **Forward Pass**: - Save the input tensor for use in the backward pass. 3. **Backward Pass**: - Compute the gradient of the function ( frac{dL}{dx} = 3x^2 + cos(x) ). Use the saved input tensor from the forward pass to compute this gradient. 4. **Disable Gradient Computation**: Include a part of the code where gradient computation is temporarily disabled for certain operations. # Input - A single PyTorch tensor `x` with `requires_grad=True`. # Output - A single PyTorch tensor representing the output of the function ( f(x) ) during the forward pass. # Constraints - Do not use any in-place operations. - Ensure that the autograd mechanics are properly utilized. # Example ```python import torch from torch.autograd import Function class CustomFunction(Function): @staticmethod def forward(ctx, input): # Save the input tensor for backward computation ctx.save_for_backward(input) # Compute the forward pass output = input ** 3 + torch.sin(input) return output @staticmethod def backward(ctx, grad_output): # Retrieve the saved input tensor input, = ctx.saved_tensors # Compute the gradient of the function grad_input = grad_output * (3 * input ** 2 + torch.cos(input)) return grad_input # Create a tensor with requires_grad=True x = torch.tensor([2.0, 3.0, 4.0], requires_grad=True) # Apply the custom autograd function output = CustomFunction.apply(x) # Perform a backward pass output.sum().backward() # Print the gradients print(x.grad) # Expected output: tensor([12.5839, 27.0100, 50.3464]) ``` # Explanation In the provided code, 1. We define a `CustomFunction` class extending `torch.autograd.Function`. 2. The `forward` method computes the output of the function ( f(x) = x^3 + sin(x) ) and saves the input tensor using `ctx.save_for_backward`. 3. The `backward` method retrieves the saved input tensor, computes the gradient ( frac{dL}{dx} = 3x^2 + cos(x) ), and returns the gradient multiplied by `grad_output`. Implement the `CustomFunction` class according to the requirements specified.","solution":"import torch from torch.autograd import Function class CustomFunction(Function): @staticmethod def forward(ctx, input): # Save the input tensor for backward computation ctx.save_for_backward(input) # Compute the forward pass output = input ** 3 + torch.sin(input) return output @staticmethod def backward(ctx, grad_output): # Retrieve the saved input tensor input, = ctx.saved_tensors # Compute the gradient of the function grad_input = grad_output * (3 * input ** 2 + torch.cos(input)) return grad_input # Create a tensor with requires_grad=True x = torch.tensor([2.0, 3.0, 4.0], requires_grad=True) # Apply the custom autograd function output = CustomFunction.apply(x) # Perform a backward pass output.sum().backward() # Print the gradients print(x.grad) # Expected output: tensor([12.5839, 27.0100, 50.3464])"},{"question":"# Command-Line Interface for a File Processing Script **Objective:** Your task is to create a Python script that simulates a basic file processing system using the `argparse` module. This script will provide functionalities to read, write, and append content to a file. It should also include options to display the file content and show a help message if the user requests it. **Requirements:** 1. **Argument Parsing Setup:** - Create an `ArgumentParser` object with a description of your program. - Define the following sub-commands using `add_subparsers`: - `read`: Reads and displays the content of the specified file. - `write`: Writes the provided content to the specified file. - `append`: Appends the provided content to the specified file. - `display`: Displays the current content of the file. 2. **Sub-Command Arguments:** - `read` should take one positional argument: - `--file`: Path to the file to be read. - `write` should take two arguments: - `--file`: Path to the file to be written to. - `--content`: Content to be written to the file. - `append` should take two arguments: - `--file`: Path to the file to be appended to. - `--content`: Content to be appended to the file. - `display` should take one positional argument: - `--file`: Path to the file whose content is to be displayed. 3. **Optional Arguments:** - Add `-v` or `--verbose` flag for verbose output, providing additional information during execution. - Ensure the `-h` or `--help` argument is available for displaying help messages. 4. **Functionality:** - Implement a function for each sub-command: - `read_file`: Read and print the content of the specified file. - `write_to_file`: Write the specified content to the specified file, overwriting existing content. - `append_to_file`: Append the specified content to the specified file. - `display_file`: Display the current content of the specified file. - Set each sub-command to call the relevant function using `set_defaults(func=<function_name>)`. 5. **Error Handling:** - Handle exceptions such as file not found, permission errors, etc., and print appropriate error messages. **Constraints:** - The script should handle command-line arguments in a graceful manner. - Ensure that the script works for various scenarios, such as empty files, non-existing files, etc. **Example Usage:** Assuming the script is saved as `file_processor.py`, here\'s how it should be used: ```sh python file_processor.py read --file sample.txt # Output: <content of sample.txt> python file_processor.py write --file sample.txt --content \\"Hello, World!\\" # Output: Content written to sample.txt python file_processor.py append --file sample.txt --content \\" New content.\\" # Output: Content appended to sample.txt python file_processor.py display --file sample.txt # Output: <current content of sample.txt> python file_processor.py -h # Output: Help message displaying usage instructions ``` Implement the script `file_processor.py` to meet the above requirements.","solution":"import argparse import os def read_file(file): try: with open(file, \'r\') as f: print(f.read()) except FileNotFoundError: print(f\\"Error: The file \'{file}\' does not exist.\\") except PermissionError: print(f\\"Error: Permission denied for file \'{file}\'.\\") def write_to_file(file, content): try: with open(file, \'w\') as f: f.write(content) print(f\\"Content written to {file}.\\") except PermissionError: print(f\\"Error: Permission denied for file \'{file}\'.\\") def append_to_file(file, content): try: with open(file, \'a\') as f: f.write(content) print(f\\"Content appended to {file}.\\") except PermissionError: print(f\\"Error: Permission denied for file \'{file}\'.\\") def display_file(file): try: with open(file, \'r\') as f: print(f.read()) except FileNotFoundError: print(f\\"Error: The file \'{file}\' does not exist.\\") except PermissionError: print(f\\"Error: Permission denied for file \'{file}\'.\\") def main(): parser = argparse.ArgumentParser(description=\\"A basic file processing script\\") subparsers = parser.add_subparsers() parser_read = subparsers.add_parser(\'read\', help=\\"Read the content of a file\\") parser_read.add_argument(\'--file\', required=True, help=\\"Path to the file to be read\\") parser_read.set_defaults(func=lambda args: read_file(args.file)) parser_write = subparsers.add_parser(\'write\', help=\\"Write content to a file\\") parser_write.add_argument(\'--file\', required=True, help=\\"Path to the file to write to\\") parser_write.add_argument(\'--content\', required=True, help=\\"Content to write to the file\\") parser_write.set_defaults(func=lambda args: write_to_file(args.file, args.content)) parser_append = subparsers.add_parser(\'append\', help=\\"Append content to a file\\") parser_append.add_argument(\'--file\', required=True, help=\\"Path to the file to append to\\") parser_append.add_argument(\'--content\', required=True, help=\\"Content to append to the file\\") parser_append.set_defaults(func=lambda args: append_to_file(args.file, args.content)) parser_display = subparsers.add_parser(\'display\', help=\\"Display the content of a file\\") parser_display.add_argument(\'--file\', required=True, help=\\"Path to the file to display\\") parser_display.set_defaults(func=lambda args: display_file(args.file)) parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\\"Increase output verbosity\\") args = parser.parse_args() if hasattr(args, \'func\'): if args.verbose: print(\\"Running in verbose mode...\\") args.func(args) else: parser.print_help() if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** You are working on a network application that requires sending and receiving data in a well-defined binary format between different systems. As an experienced programmer, you are required to implement a function that handles the packing and unpacking of data into specified formats using the `struct` module. **Task:** 1. Implement a function `pack_and_unpack_data(data, format_str, byte_order)`. This function will: * Pack the given data into a binary format based on the provided format string and byte order. * Unpack the packed data to verify the correctness of the packing. 2. The function should handle: * Correct packing and unpacking of data based on the format string. * Appropriate handling of byte order: `>` for big-endian, `<` for little-endian, and `@` for native byte order. * Edge cases such as packing values that are out of the permissible range for a given format. **Function Signature:** ```python def pack_and_unpack_data(data: tuple, format_str: str, byte_order: str) -> tuple: pass ``` **Input:** - `data`: A tuple containing the values to be packed. - `format_str`: A string defining the format according to the `struct` module conventions. - `byte_order`: A string representing the byte order (\'>\', \'<\', or \'@\'). **Output:** - A tuple containing the unpacked values, which should match the input data. **Constraints:** - All values in `data` must be compatible with the specified format. - The format string should be valid and follow the conventions outlined in the `struct` module documentation. **Example:** ```python data = (1, 2, 3) format_str = \'bhl\' byte_order = \'>\' output = pack_and_unpack_data(data, format_str, byte_order) print(output) # Expected output: (1, 2, 3) data = (255, 65535, -32768) format_str = \'BHi\' byte_order = \'<\' output = pack_and_unpack_data(data, format_str, byte_order) print(output) # Expected output: (255, 65535, -32768) ``` **Note:** - Ensure proper exception handling for cases where packing or unpacking fails due to invalid data or format issues. - Utilize the `struct` module functions like `pack`, `unpack`, and `calcsize` where necessary to complete this task.","solution":"import struct def pack_and_unpack_data(data: tuple, format_str: str, byte_order: str) -> tuple: Pack and unpack data according to the given format string and byte order. :param data: A tuple containing the values to be packed. :param format_str: A string defining the format according to the `struct` module conventions. :param byte_order: A string representing the byte order (\'>\', \'<\', or \'@\'). :return: A tuple containing the unpacked values. # Combine byte_order and format_str full_format = byte_order + format_str # Pack the data packed_data = struct.pack(full_format, *data) # Unpack the data to verify unpacked_data = struct.unpack(full_format, packed_data) return unpacked_data"},{"question":"**Question:** Using Seaborn to Create Advanced Plots with Custom Scaling # Objective: In this task, you will demonstrate your understanding of the `seaborn.objects` module by creating a complex plot with advanced scaling and aesthetic adjustments. # Instructions: 1. **Load and Preprocess Datasets:** - Use the `seaborn` library to load the `diamonds` and `mpg` datasets. - Filter the `mpg` dataset to only include rows where the number of cylinders is in `[4, 6, 8]`. 2. **Create Custom Plot:** - Using the `diamonds` dataset, create a scatter plot of `carat` versus `price`. - Set the y-axis to have a logarithmic scale. - Color-code the points based on the `clarity` variable using the \\"viridis\\" color palette. 3. **Advanced Scaling and Aesthetics:** - Adjust the point size based on the `carat` variable, using a range of (2, 10). - Customize the color scale by passing a `Continuous` scale object that: - Normalizes the `carat` values to the range (1, 3). - Applies a square root transformation (`trans=\\"sqrt\\"`). - Uses the color range from light grey (`\\"0.85\\"`) to dark blue (`\\"#2166ac\\"`). 4. **Additional Plot Customizations:** - Adjust the tick positions on the x-axis to be every 0.5 units. - Format the tick labels on the y-axis to be displayed as dollar amounts (e.g., `1000`). - Set the range of the y-axis to be between 250 and 25000. # Example Code Structure: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load and preprocess datasets diamonds = load_dataset(\\"diamonds\\") mpg = load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Step 2: Create custom plot p = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") p.add(so.Dots(), pointsize=\\"carat\\", color=\\"clarity\\").scale( y=\\"log\\", pointsize=(2, 10), color=so.Continuous([\\"0.85\\", \\"#2166ac\\"], norm=(1, 3), trans=\\"sqrt\\") ) # Step 3: Advanced scaling and aesthetics p.scale( x=so.Continuous().tick(every=0.5), y=so.Continuous().label(like=\\"{x:g}\\") ) p.limit(y=(250, 25000)) # Show the plot p.show() ``` # Constraints: - You must use the `seaborn.objects` interface and its scaling functionalities. - Ensure that your plot is clear and aesthetically pleasing. # Submission: Submit your Python code with the implementation and the resulting plot.","solution":"import seaborn.objects as so import seaborn as sns def create_custom_plot(): # Step 1: Load and preprocess datasets diamonds = sns.load_dataset(\\"diamonds\\") mpg = sns.load_dataset(\\"mpg\\").query(\\"cylinders in [4, 6, 8]\\") # Step 2: Create custom plot p = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") # Step 3: Advanced scaling and aesthetics p.add(so.Dots(), pointsize=\\"carat\\", color=\\"clarity\\").scale( y=\\"log\\", pointsize=(2, 10), color=so.Continuous([\\"0.85\\", \\"#2166ac\\"], norm=(1, 3), trans=\\"sqrt\\") ) # Step 4: Additional plot customizations p.scale( x=so.Continuous().tick(every=0.5), y=so.Continuous().label(like=\\"{x:g}\\") ).limit(y=(250, 25000)) # Show the plot p.show() create_custom_plot()"},{"question":"on Seaborn **Objective:** Demonstrate your understanding of the seaborn library, specifically focusing on KDE (Kernel Density Estimation) and the `seaborn.objects` module. **Problem Description:** You are given the `iris` dataset that contains the following columns: `\'sepal_length\'`, `\'sepal_width\'`, `\'petal_length\'`, `\'petal_width\'`, and `\'species\'`. You are required to create a series of plots to visualize the density distribution of `sepal_length` across different species, using various techniques to adjust and highlight different aspects of the KDE plots. **Tasks:** 1. **Load the Iris Dataset:** - Load the iris dataset from seaborn. 2. **Basic KDE Plot:** - Create a basic KDE plot to visualize the distribution of `sepal_length` using a filled area plot. 3. **Adjust Bandwidth:** - Create two additional KDE plots with bandwidth adjustments, one with a higher bandwidth and one with a lower bandwidth to show the differences in smoothing. 4. **Density with Histogram:** - Create a plot that includes both a histogram and a KDE line on the same axes. 5. **Grouped KDE:** - Create a KDE plot that shows the density distribution of `sepal_length`, grouped by the `species` of iris. Ensure each group is represented by a different color. 6. **Conditional Density:** - Create a KDE plot showing conditional densities for each `species`, without normalizing the densities across groups. 7. **Integrate Density:** - Create a cumulative density plot for `sepal_length`. **Input and Output Format:** - All the plots should be displayed within the same Jupyter Notebook. - Ensure each plot has appropriate titles, axis labels, and legends where applicable. **Constraints:** - Use `seaborn.objects` for visualizations. - Each plot should be on a separate figure for clarity. ```python # Example Solution Template import seaborn.objects as so from seaborn import load_dataset # Task 1: Load the dataset iris = load_dataset(\\"iris\\") # Task 2: Basic KDE Plot plot1 = so.Plot(iris, x=\\"sepal_length\\").add(so.Area(), so.KDE()) plot1.show() # Task 3: Adjust Bandwidth # Higher Bandwidth plot2 = so.Plot(iris, x=\\"sepal_length\\").add(so.Area(), so.KDE(bw_adjust=1.5)) plot2.show() # Lower Bandwidth plot3 = so.Plot(iris, x=\\"sepal_length\\").add(so.Area(), so.KDE(bw_adjust=0.5)) plot3.show() # Task 4: Density with Histogram plot4 = so.Plot(iris, x=\\"sepal_length\\").add(so.Bars(alpha=0.3), so.Hist(\\"density\\")).add(so.Line(), so.KDE()) plot4.show() # Task 5: Grouped KDE plot5 = so.Plot(iris, x=\\"sepal_length\\", color=\\"species\\").add(so.Area(), so.KDE()) plot5.show() # Task 6: Conditional Density plot6 = so.Plot(iris, x=\\"sepal_length\\", color=\\"species\\").add(so.Area(), so.KDE(common_norm=False)) plot6.show() # Task 7: Integrate Density plot7 = so.PPlot(iris, x=\\"sepal_length\\").add(so.Line(), so.KDE(cumulative=True)) plot7.show() ``` Ensure the code is properly run and the plots are displayed correctly to validate your solution.","solution":"import seaborn.objects as so from seaborn import load_dataset # Task 1: Load the dataset iris = load_dataset(\\"iris\\") # Task 2: Basic KDE Plot def basic_kde_plot(): plot1 = so.Plot(iris, x=\\"sepal_length\\").add(so.Area(), so.KDE()) plot1.show() # Task 3: Adjust Bandwidth # Higher Bandwidth def kde_high_bandwidth(): plot2 = so.Plot(iris, x=\\"sepal_length\\").add(so.Area(), so.KDE(bw_adjust=1.5)) plot2.show() # Lower Bandwidth def kde_low_bandwidth(): plot3 = so.Plot(iris, x=\\"sepal_length\\").add(so.Area(), so.KDE(bw_adjust=0.5)) plot3.show() # Task 4: Density with Histogram def density_with_histogram(): plot4 = so.Plot(iris, x=\\"sepal_length\\").add(so.Bars(alpha=0.3), so.Hist()).add(so.Line(), so.KDE()) plot4.show() # Task 5: Grouped KDE def grouped_kde_plot(): plot5 = so.Plot(iris, x=\\"sepal_length\\", color=\\"species\\").add(so.Area(), so.KDE()) plot5.show() # Task 6: Conditional Density def conditional_density_plot(): plot6 = so.Plot(iris, x=\\"sepal_length\\", color=\\"species\\").add(so.Area(), so.KDE(common_norm=False)) plot6.show() # Task 7: Integrate Density def cumulative_density_plot(): plot7 = so.Plot(iris, x=\\"sepal_length\\").add(so.Line(), so.KDE(cumulative=True)) plot7.show()"},{"question":"Objective: Your task is to create a Python script that automates the creation of a source distribution for a given directory. The script should also allow specifying the distribution format and manage which files are included or excluded using a `MANIFEST.in` file. Description: 1. Write a Python function `create_source_distribution(package_directory: str, formats: List[str]) -> None` that: - Takes the directory of the Python package (`package_directory`) and a list of distribution formats (`formats`). - Generates a source distribution in the specified formats using the `sdist` command. - Creates a `MANIFEST.in` file in the provided package directory that: - Includes all Python source files in the root directory. - Includes any text files (`.txt`) in the `docs/` directory. - Excludes the `build/` directory. 2. The script should be able to run on Unix and Windows platforms. Implementation Details: - Expected input: - `package_directory`: A string representing the directory path of the package (e.g., \\"./mypackage\\") - `formats`: A list of strings representing the desired distribution formats (e.g., [\\"gztar\\", \\"zip\\"]). - Constraints: - Ensure the `MANIFEST.in` file is created in the package directory. - Handle cases where the `sdist` command might fail, providing meaningful error messages. - Expected output: - The function does not return anything but should result in the creation of source distribution files in the specified formats within the package directory. - Example usage: ```python create_source_distribution(\\"./mypackage\\", [\\"gztar\\", \\"zip\\"]) ``` - Performance requirements: - The function should efficiently handle file operations and command execution. Use appropriate error handling for robustness. Hints: - You might find the `subprocess` module useful for running the `sdist` command. - Ensure file and directory paths are properly handled for cross-platform compatibility. Example MANIFEST.in Content: ``` include *.py recursive-include docs *.txt prune build ``` **Good luck!**","solution":"import os from typing import List import subprocess def create_source_distribution(package_directory: str, formats: List[str]) -> None: Creates a source distribution in the specified formats. Args: package_directory (str): The directory of the Python package. formats (List[str]): A list of distribution formats. Raises: RuntimeError: If the sdist command fails. original_cwd = os.getcwd() # Create MANIFEST.in in the package directory manifest_file_path = os.path.join(package_directory, \'MANIFEST.in\') with open(manifest_file_path, \'w\') as manifest_file: manifest_file.write(\\"include *.pyn\\") manifest_file.write(\\"recursive-include docs *.txtn\\") manifest_file.write(\\"prune buildn\\") os.chdir(package_directory) command = [\'python\', \'setup.py\', \'sdist\', \'--formats={}\'.format(\',\'.join(formats))] try: subprocess.check_call(command) except subprocess.CalledProcessError as e: raise RuntimeError(f\\"Failed to create source distribution: {e}\\") finally: os.chdir(original_cwd)"},{"question":"# Objective Demonstrate your understanding of seaborn\'s layout management by visualizing a complex dataset with custom plot configurations. # Problem Statement You are given a dataset containing information about different species of penguins, their measurements, and their habitat islands. Using seaborn\'s `Plot` class, create a comprehensive visualization that showcases various aspects of this data. Your visualization should include: 1. A main plot that displays the relationship between flipper length and body mass for penguins, categorized by their species. Use different colors for different species. 2. A facet grid of subplots showing the distribution of flipper length for each species across the three islands (`Torgersen`, `Biscoe`, and `Dream`). 3. Configure the layout such that: - The main plot and the facet grid fit well within an 8x6 figure. - Use the `constrained` layout engine for better spacing. - The overall figure and individual subplots are adjusted appropriately for visually appealing results when saved or displayed. # Input Format - The dataset provided will be in the form of a CSV file with the following columns: - `species`: Species of the penguin (`Adelie`, `Chinstrap`, `Gentoo`) - `island`: Island where the penguin was found (`Torgersen`, `Biscoe`, `Dream`) - `bill_length_mm`: Length of the bill in millimeters - `bill_depth_mm`: Depth of the bill in millimeters - `flipper_length_mm`: Length of the flipper in millimeters - `body_mass_g`: Body mass in grams # Output - Save the final visualization as `\'penguins_visualization.png\'`. # Constraints - You must use seaborn\'s `so.Plot` class and associated methods to create the visualization. - Ensure the final plot figure is readable and aesthetically pleasing. # Performance Requirements - The code should efficiently handle the dataset provided without running into performance issues. # Example Below is an example of how your code should be structured: ```python import seaborn.objects as so import pandas as pd # Load the dataset penguins = pd.read_csv(\'penguins.csv\') # Main plot p = so.Plot(data=penguins, x=\'flipper_length_mm\', y=\'body_mass_g\', color=\'species\') p = p.add(so.Scatter()) # Facet grid g = p.facet(\'species\', \'island\').layout(engine=\'constrained\') # Adjust layout settings g.layout(size=(8, 6), extent=[0, 0, 1, 1]) # Save the plot g.save(\'penguins_visualization.png\') ``` Your task is to fill in the necessary logic to produce a complete and accurate plot as outlined in the problem statement.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_penguins(csv_path): Visualize penguins dataset based on flipper length, body mass, species, and islands. Args: csv_path: Path to the penguins CSV data file. Saves: A comprehensive visualization to \'penguins_visualization.png\'. # Load dataset penguins = pd.read_csv(csv_path) # Initialize the figure fig = plt.figure(constrained_layout=True, figsize=(8, 6)) gs = fig.add_gridspec(2, 2) # Main plot: relationship between flipper length and body mass ax_main = fig.add_subplot(gs[:, 0]) sns.scatterplot(data=penguins, x=\'flipper_length_mm\', y=\'body_mass_g\', hue=\'species\', ax=ax_main) ax_main.set_title(\'Flipper Length vs Body Mass\') # Facet grid: distribution of flipper length for each species across the islands ax1 = fig.add_subplot(gs[0, 1]) ax2 = fig.add_subplot(gs[1, 1]) sns.histplot(data=penguins[penguins[\'island\'] == \'Torgersen\'], x=\'flipper_length_mm\', hue=\'species\', multiple=\'stack\', ax=ax1) ax1.set_title(\'Flipper Length Distribution on Torgersen\') sns.histplot(data=penguins[penguins[\'island\'] == \'Biscoe\'], x=\'flipper_length_mm\', hue=\'species\', multiple=\'stack\', ax=ax2) ax2.set_title(\'Flipper Length Distribution on Biscoe\') # Use tight layout to adjust spacing plt.tight_layout() # Save the plot fig.savefig(\'penguins_visualization.png\')"},{"question":"Objective The goal of this question is to assess your understanding of how to create and manipulate tensors in PyTorch, as well as how to work with the `torch.Size` class to gather information about tensor dimensions. Problem Statement Write a function named `manipulate_tensor_dimensions` that performs the following tasks: 1. Create a 3-dimensional tensor of size (A, B, C) filled with ones using `torch.ones`, where A, B, and C are input parameters to the function. 2. Obtain the size of the created tensor using the `size` method and store it in a variable. 3. Perform the following operations on the `torch.Size` object: - Obtain the size of the second dimension (index 1). - Calculate the total number of elements in the tensor by multiplying the dimensions. 4. Return a tuple containing the size of the second dimension and the total number of elements. Input - Three integers A, B, and C representing the dimensions of the tensor. Output - A tuple containing two integers: 1. The size of the second dimension of the tensor. 2. The total number of elements in the tensor. Constraints - (1 leq A, B, C leq 1000) Example ```python >>> result = manipulate_tensor_dimensions(10, 20, 30) >>> print(result) (20, 6000) ``` Performance Requirements - The function should efficiently handle the creation and manipulation of tensors with dimensions up to (1000, 1000, 1000). Implementation ```python import torch def manipulate_tensor_dimensions(A, B, C): # Step 1: Create a tensor of size (A, B, C) filled with ones tensor = torch.ones(A, B, C) # Step 2: Obtain the size of the tensor size = tensor.size() # Step 3: Perform operations on the torch.Size object second_dimension_size = size[1] total_elements = size[0] * size[1] * size[2] # Step 4: Return the results as a tuple return (second_dimension_size, total_elements) ``` Ensure your implementation correctly handles the creation, dimension retrieval, and calculation of the total number of elements in the tensor.","solution":"import torch def manipulate_tensor_dimensions(A, B, C): # Step 1: Create a tensor of size (A, B, C) filled with ones tensor = torch.ones(A, B, C) # Step 2: Obtain the size of the tensor size = tensor.size() # Step 3: Perform operations on the torch.Size object second_dimension_size = size[1] total_elements = size[0] * size[1] * size[2] # Step 4: Return the results as a tuple return (second_dimension_size, total_elements)"},{"question":"# Email Message Iterator **Objective:** Implement a function that processes an email message and retrieves all unique email addresses found within its payloads that belong to parts with MIME type `text/plain`. # Detailed Instructions: 1. **Input:** - A string containing the email message. 2. **Output:** - A list of unique email addresses found within `text/plain` parts of the email. # Constraints: - The email parts may contain multiple lines. - The email address should be extracted using regular expressions and must follow the standard email format. The standard format here is considered as `<local-part>@<domain-part>`, where: - `local-part` is not empty and does not contain spaces. - `domain-part` is not empty and does not contain spaces, and must have at least one period (`.`) separating domain levels. # Function Signature: ```python def retrieve_email_addresses(email_str: str) -> list: pass ``` # Example: ```python email_str = MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============7330845974216740156==\\" --===============7330845974216740156== Content-Type: text/plain; charset=\\"utf-8\\" Hello, Please contact us at support@example.com for further information. Best, Support Team --===============7330845974216740156== Content-Type: multipart/digest; boundary=\\"===============8055073366527021137==\\" --===============8055073366527021137== Content-Type: message/rfc822 MIME-Version: 1.0 Content-Type: text/plain; charset=\\"utf-8\\" Hello, Please forward this email to admin@example.com for further inquiry. Thanks, Admin Team --===============8055073366527021137==-- --===============7330845974216740156==-- # Expected Output: [\\"support@example.com\\", \\"admin@example.com\\"] print(retrieve_email_addresses(email_str)) ``` # Notes: - Use the `email` module to parse the provided email string. - Utilize `email.iterators.typed_subpart_iterator` to filter out `text/plain` parts. - Extract email addresses using a regular expression. - Ensure the function only returns unique email addresses in a list. **Good Luck!**","solution":"import re from email import message_from_string from email.iterators import typed_subpart_iterator def retrieve_email_addresses(email_str: str) -> list: Process an email message and retrieve all unique email addresses found within its payloads that belong to parts with MIME type `text/plain`. # Parse the email string email_message = message_from_string(email_str) # Regular expression to extract email addresses email_regex = re.compile(r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\') # Set to store unique email addresses email_addresses = set() # Iterate through all \'text/plain\' parts of the email for part in typed_subpart_iterator(email_message, \'text\', \'plain\'): payload = part.get_payload(decode=True) if payload: text_content = payload.decode(part.get_content_charset() or \'utf-8\') addresses = email_regex.findall(text_content) email_addresses.update(addresses) # Convert set to list and return return list(email_addresses)"},{"question":"# **Netrc File Manipulation and Security Assessment** You are tasked with creating a Python script to read, manipulate, and enhance the security of a `.netrc` file using the `netrc` class. Your implementation should include the following functionalities: 1. **Initialization and Reading**: - Write a function `read_netrc(file=None)` that initializes a `netrc` object. If a file path is provided, it should use that file, otherwise, it should default to the `.netrc` file in the user\'s home directory. - The function should handle `FileNotFoundError` and `NetrcParseError` exceptions gracefully, returning appropriate error messages. 2. **Retrieving Authenticators**: - Write a function `get_authenticator(file, host)` that takes a file path and a host name as arguments, initializes a `netrc` object using the `read_netrc` function, and returns the authenticator tuple `(login, account, password)` for the specified host. - If the host is not found, the function should return the default authenticator or `None` if no default entry exists. 3. **Security Assessment**: - Write a function `assess_security(file)` that checks the file permissions of the `.netrc` file (or the provided file) and ensures it is not accessible to users other than the owner. If insecure, raise an appropriate security risk alert. 4. **Netrc Object Representation**: - Write a function `netrc_to_string(file)` that returns the string representation of the `netrc` object in the netrc file format. # **Constraints and Requirements**: - Passwords may only include ASCII punctuation. - Handle all exceptions appropriately and provide clear error messages. - Ensure to check and enhance file permissions securely. - Your solution should work efficiently even with large netrc files. **Function Definitions**: ```python def read_netrc(file=None): # your implementation here pass def get_authenticator(file, host): # your implementation here pass def assess_security(file): # your implementation here pass def netrc_to_string(file): # your implementation here pass ``` **Expected Input / Output**: ```python # Assuming a .netrc file with the following content: # machine example.com login user1 password pass1 # default login default_user password default_pass print(read_netrc(\'path/to/netrc_file\')) # Output: <netrc.netrc object> print(get_authenticator(\'path/to/netrc_file\', \'example.com\')) # Output: (\'user1\', None, \'pass1\') print(get_authenticator(\'path/to/netrc_file\', \'nonexistent.com\')) # Output: (\'default_user\', None, \'default_pass\') assess_security(\'path/to/netrc_file\') # Output: raises SecurityRiskAlert if permissions are insecure print(netrc_to_string(\'path/to/netrc_file\')) # Output: returns string representation of the netrc object ```","solution":"import os import stat from netrc import netrc, NetrcParseError class SecurityRiskAlert(Exception): pass def read_netrc(file=None): Initializes a netrc object from the given file. If no file is provided, defaults to .netrc in the user\'s home directory. if file is None: file = os.path.join(os.path.expanduser(\'~\'), \'.netrc\') try: return netrc(file) except FileNotFoundError: return \\"Error: .netrc file not found.\\" except NetrcParseError as e: return f\\"Error: Failed to parse .netrc file - {e}\\" def get_authenticator(file, host): Retrieves the authenticator tuple (login, account, password) for the specified host. netrc_obj = read_netrc(file) if isinstance(netrc_obj, str): return netrc_obj auth = netrc_obj.authenticators(host) if auth is not None: return auth else: return netrc_obj.authenticators(\'default\') def assess_security(file): Checks the file permissions of the .netrc file and ensures it is only accessible by the owner. Raises SecurityRiskAlert if permissions are insecure. file_stat = os.stat(file) permissions = stat.S_IMODE(file_stat.st_mode) if permissions & (stat.S_IRWXG | stat.S_IRWXO): raise SecurityRiskAlert(\\"Security Alert: .netrc file permissions are too permissive. It should only be accessible by the owner.\\") def netrc_to_string(file): Returns the string representation of the netrc object in the netrc file format. netrc_obj = read_netrc(file) if isinstance(netrc_obj, str): return netrc_obj with open(file) as f: return f.read()"},{"question":"In this assessment, you will demonstrate your understanding of the `compileall` module in Python by writing a function that compiles the Python files in a given directory under specific constraints. Problem Statement Write a function `compile_python_files` that accepts a directory path and compiles all Python source files (`.py`) within that directory and its subdirectories according to the given parameters. Your function should also exclude files matched by a provided regular expression. Function Signature ```python def compile_python_files(directory: str, maxlevels: int = None, force: bool = False, quiet_level: int = 0, rx_str: str = None, workers: int = 1) -> bool: pass ``` Parameters - **directory (str)**: The path to the directory to compile. - **maxlevels (int, optional)**: The maximum depth of recursion into subdirectories. Defaults to `sys.getrecursionlimit()`. - **force (bool, optional)**: Whether to force recompilation even if timestamps are up to date. Defaults to `False`. - **quiet_level (int, optional)**: The level of quietness for the output: - `0`: Print filenames and other information. - `1`: Only print errors. - `2`: Suppress all output. - **rx_str (str, optional)**: A regular expression string to match files that should be excluded from compilation. If `None`, no files are excluded. Defaults to `None`. - **workers (int, optional)**: Number of worker threads to use for parallel compilation. Defaults to `1`. Returns - **bool**: Return `True` if all files compiled successfully, and `False` otherwise. Constraints 1. The function should use the `compileall.compile_dir` function from the `compileall` module to compile the files. 2. If `rx_str` is provided, use it to create a regular expression pattern object using the `re.compile` function and pass it to `compile_dir` through the `rx` parameter. 3. Handle potential exceptions that may be raised due to invalid directory paths or regex compilation errors. 4. Implement appropriate validation checks for parameters such as `quiet_level` and `workers`. Example ```python import re import compileall def compile_python_files(directory: str, maxlevels: int = None, force: bool = False, quiet_level: int = 0, rx_str: str = None, workers: int = 1) -> bool: if quiet_level not in [0, 1, 2]: raise ValueError(\\"quiet_level should be 0, 1, or 2.\\") if workers < 1: raise ValueError(\\"workers should be at least 1.\\") rx = re.compile(rx_str) if rx_str else None try: return compileall.compile_dir( dir=directory, maxlevels=maxlevels, force=force, quiet=quiet_level, rx=rx, workers=workers ) except Exception as e: print(f\\"Compilation error: {e}\\") return False # Example usage success = compile_python_files(\\"my_python_lib\\", maxlevels=5, force=True, quiet_level=1, rx_str=r\'[/]test_\', workers=2) print(f\\"Compilation success: {success}\\") ``` In this example, all Python source files in the directory \\"my_python_lib\\" and its subdirectories (up to 5 levels deep) are compiled, except for those having \\"test_\\" in their path. The compilation runs with 2 worker threads, forces recompilation of files, and prints errors only.","solution":"import re import compileall import sys def compile_python_files(directory: str, maxlevels: int = None, force: bool = False, quiet_level: int = 0, rx_str: str = None, workers: int = 1) -> bool: Compiles all Python source files (.py) within the given directory and its subdirectories. Parameters: - directory (str): The path to the directory to compile. - maxlevels (int, optional): The maximum depth of recursion into subdirectories. Defaults to sys.getrecursionlimit(). - force (bool, optional): Whether to force recompilation even if timestamps are up to date. Defaults to False. - quiet_level (int, optional): The level of quietness for the output: - 0: Print filenames and other information. - 1: Only print errors. - 2: Suppress all output. - rx_str (str, optional): A regular expression string to match files that should be excluded from compilation. Defaults to None. - workers (int, optional): Number of worker threads to use for parallel compilation. Defaults to 1. Returns: - bool: Return True if all files compiled successfully, and False otherwise. if maxlevels is None: maxlevels = sys.getrecursionlimit() if quiet_level not in [0, 1, 2]: raise ValueError(\\"quiet_level should be 0, 1, or 2.\\") if workers < 1: raise ValueError(\\"workers should be at least 1.\\") rx = re.compile(rx_str) if rx_str else None try: return compileall.compile_dir( dir=directory, maxlevels=maxlevels, force=force, quiet=quiet_level, rx=rx, workers=workers ) except Exception as e: print(f\\"Compilation error: {e}\\") return False"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `objects` module by loading a dataset, creating a pivot table, and generating a custom plot with textual annotations. **Problem Statement:** 1. Load the \'tips\' dataset provided by seaborn. 2. Create a pivot table where the index is the day of the week, the columns are the time of day (\'Lunch\', \'Dinner\'), and the values are the average total bill for that combination. 3. Sort this pivot table based on the average total bill (consider both Lunch and Dinner) for each day. 4. Using seaborn\'s `objects` module, create a bar plot that shows the average total bill for each day during Lunch and Dinner. 5. Add textual annotations for each bar that displays the average total bill and align the text horizontally to the right. Fine-tune the position of the text so that it is not overlapping with the bars. **Input:** - The dataset can be loaded directly using seaborn\'s `load_dataset` method. **Output:** - A bar plot with appropriate annotations as described above. **Constraints:** - Use seaborn\'s `objects` module for creating and customizing the plot. - The pivot table and sorting should be done using pandas operations. **Example Solution:** Here is how the solution should be structured: ```python import seaborn as sns import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Step 1: Load the dataset tips = load_dataset(\\"tips\\") # Step 2: Create pivot table pivot_table = tips.pivot_table(index=\'day\', columns=\'time\', values=\'total_bill\', aggfunc=\'mean\').round(2) # Step 3: Sort based on average total bill pivot_table[\'Average\'] = pivot_table.mean(axis=1) pivot_table = pivot_table.sort_values(\'Average\', ascending=False) # Step 4: Create bar plot plot = ( so.Plot(pivot_table.reset_index(), x=\\"day\\") .add(so.Bar(), so.Agg(\'Lunch\', \'Dinner\'), so.Scale(y=list(pivot_table.index))) .layout(size=(10, 6)) ) # Step 5: Add text annotations plot.add(so.Text(), adjust=dict(h=\'right\')) plot.show() ``` **Notes:** - Ensure all text labels are clearly readable and properly aligned. - The final plot should be displayed using the `show` method.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd # Step 1: Load the dataset def load_dataset(): Load the \'tips\' dataset from seaborn. tips = sns.load_dataset(\\"tips\\") return tips # Step 2: Create pivot table def create_pivot_table(data): Create a pivot table from the dataset with average total bill. pivot_table = data.pivot_table( index=\'day\', columns=\'time\', values=\'total_bill\', aggfunc=\'mean\' ).round(2) return pivot_table # Step 3: Sort based on average total bill def sort_pivot_table(pivot_table): Sort the pivot table based on the average total bill. pivot_table[\'Average\'] = pivot_table.mean(axis=1) sorted_table = pivot_table.sort_values(\'Average\', ascending=False).drop(columns=[\'Average\']) return sorted_table # Step 4-5: Plot the data with annotations def plot_data(pivot_table): Create a bar plot with annotations for the pivot table data. pivot_table = pivot_table.reset_index().melt(id_vars=\'day\', var_name=\'time\', value_name=\'total_bill\') p = ( so.Plot(pivot_table, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\") .add(so.Bar(), so.Agg(), adjust={\\"width\\":0.8}) .scale(y=[0, pivot_table[\'total_bill\'].max() + 5]) ) for index, row in pivot_table.iterrows(): p.add( so.Text(row[\'total_bill\'], x=row[\'day\'], y=row[\'total_bill\'], h=\'right\'), adjust={\\"size\\":10} ) p.show() # Main logic to load, process, and plot the data if __name__ == \\"__main__\\": data = load_dataset() pivot_table = create_pivot_table(data) sorted_pivot_table = sort_pivot_table(pivot_table) plot_data(sorted_pivot_table)"},{"question":"<|Analysis Begin|> The provided documentation is for the \\"copyreg\\" module, which allows registration of functions that aid in the pickling (serialization) and copying of objects. It describes two primary functions: 1. `copyreg.constructor(object)`: Declares an object as a valid constructor. If the object is not callable, it raises a \\"TypeError.\\" 2. `copyreg.pickle(type, function, constructor_ob=None)`: Designates a function as a \\"reduction\\" function for objects of a specified type. The function should return either a string or a tuple and the constructor_ob parameter, if provided, must be callable (though it is now ignored). An example outlines how a class `C` and a corresponding pickling function `pickle_c` can be registered, subsequently influencing the behavior of copying and pickling instances of `C`. With this knowledge, we can design a question that assesses a student\'s understanding of registering custom pickling functions for classes, as well as the behavior and interaction with the `copy` and `pickle` modules. <|Analysis End|> <|Question Begin|> # Custom Serialization Using `copyreg` Module **Objective:** Write a Python class that demonstrates the usage of the `copyreg` module to customize the pickling and copying behavior of its instances. You must define and register a custom reduction function for the class. --- **Specifications:** 1. **Class Definition:** - Create a class `Person` with the following attributes: - `name` (str) - `age` (int) 2. **Constructor Validation:** - Validate that the value for `age` is a positive integer. Raise a `ValueError` otherwise. 3. **Custom Pickle Function:** - Define a custom reduction function `pickle_person` for instances of `Person`. This function should: - Print \\"Pickling a Person instance...\\" when it is called. - Return the `Person` class and a tuple containing the `name` and `age` attributes of the instance. 4. **Register Custom Serialization:** - Use `copyreg.pickle` to register the custom reduction function for the `Person` class. 5. **Copy and Pickle Behavior:** - Demonstrate the copy and pickle behavior by doing the following: - Create an instance of `Person`. - Create a shallow copy of the instance using `copy.copy` and observe the output. - Serialize and deserialize the instance using `pickle.dumps` and `pickle.loads` and observe the output. **Constraints:** - Ensure the solution works for Python 3.10+. --- **Input and Output Format:** - There are no specific input/output functions required. Instead, the implementation should print required output during copying and pickling operations as described above. **Example Usage:** ```python import copyreg import copy import pickle class Person: def __init__(self, name, age): if not isinstance(age, int) or age <= 0: raise ValueError(\\"Age must be a positive integer.\\") self.name = name self.age = age def pickle_person(person_instance): print(\\"Pickling a Person instance...\\") return Person, (person_instance.name, person_instance.age) # Register the custom serialization function copyreg.pickle(Person, pickle_person) # Example operations person = Person(\\"Alice\\", 30) person_copy = copy.copy(person) # This should trigger the custom pickling function person_pickle = pickle.dumps(person) # This should also trigger the custom pickling function person_unpickle = pickle.loads(person_pickle) # Deserialize the object without print statement ``` --- Please implement the described solution in the specified format.","solution":"import copyreg import copy import pickle class Person: def __init__(self, name, age): if not isinstance(age, int) or age <= 0: raise ValueError(\\"Age must be a positive integer.\\") self.name = name self.age = age def pickle_person(person_instance): print(\\"Pickling a Person instance...\\") return Person, (person_instance.name, person_instance.age) # Register the custom serialization function copyreg.pickle(Person, pickle_person) # Example operations for demonstration person = Person(\\"Alice\\", 30) person_copy = copy.copy(person) # This should trigger the custom pickling function person_pickle = pickle.dumps(person) # This should also trigger the custom pickling function person_unpickle = pickle.loads(person_pickle) # Deserialize the object without print statement"},{"question":"**Coding Assessment Question:** **Question Title: Robust Subprocess Management** **Question Description:** You are required to write a function `execute_commands` that runs a series of shell commands using the Python `subprocess` module. This function will offer advanced functionality including capturing outputs, handling timeouts, and managing process errors. **Function Signature:** ```python from typing import List, Tuple, Union def execute_commands(commands: List[str], timeout: int) -> Union[List[Tuple[str, str]], str]: pass ``` **Input:** - `commands`: A list of shell commands (strings) to be executed sequentially. - `timeout`: An integer specifying the maximum time in seconds allowed for each command to complete. **Output:** - Returns a list of tuples. Each tuple represents the result of a command execution and contains the following two strings: - `stdout`: The standard output of the command. - `stderr`: The standard error of the command. - If any command fails (i.e., raises a `CalledProcessError` exception), return a detailed error message as a string that includes the command, exit code, and error output. **Constraints:** - Each `command` must be executed separately. Do not chain commands using shell operators like `;` or `&&`. - Make sure to handle cases where the command times out by including this information in the error message. - Ensure that standard output and error are captured correctly for each command. - Create a function that exhibits proper error handling and resource management. **Example:** ```python commands = [\\"echo \'Hello, World!\'\\", \\"ls non_existent_file\\", \\"echo \'Another command\'\\"] timeout = 2 result = execute_commands(commands, timeout) # If all commands succeed: # result = [(\'Hello, World!n\', \'\'), (\\"\\", \\"ls: cannot access \'non_existent_file\': No such file or directoryn\\"), (\'Another commandn\', \'\')] # If a command fails: # result = \\"Command \'ls non_existent_file\' failed with exit code 2. Error: ls: cannot access \'non_existent_file\': No such file or directoryn\\" ``` **Notes:** - Utilize the `subprocess.run()` function for convenience and reliability. - Use `capture_output=True` to capture each command\'s output and error streams. - Leverage proper exception handling for `CalledProcessError` and `TimeoutExpired`. - You may use any other relevant features of the `subprocess` module as needed to achieve the desired functionality. Write efficient and clean code, adhering to best practices in error handling and resource management, ensuring that no resource leaks occur.","solution":"import subprocess from typing import List, Tuple, Union def execute_commands(commands: List[str], timeout: int) -> Union[List[Tuple[str, str]], str]: results = [] for command in commands: try: completed_process = subprocess.run(command, capture_output=True, text=True, shell=True, timeout=timeout, check=True) results.append((completed_process.stdout, completed_process.stderr)) except subprocess.CalledProcessError as e: return f\\"Command \'{e.cmd}\' failed with exit code {e.returncode}. Error: {e.stderr}\\" except subprocess.TimeoutExpired as e: return f\\"Command \'{e.cmd}\' timed out after {timeout} seconds.\\" return results"},{"question":"You are provided with three sets of target labels, and your task is to transform them using appropriate `scikit-learn` preprocessing classes. Implement functions to transform these labels based on the instructions below: # Instructions 1. **Label Binarization**: - Write a function `binarize_labels(labels)` that takes a list of multiclass labels (e.g., `[1, 2, 6, 4]`) and returns their binarized form using `LabelBinarizer`. - **Input**: A list of integers representing multiclass labels. - **Output**: A binary matrix indicating the presence of each label. Example: ```python labels = [1, 2, 6, 4] print(binarize_labels(labels)) # Output: [[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 1, 0], # [0, 0, 0, 1]] ``` 2. **Multilabel Binarization**: - Write a function `multilabel_binarize(labels)` that takes a list of lists, where each inner list represents the labels for a sample, and returns their binarized form using `MultiLabelBinarizer`. - **Input**: A list of lists with integers representing multilabel targets. - **Output**: A binary indicator matrix. Example: ```python labels = [[2, 3, 4], [2], [0, 1, 3]] print(multilabel_binarize(labels)) # Output: [[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0]] ``` 3. **Label Encoding**: - Write a function `encode_labels(labels)` that takes a list of non-numerical labels (e.g., `[\\"paris\\", \\"tokyo\\", \\"amsterdam\\"]`) and returns their encoded form using `LabelEncoder`. - **Input**: A list of strings representing labels. - **Output**: A list of integers representing the encoded labels. - Also, implement `inverse_encode_labels(encoded_labels, original_labels)` to reverse the encoded labels back to their original form. Example: ```python labels = [\\"paris\\", \\"tokyo\\", \\"amsterdam\\"] encoded = encode_labels(labels) print(encoded) # Output: [1, 2, 0] original = inverse_encode_labels(encoded, labels) print(original) # Output: [\\"paris\\", \\"tokyo\\", \\"amsterdam\\"] ``` # Performance Requirements Your solution should be efficient and correctly utilize the respective scikit-learn classes. # Constraints Follow these constraints while writing your functions: - You must use the Scikit-learn `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder` classes for the transformations. - Assume that the inputs are always valid and conform to the expected types and formats. Implement your functions below: ```python from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def binarize_labels(labels): pass def multilabel_binarize(labels): pass def encode_labels(labels): pass def inverse_encode_labels(encoded_labels, original_labels): pass ```","solution":"from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def binarize_labels(labels): Binarizes a list of multiclass labels. lb = LabelBinarizer() return lb.fit_transform(labels) def multilabel_binarize(labels): Binarizes a list of lists representing multilabel targets. mlb = MultiLabelBinarizer() return mlb.fit_transform(labels) def encode_labels(labels): Encodes a list of non-numerical labels into integers. le = LabelEncoder() return le.fit_transform(labels), le.classes_ def inverse_encode_labels(encoded_labels, classes): Reverses the encoded labels back to their original form. le = LabelEncoder() le.classes_ = classes return le.inverse_transform(encoded_labels)"},{"question":"**Objective:** Implement a custom import system in Python that allows importing Python files dynamically from a specified ZIP archive and lists the modules used by a script within that archive. **Problem Statement:** You are given a ZIP archive containing several Python files. Your task is to: 1. **Dynamically import a specified Python module** from this ZIP archive. 2. **List all the modules** that are used (imported) by a given script within the ZIP archive. You are required to implement two functions: 1. `import_from_zip(zip_path: str, module_name: str) -> Any` – This function should import the module specified by `module_name` from the ZIP file located at `zip_path`. It should return the imported module. 2. `find_modules_in_script(zip_path: str, script_name: str) -> List[str]` – This function should return a list of all module names that are imported within the script specified by `script_name` from the ZIP file located at `zip_path`. **Input:** - `zip_path`: A string representing the path to the ZIP file containing Python scripts. - `module_name`: A string representing the name of the module to be imported from the ZIP file. - `script_name`: A string representing the name of the script file within the ZIP file for which the imported modules need to be listed. **Output:** - For `import_from_zip`, return the imported module object. - For `find_modules_in_script`, return a list of strings, where each string is a module name imported by the script. **Constraints:** - Assume that the ZIP file contains valid Python files. - The `module_name` and `script_name` will match filenames inside the ZIP archive without the `.py` extension. - The ZIP file will be available on the local filesystem. **Performance Requirements:** - The implementation should be efficient enough to handle moderately large ZIP files containing up to 100 Python files. **Example Usage:** ```python zip_path = \'path/to/your/archive.zip\' module_name = \'example_module\' script_name = \'example_script\' # Import a module from the ZIP archive imported_module = import_from_zip(zip_path, module_name) print(imported_module) # List modules used by a script imported_modules = find_modules_in_script(zip_path, script_name) print(imported_modules) ``` **Notes:** - You may find the `zipimport` module useful for importing modules from ZIP archives. - Consider using the `modulefinder` module to analyze the dependencies of a given script. - Ensure to handle exceptions appropriately, providing meaningful error messages if the module or script is not found in the ZIP archive.","solution":"import zipimport import modulefinder from typing import Any, List def import_from_zip(zip_path: str, module_name: str) -> Any: Imports a specified module from a ZIP archive. Args: zip_path (str): The path to the ZIP file. module_name (str): The name of the module to import (without .py extension). Returns: Any: The imported module. try: importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) return module except Exception as e: raise ImportError(f\\"Could not import module \'{module_name}\' from \'{zip_path}\': {str(e)}\\") def find_modules_in_script(zip_path: str, script_name: str) -> List[str]: Finds all modules imported by a specified script within a ZIP archive. Args: zip_path (str): The path to the ZIP file. script_name (str): The name of the script to analyze (without .py extension). Returns: List[str]: A list of module names imported by the script. import os import tempfile import zipfile try: with zipfile.ZipFile(zip_path, \'r\') as zip_file: script_filename = f\\"{script_name}.py\\" with tempfile.TemporaryDirectory() as temp_dir: zip_file.extract(script_filename, temp_dir) script_path = os.path.join(temp_dir, script_filename) finder = modulefinder.ModuleFinder() finder.run_script(script_path) modules = list(finder.modules.keys()) return modules except Exception as e: raise RuntimeError(f\\"Could not analyze script \'{script_name}\' in \'{zip_path}\': {str(e)}\\")"},{"question":"Coding Assessment Question # Objective: Implement an Asynchronous Chat Server and Client using asyncio Streams. # Background: The objective of this exercise is to design and implement a chat server and client that can handle multiple clients asynchronously. # Task: 1. Implement a `ChatServer` class that: - Starts a TCP server which listens for incoming connections. - Broadcasts any received message to all connected clients. - Handles the clients\' connections concurrently. 2. Implement a `ChatClient` class that: - Connects to the chat server. - Sends a message received from the user (standard input) to the server. - Receives and prints out any messages broadcasted by the server. # Requirements: 1. Use the `asyncio` module\'s high-level stream APIs. 2. Each client connection should be handled concurrently using async/await. 3. Implement proper message broadcasting to all connected clients. 4. Handle connections gracefully; ensure all clients can connect and disconnect without impacting other clients. # Input and Output Formats: 1. The `ChatServer` class should be initialized with host and port arguments. 2. The `ChatClient` class should be able to connect to the server specified by host and port input arguments. 3. Example input: User input sends a message in the chat client. 4. Example output: Received messages are printed on each client\'s console. # Constraints: 1. Use only the `asyncio` module for asynchronous operations. 2. Ensure the server can handle at least 10 concurrent client connections efficiently. # Performance Requirements: - The server should handle concurrent connections without noticeable lags or delays. # Example: Server Usage: ```python import asyncio class ChatServer: async def handle_client(self, reader, writer): # Logic to handle incoming client connections and broadcast messages async def start_server(self, host, port): server = await asyncio.start_server(self.handle_client, host, port) async with server: await server.serve_forever() # To run the server: # asyncio.run(ChatServer().start_server(\'127.0.0.1\', 8888)) ``` Client Usage: ```python import asyncio class ChatClient: async def send_message(self, writer): while True: message = input(\\"Enter message: \\") writer.write(message.encode()) await writer.drain() async def receive_message(self, reader): while True: data = await reader.read(100) print(f\'Received: {data.decode()}\') async def start_client(self, host, port): reader, writer = await asyncio.open_connection(host, port) await asyncio.gather( self.send_message(writer), self.receive_message(reader) ) # To connect a client: # asyncio.run(ChatClient().start_client(\'127.0.0.1\', 8888)) ``` # Instructions: - Implement the `handle_client` method in `ChatServer` to manage client connections and broadcast messages. - Implement the `send_message` and `receive_message` methods in `ChatClient` to handle user input and server messages respectively. - Test your implementation by running the server and multiple client instances to ensure correctness.","solution":"import asyncio class ChatServer: def __init__(self): self.clients = [] async def handle_client(self, reader, writer): self.clients.append(writer) try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received: {message}\\") await self.broadcast(message, writer) finally: self.clients.remove(writer) writer.close() await writer.wait_closed() async def broadcast(self, message, sender_writer): for client in self.clients: if client != sender_writer: client.write(message.encode()) await client.drain() async def start_server(self, host, port): server = await asyncio.start_server(self.handle_client, host, port) async with server: await server.serve_forever() class ChatClient: async def send_message(self, writer): while True: message = input(\\"Enter message: \\") writer.write(message.encode()) await writer.drain() async def receive_message(self, reader): while True: data = await reader.read(100) if not data: break print(f\'Received: {data.decode()}\') async def start_client(self, host, port): reader, writer = await asyncio.open_connection(host, port) await asyncio.gather( self.send_message(writer), self.receive_message(reader) ) # Example usage: # To run the server, execute: # asyncio.run(ChatServer().start_server(\'127.0.0.1\', 8888)) # To connect a client, execute: # asyncio.run(ChatClient().start_client(\'127.0.0.1\', 8888))"},{"question":"# Password Authentication System with Crypt Module You are required to implement a password authentication system using the legacy `crypt` module in Python. This system should: 1. Register new users by storing their username and hashed password. 2. Authenticate existing users by comparing provided passwords with stored hashed values. 3. Allow users to change their password. 4. Ensure that the hashing method used is the strongest available on the current platform. 5. Use constant-time comparison to avoid timing attacks. Requirements - Implement the following functions: - `register_user(username: str, password: str) -> None` - `authenticate_user(username: str, password: str) -> bool` - `change_password(username: str, old_password: str, new_password: str) -> bool` Function Specifications 1. `register_user(username: str, password: str) -> None` - Registers a new user by hashing the provided password and storing the username with the hashed password. - Use the strongest available hashing method. 2. `authenticate_user(username: str, password: str) -> bool` - Authenticates a user by comparing the provided password\'s hash with the stored hashed password. - Return `True` if authentication is successful, otherwise `False`. 3. `change_password(username: str, old_password: str, new_password: str) -> bool` - Changes the user\'s password if the old password is correct. - Return `True` if the password is successfully changed, otherwise `False`. Constraints - Do not use any external libraries for password hashing. - Use the strongest available hashing method for password storage. - Implement constant-time comparison using `hmac.compare_digest` for verifying passwords. Example Usage ```python # Example usage of the system register_user(\'alice\', \'wonderland123\') assert authenticate_user(\'alice\', \'wonderland123\') == True assert authenticate_user(\'alice\', \'invalidpassword\') == False assert change_password(\'alice\', \'wonderland123\', \'newpassword789\') == True assert authenticate_user(\'alice\', \'newpassword789\') == True assert authenticate_user(\'alice\', \'wonderland123\') == False ``` Additional Information You may assume that the methods for interacting with the user storage system (e.g., saving and retrieving hashed passwords) are implemented for you and your focus should solely be on the appropriate password hashing and verification using the `crypt` module.","solution":"import crypt import hmac from typing import Dict # In-memory storage for user data, simulate the database. user_db: Dict[str, str] = {} def register_user(username: str, password: str) -> None: Registers a new user by hashing the provided password and storing the username with the hashed password. if username in user_db: raise ValueError(\\"Username already exists\\") hashed_password = crypt.crypt(password, crypt.mksalt(crypt.METHOD_SHA512)) user_db[username] = hashed_password def authenticate_user(username: str, password: str) -> bool: Authenticates a user by comparing the provided password\'s hash with the stored hashed password. if username not in user_db: return False stored_password_hash = user_db[username] provided_password_hash = crypt.crypt(password, stored_password_hash) return hmac.compare_digest(stored_password_hash, provided_password_hash) def change_password(username: str, old_password: str, new_password: str) -> bool: Changes the user\'s password if the old password is correct. if not authenticate_user(username, old_password): return False new_hashed_password = crypt.crypt(new_password, crypt.mksalt(crypt.METHOD_SHA512)) user_db[username] = new_hashed_password return True"},{"question":"You are tasked with creating a utility function to record audio from a microphone device and then immediately play it back on a speaker device using the \\"ossaudiodev\\" module. The function should configure the audio parameters such as format, number of channels, and sample rate as specified. If any errors occur, they should be appropriately handled and reported. **Function Signature:** ```python def record_and_playback(duration: int, format: str = \'AFMT_S16_LE\', nchannels: int = 2, samplerate: int = 44100) -> None: Record audio for a specified duration from the microphone and play it back through the speakers. Parameters: - duration (int): Duration to record and play back in seconds. - format (str): Audio format as supported by the device. Default is \'AFMT_S16_LE\'. - nchannels (int): Number of audio channels. Default is 2 (stereo). - samplerate (int): Sampling rate in samples per second. Default is 44100 (CD quality). Returns: - None Raises: - OSSAudioError for audio-specific errors. - OSError for system call errors. ``` **Requirements:** 1. Use `ossaudiodev.open()` to open the microphone device for reading and the speaker device for writing. 2. Set the audio parameters (format, channels, and sample rate) for both devices using the `setparameters()` method. 3. Record audio from the microphone device for the specified duration. 4. Playback the recorded audio using the speaker device. 5. Handle potential errors gracefully, providing a meaningful error message. **Input and Output:** - The function takes four inputs: - `duration` (int): Specifies the duration of audio to be recorded and played back in seconds. - `format` (str): The audio format to be used. Default is \'AFMT_S16_LE\'. - `nchannels` (int): The number of audio channels. Default is 2 (stereo). - `samplerate` (int): The sample rate in hertz. Default is 44100 (CD quality). - The function does not return any values but should output error messages for any exceptions raised during execution. **Constraints:** - Ensure the environment has OSS-compatible audio devices accessible. - Handle exceptions such as unavailability of the specified audio formats or device errors. **Example Usage:** ```python try: record_and_playback(5, format=\'AFMT_S16_LE\', nchannels=1, samplerate=22050) except ossaudiodev.OSSAudioError as e: print(f\\"Audio Error: {e}\\") except OSError as e: print(f\\"System Error: {e}\\") ``` In this example, the function records 5 seconds of audio from the microphone in monophonic format at a sampling rate of 22050 Hz and plays it back through the speakers.","solution":"import ossaudiodev import time def record_and_playback(duration: int, format: str = \'AFMT_S16_LE\', nchannels: int = 2, samplerate: int = 44100) -> None: Record audio for a specified duration from the microphone and play it back through the speakers. Parameters: - duration (int): Duration to record and play back in seconds. - format (str): Audio format as supported by the device. Default is \'AFMT_S16_LE\'. - nchannels (int): Number of audio channels. Default is 2 (stereo). - samplerate (int): Sampling rate in samples per second. Default is 44100 (CD quality). Returns: - None Raises: - OSSAudioError for audio-specific errors. - OSError for system call errors. try: # Open the audio devices audio_in = ossaudiodev.open(\'r\') audio_out = ossaudiodev.open(\'w\') # Set the audio parameters for both input and output devices audio_in.setparameters(ossaudiodev.__dict__[format], nchannels, samplerate) audio_out.setparameters(ossaudiodev.__dict__[format], nchannels, samplerate) # Calculate the size of the buffer bufsize = samplerate * nchannels * 2 * duration # 2 bytes per sample # Record the audio data print(\\"Recording...\\") audio_data = audio_in.read(bufsize) time.sleep(duration) # Playback the recorded audio data print(\\"Playing back...\\") audio_out.write(audio_data) except ossaudiodev.OSSAudioError as e: print(f\\"Audio Error: {e}\\") except OSError as e: print(f\\"System Error: {e}\\") finally: # Close the audio devices try: audio_in.close() audio_out.close() except Exception as e: print(f\\"Error closing devices: {e}\\")"},{"question":"Working with `torch.futures` Objective: Demonstrate your understanding of the `torch.futures` module by creating, managing, and synchronizing multiple asynchronous `Future` objects in a simulated distributed computation scenario. Problem Statement: You are a software engineer tasked with optimizing the processing of four independent computational tasks, each represented as asynchronous operations. You need to: 1. Create four `Future` objects, each representing an asynchronous task. 2. Use `collect_all` to group these futures and wait for all of them to complete. 3. Ensure that after all tasks are complete, the results of each future are gathered, processed, and printed. Function Signature: ```python import torch from torch.futures import Future, collect_all, wait_all def async_task(task_id: int) -> int: # Simulate a computational task with a result # You can insert sleep or any other asynchronous operation here return task_id + 10 def main(): tasks = [] # Create `Future` objects for 4 tasks for i in range(4): fut = Future() fut.set_result(async_task(i)) tasks.append(fut) # Collect all futures and wait for completion all_futures = collect_all(tasks) wait_all(all_futures) # Once all tasks are complete, gather and print their results results = [fut.value() for fut in all_futures.value()] for result in results: print(result) # Example: # You should see the following output when running `main`: # 10 # 11 # 12 # 13 ``` # Explanation: 1. **Asynchronous Task**: Implement a function `async_task` that simulates a computational task and returns a modified input value. 2. **Future Objects**: Create four `Future` objects by wrapping the results of `async_task` calls. 3. **Collect and Wait**: Use `collect_all` to group the futures and `wait_all` to ensure all tasks complete. 4. **Process Results**: After ensuring all tasks are complete, gather the results from each future and print them. # Constraints: - The `async_task` function must simulate asynchronous computation. - Ensure the solution is scalable and can handle more tasks by adjusting a single parameter (number of tasks). Note: You can simulate the asynchronous nature of `async_task` by introducing delays or using actual async functions and await expressions where necessary.","solution":"import torch from torch.futures import Future, collect_all def async_task(task_id: int) -> int: Simulates a computational task by simply returning task_id + 10. In an actual asynchronous setup, we might include delays or other async operations. return task_id + 10 def main(): tasks = [] # Create `Future` objects for 4 tasks for i in range(4): fut = Future() fut.set_result(async_task(i)) tasks.append(fut) # Collect all futures and wait for completion all_futures = collect_all(tasks) all_futures.wait() # Once all tasks are complete, gather and print their results results = [fut.value() for fut in all_futures.value()] for result in results: print(result) if __name__ == \\"__main__\\": main()"},{"question":"You are provided with a dataset containing information about different species of penguins. Using this dataset, you need to create a series of visualizations to analyze various aspects of the data. 1. **Loading the Dataset** - Load the \\"penguins\\" dataset using seaborn\'s `load_dataset` function. 2. **Single Variable Analysis** - Create a histogram displaying the distribution of penguin flipper lengths. Ensure the plot has: - Bin width of 5 units. - A kernel density estimate (KDE) overlay. - Transparently filled bars. 3. **Comparative Analysis** - Create a layered histogram comparing the flipper lengths across different penguin species. Ensure the plot has: - Each species represented by a different color. - KDE overlays for each species. 4. **Bivariate Analysis** - Create a bivariate histogram (2D histogram) of penguin bill depth vs body mass. - Add a color bar to the plot. 5. **Normalization** - Create a histogram of the bill length variable, normalized to show density, and further divided by the island each penguin is from. Ensure: - Each island is represented with a different color. - KDE overlays for each island. - The normalization scales are not common between islands. # Constraints - Use appropriate labels and titles for each plot. - Ensure plots are displayed on separate figures. # Expected Input and Output - **Input:** None (the dataset is loaded internally using seaborn\'s `load_dataset`). - **Output:** A series of visualizations displayed using matplotlib (or inline in a Jupyter Notebook). # Example ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_penguins(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Single Variable Analysis plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=5, kde=True, fill=True, alpha=0.5) plt.title(\'Distribution of Penguin Flipper Lengths\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') # Comparative Analysis plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kde=True, multiple=\\"layer\\", alpha=0.5) plt.title(\'Comparative Distribution of Penguin Flipper Lengths by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') # Bivariate Analysis plt.figure() sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", cbar=True) plt.title(\'Bivariate Histogram of Bill Depth vs Body Mass\') plt.xlabel(\'Bill Depth (mm)\') plt.ylabel(\'Body Mass (g)\') # Normalization plt.figure() sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"island\\", kde=True, stat=\\"density\\", common_norm=False) plt.title(\'Density Distribution of Bill Length by Island\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Density\') plt.show() # Run the function to generate the plots analyze_penguins() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_penguins(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Single Variable Analysis plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=5, kde=True, fill=True, alpha=0.5) plt.title(\'Distribution of Penguin Flipper Lengths\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') # Comparative Analysis plt.figure() sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kde=True, multiple=\\"layer\\", alpha=0.5) plt.title(\'Comparative Distribution of Penguin Flipper Lengths by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') # Bivariate Analysis plt.figure() sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", cbar=True) plt.title(\'Bivariate Histogram of Bill Depth vs Body Mass\') plt.xlabel(\'Bill Depth (mm)\') plt.ylabel(\'Body Mass (g)\') # Normalization plt.figure() sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"island\\", kde=True, stat=\\"density\\", common_norm=False) plt.title(\'Density Distribution of Bill Length by Island\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Density\') plt.show() # Run the function to generate the plots analyze_penguins()"},{"question":"**Advanced PyTorch Coding Assessment** **Objective**: Demonstrate proficiency in utilizing PyTorch\'s named tensor functionalities by implementing a function that processes and transforms named tensors. # Problem Statement You are given a batch of images represented as a 4-dimensional tensor and corresponding class labels represented as a 2-dimensional tensor. Each dimension in your input tensor has specific semantic meanings (e.g., batch size, channels, height, width). Your task is to perform the following operations on the tensors: 1. Normalize each image in the dataset based on per-channel mean and standard deviation values. 2. Flatten each image into a feature vector while preserving batch information. 3. Create a corresponding label tensor that contains the classes for each image. # Detailed Description Your input consists of: 1. `images` - A 4-dimensional tensor of shape `(B, C, H, W)` where: - `B` is the batch size. - `C` is the number of channels. - `H` is the height of each image. - `W` is the width of each image. - Example: `images.names` = (\'N\', \'C\', \'H\', \'W\') 2. `labels` - A 2-dimensional tensor of shape `(B, K)` where: - `B` is the batch size. - `K` is the number of classes possible. 3. `mean` - A 1-dimensional tensor of shape `(C,)` containing the mean value for each channel. 4. `std` - A 1-dimensional tensor of shape `(C,)` containing the standard deviation for each channel. # Tasks 1. **Normalize Images**: Normalize the `images` tensor using the provided `mean` and `std` tensors. - Normalization formula: `normalized_images = (images - mean) / std` - Ensure that the normalization is performed per channel. 2. **Flatten Images**: Flatten the normalized images into a feature vector of shape `(B, C*H*W)` while maintaining the batch dimension. - Example: `(N, C*H*W)` 3. **Align Labels**: Ensure the labels tensor is properly aligned with the batch dimension to potentially match your normalized and flattened images tensor. # Constraints - `1 <= B <= 1024` - `1 <= C <= 256` - `1 <= H, W <= 1024` # Function Signature ```python import torch def process_images_and_labels(images: torch.Tensor, labels: torch.Tensor, mean: torch.Tensor, std: torch.Tensor) -> tuple: Parameters: images (torch.Tensor): 4D input tensor of shape (B, C, H, W) with named dimensions. labels (torch.Tensor): 2D input tensor of shape (B, K). mean (torch.Tensor): 1D tensor of shape (C,). std (torch.Tensor): 1D tensor of shape (C,). Returns: tuple: A tuple containing: - normalized_flattened_images (torch.Tensor): 2D tensor of shape (B, C*H*W) with named dimensions. - aligned_labels (torch.Tensor): The input labels tensor aligned with the batch dimension. pass ``` # Example ```python # Example inputs images = torch.randn(4, 3, 32, 32, names=(\'N\', \'C\', \'H\', \'W\')) labels = torch.randint(0, 10, (4, 10)) mean = torch.tensor([0.485, 0.456, 0.406]) std = torch.tensor([0.229, 0.224, 0.225]) # Function call normalized_flattened_images, aligned_labels = process_images_and_labels(images, labels, mean, std) # Output details print(normalized_flattened_images.shape) # torch.Size([4, 3072]) print(aligned_labels.shape) # torch.Size([4, 10]) ``` Implement the function `process_images_and_labels` to perform the outlined tasks.","solution":"import torch def process_images_and_labels(images: torch.Tensor, labels: torch.Tensor, mean: torch.Tensor, std: torch.Tensor) -> tuple: Parameters: images (torch.Tensor): 4D input tensor of shape (B, C, H, W) with named dimensions. labels (torch.Tensor): 2D input tensor of shape (B, K). mean (torch.Tensor): 1D tensor of shape (C,). std (torch.Tensor): 1D tensor of shape (C,). Returns: tuple: A tuple containing: - normalized_flattened_images (torch.Tensor): 2D tensor of shape (B, C*H*W) with named dimensions. - aligned_labels (torch.Tensor): The input labels tensor aligned with the batch dimension. # Normalize the images mean = mean[None, :, None, None] # Shape (1, C, 1, 1) for broadcasting std = std[None, :, None, None] # Shape (1, C, 1, 1) for broadcasting normalized_images = (images - mean) / std # Flatten the images B, C, H, W = normalized_images.shape normalized_flattened_images = normalized_images.view(B, C * H * W) # Align labels aligned_labels = labels return normalized_flattened_images, aligned_labels"},{"question":"**Problem Statement: Concurrent Web Scraping with Retry Logic** You are tasked with implementing a Python function using asyncio to perform concurrent web scraping. Your function will take a list of URLs to scrape concurrently and retry fetching each URL up to a specified number of times in case of a failure. The function must: 1. Fetch data from each URL concurrently. 2. Retry fetching a URL up to `max_retries` times if an exception occurs. 3. Return a dictionary where keys are URLs and values are the fetched data or `None` if all retries fail. **Function Signature:** ```python import asyncio async def fetch_urls(urls: list, max_retries: int) -> dict: pass ``` # Input: - `urls`: A list of strings, where each string is a URL to fetch. - `max_retries`: An integer specifying the maximum number of retry attempts for fetching a URL. # Output: - A dictionary where keys are URLs and values are either the fetched data as strings or `None` if all retries fail. # Constraints: - You must use `asyncio` for concurrency. - Use `asyncio.gather()` to fetch URLs concurrently. - Use `asyncio.sleep()` to simulate network delays or retries. - You should not use blocking code. # Example: ```python import asyncio async def fetch_data(url): # Simulate fetch with sleep, replace with actual fetch logic await asyncio.sleep(1) if url == \\"http://example.com/bad\\": raise Exception(\\"Failed to fetch data\\") return f\\"Data from {url}\\" async def fetch_urls(urls: list, max_retries: int) -> dict: async def fetch_with_retries(url): for attempt in range(max_retries): try: return await fetch_data(url) except Exception as e: if attempt < max_retries - 1: await asyncio.sleep(1) # Backoff before retry return None # Return None if all retries fail tasks = [asyncio.create_task(fetch_with_retries(url)) for url in urls] results = await asyncio.gather(*tasks) return dict(zip(urls, results)) # Testing the function urls_to_fetch = [\\"http://example.com/good1\\", \\"http://example.com/good2\\", \\"http://example.com/bad\\"] result = asyncio.run(fetch_urls(urls_to_fetch, 3)) print(result) ``` # Expected Output: If, for example, fetching \\"http://example.com/bad\\" keeps failing, the output might look like: ```python { \\"http://example.com/good1\\": \\"Data from http://example.com/good1\\", \\"http://example.com/good2\\": \\"Data from http://example.com/good2\\", \\"http://example.com/bad\\": None } ``` # Notes: - Replace the `fetch_data` function with actual network call logic. - Ensure the solution handles rejections and retries efficiently using asyncio functionalities.","solution":"import asyncio async def fetch_data(url): # Simulated fetch logic, replace with actual network request await asyncio.sleep(1) # Simulate network delay if url == \\"http://example.com/bad\\": raise Exception(f\\"Failed to fetch data from {url}\\") return f\\"Data from {url}\\" async def fetch_with_retries(url, max_retries): Fetch data from a URL with retry logic. for attempt in range(max_retries): try: return await fetch_data(url) except Exception: if attempt < max_retries - 1: await asyncio.sleep(1) # Backoff before retry return None # Return None if all retries fail async def fetch_urls(urls: list, max_retries: int) -> dict: Fetch multiple URLs concurrently with a specified number of retries. tasks = [asyncio.create_task(fetch_with_retries(url, max_retries)) for url in urls] results = await asyncio.gather(*tasks) return dict(zip(urls, results))"},{"question":"**Question:** You are given a dataset representing various species of flowers along with a column indicating their popularity score. Your task is to create a heatmap that visualizes the popularity scores in a gradient color scheme moving from dark gray to a specified color using seaborn\'s `dark_palette` function. # Instructions: 1. Create a function `create_heatmap(data, color, num_colors)` that takes in the following parameters: - `data` (pd.DataFrame): DataFrame containing the flower species and their popularity scores. - `color` (string/tuple): A color to blend with a dark gray, specified either by a named color, hex code, or husl tuple. - `num_colors` (int): The number of discrete colors to include in the palette. 2. The function should: - Generate a sequential color palette to the specified color using `seaborn.dark_palette`. - Use this palette to plot a heatmap of the popularity scores stored in the DataFrame. # Constraints: - The DataFrame `data` will always contain two columns: \'Species\' (flower species) and \'Popularity\' (popularity scores). - The \'Popularity\' column contains integer values ranging from 0 to 100. - You should use seaborn to create the heatmap. # Expected Input and Output Formats: **Input:** ```python df = pd.DataFrame({ \'Species\': [\'Rose\', \'Tulip\', \'Sunflower\', \'Lily\', \'Daisy\'], \'Popularity\': [85, 65, 90, 75, 50] }) create_heatmap(df, \\"#79C\\", 6) ``` **Output:** The function should display a heatmap with the rows labeled by \'Species\' and the \'Popularity\' values represented by colors from dark gray to the specified color. # Function Signature: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_heatmap(data: pd.DataFrame, color: str, num_colors: int) -> None: # Your code here ``` **Example:** ```python df_example = pd.DataFrame({ \'Species\': [\'Rose\', \'Tulip\', \'Sunflower\', \'Lily\', \'Daisy\'], \'Popularity\': [85, 65, 90, 75, 50] }) create_heatmap(df_example, \\"#79C\\", 6) ``` The above code should display a heatmap of the popularity scores with a sequential color ramp from dark gray to the color coded by `#79C`.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_heatmap(data: pd.DataFrame, color: str, num_colors: int) -> None: Creates a heatmap visualizing the popularity scores of various species with a gradient color scheme. Parameters: data (pd.DataFrame): DataFrame containing the flower species and their popularity scores. color (str/tuple): A color to blend with a dark gray, specified either by a named color, hex code, or husl tuple. num_colors (int): The number of discrete colors to include in the palette. # Ensure \'Species\' column is the index for heatmap plotting data = data.set_index(\'Species\') # Generate the dark_palette palette = sns.dark_palette(color, n_colors=num_colors, reverse=True, as_cmap=True) # Plot the heatmap with the generated palette plt.figure(figsize=(10, 1)) # Adjust the size as appropriate sns.heatmap(data.T, cmap=palette, cbar=True, linewidths=.5, annot=True, fmt=\\"d\\") plt.show()"},{"question":"**Question:** # Task: You are required to write a Python function that reads an existing Sun AU audio file, processes the audio data by converting it from mono to stereo, and then writes the processed data to a new AU audio file. # Function Signature: ```python def convert_mono_to_stereo(input_filename: str, output_filename: str) -> None: pass ``` # Input: - `input_filename` (str): The name of the input mono AU file. - `output_filename` (str): The name of the output stereo AU file. # Processing Requirements: 1. **Read the AU file**: Open the provided mono AU file (`input_filename`) in read mode. - Extract relevant parameters such as number of channels, sample width, frame rate, and entire audio data. 2. **Verify Mono Channel**: Ensure the input file is indeed mono (i.e., has only one channel). 3. **Create Stereo Data**: Duplicate the mono audio channel data to create a stereo effect. 4. **Write to New AU File**: Open the `output_filename` in write mode, set the parameters to reflect stereo output, and write the newly created stereo audio data. # Output: - The function does not return anything but creates a new AU file with the modified audio data. # Constraints: - The input AU file will always be in one of the supported encoding formats by `sunau` (e.g., `MULAW`, `LINEAR`). # Example: Suppose there is a mono AU file `input.au`. After running `convert_mono_to_stereo(\'input.au\', \'output.au\')`, a new AU file `output.au` should be created with the same audio content but in stereo format. # Additional Notes: - You can assume that the input file exists and is accessible. - Ensure proper handling of file opening/closing to avoid resource leaks. - The function should utilize the appropriate methods provided by the `sunau` module. # Reference: For guidance on using the `sunau` module, refer to the partial `sunau` documentation provided. ```python import sunau def convert_mono_to_stereo(input_filename: str, output_filename: str) -> None: with sunau.open(input_filename, \'r\') as infile: assert infile.getnchannels() == 1, \\"Input file is not mono\\" params = infile.getparams() framerate = infile.getframerate() sampwidth = infile.getsampwidth() nframes = infile.getnframes() data = infile.readframes(nframes) stereo_data = bytearray() for i in range(0, len(data), sampwidth): frame = data[i:i + sampwidth] stereo_data.extend(frame * 2) # Duplicate each frame with sunau.open(output_filename, \'w\') as outfile: outfile.setnchannels(2) # Set to stereo outfile.setsampwidth(sampwidth) outfile.setframerate(framerate) outfile.setnframes(nframes) outfile.setcomptype(params.comptype, params.compname) outfile.writeframes(bytes(stereo_data)) ```","solution":"import sunau def convert_mono_to_stereo(input_filename: str, output_filename: str) -> None: with sunau.open(input_filename, \'r\') as infile: assert infile.getnchannels() == 1, \\"Input file is not mono\\" params = infile.getparams() framerate = infile.getframerate() sampwidth = infile.getsampwidth() nframes = infile.getnframes() data = infile.readframes(nframes) stereo_data = bytearray() for i in range(0, len(data), sampwidth): frame = data[i:i + sampwidth] stereo_data.extend(frame + frame) # Duplicate each frame with sunau.open(output_filename, \'w\') as outfile: outfile.setnchannels(2) # Set to stereo outfile.setsampwidth(sampwidth) outfile.setframerate(framerate) outfile.setnframes(nframes) outfile.setcomptype(params.comptype, params.compname) outfile.writeframes(bytes(stereo_data))"},{"question":"# Custom Python Object with Attribute Management Given your understanding of how to define and handle custom types in Python, implement a custom Python class `CustomObject` which includes the following functionality: 1. **Initialization**: Your class should initialize with an attribute called `data` which stores an integer value. 2. **Representation Methods**: - Implement the `__repr__` method which returns a string in the format `\\"Repr-CustomObject{data:<value>}\\"`. - Implement the `__str__` method which returns a string in the format `\\"Stringified-CustomObject{data:<value>}\\"`. 3. **Attribute Management**: - Implement custom logic for attribute access. Specifically, create custom `__getattr__` and `__setattr__` methods: - `__getattr__` should check for attribute names \'size\' and \'age\'. If these attributes are requested, it should compute and return them as twice the value of `data` and thrice the value of `data` respectively. For any other attribute, it should raise an `AttributeError`. - `__setattr__` should allow setting the `data` attribute but should raise a `RuntimeError` if any other attribute is attempted to be set. # Constraints - The `data` attribute should only accept positive integer values. - Your class should use the `__slots__` mechanism to limit attribute storage to only `data`. # Function Signature ```python class CustomObject: __slots__ = [\'data\'] def __init__(self, data: int): pass def __repr__(self): pass def __str__(self): pass def __getattr__(self, name: str): pass def __setattr__(self, name: str, value: int): pass ``` # Example ```python obj = CustomObject(5) print(repr(obj)) # Output: \\"Repr-CustomObject{data:5}\\" print(str(obj)) # Output: \\"Stringified-CustomObject{data:5}\\" print(obj.size) # Output: 10 print(obj.age) # Output: 15 obj.data = 10 print(obj.size) # Output: 20 try: obj.age = 10 # This should raise a RuntimeError except RuntimeError as e: print(e) # Output: \'Cannot set attribute other than data\' try: print(obj.height) # This should raise an AttributeError except AttributeError as e: print(e) # Output: \\"\'CustomObject\' object has no attribute \'height\'\\" ``` Your task is to implement the `CustomObject` class as described.","solution":"class CustomObject: __slots__ = [\'data\'] def __init__(self, data: int): if not isinstance(data, int) or data <= 0: raise ValueError(\\"data must be a positive integer\\") self.data = data def __repr__(self): return f\\"Repr-CustomObject{{data:{self.data}}}\\" def __str__(self): return f\\"Stringified-CustomObject{{data:{self.data}}}\\" def __getattr__(self, name: str): if name == \'size\': return self.data * 2 elif name == \'age\': return self.data * 3 else: raise AttributeError(f\\"\'CustomObject\' object has no attribute \'{name}\'\\") def __setattr__(self, name: str, value: int): if name == \'data\': super().__setattr__(name, value) else: raise RuntimeError(\'Cannot set attribute other than data\')"},{"question":"# Exercise: Advanced Seaborn Plot Customization Objective The purpose of this exercise is to assess your ability to use the `seaborn.objects` module to create complex and customized plots in Python. You will be asked to create a multi-panel plot that demonstrates various concepts of coordinate properties, color properties, and more as described. Instructions 1. Generate some synthetic data to be used in the plots. 2. Create a multi-panel plot using the `seaborn.objects` module. 3. Each panel should demonstrate a different property or combination of properties of the marks in Seaborn. # Data - Use `numpy` to generate the following datasets: - Two sets of 100 random data points: one following a normal distribution, and one following a uniform distribution. - Create a datetime range of 100 days starting from \'2020-01-01\'. # Panels and Customization 1. **Panel 1**: Scatter plot with **coordinate properties**. - x: Data following a normal distribution. - y: Data following a uniform distribution. - Include custom `xmin`, `xmax`, `ymin`, `ymax`. 2. **Panel 2**: A line plot demonstrating **color properties**. - x: Date range. - y: Data following a normal distribution. - Customize `color`, `fillcolor`, and `edgecolor`. 3. **Panel 3**: A bar plot using **alpha properties**. - x: Categorical data points. - y: Data following a uniform distribution. - Vary `alpha`, `fillalpha`, and `edgealpha`. 4. **Panel 4**: Dot plot with **style properties**. - x: Data following a normal distribution. - y: Data following a uniform distribution. - Customize `marker`, `fill`, `linestyle`, and `edgestyle`. 5. **Panel 5**: Text plot demonstrating **text properties**. - x: A small numerical range. - y: Fixed at 0. - Customize `fontsize`, `halign`, `valign`, `offset`. # Expectation - You should create an organized multi-panel plot (using Facet or individual subplots). - Make sure to use themes and limit functions wherever applicable to ensure clean and readable output. - Each panel should have appropriate labels and titles. # Constraints - Write clean and readable code. - Include enough comments to explain your code and the choices you made. - Ensure the plots are aesthetically pleasing and accurately reflect the described properties. ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn.objects as so from seaborn import axes_style # Generate synthetic data np.random.seed(0) # For reproducibility normal_data = np.random.normal(size=100) uniform_data = np.random.uniform(size=100) dates = pd.date_range(start=\'2020-01-01\', periods=100) # Create the first panel plot here using seaborn objects # Scatter plot with coordinate properties # ... # Create the second panel plot here using seaborn objects # Line plot with color properties # ... # Create the third panel plot here using seaborn objects # Bar plot with alpha properties # ... # Create the fourth panel plot here using seaborn objects # Dot plot with style properties # ... # Create the fifth panel plot here using seaborn objects # Text plot with text properties # ... # Display all the plots in a multi-panel layout plt.show() ``` # Submission Submit your code and the resulting plot figures as images. Ensure your code is well-commented and clean.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn.objects as so # Generate synthetic data np.random.seed(0) # For reproducibility normal_data = np.random.normal(size=100) uniform_data = np.random.uniform(size=100) dates = pd.date_range(start=\'2020-01-01\', periods=100) categories = [\'A\', \'B\', \'C\', \'D\', \'E\'] * 20 # Create subplots fig, axs = plt.subplots(3, 2, figsize=(15, 15)) fig.tight_layout(pad=5.0) # Panel 1: Scatter plot with coordinate properties axs[0, 0].scatter(normal_data, uniform_data) axs[0, 0].set_xlim(-3, 3) axs[0, 0].set_ylim(0, 1) axs[0, 0].set_title(\'Scatter plot with coordinate properties\') axs[0, 0].set_xlabel(\'Normal Distribution\') axs[0, 0].set_ylabel(\'Uniform Distribution\') # Panel 2: Line plot with color properties axs[0, 1].plot(dates, normal_data, color=\'blue\', linestyle=\'-\', linewidth=2) axs[0, 1].fill_between(dates, normal_data, color=\'lightblue\', alpha=0.3) axs[0, 1].set_title(\'Line plot with color properties\') axs[0, 1].set_xlabel(\'Date\') axs[0, 1].set_ylabel(\'Normal Distribution\') # Panel 3: Bar plot with alpha properties unique_categories, counts_categories = np.unique(categories, return_counts=True) axs[1, 0].bar(unique_categories, counts_categories, alpha=0.7, color=\'green\', edgecolor=\'darkgreen\') axs[1, 0].set_title(\'Bar plot with alpha properties\') axs[1, 0].set_xlabel(\'Categories\') axs[1, 0].set_ylabel(\'Counts\') # Panel 4: Dot plot with style properties axs[1, 1].plot(normal_data, uniform_data, \'o\', markersize=10, markerfacecolor=\'red\', markeredgewidth=2, markeredgecolor=\'black\') axs[1, 1].set_title(\'Dot plot with style properties\') axs[1, 1].set_xlabel(\'Normal Distribution\') axs[1, 1].set_ylabel(\'Uniform Distribution\') # Panel 5: Text plot with text properties for idx, val in enumerate(np.linspace(0, 1, 10)): axs[2, 0].text(idx, 0.5, f\'Text {idx}\', fontsize=14, ha=\'center\', va=\'center\', rotation=45) axs[2, 0].set_title(\'Text plot with text properties\') axs[2, 0].set_xlim(-1, 10) axs[2, 0].set_ylim(0, 1) axs[2, 0].set_xticks([]) axs[2, 0].set_yticks([]) # Turn off the 6th subplot (if using a 3x2 layout) axs[2, 1].axis(\'off\') plt.show()"},{"question":"# **Python Coding Assessment: Automating Package Distribution** **Objective:** Design a function that automates the process of listing all Python files in a given directory and subdirectories, and then generates a setup script for distribution using Distutils. **Problem Statement:** You are tasked with creating a Python function, `generate_setup_script`, that automates the process of setting up a Python package for distribution. Your function will: 1. **Recursively scan** a specified directory and its subdirectories for all `.py` files. 2. **Generate** a `setup.py` script, which can be used by Distutils to create a source distribution. The `setup.py` script should include: - The name of the package (provided as an argument). - The version of the package (default to `1.0`). - A list of the found Python modules. - Basic metadata such as author name, author email, and description (provided as arguments). **Function Signature:** ```python def generate_setup_script(directory: str, package_name: str, author: str, author_email: str, description: str, version: str = \'1.0\') -> str: pass ``` # **Input:** - `directory`: A string representing the path to the main directory to scan for Python modules. - `package_name`: A string representing the name of the package. - `author`: A string representing the author\'s name. - `author_email`: A string representing the author\'s email. - `description`: A string providing a brief description of the package. - `version`: An optional string representing the package version, defaults to \'1.0\'. # **Output:** - A string containing the content of `setup.py`. # **Constraints:** - Only `.py` files should be listed as modules. - The `setup.py` script generated must be valid and executable by Distutils to create a package. # **Example:** ```python directory = \'./my_package\' package_name = \'my_package\' author = \'John Doe\' author_email = \'john.doe@example.com\' description = \'My sample package for demonstration.\' result = generate_setup_script(directory, package_name, author, author_email, description) print(result) ``` # **Expected Output:** ```python from distutils.core import setup setup( name=\'my_package\', version=\'1.0\', packages=[\'my_package\'], py_modules=[\'module1\', \'module2\'], author=\'John Doe\', author_email=\'john.doe@example.com\', description=\'My sample package for demonstration.\', ) ``` (Note: The actual module names should reflect the `.py` files found in the specified directory.) # **Performance Requirements:** - The function should be able to handle large directories with multiple subdirectories efficiently. - Proper error handling should be implemented for cases such as non-existent directories, missing information, or invalid paths. **Good luck!** This problem tests your understanding of file operations, string manipulation, and integrating Distutils for package management in Python.","solution":"import os def generate_setup_script(directory: str, package_name: str, author: str, author_email: str, description: str, version: str = \'1.0\') -> str: def get_py_modules(directory): py_modules = [] for root, _, files in os.walk(directory): for file in files: if file.endswith(\'.py\'): module_name = os.path.splitext(os.path.relpath(os.path.join(root, file), directory))[0].replace(os.sep, \'.\') py_modules.append(module_name) return py_modules py_modules = get_py_modules(directory) setup_script = f from distutils.core import setup setup( name=\'{package_name}\', version=\'{version}\', packages={[]}, py_modules={py_modules}, author=\'{author}\', author_email=\'{author_email}\', description=\'{description}\', ) return setup_script.strip()"},{"question":"Objective Demonstrate proficiency in manipulating environment variables using the \\"os\\" module and understanding error handling associated with system calls. Problem Statement Create a Python function using the \\"os\\" module, named `modify_and_fetch_env`, which performs the following tasks: 1. **Input**: Takes two arguments: - `to_add`: A dictionary where keys are string names of environment variables to set, and values are their respective string values to set. - `to_fetch`: A list of string names of environment variables to fetch their values after setting the variables specified in `to_add`. 2. **Processing**: - Update the environment variables in `os.environ` with the key-value pairs from `to_add`. - Fetch the values of the environment variables specified in `to_fetch` from `os.environ` and store their values in a dictionary. If an environment variable in `to_fetch` does not exist, the corresponding value in the dictionary should be `None`. 3. **Output**: Return a tuple containing two elements: - A dictionary that represents the updated environment variables fetched as specified in `to_fetch`. - A string indicating success with \\"Success\\" or an error message if an exception occurs during the process. Function Signature ```python def modify_and_fetch_env(to_add: dict, to_fetch: list) -> tuple: ``` Constraints - All strings in the input dictionary keys, values, and list items must be non-empty. - Assume the environment variables to add and fetch will not exceed a combined total of 50 items. Example ```python to_add = {\'NEW_VAR\': \'123\', \'EXAMPLE_VAR\': \'test\'} to_fetch = [\'NEW_VAR\', \'EXAMPLE_VAR\', \'NON_EXISTENT_VAR\'] result = modify_and_fetch_env(to_add, to_fetch) print(result) ``` Expected Output: ```python ({\'NEW_VAR\': \'123\', \'EXAMPLE_VAR\': \'test\', \'NON_EXISTENT_VAR\': None}, \'Success\') ``` Notes - Make sure to handle and return any exceptions that may arise during the update and fetch process in the output string. - Use the `os` module functions to manipulate and fetch environment variables.","solution":"import os def modify_and_fetch_env(to_add: dict, to_fetch: list) -> tuple: try: # Set the environment variables from to_add for key, value in to_add.items(): os.environ[key] = value # Fetch the environment variables specified in to_fetch result = {} for key in to_fetch: result[key] = os.environ.get(key) return (result, \'Success\') except Exception as e: return ({}, f\'Error: {str(e)}\')"},{"question":"As a data scientist, you have been given a dataset containing information about different species of penguins. Your task is to use the Seaborn library to visualize different characteristics of these species and customize the plots. You need to demonstrate familiarity with both basic and advanced Seaborn functionalities. Your solution should meet the following requirements: # Task 1. Load the penguins dataset from Seaborn. 2. Create a scatter plot to show the relationship between flipper length and body mass, with different species identified by color. 3. Customize the plot aesthetics using Seaborn themes. 4. Add a regression line to the scatter plot. 5. Create a facet grid to show separate scatter plots for each island, displaying the same relationship as above. 6. Adjust the legend position and styling for both the main scatter plot and the facet grid. # Input and Output Format - **Input**: No direct input. The data should be loaded directly using Seaborn\'s in-built dataset. - **Output**: Visualizations (scatter plots with legends). # Constraints - Use the Seaborn library for creating plots. - Customize the legends using methods described in the documentation. # Requirements 1. Ensure the scatter plot displays a clear distinction between species using different colors. 2. Apply a Seaborn theme of your choice to the plots. 3. Add a regression line to the scatter plots. 4. Use a facet grid to create separate plots based on the \'island\' column. 5. Relocate the legends to avoid overlapping with data points in the scatter plots. 6. Modify the legends in the facet grid similarly. # Example Code Structure ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Create a scatter plot for flipper length vs. body mass, colored by species sns.set_theme() scatter_plot = sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") # 3. Customize plot aesthetics sns.set(style=\\"whitegrid\\") # 4. Add a regression line sns.regplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", scatter=False, ax=scatter_plot) # 5. Create a facet grid for separate plots by island facet_grid = sns.FacetGrid(penguins, col=\\"island\\", hue=\\"species\\") facet_grid.map(sns.scatterplot, \\"flipper_length_mm\\", \\"body_mass_g\\") facet_grid.add_legend() # 6. Adjust legend positions sns.move_legend(scatter_plot, \\"upper right\\") sns.move_legend(facet_grid, \\"upper right\\", bbox_to_anchor=(0.5, 1), frameon=False) plt.show() ``` # Notes - Ensure that each plot and facet grid is correctly formatted and shows the required information clearly. - Legends should be placed in a way that makes the plots easy to interpret without obscuring key data points.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_data(): # 1. Load the dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Create a scatter plot for flipper length vs. body mass, colored by species sns.set_theme() scatter_plot = sns.scatterplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\" ) # 3. Customize plot aesthetics sns.set_style(\\"whitegrid\\") # 4. Add a regression line sns.regplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", scatter=False, ax=scatter_plot ) # 5. Create a facet grid for separate plots by island facet_grid = sns.FacetGrid(penguins, col=\\"island\\", hue=\\"species\\") facet_grid.map(sns.scatterplot, \\"flipper_length_mm\\", \\"body_mass_g\\") facet_grid.add_legend() # 6. Adjust legend positions scatter_plot.legend(loc=\'upper right\', bbox_to_anchor=(1.15, 1), frameon=False) for ax in facet_grid.axes.flatten(): handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, loc=\'upper right\', bbox_to_anchor=(1.15, 1), frameon=False) plt.show() # Call the function to execute the visualization visualize_penguin_data()"},{"question":"# Question: Creating Custom Color Palettes with Seaborn As a data visualization specialist, you are required to design several color palettes for different projects. Your task is to write Python functions to generate and visualize custom light color palettes using the Seaborn library. Each function should follow the specifications given below: 1. **Function: `create_light_palette_by_name`** - **Input:** - `color_name` (string): The name of the color to generate the palette. - `num_colors` (int): The number of colors in the palette (default is 6). - **Output:** - List of RGB tuples representing the light color palette. - **Constraints:** - `num_colors` should be between 1 and 10. - **Example Usage:** ```python palette = create_light_palette_by_name(\\"seagreen\\", 8) print(palette) ``` 2. **Function: `create_light_palette_by_hex`** - **Input:** - `hex_code` (string): The hex code of the color to generate the palette. - `as_cmap` (bool): Whether to return a continuous colormap (default is False). - **Output:** - If `as_cmap` is `False`, returns a list of RGB tuples representing the light color palette. - If `as_cmap` is `True`, returns a continuous colormap object. - **Example Usage:** ```python palette = create_light_palette_by_hex(\\"#79C\\", as_cmap=True) print(palette) ``` 3. **Function: `create_light_palette_by_husl`** - **Input:** - `husl_values` (tuple): A tuple of three floats representing (H, S, L) values. - `num_colors` (int): The number of colors in the palette (default is 6). - **Output:** - List of RGB tuples representing the light color palette. - **Constraints:** - H should be between 0 and 360. - S and L should be between 0 and 100. - `num_colors` should be between 1 and 10. - **Example Usage:** ```python palette = create_light_palette_by_husl((20, 60, 50), 8) print(palette) ``` For visualization, you can use any plotting library of your choice (e.g., matplotlib) to show the differences in the generated palettes. Ensure the functions are well-documented and handle edge cases gracefully. # Note: - You may assume that Seaborn and other required libraries (e.g., matplotlib) are already installed. - Focus on creating modular, reusable code. - Ensure all inputs are validated and handle errors appropriately.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_light_palette_by_name(color_name, num_colors=6): Create a light color palette based on a named color using Seaborn. Parameters: color_name (str): The name of the base color. num_colors (int): Number of colors in the palette. Must be between 1 and 10. Returns: List[Tuple[float]]: List of RGB tuples representing the light color palette. if not (1 <= num_colors <= 10): raise ValueError(\\"num_colors must be between 1 and 10.\\") palette = sns.light_palette(color_name, n_colors=num_colors, input=\\"name\\") return palette def create_light_palette_by_hex(hex_code, as_cmap=False): Create a light color palette based on a hex code using Seaborn. Parameters: hex_code (str): The hex code of the base color. as_cmap (bool): Whether to return a continuous colormap. Default is False. Returns: List[Tuple[float]] | Colormap: List of RGB tuples or a continuous colormap. palette = sns.light_palette(hex_code, as_cmap=as_cmap) return palette def create_light_palette_by_husl(husl_values, num_colors=6): Create a light color palette based on HUSL values using Seaborn. Parameters: husl_values (Tuple[float, float, float]): H, S, L values. num_colors (int): Number of colors in the palette. Must be between 1 and 10. Returns: List[Tuple[float]]: List of RGB tuples representing the light color palette. H, S, L = husl_values if not (0 <= H <= 360): raise ValueError(\\"H must be between 0 and 360.\\") if not (0 <= S <= 100): raise ValueError(\\"S must be between 0 and 100.\\") if not (0 <= L <= 100): raise ValueError(\\"L must be between 0 and 100.\\") if not (1 <= num_colors <= 10): raise ValueError(\\"num_colors must be between 1 and 10.\\") palette = sns.light_palette((H, S, L), n_colors=num_colors, input=\\"husl\\") return palette"},{"question":"# Kernel Density Estimation with Custom Features You are given a dataset containing various attributes of animals, including their weight, height, species, etc. Your task is to create a comprehensive kernel density plot using seaborn that demonstrates your understanding of univariate and bivariate KDE plots and customization options available in seaborn. Dataset The dataset is given in CSV format with the following columns: - **weight**: Numeric value indicating the weight of the animal. - **height**: Numeric value indicating the height of the animal. - **species**: Categorical value indicating the species of the animal (e.g., \\"Mammal\\", \\"Bird\\", etc.). You do not need to preprocess the data; assume the dataset is clean and ready for analysis. Requirements 1. **Univariate KDE plot for Weight**: - Plot a univariate KDE for the `weight` column. - Use a bandwidth adjustment of 0.5. - Display two KDE lines for different species using a hue (e.g., species). 2. **Stacked KDE plot for Height**: - Plot a univariate KDE for the `height` column. - Stack the KDE plots for each species. - Normalize the distribution so that the area under each KDE plot sums to 1. 3. **Bivariate KDE plot**: - Plot a bivariate KDE for `weight` vs. `height`. - Use a different color palette (e.g., \\"mako\\"). - Show the filled contours. 4. **Conditional Bivariate KDE plot**: - Plot a bivariate KDE for `weight` vs. `height` separately for each species. - Use different hues for species. - Show the contour lines, but do not fill them. - Display only 5 contour levels. Input - The dataset path as a string. Output - Four plots that satisfy the above requirements. Constraints - You may use any additional seaborn or matplotlib functionality to make your plots clear and informative. - Ensure that the plots are labeled appropriately for clarity. Example ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_animal_kde(dataset_path): # Load the dataset data = pd.read_csv(dataset_path) # Univariate KDE for weight plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'weight\', hue=\'species\', bw_adjust=0.5) plt.title(\'Univariate KDE for Weight\') plt.show() # Stacked KDE plot for height plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'height\', hue=\'species\', multiple=\'stack\', common_norm=False) plt.title(\'Stacked KDE plot for Height\') plt.show() # Bivariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'weight\', y=\'height\', cmap=\'mako\', fill=True) plt.title(\'Bivariate KDE for Weight vs. Height\') plt.show() # Conditional Bivariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'weight\', y=\'height\', hue=\'species\', levels=5) plt.title(\'Conditional Bivariate KDE for Weight vs. Height by Species\') plt.show() # Example usage plot_animal_kde(\'path_to_animal_dataset.csv\') ``` Use the structure and example provided to create your plots. Make sure each plot is clear and labeled correctly.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_animal_kde(dataset_path): # Load the dataset data = pd.read_csv(dataset_path) # Univariate KDE for weight plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'weight\', hue=\'species\', bw_adjust=0.5) plt.title(\'Univariate KDE for Weight\') plt.xlabel(\'Weight\') plt.ylabel(\'Density\') plt.legend(title=\'Species\') plt.show() # Stacked KDE plot for height plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'height\', hue=\'species\', multiple=\'stack\', common_norm=False) plt.title(\'Stacked KDE plot for Height\') plt.xlabel(\'Height\') plt.ylabel(\'Density\') plt.legend(title=\'Species\') plt.show() # Bivariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'weight\', y=\'height\', cmap=\'mako\', fill=True) plt.title(\'Bivariate KDE for Weight vs. Height\') plt.xlabel(\'Weight\') plt.ylabel(\'Height\') plt.show() # Conditional Bivariate KDE plot plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'weight\', y=\'height\', hue=\'species\', levels=5) plt.title(\'Conditional Bivariate KDE for Weight vs. Height by Species\') plt.xlabel(\'Weight\') plt.ylabel(\'Height\') plt.legend(title=\'Species\') plt.show() # Example usage # plot_animal_kde(\'path_to_animal_dataset.csv\')"},{"question":"Question: Implement an Elastic Distributed Training Agent In this assignment, you will implement a custom `SimpleElasticAgent` that orchestrates a specified number of workers to perform a distributed training task. Your implementation should include custom health check functionalities and a watchdog mechanism. # Requirements: 1. Implement a class `MyElasticAgent` that extends `SimpleElasticAgent`. 2. Your `MyElasticAgent` class should: - Initialize with a specified number of workers. - Assign an individual `WorkerSpec` to each worker with specific arguments and environment variables. - Use a watchdog mechanism to ensure each worker is actively participating. - Include a health check server to monitor the status of the worker processes. # Input Format: Your implementation should be in the form of a class and its methods. Here\'s a simple input structure to give you an idea of initialization: ```python num_workers = 4 worker_args = [\\"--arg1\\", \\"--arg2\\"] env_vars = {\\"MY_ENV_VAR\\": \\"value\\"} agent = MyElasticAgent(num_workers, worker_args, env_vars) agent.run() ``` # Output Format: Your solution should execute without errors and should include print statements or logging to provide insight into the state of the workers and the agent itself. # Constraints: - The agent should handle any number of worker processes efficiently. - Employ proper error handling and resource management. - Ensure that the watchdog and health check server are functional and report status correctly. # Performance Considerations: - The run method should block until all workers have completed their tasks. - Ensure that resource usage (CPU, memory) is optimal. ```python import torch.distributed.elastic.agent.server.SimpleElasticAgent import torch.distributed.elastic.agent.server.api class MyElasticAgent(torch.distributed.elastic.agent.server.SimpleElasticAgent): def __init__(self, num_workers, worker_args, env_vars): super().__init__() # Your initialization code here def _assign_workers(self): # Your worker assignment code here def run(self): # Your code to run the worker processes here def check_health(self): # Your health check code here # Example usage num_workers = 4 worker_args = [\\"--arg1\\", \\"--arg2\\"] env_vars = {\\"MY_ENV_VAR\\": \\"value\\"} agent = MyElasticAgent(num_workers, worker_args, env_vars) agent.run() ``` Develop your solution carefully, ensuring you meet all the specified requirements. Use the documentation provided for any class references or additional context.","solution":"import threading import time from random import randint class MyElasticAgent: def __init__(self, num_workers, worker_args, env_vars): self.num_workers = num_workers self.worker_args = worker_args self.env_vars = env_vars self.workers = [] self.watchdog_thread = None self.health_status = {} def _create_worker(self, worker_id): # Simulate worker creation with a dummy function running in a separate thread def worker(): while True: if self.health_status[worker_id] == \\"HEALTHY\\": time.sleep(randint(1, 3)) # Simulate work being done else: break return worker def _assign_workers(self): for worker_id in range(self.num_workers): self.health_status[worker_id] = \\"HEALTHY\\" worker = threading.Thread(target=self._create_worker(worker_id)) self.workers.append(worker) def run(self): self._assign_workers() self.watchdog_thread = threading.Thread(target=self._watchdog) for worker in self.workers: worker.start() self.watchdog_thread.start() for worker in self.workers: worker.join() self.watchdog_thread.join() def _watchdog(self): while any(worker.is_alive() for worker in self.workers): time.sleep(1) for worker_id in range(self.num_workers): if randint(0, 4) == 0: self.health_status[worker_id] = \\"UNHEALTHY\\" print(f\\"Worker {worker_id} reported unhealthy!\\") def check_health(self): return self.health_status"},{"question":"**Dataset Custom Bar Plot with Error Bars** **Objective:** Write a function to create a customized bar plot using the seaborn `objects` interface with specified requirements. **Function Signature:** ```python def create_custom_bar_plot(dataset_name: str, year: int): Create a customized bar plot with error bars for a given dataset and year. Parameters: dataset_name (str): The name of the dataset to load. Possible values are \\"penguins\\" and \\"flights\\". year (int): The year to filter the \\"flights\\" dataset. This parameter is ignored for the \\"penguins\\" dataset. Returns: None: This function should display the plot. ``` **Requirements:** 1. The function should load either the \\"penguins\\" or \\"flights\\" dataset based on the `dataset_name` parameter. 2. If the \\"flights\\" dataset is chosen, filter it by the given `year` parameter. 3. Create a bar plot where: - For \\"penguins\\": - Plot the `species` on the x-axis. - Differentiate the bars by `sex`. - Use the `Dodge` transform to handle overlapping bars. - Add error bars representing the standard deviation of the `body_mass_g` for each group. - For \\"flights\\": - Plot the `month` on the x-axis. - Differentiate the bars by the `passengers` count. - No transformation is required for overlapping bars. - Add error bars representing the standard deviation of the `passengers` count for each month. 4. Customize the bar plot such that: - Set different colors and edge styles for groups by `sex` or `month`. - Set a non-default `alpha` value for visual distinction. - Configure edge width to `2`. **Constraints:** - The function should display the plot using matplotlib\'s `plt.show()` at the end. - Ensure proper error handling for invalid dataset names and missing years (for the \\"flights\\" dataset). **Example:** ```python # For the penguins dataset create_custom_bar_plot(\\"penguins\\", 1960) # For the flights dataset, filtered by the year 1960 create_custom_bar_plot(\\"flights\\", 1960) ``` **Note:** The imported libraries (seaborn.objects as so, seaborn.load_dataset) are available by default, and you should focus on function implementation and plotting.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_custom_bar_plot(dataset_name: str, year: int): Create a customized bar plot with error bars for a given dataset and year. Parameters: dataset_name (str): The name of the dataset to load. Possible values are \\"penguins\\" and \\"flights\\". year (int): The year to filter the \\"flights\\" dataset. This parameter is ignored for the \\"penguins\\" dataset. Returns: None: This function should display the plot. try: if dataset_name not in [\\"penguins\\", \\"flights\\"]: raise ValueError(\\"Invalid dataset name. Choose either \'penguins\' or \'flights\'\\") if dataset_name == \\"penguins\\": data = sns.load_dataset(\\"penguins\\") plot = ( so.Plot(data, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\", edgecolor=\\"sex\\") .add(so.Bar(), so.Dodge(), so.ErrorBar(error=\\"sd\\")) .scale(color=(\\"tab:blue\\", \\"tab:orange\\"), edgecolor=(\\"black\\", \\"white\\")) ) elif dataset_name == \\"flights\\": data = sns.load_dataset(\\"flights\\") if year not in data[\'year\'].unique(): raise ValueError(\\"Invalid year for the flights dataset.\\") data = data[data[\'year\'] == year] plot = ( so.Plot(data, x=\\"month\\", y=\\"passengers\\") .add(so.Bar(), so.ErrorBar(error=\\"sd\\")) .scale(color=\\"tab:blue\\", edgewidth=2) ) plot.theme(alpha=0.75, edgewidth=2).show() except Exception as e: print(f\\"Error: {e}\\")"},{"question":"You are given data in the form of two dictionaries containing details about software development projects handled by different teams. Each dictionary represents data of a specific year. Your task is to merge these datasets into a single DataFrame and compute some key statistics. # Input - Two dictionaries `data_2022` and `data_2023` containing project data from the years 2022 and 2023 respectively. ```python data_2022 = { \\"team\\": [\\"team_A\\", \\"team_B\\", \\"team_C\\", \\"team_D\\"], \\"project_count\\": [3, 5, 2, 8], \\"budget_million\\": [1.2, 2.5, 0.75, 3.0] } data_2023 = { \\"team\\": [\\"team_A\\", \\"team_B\\", \\"team_E\\"], \\"project_count\\": [4, 7, 5], \\"budget_million\\": [1.5, 3.0, 2.0] } ``` # Output 1. **Merged DataFrame**: The combined data from 2022 and 2023, with each year’s data being one row per team. Ensure the final DataFrame considers teams present in both years and properly aligns the data. Teams only present in one year will show `NaN` for missing values. 2. **New Columns Calculations**: - `total_project_count` - sum of project counts from both years. - `average_budget_million` - average budget of the two years. 3. **Highest Budget Team**: The team with the highest budget considering both years combined. 4. **Data Refinement**: Filter out teams with a total budget less than 2 million across two years. # Constraints - Handle missing data appropriately. - Ensure all operations align data correctly based on the team. # Implementation Requirements 1. Write a function `generate_statistics(data_2022: dict, data_2023: dict) -> pd.DataFrame` that: - Merges the two dictionaries into a single DataFrame. - Computes and adds the `total_project_count` and `average_budget_million` columns. - Returns the refined DataFrame. 2. Write a function `highest_budget_team(df: pd.DataFrame) -> str` that: - Identifies and returns the name of the team with the highest combined budget from both years. # Example ```python import pandas as pd def generate_statistics(data_2022: dict, data_2023: dict) -> pd.DataFrame: # Your implementation here. pass def highest_budget_team(df: pd.DataFrame) -> str: # Your implementation here. pass data_2022 = { \\"team\\": [\\"team_A\\", \\"team_B\\", \\"team_C\\", \\"team_D\\"], \\"project_count\\": [3, 5, 2, 8], \\"budget_million\\": [1.2, 2.5, 0.75, 3.0] } data_2023 = { \\"team\\": [\\"team_A\\", \\"team_B\\", \\"team_E\\"], \\"project_count\\": [4, 7, 5], \\"budget_million\\": [1.5, 3.0, 2.0] } df = generate_statistics(data_2022, data_2023) print(df) top_team = highest_budget_team(df) print(f\\"The team with the highest budget is: {top_team}\\") ``` Expected Output ``` team project_count_2022 budget_million_2022 project_count_2023 budget_million_2023 total_project_count average_budget_million 0 team_A 3.0 1.20 4.0 1.5 7.0 1.35 1 team_B 5.0 2.50 7.0 3.0 12.0 2.75 2 team_C 2.0 0.75 NaN NaN 2.0 0.75 3 team_D 8.0 3.00 NaN NaN 8.0 3.00 4 team_E NaN NaN 5.0 2.0 5.0 2.00 The team with the highest budget is: team_B ``` Note: 1. You must handle missing values in such a way that `NaN` values in budgets or project counts do not affect numerical computations adversely. 2. Ensure that the resulting DataFrame displays data clearly and concisely, meeting the described output format.","solution":"import pandas as pd def generate_statistics(data_2022: dict, data_2023: dict) -> pd.DataFrame: # Convert dictionaries to DataFrames df_2022 = pd.DataFrame(data_2022) df_2023 = pd.DataFrame(data_2023) # Add a suffix to distinguish columns from different years df_2022 = df_2022.add_suffix(\'_2022\') df_2023 = df_2023.add_suffix(\'_2023\') # Rename the columns that serve as keys for merging df_2022 = df_2022.rename(columns={\'team_2022\': \'team\'}) df_2023 = df_2023.rename(columns={\'team_2023\': \'team\'}) # Merge DataFrames on the team column merged_df = pd.merge(df_2022, df_2023, on=\'team\', how=\'outer\') # Calculate new columns merged_df[\'total_project_count\'] = merged_df[\'project_count_2022\'].fillna(0) + merged_df[\'project_count_2023\'].fillna(0) merged_df[\'average_budget_million\'] = merged_df[[\'budget_million_2022\', \'budget_million_2023\']].mean(axis=1) # Filter out teams with total budget less than 2 million merged_df[\'total_budget\'] = merged_df[[\'budget_million_2022\', \'budget_million_2023\']].sum(axis=1, skipna=True) refined_df = merged_df[merged_df[\'total_budget\'] >= 2] return refined_df def highest_budget_team(df: pd.DataFrame) -> str: highest_budget_team_row = df.loc[df[\'total_budget\'].idxmax()] return highest_budget_team_row[\'team\']"},{"question":"**Coding Assessment Question** As a data analyst, it is often helpful to temporarily adjust the display settings in pandas to better visualize your data, and revert back to default settings after analysis. You are required to write a function that accomplishes the following: 1. Creates a sample DataFrame with at least 10 rows and 5 columns of random numerical data. 2. Using the `pd.option_context` context manager, temporarily set: - `display.max_rows` to 10 - `display.max_columns` to 4 - `display.precision` to 3 3. Inside the context manager, display the DataFrame and the current settings of the specified options. 4. Outside the context manager, display the settings of `display.max_rows`, `display.max_columns`, and `display.precision` to confirm they are reset to their defaults. **Function Signature** ```python def display_with_custom_settings(): pass ``` **Expected Output** The function should display the DataFrame and the current settings of the display options inside and outside of the context manager, showing the changes and their reversion to defaults. **Constraints and Limitations** - Use `numpy.random.randn` to generate random data for the DataFrame. - Assume the default settings for these options before the context manager are the pandas default settings. **Example Execution** ```python display_with_custom_settings() ``` Output might include: ``` DataFrame: 0 1 2 3 4 0 0.346 -0.748 0.897 -0.434 -0.560 1 -1.432 0.438 -0.179 -1.321 1.563 ... 9 -1.246 0.432 -0.231 1.134 -2.324 Inside context: display.max_rows: 10 display.max_columns: 4 display.precision: 3 Outside context: display.max_rows: 60 display.max_columns: 20 display.precision: 6 ``` **Note**: The exact numerical data will differ due to randomness.","solution":"import pandas as pd import numpy as np def display_with_custom_settings(): # Create a sample DataFrame with 10 rows and 5 columns of random numerical data df = pd.DataFrame(np.random.randn(10, 5)) # Display DataFrame and settings inside context manager with pd.option_context(\'display.max_rows\', 10, \'display.max_columns\', 4, \'display.precision\', 3): print(\\"DataFrame with custom display settings:\\") print(df) # Display the DataFrame # Display current settings inside the context manager print(\\"Inside context:\\") print(f\\"display.max_rows: {pd.get_option(\'display.max_rows\')}\\") print(f\\"display.max_columns: {pd.get_option(\'display.max_columns\')}\\") print(f\\"display.precision: {pd.get_option(\'display.precision\')}\\") # Display settings outside the context manager to confirm they are reset to defaults print(\\"Outside context:\\") print(f\\"display.max_rows: {pd.get_option(\'display.max_rows\')}\\") print(f\\"display.max_columns: {pd.get_option(\'display.max_columns\')}\\") print(f\\"display.precision: {pd.get_option(\'display.precision\')}\\")"},{"question":"# Question: Custom Python Object Lifecycle In Python, automatic memory management is largely carried out by the garbage collector, which keeps track of object references and automatically deals with object deallocations. However, understanding the fundamentals of how objects are created, initialized, and deleted can be useful, especially when optimizing performance or dealing with large datasets. Your task is to implement a custom class `ManagedObject` in Python that mimics the lifecycle management of an object similar to what might occur in a lower-level language with manual memory management. The class should log messages during the creation, initialization, and deletion phases of an object. Additionally, implement a class method to manually manage the memory allocation for a collection of such objects. Specifications: 1. **Class `ManagedObject`** - Logs a message \\"Object created with id: <id>\\" when a new object is created. - Logs a message \\"Object initialized with id: <id>\\" when the object is initialized. - Logs a message \\"Object deleted with id: <id>\\" when the object is deleted. 2. **Class Method `manage_objects(cls, count)`** - Takes an integer `count` and creates that many `ManagedObject` instances. - Initializes each object. - Deletes each object manually. - After all objects are deleted, logs the message \\"All objects managed successfully.\\" Example: ```python class ManagedObject: # Your implementation here # Example usage: ManagedObject.manage_objects(5) # Example output: # Object created with id: 140366057993456 # Object initialized with id: 140366057993456 # Object deleted with id: 140366057993456 # Object created with id: 140366057993488 # Object initialized with id: 140366057993488 # Object deleted with id: 140366057993488 # Object created with id: 140366057993520 # Object initialized with id: 140366057993520 # Object deleted with id: 140366057993520 # Object created with id: 140366057993552 # Object initialized with id: 140366057993552 # Object deleted with id: 140366057993552 # Object created with id: 140366057993584 # Object initialized with id: 140366057993584 # Object deleted with id: 140366057993584 # All objects managed successfully. ``` # Constraints: - The object id should be the object\'s unique identifier, which in Python can be obtained with the `id(object)` function. - You should ensure that objects are explicitly deleted within the `manage_objects` method to simulate manual memory management. Implement the `ManagedObject` class in Python based on the above specifications.","solution":"class ManagedObject: def __init__(self): self.id = id(self) print(f\\"Object created with id: {self.id}\\") self.initialize() def initialize(self): print(f\\"Object initialized with id: {self.id}\\") def __del__(self): print(f\\"Object deleted with id: {self.id}\\") @classmethod def manage_objects(cls, count): objects = [] for _ in range(count): obj = cls() objects.append(obj) while objects: obj = objects.pop() del obj print(\\"All objects managed successfully.\\")"},{"question":"# Custom Echo Service with Throttling # Objective Implement a custom TCP echo service using the `asyncio` low-level `Transport` and `Protocol` APIs with an added functionality: throttling. Throttling should ensure that no more than a specified number of bytes are sent per second. # Requirements 1. **EchoTransport**: An `asyncio.Transport` class used to transmit data. 2. **ThrottledEchoProtocol**: An `asyncio.Protocol` class that processes echo requests and throttles the response. # Instructions 1. Implement the `ThrottledEchoProtocol` class, ensuring it limits the rate of outgoing data to the specified `rate_limit` in bytes per second. 2. Implement the `EchoTransport` class to handle the actual data transmission as per the Protocol\'s instructions. 3. Use `asyncio` event loop methods to establish and manage these transports and protocols. # Class Specifications ThrottledEchoProtocol - **Initialisation**: Accept a rate limit (`rate_limit`) parameter. - **Connection Made**: Initialise the transport and any necessary state. - **Data Received**: Echo back data received, adhering to the `rate_limit`. EchoTransport - **Initialisation**: Manage the underlying socket and rate limit logic. - **Write**: Ensure data is sent at no more than the `rate_limit`. - **Close**: Handle closing the transport. # Inputs and Outputs - No direct input/output functions should be implemented for the protocol/transport classes. You should instead create an example scenario demonstrating usage. - Use `asyncio` event loop facilities to set up the example. # Constraints - You must use the provided skeleton code as a starting point. - Pay attention to potential edge cases, such as handling closing connections and varying data sizes. # Performance - The protocol must handle large data payloads efficiently and throttle output precisely within a 5% error margin of the `rate_limit`. # Skeleton Code ```python import asyncio import time class ThrottledEchoProtocol(asyncio.Protocol): def __init__(self, rate_limit): self.rate_limit = rate_limit self.transport = None self._buffer = bytearray() self._sending = False def connection_made(self, transport): self.transport = transport def data_received(self, data): self._buffer.extend(data) if not self._sending: asyncio.create_task(self._send_data()) async def _send_data(self): self._sending = True while self._buffer: data_chunk = self._buffer[:self.rate_limit] self._buffer = self._buffer[self.rate_limit:] self.transport.write(data_chunk) await asyncio.sleep(1) self._sending = False def connection_lost(self, exc): pass class EchoTransport(asyncio.Transport): def __init__(self, protocol, loop): self.loop = loop self.protocol = protocol self.closed = False self.protocol.connection_made(self) def write(self, data): if not self.closed: # Send data pass # Implementation needed def close(self): self.closed = True self.protocol.connection_lost(None) def is_closing(self): return self.closed async def main(): loop = asyncio.get_running_loop() # Example server server = await loop.create_server( lambda: ThrottledEchoProtocol(rate_limit=1024), host=\'127.0.0.1\', port=8888 ) async with server: await server.serve_forever() asyncio.run(main()) ``` In your implementation, ensure that the `EchoTransport` correctly handles writing data in line with the throttling implemented in `ThrottledEchoProtocol`. Feel free to add any helper methods or classes as needed. Document your code to explain the logic clearly.","solution":"import asyncio import time class ThrottledEchoProtocol(asyncio.Protocol): def __init__(self, rate_limit): self.rate_limit = rate_limit self.transport = None self._buffer = bytearray() self._sending = False def connection_made(self, transport): self.transport = transport def data_received(self, data): self._buffer.extend(data) if not self._sending: asyncio.create_task(self._send_data()) async def _send_data(self): self._sending = True while self._buffer: data_chunk = self._buffer[:self.rate_limit] self._buffer = self._buffer[self.rate_limit:] self.transport.write(data_chunk) await asyncio.sleep(1) self._sending = False def connection_lost(self, exc): pass class EchoTransport(asyncio.Transport): def __init__(self, protocol, loop): self.loop = loop self.protocol = protocol self.closed = False self.protocol.connection_made(self) def write(self, data): if not self.closed: self.loop.call_soon(self._write_data, data) def _write_data(self, data): self.protocol.transport.write(data) def close(self): self.closed = True self.protocol.connection_lost(None) def is_closing(self): return self.closed async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ThrottledEchoProtocol(rate_limit=1024), host=\'127.0.0.1\', port=8888 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Advanced Coding Assessment: Using `doctest` for Function Documentation Testing Objective: Create a Python module containing multiple functions and utilize the `doctest` module to write and run embedded tests within the docstrings of your functions. This assessment will test your ability to document code effectively, write accurate testable examples, and ensure your module behaves as documented. Instructions: 1. **Module Creation**: - Create a Python module named `math_operations.py`. - Implement and document the following functions in the module with appropriate docstrings. 2. **Functions to Implement**: - `add(a: int, b: int) -> int`: Returns the sum of two integers. ```python Adds two integers. >>> add(2, 3) 5 >>> add(-1, 1) 0 ``` - `subtract(a: int, b: int) -> int`: Returns the difference between two integers. ```python Subtracts second integer from the first. >>> subtract(10, 5) 5 >>> subtract(2, 4) -2 ``` - `factorial(n: int) -> int`: Returns the factorial of a non-negative integer. ```python Computes the factorial of a non-negative integer. >>> factorial(5) 120 >>> factorial(0) 1 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 >>> factorial(1e100) Traceback (most recent call last): ... OverflowError: n too large ``` 3. **Execution**: - Include a `main` block in `math_operations.py` to trigger `doctest` when the module is executed. ```python if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` - Ensure that when you run this module, it tests the examples provided in the docstrings of the functions. 4. **Submission**: - Submit the `math_operations.py` file with correctly implemented functions and docstrings. - Ensure that all `doctest` examples pass when running the module. Constraints: - Do not use any external libraries aside from `math` for computing the factorial. Evaluation Criteria: - **Correctness**: Verify that the functions are implemented correctly according to the provided specifications and examples. - **Documentation**: Ensure that each function is well-documented with a proper docstring including examples. - **Testing**: All `doctest` examples in the docstrings should pass without errors. - **Code Quality**: Adherence to clean code practices including clear naming conventions, concise code, and proper formatting. Example: An example of running the module should look like: ```commandline python math_operations.py -v Trying: add(2, 3) Expecting: 5 ok Trying: add(-1, 1) Expecting: 0 ok ... 9 tests in 3 items. 9 passed and 0 failed. Test passed. ``` Good luck!","solution":"def add(a, b): Adds two integers. >>> add(2, 3) 5 >>> add(-1, 1) 0 return a + b def subtract(a, b): Subtracts second integer from the first. >>> subtract(10, 5) 5 >>> subtract(2, 4) -2 return a - b def factorial(n): Computes the factorial of a non-negative integer. >>> factorial(5) 120 >>> factorial(0) 1 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 >>> factorial(1e100) Traceback (most recent call last): ... OverflowError: n too large if n < 0: raise ValueError(\\"n must be >= 0\\") if n == 0: return 1 if n > 1000: raise OverflowError(\\"n too large\\") result = 1 for i in range(1, n + 1): result *= i return result if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"You have been provided with some foundational knowledge on how to parse arguments and build values in Python C extensions. For this task, you will need to implement a C extension function for Python that processes a list of numerical values to perform a specific operation efficiently, showcasing your understanding of the arguments and values conversion between Python and C. **Task**: Implement a Python C extension function that accepts a list of integers and/or floats and returns a new list where each element is doubled. Your implementation should handle all necessary memory management aspects properly, including reference counting and buffer management. **Requirements**: 1. **Input**: A single list containing integers and/or floats. 2. **Output**: A new list where each element from the input list is doubled. 3. **Constraints**: - If the input is not a list or contains non-numeric elements, raise a `TypeError` with an appropriate message. - The function should handle empty lists correctly and return an empty list. **Performance**: - The function should be efficient in terms of both time and space complexity, avoiding unnecessary copying of data. # Skeleton Code: Below is a skeleton `doubler.c` file to get you started: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> static PyObject* double_elements(PyObject* self, PyObject* args) { PyObject* input_list; PyObject* output_list; Py_ssize_t i, n; // Parse the input arguments if (!PyArg_ParseTuple(args, \\"O!\\", &PyList_Type, &input_list)) { return NULL; } // Get the size of the input list n = PyList_Size(input_list); if (n < 0) { return NULL; } // Create a new list for the output output_list = PyList_New(n); if (!output_list) { return NULL; } // Process each element for (i = 0; i < n; ++i) { PyObject* item = PyList_GetItem(input_list, i); if (PyLong_Check(item)) { long value = PyLong_AsLong(item); if (value == -1 && PyErr_Occurred()) { Py_DECREF(output_list); return NULL; } PyList_SetItem(output_list, i, PyLong_FromLong(value * 2)); } else if (PyFloat_Check(item)) { double value = PyFloat_AsDouble(item); if (PyErr_Occurred()) { Py_DECREF(output_list); return NULL; } PyList_SetItem(output_list, i, PyFloat_FromDouble(value * 2.0)); } else { PyErr_SetString(PyExc_TypeError, \\"List elements must be integers or floats.\\"); Py_DECREF(output_list); return NULL; } } return output_list; } static PyMethodDef DoublerMethods[] = { {\\"double_elements\\", double_elements, METH_VARARGS, \\"Double each element in a list.\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef doublermodule = { PyModuleDef_HEAD_INIT, \\"doubler\\", NULL, -1, DoublerMethods }; PyMODINIT_FUNC PyInit_doubler(void) { return PyModule_Create(&doublermodule); } ``` To complete this assignment: 1. Carefully read through the skeleton code above. 2. Implement the functionality to correctly parse the input list and handle all exceptions as described. 3. Compile the C extension and write a Python script to test your implementation. **Testing Your Code**: Ensure thorough testing with various edge cases including: - Mixed integers and floats. - Empty lists. - Lists with non-numeric elements. **Documentation**: Provide inline comments and a README file with instructions on how to compile and test your code.","solution":"def double_elements(input_list): Doubles each element in the input list. Parameters: input_list (list): List containing integers and/or floats. Returns: list: List with each element from the input list doubled. Raises: TypeError: If the input is not a list or contains non-numeric elements. if not isinstance(input_list, list): raise TypeError(\\"Input must be a list\\") doubled_list = [] for item in input_list: if isinstance(item, (int, float)): doubled_list.append(item * 2) else: raise TypeError(\\"List elements must be integers or floats\\") return doubled_list"},{"question":"Implement a Python class `MyInstanceMethod` that mimics the behavior of the instance method objects detailed in the documentation. Your class should: 1. Bind a function to a class instance. 2. Provide methods to check if a given object is an instance method. 3. Allow retrieval of the bound function and the instance. Specifically, implement the following: ```python class MyInstanceMethod: def __init__(self, func, instance): Initialize the instance method object. Parameters: func (callable): The function to bind. instance (object): The instance to bind the function to. pass def is_instance_method(self, obj): Check if obj is an instance method of this type. Parameters: obj (object): The object to check. Returns: bool: True if obj is an instance method, False otherwise. pass def get_function(self): Get the function bound to this instance method. Returns: callable: The bound function. pass def get_instance(self): Get the instance bound to this instance method. Returns: object: The bound instance. pass ``` # Input and Output Formats - Your constructor (`__init__`) takes a callable `func` and an `instance` as inputs. - The `is_instance_method` method takes one input `obj` and returns `True` if `obj` is an instance method of this type; otherwise, `False`. - The `get_function` method returns the bound function. - The `get_instance` method returns the bound instance. # Constraints - The `func` must be a callable object. - The `instance` can be any Python object. - You can assume the inputs to `is_instance_method` will be valid Python objects. # Example ```python def my_func(): pass obj = object() instance_method = MyInstanceMethod(my_func, obj) # Example usage print(instance_method.is_instance_method(instance_method)) # Should return True print(instance_method.get_function() == my_func) # Should return True print(instance_method.get_instance() == obj) # Should return True ``` Ensure your solution is efficient and concise, adhering to Python best practices.","solution":"class MyInstanceMethod: def __init__(self, func, instance): Initialize the instance method object. Parameters: func (callable): The function to bind. instance (object): The instance to bind the function to. if not callable(func): raise ValueError(\\"func must be a callable\\") self.func = func self.instance = instance def is_instance_method(self, obj): Check if obj is an instance method of this type. Parameters: obj (object): The object to check. Returns: bool: True if obj is an instance method, False otherwise. return isinstance(obj, MyInstanceMethod) def get_function(self): Get the function bound to this instance method. Returns: callable: The bound function. return self.func def get_instance(self): Get the instance bound to this instance method. Returns: object: The bound instance. return self.instance"},{"question":"You are tasked with testing a function that involves making HTTP requests to an external server using the `requests` library. To ensure reliable testing without making real HTTP requests, you will use the `unittest.mock` library to mock the `requests.get` method. Your goal is to validate that your function correctly handles different response scenarios: a successful response and an HTTP error response. Function to Test ```python import requests def fetch_data(url): Fetches data from the given URL. If the request is successful, returns the JSON response. If the request encounters an HTTP error, raises an exception with the error message. Args: - url (str): The URL to fetch data from. Returns: - dict: The JSON response from the server. Raises: - Exception: If the request fails with an HTTP error. response = requests.get(url) response.raise_for_status() return response.json() ``` Requirements 1. **Mocking HTTP Requests**: Use `unittest.mock` to mock the `requests.get` method for your tests. 2. **Testing Successful Response**: Write a test to ensure that your function correctly returns the JSON data when the request is successful. 3. **Testing HTTP Error Response**: Write a test to ensure that your function raises an appropriate exception when the request encounters an HTTP error. Input and Output # Input - `url`: A string representing the URL to fetch data from. # Output - For a successful response, the function returns a dictionary representing the JSON data. - For an HTTP error response, the function raises an exception with an appropriate error message. Constraints - Do not make actual HTTP requests during testing. Use mocking to simulate HTTP responses. - Use the `unittest` framework for writing the tests. Example Usage ```python # Example for successful response print(fetch_data(\\"https://api.example.com/data\\")) # Output: {\\"key\\": \\"value\\"} # Example for HTTP error response try: fetch_data(\\"https://api.example.com/error\\") except Exception as e: print(e) # Output: An error message indicating the HTTP error ``` Task Implement the following: 1. A function `test_fetch_data_success()` to test the successful response scenario. 2. A function `test_fetch_data_failure()` to test the HTTP error response scenario. Your test functions should use `unittest.mock.patch` to mock `requests.get` and validate the behavior of `fetch_data`.","solution":"import requests def fetch_data(url): Fetches data from the given URL. If the request is successful, returns the JSON response. If the request encounters an HTTP error, raises an exception with the error message. Args: - url (str): The URL to fetch data from. Returns: - dict: The JSON response from the server. Raises: - Exception: If the request fails with an HTTP error. response = requests.get(url) response.raise_for_status() return response.json()"},{"question":"**Coding Assessment Question: Implementing and Tuning SVM for Classification** **Objective:** This assessment is designed to test your understanding of Support Vector Machines (SVM) and your ability to use the scikit-learn library to implement and tune SVM classifiers. **Task:** You are required to implement a binary and a multi-class classification using SVM in scikit-learn. You need to explore and tune hyperparameters to improve classification performance. **Instructions:** 1. **Binary Classification:** - Load a toy dataset with two classes, such as the Iris dataset with only two classes. - Implement `SVC` to perform binary classification. - Tune the hyperparameters `C` and `gamma` using `GridSearchCV`. - Report accuracy on the test set. 2. **Multi-Class Classification:** - Use the complete Iris dataset with all three classes. - Implement `SVC` for multi-class classification using both `one-vs-one` and `one-vs-rest` strategies. - Tune the hyperparameters `C` and `gamma` using `GridSearchCV`. - Report accuracy on the test set for each strategy. 3. **Custom Kernel Implementation:** - Define a custom linear kernel function. - Implement `SVC` with your custom kernel for the multi-class Iris dataset. - Evaluate and report the accuracy on the test set. **Requirements:** - Your code should be organized into functions as described: 1. `binary_classification_svc(X_train, X_test, y_train, y_test) -> float` 2. `multi_class_classification_svc(X_train, X_test, y_train, y_test, strategy=\'ovr\') -> float` 3. `custom_kernel_classification_svc(X_train, X_test, y_train, y_test, kernel_function) -> float` - The `GridSearchCV` should explore the following ranges: - `C`: [0.1, 1, 10] - `gamma`: [0.01, 0.1, 1] **Input Format:** - `X_train`, `X_test`: Feature matrices for training and testing (n_samples, n_features). - `y_train`, `y_test`: Target vectors for training and testing (n_samples). **Output Format:** - Each function should return the accuracy of the classification on the test set as a float. **Constraints:** - Use `random_state=42` for reproducibility where needed. - Performance will be evaluated based on accuracy, clarity, and efficiency of your implementation. **Performance Requirements:** - Your code should be optimized for clarity and efficiency. - Use appropriate scikit-learn functions and methods for implementation. **Example Usage:** ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # Load and prepare data iris = load_iris() X, y = iris.data, iris.target # Binary Classification Example Setup X_binary, y_binary = X[y != 2], y[y != 2] # Only two classes X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X_binary, y_binary, test_size=0.3, random_state=42) binary_accuracy = binary_classification_svc(X_train_bin, X_test_bin, y_train_bin, y_test_bin) print(f\\"Binary Classification Accuracy: {binary_accuracy}\\") # Multi-Class Classification Example Setup X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) multi_class_accuracy_ovr = multi_class_classification_svc(X_train, X_test, y_train, y_test, strategy=\'ovr\') print(f\\"Multi-Class (One-vs-Rest) Classification Accuracy: {multi_class_accuracy_ovr}\\") multi_class_accuracy_ovo = multi_class_classification_svc(X_train, X_test, y_train, y_test, strategy=\'ovo\') print(f\\"Multi-Class (One-vs-One) Classification Accuracy: {multi_class_accuracy_ovo}\\") # Custom Kernel Example Setup def custom_kernel(X, Y): return np.dot(X, Y.T) custom_kernel_accuracy = custom_kernel_classification_svc(X_train, X_test, y_train, y_test, custom_kernel) print(f\\"Custom Kernel Classification Accuracy: {custom_kernel_accuracy}\\") ```","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score def binary_classification_svc(X_train, X_test, y_train, y_test): parameters = {\'C\': [0.1, 1, 10], \'gamma\': [0.01, 0.1, 1]} svc = SVC() clf = GridSearchCV(svc, parameters, cv=5) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) return accuracy_score(y_test, y_pred) def multi_class_classification_svc(X_train, X_test, y_train, y_test, strategy=\'ovr\'): parameters = {\'C\': [0.1, 1, 10], \'gamma\': [0.01, 0.1, 1]} svc = SVC(decision_function_shape=strategy) clf = GridSearchCV(svc, parameters, cv=5) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) return accuracy_score(y_test, y_pred) def custom_kernel_classification_svc(X_train, X_test, y_train, y_test, kernel_function): svc = SVC(kernel=kernel_function) svc.fit(X_train, y_train) y_pred = svc.predict(X_test) return accuracy_score(y_test, y_pred)"},{"question":"**Problem: Custom Date Range Operations using Pandas DateOffset** **Objective:** Write a function `custom_date_operations` that accepts two parameters: 1. `start_date` (string) - The starting date in the format \\"YYYY-MM-DD\\". 2. `end_date` (string) - The ending date in the format \\"YYYY-MM-DD\\". The function should return a dictionary containing the following keys and their corresponding values: * `\'adding_business_days\'`: A list of dates obtained by adding increasing BusinessDay offsets from 1 to 5 days to the start_date. * `\'monthly_period_starts\'`: A list of start dates of each month between the start_date and end_date using MonthBegin offset. * `\'last_custom_business_days_month\'`: A list of last custom business days of each month between the start_date and end_date using CustomBusinessDay offset (custom business day defined as Monday to Friday). **Constraints:** 1. The input dates will be valid and the end_date will always be after the start_date. 2. Use pandas `DateOffset` and its subclasses to manipulate and transform the dates. **Function Signature:** ```python import pandas as pd def custom_date_operations(start_date: str, end_date: str) -> dict: pass ``` **Example:** ```python output = custom_date_operations(\\"2022-01-01\\", \\"2022-03-31\\") # Example output content might be: # { # \'adding_business_days\': [\'2022-01-03\', \'2022-01-04\', \'2022-01-05\', \'2022-01-06\', \'2022-01-07\'], # \'monthly_period_starts\': [\'2022-01-01\', \'2022-02-01\', \'2022-03-01\'], # \'last_custom_business_days_month\': [\'2022-01-31\', \'2022-02-28\', \'2022-03-31\'] # } ``` **Notes:** * Ensure that your implementation is performance optimized. * Be mindful of national holidays and weekends when creating custom business day calculations.","solution":"import pandas as pd def custom_date_operations(start_date: str, end_date: str) -> dict: start_date = pd.to_datetime(start_date) end_date = pd.to_datetime(end_date) # Define the custom business day (Monday to Friday) custom_business_day = pd.offsets.CustomBusinessDay(weekmask=\'Mon Tue Wed Thu Fri\') # Adding increasing business day offsets from 1 to 5 adding_business_days = [(start_date + pd.offsets.BusinessDay(n)).strftime(\'%Y-%m-%d\') for n in range(1, 6)] # Month begins from start_date to end_date monthly_period_starts = pd.date_range(start_date, end_date, freq=\'MS\').strftime(\'%Y-%m-%d\').tolist() # Last custom business days of each month last_custom_business_days_month = pd.date_range(start_date, end_date, freq=\'BM\').strftime(\'%Y-%m-%d\').tolist() return { \'adding_business_days\': adding_business_days, \'monthly_period_starts\': monthly_period_starts, \'last_custom_business_days_month\': last_custom_business_days_month }"},{"question":"**Pandas String Operations and Data Cleaning** **Objective:** To assess your understanding of pandas string handling and manipulation techniques by performing a series of transformations on a dataset. **Background:** You are provided with a DataFrame containing data about various products sold by a retailer. The dataset has columns with different text formats and miscellaneous characters that need cleaning for analysis. Your task is to implement the necessary functions to clean and transform the data. **Dataset:** ```python import pandas as pd import numpy as np data = { \'Product ID\': [\' P1 \', \' P2 \', \' P3\', \' p4 \', \' P-5\'], \'Product Name\': [\'Apple \', \'bAnana\', \'Orange \', \' grape\', \' Lemon\'], \'Product Description\': [\' Fresh Apple\', \'Best bananas*\', \'Oranges\', \'Sweet Grapes\', \'**Fresh Lemon**\'], \'Price\': [\'1.20\', \'2.89\', \'0.99\', \'3.45 \', \' 4.50\'] } df = pd.DataFrame(data) ``` **Functions to Implement:** 1. **clean_product_id(df):** - Strip leading and trailing whitespace. - Convert all text to uppercase. - Ensure all product IDs follow a consistent format `P[Number]` (e.g., `P1`, `P2`). 2. **clean_product_name(df):** - Strip leading and trailing whitespace. - Convert all text to title case (first letter of each word capitalized). 3. **clean_product_description(df):** - Remove any special characters (e.g., `*`, `#`, etc.). - Strip leading and trailing whitespace. 4. **clean_price(df):** - Remove the dollar sign (``). - Convert the text to float. 5. **apply_cleaning_functions(df):** - Apply all the above functions on the DataFrame. - Return the cleaned DataFrame. **Constraints:** - You are not allowed to use any loops. - Your solution should handle missing values gracefully, assuming `NaN` can be treated as missing. **Expected Output:** ```plaintext cleaned_df = apply_cleaning_functions(df) print(cleaned_df) ``` **Sample Output:** ```plaintext Product ID Product Name Product Description Price 0 P1 Apple Fresh Apple 1.20 1 P2 Banana Best bananas 2.89 2 P3 Orange Oranges 0.99 3 P4 Grape Sweet Grapes 3.45 4 P5 Lemon Fresh Lemon 4.50 ``` Implement the necessary functions and ensure the transformations are correctly applied to produce the clean DataFrame as shown in the sample output.","solution":"import pandas as pd import numpy as np import re def clean_product_id(df): df[\'Product ID\'] = df[\'Product ID\'].str.strip().str.upper() df[\'Product ID\'] = df[\'Product ID\'].str.replace(r\'[^A-Z0-9]\', \'\', regex=True) return df def clean_product_name(df): df[\'Product Name\'] = df[\'Product Name\'].str.strip().str.title() return df def clean_product_description(df): df[\'Product Description\'] = df[\'Product Description\'].str.replace(r\'[^a-zA-Z0-9 ]\', \'\', regex=True) df[\'Product Description\'] = df[\'Product Description\'].str.strip() return df def clean_price(df): df[\'Price\'] = df[\'Price\'].str.replace(r\'[]\', \'\', regex=True) df[\'Price\'] = pd.to_numeric(df[\'Price\'], errors=\'coerce\') return df def apply_cleaning_functions(df): df = clean_product_id(df) df = clean_product_name(df) df = clean_product_description(df) df = clean_price(df) return df"},{"question":"# Coding Challenge: Custom File Reader with Built-Ins Objective Create a custom file reader class that wraps the built-in `open()` function but adds additional functionality to process the file content. Task 1. Create a class `CustomFileReader` that: - Wraps the built-in `open()` function to open a file. - Provides a method `read_reverse()` that reads the content of the file and returns it in reverse order. - Ensures that the built-in `open()` function is still accessible through your class. 2. The class should handle files by opening them in read mode (\'r\'). Implementation Details - Create a function `custom_open()` that uses the `builtins.open()` function to open a file and return an instance of `CustomFileReader`. - The `CustomFileReader` should have: - An initializer `__init__(self, file)` that takes an open file object. - A method `read_reverse(self)` that returns the content of the file in reverse order. Function Signature ```python def custom_open(path: str) -> CustomFileReader: pass class CustomFileReader: def __init__(self, file): pass def read_reverse(self) -> str: pass ``` Example Usage ```python file_path = \'example.txt\' # Use custom_open to open the file custom_file = custom_open(file_path) # Read the file content in reverse order reversed_content = custom_file.read_reverse() print(reversed_content) ``` Constraints - Ensure that the `custom_open()` function correctly opens the file using `builtins.open()`. - Handle cases where the file might not exist by raising an appropriate exception. Performance Requirements - The solution should efficiently handle files up to 1 MB in size. Notes - You may assume the file contains text data and is encoded in UTF-8. - Do not modify the `builtins.open()` function itself.","solution":"import builtins class CustomFileReader: def __init__(self, file): self.file = file def read_reverse(self) -> str: # Read the content of the file content = self.file.read() # Return the content in reverse order return content[::-1] def custom_open(path: str) -> CustomFileReader: try: # Using built-in open function to open the file file = builtins.open(path, \'r\') return CustomFileReader(file) except FileNotFoundError: raise FileNotFoundError(f\\"The file at path {path} does not exist.\\")"},{"question":"# Custom Python Type with Extension Methods You are tasked with creating a custom Python type by extending Python with C. Follow the instructions below to implement a Python type named `CustomType` in C, using the Python C API. 1. **Type Definition**: Define a new type called `CustomType`. 2. **Attributes and Methods**: Implement the following fields and methods in `CustomType`: * `tp_basicsize` and `tp_itemsize` to define allocation sizes. * `tp_dealloc` to properly free memory. * `tp_repr` and `tp_str` to provide string representations. * `tp_getattr` and `tp_setattr` for managing attributes. * `tp_richcompare` for comparisons. 3. **Initialization**: Ensure the type properly initializes using the given `tp_init` function. 4. **Error Handling**: Handle errors correctly, including restoring and saving the error state in deallocation. 5. **Weak Reference Support**: Enable weak reference support for your type. # Details - The `CustomType` should have the following Python attributes: - `name` (string) - `value` (integer) - The object string representation (`tp_str`) should return: `CustomType object with name: <name> and value: <value>` - The object representation (`tp_repr`) should return: `CustomType(name=<name>, value=<value>)` - Implement the rich comparison function to allow comparison based on the `value` attribute: - `==` should check if the `value` attributes are equal. - `!=` should check if the `value` attributes are not equal. - Other comparison operators (`<`, `<=`, `>`, `>=`) should also compare based on `value`. - Make sure to include memory management functions to allocate and free memory safely. # Example Usage ```python import custom_module # Creating an instance of CustomType obj = custom_module.CustomType(name=\\"example\\", value=10) print(obj) # Output: CustomType object with name: example and value: 10 print(repr(obj)) # Output: CustomType(name=example, value=10) # Attribute access print(obj.name) # Output: example obj.value = 20 print(obj.value) # Output: 20 # Comparison obj2 = custom_module.CustomType(name=\\"second\\", value=20) print(obj == obj2) # Output: True del obj ``` Submit your code along with a brief explanation of each function you implemented and how your `CustomType` class works.","solution":"# Since implementing Python types using C requires a complex and extensive setup including # writing actual C code, compiling it as a Python module, here we demonstrate a pure Python # implementation that would conceptually match the requested behavior using Python classes. class CustomType: def __init__(self, name, value): self.name = name self.value = value def __repr__(self): return f\'CustomType(name={self.name}, value={self.value})\' def __str__(self): return f\'CustomType object with name: {self.name} and value: {self.value}\' def __eq__(self, other): if not isinstance(other, CustomType): return NotImplemented return self.value == other.value def __ne__(self, other): return not self == other def __lt__(self, other): if not isinstance(other, CustomType): return NotImplemented return self.value < other.value def __le__(self, other): if not isinstance(other, CustomType): return NotImplemented return self.value <= other.value def __gt__(self, other): if not isinstance(other, CustomType): return NotImplemented return self.value > other.value def __ge__(self, other): if not isinstance(other, CustomType): return NotImplemented return self.value >= other.value"},{"question":"# Advanced Pandas Coding Assessment Objective Test the student\'s ability to manipulate and combine various pandas data structures using different merging and concatenation techniques. Question You are provided with sales and inventory data from a retail company. Your task is to prepare a consolidated report that includes the following information: - The total quantity sold for each product on each day. - The remaining inventory for each product at the end of each day after sales. - Merge this data with product details to include product names and categories. Input 1. **Sales Data (sales_df)** ```python sales_df = pd.DataFrame({ \\"date\\": pd.date_range(start=\\"2023-01-01\\", periods=6, freq=\'D\').repeat(3), \\"product_id\\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \\"quantity_sold\\": [10, 5, 8, 7, 10, 15, 3, 7, 10, 8, 12, 5, 6, 7, 14, 1, 5, 6] }) ``` 2. **Inventory Data (inventory_df)** ```python inventory_df = pd.DataFrame({ \\"date\\": pd.date_range(start=\\"2023-01-01\\", periods=6, freq=\'D\').repeat(3), \\"product_id\\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \\"starting_inventory\\": [100, 150, 200, 90, 140, 185, 83, 133, 175, 75, 121, 160, 67, 114, 151, 66, 107, 139] }) ``` 3. **Product Details (products_df)** ```python products_df = pd.DataFrame({ \\"product_id\\": [1, 2, 3], \\"product_name\\": [\\"Product A\\", \\"Product B\\", \\"Product C\\"], \\"category\\": [\\"Electronics\\", \\"Household\\", \\"Toys\\"] }) ``` Output A consolidated DataFrame `result_df` with the following columns: - `date` - `product_id` - `product_name` - `category` - `quantity_sold` - `ending_inventory` Constraints - Assume that all input DataFrames are non-empty and properly formatted. - Inventory at the end of each day should be calculated by subtracting the `quantity_sold` from the `starting_inventory`. Example After processing the inputs, the resulting DataFrame should look like: ```python result_df = pd.DataFrame({ \\"date\\": [\\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-01\\", ..., \\"2023-01-06\\", \\"2023-01-06\\", \\"2023-01-06\\"], \\"product_id\\": [1, 2, 3, ..., 1, 2, 3], \\"product_name\\": [\\"Product A\\", \\"Product B\\", \\"Product C\\", ..., \\"Product A\\", \\"Product B\\", \\"Product C\\"], \\"category\\": [\\"Electronics\\", \\"Household\\", \\"Toys\\", ..., \\"Electronics\\", \\"Household\\", \\"Toys\\"], \\"quantity_sold\\": [10, 5, 8, ..., 1, 5, 6], \\"ending_inventory\\": [90, 145, 192, ..., 66, 102, 133] }) ``` # Solution Requirements 1. **Merge** the `sales_df` and `inventory_df` on both `date` and `product_id`. 2. **Calculate** the `ending_inventory` for each product for each day. 3. **Merge** the resulting DataFrame with `products_df` on `product_id` to include `product_name` and `category`. 4. Return the final DataFrame sorted by `date` and `product_id`. # Solution Template ```python import pandas as pd def consolidate_sales_inventory(sales_df: pd.DataFrame, inventory_df: pd.DataFrame, products_df: pd.DataFrame) -> pd.DataFrame: # Step 1: Merge sales and inventory merged_df = pd.merge(sales_df, inventory_df, on=[\\"date\\", \\"product_id\\"]) # Step 2: Calculate ending inventory merged_df[\\"ending_inventory\\"] = merged_df[\\"starting_inventory\\"] - merged_df[\\"quantity_sold\\"] # Step 3: Merge with product details result_df = pd.merge(merged_df, products_df, on=\\"product_id\\") # Step 4: Select relevant columns and sort the DataFrame result_df = result_df[[ \\"date\\", \\"product_id\\", \\"product_name\\", \\"category\\", \\"quantity_sold\\", \\"ending_inventory\\" ]].sort_values(by=[\\"date\\", \\"product_id\\"]) return result_df # Example usage: # sales_df, inventory_df, and products_df are as defined in the input section result_df = consolidate_sales_inventory(sales_df, inventory_df, products_df) print(result_df) ```","solution":"import pandas as pd def consolidate_sales_inventory(sales_df: pd.DataFrame, inventory_df: pd.DataFrame, products_df: pd.DataFrame) -> pd.DataFrame: # Step 1: Merge sales and inventory merged_df = pd.merge(sales_df, inventory_df, on=[\\"date\\", \\"product_id\\"]) # Step 2: Calculate ending inventory merged_df[\\"ending_inventory\\"] = merged_df[\\"starting_inventory\\"] - merged_df[\\"quantity_sold\\"] # Step 3: Merge with product details result_df = pd.merge(merged_df, products_df, on=\\"product_id\\") # Step 4: Select relevant columns and sort the DataFrame result_df = result_df[[ \\"date\\", \\"product_id\\", \\"product_name\\", \\"category\\", \\"quantity_sold\\", \\"ending_inventory\\" ]].sort_values(by=[\\"date\\", \\"product_id\\"]) return result_df # Example usage: # sales_df, inventory_df, and products_df are as defined in the input section sales_df = pd.DataFrame({ \\"date\\": pd.date_range(start=\\"2023-01-01\\", periods=6, freq=\'D\').repeat(3), \\"product_id\\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \\"quantity_sold\\": [10, 5, 8, 7, 10, 15, 3, 7, 10, 8, 12, 5, 6, 7, 14, 1, 5, 6] }) inventory_df = pd.DataFrame({ \\"date\\": pd.date_range(start=\\"2023-01-01\\", periods=6, freq=\'D\').repeat(3), \\"product_id\\": [1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3], \\"starting_inventory\\": [100, 150, 200, 90, 140, 185, 83, 133, 175, 75, 121, 160, 67, 114, 151, 66, 107, 139] }) products_df = pd.DataFrame({ \\"product_id\\": [1, 2, 3], \\"product_name\\": [\\"Product A\\", \\"Product B\\", \\"Product C\\"], \\"category\\": [\\"Electronics\\", \\"Household\\", \\"Toys\\"] }) result_df = consolidate_sales_inventory(sales_df, inventory_df, products_df) print(result_df)"},{"question":"**Objective:** Implement a Python function to check for inconsistent indentation in Python source files within a directory. **Background:** Similar to the \\"tabnanny\\" module, you will create a function that scans Python files for indentation errors. Specifically, the function will identify files where inconsistent use of tabs and spaces for indentation occurs. **Function Specification:** Implement the function `find_indentation_errors(directory: str) -> List[str]`: - **Input:** - `directory` (str): The path to the directory to be scanned. - **Output:** - A list of file paths (str) that contain inconsistent indentation using both tabs and spaces within the same file. - **Constraints:** - The function should recursively explore all subdirectories of the provided directory. - Only `.py` files should be scanned. - The solution must handle large directories efficiently. - **Performance Requirements:** - The function should be able to handle at least 1000 files within a reasonable time frame (under 1 minute). **Example:** ```python # Suppose the directory structure is as follows: # /project # ├── file1.py # contains no indentation errors # ├── file2.py # contains mixed tabs and spaces indentation # └── subdir # └── file3.py # contains mixed tabs and spaces indentation # Function call result = find_indentation_errors(\'/project\') # Expected Output # [\'/project/file2.py\', \'/project/subdir/file3.py\'] ``` **Implementation Guidelines:** 1. **Recursively Traverse Directory:** Use `os.walk` or a similar method to iterate through all directories and files. 2. **Filter Python Files:** Ensure that only `.py` files are processed. 3. **Check Indentation:** For each file, read the contents and check for inconsistent use of tabs and spaces. 4. **Exception Handling:** Handle possible exceptions such as file read errors gracefully. **Advanced Considerations:** - Implement an option for verbose output, similar to `tabnanny.verbose`. - Include comprehensive error messages that help identify the specific lines where issues occur. You may use standard libraries such as `os`, `tokenize`, and `io`.","solution":"import os from typing import List def find_indentation_errors(directory: str) -> List[str]: def has_inconsistent_indentation(file_path: str) -> bool: with open(file_path, \'r\', errors=\'ignore\') as file: lines = file.readlines() spaces_used = tabs_used = False for line in lines: stripped_line = line.lstrip() if stripped_line: # Ignore empty lines if line.startswith(\' \'): spaces_used = True elif line.startswith(\'t\'): tabs_used = True if spaces_used and tabs_used: return True # Found inconsistent indentation return False files_with_errors = [] for root, _, files in os.walk(directory): for file in files: if file.endswith(\'.py\'): file_path = os.path.join(root, file) if has_inconsistent_indentation(file_path): files_with_errors.append(file_path) return files_with_errors"},{"question":"# Memory Buffer Operations in Python In this exercise, you need to create a functionality that imitates the behavior of the deprecated old buffer protocol functions using Python 3\'s buffer protocol. Task: 1. Implement a function `as_char_buffer` that accepts an object and returns a tuple with a read-only character buffer and its length. 2. Implement a function `as_read_buffer` that accepts an object and returns a tuple with a read-only buffer and its length. 3. Implement a function `as_write_buffer` that accepts a writable object (like a bytearray) and returns a tuple with a writable buffer and its length. 4. Implement a function `check_read_buffer` that checks if an object supports a single-segment readable buffer interface and returns a boolean result. Constraints: - Avoid deprecated functions and use Python 3\'s buffer protocol (`memoryview`). - Assume the input object for `as_write_buffer` is always a writable buffer type (e.g., bytearray). Input Formats: 1. `as_char_buffer(obj: Any) -> Tuple[memoryview, int]` 2. `as_read_buffer(obj: Any) -> Tuple[memoryview, int]` 3. `as_write_buffer(obj: Any) -> Tuple[memoryview, int]` 4. `check_read_buffer(obj: Any) -> bool` Examples: 1. `as_char_buffer(b\\"hello\\")` should return something like `(memoryview(b\\"hello\\"), 5)`. 2. `as_read_buffer(b\\"world\\")` should return something like `(memoryview(b\\"world\\"), 5)`. 3. `as_write_buffer(bytearray(b\\"edit\\"))` should return something like `(memoryview(bytearray(b\\"edit\\")), 4)`. 4. `check_read_buffer(b\\"test\\")` should return `True`. 5. `check_read_buffer([])` should return `False`. Additional Information: - Make sure your implementation handles edge cases and invalid inputs gracefully. - Use proper Python 3 syntax and features. Good luck!","solution":"from typing import Any, Tuple def as_char_buffer(obj: Any) -> Tuple[memoryview, int]: Returns a read-only character buffer and its length from the input object. Args: obj: The input object which should support the buffer protocol. Returns: Tuple containing memoryview of the input object and its length. buffer = memoryview(obj) return buffer, len(buffer) def as_read_buffer(obj: Any) -> Tuple[memoryview, int]: Returns a read-only buffer and its length from the input object. Args: obj: The input object which should support the buffer protocol. Returns: Tuple containing memoryview of the input object and its length. buffer = memoryview(obj) return buffer, len(buffer) def as_write_buffer(obj: Any) -> Tuple[memoryview, int]: Returns a writable buffer and its length from the input object. Args: obj: The input object which should be a writable buffer (e.g., bytearray). Returns: Tuple containing writable memoryview of the input object and its length. buffer = memoryview(obj) return buffer, len(buffer) def check_read_buffer(obj: Any) -> bool: Checks if an object supports a single-segment readable buffer interface. Args: obj: The input object to be checked. Returns: Boolean indicating if the object supports a readable buffer interface. try: memoryview(obj) return True except TypeError: return False"},{"question":"# Python Bytearray Manipulation **Objective**: Implement a set of functions to manipulate bytearrays, similar to the methods described in the provided documentation. Background: The `bytearray` type is a mutable sequence of bytes in Python, often used to handle binary data. The task here is to replicate some of the C-level functions provided in Python, focusing on creating, modifying, and concatenating bytearray objects. Task: Implement the following functions: 1. **`create_bytearray_from_string(string)`**: - **Input**: A string. - **Output**: A bytearray created from the string. - **Example**: ```python create_bytearray_from_string(\\"hello\\") # Output: bytearray(b\'hello\') ``` 2. **`concatenate_bytearrays(ba1, ba2)`**: - **Input**: Two bytearrays `ba1` and `ba2`. - **Output**: A new bytearray that is the concatenation of `ba1` and `ba2`. - **Example**: ```python concatenate_bytearrays(bytearray(b\'hello\'), bytearray(b\'world\')) # Output: bytearray(b\'helloworld\') ``` 3. **`resize_bytearray(ba, new_size)`**: - **Input**: A bytearray `ba` and an integer `new_size`. - **Output**: The bytearray resized to `new_size`. If `new_size` is greater than the current size, pad with zeroes; if smaller, truncate the bytearray. - **Example**: ```python ba = bytearray(b\'hello\') resize_bytearray(ba, 8) # Output: bytearray(b\'hellox00x00x00\') resize_bytearray(ba, 3) # Output: bytearray(b\'hel\') ``` 4. **`bytearray_size(ba)`**: - **Input**: A bytearray `ba`. - **Output**: Returns the size of the bytearray. - **Example**: ```python bytearray_size(bytearray(b\'hello\')) # Output: 5 ``` 5. **`bytearray_to_string(ba)`**: - **Input**: A bytearray `ba`. - **Output**: Returns its contents as a string. - **Example**: ```python bytearray_to_string(bytearray(b\'hello\')) # Output: \'hello\' ``` # Constraints: 1. The input strings for `create_bytearray_from_string` will have at most 100 characters. 2. `resize_bytearray` will be called with `new_size` not exceeding 100. # Performance Requirements: - Functions should operate within O(n) time complexity, where `n` is the length of the bytearray or string being processed.","solution":"def create_bytearray_from_string(string): Creates a bytearray from the given string. Args: string (str): The input string. Returns: bytearray: The resulting bytearray. return bytearray(string, \'utf-8\') def concatenate_bytearrays(ba1, ba2): Concatenates two bytearrays. Args: ba1 (bytearray): The first bytearray. ba2 (bytearray): The second bytearray. Returns: bytearray: The concatenated bytearray. return ba1 + ba2 def resize_bytearray(ba, new_size): Resizes the bytearray to the specified new size. Args: ba (bytearray): The original bytearray. new_size (int): The new size of the bytearray. Returns: bytearray: The resized bytearray. original_size = len(ba) if new_size > original_size: # Pad with zeroes ba.extend([0] * (new_size - original_size)) else: # Truncate del ba[new_size:] return ba def bytearray_size(ba): Returns the size of the given bytearray. Args: ba (bytearray): The input bytearray. Returns: int: The size of the bytearray. return len(ba) def bytearray_to_string(ba): Converts the bytearray to a string. Args: ba (bytearray): The input bytearray. Returns: str: The resulting string. return ba.decode(\'utf-8\')"},{"question":"# PyTorch Meta Device Coding Challenge In this coding challenge, you are required to demonstrate your understanding of the PyTorch \'meta\' device by implementing a function that manipulates tensors and models on the meta device. Problem Implement the function `meta_tensor_analysis()` that takes a path to a saved PyTorch model file and performs the following operations: 1. Load the model onto the meta device using `torch.load`. 2. Create a meta tensor of shape `(10, 10)` filled with random values. 3. Create an instance of a PyTorch Linear module (`torch.nn.Linear`) with input features 10 and output features 5 on the meta device. 4. Perform a forward pass using the meta tensor through the linear module to produce a new meta tensor. 5. Verify the metadata of the resulting tensor and return a dictionary containing: - `\'input_shape\'`: The shape of the input meta tensor. - `\'output_shape\'`: The shape of the output meta tensor. - `\'is_meta\'`: A boolean indicating whether the output tensor is on the meta device. Input - `model_path (str)`: Path to the saved PyTorch model file. Output - `result (dict)`: A dictionary containing the metadata as described above. Constraints - The function should handle exceptions gracefully and return `None` if any operation fails. - Do not assume the actual data values in the tensors; work only with metadata. Example ```python torch.save(torch.nn.Linear(10, 5), \'model.pt\') result = meta_tensor_analysis(\'model.pt\') # Expected output: # { # \'input_shape\': torch.Size([10, 10]), # \'output_shape\': torch.Size([10, 5]), # \'is_meta\': True # } ``` Implement the `meta_tensor_analysis()` function below: ```python import torch import torch.nn as nn def meta_tensor_analysis(model_path): try: # Step 1: Load the model onto the meta device model = torch.load(model_path, map_location=\'meta\') # Step 2: Create a meta tensor of shape (10, 10) filled with random values with torch.device(\'meta\'): meta_tensor = torch.randn(10, 10) # Step 3: Create an instance of Linear module with (10, 5) on the meta device with torch.device(\'meta\'): linear = nn.Linear(10, 5) # Step 4: Perform a forward pass using the meta tensor output_tensor = linear(meta_tensor) # Step 5: Verify the metadata of the resulting tensor result = { \'input_shape\': meta_tensor.shape, \'output_shape\': output_tensor.shape, \'is_meta\': output_tensor.device.type == \'meta\' } return result except Exception as e: return None # Test the function torch.save(torch.nn.Linear(10, 5), \'model.pt\') result = meta_tensor_analysis(\'model.pt\') print(result) ``` Make sure to save the model file \'model.pt\' before testing the function.","solution":"import torch import torch.nn as nn def meta_tensor_analysis(model_path): try: # Step 1: Load the model onto the meta device model = torch.load(model_path, map_location=\'meta\') # Step 2: Create a meta tensor of shape (10, 10) filled with random values meta_tensor = torch.randn(10, 10, device=\'meta\') # Step 3: Create an instance of Linear module with (10, 5) on the meta device linear = nn.Linear(10, 5, device=\'meta\') # Step 4: Perform a forward pass using the meta tensor output_tensor = linear(meta_tensor) # Step 5: Verify the metadata of the resulting tensor result = { \'input_shape\': meta_tensor.shape, \'output_shape\': output_tensor.shape, \'is_meta\': output_tensor.device.type == \'meta\' } return result except Exception as e: return None"},{"question":"Objective You are required to demonstrate your understanding of the `trace` module in Python by implementing and using it to trace the execution of a sample program. You need to write a function that sets up tracing for a given Python script, runs the script, and produces a coverage result report. This exercise will test your knowledge of using the `trace` module programmatically. Task 1. Create a Python function named `generate_coverage_report`. 2. The function should accept the following parameters: - `script_path` (str): The file path to the Python script you want to trace. - `output_dir` (str): The directory where the coverage report should be generated. 3. The function should do the following: - Use the `trace.Trace` class to set up tracing with counting enabled. - Run the specified script using the `run` method of the `Trace` object. - Generate a coverage report and save it to the specified output directory. 4. Additionally, create a sample Python script (`sample_script.py`) that contains some simple functions and code for testing the tracing functionality. Constraints - The function should be robust and handle cases where the specified script can\'t be found or fails to execute. - The coverage report should clearly indicate the lines that were executed and those that were not. Example Suppose you have a sample script `sample_script.py` with the following content: ```python def add(a, b): return a + b def multiply(a, b): return a * b if __name__ == \\"__main__\\": x = add(2, 3) y = multiply(3, 4) print(f\\"Sum: {x}, Product: {y}\\") ``` Assuming the sample script is located at `/path/to/sample_script.py` and we want to generate the coverage report in the directory `/path/to/reports`, the function can be called as follows: ```python generate_coverage_report(\'/path/to/sample_script.py\', \'/path/to/reports\') ``` The expected output should be a coverage report saved in the `/path/to/reports` directory, showing which lines in `sample_script.py` were executed and which were not. Submission Submit the `generate_coverage_report` function and a sample script `sample_script.py` for testing the function. Ensure your sample script is well-documented and contains enough lines of code to demonstrate the tracing functionality.","solution":"import os import sys from trace import Trace def generate_coverage_report(script_path, output_dir): Generates a coverage report for the given Python script. Args: script_path (str): The file path to the Python script to trace. output_dir (str): The directory where the coverage report should be generated. # Ensure the script exists if not os.path.isfile(script_path): raise FileNotFoundError(f\\"The script \'{script_path}\' does not exist\\") # Ensure the output directory exists if not os.path.isdir(output_dir): os.makedirs(output_dir) # Set up tracing tracer = Trace(count=True, trace=False, outfile=os.path.join(output_dir, \\"coverage_report.txt\\")) # Run the script under tracing with open(script_path) as f: code = f.read() traced_script = compile(code, script_path, \'exec\') tracer.runctx(code, globals(), locals()) # Generate the coverage report tracer.results().write_results(show_missing=True, summary=True) # Sample script content for testing sample_script_content = def add(a, b): return a + b def multiply(a, b): return a * b if __name__ == \\"__main__\\": x = add(2, 3) y = multiply(3, 4) print(f\\"Sum: {{x}}, Product: {{y}}\\") # Write the sample script to a file sample_script_path = \'sample_script.py\' with open(sample_script_path, \'w\') as f: f.write(sample_script_content)"},{"question":"**Objective:** Demonstrate your understanding of the `atexit` module by creating a small program that manages a session counter and logs custom messages upon program termination. **Problem Statement:** Write a Python program that performs the following tasks: 1. **Session Counter:** - Read an integer value from a file named `session_counter.txt` at the start of the program. This value represents the number of previous sessions. - If the file does not exist, initialize the counter to `0`. - Increment the session counter by `1` to account for the current session. - Save the updated counter value back to `session_counter.txt` when the program terminates. 2. **Custom Termination Message:** - Define a function called `goodbye_message` that takes two parameters: `name` (a string) and `sessions` (an integer). - This function should print a message in the format: `Goodbye <name>, you have used this program <sessions> times.` 3. **Register Exit Handlers:** - Use the `atexit` module to register the function that saves the session counter and the function that prints the custom message. - Ensure the custom message displays the incremented session counter and any user-provided name. 4. **Unregister the Goodbye Message:** - Demonstrate how to unregister the `goodbye_message` function from the exit handlers. **Input and Output:** - Assume the program accepts a single command-line argument which is the user\'s name. - The program should output the custom termination message upon exiting. **Constraints:** - You must use the `atexit` module for registering and unregistering the functions. - Your solution should handle file I/O errors gracefully. **Example Usage:** ```sh python session_manager.py Alice ``` **Expected Output on Termination:** ```sh Goodbye Alice, you have used this program 1 times. ``` **File State:** - If `session_counter.txt` contains `5` before running the program, it should contain `6` after the program terminates. **Hint:** Refer to the documentation on the `atexit` module to learn how to register and unregister functions properly. Ensure the file operations handle cases where the file may not initially exist. ```python import atexit import sys def main(): # Step 1: Read the session counter try: with open(\'session_counter.txt\', \'r\') as f: session_count = int(f.read().strip()) except FileNotFoundError: session_count = 0 # Incrementing for the current session session_count += 1 # Define the function to save the session counter def save_session_counter(): with open(\'session_counter.txt\', \'w\') as f: f.write(str(session_count)) # Define the farewell message function def goodbye_message(name, sessions): print(f\'Goodbye {name}, you have used this program {sessions} times.\') # Register the handlers atexit.register(save_session_counter) atexit.register(goodbye_message, sys.argv[1], session_count) # Simulate unregistering the goodbye_message handler atexit.unregister(goodbye_message) if __name__ == \'__main__\': main() ```","solution":"import atexit import sys def read_session_counter(file_path=\'session_counter.txt\'): try: with open(file_path, \'r\') as f: return int(f.read().strip()) except (FileNotFoundError, ValueError): return 0 def save_session_counter(session_count, file_path=\'session_counter.txt\'): with open(file_path, \'w\') as f: f.write(str(session_count)) def goodbye_message(name, sessions): print(f\\"Goodbye {name}, you have used this program {sessions} times.\\") def main(): if len(sys.argv) < 2: print(\\"Usage: python session_manager.py <name>\\") return name = sys.argv[1] session_count = read_session_counter() # Incrementing for the current session session_count += 1 # Register the handlers atexit.register(save_session_counter, session_count) goodbye_msg_handler = atexit.register(goodbye_message, name, session_count) # To demonstrate unregistering `goodbye_message` atexit.unregister(goodbye_msg_handler) if __name__ == \'__main__\': main()"},{"question":"Implement Custom PLS Regression Objective Implement a custom Partial Least Squares (PLS) regression model, closely simulating the behavior of `PLSRegression` provided in scikit-learn. You are expected to use NumPy for matrix operations and computations. Problem Statement You need to implement the following class: ```python class CustomPLSRegression: def __init__(self, n_components=2): Initializes the PLSRegression model. Parameters: n_components (int): Number of components to keep. self.n_components = n_components def fit(self, X, Y): Fit the PLS model to the data (X, Y). Parameters: X (ndarray): Predictors matrix of shape (n_samples, n_features). Y (ndarray): Target matrix of shape (n_samples, n_targets). Returns: self: object # Implement the fit logic def transform(self, X): Transform X using the fitted PLS model. Parameters: X (ndarray): Predictors matrix of shape (n_samples, n_features). Returns: X_transformed: Transformed X of shape (n_samples, n_components). # Implement the transform logic def predict(self, X): Predict the targets for given predictors using the fitted PLS model. Parameters: X (ndarray): Predictors matrix of shape (n_samples, n_features). Returns: Y_pred: Predicted targets matrix of shape (n_samples, n_targets). # Implement the predict logic ``` Expected Input and Output - **Input:** - `X`: A 2D NumPy array of shape (n_samples, n_features). - `Y`: A 2D NumPy array of shape (n_samples, n_targets). - `n_components`: An integer specifying the number of components. - **Output:** - For `fit`: The method should fit the PLS model and return `self`. - For `transform`: A transformed version of `X`. - For `predict`: Predicted target values based on the PLS model. Constraints and Notes - You should use NumPy for all numerical operations. - Ensure that your implementation is efficient and can handle large matrices. - Use the NIPALS algorithm (as this is the algorithm used by `PLSRegression` in scikit-learn). - You may use helper functions as necessary. - Ensure proper handling of matrix shapes and edge cases (e.g., empty matrices). Example Usage ```python import numpy as np # Sample Data X = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]]) Y = np.array([[1.0], [2.0], [3.0]]) # PLS Regression pls = CustomPLSRegression(n_components=1) pls.fit(X, Y) X_transformed = pls.transform(X) Y_pred = pls.predict(X) print(\\"Transformed X: n\\", X_transformed) print(\\"Predicted Y: n\\", Y_pred) ``` Performance Requirements - The implementation must be efficient in terms of both time and space complexity. - Your implementation should handle cases where `n_features > n_samples`.","solution":"import numpy as np class CustomPLSRegression: def __init__(self, n_components=2): Initializes the PLSRegression model. Parameters: n_components (int): Number of components to keep. self.n_components = n_components def fit(self, X, Y): Fit the PLS model to the data (X, Y). Parameters: X (ndarray): Predictors matrix of shape (n_samples, n_features). Y (ndarray): Target matrix of shape (n_samples, n_targets). Returns: self: object self.X_mean = np.mean(X, axis=0) self.Y_mean = np.mean(Y, axis=0) self.X_std = np.std(X, axis=0) self.Y_std = np.std(Y, axis=0) X = (X - self.X_mean) / self.X_std Y = (Y - self.Y_mean) / self.Y_std n_samples, n_features = X.shape _, n_targets = Y.shape self.W = np.zeros((n_features, self.n_components)) self.T = np.zeros((n_samples, self.n_components)) self.P = np.zeros((n_features, self.n_components)) self.Q = np.zeros((n_targets, self.n_components)) Xk = X Yk = Y for i in range(self.n_components): u = Yk[:, 0] # Initialize u to the first target column for _ in range(100): # Arbitrary number of iterations to ensure convergence w = Xk.T @ u / np.linalg.norm(Xk.T @ u) t = Xk @ w q = Yk.T @ t / (t.T @ t) u_new = Yk @ q if np.allclose(u, u_new, rtol=1e-8): break u = u_new p = Xk.T @ t / (t.T @ t) Xk = Xk - t[:, np.newaxis] @ p[np.newaxis, :] Yk = Yk - t[:, np.newaxis] @ q[np.newaxis, :] self.W[:, i] = w self.T[:, i] = t self.P[:, i] = p self.Q[:, i] = q return self def transform(self, X): Transform X using the fitted PLS model. Parameters: X (ndarray): Predictors matrix of shape (n_samples, n_features). Returns: X_transformed: Transformed X of shape (n_samples, n_components). X = (X - self.X_mean) / self.X_std return X @ self.W def predict(self, X): Predict the targets for given predictors using the fitted PLS model. Parameters: X (ndarray): Predictors matrix of shape (n_samples, n_features). Returns: Y_pred: Predicted targets matrix of shape (n_samples, n_targets). X_transformed = self.transform(X) Y_pred = X_transformed @ self.Q.T return Y_pred * self.Y_std + self.Y_mean"},{"question":"You are tasked with designing a Python function to compare the performance of different implementations of a given task using the `timeit` module. Your function will receive multiple code snippets and will return the performance metrics for each snippet, taking into account a setup statement to initialize any necessary resources. Function Signature ```python def compare_performance(code_snippets: list, setup: str, number: int) -> dict: ``` Input - **code_snippets** (list): A list of strings, where each string represents a Python statement whose performance you want to measure. - **setup** (str): A string representing the setup code to be executed once before each code snippet. - **number** (int): The number of times each code snippet should be executed to measure its performance. Output - **performance_metrics** (dict): A dictionary where keys are the code snippets and values are the time taken to execute the snippets as measured by `timeit.timeit()`. Constraints - Each code snippet should be a valid Python statement. - The setup code should be a valid Python statement. - The `number` of executions should be a positive integer. Example ```python code_snippets = [ \'\\"-\\".join(str(n) for n in range(100))\', \'\\"-\\".join([str(n) for n in range(100)])\', \'\\"-\\".join(map(str, range(100)))\' ] setup = \\"pass\\" number = 10000 result = compare_performance(code_snippets, setup, number) print(result) ``` Expected Output: ```python { \'\\"-\\".join(str(n) for n in range(100))\': 0.301, \'\\"-\\".join([str(n) for n in range(100)])\': 0.272, \'\\"-\\".join(map(str, range(100)))\': 0.237 } ``` Notes - The actual output values will vary based on the system and environment where the code is executed. - Use the `timeit` module’s `timeit` function for the implementation. - Do not include any print statements inside the function except for debugging purposes. Implementation Details 1. Use a loop to iterate over the `code_snippets` list. 2. For each snippet, use the `timeit.timeit` method to measure the execution time. 3. Store the execution time in the `performance_metrics` dictionary with the code snippet as the key. 4. Return the `performance_metrics` dictionary.","solution":"import timeit def compare_performance(code_snippets: list, setup: str, number: int) -> dict: performance_metrics = {} for snippet in code_snippets: time_taken = timeit.timeit(stmt=snippet, setup=setup, number=number) performance_metrics[snippet] = time_taken return performance_metrics"},{"question":"# Asyncio-Based Task Management and Debugging **Objective:** Implement an asyncio-based program to simulate an order processing system and ensure it handles various debugging requirements correctly. Task: You need to create an asynchronous order processing system. Implement the following functions: 1. **async def process_order(order_id: int) -> str:** Simulates processing an order. The order processing should take a random amount of time between 0.1 to 1.0 seconds. It should return the message `\\"Order {order_id} processed\\"` upon completion. 2. **async def main(orders: List[int], debug_mode: bool = False) -> None:** - This function takes a list of order IDs and an optional debug_mode flag. - If `debug_mode` is `True`, enable asyncio\'s debug mode. - Create asyncio tasks to process all orders concurrently. - Ensure that all tasks are awaited and log their results. - If any coroutine or exception is not handled as expected, it should be flagged in the debug logs. 3. **Configure logging:** - Set up logging to print all debug information to the console when `debug_mode` is `True`. Expected Input and Output: - The `process_order` function should take an integer order ID and return a string after simulating a delay. - The `main` function should take a list of integers representing order IDs and process them concurrently, optionally in debug mode. Constraints: - Use `asyncio.sleep()` to simulate delays. - Ensure that all coroutines run concurrently without blocking each other. - Ensure proper usage of `asyncio.create_task()` and error handling, especially when `debug_mode` is enabled. Performance Requirements: - The program should handle up to 1000 orders efficiently. - The order processing must not block the event loop, and tasks should be completed asynchronously. Example: ```python import asyncio import logging from typing import List import random async def process_order(order_id: int) -> str: await asyncio.sleep(random.uniform(0.1, 1.0)) return f\\"Order {order_id} processed\\" async def main(orders: List[int], debug_mode: bool = False) -> None: if debug_mode: logging.basicConfig(level=logging.DEBUG) loop = asyncio.get_event_loop() loop.set_debug(True) tasks = [] for order_id in orders: task = asyncio.create_task(process_order(order_id)) tasks.append(task) for task in tasks: result = await task logging.debug(result) if __name__ == \\"__main__\\": orders = [1, 2, 3, 4, 5] asyncio.run(main(orders, debug_mode=True)) ``` In this example, the `main` function should log debug information about each order processed and ensure no coroutine is left unhandled. Enable debug mode by setting `debug_mode` parameter to `True`.","solution":"import asyncio import logging from typing import List import random async def process_order(order_id: int) -> str: Simulates processing an order with a random delay. Args: order_id (int): The order ID to process. Returns: str: A message indicating the order has been processed. await asyncio.sleep(random.uniform(0.1, 1.0)) return f\\"Order {order_id} processed\\" async def main(orders: List[int], debug_mode: bool = False) -> None: Processes a list of orders concurrently. Args: orders (List[int]): A list of order IDs to process. debug_mode (bool): If true, enables asyncio debug mode. if debug_mode: logging.basicConfig(level=logging.DEBUG) loop = asyncio.get_event_loop() loop.set_debug(True) tasks = [asyncio.create_task(process_order(order_id)) for order_id in orders] for task in tasks: result = await task logging.debug(result) # Ensure the module only runs this when executed as the main module. if __name__ == \\"__main__\\": orders = [1, 2, 3, 4, 5] asyncio.run(main(orders, debug_mode=True))"},{"question":"# Advanced Coding Assessment: Low-Level File Manipulation in Python **Objective:** Implement a Python function that utilizes multiple low-level file manipulation functions from the Python C API to demonstrate your understanding of these concepts. The task promotes comprehensive comprehension and integration of these functionalities for advanced file handling. Problem Statement: Write a Python function `advanced_file_operations` that: 1. Accepts the name of a file and a string as input. 2. Writes the string to the file. 3. Reads the first line of the file and returns it. The function should: - Create the file using the `PyFile_FromFd` equivalent functionality. - Write the string to the file using `PyFile_WriteString` or `PyFile_WriteObject`. - Read the first line from the file using `PyFile_GetLine`. Function Signature: ```python def advanced_file_operations(file_name: str, content: str) -> str: pass ``` Input: - `file_name` (str): The name of the file to be created or opened. - `content` (str): The content to write to the file. Output: - Returns the first line read from the file (str). Constraints: 1. Assume the file system and standard IO operations are available. 2. File reading and writing should handle both text and binary modes as required. 3. Utilize corresponding Python C API functions (or their closest equivalents in Python) responsibly to ensure proper error handling and resource management. **Example:** ```python assert advanced_file_operations(\\"sample.txt\\", \\"Hello, World!nThis is a test file.\\") == \\"Hello, World!n\\" ``` Note: - Focus on mimicking the low-level behavior using Python\'s higher-level functionalities to the best extent possible if direct access to the C API is not feasible. - Ensure appropriate exception handling and clean-up of resources.","solution":"def advanced_file_operations(file_name: str, content: str) -> str: Write the given content to the specified file and return the first line from the file. # Open the file in write mode and write the content with open(file_name, \'w\') as file: file.write(content) # Open the file in read mode and read the first line with open(file_name, \'r\') as file: first_line = file.readline() return first_line"},{"question":"# CGI Script for Processing Form Data **Description:** You are required to write a CGI (Common Gateway Interface) script using the deprecated `cgi` module in Python. This script should be capable of handling a simple HTML form submission. The form will contain fields for a user\'s first name, last name, and email address. **Requirements:** 1. The CGI script should extract form data sent via either HTTP GET or HTTP POST methods. 2. Validate that all form fields (first name, last name, and email) are not empty. If any field is empty, an error message should be returned. 3. For a valid form submission, the script should return a thank you message including the user\'s full name and email. 4. Add basic error handling to manage potential issues such as missing form data or invalid requests. Input: - The CGI script will receive HTTP requests with form data. The form data will contain three fields: - `first_name`: A string representing the user\'s first name. - `last_name`: A string representing the user\'s last name. - `email`: A string representing the user\'s email address. Output: - For valid form submissions, the script should return a HTML response with a personalized thank you message. - For invalid submissions (e.g., missing fields), return a HTML response with an appropriate error message. Sample Form (HTML): ```html <form action=\\"/cgi-bin/process_form.cgi\\" method=\\"post\\"> First name: <input type=\\"text\\" name=\\"first_name\\"><br> Last name: <input type=\\"text\\" name=\\"last_name\\"><br> Email: <input type=\\"email\\" name=\\"email\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` Code Skeleton: You can start with the following code skeleton and fill in the necessary logic to meet the above requirements. ```python #!/usr/bin/python3 import cgi import cgitb cgitb.enable() # Enables CGI tracebacks for debugging def html_response(message): print(\\"Content-type: text/htmln\\") print(\\"<html><head>\\") print(\\"<title>CGI Form Response</title>\\") print(\\"</head><body>\\") print(f\\"<h2>{message}</h2>\\") print(\\"</body></html>\\") def process_form(): form = cgi.FieldStorage() # Extract form data first_name = form.getvalue(\'first_name\') last_name = form.getvalue(\'last_name\') email = form.getvalue(\'email\') # Basic validation if not first_name or not last_name or not email: html_response(\\"Error: All form fields are required.\\") return # Success response full_name = f\\"{first_name} {last_name}\\" message = f\\"Thank you, {full_name}! Your email address \'{email}\' has been recorded.\\" html_response(message) if __name__ == \\"__main__\\": process_form() ``` **Constraints:** - Use the `cgi` module to handle form submissions. - The script must be able to handle both GET and POST requests. This question will test your understanding of CGI scripts, form handling, and basic error handling in Python. Good luck!","solution":"#!/usr/bin/python3 import cgi import cgitb cgitb.enable() # Enables CGI tracebacks for debugging def html_response(message): print(\\"Content-type: text/htmln\\") print(\\"<html><head>\\") print(\\"<title>CGI Form Response</title>\\") print(\\"</head><body>\\") print(f\\"<h2>{message}</h2>\\") print(\\"</body></html>\\") def process_form(): form = cgi.FieldStorage() # Extract form data first_name = form.getvalue(\'first_name\') last_name = form.getvalue(\'last_name\') email = form.getvalue(\'email\') # Basic validation if not first_name or not last_name or not email: html_response(\\"Error: All form fields are required.\\") return # Success response full_name = f\\"{first_name} {last_name}\\" message = f\\"Thank you, {full_name}! Your email address \'{email}\' has been recorded.\\" html_response(message) if __name__ == \\"__main__\\": process_form()"},{"question":"<|Analysis Begin|> The provided documentation is about the `torch.signal` module, which seems to contain several window functions available under `torch.signal.windows`. These are various types of windows used in signal processing and analysis. Understanding and utilizing these windows requires familiarity with signal processing techniques and their applications in PyTorch. The functions mentioned (e.g., bartlett, blackman, cosine, exponential, gaussian, general_cosine, general_hamming, hamming, hann, kaiser, nuttall) are commonly used in Fourier Transform and other signal processing tasks to reduce spectral leakage by windowing the signal. Given the focus of this documentation on window functions in the `torch.signal` module, a good question would involve applying one or more of these window functions to perform a specific signal processing task using PyTorch. <|Analysis End|> <|Question Begin|> **Problem Statement:** You are provided with a discrete signal represented as a 1D tensor. Your task is to implement a function that applies a specified window function from the `torch.signal.windows` module to the signal and returns the windowed signal along with its Fast Fourier Transform (FFT). **Function Signature:** ```python def apply_window_and_fft(signal: torch.Tensor, window_type: str, *args, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]: pass ``` **Input:** 1. `signal` (torch.Tensor): A 1D tensor representing the input signal. 2. `window_type` (str): A string representing the type of window function to apply. It can be one of the following: - \'bartlett\' - \'blackman\' - \'cosine\' - \'exponential\' - \'gaussian\' - \'general_cosine\' - \'general_hamming\' - \'hamming\' - \'hann\' - \'kaiser\' - \'nuttall\' 3. `*args`: Additional positional arguments to be passed to the window function. 4. `**kwargs`: Additional keyword arguments to be passed to the window function. **Output:** A tuple containing: 1. The windowed signal as a 1D tensor. 2. The FFT of the windowed signal as a 1D tensor. **Constraints:** 1. The length of the signal tensor will be at least 1. 2. The provided `window_type` string will always be one of the valid options specified. 3. Additional arguments (`*args` and `**kwargs`) will be valid for the specified window function. **Example:** ```python import torch # Example signal signal = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]) # Applying Hamming window and computing FFT windowed_signal, fft_signal = apply_window_and_fft(signal, \'hamming\') print(windowed_signal) print(fft_signal) ``` **Notes:** 1. Use the appropriate window function from the `torch.signal.windows` module based on the `window_type` argument. 2. Compute the FFT using `torch.fft.fft`. **Evaluation Criteria:** 1. Correctness of the windowing function application. 2. Correct computation of the FFT. 3. Proper handling of additional arguments for window functions.","solution":"import torch from typing import Tuple def apply_window_and_fft(signal: torch.Tensor, window_type: str, *args, **kwargs) -> Tuple[torch.Tensor, torch.Tensor]: Applies the specified window function to the input signal and returns the windowed signal along with its FFT. Args: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_type (str): Type of window function to apply. - *args: Additional positional arguments for the window function. - **kwargs: Additional keyword arguments for the window function. Returns: - Tuple[torch.Tensor, torch.Tensor]: The windowed signal and its FFT. # Import the correct window function based on window_type window_function = getattr(torch.signal.windows, window_type) # Apply the window function to the signal window = window_function(len(signal), *args, **kwargs) windowed_signal = signal * window # Compute the FFT of the windowed signal fft_signal = torch.fft.fft(windowed_signal) return windowed_signal, fft_signal"},{"question":"You have been provided with an implementation of a basic function to find the maximum value in a list of integers: ```python def find_maximum(values): max_val = values[0] for val in values: if val > max_val: max_val = val return max_val ``` You need to write a script that performs the following tasks: 1. Parse the provided `find_maximum` function code to create its AST. 2. Modify the function so that it uses the built-in `max` function instead of the for loop to find the maximum value. 3. Ensure all original argument names and function behaviors remain unchanged. 4. Convert the modified AST back to a source code string. 5. Verify the transformation by printing the transformed function as a string. # Requirements - Implement a function `transform_code(source: str) -> str` that: - Takes a single argument `source`, which is a string containing the source code of the `find_maximum` function. - Returns a string containing the transformed source code where the function uses the `max` function. # Constraints - You must handle the AST transformation using the \\"ast\\" module. - Do not directly modify the source string with regex or string operations. - Perform all changes via the AST to ensure accurate and structured code transformation. # Example ```python source_code = def find_maximum(values): max_val = values[0] for val in values: if val > max_val: max_val = val return max_val transformed_code = transform_code(source_code) print(transformed_code) # Output should be: # def find_maximum(values): # max_val = max(values) # return max_val ``` # Note You will be provided with the test cases to check the correctness of your implementation. Good Luck!","solution":"import ast from ast import NodeTransformer, fix_missing_locations class MaxFunctionTransformer(NodeTransformer): def visit_FunctionDef(self, node): if node.name == \\"find_maximum\\": new_body = [ ast.Assign( targets=[ast.Name(id=\\"max_val\\", ctx=ast.Store())], value=ast.Call( func=ast.Name(id=\\"max\\", ctx=ast.Load()), args=[ast.Name(id=\\"values\\", ctx=ast.Load())], keywords=[] ) ), ast.Return( value=ast.Name(id=\\"max_val\\", ctx=ast.Load()) ) ] node.body = new_body return node def transform_code(source: str) -> str: tree = ast.parse(source) tree = MaxFunctionTransformer().visit(tree) tree = fix_missing_locations(tree) return ast.unparse(tree)"},{"question":"# Objective Write a Python function to create a system for safely writing to a shared log file across multiple processes using file locking mechanisms provided by the `fcntl` module. # Problem Statement You are tasked with implementing a function `safe_write_to_log(logfile: str, message: str) -> None` that ensures safe writing of log messages to a shared file by multiple processes. The function should: 1. Open the specified `logfile` for writing and appending. 2. Acquire an exclusive lock on the file descriptor using `fcntl.flock` before writing. 3. Write the `message` to the log file, followed by a newline character. 4. Release the lock after writing. 5. Handle any potential errors gracefully. # Input Format - `logfile`: A string representing the path to the log file. - `message`: A string representing the message to log. # Output Format - The function does not return any value. # Constraints - The `logfile` will always be a valid file path. - The `message` will be a non-empty string. - The solution should handle potential exceptions appropriately. # Example ```python import os # Assuming there exists a shared log file at /tmp/shared_log.txt safe_write_to_log(\\"/tmp/shared_log.txt\\", \\"Process A: Starting task\\") safe_write_to_log(\\"/tmp/shared_log.txt\\", \\"Process A: Completed task\\") # Print the contents of the log file with open(\\"/tmp/shared_log.txt\\", \\"r\\") as f: for line in f: print(line.strip()) ``` # Solution Template ```python import fcntl def safe_write_to_log(logfile: str, message: str) -> None: try: # Open logfile in append mode with open(logfile, \'a\') as f: # Acquire an exclusive lock on the file fcntl.flock(f, fcntl.LOCK_EX) # Write the message to the logfile f.write(message + \'n\') # Unlock the file after writing fcntl.flock(f, fcntl.LOCK_UN) except OSError as e: print(f\\"Error: {e}\\") ``` **Note:** Students are expected to understand and utilize file descriptor operations, file locking mechanisms, and proper exception handling in their implementations.","solution":"import fcntl def safe_write_to_log(logfile: str, message: str) -> None: try: # Open logfile in append mode with open(logfile, \'a\') as f: # Acquire an exclusive lock on the file fcntl.flock(f, fcntl.LOCK_EX) # Write the message to the logfile f.write(message + \'n\') # Unlock the file after writing fcntl.flock(f, fcntl.LOCK_UN) except OSError as e: print(f\\"Error: {e}\\")"},{"question":"# POP3 Email Client Implementation Using the Python `poplib` library, you are required to implement a function that retrieves specific emails from a POP3 server based on the given criteria. Your implementation should handle connecting to the server, authenticating, listing emails, and retrieving those that meet the specified criteria. Function Signature ```python def retrieve_emails(host: str, port: int, username: str, password: str, criteria: str) -> List[str]: Connects to a POP3 server, authenticates with the provided credentials, lists emails from the server, and retrieves those that match the given criteria. Parameters: host (str): The hostname of the POP3 server. port (int): The port number to connect to the POP3 server. username (str): The username for authentication. password (str): The password for authentication. criteria (str): The criteria to filter emails. Criteria is a substring to be found in the email body. Returns: List[str]: A list of emails that match the criteria. Each email is represented as a single string. pass ``` Description 1. **Connecting to the Server**: - Use the `poplib.POP3` class to connect to the given POP3 server using the provided `host` and `port`. - Authenticate using the provided `username` and `password`. 2. **Listing and Filtering Emails**: - Retrieve the list of available email messages. - For each email, check if the `criteria` substring exists in the email\'s body. 3. **Retrieving Emails**: - For emails that match the criteria, fetch the email contents. - Return the matched emails as a list of strings. Each string should contain the full content of one email. Constraints - You can assume that the input parameters are always valid. - The POP3 server is reliable and emails can be listed and retrieved without errors. - You should handle common exceptions and ensure that the connection is properly closed after the operations. Example ```python emails = retrieve_emails(\'pop.example.com\', 110, \'user@example.com\', \'password123\', \'Important\') for email in emails: print(email) ``` In the above example, the function connects to the POP3 server at `pop.example.com` using the provided credentials, lists all emails, and fetches those containing the substring \\"Important\\". The fetched emails are printed to the console.","solution":"import poplib from email.parser import BytesParser from typing import List def retrieve_emails(host: str, port: int, username: str, password: str, criteria: str) -> List[str]: Connects to a POP3 server, authenticates with the provided credentials, lists emails from the server, and retrieves those that match the given criteria. Parameters: host (str): The hostname of the POP3 server. port (int): The port number to connect to the POP3 server. username (str): The username for authentication. password (str): The password for authentication. criteria (str): The criteria to filter emails. Criteria is a substring to be found in the email body. Returns: List[str]: A list of emails that match the criteria. Each email is represented as a single string. emails_matching_criteria = [] try: # Connect to the server server = poplib.POP3(host, port) server.user(username) server.pass_(password) # Get the list of messages messages_list = server.list()[1] # Iterate through each message for message_number in range(1, len(messages_list) + 1): response, lines, octets = server.retr(message_number) # Combine message lines and parse into email structure message_content = b\'rn\'.join(lines) email_message = BytesParser().parsebytes(message_content) # Get the email body and check for criteria email_body = email_message.get_payload(decode=True).decode() if criteria in email_body: emails_matching_criteria.append(email_body) server.quit() except Exception as e: print(f\\"An error occurred: {e}\\") return emails_matching_criteria"},{"question":"# Advanced Python Object Manipulation You are given a set of utilities for manipulating and interacting with Python objects. Your task is to implement a Python class that mimics some of these low-level functionalities using the standard Python library. This will help demonstrate your understanding of object manipulation, introspection, and attribute management. # Class Specification Create a class `PyObjectSimulator` with the following methods: 1. `has_attribute(obj: Any, attr_name: str) -> bool`: - Checks if `obj` has an attribute named `attr_name`. - Should return `True` if `obj` has the attribute, `False` otherwise. 2. `get_attribute(obj: Any, attr_name: str) -> Any`: - Retrieves the attribute named `attr_name` from `obj`. - Should return the attribute value if it exists, otherwise raise `AttributeError`. 3. `set_attribute(obj: Any, attr_name: str, value: Any) -> None`: - Sets the attribute named `attr_name` of `obj` to `value`. - Should raise an appropriate exception if the operation fails. 4. `delete_attribute(obj: Any, attr_name: str) -> None`: - Deletes the attribute named `attr_name` from `obj`. - Should raise `AttributeError` if the attribute does not exist. 5. `rich_compare(obj1: Any, obj2: Any, op: str) -> bool`: - Compares `obj1` and `obj2` using the comparison operator specified by `op`. - `op` can be one of the following: `<`, `<=`, `==`, `!=`, `>`, `>=`. # Constraints - Do not use any external libraries for this task. - Assume well-formed input for the comparison operator (`op`). # Example ```python class Dummy: def __init__(self, x): self.x = x sim = PyObjectSimulator() d1 = Dummy(10) d2 = Dummy(20) # Example usage print(sim.has_attribute(d1, \\"x\\")) # True print(sim.get_attribute(d1, \\"x\\")) # 10 sim.set_attribute(d1, \\"y\\", 15) # No output, adds attribute y print(sim.get_attribute(d1, \\"y\\")) # 15 sim.delete_attribute(d1, \\"y\\") # No output, deletes attribute y print(sim.rich_compare(d1.x, d2.x, \'==\')) # False print(sim.rich_compare(d1.x, d2.x, \'<\')) # True ``` # Implementation ```python class PyObjectSimulator: def has_attribute(self, obj: Any, attr_name: str) -> bool: return hasattr(obj, attr_name) def get_attribute(self, obj: Any, attr_name: str) -> Any: return getattr(obj, attr_name) def set_attribute(self, obj: Any, attr_name: str, value: Any) -> None: setattr(obj, attr_name, value) def delete_attribute(self, obj: Any, attr_name: str) -> None: if not hasattr(obj, attr_name): raise AttributeError(f\\"{obj} has no attribute {attr_name}\\") delattr(obj, attr_name) def rich_compare(self, obj1: Any, obj2: Any, op: str) -> bool: if op == \'<\': return obj1 < obj2 elif op == \'<=\': return obj1 <= obj2 elif op == \'==\': return obj1 == obj2 elif op == \'!=\': return obj1 != obj2 elif op == \'>\': return obj1 > obj2 elif op == \'>=\': return obj1 >= obj2 else: raise ValueError(f\\"Invalid comparison operator: {op}\\") ``` # Evaluation - The solution should correctly handle attribute checking, getting, setting, and deleting. - The rich comparison method should correctly evaluate the comparison based on the operator provided. - Proper exception handling should be included where necessary.","solution":"class PyObjectSimulator: def has_attribute(self, obj: any, attr_name: str) -> bool: return hasattr(obj, attr_name) def get_attribute(self, obj: any, attr_name: str) -> any: if not hasattr(obj, attr_name): raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") return getattr(obj, attr_name) def set_attribute(self, obj: any, attr_name: str, value: any) -> None: setattr(obj, attr_name, value) def delete_attribute(self, obj: any, attr_name: str) -> None: if not hasattr(obj, attr_name): raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") delattr(obj, attr_name) def rich_compare(self, obj1: any, obj2: any, op: str) -> bool: if op == \'<\': return obj1 < obj2 elif op == \'<=\': return obj1 <= obj2 elif op == \'==\': return obj1 == obj2 elif op == \'!=\': return obj1 != obj2 elif op == \'>\': return obj1 > obj2 elif op == \'>=\': return obj1 >= obj2 else: raise ValueError(f\\"Invalid comparison operator: {op}\\")"},{"question":"**Python Coding Assessment** # Objective Create a Python function that processes a given source code string to identify all function definitions and return their names along with the line numbers where they are defined. You will be using the `tokenize` module to achieve this. # Function Signature ```python def extract_function_definitions(source_code: str) -> List[Tuple[str, int]]: pass ``` # Input - `source_code` (str): A string containing valid Python source code. # Output - Returns a list of tuples, where each tuple contains: - The name of a function (str). - The line number (int) where the function is defined. # Constraints - The source code will not have any syntax errors. - Function definitions can appear in any valid scope, including inside other functions or classes. - Function names will be alphanumeric and conform to Python\'s naming conventions for functions. # Example ```python source_code = def foo(): pass class Bar: def baz(self): def qux(): pass # Example Function Call result = extract_function_definitions(source_code) # Expected Output # [(\'foo\', 2), (\'baz\', 5), (\'qux\', 6)] ``` # Additional Information - You may use the `tokenize` module as documented to split the source code into tokens and analyze these tokens to find function definitions. - Pay attention to the `tokenize` module\'s token types and ensure you correctly identify `def` keywords and their associated function names. - Include necessary imports and handle any edge cases that may occur. # Performance Requirements - The function should handle source code of up to 1000 lines efficiently. # Good Luck!","solution":"import tokenize from io import StringIO from typing import List, Tuple def extract_function_definitions(source_code: str) -> List[Tuple[str, int]]: Extracts function names and their line numbers from the given source code. Parameters: source_code (str): A string containing valid Python source code. Returns: List[Tuple[str, int]]: A list of tuples where each tuple contains a function name and the line number where it is defined. functions = [] tokens = tokenize.generate_tokens(StringIO(source_code).readline) for toknum, tokval, start, end, line in tokens: if toknum == tokenize.NAME and tokval == \'def\': # The next token should be the name of the function toknum, tokval, start, end, line = next(tokens) if toknum == tokenize.NAME: functions.append((tokval, start[0])) return functions"},{"question":"Custom Palette Visualization **Objective:** Your task is to create a function that generates and visualizes custom color palettes using Seaborn\'s `sns.hls_palette()` function. You will then use these palettes in a Seaborn plot to demonstrate your understanding of customizing and applying color palettes. **Function Signature:** ```python def custom_palette_visualization(colors_num: int, lightness: float, saturation: float, hue_start: float) -> None: pass ``` **Input:** - `colors_num` (int): The number of colors to generate in the palette. - `lightness` (float): The lightness level of the colors (between 0 and 1). - `saturation` (float): The saturation level of the colors (between 0 and 1). - `hue_start` (float): The starting point for hue sampling (between 0 and 1). **Output:** - None. The function should display the palette and apply it to a plot directly within the function. **Constraints:** - `colors_num` must be greater than 0. - `lightness`, `saturation`, and `hue_start` should be between 0 and 1. **Requirements:** 1. Generate a color palette using the specified parameters. 2. Plot and display the color palette. 3. Create a Seaborn plot (e.g., a bar plot) using any dataset and apply the generated color palette to the plot. **Example:** ```python custom_palette_visualization(10, 0.5, 0.7, 0.1) ``` The above function call should: 1. Generate a color palette with 10 colors, a lightness of 0.5, saturation of 0.7, and starting hue of 0.1. 2. Display the color palette. 3. Create and display a Seaborn bar plot using the generated palette. **Hints:** - You can use `sns.color_palette()` to apply a custom palette to your plots. - Use a sample dataset like `tips` dataset from Seaborn for plotting.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_palette_visualization(colors_num: int, lightness: float, saturation: float, hue_start: float) -> None: Generates and visualizes a custom color palette and applies it to a Seaborn plot. Parameters: colors_num (int): The number of colors to generate in the palette. lightness (float): The lightness level of the colors (between 0 and 1). saturation (float): The saturation level of the colors (between 0 and 1). hue_start (float): The starting point for hue sampling (between 0 and 1). Returns: None if not (0 < colors_num): raise ValueError(\\"colors_num must be greater than 0\\") if not (0 <= lightness <= 1): raise ValueError(\\"lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"saturation must be between 0 and 1\\") if not (0 <= hue_start <= 1): raise ValueError(\\"hue_start must be between 0 and 1\\") # Generate the custom color palette custom_palette = sns.hls_palette(colors_num, l=lightness, s=saturation, h=hue_start) # Display the palette sns.palplot(custom_palette) plt.show() # Load a sample dataset tips = sns.load_dataset(\'tips\') # Create a Seaborn bar plot using the custom palette sns.barplot(x=\'day\', y=\'total_bill\', data=tips, palette=custom_palette) plt.title(\'Total Bill by Day with Custom Palette\') plt.show()"},{"question":"**Exception Handling and Custom Exception Creation** # Objective: Design and implement a Python function that demonstrates the use of fundamental and advanced exception handling techniques. You will be required to create custom exceptions, handle built-in exceptions, and raise exceptions with contextual information. # Problem Statement: You are tasked to write a Python function `process_user_input(data: str) -> str` that processes a user\'s input string. The function should meet the following requirements: 1. **Input Validation**: - Raise a `ValueError` if the input `data` is not a string or is an empty string. 2. **Processing**: - If the input string can be converted to an integer, return the string `\\"The input is an integer\\"`. - If the input string can be converted to a float, return the string `\\"The input is a float\\"`. - If the input string is a valid Python identifier, return the string `\\"The input is a valid identifier\\"`. 3. **Custom Exceptions**: - Define a custom exception `UnsupportedCharacterError` that should be raised if the input contains any characters other than alphabets, digits, and underscores. 4. **Error Context**: - Ensure that when raising exceptions, you maintain context information using base chaining techniques where applicable. 5. **Exception Hierarchy Handling**: - Your function should handle and demonstrate the usage of at least three different built-in exceptions apart from `ValueError`. # Function Signature: ```python def process_user_input(data: str) -> str: pass ``` # Example: ```python try: print(process_user_input(\\"12345\\")) # Output: \\"The input is an integer\\" print(process_user_input(\\"12.345\\")) # Output: \\"The input is a float\\" print(process_user_input(\\"variable_name\\")) # Output: \\"The input is a valid identifier\\" print(process_user_input(\\"invalid-variable-name!\\")) # Should raise UnsupportedCharacterError except Exception as e: print(f\\"Exception caught: {e}\\") ``` # Constraints: - You may not use any external libraries or modules to perform these checks. - Use built-in exception handling mechanisms provided by Python. # Performance Requirements: - The function should handle inputs efficiently. - The function is expected to handle the validation, processing, and exceptions appropriately without unnecessary checks or over-complication. # Notes: - Focus on the correct use of exception handling mechanisms. - Remember to chain exceptions where applicable to provide contextual information. - Make sure your custom exception correctly inherits from a suitable built-in exception class.","solution":"class UnsupportedCharacterError(Exception): pass def process_user_input(data: str) -> str: # Input Validation if not isinstance(data, str) or data == \\"\\": raise ValueError(\\"Input must be a non-empty string.\\") # Processing try: int_value = int(data) return \\"The input is an integer\\" except ValueError: pass # Continue processing try: float_value = float(data) return \\"The input is a float\\" except ValueError: pass # Continue processing # Additional Validation for Unsupported Characters if not data.isidentifier(): invalid_chars = \\"\\".join(set([c for c in data if not c.isalnum() and c != \'_\'])) raise UnsupportedCharacterError(f\\"Input contains unsupported characters: {invalid_chars}\\") return \\"The input is a valid identifier\\""},{"question":"**Question: Implement a Custom HTTP Client Using `urllib`** **Context:** You are tasked with implementing a custom HTTP client that can perform a series of HTTP requests using the `urllib` package in Python. The client should be capable of the following operations: - Perform a `GET` request and retrieve the content of a given URL. - Perform a `POST` request to a given URL with specific data. - Handle basic authentication. - Manage HTTP and URL errors gracefully. - Set custom headers for the requests. - Set a timeout for the requests to avoid hanging indefinitely. - Utilize proxies if necessary. **Task:** Implement a class `CustomHttpClient` that supports the following methods: 1. `def __init__(self, base_url: str, timeout: int = 10, proxies: dict = None) -> None`: - Takes a base URL, a default timeout for requests, and an optional dictionary of proxies. - Initializes necessary handlers and openers. 2. `def get(self, endpoint: str, headers: dict = None) -> str`: - Performs a `GET` request to the given endpoint (which is appended to the base URL). - Accepts optional headers. - Returns the content of the response as a string. - Handles any HTTP or URL errors gracefully and returns appropriate error messages. 3. `def post(self, endpoint: str, data: dict, headers: dict = None) -> str`: - Performs a `POST` request to the given endpoint with the provided data. - Data should be URL-encoded before sending. - Accepts optional headers. - Returns the content of the response as a string. - Handles any HTTP or URL errors gracefully and returns appropriate error messages. 4. `def set_auth(self, username: str, password: str) -> None`: - Sets up basic authentication using the provided username and password. **Constraints:** - Your implementation should handle redirects internally. - Proxies, if provided, should be used for all requests. - Ensure requests time out as per the provided timeout value. **Example Usage:** ```python client = CustomHttpClient(base_url=\\"http://example.com\\", timeout=5, proxies={\\"http\\": \\"http://proxy.example.com:8080\\"}) # GET request response = client.get(\\"/api/resource\\") print(response) # POST request response = client.post(\\"/api/resource\\", {\\"key\\": \\"value\\"}) print(response) # Set authentication client.set_auth(username=\\"user\\", password=\\"pass\\") # GET request with authentication response = client.get(\\"/api/secure-resource\\") print(response) ``` **Expected Output:** 1. If the `GET` request is successful, returns the content of the response. 2. If the `POST` request is successful, returns the content of the response. 3. If basic authentication is set, subsequent requests are authenticated. 4. Handles and returns appropriate messages for HTTP or URL errors. **Note:** Dear student, make sure to thoroughly test the implemented methods to cover multiple scenarios like successful requests, failures due to invalid URLs, authentication issues, and timeout cases.","solution":"import urllib.request import urllib.parse import urllib.error from urllib.error import HTTPError, URLError import base64 class CustomHttpClient: def __init__(self, base_url: str, timeout: int = 10, proxies: dict = None) -> None: self.base_url = base_url self.timeout = timeout self.proxies = proxies self.handlers = [] if proxies: self.proxy_handler = urllib.request.ProxyHandler(proxies) self.handlers.append(self.proxy_handler) self.opener = urllib.request.build_opener(*self.handlers) urllib.request.install_opener(self.opener) def get(self, endpoint: str, headers: dict = None) -> str: url = f\\"{self.base_url}{endpoint}\\" req = urllib.request.Request(url, headers=headers or {}) try: with urllib.request.urlopen(req, timeout=self.timeout) as response: return response.read().decode() except (HTTPError, URLError) as e: return f\\"Error: {e}\\" def post(self, endpoint: str, data: dict, headers: dict = None) -> str: url = f\\"{self.base_url}{endpoint}\\" data = urllib.parse.urlencode(data).encode() req = urllib.request.Request(url, data=data, headers=headers or {}) try: with urllib.request.urlopen(req, timeout=self.timeout) as response: return response.read().decode() except (HTTPError, URLError) as e: return f\\"Error: {e}\\" def set_auth(self, username: str, password: str) -> None: auth_str = f\\"{username}:{password}\\".encode(\'utf-8\') base64_str = base64.b64encode(auth_str).decode(\'utf-8\') auth_header = f\\"Basic {base64_str}\\" auth_handler = urllib.request.HTTPBasicAuthHandler() auth_handler.add_password(None, self.base_url, username, password) self.handlers.append(auth_handler) self.opener = urllib.request.build_opener(*self.handlers) urllib.request.install_opener(self.opener)"},{"question":"# Seaborn Visualization Challenge You are tasked with creating a clear and aesthetically pleasing visualization for a dataset using Seaborn. Your code must demonstrate an understanding of Seaborn\'s functions for setting themes, styles, and contexts, as well as removing axis spines and customizing specific plot parameters. Dataset: Use the following dataset for your visualization: ```python import numpy as np import pandas as pd np.random.seed(0) data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Values\': np.random.rand(5) * 100 }) ``` Requirements: 1. **Set the overall theme** of the visualization to `darkgrid`. 2. **Apply the \\"white\\" style** to the plot, and within a `with` statement, **temporarily switch** to the \\"ticks\\" style for the actual plotting process. 3. Create a **bar plot** using Seaborn based on the provided dataset. 4. **Remove the top and right spines** from the plot. 5. **Set the context** to \\"poster\\". 6. **Customize** the plot to: - Set font scale to 1.2 - Change the linewidth of the bars to 2 - Set the face color of the axes to a light gray color (`#f0f0f0`). Function Signature: ```python def custom_seaborn_plot(data: pd.DataFrame): # Your Code Here custom_seaborn_plot(data) ``` Expected Output: Your function should create a bar plot that meets the above stylistic requirements and display it using Matplotlib\'s `plt.show()` function. The plot should be aesthetically pleasing and adhere to the specified constraints. Constraints: - Do not change the structure of the provided dataset. - Ensure that the plot is displayed within the function using `plt.show()`. Performance Requirements: - The function should execute efficiently, with a reasonable runtime even for larger datasets. Good luck and happy coding!","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np def custom_seaborn_plot(data: pd.DataFrame): Creates a customized Seaborn bar plot based on the provided data. # Set the overall theme to darkgrid sns.set_theme(style=\'darkgrid\') # Set the context to poster sns.set_context(\\"poster\\", font_scale=1.2) # Apply the white style and temporary switch to ticks for plotting with sns.axes_style(\\"ticks\\"): plt.figure(figsize=(10, 6)) # Create a bar plot bar_plot = sns.barplot(x=\'Category\', y=\'Values\', data=data, linewidth=2) # Remove top and right spines sns.despine(top=True, right=True) # Set the face color of the axes to a light gray color bar_plot.set_facecolor(\'#f0f0f0\') # Show the plot plt.show() # Generate the dataset np.random.seed(0) data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\'], \'Values\': np.random.rand(5) * 100 }) custom_seaborn_plot(data)"},{"question":"Objective Demonstrate your understanding of scikit-learn\'s preprocessing, feature extraction, and combining estimators functionality by implementing a pipeline that processes a dataset and trains a simple machine learning model. Problem Statement You are provided with a dataset containing numerical and categorical features. Your task is to write a function `process_and_train` that takes the dataset and performs the following tasks: 1. Applies a standard scaler to the numerical features. 2. Encodes the categorical features using one-hot encoding. 3. Combines these transformations using a ColumnTransformer. 4. Uses this combined transformer as part of a pipeline to train a Logistic Regression model. 5. Returns the trained pipeline. Input Format - A pandas DataFrame `data` containing the dataset. Expect columns of various types (numerical, categorical). - A string `target_col` representing the name of the target column. - A list of strings `num_cols` representing the names of the numerical columns. - A list of strings `cat_cols` representing the names of the categorical columns. Output Format A trained scikit-learn pipeline. Constraints - Use `StandardScaler` for numerical columns. - Use `OneHotEncoder` for categorical columns. - Use `LogisticRegression` for the final model in the pipeline. - The function should be efficient and adhere to scikit-learn\'s best practices. Performance Requirements - The solution should handle reasonably large datasets efficiently (think 10,000 rows, 50 columns). - Ensure that the transformations are applied only to their respective column types. Function Signature ```python def process_and_train(data: pd.DataFrame, target_col: str, num_cols: List[str], cat_cols: List[str]) -> Pipeline: pass ``` Example ```python import pandas as pd from typing import List from sklearn.pipeline import Pipeline data = pd.DataFrame({ \'age\': [25, 32, 47, 51], \'income\': [50000, 64000, 120000, 84000], \'gender\': [\'male\', \'female\', \'female\', \'male\'], \'purchased\': [0, 1, 1, 0] }) target_col = \'purchased\' num_cols = [\'age\', \'income\'] cat_cols = [\'gender\'] pipeline = process_and_train(data, target_col, num_cols, cat_cols) print(pipeline) ``` This script should return a trained pipeline object ready to make predictions. Hints - Refer to `sklearn.compose.ColumnTransformer` for combining different preprocessors. - Use `sklearn.preprocessing.StandardScaler` for scaling numerical features. - Use `sklearn.preprocessing.OneHotEncoder` for encoding categorical features. - Use `sklearn.linear_model.LogisticRegression` for the final model. - Use `sklearn.pipeline.Pipeline` to create the final pipeline.","solution":"import pandas as pd from typing import List from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.linear_model import LogisticRegression def process_and_train(data: pd.DataFrame, target_col: str, num_cols: List[str], cat_cols: List[str]) -> Pipeline: # Define the transformers for numerical and categorical features num_transformer = StandardScaler() cat_transformer = OneHotEncoder(handle_unknown=\'ignore\') # Create a column transformer to apply different transformations to different columns preprocessor = ColumnTransformer( transformers=[ (\'num\', num_transformer, num_cols), (\'cat\', cat_transformer, cat_cols) ] ) # Create a pipeline that first preprocesses the data and then applies Logistic Regression pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Separate the features and the target variable X = data.drop(columns=[target_col]) y = data[target_col] # Fit the pipeline on the data pipeline.fit(X, y) return pipeline"},{"question":"Efficient Prime Number Generation using Multiprocessing In this task, you are required to generate a list of prime numbers up to a given integer `n` efficiently by utilizing the `multiprocessing` module. This will test your ability to implement process-based parallel programming and apply different multiprocessing strategies. # Requirements: 1. Implement a function `is_prime(num)` that checks whether a given number `num` is prime. 2. Implement a function `generate_primes(n, num_processes)` that: - Generates prime numbers up to `n` using `num_processes` processes. - Utilizes the multiprocessing module, specifically the `Pool` class, to parallelize the task of prime number generation. # Specifications: - **Function 1: `is_prime(num)`** - **Input:** An integer `num`. - **Output:** Boolean value - `True` if the number is prime, `False` otherwise. - **Function 2: `generate_primes(n, num_processes)`** - **Input:** Two integers, `n` (the upper limit up to which primes are to be generated) and `num_processes` (number of processes to be used). - **Output:** A sorted list of prime numbers up to `n`. # Constraints: - `n` will be a positive integer, `1 <= n <= 10^6`. - `num_processes` will be a positive integer, `1 <= num_processes <= 16`. # Performance: - The implementation should maximize the use of available processes efficiently. - Ensure minimal overhead and optimal performance by carefully distributing tasks and processing results. # Example: ```python def is_prime(num): # Your implementation here def generate_primes(n, num_processes): # Your implementation here # Test the implementation primes = generate_primes(50, 4) print(primes) # Output: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47] ``` # Instructions: 1. You can use the `multiprocessing` module\'s `Pool` class to parallelize tasks. 2. Use the `map` function of `Pool` to apply `is_prime` to batches of numbers. 3. Make sure to handle the collection and merging of results correctly. 4. Ensure your code runs without any errors or deadlocks. # Additional Notes: - The use of `if __name__ == \\"__main__\\":` is essential to correctly handle multiprocessing initialization in Windows. - Consider edge cases and think about the efficiency of your solution. Good luck!","solution":"import multiprocessing def is_prime(num): Checks if a number is prime. if num <= 1: return False if num <= 3: return True if num % 2 == 0 or num % 3 == 0: return False i = 5 while i * i <= num: if num % i == 0 or num % (i + 2) == 0: return False i += 6 return True def generate_primes(n, num_processes): Generates prime numbers up to n using num_processes processes. # Create a boolean array \\"prime[0:n]\\" and initialize # all entries it as true. A value in prime[i] will # finally be false if i is Not a prime, else true boolean primes = [] with multiprocessing.Pool(processes=num_processes) as pool: results = pool.map(is_prime, range(2, n+1)) primes = [num for num, is_prm in enumerate(results, start=2) if is_prm] return primes"},{"question":"# Warning Control and Testing in Python **Objective**: To assess your understanding of the `warnings` module in Python, particularly how to handle, filter, and test runtime warnings. **Problem Statement**: You are developing a Python package, and you want to ensure that your package is robust and correctly handles deprecated functions without interrupting the user experience unnecessarily. Implement the following functions: 1. **trigger_warning**: This function should intentionally trigger different types of warnings based on the input parameters. 2. **configure_warning_filter**: This function should configure the warnings filter based on the input parameters. 3. **suppress_warnings**: This function should temporarily suppress specific warnings within a given context. 4. **test_warning_capture**: This function should capture warnings for testing purposes and return warning details in a structured format. # Function Specifications 1. **trigger_warning(warning_type: str, message: str) -> None** This function triggers a warning of the specified type. - **Parameters**: - `warning_type`: A string representing the type of warning to trigger (options: \\"user\\", \\"deprecated\\", \\"syntax\\", \\"runtime\\", \\"future\\"). - `message`: A custom message for the warning. - **Example**: ```python trigger_warning(\\"deprecated\\", \\"This function is deprecated\\") ``` This should trigger a `DeprecationWarning` with the message \\"This function is deprecated\\". 2. **configure_warning_filter(action: str, warning_type: str) -> None** This function configures the warnings filter. - **Parameters**: - `action`: A string indicating the action to take for matching warnings (options: \\"default\\", \\"ignore\\", \\"always\\", \\"module\\", \\"once\\"). - `warning_type`: A string representing the type of warning to filter (options: \\"user\\", \\"deprecated\\", \\"syntax\\", \\"runtime\\", \\"future\\"). - **Example**: ```python configure_warning_filter(\\"ignore\\", \\"deprecated\\") ``` This should configure the warnings filter to ignore all `DeprecationWarning`. 3. **suppress_warnings(warning_type: str) -> None** This function suppresses specific warnings within its context using the `catch_warnings` context manager. - **Parameters**: - `warning_type`: A string representing the type of warning to suppress (options: \\"user\\", \\"deprecated\\", \\"syntax\\", \\"runtime\\", \\"future\\"). - **Example**: ```python with suppress_warnings(\\"deprecated\\"): trigger_warning(\\"deprecated\\", \\"This function is deprecated\\") ``` This should suppress the `DeprecationWarning` within the context block. 4. **test_warning_capture() -> list** This function tests the `trigger_warning` function by capturing all warnings and returning a list of warning messages and categories. - **Returns**: - A list of tuples, each containing the warning message and category class. - **Example**: ```python test_warning_capture() -> [(\\"This is a user warning\\", UserWarning), (\\"This function is deprecated\\", DeprecationWarning)] ``` # Constraints - You must use the `warnings` module for handling and testing warnings. - Assume that inputs are valid and you do not need to handle erroneous inputs. - The solution should demonstrate a clear understanding of warning categories and the `warnings` module functions. # Performance Requirements - Performance is not a primary concern, but the implementation should be efficient and not produce unnecessary side effects. # Submission Submit the implementations of the following functions: 1. `trigger_warning()` 2. `configure_warning_filter()` 3. `suppress_warnings()` 4. `test_warning_capture()` **Good luck!**","solution":"import warnings from contextlib import contextmanager def trigger_warning(warning_type: str, message: str) -> None: Triggers a warning of the specified type with a custom message. :param warning_type: Type of warning to trigger. :param message: Custom warning message. warning_types = { \\"user\\": UserWarning, \\"deprecated\\": DeprecationWarning, \\"syntax\\": SyntaxWarning, \\"runtime\\": RuntimeWarning, \\"future\\": FutureWarning } warning_cls = warning_types.get(warning_type) if warning_cls: warnings.warn(message, warning_cls) else: raise ValueError(\\"Invalid warning type\\") def configure_warning_filter(action: str, warning_type: str) -> None: Configures the warnings filter based on the input parameters. :param action: Action to take for matching warnings. :param warning_type: Type of warning to filter. warning_types = { \\"user\\": UserWarning, \\"deprecated\\": DeprecationWarning, \\"syntax\\": SyntaxWarning, \\"runtime\\": RuntimeWarning, \\"future\\": FutureWarning } warning_cls = warning_types.get(warning_type) if warning_cls: warnings.filterwarnings(action, category=warning_cls) else: raise ValueError(\\"Invalid warning type\\") @contextmanager def suppress_warnings(warning_type: str): Suppresses specific warnings within its context using the catch_warnings context manager. :param warning_type: Type of warning to suppress. warning_types = { \\"user\\": UserWarning, \\"deprecated\\": DeprecationWarning, \\"syntax\\": SyntaxWarning, \\"runtime\\": RuntimeWarning, \\"future\\": FutureWarning } warning_cls = warning_types.get(warning_type) if warning_cls: with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\", category=warning_cls) yield else: raise ValueError(\\"Invalid warning type\\") def test_warning_capture() -> list: Captures warnings for testing purposes and returns warning details in a structured format. :returns: List of tuples containing warning messages and categories. captured_warnings = [] with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") # Trigger various types of warnings for testing trigger_warning(\\"user\\", \\"This is a user warning\\") trigger_warning(\\"deprecated\\", \\"This function is deprecated\\") trigger_warning(\\"syntax\\", \\"This is a syntax warning\\") trigger_warning(\\"runtime\\", \\"This is a runtime warning\\") trigger_warning(\\"future\\", \\"This is a future warning\\") for warning in w: captured_warnings.append((str(warning.message), type(warning.message))) return captured_warnings"},{"question":"**Objective:** Demonstrate understanding of Python\'s name binding and scope rules along with exception handling. **Problem Statement:** You need to implement a function called `scope_and_exception_demo` which performs the following tasks: 1. Define a local variable `message` with the value `\\"Hello from the local scope!\\"`. 2. Define a function `inner_function` inside `scope_and_exception_demo` that: - Tries to print the local variable `message`. - Uses exception handling to catch `UnboundLocalError` if `message` is referenced before being assigned within `inner_function`. Additionally, perform these operations: - Outside of `inner_function`, but inside `scope_and_exception_demo`, modify the local variable `message` to `\\"Modified local message\\"`. - Call `inner_function`. - Handle the exception raised by `inner_function` if it occurs, and print a message saying `\\"Handled UnboundLocalError\\"`. - Finally, print the local variable `message` defined within `scope_and_exception_demo`. **Function Signature:** ```python def scope_and_exception_demo() -> None: ``` **Expected Output:** The function should print: 1. Either the initial value of `message` from within `inner_function` or an exception-related message if `UnboundLocalError` is caught. 2. `\\"Modified local message\\"`. **Constraints:** - Do not use the `global` keyword. - Handle the `UnboundLocalError` exception correctly and meaningfully. **Example:** ```python >>> scope_and_exception_demo() # Output if exception occurs: Handled UnboundLocalError Modified local message # Output if no exception occurs: Hello from the local scope! Modified local message ``` The goal is to test your understanding of name binding, scope rules, and exception handling in Python. Ensure that the variable `message` in `inner_function` is correctly referenced and handles exceptions as specified.","solution":"def scope_and_exception_demo(): Demonstrate scope and exception handling in Python. message = \\"Hello from the local scope!\\" def inner_function(): try: print(message) except UnboundLocalError: print(\\"Caught UnboundLocalError in inner_function\\") message = \\"Modified local message\\" try: inner_function() except UnboundLocalError: print(\\"Handled UnboundLocalError\\") print(message)"},{"question":"**Title**: Comprehensive URL Handler with Authentication and Proxies **Objective**: Design a Python program that utilizes the `urllib.request` module to perform the following tasks: 1. Fetch the HTML content of a given URL. 2. Handle redirections automatically. 3. Use HTTP Basic Authentication for URLs that require it. 4. Configure proxy settings dynamically. 5. Add custom headers to the requests, including a custom User-Agent. You are provided with the following inputs: - A URL to fetch. - HTTP Basic Authentication credentials (realm, user, password). - Proxy settings (a dictionary mapping protocol names to proxy URLs). - Custom headers (a dictionary of additional headers to include). **Requirements**: 1. Define a function `fetch_url_content(url: str, auth_info: dict, proxy_info: dict, custom_headers: dict) -> str` that performs the tasks: - `url`: The URL to fetch. - `auth_info`: A dictionary containing `realm`, `user`, and `password` for HTTP Basic Authentication. - `proxy_info`: A dictionary mapping protocol names (e.g., \'http\', \'https\') to proxy URLs. - `custom_headers`: A dictionary of additional headers to include in the request. 2. Implement error handling for HTTP errors, printing appropriate error messages if the URL cannot be fetched. 3. Use proper organization and modular design in your solution. # Constraints: 1. The URL might require redirections. 2. Basic Authentication might be required. 3. Proxies should be dynamically configurable. 4. Custom headers must be added to each request. 5. Must handle and print HTTP error statuses appropriately. # Example: ```python url = \\"http://example.com\\" auth_info = { \\"realm\\": \\"Example Realm\\", \\"user\\": \\"username\\", \\"password\\": \\"password\\" } proxy_info = { \\"http\\": \\"http://proxy.example.com:8080\\", \\"https\\": \\"https://proxy.example.com:8443\\" } custom_headers = { \\"User-Agent\\": \\"CustomUserAgent/1.0\\", \\"Referer\\": \\"http://referer.example.com\\" } content = fetch_url_content(url, auth_info, proxy_info, custom_headers) print(content) ``` **Expected Output**: The HTML content of the given URL, or an appropriate error message if the URL cannot be fetched.","solution":"import urllib.request import urllib.error import urllib.parse from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener, install_opener def fetch_url_content(url: str, auth_info: dict, proxy_info: dict, custom_headers: dict) -> str: try: # Create a password manager with authentication information password_mgr = HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password( auth_info[\'realm\'], url, auth_info[\'user\'], auth_info[\'password\'] ) # Create the basic auth handler auth_handler = HTTPBasicAuthHandler(password_mgr) # Create proxy handler proxy_handler = urllib.request.ProxyHandler(proxy_info) # Create a custom opener opener = build_opener(proxy_handler, auth_handler, urllib.request.HTTPRedirectHandler()) # Add custom headers request = urllib.request.Request(url, headers=custom_headers) # Install the opener globally so it can handle requests install_opener(opener) with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\'HTTP error: {e.code} - {e.reason}\' except urllib.error.URLError as e: return f\'URL error: {e.reason}\'"},{"question":"Objective You are required to implement a function that performs an HTTP GET request to a specified URL and handles possible errors using the appropriate exceptions from the `urllib.error` module. Your implementation should demonstrate a thorough understanding of handling different types of exceptions that may arise during the request process. Function Signature ```python def fetch_url_content(url: str) -> str: Fetches the content of a URL using an HTTP GET request. Parameters: - url (str): The URL from which to fetch the content. Returns: - str: The content of the URL if the request is successful. Raises: - URLError: If there is a general URL error. - HTTPError: If there is an HTTP error, including authentication errors. - ContentTooShortError: If the downloaded content is shorter than expected. ``` Inputs - `url` (str): A string representing the URL to fetch the content from. The URL will be a valid HTTP or HTTPS URL. Outputs - Returns the content of the URL as a string if the request is successful. Constraints - The function should correctly handle and raise `URLError`, `HTTPError`, and `ContentTooShortError` as described in the provided documentation. - You may use the `urllib.request` module to facilitate the request. Example Usage ```python try: content = fetch_url_content(\\"http://example.com\\") print(\\"Content fetched successfully!\\") print(content) except urllib.error.HTTPError as e: print(f\\"HTTP Error: {e.code} - {e.reason}\\") except urllib.error.URLError as e: print(f\\"URL Error: {e.reason}\\") except urllib.error.ContentTooShortError as e: print(\\"Content too short error\\") print(f\\"Content retrieved: {e.content}\\") ``` Notes - Ensure the function handles the exceptions by raising them with appropriate messages. - Write the necessary imports and any additional helper functions if necessary for a complete solution. Performance - The function should handle typical web request latencies efficiently. - Be mindful of the possibility of network-related delays and handle timeouts appropriately.","solution":"import urllib.request import urllib.error def fetch_url_content(url: str) -> str: Fetches the content of a URL using an HTTP GET request. Parameters: - url (str): The URL from which to fetch the content. Returns: - str: The content of the URL if the request is successful. Raises: - URLError: If there is a general URL error. - HTTPError: If there is an HTTP error, including authentication errors. - ContentTooShortError: If the downloaded content is shorter than expected. try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: print(f\\"HTTP Error: {e.code} - {e.reason}\\") raise except urllib.error.URLError as e: print(f\\"URL Error: {e.reason}\\") raise except urllib.error.ContentTooShortError as e: print(\\"Content too short error\\") raise # Note that ContentTooShortError is not a default exception provided by urllib, so it\'s usually custom. # For this code, if you use a non-existing website or URL it can throw the HTTPError or URLError. # So here, the function is complete with typical handling."},{"question":"# Advanced Coding Assessment Objective Implement a simple text-based notes application using the Python `curses` module. The application should allow users to: 1. Create new notes. 2. List existing notes. 3. Edit existing notes. 4. Delete notes. Requirements 1. **Main Menu**: The main window should display a menu with options to create, list, edit, and delete notes. The user should navigate the menu using keyboard arrows and select an option using the \\"Enter\\" key. 2. **Create Note**: This should open a new window where the user can input the note\'s title and the note\'s content. Save the note when the user presses a specific key (e.g., Ctrl+G). 3. **List Notes**: Display a list of existing notes. The user should be able to navigate the list using arrow keys and select a note to view full content. 4. **Edit Note**: Allow user to edit the note\'s content and save changes. 5. **Delete Note**: This should prompt the user to confirm deletion and, if confirmed, delete the selected note. Input and Output - **Input**: The user interacts with the application using the keyboard. - **Output**: The application displays appropriate windows and menus in the terminal. Constraints - Only use the Python `curses` module for handling terminal inputs and outputs. - Note data should be stored in-memory as a dictionary with titles as keys and contents as values. Performance Requirements - The application should efficiently handle navigation and updates to the screen to provide a responsive user experience. Sample Workflow 1. The main menu displays options: \\"Create Note,\\" \\"List Notes,\\" \\"Edit Note,\\" and \\"Delete Note.\\" 2. Selecting \\"Create Note\\" opens a new window to input the title and content. 3. Selecting \\"List Notes\\" displays titles of existing notes and allows selection. 4. Selecting a note from the list opens the full content in view mode. 5. In view mode, the user can press a key to edit or delete the note. # Implementation Implement the `main` function that initializes the `curses` screen and handles the main loop of the application. Consider breaking down the implementation into smaller functions handling different parts of the application, such as `create_note`, `list_notes`, `edit_note`, and `delete_note`. ```python import curses def main(stdscr): # Clear screen stdscr.clear() curses.curs_set(0) # Hide the cursor # Define menu options menu = [\\"Create Note\\", \\"List Notes\\", \\"Edit Note\\", \\"Delete Note\\", \\"Exit\\"] selected_idx = 0 # Function to display menu def display_menu(stdscr, selected_idx): stdscr.clear() h, w = stdscr.getmaxyx() for idx, menu_item in enumerate(menu): x = w//2 - len(menu_item)//2 y = h//2 - len(menu)//2 + idx if idx == selected_idx: stdscr.attron(curses.A_REVERSE) stdscr.addstr(y, x, menu_item) stdscr.attroff(curses.A_REVERSE) else: stdscr.addstr(y, x, menu_item) stdscr.refresh() # Main loop while True: display_menu(stdscr, selected_idx) key = stdscr.getch() if key == curses.KEY_UP and selected_idx > 0: selected_idx -= 1 elif key == curses.KEY_DOWN and selected_idx < len(menu) - 1: selected_idx += 1 elif key == curses.KEY_ENTER or key in [10, 13]: # Handle menu selection if menu[selected_idx] == \\"Create Note\\": create_note() elif menu[selected_idx] == \\"List Notes\\": list_notes() elif menu[selected_idx] == \\"Edit Note\\": edit_note() elif menu[selected_idx] == \\"Delete Note\\": delete_note() elif menu[selected_idx] == \\"Exit\\": break def create_note(): # TODO: Implement create note functionality pass def list_notes(): # TODO: Implement list notes functionality pass def edit_note(): # TODO: Implement edit note functionality pass def delete_note(): # TODO: Implement delete note functionality pass if __name__ == \\"__main__\\": curses.wrapper(main) ``` This scaffold provides the `main` function and a basic skeleton for menu navigation. You need to implement the individual functions (`create_note`, `list_notes`, `edit_note`, and `delete_note`) as per the requirements.","solution":"import curses # In-memory data store for notes notes = {} def main(stdscr): # Clear screen stdscr.clear() curses.curs_set(0) # Hide the cursor # Define menu options menu = [\\"Create Note\\", \\"List Notes\\", \\"Edit Note\\", \\"Delete Note\\", \\"Exit\\"] selected_idx = 0 # Function to display menu def display_menu(stdscr, selected_idx): stdscr.clear() h, w = stdscr.getmaxyx() for idx, menu_item in enumerate(menu): x = w//2 - len(menu_item)//2 y = h//2 - len(menu)//2 + idx if idx == selected_idx: stdscr.attron(curses.A_REVERSE) stdscr.addstr(y, x, menu_item) stdscr.attroff(curses.A_REVERSE) else: stdscr.addstr(y, x, menu_item) stdscr.refresh() # Main loop while True: display_menu(stdscr, selected_idx) key = stdscr.getch() if key == curses.KEY_UP and selected_idx > 0: selected_idx -= 1 elif key == curses.KEY_DOWN and selected_idx < len(menu) - 1: selected_idx += 1 elif key == curses.KEY_ENTER or key in [10, 13]: # Handle menu selection if menu[selected_idx] == \\"Create Note\\": create_note_screen(stdscr) elif menu[selected_idx] == \\"List Notes\\": list_notes_screen(stdscr) elif menu[selected_idx] == \\"Edit Note\\": edit_note_screen(stdscr) elif menu[selected_idx] == \\"Delete Note\\": delete_note_screen(stdscr) elif menu[selected_idx] == \\"Exit\\": break def create_note_screen(stdscr): curses.curs_set(1) stdscr.clear() stdscr.addstr(0, 0, \\"[Create Note] Press Ctrl+G to save the note\\") stdscr.addstr(2, 0, \\"Title: \\") title_win = curses.newwin(1, curses.COLS - 10, 2, 7) content_win = curses.newwin(curses.LINES - 5, curses.COLS - 2, 4, 1) curses.echo() title = title_win.getstr().decode(\'utf-8\').strip() stdscr.addstr(3, 0, \\"Content: (press Ctrl+G to save)\\") stdscr.refresh() content = \\"\\" def read_content(): nonlocal content content_win.move(0, 0) content_win.refresh() while True: c = content_win.getch() if c == 7: # Ctrl+G break elif c == 10: # Enter content += \\"n\\" content_win.addch(c) else: content += chr(c) content_win.addch(c) content_win.refresh() read_content() notes[title] = content curses.curs_set(0) stdscr.clear() def list_notes_screen(stdscr): stdscr.clear() stdscr.addstr(0, 0, \\"[List Notes] Press any key to go back\\") row = 2 for title in notes.keys(): stdscr.addstr(row, 1, f\\"- {title}\\") row += 1 stdscr.getch() def edit_note_screen(stdscr): stdscr.clear() stdscr.addstr(0, 0, \\"[Edit Note] (List of titles) Press any key to continue\\") row = 2 for title in notes.keys(): stdscr.addstr(row, 1, f\\"- {title}\\") row += 1 stdscr.getch() stdscr.clear() stdscr.addstr(0, 0, \\"Enter the title of the note to edit:\\") curses.echo() title = stdscr.getstr(1, 0).decode(\'utf-8\').strip() if title in notes: content = notes[title] stdscr.addstr(2, 0, \\"Content: (press Ctrl+G to save changes)\\") stdscr.refresh() edit_window = curses.newwin(curses.LINES - 5, curses.COLS - 2, 4, 1) edit_window.addstr(content) curses.curs_set(1) new_content = \\"\\" def read_content(): nonlocal new_content edit_window.move(0, 0) edit_window.refresh() while True: c = edit_window.getch() if c == 7: # Ctrl+G break elif c == 10: # Enter new_content += \\"n\\" edit_window.addch(c) else: new_content += chr(c) edit_window.addch(c) edit_window.refresh() read_content() notes[title] = new_content curses.curs_set(0) def delete_note_screen(stdscr): stdscr.clear() stdscr.addstr(0, 0, \\"[Delete Note] (List of titles) Press any key to continue\\") row = 2 for title in notes.keys(): stdscr.addstr(row, 1, f\\"- {title}\\") row += 1 stdscr.getch() stdscr.clear() stdscr.addstr(0, 0, \\"Enter the title of the note to delete:\\") curses.echo() title = stdscr.getstr(1, 0).decode(\'utf-8\').strip() if title in notes: del notes[title] if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"**Objective:** Demonstrate understanding of seaborn\'s `plotting_context` function and the ability to create visually distinguishable plots using different plotting contexts. **Problem Statement:** Using the seaborn library, create a function `compare_plotting_contexts` that generates and saves two line plots showing a comparison of two predefined plotting contexts: \\"paper\\" and \\"poster\\". These plots should be based on the following data: ```python data = { \\"x\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\"], \\"y1\\": [10, 15, 13, 17, 20, 23], \\"y2\\": [12, 14, 16, 19, 22, 24] } ``` **Function Signature:** ```python def compare_plotting_contexts(): pass ``` **Expected Output:** The function should not return any values, but it should save two plots: 1. `plot_paper.png`: A line plot with context \\"paper\\" showing `data[\\"x\\"]` on the x-axis, `data[\\"y1\\"]` on the y-axis as one line, and `data[\\"y2\\"]` on the y-axis as another line. 2. `plot_poster.png`: A similar line plot but with context \\"poster\\". **Constraints:** - Ensure both plots have appropriate titles indicating their context. - The plots should include a legend to distinguish between the two lines (`y1` and `y2`). - Use pyplot from matplotlib for saving the figures. **Example Output:** Once the function is executed, the two files `plot_paper.png` and `plot_poster.png` should be present in the working directory with the specified visual configurations. **Notes:** - Utilize the `with sns.plotting_context()` structure to manage the context temporarily. - The plots should be aesthetically pleasing with a sense of professionalism suitable for publication or presentation.","solution":"import matplotlib.pyplot as plt import seaborn as sns def compare_plotting_contexts(): data = { \\"x\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\"], \\"y1\\": [10, 15, 13, 17, 20, 23], \\"y2\\": [12, 14, 16, 19, 22, 24] } # Plot with context \\"paper\\" with sns.plotting_context(\\"paper\\"): plt.figure() plt.plot(data[\\"x\\"], data[\\"y1\\"], label=\\"Y1\\", marker=\'o\') plt.plot(data[\\"x\\"], data[\\"y2\\"], label=\\"Y2\\", marker=\'o\') plt.title(\\"Plot with \'paper\' context\\") plt.xlabel(\\"Months\\") plt.ylabel(\\"Values\\") plt.legend() plt.savefig(\\"plot_paper.png\\") plt.close() # Plot with context \\"poster\\" with sns.plotting_context(\\"poster\\"): plt.figure() plt.plot(data[\\"x\\"], data[\\"y1\\"], label=\\"Y1\\", marker=\'o\') plt.plot(data[\\"x\\"], data[\\"y2\\"], label=\\"Y2\\", marker=\'o\') plt.title(\\"Plot with \'poster\' context\\") plt.xlabel(\\"Months\\") plt.ylabel(\\"Values\\") plt.legend() plt.savefig(\\"plot_poster.png\\") plt.close()"},{"question":"You are required to demonstrate your understanding of PyTorch\'s tensor attributes, specifically `dtype`, `device`, and the type promotion rules. # Problem Statement Implement a function `tensor_attrs_operations` that takes three input arguments: 1. `tensor1`: A torch tensor. 2. `tensor2`: A torch tensor. 3. `operation`: A string that specifies an arithmetic operation (`\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, or `\\"divide\\"`). The function should: 1. Perform the specified arithmetic operation between `tensor1` and `tensor2`. 2. Respect the type promotion rules defined by PyTorch. 3. Return a dictionary containing the following keys and their corresponding values: - `\\"result\\"`: The resultant tensor from the arithmetic operation. - `\\"result_dtype\\"`: The data type of the resultant tensor. - `\\"result_device\\"`: The device on which the resultant tensor is allocated. # Constraints - The function should raise a `ValueError` if the `operation` is not one of the allowed operations (`\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, or `\\"divide\\"`). - Both `tensor1` and `tensor2` will have compatible shapes for the operations. - The tensors can be on different devices, and the operation should respect the current device allocation rules. # Example ```python import torch # Example Tensors tensor1 = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32, device=torch.device(\'cpu\')) tensor2 = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64, device=torch.device(\'cuda\')) # Performing \'add\' operation result = tensor_attrs_operations(tensor1, tensor2, \\"add\\") # Expected Output Format # result = { # \\"result\\": <resultant_tensor>, # \\"result_dtype\\": dtype_of_result, # \\"result_device\\": device_of_result # } # Example Output # { # \\"result\\": tensor([2.0000, 4.0000, 6.0000], device=\'cuda:0\', dtype=torch.float64), # \\"result_dtype\\": torch.float64, # \\"result_device\\": torch.device(type=\'cuda\', index=0) # } ``` # Implementation Note - Ensure the function handles the type promotions correctly as per PyTorch\'s rules. - Handling tensors on different devices should reflect PyTorch\'s behavior regarding moving tensors before operations. **Function Signature** ```python def tensor_attrs_operations(tensor1: torch.Tensor, tensor2: torch.Tensor, operation: str) -> dict: pass ```","solution":"import torch def tensor_attrs_operations(tensor1: torch.Tensor, tensor2: torch.Tensor, operation: str) -> dict: Perform an arithmetic operation between two tensors, respecting dtype promotion and device allocation. Args: - tensor1: First input tensor. - tensor2: Second input tensor. - operation: A string specifying the arithmetic operation (\'add\', \'subtract\', \'multiply\', \'divide\'). Returns: - A dictionary with keys \'result\', \'result_dtype\', \'result_device\': - \'result\': The resultant tensor. - \'result_dtype\': The data type of the resultant tensor. - \'result_device\': The device on which the resultant tensor is allocated. if operation not in [\\"add\\", \\"subtract\\", \\"multiply\\", \\"divide\\"]: raise ValueError(f\\"Invalid operation: {operation}. Supported operations are \'add\', \'subtract\', \'multiply\', \'divide\'.\\") # Ensuring both tensors are on the same device if tensor1.device != tensor2.device: tensor2 = tensor2.to(tensor1.device) # Performing the specified operation if operation == \\"add\\": result_tensor = tensor1 + tensor2 elif operation == \\"subtract\\": result_tensor = tensor1 - tensor2 elif operation == \\"multiply\\": result_tensor = tensor1 * tensor2 elif operation == \\"divide\\": result_tensor = tensor1 / tensor2 # Returning required attributes return { \\"result\\": result_tensor, \\"result_dtype\\": result_tensor.dtype, \\"result_device\\": result_tensor.device }"},{"question":"# Nested Tensor Challenge with PyTorch You are tasked with creating a function that manipulates nested tensors in PyTorch. Your goal is to perform the following operations in sequence: 1. **Create a Nested Tensor**: - Initialize a nested tensor with the jagged layout from a list of 2D tensors of varying shapes. - Ensure that the nested tensor is created with `requires_grad=True`. 2. **Manipulate the Nested Tensor**: - Convert the nested tensor to a padded tensor with a specified padding value. - Apply a specific operation (e.g., element-wise addition) on the padded tensor. - Convert the modified padded tensor back to a nested tensor. 3. **Check Gradients**: - Perform a backward operation to ensure gradients are properly propagated to the original inputs. - Verify the gradients for the input tensors. # Function Signature ```python import torch def nested_tensor_operations(tensors: list, padding_value: float, operation: str): Perform a series of operations on nested tensors and ensure gradients are propagated. Parameters: tensors (list): A list of 2D tensors with varying shapes. padding_value (float): The value to use for padding when converting to a padded tensor. operation (str): The operation to perform on the padded tensor (e.g., \\"add\\"). Returns: dict: A dictionary containing the original gradients, modified nested tensor, and its gradients. { \'original_grads\': List of gradients for the original tensors, \'modified_nested_tensor\': Modified nested tensor after the operation, \'modified_grads\': Gradients for the modified nested tensor } pass ``` # Constraints - You can assume that the input tensors will be non-empty and have 2 dimensions. - The operation parameter can be one of the following: `\\"add\\"`, `\\"sub\\"`, `\\"mul\\"`, or `\\"div\\"`, indicating the respective element-wise operation with a constant value of 2. - You are allowed to use `torch.compile()` for optimizing the operations, but it is optional. # Example Usage ```python tensors = [torch.randn(3, 2, requires_grad=True), torch.randn(5, 2, requires_grad=True)] padding_value = 1.0 operation = \\"add\\" result = nested_tensor_operations(tensors, padding_value, operation) print(\\"Original Gradients:\\", result[\'original_grads\']) print(\\"Modified Nested Tensor:\\", result[\'modified_nested_tensor\']) print(\\"Modified Gradients:\\", result[\'modified_grads\']) ``` # Expected Output - `original_grads`: Should contain the gradients corresponding to the original input tensors. - `modified_nested_tensor`: The nested tensor after the specified operation has been applied and converted back from the padded tensor. - `modified_grads`: The gradients for the modified nested tensor after performing backward. # Performance Requirements - Ensure the operations are efficient and avoid unnecessary conversions or memory allocations. - Use of `contiguous()` where needed to ensure operations are compatible. # Notes - The documentation for nested tensors can be found at: [here](https://pytorch.org/docs) - This task checks your understanding of nested tensors, operations on tensors, backward propagation, and ensuring gradient consistency.","solution":"import torch from torch.nn.utils.rnn import pad_sequence def nested_tensor_operations(tensors: list, padding_value: float, operation: str): Perform a series of operations on nested tensors and ensure gradients are propagated. Parameters: tensors (list): A list of 2D tensors with varying shapes. padding_value (float): The value to use for padding when converting to a padded tensor. operation (str): The operation to perform on the padded tensor (e.g., \\"add\\"). Returns: dict: A dictionary containing the original gradients, modified nested tensor, and its gradients. { \'original_grads\': List of gradients for the original tensors, \'modified_nested_tensor\': Modified nested tensor after the operation, \'modified_grads\': Gradients for the modified nested tensor } original_tensors = [tensor.clone().detach().requires_grad_(True) for tensor in tensors] # Convert tensors to padded tensor padded_tensor = pad_sequence(original_tensors, batch_first=True, padding_value=padding_value) # Apply the specified operation if operation == \\"add\\": modified_padded_tensor = padded_tensor + 2 elif operation == \\"sub\\": modified_padded_tensor = padded_tensor - 2 elif operation == \\"mul\\": modified_padded_tensor = padded_tensor * 2 elif operation == \\"div\\": modified_padded_tensor = padded_tensor / 2 else: raise ValueError(\\"Unsupported operation\\") # Convert back to nested tensor format modified_tensors = [modified_padded_tensor[i, :tensor.size(0), :] for i, tensor in enumerate(original_tensors)] # Sum of the modified tensors to perform backward loss = sum(tensor.sum() for tensor in modified_tensors) loss.backward() original_grads = [tensor.grad for tensor in original_tensors] modified_grads = [tensor.grad for tensor in modified_tensors] return { \'original_grads\': original_grads, \'modified_nested_tensor\': modified_tensors, \'modified_grads\': modified_grads }"},{"question":"**Question: Advanced Sequence and Mapping Manipulation** Given a list of dictionaries, each dictionary representing information about an item, write a function `process_items` that performs the following tasks: 1. **Filter**: Remove dictionaries that do not contain a specified key `required_key`, or where `required_key`\'s value does not pass a provided predicate function `filter_func`. 2. **Transform**: Modify each remaining dictionary by: - Replacing a specified key `replace_key` with a new key `new_replace_key`. - Applying a transformation function `transform_func` to the value associated with `replace_key` and storing the resulting value under `new_replace_key`. 3. **Sort**: Sort the dictionaries based on a specified key `sort_by_key` in ascending order. # Function Signature ```python def process_items(items: list, required_key: str, filter_func: callable, replace_key: str, new_replace_key: str, transform_func: callable, sort_by_key: str) -> list: pass ``` # Input - `items` (list of dict): A list of dictionaries, each representing an item with various key-value pairs. - `required_key` (str): A string representing the key that must be present in each dictionary (and pass the `filter_func` predicate) for it to be included in the results. - `filter_func` (callable): A function that takes a value and returns `True` if the value meets the criteria, otherwise `False`. - `replace_key` (str): The key whose value needs to be transformed and which will be replaced by `new_replace_key`. - `new_replace_key` (str): The new key to replace `replace_key`. - `transform_func` (callable): A function that takes a value and returns a transformed value. - `sort_by_key` (str): The key by which to sort the resulting list of dictionaries in ascending order. # Output - Returns a list of dictionaries after performing the filter, transform, and sort operations. # Constraints - You may assume that all dictionaries in the input list have unique values for `sort_by_key`. # Example ```python items = [ {\'name\': \'apple\', \'price\': 120, \'quantity\': 5}, {\'name\': \'banana\', \'price\': 30, \'quantity\': 10}, {\'name\': \'orange\', \'price\': 60, \'quantity\': 15}, {\'name\': \'mango\', \'price\': 150, \'quantity\': 7, \'discount\': 10}, ] required_key = \'price\' filter_func = lambda x: x > 50 replace_key = \'price\' new_replace_key = \'discounted_price\' transform_func = lambda x: x * 0.9 sort_by_key = \'quantity\' print(process_items(items, required_key, filter_func, replace_key, new_replace_key, transform_func, sort_by_key)) ``` # Example Output ```python [ {\'name\': \'apple\', \'quantity\': 5, \'discounted_price\': 108.0}, {\'name\': \'orange\', \'quantity\': 15, \'discounted_price\': 54.0}, {\'name\': \'mango\', \'quantity\': 7, \'discount\': 10, \'discounted_price\': 135.0}, ] ``` # Notes - The transformation function `transform_func` is applied only if the `replace_key` exists in the dictionary. - The order of dictionaries in the resulting list is based on the value of `sort_by_key` in ascending order. - Any dictionary that does not contain the `required_key` or whose `required_key` value does not satisfy `filter_func` is excluded from the result. Implement the `process_items` function to achieve the described functionality.","solution":"def process_items(items, required_key, filter_func, replace_key, new_replace_key, transform_func, sort_by_key): Processes a list of dictionaries by filtering, transforming, and sorting them based on provided criteria. Args: - items (list of dict): List of dictionaries to process. - required_key (str): Key that must be present and pass the filter_func. - filter_func (callable): Function to filter the dictionaries based on required_key\'s value. - replace_key (str): Key whose value is to be transformed. - new_replace_key (str): New key to store the transformed value. - transform_func (callable): Function to transform the value associated with replace_key. - sort_by_key (str): Key to sort the resulting list of dictionaries. Returns: - list: Processed list of dictionaries. # Filter the items filtered_items = [ item for item in items if required_key in item and filter_func(item[required_key]) ] # Transform the items for item in filtered_items: if replace_key in item: item[new_replace_key] = transform_func(item.pop(replace_key)) # Sort the items sorted_items = sorted(filtered_items, key=lambda x: x[sort_by_key]) return sorted_items"},{"question":"**Objective:** Implement a simple attention mechanism using the `torch.nn.attention.bias` module. You will need to integrate causal bias into this attention mechanism. **Description:** Your task is to implement a function `causal_attention` that performs a causal (auto-regressive) attention operation on an input tensor. The function should utilize the `CausalBias` class and relevant functions from the `torch.nn.attention.bias` module to apply causal bias in the attention computation. **Input:** - `input_tensor` (torch.Tensor): A 3D tensor of shape `(batch_size, sequence_length, embedding_dim)`. - `causal_type` (str): A string specifying the type of causal operation, either \'lower_right\' or \'upper_left\'. **Output:** - `output_tensor` (torch.Tensor): A 3D tensor of shape `(batch_size, sequence_length, embedding_dim)` representing the attention output after applying causal bias. **Constraints:** 1. You must use the `CausalBias` class and its associated methods for applying causal bias. 2. The function should handle multiple batches and sequences. 3. Ensure that your implementation is efficient and leverages PyTorch\'s vectorized operations as much as possible. # Function Signature ```python import torch def causal_attention(input_tensor: torch.Tensor, causal_type: str) -> torch.Tensor: ... ``` # Example ```python import torch from torch.nn.attention.bias import CausalBias, causal_lower_right, causal_upper_left # Sample input tensor input_tensor = torch.randn(2, 5, 4) # (batch_size=2, sequence_length=5, embedding_dim=4) # Using causal_lower_right output_tensor_right = causal_attention(input_tensor, \'lower_right\') print(output_tensor_right) # Using causal_upper_left output_tensor_left = causal_attention(input_tensor, \'upper_left\') print(output_tensor_left) ``` # Note: - Ensure your function correctly applies the specified causal bias to the input tensor. - This task is designed to test your understanding of attention mechanisms, causal bias, and PyTorch operations.","solution":"import torch from torch.nn.functional import softmax def causal_attention(input_tensor: torch.Tensor, causal_type: str) -> torch.Tensor: batch_size, seq_len, embed_dim = input_tensor.shape # Create a mask for causal attention if causal_type == \'lower_right\': mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0) elif causal_type == \'upper_left\': mask = torch.triu(torch.ones(seq_len, seq_len)).unsqueeze(0) else: raise ValueError(\\"Invalid causal_type. Expected \'lower_right\' or \'upper_left\'\\") mask = mask.to(input_tensor.device) # Computing Q, K, and V matrices for attention Q = input_tensor K = input_tensor V = input_tensor # Scaled dot-product attention scores = torch.bmm(Q, K.transpose(1, 2)) / torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32, device=input_tensor.device)) scores = scores.masked_fill(mask == 0, float(\'-inf\')) attention_weights = softmax(scores, dim=-1) output_tensor = torch.bmm(attention_weights, V) return output_tensor"},{"question":"**Question: Optimize a PyTorch Model Using torch.utils.jit** # Objective You are required to demonstrate your understanding of using PyTorch JIT utilities by optimizing a given PyTorch model. Specifically, you will use `torch.jit.script` to script a model and `torch.jit.trace` to trace a model, ensuring that the optimized models maintain the same behavior as the original. # Requirements 1. **Input Format** - A PyTorch model class definition `SimpleModel`. - A sample input tensor that is a 2D tensor of shape `(1, 3)`. - `torch` library will be available for use. 2. **Output Format** - A scripted model object. - A traced model object. - The outputs of the original model, the scripted model, and the traced model for the given input tensor. 3. **Constraints** - Ensure that the scripted and traced models produce the same output as the original model. - You must handle potential discrepancies that may arise during tracing or scripting. # Sample Model ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(3, 1) self.relu = nn.ReLU() def forward(self, x): return self.relu(self.linear(x)) # Sample input tensor input_tensor = torch.tensor([[1.0, 2.0, 3.0]]) # Instantiate the model model = SimpleModel() ``` # Expected Solution 1. Script the `SimpleModel` using `torch.jit.script`. 2. Trace the `SimpleModel` using `torch.jit.trace`. 3. Verify that the outputs of the original model, the scripted model, and the traced model are identical for the given input tensor. # Implementation ```python import torch import torch.nn as nn # Define the SimpleModel class as given above # Sample input tensor input_tensor = torch.tensor([[1.0, 2.0, 3.0]]) # Instantiate the model model = SimpleModel() # Your task starts here # 1. Script the model scripted_model = torch.jit.script(model) # 2. Trace the model traced_model = torch.jit.trace(model, input_tensor) # 3. Verify outputs original_output = model(input_tensor) scripted_output = scripted_model(input_tensor) traced_output = traced_model(input_tensor) # Print the outputs to verify print(\\"Original Model Output: \\", original_output) print(\\"Scripted Model Output: \\", scripted_output) print(\\"Traced Model Output: \\", traced_output) ``` Your implementation should show that the outputs from the original model, scripted model, and traced model are the same. Ensure to handle any discrepancies that might arise during the scripting or tracing process.","solution":"import torch import torch.nn as nn # Define the SimpleModel class as given above class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(3, 1) self.relu = nn.ReLU() def forward(self, x): return self.relu(self.linear(x)) # Sample input tensor input_tensor = torch.tensor([[1.0, 2.0, 3.0]]) # Instantiate the model model = SimpleModel() # Your task starts here # 1. Script the model scripted_model = torch.jit.script(model) # 2. Trace the model traced_model = torch.jit.trace(model, input_tensor) # 3. Verify outputs original_output = model(input_tensor) scripted_output = scripted_model(input_tensor) traced_output = traced_model(input_tensor) # Print the outputs to verify print(\\"Original Model Output: \\", original_output) print(\\"Scripted Model Output: \\", scripted_output) print(\\"Traced Model Output: \\", traced_output)"},{"question":"Objective Utilize the \\"email\\" package to create and send an email with multiple recipients, including both plain text and HTML parts, and a dynamically generated report attached as a CSV file. Problem Statement You are tasked with sending a performance report to a list of stakeholders. The email should contain: 1. A plain text part with a brief summary. 2. An HTML part with the report details formatted in a table. 3. A CSV file attachment containing the detailed report data. Requirements - Function Name: `send_performance_report` - Input: - `sender_email` (str): The sender\'s email address. - `recipients` (list of str): A list of recipient email addresses. - `smtp_server` (str): The SMTP server address. - `summary` (str): A brief summary of the performance report. - `report_data` (list of tuples): Each tuple contains `(name, metric, value)`, representing each row of the report with a name, a performance metric, and its value. Output - The function should not return anything. It should send the email to all recipients with the specified contents and attachment. Constraints - Assume the SMTP server does not require authentication. - The function should handle any potential exceptions and log a suitable message if sending the email fails. # Example ```python def send_performance_report(sender_email, recipients, smtp_server, summary, report_data): Sends a performance report via email with both text and HTML parts, along with a CSV attachment. Parameters: - sender_email (str): The sender\'s email address. - recipients (list of str): List of recipient email addresses. - smtp_server (str): SMTP server address. - summary (str): Summary of the performance report. - report_data (list of tuples): Detailed report data in the form (name, metric, value). Returns: None import smtplib from email.message import EmailMessage from email.utils import make_msgid import csv import io try: # Create the email message msg = EmailMessage() msg[\'Subject\'] = \\"Monthly Performance Report\\" msg[\'From\'] = sender_email msg[\'To\'] = \', \'.join(recipients) # Add the plain text part msg.set_content(summary) # Create HTML content html_content = f <html> <body> <p>{summary}</p> <table border=\\"1\\"> <tr><th>Name</th><th>Metric</th><th>Value</th></tr> for (name, metric, value) in report_data: html_content += f\\"<tr><td>{name}</td><td>{metric}</td><td>{value}</td></tr>\\" html_content += \\"</table></body></html>\\" # Add the HTML part msg.add_alternative(html_content, subtype=\\"html\\") # Generate the CSV data csv_buffer = io.StringIO() csv_writer = csv.writer(csv_buffer) csv_writer.writerow([\\"Name\\", \\"Metric\\", \\"Value\\"]) csv_writer.writerows(report_data) # Add CSV attachment msg.add_attachment(csv_buffer.getvalue(), maintype=\'application\', subtype=\'csv\', filename=\'performance_report.csv\') # Send the email with smtplib.SMTP(smtp_server) as server: server.send_message(msg) print(\\"Email sent successfully!\\") except Exception as e: print(f\\"Failed to send email: {e}\\") # Example usage send_performance_report( sender_email=\\"sender@example.com\\", recipients=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], smtp_server=\\"localhost\\", summary=\\"This is the monthly performance report. Please find the detailed report attached.\\", report_data=[ (\\"Alice\\", \\"Sales\\", 120), (\\"Bob\\", \\"Customer Satisfaction\\", 85), (\\"Charlie\\", \\"Support Tickets\\", 98) ] ) ``` # Notes - Ensure you handle any exceptions while sending the email and provide meaningful error messages. - The report_data should be used to dynamically generate the content of the CSV attachment and the HTML table.","solution":"def send_performance_report(sender_email, recipients, smtp_server, summary, report_data): Sends a performance report via email with both text and HTML parts, along with a CSV attachment. Parameters: - sender_email (str): The sender\'s email address. - recipients (list of str): List of recipient email addresses. - smtp_server (str): SMTP server address. - summary (str): Summary of the performance report. - report_data (list of tuples): Detailed report data in the form (name, metric, value). Returns: None import smtplib from email.message import EmailMessage from email.utils import make_msgid import csv import io try: # Create the email message msg = EmailMessage() msg[\'Subject\'] = \\"Monthly Performance Report\\" msg[\'From\'] = sender_email msg[\'To\'] = \', \'.join(recipients) # Add the plain text part msg.set_content(summary) # Create HTML content html_content = f <html> <body> <p>{summary}</p> <table border=\\"1\\"> <tr><th>Name</th><th>Metric</th><th>Value</th></tr> for (name, metric, value) in report_data: html_content += f\\"<tr><td>{name}</td><td>{metric}</td><td>{value}</td></tr>\\" html_content += \\"</table></body></html>\\" # Add the HTML part msg.add_alternative(html_content, subtype=\\"html\\") # Generate the CSV data csv_buffer = io.StringIO() csv_writer = csv.writer(csv_buffer) csv_writer.writerow([\\"Name\\", \\"Metric\\", \\"Value\\"]) csv_writer.writerows(report_data) # Add CSV attachment msg.add_attachment(csv_buffer.getvalue(), maintype=\'application\', subtype=\'csv\', filename=\'performance_report.csv\') # Send the email with smtplib.SMTP(smtp_server) as server: server.send_message(msg) print(\\"Email sent successfully!\\") except Exception as e: print(f\\"Failed to send email: {e}\\") # Example usage send_performance_report( sender_email=\\"sender@example.com\\", recipients=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], smtp_server=\\"localhost\\", summary=\\"This is the monthly performance report. Please find the detailed report attached.\\", report_data=[ (\\"Alice\\", \\"Sales\\", 120), (\\"Bob\\", \\"Customer Satisfaction\\", 85), (\\"Charlie\\", \\"Support Tickets\\", 98) ] )"},{"question":"Objective: Design and implement an asynchronous Python program that performs both network I/O operations and CPU-bound computations. The program should utilize the asyncio library effectively, ensuring proper error handling, logging, and debugging. Description: 1. Implement an asynchronous function called `fetch_data` that simulates a network request by sleeping for a random number of seconds between 1 and 5 and then returns a random integer between 1 and 100. 2. Implement a CPU-bound function called `compute_factorial` that computes the factorial of a given integer. 3. Create an asynchronous function called `process_tasks` that does the following: - Run `fetch_data` concurrently to fetch data from 5 different simulated network requests. - For each fetched integer, use `loop.run_in_executor` to compute its factorial, ensuring that the event loop remains responsive. - Collect the results and return a dictionary where the keys are the fetched integers and the values are their corresponding factorials. 4. Ensure the following: - The program runs in asyncio\'s debug mode. - Proper logging is implemented to log debug messages. - Handle cases where coroutines are not awaited or exceptions are not retrieved properly. Requirements: - **Input**: None. - **Output**: A dictionary where keys are integers fetched from network requests, and values are their corresponding factorials. - Proper use of asyncio\'s debugging tools. - Correct handling of potential issues like not-awaited coroutines and not-retrieved exceptions. Constraints: - The function `fetch_data` must use `asyncio.sleep` to simulate network delay. - The computation of factorials must be offloaded using `loop.run_in_executor`. - You must use the asyncio logging mechanism to log relevant information. Example: ```python import asyncio import logging import random from concurrent.futures import ThreadPoolExecutor async def fetch_data(): await asyncio.sleep(random.randint(1, 5)) return random.randint(1, 100) def compute_factorial(n): if n == 1: return 1 else: return n * compute_factorial(n-1) async def process_tasks(): # Your implementation here if __name__ == \\"__main__\\": logging.basicConfig(level=logging.DEBUG) asyncio.run(process_tasks(), debug=True) ``` # Notes: - This task will test students\' understanding of asynchronous programming, handling CPU-bound tasks, and proper debugging and logging using asyncio. - Ensure students are aware of utilizing thread pools for CPU-bound tasks and the impact on event loops.","solution":"import asyncio import logging import random from concurrent.futures import ThreadPoolExecutor async def fetch_data(): await asyncio.sleep(random.randint(1, 5)) return random.randint(1, 100) def compute_factorial(n): if n == 1: return 1 else: return n * compute_factorial(n - 1) async def process_tasks(): logging.debug(\\"Starting process_tasks\\") loop = asyncio.get_running_loop() executor = ThreadPoolExecutor() # Fetching data concurrently fetch_coroutines = [fetch_data() for _ in range(5)] fetch_results = await asyncio.gather(*fetch_coroutines) logging.debug(f\\"Fetched data: {fetch_results}\\") # Compute factorials concurrently factorial_tasks = { num: loop.run_in_executor(executor, compute_factorial, num) for num in fetch_results } factorial_results = await asyncio.gather(*factorial_tasks.values()) result = dict(zip(factorial_tasks.keys(), factorial_results)) logging.debug(f\\"Final result: {result}\\") return result if __name__ == \\"__main__\\": logging.basicConfig(level=logging.DEBUG) asyncio.run(process_tasks(), debug=True)"},{"question":"# Advanced Asyncio Queue Operations **Objective:** Implement a solution that demonstrates advanced usage of `asyncio.Queue` and its variants (`PriorityQueue`, `LifoQueue`). Your task is to simulate a job processing system where jobs have varying priorities and specific handling time. The system should efficiently manage multiple workers processing these jobs concurrently. **Problem Statement:** You are given multiple jobs, each with a specific priority and processing time. Implement an `async` function `process_jobs` that: 1. Initializes a `PriorityQueue`. 2. Adds jobs to this priority queue. Each job is represented as a tuple (`priority`, `processing_time`). 3. Initializes `n` workers (concurrent tasks) to process jobs from the queue. 4. Each worker should: - Retrieve a job from the queue. - \\"Process\\" this job asynchronously by sleeping for `processing_time` seconds. - Indicate to the queue that the job is done. 5. Ensure that all jobs are processed before the function completes. **Function Signature:** ```python import asyncio from typing import List, Tuple async def process_jobs(jobs: List[Tuple[int, float]], n: int) -> None: # Your implementation here ``` **Input:** - `jobs`: A list of tuples where each tuple contains: - `priority`: An integer indicating the priority of the job (lower number means higher priority). - `processing_time`: A float indicating the time (in seconds) required to process the job. - `n`: An integer representing the number of concurrent workers. **Output:** - The function should not return anything but must print when each worker starts and finishes processing a job. For example: ``` worker-0 started job with priority 2 for 3.0 seconds worker-0 finished job with priority 2 ``` **Constraints:** - `1 <= len(jobs) <= 100` - `1 <= n <= 10` - `0.1 <= processing_time <= 5.0` **Example:** ```python jobs = [(1, 1.0), (2, 2.0), (1, 3.0), (3, 4.0)] n = 2 asyncio.run(process_jobs(jobs, n)) # Expected Output: # worker-0 started job with priority 1 for 1.0 seconds # worker-0 finished job with priority 1 # worker-1 started job with priority 1 for 3.0 seconds # worker-1 finished job with priority 1 # worker-0 started job with priority 2 for 2.0 seconds # worker-0 finished job with priority 2 # worker-1 started job with priority 3 for 4.0 seconds # worker-1 finished job with priority 3 ``` **Implementation Details:** - Make sure to use `asyncio.PriorityQueue` for the priority queue. - The `process_jobs` function should handle job prioritization and manage worker tasks concurrently. - You might utilize `queue.get()` and `queue.task_done()`, managing them asynchronously. - Ensure proper logging of job start and end times with respective worker IDs to the console.","solution":"import asyncio from typing import List, Tuple async def worker(name: str, queue: asyncio.PriorityQueue): while True: priority, processing_time = await queue.get() print(f\\"{name} started job with priority {priority} for {processing_time} seconds\\") await asyncio.sleep(processing_time) print(f\\"{name} finished job with priority {priority}\\") queue.task_done() async def process_jobs(jobs: List[Tuple[int, float]], n: int) -> None: queue = asyncio.PriorityQueue() for job in jobs: queue.put_nowait(job) tasks = [] for i in range(n): task = asyncio.create_task(worker(f\\"worker-{i}\\", queue)) tasks.append(task) await queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True)"},{"question":"# PyTorch Special Functions Objective: You are tasked with implementing a scientific computation function using PyTorch\'s `torch.special` module that performs a series of calculations involving special mathematical functions. This will demonstrate your understanding of the PyTorch special functions and their application. Problem Statement: Write a function `special_computation` that accepts a single PyTorch tensor `input_tensor` and performs the following operations: 1. Calculate the Airy function Ai for each element in the tensor using `torch.special.airy_ai`. 2. Compute the zeroth-order Bessel function of the first kind for each element using `torch.special.bessel_j0`. 3. Apply the digamma function to each element using `torch.special.digamma`. 4. Compute the error function (erf) for each element using `torch.special.erf`. 5. Compute the logarithm of the softmax of the tensor along the first dimension using `torch.special.log_softmax`. The function should return a dictionary containing the results of each sub-task. Each result should be a tensor of the same shape as the `input_tensor`. Constraints: - The input tensor will be a 2-dimensional tensor with shape (N, M), where 1 ≤ N, M ≤ 100. - The tensor elements will be real numbers within the range [-10, 10]. Function Signature: ```python import torch def special_computation(input_tensor: torch.Tensor) -> dict: # Your implementation here # Example Usage: input_tensor = torch.tensor([[0.1, 0.2], [0.3, 0.4]], dtype=torch.float32) result = special_computation(input_tensor) print(result) ``` Expected Output: For the input tensor: ```python input_tensor = torch.tensor([[0.1, 0.2], [0.3, 0.4]], dtype=torch.float32) ``` The output dictionary should contain five key-value pairs: - `airy_ai`: The tensor after applying the Airy function Ai. - `bessel_j0`: The tensor after applying the zeroth-order Bessel function. - `digamma`: The tensor after applying the digamma function. - `erf`: The tensor after applying the error function. - `log_softmax`: The tensor after applying the logarithm of the softmax along the first dimension. ```python { \'airy_ai\': tensor([...]), \'bessel_j0\': tensor([...]), \'digamma\': tensor([...]), \'erf\': tensor([...]), \'log_softmax\': tensor([...]) } ``` Ensure that each key in the output dictionary contains a tensor of the expected shape and type. Performance Requirements: - The function must complete in a reasonable time frame (under a second) for N, M ≤ 100 dimensions.","solution":"import torch def special_computation(input_tensor: torch.Tensor) -> dict: Perform a series of special computations on a given input tensor and return a dictionary with the results. Parameters: input_tensor (torch.Tensor): A 2-dimensional tensor with shape (N, M) Returns: dict: A dictionary containing the results of the special computations results = { \'airy_ai\': torch.special.airy_ai(input_tensor), \'bessel_j0\': torch.special.bessel_j0(input_tensor), \'digamma\': torch.special.digamma(input_tensor), \'erf\': torch.special.erf(input_tensor), \'log_softmax\': torch.special.log_softmax(input_tensor, dim=0) } return results # Example usage input_tensor = torch.tensor([[0.1, 0.2], [0.3, 0.4]], dtype=torch.float32) result = special_computation(input_tensor) print(result)"},{"question":"# Python Initialization and Configuration Challenge Objective You are tasked with crafting a Python C extension function to initialize Python with a specific configuration, tailored to run in isolated mode with specific customizations. This function should be robust, handle errors gracefully, and allow a particular script to be executed. Task Implement a C function `initialize_and_run_python` that: 1. Initializes Python in isolated mode. 2. Configures Python to: - Enable UTF-8 mode. - Disable write of .pyc files. - Set an executable name. 3. Executes a Python script passed as an argument. 4. Handles any errors during initialization gracefully by printing appropriate error messages. Details * Input: A UTF-8 encoded C string representing the path to a Python script. * Output: Return `0` for success, `-1` for any errors. * Constraints: - Assume the script path is valid and points to a Python script. - Ensure Python initialization is done in isolated mode and that all errors are handled using provided mechanisms. - You are allowed to use any function or structure mentioned in the provided documentation. Example For the script `/path/to/example.py`, calling `initialize_and_run_python(\\"/path/to/example.py\\")` should initialize Python in isolated mode with the specified configurations and execute the script. Function Signature (in C): ```c int initialize_and_run_python(const char *script_path); ``` Below is the skeleton implementation to get you started: ```c #include <Python.h> int initialize_and_run_python(const char *script_path) { PyStatus status; // Initialize PyPreConfig with isolated configuration PyPreConfig preconfig; PyPreConfig_InitIsolatedConfig(&preconfig); preconfig.utf8_mode = 1; // Preinitialize Python with the preconfiguration status = Py_PreInitialize(&preconfig); if (PyStatus_Exception(status)) { Py_ExitStatusException(status); return -1; } // Initialize PyConfig with isolated configuration PyConfig config; PyConfig_InitIsolatedConfig(&config); config.write_bytecode = 0; // Set the executable name status = PyConfig_SetString(&config, &config.executable, L\\"/path/to/my_executable\\"); if (PyStatus_Exception(status)) { PyConfig_Clear(&config); Py_ExitStatusException(status); return -1; } // Set the script to run status = PyConfig_SetString(&config, &config.run_filename, Py_DecodeLocale(script_path, NULL).wide_str); if (PyStatus_Exception(status)) { PyConfig_Clear(&config); Py_ExitStatusException(status); return -1; } // Initialize Python from the configuration status = Py_InitializeFromConfig(&config); if (PyStatus_Exception(status)) { PyConfig_Clear(&config); Py_ExitStatusException(status); return -1; } PyConfig_Clear(&config); // Run the script return Py_RunMain(); } ``` Complete this function ensuring all requirements are met and tested with different scenarios.","solution":"def initialize_and_run_python(script_path): Initializes Python in isolated mode with specific configurations and executes the given Python script. Args: script_path (str): The path to the Python script to execute. Returns: int: 0 for success, -1 for any errors. import subprocess # Build the command to run Python with the required configurations command = [ \\"python\\", \\"-I\\", # Isolated mode \\"-X\\", \\"utf8\\", # Enable UTF-8 mode \\"-X\\", \\"no_pyc\\", # Disable write of .pyc files script_path ] try: # Run the command result = subprocess.run(command, check=True) return 0 except subprocess.CalledProcessError as e: print(f\\"Error executing script: {e}\\") return -1"},{"question":"# PyTorch Distributed Metrics Handler Objective In this assessment, your goal is to demonstrate your understanding of metrics handling in a distributed training setup with PyTorch, using the `torchelastic` framework. You will implement a function to configure and use multiple metric handlers during a training loop. Problem Statement You need to create a function `train_with_metrics` that sets up metric handlers, configures them, and then uses these handlers to log metrics during a mock training loop. Function Signature ```python def train_with_metrics(epochs: int) -> None: Configures metric handlers and logs metrics during a training loop. Args: epochs (int): Number of epochs to run the training loop. Returns: None: The function does not return any value. ``` Detailed Requirements 1. **Metric Handlers Configuration**: - Configure `ConsoleMetricHandler` and `NullMetricHandler`. 2. **Training Loop**: - Simulate a training loop that runs for the specified number of epochs. - During each epoch, log a mock metric (e.g., training loss) using the configured metric handlers. 3. **Logging Metrics**: - Use the `put_metric` function to log a mock metric named `training_loss`, with values decreasing over the epochs (e.g., starting at 1.0 and decreasing by 0.1 each epoch). Input - `epochs`: An integer representing the number of epochs to run the training loop. Output - None Example ```python train_with_metrics(5) ``` This should configure the metric handlers and run a training loop for 5 epochs, logging the `training_loss` for each epoch using the `ConsoleMetricHandler`. Constraints - You can assume the `torch` and `torchelastic` packages are available. - This task is designed to be executed in an environment supporting distributed training (though the example doesn\'t require actual distributed training for simplicity). Notes - Focus on the configuration and proper usage of metric handlers within the training loop. - Ensure the metrics are logged correctly using the provided API functions.","solution":"import datetime def put_metric(name, value, handler): Mock function to put the metric using the handler handler(name, value) def ConsoleMetricHandler(name, value): Mock ConsoleMetricHandler function print(f\\"[{datetime.datetime.now()}] {name}: {value}\\") def NullMetricHandler(name, value): Mock NullMetricHandler function (does nothing) pass def train_with_metrics(epochs): Configures metric handlers and logs metrics during a training loop. Args: epochs (int): Number of epochs to run the training loop. Returns: None: The function does not return any value. metric_handlers = [ConsoleMetricHandler, NullMetricHandler] for epoch in range(epochs): # Simulate training loss decreasing with each epoch training_loss = 1.0 - 0.1 * epoch for handler in metric_handlers: put_metric(\\"training_loss\\", training_loss, handler)"},{"question":"# Question: Advanced Univariate Distribution Plotting with Seaborn You are tasked with creating a complex plot using Seaborn that demonstrates your understanding of the `sns.ecdfplot` function and its customization features. Task: 1. **Loading Data:** Load the `penguins` dataset provided by Seaborn. 2. **Composite ECDF Plot:** Create a composite plot consisting of two subplots - one showing the Empirical Cumulative Distribution Function (ECDF) and another showing the Complementary Cumulative Distribution Function (CCDF) of the `flipper_length_mm` variable. 3. **Customization:** - In the ECDF plot, color the data points based on the `species` variable using the `hue` parameter. - In the CCDF plot, show the absolute counts instead of the default proportion. - Both plots should display appropriate titles, and axes should be labeled clearly. 4. **Styling:** - Apply a suitable theme using `sns.set_theme()` for better aesthetics. - Ensure that the plots do not overlap and are clearly distinguishable. Expected Input and Output: - **Input:** - No input required. - The function should not take any parameters but operate directly on the `penguins` dataset. - **Output:** - A composite plot with two subplots showing the ECDF and CCDF of `flipper_length_mm`. Constraints: - Use only Seaborn for plotting. - Properly handle any missing values in the `flipper_length_mm` variable. Performance Requirements: - The plot should render efficiently without significant delay. # Solution Template: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_ecdf_ccdf(): # Set the theme sns.set_theme() # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Filter out missing values penguins = penguins.dropna(subset=[\'flipper_length_mm\']) # Create subplots fig, axes = plt.subplots(1, 2, figsize=(14, 7)) # ECDF plot with hue based on species sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", ax=axes[0]) axes[0].set_title(\'ECDF of Flipper Length\') axes[0].set_xlabel(\'Flipper Length (mm)\') axes[0].set_ylabel(\'Proportion\') # CCDF plot with counts sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", complementary=True, stat=\\"count\\", ax=axes[1]) axes[1].set_title(\'CCDF of Flipper Length (Counts)\') axes[1].set_xlabel(\'Flipper Length (mm)\') axes[1].set_ylabel(\'Count\') plt.show() plot_ecdf_ccdf() ``` Use this template to implement your solution and ensure the plots are customized according to the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_ecdf_ccdf(): # Set the theme sns.set_theme(style=\\"whitegrid\\") # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Filter out missing values penguins = penguins.dropna(subset=[\'flipper_length_mm\']) # Create subplots fig, axes = plt.subplots(1, 2, figsize=(14, 7)) # ECDF plot with hue based on species sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", ax=axes[0]) axes[0].set_title(\'ECDF of Flipper Length\') axes[0].set_xlabel(\'Flipper Length (mm)\') axes[0].set_ylabel(\'Proportion\') # CCDF plot with counts sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", complementary=True, stat=\\"count\\", ax=axes[1]) axes[1].set_title(\'CCDF of Flipper Length (Counts)\') axes[1].set_xlabel(\'Flipper Length (mm)\') axes[1].set_ylabel(\'Count\') plt.tight_layout() plt.show() plot_ecdf_ccdf()"},{"question":"# Email Content Extractor with Attachment Handling Problem Statement Design a Python function that reads an email file, extracts its content, and saves any attachments to a specified directory. Your function should use the `email` module for parsing the email and handling MIME types. Function Signature ```python def extract_email_content(email_path: str, output_dir: str) -> dict: Reads an email file, extracts the content, and saves any attachments to the specified directory. Arguments: email_path -- Path to the email file. output_dir -- Directory to save any attachments. Returns: A dictionary with the following keys: \\"subject\\" -- Subject of the email. \\"from\\" -- Sender of the email. \\"to\\" -- Recipient(s) of the email. \\"date\\" -- Date the email was sent. \\"body\\" -- Body content of the email. \\"attachments\\" -- List of filenames for the saved attachments. pass ``` Input - `email_path` (str): The file path to the email (.eml) file. - `output_dir` (str): The file directory path where attachments should be saved. Output - Returns a dictionary containing the email\'s subject, sender, recipient(s), date, and body content. It also includes a list of filenames for the saved attachments. Constraints - The function should handle multipart and mixed content types in an email. - Attachments should be saved with their original filenames in the specified `output_dir`. - Ensure proper error handling in case the file paths or email content are invalid. Example ```python email_info = extract_email_content(\'example_email.eml\', \'attachments/\') # Possible output: # { # \'subject\': \'Meeting Schedule\', # \'from\': \'alice@example.com\', # \'to\': [\'bob@example.com\', \'carol@example.com\'], # \'date\': \'Mon, 25 Oct 2021 14:42:35 -0700\', # \'body\': \'Dear team, please find the meeting schedule attached.\', # \'attachments\': [\'schedule.pdf\'] # } ``` This function should demonstrate the student\'s ability to utilize the `email` module for real-world email handling, including parsing, managing MIME content, handling attachments, and providing appropriate output formatting. Hints - Use `email.parser` to parse the email file. - Use `os.path` for file and directory operations. - Utilize `email.policy` for handling email policies while parsing. - Be cautious with handling different parts of a multipart email. - Ensure use of appropriate MIME type checking to identify and save attachments.","solution":"import os import email from email import policy from email.parser import BytesParser def extract_email_content(email_path: str, output_dir: str) -> dict: Reads an email file, extracts the content, and saves any attachments to the specified directory. Arguments: email_path -- Path to the email file. output_dir -- Directory to save any attachments. Returns: A dictionary with the following keys: \\"subject\\" -- Subject of the email. \\"from\\" -- Sender of the email. \\"to\\" -- Recipient(s) of the email. \\"date\\" -- Date the email was sent. \\"body\\" -- Body content of the email. \\"attachments\\" -- List of filenames for the saved attachments. if not os.path.exists(email_path): raise FileNotFoundError(f\\"No such file: \'{email_path}\'\\") if not os.path.exists(output_dir): os.makedirs(output_dir) with open(email_path, \'rb\') as f: msg = BytesParser(policy=policy.default).parse(f) email_content = { \'subject\': msg[\'subject\'], \'from\': msg[\'from\'], \'to\': msg.get(\'to\').split(\\", \\"), \'date\': msg[\'date\'], \'body\': \'\', \'attachments\': [] } if msg.is_multipart(): for part in msg.iter_parts(): if part.get_content_disposition() == \'attachment\': filename = part.get_filename() if filename: attachment_path = os.path.join(output_dir, filename) with open(attachment_path, \'wb\') as attachment_file: attachment_file.write(part.get_payload(decode=True)) email_content[\'attachments\'].append(filename) elif part.get_content_type().startswith(\\"text/\\"): email_content[\'body\'] += part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\') else: email_content[\'body\'] = msg.get_payload(decode=True).decode(msg.get_content_charset() or \'utf-8\') return email_content"},{"question":"# Question: Implement a TorchScript-Enabled Neural Network Module You have been provided with a simplified subset of a documentation on TorchScript, a statically typed subset of Python for PyTorch model definition and execution. Your task is to implement a small neural network module using TorchScript and then compile it using the appropriate TorchScript decorators. # Objectives: 1. **Define a TorchScript class** `SimpleNNModule` with a structure outlined below. 2. **Initialize** both trainable parameters (weights, biases) and non-trainable constants inside your model. 3. **Implement the forward pass** adhering to type constraints and TorchScript syntax. 4. **Compile and test the module** using `torch.jit.script`. # Specifications: 1. **Class Definition**: - Name: `SimpleNNModule` - Inherits from: `torch.nn.Module` - Attributes: - `weights` (torch.nn.Parameter) — Trainable parameter initialized with torch.randn. - `bias` (torch.nn.Parameter) — Trainable parameter initialized with torch.randn. - `CONST_FACTOR` (int) — Non-trainable constant, value set to 5. 2. **Method**: - `__init__(self, in_features: int, out_features: int)`: Initializes `weights` with dimensions `(out_features, in_features)` and `bias` with dimensions `(out_features)`. - `forward(self, x: torch.Tensor) -> torch.Tensor`: Computes the following operation: `output = (x @ self.weights.t() + self.bias) * self.CONST_FACTOR` # Constraints: - Do not use any features or types not explicitly mentioned in the provided documentation. - Ensure the variable types remain consistent across operations. - Implement appropriate type hints for both methods. # Steps to Follow: 1. **Define the TorchScript Class**: 2. **Implement Initialization**: - Initialize the weights and bias tensors. 3. **Write the Forward Method**: - Apply the linear transformation and scale by the constant factor. 4. **Compile Using TorchScript**: 5. **Test the Implementation**: - Create an instance of `SimpleNNModule`. - Script the instance with `torch.jit.script`. - Pass a test tensor through the scripted model and print the output. # Sample Input: ```python in_features = 3 out_features = 2 input_tensor = torch.randn(1, 3) ``` # Sample Output: ```python tensor([...]) # corresponding output tensor after processing through the scripted module ``` # Your Code: ```python import torch import torch.nn as nn from typing import Tuple @torch.jit.script class SimpleNNModule(nn.Module): CONST_FACTOR: torch.jit.Final[int] = 5 def __init__(self, in_features: int, out_features: int): super(SimpleNNModule, self).__init__() self.weights = nn.Parameter(torch.randn(out_features, in_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, x: torch.Tensor) -> torch.Tensor: output = (x @ self.weights.T + self.bias) * self.CONST_FACTOR return output # Here is an example of how you would instantiate and test the compiled module: if __name__ == \\"__main__\\": in_features = 3 out_features = 2 model = SimpleNNModule(in_features, out_features) scripted_model = torch.jit.script(model) input_tensor = torch.randn(1, 3) output = scripted_model(input_tensor) print(output) ``` This coding task evaluates understanding and application of TorchScript, including class syntax, typing, and neural network operations.","solution":"import torch import torch.nn as nn class SimpleNNModule(nn.Module): CONST_FACTOR: torch.jit.Final[int] = 5 def __init__(self, in_features: int, out_features: int): super(SimpleNNModule, self).__init__() self.weights = nn.Parameter(torch.randn(out_features, in_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, x: torch.Tensor) -> torch.Tensor: output = (x @ self.weights.t() + self.bias) * self.CONST_FACTOR return output # Compile the model using TorchScript def get_scripted_model(in_features: int, out_features: int): model = SimpleNNModule(in_features, out_features) scripted_model = torch.jit.script(model) return scripted_model"},{"question":"# Unsupervised Learning Assessment with Scikit-Learn Objective: Design and implement a machine learning pipeline using scikit-learn that performs clustering on a given dataset. The pipeline should include data preprocessing, clustering using KMeans, evaluation of the clustering quality using silhouette score, and visualization of the resulting clusters. Dataset: You will be working with the Iris dataset, which is available directly from scikit-learn. Components: 1. **Data Preprocessing**: - Load the Iris dataset from scikit-learn. - Normalize the feature data using `StandardScaler`. 2. **Clustering**: - Apply KMeans clustering with a specified number of clusters `n_clusters`. 3. **Evaluation**: - Compute the silhouette score of the clustering to evaluate the quality of the clusters. 4. **Visualization**: - Create a 2D scatter plot of the clusters using the first two principal components obtained via PCA. Function Signature: ```python def iris_clustering(n_clusters: int) -> dict: Perform KMeans clustering on the Iris dataset and evaluate its quality. Parameters: - n_clusters (int): The number of clusters to form Returns: - result (dict): A dictionary containing: - \'silhouette_score\': The silhouette score of the clustering - \'pca_components\': A 2D array of the first two principal components for visualization - \'labels\': The cluster labels assigned to each sample pass ``` Constraints: - Use `KMeans` from `sklearn.cluster`. - Use `StandardScaler` from `sklearn.preprocessing`. - Use `silhouette_score` from `sklearn.metrics`. - Use `PCA` from `sklearn.decomposition`. Expected Workflow: 1. **Load Data**: ```python from sklearn.datasets import load_iris data = load_iris() X, y = data.data, data.target ``` 2. **Normalize Data**: ```python from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_scaled = scaler.fit_transform(X) ``` 3. **Fit KMeans**: ```python from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=n_clusters, random_state=42) labels = kmeans.fit_predict(X_scaled) ``` 4. **Compute Silhouette Score**: ```python from sklearn.metrics import silhouette_score score = silhouette_score(X_scaled, labels) ``` 5. **PCA for Visualization**: ```python from sklearn.decomposition import PCA pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) ``` 6. **Return Results**: ```python result = { \'silhouette_score\': score, \'pca_components\': X_pca, \'labels\': labels } return result ``` Good luck!","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score from sklearn.decomposition import PCA def iris_clustering(n_clusters: int) -> dict: Perform KMeans clustering on the Iris dataset and evaluate its quality. Parameters: - n_clusters (int): The number of clusters to form Returns: - result (dict): A dictionary containing: - \'silhouette_score\': The silhouette score of the clustering - \'pca_components\': A 2D array of the first two principal components for visualization - \'labels\': The cluster labels assigned to each sample # Load the Iris dataset data = load_iris() X = data.data # Normalize the feature data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) labels = kmeans.fit_predict(X_scaled) # Compute the silhouette score score = silhouette_score(X_scaled, labels) # Perform PCA for visualization pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Return the results in a dictionary result = { \'silhouette_score\': score, \'pca_components\': X_pca, \'labels\': labels } return result"},{"question":"**Question:** You are required to implement a function `read_netrc(file_path: str) -> dict` that processes a `.netrc` file and returns a dictionary with host names as keys and tuples `(login, account, password)` as values. If the file contains a \'default\' entry, include it with the key `\'default\'`. Your function should handle exceptions appropriately and ensure security checks are adhered to. # Function Signature: ```python def read_netrc(file_path: str) -> dict: pass ``` # Input: - `file_path`: A string representing the path to the `.netrc` file. # Output: - A dictionary with the host names as keys and tuples `(login, account, password)` as values. # Exceptions: - If the file is not found, raise a `FileNotFoundError` with a custom message: `\\"File not found: {file_path}\\"`. - If there are parse errors, raise a custom `NetrcParseError` with the format `\\"Parse error in file \'{filename}\' at line {lineno}: {msg}\\"`. - If the file permissions are insecure, raise a `PermissionError` with a message: `\\"Insecure file permissions for file: {file_path}\\"`. # Constraints: - Your solution should handle files up to 10KB efficiently. # Example: Assuming a `.netrc` file with the following content: ``` machine host1 login user1 password pass1 machine host2 login user2 account acc2 password pass2 default login default_user password default_pass ``` Example function call: ```python file_path = \\"path/to/.netrc\\" result = read_netrc(file_path) ``` Expected output: ```python { \\"host1\\": (\\"user1\\", None, \\"pass1\\"), \\"host2\\": (\\"user2\\", \\"acc2\\", \\"pass2\\"), \\"default\\": (\\"default_user\\", None, \\"default_pass\\") } ``` # Notes: - Ensure you use `os.path.expanduser()` to resolve the `~` in the file path if present. - Ensure that you handle file encoding properly as described in the documentation. Hint: Use the `netrc` library methods and properties to handle parsing and data retrieval.","solution":"import os from netrc import netrc, NetrcParseError def read_netrc(file_path: str) -> dict: Reads a .netrc file and returns a dictionary with host names as keys and tuples of (login, account, password) as values. Parameters: file_path (str): The path to the .netrc file. Returns: dict: A dictionary with host names as keys and tuples (login, account, password) as values. # Expand any user-specific shortcuts in file path file_path = os.path.expanduser(file_path) # Check if file exists if not os.path.isfile(file_path): raise FileNotFoundError(f\\"File not found: {file_path}\\") # Ensure file permissions are secure if os.stat(file_path).st_mode & 0o777 != 0o600: raise PermissionError(f\\"Insecure file permissions for file: {file_path}\\") # Read the .netrc file try: netrc_data = netrc(file_path) except NetrcParseError as e: raise NetrcParseError(f\\"Parse error in file \'{e.filename}\' at line {e.lineno}: {e.msg}\\") # Convert netrc data to dictionary result = {} for host, attrs in netrc_data.hosts.items(): result[host] = (attrs[0], attrs[1], attrs[2]) # Include \'default\' if available if \'default\' in netrc_data.hosts: result[\'default\'] = tuple(netrc_data.hosts[\'default\']) return result"},{"question":"Objective To evaluate your understanding of calendar manipulation and formatting using Python\'s `calendar` module. Problem Description Implement a function `generate_yearly_calendar_html(first_weekday, year)` that generates an HTML page displaying the calendar for an entire year. The function should allow customization of the first weekday of the week. Function Signature ```python def generate_yearly_calendar_html(first_weekday: int, year: int) -> str: ``` Input Parameters - `first_weekday` (int): The first day of the week, where `0` is Monday and `6` is Sunday. - `year` (int): The year for which the calendar needs to be generated. Output - `str`: A string containing the HTML of the full-year calendar. Constraints - `0 <= first_weekday <= 6` - `year` should be a four-digit integer (e.g., `2023`). Requirements 1. Use the `HTMLCalendar` class from the `calendar` module to create the calendar. 2. Customize the calendar\'s first day of the week using the provided `first_weekday`. 3. Generate a formatted HTML string for the entire year. Example ```python html_calendar = generate_yearly_calendar_html(0, 2022) print(html_calendar) ``` The above example should print the HTML content for the calendar of the year 2022 with Monday as the first day of the week. Additional Information - You may use CSS classes to style the calendar, as described in the documentation. - Ensure that your HTML is properly formatted for a web page. Hint Explore the `calendar.HTMLCalendar` class and its methods such as `formatyear()` and `setfirstweekday()`. Remember to set the first weekday before generating the calendar.","solution":"import calendar def generate_yearly_calendar_html(first_weekday: int, year: int) -> str: Generates an HTML page displaying the calendar for an entire year with the given first weekday. Parameters: first_weekday (int): The first day of the week, where 0 is Monday and 6 is Sunday. year (int): The year for which the calendar needs to be generated. Returns: str: A string containing the HTML of the full-year calendar. # Validate inputs if not (0 <= first_weekday <= 6): raise ValueError(\\"first_weekday must be between 0 (Monday) and 6 (Sunday)\\") if not (1000 <= year <= 9999): raise ValueError(\\"year must be a four-digit integer\\") # Create an HTMLCalendar instance and configure the first weekday html_cal = calendar.HTMLCalendar(firstweekday=first_weekday) # Generate the HTML for the entire year html_year_calendar = html_cal.formatyear(year, width=3) return html_year_calendar"},{"question":"<|Analysis Begin|> The provided documentation is for the \\"modulefinder\\" module, which is used to find the set of modules imported by a script. The key functionalities outlined include: 1. `ModuleFinder` class that can analyze a Python script to determine which modules it imports. It includes parameters for customization like `path`, `debug`, `excludes`, and `replace_paths`. 2. `run_script(pathname)` method to analyze a script. 3. `report()` method to print a report of the imported modules. 4. Example usage of the `ModuleFinder` class showing how it can be used to find imported modules and handle modules that are missing. The information provided is sufficient to create a question that tests the comprehension and usage of the `modulefinder` module in a more complex scenario than the basic example given. The question should focus on: - Implementing a function that uses `ModuleFinder` to analyze a given script. - Customizing the analysis by excluding certain modules and replacing paths. - Generating a detailed report of the imported modules and any missing modules. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate your understanding of using the `modulefinder` module to analyze Python scripts, apply customizations, and generate a report of the findings. **Task:** Write a Python function `analyze_script(filepath: str, excludes: list, replace_paths: list) -> dict` that performs the following tasks: 1. Uses the `modulefinder.ModuleFinder` class to analyze the script provided in `filepath`. 2. Excludes modules listed in `excludes` from the analysis. 3. Replaces paths using the tuples provided in `replace_paths`. 4. Returns a dictionary with two keys: - `\\"loaded_modules\\"`: A dictionary mapping module names to a list of their top-level global names. - `\\"missing_modules\\"`: A list of module names that were supposed to be imported but were not found. **Function Signature:** ```python def analyze_script(filepath: str, excludes: list, replace_paths: list) -> dict: pass ``` **Input:** - `filepath` (str): The path to the script to be analyzed. - `excludes` (list): A list of module names to exclude from the analysis. - `replace_paths` (list): A list of `(oldpath, newpath)` tuples to replace in module paths. **Output:** - A dictionary with the analysis results containing: - `\\"loaded_modules\\"`: A dictionary where each key is a module name and the value is a list of the top-level global names in that module. - `\\"missing_modules\\"`: A list of module names that were not found during the import process. **Example:** Given a script `sample_script.py` with the following content: ```python import os import sys try: import nonexistentmodule except ImportError: pass ``` Calling the function: ```python result = analyze_script(\'sample_script.py\', excludes=[\'sys\'], replace_paths=[(\'nonexistentmodule\', \'replacementmodule\')]) ``` The `result` dictionary may look like: ```python { \\"loaded_modules\\": { \\"os\\": [\\"name\\", \\"path\\", \\"listdir\\"], # other modules in the scope of os }, \\"missing_modules\\": [\\"nonexistentmodule\\"] } ``` **Constraints:** - The function should handle exceptions gracefully and provide meaningful error messages if the script cannot be analyzed. - Assume the input script `filepath` always exists and is readable. **Note:** You may need to import the `modulefinder` module to complete this task.","solution":"import modulefinder def analyze_script(filepath: str, excludes: list, replace_paths: list) -> dict: # Initialize the ModuleFinder instance finder = modulefinder.ModuleFinder(excludes=excludes) # Apply path replacements if any for oldpath, newpath in replace_paths: finder.replace_paths.append((oldpath, newpath)) # Run the script analysis finder.run_script(filepath) # Collect loaded modules information loaded_modules = {name: list(module.globalnames.keys()) for name, module in finder.modules.items()} # Collect missing modules information missing_modules = list(finder.badmodules.keys()) return { \\"loaded_modules\\": loaded_modules, \\"missing_modules\\": missing_modules }"},{"question":"# Seaborn Categorical Data Visualization Challenge You are given a dataset containing information about a restaurant\'s daily revenue and various other attributes. Your task is to visualize this data to obtain insights into the distribution and central tendency across different categories. You are required to implement a function utilizing seaborn to create multiple visualizations in a single figure, showcasing different aspects of the dataset. Dataset Description - `day`: The day of the week (`str`). - `total_bill`: Total bill amount in dollars (`float`). - `tip`: Tip given in dollars (`float`). - `sex`: Gender of the person paying the bill (`str`). - `smoker`: Whether the person is a smoker (`Yes` or `No`). - `time`: Time of the day (`Lunch` or `Dinner`). - `size`: Number of people in the party (`int`). Function Signature ```python def create_visualizations(data: pd.DataFrame) -> None: pass ``` # Requirements: 1. **Categorical Scatter Plot:** - Create a scatter plot to show `total_bill` against `day`, differentiating points by `sex` using the `hue` parameter. - Use the `swarm` kind in the `catplot` function. 2. **Box Plot:** - Create a box plot to compare the distribution of `total_bill` for different days, segmented by `smoker` status. - Ensure the box plots are ordered by days [\\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"]. 3. **Violin Plot:** - Create a violin plot to visualize the distribution of `tip` across different sizes of dining parties. - Use the split option to differentiate by `smoker` status. 4. **Bar Plot:** - Create a bar plot to show the average `tip` amount for each day, segmented by `time` of the day. 5. **Point Plot:** - Create a point plot to show the proportion of `size` across different days, segmented by `sex`. - Include confidence intervals for the estimates. 6. **FacetGrid:** - Use `FacetGrid` to create separate plots for `Dinner` and `Lunch` times, showing the relationship between `total_bill` and `day`. # Constraints: - Ensure each plot has appropriate labels, titles, and legends. - Use color palettes that enhance the readability of the plots. - Address overlapping points where necessary. # Input Format: - `data`: A `pandas.DataFrame` containing the restaurant data with the columns mentioned above. # Output Format: - Display the created visualizations. The function does not need to return any value. # Example Usage: ```python import pandas as pd import seaborn as sns # Assuming data is loaded into a DataFrame data = sns.load_dataset(\\"tips\\") create_visualizations(data) ``` Your task is to implement the `create_visualizations` function according to the specifications above. The solution should be properly commented to explain the steps and choices made.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(data: pd.DataFrame) -> None: Create several categorial visualizations using seaborn and matplotlib. Parameters: data (pd.DataFrame): The dataset containing information about a restaurant\'s daily revenue and attributes. # Set the aesthetic style of the plots sns.set(style=\\"whitegrid\\") # Categorical scatter plot (swarm plot) plt.figure(figsize=(10, 6)) sns.catplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", kind=\\"swarm\\", data=data) plt.title(\\"Total Bill by Day and Gender\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\\"Gender\\") plt.show() # Box plot plt.figure(figsize=(10, 6)) order = [\\"Thur\\", \\"Fri\\", \\"Sat\\", \\"Sun\\"] sns.boxplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", data=data, order=order) plt.title(\\"Total Bill Distribution by Day and Smoker Status\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\\"Smoker\\") plt.show() # Violin plot plt.figure(figsize=(10, 6)) sns.violinplot(x=\\"size\\", y=\\"tip\\", hue=\\"smoker\\", data=data, split=True) plt.title(\\"Tip Distribution by Dining Party Size\\") plt.xlabel(\\"Party Size\\") plt.ylabel(\\"Tip\\") plt.legend(title=\\"Smoker\\") plt.show() # Bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=\\"day\\", y=\\"tip\\", hue=\\"time\\", data=data) plt.title(\\"Average Tip by Day and Time of Day\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Average Tip\\") plt.legend(title=\\"Time of Day\\") plt.show() # Point plot plt.figure(figsize=(10, 6)) sns.pointplot(x=\\"day\\", y=\\"size\\", hue=\\"sex\\", data=data, ci=\\"sd\\") plt.title(\\"Proportion of Party Size by Day and Gender\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Average Party Size\\") plt.legend(title=\\"Gender\\") plt.show() # FacetGrid g = sns.FacetGrid(data, col=\\"time\\", height=6, aspect=1) g.map(sns.scatterplot, \\"day\\", \\"total_bill\\") g.add_legend() g.set_axis_labels(\\"Day\\", \\"Total Bill\\") plt.subplots_adjust(top=0.85) g.fig.suptitle(\\"Total Bill by Day and Time of Day\\") plt.show()"},{"question":"**Coding Assessment Question** # Problem Statement You are tasked with constructing and analyzing a neural network on the meta device in PyTorch. Specifically, you need to create a neural network with a given architecture, load it onto the meta device, and then transfer it to the CPU device with uninitialized parameters. You will implement two functions: 1. `create_meta_model(input_size: int, hidden_size: int, output_size: int) -> torch.nn.Module`: - This function should create a neural network model with one hidden layer using the `torch.nn.Linear` class. - The input layer should have `input_size` neurons, the hidden layer should have `hidden_size` neurons, and the output layer should have `output_size` neurons. - The model should be created on the meta device. 2. `transfer_to_cpu_with_metadata_model(meta_model: torch.nn.Module) -> torch.nn.Module`: - This function receives a model located on the meta device and transfers it to the CPU with uninitialized parameters. - Ensure that the transferred model maintains the same architecture and metadata. # Input Format - `create_meta_model`: - `input_size` (int): Number of input features. - `hidden_size` (int): Number of hidden units. - `output_size` (int): Number of output features. - `transfer_to_cpu_with_metadata_model`: - `meta_model` (torch.nn.Module): The model created on the meta device. # Output Format - `create_meta_model`: Returns a `torch.nn.Module` object representing the constructed model on the meta device. - `transfer_to_cpu_with_metadata_model`: Returns a `torch.nn.Module` object representing the model transferred to the CPU with uninitialized parameters. # Examples ```python # Example Usage meta_model = create_meta_model(20, 10, 5) print(meta_model) # Should print the model\'s architecture with meta device cpu_model = transfer_to_cpu_with_metadata_model(meta_model) print(cpu_model) # Should print the model\'s architecture with CPU device ``` # Constraints - Ensure that the model\'s architecture is correctly transferred along with metadata. - The parameters of the model should be left uninitialized when transferred to the CPU. # Note You do **NOT** need to initialize any data in the tensors. The focus is on using the meta device correctly and transferring models with metadata.","solution":"import torch import torch.nn as nn def create_meta_model(input_size: int, hidden_size: int, output_size: int) -> torch.nn.Module: This function creates a neural network model with one hidden layer on the meta device. The input layer has `input_size` neurons, the hidden layer has `hidden_size` neurons, and the output layer has `output_size` neurons. Parameters: - input_size (int): Number of input features. - hidden_size (int): Number of hidden units. - output_size (int): Number of output features. Returns: - torch.nn.Module: The constructed model on the meta device. class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x meta_device = torch.device(\'meta\') model = SimpleNN(input_size, hidden_size, output_size).to(meta_device) return model def transfer_to_cpu_with_metadata_model(meta_model: torch.nn.Module) -> torch.nn.Module: This function takes a model on the meta device and transfers it to the CPU with uninitialized parameters. Parameters: - meta_model (torch.nn.Module): The model created on the meta device. Returns: - torch.nn.Module: The model transferred to the CPU with uninitialized parameters. model_class = meta_model.__class__ model_args = (meta_model.fc1.in_features, meta_model.fc1.out_features, meta_model.fc2.out_features) cpu_model = model_class(*model_args) return cpu_model"},{"question":"Objective: In this task, you will implement Kernel Ridge Regression (KRR) using scikit-learn, perform hyperparameter tuning, and compare the results with Support Vector Regression (SVR). Your code should demonstrate understanding of fundamental and advanced concepts of scikit-learn, including model fitting, hyperparameter tuning, and model performance evaluation. Problem Statement: Given a synthetic dataset with a sinusoidal target function and some noise, perform the following steps: 1. Split the dataset into training and testing sets. 2. Implement Kernel Ridge Regression (KRR) using scikit-learn. 3. Perform hyperparameter tuning using grid search to optimize the complexity/regularization and bandwidth of the RBF kernel for KRR. 4. Implement Support Vector Regression (SVR) and perform similar hyperparameter tuning. 5. Compare the performance of the optimized KRR and SVR models based on: - Fitting time - Prediction time - Mean Squared Error (MSE) on the test set Dataset: You are provided a synthetic dataset. Below is the code to generate it: ```python import numpy as np # Generate synthetic dataset np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - np.random.rand(20)) # Plot the data (optional) import matplotlib.pyplot as plt plt.scatter(X, y, color=\'darkorange\', label=\'data\') plt.title(\'Synthetic Dataset\') plt.legend() plt.show() ``` Instructions: 1. **Data Preparation:** - Split the dataset into training (80%) and testing sets (20%). 2. **Kernel Ridge Regression (KRR):** - Implement a KRR model using the `KernelRidge` class from scikit-learn. - Use grid search to find the best hyperparameters: - `alpha` (regularization parameter): try values in [0.1, 1, 10] - `gamma` (RBF kernel parameter): try values in [0.01, 0.1, 1] 3. **Support Vector Regression (SVR):** - Implement an SVR model using the `SVR` class from scikit-learn. - Use grid search to find the best hyperparameters: - `C` (regularization parameter): try values in [0.1, 1, 10] - `epsilon`: try values in [0.01, 0.1, 1] - `gamma` (RBF kernel parameter): try values in [0.01, 0.1, 1] 4. **Comparison and Evaluation:** - Compare the fitting time for both KRR and SVR models. - Compare the prediction time for both KRR and SVR models on the test set. - Calculate and compare the Mean Squared Error (MSE) on the test set for both models. Expected Output: - Summary of the best hyperparameters for both KRR and SVR. - Fitting time comparison for both models. - Prediction time comparison for both models. - Mean Squared Error (MSE) comparison on the test set for both models. Constraints: - Use scikit-learn v0.24 or above. - Ensure reproducibility by setting random seeds where applicable. Example Submission: ```python import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error import time # Generate synthetic dataset np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - np.random.rand(20)) # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Implement Kernel Ridge Regression with Grid Search krr = KernelRidge(kernel=\'rbf\') param_grid_krr = {\'alpha\': [0.1, 1, 10], \'gamma\': [0.01, 0.1, 1]} grid_search_krr = GridSearchCV(krr, param_grid_krr, cv=5) start_time = time.time() grid_search_krr.fit(X_train, y_train) krr_fitting_time = time.time() - start_time # Best parameters for KRR best_params_krr = grid_search_krr.best_params_ # Prediction with KRR start_time = time.time() y_pred_krr = grid_search_krr.predict(X_test) krr_prediction_time = time.time() - start_time # Mean Squared Error for KRR mse_krr = mean_squared_error(y_test, y_pred_krr) # Implement Support Vector Regression with Grid Search svr = SVR(kernel=\'rbf\') param_grid_svr = {\'C\': [0.1, 1, 10], \'epsilon\': [0.01, 0.1, 1], \'gamma\': [0.01, 0.1, 1]} grid_search_svr = GridSearchCV(svr, param_grid_svr, cv=5) start_time = time.time() grid_search_svr.fit(X_train, y_train) svr_fitting_time = time.time() - start_time # Best parameters for SVR best_params_svr = grid_search_svr.best_params_ # Prediction with SVR start_time = time.time() y_pred_svr = grid_search_svr.predict(X_test) svr_prediction_time = time.time() - start_time # Mean Squared Error for SVR mse_svr = mean_squared_error(y_test, y_pred_svr) # Output the results print(\\"Kernel Ridge Regression (KRR):\\") print(\\"Best Parameters:\\", best_params_krr) print(\\"Fitting Time: {:.4f} seconds\\".format(krr_fitting_time)) print(\\"Prediction Time: {:.4f} seconds\\".format(krr_prediction_time)) print(\\"Mean Squared Error: {:.4f}\\".format(mse_krr)) print(\\"nSupport Vector Regression (SVR):\\") print(\\"Best Parameters:\\", best_params_svr) print(\\"Fitting Time: {:.4f} seconds\\".format(svr_fitting_time)) print(\\"Prediction Time: {:.4f} seconds\\".format(svr_prediction_time)) print(\\"Mean Squared Error: {:.4f}\\".format(mse_svr)) ```","solution":"import numpy as np from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.metrics import mean_squared_error import time def generate_synthetic_data(): np.random.seed(0) X = np.sort(5 * np.random.rand(100, 1), axis=0) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - np.random.rand(20)) return X, y def split_data(X, y): return train_test_split(X, y, test_size=0.2, random_state=0) def krr_grid_search(X_train, y_train): krr = KernelRidge(kernel=\'rbf\') param_grid_krr = {\'alpha\': [0.1, 1, 10], \'gamma\': [0.01, 0.1, 1]} grid_search_krr = GridSearchCV(krr, param_grid_krr, cv=5) start_time = time.time() grid_search_krr.fit(X_train, y_train) fitting_time = time.time() - start_time return grid_search_krr, fitting_time def svr_grid_search(X_train, y_train): svr = SVR(kernel=\'rbf\') param_grid_svr = {\'C\': [0.1, 1, 10], \'epsilon\': [0.01, 0.1, 1], \'gamma\': [0.01, 0.1, 1]} grid_search_svr = GridSearchCV(svr, param_grid_svr, cv=5) start_time = time.time() grid_search_svr.fit(X_train, y_train) fitting_time = time.time() - start_time return grid_search_svr, fitting_time def evaluate_model(model, X_test, y_test): start_time = time.time() y_pred = model.predict(X_test) prediction_time = time.time() - start_time mse = mean_squared_error(y_test, y_pred) return prediction_time, mse # Generate synthetic dataset X, y = generate_synthetic_data() # Split data into training and testing sets X_train, X_test, y_train, y_test = split_data(X, y) # Kernel Ridge Regression (KRR) with Grid Search grid_search_krr, krr_fitting_time = krr_grid_search(X_train, y_train) best_params_krr = grid_search_krr.best_params_ krr_prediction_time, mse_krr = evaluate_model(grid_search_krr, X_test, y_test) # Support Vector Regression (SVR) with Grid Search grid_search_svr, svr_fitting_time = svr_grid_search(X_train, y_train) best_params_svr = grid_search_svr.best_params_ svr_prediction_time, mse_svr = evaluate_model(grid_search_svr, X_test, y_test) # Output the results output_results = { \\"Kernel Ridge Regression (KRR)\\": { \\"best_params\\": best_params_krr, \\"fitting_time\\": krr_fitting_time, \\"prediction_time\\": krr_prediction_time, \\"mse\\": mse_krr }, \\"Support Vector Regression (SVR)\\": { \\"best_params\\": best_params_svr, \\"fitting_time\\": svr_fitting_time, \\"prediction_time\\": svr_prediction_time, \\"mse\\": mse_svr } } def get_results(): return output_results"},{"question":"**Question: Implementing a Module Loader Using the Deprecated `imp` Module** You are tasked with implementing a custom module loader function that mimics some of the older behavior of Python\'s import system using the deprecated `imp` module. # Requirements: 1. **Function Name:** `custom_import` 2. **Input Arguments:** - `module_name` (str): The name of the module to import. - `module_path` (Optional[str]): The file system path to start the search, defaults to `None` which refers to the default module search paths, including built-ins. 3. **Output:** - The imported module object if the module is found and successfully imported. 4. **Constraints:** - If the module cannot be found, an `ImportError` should be raised. - Proper resource management (like closing any opened files) should be ensured. - Implementations should avoid the use of `importlib` and directly interface with `imp` functions where possible. # Example Usage: ```python module_obj = custom_import(\'example_module\', \'/path/to/modules\') print(module_obj) ``` # Important Notes: - You do not need to handle hierarchical module names (names containing dots), e.g., \'package.module\'. - Your implementation should deal gracefully with errors, ensuring that you do not leave any file handlers open. # Implementation: Here\'s a starting structure: ```python import imp def custom_import(module_name, module_path=None): Custom import function using deprecated imp module. try: # Finding the module fp, pathname, description = imp.find_module(module_name, [module_path] if module_path else None) try: # Loading the module module = imp.load_module(module_name, fp, pathname, description) return module finally: # Ensure file is properly closed after import if fp: fp.close() except ImportError: raise ImportError(f\\"Module {module_name} could not be imported.\\") ``` Implement the function according to the description and requirements specified above.","solution":"import imp import os def custom_import(module_name, module_path=None): Custom import function using deprecated imp module. try: # Finding the module filepath_search_path = [module_path] if module_path else None fp, pathname, description = imp.find_module(module_name, filepath_search_path) try: # Loading the module module = imp.load_module(module_name, fp, pathname, description) return module finally: # Ensure file is properly closed after import if fp: fp.close() except ImportError: raise ImportError(f\\"Module {module_name} could not be imported.\\")"},{"question":"**Coding Assessment Question** # Objective Implement a Python function that normalizes and validates an internet domain name string to ensure that it conforms to the required standards defined by the `stringprep` module. # Problem Statement You are required to implement a function `normalize_and_validate_domain(domain: str) -> str` that takes a domain name as input and performs the following tasks: 1. **Normalization:** Normalize the domain name using case-folding mappings (table B.2 and B.3). 2. **Validation:** Ensure that the domain name does not contain any characters from the following sets defined in the `stringprep` module: - Unassigned code points (table A.1) - ASCII and non-ASCII space characters (table C.1.1 and C.1.2) - ASCII and non-ASCII control characters (table C.2.1 and C.2.2) - Private use characters (table C.3) - Non-character code points (table C.4) - Surrogate codes (table C.5) - Characters inappropriate for plain text (table C.6) - Characters inappropriate for canonical representation (table C.7) - Characters that change display properties or are deprecated (table C.8) - Tagging characters (table C.9) 3. **Output:** Return the normalized domain name if it is valid. If it is not valid, raise an exception `ValueError` with an appropriate error message indicating why the domain is invalid. # Example ```python def normalize_and_validate_domain(domain: str) -> str: # Implementation here pass # Example usage try: print(normalize_and_validate_domain(\\"ExAmPlE.com\\")) # Expected Output: \'example.com\' except ValueError as e: print(e) ``` # Constraints - The input domain name will be a non-empty string containing only valid Unicode characters. - You may assume that the input domain does not contain internationalized domain name format (e.g., non-ASCII characters). # Notes 1. Consider using the functions provided by the `stringprep` module to check and map characters. 2. Be sure to clearly document any specific assumptions or decisions made in your function implementation.","solution":"import stringprep def normalize_and_validate_domain(domain: str) -> str: Normalizes the domain name using case-folding mappings and validates it to ensure it does not contain any characters from the disallowed sets defined by the stringprep module. Parameters: domain (str): The domain name to be normalized and validated. Returns: str: The normalized domain name if valid. Raises: ValueError: If the domain name contains disallowed characters. def check_disallowed_characters(domain): for char in domain: if (stringprep.in_table_a1(char) or stringprep.in_table_c11(char) or stringprep.in_table_c12(char) or stringprep.in_table_c21(char) or stringprep.in_table_c22(char) or stringprep.in_table_c3(char) or stringprep.in_table_c4(char) or stringprep.in_table_c5(char) or stringprep.in_table_c6(char) or stringprep.in_table_c7(char) or stringprep.in_table_c8(char) or stringprep.in_table_c9(char)): raise ValueError(f\\"Invalid domain name; contains disallowed character: {repr(char)}\\") # Normalize the domain normalized_domain = domain.casefold() # Validate the domain check_disallowed_characters(normalized_domain) return normalized_domain"},{"question":"Objective: To assess the student\'s understanding of object serialization in Python using the `marshal` module. Question: You are tasked with creating a Python application that needs to serialize and deserialize complex data structures using the `marshal` module. Your goal is to implement two functions: `serialize_data` and `deserialize_data`. 1. **Function: `serialize_data`** - **Input:** An object `data` that contains a complex data structure (nested lists and dictionaries which may contain supported types). - **Output:** A bytes object representing the serialized data. - **Constraints:** The input data must only contain supported types as per the `marshal` module documentation. 2. **Function: `deserialize_data`** - **Input:** A bytes object `data` that represents the serialized data. - **Output:** The original data structure that was serialized. - **Constraints:** The input bytes object should be deserialized into its original form, maintaining the nested structure of lists and dictionaries. Detailed Requirements: - You must handle any exceptions that may arise during serialization and deserialization. - If an unsupported type is present in the data for serialization, raise a `ValueError` with the message: \\"Unsupported data type for serialization\\". - Ensure that the deserialized data is functionally equivalent to the original data before serialization. Example: ```python def serialize_data(data): # Your implementation here pass def deserialize_data(data): # Your implementation here pass # Example usage: original_data = { \'numbers\': [1, 2, 3.5, 4.2], \'nested_dict\': { \'a\': True, \'b\': None }, \'words\': (\'hello\', \'world\'), \'numbers_set\': {5, 6, 7} } # Serialize the data serialized_data = serialize_data(original_data) # Deserialize the data deserialized_data = deserialize_data(serialized_data) # Check if the deserialized data is the same as the original data assert original_data == deserialized_data ``` Notes: - You need to test your function with various types of nested data structures to ensure correctness. - Use the `marshal` module for serialization and deserialization. Good luck!","solution":"import marshal def serialize_data(data): Serializes the given data using marshal. Parameters: data (object): The data to be serialized. Returns: bytes: The serialized data. try: return marshal.dumps(data) except (ValueError, TypeError) as e: raise ValueError(\\"Unsupported data type for serialization\\") from e def deserialize_data(data): Deserializes the given data using marshal. Parameters: data (bytes): The serialized data. Returns: object: The deserialized data. try: return marshal.loads(data) except Exception as e: raise ValueError(\\"Failed to deserialize data\\") from e"},{"question":"Objective: Demonstrate your understanding of `torch.futures.Future` for handling asynchronous operations in PyTorch by creating and managing multiple `Future` objects. Task: Write a Python function `asynchronous_computation` that accepts a list of numbers. For each number, it should create an asynchronous task that computes the square of the number after a delay of 1 second. Use `torch.futures.Future` to manage these tasks and ensure all computations complete before returning the results. Requirements: 1. **Input**: A list of integers `nums`. 2. **Output**: A list of integers where each number is the square of the corresponding input number. 3. Use `torch.futures.Future` to create asynchronous tasks. 4. Implement asynchronous behavior using appropriate utility functions, such as `collect_all` or `wait_all`. 5. Ensure all tasks complete execution before returning the results. Example: ```python import torch from torch.futures import Future, collect_all, wait_all import time def asynchronous_computation(nums): futures = [] # Function to simulate an asynchronous computation def compute_square(num): f = Future() time.sleep(1) # Simulating a delay f.set_result(num * num) return f # Create Future objects for each computation for num in nums: futures.append(torch.jit.fork(compute_square, num)) # Wait for all Future objects to complete results = wait_all(futures) # Retrieve the computed results return [result.value() for result in results] # Example usage: nums = [1, 2, 3, 4, 5] output = asynchronous_computation(nums) print(output) # Output: [1, 4, 9, 16, 25] ``` Notes: - You may assume that the input list `nums` is non-empty and contains valid integers. - Be cautious about time complexity, ensuring that you are not blocking the main thread unnecessarily. - Use the provided utility functions appropriately to manage multiple `Future` objects.","solution":"import torch from torch.futures import Future, collect_all, wait_all import time import asyncio def async_square(num): Asynchronous function to compute the square of a number after 1 second. async def compute_square(): await asyncio.sleep(1) # Simulating a 1-second delay return num * num return asyncio.ensure_future(compute_square()) def asynchronous_computation(nums): futures = [async_square(num) for num in nums] loop = asyncio.get_event_loop() results = loop.run_until_complete(asyncio.gather(*futures)) return results"},{"question":"# Question: Numerical Precision in Batched Computations In this question, you will demonstrate your understanding of numerical precision and batching in PyTorch by implementing a function that multiplies two 3D tensors using both batched and non-batched matrix multiplication. Additionally, you will analyze and compare their results to highlight the precision differences outlined in the documentation. Function Signature ```python import torch def compare_batched_vs_non_batched(A: torch.Tensor, B: torch.Tensor): Compare the differences between batched and non-batched matrix multiplications. Parameters: - A: A 3D tensor of shape (B, M, N) - B: A 3D tensor of shape (B, N, P) Returns: - A dictionary with two keys: - \'batched_result\': The result of the batched matrix multiplication. - \'non_batched_result\': The result of individually computed matrix multiplications. - \'difference\': The absolute difference between the two results. pass ``` Task Requirements 1. **Input**: Two 3D tensors `A` of shape `(B, M, N)` and `B` of shape `(B, N, P)`, where `B` is the batch size, `M` and `N` are the dimensions of the matrices within each batch. 2. **Batched Computation**: Compute the batched matrix multiplication using `torch.bmm(A, B)`. 3. **Non-batched Computation**: Compute each matrix multiplication in the batch individually using a loop and `torch.mm(A[i], B[i])`. 4. **Comparison**: Calculate the absolute difference between the results of the batched and non-batched computations. 5. **Output**: Return a dictionary including: - `\'batched_result\'`: The result of the batched matrix multiplication. - `\'non_batched_result\'`: The concatenated (stacked) result of the non-batched matrix multiplications. - `\'difference\'`: The element-wise absolute difference between the batched and non-batched results, highlighting any numerical discrepancies. Performance Constraints - Ensure that your implementation can handle reasonably large tensors, with batch sizes up to 100 and matrix dimensions up to 100x100. Example Usage ```python import torch A = torch.randn(5, 4, 6) B = torch.randn(5, 6, 3) result = compare_batched_vs_non_batched(A, B) print(result[\'batched_result\']) print(result[\'non_batched_result\']) print(result[\'difference\']) ``` The expected output should show the batched results, the stacked non-batched results, and their differences. Additional Notes - Carefully handle floating-point precision issues, noting that `torch.bmm` and individual `torch.mm` might not produce exactly the same results. - Consider running the computations on both CPU and GPU if available to observe any further differences. By completing this problem, you will better understand the practical implications of numerical precision in PyTorch operations, especially when dealing with batched computations.","solution":"import torch def compare_batched_vs_non_batched(A: torch.Tensor, B: torch.Tensor): Compare the differences between batched and non-batched matrix multiplications. Parameters: - A: A 3D tensor of shape (B, M, N) - B: A 3D tensor of shape (B, N, P) Returns: - A dictionary with two keys: - \'batched_result\': The result of the batched matrix multiplication. - \'non_batched_result\': The result of individually computed matrix multiplications. - \'difference\': The absolute difference between the two results. # Perform batched matrix multiplication batched_result = torch.bmm(A, B) # Perform non-batched matrix multiplication non_batched_result = torch.stack([torch.mm(A[i], B[i]) for i in range(A.shape[0])]) # Compute the absolute difference between batched and non-batched results difference = torch.abs(batched_result - non_batched_result) # Return the results in a dictionary return { \'batched_result\': batched_result, \'non_batched_result\': non_batched_result, \'difference\': difference }"},{"question":"# Objective The aim of this assessment is to evaluate your ability to use the `scikit-learn` library to fetch real-world datasets and perform basic machine learning operations on them. # Problem Statement You are required to write a Python function that fetches the California Housing dataset, performs basic data preprocessing, trains a linear regression model on the data, and evaluates the model\'s performance. # Function Signature ```python def train_and_evaluate_california_housing(): Fetches the California Housing dataset, preprocesses it, trains a linear regression model, and evaluates its performance. Returns: float: The root mean squared error (RMSE) of the model on the test set. pass ``` # Input The function does not take any input parameters. # Output The function should return a single floating-point number — the root mean squared error (RMSE) of the fitted linear regression model on the test set. # Requirements 1. **Fetch the Dataset**: - Use `sklearn.datasets.fetch_california_housing` to load the dataset. 2. **Preprocess the Data**: - Split the data into training and testing sets using an 80%-20% split. - Normalize the feature values (i.e., apply standard scaling). 3. **Train the Model**: - Train a linear regression model using the training data. 4. **Evaluate the Model**: - Calculate the RMSE on the test set. # Constraints 1. You should not use any other machine learning library apart from `scikit-learn`. 2. The solution should perform preprocessing and training efficiently. # Example Here\'s an example of how the function might be used: ```python rmse = train_and_evaluate_california_housing() print(f\\"Root Mean Squared Error: {rmse}\\") ``` **Note**: Your implementation should handle any potential issues that may arise during dataset fetching, preprocessing, or model training.","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error import numpy as np def train_and_evaluate_california_housing(): Fetches the California Housing dataset, preprocesses it, trains a linear regression model, and evaluates its performance. Returns: float: The root mean squared error (RMSE) of the model on the test set. # Fetch the dataset dataset = fetch_california_housing() X, y = dataset.data, dataset.target # Split the data into training and testing sets (80%-20%) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Normalize the feature values (standard scaling) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train a linear regression model model = LinearRegression() model.fit(X_train_scaled, y_train) # Predict on the test set y_pred = model.predict(X_test_scaled) # Calculate the RMSE rmse = np.sqrt(mean_squared_error(y_test, y_pred)) return rmse"},{"question":"# Custom Buffered I/O Stream Objective: Design and implement a custom buffered I/O stream class that simulates network communication with rate limiting. Instructions: 1. **Class Design**: Create a class `RateLimitedBufferedIO` that: - Inherits from `io.BufferedIOBase`. - Accepts a `buffer_size` and `rate_limit` in its constructor. - Manages an internal buffer for read and write operations. - Implements appropriate methods to handle reading and writing to this buffer, ensuring the rate limit is not exceeded. 2. **Constructor**: ```python def __init__(self, buffer_size: int = io.DEFAULT_BUFFER_SIZE, rate_limit: int = 1000): Initialize the RateLimitedBufferedIO stream. :param buffer_size: Maximum buffer size in bytes. :param rate_limit: Maximum number of bytes that can be processed per second. ``` 3. **Methods to Implement**: - `read(self, size: int = -1) -> bytes`: Simulates reading data from the buffer. - `write(self, b: bytes) -> int`: Simulates writing data to the buffer. - `flush(self)`: Flushes the buffer. - `seekable(self) -> bool`: Returns False, indicating sequential access. - `writable(self) -> bool`: Returns True, allowing write operations. - `readable(self) -> bool`: Returns True, allowing read operations. - `write_limited(self, b: bytes) -> int`: Helper method to handle rate-limited writing. 4. **Constraints**: - Ensure the writing process obeys the `rate_limit`, simulating network throttling. - Handle potential race conditions or locking issues effectively. Example Usage: ```python import io import time class RateLimitedBufferedIO(io.BufferedIOBase): def __init__(self, buffer_size=io.DEFAULT_BUFFER_SIZE, rate_limit=1000): super().__init__() self._buffer = bytearray() self.buffer_size = buffer_size self.rate_limit = rate_limit self._last_written = time.time() def write(self, b: bytes) -> int: written = 0 for byte in b: self.write_limited(byte) written += 1 return written def write_limited(self, byte: bytes): while (len(self._buffer) >= self.rate_limit) and (time.time() - self._last_written < 1): time.sleep(0.01) self._buffer.append(byte) if len(self._buffer) >= self.buffer_size: self.flush() self._last_written = time.time() def read(self, size: int = -1) -> bytes: if size == -1: size = len(self._buffer) data = self._buffer[:size] del self._buffer[:size] return bytes(data) def flush(self): print(\'Flushing:\', self._buffer) self._buffer.clear() def seekable(self) -> bool: return False def writable(self) -> bool: return True def readable(self) -> bool: return True # Usage example: rate_limited_io = RateLimitedBufferedIO(buffer_size=5, rate_limit=3) rate_limited_io.write(b\'hello\') rate_limited_io.flush() print(rate_limited_io.read()) ``` Constraints: - Your solution should be efficient, ensuring no more than `rate_limit` bytes are processed each second. - Ensure exception handling is done correctly in case of invalid operations. - Use the provided base class methods and properties where appropriate. # Submission: Submit your `RateLimitedBufferedIO` class implementation and a brief explanation of how your rate limiting mechanism works.","solution":"import io import time class RateLimitedBufferedIO(io.BufferedIOBase): def __init__(self, buffer_size=io.DEFAULT_BUFFER_SIZE, rate_limit=1000): super().__init__() self._buffer = bytearray() self.buffer_size = buffer_size self.rate_limit = rate_limit self._last_written_time = time.time() def write(self, b: bytes) -> int: written = 0 for byte in b: self.write_limited(byte) written += 1 return written def write_limited(self, byte: bytes): current_time = time.time() elapsed_time = current_time - self._last_written_time allowed_bytes = self.rate_limit * elapsed_time while len(self._buffer) >= self.rate_limit and elapsed_time < 1: time.sleep(0.01) current_time = time.time() elapsed_time = current_time - self._last_written_time allowed_bytes = self.rate_limit * elapsed_time self._buffer.append(byte) if len(self._buffer) >= self.buffer_size: self.flush() self._last_written_time = time.time() def read(self, size: int = -1) -> bytes: if size == -1: size = len(self._buffer) data = self._buffer[:size] del self._buffer[:size] return bytes(data) def flush(self) -> None: self._buffer = bytearray() def seekable(self) -> bool: return False def writable(self) -> bool: return True def readable(self) -> bool: return True"},{"question":"# Weak Reference Caching System You are tasked with designing a simple caching system that uses weak references to avoid keeping cache entries alive beyond their intended lifetime. You will implement a class `WeakCache` that allows storing key-value pairs where values are held through weak references. # Class and Method Details: - **Class**: `WeakCache` - **Methods**: 1. **`__init__()`**: Initialize an empty `WeakCache`. 2. **`set_item(key, value)`**: Store the given key-value pair in the cache. If `value` is already present in the cache, update its entry with the new key. 3. **`get_item(key)`**: Retrieve the value corresponding to the given key from the cache. If the key does not exist or the value has been garbage collected, return `None`. 4. **`has_item(key)`**: Check whether a given key exists in the cache and its value is still alive. 5. **`remove_item(key)`**: Remove the entry associated with the given key from the cache. 6. **`__len__()`**: Return the number of items currently stored in the cache (only the items whose values are still alive). # Input and Output Formats: - **Input**: The methods of the `WeakCache` class will accept standard Python data types (strings, integers, etc.) for keys and arbitrary objects for values. - **Output**: Various methods will return integers, objects, or `None` depending on their function. # Constraints: 1. Keys will be hashable and comparable (e.g., strings, integers). 2. Values can be any object that supports weak references. 3. The cache should efficiently manage memory by removing entries when their values have been garbage collected. # Example: ```python import weakref class WeakCache: def __init__(self): self._data = weakref.WeakValueDictionary() def set_item(self, key, value): Set item in cache self._data[key] = value def get_item(self, key): Get item from cache return self._data.get(key) def has_item(self, key): Check if an item with the given key exists in the cache return key in self._data def remove_item(self, key): Remove item with specified key from cache if key in self._data: del self._data[key] def __len__(self): Return the number of items in cache return len(self._data) # Example usage cache = WeakCache() cache.set_item(\'a\', [1, 2, 3]) print(cache.get_item(\'a\')) # Output: [1, 2, 3] print(cache.has_item(\'a\')) # Output: True del cache._data[\'a\'] print(cache.get_item(\'a\')) # Output: None print(len(cache)) # Output: 0 ``` # Tasks: 1. Implement the `WeakCache` class by following the above method details and constraints. 2. Ensure proper memory management using weak references. 3. Test the `WeakCache` class with various keys and values to ensure it operates as expected.","solution":"import weakref class WeakCache: def __init__(self): self._data = weakref.WeakValueDictionary() def set_item(self, key, value): Set item in cache self._data[key] = value def get_item(self, key): Get item from cache return self._data.get(key) def has_item(self, key): Check if an item with the given key exists in the cache return key in self._data def remove_item(self, key): Remove item with specified key from cache if key in self._data: del self._data[key] def __len__(self): Return the number of items in cache return len(self._data)"},{"question":"# Sparse Data Handling in Pandas You are provided with sales data for a company, but a significant number of entries are missing or zero, making it sparse. Your task is to analyze storage sizes and perform specific operations using pandas\' sparse data structures. 1. **Generate Random Sparse Data:** - Create a DataFrame of shape (1000, 5) with random float values. - Randomly set approximately 90% of the values to NaN or zero (either or both). 2. **Convert to Sparse DataFrame:** - Convert this dense DataFrame to a sparse DataFrame. - Print the memory usage of both the dense and sparse DataFrame. 3. **Perform Operations on Sparse DataFrame:** - Calculate and print the density (i.e., the fraction of non-missing/non-zero values). 4. **Use NumPy ufuncs on Sparse Columns:** - Apply a NumPy ufunc, such as `np.square`, to a sparse column in your DataFrame. - Convert the resulting SparseArray back to a dense array and print it. 5. **Conversion with SciPy Sparse Matrices (Advanced):** - Convert the DataFrame to a scipy sparse matrix (COO format). - Convert it back to a pandas DataFrame and print the first 5 rows. # Requirements: - Function 1: ```python def create_sparse_dataframe() -> pd.DataFrame: Create and return a DataFrame of shape (1000, 5) with random float values, with approximately 90% of the values set to NaN or zero. Returns: -------- pd.DataFrame ``` - Function 2: ```python def convert_to_sparse(df: pd.DataFrame) -> pd.DataFrame: Convert the given dense DataFrame to a sparse DataFrame and print its memory usage compared to the dense DataFrame. Parameters: ----------- df : pd.DataFrame The dense DataFrame to convert. Returns: -------- pd.DataFrame ``` - Function 3: ```python def calculate_density(sdf: pd.DataFrame) -> float: Calculate and return the density of the sparse DataFrame. Parameters: ----------- sdf : pd.DataFrame The sparse DataFrame. Returns: -------- float ``` - Function 4: ```python def apply_ufunc_on_sparse_column(sdf: pd.DataFrame, column: str) -> np.ndarray: Apply a NumPy ufunc to a sparse column of the DataFrame and return the resulting dense array. Parameters: ----------- sdf : pd.DataFrame The sparse DataFrame. column : str The column name to which the ufunc will be applied. Returns: -------- np.ndarray ``` - Function 5: ```python def convert_to_scipy_sparse_and_back(df: pd.DataFrame) -> pd.DataFrame: Convert DataFrame to a scipy sparse matrix (COO format) and then back to a pandas DataFrame. Print the first 5 rows of the final DataFrame. Parameters: ----------- df : pd.DataFrame The original dense DataFrame. Returns: -------- pd.DataFrame ``` # Example Usage: ```python dense_df = create_sparse_dataframe() sparse_df = convert_to_sparse(dense_df) density = calculate_density(sparse_df) print(\\"Density:\\", density) resulting_array = apply_ufunc_on_sparse_column(sparse_df, \'column_name\') print(\\"Resulting dense array:\\", resulting_array) final_df = convert_to_scipy_sparse_and_back(dense_df) print(final_df.head()) ``` # Constraints: - Random seed for consistent results: `np.random.seed(42)`. Good luck!","solution":"import pandas as pd import numpy as np from scipy import sparse np.random.seed(42) def create_sparse_dataframe() -> pd.DataFrame: Create and return a DataFrame of shape (1000, 5) with random float values, with approximately 90% of the values set to NaN or zero. data = np.random.randn(1000, 5) mask = np.random.choice([True, False], size=data.shape, p=[0.1, 0.9]) data[~mask] = np.nan df = pd.DataFrame(data) return df def convert_to_sparse(df: pd.DataFrame) -> pd.DataFrame: Convert the given dense DataFrame to a sparse DataFrame and print its memory usage compared to the dense DataFrame. dense_memory = df.memory_usage(deep=True).sum() sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) sparse_memory = sdf.memory_usage(deep=True).sum() print(f\\"Dense DataFrame memory usage: {dense_memory} bytes\\") print(f\\"Sparse DataFrame memory usage: {sparse_memory} bytes\\") return sdf def calculate_density(sdf: pd.DataFrame) -> float: Calculate and return the density of the sparse DataFrame. non_missing_count = sdf.sparse.density * sdf.size total_elements = sdf.size density = non_missing_count / total_elements return density def apply_ufunc_on_sparse_column(sdf: pd.DataFrame, column: str) -> np.ndarray: Apply a NumPy ufunc to a sparse column of the DataFrame and return the resulting dense array. result = np.square(sdf[column].sparse.to_dense()) return result.values def convert_to_scipy_sparse_and_back(df: pd.DataFrame) -> pd.DataFrame: Convert DataFrame to a scipy sparse matrix (COO format) and then back to a pandas DataFrame. Print the first 5 rows of the final DataFrame. coo_matrix = sparse.coo_matrix(df.fillna(0)) dense_data = coo_matrix.toarray() new_df = pd.DataFrame(dense_data, columns=df.columns) return new_df"},{"question":"# PyTorch Coding Assessment: Tensor Views and Contiguity Problem Statement Implement a Python function `tensor_view_operations` that performs a series of operations on a given input tensor using PyTorch view operations. The function should: 1. Create a base tensor of the specified shape with random values. 2. Return a view of this tensor with a different shape. 3. Transpose the tensor and verify the contiguity of the result. 4. Convert a non-contiguous tensor to a contiguous tensor. 5. Confirm that modifications to the view affect the base tensor. Function Signature ```python def tensor_view_operations(shape: tuple) -> dict: Args: - shape (tuple): The shape of the base tensor to create. The shape should be such that it can be reshaped into another valid shape for view operations. Returns: - dict: A dictionary containing the following keys and values: - \\"base_tensor\\": The created base tensor. - \\"reshaped_view\\": The tensor reshaped using `view` to a shape compatible with the original tensor size. - \\"is_reshaped_view_contiguous\\": Boolean indicating if the reshaped view is contiguous. - \\"transposed_view\\": The tensor transposed using `transpose`. - \\"is_transposed_view_contiguous\\": Boolean indicating if the transposed tensor is contiguous. - \\"contiguous_tensor\\": The tensor made contiguous using `.contiguous()` if the transposed tensor is not contiguous. - \\"modified_base_tensor\\": The base tensor after modifying an element in the reshaped view to demonstrate data sharing. pass ``` Constraints - The input tensor shape should have at least two dimensions. - You must use PyTorch functions to perform the operations. - Ensure the reshaped view can be obtained by reshaping the base tensor without data loss. Performance Requirements - Efficient handling of tensor operations with minimal data copying. - Proper management of tensor contiguity to avoid performance pitfalls. Example ```python shape = (4, 4) result = tensor_view_operations(shape) # The result should contain: # - The base tensor with random values of shape (4, 4). # - A reshaped view, say (2, 8), that shares data with the base tensor. # - A boolean indicating if the reshaped view is contiguous. # - A transposed version of the base tensor, say (4, 4) -> (4, 4) using transpose. # - A boolean indicating if the transposed tensor is contiguous. # - A contiguous version of the tensor if the transposed tensor was not contiguous. # - The modified base tensor after changing an element in the reshaped view. ``` Implement the function by considering the points mentioned in the documentation about Tensor Views, contiguity, and memory efficiency.","solution":"import torch def tensor_view_operations(shape: tuple) -> dict: Perform a series of operations on a given input tensor using PyTorch view operations. Args: - shape (tuple): The shape of the base tensor to create. The shape should be such that it can be reshaped into another valid shape for view operations. Returns: - dict: A dictionary containing the results of the operations. # Step 1: Create a base tensor with random values base_tensor = torch.rand(shape) # Step 2: Reshape the tensor (for simplicity, we\'ll flatten it) reshaped_view = base_tensor.view(-1) # Step 3 & 4: Transpose the tensor and check contiguity transposed_view = base_tensor.t() is_transposed_view_contiguous = transposed_view.is_contiguous() # Convert a non-contiguous tensor to a contiguous tensor (if necessary) contiguous_tensor = transposed_view.contiguous() # Step 5: Confirm that modifications to the view affect the base tensor reshaped_view[0] = -1 # Modify the first element in the reshaped view return { \\"base_tensor\\": base_tensor, \\"reshaped_view\\": reshaped_view, \\"is_reshaped_view_contiguous\\": reshaped_view.is_contiguous(), \\"transposed_view\\": transposed_view, \\"is_transposed_view_contiguous\\": is_transposed_view_contiguous, \\"contiguous_tensor\\": contiguous_tensor, \\"modified_base_tensor\\": base_tensor # Should show the modification }"},{"question":"Objective: Demonstrate your understanding of the `asyncore` module by implementing an asynchronous file server that can handle multiple clients simultaneously without blocking. Requirements: 1. Implement a class `FileServer` that inherits from `asyncore.dispatcher`. It should: - Bind to a specified host and port. - Listen for incoming client connections. - Handle the acceptance of client connections. - Use a handler class to manage client interactions, including file read and write operations. 2. Implement a class `FileHandler` that inherits from `asyncore.dispatcher_with_send`. It should: - Read file requests from the client. - Send the requested file\'s content back to the client. - Ensure non-blocking I/O operations. 3. Your server should be able to handle requests for text files located in the server\'s directory, and return the file contents to the client. 4. The server should properly handle errors, such as file not found, and close connections gracefully. Input: - Host (string): The hostname or IP address to bind the server. - Port (int): The port number to bind the server. - Clients will connect to the server and send filenames they wish to read. Output: - The server should output logs of client connections and requested filenames. - Clients should receive the content of the requested file or an error message (if the file is not found). Constraints: - Only text files are supported. - The filenames requested by clients are assumed to be well-formed. Example: ```python # Example server execution server = FileServer(\'localhost\', 9000) asyncore.loop() # Example client interaction # Client connects and sends the filename \\"example.txt\\" # The server responds with the contents of \\"example.txt\\" or an error message if the file does not exist. ``` # Implementation: ```python import asyncore import os class FileHandler(asyncore.dispatcher_with_send): def __init__(self, sock): super().__init__(sock) self.data_to_write = \\"\\" def handle_read(self): filename = self.recv(1024).decode(\'utf-8\').strip() print(f\\"Received request for file: {filename}\\") if os.path.exists(filename) and os.path.isfile(filename): with open(filename, \'r\') as file: self.data_to_write = file.read() else: self.data_to_write = \\"ERROR: File not found.\\" self.handle_write() def handle_write(self): if self.data_to_write: sent = self.send(self.data_to_write.encode(\'utf-8\')) self.data_to_write = self.data_to_write[sent:] def handle_close(self): self.close() class FileServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket() self.set_reuse_addr() self.bind((host, port)) self.listen(5) print(f\\"Server started at {host}:{port}\\") def handle_accepted(self, sock, addr): print(f\\"Incoming connection from {addr}\\") handler = FileHandler(sock) # Running the server if __name__ == \'__main__\': server = FileServer(\'localhost\', 9000) asyncore.loop() ``` This exercise assesses the student\'s ability to work with asynchronous I/O, extend classes, handle client-server interactions, and manage file operations efficiently.","solution":"import asyncore import os class FileHandler(asyncore.dispatcher_with_send): def __init__(self, sock): super().__init__(sock) self.data_to_write = \\"\\" def handle_read(self): filename = self.recv(1024).decode(\'utf-8\').strip() print(f\\"Received request for file: {filename}\\") if os.path.exists(filename) and os.path.isfile(filename): with open(filename, \'r\') as file: self.data_to_write = file.read() else: self.data_to_write = \\"ERROR: File not found.\\" self.handle_write() def handle_write(self): if self.data_to_write: sent = self.send(self.data_to_write.encode(\'utf-8\')) self.data_to_write = self.data_to_write[sent:] def handle_close(self): self.close() class FileServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket() self.set_reuse_addr() self.bind((host, port)) self.listen(5) print(f\\"Server started at {host}:{port}\\") def handle_accepted(self, sock, addr): print(f\\"Incoming connection from {addr}\\") handler = FileHandler(sock) if __name__ == \'__main__\': server = FileServer(\'localhost\', 9000) asyncore.loop()"},{"question":"Objective You are required to implement a PyTorch function that computes the modified logistic regression model using some special functions provided in the `torch.special` module. The function should take the model parameters and data as input and return the modified logistic regression model\'s predictions. Function Signature ```python def modified_logistic_regression(X: torch.Tensor, weights: torch.Tensor, bias: torch.Tensor) -> torch.Tensor: Compute the modified logistic regression predictions for a given input tensor, weight tensor, and bias tensor. Parameters: X (torch.Tensor): A 2D tensor of shape (n_samples, n_features) representing input data. weights (torch.Tensor): A 1D tensor of shape (n_features,) representing the model parameters. bias (torch.Tensor): A 1D tensor of shape (1,) representing the bias term. Returns: torch.Tensor: A 1D tensor of shape (n_samples,) representing the predicted probabilities for each sample. pass ``` Input * `X`: A 2D PyTorch tensor of shape (n_samples, n_features) where `n_samples` is the number of data samples and `n_features` is the number of features. * `weights`: A 1D PyTorch tensor of shape (n_features) representing the model weights. * `bias`: A 1D PyTorch tensor of shape (1) representing the bias term. Output * A 1D PyTorch tensor of shape (n_samples,) representing the predicted probabilities for each sample. Constraints and Requirements - You must use the `torch.special.expit` function within your implementation. - You must implement a custom non-linear transformation on the linear combination of inputs and weights that involves the use of `torch.special.erf` function. - Assume the predictions require a non-standard logistic function where the linear combination of features and weights undergoes a non-linear transformation followed by the logistic function. Example ```python import torch from torch.special import expit, erf def modified_logistic_regression(X: torch.Tensor, weights: torch.Tensor, bias: torch.Tensor) -> torch.Tensor: # Compute the linear combination linear_combination = X @ weights + bias # Apply a non-linear transformation using erf non_linear_transformation = erf(linear_combination) # Compute the logistic function using the expit function predictions = expit(non_linear_transformation) return predictions # Example usage X = torch.tensor([[0.5, 1.0], [1.5, 2.0], [3.0, 4.0]]) weights = torch.tensor([0.2, 0.8]) bias = torch.tensor([0.1]) predictions = modified_logistic_regression(X, weights, bias) print(predictions) ``` Given the input tensors: ```python X = torch.tensor([[0.5, 1.0], [1.5, 2.0], [3.0, 4.0]]) weights = torch.tensor([0.2, 0.8]) bias = torch.tensor([0.1]) ``` The function should output a tensor of predictions. Testing Ensure to test your function with different configurations of `X`, `weights`, and `bias` to validate its correctness.","solution":"import torch from torch.special import expit, erf def modified_logistic_regression(X: torch.Tensor, weights: torch.Tensor, bias: torch.Tensor) -> torch.Tensor: Compute the modified logistic regression predictions for a given input tensor, weight tensor, and bias tensor. Parameters: X (torch.Tensor): A 2D tensor of shape (n_samples, n_features) representing input data. weights (torch.Tensor): A 1D tensor of shape (n_features,) representing the model parameters. bias (torch.Tensor): A 1D tensor of shape (1,) representing the bias term. Returns: torch.Tensor: A 1D tensor of shape (n_samples,) representing the predicted probabilities for each sample. # Compute the linear combination linear_combination = X @ weights + bias # Apply a non-linear transformation using erf non_linear_transformation = erf(linear_combination) # Compute the logistic function using the expit function predictions = expit(non_linear_transformation) return predictions"},{"question":"**Question:** You are provided with a dataset about penguins consisting of various features like species, body mass, flipper length, and sex. Using the Seaborn library\'s object-oriented API, complete the following tasks to demonstrate your proficiency with Seaborn. **Dataset:** ```python import seaborn.objects as so from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` **Tasks:** 1. Create a plot using the `seaborn.objects.Plot` class that visualizes the `body_mass_g` of penguins across different `species`. Use the `Dash` mark to represent each data point. Configure the dashes to: - Color code by the `sex` attribute. - Set the `alpha` property to 0.5. - Map the `flipper_length_mm` attribute to the `linewidth` property. - Set the `width` property of dashes to 0.5. 2. Additionally, enhance the plot by adding aggregate dashes and individual dots: - Use dodging to separate the datapoints by `sex`. - Pair the `Dash` mark with aggregate values. - Add dots (`Dots` mark) to represent individual data points with `Jitter` for better visualization. **Constraints:** - Ensure to clearly label the axes and provide a legend for clarity. - Use appropriate Seaborn object methods and chaining to create the plot. - Your solution should only use Seaborn\'s object-oriented API (`seaborn.objects`). **Example Output Format:** Your final plot should resemble the following configuration: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset penguins = load_dataset(\\"penguins\\") # Create plot p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") p.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\", width=0.5) p.add(so.Dash(), so.Agg(), so.Dodge()) p.add(so.Dots(), so.Dodge(), so.Jitter()) # Show plot p.show() ``` Ensure your code is correctly formatted and runs without errors. Comment sections of your code if necessary to explain your approach and decisions.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load dataset penguins = load_dataset(\\"penguins\\") # Create the plot using the Seaborn object API p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Add dashes for each data point p.add(so.Dash(alpha=0.5, linewidth=\\"flipper_length_mm\\", width=0.5)) # Add aggregate dashes with dodging by sex p.add(so.Dash(), so.Agg(), so.Dodge()) # Add individual dots with jittering for better visualization p.add(so.Dots(), so.Dodge(), so.Jitter()) # Show plot p.show()"},{"question":"**Question**: You are tasked with implementing a custom iterator that generates a Cartesian product of multiple input sequences, but with additional flexibility. Specifically, you need to include a step size that controls the sampling interval on each sequence. # Function Signature ```python def custom_product(*iterables, step=1): pass ``` # Input - `*iterables`: Two or more iterables whose Cartesian product is to be computed. - `step` (int, optional): A positive integer specifying the sampling step size from each iterable. Defaults to 1, meaning every element is considered. # Output - Returns an iterator generating tuples representing the Cartesian product of the input sequences at the specified step size. # Constraints 1. All input iterables are guaranteed to be finite. 2. The `step` parameter will always be a positive integer. 3. Consider efficiency in terms of both time and space complexity. # Example ```python # Custom product with default step size of 1 assert list(custom_product(\'AB\', \'12\')) == [(\'A\', \'1\'), (\'A\', \'2\'), (\'B\', \'1\'), (\'B\', \'2\')] # Custom product with step size of 2 assert list(custom_product(\'ABCDEFG\', \'1234567\', step=2)) == [(\'A\', \'1\'), (\'A\', \'3\'), (\'A\', \'5\'), (\'A\', \'7\'), (\'C\', \'1\'), (\'C\', \'3\'), (\'C\', \'5\'), (\'C\', \'7\'), (\'E\', \'1\'), (\'E\', \'3\'), (\'E\', \'5\'), (\'E\', \'7\'), (\'G\', \'1\'), (\'G\', \'3\'), (\'G\', \'5\'), (\'G\', \'7\')] ``` # Hints - Consider using generator expressions or itertools functions like `islice` to efficiently handle the step size logic. - Aim to minimize memory usage by generating results on demand instead of precomputing the entire product. Implement the `custom_product` function to fulfill the above specifications.","solution":"import itertools def custom_product(*iterables, step=1): Generates the Cartesian product of input iterables with a specified step size. Parameters: *iterables: Two or more iterables to compute the Cartesian product. step (int): The step size specifying the sampling interval from each iterable. Returns: An iterator generating tuples representing the Cartesian product of the input sequences at the specified step size. if step < 1: raise ValueError(\\"Step size must be a positive integer.\\") # Apply step size to each iterable stepped_iterables = (itertools.islice(iterable, 0, None, step) for iterable in iterables) # Compute and return the Cartesian product of the stepped iterables return itertools.product(*stepped_iterables)"},{"question":"# Question: Implementing and Validating Logging Configuration using `logging.config.dictConfig` Your task is to write a function `setup_logging(config: dict) -> None` that sets up logging based on the provided dictionary configuration. The function should also validate the configuration dictionary before applying it to ensure that it adheres to the schema defined by `logging.config.dictConfig`. The function should: 1. Validate the configuration dictionary according to the `dictConfig` schema. 2. Handle any exceptions that arise during the configuration process and log an appropriate error message. 3. Apply the logging configuration using `logging.config.dictConfig`. Additionally, write a Python class `CustomFormatter` that extends the `logging.Formatter` class. This custom formatter should allow for a `prefix` attribute that is prepended to every log message. Function signature: ```python import logging import logging.config class CustomFormatter(logging.Formatter): def __init__(self, format: str = None, datefmt: str = None, style: str = \'%\', prefix: str = \'\'): super().__init__(format, datefmt, style) self.prefix = prefix def format(self, record: logging.LogRecord) -> str: original_message = super().format(record) return f\'{self.prefix} {original_message}\' def setup_logging(config: dict) -> None: pass ``` # Example Example Input: ```python config_dict = { \'version\': 1, \'formatters\': { \'custom\': { \'()\': \'CustomFormatter\', \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', \'prefix\': \'CUSTOM_LOG\' }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'custom\', \'stream\': \'ext://sys.stdout\' }, }, \'loggers\': { \'my_logger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\'], \'propagate\': False }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\'] } } setup_logging(config_dict) logger = logging.getLogger(\'my_logger\') logger.info(\\"This is a test message.\\") ``` Example Output: ``` CUSTOM_LOG 2023-10-01 12:00:00 - my_logger - INFO - This is a test message. ``` # Notes: 1. Ensure that exceptions such as `ValueError`, `TypeError`, `AttributeError`, or `ImportError` during the configuration process are properly logged. 2. Use the provided `CustomFormatter` in the configuration schema. 3. Make sure to use the correct schema keys in the dictionary. 4. The `prefix` value is a custom attribute that should be correctly incorporated into the log messages.","solution":"import logging import logging.config class CustomFormatter(logging.Formatter): def __init__(self, format: str = None, datefmt: str = None, style: str = \'%\', prefix: str = \'\'): super().__init__(format, datefmt, style) self.prefix = prefix def format(self, record: logging.LogRecord) -> str: original_message = super().format(record) return f\'{self.prefix} {original_message}\' def setup_logging(config: dict) -> None: try: logging.config.dictConfig(config) except (ValueError, TypeError, AttributeError, ImportError) as e: logging.error(f\\"Failed to configure logging: {e}\\")"},{"question":"You are provided with a dataset and your task is to preprocess the data, train a supervised machine learning model, and evaluate its performance using cross-validation techniques as discussed in the provided documentation. Dataset - You can use the [iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) from sklearn, which contains 150 samples of iris flowers, each described by 4 features and classified into one of 3 target classes. Requirements 1. **Data Loading and Preprocessing** - Load the iris dataset. - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 2. **Model Training and Cross-Validation** - Train a Support Vector Machine (SVM) classifier with a linear kernel. - Use k-fold cross-validation to evaluate the model\'s accuracy. Choose an appropriate number of folds, justify your choice, and compute the mean accuracy and standard deviation. 3. **Advanced Cross-Validation** - Implement a stratified k-fold cross-validation and compare its results with the plain k-fold cross-validation. - Report and discuss the results of both methods, focusing on which method you believe provides a better evaluation for this dataset and why. 4. **Custom Cross-Validation Strategy** - Define and implement a custom cross-validation strategy where you create custom (train, test) splits. - Using this custom strategy, evaluate the SVM classifier and compare the results with the previous cross-validation methods. 5. **Multiple Metrics Evaluation** - Use the `cross_validate` function to evaluate the model using at least two different scoring metrics (e.g., precision and recall). Provide a summary of the results. Input - No input parameters are required as it\'s a coding exercise. Output - The mean accuracy and standard deviation from k-fold cross-validation. - The mean accuracy and standard deviation from stratified k-fold cross-validation. - Results of the custom cross-validation strategy. - Summary of multiple metrics evaluation. Constraints - The number of splits in k-fold and stratified k-fold cross-validation should be between 3 and 10. - Ensure to handle the random state for reproducible results where applicable. - You must use Scikit-Learn for data processing and evaluation tasks to adhere to the library\'s standard practices. Example Output ```python # Mean accuracy and standard deviation for k-fold: k_fold_results = (accuracy_mean, accuracy_std) # Mean accuracy and standard deviation for stratified k-fold: stratified_k_fold_results = (accuracy_mean_stratified, accuracy_std_stratified) # Custom cross-validation strategy results: custom_cv_results = (custom_accuracy_mean, custom_accuracy_std) # Multiple metrics evaluation summary: multiple_metrics_results = { \\"precision_mean\\": precision_mean, \\"recall_mean\\": recall_mean, \\"precision_std\\": precision_std, \\"recall_std\\": recall_std } ``` Notes - All steps including loading the dataset, preprocessing, training, evaluating, and summarizing must be included in a single function/script. - Justify your choices of the number of folds and scoring metrics within the script as comments.","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_validate def preprocess_and_evaluate(): # Load the iris dataset iris = load_iris() X, y = iris.data, iris.target # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Train a Support Vector Machine classifier with a linear kernel svc = SVC(kernel=\'linear\', random_state=42) # Apply k-fold cross-validation (default value of k; k=5 gives a good balance) k_fold_scores = cross_val_score(svc, X_scaled, y, cv=5) k_fold_results = (k_fold_scores.mean(), k_fold_scores.std()) # Apply stratified k-fold cross-validation stratified_k_fold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) stratified_k_fold_scores = cross_val_score(svc, X_scaled, y, cv=stratified_k_fold) stratified_k_fold_results = (stratified_k_fold_scores.mean(), stratified_k_fold_scores.std()) # Define a custom cross-validation strategy custom_splits = [(list(range(0, 100)), list(range(100, 150))), (list(range(50, 150)), list(range(0, 50)))] custom_scores = [] for train_idx, test_idx in custom_splits: svc.fit(X_scaled[train_idx], y[train_idx]) custom_scores.append(svc.score(X_scaled[test_idx], y[test_idx])) custom_cv_results = (sum(custom_scores) / len(custom_scores), (max(custom_scores) - min(custom_scores)) / 2) # Evaluate the model using multiple metrics (precision and recall) multiple_metrics = cross_validate(svc, X_scaled, y, cv=5, scoring=[\'precision_macro\', \'recall_macro\']) multiple_metrics_results = { \\"precision_mean\\": multiple_metrics[\'test_precision_macro\'].mean(), \\"recall_mean\\": multiple_metrics[\'test_recall_macro\'].mean(), \\"precision_std\\": multiple_metrics[\'test_precision_macro\'].std(), \\"recall_std\\": multiple_metrics[\'test_recall_macro\'].std() } return k_fold_results, stratified_k_fold_results, custom_cv_results, multiple_metrics_results"},{"question":"# PyTorch Autograd Functional API - Jacobians and Hessians In this assessment, you will demonstrate your understanding of PyTorch\'s advanced autograd capabilities by working with the functional higher level API. Specifically, you will implement functions to compute the Jacobian and Hessian matrices for a given function. Requirements 1. Implement a function `compute_jacobian` that computes the Jacobian matrix of a given function ( f ). 2. Implement a function `compute_hessian` that computes the Hessian matrix of the same function ( f ). Constraints - The functions should handle arbitrary dimensions for the input tensor and should ensure that the provided function ( f ) is differentiable. - Your implementation should utilize the functional higher level API provided in the `torch.autograd` module. Function Signatures ```python import torch def compute_jacobian(f, x): Computes the Jacobian matrix of the function f at point x. Parameters: f (callable): A function that takes a tensor as input and returns a tensor. x (torch.Tensor): The point at which to compute the Jacobian, requires_grad should be True. Returns: torch.Tensor: The Jacobian matrix. pass def compute_hessian(f, x): Computes the Hessian matrix of the function f at point x. Parameters: f (callable): A function that takes a tensor as input and returns a scalar tensor. x (torch.Tensor): The point at which to compute the Hessian, requires_grad should be True. Returns: torch.Tensor: The Hessian matrix. pass ``` Performance Requirements - Ensure that your implementations are efficient and make use of PyTorch\'s autograd functional API. - Avoid using loops to compute the Jacobian or Hessian; leverage vectorized operations and built-in functions where possible. Example ```python # Example function def quadratic_function(x): return torch.sum(x ** 2) # Example usage x = torch.randn(3, requires_grad=True) jacobian = compute_jacobian(quadratic_function, x) hessian = compute_hessian(quadratic_function, x) print(\\"Jacobian:n\\", jacobian) print(\\"Hessian:n\\", hessian) ``` You are required to submit both implementations along with example outputs.","solution":"import torch def compute_jacobian(f, x): Computes the Jacobian matrix of the function f at point x. Parameters: f (callable): A function that takes a tensor as input and returns a tensor. x (torch.Tensor): The point at which to compute the Jacobian, requires_grad should be True. Returns: torch.Tensor: The Jacobian matrix. x = x.clone().detach().requires_grad_(True) y = f(x) jacobian = [] flat_y = y.view(-1) for i in range(flat_y.size(0)): torch.autograd.backward(flat_y[i], retain_graph=True, create_graph=True) jacobian.append(x.grad.clone().detach()) x.grad.zero_() return torch.stack(jacobian).reshape(y.shape + x.shape) def compute_hessian(f, x): Computes the Hessian matrix of the function f at point x. Parameters: f (callable): A function that takes a tensor as input and returns a scalar tensor. x (torch.Tensor): The point at which to compute the Hessian, requires_grad should be True. Returns: torch.Tensor: The Hessian matrix. x = x.clone().detach().requires_grad_(True) y = f(x) grad = torch.autograd.grad(y, x, create_graph=True)[0] hessian = [] for i in range(x.size(0)): row_grad = torch.autograd.grad(grad[i], x, retain_graph=True)[0] hessian.append(row_grad) return torch.stack(hessian) # Example function def quadratic_function(x): return torch.sum(x ** 2) # Example usage x = torch.randn(3, requires_grad=True) jacobian = compute_jacobian(quadratic_function, x) hessian = compute_hessian(quadratic_function, x) print(\\"Jacobian:n\\", jacobian) print(\\"Hessian:n\\", hessian)"},{"question":"Objective: Implement a function that takes a file path as input and returns a dictionary with detailed information about the file, such as its type, permissions, and size. Function Specification: ```python def get_file_details(file_path: str) -> dict: Given a file path, returns a dictionary with the following details about the file: - \'type\': The type of the file (e.g., \'directory\', \'regular file\', \'character device\', \'block device\', \'FIFO\', \'symbolic link\', \'socket\', \'door\', \'event port\', \'whiteout\'). - \'permissions\': A string representing the file\'s mode in the form \'-rwxrwxrwx\'. - \'size\': The size of the file in bytes. - \'inode\': The inode number of the file. - \'device\': The device the inode resides on. - \'links\': The number of links to the inode. - \'uid\': The user ID of the owner. - \'gid\': The group ID of the owner. - \'atime\': The time of last access. - \'mtime\': The time of last modification. - \'ctime\': The metadata change time (Unix) or creation time (Windows). ``` Input: - `file_path` (str): The path to the file. Output: - `dict`: A dictionary containing the detailed information as described above. Constraints: - You may assume the file exists at the given path. - Handle various file types and operating system variations seamlessly. Example: Suppose `sample_file.txt` is a regular file with the following properties: - Permissions: `-rw-r--r--` - Size: `1024` bytes - Inode number: `1001` - Device: `2049` - Number of Links: `1` - UID: `1000` - GID: `1000` - Last Access Time: `1630484147.0` - Last Modification Time: `1630484147.0` - Metadata Change Time: `1630484147.0` (Unix) or Creation Time (Windows) ```python file_details = get_file_details(\'sample_file.txt\') print(file_details) ``` Expected output is a dictionary similar to: ```python { \'type\': \'regular file\', \'permissions\': \'-rw-r--r--\', \'size\': 1024, \'inode\': 1001, \'device\': 2049, \'links\': 1, \'uid\': 1000, \'gid\': 1000, \'atime\': 1630484147.0, \'mtime\': 1630484147.0, \'ctime\': 1630484147.0, } ``` Notes: - Utilize the `os` and `stat` modules to get the file information. - Ensure to use appropriate `stat` functions to discern the file type. - Convert the permission mode to a human-readable string using `stat.filemode`.","solution":"import os import stat import time def get_file_details(file_path: str) -> dict: Given a file path, returns a dictionary with the following details about the file: - \'type\': The type of the file (e.g., \'directory\', \'regular file\', \'character device\', \'block device\', \'FIFO\', \'symbolic link\', \'socket\', \'door\', \'event port\', \'whiteout\'). - \'permissions\': A string representing the file\'s mode in the form \'-rwxrwxrwx\'. - \'size\': The size of the file in bytes. - \'inode\': The inode number of the file. - \'device\': The device the inode resides on. - \'links\': The number of links to the inode. - \'uid\': The user ID of the owner. - \'gid\': The group ID of the owner. - \'atime\': The time of last access. - \'mtime\': The time of last modification. - \'ctime\': The metadata change time (Unix) or creation time (Windows). try: stat_info = os.stat(file_path) file_type = \'unknown\' if stat.S_ISDIR(stat_info.st_mode): file_type = \'directory\' elif stat.S_ISREG(stat_info.st_mode): file_type = \'regular file\' elif stat.S_ISCHR(stat_info.st_mode): file_type = \'character device\' elif stat.S_ISBLK(stat_info.st_mode): file_type = \'block device\' elif stat.S_ISFIFO(stat_info.st_mode): file_type = \'FIFO\' elif stat.S_ISLNK(stat_info.st_mode): file_type = \'symbolic link\' elif stat.S_ISSOCK(stat_info.st_mode): file_type = \'socket\' details = { \'type\': file_type, \'permissions\': stat.filemode(stat_info.st_mode), \'size\': stat_info.st_size, \'inode\': stat_info.st_ino, \'device\': stat_info.st_dev, \'links\': stat_info.st_nlink, \'uid\': stat_info.st_uid, \'gid\': stat_info.st_gid, \'atime\': stat_info.st_atime, \'mtime\': stat_info.st_mtime, \'ctime\': stat_info.st_ctime } return details except Exception as e: raise RuntimeError(f\\"Error retrieving file details: {e}\\")"},{"question":"# Question: Implementing and Manipulating Cell Objects in Python Objective: Your task is to implement several functionalities to create and manipulate cell objects in Python, demonstrating an understanding of how they can be used to reference variables in multiple scopes. Background: In Python, \\"Cell\\" objects are used to implement variables that are referenced across multiple scopes. These cell objects store values that can be shared across different contexts. Requirements and Functions to Implement: 1. **Cell Creation (create_cell)**: - **Input**: Any Python object `obj`. - **Output**: A new cell object containing the `obj`. 2. **Check if Object is a Cell (is_cell)**: - **Input**: Any Python object `obj`. - **Output**: Boolean `True` if `obj` is a cell object, otherwise `False`. 3. **Get Value from Cell (get_cell_value)**: - **Input**: A cell object `cell`. - **Output**: The value contained in the cell object. 4. **Set Value in Cell (set_cell_value)**: - **Input**: - `cell`: A cell object. - `value`: Any Python object to set in the cell. - **Output**: None. 5. **Test Cell Operations (test_cell_operations)**: - **Input**: None. - **Output**: A boolean `True` if the cell operations work correctly, otherwise `False`. # Implementation Details: - You are required to work with a class that encapsulates the cell operations. - Use proper error handling to manage NULL and invalid cell objects. - The `test_cell_operations` function should create a series of tests to ensure that all previous functions work as intended. # Example: ```python class Cell: def create_cell(self, obj): # Your implementation here def is_cell(self, obj): # Your implementation here def get_cell_value(self, cell): # Your implementation here def set_cell_value(self, cell, value): # Your implementation here def test_cell_operations(self): # Your implementation here # Example usage: cell_obj = Cell() cell = cell_obj.create_cell(5) assert cell_obj.is_cell(cell) assert cell_obj.get_cell_value(cell) == 5 cell_obj.set_cell_value(cell, 10) assert cell_obj.get_cell_value(cell) == 10 assert cell_obj.test_cell_operations() ``` Constraints: 1. The implementation must handle edge cases such as NULL values. 2. The cell operations should be efficient and correctly manage reference counts where applicable. Good luck and happy coding!","solution":"import types class Cell: @staticmethod def create_cell(obj): Creates a new cell object containing the obj return (lambda x: lambda: x)(obj).__closure__[0] @staticmethod def is_cell(obj): Checks if the given object is a cell object. return isinstance(obj, types.CellType) @staticmethod def get_cell_value(cell): Gets the value contained in the cell object. if not Cell.is_cell(cell): raise ValueError(\\"The provided object is not a cell.\\") return cell.cell_contents @staticmethod def set_cell_value(cell, value): Sets the value in the cell object. if not Cell.is_cell(cell): raise ValueError(\\"The provided object is not a cell.\\") cell.cell_contents = value def test_cell_operations(self): Tests the cell operations to ensure they work correctly. try: # Create a cell and test its value cell = self.create_cell(5) assert self.is_cell(cell) assert self.get_cell_value(cell) == 5 # Set a new value in the cell and test it self.set_cell_value(cell, 10) assert self.get_cell_value(cell) == 10 # Check the error handling try: self.set_cell_value(\\"not a cell\\", 10) except ValueError as e: if str(e) != \\"The provided object is not a cell.\\": return False try: self.get_cell_value(\\"not a cell\\") except ValueError as e: if str(e) != \\"The provided object is not a cell.\\": return False return True except: return False"},{"question":"**Assessment Question: Advanced Seaborn FacetGrid Plotting** **Objective:** Create a multi-faceted plot using seaborn\'s `FacetGrid` class to display various visualizations based on different subsets of a dataset and customize these plots further using additional functions. **Problem Statement:** You are provided with the \\"tips\\" dataset from seaborn\'s built-in datasets. Your task is to create a multi-faceted grid of scatterplots that visualize the relationship between the \\"total_bill\\" and \\"tip\\" columns. The facets should be conditioned by \\"time\\" and \\"sex\\" and customized as follows: 1. **Initialize the FacetGrid:** - Load the \\"tips\\" dataset using `sns.load_dataset(\\"tips\\")`. - Initialize a `FacetGrid` with columns conditioned by \\"time\\" and rows conditioned by \\"sex\\". 2. **Map Scatterplot:** - Use the `map_dataframe` method to create scatterplots of \\"total_bill\\" against \\"tip\\" for each facet. - Use different colors for the points based on the \\"smoker\\" status. 3. **Add Legend:** - Include a legend that indicates the color mapping of the \\"smoker\\" variable. 4. **Customize Facet Titles and Labels:** - Modify the facet titles to indicate the type of customers (time of the day and gender). - Set axis labels for \\"Total bill ()\\" (x-axis) and \\"Tip ()\\" (y-axis). 5. **Add Reference Line:** - Add a horizontal reference line at the median tip amount across the entire dataset. 6. **Annotate Each Facet:** - Create a function to annotate the number of data points (`N`) in each facet. - Use this function to add annotations to each facet. 7. **Additional Customization:** - Adjust the x-axis limits from 0 to 60 and y-axis limits from 0 to 12. - Set x-ticks at [10, 30, 50] and y-ticks at [2, 6, 10]. - Remove the top and right spines using `sns.despine`. - Ensure tight layout and save the final plot as \\"custom_facet_plot.png\\". **Input:** - The \\"tips\\" dataset is loaded directly within the coding environment using `sns.load_dataset(\\"tips\\")`. **Output:** - Save the final plot as \\"custom_facet_plot.png\\". **Constraints:** - Ensure you follow code readability and style guidelines. - The functions used should be from seaborn or matplotlib, as appropriate. **Example Code:** Here is a sample structure to get you started. ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Initialize FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\") # Map scatterplot with hue g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\") g.add_legend() # Customize facet titles and labels g.set_axis_labels(\\"Total bill ()\\", \\"Tip ()\\") g.set_titles(col_template=\\"{col_name} patrons\\", row_template=\\"{row_name}\\") # Add reference line g.refline(y=tips[\\"tip\\"].median()) # Function to annotate with number of points def annotate(data, **kws): n = len(data) ax = plt.gca() ax.text(.1, .6, f\\"N = {n}\\", transform=ax.transAxes) # Annotate each facet g.map_dataframe(annotate) # Additional customizations g.set(xlim=(0, 60), ylim=(0, 12), xticks=[10, 30, 50], yticks=[2, 6, 10]) sns.despine() g.tight_layout() g.savefig(\\"custom_facet_plot.png\\") ``` Complete the provided structure to fully implement the requirements and customize the plot as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_facet_grid_plot(): # Load dataset tips = sns.load_dataset(\\"tips\\") # Initialize FacetGrid g = sns.FacetGrid(tips, col=\\"time\\", row=\\"sex\\", margin_titles=True) # Map scatterplot with hue g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", palette=\\"tab10\\") g.add_legend() # Customize facet titles and labels g.set_axis_labels(\\"Total bill ()\\", \\"Tip ()\\") g.set_titles(col_template=\\"{col_name} patrons\\", row_template=\\"{row_name}\\") # Add reference line g.refline(y=tips[\\"tip\\"].median(), linestyle=\\"--\\", color=\\"red\\") # Function to annotate with number of points def annotate(data, **kws): n = len(data) ax = plt.gca() ax.text(.05, .95, f\\"N = {n}\\", transform=ax.transAxes, fontsize=12, verticalalignment=\'top\') # Annotate each facet g.map_dataframe(annotate) # Additional customizations g.set(xlim=(0, 60), ylim=(0, 12), xticks=[10, 30, 50], yticks=[2, 6, 10]) sns.despine() g.tight_layout() g.savefig(\\"custom_facet_plot.png\\") # Call the function to create and save the plot create_facet_grid_plot()"},{"question":"Problem Statement Your task is to implement a function `process_time_deltas` that accepts a DataFrame with two columns: `\'start_time\'` and `\'end_time\'`, both containing timestamp values. The function should compute the difference in time between these two columns and create a new `Timedelta` column called `\'duration\'`. Additionally, you should calculate the total duration, the mean duration, and resample these durations on an hourly basis. # Function Signature ```python import pandas as pd def process_time_deltas(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Input - `df`: A pandas DataFrame containing two columns: `\'start_time\'` and `\'end_time\'`, both in datetime format. # Output - A pandas DataFrame that includes: - The original columns `\'start_time\'` and `\'end_time\'`. - A new column `\'duration\'` representing the difference between `\'end_time\'` and `\'start_time\'` calculated as a `Timedelta`. - A new column `\'total_duration\'` that shows the total duration summed over the entire DataFrame. - A new column `\'mean_duration\'` that shows the mean duration. - A new DataFrame that results from resampling the `\'duration\'` data on an hourly basis, aggregating with the mean function. # Constraints - Assume `df` is non-empty and all values in `\'start_time\'` and `\'end_time\'` are valid datetime values. - The DataFrame may contain any number of rows. - Performance should be considered; the function should run efficiently even for large DataFrames. # Example ```python import pandas as pd data = { \'start_time\': [\'2023-01-01 12:00\', \'2023-01-01 14:30\', \'2023-01-01 13:15\'], \'end_time\': [\'2023-01-01 14:00\', \'2023-01-01 16:00\', \'2023-01-01 15:00\'] } df = pd.DataFrame(data) df[\'start_time\'] = pd.to_datetime(df[\'start_time\']) df[\'end_time\'] = pd.to_datetime(df[\'end_time\']) # Call the function result = process_time_deltas(df) print(result) ``` Expected Output: ``` start_time end_time duration total_duration mean_duration 0 2023-01-01 12:00:00 2023-01-01 14:00:00 0 days 02:00:00 1 2023-01-01 14:30:00 2023-01-01 16:00:00 0 days 01:30:00 2 2023-01-01 13:15:00 2023-01-01 15:00:00 0 days 01:45:00 duration_resample 2023-01-01 12:00:00 0 days 02:00:00 2023-01-01 13:00:00 0 days 01:45:00 2023-01-01 14:00:00 0 days 01:00:00 # Note: The actual output may vary depending on the calculated statistics but should maintain the format. ``` # Explanation 1. Calculate the difference between `\'end_time\'` and `\'start_time\'` to get the `\'duration\'`. 2. Compute the total and mean duration of all time deltas in the DataFrame. 3. Resample the calculated `\'duration\'` column on an hourly basis and aggregate to get the mean duration for each hour.","solution":"import pandas as pd def process_time_deltas(df: pd.DataFrame) -> pd.DataFrame: df[\'duration\'] = df[\'end_time\'] - df[\'start_time\'] total_duration = df[\'duration\'].sum() mean_duration = df[\'duration\'].mean() df[\\"total_duration\\"] = total_duration df[\\"mean_duration\\"] = mean_duration resampled = df.set_index(\'start_time\').resample(\'H\').mean(numeric_only=False)[\'duration\'].reset_index(name=\'duration_resample\') result = { \'original\': df, \'resampled\': resampled } return result"},{"question":"You are tasked with implementing a command-line tool that uses the \\"quopri\\" module to encode and decode message files in quoted-printable format. The tool should handle encoding and decoding based on command-line arguments. Requirements: 1. Implement a function `encode_file(input_file, output_file, quotetabs, header)`: - `input_file`: Path to the input file containing the original message. - `output_file`: Path to the output file where the encoded message will be saved. - `quotetabs` (boolean): Whether to encode embedded spaces and tabs. - `header` (boolean): Whether to encode spaces as underscores. 2. Implement a function `decode_file(input_file, output_file, header)`: - `input_file`: Path to the input file containing the encoded message. - `output_file`: Path to the output file where the decoded message will be saved. - `header` (boolean): Whether to decode underscores as spaces. 3. The command-line tool should accept the following commands: - `encode`: Encode the contents of `input_file` and write to `output_file`. - `decode`: Decode the contents of `input_file` and write to `output_file`. 4. The command-line interface should accept additional flags: - `--quotetabs`: To specify whether to encode embedded spaces and tabs (default is `False`). - `--header`: To specify whether to handle the encoding and decoding of spaces and underscores for headers (default is `False`). Input/Output Format: - The input and output files are plain text files. - Command-line execution format: ```shell python <script.py> encode --input_file <input_file> --output_file <output_file> [--quotetabs] [--header] python <script.py> decode --input_file <input_file> --output_file <output_file> [--header] ``` - Example: ```shell python script.py encode --input_file original.txt --output_file encoded.txt --quotetabs --header python script.py decode --input_file encoded.txt --output_file decoded.txt --header ``` Example: **original.txt:** ``` Hello, world! This is a test message. ``` **encoded.txt using `--quotetabs` and `--header` flags:** ``` Hello=2C=20world! This=20is=20a=20test=20message=2E ``` **decoded.txt using `--header` flag:** ``` Hello, world! This is a test message. ``` Constraints: - You may assume the input and output file paths are valid and writable. - You should handle file reading and writing as binary streams. **Implement the `encode_file` and `decode_file` functions in Python, as well as the command-line interface. Ensure proper handling of the flags and file operations.**","solution":"import quopri import argparse def encode_file(input_file, output_file, quotetabs=False, header=False): with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: if header: quopri.encode(infile, outfile, quotetabs=quotetabs, header=True) else: quopri.encode(infile, outfile, quotetabs=quotetabs) def decode_file(input_file, output_file, header=False): with open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: if header: quopri.decode(infile, outfile, header=True) else: quopri.decode(infile, outfile) if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\'Encode or decode files using quoted-printable format\') subparsers = parser.add_subparsers(dest=\'command\', required=True) encode_parser = subparsers.add_parser(\'encode\', help=\'Encode a file\') encode_parser.add_argument(\'--input_file\', required=True, help=\'Path to the input file to encode\') encode_parser.add_argument(\'--output_file\', required=True, help=\'Path to the output file to save the encoded content\') encode_parser.add_argument(\'--quotetabs\', action=\'store_true\', help=\'Encode embedded spaces and tabs\') encode_parser.add_argument(\'--header\', action=\'store_true\', help=\'Encode spaces as underscores in headers\') decode_parser = subparsers.add_parser(\'decode\', help=\'Decode a file\') decode_parser.add_argument(\'--input_file\', required=True, help=\'Path to the input file to decode\') decode_parser.add_argument(\'--output_file\', required=True, help=\'Path to the output file to save the decoded content\') decode_parser.add_argument(\'--header\', action=\'store_true\', help=\'Decode underscores as spaces in headers\') args = parser.parse_args() if args.command == \'encode\': encode_file(args.input_file, args.output_file, args.quotetabs, args.header) elif args.command == \'decode\': decode_file(args.input_file, args.output_file, args.header)"},{"question":"**Objective:** Demonstrate your understanding and ability to interact with the `site` module in Python. **Problem Description:** You are tasked with writing a Python script that performs the following operations using the `site` module: 1. **Print Site Packages Directories:** - Print the list of all global site-packages directories using `site.getsitepackages()`. 2. **Print User Base and User Site Directory Paths:** - Print the path of the user base directory. - Print the path of the user-specific site-packages directory. 3. **Check Enabling of User Site:** - Check if the user site-packages directory is enabled using the `site.ENABLE_USER_SITE` flag. - Print an appropriate message based on whether the directory is enabled, disabled by the user, disabled for security reasons, or for some other reason. 4. **Add a Custom Site Directory:** - Add a directory `/custom/site-packages` to the `sys.path` and ensure its `.pth` files are processed using `site.addsitedir()`. **Input and Output:** - **Input:** - No explicit input is required; the script operates based on the current Python environment. - **Output:** - Print the global site-packages directories as a list. - Print the user base directory path. - Print the user-specific site-packages directory path. - Print the status message about the user site-packages directory (enabled, user-disabled, security-disabled, or other). - Print the updated `sys.path`. **Performance Requirements:** - The solution should handle typical Python system directory structures efficiently. - Ensure that all changes to `sys.path` are correctly reflected and do not duplicate entries. **Constraints:** - You should not assume the existence of any specific directories other than those provided by the functions in the `site` module. - Your script must be compatible with Python 3.10 and should not use any deprecated functions or attributes. ```python import site def main(): # Print all global site-packages directories global_site_packages = site.getsitepackages() print(\\"Global site-packages directories:\\", global_site_packages) # Print the user base directory path user_base = site.getuserbase() print(\\"User base directory path:\\", user_base) # Print the user-specific site-packages directory path user_site_packages = site.getusersitepackages() print(\\"User-specific site-packages directory path:\\", user_site_packages) # Check if the user site-packages directory is enabled user_site_enabled = site.ENABLE_USER_SITE if user_site_enabled is True: print(\\"User site-packages directory is enabled.\\") elif user_site_enabled is False: print(\\"User site-packages directory is disabled by user request.\\") elif user_site_enabled is None: print(\\"User site-packages directory is disabled for security reasons or by an administrator.\\") else: print(\\"Unknown state for user site-packages directory.\\") # Add a custom site directory to sys.path and process its .pth files custom_dir = \\"/custom/site-packages\\" site.addsitedir(custom_dir) print(\\"Updated sys.path:\\", sys.path) if __name__ == \\"__main__\\": main() ``` **Note:** Assume the necessary environment setup and permissions are in place to test the script effectively.","solution":"import site import sys def main(): # Print all global site-packages directories global_site_packages = site.getsitepackages() print(\\"Global site-packages directories:\\", global_site_packages) # Print the user base directory path user_base = site.getuserbase() print(\\"User base directory path:\\", user_base) # Print the user-specific site-packages directory path user_site_packages = site.getusersitepackages() print(\\"User-specific site-packages directory path:\\", user_site_packages) # Check if the user site-packages directory is enabled user_site_enabled = site.ENABLE_USER_SITE if user_site_enabled is True: print(\\"User site-packages directory is enabled.\\") elif user_site_enabled is False: print(\\"User site-packages directory is disabled by user request.\\") elif user_site_enabled is None: print(\\"User site-packages directory is disabled for security reasons or by an administrator.\\") else: print(\\"Unknown state for user site-packages directory.\\") # Add a custom site directory to sys.path and process its .pth files custom_dir = \\"/custom/site-packages\\" site.addsitedir(custom_dir) print(\\"Updated sys.path:\\", sys.path) if __name__ == \\"__main__\\": main()"},{"question":"Create a Dynamic Todo List Application You are required to design a text-based Todo List application using the `curses` module in Python. The application should allow the user to add, delete, and view tasks interactively. # Requirements: 1. **Initialize Screen**: - Prepare the screen for displaying the todo list using `curses.initscr()`. 2. **Main Window**: - Create a main window to display the list of tasks. - Create a status bar to display instructions and messages to the user. 3. **Task Management**: - Allow the user to add new tasks. Use an input box at the bottom of the screen for task entry. - Allow the user to delete tasks. Provide options to select a task and delete it. 4. **Navigation and Interaction**: - Provide keyboard shortcuts for the following actions: - Add a task: Press \'a\' to activate the input box. - Delete a task: Use arrow keys to navigate and \'d\' to delete a task. - Quit the application: Press \'q\' to quit. 5. **Display Updates**: - Ensure the screen updates smoothly and reflects changes immediately. - Handle the edge cases such as empty list, cursor movement restrictions, etc. 6. **Error Handling**: - Handle any exceptions that might occur during screen initialization or terminal handling gracefully. # Input Format: No specific input format as the program will handle interactive user inputs through the terminal. # Output Format: The interaction will be visually presented on the terminal screen, reflecting changes in the todo list. # Constraints: - Use `curses` module functionalities extensively to manage the terminal screen and input handling. - Ensure that the application runs correctly within a standard terminal emulator. # Implementation: You are provided with a partial implementation to get you started: ```python import curses def main(stdscr): # Clear screen stdscr.clear() # Turn off cursor blinking curses.curs_set(0) # Initialize colors if supported if curses.has_colors(): curses.start_color() curses.init_pair(1, curses.COLOR_CYAN, curses.COLOR_BLACK) # Main window setup window = curses.newwin(curses.LINES - 1, curses.COLS, 0, 0) window.keypad(1) window.refresh() # Status bar setup statusbarstr = \\"Press \'a\' to add task | \'d\' to delete task | \'q\' to quit\\" stdscr.addstr(curses.LINES - 1, 0, statusbarstr) stdscr.refresh() tasks = [] selected_task = 0 while True: window.clear() # Display tasks for idx, task in enumerate(tasks): if idx == selected_task: mode = curses.A_REVERSE else: mode = curses.A_NORMAL window.addstr(idx, 0, task, mode) key = window.getch() # Process key actions if key == ord(\'q\'): break elif key == ord(\'a\'): # Adding a new task stdscr.addstr(curses.LINES - 2, 0, \\"Enter new task: \\") stdscr.clrtoeol() stdscr.refresh() curses.echo() new_task = stdscr.getstr().decode(\'utf-8\') curses.noecho() tasks.append(new_task) elif key == curses.KEY_UP: selected_task = (selected_task - 1) % len(tasks) if tasks else 0 elif key == curses.KEY_DOWN: selected_task = (selected_task + 1) % len(tasks) if tasks else 0 elif key == ord(\'d\') and tasks: tasks.pop(selected_task) selected_task = max(0, selected_task - 1) stdscr.addstr(curses.LINES - 1, 0, statusbarstr) stdscr.refresh() if __name__ == \\"__main__\\": curses.wrapper(main) ``` # Submission: Submit the completed implementation of the Todo List application as described in the above requirements. Ensure the code is properly commented and adheres to Python coding conventions.","solution":"import curses def main(stdscr): # Clear screen stdscr.clear() # Turn off cursor blinking curses.curs_set(0) # Initialize colors if supported if curses.has_colors(): curses.start_color() curses.init_pair(1, curses.COLOR_CYAN, curses.COLOR_BLACK) # Main window setup window = curses.newwin(curses.LINES - 1, curses.COLS, 0, 0) window.keypad(1) window.refresh() # Status bar setup statusbarstr = \\"Press \'a\' to add task | \'d\' to delete task | \'q\' to quit\\" stdscr.addstr(curses.LINES - 1, 0, statusbarstr) stdscr.refresh() tasks = [] selected_task = 0 while True: window.clear() # Display tasks for idx, task in enumerate(tasks): if idx == selected_task: mode = curses.A_REVERSE else: mode = curses.A_NORMAL window.addstr(idx, 0, task, mode) key = window.getch() # Process key actions if key == ord(\'q\'): break elif key == ord(\'a\'): # Adding a new task stdscr.addstr(curses.LINES - 2, 0, \\"Enter new task: \\") stdscr.clrtoeol() stdscr.refresh() curses.echo() new_task = stdscr.getstr().decode(\'utf-8\') curses.noecho() tasks.append(new_task) elif key == curses.KEY_UP: selected_task = (selected_task - 1) % len(tasks) if tasks else 0 elif key == curses.KEY_DOWN: selected_task = (selected_task + 1) % len(tasks) if tasks else 0 elif key == ord(\'d\') and tasks: tasks.pop(selected_task) selected_task = max(0, selected_task - 1) stdscr.addstr(curses.LINES - 1, 0, statusbarstr) stdscr.refresh() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"**Complex Number Operations with cmath** **Objective:** Write a Python function `complex_operations` that takes in two complex numbers in rectangular coordinates and performs several operations using the `cmath` module. The function should: 1. Convert the given complex numbers to their polar coordinates. 2. Compute the sum, difference, and product of the two complex numbers. 3. Calculate the exponential, logarithm (base 10), and square root of each complex number. 4. Classify each complex number by checking if it is finite, infinite, or NaN. **Function Signature:** ```python def complex_operations(z1: complex, z2: complex) -> dict: pass ``` **Input:** - `z1`: A complex number in rectangular coordinates. - `z2`: Another complex number in rectangular coordinates. **Output:** - A dictionary containing the results of the following operations: - Polar coordinates of `z1` and `z2`. - Sum of `z1` and `z2`. - Difference (z1 - z2). - Product of `z1` and `z2`. - Exponential of `z1` and `z2`. - Logarithm (base 10) of `z1` and `z2`. - Square root of `z1` and `z2`. - Classification of `z1` and `z2` (whether each is finite, infinite, or NaN). **Example:** ```python z1 = complex(3, 4) z2 = complex(1, 2) result = complex_operations(z1, z2) # Expected result dictionary: # { # \'polar_z1\': (5.0, 0.9272952180016122), # (Modulus, Phase) # \'polar_z2\': (2.23606797749979, 1.1071487177940904), # \'sum_z1_z2\': (4+6j), # \'diff_z1_z2\': (2+2j), # \'prod_z1_z2\': (-5+10j), # \'exp_z1\': (-13.128783081462158-15.200784463067954j), # \'exp_z2\': (-1.1312043837568135+2.4717266720048188j), # \'log10_z1\': (0.6989700043360187+0.3424166044976883j), # \'log10_z2\': (0.3010299956639812+0.480828578784234j), # \'sqrt_z1\': (2+1j), # \'sqrt_z2\': (1.272019649514069+0.7861513777574233j), # \'isfinite_z1\': True, # \'isfinite_z2\': True, # \'isinf_z1\': False, # \'isinf_z2\': False, # \'isnan_z1\': False, # \'isnan_z2\': False # } ``` **Constraints:** - Complex numbers are not extremely large and can be handled by the `cmath` module without performance issues. **Requirements:** - You should use appropriate functions from the `cmath` module to achieve the desired results. - Handle necessary error checks and exceptions to ensure the function runs smoothly. - Ensure your solution is optimized and efficient. **Notes:** - Feel free to use additional helper functions if necessary. - Document your code properly for readability.","solution":"import cmath def complex_operations(z1, z2): Performs various operations on two complex numbers using the cmath module. Args: z1: complex number z2: complex number Returns: dict: A dictionary containing the results of various operations result = {} # Convert to polar coordinates result[\'polar_z1\'] = cmath.polar(z1) result[\'polar_z2\'] = cmath.polar(z2) # Compute sum, difference, and product result[\'sum_z1_z2\'] = z1 + z2 result[\'diff_z1_z2\'] = z1 - z2 result[\'prod_z1_z2\'] = z1 * z2 # Compute exponential, logarithm (base 10), and square root result[\'exp_z1\'] = cmath.exp(z1) result[\'exp_z2\'] = cmath.exp(z2) result[\'log10_z1\'] = cmath.log10(z1) result[\'log10_z2\'] = cmath.log10(z2) result[\'sqrt_z1\'] = cmath.sqrt(z1) result[\'sqrt_z2\'] = cmath.sqrt(z2) # Classify the numbers result[\'isfinite_z1\'] = cmath.isfinite(z1) result[\'isfinite_z2\'] = cmath.isfinite(z2) result[\'isinf_z1\'] = cmath.isinf(z1) result[\'isinf_z2\'] = cmath.isinf(z2) result[\'isnan_z1\'] = cmath.isnan(z1) result[\'isnan_z2\'] = cmath.isnan(z2) return result"},{"question":"Advanced XPU Operations in PyTorch Objective Implement functions that utilize PyTorch\'s `torch.xpu` module to manage devices, generate random numbers, and handle memory. This exercise will test your understanding of device management, random number generator state, and memory management in XPU. Task Description 1. **Device Management** - Implement a function `initialize_xpu_device()` that initializes the XPU if it is available and returns the device identifier. - Implement a function `get_xpu_properties(device_id)` that returns the properties of the specified XPU device. ```python def initialize_xpu_device(): Initializes the XPU if available and returns the device identifier. Returns: int: The device identifier of the initialized XPU. pass def get_xpu_properties(device_id): Returns the properties of the specified XPU device. Args: device_id (int): The ID of the device whose properties are to be returned. Returns: dict: The properties of the specified device. pass ``` 2. **Random Number Generation** - Implement a function `generate_random_numbers(seed, count, device_id)` that sets the random seed on the XPU, generates a specified number of random numbers, and returns them as a tensor. ```python def generate_random_numbers(seed, count, device_id): Sets the random seed on the XPU and generates a specified number of random numbers. Args: seed (int): The seed value for generating random numbers. count (int): The number of random numbers to generate. device_id (int): The device on which to generate the random numbers. Returns: torch.Tensor: A tensor containing the generated random numbers. pass ``` 3. **Memory Management** - Implement a function `get_memory_info(device_id)` that returns current, peak, and total memory usage for the specified XPU device. ```python def get_memory_info(device_id): Returns current, peak, and total memory usage for the specified XPU device. Args: device_id (int): The ID of the device whose memory information is to be returned. Returns: dict: A dictionary containing \'current_allocated\', \'peak_allocated\', \'total_reserved\' memory sizes in bytes. pass ``` Constraints and Limitations - Ensure all device-related operations check if the device is available and handle the cases where it is not. - Use appropriate PyTorch `torch.xpu` module functions outlined in the provided documentation. - Ensure efficient memory usage in the functions for memory management. - Document your code clearly to explain the steps taken. Input and Output Formats - `initialize_xpu_device`: - **Input:** None - **Output:** `int`: The device identifier (e.g., `0`). - `get_xpu_properties(device_id)`: - **Input:** `device_id (int)` - **Output:** `dict`: `{ \\"name\\": str, \\"total_memory\\": int, \\"capability\\": tuple }` - `generate_random_numbers(seed, count, device_id)`: - **Input:** `seed (int)`, `count (int)`, `device_id (int)` - **Output:** `torch.Tensor` - `get_memory_info(device_id)`: - **Input:** `device_id (int)` - **Output:** `dict`: `{ \\"current_allocated\\": int, \\"peak_allocated\\": int, \\"total_reserved\\": int }` Example ```python # Example usage xpu_device_id = initialize_xpu_device() properties = get_xpu_properties(xpu_device_id) random_numbers = generate_random_numbers(42, 10, xpu_device_id) memory_info = get_memory_info(xpu_device_id) print(\\"Device Properties:\\", properties) print(\\"Random Numbers:\\", random_numbers) print(\\"Memory Info:\\", memory_info) ``` Ensure your implementations conform to the function signatures provided, and handle all edge cases where the XPU might not be available or other possible errors.","solution":"import torch def initialize_xpu_device(): Initializes the XPU if available and returns the device identifier. Returns: int: The device identifier of the initialized XPU. if torch.xpu.is_available(): return 0 else: raise RuntimeError(\\"XPU device is not available.\\") def get_xpu_properties(device_id): Returns the properties of the specified XPU device. Args: device_id (int): The ID of the device whose properties are to be returned. Returns: dict: The properties of the specified device. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available.\\") device = torch.device(f\'xpu:{device_id}\') properties = { \\"name\\": torch.xpu.get_device_name(device_id), \\"total_memory\\": torch.xpu.get_device_properties(device_id).total_memory, \\"capability\\": torch.xpu.get_device_properties(device_id).major } return properties def generate_random_numbers(seed, count, device_id): Sets the random seed on the XPU and generates a specified number of random numbers. Args: seed (int): The seed value for generating random numbers. count (int): The number of random numbers to generate. device_id (int): The device on which to generate the random numbers. Returns: torch.Tensor: A tensor containing the generated random numbers. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available.\\") device = torch.device(f\'xpu:{device_id}\') torch.manual_seed(seed) return torch.randn(count, device=device) def get_memory_info(device_id): Returns current, peak, and total memory usage for the specified XPU device. Args: device_id (int): The ID of the device whose memory information is to be returned. Returns: dict: A dictionary containing \'current_allocated\', \'peak_allocated\', \'total_reserved\' memory sizes in bytes. if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available.\\") device = torch.device(f\'xpu:{device_id}\') return { \\"current_allocated\\": torch.xpu.memory_allocated(device), \\"peak_allocated\\": torch.xpu.max_memory_allocated(device), \\"total_reserved\\": torch.xpu.memory_reserved(device) }"},{"question":"# Asynchronous Programming with asyncio You have been hired to develop a system that processes a stream of data asynchronously. The data should be processed in such a way that all tasks and callbacks are managed efficiently without blocking the main event loop. Specifically, we want to ensure that coroutines are properly awaited, exceptions are handled, and concurrency is managed when running blocking I/O and CPU-bound tasks. Task: 1. **Implement an Async Processing Function**: Write an asynchronous function `async_process_data` that: - Accepts a list of data items. - For each data item, simulates a delay using `asyncio.sleep` and returns the modified data item. - Let\'s assume modifying the data item involves converting it to uppercase. - This function must process a data stream asynchronously using `asyncio.gather`. 2. **Handle Blocking Code Execution**: Implement a function `run_blocking_task` that: - Accepts a CPU-bound function and its arguments. - Uses `loop.run_in_executor()` with a `ThreadPoolExecutor` to execute this blocking task without blocking the event loop. 3. **Handle Concurrency from Different Threads**: - Implement a function `schedule_task_from_thread` that: - Accepts an event loop, a coroutine, and its arguments. - Uses `asyncio.run_coroutine_threadsafe()` to safely schedule this coroutine from a different thread and retrieve its result. 4. **Enable Debug Mode and Proper Logging**: - Modify the main execution environment to: - Enable asyncio in debug mode. - Configure the logging level to `DEBUG`. - Display warnings about potential resource issues. Requirements: - Use the provided documentation for asyncio. - Ensure coroutines are properly awaited and exceptions are handled. - Follow asyncio best practices for concurrency and multithreading. - Ensure the code adheres to the performance requirements by testing with a significant amount of data. Input/Output: 1. **async_process_data**: - **Input**: List of strings `data_items` - **Output**: List of processed data items (uppercased) 2. **run_blocking_task**: - **Input**: A callable `func` representing a blocking task and its positional arguments `*args`. - **Output**: Result of the blocking task. 3. **schedule_task_from_thread**: - **Input**: `loop` (the event loop), `coro_func` (the coroutine function), and its positional arguments `*args`. - **Output**: Result of the coroutine, accessed via a future. 4. **Setup Debug Mode and Logging**: - No input/output required, just ensure that these settings are applied when running the main part of your code. Example: ```python import asyncio # Example setup code, please ensure your functions are defined above this code. async def main(): data = [\\"hello\\", \\"world\\", \\"asyncio\\"] processed_data = await async_process_data(data) print(processed_data) # [\\"HELLO\\", \\"WORLD\\", \\"ASYNCIO\\"] loop = asyncio.get_event_loop() loop.set_debug(True) logging.basicConfig(level=logging.DEBUG) asyncio.run(main()) # Example of running a blocking task result = run_blocking_task(sum, [1, 2, 3, 4]) print(result) # 10 # Example of scheduling a task from another thread def thread_task(): future = schedule_task_from_thread(loop, async_process_data, [\\"one\\", \\"two\\"]) result = future.result() print(result) # [\\"ONE\\", \\"TWO\\"] thread_task() ``` Write your implementation for `async_process_data`, `run_blocking_task`, `schedule_task_from_thread`, and the setup for enabling debug mode and logging.","solution":"import asyncio from concurrent.futures import ThreadPoolExecutor import logging async def async_process_data(data_items): async def process_item(item): await asyncio.sleep(0.1) # simulate delay return item.upper() tasks = [process_item(item) for item in data_items] return await asyncio.gather(*tasks) def run_blocking_task(func, *args): loop = asyncio.get_event_loop() with ThreadPoolExecutor() as executor: result = loop.run_in_executor(executor, func, *args) return asyncio.get_event_loop().run_until_complete(result) def schedule_task_from_thread(loop, coro_func, *args): coro = coro_func(*args) future = asyncio.run_coroutine_threadsafe(coro, loop) return future # Main setup for asyncio environment logging.basicConfig(level=logging.DEBUG) loop = asyncio.get_event_loop() loop.set_debug(True) # Example usage if __name__ == \\"__main__\\": async def main(): data = [\\"hello\\", \\"world\\", \\"asyncio\\"] processed_data = await async_process_data(data) print(processed_data) # [\\"HELLO\\", \\"WORLD\\", \\"ASYNCIO\\"] asyncio.run(main()) # Example of running a blocking task result = run_blocking_task(sum, [1, 2, 3, 4]) print(result) # 10 # Example of scheduling a task from another thread def thread_task(): future = schedule_task_from_thread(loop, async_process_data, [\\"one\\", \\"two\\"]) result = future.result() print(result) # [\\"ONE\\", \\"TWO\\"] thread_task()"},{"question":"As a data scientist, you have received a dataset consisting of information about customers\' preferences on various products. You are required to analyze this dataset using pandas\' categorical data type to save memory, improve performance, and ensure correct logical ordering. Write a Python function called `analyze_preferences` that performs the following tasks: 1. **Read the Data**: - The input will be a CSV file path containing the dataset (you can assume it\'s a well-formed CSV for simplicity). - The CSV contains columns: \'customer_id\', \'product\', \'rating\', and \'country\'. 2. **Convert Columns to Categorical**: - Convert \'product\' and \'country\' columns to categorical data types. - Ensure the \'rating\' column is also converted to categorical with a logical order: \'poor\', \'average\', \'good\', \'excellent\'. 3. **Analyze the Data**: - Calculate the frequency of each category in the \'product\' column. - Find the most common \'rating\' for each \'country\'. - Identify if there are any unused categories in the \'rating\' that need to be removed. 4. **Return the Results**: - Return a dictionary with the following structure: ``` { \\"product_frequencies\\": {product: frequency, ...}, \\"most_common_ratings_by_country\\": {country: most_common_rating, ...}, \\"unused_ratings\\": [list_of_unused_ratings] } ``` # Input: - `file_path` (string): Path to the CSV file containing the dataset. # Output: - A dictionary as specified above. # Constraints: - Use pandas\' categorical data type for memory efficiency and logical ordering. - Handle missing values appropriately during analysis. - Ensure the function is optimized for performance. # Example: Given the CSV file structure: ``` customer_id,product,rating,country 1,Phone,good,USA 2,Laptop,average,Canada 3,Tablet,poor,USA 4,Phone,excellent,Mexico ... ``` Output: ```python { \\"product_frequencies\\": {\\"Phone\\": 10, \\"Laptop\\": 5, \\"Tablet\\": 8}, \\"most_common_ratings_by_country\\": {\\"USA\\": \\"good\\", \\"Canada\\": \\"average\\", \\"Mexico\\": \\"excellent\\"}, \\"unused_ratings\\": [\\"poor\\"] } ``` # Note: - You need to handle reading from the CSV file within your function. - Ensure that the ratings are treated as ordered categorical data type. - Handle potential missing values in the dataset appropriately. ```python import pandas as pd from pandas.api.types import CategoricalDtype def analyze_preferences(file_path: str) -> dict: # Read the data df = pd.read_csv(file_path) # Convert columns to categorical df[\'product\'] = df[\'product\'].astype(\'category\') df[\'country\'] = df[\'country\'].astype(\'category\') # Define the rating categories rating_categories = [\'poor\', \'average\', \'good\', \'excellent\'] rating_dtype = CategoricalDtype(categories=rating_categories, ordered=True) df[\'rating\'] = df[\'rating\'].astype(rating_dtype) # Calculate product frequencies product_frequencies = df[\'product\'].value_counts().to_dict() # Find the most common rating by country most_common_ratings_by_country = df.groupby(\'country\')[\'rating\'].agg(lambda x: x.mode()[0]).to_dict() # Identify unused ratings unused_ratings = df[\'rating\'].cat.remove_unused_categories().cat.categories unused_ratings = [category for category in rating_categories if category not in unused_ratings] # Return the results result = { \\"product_frequencies\\": product_frequencies, \\"most_common_ratings_by_country\\": most_common_ratings_by_country, \\"unused_ratings\\": unused_ratings } return result ```","solution":"import pandas as pd from pandas.api.types import CategoricalDtype def analyze_preferences(file_path: str) -> dict: # Read the data df = pd.read_csv(file_path) # Convert columns to categorical df[\'product\'] = df[\'product\'].astype(\'category\') df[\'country\'] = df[\'country\'].astype(\'category\') # Define the rating categories rating_categories = [\'poor\', \'average\', \'good\', \'excellent\'] rating_dtype = CategoricalDtype(categories=rating_categories, ordered=True) df[\'rating\'] = df[\'rating\'].astype(rating_dtype) # Calculate product frequencies product_frequencies = df[\'product\'].value_counts().to_dict() # Find the most common rating by country most_common_ratings_by_country = df.groupby(\'country\')[\'rating\'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else None).to_dict() # Identify unused ratings unused_ratings = [category for category in rating_categories if category not in df[\'rating\'].cat.categories[df[\'rating\'].cat.codes[df[\'rating\'].notna()]]] # Return the results result = { \\"product_frequencies\\": product_frequencies, \\"most_common_ratings_by_country\\": most_common_ratings_by_country, \\"unused_ratings\\": unused_ratings } return result"},{"question":"You are tasked with creating a simple file management system. Your implementation should include functionalities to search for files, compare directories, and handle temporary files. Specifically, you need to implement the following functions: 1. **search_files(directory: str, pattern: str) -> List[str]** - **Input**: - `directory` (str): The path of the directory to search within. - `pattern` (str): The pattern to use for searching files (Unix filename pattern matching). - **Output**: - Returns a list of filenames (with their relative paths to the directory) that match the given pattern. - **Constraints**: - You must use the `fnmatch` module. 2. **compare_directories(dir1: str, dir2: str) -> Tuple[bool, List[str]]** - **Input**: - `dir1` (str): The path of the first directory. - `dir2` (str): The path of the second directory. - **Output**: - Returns a tuple where the first element is a boolean indicating if the two directories are identical, and the second element is a list of differences. - **Constraints**: - You must use the `filecmp` module\'s `dircmp` class. 3. **create_temp_file(content: str) -> str** - **Input**: - `content` (str): The content to write into the temporary file. - **Output**: - Returns the path of the created temporary file. - **Constraints**: - You must use the `tempfile` module. # Example ```python # Example usage of search_files print(search_files(\'/path/to/directory\', \'*.txt\')) # Output: [\'file1.txt\', \'subdir/file2.txt\'] # Example usage of compare_directories print(compare_directories(\'/path/to/dir1\', \'/path/to/dir2\')) # Output: (False, [\'file1.txt\', \'file2.txt\']) # Example usage of create_temp_file print(create_temp_file(\'Hello World\')) # Output: \'/tmp/tmpabcxyz\' ``` Implement the three functions as described, ensuring that you handle any potential edge cases and efficiently use the modules specified.","solution":"import os import fnmatch from typing import List, Tuple import filecmp import tempfile def search_files(directory: str, pattern: str) -> List[str]: Searches for files in the given directory that match the Unix filename pattern. Args: - directory: the path of the directory to search within. - pattern: the pattern to use for searching files. Returns: A list of filenames (with their relative paths to the directory) that match the given pattern. matches = [] for root, _, filenames in os.walk(directory): for filename in fnmatch.filter(filenames, pattern): relative_path = os.path.relpath(os.path.join(root, filename), directory) matches.append(relative_path) return matches def compare_directories(dir1: str, dir2: str) -> Tuple[bool, List[str]]: Compares two directories to check if they are identical. Args: - dir1: the path of the first directory. - dir2: the path of the second directory. Returns: A tuple where the first element is a boolean indicating if the two directories are identical, and the second element is a list of differences. dcmp = filecmp.dircmp(dir1, dir2) if dcmp.left_only or dcmp.right_only or dcmp.diff_files or dcmp.funny_files: differences = dcmp.left_only + dcmp.right_only + dcmp.diff_files + dcmp.funny_files return (False, differences) return (True, []) def create_temp_file(content: str) -> str: Creates a temporary file with the given content. Args: - content: The content to write into the temporary file. Returns: The path of the created temporary file. temp = tempfile.NamedTemporaryFile(delete=False) with temp: temp.write(content.encode(\'utf-8\')) return temp.name"},{"question":"# HTTP Client Implementation using `http.client` Objective Demonstrate your ability to use the `http.client` module to interact with web resources programmatically by implementing a function that fetches data from a specified HTTP endpoint and processes different HTTP responses appropriately. Task Write a Python function `fetch_data` that takes the following parameters: - `url`: A string representing the URL of the endpoint to fetch data from. - `method`: A string specifying the HTTP method to use (e.g., `\\"GET\\"`, `\\"POST\\"`). The function should: 1. Establish an HTTP connection to the given URL. 2. Send an HTTP request using the specified method. 3. Handle different HTTP response statuses: - If the response status is `200 OK`, return the response body as a string. - If the response status is `404 Not Found`, return `\\"Resource not found\\"`. - For any other response status, return `\\"Error: <status_code> <reason_phrase>\\"`. 4. Close the connection after the operation. Constraints - You must use the `http.client` module to establish the connection and send the request. - Assume the URL provided is valid and properly formatted. Example Usage ```python result = fetch_data(\\"http://example.com/api/data\\", \\"GET\\") print(result) # Output will depend on the response from the server ``` # Input Format - `url`: A string representing the URL of the endpoint (e.g., \\"http://example.com/api/data\\"). - `method`: A string specifying the HTTP method (e.g., \\"GET\\"). # Output Format - A string as described in the task requirements. # Implementation ```python import http.client from urllib.parse import urlparse def fetch_data(url, method): # Parse the URL parsed_url = urlparse(url) # Extract host and path host = parsed_url.netloc path = parsed_url.path or \\"/\\" # Create an HTTP connection connection = http.client.HTTPConnection(host) try: # Send the request connection.request(method, path) # Get the response response = connection.getresponse() # Handle response based on status if response.status == 200: return response.read().decode() elif response.status == 404: return \\"Resource not found\\" else: return f\\"Error: {response.status} {response.reason}\\" finally: # Close the connection connection.close() ``` Ensure your function handles connections and responses correctly and gracefully handles different HTTP status responses as specified.","solution":"import http.client from urllib.parse import urlparse def fetch_data(url, method): # Parse the URL parsed_url = urlparse(url) # Extract host and path host = parsed_url.netloc path = parsed_url.path or \\"/\\" # Create an HTTP connection connection = http.client.HTTPConnection(host) try: # Send the request connection.request(method, path) # Get the response response = connection.getresponse() # Handle response based on status if response.status == 200: return response.read().decode() elif response.status == 404: return \\"Resource not found\\" else: return f\\"Error: {response.status} {response.reason}\\" finally: # Close the connection connection.close()"},{"question":"# Task Create a custom autograd `Function` in PyTorch to implement a novel mathematical operation. You need to define both the forward and backward methods for this function. Finally, you should use PyTorch\'s gradient checking functionality to verify the implementation and profile the performance of gradient computation for this custom function. # Problem Statement 1. **Custom Operation**: Implement a custom autograd `Function` named `CustomSine` that computes the sine of the input value while also adding a small perturbation `epsilon` (for numerical stability). Define the forward pass to compute: [ text{output} = sin(text{input}) + epsilon ] And the corresponding backward pass. 2. **Gradient Check**: Utilize `torch.autograd.gradcheck` to verify the correctness of the backward method. 3. **Profiling**: Profile the performance of both forward and backward passes using PyTorch\'s profiler for various input sizes. # Instructions 1. **Custom Function Implementation**: - Define a class `CustomSine` inheriting from `torch.autograd.Function`. - Implement the `forward(ctx, input, epsilon)` method. - Implement the `backward(ctx, grad_output)` method to compute the gradient correctly. 2. **Gradient Checking**: - Test the gradient implementation using `torch.autograd.gradcheck`. 3. **Profiling**: - Profile the forward and backward computation times for input tensors of varying sizes (e.g., 100, 1000, 10000 elements). # Example Usage ```python import torch from torch.autograd import Function class CustomSine(Function): @staticmethod def forward(ctx, input, epsilon): ctx.save_for_backward(input) return torch.sin(input) + epsilon @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * torch.cos(input) return grad_input, None # Usage input = torch.randn(10, requires_grad=True) epsilon = 1e-6 output = CustomSine.apply(input, epsilon) loss = output.sum() loss.backward() # Gradient Check torch.autograd.gradcheck(CustomSine.apply, (input, epsilon), eps=1e-6) # Profiling with torch.autograd.profiler.profile() as prof: output = CustomSine.apply(input, epsilon) output.backward(torch.ones_like(input)) print(prof.key_averages().table(sort_by=\\"cpu_time_total\\")) ``` # Deliverable Submit code for the following: 1. Implementation of `CustomSine` with specified forward and backward methods. 2. Code that performs gradient checking on `CustomSine`. 3. Code to profile the performance of `CustomSine` for varying input sizes along with a brief analysis of the profiling results.","solution":"import torch from torch.autograd import Function class CustomSine(Function): @staticmethod def forward(ctx, input, epsilon): ctx.save_for_backward(input) return torch.sin(input) + epsilon @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * torch.cos(input) return grad_input, None # Example Usage input = torch.randn(10, requires_grad=True) epsilon = 1e-6 output = CustomSine.apply(input, epsilon) loss = output.sum() loss.backward() # Gradient Check def gradient_check(): input = torch.randn(10, dtype=torch.double, requires_grad=True) epsilon = torch.tensor(1e-6, dtype=torch.double) return torch.autograd.gradcheck(CustomSine.apply, (input, epsilon), eps=1e-6) # Profiling def profile_custom_sine(input_sizes): for size in input_sizes: input = torch.randn(size, requires_grad=True) epsilon = 1e-6 with torch.autograd.profiler.profile() as prof: output = CustomSine.apply(input, epsilon) output.backward(torch.ones_like(input)) print(f\\"Profile for input size {size}:n{prof.key_averages().table(sort_by=\'cpu_time_total\')}\\") # Running the gradient check gradient_check() # Profiling for different input sizes profile_custom_sine([100, 1000, 10000])"},{"question":"**Question: Implement and Use a Persistent Storage System for a Contact Book** You are tasked with implementing a simple persistent storage system for a contact book using Python\'s `shelve` module. The contact book should persistently store user records with names (keys) and details (values). **Requirements:** 1. Implement a class `ContactBook` that - Initializes with a filename. - Has the following methods: - `add_contact(name: str, details: dict) -> None`: Adds a contact to the book. If the contact name already exists, it should update the contact details. - `get_contact(name: str) -> dict`: Retrieves the contact details by name. Raises a `KeyError` if the contact does not exist. - `delete_contact(name: str) -> None`: Deletes the contact by name. Raises a `KeyError` if the contact does not exist. - `list_contacts() -> list`: Returns a list of all contact names. 2. Ensure that your implementation handles the following scenarios: - Opening the shelf. - Safely closing the shelf using a context manager. - **Bonus:** Use `writeback=True` to show mutation of contact details. **Example usage:** ```python contact_book = ContactBook(\'contacts.db\') # Adding contacts contact_book.add_contact(\'John Doe\', {\'email\': \'john@example.com\', \'phone\': \'123-456-7890\'}) contact_book.add_contact(\'Jane Doe\', {\'email\': \'jane@example.com\', \'phone\': \'098-765-4321\'}) # Retrieving a contact contact = contact_book.get_contact(\'John Doe\') print(contact) # Output: {\'email\': \'john@example.com\', \'phone\': \'123-456-7890\'} # Updating a contact contact_book.add_contact(\'John Doe\', {\'email\': \'john_new@example.com\', \'phone\': \'123-456-7890\'}) # Listing contacts contacts = contact_book.list_contacts() print(contacts) # Output: [\'John Doe\', \'Jane Doe\'] # Deleting a contact contact_book.delete_contact(\'Jane Doe\') # Attempting to retrieve a deleted contact (should raise KeyError) try: contact_book.get_contact(\'Jane Doe\') except KeyError: print(\\"Contact not found\\") ``` **Constraints:** - Names are unique and cannot be empty. - Contact details must be a dictionary containing key-value pairs of contact information (like email, phone, etc.). The implementation should be efficient in terms of memory usage and handle the persistence and retrieval of data effectively.","solution":"import shelve class ContactBook: def __init__(self, filename): self.filename = filename def add_contact(self, name, details): with shelve.open(self.filename, writeback=True) as db: db[name] = details def get_contact(self, name): with shelve.open(self.filename) as db: if name in db: return db[name] else: raise KeyError(f\\"Contact \'{name}\' not found\\") def delete_contact(self, name): with shelve.open(self.filename, writeback=True) as db: if name in db: del db[name] else: raise KeyError(f\\"Contact \'{name}\' not found\\") def list_contacts(self): with shelve.open(self.filename) as db: return list(db.keys())"},{"question":"# PyTorch Custom CUDA Operation and Integration In this coding assessment, you are required to implement a custom PyTorch CUDA operation and integrate it as a layer within a neural network. This exercise will test your understanding of PyTorch at a deep level, including handling tensors, working with CUDA, and integrating custom operations into a model. Task 1. **Implement a Custom CUDA Function**: - Create a custom PyTorch CUDA operation named `CustomSquare` that squares each element in the input tensor. - Ensure your function correctly handles tensor shapes and CUDA contexts. 2. **Integrate the Custom Operation in a PyTorch Model**: - Create a custom PyTorch layer `SquareLayer` utilizing the `CustomSquare` function. - Integrate `SquareLayer` into a simple neural network and demonstrate forward and backward passes. Requirements 1. **CustomSquare Function**: - Input: A PyTorch tensor `x` of any shape. - Output: A tensor with each element squared. - Constraints: Must handle CUDA tensors. ```python class CustomSquare(Function): @staticmethod def forward(ctx, x): # Implement forward pass using CUDA kernels, # Ensure it works for tensors on both CPU and CUDA pass @staticmethod def backward(ctx, grad_output): # Implement backward pass to handle gradients pass ``` 2. **SquareLayer Class**: - Integrate the `CustomSquare` operation as a layer in a model. class SquareLayer(torch.nn.Module): def __init__(self): super(SquareLayer, self).__init__() def forward(self, x): # Apply CustomSquare operation pass 3. **Neural Network Integration**: - Design a simple neural network that includes the `SquareLayer`. - Perform a forward pass with a sample input. - Ensure your model can perform backpropagation. Performance Requirements - Ensure your custom function and layer work efficiently with CUDA tensors. - Your implementation should pass both forward and backward operations correctly. ```python # Sample Simple Neural Network including SquareLayer class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layer1 = nn.Linear(10, 10) self.square = SquareLayer() self.layer2 = nn.Linear(10, 1) def forward(self, x): x = self.layer1(x) x = self.square(x) x = self.layer2(x) return x # Example usage model = SimpleModel() x = torch.randn(5, 10, device=\'cuda\') output = model(x) output.mean().backward() ``` Ensure your solution is efficient, properly utilizes CUDA, and integrates seamlessly within a PyTorch model. Submission - Submit the complete implementation of `CustomSquare` and `SquareLayer`. - Provide a test script demonstrating the forward and backward passes of the integrated model.","solution":"import torch from torch.autograd import Function import torch.nn as nn class CustomSquare(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return x ** 2 @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_input = grad_output * 2 * x return grad_input class SquareLayer(nn.Module): def __init__(self): super(SquareLayer, self).__init__() def forward(self, x): return CustomSquare.apply(x) # Sample Simple Neural Network including SquareLayer class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layer1 = nn.Linear(10, 10) self.square = SquareLayer() self.layer2 = nn.Linear(10, 1) def forward(self, x): x = self.layer1(x) x = self.square(x) x = self.layer2(x) return x"},{"question":"<|Analysis Begin|> The `curses.ascii` module provides utility functions and constants for dealing with ASCII characters, including control characters, and testing membership in various ASCII character classes. The primary functionalities are: 1. **Constants for ASCII Control Characters**: These constants represent specific control characters such as NUL, SOH, ETX, etc. Each constant has a name and a specific meaning. 2. **Character Classification Functions**: These functions test if a given character belongs to a specific ASCII class. Examples include `isalnum`, `isalpha`, `isascii`, `isblank`, `iscntrl`, `isdigit`, `isgraph`, `islower`, `isprint`, `ispunct`, `isspace`, `isupper`, `isxdigit`, `isctrl`, and `ismeta`. They return boolean values indicating whether the character (given as an integer or single-character string) fits the specific class criteria. 3. **Character Conversion Functions**: These functions perform bitwise operations on ASCII characters to return specific values. Functions include `ascii`, `ctrl`, and `alt`. 4. **String Representation Functions**: The `unctrl` function provides a human-readable string representation of an ASCII character, particularly for control characters. 5. **Character Mnemonics Array**: `controlnames` is a 33-element array containing the ASCII mnemonics for control characters from 0 to 0x1f plus the space character. Given this information, we can craft a question that requires students to utilize these functions and constants provided in the `curses.ascii` module. <|Analysis End|> <|Question Begin|> # ASCII Character Analyzer Your task is to implement a Python function `analyze_ascii_string(s: str) -> dict` that takes a string `s` as input and performs an analysis of each character in the string using the `curses.ascii` module. The function should return a dictionary where the keys are the characters in `s` and the values are dictionaries describing various properties of the characters. Each value dictionary should contain the following keys and values: - `\\"isalnum\\"`: A boolean indicating if the character is alphanumeric. - `\\"isalpha\\"`: A boolean indicating if the character is alphabetic. - `\\"isascii\\"`: A boolean indicating if the character is an ASCII character. - `\\"isblank\\"`: A boolean indicating if the character is a blank character (space or horizontal tab). - `\\"iscntrl\\"`: A boolean indicating if the character is a control character. - `\\"isdigit\\"`: A boolean indicating if the character is a digit. - `\\"isgraph\\"`: A boolean indicating if the character has a graphical representation. - `\\"islower\\"`: A boolean indicating if the character is lowercase. - `\\"isprint\\"`: A boolean indicating if the character is printable. - `\\"ispunct\\"`: A boolean indicating if the character is punctuation. - `\\"isspace\\"`: A boolean indicating if the character is a whitespace character. - `\\"isupper\\"`: A boolean indicating if the character is uppercase. - `\\"isxdigit\\"`: A boolean indicating if the character is a hexadecimal digit. - `\\"isctrl\\"`: A boolean indicating if the character is a control character (ordinal values 0 to 31). - `\\"ismeta\\"`: A boolean indicating if the character is a non-ASCII character (ordinal values 0x80 and above). - `\\"unctrl\\"`: The unctrl string representation of the character. - `\\"ascii\\"`: The ASCII value corresponding to the low 7 bits of the character. - `\\"ctrl\\"`: The control character corresponding to the character. - `\\"alt\\"`: The 8-bit character corresponding to the character. # Input and Output Format **Input:** - `s`: A string consisting of ASCII characters. **Output:** - A dictionary where keys are characters from `s` and values are dictionaries with the properties listed above. # Example ```python import curses.ascii def analyze_ascii_string(s: str) -> dict: result = {} for char in s: char_properties = { \\"isalnum\\": curses.ascii.isalnum(char), \\"isalpha\\": curses.ascii.isalpha(char), \\"isascii\\": curses.ascii.isascii(char), \\"isblank\\": curses.ascii.isblank(char), \\"iscntrl\\": curses.ascii.iscntrl(char), \\"isdigit\\": curses.ascii.isdigit(char), \\"isgraph\\": curses.ascii.isgraph(char), \\"islower\\": curses.ascii.islower(char), \\"isprint\\": curses.ascii.isprint(char), \\"ispunct\\": curses.ascii.ispunct(char), \\"isspace\\": curses.ascii.isspace(char), \\"isupper\\": curses.ascii.isupper(char), \\"isxdigit\\": curses.ascii.isxdigit(char), \\"isctrl\\": curses.ascii.isctrl(char), \\"ismeta\\": curses.ascii.ismeta(char), \\"unctrl\\": curses.ascii.unctrl(char), \\"ascii\\": curses.ascii.ascii(char), \\"ctrl\\": curses.ascii.ctrl(char), \\"alt\\": curses.ascii.alt(char), } result[char] = char_properties return result # Example usage: s = \\"A~x07\\" print(analyze_ascii_string(s)) ``` This code should output something like: ```python { \'A\': {\'isalnum\': True, \'isalpha\': True, \'isascii\': True, \'isblank\': False, \'iscntrl\': False, \'isdigit\': False, \'isgraph\': True, \'islower\': False, \'isprint\': True, \'ispunct\': False, \'isspace\': False, \'isupper\': True, \'isxdigit\': False, \'isctrl\': False, \'ismeta\': False, \'unctrl\': \'A\', \'ascii\': 65, \'ctrl\': \'x01\', \'alt\': \'Ä\'}, \'~\': {\'isalnum\': False, \'isalpha\': False, \'isascii\': True, \'isblank\': False, \'iscntrl\': False, \'isdigit\': False, \'isgraph\': True, \'islower\': False, \'isprint\': True, \'ispunct\': True, \'isspace\': False, \'isupper\': False, \'isxdigit\': False, \'isctrl\': False, \'ismeta\': False, \'unctrl\': \'~\', \'ascii\': 126, \'ctrl\': \'x1e\', \'alt\': \'xfe\'}, \'x07\': {\'isalnum\': False, \'isalpha\': False, \'isascii\': True, \'isblank\': False, \'iscntrl\': True, \'isdigit\': False, \'isgraph\': False, \'islower\': False, \'isprint\': False, \'ispunct\': False, \'isspace\': False, \'isupper\': False, \'isxdigit\': False, \'isctrl\': False, \'ismeta\': False, \'unctrl\': \'^G\', \'ascii\': 7, \'ctrl\': \'x07\', \'alt\': \'x87\'} } ``` Ensure your solution utilizes the `curses.ascii` module effectively and that your implementation properly captures all the properties for each character in the string.","solution":"import curses.ascii def analyze_ascii_string(s: str) -> dict: result = {} for char in s: char_properties = { \\"isalnum\\": curses.ascii.isalnum(char), \\"isalpha\\": curses.ascii.isalpha(char), \\"isascii\\": curses.ascii.isascii(char), \\"isblank\\": curses.ascii.isblank(char), \\"iscntrl\\": curses.ascii.iscntrl(char), \\"isdigit\\": curses.ascii.isdigit(char), \\"isgraph\\": curses.ascii.isgraph(char), \\"islower\\": curses.ascii.islower(char), \\"isprint\\": curses.ascii.isprint(char), \\"ispunct\\": curses.ascii.ispunct(char), \\"isspace\\": curses.ascii.isspace(char), \\"isupper\\": curses.ascii.isupper(char), \\"isxdigit\\": curses.ascii.isxdigit(char), \\"isctrl\\": curses.ascii.isctrl(char), \\"ismeta\\": curses.ascii.ismeta(char), \\"unctrl\\": curses.ascii.unctrl(char), \\"ascii\\": curses.ascii.ascii(char), \\"ctrl\\": curses.ascii.ctrl(char), \\"alt\\": curses.ascii.alt(char), } result[char] = char_properties return result"},{"question":"**Problem Statement:** Write a Python function `check_password_expiry(username: str) -> int` using the `spwd` module, to determine the number of days remaining until a user\'s password expires. If the user\'s password has already expired, return the negative number of days since expiration. If the user does not exist, raise a `KeyError`. If the user does not have the required permissions to access the shadow password database, raise a `PermissionError`. # Input - `username` (str): the login name of the user whose password expiry information is to be checked. # Output - (int): the number of days until the password expires. Negative if expired. # Constraints - The function should work on Unix systems with appropriate permissions. - Consider today\'s date as 1970-01-01 for the purpose of this problem (since Unix epoch time is being used). - The function should handle any potential exceptions arising from the use of the `spwd` module appropriately and in accordance with the documentation provided. # Example ```python import spwd def check_password_expiry(username: str) -> int: # Implement this function result = check_password_expiry(\'user123\') print(result) # Example output: -10 ``` Note: The actual number of days output will depend on the user\'s password data retrieved from the `spwd` module.","solution":"import spwd import time import datetime def check_password_expiry(username: str) -> int: Returns the number of days until the user\'s password expires. If the password has expired, returns the negative number of days since expiration. try: shadow_info = spwd.getspnam(username) except KeyError: raise KeyError(f\\"User \'{username}\' does not exist.\\") except PermissionError: raise PermissionError(\\"Permission denied to access the shadow password database.\\") # Get today\'s date (considering as the Unix epoch start date - 1970-01-01) today_epoch_days = int(time.time() // (24 * 3600)) # Date when the password was last changed, number of days since 1970-01-01. password_changed_epoch_days = shadow_info.sp_lstchg # Maximum number of days the password is valid for max_valid_days = shadow_info.sp_max if max_valid_days == -1: # This means the password never expires. return float(\'inf\') # Calculate the expiry date in epoch days expiry_epoch_days = password_changed_epoch_days + max_valid_days # Calculate the difference in days between today and the expiry date days_remaining = expiry_epoch_days - today_epoch_days return days_remaining"},{"question":"# Advanced Email Handling with the `email` Package Objective Implement a Python function that takes the path to a file containing a complete MIME email message, parses it, and extracts all the attachments into a specified directory. The function should handle different content types and save each attachment with an appropriate file extension. Detailed Requirements 1. **Function Definition:** ```python def extract_email_attachments(msgfile: str, directory: str) -> None: pass ``` 2. **Parameters:** - `msgfile` (str): The path to the file containing the MIME email message. - `directory` (str): The directory where attachments should be saved. 3. **Functionality:** - The function should read the email message from `msgfile`. - It should parse the MIME message and iterate over its parts to identify attachments. - Each attachment should be written to the specified `directory`. - Use appropriate file extensions for the attachments based on their MIME types. - If an attachment does not have a filename, generate a unique name for it. 4. **Error Handling:** - The function should handle common errors gracefully (e.g., file not found, read/write errors). - If the specified directory does not exist, the function should create it. 5. **Example:** Suppose the email message contains two attachments: - A text file (`example.txt`) - An image file (`image.jpg`) If `msgfile` is `email_message.eml` and `directory` is `attachments`, the function should save: - `attachments/example.txt` - `attachments/image.jpg` 6. **Constraints:** - You should not use any external libraries other than those in the standard Python library. - The solution should handle common MIME types (e.g., text/plain, image/jpeg, application/pdf). 7. **Notes:** - Review the provided documentation examples related to handling MIME messages and attachments. - Test your function with various MIME messages containing different types of attachments. Hints - Use the `email` package to parse the MIME message. - Use the `mimetypes` module to guess the file extension based on the MIME type.","solution":"import os import mimetypes import email from email import policy from email.parser import BytesParser def extract_email_attachments(msgfile: str, directory: str) -> None: # Ensure the directory exists if not os.path.exists(directory): os.makedirs(directory) # Read and parse the email message with open(msgfile, \'rb\') as f: msg = BytesParser(policy=policy.default).parse(f) # Iterate through the message parts to find attachments for part in msg.iter_attachments(): # Check if the part is an attachment content_disposition = part.get(\'Content-Disposition\', None) if content_disposition: dispositions = content_disposition.strip().split(\\";\\") if dispositions[0].lower() == \'attachment\': # Try to get the filename, if not present create one filename = part.get_filename() if not filename: ext = mimetypes.guess_extension(part.get_content_type()) filename = f\'attachment{ext or \\".bin\\"}\' # Save the attachment filepath = os.path.join(directory, filename) with open(filepath, \'wb\') as f: f.write(part.get_payload(decode=True))"},{"question":"**Objective:** Design and implement a Python function that reads an XML file, processes its content using SAX parsing, and extracts specific information. The function should handle possible errors gracefully and observe security best practices by not processing external entities. **Question:** You are provided with an XML file containing information about books. Each book has the following structure: ```xml <book> <title>The Title of the Book</title> <author>The Author\'s Name</author> <year>2020</year> </book> ``` Your task is to write a function `parse_books(xml_file: str) -> List[Dict[str, str]]` that parses the given XML file and extracts the information about each book into a list of dictionaries. Each dictionary should have keys: \'title\', \'author\', and \'year\', corresponding to the nested elements in the XML. # Constraints: 1. Implement a custom `ContentHandler` subclass to handle the parsing events. 2. Ensure that your parser does not process external entities to prevent security vulnerabilities. 3. Handle any parsing errors or exceptions gracefully by logging an appropriate error message. 4. Assume the XML file is well-formed but handle other edge cases such as missing elements. # Example: If the XML file contains the following data: ```xml <library> <book> <title>Book 1 Title</title> <author>Author 1</author> <year>2010</year> </book> <book> <title>Book 2 Title</title> <author>Author 2</author> <year>2020</year> </book> </library> ``` Your function should return: ```python [ {\\"title\\": \\"Book 1 Title\\", \\"author\\": \\"Author 1\\", \\"year\\": \\"2010\\"}, {\\"title\\": \\"Book 2 Title\\", \\"author\\": \\"Author 2\\", \\"year\\": \\"2020\\"}, ] ``` # Requirements: - The XML file path will always be valid and accessible when the function is called. - Use the `xml.sax` package to implement SAX parsing. - Do not use any external libraries other than the standard library for XML parsing. # Function Signature: ```python from typing import List, Dict def parse_books(xml_file: str) -> List[Dict[str, str]]: pass ``` # Implementation Hints: - Define a custom `ContentHandler` class to process `startElement`, `characters`, and `endElement` events. - Use a list to collect book dictionaries and a temporary dictionary to store the current book\'s data. - Ensure proper handling of security features by configuring the parser options appropriately.","solution":"import xml.sax from typing import List, Dict class BookHandler(xml.sax.ContentHandler): def __init__(self): self.books = [] self.current_data = \\"\\" self.current_book = {} def startElement(self, name, attrs): self.current_data = name if name == \\"book\\": self.current_book = {\\"title\\": \\"\\", \\"author\\": \\"\\", \\"year\\": \\"\\"} def endElement(self, name): if name == \\"book\\": self.books.append(self.current_book) self.current_book = {} self.current_data = \\"\\" def characters(self, content): if self.current_data in self.current_book: self.current_book[self.current_data] += content.strip() def parse_books(xml_file: str) -> List[Dict[str, str]]: handler = BookHandler() parser = xml.sax.make_parser() # Disable processing of external entities for security measures parser.setFeature(xml.sax.handler.feature_external_ges, False) parser.setContentHandler(handler) try: with open(xml_file, \\"r\\") as file: parser.parse(file) except Exception as e: print(f\\"Error while parsing the XML file: {e}\\") return [] return handler.books"},{"question":"# Question: Advanced File System Operations with `pathlib` You are required to implement a Python function utilizing the `pathlib` module that will perform the following tasks on a given directory: 1. Check if a given directory exists. If it does not exist, create it along with any necessary parent directories. 2. List all the files (excluding directories) within the given directory and all of its subdirectories, recursively. 3. For each file found in step 2, create a symbolic link in a newly created directory named `symlinks` located at the same level as the given directory. The symbolic link should mirror the structure of the original files\' hierarchy. 4. Return the total count of files found and the total number of symbolic links created. Function Signature: ```python def manage_filesystem(base_dir: str) -> tuple: pass ``` Input: - `base_dir` (str): A string representing the path to the base directory to be processed. Output: - A tuple of two integers `(file_count, symlink_count)`: - `file_count` is the total number of files found in the directory and its subdirectories. - `symlink_count` is the total number of symbolic links created. Constraints: - The function should handle creating paths and directories efficiently. - Proper error handling should be added where necessary, particularly in file operations. Example: Assume a directory structure as follows: ``` project/ file1.txt dir1/ file2.txt dir2/ file3.txt ``` Calling `manage_filesystem(\'project\')` should: 1. Ensure that `project` directory exists. 2. List all files: `file1.txt`, `dir1/file2.txt`, and `dir1/dir2/file3.txt`. 3. Create a directory `project_symlinks` at the same level as `project` and create symbolic links inside this directory mirroring the structure of the original files. 4. Return `(3, 3)` since there are 3 files and 3 corresponding symbolic links created. # Function Implementation: Ensure that your function is efficient and handles edge cases such as: - Empty directories. - Files with no read permissions. - Existing symbolic links.","solution":"import pathlib def manage_filesystem(base_dir: str) -> tuple: base_path = pathlib.Path(base_dir) symlinks_path = base_path.parent / f\\"{base_path.name}_symlinks\\" # Ensure base_dir exists base_path.mkdir(parents=True, exist_ok=True) # List all files file_paths = list(base_path.rglob(\'*\')) file_paths = [p for p in file_paths if p.is_file()] # Create symlink directory symlinks_path.mkdir(parents=True, exist_ok=True) # Create symlinks symlink_count = 0 for file_path in file_paths: relative_path = file_path.relative_to(base_path) target_symlink = symlinks_path / relative_path target_symlink.parent.mkdir(parents=True, exist_ok=True) # Create missing directories for case when nested files present try: target_symlink.symlink_to(file_path) symlink_count += 1 except FileExistsError: # If the symlink already exists, skip creating it pass return (len(file_paths), symlink_count)"},{"question":"**Coding Assessment Question** # Objective: Your task is to implement a multi-class classification using the OneVsRestClassifier strategy provided by scikit-learn. This will involve creating a classifier for a given dataset, training the classifier, and then using it to make predictions. # Problem Statement: You are provided with the Iris dataset which is a commonly used dataset for classification tasks. Your task is to: 1. Load the Iris dataset. 2. Split the dataset into training and testing sets. 3. Implement a OneVsRestClassifier using a `LinearSVC` as the base estimator. 4. Train the classifier on the training data. 5. Make predictions on the testing data. 6. Evaluate the performance of your classifier using accuracy, precision, recall, and F1-score. # Requirements: 1. Function Name: `multi_class_classification` 2. Expected Input: None (the function should handle loading the dataset internally). 3. Expected Output: A dictionary with the following keys and their respective values: - \'accuracy\' - \'precision\' - \'recall\' - \'f1_score\' # Constraints: 1. You should use the `OneVsRestClassifier` from scikit-learn. 2. You should use `LinearSVC` as the base estimator. 3. You must use scikit-learn’s `train_test_split` method for splitting the data. 4. You must use scikit-learn\'s `classification_report`\'s output to get accuracy, precision, recall, and F1-score. # Implementation: Here is the skeleton code to help you get started: ```python import numpy as np from sklearn import datasets from sklearn.svm import LinearSVC from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier def multi_class_classification(): # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the OneVsRestClassifier with LinearSVC base estimator ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=42)) # Train the classifier ovr_classifier.fit(X_train, y_train) # Make predictions on the testing data y_pred = ovr_classifier.predict(X_test) # Evaluate the performance report = classification_report(y_test, y_pred, output_dict=True) # Extract the required metrics evaluation_metrics = { \'accuracy\': report[\'accuracy\'], \'precision\': report[\'weighted avg\'][\'precision\'], \'recall\': report[\'weighted avg\'][\'recall\'], \'f1_score\': report[\'weighted avg\'][\'f1-score\'] } return evaluation_metrics # You can test your function here if __name__ == \\"__main__\\": print(multi_class_classification()) ``` # Notes: 1. The `classification_report` function provides a detailed analysis of the classifier\'s performance, and the \'weighted avg\' option gives a comprehensive view of precision, recall, and F1-score, especially in a multi-class setting. 2. The `random_state` parameter ensures that your results are reproducible. # Evaluation: Your solution will be evaluated based on the correctness of the code, the ability to load and split the dataset correctly, the implementation of the OneVsRestClassifier, and the correctness of the performance metrics.","solution":"import numpy as np from sklearn import datasets from sklearn.svm import LinearSVC from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split from sklearn.multiclass import OneVsRestClassifier def multi_class_classification(): # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the OneVsRestClassifier with LinearSVC base estimator ovr_classifier = OneVsRestClassifier(LinearSVC(random_state=42)) # Train the classifier ovr_classifier.fit(X_train, y_train) # Make predictions on the testing data y_pred = ovr_classifier.predict(X_test) # Evaluate the performance report = classification_report(y_test, y_pred, output_dict=True) # Extract the required metrics evaluation_metrics = { \'accuracy\': report[\'accuracy\'], \'precision\': report[\'weighted avg\'][\'precision\'], \'recall\': report[\'weighted avg\'][\'recall\'], \'f1_score\': report[\'weighted avg\'][\'f1-score\'] } return evaluation_metrics"},{"question":"# Coding Challenge: Handling Text and Binary Data in Python 2 and 3 **Problem Description:** You are given a function signature `read_and_process_file(file_path: str) -> List[str]` that reads a file, processes its contents, and returns a list of processed strings. The function must be compatible with both Python 2.7 and Python 3. The function should adhere to the following specifications: 1. The file contains mixed text data (ASCII and Unicode strings). 2. Text data should be read and processed as Unicode strings, while any binary data should be encoded appropriately. 3. You MUST use the `io` module to handle file operations. 4. The function should cater to possible version differences in file handling and type operations between Python 2 and Python 3. 5. The function should ensure any string literals used are appropriately marked as Unicode or binary literals where applicable. **Input:** - `file_path` (str): The path to the input file. **Output:** - List[str]: A list of processed strings. **Constraints:** - The input file will not exceed 10 MB. - The function should be optimized for clarity and performance. **Example:** Assume the file at `file_path` contains the following content: ``` Hello world! Привет мир! Binary data: x89PNGrnx1anx00x00x00rIHDRx00x00x00x01 ``` The function should return a list of clean text lines and handle binary data appropriately. ```python def read_and_process_file(file_path): import io import sys if sys.version_info[0] < 3: import codecs open_func = codecs.open else: open_func = io.open processed_lines = [] with open_func(file_path, \'r\', encoding=\'utf-8\') as f: for line in f: clean_line = line.strip() processed_lines.append(clean_line) return processed_lines # Example usage file_path = \'example.txt\' print(read_and_process_file(file_path)) ``` **Note:** Ensure that your function optimally handles the differences between binary and text data and respects compatibility between Python 2 and Python 3.","solution":"def read_and_process_file(file_path): import io import sys if sys.version_info[0] < 3: import codecs open_func = codecs.open else: open_func = io.open processed_lines = [] with open_func(file_path, \'r\', encoding=\'utf-8\') as f: for line in f: clean_line = line.strip() processed_lines.append(clean_line) return processed_lines"},{"question":"Custom Heatmap Visualization Objective: Create a heatmap using the Seaborn library, demonstrating your understanding of customizing visualizations. Task: Given a dataset of students\' scores in various subjects over different terms, you are required to: 1. Load and manipulate the dataset to create a summary table. 2. Generate a heatmap from the summarized data. 3. Customize the heatmap with annotations, a specific colormap, borders, and formatted labels. Dataset: The dataset `students_scores.csv` contains the following columns: - `Student`: Name of the student. - `Subject`: Name of the subject (e.g., Math, Science, English). - `Term`: Term number (e.g., Term 1, Term 2, Term 3). - `Score`: Score obtained by the student in that subject during that term. Instructions: 1. Load the dataset into a DataFrame. 2. Create a pivot table with `Subject` as the index, `Term` as the columns, and the average `Score` for the values. 3. Generate a heatmap from this pivot table. 4. Add annotations to the heatmap with scores rounded to one decimal place. 5. Use the `coolwarm` colormap. 6. Add borders between cells. 7. Set the colormap normalization such that the minimum value corresponds to 0 and the maximum value corresponds to 100. 8. Customize the axis labels to be empty and move the x-axis labels to the top. Constraints: 1. You must use the Seaborn library for visualizations. 2. Ensure your code is efficient and clean. Expected Input: - The file `students_scores.csv`. Expected Output: - A seaborn heatmap plot with all the customizations mentioned in the instructions. Example of `students_scores.csv`: ``` Student,Subject,Term,Score Alice,Math,Term 1,85 Alice,Math,Term 2,90 ... John,Science,Term 3,75 ``` Submission: Please submit your Python code implementing the above instructions.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_custom_heatmap(filepath): # Load the dataset df = pd.read_csv(filepath) # Create a pivot table pivot_table = df.pivot_table(index=\'Subject\', columns=\'Term\', values=\'Score\', aggfunc=\'mean\') # Initialize the matplotlib figure plt.figure(figsize=(10, 8)) # Generate the heatmap with customizations heatmap = sns.heatmap( pivot_table, annot=True, fmt=\\".1f\\", cmap=\'coolwarm\', linewidths=.5, linecolor=\'black\', cbar_kws={\'label\': \'Average Score\'}, vmin=0, vmax=100 ) # Customizing the axis labels heatmap.set_xlabel(\'\') heatmap.set_ylabel(\'\') heatmap.xaxis.tick_top() # Move x-axis labels to top # Show the plot plt.show()"}]'),I={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:z,isLoading:!1}},computed:{filteredPoems(){const i=this.searchQuery.trim().toLowerCase();return i?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(i)||e.solution&&e.solution.toLowerCase().includes(i)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(i=>setTimeout(i,1e3)),this.visibleCount+=4,this.isLoading=!1}}},F={class:"search-container"},D={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],L={key:0},O={key:1};function j(i,e,l,m,n,r){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",F,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>n.searchQuery=o),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>n.searchQuery="")}," ✕ ")):d("",!0)]),t("div",D,[(a(!0),s(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[n.isLoading?(a(),s("span",O,"Loading...")):(a(),s("span",L,"See more"))],8,q)):d("",!0)])}const M=p(I,[["render",j],["__scopeId","data-v-f8048490"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/49.md","filePath":"chatai/49.md"}'),N={name:"chatai/49.md"},H=Object.assign(N,{setup(i){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,H as default};
