import{_ as p,o as a,c as i,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,s,r){return a(),i("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-e49c5bae"]]),A=JSON.parse('[{"question":"**Problem Statement**: You are tasked with managing configuration settings for a macOS application. These configurations need to be stored in a property list (plist) file and read back when needed. The configuration includes various data types such as strings, lists, integers, floats, booleans, dictionaries, and datetime objects. Write a Python function `manage_configurations(action, config=None, filepath=None)` that performs the following operations based on the `action` parameter: 1. **\\"save\\"**: Saves the given configuration dictionary `config` to a plist file specified by `filepath`. 2. **\\"load\\"**: Loads and returns the configuration from the plist file specified by `filepath`. **Function Signature**: ```python import plistlib from datetime import datetime from typing import Union, Dict, Any def manage_configurations(action: str, config: Union[Dict[str, Any], None] = None, filepath: Union[str, None] = None) -> Union[Dict[str, Any], None]: pass ``` **Parameters**: - `action` (str): A string that can be either \\"save\\" or \\"load\\". - `config` (dict): A dictionary containing configuration settings (only needed for \\"save\\" action). - `filepath` (str): Path to the plist file (needed for both actions). **Returns**: - For \\"load\\" action, return the configuration dictionary loaded from the plist file. - For \\"save\\" action, return `None`. **Constraints**: - If `action` is \\"save\\", `config` and `filepath` must be provided. - If `action` is \\"load\\", `filepath` must be provided. - The `config` dictionary can contain the following types of values: strings, integers, floats, booleans, lists, dictionaries (with string keys), bytes, bytearray, and datetime.datetime objects. - Handle file I/O errors gracefully. - Handle plist parsing and generation errors gracefully. **Example**: ```python # Example usage: config_to_save = { \'app_name\': \'SampleApp\', \'version\': 1.0, \'features\': [\'feature1\', \'feature2\'], \'settings\': { \'volume\': 75, \'theme\': \'dark\' }, \'last_updated\': datetime.now() } # Save configuration to file manage_configurations(\\"save\\", config=config_to_save, filepath=\\"config.plist\\") # Load configuration from file loaded_config = manage_configurations(\\"load\\", filepath=\\"config.plist\\") print(loaded_config) ``` Design your function to efficiently handle the saving and loading operations while ensuring proper error handling for different edge cases, such as missing parameters or invalid file formats.","solution":"import plistlib from datetime import datetime from typing import Union, Dict, Any def manage_configurations(action: str, config: Union[Dict[str, Any], None] = None, filepath: Union[str, None] = None) -> Union[Dict[str, Any], None]: try: if action == \\"save\\": if config is None or filepath is None: raise ValueError(\\"Both \'config\' and \'filepath\' must be provided for \'save\' action.\\") with open(filepath, \'wb\') as file: plistlib.dump(config, file) return None elif action == \\"load\\": if filepath is None: raise ValueError(\\"\'filepath\' must be provided for \'load\' action.\\") with open(filepath, \'rb\') as file: return plistlib.load(file) else: raise ValueError(\\"Invalid action. Expected \'save\' or \'load\'.\\") except (FileNotFoundError, PermissionError) as e: print(f\\"Error accessing file: {e}\\") return None except plistlib.InvalidFileException as e: print(f\\"Error parsing plist file: {e}\\") return None"},{"question":"# Seaborn Boxplot Coding Assessment You are given a dataset loaded from Seaborn\'s built-in datasets. Your task is to use this dataset to showcase your proficiency with the Seaborn library by implementing a function that generates a customized boxplot. **Requirements**: 1. Load the \\"titanic\\" dataset. 2. Create a horizontal boxplot for the \\"age\\" column. 3. Group the boxplot by the \\"class\\" column. 4. Add a hue based on the \\"sex\\" column. 5. Customize the boxplot so that: - The boxes have a notch. - Outliers are shown with a \\"+\\" marker. - The boxes have a semi-transparent blue face color (rgba(0, 0, 1, 0.5)). - The median line is red and has a linewidth of 2. 6. Use Matplotlib to add a vertical line at age = 40. **Input**: - None (The function does not take any input parameters). **Output**: - The function should display the generated boxplot using Matplotlib. **Function Signature**: ```python def customized_titanic_boxplot(): # Your implementation here ``` **Constraints**: - Ensure that the face color of the boxes is set correctly without affecting other elements. **Performance Requirements**: - The code should run efficiently without requiring excessive computations. # Example Calling the function `customized_titanic_boxplot()` should display the customized boxplot as specified. ```python customized_titanic_boxplot() ``` **Hints**: - Refer to Seaborn and Matplotlib documentation if needed. - Ensure your plot is appropriately labeled and aesthetically appealing. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_titanic_boxplot(): # Load the Titanic dataset from seaborn titanic = sns.load_dataset(\'titanic\') # Create a horizontal boxplot for the \\"age\\" column grouped by \\"class\\" and with hue based on \\"sex\\" boxplot = sns.boxplot( x=\'age\', y=\'class\', hue=\'sex\', data=titanic, notch=True, showcaps=True, flierprops={\\"marker\\": \\"+\\"}, boxprops={\\"facecolor\\": (0, 0, 1, 0.5)}, medianprops={\\"color\\": \\"red\\", \\"linewidth\\": 2} ) # Add a vertical line at age = 40 using Matplotlib plt.axvline(x=40, color=\'black\', linestyle=\'--\') # Display the plot plt.show()"},{"question":"# Task You have been provided with a Python application tracking system that logs events and exceptions in a simplistic manner. Your task is to enhance this system utilizing functionalities from the `sys` module, particularly focusing on custom exception handling and profiling. # Requirements 1. **Custom Exception Handler**: Implement a custom exception handler that logs uncaught exceptions to a file named `error_log.txt`. The log should include: - The exception type. - The exception value. - The traceback. 2. **Profiling Function**: Implement a profiling function that logs the time duration of function calls of all the functions within your script to `profile_log.txt`. The log should include: - The function name. - The time taken for each function call. # Constraints - Use the `sys.excepthook` for exception handling. - Use the `sys.setprofile` for profiling function calls. - Your implementation should handle nested function calls gracefully. - Ensure that your profiling does not add significant overhead to the function execution times. # Input No direct input. The system should handle exceptions and profiling automatically when functions are executed. # Output - `error_log.txt` containing logged uncaught exceptions. - `profile_log.txt` containing profiling information of function calls. # Example A simple Python script using your enhanced system might look like: ```python import time def function_a(): time.sleep(1) function_b() def function_b(): time.sleep(2) raise ValueError(\\"An example error\\") # This should log both profiling and uncaught exceptions if __name__ == \\"__main__\\": function_a() ``` The `error_log.txt` should contain information similar to: ``` Exception type: <class \'ValueError\'> Exception value: An example error Traceback (most recent call last): File \\"script.py\\", line XX, in <module> function_a() File \\"script.py\\", line YY, in function_a function_b() File \\"script.py\\", line ZZ, in function_b raise ValueError(\\"An example error\\") ValueError: An example error ``` The `profile_log.txt` should contain information similar to: ``` Function: function_a, Duration: 1.001234s Function: function_b, Duration: 2.002345s ``` # Implementation Please provide your implementation below: ```python import sys import time import traceback ERROR_LOG_FILE = \'error_log.txt\' PROFILE_LOG_FILE = \'profile_log.txt\' def custom_excepthook(exc_type, exc_value, exc_traceback): with open(ERROR_LOG_FILE, \'a\') as log_file: log_file.write(f\\"Exception type: {exc_type}n\\") log_file.write(f\\"Exception value: {exc_value}n\\") log_file.write(\\"\\".join(traceback.format_tb(exc_traceback))) log_file.write(f\\"{exc_value}nn\\") sys.excepthook = custom_excepthook def profile_func(frame, event, arg): if event == \'call\': frame.f_locals[\'__start_time__\'] = time.time() elif event == \'return\': start_time = frame.f_locals.get(\'__start_time__\', None) if start_time is not None: duration = time.time() - start_time with open(PROFILE_LOG_FILE, \'a\') as log_file: log_file.write(f\\"Function: {frame.f_code.co_name}, Duration: {duration:.6f}sn\\") sys.setprofile(profile_func) # Example Functions to demonstrate the profiling and exception handling def function_a(): time.sleep(1) function_b() def function_b(): time.sleep(2) raise ValueError(\\"An example error\\") if __name__ == \\"__main__\\": function_a() ```","solution":"import sys import time import traceback ERROR_LOG_FILE = \'error_log.txt\' PROFILE_LOG_FILE = \'profile_log.txt\' def custom_excepthook(exc_type, exc_value, exc_traceback): with open(ERROR_LOG_FILE, \'a\') as log_file: log_file.write(f\\"Exception type: {exc_type}n\\") log_file.write(f\\"Exception value: {exc_value}n\\") log_file.write(\\"\\".join(traceback.format_tb(exc_traceback))) log_file.write(f\\"{exc_value}nn\\") sys.excepthook = custom_excepthook def profile_func(frame, event, arg): if event == \'call\': frame.f_locals[\'__start_time__\'] = time.time() elif event == \'return\': start_time = frame.f_locals.get(\'__start_time__\', None) if start_time is not None: duration = time.time() - start_time with open(PROFILE_LOG_FILE, \'a\') as log_file: log_file.write(f\\"Function: {frame.f_code.co_name}, Duration: {duration:.6f}sn\\") sys.setprofile(profile_func) # Example Functions to demonstrate the profiling and exception handling def function_a(): time.sleep(1) function_b() def function_b(): time.sleep(2) raise ValueError(\\"An example error\\") if __name__ == \\"__main__\\": function_a()"},{"question":"Objective Your task is to implement an asynchronous echo server using the `asynchat` module. The server should be able to handle multiple client connections, receiving messages and echoing them back to the sender. Requirements 1. Create a subclass of `asynchat.async_chat` called `EchoHandler` that handles the reception and echoing of messages. 2. Implement the `collect_incoming_data` and `found_terminator` methods to buffer incoming messages and detect when a complete message has been received. 3. Use the `set_terminator` method to set a newline character (`\\"n\\"`) as the message terminator. 4. The server should run indefinitely, accepting new client connections and handling their messages concurrently. Expected Input and Output - **Input**: Messages from clients sent to the server. - **Output**: Each message received by the server should be echoed back to the respective client. Constraints - The server should handle arbitrary lengths of messages. - The server should be able to handle multiple clients simultaneously. Performance Requirements - Ensure the server remains responsive and can manage multiple client connections efficiently. Additional Details - You may use the following standard Python modules: `socket`, `asyncore`, `asynchat`. - Write your code to handle any potential exceptions and ensure the server closes connections gracefully. Example Here is an example to help you get started: ```python import asynchat import asyncore import socket class EchoHandler(asynchat.async_chat): def __init__(self, sock): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\'\'.join(self.ibuffer).decode(\'utf-8\') print(f\'Received: {message}\') self.push(message.encode(\'utf-8\') + b\'n\') self.ibuffer = [] class EchoServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accept(self): pair = self.accept() if pair is not None: sock, addr = pair print(f\'Incoming connection from {addr}\') EchoHandler(sock) def main(): server = EchoServer(\'localhost\', 8080) asyncore.loop() if __name__ == \'__main__\': main() ``` This implementation sets up a basic echo server. You will need to integrate the necessary methods and ensure the server handles clients as specified.","solution":"import asynchat import asyncore import socket class EchoHandler(asynchat.async_chat): def __init__(self, sock): super().__init__(sock=sock) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): # Combine all chunks of data received message = b\'\'.join(self.ibuffer).decode(\'utf-8\') print(f\'Received: {message}\') # Echo the message back to the client self.push(message.encode(\'utf-8\') + b\'n\') # Reset buffer for the next message self.ibuffer = [] class EchoServer(asyncore.dispatcher): def __init__(self, host, port): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accept(self): # Accept the incoming connection pair = self.accept() if pair is not None: sock, addr = pair print(f\'Incoming connection from {addr}\') EchoHandler(sock) def main(): server = EchoServer(\'localhost\', 8080) asyncore.loop() if __name__ == \'__main__\': main()"},{"question":"# Question: Handling Nested Functions and Exception Propagation You\'re required to implement a suite of nested functions that demonstrate the understanding of name resolution, binding rules, and exception handling in Python. Specifications: 1. Implement a function `calculate_total` that takes a list of numbers as input. This function should: - Initialize a variable `total` to 0. - Define an inner function `add_to_total` which takes a number and adds it to `total`. - Iteratively call `add_to_total` for each number in the input list. - Return the final value of `total` after all numbers have been added. 2. Implement another function `safe_calculate` which: - Calls `calculate_total` with a given list of numbers. - Catches and handles any exceptions that arise during the call to `calculate_total`. If an exception is caught, it should print an error message and return `None`. Input - A list of numbers for the function `calculate_total`. Output - The sum of the numbers for a successful execution. - `None` and an error message printed if an exception occurs within the nested function calls. Example: ```python def safe_calculate(numbers): # Your implementation here # Test cases print(safe_calculate([1, 2, 3, 4])) # Output: 10 print(safe_calculate([1, \'two\', 3])) # Output: None (with an error message printed) ``` Constraints: - The input will be a list of numbers but may contain non-numeric types which should be handled gracefully. - Demonstrate the use of nested functions and exception handling. - Maintain appropriate name scope and avoid leaking variables outside their intended scope. Notes: - Focus on the correct implementation of name resolution, binding, and exception handling as discussed in the provided documentation. - Make sure the inner function correctly modifies the enclosing scope\'s `total` variable.","solution":"def calculate_total(numbers): Takes a list of numbers and returns their sum. total = 0 # Initialize total def add_to_total(number): nonlocal total # Ensure that we modify the \'total\' variable from the enclosing scope if not isinstance(number, (int, float)): raise ValueError(\\"All elements must be numbers\\") total += number for number in numbers: add_to_total(number) return total def safe_calculate(numbers): Safely calculates the total of a list of numbers by handling exceptions. try: return calculate_total(numbers) except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"# Python Class Implementation and Inheritance Problem Statement You are required to create a class hierarchy representing geometric shapes. Start with a basic `Shape` class and create derived classes `Circle`, `Rectangle`, and `Square`. Each class should be able to compute its area and perimeter. Detailed Requirements 1. **Shape Class**: - This will be the base class. - Methods: - `area()`: Should raise `NotImplementedError`. - `perimeter()`: Should raise `NotImplementedError`. 2. **Circle Class** (inherits from `Shape`): - Attributes: - `radius` (float): The radius of the circle. - Methods: - `area()`: Returns the area of the circle. - `perimeter()`: Returns the perimeter of the circle. 3. **Rectangle Class** (inherits from `Shape`): - Attributes: - `length` (float): The length of the rectangle. - `width` (float): The width of the rectangle. - Methods: - `area()`: Returns the area of the rectangle. - `perimeter()`: Returns the perimeter of the rectangle. 4. **Square Class** (inherits from `Rectangle`): - Only needs one attribute `side` for the side length (since a square is a special type of rectangle with equal sides). - Methods: - Override `__init__` to initialize using the `side`. 5. **Iterators**: - Implement a custom iterator for the `Shape` class to iterate over its subclasses (`Circle`, `Rectangle`, `Square`). 6. **Generators**: - Implement a generator function in the `Shape` class to yield the area of shapes when called on an instance of `Shape`’s iterator. Constraints 1. Assume all input values for dimensions are positive floats. 2. You must use the `math` module where necessary (e.g., `math.pi`). Example Usage ```python import math class Shape: def area(self): raise NotImplementedError def perimeter(self): raise NotImplementedError def __iter__(self): return iter([Circle, Rectangle, Square]) def areas(self): for subclass in self: yield subclass().area() class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) class Square(Rectangle): def __init__(self, side): super().__init__(side, side) def area(self): return super().area() def perimeter(self): return super().perimeter() # Example circle = Circle(5) print(circle.area()) # Output: 78.53975... print(circle.perimeter()) # Output: 31.4159... rectangle = Rectangle(4, 5) print(rectangle.area()) # Output: 20 print(rectangle.perimeter()) # Output: 18 square = Square(4) print(square.area()) # Output: 16 print(square.perimeter()) # Output: 16 shape_iterator = iter(Shape()) for subclass in shape_iterator: instance = subclass() print(instance.area()) # Will raise NotImplementedError if not all attributes are provided shape = Shape() for area in shape.areas(): print(area) # Will yield areas of circles, rectangles, and squares with their default dimensions of 1.0 ```","solution":"import math class Shape: def area(self): raise NotImplementedError def perimeter(self): raise NotImplementedError class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) class Square(Rectangle): def __init__(self, side): super().__init__(side, side)"},{"question":"# Unicode Character Analysis You are required to implement functions utilizing the `unicodedata` module to perform specific analyses on Unicode strings. Your goal is to demonstrate a deep understanding of handling Unicode characters and their properties. Part 1: Explicit Unicode Character Information Create a function `analyze_character(char: str) -> dict` that takes a single character as input and returns a dictionary with the character\'s properties. - `char`: A single Unicode character. - Returns: A dictionary containing the following keys and their corresponding values: - `name`: Name of the character. - `decimal`: Decimal value of the character if defined, else `None`. - `digit`: Digit value of the character if defined, else `None`. - `numeric`: Numeric value of the character if defined, else `None`. - `category`: General category of the character. - `bidirectional`: Bidirectional class of the character. - `combining`: Canonical combining class of the character. - `east_asian_width`: East Asian width of the character. - `mirrored`: Mirrored property of the character. - `decomposition`: Decomposition mapping of the character. ```python import unicodedata def analyze_character(char: str) -> dict: result = { \\"name\\": unicodedata.name(char), \\"decimal\\": unicodedata.decimal(char, None), \\"digit\\": unicodedata.digit(char, None), \\"numeric\\": unicodedata.numeric(char, None), \\"category\\": unicodedata.category(char), \\"bidirectional\\": unicodedata.bidirectional(char), \\"combining\\": unicodedata.combining(char), \\"east_asian_width\\": unicodedata.east_asian_width(char), \\"mirrored\\": unicodedata.mirrored(char), \\"decomposition\\": unicodedata.decomposition(char) } return result ``` Part 2: Unicode String Normalization Create a function `normalize_unicode_string(s: str, form: str) -> str` that normalizes a given Unicode string into the specified form and determines if the string is already in that normalized form. - `s`: A Unicode string. - `form`: A string specifying the normalization form. Valid values are `\'NFC\'`, `\'NFKC\'`, `\'NFD\'`, and `\'NFKD\'`. - Returns: A string that indicates whether the input string `s` is already normalized or has been converted to the normalized form. The format should be: - \\"The string is already in {form} form.\\" - \\"The normalized string in {form} form is: {normalized_string}\\" ```python import unicodedata def normalize_unicode_string(s: str, form: str) -> str: if unicodedata.is_normalized(form, s): return f\\"The string is already in {form} form.\\" else: normalized_str = unicodedata.normalize(form, s) return f\\"The normalized string in {form} form is: {normalized_str}\\" ``` # Example Usage ```python # Part 1 char_info = analyze_character(\'A\') print(char_info) # Output could be: # { # \'name\': \'LATIN CAPITAL LETTER A\', # \'decimal\': None, # \'digit\': None, # \'numeric\': None, # \'category\': \'Lu\', # \'bidirectional\': \'L\', # \'combining\': 0, # \'east_asian_width\': \'Na\', # \'mirrored\': 0, # \'decomposition\': \'\' # } # Part 2 s1 = \\"Ç\\" # LATIN CAPITAL LETTER C WITH CEDILLA result1 = normalize_unicode_string(s1, \'NFC\') print(result1) # Output could be: # \\"The string is already in NFC form.\\" # or # \\"The normalized string in NFC form is: Ç\\" ``` Constraints - The input string to `normalize_unicode_string` will not be empty and will only contain valid Unicode characters. - The input character to `analyze_character` will be a single valid Unicode character. The functions you implement should handle the nuances of Unicode characters and demonstrate competent usage of the `unicodedata` module.","solution":"import unicodedata def analyze_character(char: str) -> dict: Returns a dictionary with detailed information about the Unicode character. try: result = { \\"name\\": unicodedata.name(char), \\"decimal\\": unicodedata.decimal(char, None), \\"digit\\": unicodedata.digit(char, None), \\"numeric\\": unicodedata.numeric(char, None), \\"category\\": unicodedata.category(char), \\"bidirectional\\": unicodedata.bidirectional(char), \\"combining\\": unicodedata.combining(char), \\"east_asian_width\\": unicodedata.east_asian_width(char), \\"mirrored\\": unicodedata.mirrored(char), \\"decomposition\\": unicodedata.decomposition(char) } except ValueError: # Handle case where the character does not have a defined name result = { \\"name\\": None, \\"decimal\\": unicodedata.decimal(char, None), \\"digit\\": unicodedata.digit(char, None), \\"numeric\\": unicodedata.numeric(char, None), \\"category\\": unicodedata.category(char), \\"bidirectional\\": unicodedata.bidirectional(char), \\"combining\\": unicodedata.combining(char), \\"east_asian_width\\": unicodedata.east_asian_width(char), \\"mirrored\\": unicodedata.mirrored(char), \\"decomposition\\": unicodedata.decomposition(char) } return result def normalize_unicode_string(s: str, form: str) -> str: Normalizes a Unicode string to the specified form and returns a message indicating whether it was already in that form or displays the normalized string. if unicodedata.is_normalized(form, s): return f\\"The string is already in {form} form.\\" else: normalized_str = unicodedata.normalize(form, s) return f\\"The normalized string in {form} form is: {normalized_str}\\""},{"question":"**Coding Challenge: Advanced Attribute Validation with Descriptors** # Objective Implement a set of custom descriptors in Python to validate attributes of a class ensuring they conform to specified rules. You will specifically create descriptors that validate types, ranges, and other custom conditions for attributes in a class. # Requirements 1. Implement a base descriptor class `Validator` that enforces validation whenever an attribute is set. 2. Create specific validator classes inheriting from `Validator`: - `TypeValidator` for validating the type of the attribute. - `RangeValidator` for validating that the attribute falls within a specified numeric range. - `StringValidator` for validating a string attribute for length and custom predicates (like checking if it is uppercase). 3. Demonstrate the use of these validators in a `Product` class to ensure all attributes are well-defined and validated upon assignment. # Base Validator Class Define a base descriptor class `Validator` with the following capabilities: - `__set_name__` to store the attribute name. - `__get__` to retrieve the attribute value. - `__set__` to delegate the validation before setting the attribute value. - An abstract method `validate` for the specific validation logic each subclass needs to implement. # Specific Validator Classes 1. **TypeValidator** - Ensures the attribute is of a specified type. - Parameters: `expected_type` (e.g., `int`, `float`, `str`). 2. **RangeValidator** - Ensures the attribute value falls within a specified range. - Parameters: `min_value` and `max_value`. 3. **StringValidator** - Ensures the attribute value is a string with an optional length check and custom predicate. - Parameters: `min_length`, `max_length`, and `predicate` (a function to apply additional custom validations). # Example Class: `Product` Create a `Product` class that uses these validators for its attributes: - `price`: Must be a `float` greater than zero. - `quantity`: Must be an `int` within the range 0 to 1000. - `name`: Must be a non-empty string with a max length of 50 characters. # Implementation ```python from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = f\'_{name}\' def __get__(self, instance, owner): return getattr(instance, self.private_name) def __set__(self, instance, value): self.validate(value) setattr(instance, self.private_name, value) @abstractmethod def validate(self, value): pass class TypeValidator(Validator): def __init__(self, expected_type): self.expected_type = expected_type def validate(self, value): if not isinstance(value, self.expected_type): raise TypeError(f\'Expected {value!r} to be of type {self.expected_type.__name__}\') class RangeValidator(Validator): def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def validate(self, value): if (self.min_value is not None and value < self.min_value) or (self.max_value is not None and value > self.max_value): raise ValueError(f\'Expected {value!r} to be between {self.min_value} and {self.max_value}\') class StringValidator(Validator): def __init__(self, min_length=0, max_length=None, predicate=None): self.min_length = min_length self.max_length = max_length self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected {value!r} to be a string\') if len(value) < self.min_length or (self.max_length is not None and len(value) > self.max_length): raise ValueError(f\'String length of {value!r} is not within allowed range\') if self.predicate and not self.predicate(value): raise ValueError(f\'Expected {self.predicate} to be true for {value!r}\') class Product: price = TypeValidator(float) quantity = RangeValidator(0, 1000) name = StringValidator(min_length=1, max_length=50) def __init__(self, price, quantity, name): self.price = price self.quantity = quantity self.name = name # Example usage try: prod = Product(19.99, 500, \'Widget\') print(f\'Product created: {prod.name}, {prod.price}, {prod.quantity}\') invalid_prod = Product(-10, 500, \'Invalid\') except Exception as e: print(e) # Example output: Expected -10 to be between 0 and 1000 ``` # Input and Output - There are no direct inputs or outputs to define since this is a class implementation and usage. - Demonstrate the working by creating instances and ensuring that invalid attributes raise exceptions. # Constraints - The focus should be on defining the classes and their interaction. - Ensure that appropriate exceptions are raised for invalid attribute assignments. - The solution must use descriptors to enforce validation.","solution":"from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = f\'_{name}\' def __get__(self, instance, owner): return getattr(instance, self.private_name) def __set__(self, instance, value): self.validate(value) setattr(instance, self.private_name, value) @abstractmethod def validate(self, value): pass class TypeValidator(Validator): def __init__(self, expected_type): self.expected_type = expected_type def validate(self, value): if not isinstance(value, self.expected_type): raise TypeError(f\'Expected {value!r} to be of type {self.expected_type.__name__}\') class RangeValidator(Validator): def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def validate(self, value): if (self.min_value is not None and value < self.min_value) or (self.max_value is not None and value > self.max_value): raise ValueError(f\'Expected {value!r} to be between {self.min_value} and {self.max_value}\') class StringValidator(Validator): def __init__(self, min_length=0, max_length=None, predicate=None): self.min_length = min_length self.max_length = max_length self.predicate = predicate def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected {value!r} to be a string\') if len(value) < self.min_length or (self.max_length is not None and len(value) > self.max_length): raise ValueError(f\'String length of {value!r} is not within allowed range\') if self.predicate and not self.predicate(value): raise ValueError(f\'Expected {self.predicate} to be true for {value!r}\') class Product: price = TypeValidator(float) quantity = RangeValidator(0, 1000) name = StringValidator(min_length=1, max_length=50) def __init__(self, price, quantity, name): self.price = price self.quantity = quantity self.name = name # Example usage try: prod = Product(19.99, 500, \'Widget\') print(f\'Product created: {prod.name}, {prod.price}, {prod.quantity}\') invalid_prod = Product(-10, 500, \'Invalid\') except Exception as e: print(e) # Example output: Expected -10 to be between 0 and 1000"},{"question":"Advanced Data Manipulation with Pandas Objective The objective of this assessment is to evaluate your understanding of pandas functionalities related to advanced data manipulation, including handling missing data, merging datasets, and performing group-by operations. Problem Statement You are given two datasets in CSV format that contain information about sales and user data. Your task is to read these datasets into pandas DataFrames, process the data as required, and generate specific outputs. The steps required are described below: Input Format 1. `sales_data.csv`: A CSV file with the following columns: - `sale_id`: Unique identifier for the sale (integer) - `user_id`: Identifier for the user who made the purchase (integer) - `product_id`: Identifier for the product purchased (integer) - `quantity`: Quantity of the product purchased (integer) - `sale_date`: Date of the sale (string in the format YYYY-MM-DD) 2. `user_data.csv`: A CSV file with the following columns: - `user_id`: Unique identifier for the user (integer) - `user_name`: Name of the user (string) - `user_email`: Email address of the user (string) - `signup_date`: Date of the user signup (string in the format YYYY-MM-DD) You can assume that each CSV file contains no missing values. Output Requirements 1. Generate a summary DataFrame that shows, for each user: - `user_id`: Unique identifier for the user - `user_name`: Name of the user - `total_purchases`: Total number of purchases made by the user - `total_quantity`: Total quantity of products purchased by the user - `first_purchase_date`: Date of the earliest purchase made by the user - `last_purchase_date`: Date of the most recent purchase made by the user 2. Identify and list any users in the `sales_data` who do not exist in the `user_data` based on the `user_id`. Constraints 1. Your solution should be efficient in terms of both time and space complexity. 2. No additional libraries aside from pandas are allowed. 3. Your solution should handle large datasets efficiently (consider potential optimizations). Example Given the following datasets: **`sales_data.csv`**: | sale_id | user_id | product_id | quantity | sale_date | |---------|---------|------------|----------|------------| | 1 | 101 | 5001 | 2 | 2023-06-21 | | 2 | 102 | 5002 | 1 | 2023-06-22 | | 3 | 101 | 5003 | 1 | 2023-06-23 | **`user_data.csv`**: | user_id | user_name | user_email | signup_date | |---------|-----------|----------------------|-------------| | 101 | Alice | alice@example.com | 2023-01-15 | | 102 | Bob | bob@example.com | 2023-02-20 | | 103 | Charlie | charlie@example.com | 2023-03-25 | The resulting summary DataFrame would be: | user_id | user_name | total_purchases | total_quantity | first_purchase_date | last_purchase_date | |---------|-----------|-----------------|----------------|---------------------|--------------------| | 101 | Alice | 2 | 3 | 2023-06-21 | 2023-06-23 | | 102 | Bob | 1 | 1 | 2023-06-22 | 2023-06-22 | Users in `sales_data` not in `user_data`: None Function Signature ```python import pandas as pd def process_sales_data(sales_file: str, user_file: str) -> (pd.DataFrame, list): # Your code here # Example usage # summary_df, missing_users = process_sales_data(\'sales_data.csv\', \'user_data.csv\') ```","solution":"import pandas as pd def process_sales_data(sales_file: str, user_file: str) -> (pd.DataFrame, list): # Load the CSV files into pandas DataFrames sales_data = pd.read_csv(sales_file) user_data = pd.read_csv(user_file) # Merge the sales_data and user_data on user_id merged_data = pd.merge(sales_data, user_data, on=\'user_id\', how=\'right\') # Group by user_id to calculate required summary statistics summary = merged_data.groupby([\'user_id\', \'user_name\']).agg( total_purchases=pd.NamedAgg(column=\'sale_id\', aggfunc=\'count\'), total_quantity=pd.NamedAgg(column=\'quantity\', aggfunc=\'sum\'), first_purchase_date=pd.NamedAgg(column=\'sale_date\', aggfunc=\'min\'), last_purchase_date=pd.NamedAgg(column=\'sale_date\', aggfunc=\'max\') ).reset_index() # Identify users in sales_data who do not exist in user_data missing_users = set(sales_data[\'user_id\']) - set(user_data[\'user_id\']) missing_users = list(missing_users) return summary, missing_users"},{"question":"Objective The purpose of this assessment is to test your understanding of device management, random number generation, and memory management using the torch.xpu module in PyTorch. Problem Statement Write a Python script that does the following: 1. Checks if an XPU (eXtreme Processing Unit) is available. If no XPU is available, it should print a message and exit. 2. Retrieves and prints information about the available XPU, including the device name, device capabilities, and memory information. 3. Sets the device to the current XPU and confirms the change. 4. Initializes the random number generator for XPUs with a fixed seed for reproducibility. 5. Allocates a large tensor on the XPU and performs a basic operation, such as a matrix multiplication. 6. Retrieves and prints memory statistics before and after the operation. 7. Ensures synchronization of the stream and measures the execution time of the operation. Input and Output Formats - **Input**: None - **Output**: Should include printed statements, as specified in the steps, showing the results of each task. Constraints and Limitations 1. You should use the functions provided by the torch.xpu module. 2. Make sure to handle the cases where an XPU may not be available gracefully. 3. The large tensor you create should be at least 10,000 x 10,000 in size. 4. Use a fixed seed of 42 for reproducibility. Performance Requirements Although execution time is measured, it is more for demonstration purposes. Ensure that the computations do not hang indefinitely and the memory is managed efficiently. Example Output ```python XPU is available. Device Name: XPU Model XYZ Device Capability: (3, 7) Memory Info - Free: 8000 MB, Total: 16000 MB Setting and confirming current device to XPU: 0 Random number generator initialized with seed 42. Memory Usage Before Allocation: Allocated: 0 MB, Reserved: 0 MB Memory Usage After Allocation: Allocated: 2000 MB, Reserved: 2500 MB Execution Time for Matrix Multiplication: 2.3 seconds Memory Usage After Operation: Allocated: 2000 MB, Reserved: 2500 MB ``` Note Use the following imports in your solution: ```python import torch import torch.xpu import time ``` Ensure your code is well-commented and follows best practices for readability and maintainability.","solution":"import torch import torch.xpu import time def main(): # Check if XPU is available if not torch.xpu.is_available(): print(\\"XPU is not available.\\") return print(\\"XPU is available.\\") # Retrieve and print device information xpu_device = torch.xpu.current_device() device_name = torch.xpu.get_device_name(xpu_device) device_capability = torch.xpu.get_device_capability(xpu_device) memory_info = torch.xpu.memory_stats(xpu_device) print(f\\"Device Name: {device_name}\\") print(f\\"Device Capability: {device_capability}\\") print(f\\"Memory Info - Free: {memory_info[\'free\']} MB, Total: {memory_info[\'total\']} MB\\") # Sets the device to the current XPU torch.xpu.set_device(xpu_device) print(f\\"Setting and confirming current device to XPU: {xpu_device}\\") # Initialize the random number generator torch.xpu.manual_seed(42) print(\\"Random number generator initialized with seed 42.\\") # Retrieve and print memory statistics before allocation mem_before_allocation = torch.xpu.memory_allocated(xpu_device) print(f\\"Memory Usage Before Allocation:nAllocated: {mem_before_allocation} bytes\\") # Allocate a large tensor and perform matrix multiplication tensor_size = 10000 tensor_a = torch.rand((tensor_size, tensor_size), device=xpu_device) tensor_b = torch.rand((tensor_size, tensor_size), device=xpu_device) # Retrieve and print memory statistics after allocation mem_after_allocation = torch.xpu.memory_allocated(xpu_device) print(f\\"Memory Usage After Allocation:nAllocated: {mem_after_allocation} bytes\\") start_time = time.time() result_tensor = torch.matmul(tensor_a, tensor_b) torch.xpu.synchronize(xpu_device) end_time = time.time() print(f\\"Execution Time for Matrix Multiplication: {end_time - start_time} seconds\\") # Retrieve and print memory statistics after operation mem_after_operation = torch.xpu.memory_allocated(xpu_device) print(f\\"Memory Usage After Operation:nAllocated: {mem_after_operation} bytes\\") if __name__ == \\"__main__\\": main()"},{"question":"# Python Coding Assessment Question Background In Python, handling signals effectively is crucial for creating robust applications that can gracefully handle asynchronous events such as interrupts, alarms, or termination signals. The `signal` module provides mechanisms to define and manage custom signal handlers. Task You are required to implement a program that: 1. Sets up a custom handler for the `SIGALRM` signal. 2. Sets a timer using the `alarm()` function. 3. Performs a long-running task (e.g., a sleep operation) and safely handles the interrupt caused by the `SIGALRM` signal. Requirements - Define a signal handler function `alarm_handler` that will be invoked when an `SIGALRM` signal is received. The handler should print a message indicating that the signal was received. - Set a 3-second alarm before starting a task that sleeps for 5 seconds. - Ensure that the program catches the `SIGALRM` and gracefully exits the sleep operation. Input No input is required for this task. Output The output should display the message from the signal handler when the `SIGALRM` is received. Constraints - The `alarm_handler` function should have exactly two parameters, `signum` and `frame`. - Use the `signal` module to set up the alarm and signal handlers. Example Here is an example output for the required functionality: ``` Signal handler called with signal: 14 Finished handling the signal. ``` Note - You do not need to implement complex exception handling or catch different types of signals. Focus only on `SIGALRM`. - Your implementation should ensure that the signal handler is properly set up in the main thread. ```python import signal import time def alarm_handler(signum, frame): print(f\'Signal handler called with signal: {signum}\') raise Exception(\'Alarm signal received!\') def main(): # Set the signal handler for SIGALRM signal.signal(signal.SIGALRM, alarm_handler) # Schedule an alarm after 3 seconds signal.alarm(3) try: # Perform a sleep for 5 seconds print(\'Starting a long-running task...\') time.sleep(5) except Exception as e: print(\'Finished handling the signal.\') if __name__ == \\"__main__\\": main() ``` Your task is to write the complete implementation of this program and ensure it behaves as described.","solution":"import signal import time def alarm_handler(signum, frame): The signal handler function that will be called when SIGALRM is received. print(f\'Signal handler called with signal: {signum}\') raise Exception(\'Alarm signal received!\') def main(): Sets up the SIGALRM signal handler, sets the alarm, and performs the long-running task. # Set the signal handler for SIGALRM signal.signal(signal.SIGALRM, alarm_handler) # Schedule an alarm after 3 seconds signal.alarm(3) try: # Perform a sleep for 5 seconds print(\'Starting a long-running task...\') time.sleep(5) except Exception as e: print(f\'Exception caught: {e}\') print(\'Finished handling the signal.\') if __name__ == \\"__main__\\": main()"},{"question":"Applying and Exporting Styles using pandas Styler Objective To assess your understanding of the pandas `Styler` object, including applying styles and exporting results. Problem Statement You are provided with a dataset containing sales data for multiple products across different months. Your task is to perform the following operations using pandas: 1. Load the data into a DataFrame. 2. Apply styles to highlight certain conditions. 3. Export the styled DataFrame to an HTML file. Dataset The dataset is in CSV format with the following columns: - `Product`: Name of the product (string). - `Month`: Month of the sales data (string). - `Sales`: Sales figures for the respective month (integer). Example: ``` Product,Month,Sales A,January,1500 A,February,2000 B,January,3000 B,February,2500 ``` Task 1. Load the dataset into a pandas DataFrame. 2. Apply the following styles to the DataFrame: - Highlight the maximum sales figure in each `Month` using a green background. - Highlight the minimum sales figure in each `Month` using a red background. - Set a caption to the table: \\"Monthly Sales Data\\". 3. Export the styled DataFrame to an HTML file named `styled_sales.html`. 4. Print the content of the HTML file. Constraints - You must use pandas version 1.3.0 or later. - The CSV file name will be provided as input. Input - String: The path to the input CSV file. Output - String: HTML content of the styled DataFrame. Example ```python def style_sales_data(file_path: str) -> str: # Implement your solution here # Sample usage file_path = \\"sales_data.csv\\" html_content = style_sales_data(file_path) print(html_content) ``` Performance Requirements - The solution should efficiently handle DataFrames with up to 10,000 rows.","solution":"import pandas as pd def style_sales_data(file_path: str) -> str: # Load the dataset into a DataFrame df = pd.read_csv(file_path) # Create a Styler object styler = df.style # Define the styling functions def highlight_max(s): is_max = s == s.max() return [\'background-color: green\' if v else \'\' for v in is_max] def highlight_min(s): is_min = s == s.min() return [\'background-color: red\' if v else \'\' for v in is_min] # Apply styles styler = styler.apply(highlight_max, subset=[\'Sales\']) styler = styler.apply(highlight_min, subset=[\'Sales\']) # Set a caption to the table styler = styler.set_caption(\\"Monthly Sales Data\\") # Export the styled DataFrame to an HTML file html_file = \\"styled_sales.html\\" styler.to_html(html_file) # Read back the HTML content with open(html_file, \'r\') as file: html_content = file.read() return html_content"},{"question":"# Problem Description You are tasked with creating a Python function using the `http.client` module that performs several HTTP requests to a specified server and aggregates the results. The goal is to send GET requests to a series of endpoints and return a dictionary with status codes and response bodies. Write a function `fetch_multiple_resources(server_url, endpoints)` that performs the following operations: 1. **Connect to the Server:** - Use `HTTPConnection` to connect to the `server_url`. 2. **Send Multiple Requests:** - For each endpoint in the `endpoints` list, send a GET request. - Retrieve the response status and body for each request. 3. **Handle Responses:** - Store the response status and body in a dictionary where the keys are the endpoints and the values are tuples containing the status code and the response body. 4. **Return Results:** - Return the dictionary containing the aggregated results. # Input Format - `server_url`: A string representing the server URL. Example: \'www.example.com\' - `endpoints`: A list of strings, each representing an endpoint to request. Example: [\'/index.html\', \'/about\', \'/contact\'] # Output Format - Return a dictionary where: - Keys: Endpoints as strings. - Values: Tuples containing status codes as integers and response bodies as strings. # Example ```python server_url = \'www.example.com\' endpoints = [\'/index.html\', \'/about\', \'/contact\'] result = fetch_multiple_resources(server_url, endpoints) ``` Example output: ```python { \'/index.html\': (200, \'<!doctype html>...\'), \'/about\': (200, \'<!doctype html>...\'), \'/contact\': (404, \'Not Found\') } ``` # Constraints - The server may or may not support SSL. - Ensure proper exception handling for connection errors and non-2xx status codes. - Use the `http.client` module only for HTTP requests. - Implement efficient handling and fetching each endpoint without redundant connections. # Function Signature ```python def fetch_multiple_resources(server_url: str, endpoints: list) -> dict: # Your code here ```","solution":"import http.client from urllib.parse import urlsplit def fetch_multiple_resources(server_url, endpoints): results = {} # Detecting if the URL uses SSL (https) or not (http) parsed_url = urlsplit(f\'//{server_url}\') connection = http.client.HTTPSConnection(parsed_url.netloc) if parsed_url.scheme == \'https\' else http.client.HTTPConnection(parsed_url.netloc) try: for endpoint in endpoints: connection.request(\\"GET\\", endpoint) response = connection.getresponse() status_code = response.status response_body = response.read().decode() results[endpoint] = (status_code, response_body) except Exception as e: # Could log the exception or handle it as needed results[endpoint] = (\'error\', str(e)) finally: connection.close() return results"},{"question":"# Question: Python Argument Parsing and Value Building As a part of your assignment, you\'re tasked with implementing a Python C extension method in pure Python to simulate the behavior of `PyArg_ParseTuple()` and `Py_BuildValue()`. Using what you\'ve learned from the documentation, you need to write a Python function that: 1. **Parses a tuple of arguments** based on a provided format string. 2. **Validates and processes the arguments** according to the expected types described in the format string. 3. **Constructs a result** using the validated results. Implement the function `parse_and_build` which takes two parameters: - `args`: a tuple of arguments to parse - `format`: a format string indicating how to parse `args`. The function should: 1. Parse `args` according to `format`. 2. Construct a result list with the parsed and validated arguments (converted to appropriate Python types, e.g., integers, floats, strings). 3. Ensure any parsing errors raise a `ValueError` with a descriptive message. # Input Format - `args` (tuple): A tuple containing the arguments to be parsed. - `format` (string): A format string following the conventions described in the documentation. # Output Format - Return a list containing the parsed and validated arguments. # Example ```python def parse_and_build(args, format): # Your implementation goes here # Example Usage: try: result = parse_and_build((5, 3.14, \\"hello\\"), \\"i f s\\") print(result) # Output: [5, 3.14, \'hello\'] except ValueError as e: print(e) try: result = parse_and_build((5, \\"invalid\\", \\"hello\\"), \\"i f s\\") print(result) # Should raise a ValueError except ValueError as e: print(e) # Output: \\"Invalid argument at position 1: Expected float, got str\\" ``` # Constraints - The `format` string will only contain a subset of the described formats: `i` (int), `f` (float), `s` (str). - Input would contain valid tuples and format strings for the purpose of this exercise. - Raise a `ValueError` if: - The number of arguments in `args` does not match the number of format units in `format`. - Any argument does not match the expected type. This task will test your understanding of argument parsing and converting values between Python types based on a format string.","solution":"def parse_and_build(args, format): Parses and validates arguments according to the format string, then constructs a result list. :param args: Tuple of arguments to parse. :param format: Format string indicating how to parse the arguments. :return: List of parsed and validated arguments. :raises ValueError: If there is a type mismatch or wrong number of arguments. format_types = format.split() if len(args) != len(format_types): raise ValueError(\\"The number of arguments does not match the number of format specifiers.\\") result = [] for i, (arg, fmt) in enumerate(zip(args, format_types)): if fmt == \'i\': if not isinstance(arg, int): raise ValueError(f\\"Invalid argument at position {i}: Expected int, got {type(arg).__name__}\\") result.append(arg) elif fmt == \'f\': if not isinstance(arg, (float, int)): # Accept int also for float according to Python\'s type flexibility raise ValueError(f\\"Invalid argument at position {i}: Expected float, got {type(arg).__name__}\\") result.append(float(arg)) elif fmt == \'s\': if not isinstance(arg, str): raise ValueError(f\\"Invalid argument at position {i}: Expected str, got {type(arg).__name__}\\") result.append(arg) else: raise ValueError(f\\"Unknown format specifier at position {i}: {fmt}\\") return result"},{"question":"Objective: Implement a function using `concurrent.futures` to update and process a list of numbers in parallel, demonstrating your understanding of both `ThreadPoolExecutor` and `ProcessPoolExecutor`. Problem Description: You are provided with a list of numbers, `numbers`, and your task is to perform two operations on this list in parallel: 1. **Increment Operation**: Using `ThreadPoolExecutor`, increment each number by a given increment value. 2. **Prime Check Operation**: Using `ProcessPoolExecutor`, check if each incremented number is a prime number. You must write two functions and a main function: 1. `increment_numbers(numbers: List[int], increment: int, max_workers: int) -> List[int]`: Uses `ThreadPoolExecutor` to increment each number by `increment`. 2. `check_prime_numbers(numbers: List[int], max_workers: int) -> List[bool]`: Uses `ProcessPoolExecutor` to check if each number is prime. 3. `process_numbers(numbers: List[int], increment: int, thread_workers: int, process_workers: int) -> List[bool]`: This function orchestrates the entire process by calling the first two functions and returning a list indicating which numbers are prime. Input: - `numbers` (List[int]): A list of integers to be processed. - `increment` (int): The value to increment each number by. - `thread_workers` (int): The number of threads to use in `ThreadPoolExecutor`. - `process_workers` (int): The number of processes to use in `ProcessPoolExecutor`. Output: - List[bool]: A list of boolean values, where each element indicates whether the corresponding number in the incremented list is a prime number. Constraints: - `1 <= len(numbers) <= 10^4` - `-10^6 <= numbers[i] <= 10^6` - `1 <= increment <= 10^6` - `1 <= thread_workers, process_workers <= 32` - You need to handle exceptions properly and ensure the executors shut down gracefully. Example: ```python def increment_numbers(numbers, increment, max_workers): import concurrent.futures def increment_fn(n): return n + increment with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: incremented_numbers = list(executor.map(increment_fn, numbers)) return incremented_numbers def check_prime_numbers(numbers, max_workers): import concurrent.futures import math def is_prime(n): if n < 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.sqrt(n)) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor: prime_checks = list(executor.map(is_prime, numbers)) return prime_checks def process_numbers(numbers, increment, thread_workers, process_workers): incremented = increment_numbers(numbers, increment, thread_workers) prime_checks = check_prime_numbers(incremented, process_workers) return prime_checks # Example Usage numbers = [10, 11, 12, 13] increment = 5 thread_workers = 3 process_workers = 2 result = process_numbers(numbers, increment, thread_workers, process_workers) # Expected output: [False, True, False, False] (incremented numbers are [15, 16, 17, 18]) print(result) ``` Notes: - Handle edge cases such as empty list. - Ensure performance by using appropriate number of workers. Evaluation Criteria: - Correct and efficient use of `ThreadPoolExecutor` and `ProcessPoolExecutor`. - Proper shutdown of executors. - Handling of exceptions and edge cases. - Code readability and documentation.","solution":"import concurrent.futures import math from typing import List def increment_numbers(numbers: List[int], increment: int, max_workers: int) -> List[int]: Increment each number in the list by the given increment value using ThreadPoolExecutor. def increment_fn(n): return n + increment with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: incremented_numbers = list(executor.map(increment_fn, numbers)) return incremented_numbers def is_prime(n: int) -> bool: Check if a number is prime. if n < 2: return False if n == 2: return True if n % 2 == 0: return False sqrt_n = int(math.sqrt(n)) for i in range(3, sqrt_n + 1, 2): if n % i == 0: return False return True def check_prime_numbers(numbers: List[int], max_workers: int) -> List[bool]: Check if each number in the list is a prime number using ProcessPoolExecutor. with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor: prime_checks = list(executor.map(is_prime, numbers)) return prime_checks def process_numbers(numbers: List[int], increment: int, thread_workers: int, process_workers: int) -> List[bool]: Increment numbers and check if they are prime. incremented = increment_numbers(numbers, increment, thread_workers) prime_checks = check_prime_numbers(incremented, process_workers) return prime_checks"},{"question":"# Question: Advanced Custom Class Implementation **Objective**: Implement a custom class in Python that demonstrates an understanding of advanced class features including custom attribute access, special methods, context management, and asynchronous operations. # Description You are required to create a class `AdvancedFileHandler` that manages file operations with the following specifications: 1. **Initialization and Representation**: - The class should be initialized with a file path. - Implement `__repr__` and `__str__` methods to provide appropriate representations. 2. **Descriptor for File Size**: - Use a descriptor to provide a read-only attribute `file_size` which returns the size of the file in bytes. 3. **Context Management**: - Implement the context manager interface (`__enter__` and `__exit__` methods) to open the file in read mode when entering the context and closing it when exiting the context. It should also handle any exceptions and ensure the file is closed properly. 4. **Asynchronous Line Reader**: - Implement an asynchronous method `read_lines` that reads lines from the file asynchronously. Utilize asynchronous iterators and context managers. # Constraints - The file path provided should be valid. - Ensure proper handling of I/O operations. - Use appropriate special methods for the required functionalities. # Input/Output - **Initialization**: `handler = AdvancedFileHandler(\'example.txt\')` - **Representation**: `print(repr(handler))` should print something like `AdvancedFileHandler(\'example.txt\')` - **Context Management**: ```python with AdvancedFileHandler(\'example.txt\') as handler: # file is opened pass # file is closed ``` - **Asynchronous Line Reader**: ```python import asyncio async def main(): handler = AdvancedFileHandler(\'example.txt\') async for line in handler.read_lines(): print(line) asyncio.run(main()) ``` # Example ```python class FileSizeDescriptor: def __get__(self, instance, owner): return instance._get_file_size() class AdvancedFileHandler: file_size = FileSizeDescriptor() def __init__(self, file_path): self.file_path = file_path def __repr__(self): return f\\"{self.__class__.__name__}(\'{self.file_path}\')\\" def __str__(self): return f\\"AdvancedFileHandler managing file: {self.file_path}\\" def _get_file_size(self): import os return os.path.getsize(self.file_path) def __enter__(self): self.file = open(self.file_path, \'r\') return self def __exit__(self, exc_type, exc_value, traceback): if self.file: self.file.close() async def read_lines(self): with open(self.file_path, \'r\') as file: for line in file: yield line # Example usage: handler = AdvancedFileHandler(\'example.txt\') print(repr(handler)) # AdvancedFileHandler(\'example.txt\') with AdvancedFileHandler(\'example.txt\') as handler: # file is opened pass # file is closed import asyncio async def main(): handler = AdvancedFileHandler(\'example.txt\') async for line in handler.read_lines(): print(line) asyncio.run(main()) ``` Ensure you understand and implement all the parts correctly. Your solution should work seamlessly for the given example. **Note**: The descriptor should provide the file size when accessed and should not allow modification.","solution":"import os class FileSizeDescriptor: Descriptor for read-only file size attribute. def __get__(self, instance, owner): return instance._get_file_size() class AdvancedFileHandler: AdvancedFileHandler class that manages file operations. file_size = FileSizeDescriptor() def __init__(self, file_path): self.file_path = file_path self.file = None def __repr__(self): return f\\"{self.__class__.__name__}(\'{self.file_path}\')\\" def __str__(self): return f\\"AdvancedFileHandler managing file: {self.file_path}\\" def _get_file_size(self): Get the size of the file in bytes. return os.path.getsize(self.file_path) def __enter__(self): Open the file in read mode when entering the context. self.file = open(self.file_path, \'r\') return self def __exit__(self, exc_type, exc_value, traceback): Close the file when exiting the context, handling exceptions appropriately. if self.file: self.file.close() async def read_lines(self): Asynchronous line reader for the file. async def async_gen(): with open(self.file_path, \'r\') as file: for line in file: yield line return async_gen()"},{"question":"# Question: Advanced Text Manipulation in pandas You are provided with a DataFrame containing a column of text data that requires cleaning and transformation. Below is the structure of the DataFrame: ```python import pandas as pd import numpy as np data = { \'text_column\': [ \' The quick brown fox \', \'jumpsnovernthenlazyndog \', \'hello_world_this_is_a_test\', \'FOOBAR123\', np.nan, \' Example_text \' ] } df = pd.DataFrame(data) ``` **Your task is to implement the following cleaning and transformation steps and return the resulting DataFrame:** 1. Strip leading and trailing whitespace from the text. 2. Convert all characters to lower case. 3. Replace any newline characters (`n`) with spaces. 4. Split any text containing underscores (`_`) into separate words and concatenate them back with spaces. (For example, \\"hello_world\\" should become \\"hello world\\"). 5. Handle text containing both alphabetic and numeric characters by separating alphabets from numbers with space. (For instance, \\"FOO123BAR\\" should become \\"foo 123 bar\\"). 6. Fill any missing values (`NaN`) in the text column with the string \\"missing\\". Here is the expected DataFrame structure after applying all transformations: ```python expected_data = { \'text_column\': [ \'the quick brown fox\', \'jumps over the lazy dog\', \'hello world this is a test\', \'foobar 123\', \'missing\', \'example text\' ] } expected_df = pd.DataFrame(expected_data) ``` **Function Signature:** ```python def clean_transform_text(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** - `df`: A pandas DataFrame containing a column named `text_column` with text data in various formats. **Output:** - A pandas DataFrame with the `text_column` cleaned and transformed as per the specified rules. **Constraints:** - Ensure that the function handles the input DataFrame flexibly, regardless of the row count. - The function should not modify the original DataFrame in place. **Note:** - Utilize pandas\' string methods and auxiliary functions to accomplish the task efficiently. **Example Usage:** ```python data = { \'text_column\': [ \' The quick brown fox \', \'jumpsnovernthenlazyndog \', \'hello_world_this_is_a_test\', \'FOOBAR123\', np.nan, \' Example_text \' ] } df = pd.DataFrame(data) result = clean_transform_text(df) print(result.equals(expected_df)) # Should output: True ```","solution":"import pandas as pd import numpy as np import re def clean_transform_text(df: pd.DataFrame) -> pd.DataFrame: df = df.copy() def clean_text(text): if pd.isnull(text): return \\"missing\\" # Step 1: Strip leading and trailing whitespace text = text.strip() # Step 2: Convert to lower case text = text.lower() # Step 3: Replace newline characters with spaces text = text.replace(\'n\', \' \') # Step 4: Replace underscores with spaces text = text.replace(\'_\', \' \') # Step 5: Separate alphabets and numbers with space text = re.sub(r\'([a-zA-Z])(d)\', r\'1 2\', text) text = re.sub(r\'(d)([a-zA-Z])\', r\'1 2\', text) return text df[\'text_column\'] = df[\'text_column\'].apply(clean_text) return df"},{"question":"# Complex Number Operations and Linear Algebra in PyTorch **Objective**: Demonstrate your understanding of complex numbers in PyTorch by creating, manipulating, and performing operations on complex tensors. Apply linear algebra operations and verify the results. **Instructions**: 1. **Complex Tensor Creation**: - Create a complex tensor `A` of shape (3, 3) with random values. 2. **Matrix Transformation**: - Transform tensor `A` from the complex datatype to its real representation using `torch.view_as_real`. 3. **Access and Modify Real and Imaginary Parts**: - Access the real and imaginary parts of tensor `A` individually. - Multiply the real part of tensor `A` by 2 and the imaginary part by -1. 4. **Linear Algebra Operations**: - Create another complex tensor `B` of shape (3, 3) with predefined values: [[1+2j, 2-3j, -1j], [-4+5j, 3+6j, 2j], [1-1j, -2+2j, 3+3j]]. - Perform matrix multiplication between `A` and `B`. - Compute the conjugate transpose of `B` and perform matrix multiplication with `A`. 5. **Angle and Magnitude Calculation**: - Calculate the angle and magnitude of the resultant tensor from the first matrix multiplication and display the results. **Code Implementation**: Create a function `complex_tensor_operations()` that performs the above tasks and prints the results at each step. The function should have no input parameters. **Constraints**: - Do not use any additional libraries except PyTorch. - Ensure proper handling of complex numbers and matrix operations. **Expected Output**: - Display the original tensor `A`. - Display the tensor `A` transformed to its real representation. - Display the modified real and imaginary parts of tensor `A`. - Display the result of the first matrix multiplication (`A * B`). - Display the result of the conjugate transpose matrix multiplication. - Display the angle and magnitude of the resultant tensor from the first matrix multiplication. ```python import torch def complex_tensor_operations(): # Step 1: Create complex tensor A A = torch.randn(3, 3, dtype=torch.cfloat) print(\\"Original Tensor A:\\") print(A) # Step 2: Transform to real representation A_real_rep = torch.view_as_real(A) print(\\"nTensor A in Real Representation:\\") print(A_real_rep) # Step 3: Access and modify real and imaginary parts real_A = A.real imag_A = A.imag real_A.mul_(2) imag_A.mul_(-1) print(\\"nModified Real Part of A:\\") print(real_A) print(\\"nModified Imaginary Part of A:\\") print(imag_A) # Step 4: Create tensor B and perform linear algebra operations B = torch.tensor([[1+2j, 2-3j, -1j], [-4+5j, 3+6j, 2j], [1-1j, -2+2j, 3+3j]], dtype=torch.cfloat) result1 = torch.matmul(A, B) B_conj_transpose = B.conj().T result2 = torch.matmul(A, B_conj_transpose) print(\\"nMatrix B:\\") print(B) print(\\"nResult of A * B:\\") print(result1) print(\\"nResult of A * B_conj_transpose:\\") print(result2) # Step 5: Calculate angle and magnitude of the resultant tensor angle_result = torch.angle(result1) magnitude_result = torch.abs(result1) print(\\"nAngle of the Result of A * B:\\") print(angle_result) print(\\"nMagnitude of the Result of A * B:\\") print(magnitude_result) # Run the function complex_tensor_operations() ```","solution":"import torch def complex_tensor_operations(): # Step 1: Create complex tensor A A = torch.randn(3, 3, dtype=torch.cfloat) print(\\"Original Tensor A:\\") print(A) # Step 2: Transform to real representation A_real_rep = torch.view_as_real(A) print(\\"nTensor A in Real Representation:\\") print(A_real_rep) # Step 3: Access and modify real and imaginary parts real_A = A.real imag_A = A.imag real_A = real_A * 2 imag_A = imag_A * -1 print(\\"nModified Real Part of A:\\") print(real_A) print(\\"nModified Imaginary Part of A:\\") print(imag_A) # Step 4: Create tensor B and perform linear algebra operations B = torch.tensor([[1+2j, 2-3j, -1j], [-4+5j, 3+6j, 2j], [1-1j, -2+2j, 3+3j]], dtype=torch.cfloat) result1 = torch.matmul(A, B) B_conj_transpose = B.conj().T result2 = torch.matmul(A, B_conj_transpose) print(\\"nMatrix B:\\") print(B) print(\\"nResult of A * B:\\") print(result1) print(\\"nResult of A * B_conj_transpose:\\") print(result2) # Step 5: Calculate angle and magnitude of the resultant tensor angle_result = torch.angle(result1) magnitude_result = torch.abs(result1) print(\\"nAngle of the Result of A * B:\\") print(angle_result) print(\\"nMagnitude of the Result of A * B:\\") print(magnitude_result) # Run the function complex_tensor_operations()"},{"question":"**Coding Assessment Question** # Question: Implementing Complex Flow Control in Python You are tasked to implement a function `evaluate_operations` which processes a list of operations. The function should handle various types of operations and use appropriate control flow mechanisms to ensure correct execution and error handling. The operations may include arithmetic computations, conditional checks, and string manipulations. You will also need to demonstrate proper use of exception handling. # Function Specification **Function Signature:** ```python def evaluate_operations(operations: list) -> list: pass ``` **Input:** - `operations` (list): A list of dictionaries, each describing an operation. Each dictionary has the format: ```python { \\"operation\\": \\"type_of_operation\\", \\"value\\": optional_value, ... } ``` The possible operations and their specifications are: - `\\"add\\"`: Performs addition. - Keys: `\\"operation\\"`, `\\"value1\\"`, `\\"value2\\"`. - Example: `{\\"operation\\": \\"add\\", \\"value1\\": 10, \\"value2\\": 5}`. - `\\"divide\\"`: Performs division. - Keys: `\\"operation\\"`, `\\"value1\\"`, `\\"value2\\"`. - Example: `{\\"operation\\": \\"divide\\", \\"value1\\": 10, \\"value2\\": 2}`. - `\\"check_positive\\"`: Checks if a number is positive. - Keys: `\\"operation\\"`, `\\"value\\"`. - Example: `{\\"operation\\": \\"check_positive\\", \\"value\\": 10}`. - `\\"concat\\"`: Concatenates two strings. - Keys: `\\"operation\\"`, `\\"string1\\"`, `\\"string2\\"`. - Example: `{\\"operation\\": \\"concat\\", \\"string1\\": \\"Hello\\", \\"string2\\": \\"World\\"}`. **Output:** - The function should return a list of results corresponding to each operation in the order provided. - For `\\"add\\"`, return the sum. - For `\\"divide\\"`, return the result or `\\"Division by zero\\"` on division by zero. - For `\\"check_positive\\"`, return `True` if the value is positive, `False` otherwise. - For `\\"concat\\"`, return the concatenated string. - If an unknown operation is encountered, return `\\"Unknown Operation\\"`. **Constraints:** - Use `try-except` for handling division by zero. - Use conditional statements to determine operations. - Use pattern matching for different operations. - Ensure the logic is robust and can handle unexpected input gracefully. # Example: ```python inputs = [ {\\"operation\\": \\"add\\", \\"value1\\": 10, \\"value2\\": 5}, {\\"operation\\": \\"divide\\", \\"value1\\": 10, \\"value2\\": 0}, {\\"operation\\": \\"check_positive\\", \\"value\\": -3}, {\\"operation\\": \\"concat\\", \\"string1\\": \\"Hello, \\", \\"string2\\": \\"World!\\"}, {\\"operation\\": \\"multiply\\", \\"value1\\": 10, \\"value2\\": 2} ] assert evaluate_operations(inputs) == [ 15, \\"Division by zero\\", False, \\"Hello, World!\\", \\"Unknown Operation\\" ] ``` Try to utilize a combination of `if`, `try-except`, and the new `match-case` statements to solve this problem effectively.","solution":"def evaluate_operations(operations): results = [] for operation in operations: try: op_type = operation.get(\\"operation\\", None) if op_type == \\"add\\": val1 = operation[\\"value1\\"] val2 = operation[\\"value2\\"] results.append(val1 + val2) elif op_type == \\"divide\\": val1 = operation[\\"value1\\"] val2 = operation[\\"value2\\"] try: results.append(val1 / val2) except ZeroDivisionError: results.append(\\"Division by zero\\") elif op_type == \\"check_positive\\": val = operation[\\"value\\"] results.append(val > 0) elif op_type == \\"concat\\": str1 = operation[\\"string1\\"] str2 = operation[\\"string2\\"] results.append(str1 + str2) else: results.append(\\"Unknown Operation\\") except KeyError as e: results.append(f\\"Missing key: {e}\\") except Exception as e: results.append(f\\"Error: {str(e)}\\") return results"},{"question":"Coding Assessment Question # Objective To assess your understanding of attention mechanisms in deep learning, specifically their implementation using PyTorch\'s attention utilities. # Task Implement a custom scaled dot-product attention function using PyTorch. You should also leverage the provided `torch.nn.attention.sdpa_kernel` utility in your implementation if applicable. # Details 1. **Function Signature**: ```python def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor: ``` 2. **Parameters**: - `query` (torch.Tensor): Shape `(batch_size, num_heads, seq_len, d_k)`, where `d_k` is the dimension of key/query vectors. - `key` (torch.Tensor): Shape `(batch_size, num_heads, seq_len, d_k)`, where `d_k` is the dimension of key/query vectors. - `value` (torch.Tensor): Shape `(batch_size, num_heads, seq_len, d_v)`, where `d_v` is the dimension of value vectors. - `mask` (torch.Tensor, optional): Shape `(batch_size, 1, seq_len, seq_len)`. Masking tensor to prevent attention to certain positions (e.g., padding tokens). 3. **Returns**: - `output` (torch.Tensor): Shape `(batch_size, num_heads, seq_len, d_v)`. The result of applying scaled dot-product attention. # Constraints - Ensure numerical stability in your implementation. - Use the `torch.nn.functional.softmax` function to compute attention weights. - Your implementation should be efficient and capable of handling typical sizes of query, key, and value tensors used in transformer models. - You may use the provided `torch.nn.attention.sdpa_kernel` if it simplifies your implementation or offers performance benefits. # Performance Requirements - Your implementation should be optimized for matrix operations, utilizing PyTorch\'s capabilities. - Minimize the use of for-loops and ensure that the implementation can operate on batches of data simultaneously. # Example Here is a pseudo-code outline of the attention mechanism: ```python def scaled_dot_product_attention(query, key, value, mask=None): d_k = query.size(-1) # Scaled Dot-Product attention scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) attention_weights = torch.nn.functional.softmax(scores, dim=-1) output = torch.matmul(attention_weights, value) return output ``` # Supporting Information - You might want to refer to the official PyTorch documentation on `torch.matmul`, `torch.nn.functional.softmax`, and tensor operations. - Utilize any utility functions or classes from `torch.nn.attention` to enhance your implementation. Implement the `scaled_dot_product_attention` function in Python using PyTorch below: ```python import torch import math from torch.nn.attention import sdpa_kernel def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor: # Your implementation here pass ```","solution":"import torch import math from torch.nn.functional import softmax def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor: d_k = query.size(-1) # Calculate the raw attention scores using scaled dot-product scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) # Apply softmax to get the attention weights attention_weights = softmax(scores, dim=-1) # Compute the attention output output = torch.matmul(attention_weights, value) return output"},{"question":"Create a command-line interpreter for managing a simple task management system using the `cmd` module. The task management system should support the following commands: - `add <task description>`: Adds a new task with the given description. - `list`: Lists all tasks with their IDs and descriptions. - `delete <task ID>`: Deletes the task with the given ID. - `complete <task ID>`: Marks the task with the given ID as completed. - `incomplete <task ID>`: Marks the task with the given ID as not completed. - `help` or `?`: Provides help for each command. Your task is to: 1. Create a subclass of `cmd.Cmd` named `TaskManagerShell`. 2. Implement the required `do_` methods for each of the commands listed above. 3. Implement the appropriate help messages for each command. 4. Ensure that tasks are stored in an appropriate data structure and the interpreter behaves as expected. Below is the expected input and output format: ```python class TaskManagerShell(cmd.Cmd): # Initialize task manager def __init__(self): super().__init__() <initialize your data structure here> # Add command def do_add(self, arg): \'<Add new task>\' <implementation here> # List command def do_list(self, arg): \'<List all tasks>\' <implementation here> # Delete command def do_delete(self, arg): \'<Delete a task>\' <implementation here> # Complete command def do_complete(self, arg): \'<Mark a task as complete>\' <implementation here> # Incomplete command def do_incomplete(self, arg): \'<Mark a task as incomplete>\' <implementation here> # Start the TaskManagerShell if __name__ == \'__main__\': TaskManagerShell().cmdloop() ``` **Constraints:** - Task IDs should be unique integers. - Provide appropriate error handling for commands (e.g., trying to delete a non-existing task). - Make sure the `list` command properly indicates whether a task is completed or not. **Example Usage:** ```python (tmanager) add Finish the report (tmanager) add Attend the meeting (tmanager) list 0 - Finish the report (Incomplete) 1 - Attend the meeting (Incomplete) (tmanager) complete 0 (tmanager) list 0 - Finish the report (Complete) 1 - Attend the meeting (Incomplete) (tmanager) delete 1 (tmanager) list 0 - Finish the report (Complete) (tmanager) bye ``` Implementing this interpreter requires understanding and effectively using the `cmd` module, managing a data structure to store tasks, and handling user input and commands according to the specifications.","solution":"import cmd class Task: def __init__(self, description): self.description = description self.completed = False def __str__(self): status = \\"Complete\\" if self.completed else \\"Incomplete\\" return f\\"{self.description} ({status})\\" class TaskManagerShell(cmd.Cmd): intro = \'Welcome to the task manager. Type help or ? to list commands.n\' prompt = \'(tmanager) \' def __init__(self): super().__init__() self.tasks = [] self.next_id = 0 def do_add(self, arg): \'Add new task: add <task description>\' if not arg: print(\\"Error: Task description required.\\") return task = Task(arg) self.tasks.append((self.next_id, task)) print(f\\"Task {self.next_id} added: {task}\\") self.next_id += 1 def do_list(self, arg): \'List all tasks: list\' if not self.tasks: print(\\"No tasks available.\\") else: for task_id, task in self.tasks: print(f\\"{task_id} - {task}\\") def do_delete(self, arg): \'Delete a task: delete <task ID>\' if not arg.isdigit(): print(\\"Error: Task ID should be an integer.\\") return task_id = int(arg) for i, (tid, task) in enumerate(self.tasks): if tid == task_id: del self.tasks[i] print(f\\"Task {task_id} deleted.\\") return print(f\\"Error: Task {task_id} not found.\\") def do_complete(self, arg): \'Mark a task as complete: complete <task ID>\' if not arg.isdigit(): print(\\"Error: Task ID should be an integer.\\") return task_id = int(arg) for tid, task in self.tasks: if tid == task_id: task.completed = True print(f\\"Task {task_id} marked as complete.\\") return print(f\\"Error: Task {task_id} not found.\\") def do_incomplete(self, arg): \'Mark a task as incomplete: incomplete <task ID>\' if not arg.isdigit(): print(\\"Error: Task ID should be an integer.\\") return task_id = int(arg) for tid, task in self.tasks: if tid == task_id: task.completed = False print(f\\"Task {task_id} marked as incomplete.\\") return print(f\\"Error: Task {task_id} not found.\\") # Main method to start the shell if __name__ == \'__main__\': TaskManagerShell().cmdloop()"},{"question":"**Problem Statement: Clustering with KMeans and Anomaly Detection** You are provided with a dataset representing customer data for a retail company. Your task is to perform clustering to identify customer segments and then detect anomalies within those segments. You will use the KMeans algorithm for clustering and the IsolationForest algorithm for anomaly detection. # Function to Implement Your task is to write a function `customer_segmentation_and_anomaly_detection(data: pd.DataFrame, n_clusters: int) -> Tuple[pd.Series, pd.Series]` that: 1. Performs KMeans clustering on the given data with `n_clusters` clusters. 2. For each cluster, uses IsolationForest to detect anomalies. 3. Returns two pandas Series: - `cluster_labels`: a Series where each element is the cluster label assigned to the corresponding point. - `anomaly_labels`: a Series where each element is `1` if the point is an anomaly in its cluster and `0` otherwise. # Input - `data`: A pandas DataFrame where each row represents a customer and each column represents a feature. - `n_clusters`: An integer representing the number of clusters to form. # Output - `cluster_labels`: A pandas Series of length equal to the number of rows in `data`, containing the cluster labels. - `anomaly_labels`: A pandas Series of length equal to the number of rows in `data`, containing `1` for anomalies and `0` for normal points. # Example ```python import pandas as pd data = pd.DataFrame({ \'age\': [23, 45, 18, 33, 25, 36, 50, 40], \'income\': [40000, 50000, 30000, 48000, 52000, 60000, 58000, 62000] }) n_clusters = 3 cluster_labels, anomaly_labels = customer_segmentation_and_anomaly_detection(data, n_clusters) print(cluster_labels) print(anomaly_labels) ``` # Constraints - You must use `KMeans` from `sklearn.cluster`. - You must use `IsolationForest` from `sklearn.ensemble`. - Assume that the data contains only numerical features. # Additional Information - Refer to the scikit-learn documentation on [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) for clustering. - Refer to [IsolationForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) for anomaly detection. Good luck, and happy coding!","solution":"import pandas as pd from sklearn.cluster import KMeans from sklearn.ensemble import IsolationForest def customer_segmentation_and_anomaly_detection(data: pd.DataFrame, n_clusters: int) -> pd.Series: Perform KMeans clustering on the given data and detect anomalies in each cluster using IsolationForest. Parameters: - data: pandas DataFrame where each row represents a customer and each column represents a feature. - n_clusters: int, representing the number of clusters to form. Returns: - cluster_labels: pandas Series containing the cluster labels for each data point. - anomaly_labels: pandas Series containing 1 for anomalies and 0 for normal points within each cluster. # Step 1: KMeans Clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(data) # Step 2: Anomaly Detection using IsolationForest for each cluster anomaly_labels = pd.Series([0] * len(data), index=data.index) for cluster in range(n_clusters): cluster_data = data[cluster_labels == cluster] if len(cluster_data) > 1: # IsolationForest needs at least 2 samples isolation_forest = IsolationForest(random_state=42, contamination=0.1) cluster_anomalies = isolation_forest.fit_predict(cluster_data) anomaly_labels[cluster_data.index] = (cluster_anomalies == -1).astype(int) return pd.Series(cluster_labels, index=data.index), anomaly_labels"},{"question":"# Pandas Coding Assessment Question Objective: To assess your understanding and implementation of pandas operations, especially with the default Copy-on-Write (CoW) behavior in effect. You are required to perform a series of data manipulations ensuring compliance with CoW principles. Problem Statement: You are given a DataFrame that represents a simple sales dataset with the following columns: - `Product`: Name of the product. - `Sales`: Number of units sold. - `Revenue`: Total revenue generated from sales. - `Discount`: Discount applied on the product. Your task is to implement the following functions while adhering to the CoW principles: 1. **Function `apply_discount`** - **Input**: A DataFrame `df`, discount value `d` (float). - **Output**: A DataFrame where the `Discount` column has been updated to `d` for all products with `Sales` greater than 50 and a new column `Discounted_Revenue` which is `Revenue` after applying the discount. 2. **Function `update_sales`** - **Input**: A DataFrame `df`, a dictionary `sales_update` where keys are product names and values are the updated sales numbers. - **Output**: An updated DataFrame with the revised `Sales` figures. 3. **Function `remove_low_sales`** - **Input**: A DataFrame `df`, a threshold `t` (integer). - **Output**: A DataFrame with entries removed where `Sales` are less than `t`. Ensure that each function operates without violating any Copy-on-Write rules and provide clear comments explaining how your implementation adheres to these rules. Example Usage: ```python import pandas as pd data = { \'Product\': [\'A\', \'B\', \'C\', \'D\'], \'Sales\': [60, 20, 15, 80], \'Revenue\': [600, 200, 150, 800], \'Discount\': [0.1, 0.0, 0.2, 0.15] } df = pd.DataFrame(data) # Apply a 10% discount to products with sales greater than 50 df_with_discount = apply_discount(df, 0.1) print(df_with_discount) # Update sales figures sales_update = {\'A\': 70, \'B\': 25} updated_df = update_sales(df, sales_update) print(updated_df) # Remove products with sales less than 30 filtered_df = remove_low_sales(df, 30) print(filtered_df) ``` The student\'s solution will be evaluated based on the correctness and adherence to the Copy-on-Write principles.","solution":"import pandas as pd def apply_discount(df, d): Apply a discount to products with sales greater than 50 and add a discounted revenue column. df_copy = df.copy() df_copy.loc[df_copy[\'Sales\'] > 50, \'Discount\'] = d df_copy[\'Discounted_Revenue\'] = df_copy[\'Revenue\'] * (1 - df_copy[\'Discount\']) return df_copy def update_sales(df, sales_update): Update sales figures based on the sales_update dictionary. df_copy = df.copy() for product, sales in sales_update.items(): df_copy.loc[df_copy[\'Product\'] == product, \'Sales\'] = sales return df_copy def remove_low_sales(df, t): Remove entries where sales are less than the threshold t. df_copy = df.copy() df_copy = df_copy[df_copy[\'Sales\'] >= t] return df_copy"},{"question":"# MLPClassifier Implementation and Analysis **Objective**: Implement and evaluate a Multi-layer Perceptron (MLP) classifier using scikit-learn. **Task**: 1. **Data Preprocessing**: - Load a provided dataset (`sklearn.datasets.load_iris`). - Scale the features using `StandardScaler`. 2. **Model Training and Evaluation**: - Split the dataset into training and testing sets (80% train, 20% test). - Create an `MLPClassifier` with the following parameters: - hidden_layer_sizes: (10,) - solver: \'adam\' - max_iter: 300 - random_state: 42 - Train the model on the training data. - Evaluate the model on the test data using `accuracy_score` and `classification_report`. 3. **Visualization**: - Plot the training loss curve using the stored loss values available in `loss_curve_`. 4. **Predicting New Samples**: - Use the trained model to predict the classes for the new samples [[5.1, 3.5, 1.4, 0.2], [6.7, 3.1, 4.7, 1.5], [7.2, 3.6, 6.1, 2.5]]. **Input and Output Formats**: - Define a function `train_evaluate_mlp()` that returns a dictionary with the following keys and respective values: - `\\"accuracy\\"`: Accuracy score on the test set. - `\\"classification_report\\"`: Classification report on the test set. - `\\"predictions\\"`: Predictions for the new samples. - `\\"loss_curve\\"`: List of loss values for plotting the loss curve. **Skeleton Code**: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, classification_report import matplotlib.pyplot as plt def train_evaluate_mlp(): # Load dataset data = load_iris() X, y = data.data, data.target # Scale features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split dataset X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Initialize and train MLPClassifier mlp = MLPClassifier(hidden_layer_sizes=(10,), solver=\'adam\', max_iter=300, random_state=42) mlp.fit(X_train, y_train) # Evaluate the model y_pred = mlp.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred, output_dict=True) # Plot loss curve plt.plot(mlp.loss_curve_) plt.title(\'MLP Training Loss Curve\') plt.xlabel(\'Iterations\') plt.ylabel(\'Loss\') plt.show() # Predict new samples new_samples = [[5.1, 3.5, 1.4, 0.2], [6.7, 3.1, 4.7, 1.5], [7.2, 3.6, 6.1, 2.5]] new_samples_scaled = scaler.transform(new_samples) predictions = mlp.predict(new_samples_scaled) return { \\"accuracy\\": accuracy, \\"classification_report\\": report, \\"predictions\\": predictions, \\"loss_curve\\": mlp.loss_curve_ } # Example usage result = train_evaluate_mlp() print(\\"Accuracy:\\", result[\\"accuracy\\"]) print(\\"Classification Report:\\", result[\\"classification_report\\"]) print(\\"Predictions:\\", result[\\"predictions\\"]) ``` **Constraints**: - Ensure that all code runs within a reasonable time frame (few seconds to a minute). - Training iterations are capped at 300 to balance between model performance and runtime. **Performance Requirements**: - The accuracy on the test set should be reasonably high (typically > 85% for the Iris dataset). - The implementation should correctly handle data preprocessing, model training, evaluation, and plotting.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, classification_report import matplotlib.pyplot as plt def train_evaluate_mlp(): # Load dataset data = load_iris() X, y = data.data, data.target # Scale features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split dataset X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Initialize and train MLPClassifier mlp = MLPClassifier(hidden_layer_sizes=(10,), solver=\'adam\', max_iter=300, random_state=42) mlp.fit(X_train, y_train) # Evaluate the model y_pred = mlp.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred, output_dict=True) # Plot loss curve plt.plot(mlp.loss_curve_) plt.title(\'MLP Training Loss Curve\') plt.xlabel(\'Iterations\') plt.ylabel(\'Loss\') plt.show() # Predict new samples new_samples = [[5.1, 3.5, 1.4, 0.2], [6.7, 3.1, 4.7, 1.5], [7.2, 3.6, 6.1, 2.5]] new_samples_scaled = scaler.transform(new_samples) predictions = mlp.predict(new_samples_scaled) return { \\"accuracy\\": accuracy, \\"classification_report\\": report, \\"predictions\\": predictions, \\"loss_curve\\": mlp.loss_curve_ }"},{"question":"**Title:** Analyzing and Categorizing Student Grades **Objective:** Write a Python function `analyze_grades(data: List[Tuple[str, int]]) -> Dict[str, int]` that analyzes a list of student grades and categorizes them into predefined grade bands. **Description:** You are provided with a list of tuples, where each tuple contains a student\'s name and their grade (integer). Your task is to analyze these grades and: 1. Categorize each grade into one of the following bands: - \'A\' : 90 <= grade <= 100 - \'B\' : 80 <= grade < 90 - \'C\' : 70 <= grade < 80 - \'D\' : 60 <= grade < 70 - \'F\' : grade < 60 2. Create and return a dictionary where the keys are the grade bands (\'A\', \'B\', \'C\', \'D\', \'F\') and the values are the number of students in each band. 3. If any grade is outside the standard numerical range (0 to 100), handle it gracefully by ignoring those entries and logging an appropriate message indicating invalid data. **Function Signature:** ```python from typing import List, Tuple, Dict def analyze_grades(data: List[Tuple[str, int]]) -> Dict[str, int]: pass ``` **Input:** - `data`: A list of tuples, where each tuple contains: - A student\'s name (string) - Their grade (integer) **Output:** - A dictionary with grade bands as keys (\'A\', \'B\', \'C\', \'D\', \'F\') and the corresponding count of students in each band as values. **Constraints:** - Grades must be integers between 0 and 100 (inclusive). Any grade outside this range should be considered invalid and ignored. **Example:** ```python # Sample Input grades_data = [ (\'Alice\', 95), (\'Bob\', 82), (\'Charlie\', 70), (\'David\', 62), (\'Eva\', 59), (\'Frank\', 105), # Invalid (\'Grace\', -5) # Invalid ] result = analyze_grades(grades_data) # Expected Output # { # \'A\': 1, # \'B\': 1, # \'C\': 1, # \'D\': 1, # \'F\': 1 # } ``` **Notes:** - The function should demonstrate effective use of control flow statements. - Proper error handling for invalid grades should be included. - Make sure to include docstrings and adhere to coding style conventions as per PEP 8. - Performance considerations should account for input data size reasonably expected in a classroom setup (up to 1000 entries). **Hints:** - Utilize the `match` statement to decide the grade bands. - You may use a dictionary to keep track of counts for each grade band. - Ensure to handle the edge cases such as invalid grade values gracefully.","solution":"from typing import List, Tuple, Dict def analyze_grades(data: List[Tuple[str, int]]) -> Dict[str, int]: Analyze a list of student grades and categorize them into predefined grade bands. :param data: List of tuples, where each tuple contains a student\'s name and their grade. :return: A dictionary with grade bands as keys (\'A\', \'B\', \'C\', \'D\', \'F\') and the corresponding count of students in each band as values. grade_bands = { \'A\': 0, \'B\': 0, \'C\': 0, \'D\': 0, \'F\': 0 } for name, grade in data: if not isinstance(grade, int) or grade < 0 or grade > 100: print(f\\"Invalid grade detected for {name}: {grade}. Ignoring this entry.\\") continue if 90 <= grade <= 100: grade_bands[\'A\'] += 1 elif 80 <= grade < 90: grade_bands[\'B\'] += 1 elif 70 <= grade < 80: grade_bands[\'C\'] += 1 elif 60 <= grade < 70: grade_bands[\'D\'] += 1 else: grade_bands[\'F\'] += 1 return grade_bands"},{"question":"# Floating-Point Precision and Exact Arithmetic Problem Statement Implement functions to handle floating-point precision issues. You will write two functions that demonstrate understanding and manipulation of floating-point numbers using Python\'s `decimal` and `fractions` modules. 1. **`floating_point_sum(lst: List[float]) -> float`**: Accepts a list of floating-point numbers and returns their sum using precise summation to avoid floating-point arithmetic errors. Make use of `math.fsum()` to achieve this. 2. **`exact_representation(value: float) -> Tuple[str, str, Tuple[int, int]]`**: Accepts a floating-point number and returns a tuple containing: - Its exact decimal representation using the `decimal.Decimal` module. - Its exact hexadecimal representation using the `float.hex()` method. - A tuple representing its exact fraction form using the `float.as_integer_ratio()` method. Input - For `floating_point_sum(lst: List[float]) -> float`: - `lst`: A list of floating-point numbers. (1 <= len(lst) <= 1000 and -10^9 <= lst[i] <= 10^9) - For `exact_representation(value: float) -> Tuple[str, str, Tuple[int, int]]`: - `value`: A floating-point number. (-10^9 <= value <= 10^9) Output - For `floating_point_sum(lst: List[float]) -> float`: - A floating-point number representing the precise sum of the input list using high precision arithmetic. - For `exact_representation(value: float) -> Tuple[str, str, Tuple[int, int]]`: - A tuple containing: - A string representing the exact decimal representation of the input float. - A string representing the exact hexadecimal representation of the input float. - A tuple of two integers representing the exact fraction (numerator, denominator) of the input float. Example ```python # Example usage of floating_point_sum print(floating_point_sum([0.1, 0.2, 0.1])) # Output should be close to 0.4 avoiding floating-point errors # Example usage of exact_representation print(exact_representation(0.1)) # Output: (\'0.1000000000000000055511151231257827021181583404541015625\', # \'0x1.999999999999ap-4\', # (3602879701896397, 36028797018963968)) ``` Constraints - Do not use any third-party libraries apart from `decimal`, `fractions`, and `math`. - Ensure precise handling of numeric values to avoid common floating-point pitfalls. Evaluation Criteria - **Correctness**: Ensure that your functions return accurate results as described in the problem statement. - **Efficiency**: Implement solutions that handle the upper bounds of input sizes gracefully. - **Clarity**: Code should be well-documented and easy to understand.","solution":"from decimal import Decimal from fractions import Fraction import math def floating_point_sum(lst): Returns the precise sum of a list of floating-point numbers using high precision arithmetic. return math.fsum(lst) def exact_representation(value): Returns a tuple containing the exact decimal, hexadecimal, and fraction representation of a floating-point number. decimal_rep = str(Decimal(value)) hex_rep = value.hex() fraction_rep = value.as_integer_ratio() return (decimal_rep, hex_rep, fraction_rep)"},{"question":"Titanic Dataset Visual Analysis in Seaborn You are provided with the Titanic dataset, which includes various features about passengers. Your task is to write a function that performs specific data visualizations using seaborn. Function Specification **Function Name:** `visualize_titanic_data` **Parameters:** - `df` (pandas.DataFrame): The Titanic dataset. **Returns:** - None (the function should display plots directly). Objectives 1. **Plot 1: Class Distribution** - Create a count plot showing the distribution of passengers across different classes (`class`). 2. **Plot 2: Survived Count by Class** - Create a count plot showing the distribution of passengers across different classes (`class`), but grouped by their survival status (`survived`). 3. **Plot 3: Normalized Survived Percentage by Class** - Create a count plot showing the distribution of passengers across different classes (`class`), normalized to show the percentage of total passengers, grouped by their survival status (`survived`). # Example Usage ```python import seaborn as sns import pandas as pd # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Define the function def visualize_titanic_data(df): import seaborn as sns import matplotlib.pyplot as plt # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\") # Plot 1: Class Distribution plt.figure(figsize=(10, 6)) sns.countplot(data=df, x=\\"class\\") plt.title(\\"Distribution of Passengers Across Classes\\") plt.show() # Plot 2: Survived Count by Class plt.figure(figsize=(10, 6)) sns.countplot(data=df, x=\\"class\\", hue=\\"survived\\") plt.title(\\"Survival Count of Passengers by Class\\") plt.show() # Plot 3: Normalized Survived Percentage by Class plt.figure(figsize=(10, 6)) sns.histplot(data=df, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\", multiple=\\"stack\\") plt.title(\\"Normalized Survival Percentage by Class\\") plt.show() # Call the function visualize_titanic_data(titanic) ``` Constraints - Use only seaborn and matplotlib for visualization. - Ensure that your plots have appropriate titles and are easy to read. - Do not modify the input DataFrame (df) in the function. **Note:** The Titanic dataset can be loaded using `sns.load_dataset(\\"titanic\\")`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(df): Visualizes the Titanic dataset using three specific plots. Parameters: - df (pandas.DataFrame): The Titanic dataset. Returns: - None (displays plots directly) # Set the seaborn theme sns.set_theme(style=\\"whitegrid\\") # Plot 1: Class Distribution plt.figure(figsize=(10, 6)) sns.countplot(data=df, x=\\"class\\") plt.title(\\"Distribution of Passengers Across Classes\\") plt.show() # Plot 2: Survived Count by Class plt.figure(figsize=(10, 6)) sns.countplot(data=df, x=\\"class\\", hue=\\"survived\\") plt.title(\\"Survival Count of Passengers by Class\\") plt.show() # Plot 3: Normalized Survived Percentage by Class plt.figure(figsize=(10, 6)) sns.histplot(data=df, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\", multiple=\\"stack\\") plt.title(\\"Normalized Survival Percentage by Class\\") plt.show()"},{"question":"**Title**: Advanced Visualization with Seaborn Objects **Objective**: Create a complex bar plot visualization using `seaborn.objects` that demonstrates your understanding of dodging and jitter transformations, and the effect of handling empty spaces. **Description**: You are given a dataset containing information about tips received by servers in a restaurant. Your task is to construct a bar plot showing the total bill amount distributed over days of the week, separated by the time of the day and gender of the customer. Use different techniques to handle overlapping, empty spaces, and add some jitter for better visual clarity. **Requirements**: 1. Load the Seaborn `tips` dataset. 2. Create a bar plot to show the **total bill** (`total_bill`) for each **day** (`day`) of the week. 3. Color the bars based on **sex** (`sex`). 4. Dodge the bars based on **time** (`time`), adding a 0.2 gap between dodged bars. 5. Handle categories with empty data (non-fully crossed variables) by filling out space. 6. Apply jitter to show variability in individual data points. **Input**: Use the Seaborn `tips` dataset. **Output**: A bar plot with the above specifications. **Constraints/Additional Information**: - If you don\'t handle the empty space properly, the plot should visually represent the missing categories. - Make sure to add jitter to reduce overplotting and to provide a clear visualization of data point variability. **Performance Requirements**: The final plot should be clear, meaningfully separated by dodging, and should handle data sparsity appropriately. **Sample Code Scaffold**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot with required transformations and customizations plot = ( so.Plot(tips, \\"day\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(by=[\\"time\\"], gap=0.2, empty=\\"fill\\")) .add(so.Dot(), so.Dodge(by=[\\"time\\"]), so.Jitter()) ) # Display the plot plot.show() ``` **Notes**: - Ensure your plot effectively communicates the required data insights. - Test the plot with various transformations to ensure that it is rendered correctly and interprets the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot with required transformations and customizations plot = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\") .add(so.Bar(), so.Agg(), so.Dodge(by=[\\"time\\", \\"sex\\"], gap=0.2, empty=\\"fill\\")) .add(so.Dot(), so.Dodge(by=[\\"time\\", \\"sex\\"]), so.Jitter()) .facet(col=\\"sex\\", row=\\"time\\") ) # Display the plot plt.show()"},{"question":"# Description In this exercise, you will implement and evaluate the `TunedThresholdClassifierCV` using a custom dataset for a binary classification problem. The goal is to achieve a high recall rate for identifying the positive class, showcased with a simulated medical diagnosis problem where identifying patients with a specific condition is critical. # Problem You are provided with a dataset that simulates medical data (`X`) and corresponding diagnosis labels (`y`). You will need to: 1. Split the data into training and validation sets. 2. Fit a classifier on the training data. 3. Use the `TunedThresholdClassifierCV` to find the optimal decision threshold to maximize the recall rate for identifying the positive class. 4. Report the performance metrics on the validation data, particularly focusing on recall, precision, and the confusion matrix. # Requirements 1. **Input Data**: - `X`: Feature matrix (numpy array) - `y`: Labels vector (numpy array) 2. **Output**: - Optimal decision threshold - Recall, precision, and confusion matrix on validation data. # Constraints - Use a Logistic Regression as the base model. - Perform a train-test split with 80% of the data for training and 20% for validation. - Focus on maximizing recall during threshold tuning. # Implementation Constraints - Use `sklearn.model_selection.TunedThresholdClassifierCV`. - Use `sklearn.metrics` for calculating performance metrics. - Implement custom scorer if needed to maximize recall. # Code Template ```python import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV, train_test_split from sklearn.metrics import recall_score, precision_score, confusion_matrix, make_scorer # Sample data generation (replace with actual data) np.random.seed(42) X = np.random.randn(1000, 20) y = np.random.randint(0, 2, 1000) # Split the data into training and validation sets X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # Define the base model base_model = LogisticRegression() # Define a custom scorer for recall recall_scorer = make_scorer(recall_score) # Create the TunedThresholdClassifierCV with recall optimization tuned_model = TunedThresholdClassifierCV(base_model, scoring=recall_scorer) # Fit the model on the training data tuned_model.fit(X_train, y_train) # Evaluate on the validation data y_pred = tuned_model.predict(X_val) optimal_threshold = tuned_model.best_threshold_ recall = recall_score(y_val, y_pred) precision = precision_score(y_val, y_pred) conf_matrix = confusion_matrix(y_val, y_pred) # Output the results print(\\"Optimal Decision Threshold:\\", optimal_threshold) print(\\"Recall:\\", recall) print(\\"Precision:\\", precision) print(\\"Confusion Matrix:n\\", conf_matrix) ``` # Notes - Ensure that you document any assumptions you make. - Provide comments and explanations for your code. - Validate your results and include meaningful interpretations of the outputs.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score, precision_score, confusion_matrix, make_scorer from sklearn.model_selection import GridSearchCV class TunedThresholdClassifierCV: def __init__(self, base_model, scoring): self.base_model = base_model self.scoring = scoring def fit(self, X, y): # Fit the base model self.base_model.fit(X, y) # Get probabilities for positive class y_proba = self.base_model.predict_proba(X)[:, 1] # Tune threshold using GridSearchCV to maximize recall thresholds = np.linspace(0, 1, 101) scores = [] for threshold in thresholds: y_pred = (y_proba >= threshold).astype(int) score = self.scoring(y, y_pred) scores.append((score, threshold)) # Find the best threshold scores.sort(reverse=True) self.best_threshold_ = scores[0][1] def predict(self, X): y_proba = self.base_model.predict_proba(X)[:, 1] return (y_proba >= self.best_threshold_).astype(int) # Sample data generation (replace with actual data) np.random.seed(42) X = np.random.randn(1000, 20) y = np.random.randint(0, 2, 1000) # Split the data into training and validation sets X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # Define the base model base_model = LogisticRegression(max_iter=1000) # Define a custom scorer for recall def recall_scorer(y_true, y_pred): return recall_score(y_true, y_pred) # Create the TunedThresholdClassifierCV with recall optimization tuned_model = TunedThresholdClassifierCV(base_model, recall_scorer) # Fit the model on the training data tuned_model.fit(X_train, y_train) # Evaluate on the validation data y_pred = tuned_model.predict(X_val) optimal_threshold = tuned_model.best_threshold_ recall = recall_score(y_val, y_pred) precision = precision_score(y_val, y_pred) conf_matrix = confusion_matrix(y_val, y_pred) # Output the results print(\\"Optimal Decision Threshold:\\", optimal_threshold) print(\\"Recall:\\", recall) print(\\"Precision:\\", precision) print(\\"Confusion Matrix:n\\", conf_matrix)"},{"question":"Safe Counter with Threads # Problem Statement You are required to implement a `SafeCounter` class that allows multiple threads to increment a shared counter safely. The class should use the `_thread` module for creating threads and managing synchronization using locks. # Requirements 1. Implement the `SafeCounter` class with the following methods: - `__init__(self)`: Initializes the counter to 0 and sets up the necessary lock. - `increment(self)`: Increments the counter safely using a lock. - `get_counter(self)`: Returns the current value of the counter. 2. Implement a `worker_thread(counter, increments)` function that increments the shared counter a given number of times. This function will be used as the target function for the threads. 3. Implement a `main()` function that: - Creates an instance of `SafeCounter`. - Starts 5 threads, each incrementing the counter 1000 times. - Waits for all threads to complete. - Outputs the final value of the counter. # Constraints - You must use the `_thread` module for threading. - Use locks (_thread.allocate_lock()) to ensure the counter increments are thread-safe. - Do not use the `threading` module or any other high-level threading utilities. # Example Usage ```python if __name__ == \'__main__\': main() ``` # Expected Output Since there are 5 threads each incrementing the counter 1000 times, the final output should be: ``` Final counter value: 5000 ``` # Implementation Notes - Ensure proper synchronization using lock mechanisms provided by the `_thread` module. - Handle any potential errors gracefully, such as thread-specific errors or lock acquire timeouts (though the latter isn\'t required explicitly here). Happy coding!","solution":"import _thread import time class SafeCounter: def __init__(self): self.counter = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.counter += 1 def get_counter(self): with self.lock: return self.counter def worker_thread(counter, increments): for _ in range(increments): counter.increment() def main(): counter = SafeCounter() threads = [] increments = 1000 num_threads = 5 for _ in range(num_threads): t = _thread.start_new_thread(worker_thread, (counter, increments)) threads.append(t) # Sleep for a bit to ensure all threads finish # Simple way since we cannot join threads created by _thread module time.sleep(1) print(\\"Final counter value:\\", counter.get_counter()) if __name__ == \'__main__\': main()"},{"question":"**Question: Advanced Data Visualization with Seaborn** You are given the \\"penguins\\" dataset which contains various measurements for different species of penguins. Your task is to create a series of visualizations using Seaborn that illustrate the differences between species and highlight other variables\' impacts. Specifically, you are to perform the following tasks: 1. **Load the Dataset**: Load the \\"penguins\\" dataset using seaborn\'s `load_dataset` function. 2. **Create a Faceted Dot Plot**: - Create a faceted dot plot showing the relationship between `body_mass_g` (x-axis) and `species` (y-axis). - Color the dots by `sex`. - Add an aggregate layer with a `so.Dot()` using `so.Agg()`. - Add a range layer representing the standard deviation as error bars using `so.Range()` and `so.Est(errorbar=\\"sd\\")`. 3. **Create a Line Plot with Error Bars and Faceting**: - Create a line plot showing the relationship between `sex` (x-axis) and `body_mass_g` (y-axis). - Draw separate lines for different `species`. - Facet the plot by `species`. - Add markers to the lines using `Line(marker=\\"o\\")`. - Include error bars representing the standard deviation using `so.Range()` and `so.Est(errorbar=\\"sd\\")`. 4. **Create a Custom Range Plot**: - Create a plot showing the range between `bill_depth_mm` and `bill_length_mm` for each penguin. - Assign the minimum (`bill_depth_mm`) and maximum (`bill_length_mm`) range directly. - Color the ranges by `island`. # Implementation Guidelines - Ensure your code is well-commented. - Make use of seaborn\'s object-oriented interface (e.g., `seaborn.objects`). - Follow best practices for creating and customizing plots. - Ensure plots are well-labelled and easy to interpret. **Sample Solution Code:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # 1. Faceted Dot Plot ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .facet(\\"species\\") .show() ) # 2. Line Plot with Error Bars and Faceting ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", linestyle=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) .show() ) # 3. Custom Range Plot ( penguins .rename_axis(index=\\"penguin\\") .pipe(so.Plot, x=\\"penguin\\", ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\") .add(so.Range(), color=\\"island\\") .show() ) ``` Ensure your visualizations are clear and the statistical details are correctly represented.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # 1. Faceted Dot Plot def create_faceted_dot_plot(penguins): Create a faceted dot plot showing the relationship between body_mass_g (x-axis) and species (y-axis), colored by sex. Add aggregate and range layers with standard deviation error bars. plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .facet(\\"species\\") ) return plot # 2. Line Plot with Error Bars and Faceting def create_line_plot_with_error_bars_and_faceting(penguins): Create a line plot showing the relationship between sex (x-axis) and body_mass_g (y-axis), with separate lines for different species. Facet the plot by species, add markers, and include standard deviation error bars. plot = ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", linestyle=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) ) return plot # 3. Custom Range Plot def create_custom_range_plot(penguins): Create a plot showing the range between bill_depth_mm and bill_length_mm for each penguin, with ranges colored by island. plot = ( penguins .rename_axis(index=\\"penguin\\") .pipe(so.Plot, x=\\"penguin\\", ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\") .add(so.Range(), color=\\"island\\") ) return plot"},{"question":"**Question**: Implement a Configuration File Handler in Python **Objective**: Utilize Python\'s `configparser` module to create a robust configuration handler that can create, read, update, and manage INI files with various customization requirements. Your task involves implementing a class `ConfigHandler` that provides the following functionalities: 1. **Initialization and File Reading**: - Initialize the class with the path to the configuration file. - Read and parse the configuration file. - If the file does not exist, create an empty configuration. 2. **Adding and Updating Sections and Options**: - Add a new section to the configuration. - Add or update an option in a specific section. 3. **Removing Sections and Options**: - Remove a specific section from the configuration. - Remove a specific option from a section. 4. **Retrieving Data**: - Retrieve the value of an option from a specific section with the ability to return a default value if the option does not exist. - Retrieve all options and their values from a specific section. 5. **Interpolation**: - Support for string interpolation using both `BasicInterpolation` and `ExtendedInterpolation`. 6. **Saving the Configuration**: - Save the current state of the configuration back to the file. **Constraints**: - Assume the configuration file will have a well-defined structure. - Handle necessary exceptions like missing sections, duplicate sections, etc. - Support the addition of custom converters if necessary. The expected class and method signatures are as follows: ```python import configparser from typing import Any, Dict, Optional class ConfigHandler: def __init__(self, file_path: str, interpolation_type: str = \\"basic\\"): # Initialize the configuration parser and read the file pass def add_section(self, section: str) -> None: # Add a new section pass def set_option(self, section: str, option: str, value: Any) -> None: # Add or update an option in a section pass def remove_section(self, section: str) -> bool: # Remove a section pass def remove_option(self, section: str, option: str) -> bool: # Remove an option from a section pass def get_option(self, section: str, option: str, fallback: Optional[Any] = None) -> Any: # Retrieve an option value with a fallback pass def get_all_options(self, section: str) -> Dict[str, Any]: # Get all options and their values for a section pass def save(self) -> None: # Save the configuration to the file pass ``` **Example Usage**: ```python # Initialize the handler with a given configuration file path config_handler = ConfigHandler(\'config.ini\', interpolation_type=\'extended\') # Add a new section config_handler.add_section(\'General\') # Set options in the section config_handler.set_option(\'General\', \'user\', \'admin\') config_handler.set_option(\'General\', \'password\', \'secret\') # Retrieve an option print(config_handler.get_option(\'General\', \'user\')) # Output: admin print(config_handler.get_option(\'General\', \'non_existing_option\', fallback=\'default\')) # Output: default # Get all options in a section print(config_handler.get_all_options(\'General\')) # Output: {\'user\': \'admin\', \'password\': \'secret\'} # Save the configuration to the file config_handler.save() ``` **Note**: Handle various edge cases, such as attempting to update a non-existing section, gracefully.","solution":"import configparser import os from typing import Any, Dict, Optional class ConfigHandler: def __init__(self, file_path: str, interpolation_type: str = \\"basic\\"): self.file_path = file_path interpolation = { \'basic\': configparser.BasicInterpolation(), \'extended\': configparser.ExtendedInterpolation(), None: None }.get(interpolation_type, configparser.BasicInterpolation()) self.config = configparser.ConfigParser(interpolation=interpolation) if os.path.exists(file_path): self.config.read(file_path) else: # If file does not exist, create a new one with open(file_path, \'w\') as configfile: self.config.write(configfile) def add_section(self, section: str) -> None: if not self.config.has_section(section): self.config.add_section(section) def set_option(self, section: str, option: str, value: Any) -> None: if not self.config.has_section(section): self.config.add_section(section) self.config.set(section, option, str(value)) def remove_section(self, section: str) -> bool: return self.config.remove_section(section) def remove_option(self, section: str, option: str) -> bool: return self.config.remove_option(section, option) def get_option(self, section: str, option: str, fallback: Optional[Any] = None) -> Any: return self.config.get(section, option, fallback=fallback) def get_all_options(self, section: str) -> Dict[str, Any]: if not self.config.has_section(section): return {} return dict(self.config.items(section)) def save(self) -> None: with open(self.file_path, \'w\') as configfile: self.config.write(configfile)"},{"question":"**Objective:** Implement a simulation to understand the statistical properties of a given dataset using the `random` module. **Question:** Write a Python function `simulate_dice_rolls(trials: int, rolls_per_trial: int) -> dict` that performs the following tasks: 1. Simulate `trials` number of experiments where each experiment consists of rolling a six-sided die `rolls_per_trial` times. 2. For each trial, record the sum of the outcomes of the dice rolls. 3. Return a dictionary with the following statistics: - `mean`: The mean (average) of the sums for all trials. - `stddev`: The standard deviation of the sums for all trials. - `distribution`: A dictionary where the keys are possible sums and the values are the frequency of each sum across all trials. - `most_common_sum`: The sum that appears most frequently across all trials. **Function Signature:** ```python def simulate_dice_rolls(trials: int, rolls_per_trial: int) -> dict: pass ``` # Constraints: - `trials` should be a positive integer (1 <= trials <= 10,000). - `rolls_per_trial` should be a positive integer (1 <= rolls_per_trial <= 100). # Input: - `trials`: An integer representing the number of experiments. - `rolls_per_trial`: An integer representing the number of dice rolls per experiment. # Output: - A dictionary with keys `mean`, `stddev`, `distribution`, and `most_common_sum` as specified above. # Example: ```python results = simulate_dice_rolls(trials=1000, rolls_per_trial=10) # Example of the output dictionary: # { # \'mean\': 35.0, # \'stddev\': 5.77, # \'distribution\': {10: 1, 11: 2, 12: 5, ..., 60: 3}, # \'most_common_sum\': 35 # } print(results) ``` # Notes: - You can use the functions from the `random` module to generate random numbers and compute the required statistics. - Use the `random.randint()` function to simulate the dice rolls. - The `mean` and `stddev` should be rounded to two decimal places. - Make sure to handle edge cases where the input values might be at their minimum or maximum constraints. - Make sure your implementation is efficient for the given constraints. Good luck!","solution":"from random import randint from collections import defaultdict import math def simulate_dice_rolls(trials: int, rolls_per_trial: int) -> dict: Simulate dice rolls and return statistics about the sums across trials. Parameters: - trials: int - Number of experiments. - rolls_per_trial: int - Number of dice rolls per experiment. Returns: - dict: Dictionary containing mean, stddev, distribution, and most_common_sum. sums = [] distribution = defaultdict(int) for _ in range(trials): trial_sum = sum(randint(1, 6) for _ in range(rolls_per_trial)) sums.append(trial_sum) distribution[trial_sum] += 1 mean = sum(sums) / trials variance = sum((x - mean) ** 2 for x in sums) / trials stddev = math.sqrt(variance) most_common_sum = max(distribution, key=distribution.get) return { \'mean\': round(mean, 2), \'stddev\': round(stddev, 2), \'distribution\': dict(distribution), \'most_common_sum\': most_common_sum }"},{"question":"You are tasked with creating a function that compares two text files and outputs a human-readable diff. The diff should highlight differences line-by-line and within lines. The comparison should use the `difflib` module in Python. # Function Specification **Function Name:** `generate_diff` **Parameters:** - `file1_path` (str): The path to the first input text file. - `file2_path` (str): The path to the second input text file. - `output_format` (str): This can be one of the following values: - `\'ndiff\'`: For a line-by-line ndiff format. - `\'context\'`: For a context diff format. - `\'unified\'`: For a unified diff format. - `\'html\'`: For an HTML table diff. **Returns:** - (str): A string containing the diff of the two files in the specified format. # Constraints: - The input text files contain text data and may be large (up to several thousand lines). - The output should be efficiently generated to handle large files within a reasonable time frame. # Example: ```python def generate_diff(file1_path, file2_path, output_format): # Your code here # Example usage: diff_output = generate_diff(\'before.txt\', \'after.txt\', \'ndiff\') print(diff_output) ``` This will read the contents of \'before.txt\' and \'after.txt\', compare them, and print the human-readable difference in \'ndiff\' format. # Notes: - Make use of classes and functions provided by the `difflib` module like `Differ`, `HtmlDiff`, `context_diff`, and `unified_diff`. - Ensure proper handling of file reading and edge cases like file not found or unreadable files. - The function should not generate the diff line-by-line in the function body but should leverage the utilities provided by `difflib` to keep the implementation clean and efficient.","solution":"import difflib def generate_diff(file1_path, file2_path, output_format): try: with open(file1_path, \'r\') as file1, open(file2_path, \'r\') as file2: file1_lines = file1.readlines() file2_lines = file2.readlines() except FileNotFoundError: return \\"One of the files was not found.\\" except Exception as e: return str(e) if output_format == \'ndiff\': diff = difflib.ndiff(file1_lines, file2_lines) elif output_format == \'context\': diff = difflib.context_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path) elif output_format == \'unified\': diff = difflib.unified_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path) elif output_format == \'html\': differ = difflib.HtmlDiff() return differ.make_file(file1_lines, file2_lines, fromdesc=file1_path, todesc=file2_path) else: return \\"Unsupported output format. Use \'ndiff\', \'context\', \'unified\', or \'html\'.\\" return \'\'.join(diff)"},{"question":"# Python Coding Assessment Question Objective: Write a Python function that uses the `marshal` module to serialize a given list of Python objects to a binary file and then deserialize the content back to check its integrity. Instructions: 1. Implement a function `check_integrity(serialized_filename, objects_list)`. 2. The function should take two arguments: - `serialized_filename`: A string representing the filename where the objects will be serialized. - `objects_list`: A list of Python objects which need to be serialized. 3. The function should: - Serialize the objects in `objects_list` and write them to the file named `serialized_filename`. - Deserialize the content of the file back into Python objects. - Compare the deserialized objects with the original list (`objects_list`) to ensure they are the same. 4. Handle the necessary exceptions that might arise during serialization/deserialization (`EOFError`, `ValueError`, `TypeError`). 5. Return `True` if the deserialized objects match the original list, otherwise `False`. Constraints: - The objects in the list should only be of types supported by the `marshal` module: booleans, integers, floats, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, and code objects. Unsupported types should raise a `ValueError`. Example Usage: ```python def check_integrity(serialized_filename, objects_list): import marshal # Implement the function here # Example objects = [True, 123, 45.67, \\"example\\", [1, 2, 3], {\\"key\\": \\"value\\"}] result = check_integrity(\\"test_file.dat\\", objects) print(result) # Should print: True ``` Notes: - For the purpose of this assessment, you can assume that the file operations (writing/reading) succeed. - The student needs to handle file opening and closing properly using context managers to ensure no resource leaks.","solution":"import marshal def check_integrity(serialized_filename, objects_list): # Supported types for marshal serialization supported_types = (bool, int, float, complex, str, bytes, bytearray, tuple, list, set, frozenset, dict) try: # Check if all objects in the list are of supported types for obj in objects_list: if not isinstance(obj, supported_types): raise ValueError(f\'Object of type {type(obj)} is not supported by marshal.\') # Serialize the objects and write to file with open(serialized_filename, \'wb\') as file: marshal.dump(objects_list, file) # Deserialize the content from the file with open(serialized_filename, \'rb\') as file: deserialized_objects = marshal.load(file) # Check if the deserialized objects match the original objects return deserialized_objects == objects_list except (EOFError, ValueError, TypeError) as e: print(f\'Error during serialization/deserialization: {e}\') return False"},{"question":"You are working on an email client application and need to parse, format, and validate email addresses from various email headers. You are required to implement a function that: 1. Extracts all recipients from \'To\', \'Cc\', and any \'Resent-To\' and \'Resent-Cc\' fields provided. 2. Formats these extracted email addresses into a single string suitable for display. 3. Ensures that all email addresses are valid as per RFC 2822. # Function Signature ```python def format_email_recipients(headers: dict) -> str: pass ``` # Input - `headers`: A dictionary where keys are header names (e.g., \'To\', \'Cc\', \'Resent-To\', \'Resent-Cc\') and values are lists of email addresses. # Output - A single string containing all formatted email addresses, separated by commas. # Constraints 1. If an email address fails validation, it should be excluded from the output string. 2. If no valid email addresses are found, return an empty string. # Example ```python headers = { \'To\': [\'John Doe <john.doe@example.com>\', \'InvalidEmail <invalid-email>\'], \'Cc\': [\'Jane <jane.doe@sample.com>\'], \'Resent-To\': [\'admin@domain.org\'], \'Resent-Cc\': [] } print(format_email_recipients(headers)) # Output: \'John Doe <john.doe@example.com>,Jani <jane.doe@sample.com>,admin@domain.org\' ``` # Notes - Use `email.utils.getaddresses` to parse the email addresses from the headers. - Use `email.utils.formataddr` to format the email addresses for display. - Use `email.utils.parseaddr` to validate the email addresses. Implement the function `format_email_recipients` to achieve the described behavior, making use of appropriate functions from the `email.utils` module.","solution":"import email.utils import re def is_valid_email(email): Check if the email address is valid as per RFC 2822. pattern = re.compile( r\\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+)\\" ) return re.match(pattern, email) is not None def format_email_recipients(headers): Extracts recipients from \'To\', \'Cc\', \'Resent-To\', and \'Resent-Cc\' fields, validates, and formats them into a single string suitable for display. recipient_fields = [\'To\', \'Cc\', \'Resent-To\', \'Resent-Cc\'] all_recipients = [] for field in recipient_fields: if field in headers: addresses = email.utils.getaddresses(headers[field]) for name, addr in addresses: if is_valid_email(addr): formatted_addr = email.utils.formataddr((name, addr)) all_recipients.append(formatted_addr) return \',\'.join(all_recipients)"},{"question":"**Problem Statement**: You are required to implement a set of functions to manipulate `set` and `frozenset` objects. Your solution should demonstrate a thorough understanding of how these data structures work, including basic manipulations and some advanced functionalities. Implement the following functions: 1. **create_set**: - **Input**: A list of integers. - **Output**: A `set` containing the unique integers from the input list. - **Example**: `create_set([1, 2, 2, 3])` should return `{1, 2, 3}`. 2. **create_frozenset**: - **Input**: A list of integers. - **Output**: A `frozenset` containing the unique integers from the input list. - **Example**: `create_frozenset([1, 2, 2, 3])` should return `frozenset({1, 2, 3})`. 3. **add_element**: - **Input**: A `set` and an integer. - **Output**: The `set` with the integer added. - **Example**: `add_element({1, 2, 3}, 4)` should return `{1, 2, 3, 4}`. 4. **remove_element**: - **Input**: A `set` and an integer. - **Output**: The `set` with the integer removed if it existed, otherwise the same set. - **Example**: `remove_element({1, 2, 3}, 2)` should return `{1, 3}`. 5. **set_union**: - **Input**: Two `set` or `frozenset` objects. - **Output**: A `set` representing the union of the two sets. - **Example**: `set_union({1, 2}, {2, 3})` should return `{1, 2, 3}`. 6. **set_intersection**: - **Input**: Two `set` or `frozenset` objects. - **Output**: A `set` representing the intersection of the two sets. - **Example**: `set_intersection({1, 2}, {2, 3})` should return `{2}`. 7. **set_difference**: - **Input**: Two `set` or `frozenset` objects. - **Output**: A `set` representing the difference of the two sets (elements in the first set but not in the second). - **Example**: `set_difference({1, 2}, {2, 3})` should return `{1}`. 8. **is_subset**: - **Input**: Two `set` or `frozenset` objects. - **Output**: A boolean indicating if the first set is a subset of the second. - **Example**: `is_subset({1, 2}, {1, 2, 3})` should return `True`. **Constraints**: - You are only allowed to use the set and frozenset data structures from Python\'s standard library. - Your functions should handle edge cases, such as empty sets, appropriately. Write the implementation for all the functions mentioned above in a Python file and include appropriate test cases to validate your implementation.","solution":"def create_set(lst): Returns a set containing the unique integers from the input list. return set(lst) def create_frozenset(lst): Returns a frozenset containing the unique integers from the input list. return frozenset(lst) def add_element(s, elem): Returns the set with the integer added. s.add(elem) return s def remove_element(s, elem): Returns the set with the integer removed if it existed, otherwise the same set. s.discard(elem) return s def set_union(s1, s2): Returns a set representing the union of the two sets. return s1.union(s2) def set_intersection(s1, s2): Returns a set representing the intersection of the two sets. return s1.intersection(s2) def set_difference(s1, s2): Returns a set representing the difference of the two sets. return s1.difference(s2) def is_subset(s1, s2): Returns a boolean indicating if the first set is a subset of the second. return s1.issubset(s2)"},{"question":"# Question: Implementing and Interpreting PCA with scikit-learn Objective: Assess the ability to apply Principal Component Analysis (PCA) for dimensionality reduction and understand its implications using scikit-learn. Task: 1. **Data Preparation**: - Load the Iris dataset from `sklearn.datasets`. - Standardize the dataset (mean=0, variance=1). 2. **PCA Implementation**: - Implement PCA to reduce the dimensionality of the Iris dataset to 2 components. - Project the standardized data onto the first two principal components. 3. **Variance Explanation**: - Calculate the cumulative variance explained by the first two principal components. - Print the variance explained by each component (as a percentage). 4. **Visualization**: - Plot the 2D projection of the data with each class in the Iris dataset represented by a different color. 5. **Inverse Transformation**: - Apply the inverse transform of PCA and demonstrate the reconstructed data. Input: - None (the data should be loaded directly within the script). Output: 1. Cumulative variance explained by the first two principal components. 2. A 2D plot of the data projected onto the first two principal components. 3. Original data and reconstructed data using the inverse transform of PCA. Constraints: - Use only scikit-learn and matplotlib libraries (or any library for data handling and visualization). - Ensure your code is optimized for readability and efficiency. Notes: - The Iris dataset consists of 150 samples with 4 features each. - Your visualization should make it easy to distinguish between different classes in the dataset. # Python Function Template: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA def pca_iris(): # Step 1: Load Iris dataset iris = load_iris() X = iris.data y = iris.target # Step 2: Standardize the dataset scaler = StandardScaler() X_std = scaler.fit_transform(X) # Step 3: Implement PCA and fit the data pca = PCA(n_components=2) X_pca = pca.fit_transform(X_std) # Step 3.1: Calculate explained variance explained_variance_ratio = pca.explained_variance_ratio_ cumulative_variance = np.sum(explained_variance_ratio) * 100 print(f\\"Cumulative variance explained by the first two principal components: {cumulative_variance:.2f}%\\") # Step 4: Plot the PCA projection plt.figure(figsize=(8, 6)) for label, color in zip(np.unique(y), (\'red\', \'green\', \'blue\')): plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], label=iris.target_names[label], c=color) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA Projection of Iris Dataset\') plt.legend() plt.show() # Step 5: Inverse transform to reconstruct the data X_reconstructed = pca.inverse_transform(X_pca) # Print original and reconstructed data for comparison print(\\"Original Data (first 5 samples):n\\", X_std[:5]) print(\\"Reconstructed Data (first 5 samples):n\\", X_reconstructed[:5]) # Call the function pca_iris() ``` **Output Example:** ``` Cumulative variance explained by the first two principal components: 97.77% [2D plot of the data] Original Data (first 5 samples): [[-0.8 -1.0 0.6 -0.7] [1.6 -0.6 0.6 -0.7] ...] Reconstructed Data (first 5 samples): [[-0.7 -0.7 0.6 -0.7] [1.6 -0.6 0.6 -0.7] ...] ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA def pca_iris(): # Step 1: Load Iris dataset iris = load_iris() X = iris.data y = iris.target # Step 2: Standardize the dataset scaler = StandardScaler() X_std = scaler.fit_transform(X) # Step 3: Implement PCA and fit the data pca = PCA(n_components=2) X_pca = pca.fit_transform(X_std) # Step 3.1: Calculate explained variance explained_variance_ratio = pca.explained_variance_ratio_ cumulative_variance = np.sum(explained_variance_ratio) * 100 print(f\\"Cumulative variance explained by the first two principal components: {cumulative_variance:.2f}%\\") # Step 4: Plot the PCA projection plt.figure(figsize=(8, 6)) for label, color in zip(np.unique(y), (\'red\', \'green\', \'blue\')): plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], label=iris.target_names[label], c=color) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'PCA Projection of Iris Dataset\') plt.legend() plt.show() # Step 5: Inverse transform to reconstruct the data X_reconstructed = pca.inverse_transform(X_pca) # Print original and reconstructed data for comparison print(\\"Original Data (first 5 samples):n\\", X_std[:5]) print(\\"Reconstructed Data (first 5 samples):n\\", X_reconstructed[:5]) return explained_variance_ratio, X_pca, X_std, X_reconstructed # Call the function pca_iris()"},{"question":"# Question: Creating and Customizing Seaborn Plot You are provided with a dataset named `anscombe`, which contains four different datasets. Your task is to perform the following: 1. Load the `anscombe` dataset. 2. Initialize a seaborn plot object with `x` and `y` along with a distinct color for each dataset. 3. Create a separate subplot for each dataset using the `facet` method, with 2 plots per row. 4. Add a polynomial fit line of order 1 and a scatter plot to each subplot. 5. Customize the plot to: - Set the background color of axes to `white (w)` and the edgecolor to `slategray`. - Set the line width of the line plots to 4. - Apply the seaborn style `ticks`. 6. Finally, display the plot. **Input:** - You don\'t need to read any input from the user. Focus on manipulating and displaying the given `anscombe` dataset. **Constraints:** - You should use seaborn and matplotlib libraries for plotting. - Ensure that your solution works correctly and efficiently as per the steps given. **Output:** - Display the generated plot meeting all the customization criteria mentioned. ```python # Your implementation here ``` **Sample Execution:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Implement the required plot configuration and customization p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Set custom theme: background and edge colors p = p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) # Set line width p = p.theme({\\"lines.linewidth\\": 4}) # Apply seaborn style from seaborn import axes_style p = p.theme(axes_style(\\"ticks\\")) # Display the plot p ```","solution":"import seaborn.objects as so from seaborn import load_dataset from seaborn import axes_style def create_and_customize_seaborn_plot(): # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Implement the required plot configuration and customization p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Set custom theme: background and edge colors p = p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) # Set line width p = p.theme({\\"lines.linewidth\\": 4}) # Apply seaborn style p = p.theme(axes_style(\\"ticks\\")) # Display the plot p.show() # Use `show()` to display the plot in a script"},{"question":"You are required to write a Python program that reads a WAV file, modifies its audio data by applying a simple transformation, and then writes the transformed audio data to a new WAV file. The transformation involves inverting the audio signal, which means reversing the sign of each audio sample. Input 1. **Input File**: A string specifying the path of the input WAV file (in read mode). 2. **Output File**: A string specifying the path where the output WAV file will be saved (in write mode). Expected Function ```python def invert_wav(input_file: str, output_file: str) -> None: Reads the given input WAV file, inverts the audio signal, and writes the modified audio data to the specified output WAV file. Parameters: input_file (str): Path to the input WAV file to be read. output_file (str): Path to the output WAV file to be written. pass ``` Additional Information - Your function should handle files where the sample width is either 1 byte or 2 bytes. - You should read from the input file and write to the output file using the `wave` module. - Ensure that the function handles closing files correctly, especially in case of any errors. Constraints - Assume the input WAV file is in the correct format (WAVE_FORMAT_PCM). Example If the input WAV file contains audio frames with samples `[1, -2, 3, -4]`, the output file should have the frames with samples `[-1, 2, -3, 4]`. Evaluation Criteria - Correct implementation of WAV file reading using `wave.open`. - Proper handling of audio data and byte manipulation for inversion. - Writing the transformed audio data back into a new WAV file. - The solution should correctly close all file handles and handle exceptions gracefully. Good luck!","solution":"import wave import numpy as np def invert_wav(input_file: str, output_file: str) -> None: Reads the given input WAV file, inverts the audio signal, and writes the modified audio data to the specified output WAV file. Parameters: input_file (str): Path to the input WAV file to be read. output_file (str): Path to the output WAV file to be written. try: with wave.open(input_file, \'rb\') as infile: params = infile.getparams() n_channels, sample_width, frame_rate, n_frames, _, _ = params frames = infile.readframes(n_frames) # Convert byte data to numpy array based on sample width if sample_width == 1: audio_data = np.frombuffer(frames, dtype=np.int8) elif sample_width == 2: audio_data = np.frombuffer(frames, dtype=np.int16) else: raise ValueError(\\"Unsupported sample width\\") # Inverting audio signal inverted_data = -audio_data # Convert numpy array back to bytes inverted_frames = inverted_data.tobytes() with wave.open(output_file, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(inverted_frames) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Question: Visualizing and Transforming Datasets in Seaborn** Your task is to write a function that performs the following steps: 1. Load the \\"flights\\" dataset using seaborn\'s `load_dataset` function. 2. Visualize the dataset using seaborn\'s `relplot` function in both long-form and wide-form formats. 3. Convert the dataset to a wide-format DataFrame. 4. Plot the wide-format data using `relplot`. 5. Convert the dataset to a long-format DataFrame. 6. Plot the long-format data using `relplot`. The function should be named `visualize_flights_dataset` and should not take any input arguments. It should return a list containing two seaborn plots: the first plot should be the line plot of the wide-format data, and the second plot should be the line plot of the long-format data. **Function Signature:** ```python def visualize_flights_dataset(): pass ``` **Expected Output:** The function should return a list containing two seaborn plots: 1. Line plot of the wide-format \\"flights\\" dataset. 2. Line plot of the long-format \\"flights\\" dataset. **Constraints:** - The wide-format plot should have lines representing each month over the years. - The long-format plot should have lines representing the monthly time series for each year. Ensure that your solution adheres to these requirements by correctly transforming and plotting the dataset using seaborn. **Example:** ```python plots = visualize_flights_dataset() plots[0].show() # Showing the wide-format plot plots[1].show() # Showing the long-format plot ``` **Note:** You may assume that seaborn, pandas, and numpy are already installed in the environment where your function will run.","solution":"import seaborn as sns import pandas as pd def visualize_flights_dataset(): # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Convert the dataset to wide-format DataFrame flights_wide = flights.pivot(index=\'year\', columns=\'month\', values=\'passengers\') # Plot the wide-format data using relplot wide_plot = sns.relplot(data=flights_wide, kind=\\"line\\") # The wide-format plot should have lines representing each month over the years wide_plot.set(title=\\"Wide-format Flights Data\\") # Convert the dataset back to a long-format DataFrame (although it starts in long form) flights_long = flights # Plot the long-format data using relplot long_plot = sns.relplot(data=flights_long, x=\'year\', y=\'passengers\', hue=\'month\', kind=\\"line\\") # The long-format plot should have lines representing the monthly time series for each year long_plot.set(title=\\"Long-format Flights Data\\") return [wide_plot, long_plot]"},{"question":"# Parallel Computation with `concurrent.futures` You are required to implement a function that performs a computationally intensive task on a list of integers using the `concurrent.futures` module. The goal is to utilize parallel processing to improve performance. Task Description Write a function `compute_squares(arr: List[int], num_workers: int) -> List[int]` that accepts: 1. `arr`: A list of integers where each integer can range from (1) to (10^6). 2. `num_workers`: An integer representing the number of worker threads or processes to use. The function should return a list of squares of the input integers, computed in parallel using either threads or processes. The choice of using `ThreadPoolExecutor` or `ProcessPoolExecutor` is up to you. Ensure that the tasks are balanced among the workers, and use the `Future` objects to retrieve the results safely. Input - `arr`: A list of integers ((1 leq |arr| leq 10^6)). - `num_workers`: An integer representing the number of worker threads or processes. Output - A list of integers representing the squares of the input integers. Constraints - The function should handle large input sizes efficiently. - Ensure proper use of concurrency handling techniques. - Handle exceptions gracefully and ensure that all resources are cleaned up appropriately. Example ```python from typing import List import concurrent.futures def compute_squares(arr: List[int], num_workers: int) -> List[int]: # Your implementation here # Example usage: arr = [1, 2, 3, 4, 5] num_workers = 3 print(compute_squares(arr, num_workers)) # Output should be [1, 4, 9, 16, 25] ``` Notes - Think about whether threading or multiprocessing is more appropriate for this task. - Consider the Global Interpreter Lock (GIL) and its impact on threading. - Ensure that your function achieves parallelism efficiently and accurately.","solution":"from typing import List import concurrent.futures def square(n: int) -> int: Returns the square of the input number. return n * n def compute_squares(arr: List[int], num_workers: int) -> List[int]: Given a list of integers and a number of workers, compute the squares of the integers using parallel processing. # Using ProcessPoolExecutor for better performance on CPU-bound tasks with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor: result_futures = [executor.submit(square, n) for n in arr] results = [future.result() for future in concurrent.futures.as_completed(result_futures)] return results"},{"question":"# Custom Python Interpreter You are required to build a custom Python interpreter using the \\"code\\" module. Your task is to implement a class named `CustomInterpreter` that allows users to input Python code interactively, execute it, and see the results. The interpreter should also handle incomplete code snippets and errors gracefully. Class: `CustomInterpreter` # Methods: 1. `__init__(self)`: Initialize the interpreter. 2. `run(self)`: Start the interpreter loop where users can input code. 3. `execute(self, code: str) -> str`: Execute the given code and return the result or an error message. # Expected Behavior: - The `run` method should start an interactive loop where the user is prompted to enter Python code. - The `execute` method should execute a provided code string and return the result. If the code is incomplete, it should indicate that more input is needed. If there is an error, it should return the error message. # Input/Output Formats: - The `execute` method takes a string `code` as input and returns a string. - The `run` method handles input and output directly within an interactive session. # Example Usage: ```python interpreter = CustomInterpreter() interpreter.run() ``` Interactive Session Example: ``` >>> a = 10 >>> a + 5 15 >>> def add(x, y): ... return x + y ... >>> add(2, 3) 5 ``` # Constraints: - Your implementation should use the \\"code\\" module for creating the interpreter. - Handle incomplete code inputs by continuing to prompt the user for additional input. - Error messages should be clear and informative. # Performance Requirements: - The interactive interpreter should respond immediately to user inputs without noticeable delays. - The interpreter should efficiently handle common Python operations and error scenarios. Good luck, and show us your ability to build interactive programming tools!","solution":"import code import sys import traceback class CustomInterpreter: def __init__(self): self.interpreter = code.InteractiveInterpreter() self.buffer = [] def run(self): try: while True: try: line = input(\'>>> \' if not self.buffer else \'... \') self.buffer.append(line) source = \'n\'.join(self.buffer) more = self.interpreter.runsource(source, symbol=\'exec\') if not more: self.buffer.clear() except EOFError: # Handle closing via Ctrl+D print(\\"Exiting interpreter.\\") break except KeyboardInterrupt: # Handle closing via Ctrl+C print(\\"nKeyboardInterrupt: Exiting interpreter.\\") sys.exit() def execute(self, code: str) -> str: try: output = [] sys.stdout = sys.stderr = output_buffer = OutputBuffer() exec(code, self.interpreter.locals) sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ return output_buffer.get_output() except Exception as e: sys.stdout = sys.__stdout__ sys.stderr = sys.__stderr__ return \'\'.join(traceback.format_exception_only(type(e), e)).strip() class OutputBuffer: def __init__(self): self.output = [] def write(self, message): self.output.append(message) def get_output(self): return \'\'.join(self.output)"},{"question":"Objective: Implement a Python class that mimics the behavior and functionalities provided by the string conversion and formatting functions described in the documentation, specifically focusing on locale-independent string to double conversion and double to string conversion. Task: Create a class `StringConverter` in Python that includes the following methods: 1. `string_to_double(self, s: str) -> float`: - Converts a string `s` to a `double` (float in Python). - The method should raise a `ValueError` if the string is not a valid representation of a floating-point number. - If the string represents a value that is too large to store in a float, return `float(\'inf\')` or `-float(\'inf\')`. 2. `double_to_string(self, val: float, format_code: str, precision: int, flags: int) -> str`: - Converts a `double` (float in Python) `val` to a string using supplied `format_code`, `precision`, and `flags`. - `format_code` must be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'. For \'r\', the supplied `precision` must be 0 and is ignored. - `flags` can be 0 or more of the values `SIGN`, `ADD_DOT_0`, or `ALT`, or-ed together: - `SIGN`: Always precede the returned string with a sign character, even if `val` is non-negative. - `ADD_DOT_0`: Ensure the returned string will not look like an integer. - `ALT`: Apply alternate formatting rules (similar to how `#` works in C\'s `printf`). - Raise a `ValueError` if the `format_code` is invalid. 3. `case_insensitive_compare(self, s1: str, s2: str) -> int`: - Performs a case-insensitive comparison of two strings `s1` and `s2`. - Returns 0 if the strings are equal, a negative number if `s1` < `s2`, and a positive number if `s1` > `s2`. 4. `case_insensitive_n_compare(self, s1: str, s2: str, size: int) -> int`: - Performs a case-insensitive comparison of up to `size` characters of two strings `s1` and `s2`. - Returns 0 if the strings are equal, a negative number if `s1` < `s2`, and a positive number if `s1` > `s2`. Constraints and Limitations: - The input string for `string_to_double` should not have leading or trailing whitespace. - Format codes for `double_to_string` should be strictly validated. - Edge cases involving very large or very small floating-point numbers should be handled appropriately. - Ensure robust error handling in all methods. Example: ```python sc = StringConverter() # Example for string_to_double try: print(sc.string_to_double(\\"123.45\\")) # Output: 123.45 print(sc.string_to_double(\\"1e500\\")) # Output: inf print(sc.string_to_double(\\"abc\\")) # Should raise ValueError except ValueError as e: print(e) # Example for double_to_string print(sc.double_to_string(123.45, \'f\', 2, 0)) # Output: \'123.45\' print(sc.double_to_string(123.45, \'e\', 2, sc.SIGN)) # Output: \'+1.23e+02\' # Example for case_insensitive_compare print(sc.case_insensitive_compare(\\"Hello\\", \\"hello\\")) # Output: 0 # Example for case_insensitive_n_compare print(sc.case_insensitive_n_compare(\\"Hello\\", \\"heLLo\\", 4)) # Output: 0 ``` Note: - Implement the `StringConverter` class with appropriate methods and error handling. - Ensure the class methods adhere to the expected behavior as described. - Do not use external libraries for the string to double conversion and vice versa.","solution":"class StringConverter: SIGN = 1 << 0 # Always show sign (+/-) ADD_DOT_0 = 1 << 1 # Always show \'.0\' if no fractional part ALT = 1 << 2 # Alternate form def string_to_double(self, s: str) -> float: Converts a string `s` to a `double` (float in Python). Raises ValueError if the string is not a valid floating-point number. Returns float(\'inf\') or -float(\'inf\') if the value is too large to store in a float. try: result = float(s) return result except ValueError: raise ValueError(f\\"\'{s}\' is not a valid floating-point number\\") def double_to_string(self, val: float, format_code: str, precision: int, flags: int) -> str: Converts a `double` (float in Python) `val` to a string using supplied `format_code`, `precision`, and `flags`. if format_code not in \'eEfFgGr\': raise ValueError(f\\"Invalid format code \'{format_code}\'\\") sign = \\"\\" if flags & self.SIGN: sign = \\"+\\" if val >= 0 else \\"-\\" val = abs(val) if format_code == \'r\': return str(val) format_spec = f\\".{precision}{format_code}\\" formatted = format(val, format_spec) if flags & self.ADD_DOT_0: if \'.\' not in formatted: formatted += \\".0\\" if flags & self.ALT: if format_code in \'eE\': formatted = formatted.rstrip(\'0\').rstrip(\'.\') if \'.\' in formatted else formatted elif format_code in \'gG\': formatted = format(val, f\\"#{format_spec}\\") return sign + formatted def case_insensitive_compare(self, s1: str, s2: str) -> int: Performs a case-insensitive comparison of two strings `s1` and `s2`. s1_lower = s1.lower() s2_lower = s2.lower() if s1_lower < s2_lower: return -1 elif s1_lower > s2_lower: return 1 else: return 0 def case_insensitive_n_compare(self, s1: str, s2: str, size: int) -> int: Performs a case-insensitive comparison of up to `size` characters of two strings `s1` and `s2`. s1_sub = s1[:size].lower() s2_sub = s2[:size].lower() if s1_sub < s2_sub: return -1 elif s1_sub > s2_sub: return 1 else: return 0"},{"question":"# Image Type Detection Objective: Using your knowledge of Python\'s file handling and byte operations, implement a function to determine the type of an image file. This function should replicate and extend the functionality of the deprecated `imghdr` module. Description: Implement the function `detect_image_type(file_path: str, byte_stream: Optional[bytes] = None) -> str`, which takes the path to an image file as a mandatory argument and an optional byte-stream. If the byte-stream is provided, the function should determine the image type based on the byte-stream; otherwise, it should use the file path to read the file contents. The function should return a string describing the image type or `None` if the image type cannot be determined. You should implement checks for at least the following image types as described in the `imghdr` documentation: - \'jpeg\' - \'png\' - \'bmp\' Develop your detection mechanism, ensuring to handle the file or byte-stream appropriately. Input: - `file_path`: A string representing the path to the image file. - `byte_stream` (optional): A byte array that represents image data. Output: - A string representing the image type. Return `None` if the image type is not recognized. Constraints: - Use efficient byte operations and file handling to minimize input/output overhead. - The function should properly handle both small and large image files. Note: The use of external libraries (e.g., `PIL`) for image type detection is not allowed. The implementation should rely solely on Python’s standard library. Example: ```python def detect_image_type(file_path: str, byte_stream: Optional[bytes] = None) -> str: # Your implementation here pass # Example Usage file_path = \'path/to/image.jpeg\' print(detect_image_type(file_path)) # Output: \'jpeg\' byte_stream = b\'x89PNGrnx1anx00x00x00rIHDRx00x00x00x10\' print(detect_image_type(\'\', byte_stream)) # Output: \'png\' ```","solution":"def detect_image_type(file_path: str, byte_stream: bytes = None) -> str: Determines the type of an image file based on the file path or byte stream. Args: file_path (str): The path to the image file. byte_stream (Optional[bytes]): The byte array that represents image data. Returns: str: The image type (\'jpeg\', \'png\', \'bmp\') or None if not recognized. def read_bytes(file_path, size=32): with open(file_path, \'rb\') as f: return f.read(size) if byte_stream is None: byte_stream = read_bytes(file_path) if byte_stream.startswith(b\'xffxd8\'): return \'jpeg\' elif byte_stream.startswith(b\'x89PNGrnx1an\'): return \'png\' elif byte_stream.startswith(b\'BM\'): return \'bmp\' else: return None"},{"question":"# Dynamic Module Import and Analysis using `importlib` Problem Statement You are required to write a Python function that dynamically imports a given module and analyzes its contents. The function should also collect and return metadata information about the imported module. Function Signature ```python import importlib from importlib.metadata import distributions def analyze_module(module_name: str) -> dict: pass ``` Input - `module_name` (string): The name of the module to dynamically import and analyze. Output - The function should return a dictionary with the following keys: - `\\"functions\\"`: A list of function names defined in the module. - `\\"classes\\"`: A list of class names defined in the module. - `\\"metadata\\"`: A dictionary containing metadata about the module. The dictionary should consist of: - `\\"name\\"`: The name of the distribution package containing the module. - `\\"version\\"`: The version of the distribution package. - `\\"files\\"`: A list of files included in the distribution package. Constraints - The function should handle the case where the module cannot be imported gracefully by returning an empty dictionary. - Only public functions and classes should be considered (those not starting with an underscore `_`). Example ```python # Example usage result = analyze_module(\'json\') print(result) ``` Expected Output (keys and types shown, actual content may differ based on the module): ```python { \\"functions\\": [\\"dumps\\", \\"loads\\", \\"dump\\", \\"load\\"], \\"classes\\": [\\"JSONDecoder\\", \\"JSONEncoder\\"], \\"metadata\\": { \\"name\\": \\"json\\", \\"version\\": \\"2.0.9\\", \\"files\\": [\\"json/__init__.py\\", \\"json/decoder.py\\", \\"json/encoder.py\\"] } } ``` Notes - You may need to explore the `importlib` and `importlib.metadata` documentation to fully implement this function. - Make sure you handle edge cases, such as missing module metadata or hidden functions/classes properly.","solution":"import importlib import inspect from importlib.metadata import distributions def analyze_module(module_name: str) -> dict: try: # Dynamically import the module module = importlib.import_module(module_name) # Collect functions and classes functions = [name for name, obj in inspect.getmembers(module, inspect.isfunction) if not name.startswith(\'_\')] classes = [name for name, obj in inspect.getmembers(module, inspect.isclass) if not name.startswith(\'_\')] # Collect metadata from the distributions metadata = { \\"name\\": None, \\"version\\": None, \\"files\\": [] } for dist in distributions(): if module_name in dist.metadata[\'Name\']: metadata[\\"name\\"] = dist.metadata[\'Name\'] metadata[\\"version\\"] = dist.metadata[\'Version\'] metadata[\\"files\\"] = dist.files break return { \\"functions\\": functions, \\"classes\\": classes, \\"metadata\\": metadata } except ModuleNotFoundError: return {}"},{"question":"Objective: The objective of this assessment is to evaluate your understanding of PyTorch\'s FX module for symbolic tracing and graph transformations. You will need to demonstrate your ability to trace a given model, modify its computational graph, and verify the correctness of the transformation. Problem Description: You are given a simple PyTorch model and your task is to implement a function that replaces all instances of `torch.sub` (subtraction) operations with `torch.add` (addition) operations in the model\'s computational graph. Finally, you should verify that the transformed model behaves as expected. Requirements: 1. Write a function `replace_sub_with_add` that takes an instance of `torch.nn.Module` and returns a transformed `torch.nn.Module` where all subtractions are replaced with additions. 2. Use the PyTorch FX module for tracing and modifying the model. 3. The function should maintain the functionality of the original model but with the specified transformation. Specifications: - **Input:** - `model` (torch.nn.Module): A PyTorch model instance. - **Output:** - `transformed_model` (torch.nn.Module): A new model instance with the transformed graph. Constraints: - The input model will only contain basic operations including addition, subtraction, multiplication, and ReLU activations. Example: ```python import torch import torch.nn as nn import torch.fx as fx class SimpleModel(nn.Module): def forward(self, x, y): return torch.sub(x, y) + torch.relu(y) def replace_sub_with_add(model: nn.Module) -> nn.Module: pass # Your implementation here # Test the function model = SimpleModel() transformed_model = replace_sub_with_add(model) x = torch.randn(5) y = torch.randn(5) # Verify the transformation assert torch.allclose(transformed_model(x, y), model(x, y) + 2*y), \\"The transformed model does not produce expected results.\\" print(\\"Transformation is correct!\\") ``` Notes: - You should utilize the FX module\'s capabilities such as `symbolic_trace`, `GraphModule`, and direct graph manipulation methods. - Make sure to recompile the graph if necessary to ensure the transformation is effective. - Use `assert` statements to verify the correctness of the transformation in your tests.","solution":"import torch import torch.nn as nn import torch.fx as fx class SimpleModel(nn.Module): def forward(self, x, y): return torch.sub(x, y) + torch.relu(y) def replace_sub_with_add(model: nn.Module) -> nn.Module: Replace all `torch.sub` operations with `torch.add` in the given model\'s computational graph. class SubToAddTransform(fx.Transformer): def call_function(self, target, args, kwargs): # Replace torch.sub with torch.add if target == torch.sub: return torch.add(*args) return super().call_function(target, args, kwargs) # Trace the model to generate its computational graph traced_model = fx.symbolic_trace(model) # Apply transformation transformed_model = SubToAddTransform(traced_model).transform() return transformed_model"},{"question":"**Question: Analyze the Palmer Penguins Dataset with Seaborn** You are provided with the Palmer Penguins dataset from the seaborn package. Your task is to visualize various aspects of this dataset using seaborn\'s `displot` function, with attention to customization and advanced parameters. Please follow the steps below to complete the task. # Requirements: 1. **Load the Dataset:** Load the Palmer Penguins dataset using `sns.load_dataset(\\"penguins\\")`. 2. **Histogram Visualization:** Create a histogram of the penguin flipper lengths. 3. **KDE Plot with Hue:** Generate a KDE plot of the flipper lengths, using the `species` column to color different species. 4. **Bivariate KDE Plot with Marginal Rug:** Create a bivariate KDE plot showing the relationship between flipper length and bill length. Add a marginal rug to display individual observations. 5. **Faceted KDE Plot:** Produce a KDE plot of the flipper lengths, faceted by the `sex` column and colored by the `species` column. 6. **ECDF Plot:** Construct an ECDF plot of flipper lengths, displaying different species with different colors. 7. **Customization:** Customize one of the plots (choose any one from the above) by: - Changing the axis labels. - Setting an appropriate title. - Adjusting the height and aspect ratio of the plot. # Expected Code Implementation: Your implementation should include the following defined functions: - `load_penguins_data() -> pd.DataFrame`: This function should load and return the penguins dataset. - `plot_histogram(data: pd.DataFrame) -> None`: This function takes the dataset and creates a histogram of flipper lengths. - `plot_kde_with_hue(data: pd.DataFrame) -> None`: This function creates a KDE plot with the species as hue. - `plot_bivariate_kde(data: pd.DataFrame) -> None`: This function creates a bivariate KDE plot of flipper length vs. bill length with a marginal rug. - `plot_faceted_kde(data: pd.DataFrame) -> None`: This function creates a faceted KDE plot of flipper lengths by sex, colored by species. - `plot_ecdf(data: pd.DataFrame) -> None`: This function creates an ECDF plot of flipper lengths by species. - `customize_plot(data: pd.DataFrame) -> None`: This function demonstrates customizations on one of the previous plots (your choice). # Additional Information: - Use the Seaborn library and its `displot` function for all visualizations. - Your code should be well-documented and follow best practices for readability and efficiency. # Example Usage: ```python import seaborn as sns import pandas as pd def load_penguins_data() -> pd.DataFrame: return sns.load_dataset(\\"penguins\\") def plot_histogram(data: pd.DataFrame) -> None: sns.displot(data=data, x=\\"flipper_length_mm\\") # Additional function implementations here # Example function calls (for your reference) data = load_penguins_data() plot_histogram(data) plot_kde_with_hue(data) plot_bivariate_kde(data) plot_faceted_kde(data) plot_ecdf(data) customize_plot(data) ``` Submit your completed implementation ensuring all functions produce the required visualizations as described.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_penguins_data() -> pd.DataFrame: Load the Palmer Penguins dataset. return sns.load_dataset(\\"penguins\\") def plot_histogram(data: pd.DataFrame) -> None: Create a histogram of the flipper lengths. sns.displot(data=data, x=\\"flipper_length_mm\\") plt.show() def plot_kde_with_hue(data: pd.DataFrame) -> None: Generate a KDE plot of flipper lengths, using species as hue. sns.displot(data=data, x=\\"flipper_length_mm\\", hue=\\"species\\", kind=\\"kde\\") plt.show() def plot_bivariate_kde(data: pd.DataFrame) -> None: Create a bivariate KDE plot of flipper length vs. bill length with a marginal rug. sns.jointplot(data=data, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\", rug=True) plt.show() def plot_faceted_kde(data: pd.DataFrame) -> None: Produce a KDE plot of flipper lengths, faceted by sex and colored by species. g = sns.displot(data=data, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\") g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") plt.show() def plot_ecdf(data: pd.DataFrame) -> None: Construct an ECDF plot of flipper lengths, colored by species. sns.ecdfplot(data=data, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.show() def customize_plot(data: pd.DataFrame) -> None: Customize the KDE plot of flipper lengths by species with a title and adjusted dimensions. g = sns.displot(data=data, x=\\"flipper_length_mm\\", hue=\\"species\\", kind=\\"kde\\", height=5, aspect=1.5) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") plt.title(\\"KDE Plot of Flipper Lengths by Species\\", fontsize=16) plt.show() # Example function calls for testing if __name__ == \\"__main__\\": data = load_penguins_data() plot_histogram(data) plot_kde_with_hue(data) plot_bivariate_kde(data) plot_faceted_kde(data) plot_ecdf(data) customize_plot(data)"},{"question":"# Context Attention mechanisms are a crucial part of many modern neural network architectures, enabling models to focus on specific parts of the input sequence when producing an output. In this task, you will use PyTorch\'s experimental attention mechanism to build a simple neural network that includes attention layers. # Task Implement a neural network class `AttentionNetwork` in PyTorch, which utilizes the experimental attention APIs provided by the module `torch.nn.attention.experimental`. Your network should be able to perform a basic sequence-to-sequence task. # Requirements 1. **Network Structure**: - Implement a simple feedforward attention-based neural network. - The network should include one or more attention layers from `torch.nn.attention.experimental`. - Include at least one fully connected (linear) layer before and after the attention mechanism. 2. **Methods**: - `__init__(self, input_size, hidden_size, output_size)`: Initializes the network layers. - `forward(self, x)`: Defines the forward pass using the attention mechanism. # Specifications - **Input**: - `x`: A tensor of shape `(batch_size, sequence_length, input_size)` representing a batch of sequences. - **Output**: - A tensor of shape `(batch_size, output_size)` representing the output for each sequence. # Constraints - You must use at least one layer from `torch.nn.attention.experimental`. - Assume `input_size`, `hidden_size`, and `output_size` are all positive integers. # Example Here is a basic structure to get you started: ```python import torch import torch.nn as nn from torch.nn.attention.experimental import [Your_Chosen_Attention_Layer] class AttentionNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(AttentionNetwork, self).__init__() # Define your layers here: a linear layer, an attention layer, and another linear layer self.fc1 = nn.Linear(input_size, hidden_size) self.attention = [Your_Chosen_Attention_Layer](hidden_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): # Implement the forward pass x = self.fc1(x) x, _ = self.attention(x) x = self.fc2(x) return x ``` # Submission Submit your properly formatted Python class implementing the `AttentionNetwork`. Ensure your code is well-documented, clean, and adheres to Python and PyTorch best practices.","solution":"import torch import torch.nn as nn # Assuming torch>=1.8.0 for the sake of this task, PyTorch does not have `torch.nn.attention.experimental`. # So, I\'ll use torch.nn.MultiheadAttention which can be considered as an attention mechanism replacement. class AttentionNetwork(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(AttentionNetwork, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=1) self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.fc1(x) # (batch_size, seq_length, hidden_size) x = x.permute(1, 0, 2) # (seq_length, batch_size, hidden_size) for MultiheadAttention attn_output, _ = self.attention(x, x, x) # (seq_length, batch_size, hidden_size) attn_output = attn_output.permute(1, 0, 2) # (batch_size, seq_length, hidden_size) output = self.fc2(attn_output.mean(dim=1)) # (batch_size, output_size) return output"},{"question":"Objective Write a Python class that demonstrates understanding of scope, exception handling, and dynamic code execution using `eval()` or `exec()`. Problem Statement You are required to implement a class named `DynamicScopeHandler` that contains methods for dynamically evaluating expressions, handling exceptions, and managing variable scope. Specifically, the class should contain the following: 1. **Attributes**: - `global_vars`: A dictionary to store global variables accessible in the class. - `local_vars`: A dictionary to store local variables specific to the method scope. 2. **Methods**: - `__init__(self)`: Initializes `global_vars` with some predefined variables. - `set_local_var(self, name, value)`: Sets a variable in the `local_vars` dictionary. - `dynamic_eval(self, expr)`: Evaluates the expression `expr` using both `global_vars` and `local_vars`. If the expression refers to a variable not in scope, it should raise a custom exception `UndefinedVariableException`. - `handle_exception(self, func, *args, **kwargs)`: Executes a given function `func` with `args` and `kwargs`, and handles any exception by returning a custom error message. 3. **Exception**: - `UndefinedVariableException`: A custom exception. Implementation Details - **Custom Exception**: ```python class UndefinedVariableException(Exception): pass ``` - **Class `DynamicScopeHandler`**: - `set_local_var(self, name, value)`: This method binds the given `name` to the provided `value` in the `local_vars` dictionary. - `dynamic_eval(self, expr)`: This method should use `eval()` to dynamically evaluate `expr` considering `global_vars` and `local_vars`. If a variable in the expression is not found in either dictionary, it should raise an `UndefinedVariableException`. - `handle_exception(self, func, *args, **kwargs)`: This method executes the provided `func` with `*args` and `**kwargs`, and returns a custom error message if an exception occurs. Example Usage ```python # Define the custom exception class UndefinedVariableException(Exception): pass # Define the main class with required methods class DynamicScopeHandler: def __init__(self): self.global_vars = {\'x\': 10, \'y\': 20} self.local_vars = {} def set_local_var(self, name, value): self.local_vars[name] = value def dynamic_eval(self, expr): try: return eval(expr, {}, {**self.global_vars, **self.local_vars}) except NameError as e: raise UndefinedVariableException(f\\"Undefined variable in expression: {str(e).split()[1]}\\") def handle_exception(self, func, *args, **kwargs): try: return func(*args, **kwargs) except Exception as e: return f\\"Exception occurred: {str(e)}\\" # Example usage handler = DynamicScopeHandler() handler.set_local_var(\'z\', 5) print(handler.dynamic_eval(\'x + y + z\')) # Output: 35 print(handler.dynamic_eval(\'a + b\')) # Should raise UndefinedVariableException def sample_func(a, b): return a / b print(handler.handle_exception(sample_func, 10, 0)) # Output: Exception occurred: division by zero ``` Constraints - You must use `eval()` within `dynamic_eval`. - The `global_vars` and `local_vars` should be dictionaries. - Handle exceptions gracefully and raise meaningful error messages. Evaluation Criteria - Correctness of the implementation. - Proper use of variable scopes and exception handling. - Use of dynamic code execution techniques. - Code readability and adherence to Python best practices.","solution":"class UndefinedVariableException(Exception): Custom exception for undefined variables in expressions. pass class DynamicScopeHandler: def __init__(self): self.global_vars = {\'x\': 10, \'y\': 20} self.local_vars = {} def set_local_var(self, name, value): self.local_vars[name] = value def dynamic_eval(self, expr): try: return eval(expr, {}, {**self.global_vars, **self.local_vars}) except NameError as e: missing_var = str(e).split(\\"\'\\")[1] raise UndefinedVariableException(f\\"Undefined variable in expression: {missing_var}\\") def handle_exception(self, func, *args, **kwargs): try: return func(*args, **kwargs) except Exception as e: return f\\"Exception occurred: {str(e)}\\" # Example usage, these lines are not part of the solution # handler = DynamicScopeHandler() # handler.set_local_var(\'z\', 5) # print(handler.dynamic_eval(\'x + y + z\')) # Output: 35 # try: # print(handler.dynamic_eval(\'a + b\')) # Should raise UndefinedVariableException # except UndefinedVariableException as e: # print(e) # def sample_func(a, b): # return a / b # print(handler.handle_exception(sample_func, 10, 0)) # Output: Exception occurred: division by zero"},{"question":"# Advanced Pickling and Custom Serialization In this challenge, you are required to implement a custom serialization and deserialization process for a class that maintains an internal state not directly picklable by default. Task Description 1. Create a `TextFileHandler` class that: - Initializes with a file path. - Can read and write lines to the file. - Tracks the current position in the file, so that read operations continue from where they left off. 2. Implement custom pickling and unpickling methods for `TextFileHandler` such that: - When pickling, the file itself is not pickled, but the path and current position are. - When unpickling, the file is re-opened, and reading resumes from the saved position. Requirements - Your class should properly implement the `__getstate__()` and `__setstate__()` methods for custom pickling behavior. - The pickled object should be able to resume file operations seamlessly after being unpickled. Input and Output Formats - Input: A series of commands to operate on the `TextFileHandler` class. ``` commands = [ \\"write \'Hello, World!\'\\", \\"write \'This is a test.\'\\", \\"write \'Serialization challenge.\'\\", \\"read\\", \\"read\\", \\"pickle\\", \\"read\\", \\"unpickle\\", \\"read\\" ] ``` - Output: The output expected after executing each command. For instance, after pickling and unpickling, reading should continue from where it left off before pickling. ``` [\\"Hello, World!\\", \\"This is a test.\\", \\"Serialization challenge.\\"] ``` Constraints 1. You should handle file operations carefully to avoid any `IOError`. 2. Ensure the `__getstate__()` and `__setstate__()` methods maintain the integrity and consistency of the data and class state. 3. You can use any standard library modules as needed. Example ```python import pickle class TextFileHandler: def __init__(self, file_path): self.file_path = file_path self.file = open(file_path, \\"a+\\") self.file.seek(0) def write(self, line): self.file.write(line + \\"n\\") self.file.flush() def read(self): self.file.seek(self.file.tell()) return self.file.readline().strip() def __getstate__(self): state = self.__dict__.copy() state[\'file\'] = None state[\'position\'] = self.file.tell() return state def __setstate__(self, state): self.__dict__.update(state) self.file = open(self.file_path, \\"a+\\") self.file.seek(self.position) # Example usage: handler = TextFileHandler(\\"testfile.txt\\") handler.write(\\"Hello, World!\\") handler.write(\\"This is a test.\\") handler.write(\\"Serialization challenge.\\") print(handler.read()) # Output: Hello, World! print(handler.read()) # Output: This is a test. # Pickling the object pickled_handler = pickle.dumps(handler) # Reading next line before unpickling print(handler.read()) # Output: Serialization challenge. # Unpickling the object handler = pickle.loads(pickled_handler) print(handler.read()) # Output: Serialization challenge. ``` In this example, we demonstrate the creation of a `TextFileHandler` class, leveraging custom pickling methods to ensure the file reading position is preserved across serialization and deserialization.","solution":"import pickle class TextFileHandler: def __init__(self, file_path): self.file_path = file_path self.position = 0 self.file = open(file_path, \\"a+\\") self.file.seek(0) def write(self, line): self.file.write(line + \\"n\\") self.file.flush() def read(self): self.file.seek(self.position) line = self.file.readline() self.position = self.file.tell() return line.strip() def __getstate__(self): state = self.__dict__.copy() state[\'file\'] = None return state def __setstate__(self, state): self.__dict__.update(state) self.file = open(self.file_path, \\"a+\\") self.file.seek(self.position)"},{"question":"Buffer Protocol Manipulation Objective: Implement a Python class that exports a buffer interface and perform certain operations on the buffer. Problem Description: You are required to create a class `BufferExporter` that wraps access to a raw memory buffer. Additionally, implement functions to manipulate this buffer. Class: `BufferExporter` Your class should: 1. Initialize with a `bytearray` of specified size. 2. Expose a method `get_buffer_view` which uses the buffer protocol to return a memoryview of the underlying buffer. 3. Implement methods to write data to the buffer and read data from the buffer using the buffer protocol. # Functions and Methods: 1. **__init__(self, size: int):** - Initializes an instance with a `bytearray` of the given size. 2. **get_buffer_view(self) -> memoryview:** - Returns a `memoryview` object of the internal buffer. 3. **write_to_buffer(self, data: bytes, offset: int = 0):** - Writes `data` bytes to the buffer starting at the specified `offset`. - Raises a `ValueError` if the data does not fit within the bounds of the buffer. 4. **read_from_buffer(self, size: int, offset: int = 0) -> bytes:** - Reads `size` bytes from the buffer starting at the specified `offset`. - Raises a `ValueError` if the read goes out of bounds of the buffer. # Constraints: - The size of the buffer (bytearray) and data operations should not exceed 10^6 bytes. - Ensure all buffer operations respect bounds and raise appropriate errors for invalid accesses. Example: ```python # Initialize buffer with size 10 buffer_exporter = BufferExporter(10) # Write data to buffer buffer_exporter.write_to_buffer(b\'hello\', 0) # Get a buffer view view = buffer_exporter.get_buffer_view() print(view.tobytes()) # Should print: b\'hellox00x00x00x00x00\' # Read data from buffer data = buffer_exporter.read_from_buffer(5, 0) print(data) # Should print: b\'hello\' ``` Implementation Requirements: - You must use the buffer protocol to obtain and manipulate the memory buffer. - Enforce strict bounds checking for read and write operations.","solution":"class BufferExporter: def __init__(self, size: int): Initializes an instance with a \'bytearray\' of the given size. if size <= 0 or size > 10**6: raise ValueError(\\"Size must be between 1 and 10^6.\\") self.buffer = bytearray(size) def get_buffer_view(self) -> memoryview: Returns a \'memoryview\' object of the internal buffer. return memoryview(self.buffer) def write_to_buffer(self, data: bytes, offset: int = 0): Writes \'data\' bytes to the buffer starting at the specified \'offset\'. Raises a \'ValueError\' if the data does not fit within the bounds of the buffer. if offset < 0 or offset >= len(self.buffer): raise ValueError(\\"Offset out of bounds.\\") if len(data) + offset > len(self.buffer): raise ValueError(\\"Data does not fit within the bounds of the buffer.\\") self.buffer[offset:offset+len(data)] = data def read_from_buffer(self, size: int, offset: int = 0) -> bytes: Reads \'size\' bytes from the buffer starting at the specified \'offset\'. Raises a \'ValueError\' if the read goes out of bounds of the buffer. if offset < 0 or offset >= len(self.buffer): raise ValueError(\\"Offset out of bounds.\\") if size < 0 or offset + size > len(self.buffer): raise ValueError(\\"Read exceeds buffer bounds.\\") return bytes(self.buffer[offset:offset+size])"},{"question":"# Python Coding Assessment Introduction You are required to create a pipeline using the deprecated `pipes` module. Your task is to implement a function that constructs a pipeline to process a text file by performing the following operations in sequence: 1. Convert all text to uppercase. 2. Replace all occurrences of a specific word with another word. Given the complexities of shell pipelines and the impending deprecation of the `pipes` module, this task will challenge your understanding of both legacy and current Python capabilities. Function Signature ```python def process_text_pipeline(input_file: str, output_file: str, target_word: str, replacement_word: str) -> None: Creates a pipeline to process the given input_file by converting all text to uppercase and replacing all occurrences of target_word with replacement_word, and writes the result to output_file. Parameters: - input_file (str): The path to the input text file. - output_file (str): The path to the output text file. - target_word (str): The word in the text to replace. - replacement_word (str): The word to replace the target_word with. pass ``` Input - `input_file`: A string representing the path to the input text file. - `output_file`: A string representing the path to the output text file. - `target_word`: A string representing the word to be replaced. - `replacement_word`: A string representing the word to replace the target word with. Output - The function writes the processed text to `output_file`. Constraints 1. You must use the `pipes` module for creating the pipeline. 2. The pipeline should first convert the text to uppercase, then replace the specified word. 3. Assume the input file and output file paths are valid and accessible. 4. The program should handle large text files efficiently. Example Usage Given an `input_file` containing the text: ``` Hello world! This is a test for the coding assessment. ``` And the function call: ```python process_text_pipeline(\\"input.txt\\", \\"output.txt\\", \\"TEST\\", \\"EXPERIMENT\\") ``` The content of `output.txt` should be: ``` HELLO WORLD! THIS IS A EXPERIMENT FOR THE CODING ASSESSMENT. ``` Additional Notes - Ensure that the pipeline handles case-sensitivity correctly during the replacement. - The method should handle errors gracefully, providing meaningful messages for typical file I/O errors.","solution":"import pipes def process_text_pipeline(input_file: str, output_file: str, target_word: str, replacement_word: str) -> None: Creates a pipeline to process the given input_file by converting all text to uppercase and replacing all occurrences of target_word with replacement_word, and writes the result to output_file. pipeline = pipes.Template() pipeline.append(\'tr \\"[:lower:]\\" \\"[:upper:]\\"\', \'--\') pipeline.append(f\'sed \\"s/{target_word.upper()}/{replacement_word.upper()}/g\\"\', \'--\') try: pipeline.copy(input_file, output_file) except Exception as e: print(f\\"An error occurred while processing the file: {e}\\")"},{"question":"Implement a class `AsyncCoordinator` that manages multiple asynchronous operations using the `asyncio` event loop. Your class should handle: 1. Asynchronous I/O operations: Simulate I/O operations by using `asyncio.sleep()` to create delays. 2. Scheduling callbacks: Schedule tasks that should execute at specific future times. 3. Managing network I/O: Simulate network request handling. 4. Graceful shutdown: Ensure that all tasks and I/O operations are completed or cancelled when shutting down. # Requirements 1. **Initialization**: The class should allow initializing a set of asynchronous tasks. 2. **Add Task**: Ability to add a new asynchronous task that sleeps for a given duration and prints a message upon completion. 3. **Schedule Callback**: Schedule a callback to be executed after a specified delay. 4. **Simulate Network Request**: Simulate an asynchronous network request that receives data after a delay. 5. **Shutdown**: Gracefully shutdown, ensuring all tasks are either completed or properly cancelled. # Input and Output Specifications - **Input**: - Initialization parameters for setting up initial tasks. - Methods to add tasks, schedule callbacks, and simulate network operations. - A method to initiate the shutdown process. - **Output**: - Messages printed to the console showing the execution of tasks, callbacks, and network operations. - Confirmation that all tasks are completed or cancelled during shutdown. # Constraints - Use only the facilities provided by `asyncio` as per the provided documentation. - Ensure that the program handles potential exceptions and provides appropriate error messages. # Example Usage ```python import asyncio class AsyncCoordinator: def __init__(self, initial_tasks): self.loop = asyncio.get_event_loop() self.tasks = [] for task in initial_tasks: self.add_task(task[\'duration\'], task[\'message\']) def add_task(self, duration, message): self.tasks.append(self.loop.create_task(self._async_task(duration, message))) def schedule_callback(self, delay, callback, *args): self.loop.call_later(delay, callback, *args) async def _async_task(self, duration, message): await asyncio.sleep(duration) print(message) async def _simulate_network_request(self, duration, message): await asyncio.sleep(duration) print(f\\"Network request completed: {message}\\") def simulate_network_request(self, duration, message): task = self.loop.create_task(self._simulate_network_request(duration, message)) self.tasks.append(task) async def shutdown(self): for task in self.tasks: task.cancel() await asyncio.gather(*self.tasks, return_exceptions=True) self.loop.stop() # Example scenario async def main(): coordinator = AsyncCoordinator([ {\'duration\': 1, \'message\': \'Task 1 Completed\'}, {\'duration\': 2, \'message\': \'Task 2 Completed\'} ]) coordinator.add_task(3, \'Task 3 Completed\') coordinator.schedule_callback(1.5, print, \'Callback executed after 1.5 seconds\') coordinator.simulate_network_request(1, \'Received Data from Server\') await asyncio.sleep(3.5) await coordinator.shutdown() if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Implementation Implement the `AsyncCoordinator` class based on the provided structure above. Ensure that you handle all exceptions gracefully and provide output indicating the state transitions of the tasks and callbacks.","solution":"import asyncio class AsyncCoordinator: def __init__(self, initial_tasks): self.loop = asyncio.get_event_loop() self.tasks = [] for task in initial_tasks: self.add_task(task[\'duration\'], task[\'message\']) def add_task(self, duration, message): task = self.loop.create_task(self._async_task(duration, message)) self.tasks.append(task) def schedule_callback(self, delay, callback, *args): self.loop.call_later(delay, callback, *args) async def _async_task(self, duration, message): await asyncio.sleep(duration) print(message) async def _simulate_network_request(self, duration, message): await asyncio.sleep(duration) print(f\\"Network request completed: {message}\\") def simulate_network_request(self, duration, message): task = self.loop.create_task(self._simulate_network_request(duration, message)) self.tasks.append(task) async def shutdown(self): for task in self.tasks: task.cancel() await asyncio.gather(*self.tasks, return_exceptions=True) self.loop.stop()"},{"question":"Suppose you are designing a library for managing a collection of books and their details. Each book has an `id`, `title`, `author`, and `year` of publication. Additionally, your library system needs to support searching for books either by `id` or by `title`. You are required to create a type-safe implementation for this library using the `typing` module in Python. # Task 1. **Define a TypedDict** called `Book` with the following fields: - `id` (type: `int`) - `title` (type: `str`) - `author` (type: `str`) - `year` (type: `int`) 2. **Create a function** `add_book` that takes a list of books and a new book as input, and adds the new book to the list, ensuring there are no duplicate `id`s. 3. **Define a Protocol** called `Searchable` that mandates the implementation of a method `search_by_title` which takes a string (`title`) as input and returns a `Book`. 4. **Implement a class** `Library` that: - Stores a collection of books. - Implements the `Searchable` protocol. - Has a method `search_by_id` that takes a book `id` as input and returns the corresponding `Book`. 5. **Create a function** `use_search` that takes an object adhering to the `Searchable` protocol and a title (str) to search for, then prints out the book details if found. # Constraints - There should be no duplicate book `id`s in the list of books. - Both `id` and `title` search methods should raise a `ValueError` if no matching book is found. - The solution should be type-safe and use the type hints from the `typing` module. # Expected Input and Output Formats Input: - List of books: List of dictionaries with fields `id`, `title`, `author`, `year`. - New book: Dictionary with fields `id`, `title`, `author`, `year`. - Title: String representing the title of the book to search for. Output: - Success message when a book is added successfully. - Book details when a book is found. - Appropriate error message if a book is not found. # Example ```python from typing import TypedDict, List, Protocol class Book(TypedDict): id: int title: str author: str year: int class Searchable(Protocol): def search_by_title(self, title: str) -> Book: pass class Library(Searchable): def __init__(self) -> None: self.books: List[Book] = [] def add_book(self, new_book: Book) -> None: if any(book[\'id\'] == new_book[\'id\'] for book in self.books): raise ValueError(\\"Book with this ID already exists.\\") self.books.append(new_book) def search_by_id(self, book_id: int) -> Book: for book in self.books: if book[\'id\'] == book_id: return book raise ValueError(\\"No book found with this ID.\\") def search_by_title(self, title: str) -> Book: for book in self.books: if book[\'title\'] == title: return book raise ValueError(\\"No book found with this title.\\") def use_search(searchable: Searchable, title: str) -> None: try: book = searchable.search_by_title(title) print(f\\"Found book: {book}\\") except ValueError as e: print(e) # Example usage: library = Library() book1 = Book(id=1, title=\\"1984\\", author=\\"George Orwell\\", year=1949) book2 = Book(id=2, title=\\"Brave New World\\", author=\\"Aldous Huxley\\", year=1932) library.add_book(book1) library.add_book(book2) use_search(library, \\"1984\\") use_search(library, \\"The Catcher in the Rye\\") ``` **Note**: This example shows the expected structure and usage of the solution. You need to implement missing pieces of the solution as described in the task.","solution":"from typing import TypedDict, List, Protocol # 1. Define a TypedDict for Book class Book(TypedDict): id: int title: str author: str year: int # 2. Create a function to add a book to the library def add_book(books: List[Book], new_book: Book) -> None: if any(book[\'id\'] == new_book[\'id\'] for book in books): raise ValueError(\\"Book with this ID already exists.\\") books.append(new_book) # 3. Define a Protocol for searchable class Searchable(Protocol): def search_by_title(self, title: str) -> Book: pass # 4. Implement the Library class class Library(Searchable): def __init__(self) -> None: self.books: List[Book] = [] def add_book(self, new_book: Book) -> None: if any(book[\'id\'] == new_book[\'id\'] for book in self.books): raise ValueError(\\"Book with this ID already exists.\\") self.books.append(new_book) def search_by_id(self, book_id: int) -> Book: for book in self.books: if book[\'id\'] == book_id: return book raise ValueError(\\"No book found with this ID.\\") def search_by_title(self, title: str) -> Book: for book in self.books: if book[\'title\'] == title: return book raise ValueError(\\"No book found with this title.\\") # 5. Create a function to use the search def use_search(searchable: Searchable, title: str) -> None: try: book = searchable.search_by_title(title) print(f\\"Found book: {book}\\") except ValueError as e: print(e)"},{"question":"Objective: Implement a function using the `pty` module that simulates running an interactive shell command, captures its output, and logs it to a specified file. The function should also allow for real-time interaction with the user. Problem Statement: Write a function `run_interactive_shell_command(cmd: List[str], log_file: str) -> int` that: 1. Uses the `pty.spawn` method to run the provided shell command (`cmd`), which is given as a list of command words (e.g., `[\\"ls\\", \\"-la\\"]`). 2. Captures both the command\'s stdout and stderr output, logging it in real-time to the specified `log_file`. 3. Allows the user to interact with the command as if running it directly in the terminal. 4. Returns the exit status of the command after it completes. ```python import pty import os from typing import List def run_interactive_shell_command(cmd: List[str], log_file: str) -> int: Runs an interactive shell command, captures its output, and logs it to a specified file. Args: cmd (List[str]): A list representing the shell command to run. log_file (str): The path to the log file where command output should be recorded. Returns: int: The exit status of the command. # Implement the function pass ``` Example Usage: ```python # Example usage to run \'ls -la\' and log its output to \'output.log\' status = run_interactive_shell_command([\\"ls\\", \\"-la\\"], \\"output.log\\") print(f\\"Command exited with status: {status}\\") ``` Constraints: - The function should use the `pty.spawn` method from the `pty` module. - Ensure that the output is logged in real-time. - Handle any potential exceptions and ensure the log file closes properly. Hint: You might find the example provided in the documentation of the `pty` module helpful. Consider modifying the `read` function to also write to the log file and handling input appropriately to allow for interaction.","solution":"import pty import os from typing import List import sys def run_interactive_shell_command(cmd: List[str], log_file: str) -> int: Runs an interactive shell command, captures its output, and logs it to a specified file. Args: cmd (List[str]): A list representing the shell command to run. log_file (str): The path to the log file where command output should be recorded. Returns: int: The exit status of the command. def read(fd): with open(log_file, \\"ab\\") as f: while True: output = os.read(fd, 1024) if not output: break sys.stdout.buffer.write(output) sys.stdout.buffer.flush() f.write(output) return pty.spawn(cmd, read) # Example usage to run \'ls -la\' and log its output to \'output.log\' if __name__ == \\"__main__\\": status = run_interactive_shell_command([\\"ls\\", \\"-la\\"], \\"output.log\\") print(f\\"Command exited with status: {status}\\")"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding and proficiency in using the `tarfile` module in Python by performing a variety of operations on tar archives. # Problem Statement You are given a tar archive file that contains various files and directories. Your task is to implement a function `process_tarfile(archive_path, action, filename=None)` that can perform different operations on this tar file based on the given action. # Function Specification **Function**: `process_tarfile(archive_path, action, filename=None)` **Parameters**: 1. `archive_path` (str): The path to the tar archive file. 2. `action` (str): The action to be performed. It can be one of `[\'list\', \'extract\', \'add\', \'remove\']`. 3. `filename` (str, optional): The name of the file to be added or removed. This is required only for `add` and `remove` actions. **Return**: - For `list` action: Return a list of filenames present in the tar archive. - For `extract` action: Extract all files into a directory named \'extracted\' in the current working directory. Return the number of files extracted. - For `add` action: Add the specified `filename` to the tar archive. Ensure symbolic links are dereferenced. Return `True` if the operation succeeds. - For `remove` action: Remove the specified `filename` from the tar archive if it exists. Return `True` if the file was removed, `False` otherwise. # Constraints - You must appropriately handle all exceptions using the tarfile module’s exceptions. - File paths should be handled in a cross-platform manner. - Perform appropriate safety checks when extracting files to prevent potential security risks. - Use appropriate extraction filters to avoid extracting problematic files. # Example ```python # Given a tar archive \'archive.tar.gz\' with files [\'file1.txt\', \'file2.py\', \'dir/file3.md\'] # List files in the archive print(process_tarfile(\'archive.tar.gz\', \'list\')) # Output: [\'file1.txt\', \'file2.py\', \'dir/file3.md\'] # Extract all files print(process_tarfile(\'archive.tar.gz\', \'extract\')) # Output: 3 (number of files extracted) # Add a new file \'file4.txt\' to the archive print(process_tarfile(\'archive.tar.gz\', \'add\', \'file4.txt\')) # Output: True # Remove \'file2.py\' from the archive print(process_tarfile(\'archive.tar.gz\', \'remove\', \'file2.py\')) # Output: True ``` # Implementation Tips - Use `tarfile.open` method to open the tar archive. - For listing files, use methods like `getnames` or `getmembers`. - For extracting, use `extractall` with an appropriate filter. - For adding, use `add` method and consider a filter to dereference symbolic links. - For removing, you might need to create a new tar archive without the specific file. **Good Luck!**","solution":"import tarfile import os def process_tarfile(archive_path, action, filename=None): def list_files(tar): return tar.getnames() def extract_files(tar): extracted_directory = \'extracted\' if not os.path.exists(extracted_directory): os.makedirs(extracted_directory) tar.extractall(extracted_directory) return len(tar.getnames()) def add_file(tar, filename): if os.path.exists(filename): with tarfile.open(archive_path, \'a\') as tar_append: tar_append.add(filename, arcname=os.path.basename(filename), recursive=True) return True return False def remove_file(tar, filename): temp_tar_path = \'temp_archive.tar\' with tarfile.open(temp_tar_path, \'w\') as temp_tar: for member in tar.getmembers(): if member.name != filename: temp_tar.addfile(member, tar.extractfile(member) if member.isfile() else None) os.replace(temp_tar_path, archive_path) return filename in tar.getnames() try: with tarfile.open(archive_path, \'r\') as tar: if action == \'list\': return list_files(tar) elif action == \'extract\': return extract_files(tar) elif action == \'add\': return add_file(tar, filename) elif action == \'remove\': return remove_file(tar, filename) else: raise ValueError(\\"Invalid action specified.\\") except (tarfile.TarError, FileNotFoundError) as e: return str(e)"},{"question":"Objective: Demonstrate your understanding of the `urllib.parse` module by writing a function that processes and manipulates URLs. Task: Write a Python function `normalize_url` that takes a URL string as input and returns a normalized URL. Normalization involves the following steps: 1. Parse the input URL using `urlparse`. 2. Ensure the scheme is lowercase. 3. Normalize the netloc (make it lowercase and remove default ports for HTTP and HTTPS). 4. Ensure the path is properly quoted. 5. Sort query parameters alphabetically by key and ensure they are properly quoted. 6. Remove the fragment from the URL. # Input: - `url` (str): A URL string. # Output: - Returns a normalized URL string according to the rules specified. # Example: ```python assert normalize_url(\\"HTTP://Example.com:80/a/b;param?b=bar&a=foo#frag\\") == \\"http://example.com/a/b;param?a=foo&b=bar\\" assert normalize_url(\\"HTTP://Example.com:443/a/b;param?b=bar&a=foo#frag\\") == \\"http://example.com/a/b;param?a=foo&b=bar\\" ``` # Constraints: - The URL should strictly follow the standard URL format. - The function should handle both HTTP and HTTPS URLs. - It should maintain compliance with the RFCs as stated in the documentation (`RFC 3986`, `RFC 2396`, etc.). # Hints: - You may find the `urlparse`, `urlunparse`, `urlencode`, `parse_qsl`, and `quote` functions from the `urllib.parse` module useful. - Careful attention to detail is required to handle default ports and case normalization correctly. Test your function with various edge cases to ensure robustness.","solution":"from urllib.parse import urlparse, urlunparse, urlencode, quote, parse_qsl def normalize_url(url): Takes a URL string as input and returns a normalized URL. # Parse the input URL parsed_url = urlparse(url) # Normalize scheme and netloc scheme = parsed_url.scheme.lower() netloc = parsed_url.netloc.lower() # Remove default ports for HTTP and HTTPS if (scheme == \'http\' and netloc.endswith(\':80\')): netloc = netloc[:-3] elif (scheme == \'https\' and netloc.endswith(\':443\')): netloc = netloc[:-4] # Ensure the path is properly quoted path = quote(parsed_url.path, safe=\'/\') # Sort query parameters alphabetically by key and ensure they are properly quoted query_params = sorted(parse_qsl(parsed_url.query)) query = urlencode(query_params) # Construct the normalized URL without fragment normalized_url = urlunparse((scheme, netloc, path, parsed_url.params, query, \'\')) return normalized_url"},{"question":"**Coding Assessment Question** # Objective Implement a Python utility that serializes a list of custom objects, stores them in an SQLite database, and then reads them back while demonstrating the advanced usage of `pickle` and `sqlite3` modules. # Problem Statement You are required to create a Python script that performs the following tasks: 1. Define a custom class representing a `Book` with attributes: - `title` (string) - `author` (string) - `year` (integer) - `isbn` (string) 2. Create and serialize a list of `Book` objects using the `pickle` module. 3. Store the serialized `Book` objects into an SQLite database using the `sqlite3` module. The database should have a table `books` with columns: - `id` (integer primary key autoincrement) - `data` (blob to store the serialized `Book` object) 4. Implement functionality to read the serialized `Book` objects back from the SQLite database and unserialize them to their original form. 5. Implement search functionality in the SQLite database to retrieve `Book` objects based on their `title`. # Requirements - Use `pickle` to handle serialization and deserialization of the `Book` objects. - Use `sqlite3` to handle storing and retrieving `Book` objects in/from the database. - Efficiently handle possible exceptions related to database operations and serialization. - Serialize the entire list of books before storing it to ensure data integrity. - Test the code with at least 3 `Book` objects. # Expected Input and Output Method `store_books(book_list: List[Book], db_path: str) -> None` **Input:** - `book_list`: A list of `Book` objects to be serialized and stored. - `db_path`: Path to the SQLite database file. **Output:** - Stores the serialized `Book` objects in the specified SQLite database. Method `retrieve_books(db_path: str) -> List[Book]` **Input:** - `db_path`: Path to the SQLite database file. **Output:** - Returns a list of unserialized `Book` objects from the database. Method `search_books_by_title(db_path: str, title: str) -> List[Book]` **Input:** - `db_path`: Path to the SQLite database file. - `title`: Title of the books to search for. **Output:** - Returns a list of `Book` objects that match the given title. # Constraints - Assume all `titles` are unique for simplicity. - Handle file/database connection appropriately to avoid resource leaks. - Ensure thread-safety for database operations. # Example ```python class Book: def __init__(self, title, author, year, isbn): self.title = title self.author = author self.year = year self.isbn = isbn # Example book list books = [ Book(\\"Python 101\\", \\"John Doe\\", 2020, \\"1234567890\\"), Book(\\"Python Advanced\\", \\"Jane Doe\\", 2021, \\"0987654321\\"), Book(\\"Python Data Science\\", \\"John Smith\\", 2022, \\"1122334455\\") ] # Store books in database store_books(books, \\"books.db\\") # Retrieve books from database retrieved_books = retrieve_books(\\"books.db\\") # Search books by title searched_books = search_books_by_title(\\"books.db\\", \\"Python Advanced\\") ```","solution":"import pickle import sqlite3 from typing import List class Book: def __init__(self, title: str, author: str, year: int, isbn: str): self.title = title self.author = author self.year = year self.isbn = isbn def store_books(book_list: List[Book], db_path: str) -> None: # Serializing the book list serialized_books = pickle.dumps(book_list) # Connecting to the SQLite database conn = sqlite3.connect(db_path) cursor = conn.cursor() # Creating the table if it doesn\'t exist cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY AUTOINCREMENT, data BLOB ) \'\'\') # Inserting the serialized book list cursor.execute(\'INSERT INTO books (data) VALUES (?)\', (serialized_books,)) # Committing and closing the database connection conn.commit() conn.close() def retrieve_books(db_path: str) -> List[Book]: # Connecting to the SQLite database conn = sqlite3.connect(db_path) cursor = conn.cursor() # Retrieving the serialized book data from the database cursor.execute(\'SELECT data FROM books\') book_data = cursor.fetchone()[0] # Deserializing the book list book_list = pickle.loads(book_data) # Closing the database connection conn.close() return book_list def search_books_by_title(db_path: str, title: str) -> List[Book]: # Connecting to the SQLite database conn = sqlite3.connect(db_path) cursor = conn.cursor() # Retrieving the serialized book data from the database cursor.execute(\'SELECT data FROM books\') book_data = cursor.fetchone()[0] # Deserializing the book list book_list = pickle.loads(book_data) # Closing the database connection conn.close() # Searching for books with the given title return [book for book in book_list if book.title == title]"},{"question":"# XML Processing with \\"xml.dom.pulldom\\" **Objective:** Demonstrate your understanding of the \\"xml.dom.pulldom\\" module by writing a function to process an XML string, identify elements with specific attributes, expand nodes, and return transformed XML. **Problem Statement:** You are given an XML document in the form of a string containing various items with `price` attributes. Your task is to write a function that processes this XML string and performs the following: 1. Parse the input XML string using the `xml.dom.pulldom` module. 2. Identify `<item>` elements where the `price` attribute is greater than 100. 3. Expand such nodes to include all their children. 4. Return a list of XML strings of these expanded nodes. **Function Signature:** ```python def process_expensive_items(xml_string: str) -> List[str]: pass ``` **Input:** - `xml_string` (str): A well-formed XML document encoded as a string. **Output:** - `List[str]`: A list of XML strings, where each string represents an expanded `<item>` element with a `price` attribute greater than 100. **Constraints:** - The input XML string will have a structure similar to the following: ```xml <items> <item price=\\"50\\"><name>Item A</name></item> <item price=\\"150\\"><name>Item B</name></item> <item price=\\"200\\"><name>Item C</name></item> </items> ``` **Example:** ```python xml_input = <items> <item price=\\"50\\"><name>Item A</name></item> <item price=\\"150\\"><name>Item B</name></item> <item price=\\"200\\"><name>Item C</name></item> </items> expected_output = [ \'<item price=\\"150\\"><name>Item B</name></item>\', \'<item price=\\"200\\"><name>Item C</name></item>\' ] assert process_expensive_items(xml_input) == expected_output ``` **Additional Information:** - Utilize the `pulldom.parseString` function to initiate parsing. - Use looping and event handling to process each XML element. - Leverage the `expandNode` method to include children elements. - Ensure the function handles any depth of nested elements within `<item>`. **Performance Requirements:** - The function should efficiently process XML strings up to 1MB in size. - Aim for a solution with linear time complexity relative to the size of the XML document.","solution":"from xml.dom import pulldom from typing import List def process_expensive_items(xml_string: str) -> List[str]: doc = pulldom.parseString(xml_string) expensive_items = [] for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \\"item\\": price = int(node.getAttribute(\\"price\\")) if price > 100: doc.expandNode(node) expensive_items.append(node.toxml()) return expensive_items"},{"question":"# Unicode String Normalization and Comparison Given two Unicode strings, you need to implement a function that normalizes and compares them. The function should handle normalization using the NFC (Normalization Form C) and also manage case-insensitive comparisons. Function Signature ```python def compare_unicode_strings(s1: str, s2: str) -> bool: pass ``` Expected Input and Output Formats - **Input**: - `s1` and `s2`: Two Unicode strings that may contain characters needing normalization or case folding. - **Output**: - `bool`: Return `True` if the normalized strings are equal (case-insensitive), otherwise `False`. Constraints - You must use the `unicodedata` module for normalization. - The function should handle basic edge cases, including: - Empty strings. - Strings with different normalization composed forms. - Strings differing only in case. Example 1. Comparing two Unicode strings with different composed forms: ```python s1 = \\"ê\\" s2 = \\"u0065u0302\\" # \'e\' followed by combining circumflex assert compare_unicode_strings(s1, s2) == True ``` 2. Comparing strings that differ only in case: ```python s1 = \\"Straße\\" s2 = \\"strasse\\" assert compare_unicode_strings(s1, s2) == True ``` 3. Comparing two identical strings: ```python s1 = \\"unicode\\" s2 = \\"unicode\\" assert compare_unicode_strings(s1, s2) == True ``` 4. Comparing two different strings: ```python s1 = \\"hello\\" s2 = \\"world\\" assert compare_unicode_strings(s1, s2) == False ``` Hints - Use the `unicodedata.normalize()` function to normalize the strings to NFC form. - Use the `casefold()` method for case-insensitive comparison. Implementation Note The function should be efficient in terms of time complexity considering typical text lengths, without unnecessary repeated operations.","solution":"import unicodedata def compare_unicode_strings(s1: str, s2: str) -> bool: Normalizes two Unicode strings to NFC form and compares them case-insensitively. Args: s1 : str : First Unicode string. s2 : str : Second Unicode string. Returns: bool: True if normalized and casefolded strings are equal, otherwise False. normalized_s1 = unicodedata.normalize(\'NFC\', s1).casefold() normalized_s2 = unicodedata.normalize(\'NFC\', s2).casefold() return normalized_s1 == normalized_s2"},{"question":"Objective: To assess the understanding and proficiency of students in using the Pandas options API to configure DataFrame display settings. Problem Statement: You are given a DataFrame representing sales data for different regions over several months. You need to configure the Pandas display options so that the output meets the following requirements: 1. Set the maximum number of rows displayed to 8. 2. Set the threshold to round numbers to zero (chop threshold) to 0.001. 3. Set the maximum width of a column to 15 characters. 4. Ensure DataFrame headers are left-justified. 5. Use a context manager to temporarily set the maximum number of columns to 5 within a specific block of code. You will be provided with initial code to create the DataFrame `df` to get started. Input Format: - A DataFrame `df` created from the given code. - No direct input from the user is required; modify the display settings only. Output Format: - The DataFrame `df` displayed according to the configured settings. Constraints: - Strictly use the Pandas options API functions (`set_option`, `reset_option`, `describe_option`, `option_context`, etc.). Initial Setup Code: ```python import pandas as pd import numpy as np np.random.seed(0) # Create DataFrame data = { \'Region\': [\'North\', \'South\', \'East\', \'West\', \'North\', \'South\', \'East\', \'West\', \'North\', \'South\'], \'Month\': [\'January\', \'January\', \'January\', \'January\', \'February\', \'February\', \'February\', \'February\', \'March\', \'March\'], \'Sales\': np.random.rand(10) * 1000, \'Profit\': np.random.rand(10) * 100 } df = pd.DataFrame(data) ``` Task: Write code to configure the Pandas display options and display the DataFrame `df` according to the given specifications. After the specified block of code where maximum columns are set to 5 using a context manager, reset all public options to their default values. Example Output: ```plaintext Region Month Sales Profit 0 North January 548.8135 79.1725 1 South January 715.1894 52.8895 2 East January 602.7634 56.8045 3 West January 544.8832 92.5597 4 North February 423.6548 07.1036 5 South February 645.8941 08.7129 6 East February 437.5872 02.0218 7 West February 891.7730 83.2619 8 ... (2 more rows not shown) ``` The above representation is for reference; exact visual output will adhere to the required display settings. ```python import pandas as pd import numpy as np # Your task is to use the pandas options API to set the below configurations: # 1. Set maximum number of rows displayed to 8 # 2. Set the threshold to round numbers to zero to 0.001 # 3. Set maximum width of a column to 15 characters # 4. Ensure DataFrame headers are left-justified # 5. Use a context manager to temporarily set the maximum number of columns to 5 # Write your code below ```","solution":"import pandas as pd import numpy as np # Initial configuration code provided np.random.seed(0) # Create DataFrame data = { \'Region\': [\'North\', \'South\', \'East\', \'West\', \'North\', \'South\', \'East\', \'West\', \'North\', \'South\'], \'Month\': [\'January\', \'January\', \'January\', \'January\', \'February\', \'February\', \'February\', \'February\', \'March\', \'March\'], \'Sales\': np.random.rand(10) * 1000, \'Profit\': np.random.rand(10) * 100 } df = pd.DataFrame(data) # Setting display options pd.set_option(\'display.max_rows\', 8) pd.set_option(\'display.chop_threshold\', 0.001) pd.set_option(\'display.max_colwidth\', 15) pd.set_option(\'display.colheader_justify\', \'left\') # Using context manager to temporarily set maximum columns to 5 within this block with pd.option_context(\'display.max_columns\', 5): print(df) # Reset all public options to their default values pd.reset_option(\'all\')"},{"question":"Implementing a Custom Generic Type **Objective**: Implement a custom generic type similar to Python\'s built-in generics, using the concepts provided in the documentation. **Description**: You are tasked with creating a Python class that mimics the behavior of Python’s `GenericAlias`. The class should allow creating generic types that can hold any data type. Specifically, you need to implement a class `MyGenericAlias` that has similar attributes and behavior as `GenericAlias`. **Requirements**: 1. Create a class `MyGenericAlias` that accepts two arguments: `origin` and `args`. - `origin` should be a type. - `args` should be a tuple or any other object (if not a tuple, automatically convert it to a single-element tuple). 2. Implement the following attributes: - `__origin__`: Stores the type provided in `origin`. - `__args__`: Stores the arguments provided in `args`. - `__parameters__`: Lazily constructs from `__args__`. 3. Implement the `__class_getitem__` method to allow the creation of generic instances with specific types. **Example Usage**: ```python class MyGenericAlias: ... # Create a generic type equivalent to List[int] generic_list = MyGenericAlias(list, (int,)) print(generic_list.__origin__) # Output: <class \'list\'> print(generic_list.__args__) # Output: (<class \'int\'>,) print(generic_list.__parameters__) # Output: (<class \'int\'>,) # Using __class_getitem__ to create generics GenericDict = MyGenericAlias(dict, (str, int)) print(GenericDict.__origin__) # Output: <class \'dict\'> print(GenericDict.__args__) # Output: (<class \'str\'>, <class \'int\'>) ``` **Constraints**: - You cannot use the `types` module directly. - Ensure that the `__parameters__` attribute is created lazily. **Input Format**: - The `origin` should be a valid Python type (e.g., `list`, `dict`, etc.). - The `args` can be any object or a tuple of objects. **Output Format**: - The class should maintain the specified attributes and behaviors for generic types. **Hints**: - Utilize Python\'s `__getitem__` special method to implement generics behavior. Your function should pass the following test case: ```python generic_list = MyGenericAlias(list, (int,)) assert generic_list.__origin__ == list assert generic_list.__args__ == (int,) assert generic_list.__parameters__ == (int,) ``` Implement the `MyGenericAlias` class accordingly.","solution":"class MyGenericAlias: def __init__(self, origin, args): self.__origin__ = origin if not isinstance(args, tuple): args = (args,) self.__args__ = args self.__parameters = None @property def __parameters__(self): if self.__parameters is None: self.__parameters = self.__args__ return self.__parameters @classmethod def __class_getitem__(cls, params): if not isinstance(params, tuple): params = (params,) return MyGenericAlias(cls, params)"},{"question":"# Python 310 PyNumber API Usage Background You have been introduced to the \\"PyNumber\\" API in Python, which provides a set of functions to perform arithmetic and bitwise operations on objects that support numeric protocols, closely mimicking Python’s built-in operators and methods. Your task is to use these functions to implement a series of operations on a list of numeric objects. Objective Create a function `perform_calculations(operations)`. This function will take a list of tuples, each containing a PyNumber function name as a string, followed by the necessary arguments for that function. Inputs - `operations` : A list of tuples. Each tuple contains: - The name of the PyNumber function to call as a string (e.g., \'PyNumber_Add\'). - The necessary arguments for that function as Python objects. Example: ```python operations = [ (\'PyNumber_Add\', 3, 5), (\'PyNumber_Subtract\', 10, 4), (\'PyNumber_Multiply\', 6, 3), (\'PyNumber_TrueDivide\', 8, 2), (\'PyNumber_FloorDivide\', 9, 2), (\'PyNumber_Power\', 2, 3, None) ] ``` Output - A list of results, each corresponding to the respective operation in the input list. If any operation fails, return the string \\"Error\\" for that operation. Example: ```python result = perform_calculations(operations) ``` For the sample input above, the expected result would be: ```python [8, 6, 18, 4.0, 4, 8] ``` Constraints 1. You must use the specified PyNumber functions in your implementation. 2. Handle error cases gracefully by returning \\"Error\\" for the failed operation\'s result. 3. Assume the PyNumber functions and necessary conversions are available as defined in the documentation. Function Signature ```python def perform_calculations(operations: list) -> list: # Your code here ``` Implement the `perform_calculations` function.","solution":"import operator def perform_calculations(operations): Perform a series of arithmetic or bitwise operations as specified in the input list. Each list entry should be a tuple containing a function name and the necessary arguments for that function. Args: operations (list): A list of tuples where each tuple contains a function name followed by the necessary arguments for that function. Returns: list: A list containing the results of each operation. # Map of operation names to the actual functions operation_map = { \'PyNumber_Add\': operator.add, \'PyNumber_Subtract\': operator.sub, \'PyNumber_Multiply\': operator.mul, \'PyNumber_TrueDivide\': operator.truediv, \'PyNumber_FloorDivide\': operator.floordiv, \'PyNumber_Power\': operator.pow, } results = [] for operation in operations: try: func_name, *args = operation func = operation_map[func_name] result = func(*args) except Exception: result = \\"Error\\" results.append(result) return results"},{"question":"You are required to create a Python module that utilizes the **readline** module\'s capabilities to enhance the user interaction in an interactive Python shell session. Your task is to implement a function `interactive_shell_with_custom_history(filename: str, max_history_length: int) -> None` that sets up an interactive shell session with custom settings: 1. **History Management**: - Load the command history from a specified history file. If the file does not exist, create it. - Ensure that the history is truncated to a maximum number of commands as specified by the `max_history_length` parameter. - Save the command history back to the same file upon exiting the interactive session, ensuring that any commands entered in the session are appended to the file if it already exists. 2. **Custom Keybindings**: - Configure the readline module to use `vi` keybindings. 3. **Custom Completion**: - Implement a simple custom completer function that suggests words from a predefined list of words whenever the Tab key is pressed. - Example words for completion: [\\"start\\", \\"stop\\", \\"status\\", \\"restart\\", \\"reload\\"] Function Signature: ```python import readline import atexit import os def interactive_shell_with_custom_history(filename: str, max_history_length: int) -> None: pass ``` Constraints: - `filename` is a valid string representing the path to the history file. - `max_history_length` is an integer greater than 0. Additional Notes: - Your implementation should ensure that the interactive session behaves as a normal Python shell. - The startup configuration should take effect as soon as the function is called and should persist across different runs of the function. # Example Usage: ```python if __name__ == \\"__main__\\": interactive_shell_with_custom_history(\\"my_python_history.txt\\", 1000) ``` This should launch an interactive Python shell with the described enhancements.","solution":"import readline import atexit import os def interactive_shell_with_custom_history(filename: str, max_history_length: int) -> None: Sets up an interactive shell session with custom command history management, keybindings, and word completion. Parameters: - filename: str, the path to the history file. - max_history_length: int, the maximum number of commands to keep in the history. # Ensure history file exists if not os.path.exists(filename): open(filename, \'wb\').close() # Setting vi keybindings readline.parse_and_bind(\\"set editing-mode vi\\") # Define custom completer words = [\\"start\\", \\"stop\\", \\"status\\", \\"restart\\", \\"reload\\"] def completer(text, state): matches = [word for word in words if word.startswith(text)] return matches[state] if state < len(matches) else None readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") # Load history file readline.read_history_file(filename) readline.set_history_length(max_history_length) # Save history on exit def save_history(): readline.write_history_file(filename) atexit.register(save_history) # Start interactive shell session import code code.interact(local=dict(globals(), **locals())) if __name__ == \\"__main__\\": interactive_shell_with_custom_history(\\"my_python_history.txt\\", 1000)"},{"question":"**Objective:** You are required to demonstrate your understanding of Seaborn\'s visualization techniques by implementing a function that generates a customized bar plot from a dataset. **Requirements:** 1. **Function Name:** `generate_custom_barplot` 2. **Inputs:** - `dataset_name` (string): The name of the dataset to load from Seaborn\'s library (e.g., \\"penguins\\" or \\"flights\\"). - `x` (string): The name of the column to be plotted on the x-axis. - `y` (string): The name of the column to be plotted on the y-axis. - `hue` (string, optional): The name of the column to be used for hue grouping. - `estimator` (function or string, optional): The statistical function to estimate within each categorical bin (e.g., \\"mean\\" or \\"sum\\"). - `show_error_bars` (boolean, optional): Whether to display error bars. - `facet_col` (string, optional): The name of the column to be used for faceting the plot. 3. **Outputs:** - A customized bar plot displayed using Seaborn. **Functionality:** - Load the specified dataset from Seaborn. - Generate a bar plot with the specified `x` and `y` columns. - If `hue` is provided, use it to add a second layer of grouping. - If `estimator` is provided, use it for aggregating the data. - Show or hide error bars based on `show_error_bars`. - If `facet_col` is provided, use `catplot` to create a faceted bar plot. - Customize the appearance by adding annotations and setting rectangle and line properties as shown in the examples. **Constraints:** - Assume that the `dataset_name` provided will always be valid and available in the Seaborn library. - The function should be efficient and handle datasets of various sizes. **Example:** ```python def generate_custom_barplot(dataset_name, x, y, hue=None, estimator=\\"mean\\", show_error_bars=True, facet_col=None): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme(style=\\"whitegrid\\") # Load dataset data = sns.load_dataset(dataset_name) # Create a bar plot if facet_col: g = sns.catplot( data=data, kind=\\"bar\\", x=x, y=y, hue=hue, col=facet_col, estimator=estimator if estimator != \\"mean\\" else \\"mean\\", errorbar=\\"sd\\" if show_error_bars else None, height=4, aspect=0.5, ) else: ax = sns.barplot( data=data, x=x, y=y, hue=hue, estimator=estimator if estimator != \\"mean\\" else \\"mean\\", errorbar=\\"sd\\" if show_error_bars else None, ) # Customize the appearance by adding annotations if show_error_bars: ax.bar_label(ax.containers[0], fontsize=10) # Additional customization with Rectangle and Line2D if needed plt.show() # Example usage: generate_custom_barplot(\\"penguins\\", \\"island\\", \\"body_mass_g\\", hue=\\"sex\\", show_error_bars=False, facet_col=\\"species\\") ``` The provided example demonstrates the function\'s capability to generate a customized bar plot by loading the `penguins` dataset from Seaborn and creating a faceted plot grouped by species with no error bars showing.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_custom_barplot(dataset_name, x, y, hue=None, estimator=\\"mean\\", show_error_bars=True, facet_col=None): Generates a customized bar plot from a Seaborn dataset. Parameters: - dataset_name (str): The name of the dataset to load. - x (str): The name of the column to be plotted on the x-axis. - y (str): The name of the column to be plotted on the y-axis. - hue (str, optional): The name of the column to be used for hue grouping. - estimator (str or function, optional): The statistical function to estimate within each categorical bin. - show_error_bars (bool, optional): Whether to display error bars. - facet_col (str, optional): The name of the column to be used for faceting the plot. sns.set_theme(style=\\"whitegrid\\") # Load dataset data = sns.load_dataset(dataset_name) # Create a bar plot if facet_col: g = sns.catplot( data=data, kind=\\"bar\\", x=x, y=y, hue=hue, col=facet_col, estimator=estimator if estimator != \\"mean\\" else \\"mean\\", errorbar=(\\"sd\\" if show_error_bars else None), height=4, aspect=1, ) else: ax = sns.barplot( data=data, x=x, y=y, hue=hue, estimator=estimator if estimator != \\"mean\\" else \\"mean\\", errorbar=(\\"sd\\" if show_error_bars else None), ) if show_error_bars: ax.bar_label(ax.containers[0], fontsize=10) plt.show()"},{"question":"# Multi-threading with `_thread` Module You are tasked with creating a multi-threaded Python application to simulate a basic banking system where multiple clients (threads) can deposit and withdraw money from a shared bank account. The solution must ensure thread safety by using the `_thread` module for thread management and synchronization. Requirements: 1. **BankAccount Class**: - Create a class `BankAccount` with the following methods: - `deposit(amount)`: Adds the specified `amount` to the account balance. - `withdraw(amount)`: Subtracts the specified `amount` from the account balance. If the amount is greater than the current balance, raise a `ValueError`. - `get_balance()`: Returns the current balance. - Use `_thread.allocate_lock()` to manage synchronization, ensuring that deposits and withdrawals are thread-safe. 2. **Client Thread Function**: - Create a function `client_actions(account, actions)` to be executed by each thread, where: - `account` is an instance of `BankAccount`. - `actions` is a list of tuples describing the actions to perform. Each tuple can be either `(\'deposit\', amount)` or `(\'withdraw\', amount)`. 3. **Main Function**: - Create a `main()` function to: - Instantiate a `BankAccount`. - Create and start 5 threads, each performing a series of random deposits or withdrawals on the account. - Ensure all threads complete their actions before printing the final account balance. Input Format: - No input is required from the user. All parameters should be generated within the code. Output Format: - Print the final balance of the `BankAccount` after all threads have completed their actions. Constraints: - The initial balance of the account is 1000. - Each action in the `actions` list should involve amounts between 1 and 500. Here is a skeleton to help you start: ```python import _thread import time import random class BankAccount: def __init__(self): self.balance = 1000 self.lock = _thread.allocate_lock() def deposit(self, amount): with self.lock: self.balance += amount def withdraw(self, amount): with self.lock: if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): with self.lock: return self.balance def client_actions(account, actions): for action in actions: if action[0] == \'deposit\': account.deposit(action[1]) elif action[0] == \'withdraw\': try: account.withdraw(action[1]) except ValueError as e: print(e) time.sleep(random.uniform(0.1, 0.5)) def main(): account = BankAccount() threads = [] for _ in range(5): actions = [(\'deposit\', random.randint(1, 500)) if random.random() < 0.5 else (\'withdraw\', random.randint(1, 500)) for _ in range(10)] thread = _thread.start_new_thread(client_actions, (account, actions)) threads.append(thread) # Optional: Implement a way to join threads or wait for all threads to complete. # Python\'s _thread module does not provide a direct join mechanism like the threading module. # You could use a simple loop with sleep to check for active threads or some other mechanism. # For demonstration purposes, we use sleep to ensure all threads complete their work time.sleep(5) # Adjust the sleep duration if necessary to ensure all actions complete print(f\\"Final account balance: {account.get_balance()}\\") if __name__ == \\"__main__\\": main() ```","solution":"import _thread import time import random class BankAccount: def __init__(self): self.balance = 1000 self.lock = _thread.allocate_lock() def deposit(self, amount): with self.lock: self.balance += amount def withdraw(self, amount): with self.lock: if amount > self.balance: raise ValueError(\\"Insufficient funds\\") self.balance -= amount def get_balance(self): with self.lock: return self.balance def client_actions(account, actions): for action in actions: if action[0] == \'deposit\': account.deposit(action[1]) elif action[0] == \'withdraw\': try: account.withdraw(action[1]) except ValueError as e: print(e) time.sleep(random.uniform(0.1, 0.5)) def main(): account = BankAccount() threads = [] for _ in range(5): actions = [(\'deposit\', random.randint(1, 500)) if random.random() < 0.5 else (\'withdraw\', random.randint(1, 500)) for _ in range(10)] _thread.start_new_thread(client_actions, (account, actions)) # Wait for threads to complete time.sleep(5) # Adjust if necessary print(f\\"Final account balance: {account.get_balance()}\\") if __name__ == \\"__main__\\": main()"},{"question":"# TorchScript Coding Assessment **Objective**: Write a TorchScript function to perform table-specific operations on data stored in different PyTorch Tensors. This task will assess your understanding of type annotations, structural types, and TorchScript operations. **Problem Statement**: You are given a Python class `DataTable` that represents a table with two columns of numeric data. The class should provide methods to: 1. **Initialize** the table with two columns. 2. **Add** a new row of data. 3. **Calculate** the sum of each column. 4. **Return** a specific column as a List of float values based on a provided index (`0` or `1`). Here are the detailed requirements: 1. **Class Definition**: - Class Name: `DataTable` - Attributes: - `column1: List[float]` - `column2: List[float]` 2. **Methods**: - `__init__(self, column1: List[float], column2: List[float])`: - Initialize the DataTable with two columns. - `add_row(self, value1: float, value2: float) -> None`: - Add a new row to the table. - `column_sum(self) -> Tuple[float, float]`: - Return the sum of each column as a Tuple. - `get_column(self, index: int) -> List[float]`: - Return the specified column as a List of float values. Index `0` for `column1` and `1` for `column2`. Raise `ValueError` for any other index. **Input Format**: - Initialization of class: Two lists of floats representing the columns of the table. - Method `add_row`: Two float values to be added to the respective columns. - Method `get_column`: An integer index (`0` or `1`). **Output Format**: - Method `column_sum`: A tuple of two floats representing the sum of `column1` and `column2`. - Method `get_column`: A list of floats corresponding to the specified column. **Constraints**: - The lengths of `column1` and `column2` during initialization should be equal. **Performance Requirements**: - Efficient addition and retrieval of rows and column sums. # Example Usage ```python import torch from typing import List, Tuple @torch.jit.script class DataTable: def __init__(self, column1: List[float], column2: List[float]) -> None: self.column1 = column1 self.column2 = column2 def add_row(self, value1: float, value2: float) -> None: self.column1.append(value1) self.column2.append(value2) def column_sum(self) -> Tuple[float, float]: return (sum(self.column1), sum(self.column2)) def get_column(self, index: int) -> List[float]: if index == 0: return self.column1 elif index == 1: return self.column2 else: raise ValueError(\\"Invalid column index\\") # Example usage table = DataTable([1.0, 2.0], [3.0, 4.0]) table.add_row(5.0, 6.0) print(table.column_sum()) # Output: (8.0, 13.0) print(table.get_column(0)) # Output: [1.0, 2.0, 5.0] ``` **Note**: Make sure to script the class using `@torch.jit.script`.","solution":"import torch from typing import List, Tuple @torch.jit.script class DataTable: def __init__(self, column1: List[float], column2: List[float]) -> None: # Validate column lengths if len(column1) != len(column2): raise ValueError(\\"Columns must have the same length\\") self.column1 = column1 self.column2 = column2 def add_row(self, value1: float, value2: float) -> None: self.column1.append(value1) self.column2.append(value2) def column_sum(self) -> Tuple[float, float]: return (float(sum(self.column1)), float(sum(self.column2))) def get_column(self, index: int) -> List[float]: if index == 0: return self.column1 elif index == 1: return self.column2 else: raise ValueError(\\"Invalid column index\\")"},{"question":"**Objective**: Implement a robust Python function to fetch web resources using the `urllib.request` module. Your function should handle different HTTP methods, manage exceptions gracefully, and support custom headers. Problem Statement Write a function `fetch_web_resource(url: str, method: str, data: dict = None, headers: dict = None, timeout: int = 10) -> str` that performs the following tasks: 1. Fetches a web resource from the given URL. 2. Supports both `GET` and `POST` HTTP methods. 3. Accepts optional data for `POST` requests as a dictionary. 4. Accepts optional HTTP headers as a dictionary. 5. Includes exception handling for network-related errors (`URLError`) and HTTP-specific errors (`HTTPError`). 6. Uses a default timeout of 10 seconds but allows customization. 7. Returns the response content as a string. Input - `url`: A string representing the URL to fetch. - `method`: A string representing the HTTP method, either `\\"GET\\"` or `\\"POST\\"`. - `data`: An optional dictionary of data to send in the case of a `POST` request. - `headers`: An optional dictionary of HTTP headers to include in the request. - `timeout`: An optional integer specifying the timeout in seconds. Output - A string representing the content of the response. Exceptions - Raise a `ValueError` if the method is not `\\"GET\\"` or `\\"POST\\"`. - Handle `URLError` and `HTTPError` exceptions and provide meaningful error messages. Constraints - Python 3.10 - Use only the `urllib` library for fetching resources (no external HTTP libraries like `requests`). Example ```python def fetch_web_resource(url: str, method: str, data: dict = None, headers: dict = None, timeout: int = 10) -> str: # Your implementation here # Example usage url = \\"https://jsonplaceholder.typicode.com/posts\\" method = \\"POST\\" data = {\\"title\\": \\"foo\\", \\"body\\": \\"bar\\", \\"userId\\": 1} headers = {\\"Content-Type\\": \\"application/json\\"} response = fetch_web_resource(url, method, data, headers) print(response) ``` # Evaluation Criteria - Correctness: The function correctly handles `GET` and `POST` methods, processes optional data and headers, and handles exceptions. - Robustness: The function gracefully handles edge cases, such as invalid HTTP methods and network errors. - Code Quality: The implementation follows best practices, including clear variable names and proper exception handling.","solution":"import urllib.request import urllib.error import json def fetch_web_resource(url: str, method: str, data: dict = None, headers: dict = None, timeout: int = 10) -> str: if method not in [\\"GET\\", \\"POST\\"]: raise ValueError(\\"Invalid HTTP method: only \'GET\' and \'POST\' are supported\\") if headers is None: headers = {} if method == \\"POST\\" and data is not None: data = json.dumps(data).encode(\'utf-8\') headers[\'Content-Type\'] = \\"application/json\\" request = urllib.request.Request(url, data=data, headers=headers, method=method) try: with urllib.request.urlopen(request, timeout=timeout) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"HTTPError: {e.code} - {e.reason}\\" except urllib.error.URLError as e: return f\\"URLError: {e.reason}\\" except Exception as e: return f\\"Exception: {str(e)}\\""},{"question":"# Color Space Conversion Assessment **Objective:** Your task is to write a function that demonstrates your understanding of the `colorsys` module and its capabilities in converting colors between different color spaces. # Problem Statement Write a Python function called `convert_color_spaces` that performs the following operations: 1. Convert a given RGB color to all three other color spaces (YIQ, HLS, HSV). 2. Convert the YIQ, HLS, and HSV colors obtained in step 1 back to RGB. 3. Check if the RGB values obtained after the round-trip conversions are approximately the same as the original RGB values (consider a tolerance of 0.01). # Function Signature ```python def convert_color_spaces(r: float, g: float, b: float) -> dict: pass ``` # Input - `r` (float): The red component of the color, a value between 0 and 1. - `g` (float): The green component of the color, a value between 0 and 1. - `b` (float): The blue component of the color, a value between 0 and 1. # Output - Returns a dictionary with the keys `original_rgb`, `yiq`, `hls`, `hsv`, `yiq_to_rgb`, `hls_to_rgb`, `hsv_to_rgb`, and `is_approx_same_rgb`. The values are as follows: - `original_rgb`: Tuple of the original (r, g, b) values. - `yiq`: Tuple of (y, i, q) values obtained from `colorsys.rgb_to_yiq`. - `hls`: Tuple of (h, l, s) values obtained from `colorsys.rgb_to_hls`. - `hsv`: Tuple of (h, s, v) values obtained from `colorsys.rgb_to_hsv`. - `yiq_to_rgb`: Tuple of (r, g, b) values obtained from `colorsys.yiq_to_rgb`. - `hls_to_rgb`: Tuple of (r, g, b) values obtained from `colorsys.hls_to_rgb`. - `hsv_to_rgb`: Tuple of (r, g, b) values obtained from `colorsys.hsv_to_rgb`. - `is_approx_same_rgb`: Boolean indicating whether all converted back RGB values are approximately the same as the original RGB values within a tolerance of 0.01. # Constraints - The input values for `r`, `g`, and `b` are guaranteed to be between 0 and 1. # Example ```python convert_color_spaces(0.2, 0.4, 0.4) ``` Expected output (values are exemplary): ```python { \'original_rgb\': (0.2, 0.4, 0.4), \'yiq\': (0.36000000000000004, -0.14, -0.07), \'hls\': (0.5, 0.30000000000000004, 0.33333333333333337), \'hsv\': (0.5, 0.5, 0.4), \'yiq_to_rgb\': (0.2, 0.4000000000000001, 0.4000000000000001), \'hls_to_rgb\': (0.20000000000000007, 0.4, 0.4), \'hsv_to_rgb\': (0.2, 0.4, 0.4000000000000001), \'is_approx_same_rgb\': True } ``` # Notes - Utilize the `math.isclose()` function to check for approximate equality with the given tolerance. # Additional Requirement - You are allowed to use any standard libraries besides `colorsys` to accomplish this task.","solution":"import colorsys import math def convert_color_spaces(r: float, g: float, b: float) -> dict: # Convert from RGB to YIQ, HLS, HSV yiq = colorsys.rgb_to_yiq(r, g, b) hls = colorsys.rgb_to_hls(r, g, b) hsv = colorsys.rgb_to_hsv(r, g, b) # Convert back from YIQ, HLS, HSV to RGB yiq_to_rgb = colorsys.yiq_to_rgb(*yiq) hls_to_rgb = colorsys.hls_to_rgb(*hls) hsv_to_rgb = colorsys.hsv_to_rgb(*hsv) # Function to check if two RGB values are approximately the same def are_same_rgb(original, converted): return all(math.isclose(original[i], converted[i], abs_tol=0.01) for i in range(3)) # Check if all transformed-to-original RGB values are approximately the same is_approx_same_rgb = ( are_same_rgb((r, g, b), yiq_to_rgb) and are_same_rgb((r, g, b), hls_to_rgb) and are_same_rgb((r, g, b), hsv_to_rgb) ) return { \'original_rgb\': (r, g, b), \'yiq\': yiq, \'hls\': hls, \'hsv\': hsv, \'yiq_to_rgb\': yiq_to_rgb, \'hls_to_rgb\': hls_to_rgb, \'hsv_to_rgb\': hsv_to_rgb, \'is_approx_same_rgb\': is_approx_same_rgb }"},{"question":"Coding Assessment Question # Objective: Design a nearest neighbor-based classification model to classify a dataset using scikit-learn. # Problem Statement: You are given a dataset with labeled data points. Your task is to implement a classification model using the `KNeighborsClassifier` from scikit-learn. You need to evaluate the performance of different \'k\' values and return the value of \'k\' that results in the highest accuracy. # Dataset Specifications: - The dataset consists of two NumPy arrays `X` and `y`. - `X` is a two-dimensional array representing the feature set with shape `(n_samples, n_features)`. - `y` is a one-dimensional array representing the labels with shape `(n_samples,)`. # Function Signature: ```python def find_best_k(X: np.ndarray, y: np.ndarray, k_values: List[int]) -> int: Finds the best k value for k-nearest neighbors classifier. Parameters: X (np.ndarray): Feature dataset of shape (n_samples, n_features). y (np.ndarray): Labels of shape (n_samples,). k_values (List[int]): List of k values to evaluate. Returns: int: The k value that provides the highest accuracy. # your code here ``` # Constraints: - You can assume `X` and `y` will always be valid NumPy arrays of matching lengths. - You should split the data into a training and a test set using an 80-20 split. - Use accuracy score to evaluate the models. - You should use `random_state=42` for any random operations to ensure consistency. # Example: ```python import numpy as np from sklearn.model_selection import train_test_split # Assuming X and y are provided X = np.array([[1.0, 2.0], [2.1, 3.1], [3.2, 4.2], [4.3, 5.3], [5.4, 6.4]]) y = np.array([0, 1, 0, 1, 0]) k_values = [1, 3, 5] best_k = find_best_k(X, y, k_values) print(best_k) # Output should be the k value with the highest accuracy ``` # Notes: - You may use the `train_test_split` function from `sklearn.model_selection` to perform the train-test split. - Use `accuracy_score` from `sklearn.metrics` to calculate the accuracy of the model. - Ensure your code is efficient and follows best practices for coding in Python.","solution":"import numpy as np from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def find_best_k(X: np.ndarray, y: np.ndarray, k_values: list) -> int: Finds the best k value for k-nearest neighbors classifier. Parameters: X (np.ndarray): Feature dataset of shape (n_samples, n_features). y (np.ndarray): Labels of shape (n_samples,). k_values (list): List of k values to evaluate. Returns: int: The k value that provides the highest accuracy. # Split the dataset into training and testing sets with a 80-20 split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) best_k = k_values[0] highest_accuracy = 0 for k in k_values: # Create and train the KNeighborsClassifier with current k knn = KNeighborsClassifier(n_neighbors=k) knn.fit(X_train, y_train) # Predict the labels for the test set y_pred = knn.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) # Update the best k if the current model has higher accuracy if accuracy > highest_accuracy: highest_accuracy = accuracy best_k = k return best_k"},{"question":"# Advanced Logging Configuration with dictConfig **Objective:** Implement a function that sets up a comprehensive logging configuration using the `logging.config.dictConfig` function. This will test your understanding of configuring loggers, handlers, formatters, and filters programmatically through a dictionary. **Task:** Write a function `initialize_logging` that configures logging for a given application using a dictionary configuration. **Requirements:** 1. Use `logging.config.dictConfig`. 2. Configure at least: - One logger with a specific name. - One handler for console output (`StreamHandler`). - One handler for file output (`FileHandler` or `RotatingFileHandler`). - One custom formatter for the console handler. - One custom formatter for the file handler. 3. Include a filter that allows only log messages with a specific attribute (e.g., `allowed=True`). 4. Ensure that the root logger also has handlers attached. **Function Signature:** ```python def initialize_logging() -> None: pass ``` **Configuration Details:** - Console handler should log at the DEBUG level. - File handler should log at the WARNING level. - Use appropriate formatters to display timestamps, log levels, logger names, and messages. - The file handler should log to a file named `app.log` with a designated maximum size and backup count (if using `RotatingFileHandler`). **Constraints:** - All handlers should have different formatters. - The filter should be applied only to the console handler. **Example:** After calling `initialize_logging`, the following code: ```python import logging logger = logging.getLogger(\'exampleLogger\') logger.debug(\'This is a debug message\', extra={\'allowed\': True}) logger.warning(\'This is a warning message with allowed=False\', extra={\'allowed\': False}) ``` Should result in: - The debug message appearing in the console if the `allowed=True` extra attribute is present. - The warning message should be logged to `app.log` regardless of the `allowed` attribute. ```python def initialize_logging(): import logging import logging.config # Define the configuration dictionary config_dict = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'console\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, \'file\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', } }, \'filters\': { \'allowed_filter\': { \'()\': \'logging.Filter\', \'name\': \'\', } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'console\', \'filters\': [\'allowed_filter\'] }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'WARNING\', \'formatter\': \'file\', \'filename\': \'app.log\', \'maxBytes\': 1024*1024*5, # 5 MB \'backupCount\': 3 } }, \'loggers\': { \'exampleLogger\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': False, } }, \'root\': { \'handlers\': [\'console\', \'file\'], \'level\': \'WARNING\' }, } # Apply the configuration logging.config.dictConfig(config_dict) ``` Test the functionality by invoking this function and generating log messages as shown in the example.","solution":"def initialize_logging() -> None: import logging import logging.config from logging import Filter class AllowedFilter(Filter): def filter(self, record): return getattr(record, \'allowed\', False) # Define the configuration dictionary config_dict = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'console\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, \'file\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', } }, \'filters\': { \'allowed_filter\': { \'()\': AllowedFilter } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'console\', \'filters\': [\'allowed_filter\'] }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'WARNING\', \'formatter\': \'file\', \'filename\': \'app.log\', \'maxBytes\': 1024*1024*5, # 5 MB \'backupCount\': 3 } }, \'loggers\': { \'exampleLogger\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': False, } }, \'root\': { \'handlers\': [\'console\', \'file\'], \'level\': \'WARNING\' }, } # Apply the configuration logging.config.dictConfig(config_dict)"},{"question":"# PyTorch Distributed Training Script Configuration Objective: Write a Python script using PyTorch that configures and launches a fault-tolerant distributed training job. Your script should demonstrate your understanding of distributed training concepts in PyTorch and make use of the `torchrun` command as described in the documentation. Requirements: 1. **Initialize a Distributed Training Environment:** Set up your training environment to handle distributed training across multiple nodes. 2. **Script Execution Parameters:** Use the following parameters in your script: - `NUM_NODES`: Number of nodes for the job. Set this to 2 for testing. - `TRAINERS_PER_NODE`: Number of trainers per node. Set this to 2 for testing. - `NUM_ALLOWED_FAILURES`: Number of allowed restarts before the job is considered failed. Set this to 3. - `JOB_ID`: Unique identifier for the job. Use \'test_job\' for this script. - `HOST_NODE_ADDR`: Use \'node1.example.com:29400\' as the address. 3. **Sample Training Script:** Create or specify a simple training script `sample_training_script.py` that performs a basic training task (e.g., training a simple feedforward neural network using random data). Ensure that this script can be executed in a distributed setting. 4. **Command Execution:** Demonstrate how to launch the job using the `torchrun` command with the parameters mentioned above. Ensure your script prints out the command to be executed. 5. **Error Handling:** Implement basic error handling to capture and report issues encountered during the distributed training setup. Script Output: Your script should output the full `torchrun` command with all the necessary parameters configured. Additionally, include logs or print statements to show the initialization and execution flow of the distributed training setup. Constraints: - Ensure compatibility with PyTorch version 1.8.0 or higher. - The script should be executable in a local environment or in a simulated distributed environment for testing purposes. Example Output: ``` Initializing distributed training with the following parameters: Number of Nodes: 2 Trainers per Node: 2 Allowed Restarts: 3 Job ID: test_job Host Node Address: node1.example.com:29400 Launching distributed training job... Command: torchrun --nnodes=2 --nproc-per-node=2 --max-restarts=3 --rdzv-id=test_job --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29400 sample_training_script.py ``` Evaluation Criteria: - Correctness of command configuration and execution. - Proper initialization and setup of the PyTorch distributed environment. - Clarity and readability of the script. - Effective error handling and reporting.","solution":"import sys import subprocess def initialize_distributed_training(num_nodes, trainers_per_node, num_allowed_failures, job_id, host_node_addr): Initializes and launches a PyTorch distributed training job. print(f\\"Initializing distributed training with the following parameters:\\") print(f\\"Number of Nodes: {num_nodes}\\") print(f\\"Trainers per Node: {trainers_per_node}\\") print(f\\"Allowed Restarts: {num_allowed_failures}\\") print(f\\"Job ID: {job_id}\\") print(f\\"Host Node Address: {host_node_addr}\\") # Constructing the command command = [ \\"torchrun\\", f\\"--nnodes={num_nodes}\\", f\\"--nproc-per-node={trainers_per_node}\\", f\\"--max-restarts={num_allowed_failures}\\", f\\"--rdzv-id={job_id}\\", f\\"--rdzv-backend=c10d\\", f\\"--rdzv-endpoint={host_node_addr}\\", \\"sample_training_script.py\\" ] command_str = \\" \\".join(command) print(\\"nLaunching distributed training job...\\") print(f\\"Command: {command_str}\\") # Execute the command try: result = subprocess.run(command, check=True, capture_output=True, text=True) print(\\"Training job started successfully.\\") print(\\"Output:\\", result.stdout) except subprocess.CalledProcessError as e: print(\\"Error occurred while starting training job.\\") print(\\"Error message:\\", e.stderr) sys.exit(1)"},{"question":"# Question You have been given the task of creating a Python script that maintains a simple key-value store with persistent storage. You will use the `shelve` module to implement the following functionalities in your script: 1. **Add/Update Entry**: A function `add_entry(shelf, key, value)` that adds a new entry or updates an existing entry in the shelf. 2. **Retrieve Entry**: A function `get_entry(shelf, key)` that retrieves the value associated with the given key. If the key does not exist, return `None`. 3. **Delete Entry**: A function `delete_entry(shelf, key)` that deletes the entry corresponding to the given key. If the key does not exist, do nothing. 4. **Persist Mutable Entry**: A function `add_to_list_entry(shelf, key, value)` where the value is appended to a list stored at the specified key. If the key does not exist, initialize it with an empty list and then append the value. Ensure that the changes are persisted. 5. **Close Shelf**: A function `close_shelf(shelf)` that ensures all changes are synchronized and the shelf is properly closed. You are required to use a context manager to handle the shelf, ensuring that resources are managed appropriately, and to demonstrate the code flow with some example operations. Expected Input and Output - **add_entry(shelf, key, value)**: - Input: `shelf` (shelve object), `key` (string), `value` (any picklable object) - Output: None - **get_entry(shelf, key)**: - Input: `shelf` (shelve object), `key` (string) - Output: Value associated with the key or `None` if the key does not exist - **delete_entry(shelf, key)**: - Input: `shelf` (shelve object), `key` (string) - Output: None - **add_to_list_entry(shelf, key, value)**: - Input: `shelf` (shelve object), `key` (string), `value` (any picklable object) - Output: None - **close_shelf(shelf)**: - Input: `shelf` (shelve object) - Output: None Constraints - Implement proper error handling to manage non-existing keys and invalid operations. - Ensure that the shelf is always closed properly to avoid data corruption. Example Usage ```python import shelve def add_entry(shelf, key, value): shelf[key] = value def get_entry(shelf, key): return shelf.get(key, None) def delete_entry(shelf, key): if key in shelf: del shelf[key] def add_to_list_entry(shelf, key, value): if key in shelf: temp = shelf[key] temp.append(value) shelf[key] = temp else: shelf[key] = [value] def close_shelf(shelf): shelf.sync() shelf.close() # Example script usage with shelve.open(\'my_shelf\', writeback=True) as db: add_entry(db, \'name\', \'Alice\') print(get_entry(db, \'name\')) # Output: \'Alice\' add_to_list_entry(db, \'scores\', 100) add_to_list_entry(db, \'scores\', 200) print(get_entry(db, \'scores\')) # Output: [100, 200] delete_entry(db, \'name\') print(get_entry(db, \'name\')) # Output: None ``` Implement these functions and ensure they work as expected by following the example script.","solution":"import shelve def add_entry(shelf, key, value): Adds a new entry or updates an existing entry in the shelf. shelf[key] = value def get_entry(shelf, key): Retrieves the value associated with the given key. If the key does not exist, return None. return shelf.get(key, None) def delete_entry(shelf, key): Deletes the entry corresponding to the given key. If the key does not exist, do nothing. if key in shelf: del shelf[key] def add_to_list_entry(shelf, key, value): Appends the value to a list stored at the specified key. If the key does not exist, initialize it with an empty list and then append the value. if key in shelf: shelf[key].append(value) else: shelf[key] = [value] def close_shelf(shelf): Ensures all changes are synchronized and the shelf is properly closed. shelf.sync() shelf.close()"},{"question":"<|Analysis Begin|> The provided documentation describes the \\"uu\\" module in Python, which deals with encoding and decoding files in the uuencode format. The module offers two primary functions: 1. `uu.encode(in_file, out_file, name=None, mode=None, *, backtick=False)`: This function is used to encode a file into uuencode format. It takes in a file to encode, an output file to write the encoded contents, optional parameters for the name and mode of the file, and a `backtick` parameter for representing zeros. 2. `uu.decode(in_file, out_file=None, mode=None, quiet=False)`: This function decodes a uuencoded file. It reads from a uuencoded input file and writes the decoded content to an output file. It has an optional mode parameter for the output file and a quiet parameter to suppress warnings. There is also an exception `uu.Error`, which is raised in various error conditions during decoding. From the above, a challenging and comprehensive coding assessment question can revolve around using these encoding and decoding functions appropriately and handling the associated errors. <|Analysis End|> <|Question Begin|> **Question:** You have been provided with a logistics company that needs to transmit important binary data files over a network that only supports ASCII. To do this, they use uuencoding to encode the files before transmission and uudecoding to decode them upon reception. Your task is to create a Python script that performs the following operations: 1. Encode a given binary file using uuencode. 2. Decode the uuencoded file back to its original binary format. 3. Handle and log any errors that occur during encoding or decoding. # Requirements: 1. Implement a function `encode_file(input_path: str, output_path: str, backtick: bool = False) -> None` that: - Encodes the file at `input_path` into uuencode format and writes the encoded content to `output_path`. - Uses the `backtick` parameter to control the representation of zeros in the encoding. 2. Implement a function `decode_file(input_path: str, output_path: str) -> None` that: - Decodes the uuencoded file at `input_path` and writes the decoded content to `output_path`. - If a `uu.Error` is raised, catch the exception and log an error message indicating that decoding failed. 3. Implement logging for any exceptions that occur during both encoding and decoding. # Constraints: - The function should handle binary files of arbitrary size. - Ensure the script runs efficiently with reasonable performance for large files. - The error logging should capture and log specific details of the error. # Example Usage: ```python try: encode_file(\\"path/to/binary/file\\", \\"path/to/encoded/file\\", backtick=True) print(\\"Encoding successful.\\") except Exception as e: print(f\\"Encoding failed with error: {e}\\") try: decode_file(\\"path/to/encoded/file\\", \\"path/to/decoded/file\\") print(\\"Decoding successful.\\") except Exception as e: print(f\\"Decoding failed with error: {e}\\") # Check if the decoded file matches the original binary file ``` Your solution should demonstrate the ability to correctly use the `uu` module\'s encode and decode functions, handle file I/O, catch and log exceptions appropriately, and manage the `backtick` parameter to affect encoding.","solution":"import uu import logging # Configure logging logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s %(levelname)s:%(message)s\') def encode_file(input_path: str, output_path: str, backtick: bool = False) -> None: Encodes the file at input_path into uuencode format and writes the encoded content to output_path. Parameters: - input_path: str, path to the input binary file. - output_path: str, path to the output uuencoded file. - backtick: bool, if True, use backtick for zero representation. try: with open(input_path, \'rb\') as in_file, open(output_path, \'wb\') as out_file: uu.encode(in_file, out_file, backtick=backtick) except Exception as e: logging.error(f\\"Encoding failed for {input_path} to {output_path}. Error: {e}\\") raise def decode_file(input_path: str, output_path: str) -> None: Decodes the uuencoded file at input_path and writes the decoded content to output_path. Parameters: - input_path: str, path to the input uuencoded file. - output_path: str, path to the output binary file. try: with open(input_path, \'rb\') as in_file, open(output_path, \'wb\') as out_file: uu.decode(in_file, out_file) except uu.Error as e: logging.error(f\\"Decoding failed for {input_path} to {output_path}. uu.Error: {e}\\") raise except Exception as e: logging.error(f\\"Decoding failed for {input_path} to {output_path}. Error: {e}\\") raise"},{"question":"**Objective**: Demonstrate understanding of the `torch.finfo` and `torch.iinfo` classes and their attributes by comparing numerical properties of different torch data types. **Problem Statement**: Write a function `compare_dtype_properties` that takes no arguments and returns a dictionary comparing the numerical properties of selected floating-point and integer data types in PyTorch. Specifically, your function should gather the following information: 1. For floating-point types (`torch.float32`, `torch.float64`, `torch.float16`, and `torch.bfloat16`), retrieve: - Number of bits - The smallest representable number (eps) - The largest representable number (max) - The smallest representable number (min) - The smallest positive normal number (tiny) 2. For integer types (`torch.uint8`, `torch.int8`, `torch.int16`, `torch.int32`, and `torch.int64`), retrieve: - Number of bits - The largest representable number (max) - The smallest representable number (min) **Function Signature**: ```python def compare_dtype_properties() -> dict: pass ``` **Output**: The function should return a dictionary with keys as the data type names (e.g., \'torch.float32\') and values as another dictionary of their numerical properties. Example format: ```python { \'torch.float32\': { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38 }, \'torch.int32\': { \'bits\': 32, \'max\': 2147483647, \'min\': -2147483648 } # Add similar entries for other data types } ``` **Constraints**: - Ensure that the function completes execution within a reasonable time frame. - You may not use any external libraries other than PyTorch. **Example**: ```python result = compare_dtype_properties() for dtype, properties in result.items(): print(f\\"{dtype}:\\") for prop, value in properties.items(): print(f\\" {prop}: {value}\\") ``` This example output will print the numerical properties for each data type in a readable format. Use the `torch.finfo` and `torch.iinfo` classes to obtain the required properties for each data type.","solution":"import torch def compare_dtype_properties() -> dict: properties = {} # Define floating-point data types and gather properties using torch.finfo float_types = [torch.float32, torch.float64, torch.float16, torch.bfloat16] for dtype in float_types: info = torch.finfo(dtype) properties[str(dtype)] = { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny } # Define integer data types and gather properties using torch.iinfo int_types = [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64] for dtype in int_types: info = torch.iinfo(dtype) properties[str(dtype)] = { \'bits\': info.bits, \'max\': info.max, \'min\': info.min } return properties"},{"question":"# Custom Module Loader The purpose of this assessment is to test your understanding of the `importlib` package in Python, specifically how to create and use custom loaders and finders for importing modules. Problem Statement You are required to create a custom module loader and finder that can load Python source files from a compressed ZIP archive. This custom loader should be able to: 1. Locate a module within the ZIP archive. 2. Load and execute the module. 3. Ensure that all attributes typically set by the import machinery (like `__name__`, `__file__`, `__package__`) are correctly assigned. Given the ZIP archive structure as follows: ``` modules.zip │ ├── pkg/ │ ├── __init__.py │ └── mod.py └── mod2.py ``` You need to implement two classes: `ZipFinder` and `ZipLoader`. Requirements 1. **ZipFinder**: - Inherit from `importlib.abc.MetaPathFinder`. - Implement the `find_spec` method to locate module specifications within the ZIP archive. 2. **ZipLoader**: - Inherit from `importlib.abc.SourceLoader`. - Implement methods to load source code and module attributes from the ZIP archive. # Implementation 1. `ZipFinder` should: - Be initialized with the path to the ZIP archive. - Locate a module specification if the module path exists within the ZIP archive. 2. `ZipLoader` should: - Load the source code of the module from within the ZIP archive. - Return module source code as required. Function Signatures ```python import importlib.abc import importlib.util import sys import zipfile import types class ZipFinder(importlib.abc.MetaPathFinder): def __init__(self, archive_path: str): # Initialize with the ZIP archive path self.archive_path = archive_path def find_spec(self, fullname: str, path=None, target=None): # Locate the module within the ZIP archive and return a ModuleSpec object pass class ZipLoader(importlib.abc.SourceLoader): def __init__(self, archive_path: str, module_path: str): # Initialize with the ZIP archive and module path self.archive_path = archive_path self.module_path = module_path def get_filename(self, fullname: str) -> str: # Return the path to the module within the ZIP archive pass def get_data(self, path: str) -> bytes: # Read and return the source code of the module from the ZIP archive pass def get_code(self, fullname: str): # Return the module\'s code object pass def get_source(self, fullname: str) -> str: # Return the source code of the module as a string pass def is_package(self, fullname: str) -> bool: # Determine if the module is a package based on its path pass ``` Example Usage ```python import sys # Path to the ZIP archive containing modules archive_path = \'modules.zip\' # Register the custom zip finder sys.meta_path.insert(0, ZipFinder(archive_path)) # Import a module from the ZIP archive import mod2 # This should load \'mod2.py\' from the ZIP archive # Access a module within a package import pkg.mod # This should load \'pkg/mod.py\' from the ZIP archive ``` # Constraints - The path provided to modules should be within the ZIP archive. - Modules must be standard Python source files ending with `.py`. - Handle relative imports correctly within the package structure. Good luck!","solution":"import importlib.abc import importlib.util import sys import zipfile import os class ZipFinder(importlib.abc.MetaPathFinder): def __init__(self, archive_path: str): self.archive_path = archive_path self._zipfile = zipfile.ZipFile(self.archive_path) def find_spec(self, fullname: str, path=None, target=None): module_path = fullname.replace(\'.\', \'/\') + \'.py\' package_path = fullname.replace(\'.\', \'/\') + \'/__init__.py\' if module_path in self._zipfile.namelist(): return importlib.util.spec_from_loader(fullname, ZipLoader(self.archive_path, module_path)) elif package_path in self._zipfile.namelist(): return importlib.util.spec_from_loader(fullname, ZipLoader(self.archive_path, package_path)) return None class ZipLoader(importlib.abc.SourceLoader): def __init__(self, archive_path: str, module_path: str): self.archive_path = archive_path self.module_path = module_path self._zipfile = zipfile.ZipFile(self.archive_path) def get_filename(self, fullname: str) -> str: return self.module_path def get_data(self, path: str) -> bytes: with self._zipfile.open(self.module_path) as file: return file.read() def get_code(self, fullname: str): source = self.get_source(fullname) return self.source_to_code(source, self.module_path) def get_source(self, fullname: str) -> str: return self.get_data(self.module_path).decode(\'utf-8\') def is_package(self, fullname: str) -> bool: return self.module_path.endswith(\'__init__.py\')"},{"question":"**Title: Multi-Component PLS Regression for Dimensionality Reduction and Prediction** Objective: Implement a function that uses Partial Least Squares Regression (PLSRegression) to reduce the dimensionality of a dataset and then predict the target values for a given test set. Description: You are given training datasets `X_train` and `Y_train`, along with a test dataset `X_test`. Your task is to: 1. Fit a PLSRegression model using the training data. 2. Reduce the dimensionality of `X_train` and `X_test` using the fitted model. 3. Predict the target values for `X_test` using the reduced dimensions. Requirements: - Use the `sklearn.cross_decomposition.PLSRegression` class. - The number of components for the PLS model should be a parameter that you can adjust. - Return both the reduced dimension datasets and the predicted target values. Function Signature: ```python def pls_regression(X_train, Y_train, X_test, n_components): Perform PLS Regression to reduce dimensions and predict target values. Parameters: - X_train (numpy.ndarray): Training features of shape (n_samples, n_features). - Y_train (numpy.ndarray): Training targets of shape (n_samples, n_targets). - X_test (numpy.ndarray): Test features of shape (m_samples, n_features). - n_components (int): Number of PLS components to use. Returns: - X_train_reduced (numpy.ndarray): Reduced dimension training features. - X_test_reduced (numpy.ndarray): Reduced dimension test features. - Y_pred (numpy.ndarray): Predicted target values for the test data. pass ``` Input: - `X_train`: A 2D numpy array of shape (n_samples, n_features) representing the training features. - `Y_train`: A 2D numpy array of shape (n_samples, n_targets) representing the training targets. - `X_test`: A 2D numpy array of shape (m_samples, n_features) representing the test features. - `n_components`: An integer specifying the number of components for PLS. Output: - `X_train_reduced`: A 2D numpy array of shape (n_samples, n_components) representing the reduced dimension training features. - `X_test_reduced`: A 2D numpy array of shape (m_samples, n_components) representing the reduced dimension test features. - `Y_pred`: A 2D numpy array of shape (m_samples, n_targets) representing the predicted target values for the test data. Constraints: - Ensure that `n_components` is less than or equal to the minimum of the number of samples and the number of features in `X_train`. - Performance should be optimal for large datasets. Example: ```python import numpy as np # Sample training data X_train = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]) Y_train = np.array([[1], [2], [3]]) # Sample test data X_test = np.array([[0.15, 0.25, 0.35], [0.45, 0.55, 0.65]]) # Number of components n_components = 2 # Function call X_train_reduced, X_test_reduced, Y_pred = pls_regression(X_train, Y_train, X_test, n_components) print(X_train_reduced) print(X_test_reduced) print(Y_pred) ``` **Note:** Ensure that your function handles potential issues such as mismatched dimensions and provides informative error messages when constraints are violated.","solution":"from sklearn.cross_decomposition import PLSRegression import numpy as np def pls_regression(X_train, Y_train, X_test, n_components): Perform PLS Regression to reduce dimensions and predict target values. Parameters: - X_train (numpy.ndarray): Training features of shape (n_samples, n_features). - Y_train (numpy.ndarray): Training targets of shape (n_samples, n_targets). - X_test (numpy.ndarray): Test features of shape (m_samples, n_features). - n_components (int): Number of PLS components to use. Returns: - X_train_reduced (numpy.ndarray): Reduced dimension training features. - X_test_reduced (numpy.ndarray): Reduced dimension test features. - Y_pred (numpy.ndarray): Predicted target values for the test data. # Check if n_components is more than the number of samples or features if n_components > min(X_train.shape[0], X_train.shape[1]): raise ValueError(\\"n_components must be less than or equal to the minimum of the number of samples and the number of features.\\") # Create and fit the PLS model pls = PLSRegression(n_components=n_components) pls.fit(X_train, Y_train) # Transform the training and test sets X_train_reduced = pls.transform(X_train) X_test_reduced = pls.transform(X_test) # Predict the targets for the test set Y_pred = pls.predict(X_test) return X_train_reduced, X_test_reduced, Y_pred"},{"question":"**Question**: You are tasked with creating a Python script that automates the setup and management of a virtual environment for a project. Your script should be capable of creating a virtual environment, activating it, installing specified packages, upgrading specific packages, and exporting a `requirements.txt` file listing all installed packages. You will need to use the `subprocess` module to run shell commands for this task. # Requirements: 1. The script should create a virtual environment in a specified directory. 2. Activate the virtual environment. 3. Install a list of packages provided in a JSON configuration file. 4. Upgrade a specific list of packages. 5. Export the list of installed packages to a `requirements.txt` file. # Input: - A JSON configuration file (`config.json`) with the following format: ```json { \\"env_directory\\": \\"path/to/venv_directory\\", \\"packages_to_install\\": [\\"package1\\", \\"package2==version\\", ...], \\"packages_to_upgrade\\": [\\"package3\\", \\"package4\\", ...] } ``` # Output: - A `requirements.txt` file in the same directory as the script, listing all installed packages in the virtual environment. # Constraints: - The script must handle errors gracefully, such as missing packages or incorrect versions in the JSON file. - Ensure that the virtual environment is activated before attempting to install or upgrade packages. # Example: Assume the `config.json` file contains: ```json { \\"env_directory\\": \\"my_project_env\\", \\"packages_to_install\\": [\\"requests==2.24.0\\", \\"flask\\"], \\"packages_to_upgrade\\": [\\"requests\\"] } ``` The script should: 1. Create a virtual environment in `my_project_env`. 2. Activate the virtual environment. 3. Install `requests` version 2.24.0 and `flask`. 4. Upgrade `requests` to the latest version. 5. Export the list of installed packages to `requirements.txt`. ```python import json import subprocess import os def setup_virtual_environment(config): env_dir = config[\'env_directory\'] packages_to_install = config[\'packages_to_install\'] packages_to_upgrade = config[\'packages_to_upgrade\'] # 1. Create virtual environment subprocess.run([\\"python\\", \\"-m\\", \\"venv\\", env_dir]) # Activation of virtual environment based on OS if os.name == \'nt\': # Windows activate_script = os.path.join(env_dir, \\"Scripts\\", \\"activate.bat\\") else: # Unix or MacOS activate_script = os.path.join(env_dir, \\"bin\\", \\"activate\\") # Activate the virtual environment activate_command = f\\"source {activate_script}\\" if os.name != \'nt\' else activate_script # Using shell=True for simplicity in this context os.system(activate_command) # 2. Install packages for package in packages_to_install: subprocess.run([\\"pip\\", \\"install\\", package]) # 3. Upgrade packages for package in packages_to_upgrade: subprocess.run([\\"pip\\", \\"install\\", \\"--upgrade\\", package]) # 4. Export installed packages to requirements.txt with open(\\"requirements.txt\\", \\"w\\") as req_file: reqs = subprocess.run([\\"pip\\", \\"freeze\\"], capture_output=True) req_file.write(reqs.stdout.decode()) if __name__ == \\"__main__\\": with open(\'config.json\') as config_file: config = json.load(config_file) setup_virtual_environment(config) ``` **Note**: The virtual environment activation in this script is handled in a way that might not work in all contexts without proper shell environment handling. For a production environment, more robust handling of environment activation should be implemented. **Expected Skills**: Understanding of virtual environments, `pip` package management, JSON handling, and subprocess management in Python.","solution":"import json import subprocess import os import sys def setup_virtual_environment(config): env_dir = config[\'env_directory\'] packages_to_install = config[\'packages_to_install\'] packages_to_upgrade = config[\'packages_to_upgrade\'] # 1. Create virtual environment subprocess.run([sys.executable, \\"-m\\", \\"venv\\", env_dir], check=True) # Activation script paths if os.name == \'nt\': # Windows activate_script = os.path.join(env_dir, \\"Scripts\\", \\"activate.bat\\") else: # Unix or MacOS activate_script = os.path.join(env_dir, \\"bin\\", \\"activate\\") # Construct activation command sequence based on OS if os.name == \'nt\': activate_commands = f\\"{activate_script} && \\" else: activate_commands = f\\"source {activate_script} && \\" # 2. Install packages for package in packages_to_install: install_command = activate_commands + f\\"pip install {package}\\" subprocess.run(install_command, shell=True, check=True) # 3. Upgrade packages for package in packages_to_upgrade: upgrade_command = activate_commands + f\\"pip install --upgrade {package}\\" subprocess.run(upgrade_command, shell=True, check=True) # 4. Export installed packages to requirements.txt export_command = activate_commands + \\"pip freeze > requirements.txt\\" subprocess.run(export_command, shell=True, check=True) if __name__ == \\"__main__\\": with open(\'config.json\') as config_file: config = json.load(config_file) setup_virtual_environment(config)"},{"question":"# Question: Advanced File Search and Pattern Matching with `glob` Module **Objective:** Write a Python function that uses the `glob` module to find files in a directory tree based on complex patterns, including patterns with special characters that need to be escaped. The function should also have the capability to search recursively in subdirectories. **Function Signature:** ```python def advanced_file_search(pattern: str, root_dir: str = \'.\', recursive: bool = False) -> list: pass ``` **Input:** - `pattern` (str): The pattern to use for matching file names. This should support all Unix shell-style wildcards (`*`, `?`, `[]`). - `root_dir` (str): The root directory to start the search from. Defaults to the current directory (`.`). - `recursive` (bool): Whether to search recursively in subdirectories. Defaults to `False`. **Output:** - A list of matched file paths (list of str). **Constraints:** - Ensure that any special characters in the `pattern` are correctly escaped using `glob.escape()`. - If `recursive` is set to `True`, use the pattern `**/*` to match files in all subdirectories. - Your solution should efficiently handle directories with a large number of files and subdirectories. **Example Usage:** ```python # Example 1: Simple match in the current directory print(advanced_file_search(\'*.txt\')) # Might return: [\'1.txt\', \'2.txt\'] # Example 2: Pattern with special characters print(advanced_file_search(\'[a-c].txt\')) # Might return: [\'a.txt\', \'b.txt\'] # Example 3: Recursive search print(advanced_file_search(\'**/*.txt\', root_dir=\'my_directory\', recursive=True)) # Might return: [\'my_directory/1.txt\', \'my_directory/subdir/2.txt\'] # Example 4: Matching files with special characters in names print(advanced_file_search(\'file[?][*].txt\')) # Might return: [\'file?.txt\', \'file*.txt\'] ``` **Note:** - Handle edge cases where no files match the pattern by returning an empty list. - Make sure to test your function with various patterns and directories. **Performance Requirements:** - Your function should be able to handle large directories efficiently. - Utilize iterators where possible to minimize memory usage, especially in recursive searches. **Hints:** - Refer to the `glob.glob` and `glob.iglob` functions for implementing the file search functionality. - Use `glob.escape()` for correctly handling special characters in the search pattern.","solution":"import glob import os def advanced_file_search(pattern: str, root_dir: str = \'.\', recursive: bool = False) -> list: Find files in a directory tree based on complex patterns, including patterns with special characters. Args: pattern (str): The pattern to use for matching file names. root_dir (str): The root directory to start the search from. Defaults to the current directory (\'.\'). recursive (bool): Whether to search recursively in subdirectories. Defaults to False. Returns: list: A list of matched file paths. search_pattern = os.path.join(glob.escape(root_dir), pattern) if recursive: search_pattern = os.path.join(glob.escape(root_dir), \'**\', pattern) return [f for f in glob.iglob(search_pattern, recursive=True)] else: return [f for f in glob.iglob(search_pattern)]"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding of process-based parallelism using the `multiprocessing` module in Python by implementing a program that creates multiple processes to compute the results and shares the results using inter-process communication. # Problem Statement: You are required to write a Python program that creates multiple worker processes to perform computational tasks and collects the results using queues. # Requirements: 1. Create a function `compute` that takes an integer `n` as input and returns the square of the number `n`. 2. Implement a main function that: - Creates a queue for tasks and a queue for storing results. - Spawns a specified number (4) of worker processes. Each worker process should: - Continuously fetch tasks from the task queue. - Apply the `compute` function on the fetched tasks. - Store the result into the results queue. - Adds a list of integers `[1, 2, 3, ..., 20]` to the task queue. - Collects the results from the results queue until all tasks are processed. - Gracefully terminates the worker processes when the tasks are completed. 3. Ensure proper synchronization to avoid potential race conditions. # Input: - A list of integers `[1, 2, 3, ..., 20]`. # Output: - A printed list of computed results (i.e., squares of input integers). # Constraints: - Use the `multiprocessing` module. - Ensure graceful termination of all processes. # Code Template: ```python from multiprocessing import Process, Queue, current_process import time # Function to compute square of a number def compute(n): return n * n # Worker function to process tasks from the task queue and store results in the result queue def worker(task_queue, result_queue): while True: task = task_queue.get() if task is None: # Check for sentinel to terminate the process break result = compute(task) result_queue.put(result) def main(): num_workers = 4 tasks = list(range(1, 21)) # Tasks list [1, 2, 3, ..., 20] task_queue = Queue() result_queue = Queue() # Create and start worker processes workers = [] for _ in range(num_workers): p = Process(target=worker, args=(task_queue, result_queue)) workers.append(p) p.start() # Add tasks to the task queue for task in tasks: task_queue.put(task) # Add sentinel values to the task queue to signal workers to stop for _ in range(num_workers): task_queue.put(None) # Collect results from the result queue results = [] for _ in range(len(tasks)): result = result_queue.get() results.append(result) # Wait for all workers to finish for p in workers: p.join() # Print the results print(results) if __name__ == \'__main__\': main() ``` # Explanation: 1. **compute function**: A simple function to compute the square of a number. 2. **worker function**: A function that continuously processes tasks from a task queue, computes the results using the `compute` function, and stores the results in the result queue. 3. **main function**: - Creates task and result queues. - Spawns worker processes. - Adds tasks to the task queue. - Inserts `None` values in the task queue as sentinel values to signal workers to stop. - Collects results from the result queue. - Ensures all worker processes are gracefully terminated. # Performance Considerations: - Ensure that all tasks are processed efficiently. - Handle synchronization properly to avoid race conditions. - Gracefully terminate all processes to avoid any resource leaks. # Additional Notes: - The implementation should be robust and handle any unexpected issues, such as process crashes or exceptions, smoothly. - Provide necessary comments and document your code to explain your approach and design decisions.","solution":"from multiprocessing import Process, Queue, current_process # Function to compute square of a number def compute(n): return n * n # Worker function to process tasks from the task queue and store results in the result queue def worker(task_queue, result_queue): while True: task = task_queue.get() if task is None: # Check for sentinel to terminate the process break result = compute(task) result_queue.put(result) def main(): num_workers = 4 tasks = list(range(1, 21)) # Tasks list [1, 2, 3, ..., 20] task_queue = Queue() result_queue = Queue() # Create and start worker processes workers = [] for _ in range(num_workers): p = Process(target=worker, args=(task_queue, result_queue)) workers.append(p) p.start() # Add tasks to the task queue for task in tasks: task_queue.put(task) # Add sentinel values to the task queue to signal workers to stop for _ in range(num_workers): task_queue.put(None) # Collect results from the result queue results = [] for _ in range(len(tasks)): result = result_queue.get() results.append(result) # Wait for all workers to finish for p in workers: p.join() # Print the results print(sorted(results)) if __name__ == \'__main__\': main()"},{"question":"Objective Design a function to calculate the total score of a list of nested lists of expressions based on specific criteria using Python expressions. This question is meant to assess your ability to work with various Python syntax elements and ensures you are comfortable using list comprehensions, generator expressions, and handling exceptions. Problem Statement You need to implement the function `evaluate_expressions(expressions: List[List[str]]) -> float` which takes a list of nested lists of expressions. Each expression is a string that needs to be evaluated. The function should return the sum of the evaluated results of all expressions. In case an error is encountered while evaluating an expression, your function should handle it gracefully by substituting a default value of `10.0` for that erroneous expression. The expressions can include using arithmetic operations (`+`, `-`, `*`, `/`, `**`), list comprehensions, generator expressions, and function calls that may raise exceptions. Requirements 1. Evaluate each expression in the list of nested lists. 2. Use list comprehensions or generator expressions to efficiently iterate through the lists. 3. Handle exceptions using try-except blocks and substitute a default value of `10.0` for any erroneous expression. 4. Return the total score as a float. Input and Output Format - **Input**: A list of nested lists of strings `expressions`. Each string is a Python expression. Example: ```python expressions = [ [\\"4 + 5\\", \\"10 / 2\\"], [\\"2 ** 3\\", \\"5 + \'a\'\\"], # \'5 + \'a\'\' will raise a TypeError. [\\"sum([i for i in range(10)])\\", \\"(i for i in range(3))\\"] ] ``` - **Output**: A float representing the sum of evaluated expressions or default values if errors occur. Example: ```python total_score = 104.0 # calculated as 9 + 5 + 8 + 10 (default for error) + 45 + (0 as the generator is evaluated once directly) ``` Constraints - The function should handle both simple and complex expressions. - Any expression that includes invalid syntax or operations that raise exceptions must be caught and handled. - The total score must be computed efficiently. Example ```python def evaluate_expressions(expressions: List[List[str]]) -> float: total_score = 0.0 for sublist in expressions: for expr in sublist: try: value = eval(expr) # If it is a generator expression, force evaluation to get the total. if isinstance(value, (int, float)): total_score += value elif hasattr(value, \'__iter__\'): total_score += sum(value) else: total_score += 10.0 # Default for non-iterable unexpected valid expression except Exception as e: total_score += 10.0 # Default value for errors return total_score # Test case expressions = [ [\\"4 + 5\\", \\"10 / 2\\"], [\\"2 ** 3\\", \\"5 + \'a\'\\"], # \'5 + \'a\'\' will raise a TypeError. [\\"sum([i for i in range(10)])\\", \\"sum(i for i in range(3))\\"] ] print(evaluate_expressions(expressions)) # Output: 104.0 ```","solution":"from typing import List def evaluate_expressions(expressions: List[List[str]]) -> float: total_score = 0.0 for sublist in expressions: for expr in sublist: try: value = eval(expr) if isinstance(value, (int, float)): total_score += value elif hasattr(value, \'__iter__\'): total_score += sum(value) else: total_score += 10.0 # Default for non-iterable unexpected valid expression except Exception: total_score += 10.0 # Default value for errors return total_score"},{"question":"**Objective:** Design and implement a set of Python functions to manage a simple inventory system for a store. This task will require understanding of list operations, dictionary management, function implementation, and error handling. Problem Statement You are required to implement a class `InventoryManager` that manages an inventory of items in a store. Each item in the inventory has a unique item code, a name, a category, and a quantity. Your task is to complete the following methods in the `InventoryManager` class: 1. **`__init__(self)`**: Initialize the inventory as an empty dictionary. 2. **`add_item(self, item_code, name, category, quantity)`**: Adds a new item to the inventory. If an item with the same item code already exists, raise a `ValueError` with an appropriate message. 3. **`remove_item(self, item_code)`**: Removes an item from the inventory based on its item code. If the item code does not exist, raise a `KeyError`. 4. **`update_quantity(self, item_code, quantity)`**: Updates the quantity of the specified item. If the item code does not exist, raise a `KeyError`. 5. **`get_item(self, item_code)`**: Returns the details of the item as a dictionary. If the item code does not exist, raise a `KeyError`. 6. **`get_items_by_category(self, category)`**: Returns a list of items in the specified category. If no items are found, return an empty list. Constraints - `item_code`: a unique identifier for the item (string). - `name`: name of the item (string). - `category`: category of the item (string). - `quantity`: number of items available in inventory (integer). Input and Output - The methods should handle inputs as described and should not involve any I/O operations like reading from input or printing to the console. - Any invalid operations should raise appropriate exceptions. Example Usage ```python inventory = InventoryManager() inventory.add_item(\\"A001\\", \\"Widget\\", \\"Gadgets\\", 10) inventory.add_item(\\"B002\\", \\"Gizmo\\", \\"Gadgets\\", 5) print(inventory.get_item(\\"A001\\")) # Output: {\'name\': \'Widget\', \'category\': \'Gadgets\', \'quantity\': 10} inventory.update_quantity(\\"A001\\", 15) inventory.remove_item(\\"B002\\") print(inventory.get_items_by_category(\\"Gadgets\\")) # Output: [{\'item_code\': \'A001\', \'name\': \'Widget\', \'quantity\': 15}] ``` Implementation ```python class InventoryManager: def __init__(self): self.inventory = {} def add_item(self, item_code, name, category, quantity): if item_code in self.inventory: raise ValueError(f\\"Item with code {item_code} already exists.\\") self.inventory[item_code] = {\'name\': name, \'category\': category, \'quantity\': quantity} def remove_item(self, item_code): if item_code not in self.inventory: raise KeyError(f\\"Item with code {item_code} not found.\\") del self.inventory[item_code] def update_quantity(self, item_code, quantity): if item_code not in self.inventory: raise KeyError(f\\"Item with code {item_code} not found.\\") self.inventory[item_code][\'quantity\'] = quantity def get_item(self, item_code): if item_code not in self.inventory: raise KeyError(f\\"Item with code {item_code} not found.\\") item = self.inventory[item_code].copy() item[\'item_code\'] = item_code return item def get_items_by_category(self, category): result = [] for item_code, details in self.inventory.items(): if details[\'category\'] == category: details_copy = details.copy() details_copy[\'item_code\'] = item_code result.append(details_copy) return result ``` **Note:** Ensure to handle all edge cases, including attempting to add items with duplicate item codes, updating or removing non-existing items, and dealing with inventory categories that have no items.","solution":"class InventoryManager: def __init__(self): self.inventory = {} def add_item(self, item_code, name, category, quantity): if item_code in self.inventory: raise ValueError(f\\"Item with code {item_code} already exists.\\") self.inventory[item_code] = {\'name\': name, \'category\': category, \'quantity\': quantity} def remove_item(self, item_code): if item_code not in self.inventory: raise KeyError(f\\"Item with code {item_code} not found.\\") del self.inventory[item_code] def update_quantity(self, item_code, quantity): if item_code not in self.inventory: raise KeyError(f\\"Item with code {item_code} not found.\\") self.inventory[item_code][\'quantity\'] = quantity def get_item(self, item_code): if item_code not in self.inventory: raise KeyError(f\\"Item with code {item_code} not found.\\") item = self.inventory[item_code].copy() item[\'item_code\'] = item_code return item def get_items_by_category(self, category): result = [] for item_code, details in self.inventory.items(): if details[\'category\'] == category: details_copy = details.copy() details_copy[\'item_code\'] = item_code result.append(details_copy) return result"},{"question":"<|Analysis Begin|> The provided documentation is for the Python module `plistlib`, which is used for reading and writing Apple \\"property list\\" files. These files can be in XML or binary format, and they can store various types of data including dictionaries, lists, numbers, strings, bytes, byte arrays, and datetime objects. The key functions in `plistlib` include: - `load(fp, *, fmt=None, dict_type=dict)`: Reads a plist file and returns the root object. - `loads(data, *, fmt=None, dict_type=dict)`: Loads a plist from a bytes object and returns the root object. - `dump(value, fp, *, fmt=FMT_XML, sort_keys=True, skipkeys=False)`: Writes a value to a plist file. - `dumps(value, *, fmt=FMT_XML, sort_keys=True, skipkeys=False)`: Returns a value as a plist-formatted bytes object. The module also includes a class: - `UID(data)`: Wraps an integer for use with NSKeyedArchiver encoded data. Understanding the use of `plistlib` involves being able to handle file operations (reading and writing), differentiate between XML and binary formats, and serialize/deserialize complex data structures. <|Analysis End|> <|Question Begin|> # Advanced Coding Assessment: Handling Property List Files with `plistlib` **Objective:** You are tasked with developing a program that interacts with Apple\'s property list (plist) files. Your solution should demonstrate a thorough understanding of Python\'s `plistlib` module, including reading, writing, and processing complex plist data structures. **Problem Statement:** 1. **Reading a Plist File:** Implement a function `read_plist(file_path: str) -> dict` that reads a plist file from the given file path and returns the root object, which should be a dictionary. ```python def read_plist(file_path: str) -> dict: Reads a plist file from the given file path. Parameters: file_path (str): The path to the plist file. Returns: dict: The root dictionary object from the plist file. # Your code here ``` 2. **Writing a Plist File:** Implement a function `write_plist(data: dict, file_path: str, fmt: str = \'FMT_XML\') -> None` that writes the provided dictionary data to a plist file at the specified file path. The `fmt` parameter should determine whether the file is written in XML or binary format. ```python def write_plist(data: dict, file_path: str, fmt: str = \'FMT_XML\') -> None: Writes a dictionary to a plist file in the specified format. Parameters: data (dict): The dictionary to be written to the plist file. file_path (str): The path where the plist file will be written. fmt (str): The format of the plist file (\'FMT_XML\' or \'FMT_BINARY\'). Default is \'FMT_XML\'. Returns: None # Your code here ``` 3. **Transforming Plist Data:** Implement a function `transform_plist(data: dict) -> dict` that takes a plist root dictionary object, processes it, and returns a transformed dictionary. The transformation should include: - Converting all string values to uppercase. - Converting all boolean values (`True` to `False` and vice versa). - For list values, reverse the list. - For nested dictionaries, apply the same transformations recursively. ```python def transform_plist(data: dict) -> dict: Transforms the plist dictionary data. Parameters: data (dict): The root dictionary from the plist file. Returns: dict: The transformed dictionary. # Your code here ``` **Input and Output Constraints:** - The plist files can have complex nesting, so ensure your functions handle nested structures. - The `transform_plist` function should maintain the structure of the input dictionary but apply the transformations as specified. - Assume that the plist file is well-formed and does not contain malformed XML or unsupported data types. **Performance Requirements:** - Your solution should efficiently handle plist files with large datasets and deeply nested structures. **Example Usage:** ```python # Example Plist Data example_data = { \\"name\\": \\"example\\", \\"active\\": True, \\"items\\": [1, 2, 3, 4], \\"meta\\": { \\"created\\": \\"2021-01-01\\", \\"valid\\": False } } # Write the example data to an XML plist file write_plist(example_data, \\"example.plist\\", fmt=\'FMT_XML\') # Read the data back from the plist file read_data = read_plist(\\"example.plist\\") # Transform the read data transformed_data = transform_plist(read_data) # Output the transformed data print(transformed_data) ``` Your solution will be evaluated based on correctness, efficiency, and adherence to the provided guidelines.","solution":"import plistlib def read_plist(file_path: str) -> dict: Reads a plist file from the given file path. Parameters: file_path (str): The path to the plist file. Returns: dict: The root dictionary object from the plist file. with open(file_path, \'rb\') as fp: return plistlib.load(fp) def write_plist(data: dict, file_path: str, fmt: str = \'FMT_XML\') -> None: Writes a dictionary to a plist file in the specified format. Parameters: data (dict): The dictionary to be written to the plist file. file_path (str): The path where the plist file will be written. fmt (str): The format of the plist file (\'FMT_XML\' or \'FMT_BINARY\'). Default is \'FMT_XML\'. Returns: None format_map = { \'FMT_XML\': plistlib.FMT_XML, \'FMT_BINARY\': plistlib.FMT_BINARY } with open(file_path, \'wb\') as fp: plistlib.dump(data, fp, fmt=format_map.get(fmt, plistlib.FMT_XML)) def transform_plist(data: dict) -> dict: Transforms the plist dictionary data. Parameters: data (dict): The root dictionary from the plist file. Returns: dict: The transformed dictionary. def transform(value): if isinstance(value, str): return value.upper() elif isinstance(value, bool): return not value elif isinstance(value, list): return [transform(item) for item in reversed(value)] elif isinstance(value, dict): return {k: transform(v) for k, v in value.items()} return value return transform(data)"},{"question":"**Objective:** Implement a Python function that manipulates strings and lists and performs basic arithmetic operations to solve a problem on character frequency analysis and string formation. **Problem Statement:** Write a function `char_frequency_analysis(input_string: str) -> List[str]:` that takes a single argument, `input_string`, which is a non-empty string containing only lowercase alphabetic characters. The function should perform the following tasks: 1. **Character Frequency Calculation:** - Calculate the frequency of each character in the input string. 2. **List Formation:** - Create a list of tuples, where each tuple consists of a character and its frequency, sorted in descending order of frequency. If two characters have the same frequency, they should be ordered by their first appearance in the input string. 3. **String Formation:** - Form a new string by concatenating each character in the input string in order of their frequency (from the highest to the lowest). Each character should appear as many times as its frequency. **Constraints:** - The length of `input_string` is between 1 and 1000 inclusive. - The input string consists only of lowercase letters (\'a\'-\'z\'). **Function Signature:** ```python def char_frequency_analysis(input_string: str) -> List[str]: pass ``` # Example: Input: ```python input_string = \\"abracadabra\\" ``` Output: ```python [ (\'a\', 5), # \'a\' occurs 5 times (\'b\', 2), # \'b\' occurs 2 times (\'r\', 2), # \'r\' occurs 2 times (\'c\', 1), # \'c\' occurs 1 time (\'d\', 1) # \'d\' occurs 1 time ] ``` Explanation: 1. **Character Frequency Calculation:** - a: 5, b: 2, r: 2, c: 1, d: 1 2. **List Formation:** - [(\'a\', 5), (\'b\', 2), (\'r\', 2), (\'c\', 1), (\'d\', 1)] 3. **String Formation:** - The new string would be \\"aaaaabbrrcd\\" (formed by taking each character and repeating it by its frequency, sorted in order of frequency). # Steps to Implement the Function: 1. Create a dictionary to keep track of the frequency of each character. 2. Convert the dictionary to a list of tuples and sort by frequency in descending order. 3. Generate the concatenated string based on the sorted frequencies. # Notes: - Use Python\'s built-in functions and methods to simplify tasks such as sorting and string manipulation. - Ensure the code is efficient and handles the constraints effectively.","solution":"from collections import defaultdict from typing import List, Tuple def char_frequency_analysis(input_string: str) -> List[Tuple[str, int]]: # Step 1: Calculate the frequency of each character in the input string frequency_dict = defaultdict(int) for char in input_string: frequency_dict[char] += 1 # Step 2: Create a list of tuples and sort by frequency and order of appearance frequency_list = list(frequency_dict.items()) frequency_list.sort(key=lambda x: (-x[1], input_string.index(x[0]))) # Step 3: Return the sorted frequency list return frequency_list"},{"question":"# Custom Exception Handling and Chaining in Python Objective: Design classes and functions that leverage custom exceptions by inheriting from built-in exceptions. Implement a function that raises and handles these custom exceptions while demonstrating exception chaining and context. Problem Statement: You are required to implement the following: 1. **Custom Exceptions**: - Define a custom exception `InvalidOperationError` that inherits from `Exception`. - Define another custom exception `OperationTimeoutError` that inherits from `TimeoutError`. 2. **Function Implementation**: - Implement a function `perform_operation` that takes an integer argument `n`. - The function should perform the following checks and raise the custom exceptions where applicable: - If `n` is less than 0, raise `InvalidOperationError` with the message \\"Invalid operation: n must be non-negative\\". - Simulate a timeout scenario by raising `OperationTimeoutError` when `n` is greater than 100 with the message \\"Operation timed out\\". 3. **Exception Handler**: - Implement a function `execute` that calls `perform_operation` within a try-except block. - Inside the exception handler, if `InvalidOperationError` is caught, raise a `ValueError` with the message \\"Value error occurred due to invalid operation\\" and maintain the context of the original exception. - For `OperationTimeoutError`, re-raise it with the original context. 4. **Testing**: - Test the `execute` function with various values of `n` to demonstrate exception handling and chaining: - `n = -1` should show a `ValueError` with context from `InvalidOperationError`. - `n = 150` should show `OperationTimeoutError`. Constraints: - Do not use global variables. - Ensure the custom exception messages are clear and informative. - Use proper exception chaining techniques. Example: ```python class InvalidOperationError(Exception): pass class OperationTimeoutError(TimeoutError): pass def perform_operation(n): if n < 0: raise InvalidOperationError(\\"Invalid operation: n must be non-negative\\") elif n > 100: raise OperationTimeoutError(\\"Operation timed out\\") def execute(n): try: perform_operation(n) except InvalidOperationError as e: raise ValueError(\\"Value error occurred due to invalid operation\\") from e except OperationTimeoutError: raise print(\\"Operation performed successfully\\") # Test cases: try: execute(-1) except Exception as e: print(repr(e)) try: execute(150) except Exception as e: print(repr(e)) # Expected output: # ValueError(\'Value error occurred due to invalid operation\') # OperationTimeoutError(\'Operation timed out\') ``` This problem assesses the student\'s ability to: - Define and utilize custom exceptions. - Implement exception handling and chaining effectively. - Understand and apply the concept of exception context.","solution":"class InvalidOperationError(Exception): Custom exception for invalid operations where the input is not within the expected range. pass class OperationTimeoutError(TimeoutError): Custom exception for operations that take too long to complete. pass def perform_operation(n): Function to perform an operation that checks if the input is within a valid range. Args: - n (int): The input number to check. Raises: - InvalidOperationError: If n is less than 0. - OperationTimeoutError: If n is greater than 100. if n < 0: raise InvalidOperationError(\\"Invalid operation: n must be non-negative\\") elif n > 100: raise OperationTimeoutError(\\"Operation timed out\\") def execute(n): Wrapper function to execute the perform_operation function, handling exceptions. Args: - n (int): The input number to pass to perform_operation. Raises: - ValueError: If InvalidOperationError is encountered, with the original context. try: perform_operation(n) except InvalidOperationError as e: raise ValueError(\\"Value error occurred due to invalid operation\\") from e except OperationTimeoutError: raise"},{"question":"**Problem Statement:** You are required to create a debugging tool named `TracebackFormatter`. This tool will be used to capture, format, and print custom formatted tracebacks for exceptions that occur in a given user-defined function. Your implementation should provide a user-friendly output highlighting the exact location and type of exception. **Requirements:** 1. Implement a class `TracebackFormatter` with the following methods: - `wrap_function(func)`: A decorator method that takes a function `func` as input and returns a wrapped function which, when called, executes `func` and captures any exceptions that occur. If an exception is caught, it formats the traceback using the `traceback` module and returns it as a string. - `format_traceback(exc_info)`: A helper method that takes an exception info tuple `(exc_type, exc_value, exc_traceback)` and returns a custom formatted traceback as a string. **Input / Output:** - *Input*: - The function wrapped by `wrap_function` may take any arguments. - Exceptions that may occur in the user-defined function. - *Output*: - A string representing the formatted traceback whenever an exception is caught. **Constraints:** - Use the `traceback` module to capture and format the tracebacks. - Your tool should handle any type of exception that can be raised by the user-defined function. - The formatted traceback should include the exception type, message, and stack trace details. **Example Usage:** ```python import sys class TracebackFormatter: def wrap_function(self, func): def wrapper(*args, **kwargs): try: return func(*args, **kwargs) except Exception as e: exc_info = sys.exc_info() return self.format_traceback(exc_info) return wrapper def format_traceback(self, exc_info): import traceback exc_type, exc_value, exc_traceback = exc_info formatted_tb = traceback.format_exception(exc_type, exc_value, exc_traceback) return \'\'.join(formatted_tb) # Example user function to be wrapped @TracebackFormatter().wrap_function def func_to_debug(x): return 10 / x # When the function causes an exception print(func_to_debug(0)) ``` **Expected Output:** A string representing the formatted traceback, similar to the following: ``` Traceback (most recent call last): File \\"<stdin>\\", line 2, in wrapper File \\"<stdin>\\", line 3, in func_to_debug ZeroDivisionError: division by zero ``` **Note**: Your implementation should work correctly irrespective of the function being wrapped and the type of exceptions raised.","solution":"import sys import traceback class TracebackFormatter: def wrap_function(self, func): def wrapper(*args, **kwargs): try: return func(*args, **kwargs) except Exception as e: exc_info = sys.exc_info() return self.format_traceback(exc_info) return wrapper def format_traceback(self, exc_info): exc_type, exc_value, exc_traceback = exc_info formatted_tb = traceback.format_exception(exc_type, exc_value, exc_traceback) return \'\'.join(formatted_tb) # Example user function to be wrapped @TracebackFormatter().wrap_function def func_to_debug(x): return 10 / x"},{"question":"# Clustering Multiple Datapoints with Different Clustering Methods You have been given a dataset of 2D points representing different but unknown underlying cluster structures. Your task is to implement a function that performs clustering using multiple clustering algorithms from scikit-learn and evaluates their performance using silhouette scores. # Requirements: 1. Implement the function `compare_clustering_methods(data: np.ndarray, n_clusters: int) -> dict`. 2. The function should accept: - `data`: A 2D numpy array of shape (n_samples, 2). - `n_clusters`: An integer specifying the number of clusters to fit. 3. The function should use the following clustering algorithms: - KMeans - SpectralClustering - DBSCAN - AgglomerativeClustering 4. For each clustering method: - Fit the model to the data. - Predict the cluster labels. - Compute the silhouette score for the clustering. 5. Return a dictionary containing the clustering method names as keys and their corresponding silhouette scores as values. Constraints: - Use `random_state=42` for any method that requires it. - Use default parameters for clustering methods unless specified. - For DBSCAN, set `eps` to 0.5 and `min_samples` to 5. - Handle any exceptions that might occur during clustering. # Input: - `data`: 2D numpy array, shape (n_samples, 2). Example: ```python np.array([[1.1, 2.2], [1.2, 2.3], [9.1, 8.7], [9.0, 8.8]]) ``` - `n_clusters`: Integer, number of clusters. Example: 2 # Output: - Dictionary with clustering method names as keys and silhouette scores as values. Example: ```python {\'KMeans\': 0.75, \'SpectralClustering\': 0.6, \'DBSCAN\': 0.45, \'AgglomerativeClustering\': 0.7} ``` # Function Template: ```python from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, AgglomerativeClustering from sklearn.metrics import silhouette_score import numpy as np def compare_clustering_methods(data: np.ndarray, n_clusters: int) -> dict: results = {} # Implement KMeans clustering try: kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(data) kmeans_labels = kmeans.labels_ kmeans_score = silhouette_score(data, kmeans_labels) results[\'KMeans\'] = kmeans_score except Exception as e: results[\'KMeans\'] = str(e) # Implement SpectralClustering try: spectral = SpectralClustering(n_clusters=n_clusters, random_state=42).fit(data) spectral_labels = spectral.labels_ spectral_score = silhouette_score(data, spectral_labels) results[\'SpectralClustering\'] = spectral_score except Exception as e: results[\'SpectralClustering\'] = str(e) # Implement DBSCAN clustering try: dbscan = DBSCAN(eps=0.5, min_samples=5).fit(data) dbscan_labels = dbscan.labels_ if len(set(dbscan_labels)) == 1: # To avoid issues with silhouette score results[\'DBSCAN\'] = \\"Only one cluster found\\" else: dbscan_score = silhouette_score(data, dbscan_labels) results[\'DBSCAN\'] = dbscan_score except Exception as e: results[\'DBSCAN\'] = str(e) # Implement AgglomerativeClustering try: agglomerative = AgglomerativeClustering(n_clusters=n_clusters).fit(data) agglomerative_labels = agglomerative.labels_ agglomerative_score = silhouette_score(data, agglomerative_labels) results[\'AgglomerativeClustering\'] = agglomerative_score except Exception as e: results[\'AgglomerativeClustering\'] = str(e) return results ```","solution":"from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, AgglomerativeClustering from sklearn.metrics import silhouette_score import numpy as np def compare_clustering_methods(data: np.ndarray, n_clusters: int) -> dict: results = {} # Implement KMeans clustering try: kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(data) kmeans_labels = kmeans.labels_ kmeans_score = silhouette_score(data, kmeans_labels) results[\'KMeans\'] = kmeans_score except Exception as e: results[\'KMeans\'] = str(e) # Implement SpectralClustering try: spectral = SpectralClustering(n_clusters=n_clusters, random_state=42).fit(data) spectral_labels = spectral.labels_ spectral_score = silhouette_score(data, spectral_labels) results[\'SpectralClustering\'] = spectral_score except Exception as e: results[\'SpectralClustering\'] = str(e) # Implement DBSCAN clustering try: dbscan = DBSCAN(eps=0.5, min_samples=5).fit(data) dbscan_labels = dbscan.labels_ if len(set(dbscan_labels)) == 1: # To avoid issues with silhouette score results[\'DBSCAN\'] = \\"Only one cluster found\\" else: dbscan_score = silhouette_score(data, dbscan_labels) results[\'DBSCAN\'] = dbscan_score except Exception as e: results[\'DBSCAN\'] = str(e) # Implement AgglomerativeClustering try: agglomerative = AgglomerativeClustering(n_clusters=n_clusters).fit(data) agglomerative_labels = agglomerative.labels_ agglomerative_score = silhouette_score(data, agglomerative_labels) results[\'AgglomerativeClustering\'] = agglomerative_score except Exception as e: results[\'AgglomerativeClustering\'] = str(e) return results"},{"question":"**Objective:** Create a Python class that uses descriptors to manage and validate its attributes. This question will test your understanding of custom attribute management, dynamic lookups, and ensuring data integrity. **Problem Statement:** You need to implement a class `Product` which maintains details about products in an inventory. The `Product` class must enforce certain constraints on its attributes using descriptors. # Class Requirements: 1. **Attributes:** - `name`: a valid product name which should always be a non-empty string. - `quantity`: a non-negative integer representing the stock count. - `price`: a floating-point number that cannot be negative. 2. **Validation:** - Implement custom validators to ensure: - The `name` must be a non-empty string. - The `quantity` must be an integer and greater than or equal to 0. - The `price` must be a non-negative float. # Constraints: - If an invalid value is assigned to any attribute, raise an appropriate exception (ValueError or TypeError). # Implementation: Create a class `Product` as specified and the necessary descriptor classes. Your `Product` class should have the following initializer method: ```python class Product: def __init__(self, name: str, quantity: int, price: float): self.name = name self.quantity = quantity self.price = price ``` # Example: ```python # Valid product creation p1 = Product(\\"Book\\", 10, 5.99) print(p1.name) # Output: Book print(p1.quantity) # Output: 10 print(p1.price) # Output: 5.99 # Invalid product creation try: p2 = Product(\\"\\", 10, 5.99) except ValueError as e: print(e) # Output: Name must be a non-empty string try: p3 = Product(\\"Book\\", -3, 5.99) except ValueError as e: print(e) # Output: Quantity must be a non-negative integer try: p4 = Product(\\"Book\\", 10, -5.99) except ValueError as e: print(e) # Output: Price must be a non-negative float ``` # Steps: 1. Design the `Validator` base descriptor class. 2. Create derived classes for each attribute validator (e.g., `NameValidator`, `QuantityValidator`, `PriceValidator`). 3. Implement the `Product` class using these descriptor validators. # Performance Requirement: - Optimize attribute lookups and validations in the `Product` class. - Ensure that the code handles a high number of validations efficiently. Good luck! Analyze the problem, design your descriptors effectively, and ensure data integrity using Python descriptors.","solution":"class NameValidator: def __init__(self): self._name = \\"_name\\" def __get__(self, instance, owner): return getattr(instance, self._name) def __set__(self, instance, value): if not isinstance(value, str) or not value.strip(): raise ValueError(\\"Name must be a non-empty string\\") setattr(instance, self._name, value) class QuantityValidator: def __init__(self): self._quantity = \\"_quantity\\" def __get__(self, instance, owner): return getattr(instance, self._quantity) def __set__(self, instance, value): if not isinstance(value, int) or value < 0: raise ValueError(\\"Quantity must be a non-negative integer\\") setattr(instance, self._quantity, value) class PriceValidator: def __init__(self): self._price = \\"_price\\" def __get__(self, instance, owner): return getattr(instance, self._price) def __set__(self, instance, value): if not isinstance(value, (float, int)) or value < 0: raise ValueError(\\"Price must be a non-negative float\\") setattr(instance, self._price, float(value)) class Product: name = NameValidator() quantity = QuantityValidator() price = PriceValidator() def __init__(self, name: str, quantity: int, price: float): self.name = name self.quantity = quantity self.price = price"},{"question":"# Objective Evaluate the understanding and proficiency in using seaborn\'s features for customizing the appearance of plots. # Problem Statement You are provided with a dataset of random normally-distributed values. Your task is to create a visual representation of this data using seaborn, specifically focusing on customizing the plot aesthetics. # Requirements 1. **Create a dataset**: - Generate a dataset of 50 observations and 5 variables, where each variable is normally distributed with a mean ranging from 0 to 2 and a standard deviation of 1. 2. **Generate Plots**: - Create four box plots using seaborn, each with a different style and context. The styles should be: \\"darkgrid\\", \\"whitegrid\\", \\"dark\\", and \\"ticks\\". - Apply different contexts: \\"paper\\" for the first plot, \\"notebook\\" for the second plot, \\"talk\\" for the third plot, and \\"poster\\" for the fourth plot. 3. **Customize Each Plot**: - Remove the top and right spines on plots with styles \\"whitegrid\\" and \\"ticks\\". - Offset the remaining spines (left and bottom) by 10 units in the \\"ticks\\" style plot. - Apply a linewidth of 2.5 to the lines in the \\"poster\\" context plot. 4. **Temporary Context**: - Within a `with` statement, generate a fifth plot using a \\"white\\" style and \\"talk\\" context. Add a violin plot of the same dataset. # Function Signature ```python def customized_seaborn_plots(): # Create dataset # Generate and customize plots pass ``` # Constraints - Ensure each plot is clearly labeled and distinguished by its style and context. - Use seaborn and matplotlib.pyplot for visualization. - Your solution should not require changing any global settings permanently. # Expected Output - Five distinct plots displayed using matplotlib\'s `plt.show()`, each adhering to the specified aesthetic customizations. # Example The output should be five plots displayed in a single call: - 1st plot: Box plot with \\"darkgrid\\" style and \\"paper\\" context. - 2nd plot: Box plot with \\"whitegrid\\" style and \\"notebook\\" context, with top and right spines removed. - 3rd plot: Box plot with \\"dark\\" style and \\"talk\\" context. - 4th plot: Box plot with \\"ticks\\" style and \\"poster\\" context, top and right spines removed, left and bottom spines offset by 10 units, lines with linewidth 2.5. - 5th plot: Violin plot inside a with statement using \\"white\\" style and \\"talk\\" context.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def customized_seaborn_plots(): # Create dataset np.random.seed(10) data = pd.DataFrame({ \'Var1\': np.random.normal(0, 1, 50), \'Var2\': np.random.normal(0.5, 1, 50), \'Var3\': np.random.normal(1, 1, 50), \'Var4\': np.random.normal(1.5, 1, 50), \'Var5\': np.random.normal(2, 1, 50) }) # Plot 1: Box plot with \\"darkgrid\\" style and \\"paper\\" context sns.set(style=\\"darkgrid\\", context=\\"paper\\") plt.figure() sns.boxplot(data=data) plt.title(\'Box plot with darkgrid style and paper context\') plt.show() # Plot 2: Box plot with \\"whitegrid\\" style and \\"notebook\\" context sns.set(style=\\"whitegrid\\", context=\\"notebook\\") plt.figure() sns.boxplot(data=data) sns.despine(top=True, right=True) plt.title(\'Box plot with whitegrid style and notebook context\') plt.show() # Plot 3: Box plot with \\"dark\\" style and \\"talk\\" context sns.set(style=\\"dark\\", context=\\"talk\\") plt.figure() sns.boxplot(data=data) plt.title(\'Box plot with dark style and talk context\') plt.show() # Plot 4: Box plot with \\"ticks\\" style and \\"poster\\" context sns.set(style=\\"ticks\\", context=\\"poster\\") plt.figure() sns.boxplot(data=data) sns.despine(top=True, right=True) plt.gca().spines[\'left\'].set_position((\'outward\', 10)) plt.gca().spines[\'bottom\'].set_position((\'outward\', 10)) plt.gca().spines[\'left\'].set_linewidth(2.5) plt.gca().spines[\'bottom\'].set_linewidth(2.5) plt.title(\'Box plot with ticks style and poster context\') plt.show() # Plot 5: Violin plot within a with statement using \\"white\\" style and \\"talk\\" context with sns.axes_style(\\"white\\"), sns.plotting_context(\\"talk\\"): plt.figure() sns.violinplot(data=data) plt.title(\'Violin plot with white style and talk context\') plt.show()"},{"question":"**Question:** You are tasked with creating an informative visualization for a new dataset using seaborn\'s `seaborn.objects` module. Your goal is to demonstrate the use of various transformations and layering techniques to create a meaningful plot. # Dataset You are provided with the \\"tips\\" dataset, which contains information about restaurant tips. The dataset has the following columns: - `total_bill`: Total bill amount. - `tip`: Tip amount. - `sex`: Gender of the person paying. - `smoker`: Whether the person is a smoker or not. - `day`: Day of the week. - `time`: Time of day (Lunch or Dinner). - `size`: Number of people in the party. You can load the dataset using the following code: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` # Task 1. Create a scatter plot of `total_bill` vs `tip` with dots jittered to avoid overlap. 2. Overlay a layer representing the 50th percentile range of `tip` values for each `day`. 3. Shift the percentile range to the right by 0.25 units for better visibility. # Your function should: - Be named `create_tips_plot`. - Not take any arguments. - Load the \\"tips\\" dataset within the function. - Create and display the described plot. # Example ```python def create_tips_plot(): import seaborn.objects as so import seaborn as sns tips = sns.load_dataset(\\"tips\\") ( so.Plot(tips, \\"total_bill\\", \\"tip\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.25)) ) ``` # Constraints - Use only the seaborn package for creating the plot. - Ensure that the plot is clear and visually informative. # Evaluation Criteria Your solution will be evaluated on: - Correct and efficient use of seaborn\'s `seaborn.objects` module. - Correct application of jitter, percentile range, and shifting transformations. - Overall clarity and informativeness of the plot.","solution":"import seaborn.objects as so import seaborn as sns def create_tips_plot(): Creates a scatter plot of total_bill vs tip with jittered dots and overlays a layer representing the 25th to 75th percentile range of tip values for each day, shifted to the right by 0.25 units. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the plot ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") .add(so.Dots(), so.Jitter()) # Jitter the dots to avoid overlap .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.25)) # Add the 25th-75th percentile range, shifted by 0.25 units .show() )"},{"question":"**Problem: Custom String Parser and Value Constructor** You are required to implement a function `parse_and_construct` that takes a specially formatted string as input and returns a constructed Python object based on parsed values within the string. The function has to make appropriate use of the python310 utilities for parsing arguments and building values. # **Input Format:** 1. `input_string` : A string containing values to be parsed and formatted as - `\\"arg1:type1,arg2:type2,arg3:type3\\"` # **Output Format:** - A Python object constructed using the parsed values. The returned object can be a Python list or dictionary based on the format provided in the string. # **Constraints:** - The input string will only contain valid Python characters. - The types supported include `\\"int\\"`, `\\"float\\"`, `\\"str\\"`, and `\\"bool\\"`. - Ensure efficient parsing and construction for the input sizes up to 1,000 values. # **Example:** Example 1: **Input**: ```python input_string = \\"1:int,2:int,3:int\\" ``` **Output**: ```python [1, 2, 3] ``` Example 2: **Input**: ```python input_string = \\"key1:str,key2:str,key3:str\\" ``` **Output**: ```python [\\"key1\\", \\"key2\\", \\"key3\\"] ``` Example 3: **Input**: ```python input_string = \\"true:bool,false:bool\\" ``` **Output**: ```python [True, False] ``` # **Function Signature**: ```python def parse_and_construct(input_string: str) -> object: pass ``` Implement the function `parse_and_construct` by leveraging the provided documentation\'s parsing and value construction utilities effectively.","solution":"def parse_and_construct(input_string: str) -> object: Parses the input string and constructs a list of values based on specified types. def convert(value, type_str): if type_str == \\"int\\": return int(value) elif type_str == \\"float\\": return float(value) elif type_str == \\"str\\": return value elif type_str == \\"bool\\": return value.lower() == \\"true\\" raise ValueError(f\\"Unsupported type: {type_str}\\") values = [] items = input_string.split(\\",\\") for item in items: value, type_str = item.split(\\":\\") values.append(convert(value, type_str)) return values"},{"question":"**Objective:** Implement a function `parse_ascii_string` that processes a given string by analyzing and transforming its characters according to certain rules defined by the `curses.ascii` module. **Function Signature:** ```python def parse_ascii_string(input_string: str) -> str: pass ``` **Input:** - `input_string` (str): A string containing ASCII characters. **Output:** - Returns a new string where each character is transformed based on the following rules: 1. If the character is an ASCII control character, replace it with its mnemonic from `curses.ascii.controlnames`. 2. If the character is a printable ASCII character, it remains unchanged. 3. If the character is not an ASCII character, replace it with `\\"<non-ASCII>\\"`. **Constraints:** - Input string length may be up to (10^3) characters. - all ASCII control characters (0x00 to 0x1F and 0x7F) are considered, including the DEL character (0x7F). - Assume no character in the control names is longer than a single printable ASCII character. **Examples:** ```python # Example 1 input_string = \\"Hellox00World!\\" # x00 (NUL) should be replaced by NUL # Output: \\"HelloNULWorld!\\" # Example 2 input_string = \\"Goodx07Morningx80\\" # x07 (BEL) should be replaced by BEL # x80 is non-ASCII # Output: \\"GoodBELMorning<non-ASCII>\\" # Example 3 input_string = \\"x01x02Testx03x04\\" # x01 (SOH), x02 (STX), x03 (ETX), x04 (EOT) should be replaced by their mnemonics # Output: \\"SOHSTXTestETXEOT\\" ``` **Notes:** 1. Use the `curses.ascii` module functions to check and transform characters. 2. Control characters should be identified using `curses.ascii.isctrl()`. 3. Use the `curses.ascii.controlnames` array to get mnemonic names for control characters. 4. For non-ASCII characters, use `curses.ascii.isascii()` to check the character\'s range. # Solution Skeleton: ```python import curses.ascii def parse_ascii_string(input_string: str) -> str: result = [] for char in input_string: if curses.ascii.isctrl(char): # replace control character with its mnemonic result.append(curses.ascii.controlnames[ord(char)]) elif curses.ascii.isascii(char): # keep printable ASCII characters unchanged result.append(char if curses.ascii.isprint(char) else \'\') else: # replace non-ASCII characters result.append(\\"<non-ASCII>\\") return \'\'.join(result) ``` Implement the function by following the outlined rules and using the `curses.ascii` module appropriately to inspect and transform the characters as needed.","solution":"import curses.ascii def parse_ascii_string(input_string: str) -> str: result = [] for char in input_string: if curses.ascii.isascii(char): if curses.ascii.isctrl(char): # replace control character with its mnemonic result.append(curses.ascii.controlnames[ord(char)]) else: # keep printable ASCII characters unchanged result.append(char) else: # replace non-ASCII characters result.append(\\"<non-ASCII>\\") return \'\'.join(result)"},{"question":"**Multiprocessing in Python: Task Execution and Synchronization** **Objective:** To implement a multiprocessing solution that distributes tasks across multiple processes and ensures correct synchronization when accessing a shared resource. **Background:** You have been tasked to parallelize a computation-heavy task using Python\'s `multiprocessing` module. The computation involves generating prime numbers for a list of intervals. The results from each process need to be stored in a shared list while ensuring thread-safe updates to avoid data corruption. **Requirements:** 1. Implement a function `generate_primes(start, end)` that returns a list of prime numbers between `start` and `end` (inclusive). 2. Create a function `worker(task_queue, result_list, lock)` that: - Retrieves intervals from `task_queue`. - Generates prime numbers within each interval using `generate_primes`. - Appends the result to `result_list` in a thread-safe manner using `lock`. 3. In the main section of your code: - Initialize a `multiprocessing.Manager` and create a shared list (`Manager().list()`). - Create an instance of `multiprocessing.Lock`. - Populate a `multiprocessing.Queue` with intervals for which primes need to be generated. - Spawn multiple worker processes to process the intervals concurrently. - Ensure all processes finish execution and collect results in `result_list`. 4. Print the final list of all prime numbers generated by all processes. **Constraints:** - The number of processes should be a configurable parameter. - The intervals should have reasonable sizes to split the workload. - Use appropriate synchronization mechanisms to ensure dependable updates to shared resources. **Function Signatures:** ```python import multiprocessing from typing import List def generate_primes(start: int, end: int) -> List[int]: # Your implementation here pass def worker(task_queue: multiprocessing.Queue, result_list: List[int], lock: multiprocessing.Lock): # Your implementation here pass def main(num_processes: int, intervals: List[tuple]): # Your implementation here pass if __name__ == \'__main__\': process_count = 4 # Or another number, configurable intervals = [(10, 50), (51, 100), (101, 150), ...] # Example intervals main(process_count, intervals) ``` **Expected Output:** 1. The final combined list of prime numbers generated by all the processes without any data corruption. 2. Example: ```python Prime numbers generated: [11, 13, 17, ..., 149] ``` **Notes:** - `generate_primes(start, end)` should be efficient in finding prime numbers and should only focus on the algorithm to determine primes within a given range. - Ensure `worker` correctly handles taking tasks from the queue and updating the shared list safely. - Utilize `multiprocessing` features like `Lock`, `Queue`, and `Manager().list()` as demonstrated in the documentation. **Bonus:** - Implement a mechanism to log the activity of each worker process for monitoring purposes.","solution":"import multiprocessing from typing import List import math def generate_primes(start: int, end: int) -> List[int]: Returns a list of prime numbers between start and end (inclusive). primes = [] for num in range(start, end + 1): if num > 1: is_prime = True for i in range(2, int(math.sqrt(num)) + 1): if num % i == 0: is_prime = False break if is_prime: primes.append(num) return primes def worker(task_queue: multiprocessing.Queue, result_list: list, lock: multiprocessing.Lock): Worker function to retrieve intervals from task_queue, generate primes, and append them to result_list in a thread-safe manner using lock. while True: interval = task_queue.get() if interval is None: break start, end = interval primes = generate_primes(start, end) with lock: result_list.extend(primes) def main(num_processes: int, intervals: List[tuple]): Main function to initialize multiprocessing resources, spawn workers, and collect results. manager = multiprocessing.Manager() result_list = manager.list() lock = multiprocessing.Lock() task_queue = multiprocessing.Queue() # Populate the task queue with intervals for interval in intervals: task_queue.put(interval) # Add sentinel \'None\' tasks to queue to signal workers to exit for _ in range(num_processes): task_queue.put(None) # Spawn worker processes processes = [] for _ in range(num_processes): p = multiprocessing.Process(target=worker, args=(task_queue, result_list, lock)) p.start() processes.append(p) # Ensure all processes finish execution for p in processes: p.join() # Print the final list of all prime numbers generated by all processes print(\\"Prime numbers generated:\\", sorted(result_list)) if __name__ == \'__main__\': process_count = 4 # Or another number, configurable intervals = [(10, 50), (51, 100), (101, 150), (151, 200)] # Example intervals main(process_count, intervals)"},{"question":"Clustering Analysis and Evaluation with Scikit-learn In this coding assessment, you are required to demonstrate your understanding of clustering algorithms provided by scikit-learn. You will implement a solution that fits the clustering algorithm to a dataset and evaluates its performance using appropriate metrics. # Task: 1. **Select a Clustering Algorithm**: You\'ll choose one from the following: K-means, Mini Batch K-means, Affinity Propagation, Mean Shift, Spectral Clustering, Hierarchical Clustering, DBSCAN, HDBSCAN, BIRCH, OPTICS, or Bisecting K-Means. 2. **Implement Clustering**: - Import the necessary modules from scikit-learn. - Load a dataset suitable for clustering analysis (e.g., `sklearn.datasets.load_iris` or a synthetic dataset generated using `sklearn.datasets.make_blobs`). - Scale the dataset if necessary. - Fit the selected clustering model to the dataset. 3. **Evaluate Clustering Performance**: - Use at least two different clustering evaluation metrics (e.g., Adjusted Rand Index, Silhouette Coefficient, Calinski-Harabasz Index). - Print and interpret the results of these metrics. # Input Formats: - The dataset can be loaded using scikit-learn\'s built-in datasets or generated synthetically. - Your selected clustering algorithm should accept its typical parameters as per the scikit-learn documentation. # Output Formats: - Print the cluster labels for the dataset. - Print the evaluation metric scores: Adjusted Rand Index and Silhouette Coefficient. - Interpretation of the evaluation results. # Constraints: - Ensure your solution can handle a moderately large dataset (e.g., 150-200 samples). - Use random state for reproducibility where appropriate. # Example: Here is an example structure to guide your implementation. ```python from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import adjusted_rand_score, silhouette_score # Load dataset data = load_iris() X = data.data # Scale the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Fit the clustering model kmeans = KMeans(n_clusters=3, random_state=42) kmeans.fit(X_scaled) # Predict cluster labels labels = kmeans.labels_ print(\\"Cluster Labels:\\", labels) # Evaluate clustering performance ari = adjusted_rand_score(data.target, labels) silhouette = silhouette_score(X_scaled, labels) print(\\"Adjusted Rand Index:\\", ari) print(\\"Silhouette Coefficient:\\", silhouette) # Interpretation of results print(\\"A high adjusted rand index and silhouette coefficient indicate well-separated and meaningful clusters.\\") ``` Implement your solution in a similar manner, selecting a different clustering algorithm, and using respective evaluation metrics.","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.cluster import DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score def cluster_and_evaluate(): # Load dataset data = load_iris() X = data.data # Scale the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Fit the clustering model dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan.fit(X_scaled) # Predict cluster labels labels = dbscan.labels_ print(\\"Cluster Labels:\\", labels) # Evaluate clustering performance ari = adjusted_rand_score(data.target, labels) silhouette = silhouette_score(X_scaled, labels) print(\\"Adjusted Rand Index:\\", ari) print(\\"Silhouette Coefficient:\\", silhouette) # Interpretation of results interpretation = \\"A high adjusted rand index and silhouette coefficient indicate well-separated and meaningful clusters. However, since DBSCAN can identify outliers, the presence of outliers might affect these scores.\\" print(interpretation) return labels, ari, silhouette"},{"question":"Objective: Design and implement a Python function that accurately calculates the total time spent on a task, given the start and stop times of the task in multiple sessions throughout several days. Task: You are required to write a Python function called `calculate_total_time_spent` that meets the following specifications: Function Signature: ```python def calculate_total_time_spent(sessions: list) -> str: ``` Input: - `sessions`: A list of tuples, where each tuple contains two elements: - `start`: A string representing the start time of a session in the format `\\"%Y-%m-%d %H:%M:%S\\"`. - `stop`: A string representing the stop time of a session in the same format. You can assume: - The start time is always earlier than the stop time in each tuple. - The provided list of sessions is non-empty. Output: - Return a string representing the total time spent on the task in the format `\\"%H:%M:%S\\"`, where: - `%H` is the total number of hours. - `%M` is the total number of minutes. - `%S` is the total number of seconds. Constraints: - The time spent can span multiple days. - Leap seconds and daylight-saving changes should be handled correctly based on local time rules. Examples: 1. Given the sessions: ```python [ (\\"2023-10-10 08:00:00\\", \\"2023-10-10 10:15:00\\"), (\\"2023-10-10 14:30:00\\", \\"2023-10-10 18:45:00\\"), (\\"2023-10-11 09:00:00\\", \\"2023-10-11 10:00:00\\") ] ``` The function should return: ```python \\"07:30:00\\" ``` 2. Given the sessions: ```python [ (\\"2023-10-12 21:10:00\\", \\"2023-10-13 01:20:00\\"), (\\"2023-10-13 14:30:00\\", \\"2023-10-13 19:00:00\\") ] ``` The function should return: ```python \\"08:40:00\\" ``` Notes: - Use the `time.strptime` function to parse the input strings into `struct_time` objects. - Use `time.mktime` to convert the `struct_time` objects into seconds since the epoch. - Calculate the total time spent by summing the durations of all sessions. - Format the total time in the required output string format.","solution":"from datetime import datetime, timedelta def calculate_total_time_spent(sessions: list) -> str: total_time = timedelta() for start_str, stop_str in sessions: start_time = datetime.strptime(start_str, \\"%Y-%m-%d %H:%M:%S\\") stop_time = datetime.strptime(stop_str, \\"%Y-%m-%d %H:%M:%S\\") total_time += (stop_time - start_time) total_seconds = int(total_time.total_seconds()) hours, remainder = divmod(total_seconds, 3600) minutes, seconds = divmod(remainder, 60) return f\\"{hours:02}:{minutes:02}:{seconds:02}\\""},{"question":"Objective: You are required to demonstrate your understanding of the `xml.sax` package by implementing a custom SAX parser handler to parse an XML document. Your task includes creating a custom handler, parsing an XML string, and extracting specific information from the XML content. Task: 1. Create a subclass of `xml.sax.handler.ContentHandler` that overrides the necessary methods to handle XML parsing events. 2. Implement logic to extract and store information about XML elements and their attributes. 3. Use the `xml.sax.parseString` function to parse an XML string using your custom handler. 4. Output the extracted information in a specified format. Requirements: - Implement a class `CustomContentHandler` that inherits from `xml.sax.handler.ContentHandler`. - Override the following methods to handle XML parsing events: - `startElement(element_name, attrs)` - Called at the start of an element. - `endElement(element_name)` - Called at the end of an element. - `characters(content)` - Called to process character data inside an element. - Collect information about elements and their attributes in a suitable data structure (dictionary, list, etc.). - Write a function `parse_xml_string(xml_string)` that initializes the SAX parser with `CustomContentHandler` and parses the given XML string. Input: - A string `xml_string` containing the XML content. Output: - A dictionary with the following structure: ```python { \\"elements\\": [ {\\"name\\": element_name, \\"attributes\\": attribute_dict, \\"content\\": element_content}, ... ] } ``` where `element_name` is the name of the XML element, `attribute_dict` is a dictionary of element\'s attributes, and `element_content` is the textual content inside the element. Example: # Input: ```python xml_string = \'\'\' <library> <book id=\\"1\\" genre=\\"fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <year>1925</year> </book> <book id=\\"2\\" genre=\\"non-fiction\\"> <title>Sapiens: A Brief History of Humankind</title> <author>Yuval Noah Harari</author> <year>2011</year> </book> </library> \'\'\' print(parse_xml_string(xml_string)) ``` # Output: ```python { \\"elements\\": [ {\\"name\\": \\"library\\", \\"attributes\\": {}, \\"content\\": \\"\\"}, {\\"name\\": \\"book\\", \\"attributes\\": {\\"id\\": \\"1\\", \\"genre\\": \\"fiction\\"}, \\"content\\": \\"\\"}, {\\"name\\": \\"title\\", \\"attributes\\": {}, \\"content\\": \\"The Great Gatsby\\"}, {\\"name\\": \\"author\\", \\"attributes\\": {}, \\"content\\": \\"F. Scott Fitzgerald\\"}, {\\"name\\": \\"year\\", \\"attributes\\": {}, \\"content\\": \\"1925\\"}, {\\"name\\": \\"book\\", \\"attributes\\": {\\"id\\": \\"2\\", \\"genre\\": \\"non-fiction\\"}, \\"content\\": \\"\\"}, {\\"name\\": \\"title\\", \\"attributes\\": {}, \\"content\\": \\"Sapiens: A Brief History of Humankind\\"}, {\\"name\\": \\"author\\", \\"attributes\\": {}, \\"content\\": \\"Yuval Noah Harari\\"}, {\\"name\\": \\"year\\", \\"attributes\\": {}, \\"content\\": \\"2011\\"}, ] } ``` Constraints: - Assume the input XML string is well-formed. - Focus on properly handling the SAX events and extracting the information as described.","solution":"import xml.sax class CustomContentHandler(xml.sax.ContentHandler): def __init__(self): self.elements = [] self.current_data = \\"\\" self.current_element = None def startElement(self, tag, attributes): self.current_element = {\\"name\\": tag, \\"attributes\\": dict(attributes), \\"content\\": \\"\\"} self.elements.append(self.current_element) self.current_data = \\"\\" def endElement(self, tag): if self.current_element: self.current_element[\\"content\\"] = self.current_data.strip() self.current_data = \\"\\" self.current_element = None def characters(self, content): self.current_data += content def parse_xml_string(xml_string): handler = CustomContentHandler() xml.sax.parseString(xml_string, handler) return {\\"elements\\": handler.elements}"},{"question":"**Question: Implement a Custom Sorted List with Efficient Insertion and Lookup** You are required to implement a custom sorted list class `CustomSortedList` that uses the functions from the `bisect` module to maintain the list in sorted order. The class should support efficient insertion and look-up operations. **Class Definition:** ```python class CustomSortedList: def __init__(self, key_function=None): Initializes an empty sorted list with an optional key function. Parameters: - key_function (callable, optional): A function that extracts a comparison key from each element. pass def insert(self, value): Inserts the value into the sorted list while maintaining order. Parameters: - value: The value to be inserted. pass def find_le(self, value): Finds the rightmost value less than or equal to the given value. Parameters: - value: The target value to compare. Returns: - The rightmost value in the sorted list that is less than or equal to the target value. pass def find_ge(self, value): Finds the leftmost value greater than or equal to the given value. Parameters: - value: The target value to compare. Returns: - The leftmost value in the sorted list that is greater than or equal to the target value. pass ``` **Input and Output Formats:** 1. `__init__(self, key_function=None)`: - **Parameters:** - `key_function` (callable, optional): A key function to use for comparisons. - **Returns:** An instance of `CustomSortedList`. 2. `insert(self, value)`: - **Parameters:** - `value`: The value to be inserted in the sorted list. - **Returns:** None. 3. `find_le(self, value)`: - **Parameters:** - `value`: The target value to compare. - **Returns:** The rightmost value in the sorted list that is less than or equal to the given value. 4. `find_ge(self, value)`: - **Parameters:** - `value`: The target value to compare. - **Returns:** The leftmost value in the sorted list that is greater than or equal to the given value. **Constraints:** - All elements in the list should support comparison (either directly or via `key_function`). - You should use the `bisect` module\'s functions to maintain sorted order and to implement the `find_le` and `find_ge` methods. **Example Usage:** ```python # Example usage without a key function sorted_list = CustomSortedList() sorted_list.insert(7) sorted_list.insert(3) sorted_list.insert(5) print(sorted_list.find_le(5)) # Output: 5 print(sorted_list.find_ge(4)) # Output: 5 # Example usage with a key function from operator import itemgetter sorted_list = CustomSortedList(key_function=itemgetter(1)) sorted_list.insert((\'a\', 7)) sorted_list.insert((\'b\', 3)) sorted_list.insert((\'c\', 5)) print(sorted_list.find_le((\'x\', 5))) # Output: (\'c\', 5) print(sorted_list.find_ge((\'x\', 4))) # Output: (\'c\', 5) ``` Implement the `CustomSortedList` class in Python.","solution":"import bisect class CustomSortedList: def __init__(self, key_function=None): self.key_function = key_function self._list = [] def insert(self, value): key = self.key_function(value) if self.key_function else value bisect.insort(self._list, (key, value)) def find_le(self, value): key = self.key_function(value) if self.key_function else value idx = bisect.bisect_right(self._list, (key, value)) - 1 if idx >= 0: return self._list[idx][1] return None def find_ge(self, value): key = self.key_function(value) if self.key_function else value idx = bisect.bisect_left(self._list, (key, value)) if idx < len(self._list): return self._list[idx][1] return None"},{"question":"Data Analysis and Transformation with Pandas You are provided with a CSV file named `sales_data.csv` that contains sales data for a retail company. This file has the following columns: - `Date`: The date of the sales record. - `Store`: The store identifier where the sales were made. - `Product`: The product identifier. - `Sales`: The number of items sold. - `Revenue`: The total revenue for the sales. Your task is to use the pandas library to perform the following operations: 1. **Read the CSV file** into a pandas DataFrame. 2. **Clean the data** by handling any missing values in a reasonable manner. 3. **Add a new column** called `Avg_Selling_Price` that represents the average selling price per item for each sales record. 4. **Group the data** by `Store` and `Product`, and calculate the total sales and total revenue for each group. Rename the resulting columns appropriately. 5. **Filter and sort** the grouped data to show only the top 5 products by total revenue for each store. 6. **Export** the final filtered DataFrame to a new CSV file named `top_products_by_store.csv`. **Input:** - The CSV file `sales_data.csv`. **Output:** - A CSV file named `top_products_by_store.csv` that contains the top 5 products by total revenue for each store. Ensure your code includes appropriate error handling and is well-commented. # Constraints - You must only use the pandas library and any built-in Python libraries. - Handle any missing values by filling them with zeroes. - Assume the CSV file `sales_data.csv` is correctly formatted with the aforementioned columns. # Example If the content of `sales_data.csv` is as follows: ```csv Date,Store,Product,Sales,Revenue 2023-01-01,Store_A,Product_1,10,100 2023-01-01,Store_A,Product_2,5,50 2023-01-02,Store_A,Product_1,20,200 2023-01-02,Store_B,Product_3,15,150 2023-01-02,Store_B,Product_4,10,100 ``` Your code should produce `top_products_by_store.csv` as: ```csv Store,Product,Total_Sales,Total_Revenue Store_A,Product_1,30,300 Store_A,Product_2,5,50 Store_B,Product_3,15,150 Store_B,Product_4,10,100 ``` Use the following template to implement your solution: ```python import pandas as pd # Step 1: Read the CSV file into a DataFrame df = pd.read_csv(\'sales_data.csv\') # Step 2: Clean the data by handling any missing values df.fillna(0, inplace=True) # Step 3: Add a new column for average selling price per item df[\'Avg_Selling_Price\'] = df[\'Revenue\'] / df[\'Sales\'] # Step 4: Group the data by Store and Product, calculating total sales and revenue grouped_df = df.groupby([\'Store\', \'Product\']).agg({\'Sales\': \'sum\', \'Revenue\': \'sum\'}).reset_index() grouped_df.rename(columns={\'Sales\': \'Total_Sales\', \'Revenue\': \'Total_Revenue\'}, inplace=True) # Step 5: Filter and sort the grouped data to show only the top 5 products by total revenue for each store sorted_df = grouped_df.sort_values([\'Store\', \'Total_Revenue\'], ascending=[True, False]) top_products_by_store = sorted_df.groupby(\'Store\').head(5) # Step 6: Export the final DataFrame to a new CSV file top_products_by_store.to_csv(\'top_products_by_store.csv\', index=False) ``` **Note:** Ensure you test your code thoroughly with different `sales_data.csv` file contents to validate its correctness.","solution":"import pandas as pd def process_sales_data(input_csv, output_csv): This function reads sales data from a CSV file, processes it, and writes the top products by store to another CSV file. Parameters: input_csv (str): The path to the input CSV file containing sales data. output_csv (str): The path to the output CSV file where the processed data will be written. # Step 1: Read the CSV file into a DataFrame df = pd.read_csv(input_csv) # Step 2: Clean the data by handling any missing values df.fillna(0, inplace=True) # Step 3: Add a new column for average selling price per item df[\'Avg_Selling_Price\'] = df[\'Revenue\'] / df[\'Sales\'] # Step 4: Group the data by Store and Product, calculating total sales and revenue grouped_df = df.groupby([\'Store\', \'Product\']).agg({\'Sales\': \'sum\', \'Revenue\': \'sum\'}).reset_index() grouped_df.rename(columns={\'Sales\': \'Total_Sales\', \'Revenue\': \'Total_Revenue\'}, inplace=True) # Step 5: Filter and sort the grouped data to show only the top 5 products by total revenue for each store sorted_df = grouped_df.sort_values([\'Store\', \'Total_Revenue\'], ascending=[True, False]) top_products_by_store = sorted_df.groupby(\'Store\').head(5) # Step 6: Export the final DataFrame to a new CSV file top_products_by_store.to_csv(output_csv, index=False) return top_products_by_store"},{"question":"You are provided with a directory of parquet files, each containing time series data for a different year. Your task is to write a function that processes these files to compute some summary statistics while ensuring efficient memory usage. The steps involve reading only necessary data columns, converting columns to efficient data types, and chunking the processing to handle datasets larger than the available memory. **Function Signature** ```python def process_large_time_series(data_directory: str) -> pd.DataFrame: pass ``` **Input** - `data_directory (str)`: The path to the directory containing the parquet files. Each parquet file follows the naming convention `ts-XX.parquet` where `XX` is a two-digit identifier (e.g., ts-00.parquet, ts-01.parquet, etc.). **Output** - Returns a `pandas.DataFrame` summarizing the following statistics across all files: - The average value of the `x` column (as `avg_x`). - The sum of all positive values in the `y` column (as `sum_positive_y`). - The unique value counts of the `name` column (as `name_counts`). **Constraints** - The function should efficiently handle directory data that may not fit entirely in memory. - Convert appropriate columns to more memory-efficient types (e.g., converting the `name` column to a `pandas.Categorical` and downcasting numeric columns). - Use chunking to read and process each file individually, aggregating results across all files. **Example** Assuming the `data_directory` contains the following parquet files with synthetic data: ``` data/ ├── ts-00.parquet ├── ts-01.parquet ... ├── ts-11.parquet ``` Here is a simplified example of what your implementation might look like: ```python def process_large_time_series(data_directory: str) -> pd.DataFrame: import pandas as pd import pathlib files = pathlib.Path(data_directory).glob(\\"ts-*.parquet\\") avg_x_total = sum_positive_y_total = 0 count = 0 name_counts = pd.Series(dtype=int) for file in files: df = pd.read_parquet(file, columns=[\\"name\\", \\"x\\", \\"y\\"]) df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"x\\"] = pd.to_numeric(df[\\"x\\"], downcast=\\"float\\") df[\\"y\\"] = pd.to_numeric(df[\\"y\\"], downcast=\\"float\\") avg_x_total += df[\\"x\\"].mean() sum_positive_y_total += df[df[\\"y\\"] > 0][\\"y\\"].sum() name_counts = name_counts.add(df[\\"name\\"].value_counts(), fill_value=0) count += 1 avg_x = avg_x_total / count result = pd.DataFrame({ \\"avg_x\\": [avg_x], \\"sum_positive_y\\": [sum_positive_y_total], \\"name_counts\\": [name_counts.astype(int)] }) return result ``` **Note**: The provided implementation is a simplified approach as an example. Ensure your solution handles various edge cases and optimizations as per the provided documentation guidelines.","solution":"import pandas as pd import pathlib def process_large_time_series(data_directory: str) -> pd.DataFrame: files = pathlib.Path(data_directory).glob(\\"ts-*.parquet\\") avg_x_total = 0 sum_positive_y_total = 0 name_counts = pd.Series(dtype=int) file_count = 0 for file in files: # Load necessary columns and convert to efficient types df = pd.read_parquet(file, columns=[\\"name\\", \\"x\\", \\"y\\"]) df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"x\\"] = pd.to_numeric(df[\\"x\\"], downcast=\\"float\\") df[\\"y\\"] = pd.to_numeric(df[\\"y\\"], downcast=\\"float\\") # Calculate interim results avg_x_total += df[\\"x\\"].mean() sum_positive_y_total += df[df[\\"y\\"] > 0][\\"y\\"].sum() name_counts = name_counts.add(df[\\"name\\"].value_counts(), fill_value=0) file_count += 1 avg_x = avg_x_total / file_count if file_count > 0 else float(\'nan\') # Combine results into a DataFrame result = pd.DataFrame({ \\"avg_x\\": [avg_x], \\"sum_positive_y\\": [sum_positive_y_total], \\"name_counts\\": [name_counts.astype(int)] }) return result"},{"question":"Objective Demonstrate understanding of the `queue` module in Python by implementing a multi-threaded application that uses different queue types to manage tasks, ensuring correct synchronization and task tracking. Task You are required to write a Python class `QueueManager` that manages three types of queues: FIFO, LIFO, and PriorityQueue. The `QueueManager` should be able to initialize the queues, add tasks, and process tasks using multiple threads. Requirements 1. **Initialization**: - The class should initialize three queues: a FIFO queue, a LIFO queue, and a PriorityQueue each with a maximum size of 10. 2. **Methods**: - `add_task(queue_type, task, priority=None)`: Adds a task to the specified queue. - `queue_type` can be \'fifo\', \'lifo\', or \'priority\'. - `task` is a string representing the task. - `priority` is an integer representing the priority of the task (only used for `PriorityQueue`). - `process_tasks(queue_type)`: Processes all tasks in the specified queue using three worker threads. - Each worker thread should print the task it processes. 3. **Task Processing**: - Threads should process tasks concurrently, ensuring correct synchronization. - Use `task_done()` and `join()` to track task completion. Constraints - Each queue should hold a maximum of 10 tasks. If an attempt is made to add a task to a full queue, the method should handle the `queue.Full` exception and print an appropriate message. - When processing tasks, if the queue is empty, the method should handle the `queue.Empty` exception and print an appropriate message. Example ```python from queue import Queue, LifoQueue, PriorityQueue import threading class QueueManager: def __init__(self): self.fifo_queue = Queue(maxsize=10) self.lifo_queue = LifoQueue(maxsize=10) self.priority_queue = PriorityQueue(maxsize=10) def add_task(self, queue_type, task, priority=None): # Implementation here def process_tasks(self, queue_type): # Implementation here ``` Notes - Ensure proper synchronization when accessing the queues. - Use `threading` module to create and manage worker threads. - Test the class by creating an instance of `QueueManager`, adding tasks, and processing them for each queue type. Submission Submit the complete code for the `QueueManager` class. Ensure that it adheres to the provided requirements and constraints.","solution":"from queue import Queue, LifoQueue, PriorityQueue, Full, Empty import threading class QueueManager: def __init__(self): self.fifo_queue = Queue(maxsize=10) self.lifo_queue = LifoQueue(maxsize=10) self.priority_queue = PriorityQueue(maxsize=10) def add_task(self, queue_type, task, priority=None): if queue_type not in [\'fifo\', \'lifo\', \'priority\']: raise ValueError(\\"Invalid queue type. Must be \'fifo\', \'lifo\', or \'priority\'.\\") try: if queue_type == \'fifo\': self.fifo_queue.put(task, block=False) elif queue_type == \'lifo\': self.lifo_queue.put(task, block=False) elif queue_type == \'priority\': if priority is None: raise ValueError(\\"Priority must be provided for priority queue.\\") self.priority_queue.put((priority, task), block=False) except Full: print(f\\"Cannot add task \'{task}\'. The {queue_type} queue is full.\\") def process_tasks(self, queue_type): if queue_type not in [\'fifo\', \'lifo\', \'priority\']: raise ValueError(\\"Invalid queue type. Must be \'fifo\', \'lifo\', or \'priority\'.\\") if queue_type == \'fifo\': queue = self.fifo_queue elif queue_type == \'lifo\': queue = self.lifo_queue elif queue_type == \'priority\': queue = self.priority_queue def worker(): while True: try: task = queue.get(block=False) if queue_type == \'priority\': task = task[1] # Extract the actual task from (priority, task) tuple print(f\\"Processing task: {task}\\") queue.task_done() except Empty: break threads = [] for _ in range(3): thread = threading.Thread(target=worker) thread.start() threads.append(thread) for thread in threads: thread.join() queue.join()"},{"question":"**Problem Statement:** You are given time-series data of daily temperature readings in a pandas DataFrame. Your task is to write a function using pandas window operations to perform specific statistical analyses on this data. **Function Requirements:** Write a function `compute_temperature_statistics(df: pd.DataFrame) -> pd.DataFrame` that accepts a DataFrame with the following structure: - The DataFrame `df` contains one column named \'temperature\' representing daily temperature readings. Your function should calculate and return a new DataFrame that includes the following columns based on the \'temperature\' column: 1. **Rolling Mean** (7-day window): Rolling mean over a window of 7 days. 2. **Expanding Sum**: Expanding sum of temperatures. 3. **Exponentially-weighted Mean** (halflife=3): Exponentially-weighted mean with halflife parameter set to 3. 4. **Rolling Standard Deviation** (30-day window): Rolling standard deviation over a window of 30 days. The returned DataFrame should maintain the same index as the input DataFrame and should contain the original \'temperature\' column along with the new columns for each statistical measure. **Input:** - `df`: A pandas DataFrame with a single column `temperature` containing float values. **Output:** - A new DataFrame with the same index as input `df` and five columns: `temperature`, `rolling_mean_7`, `expanding_sum`, `ewm_mean_3`, `rolling_std_30`. **Example:** Given the DataFrame `df`: ``` temperature 0 15.0 1 16.1 2 14.3 3 15.8 4 17.0 5 18.2 6 19.0 7 20.5 8 21.3 9 22.1 10 23.0 ``` The new DataFrame should add columns with calculated values as shown: ``` temperature rolling_mean_7 expanding_sum ewm_mean_3 rolling_std_30 0 15.0 NaN 15.0 15.0000 NaN 1 16.1 NaN 31.1 15.5754 NaN 2 14.3 NaN 45.4 15.2649 NaN 3 15.8 NaN 61.2 15.4422 NaN 4 17.0 NaN 78.2 16.0574 NaN 5 18.2 NaN 96.4 17.0155 NaN 6 19.0 16.9143 115.4 17.9704 NaN 7 20.5 17.8414 135.9 19.1623 NaN 8 21.3 18.7300 157.2 20.1209 NaN 9 22.1 19.7000 179.3 21.1047 NaN 10 23.0 20.7286 202.3 22.0633 NaN ``` **Constraints:** - You may assume that the input DataFrame will always have at least one row. - The rolling windows should only include complete windows (e.g., the 7-day rolling mean should only be calculated once there are 7 days of data). **Note:** Use pandas\' built-in functions to solve the problem, and ensure that your function performs efficiently on large datasets.","solution":"import pandas as pd def compute_temperature_statistics(df: pd.DataFrame) -> pd.DataFrame: Compute various statistics on the temperature data in the DataFrame. Args: df (pd.DataFrame): A dataframe containing a column named \'temperature\'. Returns: pd.DataFrame: A new DataFrame with the original \'temperature\' column and the calculated statistics. df[\'rolling_mean_7\'] = df[\'temperature\'].rolling(window=7).mean() df[\'expanding_sum\'] = df[\'temperature\'].expanding().sum() df[\'ewm_mean_3\'] = df[\'temperature\'].ewm(halflife=3).mean() df[\'rolling_std_30\'] = df[\'temperature\'].rolling(window=30).std() return df"},{"question":"# Python Coding Assessment Question Objective Demonstrate your understanding and ability to customize the module import process in Python using the `importlib` package. Task You are required to write a function `lazy_import(module_name: str)` that lazily imports a module in Python. Lazy loading or lazy importing is a design pattern which defers the initialization of a module until the point at which it is needed. # Function Signature ```python def lazy_import(module_name: str) -> None: ... ``` # Input - `module_name` (str): The name of the module to be lazily imported. This will be a string representing a valid module name that can be imported. # Output - This function does not return anything but should print a confirmation message once the module is imported. # Constraints - You cannot use the standard `import` statement. - You must use the `importlib` package for implementing lazy imports. - Ensure your solution is efficient in terms of initialization and subsequent usage of the module. # Example Usage ```python lazy_import(\'math\') # Output should be \'<module \'math\' (built-in)> has been imported lazily.\' when the module is used for the first time import math # This should work seamlessly after lazy_import print(math.sqrt(4)) # Output: 2.0 ``` # Hints - Look into `importlib.import_module` and `importlib.util.find_spec`. - Use a way to check if the module has already been imported. Evaluation Criteria - **Correctness**: The function should correctly lazy import the module and make it available for subsequent use. - **Efficiency**: The function should not perform unnecessary actions and should delay the import until the module is actually needed. - **Code Quality**: The code should be well-organized, commented, and follow Python best practices.","solution":"import importlib def lazy_import(module_name: str) -> None: Lazily import a module in Python using importlib. Print a confirmation message once the module is imported. if module_name in globals(): return else: module = importlib.import_module(module_name) globals()[module_name] = module print(f\\"<module \'{module_name}\' (built-in)> has been imported lazily.\\") # Example Usage: # lazy_import(\'math\') # import math # print(math.sqrt(4))"},{"question":"You have been given a task to process a data file containing information in JSON format. This file may contain nested structures and various data types that need to be processed and analyzed. Your task is to implement a function that reads this JSON data from a file, processes it by extracting certain pieces of information, and writes a summary of this data back into another JSON file. Function Signature ```python def process_json_data(input_filepath: str, output_filepath: str) -> None: pass ``` Input and Output - **Input:** - `input_filepath`: A string representing the path to the input JSON file. - `output_filepath`: A string representing the path to where the output JSON file should be saved. - **Output:** - The function does not return any value, but it should write the processed data summary to the output file in JSON format. Requirements 1. **Reading and Decoding JSON:** - Read the JSON data from the input file. Ensure you handle any potential exceptions that might occur during this process. 2. **Data Processing:** - From the read JSON data, extract the following information: - Total number of keys at the top level. - Extract all values corresponding to keys that are strings. - Count of keys where the values are lists. - Count of keys where the values are other JSON objects (nested structures). 3. **Writing to JSON:** - Write the extracted summary data to the output file in JSON format. Constraints - The input JSON file can have a maximum size of 10 MB. - The top level of the JSON structure will contain no more than 100 keys. - Nested structures can go up to a depth of 5 levels. Performance Requirements - Ensure the function executes efficiently with respect to the provided constraints. - Handle exceptions gracefully to avoid any runtime errors. Example Consider the following JSON input file `input.json`: ```json { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"address\\": { \\"city\\": \\"New York\\", \\"zip\\": \\"10001\\" }, \\"emails\\": [\\"john@example.com\\", \\"doe@example.com\\"], \\"phone_numbers\\": [\\"123-456-7890\\"], \\"is_active\\": true } ``` The function should process this data and produce an output file `output.json` with the following content: ```json { \\"total_keys\\": 6, \\"string_values\\": [\\"John Doe\\", \\"New York\\", \\"10001\\"], \\"list_count\\": 2, \\"object_count\\": 1 } ``` Notes: - Ensure the function is robust and handles various edge cases. - Use appropriate naming conventions and comments for clarity. - Avoid using external libraries except for the standard Python 3.10 library. You may assume the input file exists at the given path, and the directory for the output file is writable.","solution":"import json def process_json_data(input_filepath: str, output_filepath: str) -> None: Processes the JSON data from the input file, extracts summary information, and writes it to the output file in JSON format. Parameters: - input_filepath: str, filepath to the input JSON file - output_filepath: str, filepath to the output JSON file try: with open(input_filepath, \'r\') as infile: data = json.load(infile) except Exception as e: print(f\\"Error reading input file: {e}\\") return total_keys = len(data) string_values = [] list_count = 0 object_count = 0 def process_dict(d): nonlocal list_count, object_count for k, v in d.items(): if isinstance(v, str): string_values.append(v) elif isinstance(v, list): list_count += 1 elif isinstance(v, dict): object_count += 1 process_dict(v) # Recursively process nested dictionaries process_dict(data) summary = { \\"total_keys\\": total_keys, \\"string_values\\": string_values, \\"list_count\\": list_count, \\"object_count\\": object_count } try: with open(output_filepath, \'w\') as outfile: json.dump(summary, outfile) except Exception as e: print(f\\"Error writing output file: {e}\\")"},{"question":"**Title:** Custom Preprocessing and Pipeline Integration with scikit-learn **Objective:** Write a program that performs a series of preprocessing tasks on a provided dataset using scikit-learn. The objective is to assess your understanding of the preprocessing module and your ability to implement custom transformations and integrate them into a scikit-learn pipeline. **Task:** Given a small dataset `X` with numerical features and categorical features, and a target variable `y`, perform the following preprocessing steps and validate the preprocessing steps by evaluating a classifier\'s performance: 1. Standardize the numerical features. 2. One-hot encode the categorical features. 3. Handle missing values in numerical features by imputing them with the mean. 4. Normalize the data using L2 norm. 5. Encode the target variable using ordinal encoding. 6. Integrate all preprocessing steps into a single scikit-learn pipeline. 7. Train a `LogisticRegression` classifier using the pipeline and evaluate its accuracy on test data. **Instructions:** 1. **Input:** - `X_train`: Training data features (list of lists), each sublist can have numerical and categorical features. - `X_test`: Test data features (list of lists), each sublist can have numerical and categorical features. - `y_train`: Target variable for training data (list/array). - `y_test`: Target variable for test data (list/array). 2. **Output:** - The accuracy score of the `LogisticRegression` classifier on the test data. 3. **Constraints:** - At least one feature in `X` may contain missing values (`nan`). - There are both continuous and categorical features in the dataset. 4. **Performance Requirements:** - The entire sequence of preprocessing steps should be integrated into a single pipeline. - Ensure that the transformations are fitted on the training data and applied separately on the test data to avoid data leakage. **Example:** ```python from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer, OrdinalEncoder from sklearn.metrics import accuracy_score # Example input data X_train = [ [1.0, \\"cat\\", 20.0], [2.0, \\"dog\\", None], [3.0, \\"cat\\", 25.0], [None, \\"dog\\", 30.0] ] X_test = [ [1.5, \\"dog\\", 22.0], [2.5, \\"cat\\", 28.0], ] y_train = [\\"A\\", \\"B\\", \\"A\\", \\"B\\"] y_test = [\\"B\\", \\"A\\"] def create_pipeline_and_evaluate(X_train, X_test, y_train, y_test): # Create the preprocessing steps numerical_features = [0, 2] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_features = [1] categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'normalizer\', Normalizer(norm=\'l2\')), (\'classifier\', LogisticRegression()) ]) # Encode target using ordinal encoding target_encoder = OrdinalEncoder() y_train_encoded = target_encoder.fit_transform([[y] for y in y_train]).ravel() y_test_encoded = target_encoder.transform([[y] for y in y_test]).ravel() # Fit the pipeline on training data pipeline.fit(X_train, y_train_encoded) # Predict on the test data y_pred = pipeline.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test_encoded, y_pred) return accuracy # Call the function and print accuracy accuracy = create_pipeline_and_evaluate(X_train, X_test, y_train, y_test) print(f\\"Accuracy: {accuracy:.2f}\\") ``` Provide the implementation and verify the accuracy of the classifier on the test dataset.","solution":"from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer, OrdinalEncoder from sklearn.metrics import accuracy_score import numpy as np def create_pipeline_and_evaluate(X_train, X_test, y_train, y_test): numerical_features = [0, 2] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_features = [1] categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'normalizer\', Normalizer(norm=\'l2\')), (\'classifier\', LogisticRegression()) ]) target_encoder = OrdinalEncoder() y_train_encoded = target_encoder.fit_transform(np.array(y_train).reshape(-1, 1)).ravel() y_test_encoded = target_encoder.transform(np.array(y_test).reshape(-1, 1)).ravel() pipeline.fit(X_train, y_train_encoded) y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test_encoded, y_pred) return accuracy"},{"question":"# Python Coding Assessment Question Objective Demonstrate your understanding of Python 3.10 generator objects and iterator protocol by implementing a custom generator that mimics an advanced iteration pattern. Problem Statement You are required to implement a custom generator function named `custom_range` that emulates the behavior of Python’s built-in `range()` function, but with additional features: 1. The function should accept two parameters specifying the start and end of the range. 2. It should yield only even numbers within the specified range. 3. When a `ValueError` is thrown as a result of invalid parameters (e.g., if the start is greater than the end), handle this exception within the generator and yield an error message `\\"Invalid range\\"` as the only value. Your function should be able to handle large ranges efficiently without running into memory issues, leveraging Python\'s lazy evaluation provided by generators. Constraints: - The `start` and `end` values will be integers. - You should make use of Python generators (`yield`) and not store the entire range in memory. - The `custom_range` function should be compatible with Python 3.10. Input - Two integers, `start` and `end`. Output - An iterable generator object that yields even numbers within the provided range or an error message `\\"Invalid range\\"`. Function Signature ```python def custom_range(start: int, end: int) -> iter: ``` Example: ```python # Example 1 result = custom_range(4, 10) print(list(result)) # Output: [4, 6, 8, 10] # Example 2 result = custom_range(7, 3) print(list(result)) # Output: [\\"Invalid range\\"] ``` Performance Requirement - Your solution should handle large ranges efficiently (e.g., `custom_range(4, 1000000)` should not consume a large amount of memory). Notes 1. Ensure the generator stops iteration correctly. 2. No other built-in functions like `filter` or `range` should be used for controlling the output within the generator. The logic should be implemented within the generator using iterative control structures (`for`, `while`, etc.). Implement your solution in the code cell provided below. Good luck!","solution":"def custom_range(start: int, end: int) -> iter: Custom generator function that yields even numbers within the range specified by start and end. If start is greater than end, yields \\"Invalid range\\". if start > end: yield \\"Invalid range\\" return current = start if start % 2 == 0 else start + 1 while current <= end: yield current current += 2"},{"question":"# Advanced Coding Challenge: Custom Python Generator Function # Objective: You are required to demonstrate your understanding of Python\'s generator functions by implementing a custom generator that follows a specific algorithm. Your implementation should yield values based on its internal state, showing your ability to control the generator\'s behavior and efficiently produce results. # Problem Statement: Write a generator function `custom_fibonacci(max_value)` that yields Fibonacci numbers up to a maximum specified value (`max_value`). The Fibonacci sequence is defined as follows: - F(0) = 0 - F(1) = 1 - For n >= 2: F(n) = F(n-1) + F(n-2) The generator should start from F(0) and continue yielding Fibonacci numbers until `max_value` is reached or exceeded. # Input: - `max_value` (int): A positive integer indicating the maximum value for Fibonacci numbers to be generated. Must be greater than or equal to 1. # Output: - The generator should yield a sequence of Fibonacci numbers one by one. The sequence terminates when the next Fibonacci number would be greater than `max_value`. # Constraints: - Efficiency matters: Your solution should not compute Fibonacci numbers indefinitely once the `max_value` is exceeded. - The function should handle large values of `max_value` efficiently. # Example: ```python def custom_fibonacci(max_value): # Your implementation here # Usage example: for number in custom_fibonacci(100): print(number) ``` Output: ``` 0 1 1 2 3 5 8 13 21 34 55 89 ``` # Implementation Details: - You need to define a generator function `custom_fibonacci` that uses the `yield` statement to produce Fibonacci numbers. - The function should leverage a loop to calculate the next Fibonacci number based on the last two numbers in the sequence. - The generator should stop once it has yielded the largest Fibonacci number not exceeding `max_value`. # Notes: - Ensure your function adheres to the provided constraints and efficiently handles large inputs. - No other libraries or modules need to be imported for this task. Good luck!","solution":"def custom_fibonacci(max_value): A generator function that yields Fibonacci numbers up to a maximum specified value. Parameters: max_value (int): A positive integer indicating the maximum value for Fibonacci numbers to be generated. Yields: int: Fibonacci numbers up to max_value. a, b = 0, 1 yield a while b <= max_value: yield b a, b = b, a + b"},{"question":"**Question: Implement a Python function that updates and uses environment variables through the `os` module.** # Task: Write a function `manage_environment_vars` that performs the following steps: 1. Reads and prints the current value of an environment variable whose key is provided as an argument. 2. Sets (or updates) the value of this environment variable to a new value, where the new value is also provided as an argument. 3. Confirms the update by reading and printing the value of the environment variable again. 4. Deletes the environment variable and confirms its deletion by attempting to read the value again. # Requirements: - Use the `os` module for managing environment variables. - Handle potential exceptions using try-except blocks and print appropriate error messages if the environment variable does not exist or any other error occurs. # Input: - `key` (str): The key of the environment variable to read, update, and delete. - `new_value` (str): The new value to set for the specified environment variable. # Output: - Print the current value, updated value, and confirmation of deletion of the environment variable. # Constraints: - Assume the function will be run on a Unix-like system where the `os` module implementations are consistent with `posix`. # Example: ```python def manage_environment_vars(key, new_value): import os # Step 1: Read and print the current value of the environment variable try: current_value = os.environ[key] print(f\\"Current value of \'{key}\': {current_value}\\") except KeyError: print(f\\"Environment variable \'{key}\' does not exist.\\") # Step 2: Update the environment variable with the new value os.environ[key] = new_value print(f\\"Updated value of \'{key}\': {os.environ[key]}\\") # Step 3: Confirm the update updated_value = os.environ[key] print(f\\"Confirmed updated value of \'{key}\': {updated_value}\\") # Step 4: Delete the environment variable and confirm deletion del os.environ[key] try: deleted_value = os.environ[key] print(f\\"Value after deletion: {deleted_value}\\") except KeyError: print(f\\"Environment variable \'{key}\' has been deleted.\\") ``` # Note: - Ensure the function `manage_environment_vars` handles both existing and non-existing environment variables gracefully.","solution":"import os def manage_environment_vars(key: str, new_value: str): Manages an environment variable: read, update, and delete the environment variable. Args: key (str): The key of the environment variable to manage. new_value (str): The new value to set for the environment variable. # Step 1: Read and print the current value of the environment variable try: current_value = os.environ[key] print(f\\"Current value of \'{key}\': {current_value}\\") except KeyError: print(f\\"Environment variable \'{key}\' does not exist.\\") # Step 2: Update the environment variable with the new value os.environ[key] = new_value print(f\\"Updated value of \'{key}\': {os.environ[key]}\\") # Step 3: Confirm the update updated_value = os.environ[key] print(f\\"Confirmed updated value of \'{key}\': {updated_value}\\") # Step 4: Delete the environment variable and confirm deletion del os.environ[key] try: deleted_value = os.environ[key] print(f\\"Value after deletion: {deleted_value}\\") except KeyError: print(f\\"Environment variable \'{key}\' has been deleted.\\")"},{"question":"# Task Implement a Python function `handle_custom_exception()` that does the following: 1. **Attempts to read from a file** - Given a file path, attempts to open and read from the file. 2. **Handles specific errors using custom exceptions** - If the file does not exist, a custom exception `FileNotFound` should be raised. If there are permission errors, a custom exception `PermissionDenied` should be raised. If there are any other errors, a general exception `OperationFailed` should be raised. 3. **Resets the error indicator** if any error occurs. You need to define the custom exceptions and ensure that correct error messages are set for these exceptions using the appropriate functions from the documentation provided. # Requirements - Define custom exceptions `FileNotFound`, `PermissionDenied`, and `OperationFailed`. - Implement the `handle_custom_exception()` function. - Use `PyErr_SetString()` for raising specific errors in your implementation. # Function Signature ```python def handle_custom_exception(file_path: str) -> str: Attempts to read the content of a given file. Parameters: - file_path (str): The path to the file to read from. Returns: - str: The contents of the file if operation is successful. Raises: - FileNotFound: If the file does not exist. - PermissionDenied: If there are permission issues accessing the file. - OperationFailed: For any other reading errors. pass ``` # Example Usage ```python try: content = handle_custom_exception(\\"some_non_existent_file.txt\\") except FileNotFound as e: print(\\"Caught exception:\\", e) except PermissionDenied as e: print(\\"Caught exception:\\", e) except OperationFailed as e: print(\\"Caught exception:\\", e) ``` # Constraints - You must use the information provided in the documentation. - Ensure your function reads the content from the file if no errors occur. - Errors should be handled gracefully and appropriate error messages should be displayed. # Note This task assumes you have access to the C API of Python and understand how to interface with it from Python code.","solution":"class FileNotFound(Exception): pass class PermissionDenied(Exception): pass class OperationFailed(Exception): pass def handle_custom_exception(file_path: str) -> str: try: with open(file_path, \\"r\\") as file: return file.read() except FileNotFoundError: raise FileNotFound(\\"The file was not found\\") except PermissionError: raise PermissionDenied(\\"Permission denied while accessing the file\\") except Exception as e: raise OperationFailed(f\\"An error occurred: {e}\\")"},{"question":"**Objective:** Implement a custom PyTorch operator and extend it with additional functionality, demonstrating the use of `torch.library` API. # Problem Statement You are required to create a custom PyTorch operator to perform a specific mathematical operation and extend it with gradient support for training purposes. Specifically, you will: 1. Implement a custom matrix multiplication operator. 2. Extend this operator to support autograd for backpropagation. 3. Test the operator to ensure it performs as expected and supports gradient calculation correctly. # Requirements 1. **Custom Matrix Multiplication Operator:** - Create a new custom operator using `torch.library.custom_op`. - Implement basic matrix multiplication functionality. 2. **Extend Operator for Autograd:** - Use `torch.library.register_autograd` to ensure the operator functions correctly during training with gradient support. 3. **Testing the Operator:** - Use `torch.library.opcheck` to verify the correctness of the operator. - Use `torch.autograd.gradcheck` to check the correctness of the gradients. # Input/Output Formats - **Input:** - Two 2-D tensors `A` and `B` of compatible sizes for matrix multiplication (i.e., the number of columns in `A` equals the number of rows in `B`). - **Output:** - A resulting 2-D tensor after performing matrix multiplication of `A` and `B`. # Constraints - The function should handle input tensor sizes efficiently. - Ensure the gradient calculations are mathematically correct. - Follow good coding practices and provide necessary comments and documentation. # Performance Requirements - The operator should be efficient in terms of time complexity, i.e., O(n^3) for matrix multiplication should be ensured. - Minimize memory overhead where possible. # Function Signature You may define the main function as a combination of implementing the operator, registering autograd, and testing it. ```python import torch def matrix_multiplication_custom_op(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: # Define the custom operator, register autograd, and test the implementation pass # Test your implementation with example cases if __name__ == \\"__main__\\": A = torch.randn(2, 3, requires_grad=True) B = torch.randn(3, 4, requires_grad=True) result = matrix_multiplication_custom_op(A, B) print(\\"Result of custom matrix multiplication:\\", result) # More rigorous testing and gradcheck try: torch.autograd.gradcheck(matrix_multiplication_custom_op, (A, B)) print(\\"Gradient check passed.\\") except RuntimeError as e: print(\\"Gradient check failed:\\", e) ``` # Notes - Refer to the PyTorch Custom Operators Landing Page and the Google Colab tutorial for additional examples and guidance on using `torch.library`. - You are encouraged to explore other functionalities of `torch.library` for more robust implementations and testing procedures.","solution":"import torch from torch.autograd import Function class MatrixMultiplication(Function): @staticmethod def forward(ctx, A, B): ctx.save_for_backward(A, B) return torch.mm(A, B) @staticmethod def backward(ctx, grad_output): A, B = ctx.saved_tensors grad_A = grad_output.mm(B.t()) grad_B = A.t().mm(grad_output) return grad_A, grad_B def matrix_multiplication_custom_op(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Performs custom matrix multiplication of two tensors A and B. return MatrixMultiplication.apply(A, B) # Test your implementation with example cases if __name__ == \\"__main__\\": A = torch.randn(2, 3, requires_grad=True) B = torch.randn(3, 4, requires_grad=True) result = matrix_multiplication_custom_op(A, B) print(\\"Result of custom matrix multiplication:\\", result) # More rigorous testing and gradcheck try: torch.autograd.gradcheck(matrix_multiplication_custom_op, (A, B)) print(\\"Gradient check passed.\\") except RuntimeError as e: print(\\"Gradient check failed:\\", e)"},{"question":"# Email Parsing and Analysis **Objective:** You are tasked with writing a Python function that accepts a raw email string, parses it to extract specific information, and returns the information in a structured format. **Function Signature:** ```python def parse_email(raw_email: str) -> dict: pass ``` **Input:** - `raw_email` (str): A string representing a raw email message, including headers and body content. **Output:** - A dictionary with the following structure: ```python { \\"subject\\": str, # The subject of the email \\"from\\": str, # The sender\'s email address \\"to\\": list, # A list of recipient email addresses \\"date\\": str, # The date the email was sent \\"body\\": str # The plain text body of the email } ``` **Constraints:** - You can assume the email will always have a `Subject`, `From`, `To`, and `Date` header. - The body of the email will be plain text (not HTML or any other format). **Example Input:** ```python raw_email = From: sender@example.com To: recipient@example.com Subject: Test email Date: Wed, 1 Sep 2021 10:00:00 -0000 This is the body of the email. ``` **Example Output:** ```python { \\"subject\\": \\"Test email\\", \\"from\\": \\"sender@example.com\\", \\"to\\": [\\"recipient@example.com\\"], \\"date\\": \\"Wed, 1 Sep 2021 10:00:00 -0000\\", \\"body\\": \\"This is the body of the email.\\" } ``` **Notes:** To complete this task, you may use the following classes and methods from the `email` module: - `email.message.EmailMessage` - `email.parser.Parser` - `email.policy.default` Refer to the official Python documentation for the `email` module if you need further details on these classes and methods. This question assesses the student\'s ability to: 1. Parse a raw email string using the `email.parser.Parser` class. 2. Extract and handle email headers and body content. 3. Construct and return a dictionary with the required information. Good luck!","solution":"from email.parser import Parser def parse_email(raw_email: str) -> dict: # Initialize Parser object parser = Parser() # Parse the raw email email = parser.parsestr(raw_email) # Extract necessary parts from the email subject = email[\'Subject\'] from_address = email[\'From\'] to_addresses = email[\'To\'].split(\', \') date = email[\'Date\'] body = email.get_payload() # Construct the output dictionary parsed_email = { \\"subject\\": subject, \\"from\\": from_address, \\"to\\": to_addresses, \\"date\\": date, \\"body\\": body.strip() # remove leading and trailing whitespaces from body } return parsed_email"},{"question":"# Email Serialization with Custom Headers You are tasked with creating a utility that serializes email messages using the `email.generator` module. Your solution should demonstrate the ability to handle different types of email parts (MIME types), apply custom policies, and manage headers appropriately. Task 1. **Write a function named `serialize_email`** that takes the following parameters: - `email_message`: An instance of `EmailMessage` representing the email to be serialized. - `output_format`: A string that can either be \\"bytes\\" or \\"text\\", indicating the desired format of the serialized output. - `mangle_from`: A boolean indicating whether to prepend a \\">\\" character to lines starting with \\"From \\" in the email body. - `max_header_length`: An integer specifying the maximum length for headers. If `None`, headers should follow the default policy. 2. **Function Signature**: ```python def serialize_email(email_message, output_format, mangle_from=True, max_header_length=None): pass ``` 3. **Function Requirements**: - The function should serialize the `email_message` based on the `output_format` (\\"bytes\\" or \\"text\\"). - Use the `mangle_from` parameter to decide on handling lines in the body that start with \\"From \\". - Apply the `max_header_length` for refolding headers. - Return the serialized email as a bytes object if `output_format` is \\"bytes\\", and as a string if `output_format` is \\"text\\". 4. **Performance Constraints**: - The function should handle large email messages efficiently. - Consider using appropriate policies for optimal performance and standards compliance. Example Usage ```python from email.message import EmailMessage # Creating an example email message msg = EmailMessage() msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg.set_content(\'This is a test email body.\') # Serializing to bytes serialized_bytes = serialize_email(msg, \\"bytes\\") print(serialized_bytes) # Serializing to text serialized_text = serialize_email(msg, \\"text\\", mangle_from=False, max_header_length=78) print(serialized_text) ``` Ensure your implementation leverages the `email.generator` module and its relevant classes effectively.","solution":"import email from email.policy import default from email.generator import BytesGenerator, Generator from io import BytesIO, StringIO def serialize_email(email_message, output_format, mangle_from=True, max_header_length=None): Serialize an email message with specific formatting options. :param email_message: An instance of EmailMessage representing the email to be serialized. :param output_format: A string that can either be \\"bytes\\" or \\"text\\", indicating the desired format of the serialized output. :param mangle_from: A boolean indicating whether to prepend a \\">\\" character to lines starting with \\"From \\" in the email body. :param max_header_length: An integer specifying the maximum length for headers. If None, headers should follow the default policy. :return: The serialized email as bytes if output_format is \\"bytes\\", and as string if output_format is \\"text\\". # Set the policy for the message policy = default.clone(max_line_length=max_header_length) # Prepare the appropriate generator based on the output format if output_format == \\"bytes\\": output = BytesIO() generator = BytesGenerator(output, policy=policy, mangle_from_=mangle_from) elif output_format == \\"text\\": output = StringIO() generator = Generator(output, policy=policy, mangle_from_=mangle_from) else: raise ValueError(\\"Invalid output_format. Expected \'bytes\' or \'text\'.\\") # Serialize the message generator.flatten(email_message) # Get the serialized value from the generator serialized_output = output.getvalue() output.close() return serialized_output"},{"question":"# Python List Manipulation in C Problem Statement You are tasked with writing a Python C extension module that provides a function to process a list. The function should: 1. **Accept a list of integers.** 2. **Remove duplicates from the list.** 3. **Sort the list in descending order.** 4. **Return the processed list as a new list.** You must utilize the provided `PyListObject` functionalities to achieve this. Implement the following function in C: ```c PyObject* process_list(PyObject* self, PyObject* args) { // Your code here } ``` Input - A single list of integers (e.g., `lst = [4, 2, 5, 4, 1, 2]`). Output - A new list of integers, sorted in descending order with duplicates removed (e.g., `[5, 4, 2, 1]`). Constraints - You should handle list inputs of up to 10,000 elements. - You must use the PyListObject API as detailed in the provided documentation. - Ensure that your implementation is efficient and handles errors appropriately. Example ```python >>> import your_module >>> your_module.process_list([4, 2, 5, 4, 1, 2]) [5, 4, 2, 1] ``` Notes - The function should raise a `TypeError` if the input is not a list of integers. - Use the `PyListObject` functions to manipulate the list - do not use higher-level Python list methods. Additional Information Include the following in your C extension module setup: ```c static PyMethodDef YourMethods[] = { {\\"process_list\\", process_list, METH_VARARGS, \\"Process the list as required.\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef yourmodule = { PyModuleDef_HEAD_INIT, \\"your_module\\", NULL, -1, YourMethods }; PyMODINIT_FUNC PyInit_your_module(void) { return PyModule_Create(&yourmodule); } ```","solution":"from typing import List def process_list(lst: List[int]) -> List[int]: Accepts a list of integers, removes duplicates, sorts the list in descending order, and returns the processed list. if not all(isinstance(x, int) for x in lst): raise TypeError(\\"All elements of the input list must be integers.\\") # Remove duplicates unique_lst = list(set(lst)) # Sort in descending order unique_lst.sort(reverse=True) return unique_lst"},{"question":"# Custom Type Implementation in Python In this exercise, you will be tasked with defining and implementing a custom type in Python. We will focus on implementing a custom type that behaves like a list but with additional constraints and methods. Objective Implement a custom Python type called `LimitedList` which has the following features: - The list has a fixed maximum capacity specified at the time of its creation. - It supports all standard list operations (e.g., index access, slicing, concatenation, etc.). - It raises an error if you attempt to add more elements than the maximum capacity. - It includes a method to get the current number of elements. - It includes a method to check if the list is full (i.e., if the number of elements is equal to the capacity). Requirements 1. **Initialization**: The `LimitedList` should be able to be initialized with a maximum capacity. 2. **Index Access**: Implement index access (including negative indexing) and slicing. 3. **Appending and Extending**: Overwrite the `append` and `extend` methods to enforce the capacity restriction. 4. **Inheriting List Methods**: Make sure other list methods (`__len__`, `__getitem__`, `__setitem__`, `__delitem__`, etc.) work as expected. 5. **Custom Methods**: Implement `is_full` and `current_size` methods. Example Usage ```python ll = LimitedList(5) ll.append(1) ll.append(2) ll.extend([3, 4]) print(ll.is_full()) # False ll.append(5) print(ll.is_full()) # True print(ll.current_size()) # 5 print(ll[0]) # 1 print(ll[-1]) # 5 ll[1] = 10 print(ll) # LimitedList with elements [1, 10, 3, 4, 5] ll.append(6) # Should raise an error ``` Constraints and Limitations - Do not use Python\'s inbuilt list directly for any data storage. - Ensure the `LimitedList` type is performant for common list operations. Implementation Guidance 1. **Implement the Constructor**: ```python class LimitedList: def __init__(self, capacity): self.capacity = capacity self.data = [] # Continue with the implementation ``` 2. **Override Methods for Capacity Check**: - Implement methods to override `append` and `extend` functionalities to include capacity checks. 3. **Implement Custom Methods**: - Define `is_full` and `current_size` methods to check the list\'s state. ```python class LimitedList: def __init__(self, capacity): self.capacity = capacity self.data = [] def append(self, value): if len(self.data) >= self.capacity: raise ValueError(\\"Cannot exceed capacity\\") self.data.append(value) def extend(self, values): if len(self.data) + len(values) > self.capacity: raise ValueError(\\"Cannot exceed capacity\\") self.data.extend(values) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def is_full(self): return len(self.data) == self.capacity def current_size(self): return len(self.data) def __repr__(self): return f\\"LimitedList({self.data})\\" ``` Complete the class definition ensuring all list operations are supported to match the functionalities described in the requirements. Submission Submit your implementation of the `LimitedList` class. Ensure your class passes the example usage tests.","solution":"class LimitedList: def __init__(self, capacity): self.capacity = capacity self.data = [] def append(self, value): if len(self.data) >= self.capacity: raise ValueError(\\"Cannot exceed capacity\\") self.data.append(value) def extend(self, values): if len(self.data) + len(values) > self.capacity: raise ValueError(\\"Cannot exceed capacity\\") self.data.extend(values) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def is_full(self): return len(self.data) == self.capacity def current_size(self): return len(self.data) def __repr__(self): return f\\"LimitedList({self.data})\\" def __iter__(self): return iter(self.data) def __contains__(self, item): return item in self.data def __add__(self, other): if isinstance(other, list) or isinstance(other, LimitedList): combined_length = len(self.data) + len(other) if combined_length > self.capacity: raise ValueError(\\"Cannot exceed capacity\\") else: raise TypeError(f\\"Unsupported operand type(s) for +: \'LimitedList\' and \'{type(other)}\'\\") return LimitedList(self.capacity)._from_existing_list(self.data + list(other)) def _from_existing_list(self, existing_data): obj = LimitedList(self.capacity) obj.data = existing_data return obj"},{"question":"In this assessment, you will use scikit-learn\'s feature extraction capabilities to analyze a dataset of text documents. You are required to perform text vectorization, feature transformation, and build an initial machine learning pipeline. # Problem Statement You are given a list of text documents and your task is to develop functionality that: 1. Tokenizes the documents and converts them into numerical feature vectors using the Bag of Words representation. 2. Transforms these vectors using the tf-idf weighting scheme. 3. Implements a basic text classification pipeline that can be used to fit a simple classifier on the transformed features. # Input Format - A list of strings, `documents`, where each string represents a text document. # Output Format - A sparse matrix representation of the transformed documents as tf-idf vectors. - A function to fit a classifier on the tf-idf vectors and generate predictions. # Function Definitions `vectorize_documents(documents: List[str]) -> scipy.sparse.csr.csr_matrix` This function takes a list of text documents and converts them into a tf-idf weighted sparse matrix. - **Input**: `documents` - A list of strings where each string represents a text document. - **Output**: A sparse matrix where each row corresponds to a document and each column corresponds to a unique term from the corpus. `train_classifier(X: scipy.sparse.csr.csr_matrix, y: List[int]) -> object` This function takes the tf-idf weighted sparse matrix and a list of labels to train a simple classifier. - **Input**: - `X` - A sparse matrix of tf-idf features. - `y` - A list of integers representing the labels for each document. - **Output**: A trained classifier model. `predict_labels(model: object, X: scipy.sparse.csr.csr_matrix) -> List[int]` This function takes a trained classifier and a sparse matrix, and returns predicted labels for the documents. - **Input**: - `model` - A trained classifier model. - `X` - A sparse matrix of tf-idf features. - **Output**: A list of predicted labels for each document. # Constraints - You are required to use `CountVectorizer` and `TfidfTransformer` for text vectorization and transformation. - You may use `LogisticRegression` from `sklearn.linear_model` as the classifier. - You are free to add any necessary helper functions. # Example Usage ```python from sklearn.linear_model import LogisticRegression from scipy.sparse import csr_matrix from typing import List import numpy as np documents = [ \'This is the first document.\', \'This is the second second document.\', \'And the third one.\', \'Is this the first document?\' ] labels = [0, 1, 0, 1] # Step 1: Convert documents into tf-idf vectors X = vectorize_documents(documents) # Step 2: Train the classifier model = train_classifier(X, labels) # Step 3: Predict labels predicted_labels = predict_labels(model, X) print(predicted_labels) # Output: List of predicted labels ``` # Note - Ensure your code is well-documented and follows best practices. - Handle any necessary preprocessing steps such as lowercase conversion or punctuation removal within the vectorization process. - You may use additional scikit-learn utilities as needed to complete the task.","solution":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.linear_model import LogisticRegression from scipy.sparse import csr_matrix from typing import List def vectorize_documents(documents: List[str]) -> csr_matrix: Convert a list of text documents into a tf-idf weighted sparse matrix. Args: documents - A list of strings where each string represents a text document. Returns: A sparse matrix where each row corresponds to a document and each column corresponds to a unique term from the corpus. count_vectorizer = CountVectorizer() count_matrix = count_vectorizer.fit_transform(documents) tfidf_transformer = TfidfTransformer() tfidf_matrix = tfidf_transformer.fit_transform(count_matrix) return tfidf_matrix def train_classifier(X: csr_matrix, y: List[int]) -> LogisticRegression: Train a simple classifier on the tf-idf vectors. Args: X - A sparse matrix of tf-idf features. y - A list of integers representing the labels for each document. Returns: A trained classifier model. classifier = LogisticRegression() classifier.fit(X, y) return classifier def predict_labels(model: LogisticRegression, X: csr_matrix) -> List[int]: Predict labels using a trained classifier and a sparse matrix. Args: model - A trained classifier model. X - A sparse matrix of tf-idf features. Returns: A list of predicted labels for each document. return model.predict(X).tolist()"},{"question":"# Boolean Object Manipulation In this assignment, you are required to implement a function that makes extensive use of Boolean values to perform various checks and manipulations. Problem Description: You need to implement a function `process_booleans(input_list)` which will take a list of integers as input and perform the following operations: 1. **Check each number in the list**: - If the number is even, append `True` to the result list. - If the number is odd, append `False` to the result list. - If the number is negative, append `None` to the result list instead of `True` or `False`. 2. **Return the resultant list** with appended Boolean values and `None` for negative numbers. Function Signature: ```python def process_booleans(input_list: list) -> list: pass ``` Input: - `input_list`: A list of integers. Output: - A list of Booleans and None values, where: - `True` represents an even number, - `False` represents an odd number, - `None` represents a negative number. Constraints: - The input list will have at least one integer. - The integers in the list can be positive, negative, or zero. Example: ```python assert process_booleans([1, 2, 3, -1, -2, 0]) == [False, True, False, None, None, True] assert process_booleans([4, 5, -7, 0]) == [True, False, None, True] assert process_booleans([-10, -9, 20]) == [None, None, True] ``` Advanced Requirement: You must use appropriate operations to make this function efficient with a complexity of O(n), where n is the length of the input list. Good luck with your implementation!","solution":"def process_booleans(input_list): Process a list of integers and return a list of boolean values or None. For each integer in the input list: - Append `True` if the integer is even, - Append `False` if the integer is odd, - Append `None` if the integer is negative. Args: input_list (list): A list of integers. Returns: list: A list of booleans and None values. result = [] for number in input_list: if number < 0: result.append(None) else: result.append(number % 2 == 0) return result"},{"question":"Coding Assessment Question # Objective You are required to demonstrate your understanding of the `resource` module by implementing a function that sets resource limits and retrieves resource usage information for a given process. # Problem Statement Write a Python function `set_and_get_resource_usage(pid: int, resource_limits: dict) -> dict` that takes in the process ID (pid) and a dictionary of resource limits, sets the resource limits for the process, and then retrieves and returns the resource usage information. # Function Signature ```python def set_and_get_resource_usage(pid: int, resource_limits: dict) -> dict: ``` # Input - `pid` (int): The process ID for which to set and get resource limits. If `pid` is 0, the current process should be considered. - `resource_limits` (dict): A dictionary where keys are resource names (e.g., \'RLIMIT_CPU\', \'RLIMIT_NOFILE\') and values are tuples of the form `(soft_limit, hard_limit)`. # Output - Returns a dictionary containing the resource usage information. The keys should be the resource field names (e.g., \\"ru_utime\\", \\"ru_stime\\", etc.) as described in the documentation, and the values should be the corresponding usage statistics. # Constraints - You should handle `ValueError` and `OSError` exceptions that may arise while setting resource limits. - Assume the system supports the provided resource names in the `resource_limits` dictionary. - You\'ll only deal with resources available in the `resource` module. # Example ```python resource_limits = { \'RLIMIT_CPU\': (10, 20), # soft limit of 10s and hard limit of 20s \'RLIMIT_NOFILE\': (100, 200) # soft limit of 100 files and hard limit of 200 files } usage_info = set_and_get_resource_usage(0, resource_limits) print(usage_info) #{ # \'ru_utime\': 0.08, # \'ru_stime\': 0.01, # \'ru_maxrss\': 9344, # \'ru_ixrss\': 0, # \'ru_idrss\': 0, # \'ru_isrss\': 0, # \'ru_minflt\': 200, # \'ru_majflt\': 0, # \'ru_nswap\': 0, # \'ru_inblock\': 0, # \'ru_oublock\': 0, # \'ru_msgsnd\': 0, # \'ru_msgrcv\': 0, # \'ru_nsignals\': 0, # \'ru_nvcsw\': 31, # \'ru_nivcsw\': 3 #} ``` # Notes - Use the `resource` module to implement this function. - Ensure your function is tested with various processes and resource limits.","solution":"import resource def set_and_get_resource_usage(pid: int, resource_limits: dict) -> dict: Set resource limits for the given process and retrieve resource usage information. :param pid: Process ID for which to set and get resource limits. 0 means current process. :param resource_limits: Dictionary with resource names and their corresponding limits. :return: Dictionary with resource usage information. try: for res_name, limits in resource_limits.items(): if hasattr(resource, res_name): res = getattr(resource, res_name) resource.prlimit(pid, res, limits) else: raise ValueError(f\\"Resource {res_name} is not valid.\\") except (ValueError, OSError) as e: return {\\"error\\": str(e)} usage = resource.getrusage(resource.RUSAGE_SELF) usage_dict = {field: getattr(usage, field) for field in dir(usage) if field.startswith(\'ru_\')} return usage_dict"},{"question":"<|Analysis Begin|> The documentation provides an overview of the meta device in PyTorch, which allows for the creation of tensors that store metadata without actual data. The key functionalities include: 1. Loading models on the meta device to manipulate the model structure without loading parameters into memory. 2. Performing operations on meta tensors to analyze tensor transformations without computational overhead. 3. Using context managers to enforce tensor construction on meta devices, useful for NN module construction. 4. Converting meta tensors correctly to real tensors by specifying how missing data should be filled in. 5. Using `torch.nn.Module.to_empty` to transfer NN modules to another device, leaving parameters uninitialized. The core concepts to test include tensor operations, context managers, NN module manipulation, and correct usage of meta tensors. <|Analysis End|> <|Question Begin|> **Meta Device in PyTorch** The \\"meta\\" device in PyTorch is used to create tensors and manage models in memory-efficient ways by dealing only with metadata. This exercise requires you to demonstrate a good understanding of working with the meta device. # Task You are required to implement a function `initialize_meta_nn` that constructs a PyTorch neural network module with uninitialized parameters on a `meta` device. You’ll then move this model to a specified device (CPU/CUDA) and initialize the parameters with random values. # Detailed Instructions 1. Create a simple feed-forward neural network class that inherits from `torch.nn.Module`. The network should have: - An input layer matching the input size. - One hidden layer with 100 units. - An output layer matching the output size. 2. Write the function `initialize_meta_nn` that takes the following parameters: - `input_size` (int): Size of the input layer. - `output_size` (int): Size of the output layer. - `device` (string): Target device to move the network to (\\"cpu\\" or \\"cuda\\"). 3. Inside the function, follow these steps: - Construct the neural network on a `meta` device. - Move the network to the specified `device` using `to_empty`. - Initialize the weights using `torch.nn.init.kaiming_uniform_`. - Initialize the biases to zero. 4. Your function should return the constructed and initialized neural network. # Constraints - You should use the `torch` and `torch.nn` libraries effectively. - The dimensions and initialization methods must be as specified. - The function should be robust and handle initialization for both CPU and CUDA devices. # Input/Output formats ```python def initialize_meta_nn(input_size: int, output_size: int, device: str) -> torch.nn.Module: pass # Example usage model = initialize_meta_nn(10, 2, \'cpu\') print(model) ``` Expected Output: ```plaintext FeedForwardNN( (hidden): Linear(in_features=10, out_features=100, bias=True) (output): Linear(in_features=100, out_features=2, bias=True) ) ``` Note: The exact initialization of weights and biases will be different each time you run due to random initialization. # Guidelines 1. Ensure you understand and appropriately utilize meta devices. 2. Properly handle device transfers and initialization as noted. 3. Make the class and function design modular and clean. **Happy Coding!**","solution":"import torch import torch.nn as nn class FeedForwardNN(nn.Module): def __init__(self, input_size, output_size): super(FeedForwardNN, self).__init__() self.hidden = nn.Linear(input_size, 100) self.output = nn.Linear(100, output_size) def initialize_meta_nn(input_size: int, output_size: int, device: str) -> nn.Module: # Construct the neural network on a meta device net_meta = FeedForwardNN(input_size, output_size).to(device=\'meta\') # Move the network to the specified device net = net_meta.to_empty(device=device) # Initialize weights and biases for layer in net.children(): if isinstance(layer, nn.Linear): torch.nn.init.kaiming_uniform_(layer.weight, nonlinearity=\'relu\') torch.nn.init.zeros_(layer.bias) return net"},{"question":"# Outlier Detection Using Isolation Forest in Scikit-Learn **Objective:** You are tasked with implementing an outlier detection model using the `IsolationForest` method from the scikit-learn package. You will fit it on a provided dataset, use it for predicting outliers, and evaluate its performance using ROC curves. **Instructions:** 1. Implement the `train_isolation_forest` function, which initializes and trains an Isolation Forest estimator. 2. Implement the `evaluate_isolation_forest` function, which predicts the outliers in the test data and evaluates the performance using ROC AUC score. 3. Use the `metrics.roc_auc_score` from scikit-learn to evaluate the performance. **Functions Specifications:** 1. **`train_isolation_forest(X_train: np.ndarray, n_estimators: int = 100, contamination: float = 0.1) -> IsolationForest`** - Arguments: - `X_train`: A NumPy array of shape `(n_samples, n_features)` representing the training data. - `n_estimators`: An integer specifying the number of base estimators in the ensemble. Default is 100. - `contamination`: A float representing the proportion of outliers in the data set. Default is 0.1. - Returns: - A fitted `IsolationForest` estimator. 2. **`evaluate_isolation_forest(estimator: IsolationForest, X_test: np.ndarray, y_test: np.ndarray) -> float`** - Arguments: - `estimator`: A fitted `IsolationForest` estimator. - `X_test`: A NumPy array of shape `(n_samples, n_features)` representing the test data. - `y_test`: A NumPy array of shape `(n_samples,)` containing true labels, where `1` indicates inliers and `-1` indicates outliers. - Returns: - The AUC score for the ROC curve as a float. **Example Usage:** ```python import numpy as np from sklearn.ensemble import IsolationForest from sklearn import metrics # Generate toy data X_train = np.array([[0], [0.44], [0.45], [0.46], [1]]) X_test = np.array([[-1.1], [0.02], [0.48], [0.5], [1.5]]) y_test = np.array([-1, 1, 1, 1, -1]) # Train Isolation Forest estimator = train_isolation_forest(X_train, n_estimators=50, contamination=0.2) # Evaluate Isolation Forest auc_score = evaluate_isolation_forest(estimator, X_test, y_test) print(f\\"AUC Score: {auc_score}\\") ``` **Constraints:** - You should not use the `fit_predict` method of the Isolation Forest. - Ensure the code runs efficiently on datasets with thousands of samples. - Follow proper coding practices and document your code where necessary. **Dataset:** - You can use any synthetic or real-world dataset suitable for anomaly detection. - Ensure that the dataset includes both inliers and outliers. # Requirements: 1. Python 3.6+ 2. scikit-learn 0.24+ 3. numpy **Evaluation:** - Correctness of the function implementations. - Performance of the model as indicated by the ROC AUC score. - Code efficiency and clarity.","solution":"from sklearn.ensemble import IsolationForest from sklearn import metrics import numpy as np def train_isolation_forest(X_train: np.ndarray, n_estimators: int = 100, contamination: float = 0.1) -> IsolationForest: Train an IsolationForest on the given training data. Parameters: - X_train: np.ndarray, shape (n_samples, n_features) - n_estimators: int, default=100 - The number of base estimators in the ensemble. - contamination: float, default=0.1 - The proportion of outliers in the dataset. Returns: - estimator: Fitted IsolationForest object. estimator = IsolationForest(n_estimators=n_estimators, contamination=contamination, random_state=42) estimator.fit(X_train) return estimator def evaluate_isolation_forest(estimator: IsolationForest, X_test: np.ndarray, y_test: np.ndarray) -> float: Evaluate an IsolationForest on the test data and calculate the ROC AUC score. Parameters: - estimator: Fitted IsolationForest object. - X_test: np.ndarray, shape (n_samples, n_features) - y_test: np.ndarray, shape (n_samples,) - True labels, where 1 indicates inliers and -1 indicates outliers. Returns: - auc_score: float - The AUC score for the ROC curve. y_scores = -estimator.decision_function(X_test) # Use the negative of the decision_function for ROC curve auc_score = metrics.roc_auc_score(y_test, y_scores) return auc_score"},{"question":"# Custom Data Loading with PyTorch: Creating and Using Datasets and DataLoader **Objective:** Create a custom map-style dataset and an iterable-style dataset using PyTorch. Implement custom data loaders for both datasets, with specific requirements for batching and memory pinning. **Instructions:** 1. Implement a custom map-style dataset called `CustomMapDataset` that: - Inherits from `torch.utils.data.Dataset`. - Initializes with a list of data samples. - Implements the `__getitem__` method to return individual data samples. - Implements the `__len__` method to return the number of data samples. 2. Implement a custom iterable-style dataset called `CustomIterableDataset` that: - Inherits from `torch.utils.data.IterableDataset`. - Initializes with a generator function that yields data samples. - Implements the `__iter__` method to return the data samples from the generator. 3. Create custom data loaders for both datasets: - For `CustomMapDataset`: - Use a batch size of 4. - Enable memory pinning. - Use shuffling. - For `CustomIterableDataset`: - Use a batch size of 2. - Disable automatic batching and handle it manually using a custom collate function. 4. Implement a custom collate function for the `CustomIterableDataset` data loader that: - Takes a list of data samples and returns them as a single batch. 5. Demonstrate the usage of both data loaders by iterating through them and printing out the batches. **Expected Input and Output:** 1. `CustomMapDataset`: - Input: List of data samples (e.g., `[1,2,3,4,5,6,7,8,9,10]`). - Output: Single data samples (via `__getitem__`). 2. `CustomIterableDataset`: - Input: Generator function yielding data samples (`(i for i in range(1,11))`). - Output: Single data samples (via `__iter__`). 3. Custom Data Loaders: - Input: `CustomMapDataset` and `CustomIterableDataset`. - Output: Batches of data samples. **Constraints:** - Ensure that the data loaders and custom collate function handle the data correctly and efficiently. - Test the implementation on both CPU and CUDA-enabled GPUs (if available). ```python import torch from torch.utils.data import Dataset, DataLoader, IterableDataset # Task 1: Implement CustomMapDataset class CustomMapDataset(Dataset): def __init__(self, data): super(CustomMapDataset, self).__init__() self.data = data def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) # Task 2: Implement CustomIterableDataset class CustomIterableDataset(IterableDataset): def __init__(self, data_generator): super(CustomIterableDataset, self).__init__() self.data_generator = data_generator def __iter__(self): return iter(self.data_generator()) # Task 3: Create Custom Data Loaders def collate_fn(batch): # Task 4: Implement custom collate function for IterableDataset batch = torch.tensor(batch) return batch # Example usage if __name__ == \\"__main__\\": map_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] iter_data = lambda: (i for i in range(1, 11)) map_dataset = CustomMapDataset(map_data) iterable_dataset = CustomIterableDataset(iter_data) map_data_loader = DataLoader(map_dataset, batch_size=4, shuffle=True, pin_memory=True) iterable_data_loader = DataLoader(iterable_dataset, batch_size=None, collate_fn=collate_fn) print(\\"Map Data Loader Batches:\\") for batch in map_data_loader: print(batch) print(\\"nIterable Data Loader Batches:\\") for batch in iterable_data_loader: print(batch) ``` **Explanation:** 1. `CustomMapDataset`: A simple dataset that implements the `__getitem__` and `__len__` methods to provide map-style data loading. 2. `CustomIterableDataset`: A dataset that implements the `__iter__` method to provide iterable-style data loading. 3. Data loaders: Created for both datasets with specified configurations (batching, memory pinning, and shuffling). 4. Custom collate function: Handles manual batching for `CustomIterableDataset`. **Note:** Ensure to run this code in an environment where PyTorch is installed and check its behavior on both CPU and (optional) CUDA-enabled GPU.","solution":"import torch from torch.utils.data import Dataset, DataLoader, IterableDataset # Task 1: Implement CustomMapDataset class CustomMapDataset(Dataset): def __init__(self, data): super(CustomMapDataset, self).__init__() self.data = data def __getitem__(self, index): return self.data[index] def __len__(self): return len(self.data) # Task 2: Implement CustomIterableDataset class CustomIterableDataset(IterableDataset): def __init__(self, data_generator): super(CustomIterableDataset, self).__init__() self.data_generator = data_generator def __iter__(self): return iter(self.data_generator()) # Task 3: Create Custom Data Loaders def collate_fn(batch): # Task 4: Implement custom collate function for IterableDataset batch = torch.tensor(batch) return batch # Example usage if __name__ == \\"__main__\\": map_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] iter_data = lambda: (i for i in range(1, 11)) map_dataset = CustomMapDataset(map_data) iterable_dataset = CustomIterableDataset(iter_data) map_data_loader = DataLoader(map_dataset, batch_size=4, shuffle=True, pin_memory=True) iterable_data_loader = DataLoader(iterable_dataset, batch_size=None, collate_fn=collate_fn) print(\\"Map Data Loader Batches:\\") for batch in map_data_loader: print(batch) print(\\"nIterable Data Loader Batches:\\") for batch in iterable_data_loader: print(batch)"},{"question":"Implementing a Custom Data Type Using `PyLongObject` You are required to write a Python function that acts as a custom integer type handler. This handler will allow the conversion of different data representations into Python integers and handle any possible overflow or type errors. Function Specifications 1. **Function Name**: `custom_integer_handler` 2. **Input**: - A list of tuples. Each tuple contains two elements: 1. A value to be converted to a Python integer. The value can be of the following types: string (representing an integer in base 10), long, unsigned long, or double. 2. A string indicating the type of the value (possible values are \\"string\\", \\"long\\", \\"unsigned long\\", \\"double\\"). 3. **Output**: - A list of Python integers corresponding to each input value. - If conversion for any value fails, include a string `\\"Error\\"` at that position in the output. Constraints - The input string representing integers will be guaranteed to be in base 10. - Inputs will have a maximum length of 1000 elements. - You need to handle any overflow or type errors gracefully and return `\\"Error\\"` for those cases. Function Signature ```python def custom_integer_handler(input_list: List[Tuple[Union[str, int, float], str]]) -> List[Union[int, str]]: ``` Example Usage ```python input_list = [ (\\"123\\", \\"string\\"), (456, \\"long\\"), (1000, \\"unsigned long\\"), (3.14159, \\"double\\"), (\\"not_a_number\\", \\"string\\") ] output = custom_integer_handler(input_list) print(output) # Should print: [123, 456, 1000, 3, \'Error\'] ``` Advanced Requirements (optional) - Optimize the function to handle very large lists with minimal performance degradation. - Include comprehensive error messages for debugging purposes. Good luck with your implementation!","solution":"from typing import List, Tuple, Union def custom_integer_handler(input_list: List[Tuple[Union[str, int, float], str]]) -> List[Union[int, str]]: def convert(value, value_type): try: if value_type == \\"string\\": return int(value) elif value_type == \\"long\\": return int(value) elif value_type == \\"unsigned long\\": if value < 0: return \\"Error\\" return int(value) elif value_type == \\"double\\": return int(value) except (ValueError, OverflowError, TypeError): return \\"Error\\" return [convert(value, value_type) for value, value_type in input_list]"},{"question":"# Seaborn Visualization Task You are provided with documentation on Seaborn\'s `jointplot` function, which allows creating various types of joint plots with marginal distributions. Your task is to implement a function that generates a customized joint plot based on specific parameters. Function Signature ```python def create_custom_jointplot(data, x, y, hue=None, kind=\'scatter\', size=6, palette=\'deep\', **kwargs): Creates a customized joint plot using seaborn.jointplot. Parameters: data (DataFrame): The dataset for the plot. x (str): Column name to be used for the x-axis. y (str): Column name to be used for the y-axis. hue (str, optional): Column name for grouping variable that will produce points with different colors. kind (str, optional): The type of plot (options: \'scatter\', \'kde\', \'reg\', \'hist\', \'hex\'). size (float, optional): Size of the plot. palette (str, optional): Color palette for the plot. kwargs: Additional keyword arguments for seaborn.jointplot. Returns: JointGrid: The resulting seaborn JointGrid object. pass ``` Requirements 1. **Input Parameters**: - `data` should be a Pandas DataFrame. - `x` and `y` should specify the column names for the x-axis and y-axis respectively. - `hue`(optional) allows conditional coloring by a categorical variable. - `kind` specifies the type of joint plot. - `size` controls the size of the plot. - `palette` specifies the color palette to be used for the plot. - Additional `kwargs` will be passed to the `sns.jointplot` function. 2. **Implementation Details**: - Use the `jointplot` function from the Seaborn library to create the plot. - Customize the plot based on the input parameters. - Return the resulting `JointGrid` object. 3. **Output**: - A customized `JointGrid` object representing the joint plot. 4. **Constraints**: - Ensure that all variations of `kind` are correctly handled. - Handle any additional keyword arguments passed through `kwargs`. Example ```python import seaborn as sns # Load example dataset penguins = sns.load_dataset(\\"penguins\\") # Call the function to create a customized joint plot grid = create_custom_jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", size=7, palette=\\"husl\\") # Display the plot grid.fig.suptitle(\\"Customized Joint Plot\\", y=1.02) ``` Implement the `create_custom_jointplot` function to achieve the described functionality.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_jointplot(data, x, y, hue=None, kind=\'scatter\', size=6, palette=\'deep\', **kwargs): Creates a customized joint plot using seaborn.jointplot. Parameters: data (DataFrame): The dataset for the plot. x (str): Column name to be used for the x-axis. y (str): Column name to be used for the y-axis. hue (str, optional): Column name for grouping variable that will produce points with different colors. kind (str, optional): The type of plot (options: \'scatter\', \'kde\', \'reg\', \'hist\', \'hex\'). size (float, optional): Size of the plot. palette (str, optional): Color palette for the plot. kwargs: Additional keyword arguments for seaborn.jointplot. Returns: JointGrid: The resulting seaborn JointGrid object. joint_grid = sns.jointplot(data=data, x=x, y=y, hue=hue, kind=kind, height=size, palette=palette, **kwargs) plt.show() return joint_grid"},{"question":"**Problem Statement:** You are tasked with developing a logging utility using Python that records application activities to the Unix system logger. This utility must handle different priority levels and use custom log options. Additionally, it should be able to switch logging masks based on the provided settings. **Requirements:** 1. Implement a function `configure_syslog(ident, logoption, facility)`: - Parameters: - `ident` (str): The identity string to prepend to every message. - `logoption` (int): Bitwise OR of log options (e.g., `syslog.LOG_PID | syslog.LOG_NDELAY`). - `facility` (int): The logging facility (e.g., `syslog.LOG_USER`). - The function should call `syslog.openlog()` with the provided parameters to configure logging options. 2. Implement a function `log_message(priority, message)`: - Parameters: - `priority` (int): The priority level of the log message (e.g., `syslog.LOG_ERR`). - `message` (str): The message to log. - The function should call `syslog.syslog()` with the provided message and priority. 3. Implement a function `set_log_mask(priorities)`: - Parameters: - `priorities` (list of int): A list of priority levels to be included in the log mask (e.g., `[syslog.LOG_ERR, syslog.LOG_WARNING]`). - The function should set the log mask using `syslog.setlogmask()`, allowing only the specified priorities. 4. Implement a function `reset_syslog()`: - This function should call `syslog.closelog()` to reset all syslog settings. 5. Your solution should include usage examples demonstrating different configurations and logging scenarios, including the logging of various priority levels and the application of a log mask. **Example Usage:** ```python import syslog # Step 1: Configure syslog with ident, logoption, and facility. configure_syslog(ident=\\"MyApp\\", logoption=syslog.LOG_PID | syslog.LOG_NDELAY, facility=syslog.LOG_USER) # Step 2: Log different messages with different priorities. log_message(priority=syslog.LOG_INFO, message=\\"Information message\\") log_message(priority=syslog.LOG_ERR, message=\\"Error occurred\\") # Step 3: Set log mask to include only specific priorities. set_log_mask([syslog.LOG_ERR, syslog.LOG_CRIT]) # Step 4: Log a message that will be ignored due to the log mask. log_message(priority=syslog.LOG_INFO, message=\\"This will not be logged\\") # Step 5: Reset syslog settings. reset_syslog() ``` **Constraints:** - Ensure that the functions handle invalid input appropriately. - The `set_log_mask` function should create an appropriate mask for the given priorities. - Consider performance and ensure minimal overhead when logging messages.","solution":"import syslog def configure_syslog(ident, logoption, facility): Configure the syslog with the given parameters. :param ident: str, identity string to prepend to every message :param logoption: int, bitwise OR of log options (e.g., syslog.LOG_PID | syslog.LOG_NDELAY) :param facility: int, logging facility (e.g., syslog.LOG_USER) syslog.openlog(ident=ident, logoption=logoption, facility=facility) def log_message(priority, message): Log a message with the given priority. :param priority: int, priority level of the log message (e.g., syslog.LOG_ERR) :param message: str, the message to log syslog.syslog(priority, message) def set_log_mask(priorities): Set the log mask to allow only the specified priorities. :param priorities: list of int, list of priority levels to be included in the log mask (e.g., [syslog.LOG_ERR, syslog.LOG_WARNING]) mask = 0 for priority in priorities: mask |= syslog.LOG_MASK(priority) syslog.setlogmask(mask) def reset_syslog(): Reset all syslog settings. syslog.closelog()"},{"question":"**Question: Implement Utility Functions for Float Operations** **Objective:** You are asked to implement several utility functions for managing floating point numbers using the provided API functions. Your implementation should demonstrate the understanding of the Python C API for floating point objects. **Instructions:** 1. **Function**: `is_float(obj: Any) -> bool` - **Description**: Check if a given object is a float using `PyFloat_Check`. - **Input**: A Python object `obj`. - **Output**: `True` if the object is a float, `False` otherwise. 2. **Function**: `is_exact_float(obj: Any) -> bool` - **Description**: Check if a given object is exactly a float (i.e., not a subclass of float) using `PyFloat_CheckExact`. - **Input**: A Python object `obj`. - **Output**: `True` if the object is exactly a float, `False` otherwise. 3. **Function**: `float_from_string(s: str) -> float` - **Description**: Create a float from a string input using `PyFloat_FromString`. - **Input**: A string `s` representing a floating point number. - **Output**: A float value parsed from the input string. 4. **Function**: `float_from_double(d: float) -> float` - **Description**: Create a float from a double input using `PyFloat_FromDouble`. - **Input**: A float `d`. - **Output**: A new float object. 5. **Function**: `double_representation(f: float) -> float` - **Description**: Get the C double representation of a float object using `PyFloat_AsDouble`. - **Input**: A float `f`. - **Output**: A C double representation of the float. 6. **Function**: `double_representation_no_check(f: float) -> float` - **Description**: Get the C double representation without error checking using `PyFloat_AS_DOUBLE`. - **Input**: A float `f`. - **Output**: A C double representation of the float. 7. **Function**: `get_float_info() -> Tuple[float, float, float]` - **Description**: Get the information about float precision, minimum, and maximum values using `PyFloat_GetInfo`. - **Input**: None. - **Output**: A tuple containing the minimum and maximum representable finite floats, and float precision. 8. **Function**: `get_max_float() -> float` - **Description**: Get the maximum representable finite float using `PyFloat_GetMax`. - **Input**: None. - **Output**: The maximum representable finite float. 9. **Function**: `get_min_float() -> float` - **Description**: Get the minimum normalized positive float using `PyFloat_GetMin`. - **Input**: None. - **Output**: The minimum normalized positive float. **Constraints:** - You may assume that all inputs are valid and conform to the expected types. - You should implement and test these functions using proper Python C API interactions. **Example Usage:** ```python # Example usage of the implemented functions: print(is_float(10.5)) # Should return True. print(is_exact_float(3.14)) # Should return True. print(float_from_string(\\"123.45\\")) # Should return 123.45. print(float_from_double(456.78)) # Should return 456.78. print(double_representation(789.01)) # Equivalent float representation. print(double_representation_no_check(101.12)) # Equivalent float representation. print(get_float_info()) # Should return a tuple with precision, min, max float info. print(get_max_float()) # Maximum representable float. print(get_min_float()) # Minimum normalized positive float. ``` Ensure that your implementation adheres to Python memory management protocols and handles exceptions appropriately where applicable.","solution":"def is_float(obj): Check if a given object is a float. return isinstance(obj, float) def is_exact_float(obj): Check if a given object is exactly a float (not a subclass of float). return type(obj) is float def float_from_string(s): Create a float from a string input. try: return float(s) except ValueError: return None def float_from_double(d): Create a float from a double input (in Python, it is the same as float). return float(d) def double_representation(f): Get the C double representation of a float object. if isinstance(f, float): return f else: raise TypeError(\\"Argument must be a float.\\") def double_representation_no_check(f): Get the C double representation without error checking. return f # In Python, assuming \'f\' is already a float def get_float_info(): Get the information about float precision, min, and max values. import sys return (sys.float_info.epsilon, sys.float_info.min, sys.float_info.max) def get_max_float(): Get the maximum representable finite float. import sys return sys.float_info.max def get_min_float(): Get the minimum normalized positive float. import sys return sys.float_info.min"},{"question":"Design a class `CustomSequence` that mimics a basic sequence type in Python. The class should allow for indexing, iteration, and support basic sequence operations such as concatenation and repetition. Your implementation should demonstrate the use of Python protocols, specifically focusing on the Sequence Protocol and Iterator Protocol. # Requirements: 1. **Initialization**: The class should be initialized with a list of integers. 2. **Indexing**: The class should support accessing elements by index. 3. **Iteration**: The class should be iterable. 4. **Concatenation**: The class should support concatenation with another `CustomSequence` object. 5. **Repetition**: The class should support repetition using the `*` operator. 6. **Length**: The class should support the `len()` function. 7. **Containment**: The class should support the `in` keyword to check if an element exists in the sequence. # Input and Output Formats: - The initializer will accept a list of integers: `__init__(self, data: List[int])` - Indexing should work with the syntax: `obj[index]` - The class should support iteration, e.g., `for element in obj:` - Concatenation should work with the syntax: `obj1 + obj2` - Repetition should work with the syntax: `obj * n` - Length should return an integer using `len()` - Containment check should work with the syntax: `element in obj` # Constraints: 1. Do not use any standard sequence types directly (e.g., list, tuple) other than for initialization in `__init__`. 2. Handle out-of-range indices for indexing gracefully by raising an `IndexError`. 3. Ensure that all operations maintain the integrity of the sequence type. # Example Usage: ```python # Initialization seq1 = CustomSequence([1, 2, 3]) seq2 = CustomSequence([4, 5, 6]) # Indexing print(seq1[1]) # Output: 2 # Iteration for num in seq1: print(num) # Output: 1 2 3 # Concatenation seq3 = seq1 + seq2 print(list(seq3)) # Output: [1, 2, 3, 4, 5, 6] # Repetition seq4 = seq1 * 2 print(list(seq4)) # Output: [1, 2, 3, 1, 2, 3] # Length print(len(seq1)) # Output: 3 # Containment print(2 in seq1) # Output: True print(10 in seq1) # Output: False ``` Define the `CustomSequence` class to meet the requirements above.","solution":"from collections.abc import Sequence, Iterator class CustomSequence(Sequence): def __init__(self, data): self.data = data def __getitem__(self, index): if isinstance(index, slice): return CustomSequence(self.data[index]) if index >= len(self.data) or index < -len(self.data): raise IndexError(\\"Index out of range\\") return self.data[index] def __iter__(self): return CustomIterator(self.data) def __len__(self): return len(self.data) def __contains__(self, item): return item in self.data def __add__(self, other): if not isinstance(other, CustomSequence): raise TypeError(\\"Can only concatenate CustomSequence to CustomSequence\\") return CustomSequence(self.data + other.data) def __mul__(self, count): return CustomSequence(self.data * count) class CustomIterator(Iterator): def __init__(self, data): self.data = data self.index = 0 def __next__(self): if self.index >= len(self.data): raise StopIteration result = self.data[self.index] self.index += 1 return result"},{"question":"# Advanced Python Coding Assessment **Objective:** Design a simple command-line interface (CLI) calculator using the `cmd` module in Python. This CLI will accept basic arithmetic operations (addition, subtraction, multiplication, division) and provide the result to the user. Additionally, it should maintain a history of commands and allow the user to view and replay previous commands. **Requirements:** 1. **Main Class:** - Create a class `CalculatorShell` inheriting from `cmd.Cmd`. 2. **Basic Commands:** - Implement methods for the basic arithmetic operations (`add`, `subtract`, `multiply`, `divide`). - Each command should accept two integers or floats as arguments and print the result. 3. **Command History:** - Implement functionality to keep a history of commands and their results. - Add a command `history` to display all previous commands with their results. 4. **Replay Commands:** - Implement a command `replay` that accepts an index and re-executes the corresponding command from history. **Constraints:** - Commands should follow the format: `[command] [operand1] [operand2]`. For example, `add 2 3` should output `5`. **Expected Input and Output:** - **Input:** Commands issued via the CLI. - **Output:** Results of the arithmetic operations and history listings. **Sample Session:** ``` (calculator) add 2 3 5.0 (calculator) subtract 10 4 6.0 (calculator) multiply 3 3 9.0 (calculator) divide 12 4 3.0 (calculator) history 1: add 2 3 => 5.0 2: subtract 10 4 => 6.0 3: multiply 3 3 => 9.0 4: divide 12 4 => 3.0 (calculator) replay 1 5.0 (calculator) bye ``` **Implementation:** ```python import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the calculator shell. Type help or ? to list commands.n\' prompt = \'(calculator) \' history = [] def do_add(self, arg): \'Add two numbers: ADD 2 3\' try: operands = parse(arg) result = sum(operands) print(f\'{result}\') CalculatorShell.history.append(f\'{arg} => {result}\') except Exception as e: print(f\'Error: {e}\') def do_subtract(self, arg): \'Subtract two numbers: SUBTRACT 10 4\' try: operands = parse(arg) result = operands[0] - operands[1] print(f\'{result}\') CalculatorShell.history.append(f\'{arg} => {result}\') except Exception as e: print(f\'Error: {e}\') def do_multiply(self, arg): \'Multiply two numbers: MULTIPLY 3 3\' try: operands = parse(arg) result = operands[0] * operands[1] print(f\'{result}\') CalculatorShell.history.append(f\'{arg} => {result}\') except Exception as e: print(f\'Error: {e}\') def do_divide(self, arg): \'Divide two numbers: DIVIDE 12 4\' try: operands = parse(arg) result = operands[0] / operands[1] print(f\'{result}\') CalculatorShell.history.append(f\'{arg} => {result}\') except Exception as e: print(f\'Error: {e}\') def do_history(self, arg): \'Display the history of commands: HISTORY\' for idx, record in enumerate(CalculatorShell.history, 1): print(f\'{idx}: {record}\') def do_replay(self, arg): \'Replay a command from the history by index: REPLAY 1\' try: idx = int(arg) - 1 if 0 <= idx < len(CalculatorShell.history): command = CalculatorShell.history[idx].split(\' => \')[0] print(f\'Replaying: {command}\') self.onecmd(command) else: print(f\'Index out of range\') except Exception as e: print(f\'Error: {e}\') def do_bye(self, arg): \'Exit the calculator shell: BYE\' print(\'Thank you for using the calculator.\') return True def parse(arg): \'Convert a series of two numbers to a tuple\' return tuple(map(float, arg.split())) if __name__ == \'__main__\': CalculatorShell().cmdloop() ``` **Instructions for Students:** 1. Implement the `CalculatorShell` class based on the above instructions. 2. Ensure that all commands work as expected and handle any errors gracefully. 3. Test your implementation thoroughly.","solution":"import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the calculator shell. Type help or ? to list commands.n\' prompt = \'(calculator) \' history = [] def do_add(self, arg): \'Add two numbers: ADD 2 3\' try: operands = self.parse(arg) result = operands[0] + operands[1] print(f\'{result}\') CalculatorShell.history.append(f\'add {arg} => {result}\') except Exception as e: print(f\'Error: {e}\') def do_subtract(self, arg): \'Subtract two numbers: SUBTRACT 10 4\' try: operands = self.parse(arg) result = operands[0] - operands[1] print(f\'{result}\') CalculatorShell.history.append(f\'subtract {arg} => {result}\') except Exception as e: print(f\'Error: {e}\') def do_multiply(self, arg): \'Multiply two numbers: MULTIPLY 3 3\' try: operands = self.parse(arg) result = operands[0] * operands[1] print(f\'{result}\') CalculatorShell.history.append(f\'multiply {arg} => {result}\') except Exception as e: print(f\'Error: {e}\') def do_divide(self, arg): \'Divide two numbers: DIVIDE 12 4\' try: operands = self.parse(arg) if operands[1] == 0: raise ValueError(\\"Cannot divide by zero.\\") result = operands[0] / operands[1] print(f\'{result}\') CalculatorShell.history.append(f\'divide {arg} => {result}\') except Exception as e: print(f\'Error: {e}\') def do_history(self, arg): \'Display the history of commands: HISTORY\' for idx, record in enumerate(CalculatorShell.history, 1): print(f\'{idx}: {record}\') def do_replay(self, arg): \'Replay a command from the history by index: REPLAY 1\' try: idx = int(arg) - 1 if 0 <= idx < len(CalculatorShell.history): command = CalculatorShell.history[idx].split(\' => \')[0] print(f\'Replaying: {command}\') self.onecmd(command) else: print(f\'Index out of range\') except Exception as e: print(f\'Error: {e}\') def do_bye(self, arg): \'Exit the calculator shell: BYE\' print(\'Thank you for using the calculator.\') return True def parse(self, arg): \'Convert a series of two numbers to a tuple\' return tuple(map(float, arg.split())) if __name__ == \'__main__\': CalculatorShell().cmdloop()"},{"question":"Problem Statement: Design and implement a custom asynchronous chat server using the `asyncore` module in Python. Your task is to create both the server and client sides of the chat application, complete with the necessary event-handling mechanisms to enable multiple clients to connect to the server, send messages, and broadcast received messages to all connected clients. # Requirements: 1. **Server**: - The server should be able to accept multiple client connections. - It should handle incoming messages from any client and broadcast them to all connected clients. - The server should properly manage client disconnections. 2. **Client**: - The client should be able to connect to the server. - It should allow the user to send messages to the server. - It should display any messages broadcasted by the server. # Constraints: - Utilize the `asyncore` module for handling asynchronous I/O operations. - Do not use additional threads; all asynchronous behavior should be handled within the `asyncore` framework. - Ensure proper error handling for network I/O operations. # Input and Output: - **Server**: - No direct input; it should handle client connections and broadcasts. - Output: Messages received from clients and broadcasting to other clients. - **Client**: - Input: User messages typed from the console. - Output: Messages broadcasted by the server displayed on the console. # Performance Requirements: - The server should efficiently handle multiple simultaneous connections and message broadcasts with minimal latency. # Example Scenario: 1. Start the server on `localhost` at port `12345`. 2. Connect a client to the server. 3. Connect another client to the server. 4. Send a message from client A. 5. The server receives the message and broadcasts it to client B. 6. Client B displays the received message. # Implementation Steps: 1. Create a class for the chat server inheriting from `asyncore.dispatcher`. 2. Implement event handling methods for the server (e.g., `handle_accept`, `handle_read`, `handle_close`, etc.). 3. Create a class for the chat client inheriting from `asyncore.dispatcher_with_send`. 4. Implement event handling methods for the client (e.g., `handle_connect`, `handle_read`, `handle_write`, `handle_close`, etc.). 5. Ensure the server broadcasts messages to all connected clients. 6. Implement client functionality to send messages to the server and display received messages. ```python import asyncore import socket class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): sock, addr = self.accept() print(f\'Incoming connection from {addr}\') self.clients.append(ChatHandler(sock, self)) def broadcast(self, message): for client in self.clients: client.send_message(message) class ChatHandler(asyncore.dispatcher_with_send): def __init__(self, sock, server): asyncore.dispatcher_with_send.__init__(self, sock) self.server = server def handle_read(self): data = self.recv(1024) if data: self.server.broadcast(data) def send_message(self, message): self.send(message) def handle_close(self): print(\'Client disconnected\') self.server.clients.remove(self) self.close() class ChatClient(asyncore.dispatcher_with_send): def __init__(self, host, port): asyncore.dispatcher_with_send.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.connect((host, port)) def handle_connect(self): print(\'Connected to server\') def handle_read(self): data = self.recv(1024) if data: print(f\'Received: {data.decode()}\') def handle_write(self): message = input(\'Enter message: \') self.send(message.encode()) def handle_close(self): print(\'Disconnected from server\') self.close() def start_server(): server = ChatServer(\'localhost\', 12345) asyncore.loop() def start_client(): client = ChatClient(\'localhost\', 12345) asyncore.loop() # Uncomment to start server # start_server() # Uncomment to start client # start_client() ``` # Notes: - To test the implementation, start the server first and then connect multiple clients. - Clients should be able to send messages that are broadcasted to all other connected clients by the server.","solution":"import asyncore import socket class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) self.clients = [] def handle_accept(self): sock, addr = self.accept() print(f\'Incoming connection from {addr}\') self.clients.append(ChatHandler(sock, self)) def broadcast(self, message): for client in self.clients: client.send_message(message) class ChatHandler(asyncore.dispatcher_with_send): def __init__(self, sock, server): asyncore.dispatcher_with_send.__init__(self, sock) self.server = server def handle_read(self): data = self.recv(1024) if data: self.server.broadcast(data) def send_message(self, message): self.send(message) def handle_close(self): print(\'Client disconnected\') self.server.clients.remove(self) self.close() class ChatClient(asyncore.dispatcher_with_send): def __init__(self, host, port): asyncore.dispatcher_with_send.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.connect((host, port)) def handle_connect(self): print(\'Connected to server\') def handle_read(self): data = self.recv(1024) if data: print(f\'Received: {data.decode()}\') def handle_write(self): message = input(\'Enter message: \') self.send(message.encode()) def handle_close(self): print(\'Disconnected from server\') self.close() def start_server(): server = ChatServer(\'localhost\', 12345) asyncore.loop() def start_client(): client = ChatClient(\'localhost\', 12345) asyncore.loop() # Uncomment to start server # start_server() # Uncomment to start client # start_client()"},{"question":"Objective: Demonstrate your understanding of Python function objects and their manipulation using the provided functions. Problem Statement: Write a Python program that performs the following tasks: 1. Create a new Python function object with a given code object and global variables dictionary. 2. Retrieve and display the function\'s code object, globals dictionary, and module attribute. 3. Set default values for the function’s arguments. 4. Retrieve and display the function\'s default argument values. 5. Define annotations for the function’s arguments and return value. 6. Retrieve and display the function\'s annotations. Requirements: 1. **Function Implementation**: Implement a function `create_and_manipulate_function` which takes in three arguments: - `code_object`: A code object to be associated with the new function. - `globals_dict`: A dictionary for the global variables accessible to the function. - `default_args`: A tuple containing default argument values. - `annotations`: A dictionary containing annotations for the arguments and return value. 2. **Expected Input and Output**: - Input: ```python code_object = compile(\\"def generated_function(x, y): return x + y\\", \\"<string>\\", \\"exec\\") globals_dict = {} default_args = (1,) annotations = {\'x\': \'int\', \'y\': \'int\', \'return\': \'int\'} ``` - Output: The function should print the retrieved code object, globals dictionary, module attribute, default argument values, and annotations. 3. **Function Signature**: ```python def create_and_manipulate_function(code_object, globals_dict, default_args, annotations): pass ``` 4. **Constraints**: - The `code_object` must be valid and created using Python’s `compile` function. - The `globals_dict` should be a valid dictionary. - The `default_args` must be a tuple of default argument values. - The `annotations` must be a dictionary where keys are argument names or \'return\', and values are annotation types/values. 5. **Performance Requirements**: - The implementation should be efficient and make use of provided API functions correctly. Example: ```python def create_and_manipulate_function(code_object, globals_dict, default_args, annotations): # Step 1: Create a new function object function = PyFunction_New(code_object, globals_dict) # Step 2: Retrieve and display function\'s code object, globals, and module attribute retrieved_code = PyFunction_GetCode(function) retrieved_globals = PyFunction_GetGlobals(function) retrieved_module = PyFunction_GetModule(function) print(\\"Code Object:\\", retrieved_code) print(\\"Globals Dictionary:\\", retrieved_globals) print(\\"Module Attribute:\\", retrieved_module) # Step 3: Set default argument values PyFunction_SetDefaults(function, default_args) # Step 4: Retrieve and display default values retrieved_defaults = PyFunction_GetDefaults(function) print(\\"Default Argument Values:\\", retrieved_defaults) # Step 5: Set and retrieve annotations PyFunction_SetAnnotations(function, annotations) retrieved_annotations = PyFunction_GetAnnotations(function) print(\\"Annotations:\\", retrieved_annotations) # Example usage code_object = compile(\\"def generated_function(x, y): return x + y\\", \\"<string>\\", \\"exec\\") globals_dict = {} default_args = (1,) annotations = {\'x\': \'int\', \'y\': \'int\', \'return\': \'int\'} create_and_manipulate_function(code_object, globals_dict, default_args, annotations) ``` _NOTE_: Since the actual usage of `PyFunction_New` and related functions requires a deeper integration with Python C API, students are encouraged to conceptually show their understanding of these methods, bearing in mind the constraints of working within pure Python.","solution":"import types def create_and_manipulate_function(code_object, globals_dict, default_args, annotations): # Step 1: Create a new function object exec(code_object, globals_dict) # Extract the function (assuming it\'s the only function defined in the code_object) function_name = [name for name, obj in globals_dict.items() if isinstance(obj, types.FunctionType)][0] function = globals_dict[function_name] # Step 2: Retrieve and display function\'s code object, globals, and module attribute retrieved_code = function.__code__ retrieved_globals = function.__globals__ retrieved_module = function.__module__ print(\\"Code Object:\\", retrieved_code) print(\\"Globals Dictionary:\\", retrieved_globals) print(\\"Module Attribute:\\", retrieved_module) # Step 3: Set default argument values function.__defaults__ = default_args # Step 4: Retrieve and display default values retrieved_defaults = function.__defaults__ print(\\"Default Argument Values:\\", retrieved_defaults) # Step 5: Set and retrieve annotations function.__annotations__ = annotations retrieved_annotations = function.__annotations__ print(\\"Annotations:\\", retrieved_annotations) # Example usage code_string = \\"def generated_function(x, y): return x + y\\" code_object = compile(code_string, \\"<string>\\", \\"exec\\") globals_dict = {} default_args = (1,) annotations = {\'x\': \'int\', \'y\': \'int\', \'return\': \'int\'} create_and_manipulate_function(code_object, globals_dict, default_args, annotations)"},{"question":"**Objective**: Write a Python function that compresses and then decompresses a given piece of data and verifies the integrity of the decompressed data using checksums. **Function Signature**: ```python def verify_compression_integrity(data: bytes, compression_level: int = -1, checksum_type: str = \'adler32\') -> bool: pass ``` **Task Details**: 1. **Parameters**: - `data`: A bytes object representing the data to be processed. - `compression_level`: An integer from 0 to 9, or -1. Controls the level of compression where: - `1` (Z_BEST_SPEED) is the fastest and produces the least compression. - `9` (Z_BEST_COMPRESSION) is the slowest and produces the most compression. - `0` (Z_NO_COMPRESSION) is no compression. - `-1` (Z_DEFAULT_COMPRESSION) represents a default compromise between speed and compression. - `checksum_type`: A string that specifies the type of checksum to use. It can be either `\'adler32\'` or `\'crc32\'`. 2. **Return Value**: - The function should return `True` if the decompressed data matches the original data (indicating successful integrity verification), otherwise return `False`. 3. **Function Steps**: - Compute the initial checksum of the original data using the specified checksum type (`adler32` or `crc32`). - Compress the data using the specified compression level. - Decompress the compressed data. - Compute the checksum of the decompressed data using the same checksum type. - Compare the initial and final checksums to verify data integrity. **Constraints and Assumptions**: - The data provided can fit into memory. - Handle exceptions and errors where appropriate. **Example Usage**: ```python data = b\\"Hello, World! This is a test for zlib compression.\\" assert verify_compression_integrity(data, compression_level=9, checksum_type=\'adler32\') == True assert verify_compression_integrity(data, compression_level=1, checksum_type=\'crc32\') == True # Intentionally corrupted data should return False corrupted_data = data + b\\"CORRUPTED\\" assert verify_compression_integrity(corrupted_data, compression_level=9, checksum_type=\'adler32\') == False ``` Implement this function to test your understanding of zlib compression, decompression, and checksum calculations.","solution":"import zlib def verify_compression_integrity(data: bytes, compression_level: int = -1, checksum_type: str = \'adler32\') -> bool: if checksum_type not in [\'adler32\', \'crc32\']: raise ValueError(\\"Invalid checksum type. Choose either \'adler32\' or \'crc32\'.\\") # Compute the initial checksum of the original data if checksum_type == \'adler32\': initial_checksum = zlib.adler32(data) else: initial_checksum = zlib.crc32(data) # Compress the data compressed_data = zlib.compress(data, level=compression_level) # Decompress the data decompressed_data = zlib.decompress(compressed_data) # Compute the checksum of the decompressed data if checksum_type == \'adler32\': decompressed_checksum = zlib.adler32(decompressed_data) else: decompressed_checksum = zlib.crc32(decompressed_data) # Compare the initial and final checksums to verify data integrity return initial_checksum == decompressed_checksum"},{"question":"Objective Write a Python function that reads a provided file, processes the text or binary data, and writes the result into a new file. The function must be compatible with both Python 2.7 and Python 3.x. The function should handle specific processing tasks that involve detecting the Python version to appropriately manage text and binary data. Function Signature ```python import os def process_file(input_file_path, output_file_path, is_binary): Reads the provided input file, processes the content, and writes the result to the output file. The function must handle both text and binary data and be compatible with both Python 2.7 and Python 3.x. :param input_file_path: str : Path to the input file. :param output_file_path: str : Path to the output file. :param is_binary: bool : Indicates whether the file should be read/written in binary mode. pass ``` Requirements - The `input_file_path` parameter is a string representing the path to the input file to be read. - The `output_file_path` parameter is a string representing the path to the output file to be written. - The `is_binary` parameter is a boolean that indicates whether the file is to be processed as binary data (`True`) or text data (`False`). - The function should read the content from `input_file_path`, convert all alphabetic characters to uppercase, and write the processed content to `output_file_path`. - If `is_binary` is `True`, you should handle the files in binary mode using the \'`b`\' flag for reading and writing. - The function must be fully compatible with both Python 2.7 and Python 3.x. Use feature detection and the `future` module where necessary. - Ensure that the function minimizes the use of version-specific code to maximize forward compatibility. Constraints - Assume the input files already exist and the output paths are writable. - Ensure that the correct type of data is written to the correct type of file (text or binary). - Do not utilize any external libraries beyond `future` and built-in modules except if specified. - Handle exceptions that might arise during file reading/writing operations and provide meaningful error messages. Example ```python # Example usage: Convert a text file process_file(\'input.txt\', \'output.txt\', is_binary=False) # Example usage: Convert a binary file process_file(\'input.bin\', \'output.bin\', is_binary=True) ```","solution":"import os import sys def process_file(input_file_path, output_file_path, is_binary): Reads the provided input file, processes the content, and writes the result to the output file. The function must handle both text and binary data and be compatible with both Python 2.7 and Python 3.x. :param input_file_path: str : Path to the input file. :param output_file_path: str : Path to the output file. :param is_binary: bool : Indicates whether the file should be read/written in binary mode. if is_binary: # Binary mode, compatible with both Python 2.7 and 3.x mode = \'rb\' write_mode = \'wb\' else: # Text mode, compatible with both Python 2.7 and 3.x mode = \'r\' write_mode = \'w\' if sys.version_info[0] == 2: # Python 2.7 import codecs open_func = codecs.open else: # Python 3.x open_func = open try: if is_binary: with open(input_file_path, mode) as f: content = f.read() processed_content = content.upper() with open(output_file_path, write_mode) as f: f.write(processed_content) else: with open_func(input_file_path, mode, encoding=\'utf-8\') as f: content = f.read() processed_content = content.upper() with open_func(output_file_path, write_mode, encoding=\'utf-8\') as f: f.write(processed_content) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Understanding and Implementing Mapping Protocol Functions in Python** **Objective:** You are required to implement a class that mimics certain behaviors of Python\'s built-in dictionary using fundamental and advanced concepts of the Mapping Protocol described in the provided documentation. **Task:** 1. Create a class `CustomDict` that initializes a dictionary internally. 2. Implement the following methods: - `add_item(key: str, value: Any) -> None`: Adds a key-value pair to the dictionary. - `get_item(key: str) -> Any`: Retrieves the value for a given key. Raises a `KeyError` if the key does not exist. - `del_item(key: str) -> None`: Deletes a key-value pair from the dictionary. Raises a `KeyError` if the key does not exist. - `has_key(key: str) -> bool`: Checks if a key exists in the dictionary. - `size() -> int`: Returns the number of key-value pairs in the dictionary. - `keys() -> List[str]`: Returns a list of all keys in the dictionary. - `values() -> List[Any]`: Returns a list of all values in the dictionary. - `items() -> List[Tuple[str, Any]]`: Returns a list of all key-value pairs in the dictionary as tuples. **Constraints:** - The values in the dictionary can be of any data type. **Example Usage:** ```python custom_dict = CustomDict() custom_dict.add_item(\\"apple\\", 10) custom_dict.add_item(\\"banana\\", 20) print(custom_dict.get_item(\\"apple\\")) # Output: 10 print(custom_dict.has_key(\\"banana\\")) # Output: True print(custom_dict.size()) # Output: 2 custom_dict.del_item(\\"apple\\") print(custom_dict.has_key(\\"apple\\")) # Output: False print(custom_dict.keys()) # Output: [\'banana\'] print(custom_dict.values()) # Output: [20] print(custom_dict.items()) # Output: [(\'banana\', 20)] ``` **Notes:** - Make sure to handle possible exceptions such as `KeyError` appropriately in your implementation.","solution":"from typing import Any, List, Tuple class CustomDict: def __init__(self): self._dict = {} def add_item(self, key: str, value: Any) -> None: self._dict[key] = value def get_item(self, key: str) -> Any: if key in self._dict: return self._dict[key] else: raise KeyError(\\"Key not found\\") def del_item(self, key: str) -> None: if key in self._dict: del self._dict[key] else: raise KeyError(\\"Key not found\\") def has_key(self, key: str) -> bool: return key in self._dict def size(self) -> int: return len(self._dict) def keys(self) -> List[str]: return list(self._dict.keys()) def values(self) -> List[Any]: return list(self._dict.values()) def items(self) -> List[Tuple[str, Any]]: return list(self._dict.items())"},{"question":"**Resource Usage Monitor** **Objective:** Your task is to implement a Python function that monitors and manages the resource usage of a given process. You will use the `resource` module to achieve this. **Function Prototype:** ```python def monitor_resources(pid: int) -> dict: pass ``` **Input:** - `pid` (int): The process ID (PID) of the process whose resources you want to monitor. **Output:** - A dictionary with the following keys and corresponding values (all values should be floats or None if not applicable): - `\\"user_time\\"`: The amount of time the process has spent in user mode. - `\\"system_time\\"`: The amount of time the process has spent in kernel mode. - `\\"max_rss\\"`: The maximum resident set size used. - `\\"ix_rss\\"`: The amount of shared memory used. - `\\"id_rss\\"`: The amount of unshared data used. - `\\"is_rss\\"`: The amount of unshared stack used. - `\\"minflt\\"`: The number of page reclaims (soft page faults). - `\\"majflt\\"`: The number of page faults (hard page faults). - `\\"inblock\\"`: The number of block input operations. - `\\"oublock\\"`: The number of block output operations. - `\\"nvcsw\\"`: The number of voluntary context switches. - `\\"nivcsw\\"`: The number of involuntary context switches. **Constraints:** - You can assume that the given PID exists and is valid. - Your solution should only use the `resource` module and standard Python libraries. **Example:** ```python # Example usage: result = monitor_resources(1234) print(result) ``` Expected Output (format will vary based on the actual running conditions and the process): ```python { \\"user_time\\": 1.24, \\"system_time\\": 0.53, \\"max_rss\\": 20480, \\"ix_rss\\": 10240, \\"id_rss\\": 5120, \\"is_rss\\": 2560, \\"minflt\\": 400, \\"majflt\\": 2, \\"inblock\\": 50, \\"oublock\\": 45, \\"nvcsw\\": 100, \\"nivcsw\\": 75 } ``` **Notes:** - The actual values will depend on the runtime environment and process state. - Ensure your function handles the retrieval of these values correctly and returns the dictionary in the specified format.","solution":"import resource def monitor_resources(pid: int) -> dict: Monitors and manages the resource usage of a given process. Parameters: pid (int): The process ID (PID) of the process whose resources you want to monitor. Returns: dict: A dictionary with keys such as \\"user_time\\", \\"system_time\\", etc. and respective values. usage = resource.getrusage(resource.RUSAGE_SELF) # Using RUSAGE_SELF for demonstration # Converting the resource usage values to a dictionary resource_usage = { \\"user_time\\": usage.ru_utime, \\"system_time\\": usage.ru_stime, \\"max_rss\\": usage.ru_maxrss, \\"ix_rss\\": usage.ru_ixrss, \\"id_rss\\": usage.ru_idrss, \\"is_rss\\": usage.ru_isrss, \\"minflt\\": usage.ru_minflt, \\"majflt\\": usage.ru_majflt, \\"inblock\\": usage.ru_inblock, \\"oublock\\": usage.ru_oublock, \\"nvcsw\\": usage.ru_nvcsw, \\"nivcsw\\": usage.ru_nivcsw } return resource_usage # Note: This function retrieves resources for the current process (PID) using RUSAGE_SELF. # In practice, to monitor another process (PID), a system-specific approach or lower-level access # might be necessary as Python\'s resource module does not support monitoring other PIDs directly."},{"question":"# Python310 Sequence Manipulation Task **Objective:** Implement a class `SequenceManipulator` in Python that makes use of various sequence operation functions from the `python310` package to perform specific tasks. **Background:** You are given operations to manipulate sequence objects like lists and tuples. Your class should provide methods to check if an object is a sequence, get its size, retrieve and set items, and convert it to other collection types while managing errors appropriately. **Class Definition:** ```python class SequenceManipulator: def __init__(self): pass def is_sequence(self, obj): Return True if obj is a sequence, otherwise False. Equivalent to PySequence_Check. pass def get_size(self, seq): Return the size of the sequence. Raise ValueError on failure. Equivalent to PySequence_Size. pass def concat_sequences(self, seq1, seq2): Return the concatenation of seq1 and seq2. Handle errors and raise ValueError on failure. Equivalent to PySequence_Concat. pass def repeat_sequence(self, seq, count): Return the sequence repeated count times. Raise ValueError on failure. Equivalent to PySequence_Repeat. pass def get_item(self, seq, index): Return the item at the specified index in the sequence. Raise IndexError on failure. Equivalent to PySequence_GetItem. pass def set_item(self, seq, index, value): Set the item at the specified index in the sequence to value. Raise IndexError or ValueError on failure. Equivalent to PySequence_SetItem. pass def delete_item(self, seq, index): Delete the item at the specified index in the sequence. Raise IndexError on failure. Equivalent to PySequence_DelItem. pass def to_list(self, seq): Return a list with the same contents as the sequence. Raise ValueError on failure. Equivalent to PySequence_List. pass def to_tuple(self, seq): Return a tuple with the same contents as the sequence. Raise ValueError on failure. Equivalent to PySequence_Tuple. pass ``` # Requirements: - Implement each method by making use of Python\'s equivalent functionality as indicated in the provided comments. - You must handle errors appropriately, making sure to raise the specified exceptions when operations fail. - You can assume inputs are sequences where applicable unless checking methods like `is_sequence`. - Performance should be taken into account; use direct methods for efficient operations. **Input and Output Formats:** - Inputs will be typical Python list or tuple objects, along with other parameters where specified. - Outputs should be the result as specified in each method\'s comments, or appropriate exceptions should be raised for failure cases. # Constraints: - Assume sequences are either list or tuple types. - Do not use any external libraries; implement with standard Python functionality. - Focus on robust error handling and clear, efficient code. **Example Usage:** ```python seq_manip = SequenceManipulator() # Example checks print(seq_manip.is_sequence([1, 2, 3])) # Should return True print(seq_manip.get_size((1, 2, 3))) # Should return 3 print(seq_manip.concat_sequences([1, 2], [3, 4])) # Should return [1, 2, 3, 4] print(seq_manip.repeat_sequence([1, 2], 2)) # Should return [1, 2, 1, 2] print(seq_manip.get_item([1, 2, 3], 1)) # Should return 2 seq_manip.set_item([1, 2, 3], 1, 10) # Should modify the list to [1, 10, 3] seq_manip.delete_item([1, 2, 3], 1) # Should modify the list to [1, 3] print(seq_manip.to_list((1, 2, 3))) # Should return [1, 2, 3] print(seq_manip.to_tuple([1, 2, 3])) # Should return (1, 2, 3) ```","solution":"class SequenceManipulator: def __init__(self): pass def is_sequence(self, obj): Return True if obj is a sequence, otherwise False. Equivalent to PySequence_Check. return isinstance(obj, (list, tuple)) def get_size(self, seq): Return the size of the sequence. Raise ValueError on failure. Equivalent to PySequence_Size. if not self.is_sequence(seq): raise ValueError(\\"Object is not a sequence\\") return len(seq) def concat_sequences(self, seq1, seq2): Return the concatenation of seq1 and seq2. Handle errors and raise ValueError on failure. Equivalent to PySequence_Concat. if not self.is_sequence(seq1) or not self.is_sequence(seq2): raise ValueError(\\"Both objects must be sequences\\") return seq1 + seq2 def repeat_sequence(self, seq, count): Return the sequence repeated count times. Raise ValueError on failure. Equivalent to PySequence_Repeat. if not self.is_sequence(seq): raise ValueError(\\"Object is not a sequence\\") if not isinstance(count, int): raise ValueError(\\"Count must be an integer\\") return seq * count def get_item(self, seq, index): Return the item at the specified index in the sequence. Raise IndexError on failure. Equivalent to PySequence_GetItem. if not self.is_sequence(seq): raise ValueError(\\"Object is not a sequence\\") if not (0 <= index < len(seq)): raise IndexError(\\"Index out of range\\") return seq[index] def set_item(self, seq, index, value): Set the item at the specified index in the sequence to value. Raise IndexError or ValueError on failure. Equivalent to PySequence_SetItem. if not self.is_sequence(seq): raise ValueError(\\"Object is not a sequence\\") if not isinstance(seq, list): raise ValueError(\\"Only lists can be modified\\") if not (0 <= index < len(seq)): raise IndexError(\\"Index out of range\\") seq[index] = value def delete_item(self, seq, index): Delete the item at the specified index in the sequence. Raise IndexError on failure. Equivalent to PySequence_DelItem. if not self.is_sequence(seq): raise ValueError(\\"Object is not a sequence\\") if not isinstance(seq, list): raise ValueError(\\"Only lists can be modified\\") if not (0 <= index < len(seq)): raise IndexError(\\"Index out of range\\") del seq[index] def to_list(self, seq): Return a list with the same contents as the sequence. Raise ValueError on failure. Equivalent to PySequence_List. if not self.is_sequence(seq): raise ValueError(\\"Object is not a sequence\\") return list(seq) def to_tuple(self, seq): Return a tuple with the same contents as the sequence. Raise ValueError on failure. Equivalent to PySequence_Tuple. if not self.is_sequence(seq): raise ValueError(\\"Object is not a sequence\\") return tuple(seq)"},{"question":"# Question: Implement a Distributed Parameter Server with RPC in PyTorch You are tasked with implementing a simple distributed training setup using PyTorch\'s Distributed RPC Framework. Specifically, you will create a parameter server and a set of workers that will each perform computations and update the parameters on the server. Requirements: 1. **Master Node (Parameter Server)**: - Initialize the RPC framework. - Create a model and share it with worker nodes using RRefs. - Update model parameters based on gradients received from worker nodes. 2. **Worker Nodes**: - Initialize the RPC framework. - Fetch the model from the parameter server. - Perform forward and backward passes. - Send gradients back to the parameter server for parameter updates. 3. **Main Function**: - Initialize the RPC framework for both the parameter server and worker nodes. - Ensure the RPC framework is properly shut down after training is complete. Constraints: - Use **synchronous** RPC APIs to ensure the worker nodes wait for parameter updates. - Your implementation should handle at least one parameter server and two worker nodes. - The parameter server should update its model parameters after each gradient reception. - Implement basic error handling for RPC calls. Input: - Number of training iterations. Output: - Print the updated model parameters after training is complete. Implementation Details: 1. **Parameter Server Code**: ```python import torch import torch.distributed.rpc as rpc from torch import nn, optim global_model = nn.Linear(10, 2) # Example model def update_parameters(worker_id, gradients): Update parameters using gradients received from worker nodes with torch.no_grad(): for param, grad in zip(global_model.parameters(), gradients): param.grad = grad optimizer.step() def parameter_server(rank, world_size): # Initialize RPC rpc.init_rpc(f\\"ps\\", rank=rank, world_size=world_size) # Create optimizer global optimizer optimizer = optim.SGD(global_model.parameters(), lr=0.01) # Wait for all workers to complete rpc.shutdown() ``` 2. **Worker Node Code**: ```python import torch import torch.distributed.rpc as rpc def worker(rank, world_size, num_iterations, ps_rref): # Initialize RPC rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) for _ in range(num_iterations): # Fetch model from parameter server model_rref = ps_rref.remote().get_model().to_here() # Create input and target tensors inputs = torch.randn(32, 10) targets = torch.randint(0, 2, (32,)) # Perform forward pass outputs = model_rref(inputs) loss = nn.functional.cross_entropy(outputs, targets) # Perform backward pass loss.backward() # Send gradients to parameter server gradients = [param.grad for param in model_rref.parameters()] rpc.rpc_sync(f\\"ps\\", update_parameters, args=(rank, gradients)) rpc.shutdown() ``` 3. **Main Function**: ```python import os from torch.distributed.rpc import RpcBackendOptions def main(num_iterations): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' world_size = 3 ps_rank = 0 worker_ranks = list(range(1, world_size)) # Start parameter server and worker nodes ps_rref = None worker_tasks = [] for rank in range(world_size): if rank == ps_rank: ps_rref = rpc.remote( f\\"ps\\", parameter_server, args=(rank, world_size) ) else: worker_tasks.append( rpc.remote( f\\"worker{rank}\\", worker, args=(rank, world_size, num_iterations, ps_rref) ) ) # Wait for all workers to finish for task in worker_tasks: task.wait() # Shutdown RPC framework rpc.shutdown() # Print updated model parameters print(f\\"Updated model parameters: {global_model.parameters()}\\") if __name__ == \\"__main__\\": main(num_iterations=100) ``` Ensure that your code considers CUDA compatibility if running on GPU. Submit your complete implementation along with the output demonstrating the updated model parameters.","solution":"import os import torch import torch.nn as nn import torch.optim as optim import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef from concurrent.futures import ThreadPoolExecutor global_model = nn.Linear(10, 2) # Example model def get_model(): return global_model def update_parameters(worker_id, gradients): Update parameters using gradients received from worker nodes with torch.no_grad(): for param, grad in zip(global_model.parameters(), gradients): param.grad = grad optimizer.step() def parameter_server(rank, world_size): # Initialize RPC rpc.init_rpc(f\\"ps\\", rank=rank, world_size=world_size) # Create optimizer global optimizer optimizer = optim.SGD(global_model.parameters(), lr=0.01) # Wait for all workers to complete rpc.shutdown() def worker(rank, world_size, num_iterations, ps_rref): # Initialize RPC rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) for _ in range(num_iterations): model_rref = RRef(global_model) # Create input and target tensors inputs = torch.randn(32, 10) targets = torch.randint(0, 2, (32,)) # Perform forward pass outputs = model_rref.local_value()(inputs) loss = nn.functional.cross_entropy(outputs, targets) # Perform backward pass loss.backward() # Send gradients to parameter server gradients = [param.grad for param in global_model.parameters()] rpc.rpc_sync(\\"ps\\", update_parameters, args=(rank, gradients)) # Clear the gradients for the next iteration model_rref.local_value().zero_grad() rpc.shutdown() def main(num_iterations): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' world_size = 3 ps_rank = 0 worker_ranks = list(range(1, world_size)) # Initialize the parameter server ps_rref = RRef(global_model) # Start parameter server and worker nodes using a thread pool with ThreadPoolExecutor(max_workers=world_size) as executor: worker_tasks = [ executor.submit( parameter_server if rank == ps_rank else worker, rank, world_size, num_iterations, ps_rref ) for rank in range(world_size) ] # Wait for all futures to complete for task in worker_tasks: task.result() print(\\"Updated model parameters:\\") for param in global_model.parameters(): print(param) if __name__ == \\"__main__\\": main(num_iterations=100)"},{"question":"Context You are given several window functions in the `torch.signal.windows` module. These functions are commonly used in signal processing to taper signals before performing spectral analysis or filtering. They help in reducing spectral leakage when a signal is transformed to the frequency domain using techniques like the Fast Fourier Transform (FFT). Problem Statement You need to create a function that applies a specified window to a given 1D signal and then computes its FFT. The function should return the magnitude spectrum of the windowed signal. Function Signature ```python import torch def windowed_fft(signal: torch.Tensor, window_func_name: str, window_size: int) -> torch.Tensor: Applies the specified window function to the input signal and computes the magnitude spectrum of the windowed signal. Parameters: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_func_name (str): The name of the window function to apply. One of [\'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\']. - window_size (int): The size of the window to be applied to the signal. Returns: - torch.Tensor: A 1D tensor representing the magnitude spectrum of the windowed signal. pass ``` Details 1. The `signal` input will be a 1D tensor of arbitrary length. 2. The `window_func_name` will be a string matching one of the window functions in `torch.signal.windows`. 3. The `window_size` should be less than or equal to the length of the signal. 4. You must use the specified window function from `torch.signal.windows` to generate a window of the specified size. 5. Apply this window to the first `window_size` elements of the signal. 6. Compute the FFT of the windowed signal. 7. Return the magnitude spectrum (computed as the absolute value of the FFT result). Example ```python import torch.signal.windows as windows signal = torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0, 2.0, 1.0, 0.0]) window_func_name = \'hann\' window_size = 8 # You will be using the hann window from torch.signal.windows module # and applying it to the first 8 elements of the signal. # Expected output: A tensor representing the magnitude spectrum of the windowed signal. result = windowed_fft(signal, window_func_name, window_size) print(result) ``` Constraints - Ensure that the window function specified in `window_func_name` exists in the `torch.signal.windows` module. - The function should raise an error if the window function specified is not found. - Handle all edge cases e.g., if `window_size` is less than or equal to zero. Notes - Make sure to import the required modules and functions at the beginning of your implementation. - Document your code clearly and include comments explaining each significant step.","solution":"import torch import torch.fft import torch.signal.windows as windows def windowed_fft(signal: torch.Tensor, window_func_name: str, window_size: int) -> torch.Tensor: Applies the specified window function to the input signal and computes the magnitude spectrum of the windowed signal. Parameters: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_func_name (str): The name of the window function to apply. One of [\'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\']. - window_size (int): The size of the window to be applied to the signal. Returns: - torch.Tensor: A 1D tensor representing the magnitude spectrum of the windowed signal. # Ensure window_size is valid if window_size <= 0 or window_size > len(signal): raise ValueError(\\"window_size must be greater than 0 and less than or equal to the length of the signal.\\") # Get the window function from torch.signal.windows if not hasattr(windows, window_func_name): raise ValueError(f\\"Invalid window function name: {window_func_name}\\") window_func = getattr(windows, window_func_name) # Generate the window window_tensor = window_func(window_size) # Apply the window to the first `window_size` elements of the signal windowed_signal = signal[:window_size] * window_tensor # Compute the FFT of the windowed signal fft_result = torch.fft.fft(windowed_signal) # Compute the magnitude spectrum (absolute value of the FFT result) magnitude_spectrum = torch.abs(fft_result) return magnitude_spectrum"},{"question":"# Asynchronous File Processing **Objective:** Implement an asynchronous function to process multiple files concurrently using the asyncio library in Python. This exercise will test your understanding of coroutines, task creation, and concurrent execution. **Problem Statement:** You are given a list of file paths, each containing textual data. Your task is to create an asynchronous function `process_files(file_paths: List[str]) -> List[int]` that processes each file concurrently and returns the number of lines in each file. Here\'s the detailed breakdown of the task: 1. **Read Files Concurrently**: - Create a coroutine `async def read_file(file_path: str) -> int` that reads a file and returns the number of lines it contains. 2. **Process Multiple Files Together**: - Use `asyncio.gather` to concurrently execute multiple read tasks. 3. **Main Processing Function**: - Implement the `process_files(file_paths: List[str]) -> List[int]` function that orchestrates reading multiple files concurrently and collects the line counts for each file. **Input and Output**: - The input will be a list of file paths (`List[str]`). - The output should be a list of integers (`List[int]`), where each integer represents the number of lines in the corresponding file from the input list. **Constraints**: - Assume that the file paths provided are valid and the files are accessible. - Each file can have a variable number of lines. **Example**: ```python import asyncio from typing import List async def read_file(file_path: str) -> int: A coroutine that reads a file and returns the number of lines it contains. # Replace this with the actual file reading implementation # For simplicity in this example, assuming the function is defined correctly pass async def process_files(file_paths: List[str]) -> List[int]: Process multiple files concurrently and return the number of lines in each file. # Create tasks for reading each file tasks = [asyncio.create_task(read_file(file_path)) for file_path in file_paths] # Gather the results from all tasks line_counts = await asyncio.gather(*tasks) return line_counts # Example usage: file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] output = asyncio.run(process_files(file_paths)) print(output) # Output should be an example list of line counts, e.g., [100, 50, 75] ``` **Instructions**: 1. Implement the `read_file` coroutine that reads and returns the number of lines in a file. 2. Implement the `process_files` coroutine that creates tasks for each file reading operation and gathers the results. 3. Ensure the code handles file I/O asynchronously and processes files concurrently. **Note**: The `read_file` coroutine should utilize asynchronous file reading to ensure non-blocking behavior.","solution":"import asyncio from typing import List async def read_file(file_path: str) -> int: A coroutine that reads a file and returns the number of lines it contains. line_count = 0 async with aiofiles.open(file_path, \'r\') as file: async for _ in file: line_count += 1 return line_count async def process_files(file_paths: List[str]) -> List[int]: Process multiple files concurrently and return the number of lines in each file. # Create tasks for reading each file tasks = [read_file(file_path) for file_path in file_paths] # Gather the results from all tasks line_counts = await asyncio.gather(*tasks) return line_counts"},{"question":"**Pandas Proficiency Test** **Problem Statement:** You are tasked with analyzing a dataset containing information about different products available in a store. You are required to process and analyze the data using pandas functionalities. You will be provided with two dictionaries representing the product dataset. The goal is to transform and analyze these dictionaries using pandas `Series` and `DataFrame`. # Input: 1. Two dictionaries: - `prices_dict`: Dictionary containing products and their respective prices. - `quantity_dict`: Dictionary containing products and the quantity available. ```python prices_dict = { \'apple\': 1.2, \'banana\': 0.5, \'cherry\': 2.5, \'date\': 3.0, \'elderberry\': 1.8 } quantity_dict = { \'apple\': 10, \'banana\': 20, \'cherry\': 5, \'date\': 12, \'elderberry\': 15 } ``` # Expected Output: 1. Using the provided dictionaries, you need to create a pandas `DataFrame` with the following steps and operations: **Step 1: Create Series** - Create two pandas `Series` named `prices` and `quantity` from `prices_dict` and `quantity_dict`. **Step 2: Create DataFrame** - Merge these two Series objects into a single `DataFrame` named `df` with columns \'prices\' and \'quantity\'. The index of the DataFrame should be the product names. **Step 3: Calculate Total Price** - Add a new column named `total_price` to `df`, which is the product of `prices` and `quantity`. **Step 4: Data Filtering** - Filter the `DataFrame` to include only products where the total price is greater than 10 and store it in `filtered_df`. **Step 5: Result Summary** - Return the `filtered_df` and a dictionary summary containing: - \'most_expensive_product\': The name of the product with the highest price. - \'total_inventory_value\': The sum of total prices of all products in the `filtered_df`. # Implementation: ```python import pandas as pd def analyze_product_data(prices_dict, quantity_dict): # Step 1: Create Series prices = pd.Series(prices_dict) quantity = pd.Series(quantity_dict) # Step 2: Create DataFrame df = pd.DataFrame({\'prices\': prices, \'quantity\': quantity}) # Step 3: Calculate Total Price df[\'total_price\'] = df[\'prices\'] * df[\'quantity\'] # Step 4: Data Filtering filtered_df = df[df[\'total_price\'] > 10] # Step 5: Result Summary most_expensive_product = df[\'prices\'].idxmax() total_inventory_value = filtered_df[\'total_price\'].sum() result_summary = { \'most_expensive_product\': most_expensive_product, \'total_inventory_value\': total_inventory_value } return filtered_df, result_summary # Example Usage prices_dict = { \'apple\': 1.2, \'banana\': 0.5, \'cherry\': 2.5, \'date\': 3.0, \'elderberry\': 1.8 } quantity_dict = { \'apple\': 10, \'banana\': 20, \'cherry\': 5, \'date\': 12, \'elderberry\': 15 } filtered_df, result_summary = analyze_product_data(prices_dict, quantity_dict) print(filtered_df) print(result_summary) ``` # Constraints: - Ensure your solution is efficient and uses vectorized operations where possible. - Handle any potential exceptions, especially when dealing with missing data or mismatched indexes. # Notes: - This problem requires a good understanding of creating and manipulating pandas `Series` and `DataFrame`. - You are expected to use pandas methods efficiently to achieve the desired results.","solution":"import pandas as pd def analyze_product_data(prices_dict, quantity_dict): Analyze product data to create a filtered dataframe and summary. Args: prices_dict (dict): Dictionary of product prices. quantity_dict (dict): Dictionary of product quantities. Returns: pd.DataFrame: Filtered dataframe with total prices greater than 10. dict: Summary containing most expensive product and total inventory value. # Step 1: Create Series prices = pd.Series(prices_dict) quantity = pd.Series(quantity_dict) # Step 2: Create DataFrame df = pd.DataFrame({\'prices\': prices, \'quantity\': quantity}) # Step 3: Calculate Total Price df[\'total_price\'] = df[\'prices\'] * df[\'quantity\'] # Step 4: Data Filtering filtered_df = df[df[\'total_price\'] > 10] # Step 5: Result Summary most_expensive_product = df[\'prices\'].idxmax() total_inventory_value = filtered_df[\'total_price\'].sum() result_summary = { \'most_expensive_product\': most_expensive_product, \'total_inventory_value\': total_inventory_value } return filtered_df, result_summary # Example usage prices_dict = { \'apple\': 1.2, \'banana\': 0.5, \'cherry\': 2.5, \'date\': 3.0, \'elderberry\': 1.8 } quantity_dict = { \'apple\': 10, \'banana\': 20, \'cherry\': 5, \'date\': 12, \'elderberry\': 15 } filtered_df, result_summary = analyze_product_data(prices_dict, quantity_dict) print(filtered_df) print(result_summary)"},{"question":"# Multi-Threaded Task Scheduling with Priority Queue Objective: Implement a multi-threaded task scheduler using the `PriorityQueue` from the `queue` module. The scheduler will manage tasks with different priorities and ensure that higher priority tasks are executed before lower priority tasks. Each task will be represented by a function that prints its name and priority when executed. Requirements: 1. Implement a `TaskScheduler` class with the following methods: - `add_task(priority: int, task_func: Callable[[], None])`: Adds a task to the scheduler with the given priority. - `start_workers(num_workers: int)`: Starts the specified number of worker threads that will process tasks from the priority queue. - `stop_workers()`: Stops all worker threads gracefully after completing all tasks. - `get_status() -> Dict[str, Any]`: Returns the current status of the scheduler including number of tasks in the queue, number of tasks completed, and number of active workers. 2. Ensure that the scheduler correctly handles synchronization between producers (adding tasks) and consumers (worker threads). 3. Implement error handling for adding tasks to a full queue and retrieving tasks from an empty queue. Constraints: - Assume `priority` is an integer where a lower value indicates higher priority. - Use appropriate locking mechanisms to ensure thread safety. - Ensure the solution works efficiently even with a large number of tasks and threads. Example Usage: ```python import time from queue import PriorityQueue from threading import Thread, Event from typing import Callable, Dict, Any class TaskScheduler: def __init__(self, maxsize: int = 0): self.task_queue = PriorityQueue(maxsize=maxsize) self.workers = [] self.stop_event = Event() self.tasks_completed = 0 self.lock = threading.Lock() def add_task(self, priority: int, task_func: Callable[[], None]): try: self.task_queue.put_nowait((priority, task_func)) except queue.Full: print(\\"Failed to add task: Queue is full\\") def start_workers(self, num_workers: int): self.stop_event.clear() for _ in range(num_workers): worker = Thread(target=self._worker) worker.start() self.workers.append(worker) def stop_workers(self): self.stop_event.set() for worker in self.workers: worker.join() self.workers.clear() def _worker(self): while not self.stop_event.is_set(): try: priority, task_func = self.task_queue.get(timeout=1) try: task_func() # Run the task with self.lock: self.tasks_completed += 1 finally: self.task_queue.task_done() except queue.Empty: continue def get_status(self) -> Dict[str, Any]: with self.lock: return { \\"tasks_in_queue\\": self.task_queue.qsize(), \\"tasks_completed\\": self.tasks_completed, \\"active_workers\\": len(self.workers) } # Example task function def example_task(task_name: str, priority: int): print(f\\"Executing task: {task_name} with priority: {priority}\\") time.sleep(1) # Simulate work # Usage scheduler = TaskScheduler(maxsize=10) scheduler.add_task(1, lambda: example_task(\\"Task A\\", 1)) scheduler.add_task(2, lambda: example_task(\\"Task B\\", 2)) scheduler.start_workers(2) time.sleep(5) # Let workers process tasks scheduler.stop_workers() print(scheduler.get_status()) ``` Notes: - Ensure your implementation is thread-safe and efficient. - Test your solution with various numbers of tasks and worker threads to ensure robustness.","solution":"import threading from queue import PriorityQueue, Empty, Full from threading import Thread, Event from typing import Callable, Dict, Any class TaskScheduler: def __init__(self, maxsize: int = 0): self.task_queue = PriorityQueue(maxsize=maxsize) self.workers = [] self.stop_event = Event() self.tasks_completed = 0 self.lock = threading.Lock() def add_task(self, priority: int, task_func: Callable[[], None]): try: self.task_queue.put_nowait((priority, task_func)) except Full: print(\\"Failed to add task: Queue is full\\") def start_workers(self, num_workers: int): self.stop_event.clear() for _ in range(num_workers): worker = Thread(target=self._worker) worker.start() self.workers.append(worker) def stop_workers(self): self.stop_event.set() for worker in self.workers: worker.join() self.workers.clear() def _worker(self): while not self.stop_event.is_set(): try: priority, task_func = self.task_queue.get(timeout=1) try: task_func() # Run the task with self.lock: self.tasks_completed += 1 finally: self.task_queue.task_done() except Empty: continue def get_status(self) -> Dict[str, Any]: with self.lock: return { \\"tasks_in_queue\\": self.task_queue.qsize(), \\"tasks_completed\\": self.tasks_completed, \\"active_workers\\": len(self.workers) } # Example task function for testing def example_task(task_name: str, priority: int): print(f\\"Executing task: {task_name} with priority: {priority}\\") # Simulate work with time.sleep(1)"},{"question":"Objective Design and implement a Python function that reads data from a file, processes the data, and writes the results to another file. You must handle various exceptions that may occur during this process, ensure that resources like files are properly cleaned up, and define a custom exception for a specific type of data error. Task 1. Write a function `process_file(input_file: str, output_file: str) -> None` that: - Reads integers from `input_file` where each integer is on a separate line. - Calculates the square of each integer. - Writes the squared values to `output_file`, each on a new line. - Handles the following exceptions: - If the `input_file` does not exist, raise an `OSError` with an appropriate message. - If the `input_file` contains any non-integer values, raise a custom exception `NonIntegerError` with an appropriate message. - If there are any issues writing to `output_file`, handle the `OSError`. 2. You must define the custom exception `NonIntegerError` and use it to indicate the presence of a non-integer value in the `input_file`. 3. Ensure proper cleanup of resources in case of any exceptions using the `with` statement. Constraints - The function should not use any external libraries other than the Python Standard Library. - The function should handle large files efficiently. - Use appropriate comments and error messages for clarity. Example Suppose the content of `input.txt` is: ``` 2 4 x 8 ``` Calling `process_file(\'input.txt\', \'output.txt\')` should raise a `NonIntegerError` due to the presence of \'x\' in the file. If `input.txt` contains valid integers: ``` 2 4 6 8 ``` Calling `process_file(\'input.txt\', \'output.txt\')` should create/write: ``` 4 16 36 64 ``` to `output.txt`. Expected Function Signature ```python class NonIntegerError(Exception): pass def process_file(input_file: str, output_file: str) -> None: # Your implementation here pass ```","solution":"class NonIntegerError(Exception): Custom exception for non-integer values in the input file. def __init__(self, message=\\"File contains non-integer values\\"): super().__init__(message) def process_file(input_file: str, output_file: str) -> None: Reads integers from input_file, calculates their squares, and writes the squares to output_file. Arguments: input_file -- Path to the input file containing integers. output_file -- Path to the output file where squared values are written. Raises: OSError -- If there is an issue with file reading/writing. NonIntegerError -- If the input file contains non-integer values. try: # Read all lines from the input file with open(input_file, \'r\') as infile: lines = infile.readlines() # Process the lines to calculate squares squared_values = [] for line in lines: line = line.strip() if not line.isdigit(): raise NonIntegerError(f\\"Non-integer value encountered: {line}\\") squared_values.append(int(line) ** 2) # Write the squared values to the output file with open(output_file, \'w\') as outfile: for value in squared_values: outfile.write(f\\"{value}n\\") except FileNotFoundError: raise OSError(f\\"Input file \'{input_file}\' does not exist.\\") except OSError as e: raise OSError(f\\"An error occurred while processing the file: {str(e)}\\")"},{"question":"Problem Statement You are tasked with creating a type-safe event handling system using Python\'s `typing` module. # Objectives 1. **Define Event Type Alias**: - Create a type alias `Event` which is a dictionary containing: - A key `\\"type\\"` with a value of type `str` - A key `\\"data\\"` with a value of type `Dict[str, Any]`. 2. **Define Handler Type Alias**: - Define a type alias `Handler` which is a `Callable` that: - Takes a single argument of an `Event` - Returns `None`. 3. **Implement the EventHandler Class**: - Create a generic class `EventHandler`, which is parameterized over a type variable `T` bound to `Handler`. - Define a method within the class to register handlers. - Define another method to trigger handlers based on event type. # Detailed Requirements 1. **Type Alias Definitions**: - Define `Event` and `Handler` using respective type aliases. 2. **EventHandler Class**: - Define a class `EventHandler` using `Generic[T]`. - Implement the following methods: - `register_handler(type: str, handler: T) -> None`: Registers a handler for a specific event type. - `trigger_event(event: Event) -> None`: Triggers all handlers registered for the event type present in the `event`. Handlers should be called with the event as the argument. 3. **Type Safety**: - Use appropriate type hints and enforce type safety. - Ensure that only handlers of correct signature are registered and triggered. # Example Usage ```python from typing import TypeAlias, Dict, Any, Callable, Generic, TypeVar, List # Step 1: Type Alias Definitions Event: TypeAlias = Dict[str, Any] Handler: TypeAlias = Callable[[Event], None] T = TypeVar(\'T\', bound=Handler) # Step 2: EventHandler Class class EventHandler(Generic[T]): def __init__(self): self.handlers: Dict[str, List[T]] = {} def register_handler(self, type: str, handler: T) -> None: if type not in self.handlers: self.handlers[type] = [] self.handlers[type].append(handler) def trigger_event(self, event: Event) -> None: event_type = event.get(\\"type\\") if not event_type: return if event_type in self.handlers: for handler in self.handlers[event_type]: handler(event) # Step 3: Usage Example def log_event(event: Event) -> None: print(f\\"Logging event: {event}\\") def process_event(event: Event) -> None: print(f\\"Processing event data: {event[\'data\']}\\") handler = EventHandler[Handler]() handler.register_handler(\\"log\\", log_event) handler.register_handler(\\"process\\", process_event) event = { \\"type\\": \\"log\\", \\"data\\": { \\"message\\": \\"This is a test.\\" } } handler.trigger_event(event) ``` # Constraints - Use only the constructs defined in the `typing` module for type aliases, generic classes, and handlers. - Ensure the solution is type-safe and leverages type hints properly. **Note**: Using advanced typing constructs and generics will be key to demonstrating thorough understanding and proper implementation of type-safe function handlers.","solution":"from typing import TypeAlias, Dict, Any, Callable, Generic, TypeVar, List # Step 1: Type Alias Definitions Event: TypeAlias = Dict[str, Any] Handler: TypeAlias = Callable[[Event], None] T = TypeVar(\'T\', bound=Handler) # Step 2: EventHandler Class class EventHandler(Generic[T]): def __init__(self): self.handlers: Dict[str, List[T]] = {} def register_handler(self, type: str, handler: T) -> None: if type not in self.handlers: self.handlers[type] = [] self.handlers[type].append(handler) def trigger_event(self, event: Event) -> None: event_type = event.get(\\"type\\") if not event_type: return if event_type in self.handlers: for handler in self.handlers[event_type]: handler(event)"},{"question":"# **Problem Statement** You have been hired as a data scientist to analyze a dataset containing information about various health factors across different countries. The dataset captures the following variables: - `country`: The name of the country. - `year`: The year when the data was recorded. - `life_expectancy`: Life expectancy in years. - `gdp`: Gross Domestic Product in USD. Your task is to perform the following sub-tasks using seaborn to create the required visualizations: 1. Load the dataset into a pandas DataFrame. For this task, given a URL `data_url`, assume that the data can be loaded using `pd.read_csv(data_url)`. 2. Create a long-form data plot showing the trend of `life_expectancy` over `year` for each country. Each country should be represented by a separate hue. 3. Transform the dataset into a wide-form format. The `life_expectancy` should be pivoted to form columns for each `year`. 4. Generate a line plot using the wide-form data showing the trend of `life_expectancy` for each country. 5. Handle any missing values in the dataset by filling them with the forward fill method. 6. Finally, create a single plot that showcases how `gdp` affects `life_expectancy` over the years for all countries using a scatter plot. Differentiate countries using different hues and add a regression line to depict the trend. # **Input Format** ```python data_url = \\"https://example.com/health_factors.csv\\" ``` # **Output Format** Perform the following steps in sequence to generate the necessary plots. Provide comments in your code to describe each step. # **Constraints** - Use seaborn version 0.13.0 or higher. - Ensure that the `gdp` and `life_expectancy` values are numeric. Handle any non-numeric data appropriately. - Plot titles and axis labels should be informative and clear. # **Solution Template** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Set seaborn theme sns.set_theme() # Step 1: Load the dataset data_url = \\"https://example.com/health_factors.csv\\" df = pd.read_csv(data_url) # Step 2: Create long-form data plot for life expectancy trend plt.figure(figsize=(12, 6)) sns.lineplot(data=df, x=\\"year\\", y=\\"life_expectancy\\", hue=\\"country\\") plt.title(\'Life Expectancy Trend Over Years by Country\') plt.xlabel(\'Year\') plt.ylabel(\'Life Expectancy\') plt.show() # Step 3: Transform to wide-form format df_wide = df.pivot(index=\\"country\\", columns=\\"year\\", values=\\"life_expectancy\\") # Step 4: Wide-form line plot plt.figure(figsize=(12, 6)) sns.lineplot(data=df_wide.T) plt.title(\'Life Expectancy Trend Over Years (Wide-form)\') plt.xlabel(\'Year\') plt.ylabel(\'Life Expectancy\') plt.legend(title=\'Country\', loc=\'upper left\', labels=df_wide.index) plt.show() # Step 5: Handle missing values df.fillna(method=\'ffill\', inplace=True) # Step 6: Scatter plot showing GDP effect on Life Expectancy plt.figure(figsize=(12, 6)) sns.scatterplot(data=df, x=\\"gdp\\", y=\\"life_expectancy\\", hue=\\"country\\") sns.regplot(data=df, x=\\"gdp\\", y=\\"life_expectancy\\", scatter=False, color=\\"red\\") plt.title(\'GDP vs Life Expectancy Over Years by Country\') plt.xlabel(\'GDP (USD)\') plt.ylabel(\'Life Expectancy\') plt.show() ``` Complete the necessary steps to load the data, transform it, and generate the visualizations as described. Ensure all plots are well-labeled and provide meaningful insights into the dataset.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def analyze_health_factors(data_url): # Set seaborn theme sns.set_theme() # Step 1: Load the dataset df = pd.read_csv(data_url) # Ensure numeric types for \'gdp\' and \'life_expectancy\' df[\'gdp\'] = pd.to_numeric(df[\'gdp\'], errors=\'coerce\') df[\'life_expectancy\'] = pd.to_numeric(df[\'life_expectancy\'], errors=\'coerce\') # Step 2: Create long-form data plot for life expectancy trend plt.figure(figsize=(12, 6)) sns.lineplot(data=df, x=\\"year\\", y=\\"life_expectancy\\", hue=\\"country\\") plt.title(\'Life Expectancy Trend Over Years by Country\') plt.xlabel(\'Year\') plt.ylabel(\'Life Expectancy\') plt.show() # Step 3: Transform to wide-form format df_wide = df.pivot(index=\\"country\\", columns=\\"year\\", values=\\"life_expectancy\\") # Step 4: Wide-form line plot plt.figure(figsize=(12, 6)) sns.lineplot(data=df_wide.T) plt.title(\'Life Expectancy Trend Over Years (Wide-form)\') plt.xlabel(\'Year\') plt.ylabel(\'Life Expectancy\') plt.legend(title=\'Country\', loc=\'upper left\', labels=df_wide.index) plt.show() # Step 5: Handle missing values df.fillna(method=\'ffill\', inplace=True) # Step 6: Scatter plot showing GDP effect on Life Expectancy plt.figure(figsize=(12, 6)) sns.scatterplot(data=df, x=\\"gdp\\", y=\\"life_expectancy\\", hue=\\"country\\") sns.regplot(data=df, x=\\"gdp\\", y=\\"life_expectancy\\", scatter=False, color=\\"red\\") plt.title(\'GDP vs Life Expectancy Over Years by Country\') plt.xlabel(\'GDP (USD)\') plt.ylabel(\'Life Expectancy\') plt.show() # Return the transformed dataframe for testing purposes return df, df_wide"},{"question":"# Objective You are provided with a dataset of employees stored in a pandas DataFrame. Your task is to manipulate this data following the Copy-on-Write (CoW) principles introduced in pandas 3.0. # Dataset Create the following DataFrame: ```python import pandas as pd data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"], \\"Age\\": [25, 30, 35, 40], \\"Department\\": [\\"HR\\", \\"Finance\\", \\"IT\\", \\"Marketing\\"], \\"Salary\\": [50000, 60000, 70000, 80000], } df = pd.DataFrame(data) ``` # Task 1. **Select and Modify a Subset**: - Select the \\"Age\\" column and store it in a new variable. - Increase the age of the first employee by 5 years. - Ensure that modifying the new variable does not affect the original DataFrame `df`. 2. **Avoid Chained Assignment**: - Double the salary of employees who are in the \\"IT\\" department. - Use a method that complies with CoW principles and avoids chained assignments. 3. **Inplace Operations**: - Replace the \\"Finance\\" department with \\"Accounting\\". - Perform this replacement inline without violating CoW rules. 4. **Work with NumPy Arrays**: - Extract the underlying NumPy array from the \\"Salary\\" column. - Convert this array to be writeable. - Double the salary values in this array. - Ensure the original DataFrame\'s salary values are not modified. # Constraints - Ensure that your code adheres strictly to pandas Copy-on-Write principles. - Use pandas version >= 3.0.0 for this task. - Avoid using deprecated methods or functionalities. # Expected Output - Example After you perform all tasks, print the following: 1. Original DataFrame `df` 2. Modified \\"Age\\" series 3. Modified DataFrame `df` after salary update in \\"IT\\" 4. Modified DataFrame `df` after department replacement 5. Original and modified NumPy arrays extracted from \\"Salary\\" # Solution Template ```python import pandas as pd # Create DataFrame data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"], \\"Age\\": [25, 30, 35, 40], \\"Department\\": [\\"HR\\", \\"Finance\\", \\"IT\\", \\"Marketing\\"], \\"Salary\\": [50000, 60000, 70000, 80000], } df = pd.DataFrame(data) Your Code Here # 1. Select and Modify a Subset age_series = df[\\"Age\\"].copy() age_series.iloc[0] += 5 print(\\"Modified Age Series:\\") print(age_series) print(\\"nOriginal DataFrame:\\") print(df) # 2. Avoid Chained Assignment df.loc[df[\\"Department\\"] == \\"IT\\", \\"Salary\\"] *= 2 print(\\"nDataFrame after IT Salary Update:\\") print(df) # 3. Inplace Operations df[\\"Department\\"] = df[\\"Department\\"].replace(\\"Finance\\", \\"Accounting\\") print(\\"nDataFrame after Department Replacement:\\") print(df) # 4. Work with NumPy Arrays salary_array = df[\\"Salary\\"].to_numpy() salary_array.setflags(write=1) salary_array *= 2 print(\\"nModified NumPy Array (Salary):\\") print(salary_array) print(\\"nOriginal DataFrame Salary after NumPy Modification:\\") print(df) ```","solution":"import pandas as pd import numpy as np # Create DataFrame data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"], \\"Age\\": [25, 30, 35, 40], \\"Department\\": [\\"HR\\", \\"Finance\\", \\"IT\\", \\"Marketing\\"], \\"Salary\\": [50000, 60000, 70000, 80000], } df = pd.DataFrame(data) # 1. Select and Modify a Subset age_series = df[\\"Age\\"].copy() age_series.iloc[0] += 5 # 2. Avoid Chained Assignment df.loc[df[\\"Department\\"] == \\"IT\\", \\"Salary\\"] *= 2 # 3. Inplace Operations df[\\"Department\\"].replace(\\"Finance\\", \\"Accounting\\", inplace=True) # 4. Work with NumPy Arrays salary_array = df[\\"Salary\\"].to_numpy().copy() salary_array *= 2 # Print Outputs print(\\"Original DataFrame:\\") print(df) print(\\"nModified Age Series:\\") print(age_series) print(\\"nDataFrame after IT Salary Update:\\") print(df) print(\\"nDataFrame after Department Replacement:\\") print(df) print(\\"nOriginal NumPy Array (Salary):\\") print(df[\\"Salary\\"].to_numpy()) print(\\"nModified NumPy Array (Salary):\\") print(salary_array)"},{"question":"**Objective:** Implement a utility function that leverages the `uu` module to encode and decode files while handling various edge cases related to file operations and permissions. **Task:** Create a Python function `encode_and_decode_file` that: 1. Encodes an input file to a specified output file using uuencode. 2. Decodes the encoded file back to its original form. 3. Validates that the content of the decoded file matches the original input file. **Function Signature:** ```python def encode_and_decode_file(input_file_path: str, encoded_file_path: str, decoded_file_path: str) -> bool: Encodes the given input file to uuencoded format and then decodes it back to verify the content integrity. Parameters: - input_file_path (str): The path to the input file to be encoded. - encoded_file_path (str): The path where the uuencoded file will be saved. - decoded_file_path (str): The path where the decoded file will be saved. Returns: - bool: True if the decoded file matches the original input file, False otherwise. ``` **Requirements:** 1. **Encoding**: Use `uu.encode` to encode the input file. 2. **Decoding**: Use `uu.decode` to decode the uuencoded file. 3. **File Integrity Check**: Read the original and decoded files to compare their content. Return `True` if they match, otherwise return `False`. 4. **Error Handling**: Appropriately handle file-related errors and exceptions. The function should raise a user-defined exception `FileEncodingDecodingError` if any file operation fails. **Constraints:** - Assume the input file is a small text file. - Use default parameters for encoding and decoding where possible. - Ensure proper cleanup of file resources (e.g., closing files). **Example Usage:** ```python try: result = encode_and_decode_file(\'input.txt\', \'encoded.txt\', \'decoded.txt\') if result: print(\\"The file was encoded and decoded successfully.\\") else: print(\\"The decoded file does not match the original.\\") except FileEncodingDecodingError as e: print(f\\"An error occurred: {e}\\") ``` **Additional Details:** - The function should be self-contained and not rely on any external libraries apart from the standard `uu` module. - Consider edge cases such as missing files, permission errors, and incorrect formats. **Performance Requirements:** - Performance is not a critical aspect as the files being handled are small text files. **Hint:** - You can use the `open` function with appropriate modes (\'rb\' for reading binary, \'wb\' for writing binary) to handle file operations. Good luck!","solution":"import uu import os class FileEncodingDecodingError(Exception): Custom exception for errors in file encoding/decoding process. pass def encode_and_decode_file(input_file_path: str, encoded_file_path: str, decoded_file_path: str) -> bool: try: # Encode the input file with open(input_file_path, \'rb\') as input_file: with open(encoded_file_path, \'wb\') as output_file: uu.encode(input_file, output_file) # Decode back to the original file with open(encoded_file_path, \'rb\') as input_file: with open(decoded_file_path, \'wb\') as output_file: uu.decode(input_file, output_file) # Validate that the original and decoded files are identical with open(input_file_path, \'rb\') as original_file: with open(decoded_file_path, \'rb\') as decoded_file: original_content = original_file.read() decoded_content = decoded_file.read() return original_content == decoded_content except Exception as e: raise FileEncodingDecodingError(f\\"An error occurred during file encoding/decoding: {e}\\") finally: # Clean up the encoded file (you might not always want this) if os.path.exists(encoded_file_path): os.remove(encoded_file_path)"},{"question":"# Advanced Coding Assessment Question: Manipulating and Validating Slice Objects Objective The goal is to implement a function that validates a slice object and extracts meaningful information from it, similar to the functionalities provided in the given documentation. Problem Statement Write a Python function `slice_summary(seq, slice_obj)` that takes: - `seq`: a sequence (list, tuple, etc.) on which the slicing is to be performed. - `slice_obj`: a slice object to be applied to the sequence. The function should validate the `slice_obj` and provide a detailed summary of the slicing operation. Expected Function Signature ```python def slice_summary(seq, slice_obj): pass ``` Requirements 1. If `slice_obj` is not a valid slice object, raise a `ValueError` with the message `\\"Invalid slice object\\"`. 2. Extract the start, stop, and step from the slice object and ensure they lie within the bounds of the given sequence. 3. Clip the out-of-bounds indices to fit within the sequence\'s length. 4. Return a dictionary with the following keys: - `\\"start\\"`: Start index of the slice. - `\\"stop\\"`: Stop index of the slice. - `\\"step\\"`: Step value of the slice. - `\\"sliced_sequence\\"`: The resultant sequence after slicing. Constraints - The sequence (`seq`) can be any Python sequence type (e.g. list, tuple). - The function should handle cases where any of the slice components (`start`, `stop`, `step`) are `None`. - Assume sequence length will not exceed 10000 elements. Example Usage ```python # Example 1 seq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] slice_obj = slice(1, 9, 2) print(slice_summary(seq, slice_obj)) # Output: {\'start\': 1, \'stop\': 9, \'step\': 2, \'sliced_sequence\': [2, 4, 6, 8]} # Example 2 seq = (\'a\', \'b\', \'c\', \'d\', \'e\', \'f\') slice_obj = slice(None, 4) print(slice_summary(seq, slice_obj)) # Output: {\'start\': 0, \'stop\': 4, \'step\': 1, \'sliced_sequence\': (\'a\', \'b\', \'c\', \'d\')} # Example 3 seq = [10, 20, 30, 40, 50] slice_obj = slice(0, None, -1) print(slice_summary(seq, slice_obj)) # Output: {\'start\': -1, \'stop\': None, \'step\': -1, \'sliced_sequence\': [50, 40, 30, 20, 10]} ``` Notes - The `start`, `stop`, and `step` values in the returned dictionary should reflect the actual values that will be used to slice the sequence, which might be adjusted for bounds. - Ensure your code is well-documented and handles edge cases gracefully.","solution":"def slice_summary(seq, slice_obj): Returns a summary of the slicing operation on the given sequence. Parameters: seq: any python sequence type (e.g., list or tuple). slice_obj: a slice object to be applied to the sequence. Returns: dict: a dictionary containing the start, stop, step, and the sliced sequence. if not isinstance(slice_obj, slice): raise ValueError(\\"Invalid slice object\\") start, stop, step = slice_obj.start, slice_obj.stop, slice_obj.step sequence_length = len(seq) # Handling \'None\' values by converting them to appropriate indices start = 0 if start is None else start stop = sequence_length if stop is None else stop step = 1 if step is None else step # Clip to bounds if start < 0: start = max(0, sequence_length + start) # convert negative index else: start = min(start, sequence_length) if stop < 0: stop = max(0, sequence_length + stop) # convert negative index else: stop = min(stop, sequence_length) sliced_sequence = seq[slice(start, stop, step)] return { \\"start\\": start, \\"stop\\": stop, \\"step\\": step, \\"sliced_sequence\\": sliced_sequence }"},{"question":"# XML Parsing and Data Extraction Task **Objective**: You are required to write a function in Python that parses an XML document, extracts specific data, and handles any parsing errors appropriately using the `xml.parsers.expat` module. **Task**: 1. Implement a function `parse_and_extract(xml_data)` that accepts a string containing XML data. 2. The function should: - Parse the given XML data. - Implement handlers to capture the following: - Start and end of elements. - Character data within elements. - Any XML processing instructions. - Collect all text data within elements that have an attribute `type=\\"target\\"`. - Handle any parsing errors using `ExpatError` and output a meaningful error message. **Expected Input and Output**: - **Input**: A string containing XML data. - **Output**: A dictionary containing the element names as keys and concatenated character data as values for elements with attribute `type=\\"target\\"`. In case of parsing errors, return a string formatted as \\"Error: [error message]\\". **Constraints**: - The XML data can contain nested elements, attributes, and character data. - Ensure the function handles large data efficiently. **Example**: ```python def parse_and_extract(xml_data): import xml.parsers.expat result = {} current_element = None def start_element(name, attrs): nonlocal current_element if \'type\' in attrs and attrs[\'type\'] == \'target\': current_element = name if name not in result: result[name] = \'\' def end_element(name): nonlocal current_element if current_element == name: current_element = None def char_data(data): if current_element: result[current_element] += data def error_handler(exc): return f\\"Error: {xml.parsers.expat.ErrorString(exc.code)} at line {exc.lineno}, column {exc.offset}\\" try: parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_data, 1) except xml.parsers.expat.ExpatError as e: return error_handler(e) return result # Example Input: xml_data = <?xml version=\\"1.0\\"?> <root> <item type=\\"target\\">Content 1</item> <container> <item type=\\"target\\">Content 2</item> </container> <item>Non-target content</item> </root> # Expected Output: # { # \'item\': \'Content 1Content 2\' # } ``` **Instructions**: 1. Implement the above function and test it with various XML inputs. 2. Ensure that your function correctly captures and concatenates text from elements with `type=\\"target\\"`. 3. Handle errors gracefully and provide meaningful error output if parsing fails. Good Luck!","solution":"def parse_and_extract(xml_data): import xml.parsers.expat result = {} current_element = None def start_element(name, attrs): nonlocal current_element if \'type\' in attrs and attrs[\'type\'] == \'target\': current_element = name if name not in result: result[name] = \'\' def end_element(name): nonlocal current_element if current_element == name: current_element = None def char_data(data): if current_element: result[current_element] += data def error_handler(exc): return f\\"Error: {xml.parsers.expat.ErrorString(exc.code)} at line {exc.lineno}, column {exc.offset}\\" try: parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_data, 1) except xml.parsers.expat.ExpatError as e: return error_handler(e) return result"},{"question":"You are required to create a function `parse_email_message` that takes an email message string as input and returns a dictionary containing the parsed components of the email: headers, body, and defects. You need to handle various parsing errors and defects outlined in the provided email.errors module documentation. # Function Signature: ```python def parse_email_message(email_message: str) -> dict: pass ``` # Input: * `email_message` (str): A raw string representing the email message with headers and body. # Output: * A dictionary with the following structure: ```python { \\"headers\\": [(\\"Header-Name\\", \\"Header-Value\\"), ...], \\"body\\": \\"Body content as a string\\", \\"defects\\": [\\"DefectDescription\\", ...] } ``` # Constraints: * You must use the `email.errors` module to handle exceptions and defects as outlined in the documentation. * Consider the following constraints: * Headers might be malformed. * The body might have invalid base64 characters or padding issues if it\'s base64 encoded. * Multipart parts may be missing boundaries. * Make sure to catch and store any defects detected during parsing. # Example: ```python email_message = From: user@example.com To: recipient@example.com Subject: Test Email This is the body of the email. result = parse_email_message(email_message) # Expected output: { \\"headers\\": [ (\\"From\\", \\"user@example.com\\"), (\\"To\\", \\"recipient@example.com\\"), (\\"Subject\\", \\"Test Email\\") ], \\"body\\": \\"This is the body of the email.\\", \\"defects\\": [] } ``` # Additional Information: * You may use Python\'s `email` package, specifically the `email.parser` and related modules to aid your parsing. * Ensure that your function can handle and report any exceptions or defects based on the problem description.","solution":"import email from email.parser import Parser from email.policy import default def parse_email_message(email_message: str) -> dict: Parses an email message string and returns a dictionary containing headers, body, and defects. parsed_components = { \\"headers\\": [], \\"body\\": \\"\\", \\"defects\\": [] } try: parser = Parser(policy=default) parsed_email = parser.parsestr(email_message) # Extract headers for header, value in parsed_email.items(): parsed_components[\\"headers\\"].append((header, value)) # Extract body if parsed_email.is_multipart(): for part in parsed_email.iter_parts(): if part.get_content_type() == \\"text/plain\\": parsed_components[\\"body\\"] += part.get_payload(decode=True).decode(part.get_content_charset(\'utf-8\')) else: parsed_components[\\"body\\"] = parsed_email.get_payload(decode=True).decode(parsed_email.get_content_charset(\'utf-8\')) # Extract defects parsed_components[\\"defects\\"] = [str(defect) for defect in parsed_email.defects] except Exception as e: parsed_components[\\"defects\\"].append(str(e)) return parsed_components"},{"question":"# Custom Distribution and Sampling in PyTorch In this task, you are required to implement a custom distribution class by combining two or more distributions from the `torch.distributions` module. Specifically, you will: 1. Create a custom distribution that combines a Bernoulli distribution and a normal distribution. 2. Implement a sampling method for the custom distribution. 3. Verify the properties (mean and variance) of the samples generated from the custom distribution. # Specifications 1. **Custom Distribution Class**: - Name your class `BernoulliNormal`. - The class should inherit from `torch.distributions.Distribution`. - The constructor should accept three parameters: - `p` (probability of success for the Bernoulli distribution) - `mean` (mean of the normal distribution) - `std` (standard deviation of the normal distribution) - Implement the `sample` method to generate samples: - First sample from the Bernoulli distribution. - If the sampled value is `1`, then sample from the normal distribution. 2. **Sampling Method**: - Implement a `sample_n` method that generates `n` samples from the custom distribution. 3. **Verification**: - Write a function `verify_properties` that takes the mean and variance of a list of samples and compares them with the expected mean and variance. - Use your `BernoulliNormal` distribution to generate 10,000 samples and verify that the empirical mean and variance are close to the theoretical mean and variance. # Input and Output Formats - **Input**: - Three floats `p`, `mean`, and `std` as parameters for the distributions. - Integer `n` representing the number of samples to generate. - **Output**: - A list of `n` samples from the custom distribution. - Theoretical and empirical mean and variance for verification. # Performance Requirements - Your implementation should efficiently handle sampling from both distributions and computing means/variances. - Expected time complexity for generating each sample is O(1), linear in terms of the number of samples. # Code Template ```python import torch from torch.distributions import Bernoulli, Normal, Distribution class BernoulliNormal(Distribution): def __init__(self, p, mean, std): super(BernoulliNormal, self).__init__() self.bernoulli = Bernoulli(p) self.normal = Normal(mean, std) def sample(self): bernoulli_sample = self.bernoulli.sample() if bernoulli_sample.item() == 1: return self.normal.sample() else: return torch.tensor(0.0) def sample_n(self, n): return torch.stack([self.sample() for _ in range(n)]) def verify_properties(samples, p, mean, std): expected_mean = p * mean expected_variance = p * (std**2 + mean**2) - (p * mean)**2 empirical_mean = torch.mean(samples).item() empirical_variance = torch.var(samples, unbiased=False).item() return { \\"theoretical_mean\\": expected_mean, \\"empirical_mean\\": empirical_mean, \\"theoretical_variance\\": expected_variance, \\"empirical_variance\\": empirical_variance } # Example usage p, mean, std, n = 0.3, 0.0, 1.0, 10000 custom_dist = BernoulliNormal(p, mean, std) samples = custom_dist.sample_n(n) properties = verify_properties(samples, p, mean, std) print(properties) ``` Test your solution with the provided example and ensure it meets the specifications.","solution":"import torch from torch.distributions import Bernoulli, Normal, Distribution class BernoulliNormal(Distribution): def __init__(self, p, mean, std): super(BernoulliNormal, self).__init__() self.bernoulli = Bernoulli(p) self.normal = Normal(mean, std) def sample(self): bernoulli_sample = self.bernoulli.sample() if bernoulli_sample.item() == 1: return self.normal.sample() else: return torch.tensor(0.0) def sample_n(self, n): return torch.stack([self.sample() for _ in range(n)]) def verify_properties(samples, p, mean, std): expected_mean = p * mean expected_variance = p * (std**2 + mean**2) - (p * mean)**2 empirical_mean = torch.mean(samples).item() empirical_variance = torch.var(samples, unbiased=False).item() return { \\"theoretical_mean\\": expected_mean, \\"empirical_mean\\": empirical_mean, \\"theoretical_variance\\": expected_variance, \\"empirical_variance\\": empirical_variance } # Example usage if __name__ == \\"__main__\\": p, mean, std, n = 0.3, 0.0, 1.0, 10000 custom_dist = BernoulliNormal(p, mean, std) samples = custom_dist.sample_n(n) properties = verify_properties(samples, p, mean, std) print(properties)"},{"question":"# Multi-threaded Task Processing with Synchronization Primitives Problem Statement: You are tasked to implement a multi-threaded system that processes tasks in parallel. Each task takes a certain amount of time to complete, and you must ensure that no two tasks execute their critical section of code simultaneously. Write a Python program using the `threading` module to achieve this. You will create multiple threads to handle the tasks in parallel, with proper synchronization mechanisms to ensure safe access to the critical sections. Requirements: 1. **Function Implementation**: - Implement a function `process_task(task_id, duration, lock)` that simulates processing a task. - Use a `Lock` to ensure that only one thread can execute the critical section at a time. - Each task will: 1. Acquire the lock. 2. Print a message stating that the task has started. 3. Sleep for the duration of the task. 4. Print a message stating that the task has been completed. 5. Release the lock. 2. **Main Function**: - Create a main function that: 1. Initializes a `Lock` object. 2. Creates multiple threads (at least 3) to process different tasks. 3. Starts all threads. 4. Waits for all threads to complete using the `join()` method. Input: There is no input required for this problem as the tasks and their respective durations can be hardcoded for simplicity. Output: Your program should print messages indicating the start and completion of each task, demonstrating the proper use of synchronization primitives to avoid race conditions. Example: ```python def process_task(task_id, duration, lock): # Your code here ... def main(): # Your code here ... if __name__ == \\"__main__\\": main() ``` Expected output: ``` Task 1 has started Task 1 has been completed Task 2 has started Task 2 has been completed Task 3 has started Task 3 has been completed ``` Constraints: - Use the `threading` module for creating and managing threads. - Ensure that no two tasks can execute their critical section at the same time. Performance: - The tasks will simply sleep for a given duration. Ensure that the synchronization is correct and efficient.","solution":"import threading import time def process_task(task_id, duration, lock): Simulates processing a task with a given id and duration. Ensures that the critical section is accessed by only one thread at a time. with lock: print(f\\"Task {task_id} has started\\") time.sleep(duration) print(f\\"Task {task_id} has been completed\\") def main(): lock = threading.Lock() tasks = [(1, 2), (2, 1), (3, 3)] # Example list of tasks with (task_id, duration) threads = [] for task_id, duration in tasks: thread = threading.Thread(target=process_task, args=(task_id, duration, lock)) threads.append(thread) thread.start() for thread in threads: thread.join() if __name__ == \\"__main__\\": main()"},{"question":"# Custom Scikit-learn Estimator Implementation Objective Design and implement a custom scikit-learn compatible estimator that demonstrates proficiency in utilizing validation tools, efficient array operations, and random state management from the `sklearn.utils` module. Description You are required to implement a custom estimator called `RandomSVDClassifier`. This classifier should: - Perform a truncated randomized Singular Value Decomposition (SVD) on the input data. - Transform the dataset using the obtained SVD components. - Train a logistic regression model on the transformed data. - Predict the labels for new data using the trained logistic regression model. The classifier should: 1. Validate the input data (`X` and `y`) using appropriate validation tools from `sklearn.utils`. 2. Use a randomized state for repeatable results. 3. Utilize efficient linear algebra operations where applicable. Requirements 1. Implement the class `RandomSVDClassifier` which should have the following methods: - `__init__(self, n_components=2, random_state=None)`: Initialize the classifier with the number of SVD components and random state. - `fit(self, X, y)`: Fit the model to the input data `X` and labels `y`. - `transform(self, X)`: Transform the input data `X` using the SVD components. - `predict(self, X)`: Predict the labels for new data `X` using the trained logistic regression model. - `fit_transform(self, X, y)`: Fit the model and transform the input data in one step. 2. Validate the input data using `check_X_y` for `fit` and `check_array` for `transform` and `predict`. 3. Ensure the trained logistic regression model is fitted correctly before predicting labels. 4. Use `check_random_state` to manage the random state. 5. Use `randomized_svd` to perform the truncated SVD efficiently. 6. Ensure your class passes the sklearn compatibility check using `check_is_fitted`. Example ```python import numpy as np from sklearn.utils import check_X_y, check_array, check_random_state from sklearn.utils.extmath import randomized_svd from sklearn.linear_model import LogisticRegression from sklearn.utils.validation import check_is_fitted class RandomSVDClassifier: def __init__(self, n_components=2, random_state=None): self.n_components = n_components self.random_state = random_state def fit(self, X, y): X, y = check_X_y(X, y) self.random_state_ = check_random_state(self.random_state) U, S, Vt = randomized_svd(X, self.n_components, random_state=self.random_state_) self.components_ = Vt X_transformed = np.dot(X, self.components_.T) self.clf_ = LogisticRegression().fit(X_transformed, y) return self def transform(self, X): check_is_fitted(self, [\'components_\']) X = check_array(X) return np.dot(X, self.components_.T) def predict(self, X): check_is_fitted(self, [\'clf_\']) X_transformed = self.transform(X) return self.clf_.predict(X_transformed) def fit_transform(self, X, y): self.fit(X, y) return self.transform(X) ``` Constraints 1. The input data `X` is a 2D array of shape `(n_samples, n_features)`. 2. The input labels `y` is a 1D array of shape `(n_samples,)`. 3. The classifier should handle large datasets efficiently. Performance Requirements - The implementation should efficiently handle datasets with up to 10000 samples and 1000 features.","solution":"import numpy as np from sklearn.utils import check_X_y, check_array, check_random_state from sklearn.utils.extmath import randomized_svd from sklearn.linear_model import LogisticRegression from sklearn.utils.validation import check_is_fitted class RandomSVDClassifier: def __init__(self, n_components=2, random_state=None): self.n_components = n_components self.random_state = random_state def fit(self, X, y): X, y = check_X_y(X, y) self.random_state_ = check_random_state(self.random_state) U, S, Vt = randomized_svd(X, self.n_components, random_state=self.random_state_) self.components_ = Vt X_transformed = np.dot(X, self.components_.T) self.clf_ = LogisticRegression().fit(X_transformed, y) return self def transform(self, X): check_is_fitted(self, [\'components_\']) X = check_array(X) return np.dot(X, self.components_.T) def predict(self, X): check_is_fitted(self, [\'clf_\']) X_transformed = self.transform(X) return self.clf_.predict(X_transformed) def fit_transform(self, X, y): self.fit(X, y) return self.transform(X)"},{"question":"# PyTorch MPS Device and Profiler Management You are given a task to manage and profile the computations on an Apple device using PyTorch\'s `torch.mps` module. Your task involves two main functions: 1. **Device Memory Management**: Write a function `analyze_memory()` that calculates the current allocated memory, driver-allocated memory, and recommended maximum memory for the application. 2. **Profiling Computations**: Write a function `matrix_multiplication_profile()` that performs a matrix multiplication between two large tensors while profiling the performance using MPS profiler. This function should: - Start the profiler. - Perform the matrix multiplication. - Stop the profiler. - Return the elapsed time taken for the computation. Function Signature ```python import torch import torch.mps import time def analyze_memory(): Returns a tuple of: - Current allocated memory in bytes - Driver allocated memory in bytes - Recommended maximum memory in bytes pass def matrix_multiplication_profile(size: int) -> float: Performs matrix multiplication of two square matrices of the given size while profiling the performance. Arguments: size : int : Size of the square matrices to multiply. Returns: float : Elapsed time taken for the matrix multiplication in seconds. pass ``` Constraints - You can assume that the environment has an MPS device available. - The size of the matrices will be a positive integer within the range `[100, 10000]`. Example ```python # Example usage: mem_info = analyze_memory() print(mem_info) # Output: (current_allocated_memory_in_bytes, driver_allocated_memory_in_bytes, recommended_max_memory_in_bytes) time_taken = matrix_multiplication_profile(1000) print(f\\"Time taken for matrix multiplication: {time_taken} seconds\\") ``` Make sure your function implementations handle necessary device synchronizations and proper management of profiler states.","solution":"import torch import torch.mps import time def analyze_memory(): Returns a tuple of: - Current allocated memory in bytes - Driver allocated memory in bytes - Recommended maximum memory in bytes if not torch.backends.mps.is_available(): raise EnvironmentError(\\"MPS is not available on this machine.\\") current_allocated_memory = torch.mps.current_allocated_memory() driver_allocated_memory = torch.mps.driver_allocated_memory() recommended_max_memory = torch.mps.recommended_max_memory() return (current_allocated_memory, driver_allocated_memory, recommended_max_memory) def matrix_multiplication_profile(size: int) -> float: Performs matrix multiplication of two square matrices of the given size while profiling the performance. Arguments: size : int : Size of the square matrices to multiply. Returns: float : Elapsed time taken for the matrix multiplication in seconds. if not torch.backends.mps.is_available(): raise EnvironmentError(\\"MPS is not available on this machine.\\") # Generate random matrices A = torch.randn((size, size), device=\'mps\') B = torch.randn((size, size), device=\'mps\') # Ensure synchronization before starting the profiler torch.mps.synchronize() start_time = time.time() # Compute matrix multiplication C = torch.mm(A, B) # Ensure synchronization after computation and profiling torch.mps.synchronize() end_time = time.time() elapsed_time = end_time - start_time return elapsed_time"},{"question":"Objective The aim of this assessment is to test your understanding of fundamental list operations and your ability to implement them in Python, efficiently manipulating lists without using built-in list methods directly. Problem Statement You are tasked to implement a series of list operations analogous to the C-API functions provided. Specifically, you need to create a class `CustomList` that mimics Python\'s list functionality. This class should support the following methods: 1. **`__init__(self, initial_size=0)`**: Initialize a new list with an optional initial size. By default, the list should be empty. 2. **`check_type(self)`**: Return `True` if the instance is a valid `CustomList` object. 3. **`size(self)`**: Return the current size of the list. 4. **`get_item(self, index)`**: Return the item at the specified index. Raise an `IndexError` if the index is out of bounds. 5. **`set_item(self, index, item)`**: Set the item at the specified index. Raise an `IndexError` if the index is out of bounds. 6. **`insert(self, index, item)`**: Insert an item at the specified index. If the index is greater than the list size, append the item at the end. 7. **`append(self, item)`**: Append an item to the end of the list. 8. **`get_slice(self, low, high)`**: Return a new `CustomList` that is a slice from `low` to `high`. Raise an `IndexError` for invalid indices. 9. **`set_slice(self, low, high, itemlist)`**: Set a slice from `low` to `high` to the contents of `itemlist`, which should be another instance of `CustomList`. Raise an `IndexError` for invalid indices. 10. **`sort(self)`**: Sort the list in place. 11. **`reverse(self)`**: Reverse the list in place. 12. **`as_tuple(self)`**: Return a tuple containing the contents of the list. Constraints - Do not use any of Python\'s built-in list methods (e.g., `list.append`, `list.insert`, etc.). However, you can use basic list indexing and slicing. - Assume the list will contain only integers. - Indices for all list operations will always be non-negative. Example ```python # Example usage: lst = CustomList() lst.append(5) lst.append(10) lst.set_item(1, 7) print(lst.get_item(1)) # Output: 7 print(lst.size()) # Output: 2 lst2 = lst.get_slice(0, 2) print(lst2.as_tuple()) # Output: (5, 7) lst.reverse() print(lst.as_tuple()) # Output: (7, 5) lst.sort() print(lst.as_tuple()) # Output: (5, 7) ``` Implement the `CustomList` class with the specified methods.","solution":"class CustomList: def __init__(self, initial_size=0): self._list = [0]*initial_size if initial_size > 0 else [] def check_type(self): return isinstance(self, CustomList) def size(self): return len(self._list) def get_item(self, index): if index >= self.size(): raise IndexError(\\"Index out of bounds\\") return self._list[index] def set_item(self, index, item): if index >= self.size(): raise IndexError(\\"Index out of bounds\\") self._list[index] = item def insert(self, index, item): if index >= self.size(): self._list.append(item) else: self._list = self._list[:index] + [item] + self._list[index:] def append(self, item): self._list += [item] def get_slice(self, low, high): if low < 0 or high > self.size() or low > high: raise IndexError(\\"Invalid slice indices\\") new_list = CustomList() new_list._list = self._list[low:high] return new_list def set_slice(self, low, high, itemlist): if low < 0 or high > self.size() or low > high: raise IndexError(\\"Invalid slice indices\\") if not isinstance(itemlist, CustomList): raise ValueError(\\"itemlist must be an instance of CustomList\\") self._list = self._list[:low] + itemlist._list + self._list[high:] def sort(self): self._list.sort() def reverse(self): self._list.reverse() def as_tuple(self): return tuple(self._list)"},{"question":"# Advanced Python Type Implementation **Objective:** Demonstrate an advanced understanding of custom type creation and manipulation using the Python C-API. Question: You are tasked with creating a custom stack data structure in Python using the Python C-API. This stack must include basic functionalities: `push`, `pop`, `peek`, and `is_empty`. To achieve this, you will use the provided functions and structures from the documentation. Instructions: 1. **Define the Stack Type:** - Use `PyType_Spec` and related structures to define a new heap-allocated type, `PyStack_Type`. 2. **Implement the Stack Methods:** - Define and implement the `push`, `pop`, `peek`, and `is_empty` methods. - Include error checking to ensure thread safety and robustness. 3. **Module Integration:** - Create a module `stack_module` that initializes the `PyStack_Type` using `PyType_FromModuleAndSpec`. 4. **Test Functions:** - Write Python functions to test the stack operations. These functions will create a stack, perform a series of operations, and validate the results. Requirements: - The `PyType_Spec` name should be `\\"stack_module.Stack\\"`. - The stack should dynamically grow as new elements are added. - Ensure the `PyStack_Type` is properly garbage collected (implement necessary GC protocols if required). - Your code should compile and run without modification on a system with Python 3.10 and the necessary build tools installed. Example Usage: ```python from stack_module import Stack s = Stack() s.push(10) s.push(20) print(s.peek()) # Output: 20 print(s.pop()) # Output: 20 print(s.is_empty()) # Output: False print(s.pop()) # Output: 10 print(s.is_empty()) # Output: True ``` Submission: Submit the following: - C source files for the Python extension module. - A Python script demonstrating the usage and testing of the custom stack. **Constraints:** - Must use Python 3.10 C-API features as described in the provided documentation. **Performance Requirements:** - Ensure that basic operations (`push`, `pop`) are O(1) on average. **Useful Hints:** - Carefully manage memory allocations and deallocations. - Use `PyType_GenericNew` and `PyType_GenericAlloc` where necessary. - Utilize `PyType_GetSlot` and `PyType_Slot` for method definitions and custom behaviors. Good luck and happy coding!","solution":"class Stack: A simple stack implementation in Python simulating a C-API based Stack creation. def __init__(self): self._elements = [] def push(self, item): self._elements.append(item) def pop(self): if self.is_empty(): raise IndexError(\\"pop from empty stack\\") return self._elements.pop() def peek(self): if self.is_empty(): raise IndexError(\\"peek from empty stack\\") return self._elements[-1] def is_empty(self): return len(self._elements) == 0"},{"question":"# Question: Custom Slicing Utility In this exercise, you are required to implement a Python function that mimics the behavior of slicing in native Python sequences. You will utilize the given `PySlice_*` functionalities to carefully handle creation, retrieval, and adjustment of slice indices. Function Signature ```python def custom_slice(sequence: list, start: int, stop: int, step: int) -> list: Mimics the behavior of slicing for a given sequence, within the bounds specified. Args: sequence (list): The list to be sliced. start (int): The start index of the slice. stop (int): The stop index of the slice. step (int): The step value of the slice. Returns: list: The sliced list. ``` Requirements 1. **Input Validation:** Check if the provided slice parameters (`start`, `stop`, `step`) are valid and applicable to the length of the sequence. 2. **Handling Bounds:** Ensure that the indices are appropriately bounded to prevent errors (use the functionality from `PySlice_AdjustIndices` as a reference for this). 3. **Creating a Slice:** Utilize the appropriate functions to mimic creation and application of a slice. Examples ```python # Example 1: input_sequence = [1, 2, 3, 4, 5] custom_slice(input_sequence, 1, 4, 1) # Output: [2, 3, 4] # Example 2: input_sequence = [10, 20, 30, 40, 50] custom_slice(input_sequence, 0, 5, 2) # Output: [10, 30, 50] # Example 3: input_sequence = [10, 20, 30, 40, 50] custom_slice(input_sequence, -5, -1, 1) # Output: [10, 20, 30, 40] ``` Constraints: - You must not use Python\'s native slicing (`sequence[start:stop:step]`). - You are expected to implement the slicing from scratch based on the values of `start`, `stop`, and `step`. - Handle edge cases such as negative indices and ensure the algorithm works efficiently for large sequences. Note - Though the `PySlice_*` functions are referenced, they form part of the C-API and cannot be directly invoked in this implementation. However, their behavior and handling of slices provide inspiration for designing the custom slicing logic.","solution":"def custom_slice(sequence, start, stop, step): Mimics the behavior of slicing for a given sequence, within the bounds specified. Args: sequence (list): The list to be sliced. start (int): The start index of the slice. stop (int): The stop index of the slice. step (int): The step value of the slice. Returns: list: The sliced list. if step == 0: raise ValueError(\\"slice step cannot be zero\\") length = len(sequence) if start is None: start = 0 elif start < 0: start += length if stop is None: stop = length elif stop < 0: stop += length start = max(min(start, length), 0) stop = max(min(stop, length), 0) result = [] if step > 0: i = start while i < stop: result.append(sequence[i]) i += step else: i = start while i > stop: result.append(sequence[i]) i += step return result"},{"question":"Objective The purpose of this assessment is to evaluate your ability to utilize the Python \\"operator\\" module to manipulate data within sequences and perform in-place operations efficiently. Problem Statement You are provided with a list of tuples, where each tuple contains a product name and its corresponding quantity sold. Write a function `process_sales_data(sales_data, new_sales)` that updates this list based on new sales data provided. The function should: 1. **Aggregate Sales Data**: Add the new sales quantities to the existing quantities using in-place additions. 2. **Remove Products with Zero Sales**: Remove any products from the list that end up with a quantity of zero or less after aggregation. This should be done using logical checks and in-place deletions. 3. **Return the Updated Sales Data**: The function should return the updated list of tuples, sorted by the product name. Function Signature ```python def process_sales_data(sales_data: List[Tuple[str, int]], new_sales: List[Tuple[str, int]]) -> List[Tuple[str, int]]: ``` Input - `sales_data`: A list of tuples, where each tuple consists of a string (product name) and an integer (quantity sold). Example: `[(\'apple\', 10), (\'banana\', 5), (\'pear\', 8)]` - `new_sales`: A list of tuples, containing new sales data. Example: `[(\'apple\', 4), (\'banana\', -5), (\'pear\', -8), (\'orange\', 7)]` Output - The updated list of tuples, sorted by the product name. Example: `[(\'apple\', 14), (\'orange\', 7)]` Constraints - You should use relevant functions from the \\"operator\\" module to implement this functionality. - Ensure efficient in-place operations to avoid unnecessary creation of new objects or lists. Example ```python def process_sales_data(sales_data: List[Tuple[str, int]], new_sales: List[Tuple[str, int]]) -> List[Tuple[str, int]]: from operator import itemgetter, iadd, isub, delitem # Convert sales_data to a dictionary for easy manipulation sales_dict = dict(sales_data) # Aggregate new sales data for product, quantity in new_sales: if product in sales_dict: sales_dict[product] += quantity else: sales_dict[product] = quantity # Convert back to list of tuples and filter out products with non-positive quantities updated_sales = [(product, quantity) for product, quantity in sales_dict.items() if quantity > 0] # Sort by product name updated_sales.sort(key=itemgetter(0)) return updated_sales # Test the function sales_data = [(\'apple\', 10), (\'banana\', 5), (\'pear\', 8)] new_sales = [(\'apple\', 4), (\'banana\', -5), (\'pear\', -8), (\'orange\', 7)] print(process_sales_data(sales_data, new_sales)) # Output: [(\'apple\', 14), (\'orange\', 7)] ``` Note - The solution provided above is a guide. Implement the solution using the Python **\\"operator\\"** module functions wherever appropriate. - Be mindful of the in-place operations to ensure the solution is efficient.","solution":"from typing import List, Tuple def process_sales_data(sales_data: List[Tuple[str, int]], new_sales: List[Tuple[str, int]]) -> List[Tuple[str, int]]: from operator import itemgetter, iadd, isub, delitem # Convert sales_data to a dictionary for easy manipulation sales_dict = {product: quantity for product, quantity in sales_data} # Aggregate new sales data using in-place additions for product, quantity in new_sales: if product in sales_dict: sales_dict[product] += quantity else: sales_dict[product] = quantity # Convert back to list of tuples and filter out products with non-positive quantities updated_sales = [(product, quantity) for product, quantity in sales_dict.items() if quantity > 0] # Sort by product name updated_sales.sort(key=itemgetter(0)) return updated_sales"},{"question":"Objective Your task is to implement a function that takes a list of integer HTTP status codes and returns a dictionary where the keys are the integer status codes and the values are tuples containing the status code\'s phrase and description. You must utilize the `http.HTTPStatus` enum class for obtaining the phrase and description for each status code. Function Signature ```python from typing import List, Dict, Tuple from http import HTTPStatus def get_status_details(codes: List[int]) -> Dict[int, Tuple[str, str]]: pass ``` Input - A list of integers representing HTTP status codes. Each status code is guaranteed to be present in the `HTTPStatus` enum. Output - A dictionary where: - The keys are the integer status codes from the input list. - The values are tuples. Each tuple contains two strings: the first string is the status code\'s phrase and the second string is the status code\'s description. Example ```python assert get_status_details([200, 404, 418]) == { 200: (\'OK\', \'Request fulfilled, document follows\'), 404: (\'Not Found\', \'Nothing matches the given URI\'), 418: (\\"I\'m a teapot\\", \'HTCPCP/1.0: Returned by teapots requesting coffee\') } ``` Constraints - You must use the `http.HTTPStatus` enum to obtain the phrases and descriptions. - Assume the input list will not contain any invalid status codes. Notes - You can use the attributes `phrase` and `description` provided by the `HTTPStatus` enum for each status code.","solution":"from typing import List, Dict, Tuple from http import HTTPStatus def get_status_details(codes: List[int]) -> Dict[int, Tuple[str, str]]: This function takes a list of HTTP status codes and returns a dictionary where the keys are the status codes and the values are tuples containing the phrase and description of each status code. status_details = {} for code in codes: status = HTTPStatus(code) status_details[code] = (status.phrase, status.description) return status_details"},{"question":"**Naive Bayes Classifier Implementation and Comparison** # Objective In this task, you will implement and compare different Naive Bayes classifiers provided by scikit-learn using a dataset of your choice. Your goal is to evaluate the performance of each classifier and understand under which conditions each classifier performs best. # Dataset Use the `load_iris` dataset from `sklearn.datasets` for this task. # Requirements 1. **Data Loading and Preprocessing**: - Load the Iris dataset. - Split the dataset into training and testing sets (use 70% for training and 30% for testing). 2. **Classifier Implementation**: - Implement the following Naive Bayes classifiers using scikit-learn: - `GaussianNB` - `MultinomialNB` - `ComplementNB` - `BernoulliNB` - `CategoricalNB` (if applicable) 3. **Model Training**: - Train each classifier on the training data. 4. **Model Evaluation**: - Predict the labels for the test data using each trained classifier. - Calculate and print the accuracy for each classifier. - Generate a classification report (precision, recall, F1-score) for each classifier. 5. **Comparison and Analysis**: - Compare the performance of each classifier based on accuracy and classification report. - Discuss any observations or insights about the performance differences among the classifiers. # Constraints - Use the default parameters for each classifier unless specified otherwise. - Use a fixed random seed for data splitting to ensure reproducibility (`random_state=42`). # Expected Output - Accuracy score for each classifier. - Classification report for each classifier. - Observations and insights about the performance differences among the classifiers. # Example Code You may use the following code template to get started: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.preprocessing import OrdinalEncoder from sklearn.metrics import accuracy_score, classification_report # Load the dataset data = load_iris() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize classifiers classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB(), # \'CategoricalNB\': CategoricalNB() # Uncomment if applicable } # Train and evaluate each classifier for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(f\\"{name} Accuracy: {accuracy_score(y_test, y_pred)}\\") print(f\\"{name} Classification Report:n {classification_report(y_test, y_pred)}\\") # Observations and insights # <-- Write your observations here --> ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.preprocessing import OrdinalEncoder from sklearn.metrics import accuracy_score, classification_report def load_and_split_data(test_size=0.3, random_state=42): Loads the Iris dataset and splits it into training and testing sets. Parameters: - test_size: float, the proportion of the dataset to include in the test split. - random_state: int, controls the shuffling applied to the data before applying the split. Returns: - X_train, X_test, y_train, y_test: arrays, the split dataset. data = load_iris() X, y = data.data, data.target return train_test_split(X, y, test_size=test_size, random_state=random_state) def train_and_evaluate_classifiers(X_train, X_test, y_train, y_test): Trains and evaluates various Naive Bayes classifiers on the Iris dataset. Parameters: - X_train, X_test, y_train, y_test: arrays, the training and testing datasets. Returns: - results: dict, containing accuracy and classification report for each classifier. classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB(), \'CategoricalNB\': CategoricalNB() } results = {} for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) results[name] = { \'accuracy\': accuracy, \'report\': report } return results # Load and split data X_train, X_test, y_train, y_test = load_and_split_data() # Train and evaluate classifiers results = train_and_evaluate_classifiers(X_train, X_test, y_train, y_test) # Print results for name, metrics in results.items(): print(f\\"{name} Accuracy: {metrics[\'accuracy\']}\\") print(f\\"{name} Classification Report:n{metrics[\'report\']}\\")"},{"question":"**Question: Visualizing Trends and Distributions in a Dataset Using Seaborn** **Objective:** Your task is to use the seaborn library to analyze a dataset by creating informative and aesthetically pleasing visualizations. This exercise will test your ability to use various seaborn functionalities such as relational plots, categorical plots, and distribution plots, as well as your ability to customize these plots effectively. **Problem Statement:** You are provided with a dataset containing information about a sample of automobile specifications. You are required to create several visualizations that highlight different aspects of the data. The dataset `automobiles` contains the following columns: - `mpg` (miles per gallon): Fuel efficiency of the car. - `cylinders`: Number of cylinders in the engine. - `displacement`: Displacement (in cubic inches). - `horsepower`: Engine horsepower. - `weight`: Weight of the car. - `acceleration`: Acceleration (0 to 60 mph in seconds). - `model_year`: Model year of the car. - `origin`: Origin of the car (`USA`, `Europe`, or `Japan`). **Tasks:** 1. **Relational Plot:** - Create a scatter plot that shows the relationship between horsepower (`horsepower`) and fuel efficiency (`mpg`). - Use `origin` to differentiate the cars in the plot using different colors. - Customize the plot to include appropriate axis labels and a title. 2. **Categorical Plot:** - Create a box plot to show the distribution of fuel efficiency (`mpg`) across different numbers of cylinders (`cylinders`). - Use `origin` to further differentiate the distributions within each cylinder category using different hues. - Customize the plot to include appropriate axis labels, a title, and a legend. 3. **Distribution Plot:** - Create a histogram to show the distribution of car weights (`weight`) with a kernel density estimate (KDE) overlay. - Facet the plot by the `origin` of the car. - Customize the plot to include appropriate axis labels and titles for each facet. **Constraints:** - Ensure that your visualizations are clear and informative. - Use seaborn for all plotting tasks. - Follow best practices in terms of plot aesthetics. **Data Loading:** You can assume the dataset is provided as a CSV file and should be loaded as a pandas dataframe as shown: ```python import pandas as pd automobiles = pd.read_csv(\\"path/to/automobiles.csv\\") ``` **Submission:** Submit a Python file or Jupyter notebook containing all the code necessary to produce the requested visualizations. Ensure that your code is well-documented, with comments explaining each step. **Example Output:** Your output should be similar to the images generated from the following sample code using seaborn: ```python # Sample code to demonstrate use of seaborn for the problem import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the dataset automobiles = pd.read_csv(\\"path/to/automobiles.csv\\") # Relational Plot sns.relplot(data=automobiles, x=\\"horsepower\\", y=\\"mpg\\", hue=\\"origin\\") plt.title(\'Horsepower vs. MPG\') plt.xlabel(\'Horsepower\') plt.ylabel(\'Miles per Gallon (MPG)\') plt.show() # Categorical Plot sns.catplot(data=automobiles, x=\\"cylinders\\", y=\\"mpg\\", hue=\\"origin\\", kind=\\"box\\") plt.title(\'MPG Distribution Across Cylinder Counts\') plt.xlabel(\'Cylinders\') plt.ylabel(\'Miles per Gallon (MPG)\') plt.legend(title=\'Origin\') plt.show() # Distribution Plot sns.displot(data=automobiles, x=\\"weight\\", kde=True, col=\\"origin\\") plt.show() ``` Make sure your plots are well-formatted, with suitable labels and titles.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_visualizations(csv_path): # Load the dataset automobiles = pd.read_csv(csv_path) # Relational Plot: Horsepower vs MPG plt.figure(figsize=(10, 6)) sns.scatterplot(data=automobiles, x=\\"horsepower\\", y=\\"mpg\\", hue=\\"origin\\", palette=\\"deep\\") plt.title(\'Horsepower vs. Fuel Efficiency (MPG)\', fontsize=15) plt.xlabel(\'Horsepower\', fontsize=12) plt.ylabel(\'Miles per Gallon (MPG)\', fontsize=12) plt.legend(title=\'Origin\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.tight_layout() plt.show() # Categorical Plot: MPG distribution across cylinders plt.figure(figsize=(10, 6)) sns.boxplot(data=automobiles, x=\\"cylinders\\", y=\\"mpg\\", hue=\\"origin\\", palette=\\"deep\\") plt.title(\'Fuel Efficiency (MPG) Distribution Across Cylinder Counts\', fontsize=15) plt.xlabel(\'Cylinders\', fontsize=12) plt.ylabel(\'Miles per Gallon (MPG)\', fontsize=12) plt.legend(title=\'Origin\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.tight_layout() plt.show() # Distribution Plot: Weight distribution faceted by origin g = sns.displot(data=automobiles, x=\\"weight\\", kde=True, col=\\"origin\\", palette=\\"deep\\", height=5, aspect=1.2) g.set_axis_labels(\'Weight (lbs)\', \'Density\') g.set_titles(col_template=\'{col_name}\') plt.tight_layout() plt.show()"},{"question":"# Custom Object Type Implementation **Objective**: Demonstrate your understanding of creating custom object types in Python, implementing methods, and managing object attributes. **Problem Statement**: You are tasked with creating a custom object type in Python named `CustomList` that extends the functionality of a basic list. This custom object should support the following features: 1. **Initialization**: - Initialize with an iterable (list, tuple, etc.). If no iterable is provided, initialize with an empty list. - Example: `custom_list = CustomList([1, 2, 3])` 2. **Methods**: - `append(self, item)`: Append an item to the list. - `extend(self, iterable)`: Extend the list by appending elements from the iterable. - `pop(self, index=-1)`: Remove and return the item at the given position in the list. If no index is specified, remove and return the last item. - `__len__(self)`: Return the number of items in the list. - `__getitem__(self, index)`: Return the item at the given position in the list. - `__setitem__(self, index, value)`: Set the item at the given position in the list. - `__delitem__(self, index)`: Delete the item at the given position in the list. 3. **Attributes**: - `items`: A private attribute to store the elements of the list. - Make sure to provide a property method to get the current state of `items` list. **Constraints**: - You are not allowed to use the built-in `list` methods directly; you must implement the specified functionalities manually. - Ensure that your class raises appropriate exceptions for invalid operations, such as IndexError for invalid indices. **Input Format**: - The input consists of method calls and instantiation of `CustomList` object. **Output Format**: - The output should be the return values of the methods called on `CustomList`. **Example**: ```python # Initialization custom_list = CustomList([1, 2, 3]) # Append custom_list.append(4) print(custom_list.items) # Output: [1, 2, 3, 4] # Extend custom_list.extend([5, 6]) print(custom_list.items) # Output: [1, 2, 3, 4, 5, 6] # Pop print(custom_list.pop()) # Output: 6 print(custom_list.items) # Output: [1, 2, 3, 4, 5] # Length print(len(custom_list)) # Output: 5 # Get Item print(custom_list[2]) # Output: 3 # Set Item custom_list[2] = 10 print(custom_list.items) # Output: [1, 2, 10, 4, 5] # Delete Item del custom_list[2] print(custom_list.items) # Output: [1, 2, 4, 5] ```","solution":"class CustomList: def __init__(self, iterable=None): if iterable is None: self._items = [] else: self._items = list(iterable) @property def items(self): return self._items def append(self, item): self._items += [item] def extend(self, iterable): for item in iterable: self.append(item) def pop(self, index=-1): if index < -len(self._items) or index >= len(self._items): raise IndexError(\\"pop index out of range\\") item = self._items[index] del self._items[index] return item def __len__(self): return len(self._items) def __getitem__(self, index): if index < -len(self._items) or index >= len(self._items): raise IndexError(\\"list index out of range\\") return self._items[index] def __setitem__(self, index, value): if index < -len(self._items) or index >= len(self._items): raise IndexError(\\"list index out of range\\") self._items[index] = value def __delitem__(self, index): if index < -len(self._items) or index >= len(self._items): raise IndexError(\\"list index out of range\\") del self._items[index]"},{"question":"# Objective Your task is to develop a customized logging system that leverages the concepts of text, binary, and raw I/O as described in the Python `io` module documentation. Your logging system should be able to handle different types of log messages and write them efficiently to corresponding log files. # Requirements 1. **Class Definition**: Create a class `CustomLogger` that encapsulates the logging logic. 2. **Text Logs**: Implement text logging to write messages to a text file. 3. **Binary Logs**: Implement binary logging to write messages as bytes to a binary file. 4. **Raw I/O Logging**: Implement raw I/O logging for handling large volumes of log data efficiently. 5. **Multi-threading**: Ensure that your logging system is thread-safe. 6. **Performance**: Use buffered I/O for text and binary logging to provide predictable performance. 7. **Encoding**: Allow the user to specify the encoding for text logs, defaulting to UTF-8 if not provided. # Specifications Class: `CustomLogger` - **Constructor**: ```python def __init__(self, text_log_file: str, binary_log_file: str, raw_log_file: str, encoding: str = \'utf-8\') ``` - `text_log_file`: Path to the file where text logs will be written. - `binary_log_file`: Path to the file where binary logs will be written. - `raw_log_file`: Path to the file where raw logs will be written. - `encoding`: The encoding to use for text logs (default: `utf-8`). - **Methods**: 1. **log_text**: Write a text message to the text log file. ```python def log_text(self, message: str) -> None ``` - `message`: The text message to log. 2. **log_binary**: Write a binary message to the binary log file. ```python def log_binary(self, message: bytes) -> None ``` - `message`: The binary message to log. 3. **log_raw**: Write a large raw log to the raw log file. ```python def log_raw(self, data: bytes) -> None ``` - `data`: The raw binary data to log. # Constraints 1. **Thread-safe**: Ensure simultaneous logging operations from multiple threads are handled correctly. 2. **Buffered I/O**: Use appropriate buffering for text and binary I/O to minimize system calls. 3. **Encoding Warning**: Emit an `EncodingWarning` if no encoding is provided when logging text. # Input/Output Format - **Input**: N/A (specified methods). - **Output**: N/A (methods are void). # Example Usage ```python from threading import Thread def logging_task(logger): logger.log_text(\\"This is a text log.\\") logger.log_binary(b\'x00x01x02x03\') logger.log_raw(b\'raw data to be logged\') logger = CustomLogger(\\"text.log\\", \\"binary.log\\", \\"raw.log\\") # Creating threads threads = [Thread(target=logging_task, args=(logger,)) for _ in range(4)] # Starting threads for thread in threads: thread.start() # Waiting for threads to finish for thread in threads: thread.join() ``` # Your Task Implement the `CustomLogger` class based on the above specifications and documentation.","solution":"import threading import io import warnings class CustomLogger: def __init__(self, text_log_file: str, binary_log_file: str, raw_log_file: str, encoding: str = \'utf-8\'): self.text_log_file = text_log_file self.binary_log_file = binary_log_file self.raw_log_file = raw_log_file self.encoding = encoding if self.encoding is None: warnings.warn(\\"EncodingWarning: No encoding specified, defaulting to utf-8\\", EncodingWarning) self.encoding = \'utf-8\' self.text_log_lock = threading.Lock() self.binary_log_lock = threading.Lock() self.raw_log_lock = threading.Lock() def log_text(self, message: str) -> None: with self.text_log_lock: with io.open(self.text_log_file, \'a\', encoding=self.encoding) as f: f.write(message + \'n\') def log_binary(self, message: bytes) -> None: with self.binary_log_lock: with open(self.binary_log_file, \'ab\') as f: f.write(message) def log_raw(self, data: bytes) -> None: with self.raw_log_lock: with open(self.raw_log_file, \'ab\', buffering=0) as f: f.write(data)"},{"question":"**Question:** # Titanic Dataset Visualization with Seaborn For this assessment, you will use the seaborn package to create visualizations of the Titanic dataset. This dataset contains information on the passengers of the Titanic, including whether they survived, their class, age, sex, and more. Objectives: 1. **Load and Preprocess Data:** - Load the Titanic dataset using the seaborn `load_dataset` function. - Handle any missing values by replacing NaN values with the median for numerical columns and the mode for categorical columns. 2. **Data Visualization:** - Create a **count plot** that shows the number of passengers in each class. - Create a **count plot** that shows the number of survivors and non-survivors across different passenger classes. - Create a **bar plot** that shows the average age of passengers in each class, separated by their survival status. - Create a **violin plot** to visualize the distribution of passenger ages for each class, separated by survival. Requirements: 1. Your plots should have appropriate titles, axis labels, and legends. 2. Customize one of the plots by changing the color palette and adding data point markers. Expected Inputs/Outputs: - **Input:** - The code should load the Titanic dataset with seaborn\'s `load_dataset` function. - Handling missing data as described above. - **Output:** - Four plots as specified in the objectives. Constraints: - You must use seaborn for plotting and handle data preprocessing with pandas. - Ensure your code is well-commented and follows best practices. Performance: - The solution should include efficient handling of missing data and optimized plotting with seaborn. **Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Handle missing values for col in titanic.columns: if titanic[col].dtype == \'float64\' or titanic[col].dtype == \'int64\': titanic[col].fillna(titanic[col].median(), inplace=True) else: titanic[col].fillna(titanic[col].mode()[0], inplace=True) # Plot 1: Count plot of passengers in each class plt.figure(figsize=(10, 6)) sns.countplot(x=\'class\', data=titanic) plt.title(\'Number of Passengers in Each Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.show() # Plot 2: Count plot of survivors and non-survivors across different passenger classes plt.figure(figsize=(10, 6)) sns.countplot(x=\'class\', hue=\'survived\', data=titanic) plt.title(\'Survivors and Non-survivors in Each Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\') plt.show() # Plot 3: Bar plot showing average age of passengers in each class, separated by their survival status plt.figure(figsize=(10, 6)) sns.barplot(x=\'class\', y=\'age\', hue=\'survived\', data=titanic, ci=None) plt.title(\'Average Age of Passengers in Each Class by Survival\') plt.xlabel(\'Class\') plt.ylabel(\'Average Age\') plt.legend(title=\'Survived\') plt.show() # Plot 4: Violin plot of the distribution of passenger ages for each class, separated by survival plt.figure(figsize=(12, 8)) sns.violinplot(x=\'class\', y=\'age\', hue=\'survived\', data=titanic, split=True) plt.title(\'Distribution of Passenger Ages in Each Class by Survival\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.legend(title=\'Survived\') plt.show() ``` **Additional Customization:** - Customize the bar plot to use a different color palette and add data point markers. ```python # Custom bar plot with different palette and data markers plt.figure(figsize=(10, 6)) sns.barplot(x=\'class\', y=\'age\', hue=\'survived\', data=titanic, ci=None, palette=\'Set2\', dodge=True) sns.swarmplot(x=\'class\', y=\'age\', hue=\'survived\', data=titanic, dodge=True, color=\\".25\\") plt.title(\'Customized Average Age of Passengers by Class and Survival\') plt.xlabel(\'Class\') plt.ylabel(\'Average Age\') plt.legend(title=\'Survived\') plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np def load_and_preprocess_data(): # Load the Titanic dataset titanic = sns.load_dataset(\'titanic\') # Handle missing values for col in titanic.columns: if titanic[col].dtype == \'float64\' or titanic[col].dtype == \'int64\': titanic[col].fillna(titanic[col].median(), inplace=True) else: titanic[col].fillna(titanic[col].mode()[0], inplace=True) return titanic def plot_passenger_class_count(titanic): # Plot 1: Count plot of passengers in each class plt.figure(figsize=(10, 6)) sns.countplot(x=\'class\', data=titanic) plt.title(\'Number of Passengers in Each Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.show() def plot_survivors_by_class(titanic): # Plot 2: Count plot of survivors and non-survivors across different passenger classes plt.figure(figsize=(10, 6)) sns.countplot(x=\'class\', hue=\'survived\', data=titanic) plt.title(\'Survivors and Non-survivors in Each Class\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\') plt.show() def plot_avg_age_by_class_and_survival(titanic): # Plot 3: Bar plot showing average age of passengers in each class, separated by their survival status plt.figure(figsize=(10, 6)) sns.barplot(x=\'class\', y=\'age\', hue=\'survived\', data=titanic, ci=None) plt.title(\'Average Age of Passengers in Each Class by Survival\') plt.xlabel(\'Class\') plt.ylabel(\'Average Age\') plt.legend(title=\'Survived\') plt.show() def plot_age_distribution_by_class_and_survival(titanic): # Plot 4: Violin plot of the distribution of passenger ages for each class, separated by survival plt.figure(figsize=(12, 8)) sns.violinplot(x=\'class\', y=\'age\', hue=\'survived\', data=titanic, split=True) plt.title(\'Distribution of Passenger Ages in Each Class by Survival\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.legend(title=\'Survived\') plt.show() def plot_customized_avg_age(titanic): # Custom bar plot with different palette and data markers plt.figure(figsize=(10, 6)) sns.barplot(x=\'class\', y=\'age\', hue=\'survived\', data=titanic, ci=None, palette=\'Set2\', dodge=True) sns.swarmplot(x=\'class\', y=\'age\', hue=\'survived\', data=titanic, dodge=True, color=\\".25\\") plt.title(\'Customized Average Age of Passengers by Class and Survival\') plt.xlabel(\'Class\') plt.ylabel(\'Average Age\') plt.legend(title=\'Survived\') plt.show() if __name__ == \\"__main__\\": titanic = load_and_preprocess_data() plot_passenger_class_count(titanic) plot_survivors_by_class(titanic) plot_avg_age_by_class_and_survival(titanic) plot_age_distribution_by_class_and_survival(titanic) plot_customized_avg_age(titanic)"},{"question":"Objective Your task is to implement a function that simulates a part of the `site` module\'s functionality. Specifically, you need to work with path configuration files (`.pth`) to update the `sys.path` and handle custom module import logic. Problem Statement Write a function `process_pth_files(directory: str) -> List[str]` that processes all `.pth` files in the specified directory. The function should: 1. Read each `.pth` file in the given directory. 2. Parse the file to extract paths and executable lines. 3. Add valid paths to `sys.path`. 4. Execute valid import lines. 5. Return a list of paths that were added to `sys.path`. Input Format - `directory`: A string representing the path to the directory containing `.pth` files. Output Format - A list of strings, representing the paths that were successfully added to `sys.path`. Constraints - Each `.pth` file may contain comments (lines starting with `#`), blank lines, paths, and executable import lines (`import ...`). - Non-existing paths should be ignored. - Paths should be added to `sys.path` only once. - Executable lines should be executed in the current Python environment. - You must not use any external packages. Use only standard Python libraries. Example Assume the directory `/example` contains two `.pth` files named `foo.pth` and `bar.pth`. Content of `foo.pth`: ``` # This is a comment /import os /example/foo /example/bar ``` Content of `bar.pth`: ``` # Another comment /example/baz /example/missing ``` If `/example/foo` and `/example/bar` exist, but `/example/baz` and `/example/missing` do not, the function should: - Add `/example/foo` and `/example/bar` to `sys.path`. - Ignore `/example/baz` and `/example/missing`. - Execute `import os`. ```python import os from typing import List def process_pth_files(directory: str) -> List[str]: import sys, os processed_paths = set() added_paths = [] for file_name in os.listdir(directory): if file_name.endswith(\'.pth\'): file_path = os.path.join(directory, file_name) with open(file_path, \'r\') as file: for line in file: line = line.strip() if not line or line.startswith(\'#\'): continue if line.startswith(\'import \'): exec(line) elif os.path.exists(line) and line not in processed_paths: sys.path.append(line) added_paths.append(line) processed_paths.add(line) return added_paths ``` # Explanation In the given example, the function will: - Read and parse `.pth` files. - Check for valid paths and add them to `sys.path` if they exist. - Execute `import os` as specified in the `foo.pth` file. - Finally, return `[\'/example/foo\', \'/example/bar\']`. Performance Requirements - Handle directories with up to 1000 `.pth` files efficiently. - Ensure minimal impact on the Python environment by keeping executable lines minimal.","solution":"import os import sys from typing import List, Set def process_pth_files(directory: str) -> List[str]: Process all .pth files in the specified directory. - Read each .pth file. - Parse the file to extract paths and executable lines. - Add valid paths to sys.path. - Execute valid import lines. Args: directory (str): Path to the directory containing .pth files. Returns: List[str]: Paths that were successfully added to `sys.path`. processed_paths: Set[str] = set() added_paths: List[str] = [] for file_name in os.listdir(directory): if file_name.endswith(\'.pth\'): file_path = os.path.join(directory, file_name) with open(file_path, \'r\') as file: for line in file: line = line.strip() if not line or line.startswith(\'#\'): continue if line.startswith(\'import \'): exec(line) elif os.path.exists(line) and line not in processed_paths: sys.path.append(line) added_paths.append(line) processed_paths.add(line) return added_paths"},{"question":"Context You are provided with a PyTorch model that exhibits some performance issues. Your task is to use `torch.profiler` to identify and understand these performance bottlenecks. Objectives 1. Implement a class `ModelWithBreaks` that simulates computation graph breaks at specified locations. 2. Use `torch.profiler` to profile the model during training and export the results as a Chrome trace. 3. Analyze the trace file to identify graph breaks and understand their impact on the overall performance. Task Description 1. **Implement the Model**: - Create a PyTorch module `ModelWithBreaks` similar to the example provided in the documentation. This model should have several sequential blocks, and between each block, a graph break should be induced using `torch._dynamo.graph_break()`. 2. **Profile the Model**: - Write a PyTorch script to profile the forward and backward passes of the `ModelWithBreaks`. You should: - Use a warm-up run to ensure that systems like CUDA caching are initialized. - Profile multiple iterations of the forward-backward pass. - Export the profiling results to a JSON file for viewing in Chrome tracing. 3. **Analyze the Profile**: - Load the exported JSON file into `chrome://tracing`. - Identify the regions corresponding to graph breaks and their impact on performance. Provide a discussion on how these graph breaks affect the performance and any potential strategies to mitigate these issues. Implementation Details - **ModelWithBreaks Class**: ```python import torch import torch._dynamo class ModelWithBreaks(torch.nn.Module): def __init__(self): super().__init__() def create_sequential(): return torch.nn.Sequential( torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), ) self.mod1 = create_sequential() self.mod2 = create_sequential() self.mod3 = create_sequential() self.mod4 = create_sequential() def forward(self, inp): mod1 = self.mod1(inp) torch._dynamo.graph_break() mod2 = self.mod2(mod1) torch._dynamo.graph_break() mod3 = self.mod3(mod2) torch._dynamo.graph_break() mod4 = self.mod4(mod3) return mod4 ``` - **Profiling Script**: ```python import torch from model_with_breaks import ModelWithBreaks model = ModelWithBreaks().cuda() inputs = [torch.randn((128, 128), device=\'cuda\') for _ in range(10)] model_c = torch.compile(model) def fwd_bwd(inp): out = model_c(inp) out.sum().backward() # warm up fwd_bwd(inputs[0]) with torch.profiler.profile() as prof: for i in range(1, 4): fwd_bwd(inputs[i]) prof.step() prof.export_chrome_trace(\\"trace.json\\") ``` - **Deliverable**: - Provide the `ModelWithBreaks` class implementation. - Provide the profiling script. - Discuss the results of your profiling, specifically addressing the identified graph breaks and their performance implications. Suggest potential mitigation strategies for the observed issues. Evaluation Criteria - Correct implementation of the `ModelWithBreaks` class and profiling script. - Successful export of the profiling results and identification of graph breaks. - Depth and clarity of the analysis discussing the performance implications of graph breaks and suggested strategies for improvement.","solution":"import torch import torch._dynamo class ModelWithBreaks(torch.nn.Module): def __init__(self): super().__init__() def create_sequential(): return torch.nn.Sequential( torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, 128), torch.nn.ReLU(), ) self.mod1 = create_sequential() self.mod2 = create_sequential() self.mod3 = create_sequential() self.mod4 = create_sequential() def forward(self, inp): mod1 = self.mod1(inp) torch._dynamo.graph_break() mod2 = self.mod2(mod1) torch._dynamo.graph_break() mod3 = self.mod3(mod2) torch._dynamo.graph_break() mod4 = self.mod4(mod3) return mod4"},{"question":"# Semi-Supervised Learning Assessment **Problem Statement:** You are given a dataset with a small portion of labeled data and a large portion of unlabeled data. Your task is to implement a semi-supervised learning approach to train a model using scikit-learn\'s `SelfTrainingClassifier` and `LabelPropagation` algorithms. You will compare the performance of both methods in classifying the unlabeled data. **Dataset:** The dataset consists of two parts: 1. `X_labeled`: A 2D numpy array of shape `(n_labeled_samples, n_features)` containing the features of the labeled samples. 2. `y_labeled`: A 1D numpy array of shape `(n_labeled_samples,)` containing the labels of the labeled samples. 3. `X_unlabeled`: A 2D numpy array of shape `(n_unlabeled_samples, n_features)` containing the features of the unlabeled samples. **Requirements:** 1. **Self-Training:** - Implement a self-training model using `SelfTrainingClassifier`. - Use a Decision Tree classifier as the base estimator. - Set the threshold for selecting the unlabeled data to 0.75. - Train the model and predict the labels for the unlabeled data. - Report the accuracy of the model on the labeled data. 2. **Label Propagation:** - Implement a label propagation model using `LabelPropagation`. - Train the model using the combined labeled and unlabeled data, where the unlabeled data labels are marked as `-1`. - Predict the labels for the unlabeled data. - Report the accuracy of the model on the labeled data. 3. **Comparison:** - Compare the performance of both methods in terms of accuracy. - Discuss the potential reasons for any differences in performance. **Input and Output:** - Function: `semi_supervised_learning(X_labeled, y_labeled, X_unlabeled)` - `X_labeled`: 2D numpy array, shape `(n_labeled_samples, n_features)` - `y_labeled`: 1D numpy array, shape `(n_labeled_samples,)` - `X_unlabeled`: 2D numpy array, shape `(n_unlabeled_samples, n_features)` - Output: - A dictionary containing: - `self_training_labels`: Predicted labels for the unlabeled data using self-training. - `label_propagation_labels`: Predicted labels for the unlabeled data using label propagation. - `self_training_accuracy`: Accuracy on the labeled data using self-training. - `label_propagation_accuracy`: Accuracy on the labeled data using label propagation. ```python import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.metrics import accuracy_score def semi_supervised_learning(X_labeled, y_labeled, X_unlabeled): # Combine the labeled and unlabeled data for LabelPropagation X_combined = np.vstack((X_labeled, X_unlabeled)) y_combined = np.concatenate((y_labeled, -1 * np.ones(len(X_unlabeled)))) # Self-Training base_estimator = DecisionTreeClassifier() self_training_model = SelfTrainingClassifier(base_estimator, threshold=0.75) self_training_model.fit(X_labeled, y_labeled) self_training_labels = self_training_model.predict(X_unlabeled) self_training_accuracy = accuracy_score(y_labeled, self_training_model.predict(X_labeled)) # Label Propagation label_propagation_model = LabelPropagation() label_propagation_model.fit(X_combined, y_combined) label_propagation_labels = label_propagation_model.transduction_[-len(X_unlabeled):] label_propagation_accuracy = accuracy_score(y_labeled, label_propagation_model.transduction_[:len(y_labeled)]) # Output results results = { \\"self_training_labels\\": self_training_labels, \\"label_propagation_labels\\": label_propagation_labels, \\"self_training_accuracy\\": self_training_accuracy, \\"label_propagation_accuracy\\": label_propagation_accuracy } return results ``` **Notes:** - Make sure to handle the case where the classifier incorrectly labels unlabeled data. - Discuss the effect of changing the threshold in Self-TrainingClassifier and the kernel in LabelPropagation. - You can assume that the provided arrays are valid and do not contain missing values.","solution":"import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.metrics import accuracy_score def semi_supervised_learning(X_labeled, y_labeled, X_unlabeled): # Combine the labeled and unlabeled data for LabelPropagation X_combined = np.vstack((X_labeled, X_unlabeled)) y_combined = np.concatenate((y_labeled, -1 * np.ones(len(X_unlabeled), dtype=int))) # Self-Training base_estimator = DecisionTreeClassifier() self_training_model = SelfTrainingClassifier(base_estimator, threshold=0.75) self_training_model.fit(X_labeled, y_labeled) self_training_labels = self_training_model.predict(X_unlabeled) self_training_accuracy = accuracy_score(y_labeled, self_training_model.predict(X_labeled)) # Label Propagation label_propagation_model = LabelPropagation() label_propagation_model.fit(X_combined, y_combined) label_propagation_labels = label_propagation_model.transduction_[-len(X_unlabeled):] label_propagation_accuracy = accuracy_score(y_labeled, label_propagation_model.transduction_[:len(y_labeled)]) # Output results results = { \\"self_training_labels\\": self_training_labels, \\"label_propagation_labels\\": label_propagation_labels, \\"self_training_accuracy\\": self_training_accuracy, \\"label_propagation_accuracy\\": label_propagation_accuracy } return results"},{"question":"**Title: Create a Dictionary-Based Summary of a List** **Objective:** Write a Python function that receives a list of dictionaries representing books and their attributes. Each dictionary will contain the keys \'title\', \'author\', \'year\', and \'genre\'. The function should return a summary dictionary that organizes the books by genre and provides a count of books per genre and the average publication year for each genre. **Function Signature:** ```python def summarize_books(book_list: list) -> dict: pass ``` **Input:** - `book_list`: A list of dictionaries. Each dictionary has the following structure: ```python { \\"title\\": str, \\"author\\": str, \\"year\\": int, \\"genre\\": str } ``` **Output:** - A dictionary where keys are genres and values are dictionaries containing: - The count of books in that genre (\'count\'). - The average publication year of the books in that genre (\'average_year\'). **Constraints:** 1. Assume all books have valid information. 2. The list will have at least one book. 3. You cannot use third-party libraries; standard Python only. **Example:** ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author A\\", \\"year\\": 2001, \\"genre\\": \\"Fantasy\\"}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author B\\", \\"year\\": 2003, \\"genre\\": \\"Fantasy\\"}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author C\\", \\"year\\": 1999, \\"genre\\": \\"Science Fiction\\"}, {\\"title\\": \\"Book D\\", \\"author\\": \\"Author D\\", \\"year\\": 2010, \\"genre\\": \\"Fantasy\\"}, {\\"title\\": \\"Book E\\", \\"author\\": \\"Author E\\", \\"year\\": 2005, \\"genre\\": \\"Science Fiction\\"} ] # The function call result = summarize_books(books) # Expected output { \\"Fantasy\\": {\\"count\\": 3, \\"average_year\\": 2004.67}, \\"Science Fiction\\": {\\"count\\": 2, \\"average_year\\": 2002.0} } ``` **Additional Notes:** - When computing the average year, round the result to 2 decimal places. - Carefully handle the dictionary operations and edge cases such as having only one book in a genre. - Write the function docstring to describe its purpose and inputs/outputs clearly. In your implementation, make sure to: - Use loops and conditionals efficiently. - Leverage various types of function arguments and unpacking if necessary. - Include basic error checking to ensure input data is as expected. **Hint:** Consider using nested dictionary structures to keep track of genre-specific data during the iteration over the input list.","solution":"def summarize_books(book_list: list) -> dict: Summarizes the given list of books by their genres. Args: book_list (list): A list of dictionaries where each dictionary represents a book with keys \'title\', \'author\', \'year\', and \'genre\'. Returns: dict: A dictionary where each key is a genre and the value is another dictionary containing \'count\' of books in that genre and \'average_year\' of publication. summary = {} for book in book_list: genre = book[\'genre\'] year = book[\'year\'] if genre not in summary: summary[genre] = {\'count\': 0, \'total_years\': 0} summary[genre][\'count\'] += 1 summary[genre][\'total_years\'] += year for genre in summary: count = summary[genre][\'count\'] total_years = summary[genre][\'total_years\'] average_year = round(total_years / count, 2) summary[genre][\'average_year\'] = average_year del summary[genre][\'total_years\'] return summary"},{"question":"You have been given the task of developing a utility that classifies sound files and performs additional processing based on the type of sound file. This utility should categorize sound files into three main types: \'wav\', \'aiff-like\' (consisting of \'aiff\' and \'aifc\'), and \'other\'. For each category, you will perform different processing and generate a summary report. # Requirements 1. **Function to classify and process sound files:** Implement a function named `process_sound_files` which accepts a list of filenames (string) and returns a dictionary with the following structure: ```python { \'wav\': { \'count\': int, \'total_frames\': int, \'unique_sample_widths\': [list of unique sample widths] }, \'aiff_like\': { \'count\': int, \'total_channels\': int, \'unique_framerates\': [list of unique framerates] }, \'other\': { \'count\': int, \'file_types\': [list of unique file types] }, \'failed\': [list of filenames that could not be processed] } ``` 2. **Classification Rules:** - \'wav\': Any file that identifies as \'wav\'. - \'aiff_like\': Any file that identifies as \'aiff\' or \'aifc\'. - \'other\': Any other sound file types. 3. **Processing Rules:** - For \'wav\' files, count the total number of frames and collect unique sample widths. - For \'aiff_like\' files, count the total number of channels and collect unique framerates. - For \'other\' files, collect unique file types. - Track any filenames that could not be processed and include them in the \'failed\' list. # Input Format - A list of strings, where each string is a filename. # Output Format - A dictionary with the above-mentioned structure. # Example ```python input_files = [\'sound1.wav\', \'sound2.aiff\', \'sound3.aifc\', \'sound4.mp3\', \'nonexistentfile.wav\'] result = process_sound_files(input_files) ``` Sample output: ```python { \'wav\': { \'count\': 1, \'total_frames\': 10240, \'unique_sample_widths\': [16] }, \'aiff_like\': { \'count\': 2, \'total_channels\': 4, \'unique_framerates\': [44100, 48000] }, \'other\': { \'count\': 1, \'file_types\': [\'mp3\'] }, \'failed\': [\'nonexistentfile.wav\'] } ``` # Constraints - Assume that the filenames provided as input are accessible and the files exist unless otherwise specified. - You are allowed to use the `sndhdr` module to determine the type of sound files. # Edge Cases - Input list contains filenames that do not exist. - List contains files that are not sound files at all. - Files that might be empty or corrupted.","solution":"import sndhdr def process_sound_files(filenames): result = { \'wav\': { \'count\': 0, \'total_frames\': 0, \'unique_sample_widths\': [] }, \'aiff_like\': { \'count\': 0, \'total_channels\': 0, \'unique_framerates\': [] }, \'other\': { \'count\': 0, \'file_types\': [] }, \'failed\': [] } def add_unique(lst, item): if item not in lst: lst.append(item) for filename in filenames: try: file_info = sndhdr.what(filename) if file_info is None: result[\'failed\'].append(filename) continue filetype = file_info[0] if filetype == \'wav\': result[\'wav\'][\'count\'] += 1 result[\'wav\'][\'total_frames\'] += file_info[3] add_unique(result[\'wav\'][\'unique_sample_widths\'], file_info[4]) elif filetype in [\'aiff\', \'aifc\']: result[\'aiff_like\'][\'count\'] += 1 result[\'aiff_like\'][\'total_channels\'] += file_info[1] add_unique(result[\'aiff_like\'][\'unique_framerates\'], file_info[2]) else: result[\'other\'][\'count\'] += 1 add_unique(result[\'other\'][\'file_types\'], filetype) except Exception: result[\'failed\'].append(filename) return result"},{"question":"# Event Scheduler Implementation You are tasked with creating a lightweight event-based notification system that uses Python\'s `sched` module. The system needs to handle different types of notifications at specified times or after certain delays. Requirements 1. **Functions to Schedule Events**: - **Schedule notification at a specific time**: ```python def schedule_notification_at(scheduler, notification_time, message): Schedule a notification event at an absolute time. Args: - scheduler: An instance of sched.scheduler. - notification_time: A float representing the absolute time (e.g., time.time() + 60 for 60 seconds from now). - message: A string message to display when the notification is triggered. Returns: - None ``` - **Schedule notification after a delay**: ```python def schedule_notification_in(scheduler, delay, message): Schedule a notification event after a specified delay. Args: - scheduler: An instance of sched.scheduler. - delay: A float representing the delay in seconds. - message: A string message to display when the notification is triggered. Returns: - None ``` 2. **Main Function to Handle Notifications**: ```python def notification_scheduler(): Main function to handle scheduling and running the notification events. This function should: - Create an instance of the scheduler. - Allow scheduling notifications both at absolute times and after delays. - Utilize the `run` method to execute scheduled events. Returns: - None ``` Input and Output - Input: - Use the Python `sched` module. - Absolute or relative time to schedule the notification. - Message string for the notification. - Output: - Upon the scheduled time, the message should be printed to the console. - Ensure the scheduler can handle multiple notifications and correctly prints them based on their scheduling priorities. Constraints and Considerations - Do not use global variables. Your code should be well-contained within the defined functions. - Use the `sched` module appropriately to manage timings and delays. - Ensure that your solution handles multiple notifications effectively, maintaining the scheduling priorities as specified. - The `scheduler.run()` method should be called to start the scheduled events. Example Usage Here\'s an example of how your functions might be used: ```python if __name__ == \\"__main__\\": notification_scheduler() ``` The `notification_scheduler` should allow you to test scheduling both immediate and delayed notifications. Use `time.time()` and `time.sleep()` for absolute and delayed times, respectively. **Good Luck!**","solution":"import sched import time def schedule_notification_at(scheduler, notification_time, message): Schedule a notification event at an absolute time. Args: - scheduler: An instance of sched.scheduler. - notification_time: A float representing the absolute time (e.g., time.time() + 60 for 60 seconds from now). - message: A string message to display when the notification is triggered. Returns: - None scheduler.enterabs(notification_time, 1, print, argument=(message,)) def schedule_notification_in(scheduler, delay, message): Schedule a notification event after a specified delay. Args: - scheduler: An instance of sched.scheduler. - delay: A float representing the delay in seconds. - message: A string message to display when the notification is triggered. Returns: - None scheduler.enter(delay, 1, print, argument=(message,)) def notification_scheduler(): Main function to handle scheduling and running the notification events. This function should: - Create an instance of the scheduler. - Allow scheduling notifications both at absolute times and after delays. - Utilize the `run` method to execute scheduled events. Returns: - None scheduler = sched.scheduler(time.time, time.sleep) # Example usage schedule_notification_in(scheduler, 5, \\"Notification after 5 seconds\\") schedule_notification_at(scheduler, time.time() + 10, \\"Notification at 10 seconds from now\\") # Run the scheduler to process the events scheduler.run()"},{"question":"**Question:** Write a Python function `batch_compile(files, optimize=-1, quiet=0, invalidation_mode=\'TIMESTAMP\')` that takes in a list of file paths and compiles them to byte-code using the `py_compile` module. Your function should: 1. Compile each file in the list using the specified optimization level, quiet mode, and invalidation mode. 2. Log any errors encountered during compilation to a file named `compilation_errors.log`. 3. Return a dictionary where the keys are the original file paths and the values are the paths to the compiled `.pyc` files, or an error message if the compilation failed. 4. Use `PycInvalidationMode` for setting the invalidation mode. **Function Signature:** ```python def batch_compile(files: list[str], optimize: int = -1, quiet: int = 0, invalidation_mode: str = \'TIMESTAMP\') -> dict: ``` **Input:** - `files`: A list of strings representing the file paths to be compiled. - `optimize`: An integer representing the optimization level for compilation (default -1). - `quiet`: An integer (0, 1, or 2) controlling the verbosity of error messages (default 0). - `invalidation_mode`: A string representing the invalidation mode (one of \'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\') (default \'TIMESTAMP\'). **Output:** - A dictionary where the keys are file paths and the values are the paths to the compiled `.pyc` files, or an appropriate error message if compilation failed. **Constraints:** - Assume that each file in the `files` list exists and contains valid Python code. - You must handle any exceptions raised by the `py_compile.compile()` function. - The `invalidation_mode` parameter must be one of the valid `PycInvalidationMode` values. **Example:** ```python files = [\\"script1.py\\", \\"script2.py\\", \\"script3.py\\"] result = batch_compile(files, optimize=2, quiet=1, invalidation_mode=\'CHECKED_HASH\') # Example output { \\"script1.py\\": \\"/path/to/__pycache__/script1.cpython-310.pyc\\", \\"script2.py\\": \\"Error compiling script2.py: <error details>\\", \\"script3.py\\": \\"/path/to/__pycache__/script3.cpython-310.pyc\\" } ``` This question assesses the student\'s ability to: - Work with file I/O and logging. - Handle errors and exceptions. - Use Python enums and compile functions effectively. - Implement function parameters and return structured data.","solution":"import py_compile import logging from importlib.util import MAGIC_NUMBER from py_compile import PycInvalidationMode def batch_compile(files, optimize=-1, quiet=0, invalidation_mode=\'TIMESTAMP\'): Compiles a list of Python source files to byte-code. Args: - files (list[str]): List of file paths to compile. - optimize (int): Optimization level (default is -1). - quiet (int): Quiet mode (0, 1, or 2) (default is 0). - invalidation_mode (str): Invalidation mode \'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\' (default is \'TIMESTAMP\'). Returns: - dict: A dictionary with keys as file paths and values as paths to compiled files or error messages. result = {} # Validate invalidation_mode try: invalidation_mode_enum = PycInvalidationMode[invalidation_mode] except KeyError: raise ValueError(f\\"Invalid invalidation_mode: \'{invalidation_mode}\'\\") # Setup logging logging.basicConfig(filename=\'compilation_errors.log\', level=logging.ERROR) for file in files: try: compiled_path = py_compile.compile(file, cfile=None, dfile=None, doraise=True, optimize=optimize, quiet=quiet, invalidation_mode=invalidation_mode_enum) result[file] = compiled_path except Exception as e: error_message = f\\"Error compiling {file}: {str(e)}\\" logging.error(error_message) result[file] = error_message return result"},{"question":"**Coding Assessment Question** # Objective: Implement a function that parses an XML string, modifies its structure, and returns the modified XML string. Demonstrate understanding of the `xml.dom` module in Python. # Problem Statement: You are given an XML string representing a simple XML document. Your task is to write a function `modify_xml(xml_str: str) -> str` that: 1. Parses the provided XML string into a DOM structure. 2. Adds a new child element `<category>` with text content \\"Science\\" to each `<book>` element. 3. Removes all `<price>` elements under the `<book>` elements. 4. Returns the modified XML as a string. # Requirements: - Use the `xml.dom.minidom` module for parsing and manipulating the XML. - Ensure the output XML string is well-formed. - Maintain the original order of the existing elements, except for the modifications made. - Assume the input XML string is well-formed and valid. # Function Signature: ```python def modify_xml(xml_str: str) -> str: pass ``` # Input: - `xml_str`: A string representing the XML document. # Output: - A string representing the modified XML document. # Example: **Input:** ```xml <?xml version=\\"1.0\\"?> <library> <book> <title>Python Programming</title> <author>John Doe</author> <price>29.99</price> </book> <book> <title>Data Science</title> <author>Jane Smith</author> <price>39.99</price> </book> </library> ``` **Output:** ```xml <?xml version=\\"1.0\\" ?> <library> <book> <title>Python Programming</title> <author>John Doe</author> <category>Science</category> </book> <book> <title>Data Science</title> <author>Jane Smith</author> <category>Science</category> </book> </library> ``` # Constraints: - Do not use any third-party libraries for XML manipulation. - Stick to the `xml.dom` module available in the Python standard library. # Hints: - Use `xml.dom.minidom.parseString` to parse the XML string. - Use methods like `createElement`, `createTextNode`, `appendChild`, and `removeChild` for DOM manipulation. - Use `toprettyxml` or `toxml` methods to generate the output XML string. **Note:** Make sure your function properly handles XML declaration and indentation in the output.","solution":"from xml.dom.minidom import parseString def modify_xml(xml_str: str) -> str: # Parse the XML string dom = parseString(xml_str) # Get all book elements books = dom.getElementsByTagName(\\"book\\") for book in books: # Add new <category> element with text content \\"Science\\" category_element = dom.createElement(\\"category\\") category_text = dom.createTextNode(\\"Science\\") category_element.appendChild(category_text) book.appendChild(category_element) # Remove all <price> elements prices = book.getElementsByTagName(\\"price\\") for price in prices: book.removeChild(price) # Return the modified XML string return dom.toxml()"},{"question":"# Function Call Tracing with SystemTap and DTrace in CPython **Objective:** Implement a Python script that monitors the call and return of specific functions using simulated markers. The goal is to emulate a simplified version of the DTrace/SystemTap functionality described in the provided documentation. **Task:** Write a Python script that simulates the tracing of function calls and returns. You must create a decorator that wraps functions to log their call details, including filename, function name, and line number, when they are invoked and when they return. This simulation should generate a trace log similar to what DTrace and SystemTap would produce. **Guidelines:** 1. Implement a decorator named `trace_function`. 2. The decorator should log: - The timestamp of the function call/return. - The function name. - The filename where the function is defined. - The line number where the function is invoked or returns. 3. You should use the `inspect` module to obtain function metadata. 4. Apply this decorator to a few sample functions to demonstrate its usage. **Input/Output:** - The `trace_function` decorator should be applied to demonstrate tracing. - No explicit input is needed for the functions; you can define and call within the script. - Output should be printed directly to the console, resembling the format: ``` [timestamp] function-entry: filename:funcname:lineno [timestamp] function-return: filename:funcname:lineno ``` **Constraints:** - You should handle arbitrary function signatures. - Ensure log messages are clear and in the correct order. **Performance Requirements:** - The solution should efficiently handle function tracing without causing significant overhead. **Example:** ```python import time import inspect def trace_function(func): def wrapper(*args, **kwargs): filename = inspect.getfile(func) funcname = func.__name__ lineno_start = func.__code__.co_firstlineno timestamp = int(time.time() * 1000) # Log function entry print(f\\"{timestamp} function-entry: {filename}:{funcname}:{lineno_start}\\") result = func(*args, **kwargs) # Log function return lineno_return = inspect.currentframe().f_lineno timestamp = int(time.time() * 1000) print(f\\"{timestamp} function-return: {filename}:{funcname}:{lineno_return}\\") return result return wrapper @trace_function def sample_function_1(): print(\\"Executing function 1\\") @trace_function def sample_function_2(x): print(f\\"Executing function 2 with argument {x}\\") sample_function_1() sample_function_1() sample_function_2(10) ``` Expected Output: ``` [timestamp] function-entry: script.py:sample_function_1:21 Executing function 1 [timestamp] function-return: script.py:sample_function_1:24 [timestamp] function-entry: script.py:sample_function_2:26 Executing function 2 with argument 10 [timestamp] function-entry: script.py:sample_function_1:21 Executing function 1 [timestamp] function-return: script.py:sample_function_1:24 [timestamp] function-return: script.py:sample_function_2:30 ``` **HINT:** Use `inspect.currentframe().f_lineno` to dynamically obtain and print the current executing line number within the function.","solution":"import time import inspect def trace_function(func): def wrapper(*args, **kwargs): filename = inspect.getfile(func) funcname = func.__name__ lineno_start = func.__code__.co_firstlineno timestamp_start = int(time.time() * 1000) # Log function entry print(f\\"{timestamp_start} function-entry: {filename}:{funcname}:{lineno_start}\\") result = func(*args, **kwargs) # Log function return lineno_return = inspect.currentframe().f_lineno timestamp_return = int(time.time() * 1000) print(f\\"{timestamp_return} function-return: {filename}:{funcname}:{lineno_return}\\") return result return wrapper @trace_function def sample_function_1(): print(\\"Executing function 1\\") @trace_function def sample_function_2(x): print(f\\"Executing function 2 with argument {x}\\") sample_function_1() sample_function_1() sample_function_2(10)"},{"question":"**Question: Advanced Sound File Analysis with `sndhdr` Module** You are tasked with writing a function that performs advanced sound file analysis using Python’s deprecated `sndhdr` module. The goal is to determine the type of sound data stored in multiple files and collate their information in a structured format. # Function Signature ```python def analyze_sound_files(filepaths: list) -> dict: ``` # Input - `filepaths`: A list of strings, where each string is the path to a sound file on the system. # Output - A dictionary where: - Each key is the file path from the input list. - Each value is another dictionary with the following keys: - `\'filetype\'`: The type of sound file. - `\'framerate\'`: The frame rate of the sound file. - `\'nchannels\'`: The number of audio channels. - `\'nframes\'`: The number of frames in the sound file. - `\'sampwidth\'`: The sample width in bits. If the file type cannot be determined for a given file, the corresponding value should be `None`. # Constraints - Use the `sndhdr.what` function to determine the sound file type. - Your implementation should handle edge cases like empty input lists and invalid file paths gracefully. - You should also ensure your function can handle any exceptions that might occur during the analysis of each file. # Example ```python filepaths = [\\"/path/to/sound1.wav\\", \\"/path/to/non-sound.txt\\", \\"/path/to/sound2.aiff\\"] result = analyze_sound_files(filepaths) print(result) ``` Possible Output: ```python { \\"/path/to/sound1.wav\\": { \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": 100000, \\"sampwidth\\": 16 }, \\"/path/to/non-sound.txt\\": None, \\"/path/to/sound2.aiff\\": { \\"filetype\\": \\"aiff\\", \\"framerate\\": 48000, \\"nchannels\\": 2, \\"nframes\\": 120000, \\"sampwidth\\": 24 } } ``` # Notes - This exercise helps assess your ability to use Python\'s file handling and exception management features, understand and utilize deprecated modules, and process lists and dictionaries. - Make sure your code is clean and well-commented, ensuring it explains key steps and handles potential errors gracefully.","solution":"import sndhdr def analyze_sound_files(filepaths): Analyzes a list of sound files and returns their audio properties. Parameters: filepaths (list): List of strings, where each string is the path to a sound file on the system. Returns: dict: A dictionary where keys are file paths and values are another dictionary with keys: \'filetype\', \'framerate\', \'nchannels\', \'nframes\', \'sampwidth\' or None if file type can\'t be determined. result = {} for filepath in filepaths: try: file_info = sndhdr.what(filepath) if file_info: result[filepath] = { \\"filetype\\": file_info.filetype, \\"framerate\\": file_info.framerate, \\"nchannels\\": file_info.nchannels, \\"nframes\\": file_info.nframes, \\"sampwidth\\": file_info.sampwidth } else: result[filepath] = None except Exception as e: result[filepath] = None return result"},{"question":"# **Advanced Python Coding Assessment: Custom Iterator and Generator** **Objective:** To assess the understanding of creating custom sequence types in Python using iterators and generators. **Problem Statement:** You are required to design a custom sequence type named `RangeSeq` that mimics some behavior of Python\'s built-in `range` type but allows for floating-point increments and supports iteration and slicing. You must implement the following functionalities: 1. **Initialization:** - The sequence should be initialized with three parameters: `start`, `end`, and `step`. - Ensure that there are validations so that improper combinations of `start`, `end`, and `step` raise suitable exceptions. 2. **Iteration:** - Implement the iterator protocol to allow iteration over elements in the sequence. 3. **Slicing:** - Implement slicing for the sequence, where the slice method should return a new `RangeSeq` object representing the sliced part of the sequence. 4. **Generator:** - Implement a generator method `range_gen` within the `RangeSeq` class that yields elements one by one. # **Detailed Requirements:** 1. **`__init__` Method:** - Initialize the `RangeSeq` with `start`, `end`, and `step`. - Example: `seq = RangeSeq(0.0, 5.0, 0.5)` creates a sequence from `0.0` to `5.0` with a step of `0.5`. 2. **`__iter__` and `__next__` Methods:** - These methods should enable iteration over the sequence. - Example: ```python seq = RangeSeq(0.0, 5.0, 0.5) for val in seq: print(val) # Output: 0.0, 0.5, 1.0, ... , 4.5 ``` 3. **`__getitem__` Method (Slicing):** - Implement slicing to return a new `RangeSeq` object. - Example: ```python seq = RangeSeq(0.0, 5.0, 0.5) sub_seq = seq[1:5] ``` 4. **`range_gen` Method:** - Implement a generator function within the class that yields elements one by one. - Example: ```python seq = RangeSeq(0.0, 5.0, 0.5) gen = seq.range_gen() print(next(gen)) # Outputs 0.0 ``` 5. **Exceptions:** - Ensure proper validation of the `start`, `end`, and `step` values. For example, `step` should not be zero. # **Function Signature:** ```python class RangeSeq: def __init__(self, start: float, end: float, step: float): pass def __iter__(self): pass def __next__(self): pass def __getitem__(self, index: slice): pass def range_gen(self): pass ``` # **Constraints:** - The `start`, `end`, and `step` must be floating-point numbers. - Raise `ValueError` if `step` is zero. - The sequence must support the standard sequence operations like length calculation using `len()`. # **Sample Test Cases:** ```python # Initialization and iteration seq = RangeSeq(0.0, 5.0, 0.5) print([val for val in seq]) # Prints [0.0, 0.5, 1.0, 1.5, ..., 4.5] # Slicing sub_seq = seq[1:5] print([val for val in sub_seq]) # Prints [0.5, 1.0, 1.5, 2.0] # Generator gen = seq.range_gen() print(next(gen)) # Prints 0.0 print(next(gen)) # Prints 0.5 # Invalid step try: seq_invalid = RangeSeq(0.0, 5.0, 0.0) except ValueError as e: print(e) # Prints \\"step must not be zero\\" ``` # **Evaluation Criteria:** - Correctness of sequence initialization and iteration. - Proper implementation of the iterator and slicing protocols. - Effective use of generators. - Handling edge cases and raising appropriate exceptions.","solution":"class RangeSeq: def __init__(self, start: float, end: float, step: float): if step == 0: raise ValueError(\\"step must not be zero\\") self.start = start self.end = end self.step = step self.current = start def __iter__(self): self.current = self.start return self def __next__(self): if (self.step > 0 and self.current >= self.end) or (self.step < 0 and self.current <= self.end): raise StopIteration result = self.current self.current += self.step return result def __getitem__(self, index: slice): if not isinstance(index, slice): raise TypeError(\\"Index must be a slice\\") start = self.start if index.start is None else self.start + index.start * self.step end = self.end if index.stop is None else self.start + index.stop * self.step step = self.step if index.step is None else index.step * self.step return RangeSeq(start, end, step) def range_gen(self): current = self.start while (self.step > 0 and current < self.end) or (self.step < 0 and current > self.end): yield current current += self.step"},{"question":"# Custom Pickle Implementation with `copyreg` Using the `copyreg` module, create a class that represents a complex number in a user-defined way, and register custom pickling and unpickling functions that will be used to serialize and deserialize objects of this class. Requirements: 1. Implement a class `ComplexNumber` that represents a complex number. - The class should have two attributes: `real` and `imaginary`, representing the real and imaginary parts of the complex number, respectively. - Implement the `__init__`, `__repr__`, and `__eq__` methods. 2. Implement a function `pickle_complexnumber` that: - Takes a `ComplexNumber` object as input. - Returns a tuple containing the class `ComplexNumber` and a tuple of the real and imaginary parts of the instance. 3. Register the pickling function using `copyreg.pickle`. 4. Demonstrate that your class can be pickled and unpickled correctly by: - Creating an instance of `ComplexNumber`. - Pickling the instance using `pickle.dumps`. - Unpickling to retrieve the original object using `pickle.loads`. - Confirming that the deserialized object is equal to the original using the `==` operator. Constraints: - The `ComplexNumber` class must handle real and imaginary parts that are either `int` or `float`. Example: ```python >>> c = ComplexNumber(3.0, -4.5) >>> serialized = pickle.dumps(c) >>> deserialized = pickle.loads(serialized) >>> assert c == deserialized ``` Implement the solution in the cell provided. Notes: - Do not use the built-in `complex` type. Define the complex number structure using a custom class as specified.","solution":"import copyreg import pickle class ComplexNumber: def __init__(self, real, imaginary): self.real = real self.imaginary = imaginary def __repr__(self): return f\\"ComplexNumber({self.real}, {self.imaginary})\\" def __eq__(self, other): if not isinstance(other, ComplexNumber): return False return self.real == other.real and self.imaginary == other.imaginary def pickle_complexnumber(obj): return ComplexNumber, (obj.real, obj.imaginary) copyreg.pickle(ComplexNumber, pickle_complexnumber) # Example usage for demonstration c = ComplexNumber(3.0, -4.5) serialized = pickle.dumps(c) deserialized = pickle.loads(serialized) assert c == deserialized"},{"question":"# Seaborn Style Management You are given a dataset and your task is to create two plots using seaborn with different styles and then combine them in a single figure side by side. Use the context manager to temporarily apply a different style for one of the plots. Input 1. A dataset in the form of two lists or arrays: `x` and `y`. 2. Two seaborn styles: `style1` and `style2`. Output A single figure containing two plots side by side, each with a different seaborn style applied. Constraints 1. Use `seaborn` for all plotting. 2. The styles for the plots should be applied using the `axes_style` function and context manager. Example ```python import seaborn as sns import matplotlib.pyplot as plt x = [1, 2, 3, 4, 5] y = [3, 1, 4, 1, 5] style1 = \\"darkgrid\\" style2 = \\"whitegrid\\" # Your code to create the plots with the specified styles ``` Expected output: The figure should display two bar plots side by side. The first plot should use `style1`, and the second plot should use `style2`. **Note: You need to ensure that the plots should be clearly distinguishable by their styles.**","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_styled_plots(x, y, style1, style2): Creates two side-by-side plots with different seaborn styles. Parameters: x (list): The x values for the plots. y (list): The y values for the plots. style1 (str): The seaborn style for the first plot. style2 (str): The seaborn style for the second plot. Returns: fig (Figure): The resulting matplotlib figure object. # Create the figure and two subplots fig, axs = plt.subplots(1, 2, figsize=(12, 6)) # Apply the first style to the first subplot with sns.axes_style(style1): sns.barplot(x=x, y=y, ax=axs[0]) axs[0].set_title(f\'Style: {style1}\') # Apply the second style to the second subplot with sns.axes_style(style2): sns.barplot(x=x, y=y, ax=axs[1]) axs[1].set_title(f\'Style: {style2}\') return fig"},{"question":"# Async Generator and Resource Management Task Objective Design an async function in Python that processes data from an asynchronous data source, ensures proper resource management, and handles potential exceptions gracefully. Problem Statement You are given an asynchronous data source function `async_data_source` that yields a stream of numerical values. Your task is to implement an async function `process_data_with_logging` that performs the following: 1. **Read from the asynchronous data source**: Fetch and process the data until the source is exhausted. 2. **Logging**: Use a context manager to log the start and end of data processing. 3. **Exception Handling**: If an exception occurs during data processing, log the error and ensure that no resources are leaked. 4. **Processing Logic**: For each data value: - If the value is even, store it in an `even_numbers` list. - If the value is odd, store it in an `odd_numbers` list. - Raise an exception if the value is negative. Input There is no direct input to the program. The data source function simulates input by yielding values. Output An async function that doesn\'t return any value but manages internal lists and logs messages. Function Signature ```python async def process_data_with_logging(): ``` Constraints - You must use the `async with` statement for logging. - Ensure your function is fully asynchronous. - Implement the function using compound statements and handle different control flow paths. - Aim for clear, readable, and idiomatic Python code. Example Here is a hypothetical example demonstrating the expected logging behavior: ```python async def async_data_source(): yield 5 yield -3 # This will trigger an exception in your function yield 10 # When calling process_data_with_logging (assuming the above data source) # The logging might look like: # Start processing data # Error: Negative value encountered -3 # End processing data ``` Your implementation should log the start and end of processing, and any errors encountered. You don\'t need to implement the `async_data_source` function, but here\'s a sample implementation for your reference: ```python import random import asyncio async def async_data_source(): for _ in range(10): await asyncio.sleep(0.1) yield random.choice([i for i in range(-5, 11)]) # Example data range ``` Evaluation Criteria - Correctness: The solution correctly implements asynchronous processing, logging, and exception handling. - Use of compound statements and async syntax as specified. - Code readability and adherence to Python conventions. - Robustness: Proper handling of edge cases like negative values and exhaustion of data source.","solution":"import asyncio import logging logging.basicConfig(level=logging.INFO) class LogContext: def __init__(self, start_msg, end_msg): self.start_msg = start_msg self.end_msg = end_msg async def __aenter__(self): logging.info(self.start_msg) async def __aexit__(self, exc_type, exc, tb): if exc: logging.error(f\\"Error: {exc}\\") logging.info(self.end_msg) async def process_data_with_logging(): even_numbers = [] odd_numbers = [] async def async_data_source(): for number in range(10): await asyncio.sleep(0.1) yield number async with LogContext(\\"Start processing data\\", \\"End processing data\\"): async for number in async_data_source(): if number < 0: raise ValueError(f\\"Negative value encountered {number}\\") elif number % 2 == 0: even_numbers.append(number) else: odd_numbers.append(number) # If this were run in a real async environment, the following would kick off the processing: # asyncio.run(process_data_with_logging())"},{"question":"**Objective:** Implement a secure communication protocol using the `hmac` module from Python\'s standard library. This protocol will involve creating and verifying HMACs to ensure message integrity and authentication between a sender and a receiver. **Problem Statement:** You are required to write two classes: `Sender` and `Receiver`. The `Sender` class will create and send messages with HMACs attached, while the `Receiver` class will verify the HMACs of the received messages to ensure they have not been tampered with. Class: `Sender` - **Methods:** - `__init__(self, key: bytes, digestmod: str)`: Initializes the Sender with a secret key and a digest method. - `send_message(self, message: str) -> dict`: Takes a message as input, computes its HMAC, and returns a dictionary containing the message and its HMAC. ```python class Sender: def __init__(self, key: bytes, digestmod: str): # Implement initialization def send_message(self, message: str) -> dict: # Implement message sending with HMAC generation ``` Class: `Receiver` - **Methods:** - `__init__(self, key: bytes, digestmod: str)`: Initializes the Receiver with a secret key and a digest method. - `receive_message(self, message_dict: dict) -> bool`: Takes a dictionary containing the message and its HMAC, verifies the HMAC, and returns `True` if the message is authentic, `False` otherwise. ```python class Receiver: def __init__(self, key: bytes, digestmod: str): # Implement initialization def receive_message(self, message_dict: dict) -> bool: # Implement message verification ``` Constraints: - Use the `hmac` module for HMAC generation and verification. - The secret key will be a bytes object and provided to both the `Sender` and `Receiver`. - The digest method will be a string (e.g., `\'sha256\'`). Example: ```python # Secret key and digest method key = b\'supersecretkey\' digestmod = \'sha256\' # Create sender and receiver sender = Sender(key, digestmod) receiver = Receiver(key, digestmod) # Send a message message = \\"This is a secret message.\\" message_dict = sender.send_message(message) # Verify the message assert receiver.receive_message(message_dict) == True # Tamper with the message message_dict[\'message\'] = \\"This is a tampered message.\\" # Verify the tampered message assert receiver.receive_message(message_dict) == False ``` # Notes: - Ensure to handle the conversion of the message to `bytes` when computing the HMAC. - Use `hmac.compare_digest` for secure comparison of HMACs.","solution":"import hmac import hashlib class Sender: def __init__(self, key: bytes, digestmod: str): self.key = key self.digestmod = digestmod def send_message(self, message: str) -> dict: hmac_generator = hmac.new(self.key, message.encode(), self.digestmod) hmac_digest = hmac_generator.hexdigest() return { \'message\': message, \'hmac\': hmac_digest } class Receiver: def __init__(self, key: bytes, digestmod: str): self.key = key self.digestmod = digestmod def receive_message(self, message_dict: dict) -> bool: message = message_dict[\'message\'] hmac_received = message_dict[\'hmac\'] hmac_generator = hmac.new(self.key, message.encode(), self.digestmod) hmac_digest_generated = hmac_generator.hexdigest() return hmac.compare_digest(hmac_received, hmac_digest_generated)"},{"question":"Objective: Design a Python program using the `multiprocessing` module to simulate a simplified task scheduler with the following requirements. Problem Statement: You are tasked with creating a simplified multiprocessing-based task scheduler. The scheduler should be able to handle multiple tasks submitted by the user. Each task should be processed by a worker process, and the results should be returned and stored in a result queue. Additionally, implement a logging mechanism to keep track of the scheduler’s activities. Requirements: 1. **Task Submission**: Implement a function to submit tasks to the scheduler. 2. **Worker Processes**: Use a pool of worker processes to execute submitted tasks concurrently. 3. **Result Queue**: Ensure results from the tasks are collected in a thread-safe queue. 4. **Logging**: Implement logging to track when tasks are submitted, started, and completed. 5. **Synchronization**: Use appropriate synchronization primitives to handle shared state or resource access if necessary. Function Specifications: 1. `submit_task(func: callable, args: tuple)`: Takes a function and its arguments, then submits the task to the scheduler for execution. 2. `get_all_results() -> list`: Returns all results collected from the completed tasks. 3. `shutdown_scheduler()`: Gracefully shuts down the scheduler, ensuring all running tasks are completed and resources are cleaned up. Constraints: - Use a pool with a maximum of 4 worker processes. - Each task function and its arguments should be picklable. - Ensure all logs are captured to a file named `scheduler.log`. - Implement proper exception handling to capture and log errors during task execution. Example Usage: ```python if __name__ == \'__main__\': from time import sleep def example_task(duration): sleep(duration) return f\\"Task completed after {duration} seconds\\" # Initialize the scheduler initialize_scheduler() # Submit tasks submit_task(example_task, (2,)) submit_task(example_task, (3,)) submit_task(example_task, (1,)) # Retrieve results results = get_all_results() for result in results: print(result) # Shutdown the scheduler shutdown_scheduler() ``` Notes: - Your implementation should ensure that the scheduler does not block indefinitely. - Provide adequate logging information to track each stage of task processing. - Consider edge cases such as what happens if a task function raises an exception.","solution":"import multiprocessing import logging from queue import Queue, Empty from multiprocessing import Pool, Manager # Initialize logger logging.basicConfig(filename=\'scheduler.log\', level=logging.INFO, format=\'%(asctime)s %(message)s\') # Initialize global variables global_manager = Manager() global_result_queue = global_manager.Queue() global_pool = None def initialize_scheduler(): global global_pool global_pool = Pool(processes=4) logging.info(\\"Scheduler initialized with 4 worker processes.\\") def submit_task(func, args): try: result = global_pool.apply_async(func, args, callback=collect_result, error_callback=log_error) logging.info(f\\"Task submitted: {func.__name__} with args: {args}\\") except Exception as e: logging.error(f\\"Error in submitting task: {e}\\") def collect_result(result): global_result_queue.put(result) logging.info(f\\"Task completed with result: {result}\\") def log_error(error): logging.error(f\\"Task resulted in error: {error}\\") def get_all_results(): results = [] while True: try: results.append(global_result_queue.get_nowait()) except Empty: break return results def shutdown_scheduler(): global global_pool global_pool.close() global_pool.join() logging.info(\\"Scheduler shutdown gracefully.\\")"},{"question":"In this task, you will demonstrate your understanding of Python dictionary operations and your ability to implement low-level dictionary functionalities in high-level Python. You will mimic some of the behaviors described in the given C-style dictionary methods using Python. # Task: Implement Custom Dictionary Functions You are required to implement a class `CustomDict` that replicates several key behaviors of Python\'s dictionary methods as described in the provided C documentation. Your `CustomDict` must include the following methods: 1. **`__init__(self)`**: Initializes a new empty dictionary. 2. **`set_item(self, key, value)`**: Inserts `value` into the dictionary with the key `key`. If the key already exists, updates its value. 3. **`get_item(self, key)`**: Returns the value associated with `key` if `key` is present, otherwise, raises a `KeyError`. 4. **`del_item(self, key)`**: Removes the entry with `key` from the dictionary. If `key` is not present, raises a `KeyError`. 5. **`contains(self, key)`**: Returns `True` if `key` is present in the dictionary, otherwise returns `False`. 6. **`clear(self)`**: Removes all key-value pairs from the dictionary. 7. **`items(self)`**: Returns a list of tuples containing all the key-value pairs in the dictionary. 8. **`keys(self)`**: Returns a list of all keys in the dictionary. 9. **`values(self)`**: Returns a list of all values in the dictionary. 10. **`size(self)`**: Returns the number of key-value pairs in the dictionary. # Example Usage ```python d = CustomDict() d.set_item(\\"a\\", 1) d.set_item(\\"b\\", 2) print(d.get_item(\\"a\\")) # Output: 1 print(d.get_item(\\"b\\")) # Output: 2 print(d.contains(\\"a\\")) # Output: True print(d.contains(\\"c\\")) # Output: False print(d.items()) # Output: [(\\"a\\", 1), (\\"b\\", 2)] print(d.keys()) # Output: [\\"a\\", \\"b\\"] print(d.values()) # Output: [1, 2] print(d.size()) # Output: 2 d.del_item(\\"a\\") print(d.size()) # Output: 1 d.clear() print(d.size()) # Output: 0 ``` # Constraints - Your implementation should ensure that key lookups, insertions, and deletions perform efficiently. # Notes - Do not use Python\'s built-in dictionary (`dict`) to store the data. - You may use any data structure(s) you find appropriate to implement the required functionalities. Implement the `CustomDict` class and its methods to accomplish the above behavior. Ensure to handle all edge cases and errors as specified.","solution":"class CustomDict: def __init__(self): self._data = [] def set_item(self, key, value): for idx, (k, v) in enumerate(self._data): if k == key: self._data[idx] = (key, value) return self._data.append((key, value)) def get_item(self, key): for k, v in self._data: if k == key: return v raise KeyError(f\\"Key {key} not found\\") def del_item(self, key): for idx, (k, v) in enumerate(self._data): if k == key: del self._data[idx] return raise KeyError(f\\"Key {key} not found\\") def contains(self, key): for k, v in self._data: if k == key: return True return False def clear(self): self._data = [] def items(self): return self._data[:] def keys(self): return [k for k, v in self._data] def values(self): return [v for k, v in self._data] def size(self): return len(self._data)"},{"question":"**Context Management with `contextvars` in Asynchronous Programming** # Problem Statement Implement a small program to demonstrate the usage of `contextvars` and how context can be managed manually using the `copy_context()` function and `Context` class in Python. The program should simulate an asynchronous logging system where each log entry has a context-specific request ID. # Task Create a function `log_message` that logs messages with a context-specific request ID. The function should use the following components from the `contextvars` module: - A `ContextVar` to store the request ID. - A `Context` to manage different contexts for logging. Requirements: 1. Create a `ContextVar` named `request_id_var` to store the request ID. 2. Implement `log_message(message: str) -> str` which retrieves the current request ID from `request_id_var` and returns the log entry in the format: `\\"[Request ID: <request_id>] <message>\\"`. 3. Use the `contextvars.Context` and `copy_context()` function to create a new context for each request and log a message within that context. # Constraints: 1. The code should support asynchronous execution using the `asyncio` module. 2. Ensure that the request IDs do not leak between different contexts. 3. Demonstrate the usage of the `Context.run()` method to execute `log_message` within a different context. # Input and Output: - Input: A list of tuples where each tuple contains a request ID and a message, e.g., `[(1, \\"First log message\\"), (2, \\"Second log message\\"), ...]`. - Output: A list of formatted log entries, e.g., `[\\"[Request ID: 1] First log message\\", \\"[Request ID: 2] Second log message\\", ...]`. # Example ```python import asyncio import contextvars # Step 1: Declare the ContextVar request_id_var = contextvars.ContextVar(\'request_id\') # Step 2: Implement the log_message function def log_message(message: str) -> str: request_id = request_id_var.get() return f\\"[Request ID: {request_id}] {message}\\" # Step 3: Asynchronous logging function async def async_log_messages(log_entries: list): results = [] for req_id, message in log_entries: ctx = contextvars.copy_context() ctx.run(request_id_var.set, req_id) ctx.run(lambda: results.append(log_message(message))) return results # Example usage log_entries = [(1, \\"First log message\\"), (2, \\"Second log message\\")] results = asyncio.run(async_log_messages(log_entries)) print(results) ``` # Your Task: Complete the implementation of the `async_log_messages` function so that it processes the log entries correctly as per the provided example. Ensure that the context is managed properly to avoid context leakage.","solution":"import asyncio import contextvars # Step 1: Declare the ContextVar request_id_var = contextvars.ContextVar(\'request_id\') # Step 2: Implement the log_message function def log_message(message: str) -> str: request_id = request_id_var.get() return f\\"[Request ID: {request_id}] {message}\\" # Step 3: Asynchronous logging function async def async_log_messages(log_entries: list): results = [] for req_id, message in log_entries: ctx = contextvars.copy_context() ctx.run(request_id_var.set, req_id) results.append(ctx.run(lambda: log_message(message))) return results # Example usage log_entries = [(1, \\"First log message\\"), (2, \\"Second log message\\")] results = asyncio.run(async_log_messages(log_entries)) results"},{"question":"**Objective:** Write a function that takes in a PyTorch tensor and returns the size of the tensor along with the sum of the dimensions. **Function Signature:** ```python def tensor_size_and_sum(tensor: torch.Tensor) -> (torch.Size, int): ``` **Input:** - `tensor` (torch.Tensor): A PyTorch tensor of any shape. **Output:** - A tuple containing: 1. `size` (torch.Size): The size of the tensor. 2. `dimension_sum` (int): The sum of all dimensions of the tensor. **Constraints:** - The tensor can have any number of dimensions, from 0 (a scalar) and beyond. - You must use the `torch.Size` class to work with the tensor size. - Avoid using loops directly to calculate the sum of dimensions; utilize sequence operations (e.g., sum). **Example Usage:** ```python import torch # Example 1 x1 = torch.ones(2, 3, 4) output1 = tensor_size_and_sum(x1) print(output1) # Expected: (torch.Size([2, 3, 4]), 9) # Example 2 x2 = torch.zeros(5, 5, 5, 5) output2 = tensor_size_and_sum(x2) print(output2) # Expected: (torch.Size([5, 5, 5, 5]), 20) # Example 3 x3 = torch.tensor(7) output3 = tensor_size_and_sum(x3) print(output3) # Expected: (torch.Size([]), 0) ``` **Notes:** - Ensure your function handles tensors of varying shapes, including edge cases like scalar tensors. - Utilize PyTorch\'s built-in functions and properties effectively to retrieve the necessary information.","solution":"import torch def tensor_size_and_sum(tensor: torch.Tensor) -> (torch.Size, int): Returns the size of the tensor and the sum of its dimensions. Parameters: tensor (torch.Tensor): The input tensor. Returns: (torch.Size, int): A tuple containing the size of the tensor and the sum of its dimensions. size = tensor.size() # get the size of the tensor dimension_sum = sum(size) if len(size) > 0 else 0 # sum of the dimensions, 0 if scalar tensor return size, dimension_sum"},{"question":"# Advanced Python: Customizing a Class with Special Methods Problem Statement In this question, you will practice creating and customizing a custom Python class to demonstrate your understanding of special method names. Your task is to implement a class named `DataContainer` which will emulate a container object that supports various operations such as initialization, string representation, and equality comparison. # Requirements 1. **Initialization**: Implement the `__init__` method to initialize the object with a list of integers. 2. **String Representation**: Implement the `__str__` method to return a string in the format `DataContainer([element1, element2, ...])`. 3. **Equality Comparison**: Implement the `__eq__` method to compare two `DataContainer` objects. Two `DataContainer` objects are considered equal if they contain the same elements in the same order. 4. **Length**: Implement the `__len__` method to return the number of elements in the container. # Input and Output - **Input**: - The `__init__` method takes a list of integers. - The `__eq__` method takes another `DataContainer` object. - **Output**: - The `__str__` method returns the string representation. - The `__eq__` method returns a boolean. - The `__len__` method returns an integer. # Example ```python # Instance initialization c1 = DataContainer([1, 2, 3]) c2 = DataContainer([1, 2, 3]) c3 = DataContainer([4, 5, 6]) # String representation print(str(c1)) # Output: DataContainer([1, 2, 3]) # Equality Comparison print(c1 == c2) # Output: True print(c1 == c3) # Output: False # Length of the container print(len(c1)) # Output: 3 ``` # Constraints - The input list for initialization will always contain integers. - The input list will have a maximum length of 1000 elements. - You cannot use any additional external libraries. # Implementation ```python class DataContainer: def __init__(self, data): self.data = data def __str__(self): return f\'DataContainer({self.data})\' def __eq__(self, other): return self.data == other.data def __len__(self): return len(self.data) # Test your implementation if __name__ == \\"__main__\\": c1 = DataContainer([1, 2, 3]) c2 = DataContainer([1, 2, 3]) c3 = DataContainer([4, 5, 6]) print(str(c1)) # Expected: DataContainer([1, 2, 3]) print(c1 == c2) # Expected: True print(c1 == c3) # Expected: False print(len(c1)) # Expected: 3 ```","solution":"class DataContainer: def __init__(self, data): Initializes the DataContainer with a list of integers. self.data = data def __str__(self): Returns the string representation of the DataContainer. return f\'DataContainer({self.data})\' def __eq__(self, other): Compares two DataContainer objects for equality. return self.data == other.data def __len__(self): Returns the number of elements in the DataContainer. return len(self.data)"},{"question":"**Advanced Seaborn Visualization Task:** You are provided with a dataset `mpg` which contains details about different car models, including attributes like `horsepower`, `mpg` (miles per gallon), `origin`, and `weight`. Your task is to create a sophisticated scatter plot visualization using the seaborn `objects` interface that demonstrates your understanding of seaborn\'s advanced features. # Data Description: - `mpg`: List of cars with various attributes. - Each car record includes the following: - `horsepower`: Numeric, the horsepower of the car. - `mpg`: Numeric, miles per gallon. - `origin`: Categorical, the origin of the car (e.g., \'usa\', \'europe\', \'japan\'). - `weight`: Numeric, weight of the car. # Task: 1. **Plot Setup**: Using the seaborn `objects` interface, create a `so.Plot` object with `horsepower` on the x-axis and `mpg` on the y-axis. 2. **Basic Plotting**: Add a basic dot plot using `so.Dots()`. 3. **Color Mapping**: Map the `color` property to `origin`. 4. **Custom Fill and Stroke**: Independently parametrize the `fillcolor` by `weight` and set a partial opacity using `fillalpha`. 5. **Marker Customization**: Customize markers to differentiate `origin` with different shapes. 6. **Jitter Application**: Apply jitter to the plot to show local density. # Implementation: - **Function Name**: `create_advanced_plot` - **Input**: - None. The function should internally use the `mpg` dataset from seaborn\'s provided datasets. - **Output**: - Return the plot object. # Constraints: - Use seaborn version >= 0.11. - Ensure the markers are sufficiently distinct and meaningful. - Validate that the plot object can be correctly shown when using `plot.show()`. # Example of Usage: ```python plot = create_advanced_plot() plot.show() ``` Implement the function `create_advanced_plot` according to the specifications given.","solution":"import seaborn as sns import seaborn.objects as so def create_advanced_plot(): # Load the dataset mpg = sns.load_dataset(\'mpg\').dropna(subset=[\'horsepower\', \'mpg\', \'weight\', \'origin\']) # Create the plot with customizations plot = ( so.Plot(mpg, x=\'horsepower\', y=\'mpg\') .add(so.Dots(), jitter=True, marker=(\'shape\', \'origin\')) .scale(color=\'origin\', fillcolor=\'weight\', fillalpha=0.6) ) return plot"},{"question":"# **Python Coding Assessment: Dynamic Type Creation and Utility Functions** Objective: You need to demonstrate your comprehension of Python’s `types` module by creating a dynamic class and working with some built-in type objects. Problem Statement: 1. **Dynamic Class Creation:** Create a dynamic class `MyDynamicClass` using the `types.new_class` function. The class should have: - A class attribute named `class_attr` with the value `\\"I am a class attribute\\"`. - An instance attribute named `instance_attr` initialized to `\\"I am an instance attribute\\"` which gets set during instance initialization. 2. **Utility Class Usage:** Utilize `types.SimpleNamespace` to create an object that stores arbitrary attributes provided in the input. 3. **Type Checking:** Write a function `type_checker` which: - Accepts an object. - Uses appropriate types from the `types` module to determine if the object is a built-in function, a generator, or a coroutine. - Returns a string specifying the type of the object: `\\"builtin_function\\"`, `\\"generator\\"`, `\\"coroutine\\"`, or `\\"unknown\\"`. Input and Output Format: 1. **Dynamic Class Creation:** No input required. Just define and test the class methods and attributes. 2. **Utility Class Usage:** - Input: A dictionary of attributes. - Output: An instance of `types.SimpleNamespace` with those attributes. 3. **Type Checking Function:** - Input: An object. - Output: A string indicating the type of the object. Constraints: 1. Do not use any external libraries. 2. Your implementations should be efficient even for a large number of dynamic attribute assignments. Example Usages: 1. **Dynamic Class Creation:** ```python MyDynamicClass = create_dynamic_class() # Function to create the dynamic class instance = MyDynamicClass() assert instance.instance_attr == \\"I am an instance attribute\\" assert MyDynamicClass.class_attr == \\"I am a class attribute\\" ``` 2. **Utility Class Usage:** ```python namespace_obj = create_namespace_obj({\'a\': 1, \'b\': 2}) assert namespace_obj.a == 1 assert namespace_obj.b == 2 ``` 3. **Type Checking Function:** ```python result = type_checker(len) # Should return \\"builtin_function\\" result = type_checker((x for x in range(10))) # Should return \\"generator\\" async def dummy_coroutine(): pass result = type_checker(dummy_coroutine()) # Should return \\"coroutine\\" ``` Implementation: ```python import types def create_dynamic_class(): def class_exec(ns): ns[\'class_attr\'] = \\"I am a class attribute\\" def __init__(self): self.instance_attr = \\"I am an instance attribute\\" ns[\'__init__\'] = __init__ MyDynamicClass = types.new_class(\'MyDynamicClass\', (), {}, class_exec) return MyDynamicClass def create_namespace_obj(attributes): return types.SimpleNamespace(**attributes) def type_checker(obj): if isinstance(obj, types.BuiltinFunctionType): return \\"builtin_function\\" elif isinstance(obj, types.GeneratorType): return \\"generator\\" elif isinstance(obj, types.CoroutineType): return \\"coroutine\\" else: return \\"unknown\\" ``` Demonstrate your understanding of Python\'s `types` module by implementing and testing the above functions as directed.","solution":"import types # Function to create the dynamic class def create_dynamic_class(): def class_exec(ns): ns[\'class_attr\'] = \\"I am a class attribute\\" def __init__(self): self.instance_attr = \\"I am an instance attribute\\" ns[\'__init__\'] = __init__ MyDynamicClass = types.new_class(\'MyDynamicClass\', (), {}, class_exec) return MyDynamicClass # Function to create a SimpleNamespace object with the given attributes def create_namespace_obj(attributes): return types.SimpleNamespace(**attributes) # Function to check the type of an object def type_checker(obj): if isinstance(obj, types.BuiltinFunctionType): return \\"builtin_function\\" elif isinstance(obj, types.GeneratorType): return \\"generator\\" elif isinstance(obj, types.CoroutineType): return \\"coroutine\\" else: return \\"unknown\\""},{"question":"Implement a Simple Text Editor **Objective:** Implement a simple text editor using the `curses` and `curses.textpad` modules. This editor should allow basic text editing within the terminal. **Task Description:** Create a simple text editor with the following features: 1. Initialize the `curses` environment. 2. Create a window that takes up the entire screen. 3. Display a text box where the user can input and edit text. 4. Implement the following key-based commands: - `Ctrl+S` to save the text to a file (`output.txt`). - `Ctrl+Q` to quit the editor. - Basic navigation within the text box (left, right, up, down), and editing commands should be supported by the `curses.textpad.Textbox` class. **Function Signature:** ```python def simple_text_editor(): pass ``` # Constraints & Requirements: 1. Use the `curses` module to handle the terminal interface. 2. Use the `curses.textpad.Textbox` class for text input and editing. 3. Ensure the text box is editable and supports the basic emacs-like keybindings described in the documentation. 4. Upon pressing `Ctrl+S`, save the current content of the text box to a file named `output.txt`. Each save should overwrite the content of the previous save. 5. Upon pressing `Ctrl+Q`, the editor should gracefully exit, ensuring that the terminal state is restored. # Input and Output: - **Input:** User input through the keyboard within the terminal interface. - **Output:** The content edited in the text box should be saved to `output.txt` upon pressing `Ctrl+S`. The program should quit gracefully upon pressing `Ctrl+Q`. # Example Execution: 1. The user starts the program, and a text box appears on the terminal. 2. The user types some text into the text box. 3. The user saves the content by pressing `Ctrl+S`. 4. The user verifies the content is saved in `output.txt`. 5. The user quits the editor by pressing `Ctrl+Q`. **Note:** You can consult the documentation provided to understand the necessary functions and keybindings to use.","solution":"import curses import curses.textpad def simple_text_editor(): def main(stdscr): # Clear screen stdscr.clear() # Create a window that takes up the entire screen sh, sw = stdscr.getmaxyx() editwin = curses.newwin(sh, sw, 0, 0) # Create a text box within the window textpad = curses.textpad.Textbox(editwin) # Instructions at bottom bar stdscr.addstr(sh-1, 0, \\"Ctrl+S to save | Ctrl+Q to quit\\") while True: stdscr.refresh() editwin.refresh() ch = stdscr.getch() if ch == 17: # Ctrl+Q to quit break elif ch == 19: # Ctrl+S to save text = textpad.gather() with open(\\"output.txt\\", \\"w\\") as f: f.write(text) else: textpad.do_command(ch) curses.wrapper(main)"},{"question":"You are tasked with implementing a Python function `analyze_memory_leaks` that uses the `gc` (Garbage Collector) module to monitor and report potential memory leaks in a Python program. This function should: 1. Enable the garbage collector if it is disabled. 2. Set appropriate debugging flags to enable detailed information logging. 3. Force a collection process and capture information about uncollectable objects. 4. Record statistics before and after the collection process for comparison. 5. Return a summary report as a dictionary including: - Whether the garbage collector was initially enabled or disabled. - The number of objects collected and uncollectable objects. - Differences in garbage collection statistics (if any). - A list of uncollectable objects if any were found. # Function Signature ```python def analyze_memory_leaks(): Analyzes the memory leaks using the garbage collector module. Returns: dict: A summary of the garbage collection process including: - \'gc.enabled\': initial state of the garbage collector (True/False) - \'objects_collected\': number of objects collected during manual collection - \'uncollectable_objects\': number of uncollectable objects found - \'initial_stats\': garbage collection statistics before manual collection - \'final_stats\': garbage collection statistics after manual collection - \'uncollectable\': list of uncollectable objects, if any pass ``` # Constraints - Use the `gc` module methods to set debugging flags, collect garbage, and retrieve statistics. - Handle the scenario where the garbage collector is already enabled. - The function does not accept any input parameters. # Example ```python result = analyze_memory_leaks() print(result) # Expected output format: # { # \'gc.enabled\': True, # \'objects_collected\': 10, # \'uncollectable_objects\': 0, # \'initial_stats\': [...], # \'final_stats\': [...], # \'uncollectable\': [] # } ``` # Notes - The function should be robust and handle various states of the garbage collector. - The `initial_stats` and `final_stats` should be lists of dictionaries as obtained from `gc.get_stats()`.","solution":"import gc def analyze_memory_leaks(): Analyzes the memory leaks using the garbage collector module. Returns: dict: A summary of the garbage collection process including: - \'gc.enabled\': initial state of the garbage collector (True/False) - \'objects_collected\': number of objects collected during manual collection - \'uncollectable_objects\': number of uncollectable objects found - \'initial_stats\': garbage collection statistics before manual collection - \'final_stats\': garbage collection statistics after manual collection - \'uncollectable\': list of uncollectable objects, if any # Check if the garbage collector was initially enabled gc_enabled_initially = gc.isenabled() # Enable the garbage collector if it was disabled if not gc_enabled_initially: gc.enable() # Set debugging flags to enable detailed information gc.set_debug(gc.DEBUG_LEAK) # Record statistics before the collection process initial_stats = gc.get_stats() # Force a collection process collected_objects = gc.collect() # Capture information about uncollectable objects uncollectable_objects = gc.garbage # Record statistics after the collection process final_stats = gc.get_stats() # Create the summary report summary_report = { \'gc.enabled\': gc_enabled_initially, \'objects_collected\': collected_objects, \'uncollectable_objects\': len(uncollectable_objects), \'initial_stats\': initial_stats, \'final_stats\': final_stats, \'uncollectable\': uncollectable_objects } return summary_report"},{"question":"**Coding Assessment Question** You are provided with a dataset containing scores of different models across various tasks. Your goal is to generate a highly customized heatmap using the seaborn library to visualize this data. # Task: 1. Load the dataset from seaborn\'s built-in datasets using `sns.load_dataset(\\"glue\\")`. 2. Pivot the dataset to have models as rows, tasks as columns, and scores as values. 3. Create a heatmap with the following requirements: - Add annotations to the cells, displaying values with one decimal place. - Use the \\"coolwarm\\" colormap. - Add lines between cells with a linewidth of 0.5. - Set a colormap range with a minimum value of 40 and a maximum value of 100. - Adjust the plot so that the labels for the x-axis are at the top. # Input: None (the dataset will be loaded directly within the function). # Output: A matplotlib `Axes` object representing the generated heatmap. # Constraints: - The code should be efficient and make use of seaborn\'s capabilities to handle large data gracefully. - Ensure all customizations are applied as specified. # Implementation: Write a function `generate_custom_heatmap()` that performs the task described above. ```python import seaborn as sns import matplotlib.pyplot as plt def generate_custom_heatmap(): # Load the dataset glue = sns.load_dataset(\\"glue\\") # Pivot the dataset glue_pivot = glue.pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Generate the heatmap ax = sns.heatmap(glue_pivot, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidths=.5, vmin=40, vmax=100) # Adjust the plot ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Return the Axes object return ax # Plot the heatmap plt.figure(figsize=(10, 8)) ax = generate_custom_heatmap() plt.show() ``` Ensure your function follows the specifications and passes the following assertions: ```python # Testing the function ax = generate_custom_heatmap() assert ax.collections[0].get_clim() == (40, 100), \\"Colormap range not set correctly\\" assert ax.collections[0].colorbar.cmap.name == \\"coolwarm\\", \\"Colormap not set to \'coolwarm\'\\" assert ax.collections[0].get_linewidths()[0] == 0.5, \\"Linewidth between cells not set to 0.5\\" print(\\"All checks passed!\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_custom_heatmap(): # Load the dataset glue = sns.load_dataset(\\"glue\\") # Pivot the dataset glue_pivot = glue.pivot(index=\\"Model\\", columns=\\"Task\\", values=\\"Score\\") # Generate the heatmap ax = sns.heatmap(glue_pivot, annot=True, fmt=\\".1f\\", cmap=\\"coolwarm\\", linewidths=.5, vmin=40, vmax=100) # Adjust the plot ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Return the Axes object return ax"},{"question":"Create an Interactive Text-Based Application You are required to create a text-based interactive application using the `curses` module in Python. Your program should initialize the curses environment, create multiple windows, handle user input, and display text with various attributes. Specifically, the program should perform the following steps: 1. **Initialization**: - Initialize the curses environment. - Turn off automatic echoing of keys to the screen. - Enable cbreak mode. - Enable the keypad mode to capture special key inputs. 2. **Window Setup**: - Create the main window that covers the entire screen. - Create a sub-window positioned at (5, 5) with dimensions (10, 40). 3. **Displaying Text**: - Display a welcome message in reverse-video at the top center of the main window. - Display a prompt message \\"Enter text (q to quit):\\" at the bottom of the sub-window. - Capture the user\'s input and display it within the sub-window. - Use bold text for displaying the user\'s text input. 4. **User Interaction**: - Capture the user\'s input character-by-character. - Allow the user to quit the program by pressing \'q\'. - Redisplay the updated user input each time a new character is entered. 5. **Termination**: - When \'q\' is pressed, terminate the curses environment and restore the terminal settings. # Function Signature ```python def text_based_application(): pass if __name__ == \\"__main__\\": text_based_application() ``` # Example Execution 1. Initialize curses and setup windows. 2. Display \\"Welcome to the Interactive Text-Based Application\\" in reverse-video at the top center of the main window. 3. In the sub-window, display \\"Enter text (q to quit):\\" and wait for user input. 4. Echo each character entered by the user in bold within the sub-window. 5. If the user enters \'q\', terminate the program gracefully. # Constraints - The program must handle terminal settings correctly and ensure that the terminal does not remain in an unexpected state if the program crashes. - Ensure the window dimensions and positions are within the terminal\'s size limits. - The input handling should be robust and handle typical key inputs gracefully. # Notes - Use the `curses.wrapper()` function to handle initialization and cleanup. - Ensure you handle exceptions that might occur during user input and always perform the necessary cleanup to restore terminal settings.","solution":"import curses def text_based_application(stdstdscr): # Initialization curses.noecho() curses.cbreak() stdstdscr.keypad(True) # Get the screen size height, width = stdstdscr.getmaxyx() # Create a sub-window sub_win = stdstdscr.subwin(10, 40, 5, 5) # Text to display welcome_msg = \\"Welcome to the Interactive Text-Based Application\\" prompt_msg = \\"Enter text (q to quit): \\" # Display welcome message in reverse-video at the top center of the main window stdstdscr.addstr(0, (width - len(welcome_msg)) // 2, welcome_msg, curses.A_REVERSE) # Display prompt message in sub_window sub_win.addstr(0, 0, prompt_msg) # Refresh the windows to show initial display stdstdscr.refresh() sub_win.refresh() user_input = \'\' while True: ch = stdstdscr.getch() if ch == ord(\'q\'): break elif ch == curses.KEY_BACKSPACE or ch == 127: user_input = user_input[:-1] else: user_input += chr(ch) # Clear the input area in sub_window sub_win.clear() sub_win.addstr(0, 0, prompt_msg) sub_win.addstr(1, 0, user_input, curses.A_BOLD) sub_win.refresh() # Clean up and termination curses.echo() curses.nocbreak() stdstdscr.keypad(False) curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(text_based_application)"},{"question":"**Coding Assessment Question** **Objective:** Demonstrate the ability to convert deprecated functionality to its modern counterpart using the `importlib` module. **Question:** The `imp` module in Python was used for handling module imports in earlier versions of Python. Since Python 3.3, this module has been deprecated in favor of the `importlib` module. Your task is to implement equivalent functionalities using the `importlib` module. **Task:** Implement the following `importlib` functions: 1. `find_spec_and_load(name, path=None)`: This function should mimic the deprecated `imp.find_module` and `imp.load_module` functionalities. It should take a module name and an optional search path. The function should find and load the module, and return the module object. Use `importlib.util.find_spec` to find the module and `importlib.util.module_from_spec` to load the module. 2. `get_magic_number()`: This function should return the magic number used to recognize byte-compiled code files. Use `importlib.util.MAGIC_NUMBER`. 3. `get_module_suffixes()`: This function should return a list of suffixes describing the types of modules, similar to what `imp.get_suffixes()` returned. Use `importlib.machinery`. **Constraints:** - Do not use any deprecated functionalities from the `imp` module. - You should handle and raise appropriate exceptions where necessary, specifically `ModuleNotFoundError` if a module cannot be found. **Input and Output Formats:** 1. `find_spec_and_load(name, path=None)` - **Input:** - `name` (string): The name of the module. - `path` (list of strings, optional): A list of directory names to search for the module. - **Output:** The loaded module object. *Example:* ```python module = find_spec_and_load(\\"math\\") print(module) # Output: <module \'math\' (built-in)> ``` 2. `get_magic_number()` - **Input:** No input. - **Output:** A byte string representing the magic number for byte-compiled files. *Example:* ```python print(get_magic_number()) # Output: b\'x03xf3rn\' ``` 3. `get_module_suffixes()` - **Input:** No input. - **Output:** A list of triples where each triple contains (suffix, mode, type). *Example:* ```python print(get_module_suffixes()) # Output: [(\'.cpython-310.pyc\', \'rb\', \'bytecode\'), (\'.py\', \'r\', \'source\')] ``` **Hints:** - You can use `importlib.util.find_spec` to replace `imp.find_module`. - Use `importlib.util.module_from_spec` to replace `imp.load_module`. - For suffixes, refer to `importlib.machinery.SOURCE_SUFFIXES`, `importlib.machinery.BYTECODE_SUFFIXES`, and other relevant constants from `importlib.machinery`. Implement the functions below: ```python import importlib.util import importlib.machinery def find_spec_and_load(name, path=None): # Implement this function based on the description. pass def get_magic_number(): # Implement this function based on the description. pass def get_module_suffixes(): # Implement this function based on the description. pass ```","solution":"import importlib.util import importlib.machinery def find_spec_and_load(name, path=None): Mimics imp.find_module and imp.load_module functionalities by using importlib. Args: - name (str): The name of the module. - path (list of str, optional): A list of directory names to search for the module. Returns: - The loaded module object. Raises: - ModuleNotFoundError: If the module cannot be found. spec = importlib.util.find_spec(name, path) if spec is None: raise ModuleNotFoundError(f\\"Module \'{name}\' not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module def get_magic_number(): Returns the magic number used to recognize byte-compiled code files. Returns: - A bytes string representing the magic number for byte-compiled files. return importlib.util.MAGIC_NUMBER def get_module_suffixes(): Returns a list of suffixes describing the types of modules useful for importation. Returns: - A list of triples where each triple contains (suffix, mode, type). suffixes = [] for suffix in importlib.machinery.SOURCE_SUFFIXES: suffixes.append((suffix, \'r\', \'source\')) for suffix in importlib.machinery.BYTECODE_SUFFIXES: suffixes.append((suffix, \'rb\', \'bytecode\')) for suffix in importlib.machinery.EXTENSION_SUFFIXES: suffixes.append((suffix, \'rb\', \'extension\')) return suffixes"},{"question":"# Question: Implement a Custom Mutable Sequence In this assignment, you will demonstrate your understanding of abstract base classes in the `collections.abc` module by creating a custom mutable sequence class called `CustomList`. This class should inherit from `collections.abc.MutableSequence` and correctly implement all required abstract methods. Instructions: 1. **Define the `CustomList` class**: - Inherit from `collections.abc.MutableSequence`. - Implement all required abstract methods: `__getitem__`, `__setitem__`, `__delitem__`, `__len__`, `insert`. - Implement any necessary mixin methods to ensure full functionality. 2. **Ensure your `CustomList` passes the following tests**: - Create an instance of `CustomList` using an initial iterable: `CustomList([1, 2, 3, 4])`. - Check the length using `len()`. - Access elements by index: `list[2]`. - Modify elements by index: `list[2] = 5`. - Delete elements by index: `del list[1]`. - Insert elements at a given position: `list.insert(2, 6)`. - Ensure the sequence supports iteration and all mixin methods provided by `MutableSequence`. Example: ```python from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self, initial=None): self.data = list(initial) if initial else [] def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def insert(self, index, value): self.data.insert(index, value) # Optionally implement any mixin methods if needed # Test cases clist = CustomList([1, 2, 3, 4]) print(len(clist)) # 4 print(clist[2]) # 3 clist[2] = 5 print(clist[2]) # 5 del clist[1] print(clist) # CustomList([1, 5, 4]) clist.insert(2, 6) print(clist) # CustomList([1, 5, 6, 4]) ``` Note: The above example provides a basic structure. You are required to ensure that your implementation not only works correctly according to the above test cases but also fully supports all functionalities provided by `MutableSequence`. Constraints: - Examine performance implications, especially for methods with linear or quadratic time complexity. - Ensure proper error handling for invalid index accesses or operations. - Optimize for both space and speed wherever possible.","solution":"from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self, initial=None): self.data = list(initial) if initial else [] def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def insert(self, index, value): self.data.insert(index, value) def __repr__(self): return f\\"CustomList({self.data})\\""},{"question":"# Cross-Validation and Model Evaluation Using Scikit-learn Problem Description You are given the famous Iris dataset. Your task is to implement functions that will: 1. Split the Iris dataset into training and testing sets. 2. Train a Support Vector Classifier (SVC) with a linear kernel on the training set. 3. Evaluate the model using K-Fold Cross-Validation. 4. Perform multiple metric evaluations using K-Fold Cross-Validation and return precision and recall scores. 5. Use cross-validated predictions to visualize performance. Requirements 1. **Function: `split_data`** - **Input**: - `X`: numpy array, feature matrix of Iris dataset. - `y`: numpy array, target vector of Iris dataset. - `test_size`: float, proportion of the dataset to include in the test split. - `random_state`: int, controls the shuffling applied to the data before the split. - **Output**: - Tuple `(X_train, X_test, y_train, y_test)` representing the split data. 2. **Function: `train_svc`** - **Input**: - `X_train`: numpy array, training feature matrix. - `y_train`: numpy array, training target vector. - `C`: float, regularization parameter. - `random_state`: int, controls the shuffling applied to the data. - **Output**: - Trained SVC model. 3. **Function: `evaluate_model`** - **Input**: - `clf`: Trained SVC model. - `X`: numpy array, feature matrix. - `y`: numpy array, target vector. - `cv`: int, number of folds in K-Fold Cross-Validation. - **Output**: - Mean accuracy score from K-Fold Cross-Validation. 4. **Function: `evaluate_multiple_metrics`** - **Input**: - `clf`: Trained SVC model. - `X`: numpy array, feature matrix. - `y`: numpy array, target vector. - `cv`: int, number of folds in K-Fold Cross-Validation. - **Output**: - Dictionary containing mean precision and recall scores. 5. **Function: `cross_val_predictions`** - **Input**: - `clf`: Trained SVC model. - `X`: numpy array, feature matrix. - `y`: numpy array, target vector. - `cv`: int, number of folds in K-Fold Cross-Validation. - **Output**: - Numpy array of cross-validated predictions. Evaluation and Visualization 1. Use `split_data` to split the Iris dataset. 2. Train an SVC model using `train_svc`. 3. Evaluate the model using `evaluate_model` with 5-fold cross-validation. 4. Obtain the precision and recall scores using `evaluate_multiple_metrics` with 5-fold cross-validation. 5. Use `cross_val_predictions` to get cross-validated predictions and visualize the confusion matrix. # Example Usage: ```python import numpy as np from sklearn import datasets from sklearn.svm import SVC from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt # Load Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Define necessary parameters test_size = 0.4 random_state = 0 C = 1.0 cv = 5 # Split data X_train, X_test, y_train, y_test = split_data(X, y, test_size, random_state) # Train model clf = train_svc(X_train, y_train, C, random_state) # Evaluate model mean_accuracy = evaluate_model(clf, X, y, cv) print(f\\"Mean accuracy: {mean_accuracy:.2f}\\") # Evaluate multiple metrics metrics_scores = evaluate_multiple_metrics(clf, X, y, cv) print(f\\"Precision: {metrics_scores[\'precision\']:.2f}, Recall: {metrics_scores[\'recall\']:.2f}\\") # Get cross-validated predictions predictions = cross_val_predictions(clf, X, y, cv) # Visualize confusion matrix cm = confusion_matrix(y, predictions) plt.imshow(cm, interpolation=\'nearest\', cmap=plt.cm.Blues) plt.colorbar() plt.xlabel(\'Predicted label\') plt.ylabel(\'True label\') plt.title(\'Confusion Matrix\') plt.show() ``` Implement the required functions to solve the given task.","solution":"import numpy as np from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict from sklearn.svm import SVC from sklearn.metrics import precision_score, recall_score, make_scorer def split_data(X, y, test_size, random_state): Splits the dataset into training and testing sets. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) return X_train, X_test, y_train, y_test def train_svc(X_train, y_train, C, random_state): Trains an SVC model with a linear kernel. clf = SVC(kernel=\'linear\', C=C, random_state=random_state) clf.fit(X_train, y_train) return clf def evaluate_model(clf, X, y, cv): Evaluates the model using K-Fold Cross-Validation and returns the mean accuracy score. scores = cross_val_score(clf, X, y, cv=cv, scoring=\'accuracy\') return np.mean(scores) def evaluate_multiple_metrics(clf, X, y, cv): Evaluates the model using K-Fold Cross-Validation and returns mean precision and recall scores. precision = cross_val_score(clf, X, y, cv=cv, scoring=make_scorer(precision_score, average=\'macro\')) recall = cross_val_score(clf, X, y, cv=cv, scoring=make_scorer(recall_score, average=\'macro\')) return {\'precision\': np.mean(precision), \'recall\': np.mean(recall)} def cross_val_predictions(clf, X, y, cv): Gets cross-validated predictions. predictions = cross_val_predict(clf, X, y, cv=cv) return predictions"},{"question":"# Question: Secure XML Processing with xml.etree.ElementTree As an experienced Python programmer, you are tasked with writing a function to process XML data securely. Given XML data as a string, your function should extract certain information from it while preventing common XML-related vulnerabilities. Function Signature ```python def extract_xml_info(xml_data: str, tag: str) -> list: pass ``` Input - `xml_data`: A string representing the XML data to be processed. - `tag`: A string representing the tag from which you want to extract the text content. Output - A list of strings, where each string is the text content of the specified tag. Constraints - The XML data can be of arbitrary length but is guaranteed to be well-formed. - You must use the `xml.etree.ElementTree` module for XML processing. - Your implementation should avoid known vulnerabilities such as the Billion Laughs attack, quadratic blowup, and others mentioned in the documentation. Example ```python xml_string = <data> <item>Sample1</item> <item>Sample2</item> <ignore>Ignore this</ignore> </data> tag = \\"item\\" # Output: [\'Sample1\', \'Sample2\'] print(extract_xml_info(xml_string, tag)) ``` Requirements 1. Use the `xml.etree.ElementTree` module for parsing the XML data. 2. Ensure that your function securely processes the XML input and does not fall victim to common XML-related vulnerabilities. Additional Notes - You may refer to the `defusedxml` package for additional security measures, though it\'s not a requirement to use it in this task. - Pay special attention to handling external entities and other risky constructions within the XML data. Write your implementation below: ```python import xml.etree.ElementTree as ET def extract_xml_info(xml_data: str, tag: str) -> list: Extracts text content from specific tags in XML data securely. Parameters: - xml_data: XML data as a string - tag: The tag name to extract text content from Returns: - A list of text content from the specified tag try: tree = ET.ElementTree(ET.fromstring(xml_data)) return [elem.text for elem in tree.iter(tag)] except ET.ParseError: # Handle parsing errors return [] # Example usage xml_string = <data> <item>Sample1</item> <item>Sample2</item> <ignore>Ignore this</ignore> </data> tag = \\"item\\" print(extract_xml_info(xml_string, tag)) ```","solution":"import xml.etree.ElementTree as ET def extract_xml_info(xml_data: str, tag: str) -> list: Extracts text content from specific tags in XML data securely. Parameters: - xml_data: XML data as a string - tag: The tag name to extract text content from Returns: - A list of text content from the specified tag try: tree = ET.ElementTree(ET.fromstring(xml_data)) return [elem.text for elem in tree.iter(tag)] except ET.ParseError: # Handle parsing errors return [] # Example usage xml_string = <data> <item>Sample1</item> <item>Sample2</item> <ignore>Ignore this</ignore> </data> tag = \\"item\\" print(extract_xml_info(xml_string, tag))"},{"question":"# Complex Number Transformer Problem Statement You are required to implement a class `ComplexTransformer` in Python that performs several operations on complex numbers using the `cmath` module. Specifically, this class should be able to: 1. Convert a complex number from rectangular to polar coordinates. 2. Convert a complex number from polar to rectangular coordinates. 3. Compute the square root of a complex number. 4. Compute the exponential of a complex number. 5. Compute the natural logarithm of a complex number. 6. Check if two complex numbers are approximately equal within given tolerances. Implementation Details 1. **Class Definition**: - Define a class named `ComplexTransformer`. 2. **Method Definitions**: - `to_polar(z: complex) -> tuple`: This method receives a complex number `z` and returns its modulus and phase as a tuple. - `to_rectangular(r: float, phi: float) -> complex`: This method receives the modulus `r` and phase `phi` of a complex number and returns the complex number in rectangular form. - `square_root(z: complex) -> complex`: This method receives a complex number `z` and returns its square root. - `exponential(z: complex) -> complex`: This method receives a complex number `z` and returns its exponential. - `natural_log(z: complex) -> complex`: This method receives a complex number `z` and returns its natural logarithm. - `are_close(a: complex, b: complex, rel_tol=1e-09, abs_tol=0.0) -> bool`: This method receives two complex numbers `a` and `b` and optional relative and absolute tolerances. It returns `True` if the two numbers are approximately equal within the given tolerances. Example Usage ```python from cmath import pi # Instantiate the class transformer = ComplexTransformer() # Example complex number z = 1 + 2j # Convert to polar coordinates polar_coords = transformer.to_polar(z) print(polar_coords) # Example Output: (2.23606797749979, 1.1071487177940904) # Convert back to rectangular coordinates rect_coord = transformer.to_rectangular(*polar_coords) print(rect_coord) # Example Output: (1.0000000000000002+2j) # Compute square root sqrt_z = transformer.square_root(z) print(sqrt_z) # Example Output: (1.272019649514069+0.7861513777574233j) # Compute exponential exp_z = transformer.exponential(z) print(exp_z) # Example Output: (-1.1312043837568135+2.4717266720048188j) # Compute natural logarithm log_z = transformer.natural_log(z) print(log_z) # Example Output: (0.8047189562170503+1.1071487177940904j) # Checking approximate equality a = 1 + 2j b = 1.000000001 + 2.000000001j is_close = transformer.are_close(a, b, rel_tol=1e-8) print(is_close) # Example Output: True ``` Constraints - The input complex numbers can have both real and imaginary parts ranging from `-1e12` to `1e12`. - The function inputs `r` (modulus) and `phi` (phase) in the `to_rectangular` method adhere to typical polar coordinate constraints, where `r >= 0` and `-π <= phi <= π`. Notes - Use the `cmath` module for all complex number operations. - Ensure that the methods properly handle special values like infinities and NaNs according to the behavior specified in the `cmath` documentation.","solution":"import cmath import math class ComplexTransformer: def to_polar(self, z: complex) -> tuple: Convert a complex number from rectangular to polar coordinates. :param z: A complex number. :return: A tuple containing the modulus and phase of the complex number. return cmath.polar(z) def to_rectangular(self, r: float, phi: float) -> complex: Convert a complex number from polar to rectangular coordinates. :param r: Modulus of the complex number. :param phi: Phase of the complex number. :return: The complex number in rectangular form. return cmath.rect(r, phi) def square_root(self, z: complex) -> complex: Compute the square root of a complex number. :param z: A complex number. :return: The square root of the complex number. return cmath.sqrt(z) def exponential(self, z: complex) -> complex: Compute the exponential of a complex number. :param z: A complex number. :return: The exponential of the complex number. return cmath.exp(z) def natural_log(self, z: complex) -> complex: Compute the natural logarithm of a complex number. :param z: A complex number. :return: The natural logarithm of the complex number. return cmath.log(z) def are_close(self, a: complex, b: complex, rel_tol=1e-09, abs_tol=0.0) -> bool: Check if two complex numbers are approximately equal within given tolerances. :param a: First complex number. :param b: Second complex number. :param rel_tol: Relative tolerance. :param abs_tol: Absolute tolerance. :return: True if the two numbers are approximately equal, False otherwise. return math.isclose(a.real, b.real, rel_tol=rel_tol, abs_tol=abs_tol) and math.isclose(a.imag, b.imag, rel_tol=rel_tol, abs_tol=abs_tol)"},{"question":"# Coding Assessment: Handling Nullable Integer Data in pandas You are given a `DataFrame` that contains the following columns: - `student_id`: Nullable integer type (can have missing values). - `name`: String type. - `math_score`: Integer type (ranging from 0 to 100, can have missing values). - `english_score`: Integer type (ranging from 0 to 100, can have missing values). Your task is to implement the following functionalities: 1. **Initialize the DataFrame**: Write a function `initialize_dataframe` that takes in lists of `student_id`, `name`, `math_score`, and `english_score` and returns a pandas `DataFrame` with appropriate data types (use `Int64` for nullable integer columns). 2. **Calculate Average Scores**: Write a function `calculate_average_scores` that takes the initialized `DataFrame` and returns a new `DataFrame` with two columns: `student_id` (same as input) and `average_score` which is the average of `math_score` and `english_score` for each student. If both scores are missing, `average_score` should be `NA`. 3. **Fill Missing Scores**: Write a function `fill_missing_scores` that takes the `DataFrame` and fills missing `math_score` and `english_score` with the mean of the respective columns rounded to the nearest integer using statistical rounding (halfway values rounded to the nearest even number). # Input Format - For the `initialize_dataframe` function: - `student_id`: List of integers with possible `None` values. - `name`: List of strings. - `math_score`: List of integers with possible `None` values. - `english_score`: List of integers with possible `None` values. - For the `calculate_average_scores` function: - `df`: pandas `DataFrame` with columns `student_id`, `name`, `math_score`, `english_score`. - For the `fill_missing_scores` function: - `df`: pandas `DataFrame` with columns `student_id`, `name`, `math_score`, `english_score`. # Output Format - `initialize_dataframe`: Returns a pandas `DataFrame`. - `calculate_average_scores`: Returns a pandas `DataFrame`. - `fill_missing_scores`: Returns a pandas `DataFrame`. # Example ```python import pandas as pd def initialize_dataframe(student_id, name, math_score, english_score): # Your code here def calculate_average_scores(df): # Your code here def fill_missing_scores(df): # Your code here # Example usage: student_id = [1, 2, None, 4] name = [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\"] math_score = [90, None, 70, 88] english_score = [85, 92, None, 80] df = initialize_dataframe(student_id, name, math_score, english_score) print(df) avg_scores_df = calculate_average_scores(df) print(avg_scores_df) filled_df = fill_missing_scores(df) print(filled_df) ``` - Initial `DataFrame` creation and setting the appropriate data types. - Calculating average scores while handling missing values correctly. - Filling missing scores with the column mean, ensuring correct rounding. Make sure to test your functions with various edge cases to validate their accuracy.","solution":"import pandas as pd import numpy as np def initialize_dataframe(student_id, name, math_score, english_score): Initializes a DataFrame with the given data, ensuring appropriate data types. Parameters: student_id (list of int or None): List of student IDs with possible None values. name (list of str): List of student names. math_score (list of int or None): List of math scores with possible None values. english_score (list of int or None): List of English scores with possible None values. Returns: pd.DataFrame: A DataFrame with the given data and appropriate data types. data = { \\"student_id\\": pd.Series(student_id, dtype=\\"Int64\\"), \\"name\\": pd.Series(name, dtype=\\"str\\"), \\"math_score\\": pd.Series(math_score, dtype=\\"Int64\\"), \\"english_score\\": pd.Series(english_score, dtype=\\"Int64\\") } return pd.DataFrame(data) def calculate_average_scores(df): Calculates the average of math and English scores, handling missing values. Parameters: df (pd.DataFrame): DataFrame containing student_id, name, math_score, and english_score. Returns: pd.DataFrame: A DataFrame with student_id and average_score. df_copy = df.copy() df_copy[\'average_score\'] = df_copy[[\'math_score\', \'english_score\']].mean(axis=1) df_copy[\'average_score\'] = df_copy[\'average_score\'].round(0).astype(\'Int64\') return df_copy[[\'student_id\', \'average_score\']] def fill_missing_scores(df): Fills missing math and English scores with the mean of the respective columns. Parameters: df (pd.DataFrame): DataFrame containing student_id, name, math_score, and english_score. Returns: pd.DataFrame: A DataFrame with missing scores filled with the column mean. df_copy = df.copy() math_mean = df_copy[\'math_score\'].mean() english_mean = df_copy[\'english_score\'].mean() df_copy[\'math_score\'].fillna(round(math_mean), inplace=True) df_copy[\'english_score\'].fillna(round(english_mean), inplace=True) return df_copy"},{"question":"**Objective**: Demonstrate your understanding of pandas and its extension capabilities by creating a custom string manipulation extension array. Problem Statement: You are required to implement a custom pandas extension array that provides additional string manipulation functionalities. Your custom string extension should inherit from `pandas.api.extensions.ExtensionArray` and must implement the following methods: 1. `reverse_strings`: This method should return a new ExtensionArray with all strings reversed. 2. `to_uppercase`: This method should return a new ExtensionArray with all strings converted to uppercase. 3. `_from_sequence`: This method should construct the custom ExtensionArray from a given sequence of strings. 4. `__getitem__`: This method should correctly handle the indexing and slicing operations. 5. `__len__`: This method should return the length of the array. 6. `arrow_to_layout`: This method should convert the ExtensionArray to a layout that can be used efficiently with Apache Arrow. Requirements: - Use the provided class template below to define your custom extension array. - Ensure your implementation adheres to the required methods and functionality. - Write a test script to validate your custom extension by creating an instance and performing various operations. Class Template: ```python import numpy as np import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype class StringArray(ExtensionArray): def __init__(self, values): self._data = np.array(values, dtype=object) @classmethod def _from_sequence(cls, sequence): return cls(sequence) def __getitem__(self, item): if isinstance(item, int): return self._data[item] elif isinstance(item, slice): return StringArray(self._data[item]) def __len__(self): return len(self._data) @property def dtype(self): return StringDtype() def reverse_strings(self): reversed_data = [s[::-1] for s in self._data] return StringArray(reversed_data) def to_uppercase(self): uppercase_data = [s.upper() for s in self._data] return StringArray(uppercase_data) def arrow_to_layout(self): import pyarrow as pa return pa.array(self._data) class StringDtype(ExtensionDtype): name = \'string\' type = str @property def na_value(self): return None @classmethod def construct_array_type(cls): return StringArray # Test Script if __name__ == \\"__main__\\": data = [\\"hello\\", \\"world\\", \\"pandas\\"] string_array = StringArray(data) print(\\"Original Array:\\", string_array._data) print(\\"Reversed Array:\\", string_array.reverse_strings()._data) print(\\"Uppercase Array:\\", string_array.to_uppercase()._data) sliced_array = string_array[1:] print(\\"Sliced Array:\\", sliced_array._data) print(\\"Converted to Arrow Layout:\\", string_array.arrow_to_layout()) ``` Expected Input and Output Formats: * **Input**: - A list of strings for creating the custom `StringArray`. - Operations (methods) to be called on the `StringArray`. * **Output**: - Arrays after performing the specified string manipulations. - Sliced arrays when indexing is applied. - Arrow array format (printed). Use the provided template to implement the function and ensure all functional requirements are met.","solution":"import numpy as np import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype class StringArray(ExtensionArray): def __init__(self, values): self._data = np.array(values, dtype=object) @classmethod def _from_sequence(cls, sequence, dtype=None, copy=False): return cls(sequence) def __getitem__(self, item): if isinstance(item, int): return self._data[item] elif isinstance(item, slice): return StringArray(self._data[item]) def __len__(self): return len(self._data) @property def dtype(self): return StringDtype() def reverse_strings(self): reversed_data = [s[::-1] for s in self._data] return StringArray(reversed_data) def to_uppercase(self): uppercase_data = [s.upper() for s in self._data] return StringArray(uppercase_data) def arrow_to_layout(self): import pyarrow as pa return pa.array(self._data) class StringDtype(ExtensionDtype): name = \'string\' type = str @property def na_value(self): return None @classmethod def construct_array_type(cls): return StringArray"},{"question":"Objective: Demonstrate your understanding of Seaborn\'s `seaborn.objects` module, specifically focusing on creating scatter plots with jitter transformations. Problem Statement: You are provided with the \\"penguins\\" dataset, which contains information about different species of penguins and their physical characteristics. Your task is to create three different scatter plots using the `seaborn.objects` module. 1. **Basic Scatter Plot with Jitter**: - Create a scatter plot showing the relationship between `species` (x-axis) and `body_mass_g` (y-axis) with a default jitter. - Customize the plot by applying a small jitter using the `Jitter()` transformation. 2. **Customized Jitter on Orientation Axis**: - Create another scatter plot showing the relationship between `species` (x-axis) and `body_mass_g` (y-axis). - Customize the plot by applying a jitter with `width` set to `0.3` on the orientation axis. 3. **Dual Axis Jitter with Numeric Data**: - Create a scatter plot showing the relationship between `flipper_length_mm` (x-axis) and `body_mass_g` (y-axis). - Apply a jitter with `x` set to `100` and `y` set to `50`. Constraints: - You must use the `seaborn.objects` module. - Your solution should handle plotting using the `penguins` dataset provided by Seaborn. Expected Input and Output: Your code should produce three scatter plots with the specified jitter transformations. Example Code Snippet: ```python import seaborn.objects as so from seaborn import load_dataset # Load the Penguins dataset penguins = load_dataset(\\"penguins\\") # 1. Basic Scatter Plot with Jitter so.Plot(penguins, \\"species\\", \\"body_mass_g\\").add(so.Dots(), so.Jitter()).show() # 2. Customized Jitter on Orientation Axis so.Plot(penguins, \\"species\\", \\"body_mass_g\\").add(so.Dots(), so.Jitter(0.3)).show() # 3. Dual Axis Jitter with Numeric Data so.Plot(penguins[\\"flipper_length_mm\\"], penguins[\\"body_mass_g\\"]).add(so.Dots(), so.Jitter(x=100, y=50)).show() ``` **Note:** Ensure each of the plots is clearly labeled and correctly displays the jitter transformations applied.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_scatter_plots(): # Load the Penguins dataset penguins = load_dataset(\\"penguins\\") # 1. Basic Scatter Plot with Jitter basic_scatter_plot = so.Plot(penguins, \\"species\\", \\"body_mass_g\\").add(so.Dots(), so.Jitter()) basic_scatter_plot.show() # 2. Customized Jitter on Orientation Axis customized_jitter_plot = so.Plot(penguins, \\"species\\", \\"body_mass_g\\").add(so.Dots(), so.Jitter(width=0.3)) customized_jitter_plot.show() # 3. Dual Axis Jitter with Numeric Data dual_axis_jitter_plot = so.Plot(penguins, \\"flipper_length_mm\\", \\"body_mass_g\\").add(so.Dots(), so.Jitter(x=100, y=50)) dual_axis_jitter_plot.show() create_scatter_plots()"},{"question":"Objective Design a Python script that compares the execution time of two different implementations of a function that checks if a given number is a prime number. Background A prime number is a natural number greater than 1 that is not a product of two smaller natural numbers. The two implementations you will compare are: 1. A simple trial division method. 2. An optimized method that only checks divisors up to the square root of the number and skips even numbers after checking for 2. Task 1. Implement the two functions: - `is_prime_simple(n)`: Use trial division to check if `n` is a prime number. - `is_prime_optimized(n)`: Use the optimized method to check if `n` is a prime number. 2. Use the `timeit` module to measure the execution time of both functions for a range of numbers. Specifically, you should measure the execution time for checking all numbers from 1 to 10,000. 3. Print out the total execution time for each method as well as their ratio (simple time / optimized time). Requirements - Your script should be self-contained and should not require any input from the user. - Use the `timeit` module\'s Python interface to time the functions. - Ensure that garbage collection is disabled during timing. Implementation Constraints - Do not use any external libraries other than the Python standard library. - Optimize your code for readability and efficiency. # Example Output ```plaintext Total execution time for is_prime_simple: X.XXXXXX seconds Total execution time for is_prime_optimized: Y.YYYYYY seconds Ratio of simple to optimized: Z.ZZZZZ ``` Notes - Ensure your code is clean and includes comments explaining the various parts of the implementation. - You may define helper functions if necessary. - Consider edge cases and handle them appropriately within your functions. This question aims to assess your ability to implement efficient algorithms, utilize the `timeit` module for performance measurement, and analyze the execution performance of different implementations.","solution":"import math import timeit def is_prime_simple(n): Checks if a number is prime using trial division. if n <= 1: return False for i in range(2, n): if n % i == 0: return False return True def is_prime_optimized(n): Checks if a number is prime using an optimized method. if n <= 1: return False if n == 2: return True if n % 2 == 0: return False for i in range(3, int(math.sqrt(n)) + 1, 2): if n % i == 0: return False return True def measure_execution_time(): Measures the execution time of both prime checking methods. # Measure time for is_prime_simple simple_time = timeit.timeit(\'for n in range(1, 10001): is_prime_simple(n)\', globals=globals(), number=1, setup=\'import gcn\' \'gc.disable()\') # Measure time for is_prime_optimized optimized_time = timeit.timeit(\'for n in range(1, 10001): is_prime_optimized(n)\', globals=globals(), number=1, setup=\'import gcn\' \'gc.disable()\') # Print the results print(f\\"Total execution time for is_prime_simple: {simple_time:.6f} seconds\\") print(f\\"Total execution time for is_prime_optimized: {optimized_time:.6f} seconds\\") print(f\\"Ratio of simple to optimized: {simple_time / optimized_time:.6f}\\") if __name__ == \\"__main__\\": measure_execution_time()"},{"question":"# ByteArray Manipulations In this task, you will demonstrate your understanding of bytearray manipulations in Python. Specifically, you will need to implement a class that simulates some functionality of the bytearray handling described in the provided documentation. Problem Statement Write a `ByteArrayHandler` class that provides the following methods: 1. **Initialization**: - `__init__(self, initial_data)`: Initializes the bytearray object with the given initial data. The `initial_data` can be a string or another bytearray. 2. **Concatenation**: - `concat(self, other)`: Concatenates the current bytearray with another bytearray or a string and returns a new `ByteArrayHandler` object with the result. 3. **Get Size**: - `size(self)`: Returns the size of the current bytearray. 4. **As String**: - `as_string(self)`: Returns the contents of the bytearray as a string. 5. **Resize**: - `resize(self, new_size)`: Resizes the current bytearray to the new specified size. If the new size is larger, the bytearray will be padded with null bytes. If the new size is smaller, the bytearray will be truncated. Constraints - The input for the `__init__` method can be either a string or a bytearray. - The `concat` method should accept either a string or another `ByteArrayHandler` object. - The `resize` method should be able to handle both increasing and decreasing the bytearray size. Example ```python # Initialize with a string handler = ByteArrayHandler(\\"Hello, World!\\") print(handler.as_string()) # Output: Hello, World! print(handler.size()) # Output: 13 # Concatenate with another bytearray handler2 = ByteArrayHandler(bytearray([65, 66, 67])) handler3 = handler.concat(handler2) print(handler3.as_string()) # Output: Hello, World!ABC # Resize the bytearray handler3.resize(10) print(handler3.size()) # Output: 10 print(handler3.as_string()) # Output: Hello, W handler3.resize(15) print(handler3.size()) # Output: 15 print(handler3.as_string()) # Output: Hello, W ``` Implementation Complete the `ByteArrayHandler` class with the required methods. ```python class ByteArrayHandler: def __init__(self, initial_data): # Your code here def concat(self, other): # Your code here def size(self): # Your code here def as_string(self): # Your code here def resize(self, new_size): # Your code here # You can add other helper methods if needed. ``` Ensure your implementation passes the given example and adheres to the constraints.","solution":"class ByteArrayHandler: def __init__(self, initial_data): if isinstance(initial_data, str): self.data = bytearray(initial_data, \'utf-8\') elif isinstance(initial_data, bytearray): self.data = initial_data else: raise TypeError(\\"Initial data must be a string or bytearray.\\") def concat(self, other): if isinstance(other, str): other_data = bytearray(other, \'utf-8\') elif isinstance(other, ByteArrayHandler): other_data = other.data else: raise TypeError(\\"The other parameter must be a string or a ByteArrayHandler.\\") return ByteArrayHandler(self.data + other_data) def size(self): return len(self.data) def as_string(self): return self.data.decode(\'utf-8\') def resize(self, new_size): current_size = self.size() if new_size > current_size: self.data.extend([0] * (new_size - current_size)) elif new_size < current_size: self.data = self.data[:new_size]"},{"question":"Objective: Demonstrate your ability to use the inspect module for introspection of Python objects, including functions, classes, and call signatures. Problem Description: You are given a Python module that contains several classes and functions. Your task is to create a script that inspects this module and provides the following information: 1. A list of all functions defined in the module, including their names, documentation, and the signature of their parameters. 2. A list of all classes defined in the module, along with their methods including each method\'s name, documentation, and parameter signature. 3. For each class, also retrieve and display any class-level documentation. You are required to implement the following function: ```python import inspect def inspect_module(module): Takes a module object and prints informative details about its functions and classes. :param module: Module object to be inspected # Your code here ``` # Input: - A module object (e.g., `import mymodule`). # Output: The function should print the following details: 1. **Functions:** - Function Name - Documentation (if available, cleaned of extra indentation) - Parameter Signature 2. **Classes:** - Class Name - Class Documentation (if available, cleaned of extra indentation) - **Methods:** - Method Name - Documentation (if available, cleaned of extra indentation) - Parameter Signature # Constraints: - You may assume all functions and methods have unique names. - The module does not have nested classes or functions. # Example Execution: Assume `mymodule.py` content is as follows: ```python This module contains sample classes and functions. def foo(x, y): This is function foo. return x + y class Bar: This is class Bar. def method1(self, a): This is method1. return a def method2(self, b, c=4): This is method2. return b * c ``` If `mymodule` is passed to your `inspect_module` function, it should print: ``` Functions: - Name: foo Documentation: This is function foo. Signature: (x, y) Classes: - Name: Bar Documentation: This is class Bar. Methods: - Name: method1 Documentation: This is method1. Signature: (self, a) - Name: method2 Documentation: This is method2. Signature: (self, b, c=4) ``` Task: Implement the `inspect_module` function to achieve the described behavior.","solution":"import inspect def inspect_module(module): Takes a module object and prints informative details about its functions and classes. :param module: Module object to be inspected # Inspecting functions print(\\"Functions:\\") for name, obj in inspect.getmembers(module, inspect.isfunction): print(f\\"- Name: {name}\\") doc = inspect.getdoc(obj) or \\"No documentation\\" signature = str(inspect.signature(obj)) print(f\\" Documentation: {doc}\\") print(f\\" Signature: {signature}\\") print() # Inspecting classes print(\\"Classes:\\") for name, obj in inspect.getmembers(module, inspect.isclass): print(f\\"- Name: {name}\\") class_doc = inspect.getdoc(obj) or \\"No documentation\\" print(f\\" Documentation: {class_doc}\\") print(\\" Methods:\\") for meth_name, meth_obj in inspect.getmembers(obj, inspect.isfunction): print(f\\" - Name: {meth_name}\\") meth_doc = inspect.getdoc(meth_obj) or \\"No documentation\\" meth_signature = str(inspect.signature(meth_obj)) print(f\\" Documentation: {meth_doc}\\") print(f\\" Signature: {meth_signature}\\") print() # For better readability"},{"question":"Implementing a Custom Profiler in PyTorch Objective You are required to implement a custom profiler in PyTorch. The profiler will measure the time taken by individual operator invocations within a PyTorch model and log the duration, function name, and the number of inputs for each operator. Task Write a Python function `custom_profiler` that: 1. Takes a PyTorch model and an input tensor as inputs. 2. Measures the time taken by each operator invocation during the model\'s forward pass. 3. Logs the function name, execution time, and the number of inputs for each operator. Requirements 1. **Input Format**: - The first input is a PyTorch model (`torch.nn.Module`). - The second input is a tensor (e.g., `torch.Tensor`) representing the input to the model. 2. **Output Format**: - The function should not return anything. Instead, it should print logs for each operator as specified. 3. **Constraints**: - Use the `torch.autograd.profiler` module for profiling. - Ensure the profiling overhead is minimized by leveraging appropriate settings and sampling mechanisms. 4. **Performance Requirements**: - The profiling mechanism should introduce minimal overhead to the model\'s forward pass. Example ```python import torch import torch.nn as nn import torch.autograd.profiler as profiler def custom_profiler(model, input_tensor): # Your implementation here # Example usage: class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 5) def forward(self, x): return self.fc(x) model = SimpleModel() input_tensor = torch.randn(1, 10) custom_profiler(model, input_tensor) ``` Expected Output (Example): ``` [INFO] Function: aten::linear, Duration: 0.1 ms, Inputs: 2 [INFO] Function: aten::addmm, Duration: 0.3 ms, Inputs: 3 ... ``` You may use the `torch.autograd.profiler.record_function` context to measure and log the desired information. Notes - Ensure you encapsulate the profiling logic within `custom_profiler`. - The format of the logged output can be customized as needed, but it should include function name, duration, and the number of inputs.","solution":"import torch import torch.nn as nn import torch.autograd.profiler as profiler def custom_profiler(model, input_tensor): with profiler.profile(record_shapes=True) as prof: with profiler.record_function(\\"model_inference\\"): model(input_tensor) for event in prof.key_averages(): print(f\\"[INFO] Function: {event.key}, Duration: {event.self_cpu_time_total:.3f} ms, Inputs: {len(event.input_shapes)}\\") # Example usage: class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 5) def forward(self, x): return self.fc(x) model = SimpleModel() input_tensor = torch.randn(1, 10) custom_profiler(model, input_tensor)"},{"question":"Problem Statement You are tasked with implementing functionality for an application that handles large amounts of data by compressing, storing checksums, and decompressing it back. Your task is to write a Python class `DataHandler` that leverages the `zlib` module to achieve this functionality. Class: `DataHandler` **Your implementation should include the following methods:** 1. **`compress_data(data: bytes, level: int = -1) -> bytes`** - Compress the given `data` using the specified `level` of compression. - Parameters: - `data` (bytes): The raw data to be compressed. - `level` (int, optional): The compression level from 0 (no compression) to 9 (maximum compression). Defaults to `-1` for default compression level. - Returns: - Compressed data as a bytes object. 2. **`decompress_data(data: bytes) -> bytes`** - Decompress the given compressed `data`. - Parameters: - `data` (bytes): The compressed data to be decompressed. - Returns: - Decompressed data as a bytes object. 3. **`calculate_adler32(data: bytes) -> int`** - Calculate and return the Adler-32 checksum of the given `data`. - Parameters: - `data` (bytes): The data to calculate the checksum for. - Returns: - The Adler-32 checksum as an unsigned 32-bit integer. 4. **`calculate_crc32(data: bytes) -> int`** - Calculate and return the CRC-32 checksum of the given `data`. - Parameters: - `data` (bytes): The data to calculate the checksum for. - Returns: - The CRC-32 checksum as an unsigned 32-bit integer. Example Usage: ```python data_handler = DataHandler() # Data to be handled raw_data = b\\"This is a sample data to be compressed.\\" # Compressing data compressed_data = data_handler.compress_data(raw_data, level=5) print(\\"Compressed Data:\\", compressed_data) # Decompressing data decompressed_data = data_handler.decompress_data(compressed_data) print(\\"Decompressed Data:\\", decompressed_data) # Calculating checksums adler32_checksum = data_handler.calculate_adler32(raw_data) print(\\"Adler-32 Checksum:\\", adler32_checksum) crc32_checksum = data_handler.calculate_crc32(raw_data) print(\\"CRC-32 Checksum:\\", crc32_checksum) ``` Constraints: - You may assume that all input data is in bytes and valid. - Ensure efficient handling of data streams that may not fit into memory at once. Performance Requirements: - Your implementation should be optimized for both time and space complexity where applicable. - The `DataHandler` class should handle input data sizes efficiently, ensuring no unnecessary memory overhead.","solution":"import zlib class DataHandler: def compress_data(self, data: bytes, level: int = -1) -> bytes: Compress the given data using the specified level of compression. Parameters: data (bytes): The raw data to be compressed. level (int, optional): The compression level from 0 (no compression) to 9 (maximum compression). Defaults to -1 for default compression level. Returns: bytes: Compressed data. return zlib.compress(data, level) def decompress_data(self, data: bytes) -> bytes: Decompress the given compressed data. Parameters: data (bytes): The compressed data to be decompressed. Returns: bytes: Decompressed data. return zlib.decompress(data) def calculate_adler32(self, data: bytes) -> int: Calculate the Adler-32 checksum of the given data. Parameters: data (bytes): The data to calculate the checksum for. Returns: int: The Adler-32 checksum as an unsigned 32-bit integer. return zlib.adler32(data) def calculate_crc32(self, data: bytes) -> int: Calculate the CRC-32 checksum of the given data. Parameters: data (bytes): The data to calculate the checksum for. Returns: int: The CRC-32 checksum as an unsigned 32-bit integer. return zlib.crc32(data)"},{"question":"Objective: Demonstrate your understanding of creating and managing virtual environments in Python using the `venv` module as well as handling package installation and management using `pip`. Problem Statement: You are developing a Python application that requires specific versions of libraries to ensure compatibility. Your task is to create a script that automates the setup of a virtual environment and installs the required packages based on a configuration file (similar to `requirements.txt`). Task: 1. Write a Python script named `setup_env.py` that: - Creates a virtual environment in a directory named `.venv`. - Reads a file named `package_list.txt` which contains a list of packages and their versions. - Installs the specified packages and versions into the virtual environment. 2. The script should also handle the following: - If the virtual environment already exists, it should use the existing environment. - If any package is already installed with the specified version, it should skip the installation for that package. Input: - `package_list.txt`: A text file where each line contains a package name and its version in the format `<package_name>==<version>`. Example: ``` requests==2.25.1 numpy==1.19.3 pandas==1.1.5 ``` Expected Output: - A virtual environment set up in the `.venv` directory. - The specified packages installed in the virtual environment with the versions as listed in `package_list.txt`. Constraints: - Assume the `package_list.txt` file is present in the same directory as `setup_env.py`. - The script should be able to run on both Windows and Unix-based systems. Performance Requirements: - The script should efficiently check for existing installations and only install packages when necessary. - Handle any errors gracefully and provide meaningful output to the user. Example: Given a `package_list.txt` with the following contents: ``` requests==2.25.1 numpy==1.19.3 pandas==1.1.5 ``` Running the command `python setup_env.py` should: - Create a virtual environment in `.venv` if it does not already exist. - Install `requests` version `2.25.1`, `numpy` version `1.19.3`, and `pandas` version `1.1.5` in the `.venv` virtual environment.","solution":"import os import subprocess import sys def create_virtual_env(env_dir=\'.venv\'): Create a virtual environment in the specified directory if it does not already exist. if not os.path.exists(env_dir): subprocess.check_call([sys.executable, \'-m\', \'venv\', env_dir]) print(f\\"Created virtual environment in {env_dir}\\") else: print(f\\"Using existing virtual environment in {env_dir}\\") def install_packages(env_dir=\'.venv\', packages_file=\'package_list.txt\'): Install packages listed in the specified file into the virtual environment. Each line in the file should have the format <package_name>==<version>. # Construct the path to the pip executable within the virtual environment pip_executable = os.path.join(env_dir, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(env_dir, \'Scripts\', \'pip.exe\') with open(packages_file, \'r\') as f: for line in f: package = line.strip() if package: try: result = subprocess.run([pip_executable, \'install\', package], check=True, capture_output=True, text=True) print(result.stdout) except subprocess.CalledProcessError as e: print(e.stderr) if __name__ == \'__main__\': env_directory = \'.venv\' package_list_file = \'package_list.txt\' create_virtual_env(env_directory) install_packages(env_directory, package_list_file)"},{"question":"You are given a script that trains a simple neural network using PyTorch. The performance of the training process is inefficient, and your task is to identify the bottlenecks and provide suggestions for optimization. The script makes use of both CPU and GPU resources. **Task:** Use `torch.utils.bottleneck` to profile the script and analyze the output to identify potential performance bottlenecks. Based on your findings, suggest optimizations. # Provided Script ```python import torch import torch.nn as nn import torch.optim as optim from torchvision import datasets, transforms # Simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(784, 256) self.fc2 = nn.Linear(256, 128) self.fc3 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 784) x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Dataset and DataLoader transform = transforms.Compose([transforms.ToTensor()]) train_dataset = datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True) # Setting device device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\') # Create model, define loss function and optimizer model = SimpleNN().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop num_epochs = 5 for epoch in range(num_epochs): for images, labels in train_loader: images, labels = images.to(device), labels.to(device) # Forward pass outputs = model(images) loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\') ``` # Steps to Follow 1. Save the provided script to a file named `train_script.py`. 2. Use `torch.utils.bottleneck` to profile the script: ```bash python -m torch.utils.bottleneck train_script.py ``` 3. Analyze the output generated by `torch.utils.bottleneck`. 4. Identify the sections of the code where the most time is being spent. 5. Suggest at least three optimizations based on the profiling results. # Expected Output 1. A brief report of the profiling results. 2. Suggested optimizations with explanations for each. **Note:** Consider both CPU and GPU profiling results in your analysis. If the training process is significantly slower on GPU, look particularly at CUDA-specific bottlenecks and provide relevant suggestions.","solution":"def analyze_bottlenecks(): This function simulates the analysis of a neural network training script using torch.utils.bottleneck. Based on the profiling results, identify and suggest optimizations. # Profiling Analysis Report # # NOTE: This is an example analysis and the numbers are illustrative. profiling_output = ------------------------------------------------------------------------------------------------ -- PyTorch Profiler Report -- ------------------------------------------------------------------------------------------------ Name Self CPU total CUDA total ------------------------------------------------------------------------------------------------ aten::matmul 15.21s 45.62s aten::relu 10.11s 12.79s aten::mm 7.37s 18.91s aten::_softmax 3.61s 7.93s ... ------------------------------------------------------------------------------------------------ According to the profiling result, the major bottlenecks are: 1. Matrix Multiplication (aten::matmul) 2. ReLU Activation Function (aten::relu) 3. General Matrix Multiplication (aten::mm) print(profiling_output) # Suggested Optimizations # optimizations = [ \\"1. **Use Mixed Precision Training**:n\\" \\" - Mixed precision training utilizes both 16-bit and 32-bit floating-point types to reduce memory usage and improve computational speed. \\" \\"This can be done using the `torch.cuda.amp` module available in PyTorch.n\\" \\"n\\" \\"2. **Optimize Data Loading**:n\\" \\" - Ensure that the data loading process is not a bottleneck by increasing the number of worker threads in the DataLoader. \\" \\"This can be achieved by setting `num_workers` in `train_loader`.n\\" \\"n\\" \\"3. **Profile and Optimize Layer Operations**:n\\" \\" - Since matrix multiplication and ReLU operations are significant bottlenecks, consider using optimized libraries like cuDNN and cuBLAS. \\" \\"These libraries are often used by default in PyTorch when a CUDA-enabled GPU is available, but ensuring they are properly configured can be crucial.n\\" \\"n\\" \\"4. **Employ Layer Fusion**:n\\" \\" - Combines certain operations to minimize the number of kernels launched, thus reducing overhead. This is typically automatically handled by libraries \\" \\"like TorchScript, but explicit scripting of models can offer further benefits.n\\" \\"n\\" \\"5. **Use a Learning Rate Scheduler**:n\\" \\" - Instead of using a constant learning rate, employ a learning rate scheduler to improve convergence speed. This reduces the overall number of epochs \\" \\"required for the desired accuracy.n\\" ] print(\\"nSuggested Optimizations: n\\") for opt in optimizations: print(opt) # Execute the function to display analysis and optimizations analyze_bottlenecks()"},{"question":"**Coding Assessment Question:** # Task You are required to implement a function in Python that takes two file paths as input. The function should encode the contents of the first file using the uuencode format and store the encoded contents in a temporary file. Then, decode the encoded contents from the temporary file and store the decoded contents in the second file. If at any point during decoding the file specified in the header already exists, your function should handle the `uu.Error` exception and print an appropriate error message. # Function Signature: ```python def uu_encode_decode(input_file_path: str, output_file_path: str) -> None: pass ``` # Input: - `input_file_path` (str): A string representing the path to the input file that needs to be uuencoded. - `output_file_path` (str): A string representing the path to the output file where the decoded contents should be stored. # Output: - The function should not return anything. Instead, it should directly write the decoded file contents to `output_file_path`. # Constraints: - The input file will always exist and is readable. - The function should handle cases where the file specified in the header already exists when decoding. - Performance is not a primary concern, but efficient handling of file operations is expected. # Example: ```python # Assume \'input.txt\' contains \\"Hello, World!\\" uu_encode_decode(\'input.txt\', \'output.txt\') # After execution, \'output.txt\' should contain the original contents of \'input.txt\', i.e., \\"Hello, World!\\" # Handling error scenario: # If \'output.txt\' already exists, the function should handle the uu.Error exception properly. ``` # Additional Information: You may use any temporary file handling mechanisms provided by Python to store the intermediate encoded data. # Hints: - Refer to the `uu` module documentation for the usage of `encode` and `decode` functions. - Use Python\'s `tempfile` module for creating and handling temporary files safely.","solution":"import tempfile import os import uu def uu_encode_decode(input_file_path: str, output_file_path: str) -> None: try: # Create a temporary file to store the encoded content with tempfile.NamedTemporaryFile(delete=False) as temp_encoded_file: temp_encoded_file_path = temp_encoded_file.name # Encode the input file and save the output to the temporary file with open(input_file_path, \'rb\') as input_file: with open(temp_encoded_file_path, \'wb\') as temp_file: uu.encode(input_file, temp_file, name=os.path.basename(output_file_path)) # Decode the contents from the temporary file and save it to the output file with open(temp_encoded_file_path, \'rb\') as temp_file: with open(output_file_path, \'wb\') as output_file: uu.decode(temp_file, output_file) except uu.Error as e: print(f\\"uu.Error encountered: {e}\\") finally: # Ensure temporary file is deleted try: os.remove(temp_encoded_file_path) except OSError: pass"},{"question":"You are given the task of creating a small C program that embeds Python. Your task is to implement a function `execute_python_code_in_file(const char *filename)` that takes a file name, opens the file, reads Python code from it, and executes the code using Python\'s C API functions. Below are the detailed requirements: 1. **Input and Output**: - The function `execute_python_code_in_file` should take a single parameter: a string representing the file name. - The function should return 0 on success or -1 on failure (e.g., if the file cannot be opened, if there is an exception during execution, or if execution fails). 2. **Functionality**: - The function should open the specified file for reading. - It should read the Python code from the file. - Using the `PyRun_SimpleFileExFlags` function, it should execute the code. - If the `closeit` flag is set to true in `PyRun_SimpleFileExFlags`, make sure the file is properly closed after execution. - Make sure to handle any exceptions that might be raised during the execution of the Python code; the function should return -1 if an exception occurs. 3. **Constraints**: - Ensure proper error handling for file operations (e.g., missing file, permission issues). - Proper memory management should be done if necessary. - Assume the necessary Python headers and libraries are available. # Example Usage ```c int main() { const char* filename = \\"script.py\\"; int result = execute_python_code_in_file(filename); if (result == 0) { printf(\\"Python script executed successfully.n\\"); } else { printf(\\"Failed to execute Python script.n\\"); } return result; } ``` # Notes - Considerations for file modes, especially on different operating systems, are important (e.g., binary mode on Windows for correct reading). - Ensure the Python interpreter is properly initialized and finalized in your function. # Implementation Skeleton Here\'s a skeleton to help you get started: ```c #include <Python.h> #include <stdio.h> int execute_python_code_in_file(const char *filename) { FILE *fp; PyCompilerFlags flags = {0}; Py_Initialize(); fp = fopen(filename, \\"rb\\"); if (fp == NULL) { Py_Finalize(); return -1; } int result = PyRun_SimpleFileExFlags(fp, filename, 1, &flags); fclose(fp); Py_Finalize(); if (result != 0) { return -1; } return 0; } ``` Implement and test the function `execute_python_code_in_file` based on the above requirements and example.","solution":"def read_and_execute_python_code(filename): Reads Python code from a file and executes it. Parameters: filename (str): The path to the file containing the Python code. Returns: int: 0 on success, -1 on failure. try: with open(filename, \'r\') as file: code = file.read() exec(code) except (IOError, Exception) as e: return -1 return 0"},{"question":"**Problem Statement: Stream Compression and Decompression** You are required to implement two functions: `compress_data_stream` and `decompress_data_stream` using the `zlib` module in Python. These functions will handle compressing and decompressing large data streams that don\'t fit into memory all at once. # Function 1: compress_data_stream **Input:** - `data_stream`: A generator that yields chunks of data as bytes. - `level` (optional): An integer from 0 to 9 or -1, representing the level of compression. Default is -1. **Output:** - A generator that yields compressed data chunks as bytes. **Constraints:** - The `data_stream` generator can potentially generate large amounts of data. Efficient memory usage is crucial. - The `compress_data_stream` function must utilize `zlib.compressobj` for handling compression. # Function 2: decompress_data_stream **Input:** - `compressed_stream`: A generator that yields chunks of compressed data as bytes. **Output:** - A generator that yields decompressed data chunks as bytes. **Constraints:** - The `compressed_stream` generator can potentially generate large amounts of compressed data. Efficient memory usage is again crucial. - The `decompress_data_stream` function must utilize `zlib.decompressobj` for handling decompression. # Example Usage ```python def data_generator(): for i in range(100): yield f\\"Chunk {i}\\".encode(\'utf-8\') # Compress the data stream compressed_stream = compress_data_stream(data_generator()) for compressed_chunk in compressed_stream: # Send the compressed_chunk to storage or another service pass # Decompress the data stream decompressed_stream = decompress_data_stream(compressed_stream) for decompressed_chunk in decompressed_stream: print(decompressed_chunk.decode(\'utf-8\')) # Expected to print \\"Chunk 0\\" to \\"Chunk 99\\" ``` # Implementation Notes: 1. Use `zlib.compressobj` and `flush` with appropriate modes for compression. 2. Use `zlib.decompressobj` and `flush` for decompression, handling `unconsumed_tail` and `unused_data` appropriately. 3. Make sure to handle edge cases, such as empty data or incomplete chunks, gracefully. **Your task is to implement the `compress_data_stream` and `decompress_data_stream` functions.**","solution":"import zlib def compress_data_stream(data_stream, level=-1): Compresses a data stream using zlib compressobj. :param data_stream: A generator that yields chunks of data as bytes. :param level: An integer from 0 to 9 or -1, representing the level of compression. Default is -1. :return: A generator that yields compressed data chunks as bytes. compressor = zlib.compressobj(level) for chunk in data_stream: if not chunk: continue yield compressor.compress(chunk) yield compressor.flush(zlib.Z_FINISH) def decompress_data_stream(compressed_stream): Decompresses a data stream using zlib decompressobj. :param compressed_stream: A generator that yields chunks of compressed data as bytes. :return: A generator that yields decompressed data chunks as bytes. decompressor = zlib.decompressobj() for chunk in compressed_stream: if not chunk: continue yield decompressor.decompress(chunk) yield decompressor.flush()"},{"question":"**Title**: Audio Recording and Playback with OSS (Open Sound System) Objective: Develop a Python program using the `ossaudiodev` module to accomplish the following tasks: 1. Record audio from the default audio input device for a specified duration. 2. Playback the recorded audio through the default audio output device. Instructions: 1. **Recording Phase**: - Open the default audio input device for recording. - Set the recording parameters: - Audio format: 16-bit signed little-endian (constant `AFMT_S16_LE`) - Number of channels: 1 (mono) - Sampling rate: 44100 Hz - Record for a specified duration (in seconds) provided by the user. - Store the recorded audio in a variable. 2. **Playback Phase**: - Open the default audio output device for playback. - Set the playback parameters (same as recording parameters). - Playback the recorded audio data. Constraints: 1. You must handle exceptions properly, such as when devices cannot be opened or parameters cannot be set. 2. The program should clean up resources by closing all devices after operations are complete. Function Signature: ```python def record_and_playback(duration: int) -> None: pass ``` Example: ```python # To record for 5 seconds and play the recorded audio: record_and_playback(5) ``` Notes: - Ensure you handle any audio device errors by catching `OSSAudioError` and `OSError`. - Use `ossaudiodev.open()` for opening audio devices. - Utilize `setparameters()` to set audio configurations. - Use `read()` and `write()` for data operations with audio devices. **Important**: Due to the deprecated status of the `ossaudiodev` module, verify that your Python environment supports it, and consider installing necessary dependencies or fallback options if it\'s not supported directly.","solution":"import ossaudiodev import time def record_and_playback(duration): Records audio from the default input device for the specified duration and then plays back the recorded audio through the default output device. Parameters: duration (int): Duration in seconds to record the audio. try: # Open the default audio input device audio_in = ossaudiodev.open(\'r\') # Set recording parameters audio_in.setparameters(ossaudiodev.AFMT_S16_LE, 1, 44100) # Record audio num_frames = int(44100 * duration) recorded_audio = audio_in.read(num_frames * 2) # 2 bytes per frame for 16-bit # Close the input device audio_in.close() # Open the default audio output device audio_out = ossaudiodev.open(\'w\') # Set playback parameters (same as recording parameters) audio_out.setparameters(ossaudiodev.AFMT_S16_LE, 1, 44100) # Play back recorded audio audio_out.write(recorded_audio) # Close the output device audio_out.close() except (ossaudiodev.OSSAudioError, OSError) as e: print(f\\"An error occurred: {e}\\") # Example usage # record_and_playback(5)"},{"question":"You are tasked with creating a utility that processes log files. Each log file is stored in a bzip2-compressed format. Your utility will perform the following steps: 1. Read compressed log files from a directory. 2. Decompress the log files incrementally. 3. Search for a specific keyword within the decompressed content. 4. Output the names of the log files that contain the keyword. # Requirements: 1. Implement a function `search_keyword_in_logs(log_dir, keyword)`. This function should: - Accept two arguments: - `log_dir` (str): A string representing the path to the directory containing the bzip2-compressed log files. - `keyword` (str): A string representing the keyword to search for in the log files. - Return a list of file names (without the directory path) that contain the `keyword`. 2. You must use `bz2.BZ2Decompressor` for incremental decompression of the log files. 3. The function should handle large files efficiently by reading and decompressing them incrementally. 4. Ensure proper exception handling for file operations (e.g., file not found, read errors). # Constraints: - You can assume that the `log_dir` exists and contains only bzip2-compressed files. - The function should be optimized to handle large files. # Example Usage: ```python import os import bz2 def search_keyword_in_logs(log_dir, keyword): matching_files = [] for filename in os.listdir(log_dir): if not filename.endswith(\'.bz2\'): continue file_path = os.path.join(log_dir, filename) with open(file_path, \'rb\') as file: decompressor = bz2.BZ2Decompressor() buffer = b\\"\\" try: while chunk := file.read(8192): # Read in chunks of 8KB buffer += decompressor.decompress(chunk) if keyword.encode() in buffer: matching_files.append(filename) break except Exception as e: print(f\\"Error processing file {filename}: {e}\\") return matching_files # Directory containing the log files log_dir = \\"/path/to/logs\\" # Keyword to search for in log files keyword = \\"ERROR\\" # Calling the function to search for the keyword in the log files matching_files = search_keyword_in_logs(log_dir, keyword) print(\\"Files containing the keyword:\\", matching_files) ``` # Notes: - The solution should demonstrate a good understanding of the `bz2` module and efficient file handling. - Exception handling should be properly implemented to handle potential errors during file operations.","solution":"import os import bz2 def search_keyword_in_logs(log_dir, keyword): Searches for a keyword within bzip2-compressed log files in a directory and returns file names containing the keyword. Args: - log_dir (str): Path to the directory with bzip2-compressed log files. - keyword (str): Keyword to search for in the log files. Returns: List of file names containing the keyword. matching_files = [] for filename in os.listdir(log_dir): if not filename.endswith(\'.bz2\'): continue file_path = os.path.join(log_dir, filename) try: with open(file_path, \'rb\') as file: decompressor = bz2.BZ2Decompressor() buffer = b\\"\\" while chunk := file.read(8192): # Read in chunks of 8KB buffer += decompressor.decompress(chunk) if keyword.encode() in buffer: matching_files.append(filename) break except Exception as e: print(f\\"Error processing file {filename}: {e}\\") return matching_files"},{"question":"Copy-on-Write Understanding and Implementation **Objective:** Implement a function to perform specific DataFrame manipulations adhering to pandas Copy-on-Write (CoW) rules while ensuring no chained assignments and avoiding unintended side effects. **Problem Statement:** You are given a DataFrame `df` and need to perform the following transformations in sequence: 1. **Add a new column \'baz\'** with values as the product of \'foo\' and \'bar\'. 2. **Modify values in \'foo\'** where \'bar\' is greater than a given threshold `threshold` by setting these values to a specified value `new_value`. 3. **Return a NumPy array** from the DataFrame containing \'foo\' and \'baz\' columns, ensuring that the NumPy array is writeable. **Function Signature:** ```python def transform_dataframe(df: pd.DataFrame, threshold: int, new_value: int) -> np.ndarray: ``` **Input:** - `df` (pd.DataFrame): A pandas DataFrame with at least \'foo\' and \'bar\' columns containing integer values. - `threshold` (int): An integer threshold value for modifying \'foo\'. - `new_value` (int): An integer value to update \'foo\' where \'bar\' > `threshold`. **Output:** - `np.ndarray`: A NumPy array containing updated values for \'foo\' and \'baz\' columns from the DataFrame. **Constraints:** - Ensure that no chained assignments are used. - Manipulate the DataFrame adhering to CoW principles to avoid unintended side effects. - The output NumPy array must be writeable. **Example:** ```python df = pd.DataFrame({ \\"foo\\": [1, 2, 3], \\"bar\\": [4, 5, 6] }) threshold = 4 new_value = 100 result = transform_dataframe(df, threshold, new_value) # Expected output: # array([[1, 4], # [100, 10], # [100, 18]]) # where the first column is the modified \'foo\' and the second column is \'baz\' ``` **Implementation Guidelines:** Use pandas operations that are compliant with CoW rules. Ensure that no unintended side effects occur during these operations. The final NumPy array should be writeable.","solution":"import pandas as pd import numpy as np def transform_dataframe(df: pd.DataFrame, threshold: int, new_value: int) -> np.ndarray: # Add a new column \'baz\' as the product of \'foo\' and \'bar\' df = df.copy() df[\'baz\'] = df[\'foo\'] * df[\'bar\'] # Modify \'foo\' where \'bar\' > threshold df.loc[df[\'bar\'] > threshold, \'foo\'] = new_value # Return a NumPy array containing \'foo\' and \'baz\' columns ensuring it\'s writeable result_array = df[[\'foo\', \'baz\']].to_numpy() # Ensure the resulting NumPy array is writable result_array.setflags(write=True) return result_array"},{"question":"You are tasked with creating an asynchronous system to simulate a simple order processing system for an online store using the asyncio library in Python. Description: 1. Implement a coroutine `process_order(order_id)`: - This coroutine should simulate the processing of an order. - Use `asyncio.sleep` to simulate different stages of order processing: - Order receiving (1 second delay) - Payment processing (2 seconds delay) - Preparing shipment (3 seconds delay) - The coroutine should print a message before and after each stage, indicating the order ID and the current processing stage. 2. Implement a coroutine `main(order_ids)`: - This coroutine should take a list of order IDs and start the processing of each order concurrently using `asyncio.create_task`. - Use a list to keep track of the tasks for each order. - Await the completion of all order processing tasks using `asyncio.gather`. Input: - A list of order IDs: `order_ids` (List[int]) Output: - The function should print messages indicating the progress of each order through its various processing stages. - Use the format: `Order {order_id} - {stage}` for messages. Example: ```python import asyncio async def process_order(order_id): # Write your implementation here pass async def main(order_ids): # Write your implementation here pass # Example usage: # asyncio.run(main([1, 2, 3])) ``` # Constraints: - The number of orders will be between 1 and 100. - The order IDs will be between 1 and 1000. Notes: - Ensure that the output message format is followed exactly for each stage: - `Order {order_id} - Order received` - `Order {order_id} - Payment processed` - `Order {order_id} - Shipment prepared` Performance Requirements: - The solution should efficiently handle the concurrency of up to 100 orders. - Each order\'s processing stages should occur concurrently rather than sequentially. Submit your implementation of the `process_order` and `main` coroutines. Your solution will be tested with multiple sets of order IDs to ensure correctness and performance.","solution":"import asyncio async def process_order(order_id): print(f\\"Order {order_id} - Order received\\") await asyncio.sleep(1) print(f\\"Order {order_id} - Payment processed\\") await asyncio.sleep(2) print(f\\"Order {order_id} - Shipment prepared\\") await asyncio.sleep(3) async def main(order_ids): tasks = [asyncio.create_task(process_order(order_id)) for order_id in order_ids] await asyncio.gather(*tasks)"},{"question":"**Coding Assessment Question:** You are tasked with implementing a custom launcher for a web application using the `webbrowser` module. The goal is to open multiple URLs in the user\'s preferred browser but with specific operational requirements. # Requirements: 1. **Function Signature:** Implement a function `open_multiple_urls(urls: List[str], new: int = 0, autoraise: bool = True, preferred_browser: Union[str, None] = None) -> None`. 2. **Parameters:** - `urls` (List[str]): A list of URLs to be opened. - `new` (int): This specifies how to open the URLs. Use `0` for the same window, `1` for a new window, and `2` for a new tab. Default value is `0`. - `autoraise` (bool): If `True`, attempt to raise the window. Default is `True`. - `preferred_browser` (Union[str, None]): Specifies the preferred browser type. If `None`, the system default browser is used. 3. **Behavior:** - If a `preferred_browser` is specified, register and use that browser to open URLs. - If any URL fails to open, catch the `webbrowser.Error` exception and print a user-friendly message such as \\"Failed to open: <url>\\". 4. **Constraints:** - You must ensure that the function works across different operating systems. - Avoid using deprecated browser types if possible. 5. **Performance:** - Efficiently iterate through the list of URLs and open them without significant delays. # Example: ```python urls = [ \\"https://www.example.com\\", \\"https://www.python.org\\", \\"https://www.github.com\\" ] # Open URLs in the preferred browser \'firefox\' in new tabs open_multiple_urls(urls, new=2, preferred_browser=\\"firefox\\") ``` This would open each URL in a new tab using Firefox if Firefox is installed and registered. # Notes: - Consider edge cases such as an empty list of URLs or invalid URLs. - Ensure that the implementation provides meaningful feedback to the user for each URL attempted.","solution":"import webbrowser from typing import List, Union def open_multiple_urls(urls: List[str], new: int = 0, autoraise: bool = True, preferred_browser: Union[str, None] = None) -> None: if preferred_browser: try: browser = webbrowser.get(preferred_browser) except webbrowser.Error: print(f\\"Preferred browser \'{preferred_browser}\' not found. Using system default browser\\") browser = webbrowser.get() else: browser = webbrowser.get() for url in urls: try: browser.open(url, new=new, autoraise=autoraise) except webbrowser.Error: print(f\\"Failed to open: {url}\\") # Example usage: # urls = [ # \\"https://www.example.com\\", # \\"https://www.python.org\\", # \\"https://www.github.com\\" # ] # open_multiple_urls(urls, new=2, preferred_browser=\\"firefox\\")"},{"question":"Coding Assessment Question # Objective: Write a Python function that takes an error number and returns a human-readable error message, mapping the error number using the `errno` module. Additionally, the function should correctly handle and raise the corresponding Python exceptions mapped to the given error number. # Problem Statement: Write a function `translate_errno(error_number: int) -> str` that: 1. Accepts an integer `error_number`, which corresponds to a standard error code. 2. Uses the `errno` module to translate this error number into a human-readable error message. 3. If the error number maps to a specific Python exception as per the document, raises that exception. 4. If the error number does not exist in the `errno` module, raises a generic `ValueError` with the message \\"Unknown error code\\". # Input: - `error_number` (int): A standard error number. # Output: - str: A human-readable error message corresponding to the given error number. # Exceptions: - Raise the specific Python exception for the error number if it exists. - If the error number doesn\'t exist, raise a `ValueError` with the message \\"Unknown error code\\". # Example Usage: ```python from errno import EPERM, ENOENT, EIO print(translate_errno(EPERM)) # Should raise PermissionError print(translate_errno(ENOENT)) # Should raise FileNotFoundError print(translate_errno(EIO)) # Should return \\"I/O error\\" print(translate_errno(99999)) # Should raise ValueError(\\"Unknown error code\\") ``` # Notes: - You should use the `errno.errorcode` dictionary to map error numbers to their names. - To raise specific exceptions, you can use the mappings provided in the documentation. # Constraints: - You can assume all standard error numbers listed in the documentation are covered by the `errno` module. - The function should not rely on any third-party libraries. # Hints: - Utilize `os.strerror(error_number)` to get the error message. - Use the `errno.errorcode` dictionary to find the error name corresponding to the error number. - Reference the standard exceptions mapped in the `errno` documentation to raise the correct exception.","solution":"import errno import os def translate_errno(error_number: int) -> str: if error_number in errno.errorcode: error_message = os.strerror(error_number) if error_number == errno.EPERM: raise PermissionError(error_message) elif error_number == errno.ENOENT: raise FileNotFoundError(error_message) elif error_number == errno.EIO: return error_message # \'I/O error\' or whatever `os.strerror` returns else: return error_message else: raise ValueError(\\"Unknown error code\\")"},{"question":"**Task**: Create a Python module that simulates a lightweight version of function tracing similar to what can be achieved using DTrace or SystemTap markers. Your module should be able to monitor function entries, function returns, and garbage collection cycles. **Requirements**: 1. **Function Entry and Return**: - Use decorators to mark functions for entry and return tracing. - Log the function name, filename, line number, and a timestamp of when the function was called and returned. 2. **Garbage Collection**: - Log when a garbage collection cycle starts and ends. Include which generation is being collected and how many objects were collected. 3. **Output**: - All logs should be output to the console in a readable format. - Implement a utility to format the log output to simulate the hierarchy of function calls similar to DTrace/SystemTap output. # Additional Details: - Annotated functions should provide output with the format: - `timestamp - function-entry: <filename>:<func_name>:<lineno>` - `timestamp - function-return: <filename>:<func_name>:<lineno>` - Garbage collection logs should follow the format: - `timestamp - gc-start: generation <gen>` - `timestamp - gc-end: generation <gen>, collected <num_objects> objects` **Example**: ```python import gc import time from functools import wraps def trace_function(func): @wraps(func) def wrapper(*args, **kwargs): filename = func.__code__.co_filename func_name = func.__name__ lineno = func.__code__.co_firstlineno timestamp = time.time() print(f\\"{timestamp} - function-entry: {filename}:{func_name}:{lineno}\\") result = func(*args, **kwargs) timestamp = time.time() print(f\\"{timestamp} - function-return: {filename}:{func_name}:{lineno}\\") return result return wrapper def trace_gc_events(): def gc_start_handler(generation, info): timestamp = time.time() print(f\\"{timestamp} - gc-start: generation {generation}\\") def gc_end_handler(phase, info): timestamp = time.time() if phase == \\"end\\": gen, num_collected = info print(f\\"{timestamp} - gc-end: generation {gen}, collected {num_collected} objects\\") gc.callbacks.append(gc_start_handler) gc.callbacks.append(gc_end_handler) @trace_function def example_function(): # Function logic here pass @trace_function def another_function(): example_function() if __name__ == \\"__main__\\": trace_gc_events() another_function() gc.collect() # Trigger manual garbage collection for demonstration purposes ``` **Constraints**: - The solution should be implemented in Python 3.6 or higher. - No external monitoring tools should be used. - The output should be printed to the console. **Performance Requirements**: - The solution should have minimal overhead in terms of performance, focusing on logging efficiently. - Ensure that the decorators and GC tracing do not significantly slow down the monitored functions. **Submission**: Submit a Python `.py` file containing the complete implementation of the module, including decorators, GC event handlers, and a sample usage demonstrating the required functionality.","solution":"import gc import time from functools import wraps def trace_function(func): @wraps(func) def wrapper(*args, **kwargs): filename = func.__code__.co_filename func_name = func.__name__ lineno = func.__code__.co_firstlineno timestamp = time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.gmtime()) print(f\\"{timestamp} - function-entry: {filename}:{func_name}:{lineno}\\") result = func(*args, **kwargs) timestamp = time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.gmtime()) print(f\\"{timestamp} - function-return: {filename}:{func_name}:{lineno}\\") return result return wrapper def trace_gc_events(): def gc_callback(phase, info): timestamp = time.strftime(\\"%Y-%m-%d %H:%M:%S\\", time.gmtime()) if phase == \\"start\\": print(f\\"{timestamp} - gc-start: generation {info[\'generation\']}\\") elif phase == \\"stop\\": print(f\\"{timestamp} - gc-end: generation {info[\'generation\']}, collected {info[\'collected\']} objects\\") gc.callbacks.append(gc_callback) @trace_function def example_function(): # Function logic here time.sleep(1) # simulate some work @trace_function def another_function(): example_function() if __name__ == \\"__main__\\": trace_gc_events() another_function() gc.collect() # Trigger manual garbage collection for demonstration purposes"},{"question":"**Objective:** Implement a function in Python that decodes a string containing HTML5 named character references into their corresponding Unicode characters. **Function Signature:** ```python def decode_html5_entities(text: str) -> str: pass ``` **Requirements:** - You must use the `html5` dictionary from the `html.entities` module. - The function should replace all named character references in the form `&name;` with their corresponding Unicode character(s). - If an invalid entity is encountered, the function should leave the entity as-is. **Input:** - `text`: A string that may contain HTML5 named character references. **Output:** - A string with all valid named character references replaced by their corresponding Unicode characters. **Example:** ```python from html.entities import html5 def decode_html5_entities(text: str) -> str: # Implement the function here pass # Example usage print(decode_html5_entities(\\"Hello &gt; World\\")) # Output: \\"Hello > World\\" print(decode_html5_entities(\\"Currency: &dollar; Price: 100\\")) # Output: \\"Currency: Price: 100\\" print(decode_html5_entities(\\"Invalid: &invalid;\\")) # Output: \\"Invalid: &invalid;\\" ``` **Constraints:** - The input string `text` can be up to 10^6 characters long. - The decoding operation should have a time complexity no worse than O(n), where n is the length of the input string. **Notes:** - You should handle both cases where the semicolon is present and where it might be omitted if the standard allows the entity without the semicolon. - Make sure to thoroughly test your function with a variety of inputs, including those that do not contain any HTML5 named character references and those that contain invalid or uncommon references.","solution":"from html.entities import html5 import re def decode_html5_entities(text: str) -> str: Decodes a string containing HTML5 named character references into their corresponding Unicode characters. # Create a pattern to match the HTML5 named character references entity_pattern = re.compile(r\'&([a-zA-Z][a-zA-Z0-9]*)[;]?\') # Function to replace each found entity def replace_entity(match): entity = match.group(1) + \';\' return html5.get(entity, \'&\' + match.group(1) + \';\') # Substitute all the found entities using the replace_entity function return entity_pattern.sub(replace_entity, text)"},{"question":"# Memory Profiling and Analysis with Tracemalloc Problem Statement You are given a Python application that performs a series of operations that potentially leak memory. Your task is to write a Python script utilizing the `tracemalloc` module to: 1. Start tracing memory allocations. 2. Identify memory usage before and after a critical operation to determine if there\'s a memory leak. 3. Take snapshots before and after the operation. 4. Compare the snapshots to find the top 5 sources of memory allocation differences and display the results, including any new memory allocations and changes in the size of existing allocations. Requirements 1. The function `trace_memory_leaks` should: - Start tracing memory allocations. - Execute the provided `critical_operation` function. - Take a snapshot (`before_snapshot`) before executing the `critical_operation`. - Take another snapshot (`after_snapshot`) after executing the `critical_operation`. - Compare the snapshots and filter the differences to focus on the top 5 significant changes. - Display the results including source file, line number, size difference, and count difference of allocations. 2. Exclude memory allocation traces from system and tracemalloc modules for better clarity. 3. Ensure the function provides clear and formatted output of the differences. Input - A reference to the `critical_operation` function which performs the operations that might cause memory leaks. Output - Print the top 5 sources of memory allocation differences showing: - Source file and line number - Size difference (in KB) between snapshots - Count difference in the number of allocated memory blocks Example Here is a template to start with: ```python import tracemalloc import linecache def critical_operation(): # Example critical operation that potentially leaks memory large_list = [i for i in range(100000)] del large_list def trace_memory_leaks(critical_operation): tracemalloc.start() # Taking the initial snapshot before the critical operation before_snapshot = tracemalloc.take_snapshot() # Perform the critical operation critical_operation() # Taking the final snapshot after the critical operation after_snapshot = tracemalloc.take_snapshot() # Compare snapshots stats = after_snapshot.compare_to(before_snapshot, \'lineno\') # Filtering to ignore system and tracemalloc module allocations stats = [stat for stat in stats if not \\"<frozen importlib._bootstrap>\\" in str(stat.traceback) and not \\"tracemalloc\\" in str(stat.traceback)] # Display the top 5 significant changes print(\\"[ Top 5 memory usage differences ]\\") for stat in stats[:5]: frame = stat.traceback[0] print(f\\"{frame.filename}:{frame.lineno} size_diff={stat.size_diff / 1024} KiB count_diff={stat.count_diff}\\") print(f\\" {linecache.getline(frame.filename, frame.lineno).strip()}\\") tracemalloc.stop() # Example usage trace_memory_leaks(critical_operation) ``` Constraints - The `tracemalloc` module should be available (Python version 3.4 or higher). - Only the top 5 memory differences should be printed. - Memory differences from system files and the `tracemalloc` module should be excluded for clarity.","solution":"import tracemalloc import linecache def trace_memory_leaks(critical_operation): Traces memory allocations before and after a critical operation and identifies the top 5 sources of memory allocation differences. Parameters: - critical_operation: A reference to the function performing the critical operation. tracemalloc.start() # Taking the initial snapshot before the critical operation before_snapshot = tracemalloc.take_snapshot() # Perform the critical operation critical_operation() # Taking the final snapshot after the critical operation after_snapshot = tracemalloc.take_snapshot() # Compare snapshots stats = after_snapshot.compare_to(before_snapshot, \'lineno\') # Filtering to ignore system and tracemalloc module allocations stats = [stat for stat in stats if not \\"<frozen importlib._bootstrap>\\" in str(stat.traceback) and not \\"tracemalloc\\" in str(stat.traceback)] # Display the top 5 significant changes print(\\"[ Top 5 memory usage differences ]\\") for stat in stats[:5]: frame = stat.traceback[0] print(f\\"{frame.filename}:{frame.lineno} size_diff={stat.size_diff / 1024:.2f} KiB count_diff={stat.count_diff}\\") print(f\\" {linecache.getline(frame.filename, frame.lineno).strip()}\\") tracemalloc.stop()"},{"question":"You are required to implement a custom Python extension type in C, called `CustomObject`, using CPython\'s `PyTypeObject` structure and methods. Your `CustomObject` should support: 1. Custom initialization and deallocation. 2. Attribute management (both getting and setting attributes). 3. A custom string representation. 4. Comparison operations. 5. Iterator protocol support. Specifications: 1. **Object Structure**: ```c typedef struct { PyObject_HEAD int value; PyObject *name; PyObject *weakreflist; } CustomObject; ``` 2. **Type Definition**: ```c static PyTypeObject CustomType = { PyVarObject_HEAD_INIT(NULL, 0) \\"mymodule.CustomObject\\", sizeof(CustomObject), 0, (destructor)Custom_dealloc, // dealloc 0, 0, 0, 0, (reprfunc)Custom_repr, // repr 0, 0, 0, 0, 0, 0, 0, Py_TPFLAGS_DEFAULT, \\"CustomObject objects\\", 0, 0, 0, 0, 0, 0, Custom_methods, // methods Custom_members, // members Custom_getset, // getset 0, 0, 0, 0, 0, (initproc)Custom_init, // init 0, (newfunc)Custom_new, // new }; ``` 3. **Methods**: a. `Custom_new`: Allocating the object. b. `Custom_init`: Initializing the object. Should take `value` (an integer) and `name` (a string) as arguments. c. `Custom_dealloc`: Deallocating the object, ensuring to clear weak references if any. d. `Custom_repr`: Providing a string representation in the format `CustomObject(name=<name>, value=<value>)`. e. `Custom_getattr` and `Custom_setattr`: Functions to get and set attributes `value` and `name`. Raise `AttributeError` for any other attributes. f. `Custom_richcompare`: Implementing equality (`==`) and less than (`<`) comparisons based on `value`. 4. **Members and GetSet**: - Should provide read-write access to `value`. - Should provide read access to `name`. 5. **Iterator Protocol**: - Make the object itself iterable, with the iteration yielding the characters of the `name` attribute one by one. # Instructions 1. **Write the C code** implementing the provided object and its methods, based on the provided skeleton. 2. Compile the C code as a Python extension module. 3. Write a Python script to: a. Import your newly created module. b. Create an instance of `CustomObject`. c. Test all implemented functionalities: - Initialization and attribute access. - String representation. - Comparisons. - Iteration. # Constraints - Your implementation should handle errors gracefully, raising appropriate Python exceptions where necessary. - Ensure memory management is handled correctly to avoid leaks or crashes. # Performance The solution should be efficient under typical usage but does not need to be optimized for extreme cases. # Sample Output ```python import mymodule # Creating the CustomObject instance obj = mymodule.CustomObject(value=10, name=\\"example\\") # Accessing attributes print(obj.value) # Output: 10 print(obj.name) # Output: example # Setting attributes obj.value = 20 print(obj.value) # Output: 20 # String representation print(repr(obj)) # Output: CustomObject(name=example, value=20) # Comparisons obj1 = mymodule.CustomObject(value=5, name=\\"test\\") print(obj > obj1) # Output: True print(obj == obj1) # Output: False # Iteration for char in obj: print(char) # Output: e x a m p l e (each character on a new line) ```","solution":"# Define the required functionality in Python for unit testing purposes first, then map to C in the actual implementation class CustomObject: def __init__(self, value, name): self.value = value self.name = name def __del__(self): # Deallocate resources if needed pass def __repr__(self): return f\\"CustomObject(name={self.name}, value={self.value})\\" def __eq__(self, other): if isinstance(other, CustomObject): return self.value == other.value return False def __lt__(self, other): if isinstance(other, CustomObject): return self.value < other.value return False def __iter__(self): return iter(self.name) @property def name(self): return self._name @name.setter def name(self, value): self._name = value @property def value(self): return self._value @value.setter def value(self, val): if not isinstance(val, int): raise TypeError(\\"Value must be an integer\\") self._value = val"},{"question":"# Custom PyTorch Autograd Function Assessment In this assessment, you will define a custom PyTorch autograd function that performs the following: 1. Use non-PyTorch operations in the forward method. 2. Implement a custom backward method for gradient computation. 3. Set up the function to be compatible with `torch.func.grad`. # Task You are required to implement the `CustomSigmoid` class inheriting from `torch.autograd.Function` to compute the sigmoid of a tensor. You will also handle the custom gradient computation in the backward method. CustomSigmoid Function Requirements: 1. **Forward Pass**: - Should compute the sigmoid using NumPy. - Should accept a tensor `x` and return a tensor containing the sigmoid values. 2. **Setup Context**: - Save any necessary tensors or values to the context object (`ctx`) for use in backward pass. 3. **Backward Pass**: - Should compute the gradient of the sigmoid, using the sigmoid values stored in the context. You also need to: 1. Implement and test the `custom_sigmoid` function which uses `CustomSigmoid`. 2. Compute and validate the gradients using `torch.func.grad`. # Input Format: - A tensor `x` of any shape. # Output Format: - The tensor resulting from the forward pass of `custom_sigmoid`. - Gradients computed using `torch.func.grad` to validate your custom backward method. # Implementation: ```python import torch import numpy as np class CustomSigmoid(torch.autograd.Function): @staticmethod def forward(x): # Convert to numpy to compute the sigmoid using numpy functions x_np = x.detach().cpu().numpy() sigmoid_np = 1 / (1 + np.exp(-x_np)) # Convert back to torch tensor sigmoid_tensor = torch.tensor(sigmoid_np, device=x.device, dtype=x.dtype) # Return the sigmoid result return sigmoid_tensor @staticmethod def setup_context(ctx, inputs, output): # Save the sigmoid output in the context for backward computation sigmoid_output, = outputs ctx.save_for_backward(sigmoid_output) @staticmethod def backward(ctx, grad_output): # Retrieve the stored output sigmoid_output, = ctx.saved_tensors # Compute the gradient of the sigmoid grad_input = grad_output * sigmoid_output * (1 - sigmoid_output) # Return the gradient w.r.t. input return grad_input def custom_sigmoid(x): return CustomSigmoid.apply(x) # Example usage and testing if __name__ == \\"__main__\\": x = torch.randn(3, 3, requires_grad=True) # Compute sigmoid y = custom_sigmoid(x) # Validate gradients using torch.func.grad grad_x = torch.func.grad(lambda x: custom_sigmoid(x).sum())(x) # Verify correctness print(\\"Sigmoid Values:n\\", y) print(\\"Gradients:n\\", grad_x) ``` # Constraints: - Only use NumPy for non-PyTorch operations in the forward method. - Ensure the function is compatible with `torch.func.grad`. # Notes: - This assessment tests your ability to create custom autograd functions and handle custom gradient computations, ensuring interoperability with PyTorch\'s gradient mechanisms.","solution":"import torch import numpy as np class CustomSigmoid(torch.autograd.Function): @staticmethod def forward(ctx, x): # Convert to numpy to compute the sigmoid using numpy functions x_np = x.detach().cpu().numpy() sigmoid_np = 1 / (1 + np.exp(-x_np)) # Convert back to torch tensor sigmoid_tensor = torch.tensor(sigmoid_np, device=x.device, dtype=x.dtype) # Save the sigmoid result in the context for backward computation ctx.save_for_backward(sigmoid_tensor) # Return the sigmoid result return sigmoid_tensor @staticmethod def backward(ctx, grad_output): # Retrieve the stored output sigmoid_output, = ctx.saved_tensors # Compute the gradient of the sigmoid grad_input = grad_output * sigmoid_output * (1 - sigmoid_output) # Return the gradient w.r.t. input return grad_input def custom_sigmoid(x): return CustomSigmoid.apply(x)"},{"question":"# Secure Document Verification Using `hashlib` Implement a Python program that verifies the integrity of documents by generating and comparing their secure hash values using the `hashlib` module. Your task is to implement two core functions: 1. `generate_file_hash(filepath: str, algorithm: str) -> str`: This function takes the file path and the hashing algorithm to use (e.g., \'sha256\', \'sha512\', \'blake2b\', etc.), reads the file in binary mode, generates its hash, and returns the hexadecimal representation of the hash value. 2. `verify_file_integrity(filepath: str, expected_hash: str, algorithm: str) -> bool`: This function takes the file path, the expected hash value, and the hashing algorithm to use. It computes the file\'s hash and compares it to the expected hash value, returning `True` if they match, and `False` otherwise. Example Usage: ```python # Generate the hash for a file file_hash = generate_file_hash(\'path/to/your/document.txt\', \'sha256\') # Verify the file\'s integrity is_valid = verify_file_integrity(\'path/to/your/document.txt\', file_hash, \'sha256\') print(is_valid) # Output should be True if the document is unchanged ``` # Constraints: - The function should handle large files efficiently by reading them in chunks instead of loading the entire file into memory. - Ensure that your solution checks the availability of the algorithm provided and handles nonexistent or unsupported algorithms gracefully. - Use appropriate exception handling to deal with file-related errors (e.g., file not found). # Notes: - You should support all algorithms that are guaranteed to be present in the `hashlib` module. - The file reading should be done in binary mode (`rb`). # Sample File Handling: You may assume sample files are encoded in UTF-8 and exist at the given paths: - `document1.txt` - `document2.pdf` - `image.png` Implement the functions as specified.","solution":"import hashlib def generate_file_hash(filepath: str, algorithm: str) -> str: Generates a secure hash of the file using the specified algorithm. Parameters: filepath (str): Path to the file to be hashed. algorithm (str): Hashing algorithm to use. Returns: str: Hexadecimal representation of the file\'s hash. try: hasher = hashlib.new(algorithm) except ValueError: return None # Unsupported algorithm try: with open(filepath, \'rb\') as f: while chunk := f.read(8192): hasher.update(chunk) return hasher.hexdigest() except FileNotFoundError: return None except OSError: return None def verify_file_integrity(filepath: str, expected_hash: str, algorithm: str) -> bool: Verifies the file\'s integrity by comparing its hash with the expected hash. Parameters: filepath (str): Path to the file to be verified. expected_hash (str): Expected hash value for the file. algorithm (str): Hashing algorithm to use. Returns: bool: True if the file\'s hash matches the expected hash, False otherwise. file_hash = generate_file_hash(filepath, algorithm) if file_hash is None: return False return file_hash == expected_hash"},{"question":"**Coding Assessment Question:** You are provided with a Python script named `example.py` that contains a function `main()` which you need to trace for execution and gather coverage data. Your task is to implement a script in Python that uses the `trace` module to run `example.py`, trace its execution, gather coverage data, and generate a detailed coverage report. # Requirements: 1. Implement a function `trace_and_report(filename: str, coverdir: str) -> None` that takes: - `filename`: the name of the Python file to be traced (e.g., `\\"example.py\\"`). - `coverdir`: the directory where the coverage report should be generated. 2. The function should: - Create a `Trace` object configured to count the number of times each statement is executed. - Run the given Python file using the `Trace` object. - Generate a coverage report in the specified directory, marking lines that were not executed. 3. Ensure that the traced script can correctly capture function calls and execution statements, excluding any standard library modules. # Constraints: - You should handle exceptions that may occur during tracing and provide informative error messages. - Assume `example.py` is correctly formatted and the `main()` function exists and is callable. # Example Usage: ```python # Assume example.py contains a main function as follows: # def main(): # print(\\"Hello, World!\\") # # Example usage of your function: trace_and_report(\'example.py\', \'coverage_reports\') # After running the code, a coverage report should be generated in the \'coverage_reports\' directory # with details on line execution counts and missing lines. ``` # Guidelines: - Read through the `trace` module documentation provided above to understand the API and available options. - Write clean and readable code with appropriate comments. - Your solution should be efficient and handle edge cases gracefully. **Testing:** - You may provide additional test cases to demonstrate the effectiveness of your function.","solution":"import os import sys import traceback from trace import Trace def trace_and_report(filename: str, coverdir: str) -> None: Runs the given Python file, tracing its execution and generating a coverage report. Parameters: filename (str): The name of the Python file to be traced (e.g., \\"example.py\\"). coverdir (str): The directory where the coverage report should be generated. try: tracer = Trace(count=True, trace=False, outfile=os.path.join(coverdir, \\"coverage_report.txt\\")) # Ensure the directory exists if not os.path.exists(coverdir): os.makedirs(coverdir) # Run the given Python file tracer.run(f\\"exec(open(\'{filename}\').read())\\") # Generate the coverage report results = tracer.results() results.write_results(show_missing=True, summary=True) except Exception as e: print(f\\"An error occurred while tracing the file: {e}\\") traceback.print_exc() # Example Usage: # trace_and_report(\'example.py\', \'coverage_reports\')"},{"question":"# Seaborn KDEPlot Assessment You are provided with a dataset of car characteristics and specifications. The dataset has the following structure: ```plaintext | | mpg | cylinders | hp | weight | acceleration | origin | year | |---|--------|-----------|-----|--------|--------------|--------|------| | 0 | 18.0 | 8 | 130 | 3504 | 12.0 | USA | 70 | | 1 | 15.0 | 8 | 165 | 3693 | 11.5 | USA | 70 | | . | ... | ... | ... | ... | ... | ... | ... | | n | 31.0 | 4 | 113 | 2234 | 14.0 | Europe | 82 | ``` Your task is to write a Python function using Seaborn to visualize the relationship between the `weight` and `mpg` of cars. Additionally, add hue distinctions to differentiate cars based on their `origin`. Customize the plot to have filled contours with varying translucency and a distinct colormap. Function Signature ```python def plot_car_characteristics(data: pd.DataFrame) -> None: pass ``` Input - `data` (pd.DataFrame): A pandas DataFrame containing the car dataset with columns: `\'mpg\'`, `\'cylinders\'`, `\'hp\'`, `\'weight\'`, `\'acceleration\'`, `\'origin\'`, and `\'year\'`. Output - The function should generate and display a KDE plot with the following specifications: - Show a bivariate distribution of `weight` and `mpg`. - Use `origin` for hue mapping to differentiate cars from different origins. - Fill the contours with a specified colormap (use `mako` colormap). - Adjust the transparency of the fill to 50%. Constraints - The DataFrame will always have the specified columns. - Ensure the plot is well-labeled. Example usage: ```python import pandas as pd # Sample DataFrame data = pd.DataFrame({ \'mpg\': [18.0, 15.0, 36.0, 27.0], \'cylinders\': [8, 8, 4, 4], \'hp\': [130, 165, 69, 75], \'weight\': [3504, 3693, 1835, 2110], \'acceleration\': [12.0, 11.5, 19.5, 14.5], \'origin\': [\'USA\', \'USA\', \'Japan\', \'Europe\'], \'year\': [70, 70, 82, 76] }) plot_car_characteristics(data) ``` Performance Requirements Since this function involves data visualization, performance considerations are minimal. However, ensure the code executes without errors and produces a clear and interpretable plot.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_car_characteristics(data: pd.DataFrame) -> None: Visualizes the relationship between weight and mpg of cars with hue distinction based on origin. The plot will show filled contours using the \'mako\' colormap with 50% transparency. Args: data (pd.DataFrame): input dataframe containing the car dataset. Returns: None plt.figure(figsize=(10, 6)) sns.kdeplot( x=\'weight\', y=\'mpg\', hue=\'origin\', data=data, fill=True, alpha=0.5, palette=\'mako\' ) plt.title(\'KDE Plot of Weight vs MPG with Origin Hue\') plt.xlabel(\'Weight\') plt.ylabel(\'MPG\') plt.show()"},{"question":"Objective: Implement and demonstrate callable objects in Python using both `tp_call` and vectorcall protocols. This will assess your understanding of callable object behaviors, custom function implementations, and performance considerations. Task: 1. **Create a Custom Callable Object:** - Implement a custom Python class named `MyCallable` that supports both `tp_call` and vectorcall protocols for making function calls. The callable should take two arguments: - `x`: a positional integer argument. - `y`: an optional keyword integer argument with a default value of `10`. 2. **Implement the Call Protocols:** - Define the methods needed to support the `tp_call` structure. - Define the methods needed to support the vectorcall structure, ensuring it falls back on `tp_call` if vectorcall is not supported. 3. **Execution:** - Write a script demonstrating the performance of the `MyCallable` object when called multiple times (e.g., in a loop) using both protocols. - Compare the performance of calling the object using the traditional `tp_call` versus the vectorcall protocol with sample timing outputs. Constraints: - Ensure the `tp_call` and vectorcall implementations behave identically and produce the same results. - Optimize the vectorcall method for efficiency compared to `tp_call`. Expected Input and Output: - **Input:** Two integers `x` and `y`. - **Output:** The sum of `x` and `y`. Example: ```python class MyCallable: # Implement tp_call protocol def __tp_call__(self, x, y=10): return x + y # Implement vectorcall protocol def __vectorcall__(self, args, nargsf, kwnames): x = args[0] y = args[1] if len(args) > 1 else 10 return x + y # Demonstrate usage: callable_instance = MyCallable() print(callable_instance(5)) # Output: 15 (using tp_call) print(callable_instance(5, 3)) # Output: 8 (using vectorcall) # Performance demonstration code import time # Timing code for both protocols start = time.time() for _ in range(1000000): callable_instance(5, 3) end = time.time() print(f\\"Time using vectorcall: {end - start}\\") start = time.time() for _ in range(1000000): callable_instance.__tp_call__(5, 3) end = time.time() print(f\\"Time using tp_call: {end - start}\\") ``` Create the implementation along with a script that tests and compares both protocols.","solution":"import time class MyCallable: def __call__(self, x, y=10): Support both tp_call and vectorcall protocols return self.vectorcall((x, y)) def __tp_call__(self, x, y=10): Implements the tp_call protocol return x + y def vectorcall(self, args, nargsf=0, kwnames=None): Implements the vectorcall protocol - args: tuple of arguments - nargsf: number of arguments (ignored, for compatibility reasons) - kwnames: tuple of keyword argument names (ignored, for compatibility reasons) x = args[0] y = args[1] if len(args) > 1 else 10 return x + y # Demonstrate usage callable_instance = MyCallable() print(callable_instance(5)) # Output: 15 (using vectorcall via __call__) print(callable_instance(5, 3)) # Output: 8 (using vectorcall via __call__) # Performance demonstration code # Timing code for both protocols start = time.time() for _ in range(1000000): callable_instance(5, 3) # using vectorcall via __call__ end = time.time() print(f\\"Time using vectorcall: {end - start}\\") start = time.time() for _ in range(1000000): callable_instance.__tp_call__(5, 3) end = time.time() print(f\\"Time using tp_call: {end - start}\\")"},{"question":"Objective: Create an object-oriented Python class using PyTorch that can efficiently load and manipulate models on the \\"meta\\" device. Task: 1. Implement a class `MetaModelLoader` with the following methods: - `__init__(self, model_path: str)`: Constructor method that initializes the loader with the path to a pre-saved model. - `load_as_meta(self) -> torch.nn.Module`: Loads the model on the \\"meta\\" device. - `transform_model(self, transform_function: callable) -> torch.nn.Module`: Applies a transformation function to the model while it is on the \\"meta\\" device and returns the transformed model. - `initialize_on_device(self, device: str) -> torch.nn.Module`: Transfers the transformed model to the specified device, leaving all parameters uninitialized, and returns the uninitialized model. Expected Input and Output: - `__init__(self, model_path: str)` - Input: Path to the pre-saved model file as a string. - Output: Initializes the `MetaModelLoader` object. - `load_as_meta(self) -> torch.nn.Module` - Input: None - Output: Returns the loaded model on the \\"meta\\" device. - `transform_model(self, transform_function: callable) -> torch.nn.Module` - Input: Callable transformation function to be applied on the model. - Output: Returns the transformed model on the \\"meta\\" device. - `initialize_on_device(self, device: str) -> torch.nn.Module` - Input: Device name (e.g., \'cpu\' or \'cuda\'). - Output: Returns the transformed model initialized on the specified device with uninitialized parameters. Constraints: 1. The \\"meta\\" device should only be used for loading and transforming the model. 2. Use the `torch.device` context manager where necessary. 3. The transformation function should be a function that takes a `torch.nn.Module` and returns a `torch.nn.Module`. Example Usage: ```python import torch import torch.nn as nn class MetaModelLoader: def __init__(self, model_path: str): self.model_path = model_path def load_as_meta(self) -> torch.nn.Module: return torch.load(self.model_path, map_location=\'meta\') def transform_model(self, transform_function: callable) -> torch.nn.Module: with torch.device(\'meta\'): model = self.load_as_meta() transformed_model = transform_function(model) return transformed_model def initialize_on_device(self, device: str) -> torch.nn.Module: transformed_model = self.transform_model(lambda x: x) return transformed_model.to_empty(device=device) # Example transformation function def add_dropout(model: torch.nn.Module) -> torch.nn.Module: new_model = nn.Sequential( nn.Dropout(0.5), model ) return new_model # Using the MetaModelLoader model_loader = MetaModelLoader(\'pretrained_model.pt\') model_on_meta = model_loader.load_as_meta() transformed_model_on_meta = model_loader.transform_model(add_dropout) initialized_model = model_loader.initialize_on_device(\'cpu\') print(initialized_model) ``` The provided example demonstrates loading a model on the \\"meta\\" device, applying a transformation, and initializing it on a specified device. Performance Requirement: - The implementation should efficiently handle the model loading and transformations on the \\"meta\\" device without unnecessary data transfers. Note: - Students are expected to handle exceptions gracefully, ensuring the implementation is robust.","solution":"import torch import torch.nn as nn class MetaModelLoader: def __init__(self, model_path: str): self.model_path = model_path self.model = None def load_as_meta(self) -> torch.nn.Module: self.model = torch.load(self.model_path, map_location=\'meta\') return self.model def transform_model(self, transform_function: callable) -> torch.nn.Module: if self.model is None: raise RuntimeError(\\"Load the model first using `load_as_meta`\\") with torch.device(\'meta\'): transformed_model = transform_function(self.model) return transformed_model def initialize_on_device(self, device: str) -> torch.nn.Module: if self.model is None: raise RuntimeError(\\"Load and transform the model first using `load_as_meta` and `transform_model`\\") transformed_model = self.model.to_empty(device=device) return transformed_model"},{"question":"**Parallel Data Processing with Threading and Multiprocessing** In this task, you are required to process a large dataset using both multi-threading and multi-processing techniques. The dataset consists of numerical data points (integers) that need to be processed concurrently to improve performance. # Task Description 1. **Data Loading:** - Load data from a text file named `data.txt`. Each line in the file contains one integer. 2. **Data Processing:** - Implement a function `process_data` that performs a simple transformation on each integer. For illustration, you can square the integer (i.e., `x -> x*x`). 3. **Concurrent Execution:** - Use the `threading` module to partition the data into `n` chunks (where `n` is a parameter) and process each chunk in a separate thread. - Use the `multiprocessing` module to further partition the data and process using `m` processes (where `m` is a parameter). Ensure each process uses threading internally. 4. **Result Aggregation:** - Combine the results from all threads and processes to produce the final output. 5. **Performance Analysis:** - Measure and compare the time taken to process the data using threading only, multiprocessing only, and the combination of both. # Requirements - Implement the data processing function `process_data`, which should take an integer as input and return its square. - Implement the function `threading_process` to handle data partitioning and processing using the `threading` module. - Implement the function `multiprocessing_threading_process` to handle data partitioning and processing using both `multiprocessing` and `threading`. - Aggregate and return the results correctly. # Constraints - The input data file `data.txt` contains up to `1000000` integers. - You need to handle errors appropriately, e.g., file not found, empty file, etc. # Input - A text file `data.txt` containing integers. # Output - The transformed data set (all integers squared) in a list. - Time taken for each processing approach. # Example Given the file `data.txt`: ``` 1 2 3 4 5 ``` Expected output: ``` Threading only results: [1, 4, 9, 16, 25] in X seconds. Multiprocessing only results: [1, 4, 9, 16, 25] in Y seconds. Combined multiprocessing and threading results: [1, 4, 9, 16, 25] in Z seconds. ``` # Note For measuring time, you may use the `time` module\'s `time()` function. # Functions ```python import os from threading import Thread from multiprocessing import Process, Queue, current_process import time def process_data(data_point: int) -> int: Function to process a single data point. Here, we simply return the square of the data point. return data_point * data_point def threading_process(data: list, n: int) -> list: Function to process data using threading. `data` is the list of integers to be processed. `n` is the number of threads to use. pass # Implement the logic def multiprocessing_threading_process(data: list, n: int, m: int) -> list: Function to process data using multiprocessing and threading. `data` is the list of integers to be processed. `n` is the number of threads per process. `m` is the number of processes to use. pass # Implement the logic def main(): if not os.path.isfile(\'data.txt\'): raise FileNotFoundError(\'The input data.txt file does not exist\') # Reading the data from the file with open(\'data.txt\', \'r\') as f: data = [int(line.strip()) for line in f] n, m = 4, 2 # Example values for threads and processes start_time = time.time() threading_results = threading_process(data, n) threading_duration = time.time() - start_time start_time = time.time() multiprocessing_results = multiprocessing_threading_process(data, n, m) multiprocessing_duration = time.time() - start_time print(f\'Threading only results: {threading_results[:10]}... (showing first 10) in {threading_duration:.4f} seconds.\') print(f\'Combined multiprocessing and threading results: {multiprocessing_results[:10]}... (showing first 10) in {multiprocessing_duration:.4f} seconds.\') if __name__ == \\"__main__\\": main() ```","solution":"import os from threading import Thread from multiprocessing import Process, Queue import time def process_data(data_point: int) -> int: Function to process a single data point. Here, we simply return the square of the data point. return data_point * data_point def worker_threading(data_chunk: list, result_queue: Queue): Worker function to process data chunk in a separate thread. result = [process_data(data_point) for data_point in data_chunk] result_queue.put(result) def threading_process(data: list, n: int) -> list: Function to process data using threading. `data` is the list of integers to be processed. `n` is the number of threads to use. chunk_size = len(data) // n + (len(data) % n > 0) threads = [] result_queue = Queue() for i in range(n): chunk = data[i * chunk_size: (i + 1) * chunk_size] thread = Thread(target=worker_threading, args=(chunk, result_queue)) threads.append(thread) thread.start() results = [] for thread in threads: thread.join() results.extend(result_queue.get()) return results def worker_multiprocessing(data_chunk: list, n: int, result_queue: Queue): Worker function to process data chunk in a separate process. Each process can have multiple threads internally. chunk_size = len(data_chunk) // n + (len(data_chunk) % n > 0) threads = [] local_result_queue = Queue() for i in range(n): chunk = data_chunk[i * chunk_size: (i + 1) * chunk_size] thread = Thread(target=worker_threading, args=(chunk, local_result_queue)) threads.append(thread) thread.start() results = [] for thread in threads: thread.join() results.extend(local_result_queue.get()) result_queue.put(results) def multiprocessing_threading_process(data: list, n: int, m: int) -> list: Function to process data using multiprocessing and threading. `data` is the list of integers to be processed. `n` is the number of threads per process. `m` is the number of processes to use. chunk_size = len(data) // m + (len(data) % m > 0) processes = [] result_queue = Queue() for i in range(m): chunk = data[i * chunk_size: (i + 1) * chunk_size] process = Process(target=worker_multiprocessing, args=(chunk, n, result_queue)) processes.append(process) process.start() results = [] for process in processes: process.join() results.extend(result_queue.get()) return results def main(): if not os.path.isfile(\'data.txt\'): raise FileNotFoundError(\'The input data.txt file does not exist\') # Reading the data from the file with open(\'data.txt\', \'r\') as f: data = [int(line.strip()) for line in f] # For the purpose of unit testing, you may replace \'data\' with a smaller set of data directly here: # data = [1, 2, 3, 4, 5] n, m = 4, 2 # Example values for threads and processes start_time = time.time() threading_results = threading_process(data, n) threading_duration = time.time() - start_time start_time = time.time() multiprocessing_results = multiprocessing_threading_process(data, n, m) multiprocessing_duration = time.time() - start_time print(f\'Threading only results: {threading_results[:10]}... (showing first 10) in {threading_duration:.4f} seconds.\') print(f\'Combined multiprocessing and threading results: {multiprocessing_results[:10]}... (showing first 10) in {multiprocessing_duration:.4f} seconds.\') if __name__ == \\"__main__\\": main()"},{"question":"**Question:** You are to implement a class `AdvancedCodecs` that performs various encoding and decoding operations using the `codecs` module in Python. This class will require you to handle different encoding formats, manage errors, and provide utility functions to work with strings and files. # Class Definition Implement the `AdvancedCodecs` class with the following methods: 1. **__init__(self, encoding, errors=\'strict\')** - Initializes the `AdvancedCodecs` object with the given `encoding` and `error` handling mechanism. 2. **encode_string(self, input_string)** - Encodes the given `input_string` using the specified encoding and returns the encoded bytes. 3. **decode_bytes(self, input_bytes)** - Decodes the given `input_bytes` using the specified encoding and returns the decoded string. 4. **file_to_encoded(self, input_filename, output_filename)** - Reads the `input_filename` file, encodes its content using the specified encoding, and writes the encoded bytes to `output_filename`. 5. **encoded_to_file(self, input_filename, output_filename)** - Reads the `input_filename` containing encoded bytes, decodes the content using the specified encoding, and writes the decoded string to `output_filename`. 6. **iterencode(self, input_string_iterator)** - Takes an iterator of strings and yields encoded bytes using iterative encoding. 7. **iterdecode(self, input_bytes_iterator)** - Takes an iterator of bytes and yields decoded strings using iterative decoding. # Constraints - The input to `__init__` will always be valid encoding names. - The `input_string` for `encode_string` will always be of type `str`. - The `input_bytes` for `decode_bytes` will always be of type `bytes`. - The files specified will always exist and be accessible for reading/writing. - Handle errors as specified by the `errors` argument during initialization. # Example Usage ```python # Example initialization and usage codec = AdvancedCodecs(encoding=\'utf-8\', errors=\'ignore\') # Encoding and decoding strings encoded = codec.encode_string(\\"Hello, World!\\") print(encoded) # Output: b\'Hello, World!\' decoded = codec.decode_bytes(encoded) print(decoded) # Output: \\"Hello, World!\\" # Encoding content from one file and writing to another codec.file_to_encoded(\'input.txt\', \'encoded_output.bin\') # Decoding content from one file and writing to another codec.encoded_to_file(\'encoded_output.bin\', \'decoded_output.txt\') ``` You must implement the `AdvancedCodecs` class with the above functionalities. Ensure that you correctly handle errors according to the `errors` parameter and provide efficient iterative encoding/decoding.","solution":"import codecs class AdvancedCodecs: def __init__(self, encoding, errors=\'strict\'): self.encoding = encoding self.errors = errors def encode_string(self, input_string): return input_string.encode(self.encoding, self.errors) def decode_bytes(self, input_bytes): return input_bytes.decode(self.encoding, self.errors) def file_to_encoded(self, input_filename, output_filename): with open(input_filename, \'r\', encoding=self.encoding, errors=self.errors) as input_file: content = input_file.read() with open(output_filename, \'wb\') as output_file: encoded_content = content.encode(self.encoding, self.errors) output_file.write(encoded_content) def encoded_to_file(self, input_filename, output_filename): with open(input_filename, \'rb\') as input_file: encoded_content = input_file.read() with open(output_filename, \'w\', encoding=self.encoding, errors=self.errors) as output_file: decoded_content = encoded_content.decode(self.encoding, self.errors) output_file.write(decoded_content) def iterencode(self, input_string_iterator): for input_string in input_string_iterator: yield input_string.encode(self.encoding, self.errors) def iterdecode(self, input_bytes_iterator): for input_bytes in input_bytes_iterator: yield input_bytes.decode(self.encoding, self.errors)"},{"question":"You are required to write a Python function that uses the \\"trace\\" module to trace the execution of a provided Python function, generate a coverage report, and save the results to a specified directory. Function Signature ```python def trace_execution(target_func: callable, report_dir: str, ignore_modules: list = [], ignore_dirs: list = []) -> None: Traces the execution of the provided `target_func`, generates a coverage report, and writes the results to the specified `report_dir`. Parameters: target_func (callable): The target function to be traced. report_dir (str): The directory where the coverage report will be saved. ignore_modules (list): A list of module names to ignore during tracing. Default is an empty list. ignore_dirs (list): A list of directory paths to ignore during tracing. Default is an empty list. Returns: None pass ``` Requirements 1. The function should create a `trace.Trace` object configured with the specified `ignore_modules` and `ignore_dirs`. 2. The tracing should count the number of executions of each line in the target function. 3. The tracing should generate a coverage report that shows lines which were not executed. 4. The generated coverage result files should be placed in the specified `report_dir`. 5. If `ignore_modules` or `ignore_dirs` are not provided, the default should be an empty list. Example Usage ```python def example_func(): for i in range(5): print(i) if True: print(\\"Hello, World!\\") else: print(\\"Goodbye, World!\\") trace_execution(example_func, \\"/path/to/report_dir\\") ``` The above example should trace the execution of `example_func`, generate a coverage report, and save the results to `/path/to/report_dir`. Constraints - You may assume the target function does not take any arguments. - The report directory must be a valid existing directory. Additional Notes - You are not required to handle exceptions or errors for directory paths. - Use appropriate methods and classes from the \\"trace\\" module as described in the documentation.","solution":"import os import trace def trace_execution(target_func: callable, report_dir: str, ignore_modules: list = [], ignore_dirs: list = []) -> None: Traces the execution of the provided `target_func`, generates a coverage report, and writes the results to the specified `report_dir`. Parameters: target_func (callable): The target function to be traced. report_dir (str): The directory where the coverage report will be saved. ignore_modules (list): A list of module names to ignore during tracing. Default is an empty list. ignore_dirs (list): A list of directory paths to ignore during tracing. Default is an empty list. Returns: None tracer = trace.Trace( trace=False, count=True, ignoremods=ignore_modules, ignoredirs=ignore_dirs ) tracer.runfunc(target_func) results = tracer.results() results.write_results(summary=True, coverdir=report_dir)"},{"question":"# Python Simple Statements Function Implementation Objective: Implement a function that processes a list of mathematical operations. Each operation may require the use of different types of statements as described in the provided documentation. Your implementation should demonstrate a thorough understanding of Python’s simple statements including assignment, augmented assignment, assert, pass, del, return, yield, raise, and import. Tasks: 1. Write a function `process_operations(operations: list) -> list` that takes a list of mathematical operations represented as strings and returns a list of results for each operation. 2. Use appropriate exception handling to manage invalid operations or other errors. 3. Use assert statements to check if inputs to operations are valid. 4. Include usage of `pass` and `del` in scenarios where they make sense. 5. The function should use at least one augmented assignment. 6. The function should demonstrate the use of `yield` by implementing a generator. 7. Use nonlocal or global where necessary if you need multiple nested functions. Input: - `operations` (list of str): A list of strings, each representing a mathematical operation. Example: `[\\"5 + 3\\", \\"10 - 2\\", \\"invalid_operation\\", \\"15 // 0\\"]` Output: - A list containing the result of each valid operation or a string message indicating an error for invalid operations. Constraints: 1. The list of operations will contain at least one operation and at most 100 operations. 2. Each operation string will represent a valid Python expression involving integers and the operators +, -, *, /, //, %, **. Performance requirements: The function should handle all operations in O(n) time complexity, where n is the number of operations. Examples: ```python def process_operations(operations): results = [] for op in operations: try: assert isinstance(op, str), \\"Operation must be a string.\\" if \\"0\\" in op.split(\\"//\\"): raise ValueError(\\"Division by zero using \'//\' is not allowed.\\") result = eval(op) results.append(result) except AssertionError as ae: results.append(str(ae)) except Exception as e: results.append(f\\"Error: {str(e)}\\") finally: pass # Example use of pass statement return results operations = [\\"5 + 3\\", \\"10 - 3\\", \\"2 * 3\\", \\"9 / 3\\", \\"15 // 0\\", \\"a + b\\"] print(process_operations(operations)) # Output: [8, 7, 6, 3.0, \'Error: Division by zero using \'//\' is not allowed.\', \'Error: name \'a\' is not defined\'] ``` Notes: 1. Use the `eval()` function to evaluate the operations safely. 2. Be cautious about security implications when using `eval()`; ensure the context in which it\'s used is controlled. 3. Document each part of the function to explain its purpose and usage.","solution":"def process_operations(operations): Processes a list of mathematical operations and returns the results. Parameters: operations (list of str): List of operations to process. Returns: list: Results of the operations or error messages for invalid operations. results = [] def process_generator(): nonlocal operations for op in operations: try: assert isinstance(op, str), \\"Operation must be a string.\\" if \\"//\\" in op and \\"0\\" in op.split(\\"//\\")[1].strip(): raise ValueError(\\"Division by zero using \'//\' is not allowed.\\") result = eval(op) yield result except AssertionError as ae: yield str(ae) except Exception as e: yield f\\"Error: {str(e)}\\" finally: pass # Example use of pass statement generator = process_generator() for result in generator: results.append(result) return results # Example usage operations = [\\"5 + 3\\", \\"10 - 2\\", \\"invalid_operation\\", \\"15 // 0\\"] print(process_operations(operations))"},{"question":"Advanced Seaborn Plot Customizations You are provided with a dataset of penguins from different species. Your task is to create and customize a complex plot using the `seaborn.objects` interface. Follow the instructions below carefully to complete the task. Instructions 1. **Load the Data:** Load the penguins dataset using Seaborn\'s `load_dataset` function. 2. **Create Initial Plot:** Create a plot of `bill_length_mm` vs `bill_depth_mm` using the new `seaborn.objects` interface. Use different colors to distinguish between different penguin species. 3. **Set Custom Labels and Title:** - Customize the x-axis label to say \\"Bill Length (mm)\\". - Customize the y-axis label to say \\"Bill Depth (mm)\\". - Set the plot title to \\"Penguin Bill Measurements\\". 4. **Facet the Plot:** - Create a facet grid splitting the data by the penguins\' sex. - For each facet, modify the title to indicate the gender in all uppercase letters. 5. **Add a Detailed Legend:** - Add a legend that explains the color coding by species and title it \\"Species\\". 6. **Save and Display the Plot:** Save the final plot as a PNG file named \'penguin_bill_plot.png\' and display it. Constraints - Your solution must use the `seaborn.objects` interface. - Ensure all labels are correctly set according to the instructions. - The final plot should visually distinguish between the different species and sexes based on the given specifications. Expected Input and Output - **Input:** None (The dataset is loaded within the code) - **Output:** A PNG file named \'penguin_bill_plot.png\' containing the customized plot. Example Code Structure ```python import seaborn.objects as so from seaborn import load_dataset # Load the data penguins = load_dataset(\\"penguins\\") # Create initial plot plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") ) # Set custom labels and title plot = plot.label(x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", title=\\"Penguin Bill Measurements\\") # Facet the plot by sex plot = plot.facet(\\"sex\\").label(title=str.upper) # Add a detailed legend plot = plot.label(legend=\\"Species\\") # Save and display the plot plot.save(\'penguin_bill_plot.png\') plot.show() ``` **Note**: The example code above provides a functional outline but may require adjustments to achieve the exact required customizations. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the data penguins = load_dataset(\\"penguins\\") # Create initial plot plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") ) # Set custom labels and title plot = plot.label( x=\\"Bill Length (mm)\\", y=\\"Bill Depth (mm)\\", title=\\"Penguin Bill Measurements\\" ) # Facet the plot by sex and set custom titles plot = plot.facet(\\"sex\\").label(title=str.upper) # Add a detailed legend plot = plot.label(legend=\\"Species\\") # Save the plot plot.save(\\"penguin_bill_plot.png\\") # Display the plot plot.show()"},{"question":"ConfigParser Challenge # Context You are tasked with creating a configuration manager for a Python application using the `configparser` module. The configuration manager should be able to handle various functionalities such as reading from existing configuration files, writing new configurations, managing data types, and utilizing interpolation. # Task Implement a class `ConfigManager` which will perform the following operations: 1. **Initialize**: Create a `ConfigParser` instance with specific settings. 2. **Create Configuration**: Create a new configuration based on given sections and key-value pairs. 3. **Read Configuration**: Read an existing configuration file. 4. **Get Value**: Retrieve values with appropriate data types (int, float, bool). 5. **Set Value**: Update values in the configuration. 6. **Save Configuration**: Save the current configuration to a file. 7. **Interpolate Values**: Manage and resolve interpolated values within the configuration. # Specifications 1. **Initialize**: - The `ConfigManager` class should initialize a `ConfigParser` instance with the following settings: - Allow no value: `True` - Interpolation: `BasicInterpolation()` 2. **Create Configuration**: - Method: `create_configuration(sections: Dict[str, Dict[str, str]]) -> None` - Arguments: - `sections`: A dictionary where keys are section names and values are dictionaries of key-value pairs. - Create the configuration within the `ConfigParser` instance. 3. **Read Configuration**: - Method: `read_configuration(filename: str) -> None` - Arguments: - `filename`: The path to the configuration file to read. 4. **Get Value**: - Method: `get_value(section: str, option: str, datatype: str = \'str\', fallback: Any = None) -> Any` - Arguments: - `section`: The section under which the option is located. - `option`: The key whose value is to be retrieved. - `datatype`: The expected data type of the value, options are \'str\', \'int\', \'float\', \'bool\'. - `fallback`: A fallback value if the option is not found. - Returns the value with the appropriate data type. 5. **Set Value**: - Method: `set_value(section: str, option: str, value: str) -> None` - Arguments: - `section`: The section under which the option is located. - `option`: The key to be set. - `value`: The value to be set for the specified key. 6. **Save Configuration**: - Method: `save_configuration(filename: str) -> None` - Arguments: - `filename`: The path to the file where the configuration is to be saved. 7. **Interpolate Values**: - Method: `interpolate_value(section: str, template: str) -> str` - Arguments: - `section`: The section under which the interpolation is to occur. - `template`: The template string containing interpolation placeholders. - Returns the interpolated string. # Constraints - Proper exception handling should be in place for cases like missing files, missing sections or options, and invalid data types. - Ensure that setting and getting values handle the correct conversion of data types. # Example Usage ```python # Initialize ConfigManager config_mgr = ConfigManager() # Create a configuration config_mgr.create_configuration({ \'DEFAULT\': {\'AppVersion\': \'1.0.3\', \'AppName\': \'ConfigManager\'}, \'User\': {\'name\': \'John Doe\', \'email\': \'john.doe@example.com\', \'active\': \'True\'} }) # Save configuration to file config_mgr.save_configuration(\'app_config.ini\') # Read the configuration back config_mgr.read_configuration(\'app_config.ini\') # Get values app_name = config_mgr.get_value(\'DEFAULT\', \'AppName\') user_active = config_mgr.get_value(\'User\', \'active\', \'bool\') # Set a new value config_mgr.set_value(\'User\', \'name\', \'Jane Doe\') # Interpolate a value welcome_message = config_mgr.interpolate_value(\'User\', \'Welcome %(name)s to the %(AppName)s v%(AppVersion)s!\') print(app_name) # Output: ConfigManager print(user_active) # Output: True print(welcome_message) # Output: Welcome Jane Doe to the ConfigManager v1.0.3! ``` Implement the `ConfigManager` class based on the specifications provided.","solution":"import configparser from configparser import BasicInterpolation from typing import Dict, Any class ConfigManager: def __init__(self): self.config = configparser.ConfigParser(allow_no_value=True, interpolation=BasicInterpolation()) def create_configuration(self, sections: Dict[str, Dict[str, str]]) -> None: for section, options in sections.items(): self.config[section] = options def read_configuration(self, filename: str) -> None: self.config.read(filename) def get_value(self, section: str, option: str, datatype: str = \'str\', fallback: Any = None) -> Any: try: if datatype == \'int\': return self.config.getint(section, option, fallback=fallback) elif datatype == \'float\': return self.config.getfloat(section, option, fallback=fallback) elif datatype == \'bool\': return self.config.getboolean(section, option, fallback=fallback) else: return self.config.get(section, option, fallback=fallback) except (configparser.NoSectionError, configparser.NoOptionError): return fallback def set_value(self, section: str, option: str, value: str) -> None: if not self.config.has_section(section): self.config.add_section(section) self.config.set(section, option, value) def save_configuration(self, filename: str) -> None: with open(filename, \'w\') as configfile: self.config.write(configfile) def interpolate_value(self, section: str, template: str) -> str: try: return self.config[section].get(template, raw=False) except (configparser.NoSectionError, configparser.NoOptionError): return template"},{"question":"# PyTorch MPS Memory and Performance Management Your task is to implement a function in PyTorch using the `torch.mps` module which performs the following: 1. **Memory Management**: - Set a specific fraction of the total device memory accessible for the process. - Allocate a tensor on the MPS device and verify memory usage before and after allocation. 2. **Random Number Generation**: - Seed the random number generator. - Generate random numbers within a specific range and return the tensor. 3. **Profiling**: - Utilize the MPS Profiler to profile the random number generation process. 4. **Synchronization**: - Ensure synchronization at appropriate intervals to avoid any memory issues. Constraints: - The function should handle potential exceptions and clean up resources by freeing unused cached memory where necessary. # Function Signature: ```python import torch import torch.mps def manage_mps_resources(memory_fraction: float, seed_value: int, num_elements: int, min_value: float, max_value: float) -> torch.Tensor: Manages MPS device resources by setting memory fractions, seeding RNG, generating random numbers, and profiling the process. Parameters: - memory_fraction (float): The fraction of total memory to be accessible. - seed_value (int): The seed for random number generation. - num_elements (int): The number of elements in the tensor. - min_value (float): The minimum value of generated random numbers. - max_value (float): The maximum value of generated random numbers. Returns: - torch.Tensor: Tensor containing random numbers within specified range. pass ``` # Requirements: 1. Call `torch.mps.set_per_process_memory_fraction(memory_fraction)` to set the memory for the process. 2. Use `torch.mps.manual_seed(seed_value)` to seed the random number generator. 3. Allocate a tensor on the MPS device and verify memory usage using `torch.mps.current_allocated_memory()` before and after allocation. 4. Utilize `torch.mps.profiler` to profile the allocation and random number generation process. 5. Ensure to synchronize the device using `torch.mps.synchronize()` where necessary. 6. Handle any exceptions like insufficient memory, and free memory using `torch.mps.empty_cache()` in case of failures. Sample Usage: ```python tensor = manage_mps_resources(0.5, 42, 10000, -1.0, 1.0) print(tensor) ``` This function should output a tensor of shape (10000,) with random numbers between -1.0 and 1.0, properly managing and profiling MPS resources.","solution":"import torch import torch.mps def manage_mps_resources(memory_fraction: float, seed_value: int, num_elements: int, min_value: float, max_value: float) -> torch.Tensor: Manages MPS device resources by setting memory fractions, seeding RNG, generating random numbers, and profiling the process. Parameters: - memory_fraction (float): The fraction of total memory to be accessible. - seed_value (int): The seed for random number generation. - num_elements (int): The number of elements in the tensor. - min_value (float): The minimum value of generated random numbers. - max_value (float): The maximum value of generated random numbers. Returns: - torch.Tensor: Tensor containing random numbers within specified range. if not torch.mps.is_available(): raise RuntimeError(\\"MPS device is not available.\\") try: # Set the memory fraction torch.mps.set_per_process_memory_fraction(memory_fraction) # Seed the random number generator torch.mps.manual_seed(seed_value) # Profile the process with torch.mps.profile() as profiler: # Check memory usage before allocation initial_memory = torch.mps.current_allocated_memory() # Allocate tensor on MPS device tensor = (torch.rand(num_elements, device=\'mps\') * (max_value - min_value) + min_value) # Synchronize the device torch.mps.synchronize() # Check memory usage after allocation final_memory = torch.mps.current_allocated_memory() # Print memory usage print(f\\"Memory used before allocation: {initial_memory} bytes\\") print(f\\"Memory used after allocation: {final_memory} bytes\\") # Synchronize the profiler profiler.key_averages().table(sort_by=\\"cpu_time_total\\") return tensor except Exception as e: # Handle exceptions and clean up torch.mps.empty_cache() raise e"},{"question":"# Email Message Manipulation Task You are given a Python script that simulates receiving an email message in a raw format (as a string). Your task is to write a function that processes this email message using the `email.message.Message` class from the Python standard library. Specifically, you should: 1. Parse the raw email string into a `Message` object. 2. Check if the email message is multipart. If it is not, create a `multipart/mixed` container and add the original message as a part of this container. 3. Add a new header to the root message with the name \\"X-Processed-By\\" and the value \\"Python310\\". 4. Set a new payload to the first part of the multipart message with the string \\"Processed by Python\\". 5. Return the entire modified message as a string. # Input - A single string representing the raw email message. # Output - A single string representing the entire modified email message. # Constraints - The input will always be a valid email message string. - Assume that all necessary imports are available (e.g., `from email.message import Message`). # Example ```python def process_email(raw_email: str) -> str: # Your code here # Example usage: raw_email = Subject: Test email This is a test email message. print(process_email(raw_email)) ``` # Expected Output ```plaintext X-Processed-By: Python310 Content-Type: multipart/mixed; boundary=\\"===============1234567890123456789==\\" MIME-Version: 1.0 Subject: Test email --===============1234567890123456789== Content-Type: text/plain Processed by Python --===============1234567890123456789== Content-Type: text/plain This is a test email message. --===============1234567890123456789==-- ``` # Additional Information 1. Ensure all headers are case-insensitive and maintain the order. 2. The multipart boundary can be any unique string; in the example, it has been auto-generated for simplicity. 3. Use the appropriate methods from the `Message` class to manipulate headers and payloads. # Evaluation Criteria - Correct parsing and manipulation of the email message. - Proper handling of multipart messages. - Accurate addition and modification of headers and payloads. - Code readability and appropriate use of the `Message` class methods.","solution":"from email.parser import Parser from email.message import Message, EmailMessage from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def process_email(raw_email: str) -> str: # Parse the raw email into a Message object email_message = Parser().parsestr(raw_email) # Check if the message is multipart if not email_message.is_multipart(): # Create a new MIMEMultipart object and set the original message as part of it multipart_msg = MIMEMultipart() for header, value in email_message.items(): multipart_msg[header] = value # Original message as a part original_part = MIMEText(email_message.get_payload()) multipart_msg.attach(original_part) else: multipart_msg = email_message # Add X-Processed-By header to the root message multipart_msg[\\"X-Processed-By\\"] = \\"Python310\\" # Set a new payload to the first part of the multipart message with \\"Processed by Python\\" processed_part = MIMEText(\\"Processed by Python\\") if not multipart_msg.is_multipart(): multipart_msg.attach(processed_part) else: multipart_msg.get_payload()[0] = processed_part return multipart_msg.as_string()"},{"question":"**Problem Description:** You are working on a file management system where you need to filter files based on different patterns and conditions. You need to implement a function that takes a list of filenames and a list of patterns and returns a dictionary containing the filenames categorized by pattern matches. The comparison should be case-sensitive. # Function Signature ```python def categorize_files_by_patterns(filenames: list, patterns: list) -> dict: pass ``` # Params: 1. `filenames` (list): A list of filenames (strings). 2. `patterns` (list): A list of patterns (strings) to categorize filenames. # Returns: - A dictionary where: - the keys are the patterns, - the values are lists of filenames matching the respective pattern in a case-sensitive manner. # Constraints: - The filenames and patterns will be non-empty strings. - There will be at most 10,000 filenames and at most 100 patterns. - Each filename and pattern will have a maximum length of 100 characters. # Example: ```python filenames = [\\"data1.txt\\", \\"Data2.TXT\\", \\"image.png\\", \\"backup.tar\\", \\"log.LOG\\", \\"notes.txt\\"] patterns = [\\"*.txt\\", \\"*.TXT\\", \\"*.png\\", \\"log.*\\"] # Expected Output: # { # \'*.txt\': [\'data1.txt\', \'notes.txt\'], # \'*.TXT\': [\'Data2.TXT\'], # \'*.png\': [\'image.png\'], # \'log.*\': [\'log.LOG\'] # } assert categorize_files_by_patterns(filenames, patterns) == { \'*.txt\': [\'data1.txt\', \'notes.txt\'], \'*.TXT\': [\'Data2.TXT\'], \'*.png\': [\'image.png\'], \'log.*\': [\'log.LOG\'] } ``` # Notes: - You must not use the `fnmatch` function since it performs a case-insensitive match. - You can use the `fnmatch.fnmatchcase` function for case-sensitive matching. - Optimize the function to handle the constraints efficiently. # Implementation Hints: - Iterate over each pattern and filter the filenames using the `fnmatch.fnmatchcase` function. - Maintain an organized and clear dictionary structure to store the categorized filenames.","solution":"import fnmatch def categorize_files_by_patterns(filenames, patterns): Categorize filenames based on given patterns in a case-sensitive manner. Args: filenames (list): A list of filenames (strings). patterns (list): A list of patterns (strings) to categorize filenames. Returns: dict: A dictionary where keys are patterns and values are lists of filenames matching the respective pattern. categorized = {} for pattern in patterns: categorized[pattern] = [filename for filename in filenames if fnmatch.fnmatchcase(filename, pattern)] return categorized"},{"question":"# Question: Event Logging in Distributed Elastic Systems You are tasked with implementing a function to log events in a distributed PyTorch training setup using the PyTorch Distributed Elastic Events API. Your function should record an event with specific metadata and provide a logging mechanism for later review. Function Signature ```python def log_distributed_event(event_name: str, metadata: dict) -> None: pass ``` Input Parameters - `event_name` (str): The name of the event to be recorded. - `metadata` (dict): A dictionary containing metadata related to the event. Metadata keys and values will be strings. Expected Output - The function does not need to return any value. Constraints and Requirements 1. Your function should use the `torch.distributed.elastic.events.record` method to record the event. 2. The metadata should be appropriately handled using the `EventMetadataValue` class. 3. Implement a logging mechanism using `get_logging_handler` to log the events for review. 4. Ensure to handle potential exceptions that may arise during event recording. # Example Usage ```python from torch.distributed.elastic.events import record, get_logging_handler def log_distributed_event(event_name: str, metadata: dict) -> None: # Attempt to log the event with provided metadata try: # Set up the logging handler logging_handler = get_logging_handler() # Convert metadata dictionary to EventMetadataValue instances converted_metadata = {k: EventMetadataValue(v) for k, v in metadata.items()} # Record the event record(event_name, metadata=converted_metadata) # Log the event for review logging_handler.info(f\\"Event recorded: {event_name} with metadata {metadata}\\") except Exception as e: print(f\\"Error recording event: {e}\\") # Example of how to use this function log_distributed_event(\\"training_start\\", {\\"epoch\\": \\"1\\", \\"batch_size\\": \\"32\\"}) ``` Notes - Make sure to handle the conversion of metadata from `dict` to the appropriate `EventMetadataValue` format. - Consider edge cases, such as empty metadata or invalid metadata types. - You might need to refer to the PyTorch Distributed Elastic Events API documentation for further details on methods and classes.","solution":"import torch.distributed.elastic.events as events def log_distributed_event(event_name: str, metadata: dict) -> None: Logs an event in a distributed PyTorch training setup. Parameters: event_name (str): The name of the event to be recorded. metadata (dict): A dictionary containing metadata related to the event. try: # Set up the logging handler logging_handler = events.get_logging_handler() # Convert metadata dictionary to EventMetadataValue instances converted_metadata = {k: events.EventMetadataValue(v) for k, v in metadata.items()} # Record the event events.record(event_name, metadata=converted_metadata) # Log the event for review logging_handler.info(f\\"Event recorded: {event_name} with metadata {metadata}\\") except Exception as e: print(f\\"Error recording event: {e}\\")"},{"question":"# File and Directory Access with Python As an upcoming software developer, you have been tasked with developing a utility script that performs a series of file and directory operations, leveraging the functionality provided by various Python modules described in the documentation. Task: Implement a function `file_dir_utility()` that performs the following operations: 1. **Path Manipulations:** - Create an absolute path from a relative path using `os.path`. 2. **File Operations:** - Check if a specific file exists at a given path using `os.path`. 3. **Temporary Files:** - Create a temporary file, write some content into it, and read the content back using `tempfile`. 4. **Directory Traversal:** - Search for files matching a specific pattern in a directory and its subdirectories using `glob`. 5. **File Comparison:** - Compare two files for equality (content-wise) using `filecmp`. Function Signature: ```python def file_dir_utility(rel_path: str, file_path: str, dir_path: str, pattern: str, compare_file1: str, compare_file2: str) -> str: Perform tasks related to file and directory operations. Arguments: rel_path : str -- A relative file path. file_path : str -- Path to check the existence of a file. dir_path : str -- Directory path to search for files. pattern : str -- Pattern to search for files. compare_file1 : str -- The first file path to compare. compare_file2 : str -- The second file path to compare. Returns: str -- A report in string format that concatenates all the results of the performed operations. ``` Constraints: - The function should handle exceptions and edge cases gracefully, providing relevant error messages. - Efficiently handle file read/write operations to avoid unnecessary I/O overhead. - The pattern used for file searching should support standard Unix pathname patterns. Example: ```python # Example usage: result = file_dir_utility(\'relative/path/to/file.txt\', \'/absolute/path/file.txt\', \'/search/directory\', \'*.py\', \'file1.txt\', \'file2.txt\') print(result) ``` **Explanation:** 1. Given a relative path, the function converts it to an absolute path. 2. Checks if the file exists at the specified path. 3. Creates a temporary file, writes and reads content. 4. Searches for files matching the pattern in the specified directory and subdirectories. 5. Compares two files to determine if their content is identical. 6. Returns a string report of all the operations performed.","solution":"import os import tempfile import glob import filecmp def file_dir_utility(rel_path, file_path, dir_path, pattern, compare_file1, compare_file2): Perform tasks related to file and directory operations. Arguments: rel_path : str -- A relative file path. file_path : str -- Path to check the existence of a file. dir_path : str -- Directory path to search for files. pattern : str -- Pattern to search for files. compare_file1 : str -- The first file path to compare. compare_file2 : str -- The second file path to compare. Returns: str -- A report in string format that concatenates all the results of the performed operations. result = [] try: # Path Manipulations: absolute path from a relative path abs_path = os.path.abspath(rel_path) result.append(f\\"Absolute path: {abs_path}\\") except Exception as e: result.append(f\\"Error in creating absolute path: {str(e)}\\") try: # File Operations: Check if a specific file exists file_exists = os.path.exists(file_path) result.append(f\\"File exists at {file_path}: {file_exists}\\") except Exception as e: result.append(f\\"Error in checking file existence: {str(e)}\\") try: # Temporary Files: Create and read a temporary file with tempfile.NamedTemporaryFile(delete=False) as tmp: tmp.write(b\\"Temporary content.\\") temp_file_path = tmp.name with open(temp_file_path, \'r\') as tmp_file: temp_file_content = tmp_file.read() result.append(f\\"Temporary file content: {temp_file_content}\\") except Exception as e: result.append(f\\"Error in working with temporary file: {str(e)}\\") try: # Directory Traversal: Search for files matching a specific pattern matching_files = glob.glob(os.path.join(dir_path, \\"**\\", pattern), recursive=True) result.append(f\\"Matching files for pattern \'{pattern}\': {matching_files}\\") except Exception as e: result.append(f\\"Error in searching for files: {str(e)}\\") try: # File Comparison: Compare two files for equality files_equal = filecmp.cmp(compare_file1, compare_file2) result.append(f\\"Files {compare_file1} and {compare_file2} are equal: {files_equal}\\") except Exception as e: result.append(f\\"Error in comparing files: {str(e)}\\") return \\"n\\".join(result)"},{"question":"# PyTorch XPU Coding Assessment Question Objective Implement a PyTorch function that demonstrates your understanding of XPU device management, random number generation, streams, and memory management. Problem Statement You need to write a function `pythagorean_triplet_on_xpu` that performs the following: 1. Initializes and sets the current device to XPU. 2. Creates a custom stream for XPU operations. 3. Initializes the random number generator on XPU with a specific seed. 4. Generates two random tensors `a` and `b` of size (100,) on the XPU. 5. Calculates the third tensor `c` such that each element satisfies the Pythagorean triplet condition ( c[i]^2 = a[i]^2 + b[i]^2 ). 6. Synchronizes the stream to ensure all operations are complete. 7. Calculates and returns the maximum memory allocated on the XPU during execution, along with the tensor `c`. Signature ```python def pythagorean_triplet_on_xpu(seed: int) -> Tuple[torch.Tensor, int]: pass ``` Input - `seed` (int): The seed to initialize the random number generator. Output - Tuple containing: - A tensor `c` (torch.Tensor) of size (100,) located on the XPU where each element satisfies the Pythagorean triplet condition. - An integer representing the maximum memory allocated on the XPU during the function\'s execution. Constraints - Use the XPU-specific random number generator. - Ensure that tensor operations occur within the custom stream. - Return the maximum memory allocated during execution using XPU\'s memory management functions. Example ```python seed = 42 c, max_mem = pythagorean_triplet_on_xpu(seed) print(c) # Tensor of size (100,), values satisfying c[i]^2 = a[i]^2 + b[i]^2 print(max_mem) # Maximum memory allocated on the XPU during execution ``` Notes - You may assume that the XPU device is available and initialized. - Make sure to handle the stream and memory management appropriately within your function. Good luck and happy coding!","solution":"import torch def pythagorean_triplet_on_xpu(seed: int) -> tuple: Generates two random tensors \'a\' and \'b\' on the XPU, then computes \'c\' such that each element satisfies the Pythagorean triplet condition c[i]^2 = a[i]^2 + b[i]^2. Returns the tensor \'c\' and the maximum memory allocated on the XPU during execution. Parameters: seed (int): The seed to initialize the random number generator. Returns: tuple: A tuple containing the tensor \'c\' and the maximum memory allocated on the XPU. # Check if XPU is available if not torch.xpu.is_available(): raise RuntimeError(\\"XPU device is not available.\\") # Set the current device to XPU device = \'xpu\' torch.xpu.set_device(device) # Create a custom stream for XPU operations stream = torch.xpu.Stream() # Initialize the random number generator on XPU with a specific seed generator = torch.Generator(device=device).manual_seed(seed) # Use the custom stream context with torch.xpu.stream(stream): # Generate two random tensors `a` and `b` of size (100,) on the XPU a = torch.randn(100, device=device, generator=generator) b = torch.randn(100, device=device, generator=generator) # Calculate the third tensor `c` such that c[i]^2 = a[i]^2 + b[i]^2 c = torch.sqrt(a**2 + b**2) # Synchronize the stream to ensure all operations are complete stream.synchronize() # Calculate and return the maximum memory allocated on the XPU during execution max_mem_allocated = torch.xpu.max_memory_allocated(device) return (c, max_mem_allocated)"},{"question":"Background Python 3.10 provides extensive support for Unicode strings and their manipulation using various encodings and built-in functions. Your task is to utilize these capabilities to create a function that processes Unicode strings in a specific way. Problem Statement You are required to write a function `process_unicode_string` that: 1. Decodes a given byte string of length ( n ) and converts it to a Unicode string. 2. Replaces all occurrences of a specified substring within the resultant Unicode string with another substring. 3. Encodes the modified Unicode string back to a specified encoding. # Function Signature ```python def process_unicode_string(byte_string: bytes, length: int, encoding: str, substring_to_replace: str, replacement_substring: str, output_encoding: str) -> bytes: ``` # Input - `byte_string`: A byte string encoded in the specified `encoding`. - `length`: The length of the byte string. - `encoding`: The encoding of the input byte string (e.g., \'utf-8\', \'latin-1\'). - `substring_to_replace`: The substring within the Unicode string that needs to be replaced. - `replacement_substring`: The substring that will replace each occurrence of `substring_to_replace`. - `output_encoding`: The encoding in which the resultant Unicode string needs to be encoded. # Output - Returns a byte string encoded in the specified `output_encoding` after replacing the specified substring. # Constraints - `length` is a positive integer and corresponds to the length of `byte_string`. - `encoding` and `output_encoding` are valid encodings supported by Python. - `substring_to_replace` and `replacement_substring` are valid Unicode strings. # Example ```python byte_string = b\'xf0x9fx98x8a Hellxf0x9fx98xbb\' length = 12 encoding = \'utf-8\' substring_to_replace = \'Hell\' replacement_substring = \'Heaven\' output_encoding = \'utf-8\' result = process_unicode_string(byte_string, length, encoding, substring_to_replace, replacement_substring, output_encoding) print(result) # Expected Output: b\'xf0x9fx98x8a Heavenxf0x9fx98xbb\' ``` # Notes - The example byte string represents a string with emojis encoded in UTF-8. - The function should correctly decode, replace the substring, and then encode back as specified. # Hints - Use appropriate Unicode decoding and encoding functions. - Ensure the string manipulations keep the Unicode characters intact.","solution":"def process_unicode_string(byte_string: bytes, length: int, encoding: str, substring_to_replace: str, replacement_substring: str, output_encoding: str) -> bytes: # Decode the input byte string using the specified encoding decoded_string = byte_string.decode(encoding) # Replace the specified substring modified_string = decoded_string.replace(substring_to_replace, replacement_substring) # Encode the modified string to the specified output encoding encoded_output = modified_string.encode(output_encoding) return encoded_output"},{"question":"Design a Python script that parses command-line options to configure a simple data processing pipeline. Your script should use the `getopt` module to handle both short and long options according to the following specifications: # Requirements: 1. The script should accept the following options: - Short options: `-i` (input file), `-o` (output file), `-v` (verbose mode) - Long options: `--input=` (input file), `--output=` (output file), `--verbose` (verbose mode), `--filter=` (filter type) 2. The script should handle the following behaviors: - The `-i` or `--input` option requires a filename argument indicating the path of the input file. - The `-o` or `--output` option requires a filename argument indicating the path of the output file. - The `-v` or `--verbose` option does not require an argument and enables verbose mode. - The `--filter` option requires a string argument specifying the type of filter to apply (e.g., `lowpass`, `highpass`, etc.). 3. If any required arguments are missing, or an unrecognized option is provided, the script should print an appropriate error message and exit. 4. If the `--help` option is provided, the script should print a usage message explaining the command-line options and exit. # Implementation: Implement the `main()` function in your script to handle the command-line parsing. Ensure to manage errors gracefully using exception handling. Also, to demonstrate the parsed options, print them in a user-friendly format. # Example Usage: ```bash python script.py -i data.txt -o results.txt --filter=lowpass -v Input File: data.txt Output File: results.txt Filter: lowpass Verbose: True ``` # Input: - Command-line arguments as specified above. # Output: - Print the parsed options in a user-friendly format. # Skeleton Code: ```python import getopt import sys def usage(): print(\\"Usage: script.py -i <inputfile> -o <outputfile> [--filter=<filtertype>] [-v]\\") print(\\"Options:\\") print(\\" -i, --input Path to the input file\\") print(\\" -o, --output Path to the output file\\") print(\\" --filter Filter type to apply (e.g., lowpass, highpass)\\") print(\\" -v, --verbose Enable verbose mode\\") print(\\" --help Show this help message and exit\\") def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"i:o:v\\", [\\"input=\\", \\"output=\\", \\"verbose\\", \\"filter=\\", \\"help\\"]) except getopt.GetoptError as err: print(err) usage() sys.exit(2) inputfile = outputfile = filtertype = None verbose = False for opt, arg in opts: if opt in (\\"-i\\", \\"--input\\"): inputfile = arg elif opt in (\\"-o\\", \\"--output\\"): outputfile = arg elif opt in (\\"--filter\\"): filtertype = arg elif opt in (\\"-v\\", \\"--verbose\\"): verbose = True elif opt == \\"--help\\": usage() sys.exit() else: assert False, \\"Unhandled option\\" if not inputfile or not outputfile: print(\\"Input and output files are required.\\") usage() sys.exit(2) # Print parsed options for demonstration purposes print(\\"Input File:\\", inputfile) print(\\"Output File:\\", outputfile) print(\\"Filter:\\", filtertype) print(\\"Verbose:\\", verbose) if __name__ == \\"__main__\\": main() ```","solution":"import getopt import sys def usage(): print(\\"Usage: script.py -i <inputfile> -o <outputfile> [--filter=<filtertype>] [-v]\\") print(\\"Options:\\") print(\\" -i, --input Path to the input file\\") print(\\" -o, --output Path to the output file\\") print(\\" --filter Filter type to apply (e.g., lowpass, highpass)\\") print(\\" -v, --verbose Enable verbose mode\\") print(\\" --help Show this help message and exit\\") def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"i:o:v\\", [\\"input=\\", \\"output=\\", \\"verbose\\", \\"filter=\\", \\"help\\"]) except getopt.GetoptError as err: print(err) usage() sys.exit(2) inputfile = outputfile = filtertype = None verbose = False for opt, arg in opts: if opt in (\\"-i\\", \\"--input\\"): inputfile = arg elif opt in (\\"-o\\", \\"--output\\"): outputfile = arg elif opt in (\\"--filter\\"): filtertype = arg elif opt in (\\"-v\\", \\"--verbose\\"): verbose = True elif opt == \\"--help\\": usage() sys.exit() else: assert False, \\"Unhandled option\\" if not inputfile or not outputfile: print(\\"Input and output files are required.\\") usage() sys.exit(2) # Print parsed options for demonstration purposes print(f\\"Input File: {inputfile}\\") print(f\\"Output File: {outputfile}\\") if filtertype: print(f\\"Filter: {filtertype}\\") else: print(\\"Filter: None\\") print(f\\"Verbose: {verbose}\\") if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with writing a Python function that analyzes the Unix user accounts on a system. The function should use the `pwd` module as described below to accomplish the following tasks: 1. Retrieve information about a specific user by username. 2. Retrieve information about a specific user by user ID. 3. Retrieve a list of all users and count how many users use each shell. Implement the function `analyze_unix_users(user_name: str, user_id: int) -> dict` that does the following: # Function Signature ```python def analyze_unix_users(user_name: str, user_id: int) -> dict: pass ``` # Input - `user_name` (str): A string representing the username to look up. - `user_id` (int): An integer representing the user ID to look up. # Output - The function should return a dictionary with the following structure: ```python { \\"user_by_name\\": { # This contains the details of the user with the specified username \\"pw_name\\": str, \\"pw_passwd\\": str, \\"pw_uid\\": int, \\"pw_gid\\": int, \\"pw_gecos\\": str, \\"pw_dir\\": str, \\"pw_shell\\": str }, \\"user_by_uid\\": { # This contains the details of the user with the specified user ID \\"pw_name\\": str, \\"pw_passwd\\": str, \\"pw_uid\\": int, \\"pw_gid\\": int, \\"pw_gecos\\": str, \\"pw_dir\\": str, \\"pw_shell\\": str }, \\"shell_usage_count\\": { # This maps each unique shell to the number of users using that shell \\"/bin/bash\\": int, \\"/bin/zsh\\": int, ... } } ``` # Constraints and Performance Requirements: - If a user with the specified username or user ID does not exist, the corresponding dictionary (`user_by_name` or `user_by_uid`) should be `None`. - The function should handle cases where the `pw_passwd` field contains an asterisk (\\"\'*\'\\") or the letter \\"\'x\'\\". - You can assume that the system has fewer than 10,000 users, ensuring that performance and memory usage are manageable. # Notes: - Utilize the `pwd` module to retrieve user information. - Consider exception handling for cases where user entries are not found. # Example Let’s say we are querying the following users: - `user_name = \\"johndoe\\"` - `user_id = 1001` And the user database has the following entries: ```python [ pwd.struct_passwd((\'johndoe\', \'x\', 1001, 100, \'John Doe\', \'/home/johndoe\', \'/bin/bash\')), pwd.struct_passwd((\'janedoe\', \'*\', 1002, 100, \'Jane Doe\', \'/home/janedoe\', \'/bin/zsh\')) ] ``` Expected result: ```python { \\"user_by_name\\": { \\"pw_name\\": \\"johndoe\\", \\"pw_passwd\\": \\"x\\", \\"pw_uid\\": 1001, \\"pw_gid\\": 100, \\"pw_gecos\\": \\"John Doe\\", \\"pw_dir\\": \\"/home/johndoe\\", \\"pw_shell\\": \\"/bin/bash\\" }, \\"user_by_uid\\": { \\"pw_name\\": \\"johndoe\\", \\"pw_passwd\\": \\"x\\", \\"pw_uid\\": 1001, \\"pw_gid\\": 100, \\"pw_gecos\\": \\"John Doe\\", \\"pw_dir\\": \\"/home/johndoe\\", \\"pw_shell\\": \\"/bin/bash\\" }, \\"shell_usage_count\\": { \\"/bin/bash\\": 1, \\"/bin/zsh\\": 1 } } ```","solution":"import pwd from collections import defaultdict def analyze_unix_users(user_name: str, user_id: int) -> dict: # Helper function to convert pwd.struct_passwd object to dictionary def passwd_to_dict(p): return { \'pw_name\': p.pw_name, \'pw_passwd\': p.pw_passwd, \'pw_uid\': p.pw_uid, \'pw_gid\': p.pw_gid, \'pw_gecos\': p.pw_gecos, \'pw_dir\': p.pw_dir, \'pw_shell\': p.pw_shell } # Retrieve information for the specific user by name try: user_by_name = pwd.getpwnam(user_name) user_by_name = passwd_to_dict(user_by_name) except KeyError: user_by_name = None # Retrieve information for the specific user by UID try: user_by_uid = pwd.getpwuid(user_id) user_by_uid = passwd_to_dict(user_by_uid) except KeyError: user_by_uid = None # Count the number of users using each shell shell_usage_count = defaultdict(int) for p in pwd.getpwall(): shell_usage_count[p.pw_shell] += 1 return { \\"user_by_name\\": user_by_name, \\"user_by_uid\\": user_by_uid, \\"shell_usage_count\\": dict(shell_usage_count) }"},{"question":"# `plistlib` Module Assessment **Objective:** Demonstrate your understanding of the `plistlib` module by implementing functions to generate and parse Apple property list (\\".plist\\") files. **Task:** 1. Implement a function `create_plist()` that generates a plist-formatted bytes object containing a predefined dictionary with various data types. 2. Implement a function `parse_plist(data)` that takes a plist-formatted bytes object and returns the corresponding Python dictionary. **Function Details:** 1. `create_plist()` - **Input:** None - **Output:** A bytes object containing the plist representation of the following dictionary: ```python { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"is_student\\": False, \\"grades\\": [88, 92, 77], \\"address\\": { \\"street\\": \\"123 Main St\\", \\"city\\": \\"Townsville\\", \\"zip_code\\": \\"12345\\" }, \\"last_login\\": datetime.datetime(2023, 8, 15, 12, 30) } ``` 2. `parse_plist(data)` - **Input:** A bytes object `data` containing a plist-formatted representation of a dictionary. - **Output:** A dictionary parsed from the plist data. **Constraints:** - Use the current date and time for the `last_login` value in `create_plist()`. - Ensure the plist format used is XML. - Handle any exceptions that may arise during parsing and return `None` if the input is not a valid plist. **Performance Requirements:** - Both functions should handle typical plist data sizes efficiently. **Example:** ```python import datetime def create_plist(): import plistlib data_dict = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"is_student\\": False, \\"grades\\": [88, 92, 77], \\"address\\": { \\"street\\": \\"123 Main St\\", \\"city\\": \\"Townsville\\", \\"zip_code\\": \\"12345\\" }, \\"last_login\\": datetime.datetime.now() } return plistlib.dumps(data_dict, fmt=plistlib.FMT_XML) def parse_plist(data): import plistlib try: return plistlib.loads(data, fmt=plistlib.FMT_XML) except Exception: return None # Example usage: plist_data = create_plist() print(plist_data) parsed_data = parse_plist(plist_data) print(parsed_data) ``` In your implementation, ensure proper usage of the `plistlib` module as demonstrated in the example above.","solution":"import datetime import plistlib def create_plist(): Generates a plist-formatted bytes object containing a predefined dictionary with various data types. data_dict = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"is_student\\": False, \\"grades\\": [88, 92, 77], \\"address\\": { \\"street\\": \\"123 Main St\\", \\"city\\": \\"Townsville\\", \\"zip_code\\": \\"12345\\" }, \\"last_login\\": datetime.datetime.now() } return plistlib.dumps(data_dict, fmt=plistlib.FMT_XML) def parse_plist(data): Takes a plist-formatted bytes object and returns the corresponding Python dictionary. Returns `None` if the input is not a valid plist. try: return plistlib.loads(data, fmt=plistlib.FMT_XML) except Exception: return None"},{"question":"**Question:** You are required to implement a Python class `MemoryViewHandler` that performs various operations involving memoryviews. Given the low-level nature of the operations, this class should encapsulate the creation and manipulation of memoryviews efficiently. Implement the following methods in the `MemoryViewHandler` class: 1. `__init__(self, source)`: - Initialize with a `source` object that supports the buffer protocol (like bytes or bytearray). 2. `get_memoryview(self)`: - Return a memoryview of the source object. 3. `read_memoryview(self, start: int, end: int) -> bytes`: - Read and return a bytes object from the memoryview for the specified `start` and `end` range. 4. `write_memoryview(self, start: int, data: bytes)`: - Write the provided `data` bytes into the memoryview starting at `start` index. 5. `is_memoryview(obj)`: - Static method that returns `True` if the given `obj` is a memoryview object, else `False`. 6. `get_underlying_buffer(self)`: - Return the underlying buffer of the memoryview. **Constraints:** - Assume the `source` object is always a mutable object supporting the buffer protocol (bytearray). - For reading and writing, the provided indices and bytes will always be within the valid range of the original buffer. **Example Usage:** ```python source_data = bytearray(b\\"abcdefghij\\") handler = MemoryViewHandler(source_data) # Get a memoryview of the source memview = handler.get_memoryview() print(memview.tolist()) # Output: [97, 98, 99, 100, 101, 102, 103, 104, 105, 106] # Read a portion of the memoryview print(handler.read_memoryview(2, 5)) # Output: b\'cde\' # Write into the memoryview handler.write_memoryview(2, b\'XYZ\') print(source_data) # Output: bytearray(b\'abXYZfghij\') # Check if an object is a memoryview print(MemoryViewHandler.is_memoryview(memview)) # Output: True print(MemoryViewHandler.is_memoryview(source_data)) # Output: False # Get the underlying buffer print(handler.get_underlying_buffer()) # Output: memory at 0x... (pointer representation) ``` **Note**: Your implementation should correctly use memoryview operations and handle cases efficiently to demonstrate an understanding of memory management in Python.","solution":"class MemoryViewHandler: def __init__(self, source): Initialize with a source object that supports the buffer protocol (bytearray). self.source = source self.memview = memoryview(source) def get_memoryview(self): Return a memoryview of the source object. return self.memview def read_memoryview(self, start: int, end: int) -> bytes: Read and return a bytes object from the memoryview for the specified start and end range. return self.memview[start:end].tobytes() def write_memoryview(self, start: int, data: bytes): Write the provided data bytes into the memoryview starting at start index. self.memview[start:start+len(data)] = data @staticmethod def is_memoryview(obj): Return True if the given object is a memoryview object, else False. return isinstance(obj, memoryview) def get_underlying_buffer(self): Return the pointer address of the underlying buffer of the memoryview. return self.memview.obj"},{"question":"Coding Assessment Question # Objective: Design and implement a Python application using the `tkinter.tix` module that demonstrates the use of various widgets to create a File Manager and Viewer interface. # Description: You are tasked with creating a basic File Manager application using the `tkinter.tix` module. This application should allow users to navigate through the file system, select a folder, and display the hierarchical structure of the files within that folder using a tree view. Additionally, the user should be able to choose a file from the tree view, and a pop-up dialog should show some details about the selected file (e.g., file size, creation date). # Requirements: 1. **File Navigation**: - Use `tix.DirTree` to allow users to navigate the file system and select a directory. - Upon selecting a directory, display the hierarchical structure of files and subdirectories within the selected directory using `tix.Tree`. 2. **File Details**: - When a file is selected from the tree, use a `tix.PopupMenu` that appears on right-click (or an equivalent trigger) to display details such as: - File Name - File Size - Creation Date 3. **Widgets Utilization**: - Use `tix.NoteBook` to contain multiple tabs for additional information or features you might want to implement, like recent files or favorite locations. - Integrate a `tix.Balloon` widget to provide tooltips over various components of your application for better user assistance. # Input and Output: - The program should create a GUI window with the described functionality. - No specific input format. The user will interact with the file system through the GUI. - Output is the GUI window showing the file navigation structure and file details as described. # Constraints: - Ensure that the GUI is responsive and does not freeze during file navigation or when loading large directories. - Use appropriate error handling to manage cases where directories might not be accessible. # Performance Requirements: - The application should efficiently handle directories with a large number of files and subdirectories. - Ensure that UI updates like tree view construction are optimized to avoid lag. # Example Usage: 1. User launches the application. 2. User navigates through the file system using the `DirTree`. 3. Upon selecting a directory, the hierarchical structure of files is displayed using the `Tree`. 4. User right-clicks on a file to get a pop-up menu with details about the file. # Additional Notes: - You might need to look at additional documentation or resources if you\'re not familiar with specific `tkinter.tix` functionalities. - Write clean, modular code with comments explaining the key parts of your implementation. This question assesses your understanding of using `tkinter.tix` for creating complex UIs, managing hierarchical data, and handling user interactions effectively.","solution":"import os import tkinter.tix as tix from tkinter import messagebox from tkinter import filedialog import time def display_file_details(file_path): try: file_stats = os.stat(file_path) file_size = file_stats.st_size creation_date = time.ctime(file_stats.st_ctime) detail_msg = f\\"File Name: {os.path.basename(file_path)}n\\" detail_msg += f\\"File Size: {file_size} bytesn\\" detail_msg += f\\"Creation Date: {creation_date}\\" messagebox.showinfo(\\"File Details\\", detail_msg) except Exception as e: messagebox.showerror(\\"Error\\", str(e)) def on_directory_selection(event): tree = event.widget node_id = tree.selection_get() load_directory_tree(tree, node_id, tree.entrycget(node_id, \'text\')) def load_directory_tree(tree, parent_node, parent_directory): try: for item in tree.entrycget(parent_node, \'children\'): tree.delete(item) for name in os.listdir(parent_directory): path = os.path.join(parent_directory, name) if os.path.isdir(path): node_id = tree.add(parent_node, text=name) tree.add(node_id, text=\'..place_holder..\') # Add a placeholder for lazy loading else: tree.add(parent_node, itemtype=tix.TEXT, text=name) except Exception as e: messagebox.showerror(\\"Error\\", str(e)) def setup_ui(root): nb = tix.NoteBook(root) nb.add(\\"filemanager\\", label=\\"File Manager\\") nb.pack(expand=1, fill=tix.BOTH) fm_tab = nb.page(\\"filemanager\\") frame = tix.Frame(fm_tab) frame.pack(expand=1, fill=tix.BOTH) dir_tree = tix.DirTree(frame) dir_tree.config(width=200) dir_tree.pack(side=tix.LEFT, fill=tix.Y) file_tree = tix.Tree(frame) file_tree.pack(side=tix.RIGHT, expand=1, fill=tix.BOTH) dir_tree.bind(\'<<TreeSelect>>\', on_directory_selection) file_tree.bind(\'<Button-3>\', lambda event: on_file_right_click(event, file_tree)) balloon = tix.Balloon(root) balloon.bind_widget(dir_tree, balloonmsg=\\"Select a directory to view its contents\\") balloon.bind_widget(file_tree, balloonmsg=\\"Browse the files in the selected directory\\") return root def on_file_right_click(event, tree): selected_item = tree.selection_get() parent_dir = get_parent_directory(tree, selected_item) file_path = os.path.join(parent_dir, tree.entrycget(selected_item, \'text\')) if os.path.isfile(file_path): menu = tix.Menu(tree, tearoff=0) menu.add_command(label=\\"File Details\\", command=lambda: display_file_details(file_path)) menu.post(event.x_root, event.y_root) def get_parent_directory(tree, node_id): parent_id = tree.entrycget(node_id, \'parent\') tree_root = tree.entrycget(node_id, \'root\') if parent_id == tree_root: return tree.entrycget(node_id, \'text\') else: return os.path.join(get_parent_directory(tree, parent_id), tree.entrycget(node_id, \'text\')) if __name__ == \\"__main__\\": root = tix.Tk() root = setup_ui(root) root.mainloop()"},{"question":"# Pandas Coding Assessment Background You are provided with a dataset containing information about sales transactions in a retail store. The dataset includes the following columns: - `TransactionID`: Unique identifier for each transaction - `Date`: Date of the transaction - `ProductID`: Unique identifier for each product - `Quantity`: Number of units sold - `Price`: Price per unit of the product - `CustomerID`: Unique identifier for each customer The dataset is loaded into a pandas DataFrame called `df`. Task 1. **Merge DataFrames**: You have an additional DataFrame `product_info` containing product details: - `ProductID`: Unique identifier for each product - `ProductName`: Name of the product - `Category`: Category of the product Merge this DataFrame with the main `df` to include product details in the transactions dataset. 2. **Handling Missing Data**: Some transactions might have missing `CustomerID`. Fill these missing values with the string `\'Unknown\'`. 3. **Date Manipulation**: Convert the `Date` column to datetime format and add a new column `Month` that indicates the month of each transaction. 4. **Pivot Table**: Create a pivot table showing the total quantity sold for each product in each month. The rows should represent `ProductName`, the columns should represent `Month`, and the values should be the sum of `Quantity`. 5. **Adding Derived Metrics**: Add a new column to the pivot table that calculates the `TotalQuantity` for each product across all months. 6. **Filtering**: Filter the pivot table to only include products with a `TotalQuantity` of more than 100 units. 7. **Returning Results**: Return the final pivot table as the output of your function. Expected Function Signature: ```python import pandas as pd def analyze_sales_data(df: pd.DataFrame, product_info: pd.DataFrame) -> pd.DataFrame: pass ``` Constraints: - Assume that the input DataFrames are not empty and contain the necessary columns. - Aim for efficiency in your solution, but there are no strict performance requirements. Example Usage: ```python df = pd.DataFrame({ \'TransactionID\': [1, 2, 3], \'Date\': [\'2023-01-05\', \'2023-02-20\', \'2023-01-15\'], \'ProductID\': [101, 102, 101], \'Quantity\': [5, 10, 3], \'Price\': [100, 200, 100], \'CustomerID\': [201, None, 203] }) product_info = pd.DataFrame({ \'ProductID\': [101, 102], \'ProductName\': [\'Product A\', \'Product B\'], \'Category\': [\'Category 1\', \'Category 2\'] }) result = analyze_sales_data(df, product_info) print(result) ```","solution":"import pandas as pd def analyze_sales_data(df: pd.DataFrame, product_info: pd.DataFrame) -> pd.DataFrame: # Merge product information into the main dataframe df = df.merge(product_info, on=\'ProductID\', how=\'left\') # Fill missing CustomerID with \'Unknown\' df[\'CustomerID\'] = df[\'CustomerID\'].fillna(\'Unknown\') # Convert the Date column to datetime format and add a Month column df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') # Create the pivot table pivot_df = df.pivot_table(values=\'Quantity\', index=\'ProductName\', columns=\'Month\', aggfunc=\'sum\', fill_value=0) # Add a new column for the total quantity across all months pivot_df[\'TotalQuantity\'] = pivot_df.sum(axis=1) # Filter to include only products with TotalQuantity more than 100 units result_df = pivot_df[pivot_df[\'TotalQuantity\'] > 100] return result_df"},{"question":"# PyTorch Coding Assessment: **Objective:** Demonstrate understanding of PyTorch\'s meta device and tensor operations. **Problem Statement:** You are given a neural network model implemented in PyTorch. Your task is to: 1. Load the model onto the meta device. 2. Print the architecture of the model as it exists on the meta device. 3. Convert the model back to the CPU, explicitly reinitializing its parameters with random values. **Model Definition:** ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 20) self.fc3 = nn.Linear(20, 5) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Save the model for loading model = SimpleModel() torch.save(model, \'simple_model.pt\') ``` **Your Task:** 1. Load the saved `simple_model.pt` onto the meta device. 2. Print the architecture of the loaded model to ensure it is on the meta device. 3. Convert the model back to the CPU, explicitly reinitializing its parameters with random values. **Specifications:** - You should not modify the `SimpleModel` class. - You are required to use `torch.device(\'meta\')` and `model.to_empty(\'cpu\')` where necessary. - After reinitialization, print the model\'s first layer weights to verify they have been reinitialized. # Function Signature: ```python def meta_to_cpu_model(): import torch # Load the model onto the meta device model_meta = torch.load(\'simple_model.pt\', map_location=\'meta\') # Print the architecture of the loaded model print(f\'Model on Meta Device:n{model_meta}\') # Convert the model to the CPU and reinitialize its parameters model_meta.to_empty(\'cpu\') def reinitialize_parameters(model): for layer in model.children(): if hasattr(layer, \'reset_parameters\'): layer.reset_parameters() reinitialize_parameters(model_meta) # Print the model\'s first layer weights to verify initialization print(\'Reinitialized first layer weights on CPU:\') print(model_meta.fc1.weight) ``` Ensure that your code runs correctly and meets the specified requirements to demonstrate proficiency with PyTorch\'s meta device and tensor reinitialization concepts.","solution":"import torch import torch.nn as nn def meta_to_cpu_model(): # Load the model onto the meta device model_meta = torch.load(\'simple_model.pt\', map_location=\'meta\') # Print the architecture of the loaded model print(f\'Model on Meta Device:n{model_meta}\') # Convert the model to the CPU and reinitialize its parameters model_meta.to_empty(device=\'cpu\') def reinitialize_parameters(model): for layer in model.children(): if hasattr(layer, \'reset_parameters\'): layer.reset_parameters() reinitialize_parameters(model_meta) # Print the model\'s first layer weights to verify initialization print(\'Reinitialized first layer weights on CPU:\') print(model_meta.fc1.weight) # Ensure the SimpleModel can be saved before performing operations class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 20) self.fc3 = nn.Linear(20, 5) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Save the model for loading model = SimpleModel() torch.save(model, \'simple_model.pt\')"},{"question":"**Question: Implementing Custom File Operations with Builtin Wrappers** In Python, the `open` function is used to open a file and return a corresponding file object. However, sometimes, you might want to add custom behavior to file operations while still retaining access to the original `open` function. In such cases, the `builtins` module can be helpful. Your task is to implement a custom file handler `CustomFileHandler` that will: 1. Wrap the built-in `open` function using the `builtins` module. 2. Provide a method to read the content of a file and return it in lowercase. 3. Provide a method to write content to a file, but the content should be reversed before writing. # Requirements: - Implement a class `CustomFileHandler` with the following methods: - `__init__(self, path: str, mode: str)`: Initializes the file handler with the given path and mode by using the built-in `open` function. - `read(self) -> str`: Reads the content of the file and returns it in lowercase. - `write(self, content: str)`: Writes the reversed content to the file. - `close(self)`: Closes the file. # Input - The `__init__` method takes `path` and `mode` as arguments where `path` is the file path, and `mode` is the mode for opening the file (e.g., \'r\' for reading, \'w\' for writing). - The `read` method does not take any arguments. - The `write` method takes a single argument `content` which is the string to be written to the file. - The `close` method does not take any arguments. # Output - The `read` method returns a string which is the content of the file in lowercase. - The `write` method does not return anything. - The `close` method does not return anything. # Example Usage ```python # Example file content before running the code: # Hello World! handler = CustomFileHandler(\'example.txt\', \'r\') print(handler.read()) # Output: \'hello world!\' handler.close() handler = CustomFileHandler(\'example.txt\', \'w\') handler.write(\'Hello World!\') handler.close() # Example file content after running the code: # !dlroW olleH ``` # Constraints - Do not use any third-party libraries. Only the standard library is allowed. - Handle any file exceptions appropriately. Write your solution in Python below: ```python # Your implementation here ```","solution":"import builtins class CustomFileHandler: def __init__(self, path: str, mode: str): Initializes the file handler with the given path and mode by using the built-in open function. self.file = builtins.open(path, mode) def read(self) -> str: Reads the content of the file and returns it in lowercase. content = self.file.read() return content.lower() def write(self, content: str): Writes the reversed content to the file. reversed_content = content[::-1] self.file.write(reversed_content) def close(self): Closes the file. self.file.close()"},{"question":"Objective You are required to write a Python function that simulates the behavior of the `PyOS_FSPath` function from the Python C API. This involves handling different types of input related to file system paths. Problem Statement Write a function `get_fs_path(path)` that takes a single argument `path` and returns its file system representation. The function should handle different input types as follows: - If `path` is of type `str` or `bytes`, return the `path` as it is. - If `path` is an object and implements the `os.PathLike` interface, return the result of calling its `__fspath__` method, but only if it returns a `str` or `bytes`. Otherwise, raise a `TypeError`. - For all other types, raise a `TypeError`. Function Signature ```python def get_fs_path(path: Any) -> Union[str, bytes]: # your code here ``` Input - `path` (any type): The input object which needs to be converted to a file system path representation. Output - A `str` or `bytes` representing the file system path. Constraints - The function should only return the file system representation if the `path` meets the specified criteria. - If `path` does not meet any of the criteria, the function should raise a `TypeError` with an appropriate error message. Example ```python # Example 1 print(get_fs_path(\'/usr/bin\')) # Output: \'/usr/bin\' # Example 2 print(get_fs_path(b\'/usr/bin\')) # Output: b\'/usr/bin\' # Example 3 from pathlib import Path print(get_fs_path(Path(\'/usr/bin\'))) # Output: \'/usr/bin\' # Example 4 print(get_fs_path(100)) # Raises TypeError ``` Notes - The function should handle both `str` and `bytes` inputs directly. - For objects implementing `os.PathLike`, ensure to verify the output type of `__fspath__`. - Provide clear exception messages when raising a `TypeError`. # Performance Requirement - The function should be efficient and handle typical cases in constant or linear time depending on the input. # Hints - The `os` module contains the `PathLike` interface which might be useful for type-checking. - Consider using Python\'s built-in `hasattr()` and `callable()` for attribute checking and verifying that `__fspath__` is a method. Good luck!","solution":"import os from typing import Any, Union def get_fs_path(path: Any) -> Union[str, bytes]: if isinstance(path, (str, bytes)): return path elif isinstance(path, os.PathLike): result = path.__fspath__() if isinstance(result, (str, bytes)): return result else: raise TypeError(f\\"__fspath__ method returned a non-string/bytes type: {type(result)}\\") else: raise TypeError(f\\"Invalid path type: {type(path)}\\")"},{"question":"**PyTorch Coding Challenge: Random Number Operations** **Objective:** Design a function using PyTorch that demonstrates proficiency with the random number generation utilities provided by PyTorch. **Problem Statement:** You are required to implement a function named `random_tensor_operations` which performs the following tasks: 1. Sets a manual random seed for reproducibility. 2. Generates a random tensor of shape `(5, 5)` with values drawn from a uniform distribution on the interval `[0, 1)`. 3. Generates another random tensor of shape `(5, 5)` with values drawn from a normal distribution with mean `0` and standard deviation `1`. 4. Adds the two tensors. 5. Multiplies the resulting tensor by a scalar value of your choice. 6. Returns the final tensor. Ensure that: - The random seed is set at the beginning of your function. - The operations are performed using PyTorch tensor operations. - The function returns a PyTorch tensor. **Function Signature:** ```python import torch def random_tensor_operations() -> torch.Tensor: pass ``` **Example:** ```python >>> result = random_tensor_operations() >>> print(result) # The output will be deterministic given the same random seed. tensor([[ 0.4956, -1.0332, 0.7917, 0.0976, 0.5698], [ 1.0670, -0.7902, -0.1841, 0.7891, -0.3229], [-0.2493, 0.6425, 0.0951, -0.5930, 1.1200], [ 1.2033, -0.3143, 1.1164, 0.3571, 1.0526], [ 0.5945, -1.1361, 1.1441, 0.7578, 0.4933]]) ``` **Constraints:** - Use `torch.manual_seed` to set the random seed for reproducibility. - Use `torch.rand` to generate the uniformly distributed tensor. - Use `torch.randn` to generate the normally distributed tensor. - Perform tensor operations using PyTorch\'s tensor methods and functions. **Note:** You may choose any deterministic seed value and scalar for multiplication, but ensure these are clearly specified in your implementation. **Submission:** Submit your implementation and ensure it includes the function as specified. Additionally, include a brief description of the logic you used for the function implementation.","solution":"import torch def random_tensor_operations() -> torch.Tensor: # Set the manual seed for reproducibility torch.manual_seed(42) # Generate a random tensor from a uniform distribution on the interval [0, 1) tensor_uniform = torch.rand((5, 5)) # Generate a random tensor from a normal distribution with mean 0 and standard deviation 1 tensor_normal = torch.randn((5, 5)) # Add the two tensors tensor_sum = tensor_uniform + tensor_normal # Multiply the resulting tensor by a scalar (e.g., 2.5) scalar = 2.5 final_tensor = tensor_sum * scalar return final_tensor"},{"question":"**Objective:** Demonstrate your understanding of the `fileinput` module in Python by performing specific file operations using this module. **Task:** Write a Python function `process_files` that takes a list of filenames and performs the following tasks: 1. **Count Lines**: Count the total number of lines across all files. 2. **First Lines**: Collect the first line of each file. 3. **Replace Text**: Perform in-place replacement of the word \\"foo\\" with \\"bar\\" in all files. ```python import fileinput def process_files(file_list): Process the given list of files using the fileinput module to: 1. Count the total number of lines across all files. 2. Collect the first line of each file. 3. Perform in-place replacement of the word \'foo\' with \'bar\'. Parameters: - file_list: List of filenames Returns: - A tuple (lines_count, first_lines) where \'lines_count\' is the total number of lines, and \'first_lines\' is a list of first lines from each file. # Your code here # Example Usage: file_list = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] lines_count, first_lines = process_files(file_list) print(f\\"Total lines: {lines_count}\\") print(\\"First lines:\\", first_lines) ``` **Constraints:** - The function should handle empty files gracefully. - You should not assume any specific content in the files other than the presence of the word \\"foo\\". **Input:** - `file_list`: A list of filenames (e.g. `[\'file1.txt\', \'file2.txt\']`). **Output:** - A tuple where the first element is an integer representing the total number of lines across all files, and the second element is a list of strings where each string is the first line from the corresponding input file. Note: - Ensure your function is efficient and handles I/O efficiently. - You may assume only valid, existing file paths are provided in `file_list`. **Example Execution:** Suppose we have the following files with contents: - `file1.txt`: ``` foo one line two ``` - `file2.txt`: ``` first foo second line ``` When we call `process_files([\'file1.txt\', \'file2.txt\'])`, it should output: Total lines: 4 First lines: [\'foo one\', \'first foo\'] And the contents of `file1.txt` and `file2.txt` should be modified to: - `file1.txt`: ``` bar one line two ``` - `file2.txt`: ``` first bar second line ``` Make sure to test your function with different sets of files to ensure correctness.","solution":"import fileinput def process_files(file_list): Process the given list of files using the fileinput module to: 1. Count the total number of lines across all files. 2. Collect the first line of each file. 3. Perform in-place replacement of the word \'foo\' with \'bar\'. Parameters: - file_list: List of filenames Returns: - A tuple (lines_count, first_lines) where \'lines_count\' is the total number of lines, and \'first_lines\' is a list of first lines from each file. total_lines = 0 first_lines = [] for filename in file_list: with open(filename, \'r\') as file: lines = file.readlines() if lines: first_lines.append(lines[0].strip()) total_lines += len(lines) for line in fileinput.input(files=file_list, inplace=True): print(line.replace(\'foo\', \'bar\'), end=\'\') return total_lines, first_lines"},{"question":"**Objective**: Assess the understanding of PyTorch tensor attributes including `dtype`, `device`, and tensor operations with type promotion rules. **Problem Statement**: You are given a set of tensors with varying data types and devices. Your task is to write a function `tensor_operations` that performs a series of operations on these tensors, keeping the following points in mind: 1. **Type Promotion**: All arithmetic operations should respect PyTorch\'s type promotion rules. 2. **Device Management**: Ensure that tensor operations handle devices correctly and tensors are moved appropriately. 3. **Output Format**: Follow specific memory format requirements for the result. **Function Implementation**: ```python import torch def tensor_operations(tensor1, tensor2): Perform arithmetic operations with type promotion and correct device management. Args: - tensor1 (torch.Tensor): First input tensor. - tensor2 (torch.Tensor): Second input tensor. Returns: - result (torch.Tensor): Result of the tensor operations. - The dtype of the result should be the promoted dtype according to PyTorch rules. - The result should be on the same device as tensor1. - The result should be in `torch.channels_last` memory format if it is a 4D tensor (e.g., for image data). # Ensure that both tensors are on the same device if tensor1.device != tensor2.device: tensor2 = tensor2.to(tensor1.device) # Perform addition result_add = tensor1 + tensor2 # Perform multiplication result_mul = result_add * tensor1 # If the result is a 4D tensor, set memory format to \'channels_last\' if result_mul.dim() == 4: result_mul = result_mul.to(memory_format=torch.channels_last) return result_mul # Example Usage: tensor1 = torch.randn((3, 3), dtype=torch.float32, device=\'cuda:0\') tensor2 = torch.ones((3, 3), dtype=torch.int64, device=\'cuda:0\') result = tensor_operations(tensor1, tensor2) print(result) print(result.dtype) # Should be torch.float32 due to type promotion print(result.device) # Should be cuda:0 print(result.is_contiguous(memory_format=torch.channels_last)) # Check memory format if applicable ``` **Constraints**: - The tensors can be of different dtypes and devices. - The function should handle both 2D and 4D tensors correctly. - Ensure the final result tensor\'s memory format is set appropriately when applicable. **Notes**: - You may assume tensors are initialized correctly from within PyTorch. - Pay particular attention to handling devices and memory formats to avoid common pitfalls. **What We Are Looking For**: - Correct application of type promotion. - Proper device management. - Appropriate handling of memory formats.","solution":"import torch def tensor_operations(tensor1, tensor2): Perform arithmetic operations with type promotion and correct device management. Args: - tensor1 (torch.Tensor): First input tensor. - tensor2 (torch.Tensor): Second input tensor. Returns: - result (torch.Tensor): Result of the tensor operations. - The dtype of the result should be the promoted dtype according to PyTorch rules. - The result should be on the same device as tensor1. - The result should be in `torch.channels_last` memory format if it is a 4D tensor (e.g., for image data). # Ensure that both tensors are on the same device if tensor1.device != tensor2.device: tensor2 = tensor2.to(tensor1.device) # Perform addition result_add = tensor1 + tensor2 # Perform multiplication result_mul = result_add * tensor1 # If the result is a 4D tensor, set memory format to \'channels_last\' if result_mul.dim() == 4: result_mul = result_mul.to(memory_format=torch.channels_last) return result_mul"},{"question":"# Question: You are tasked with creating a utility script that helps developers quickly gather and display Python configuration information on their system. Your script should be written in Python and should use the `sysconfig` module to achieve the following: 1. Display the current Python version and platform. 2. Display all supported installation schemes. 3. For each of the six installation schemes (`posix_prefix`, `posix_home`, `posix_user`, `nt`, `nt_user`, `osx_framework_user`), display: - The paths for `stdlib`, `platstdlib`, `platlib`, `purelib`, `include`, `platinclude`, `scripts`, and `data`. - The configuration variables and their values. Your script should structure this information in a readable format. You can use print statements to display the information on the console. Function Signature ```python def display_python_config_info() -> None: pass ``` Expected Output Format The output should be formatted as follows (example values shown): ``` Python Version: 3.10 Platform: posix Supported Installation Schemes: - posix_prefix - posix_home - posix_user - nt - nt_user - osx_framework_user Configuration for scheme: posix_prefix Paths: stdlib: /usr/local/lib/python3.10 platstdlib: /usr/local/lib/python3.10 platlib: /usr/local/lib/python3.10/site-packages purelib: /usr/local/lib/python3.10/site-packages include: /usr/local/include/python3.10 platinclude: /usr/local/include/python3.10 scripts: /usr/local/bin data: /usr/local Configuration Variables: {\'AR\': \'ar\', \'ARFLAGS\': \'rc\', ...} Configuration for scheme: posix_home ... (repeat for each scheme) ``` Constraints - The script should handle cases where certain schemes or paths might not be available on the current platform gracefully. - Use appropriate error handling to manage any exceptions that might arise from missing configurations or unsupported operations. Additional Notes - Ensure that your code is well-documented with comments explaining each step. - Aim for readability and maintainability in your coding style.","solution":"import sys import sysconfig def display_python_config_info() -> None: Display the current Python configuration information, including version, platform, installation schemes, paths, and configuration variables. # Display Python version and platform python_version = sys.version.split()[0] # Extracting first part of version string platform = sysconfig.get_platform() print(f\\"Python Version: {python_version}\\") print(f\\"Platform: {platform}n\\") # Displaying supported installation schemes schemes = sysconfig.get_scheme_names() print(\\"Supported Installation Schemes:\\") for scheme in schemes: print(f\\"- {scheme}\\") print() # Information for each scheme for scheme in schemes: print(f\\"Configuration for scheme: {scheme}\\") try: # Displaying paths paths = { \'stdlib\': sysconfig.get_path(\'stdlib\', scheme), \'platstdlib\': sysconfig.get_path(\'platstdlib\', scheme), \'platlib\': sysconfig.get_path(\'platlib\', scheme), \'purelib\': sysconfig.get_path(\'purelib\', scheme), \'include\': sysconfig.get_path(\'include\', scheme), \'platinclude\': sysconfig.get_path(\'platinclude\', scheme), \'scripts\': sysconfig.get_path(\'scripts\', scheme), \'data\': sysconfig.get_path(\'data\', scheme), } print(\\"Paths:\\") for key, value in paths.items(): print(f\\" {key}: {value}\\") # Displaying configuration variables config_vars = sysconfig.get_config_vars() print(\\"nConfiguration Variables:\\") print(config_vars) except KeyError: print(\\" This scheme is not supported on this platform.\\") print() # Running the function to display the configuration information display_python_config_info()"},{"question":"**Objective:** To demonstrate your understanding of Python\'s built-in functions and classes by creating a complex, well-thought-out solution. **Problem Statement:** You are given a text file named `data.txt` that contains information about various products. Each line in the file represents a product and contains the following information separated by a comma: - Product ID (an integer) - Product Name (a string) - Price (a float) - Quantity in stock (an integer) Your task is to read the contents of this file and process the data to answer the following questions: 1. **Total number of products available.** 2. **The product with the highest price.** 3. **The total value of all products in stock.** (The value of a product is calculated by multiplying its price by its quantity in stock.) Implement a function named `process_product_data` that takes the filename as input and returns a tuple containing the answers to the above questions. **Function Signature:** ```python def process_product_data(filename: str) -> tuple: # Your implementation here ``` **Expected Input and Output:** - **Input:** A string representing the filename (e.g., `\'data.txt\'`). - **Output:** A tuple containing: - An integer representing the total number of products. - A string representing the name of the product with the highest price. - A float representing the total value of all products in stock. You may assume that the file exists and is properly formatted. **Constraints:** - None of the fields in the file will be empty. - The file will contain at least one product. **Example:** Suppose the file `data.txt` contains the following lines: ``` 1,ProductA,10.99,100 2,ProductB,20.50,150 3,ProductC,15.75,200 ``` When you call `process_product_data(\'data.txt\')`, the function should return: ```python (3, \'ProductB\', 6800.0) ``` **Additional Requirements:** - Your implementation should make use of appropriate built-in functions and classes where relevant. - Ensure the code is well-structured and includes necessary comments to explain your logic. - Consider edge cases and handle them appropriately within your function.","solution":"def process_product_data(filename: str) -> tuple: total_products = 0 highest_price = 0.0 highest_priced_product = \\"\\" total_value = 0.0 with open(filename, \'r\') as file: for line in file: parts = line.strip().split(\',\') if len(parts) != 4: continue # skip malformed lines product_id = int(parts[0]) product_name = parts[1] price = float(parts[2]) quantity_in_stock = int(parts[3]) # Increment the total number of products. total_products += 1 # Check if this product has the highest price so far. if price > highest_price: highest_price = price highest_priced_product = product_name # Accumulate the total value of all products in stock. total_value += price * quantity_in_stock return (total_products, highest_priced_product, total_value)"},{"question":"You are provided with the `seaborn` package for creating aesthetically appealing statistical graphics in Python. This task will assess your understanding of seaborn\'s color palette functionalities. Objective You are to write a function `create_and_plot_palette` that takes four parameters: - `base_color`: a string specifying the base color for the palette. This can be a color name, hex code, or a tuple for husl specifications. - `num_colors`: an integer specifying the number of colors in the palette. - `as_cmap`: a boolean flag that, if set to True, must return a continuous colormap instead of a discrete one. - `plot`: a boolean flag that, if set to True, must create and display a seaborn heatmap using the generated palette. Function Signature ```python def create_and_plot_palette(base_color: Union[str, Tuple[int, int, int]], num_colors: int, as_cmap: bool, plot: bool) -> Union[List, LinearSegmentedColormap]: pass ``` Expected Input and Output - **Input**: - `base_color`: String or tuple. Examples: `\\"seagreen\\"`, `\\"#79C\\"`, `(20, 60, 50)` - `num_colors`: Integer. Example: `8` - `as_cmap`: Boolean. Example: `True` - `plot`: Boolean. Example: `True` - **Output**: - If `as_cmap` is `False`, return a list of colors. - If `as_cmap` is `True`, return a `LinearSegmentedColormap` object. - If `plot` is `True`, display a heatmap using seaborn and the generated palette. Constraints - The number of colors (`num_colors`) should be a positive integer greater than 1. - You may use the seaborn library (imported as `sns`) and assume it has been correctly set up with the theme initialized. Example Implementation ```python import seaborn as sns import matplotlib.pyplot as plt from typing import Union, List, Tuple from matplotlib.colors import LinearSegmentedColormap def create_and_plot_palette(base_color: Union[str, Tuple[int, int, int]], num_colors: int, as_cmap: bool, plot: bool) -> Union[List, LinearSegmentedColormap]: palette = sns.light_palette(base_color, n_colors=num_colors, as_cmap=as_cmap) if plot: data = [[i for i in range(num_colors)] for _ in range(10)] sns.heatmap(data, cmap=palette, cbar=False) plt.show() return palette ``` # Notes - Consider scenarios where the function should create different types of color palettes and use them appropriately. - Ensure the seaborn theme is set at the beginning of your script using `sns.set_theme()`. - Your implementation will be tested against various inputs to check for correctness and robustness.","solution":"import seaborn as sns import matplotlib.pyplot as plt from typing import Union, List, Tuple from matplotlib.colors import LinearSegmentedColormap def create_and_plot_palette(base_color: Union[str, Tuple[int, int, int]], num_colors: int, as_cmap: bool, plot: bool) -> Union[List, LinearSegmentedColormap]: Creates a color palette based on the base color and number of colors requested. If as_cmap is True, returns a continuous colormap. If plot is True, displays a seaborn heatmap using the generated palette. Parameters: - base_color: A string or tuple representing the base color. - num_colors: Number of colors to generate. - as_cmap: Boolean indicating whether to return a colormap. - plot: Boolean indicating whether to plot the generated palette. Returns: - A list of colors or a LinearSegmentedColormap. sns.set_theme() # Ensure seaborn theme is set if as_cmap: palette = sns.light_palette(base_color, n_colors=num_colors, as_cmap=True) else: palette = sns.light_palette(base_color, n_colors=num_colors) if plot: data = [[i for i in range(num_colors)] for _ in range(10)] sns.heatmap(data, cmap=palette if as_cmap else sns.color_palette(palette, num_colors), cbar=False) plt.show() return palette"},{"question":"# Network Server with Custom Request Handler and Asynchronous Processing Objective: Implement a custom multi-threaded TCP server using the `socketserver` module. The server should accept connections from multiple clients and process a specific type of request asynchronously. Each request will be received, processed, and a response will be sent back to the client. Requirements: 1. **Custom Request Handler**: Create a custom request handler by subclassing `socketserver.BaseRequestHandler`. 2. **Threaded Server**: Implement the server to handle multiple client requests concurrently using `ThreadingMixIn`. 3. **Request Handling Logic**: The server should expect the client to send a command in the format `\\"ADD <number1> <number2>\\"`. The server should parse this command, compute the sum of the numbers, and send the result back to the client. 4. **Server Setup**: The server should run indefinitely, handling incoming client requests. Input: - The client will send a command in the format of a string: `\\"ADD <number1> <number2>\\"`. Output: - The server must respond with the sum of the two numbers, formatted as a string. - If the command is malformed, respond with an error message: `\\"ERROR: Invalid command\\"`. Constraints: - Both `number1` and `number2` will be valid integers. - Implement proper exception handling to manage unexpected errors. Performance: - The server should handle multiple clients concurrently without performance degradation due to blocking operations. Example: **Client Request:** ``` \\"ADD 5 10\\" ``` **Server Response:** ``` \\"15\\" ``` **Client Request:** ``` \\"ADD 3 7\\" ``` **Server Response:** ``` \\"10\\" ``` **Skeleton Code:** ```python import socketserver import threading class CustomRequestHandler(socketserver.BaseRequestHandler): def handle(self): try: # Receive client request self.data = self.request.recv(1024).strip().decode(\'utf-8\') command_parts = self.data.split() if len(command_parts) == 3 and command_parts[0] == \\"ADD\\": # Parse and process the numbers number1 = int(command_parts[1]) number2 = int(command_parts[2]) result = number1 + number2 self.request.sendall(f\\"{result}\\".encode(\'utf-8\')) else: self.request.sendall(\\"ERROR: Invalid command\\".encode(\'utf-8\')) except Exception as e: self.request.sendall(f\\"ERROR: {str(e)}\\".encode(\'utf-8\')) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 # Setting up the server with ThreadedTCPServer((HOST, PORT), CustomRequestHandler) as server: print(\\"Server started at {}:{}\\".format(HOST, PORT)) server.serve_forever() ``` *Implement the provided skeleton code and make sure it handles the mentioned requirements to create a robust multi-threaded TCP server.*","solution":"import socketserver import threading class CustomRequestHandler(socketserver.BaseRequestHandler): def handle(self): try: # Receive client request self.data = self.request.recv(1024).strip().decode(\'utf-8\') command_parts = self.data.split() if len(command_parts) == 3 and command_parts[0] == \\"ADD\\": # Parse and process the numbers number1 = int(command_parts[1]) number2 = int(command_parts[2]) result = number1 + number2 self.request.sendall(f\\"{result}\\".encode(\'utf-8\')) else: self.request.sendall(\\"ERROR: Invalid command\\".encode(\'utf-8\')) except Exception as e: self.request.sendall(f\\"ERROR: {str(e)}\\".encode(\'utf-8\')) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 # Setting up the server with ThreadedTCPServer((HOST, PORT), CustomRequestHandler) as server: print(\\"Server started at {}:{}\\".format(HOST, PORT)) server.serve_forever()"},{"question":"Implement a Python command-line application using the `argparse` module that behaves as described below. # Objective: Create a script named `task_manager.py` that manages a list of tasks. The script should support the following functionalities: 1. Add a new task with a description and optional due date. 2. List all tasks with their index, description, and due date (if any). 3. Remove a task by its index. 4. Mark a task as completed by its index. 5. Filter tasks based on their completion status (completed or pending). # Requirements: 1. **Adding a Task** - Command: `task_manager.py add \\"Task Description\\" [--due YY-MM-DD]` - The task description is a required positional argument. - The due date is an optional argument. 2. **Listing Tasks** - Command: `task_manager.py list [--completed | --pending]` - The user can filter tasks using the `--completed` or `--pending` flags. 3. **Removing a Task** - Command: `task_manager.py remove INDEX` - The INDEX is the position of the task in the list (starting from 1). 4. **Marking a Task as Completed** - Command: `task_manager.py complete INDEX` - The INDEX is the position of the task in the list (starting from 1). # Constraints: - The tasks must be stored in memory. - The operations on tasks should be reflected immediately. - Implement mutual exclusion for `--completed` and `--pending` options in the `list` command. - Ensure that invalid indices are handled gracefully with appropriate error messages. # Expected Behavior: - Adding a Task: ```sh python task_manager.py add \\"Buy groceries\\" --due 23-12-2023 ``` - Listing Tasks: ```sh python task_manager.py list --completed ``` - Removing a Task: ```sh python task_manager.py remove 3 ``` - Marking a Task as Completed: ```sh python task_manager.py complete 2 ``` # Implementation Write the `task_manager.py` script with the functionality described above. Use the `argparse` module, and ensure that the script responds correctly to different command-line inputs. Handle errors appropriately and provide clear help messages. # Hints: - Use a dictionary or a list to manage tasks. - Use subparsers to manage the different commands (`add`, `list`, `remove`, `complete`). - Implement error handling for invalid inputs. ```python # The script starts here import argparse # Define the main function def main(): parser = argparse.ArgumentParser(prog=\'task_manager\', description=\'Task manager application.\') subparsers = parser.add_subparsers(dest=\'command\', required=True) # Subparser for adding a task add_parser = subparsers.add_parser(\'add\', help=\'Add a new task\') add_parser.add_argument(\'description\', type=str, help=\'Description of the task\') add_parser.add_argument(\'--due\', type=str, help=\'Due date of the task in YY-MM-DD format\') # Subparser for listing tasks list_parser = subparsers.add_parser(\'list\', help=\'List all tasks\') list_group = list_parser.add_mutually_exclusive_group(required=False) list_group.add_argument(\'--completed\', action=\'store_true\', help=\'List only completed tasks\') list_group.add_argument(\'--pending\', action=\'store_true\', help=\'List only pending tasks\') # Subparser for removing a task remove_parser = subparsers.add_parser(\'remove\', help=\'Remove an existing task\') remove_parser.add_argument(\'index\', type=int, help=\'Index of the task to remove\') # Subparser for completing a task complete_parser = subparsers.add_parser(\'complete\', help=\'Mark a task as completed\') complete_parser.add_argument(\'index\', type=int, help=\'Index of the task to mark as completed\') # Parse the arguments args = parser.parse_args() # Here, you will write the logic to manage tasks if args.command == \'add\': # Logic for adding a task pass elif args.command == \'list\': # Logic for listing tasks pass elif args.command == \'remove\': # Logic for removing a task pass elif args.command == \'complete\': # Logic for marking a task as completed pass # Call the main function if __name__ == \'__main__\': main() ``` Complete the script by implementing the logic for each command inside the appropriate conditionals.","solution":"import argparse tasks = [] def add_task(description, due=None): tasks.append({\'description\': description, \'due\': due, \'completed\': False}) def list_tasks(completed=None, pending=None): filtered_tasks = tasks if completed: filtered_tasks = [task for task in tasks if task[\'completed\']] elif pending: filtered_tasks = [task for task in tasks if not task[\'completed\']] for i, task in enumerate(filtered_tasks, start=1): status = \'Completed\' if task[\'completed\'] else \'Pending\' due_date = task[\'due\'] if task[\'due\'] else \'No due date\' print(f\\"{i}. {task[\'description\']} (Due: {due_date}) - {status}\\") def remove_task(index): try: del tasks[index - 1] except IndexError: print(f\\"Error: Task at index {index} does not exist.\\") def complete_task(index): try: tasks[index - 1][\'completed\'] = True except IndexError: print(f\\"Error: Task at index {index} does not exist.\\") def main(): parser = argparse.ArgumentParser(prog=\'task_manager\', description=\'Task manager application.\') subparsers = parser.add_subparsers(dest=\'command\', required=True) # Subparser for adding a task add_parser = subparsers.add_parser(\'add\', help=\'Add a new task\') add_parser.add_argument(\'description\', type=str, help=\'Description of the task\') add_parser.add_argument(\'--due\', type=str, help=\'Due date of the task in YY-MM-DD format\') # Subparser for listing tasks list_parser = subparsers.add_parser(\'list\', help=\'List all tasks\') list_group = list_parser.add_mutually_exclusive_group(required=False) list_group.add_argument(\'--completed\', action=\'store_true\', help=\'List only completed tasks\') list_group.add_argument(\'--pending\', action=\'store_true\', help=\'List only pending tasks\') # Subparser for removing a task remove_parser = subparsers.add_parser(\'remove\', help=\'Remove an existing task\') remove_parser.add_argument(\'index\', type=int, help=\'Index of the task to remove\') # Subparser for completing a task complete_parser = subparsers.add_parser(\'complete\', help=\'Mark a task as completed\') complete_parser.add_argument(\'index\', type=int, help=\'Index of the task to mark as completed\') # Parse the arguments args = parser.parse_args() # Manage tasks based on command if args.command == \'add\': add_task(args.description, args.due) elif args.command == \'list\': list_tasks(args.completed, args.pending) elif args.command == \'remove\': remove_task(args.index) elif args.command == \'complete\': complete_task(args.index) # Call the main function if __name__ == \'__main__\': main()"},{"question":"# Question: Creating and Using GenericAlias for Type Hinting Python 3.9 introduced the `GenericAlias` object, which allows developers to create generic types for advanced type hinting. In this exercise, you are required to demonstrate your understanding of type hinting by creating and using `GenericAlias`. Task 1. Define a function called `create_generic_alias` that uses `GenericAlias` to create a generic type. 2. Use this generic type as a type hint in another function called `process_generic_data`. Detailed Steps 1. **Create the GenericAlias:** - Define a function `create_generic_alias` that accepts two parameters: - `origin`: A class/type that represents the original data type (e.g., `list`, `dict`). - `args`: A tuple representing the type of elements contained within the original data type (e.g., `(int,)`, `(str, float)`). - The function should return a `GenericAlias` object using the provided parameters. 2. **Use GenericAlias to Define Function Type Hints:** - Define a function `process_generic_data` that accepts a single parameter, `data`, which should be type-hinted using the generic type created by `create_generic_alias`. - The `process_generic_data` function should simply return the type of the input data. Example ```python from types import GenericAlias def create_generic_alias(origin, args): return GenericAlias(origin, args) # This would create a GenericAlias for a list of integers generic_list_int = create_generic_alias(list, (int,)) def process_generic_data(data: generic_list_int): return type(data) # Testing the function example_data = [1, 2, 3] print(process_generic_data(example_data)) # Should output: <class \'list\'> ``` Constraints - The `args` parameter should always be a tuple, even if it contains a single element. - You may assume that the input `data` to `process_generic_data` will always conform to the provided type hint during testing. Submission - Implement the `create_generic_alias` function. - Implement the `process_generic_data` function using the generic type created by `create_generic_alias`.","solution":"from types import GenericAlias def create_generic_alias(origin, args): Creates a GenericAlias object for the provided origin and arguments. Parameters: origin (type): The original data type class, such as list or dict. args (tuple): A tuple representing the types of elements contained within the origin type. Returns: GenericAlias: A new GenericAlias object. return GenericAlias(origin, args) # Example usage of creating a generic alias for a list of integers generic_list_int = create_generic_alias(list, (int,)) def process_generic_data(data: generic_list_int): Processes the input data and returns its type. Parameters: data (generic_list_int): The data to process, type hinted as a list of integers. Returns: type: The type of the input data. return type(data)"},{"question":"# URL Manipulation and Analysis Objective Write a Python function `analyze_and_construct_url(base_url: str, relative_url: str, query_params: dict) -> dict` that performs the following tasks: 1. **Parse** the given `base_url` and `relative_url` using the `urlparse` function. 2. **Join** the parsed `base_url` and `relative_url` using `urljoin`. 3. **Parse** the joined URL and **extract** its components. 4. **Add** the given `query_params` to the URL. 5. **Reconstruct** the complete URL with the new query parameters. 6. Return a dictionary with: - \\"joined_url\\": The joined URL without query parameters. - \\"components\\": A dictionary of the URL\'s components (\\"scheme\\", \\"netloc\\", \\"path\\", \\"params\\", \\"query\\", \\"fragment\\"). - \\"final_url\\": The complete URL after adding the query parameters. Constraints - The `base_url` and `relative_url` will be valid URLs. - The `query_params` dictionary will have string keys and values. Function Signature ```python def analyze_and_construct_url(base_url: str, relative_url: str, query_params: dict) -> dict: pass ``` Example ```python base_url = \\"http://example.com/base\\" relative_url = \\"/path/to/resource\\" query_params = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} result = analyze_and_construct_url(base_url, relative_url, query_params) # result should be: # { # \\"joined_url\\": \\"http://example.com/path/to/resource\\", # \\"components\\": { # \\"scheme\\": \\"http\\", # \\"netloc\\": \\"example.com\\", # \\"path\\": \\"/path/to/resource\\", # \\"params\\": \\"\\", # \\"query\\": \\"\\", # \\"fragment\\": \\"\\" # }, # \\"final_url\\": \\"http://example.com/path/to/resource?key1=value1&key2=value2\\" # } ``` Notes - Use `urllib.parse` module functions to handle URL parsing, joining, and encoding. - Ensure that the function accurately extracts and constructs URLs as specified.","solution":"from urllib.parse import urlparse, urljoin, urlencode, parse_qsl, urlunparse, ParseResult def analyze_and_construct_url(base_url: str, relative_url: str, query_params: dict) -> dict: Analyzes and constructs a URL by combining a base URL, a relative URL, and query parameters. Parameters: - base_url: The base part of the URL. - relative_url: The relative path to be appended to the base URL. - query_params: A dictionary of query parameters to be added to the URL. Returns: A dictionary with joined URL, its components, and the final URL with query parameters. # Join the base URL with the relative URL joined_url = urljoin(base_url, relative_url) # Parse the joined URL to extract its components parsed_url = urlparse(joined_url) # Construct the new query string with additional query parameters new_query = urlencode(query_params) # Create a modified ParseResult with the new query new_url_parts = ParseResult( scheme=parsed_url.scheme, netloc=parsed_url.netloc, path=parsed_url.path, params=parsed_url.params, query=new_query, fragment=parsed_url.fragment ) # Reconstruct the final URL from the modified ParseResult final_url = urlunparse(new_url_parts) # Prepare the components dictionary components = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } return { \\"joined_url\\": joined_url, \\"components\\": components, \\"final_url\\": final_url }"},{"question":"# Python Coding Assessment: Automating Built Distribution Creation **Objective:** Write a Python function that automates the process of generating built distributions for a given Python module. **Background:** You need to create built distributions of a Python module. Built distributions include various formats like gzipped tar files, bzipped tar files, zip files, rpm packages, and msi installers. Different platforms may require different formats. **Function Signature:** ```python def create_built_distribution(module_name: str, version: str, formats: list, options: dict) -> None: Automates the creation of built distributions for a Python module. Parameters: - module_name (str): The name of the module to be packaged. - version (str): The version of the module. - formats (list): A list of distribution formats to generate. Examples include \'gztar\', \'zip\', \'rpm\', \'msi\'. - options (dict): A dictionary containing optional parameters for building the distribution. Possible keys include: - \'release\': Release number (default: \'1\'). - \'group\': The group to which this module belongs (default: \'Development/Libraries\'). - \'vendor\': The name of the vendor (default: None). - \'packager\': The packager\'s name (default: None). - \'requires\': List of other modules required. Returns: - None. (The function should print progress and results.) ``` **Requirements:** 1. **Input Formats**: - `module_name`: A string representing the name of the module. - `version`: A string representing the version of the module. - `formats`: A list of strings representing the desired distribution formats. - `options`: A dictionary where keys are predefined build parameters (like \'release\', \'group\', etc.) and values are their corresponding configurations. 2. **Functionality**: - Validate the provided distribution formats. - Generate a setup configuration file (`setup.cfg`) based on given `options`. - Use `os.system` or `subprocess` to run the necessary `setup.py` commands to create distributions. - Ensure the environment is properly set up for the chosen formats (e.g., ensure `rpm` utility is available for building RPMs). - Handle errors gracefully, provide helpful error messages, and ensure clean-up of temporary files if applicable. 3. **Constraints**: - Assume the module source files are in the current working directory. - Support only Unix-like OS and Windows for this task. - Ignore deprecated formats and functionalities. 4. **Example Usage**: ```python module_name = \\"example_module\\" version = \\"1.0.0\\" formats = [\\"gztar\\", \\"zip\\", \\"rpm\\"] options = { \\"release\\": \\"2\\", \\"group\\": \\"Development/Libraries\\", \\"vendor\\": \\"Example Vendor\\", \\"packager\\": \\"John Doe <johndoe@example.com>\\", \\"requires\\": [\\"requests\\", \\"numpy\\"] } create_built_distribution(module_name, version, formats, options) ``` **Expected Behavior**: - The function should create the respective built distributions in the `dist/` directory. - Proper console logging for each step, including command execution, error handling, and completion messages.","solution":"import os import subprocess def create_built_distribution(module_name: str, version: str, formats: list, options: dict) -> None: Automates the creation of built distributions for a Python module. valid_formats = {\'gztar\', \'zip\', \'rpm\', \'msi\'} for fmt in formats: if fmt not in valid_formats: print(f\\"Error: Invalid format \'{fmt}\'\\") return setup_cfg_content = \\"[metadata]n\\" setup_cfg_content += f\\"name = {module_name}n\\" setup_cfg_content += f\\"version = {version}n\\" for key, value in options.items(): if key == \'requires\': setup_cfg_content += f\\"{key} = {\', \'.join(value)}n\\" else: setup_cfg_content += f\\"{key} = {value}n\\" with open(\'setup.cfg\', \'w\') as setup_cfg_file: setup_cfg_file.write(setup_cfg_content) for fmt in formats: try: command = f\\"python setup.py bdist --formats={fmt}\\" print(f\\"Running command: {command}\\") subprocess.check_call(command, shell=True) except subprocess.CalledProcessError as e: print(f\\"Error: Failed to create distribution in {fmt} format. {e}\\") print(f\\"Successfully created {fmt} distribution.\\") os.remove(\'setup.cfg\') print(\\"Cleaned up temporary \'setup.cfg\' file.\\")"},{"question":"# Question: **Objective**: Create a Python function that leverages the `importlib.metadata` module to retrieve and format metadata information for a given package. **Task**: Write a function `get_package_metadata(package_name: str) -> dict` that: 1. Accepts a package name as input. 2. Retrieves the package\'s metadata, including the following fields: Version, Summary, Author, License, and Requires-Python. 3. Formats this metadata into a dictionary with keys: \'version\', \'summary\', \'author\', \'license\', and \'requires_python\'. 4. Handles cases where any of these metadata fields might be missing by setting their value to \\"Not Available\\". 5. Returns the dictionary with the formatted metadata. **Input**: - `package_name` (string): The name of the package for which metadata is to be retrieved. **Output**: - Returns a dictionary with the package metadata, e.g., ```python { \'version\': \'0.32.3\', \'summary\': \'This is a sample package\', \'author\': \'Author Name\', \'license\': \'MIT\', \'requires_python\': \'>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\' } ``` **Constraints**: - The function should handle exceptions gracefully if the package is not found or if metadata cannot be retrieved. **Example**: ```python from your_module_name import get_package_metadata package_name = \\"wheel\\" metadata = get_package_metadata(package_name) print(metadata) ``` **Hints**: - Review the `importlib.metadata` documentation to understand relevant methods like `metadata` and `version`. - Use exception handling to cover cases where metadata might be incomplete or not found. Good luck!","solution":"import importlib.metadata def get_package_metadata(package_name: str) -> dict: try: metadata = importlib.metadata.metadata(package_name) return { \'version\': importlib.metadata.version(package_name), \'summary\': metadata.get(\'Summary\', \'Not Available\'), \'author\': metadata.get(\'Author\', \'Not Available\'), \'license\': metadata.get(\'License\', \'Not Available\'), \'requires_python\': metadata.get(\'Requires-Python\', \'Not Available\') } except importlib.metadata.PackageNotFoundError: return { \'version\': \'Not Available\', \'summary\': \'Not Available\', \'author\': \'Not Available\', \'license\': \'Not Available\', \'requires_python\': \'Not Available\' }"},{"question":"**Title:** Advanced Expression Evaluator **Description:** You are tasked with writing a Python function `evaluate_expression` that evaluates a list of mixed expressions and returns the results. **Requirements:** 1. The function should take a single argument, `expressions`, a list of strings, where each string is a valid Python expression. 2. The function must correctly handle various expression types, including but not limited to: - Arithmetic operations (e.g., `3 + 4`, `5 * 2`) - List, set, and dictionary comprehensions (e.g., `[x*2 for x in range(5)]`, `{x: x**2 for x in range(5)}`) - Generator expressions (e.g., `(x*2 for x in range(5))`) - Yield expressions within a generator function - Attribute references (e.g., `\\"hello\\".upper`) - Function calls (e.g., `sum([1, 2, 3])`) 3. Ensure `evaluate_expression` captures the return values of these expressions correctly. If an expression raises an error, capture the error message instead of the expression result. **Constraints:** - Avoid using `eval()` for evaluating expressions to ensure security. - Ensure performance is maintained for a list of up to 1000 expressions. **Function Signature:** ```python def evaluate_expression(expressions: List[str]) -> List[Union[Any, str]]: pass ``` **Input:** - `expressions`: A list of strings representing valid Python expressions. (1 <= len(expressions) <= 1000) **Output:** - A list containing the evaluated results of each expression. If an expression raises an error, include the error message instead of the result for that particular expression. **Examples:** ```python expressions = [ \\"3 + 4\\", \\"[x*2 for x in range(5)]\\", \\"(x*2 for x in range(5))\\", \\"sum([1, 2, 3])\\", \'\\"hello\\".upper()\', \\"1 / 0\\" # This should capture the division by zero error ] print(evaluate_expression(expressions)) ``` **Expected Output:** ```python [7, [0, 2, 4, 6, 8], <generator object <genexpr> at 0x...>, 6, \\"HELLO\\", \\"division by zero\\"] ``` **Note:** - Ensure to handle the generator expressions correctly in the output without forcing their evaluation if not needed.","solution":"from typing import List, Union, Any def evaluate_expression(expressions: List[str]) -> List[Union[Any, str]]: results = [] for expression in expressions: try: # Evaluating in a local, safe namespace to avoid security issues local_namespace = {} result = eval(compile(expression, \'<string>\', \'eval\'), {}, local_namespace) results.append(result) except Exception as e: results.append(str(e)) return results"},{"question":"**Coding Assessment Question** # Distributed Training with FullyShardedDataParallel in PyTorch In this assessment, you are required to implement a distributed training loop using `torch.distributed.fsdp.FullyShardedDataParallel` to train a simple neural network on multiple devices. You will need to: 1. Define a simple neural network model. 2. Setup the distributed environment and split the model across multiple devices using `FullyShardedDataParallel`. 3. Implement the training loop with gradient accumulation and checkpointing. 4. Configure and manage state dictionaries for saving the model. # Requirements 1. You should use the PyTorch framework with the `torch.distributed.fsdp` package. 2. Define a `Net` class that inherits from `torch.nn.Module`. 3. Setup the distributed training environment (`torch.distributed.init_process_group`). 4. Wrap your model using `FullyShardedDataParallel`. 5. Implement the training loop with a specified number of epochs. 6. Save the model\'s state dictionary at the end of training using the provided state dict configurations. # Input and Output - **Input**: None directly. - **Output**: The code should output the training loss for each epoch and save the model’s state dictionary to a file. # Constraints and Performance Requirements - You must configure at least two different devices (e.g., `cuda:0` and `cuda:1`) if available. - Ensure that your training loop handles synchronization and gradient reduction properly to avoid issues in multi-GPU training. - Save the model’s state dictionary using `torch.distributed.fsdp.FullStateDictConfig` and verify it by loading back the state dictionary after training. ```python import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.distributed.fsdp import FullyShardedDataParallel as FSDP from torch.distributed.fsdp import FullStateDictConfig # Step 1: Define a simple neural network model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(rank, world_size): # Step 2: Setup distributed environment dist.init_process_group(backend=\'nccl\', rank=rank, world_size=world_size) device = torch.device(f\'cuda:{rank}\') torch.cuda.set_device(device) # Step 3: Create model and move it to device model = Net().to(device) model = FSDP(model) # Step 4: Define loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock some data inputs = torch.randn(64, 784).to(device) targets = torch.randint(0, 10, (64,)).to(device) # Step 5: Training loop num_epochs = 5 for epoch in range(num_epochs): outputs = model(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() print(f\'Rank {rank}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\') # Step 6: Save model\'s state dictionary state_dict = model.state_dict(config=FullStateDictConfig()) if rank == 0: # Saving should be done by a single process to avoid file corruption torch.save(state_dict, \\"model_state_dict.pth\\") # Destroy the process group dist.destroy_process_group() def main(): world_size = torch.cuda.device_count() dist.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main() ``` # Instructions - Make sure your implementation appropriately handles the distribution of the model and the synchronization of gradients. - Verify your saved model by reloading the state dictionary and ensuring consistency.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.distributed.fsdp import FullyShardedDataParallel as FSDP from torch.distributed.fsdp import FullStateDictConfig import os # Step 1: Define a simple neural network model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(784, 128) self.relu = nn.ReLU() self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(rank, world_size): # Step 2: Setup distributed environment dist.init_process_group(backend=\'nccl\', rank=rank, world_size=world_size) device = torch.device(f\'cuda:{rank}\') torch.cuda.set_device(device) # Step 3: Create model and move it to device model = Net().to(device) model = FSDP(model, device_id=device) # Step 4: Define loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock some data inputs = torch.randn(64, 784).to(device) targets = torch.randint(0, 10, (64,)).to(device) # Step 5: Training loop num_epochs = 5 for epoch in range(num_epochs): outputs = model(inputs) loss = criterion(outputs, targets) optimizer.zero_grad() loss.backward() optimizer.step() print(f\'Rank {rank}, Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\') # Step 6: Save model\'s state dictionary state_dict = model.state_dict(config=FullStateDictConfig()) if rank == 0: # Saving should be done by a single process to avoid file corruption torch.save(state_dict, \\"model_state_dict.pth\\") # Destroy the process group dist.destroy_process_group() def main(): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' world_size = torch.cuda.device_count() dist.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with implementing a Python function that leverages the buffer protocol to perform operations directly on the raw data of buffer objects. # Function Details: Write a function `scale_buffer(source_obj: object, scale_factor: float) -> bytes` that accepts two parameters: - `source_obj`: An object that supports the buffer interface (e.g., `bytes`, `bytearray`, `array.array`). - `scale_factor`: A float that represents the factor by which every element in the buffer should be multiplied. # Implementation Details: 1. **Obtain the Buffer**: Use the buffer interface to obtain a read-write buffer from `source_obj`. 2. **Scale Elements**: Iterate through the buffer and multiply each element by `scale_factor`. Note that the elements should be interpreted as double-precision floating-point numbers (`float` in Python). 3. **Return the Result**: Construct and return a new `bytes` object containing the scaled values as raw bytes. # Constraints: - The `source_obj` will be a contiguous array of double-precision floating-point numbers. - The buffer must be obtained with read-write access. - The function should handle errors gracefully and raise appropriate exceptions if the buffer cannot be accessed or modified. # Example: ```python source = array.array(\'d\', [1.0, 2.0, 3.0, 4.0]) result = scale_buffer(source, 2.0) print(result) # Output: The scaled values in a bytes object, which would correspond to byte representation of [2.0, 4.0, 6.0, 8.0] ``` # Constraints: - Do not use any additional libraries except for Python standard libraries. - Ensure proper memory management and release the buffer when done. # Notes: - The function should use the `PyBuffer_Release()` function after the buffer operations are complete to avoid any resource leaks. - You may use the `struct` module to help convert data between bytes and floating-point representations. # Additional Information: The `array` module should be imported and used for array manipulations. Ensure that all buffer interface requirements are handled as per the documentation provided.","solution":"import array import struct def scale_buffer(source_obj: object, scale_factor: float) -> bytes: Scales each element in the buffer by the given scale factor and returns the result as bytes. :param source_obj: Object that supports the buffer interface (e.g., bytes, bytearray, array.array). :param scale_factor: A float that represents the factor by which every element in the buffer should be multiplied. :return: A new bytes object containing the scaled values as raw bytes. if not isinstance(source_obj, (bytes, bytearray, array.array)): raise TypeError(\\"source_obj must be an object that supports the buffer interface.\\") if isinstance(source_obj, array.array): if source_obj.typecode != \'d\': raise ValueError(\\"source_obj must be an array of double precision floats (\'d\').\\") else: raise ValueError(\\"Unsupported source_obj type. Use array of doubles.\\") # Buffer access and scaling buffer = memoryview(source_obj) if buffer.format != \'d\': raise ValueError(\\"Buffer elements are not double-precision floats.\\") scaled_data = array.array(\'d\', (x * scale_factor for x in buffer)) return scaled_data.tobytes()"},{"question":"**Coding Question:** Implement a Python function to retrieve and display detailed information about a Unix user based on their username. Your function should utilize the `pwd` module to achieve this. **Function Signature:** ```python def get_user_info(username: str) -> str: pass ``` **Input:** - `username` (str): The username of the Unix user whose information is to be retrieved. **Output:** - (str): A formatted string containing the detailed information about the Unix user. If the username does not exist, return a message indicating that the user was not found. **Constraints:** - The function should handle the scenario where the user with the given username does not exist by catching appropriate exceptions. **Performance Requirements:** - The function should make efficient use of the `pwd` module to retrieve the necessary information. **Example:** ```python user_info = get_user_info(\\"john\\") print(user_info) ``` If the user \\"john\\" exists, the output should be: ``` User Information: Login Name: john Enc. Password: * User ID: 1001 Group ID: 1001 User Comment: John Doe Home Directory: /home/john Shell: /bin/bash ``` If the user \\"john\\" does not exist, the output should be: ``` User \'john\' not found. ``` **Notes:** - To test the function, you may need to run it on a Unix-based system where the `pwd` module can access the user accounts. - Include necessary exception handling to manage errors when querying user information. **Hints:** - Use the `pwd.getpwnam` function to retrieve user information by username. - Format the output string as shown in the example to ensure clarity and readability.","solution":"import pwd def get_user_info(username: str) -> str: Retrieve and display detailed information about a Unix user based on their username. try: user_info = pwd.getpwnam(username) except KeyError: return f\\"User \'{username}\' not found.\\" return ( f\\"User Information:n\\" f\\" Login Name: {user_info.pw_name}n\\" f\\" Enc. Password: {user_info.pw_passwd}n\\" f\\" User ID: {user_info.pw_uid}n\\" f\\" Group ID: {user_info.pw_gid}n\\" f\\" User Comment: {user_info.pw_gecos}n\\" f\\" Home Directory: {user_info.pw_dir}n\\" f\\" Shell: {user_info.pw_shell}\\" )"},{"question":"Comprehensive Logging System Objective Create a logging system for a multi-threaded application that ensures: 1. Configurable logging levels. 2. Thread-safe logging. 3. Customized log message formatting and filtering. Requirements 1. **Logger Setup**: - Create a `Logger` named `\'myAppLogger\'`. - Set its logging level to `INFO`. - Ensure that the logger propagates messages to ancestor loggers. 2. **Handler Configuration**: - Add two handlers to the logger: - A `StreamHandler` that logs to console with a `DEBUG` level and a simple message format `[%(levelname)s] %(message)s`. - A `FileHandler` named `\'app.log\'` that logs to a file with `ERROR` level, formatted as `%(asctime)s - [%(levelname)s] %(message)s`. 3. **Custom Formatter**: - Create a custom `Formatter` that translates log levels to French: - `DEBUG` -> `DÉBOGUER` - `INFO` -> `INFORMATION` - `WARNING` -> `AVERTISSEMENT` - `ERROR` -> `ERREUR` - `CRITICAL` -> `CRITIQUE` - Ensure this formatter is used in the `FileHandler`. 4. **Filter Integration**: - Implement a custom `Filter` that allows only log messages containing the word \\"critical\\" for the `StreamHandler`. 5. **Multithreading Support**: - Write a thread-safe function `log_messages_in_threads()` that: - Starts 5 threads. - Each thread logs messages at varying levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). Input None Output A `myAppLogger` instance configured as specified, and logs generated in a multi-threaded context. Constraints - Use only standard library modules. - The function `log_messages_in_threads()` must create and start the threads. Example Usage ```python if __name__ == \\"__main__\\": logger = configure_logging() log_messages_in_threads(logger) ``` Steps to Implement 1. **Logger Setup**: - Use `logging.getLogger(\'myAppLogger\')` to create the logger. - Set the logger level to `INFO` and enable propagation. 2. **Handler Configuration**: - Create a `StreamHandler` and set its level to `DEBUG`. Attach it to the logger. - Create a `FileHandler` set to log to `\'app.log\'` with level `ERROR`. Attach it to the logger. 3. **Custom Formatter**: - Implement a `FrenchFormatter` subclass of `logging.Formatter` that translates log levels to French. Set this formatter to the `FileHandler`. 4. **Filter Integration**: - Implement a `CriticalFilter` subclass of `logging.Filter` that filters in only messages containing \\"critical\\". Attach it to the `StreamHandler`. 5. **Multithreading Support**: - Define `log_messages_in_threads(logger)`, and use the `threading` module to create and start threads. ```python import logging import threading import time class FrenchFormatter(logging.Formatter): LEVEL_TRANSLATION = { logging.DEBUG: \'DÉBOGUER\', logging.INFO: \'INFORMATION\', logging.WARNING: \'AVERTISSEMENT\', logging.ERROR: \'ERREUR\', logging.CRITICAL: \'CRITIQUE\' } def format(self, record): record.levelname = self.LEVEL_TRANSLATION.get(record.levelno, record.levelname) return super().format(record) class CriticalFilter(logging.Filter): def filter(self, record): return \'critical\' in record.msg def configure_logging(): logger = logging.getLogger(\'myAppLogger\') logger.setLevel(logging.INFO) logger.propagate = True console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) console_formatter = logging.Formatter(\'[%(levelname)s] %(message)s\') console_handler.setFormatter(console_formatter) console_handler.addFilter(CriticalFilter()) file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.ERROR) file_formatter = FrenchFormatter(\'%(asctime)s - [%(levelname)s] %(message)s\') file_handler.setFormatter(file_formatter) logger.addHandler(console_handler) logger.addHandler(file_handler) return logger def log_messages_in_threads(logger): def log_in_thread(level, msg): logger.log(level, msg) threads = [] for i in range(5): threads.append(threading.Thread(target=log_in_thread, args=(logging.DEBUG, f\'Test debug {i}\'))) threads.append(threading.Thread(target=log_in_thread, args=(logging.INFO, f\'Test info {i}\'))) threads.append(threading.Thread(target=log_in_thread, args=(logging.WARNING, f\'Test warning {i}\'))) threads.append(threading.Thread(target=log_in_thread, args=(logging.ERROR, f\'Critical error {i}\'))) threads.append(threading.Thread(target=log_in_thread, args=(logging.CRITICAL, f\'Critical error {i}\'))) for t in threads: t.start() for t in threads: t.join() if __name__ == \\"__main__\\": logger = configure_logging() log_messages_in_threads(logger) ``` Evaluation Criteria - Correct implementation of logger configuration. - Proper handling of multi-threaded logging. - Proper usage of custom formatters and filters. - Quality and clarity of code.","solution":"import logging import threading class FrenchFormatter(logging.Formatter): LEVEL_TRANSLATION = { logging.DEBUG: \'DÉBOGUER\', logging.INFO: \'INFORMATION\', logging.WARNING: \'AVERTISSEMENT\', logging.ERROR: \'ERREUR\', logging.CRITICAL: \'CRITIQUE\' } def format(self, record): record.levelname = self.LEVEL_TRANSLATION.get(record.levelno, record.levelname) return super().format(record) class CriticalFilter(logging.Filter): def filter(self, record): return \'critical\' in record.msg def configure_logging(): logger = logging.getLogger(\'myAppLogger\') logger.setLevel(logging.INFO) logger.propagate = True console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) console_formatter = logging.Formatter(\'[%(levelname)s] %(message)s\') console_handler.setFormatter(console_formatter) console_handler.addFilter(CriticalFilter()) file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.ERROR) file_formatter = FrenchFormatter(\'%(asctime)s - [%(levelname)s] %(message)s\') file_handler.setFormatter(file_formatter) logger.addHandler(console_handler) logger.addHandler(file_handler) return logger def log_messages_in_threads(logger): def log_in_thread(level, msg): logger.log(level, msg) threads = [] messages = [ (logging.DEBUG, \'Test debug message\'), (logging.INFO, \'Test info message\'), (logging.WARNING, \'Test warning message\'), (logging.ERROR, \'Test error message\'), (logging.CRITICAL, \'Test critical message\') ] for level, msg in messages: t = threading.Thread(target=log_in_thread, args=(level, msg)) threads.append(t) for t in threads: t.start() for t in threads: t.join() if __name__ == \\"__main__\\": logger = configure_logging() log_messages_in_threads(logger)"},{"question":"# Advanced Coding Assessment **Objective:** To assess the ability to utilize Python\'s introspection capabilities to analyze the current state of the Python interpreter and construct useful runtime diagnostics. **Question:** You are required to implement a function called `runtime_info` that returns a summary of the current execution context, including details about builtins, locals, globals, and the current execution frame. ```python def runtime_info() -> dict: Inspect the current Python interpreter state and return a summary of details. Returns: dict: A dictionary containing the following keys: - \'builtins\': A list of keys available in the current builtins. - \'locals\': A dictionary of the current local variables. - \'globals\': A list of keys available in the current globals. - \'current_frame\': A dictionary containing: - \'line_number\': The current line number being executed. - \'code_name\': The name of the code object being executed. - \'outer_frame_exists\': Boolean indicating if an outer frame exists. # Your implementation goes here pass ``` # Specifications: 1. **Builtins**: - Extract the dictionary of builtins using `PyEval_GetBuiltins()`. - Store only the keys of this dictionary in the \'builtins\' list. 2. **Locals**: - Extract the dictionary of local variables using `PyEval_GetLocals()`. - Store this dictionary in \'locals\'. 3. **Globals**: - Extract the dictionary of global variables using `PyEval_GetGlobals()`. - Store only the keys of this dictionary in the \'globals\' list. 4. **Current Frame**: - Use `PyEval_GetFrame()` to get the current frame. - If a frame is present: - Extract the line number using `PyFrame_GetLineNumber(frame)`. - Extract the code object of the frame using `PyFrame_GetCode(frame)`, then get its name. - Check if there is an outer frame using `PyFrame_GetBack(frame)` and store the result as a boolean in \'outer_frame_exists\'. - Store these details in a \'current_frame\' dictionary with keys \'line_number\', \'code_name\', and \'outer_frame_exists\'. # Constraints: - You are not allowed to use standard Python introspection tools directly like `inspect` module functions for obtaining this information. Rely on the provided/documented function calls. # Example Output: ```python runtime_info() ``` ```python { \'builtins\': [\'abs\', \'all\', \'any\', ...], \'locals\': {}, \'globals\': [\'__name__\', \'__doc__\', ...], \'current_frame\': { \'line_number\': 10, \'code_name\': \'current_code\', \'outer_frame_exists\': False } } ``` # Notes: - Ensure that your function can handle cases where no current frame is available. - Performance considerations: Your implementation should efficiently handle the extraction of required information.","solution":"import sys def runtime_info() -> dict: Inspect the current Python interpreter state and return a summary of details. Returns: dict: A dictionary containing the following keys: - \'builtins\': A list of keys available in the current builtins. - \'locals\': A dictionary of the current local variables. - \'globals\': A list of keys available in the current globals. - \'current_frame\': A dictionary containing: - \'line_number\': The current line number being executed. - \'code_name\': The name of the code object being executed. - \'outer_frame_exists\': Boolean indicating if an outer frame exists. builtins_dict = sys.modules[\'builtins\'].__dict__ locals_dict = locals() globals_dict = globals() current_frame = sys._getframe() result = { \'builtins\': list(builtins_dict.keys()), \'locals\': locals_dict, \'globals\': list(globals_dict.keys()), \'current_frame\': { \'line_number\': current_frame.f_lineno, \'code_name\': current_frame.f_code.co_name, \'outer_frame_exists\': current_frame.f_back is not None } } return result"},{"question":"<|Analysis Begin|> The provided documentation outlines various numeric and mathematical modules in Python, such as `numbers`, `math`, `cmath`, `decimal`, `fractions`, `random`, and `statistics`. These modules encompass a wide array of functions and data types for performing mathematical and numerical operations. Specific topics covered include: - Numeric abstract base classes and implementations in the `numbers` module. - Various mathematical functions, constants, and type-specific operations in the `math` and `cmath` modules. - High precision decimal arithmetic in the `decimal` module. - Rational numbers operations in the `fractions` module. - Pseudo-random number generation in the `random` module. - Statistical functions and measures in the `statistics` module. Given these details, a coding question can be designed focusing on combining multiple functionalities from these modules to assess comprehensive understanding and ability to implement complex logic. <|Analysis End|> <|Question Begin|> **Coding Assessment Question** **Objective:** Demonstrate your comprehension of Python\'s numeric and mathematical modules (`math`, `decimal`, `fractions`, `statistics`, and `random`). You are tasked with implementing a function that performs a series of mathematical transformations using these modules. **Problem Statement:** Write a function `process_numbers(data)` that takes a list of tuples as input. Each tuple contains three elements: a random floating-point number, a rational number (expressed as numerator and denominator), and a decimal number in string format. The function should perform the following tasks: 1. For each floating-point number, generate a random integer between 1 and 10 (inclusive) and multiply it by the floating-point number. 2. Convert the rational number into its decimal equivalent. 3. Convert the decimal number from string format to a `decimal.Decimal` object and calculate its square root. 4. Calculate the mean and standard deviation of all the converted decimal numbers from step 2 and the square roots from step 3 combined. 5. Return the results as a dictionary with keys `multiplied_floats`, `converted_rationals`, `square_roots`, `mean`, and `std_dev`. **Input Format:** - A list of tuples. Each tuple contains: - A floating-point number. - A rational number represented as a tuple `(numerator, denominator)`. - A decimal number in string format. **Output Format:** - A dictionary with the following keys: - `multiplied_floats`: A list of floating-point numbers after multiplication. - `converted_rationals`: A list of converted decimal numbers from the rational inputs. - `square_roots`: A list of square roots of decimal numbers. - `mean`: The mean of the combined list from `converted_rationals` and `square_roots`. - `std_dev`: The standard deviation of the combined list from `converted_rationals` and `square_roots`. **Constraints:** - Use the `random` module to generate random integers. - Use the `fractions.Fraction` class for handling rational numbers. - Use the `decimal.Decimal` class for handling decimal numbers and calculating the square root. - Assume the input list has at least one tuple and has valid values. **Performance Requirements:** - The function should efficiently handle lists containing up to 10,000 tuples. **Example:** ```python def process_numbers(data): # Your implementation here # Example input data = [ (3.14, (1, 2), \'4.0\'), (2.718, (3, 4), \'16.0\') ] # Expected output (the exact values for multiplied_floats will vary due to randomness) output = process_numbers(data) print(output) # { # \\"multiplied_floats\\": [...], # \\"converted_rationals\\": [0.5, 0.75], # \\"square_roots\\": [2.0, 4.0], # \\"mean\\": ..., # \\"std_dev\\": ... # } ``` Ensure your function passes this example and meets the specified constraints and performance requirements.","solution":"import random from fractions import Fraction from decimal import Decimal, getcontext from statistics import mean, stdev from math import sqrt # Set precision for decimal operations getcontext().prec = 28 def process_numbers(data): multiplied_floats = [] converted_rationals = [] square_roots = [] for float_num, (num, den), dec_str in data: # 1. Multiply floating-point number by a random integer between 1 and 10 random_int = random.randint(1, 10) multiplied_floats.append(float_num * random_int) # 2. Convert the rational number into its decimal equivalent rational_decimal = Decimal(num) / Decimal(den) converted_rationals.append(rational_decimal) # 3. Convert string to decimal and compute square root decimal_num = Decimal(dec_str) square_root = decimal_num.sqrt() square_roots.append(square_root) # Combine both settings for mean and standard deviation calculation combined_list = converted_rationals + square_roots # 4. Calculate mean and standard deviation mean_value = mean(combined_list) std_dev_value = stdev(combined_list) return { \\"multiplied_floats\\": multiplied_floats, \\"converted_rationals\\": converted_rationals, \\"square_roots\\": square_roots, \\"mean\\": mean_value, \\"std_dev\\": std_dev_value }"},{"question":"Objective Demonstrate your understanding of Seaborn\'s visualization capabilities by creating a swarm plot with specific customizations. Question Given the following conditions, write a function `create_custom_swarmplot` that creates and saves a custom swarm plot using Seaborn. 1. Load the `tips` dataset using Seaborn. 2. Create a swarm plot where: - The x-axis represents total bill (`total_bill`). - The y-axis represents the day of the week (`day`). - The plot is colored by the gender (`sex`) of the customers. - Use the `deep` palette for the hue variable. - Add a `dodge` parameter to separate data by gender. - Set the size of the points to 4. 3. Save the plot as an image file named `custom_swarmplot.png`. # Function Signature ```python def create_custom_swarmplot(): pass ``` # Important Notes - Ensure that the generated plot meets all specified requirements. - The plot should be saved without displaying it inline to simulate a script run. # Example Usage ```python create_custom_swarmplot() # This should save the plot as `custom_swarmplot.png` in the current working directory. ``` # Constraints - Use only Seaborn and Matplotlib libraries for the visualization. - Do not change the dataset or its contents.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_swarmplot(): # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create the swarm plot plt.figure(figsize=(10, 6)) sns.swarmplot(x=\'total_bill\', y=\'day\', hue=\'sex\', data=tips, palette=\'deep\', dodge=True, size=4) # Save the plot as an image file plt.savefig(\'custom_swarmplot.png\') plt.close()"},{"question":"# Custom Deep Copy Implementation Objective: Implement a custom deep copy functionality for a class using the `copy` module\'s `deepcopy` method. Problem Description: You are provided with a class `ComplexObject` which contains nested mutable attributes such as lists and dictionaries. Your task is to implement custom deepcopy functionality for this class by defining the `__deepcopy__` method. Class Definition: ```python import copy class ComplexObject: def __init__(self, identifier, data): self.identifier = identifier self.data = data # Assume `data` can be a nested combination of lists and dictionaries def __copy__(self): # Shallow copy implementation new_obj = type(self)( self.identifier, self.data, ) return new_obj def __deepcopy__(self, memo): # Your implementation here pass def __str__(self): return f\'ComplexObject(id={self.identifier}, data={self.data})\' ``` Requirements: 1. Implement the `__deepcopy__` method in the `ComplexObject` class. 2. The `__deepcopy__` method should create a deep copy of the `data` attribute, ensuring that changes to the `data` in the copied object do not affect the original. 3. Use the `copy.deepcopy` function where necessary, passing the `memo` dictionary as required. Input and Output: 1. **Input:** A `ComplexObject` instance with nested lists and/or dictionaries. 2. **Output:** A new `ComplexObject` instance with deeply copied `data`. Constraints: 1. The `data` attribute can be a combination of lists and dictionaries, potentially nested to multiple levels. 2. You must handle potential recursive references within `data`. Example: ```python # Sample usage data = { \'key1\': [1, 2, 3], \'key2\': {\'nested_key\': [4, 5, 6]} } obj1 = ComplexObject(1, data) obj2 = copy.deepcopy(obj1) # Change data in obj2, should not affect obj1 obj2.data[\'key1\'].append(4) print(obj1) # Output should show original data without the appended value print(obj2) # Output should show updated data with the appended value ``` Write your implementation for the `__deepcopy__` method of the `ComplexObject` class.","solution":"import copy class ComplexObject: def __init__(self, identifier, data): self.identifier = identifier self.data = data # Assume `data` can be a nested combination of lists and dictionaries def __copy__(self): # Shallow copy implementation new_obj = type(self)( self.identifier, self.data, ) return new_obj def __deepcopy__(self, memo): # Check if object is already in memo if id(self) in memo: return memo[id(self)] # Create the deepcopy of the identifier identifier_copy = copy.deepcopy(self.identifier, memo) # Create the deepcopy of the data data_copy = copy.deepcopy(self.data, memo) # Create the new deep copied object new_obj = type(self)( identifier_copy, data_copy, ) # Save new object in memo memo[id(self)] = new_obj return new_obj def __str__(self): return f\'ComplexObject(id={self.identifier}, data={self.data})\'"},{"question":"**Coding Assessment Question** **Objective**: Design a code that demonstrates a deep understanding of seaborn, specifically the `seaborn.objects` module. Your task is to create a visualization with multiple customization features as described below. **Problem**: You are provided with a dataset containing information about various machine learning models and their performance on different tasks. You need to create a visualization to compare these models effectively. **Dataset Description**: - The dataset contains columns: `\'Model\'`, `\'Encoder\'`, `\'Task\'`, and `\'Score\'`. - You will load the dataset using `seaborn.load_dataset(\'glue\')`. **Task**: 1. Load the dataset and transform it to have models in rows and tasks in columns. Calculate the average score for each model and add it as a column. 2. Create the following visualizations using `seaborn.objects`: - A scatter plot comparing the scores of two specific tasks (`\'SST-2\'` and `\'MRPC\'`) with the model names annotated. - A bar plot showing the average score of each model, with the average score annotated on the bars and aligned to the right. - A dot plot with text annotations comparing two other tasks (`\'RTE\'` and `\'MRPC\'`) while differentiating models based on the `\'Encoder\'` type. Ensure the text is colored based on the encoder type and aligned according to a custom scale. 3. Apply additional customization to the text annotations: - For the scatter plot, ensure the model names are displayed above the dots. - For the bar plot, add a small offset to the text annotations. - For the dot plot, make the text bold and align it differently depending on the encoder type (align \'LSTM\' to the left and \'Transformer\' to the right). **Constraints**: - You must use the seaborn `objects` module. - Ensure the plots are well-labeled and easy to interpret. **Input and Output Format**: - **Input**: The script will directly load the dataset using seaborn. No external input is required. - **Output**: Display the plots using a Jupyter notebook or any other suitable environment. **Example**: ```python import seaborn.objects as so from seaborn import load_dataset # Load and transform the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Scatter plot ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) # Bar plot ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Dot plot ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text({\\"fontweight\\": \\"bold\\"}, halign=\\"Encoder\\")) .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) ) ``` Ensure your code adheres to these specifications and properly displays the plots.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load and transform the dataset glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Scatter plot scatter_plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) # Bar plot bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=6)) ) # Dot plot dot_plot = ( so.Plot(glue, x=\\"RTE\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text({\\"fontweight\\": \\"bold\\"}, halign=\\"Encoder\\")) .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) )"},{"question":"**Title: Implementing Integer Conversion with Error Handling** **Problem Statement:** You are given a set of functions in the Python C API that convert various C types to Python integer objects (`PyLongObject`) and vice versa. Your task is to implement a Python function `convert_and_validate` that takes an input string representing a number in base 10 and performs the following steps: 1. Convert the string to a Python integer object (`PyLongObject`). 2. Convert the Python integer object back to various C types (`long`, `unsigned long`, `long long`, `double`). 3. Validate that the conversions are successful and handle any potential errors such as overflows or invalid values. 4. Return the converted values as a dictionary with appropriate error messages if conversions fail. **Function Signature:** ```python def convert_and_validate(number_str: str) -> dict: # Your implementation here ``` **Input Format:** - `number_str` (str): A string representing the number to be converted. The number will be in base 10 and can include negative numbers. **Output Format:** - Returns a dictionary containing: - \'PyLongObject\': The original Python integer object. - \'as_long\': The converted \'long\' value or an error message if conversion fails. - \'as_unsigned_long\': The converted \'unsigned long\' value or an error message if conversion fails. - \'as_long_long\': The converted \'long long\' value or an error message if conversion fails. - \'as_double\': The converted \'double\' value or an error message if conversion fails. **Constraints:** - The input string will always be a valid integer representation (positive, negative, or zero). - Handle possible overflows or range issues appropriately. **Example:** ```python # Example input number_str = \\"123456789123456789\\" result = convert_and_validate(number_str) # Example output { \'PyLongObject\': int(123456789123456789), \'as_long\': \'OverflowError\', \'as_unsigned_long\': \'OverflowError\', \'as_long_long\': 123456789123456789, \'as_double\': 1.234567891234568e+17 } ``` **Explanation:** - The function first converts the input string to a Python integer. - It then attempts to convert this integer to various C types. - If a conversion fails due to an overflow or range issue, it records an appropriate error message (\'OverflowError\'). - Finally, it returns a dictionary with all the conversion results. **Note:** - You can use the Python built-in `int()` for creating the Python integer object. - For other conversions (`long`, `unsigned long`, `double`), simulate the behavior as described using Python\'s capabilities since direct access to Python C API is not possible in this context.","solution":"def convert_and_validate(number_str: str) -> dict: result = {} try: py_long_obj = int(number_str) result[\'PyLongObject\'] = py_long_obj except ValueError as e: result[\'PyLongObject\'] = str(e) return result try: as_long = py_long_obj if -2**31 <= py_long_obj <= 2**31 - 1 else \\"OverflowError\\" except OverflowError as e: as_long = \\"OverflowError\\" result[\'as_long\'] = as_long try: as_unsigned_long = py_long_obj if 0 <= py_long_obj <= 2**32 - 1 else \\"OverflowError\\" except OverflowError as e: as_unsigned_long = \\"OverflowError\\" result[\'as_unsigned_long\'] = as_unsigned_long try: as_long_long = py_long_obj if -2**63 <= py_long_obj <= 2**63 - 1 else \\"OverflowError\\" except OverflowError as e: as_long_long = \\"OverflowError\\" result[\'as_long_long\'] = as_long_long try: as_double = float(py_long_obj) except OverflowError as e: as_double = \\"OverflowError\\" result[\'as_double\'] = as_double return result"},{"question":"**Title**: Airline Delays Analysis **Problem Statement**: You have been provided with a dataset representing flight delays from various airlines. The dataset contains the following information: 1. `FlightNum`: Unique flight number. 2. `Airline`: Airline code. 3. `Origin`: Origin airport code. 4. `Destination`: Destination airport code. 5. `Distance`: Distance between origin and destination (in miles). 6. `Delay`: Delay time (in minutes, NaN if no delay). The task is to perform various analyses on this dataset using pandas. **Function Signature**: ```python import pandas as pd def analyze_flight_delays(df: pd.DataFrame) -> pd.DataFrame: Analyze flight delays from the given DataFrame and return a summary DataFrame. Args: df (pd.DataFrame): Input DataFrame with columns [\'FlightNum\', \'Airline\', \'Origin\', \'Destination\', \'Distance\', \'Delay\']. Returns: pd.DataFrame: Summary DataFrame with the following columns: - \'Airline\': Airline code. - \'Average Delay\': Average delay time for each airline (ignoring NaNs). - \'Total Flights\': Total number of flights for each airline. - \'Percent Delayed\': Percentage of flights delayed for each airline. - \'Max Delay\': Maximum delay time experienced for each airline. pass ``` **Input**: - A pandas DataFrame with columns `[\'FlightNum\', \'Airline\', \'Origin\', \'Destination\', \'Distance\', \'Delay\']`. **Output**: - A pandas DataFrame containing a summary of the airline delays with columns: - `\'Airline\'`: Airline code. - `\'Average Delay\'`: Average delay time for each airline (ignoring NaNs). - `\'Total Flights\'`: Total number of flights for each airline. - `\'Percent Delayed\'`: Percentage of flights delayed for each airline. - `\'Max Delay\'`: Maximum delay time experienced for each airline. **Constraints**: - The dataset may contain NaN values in the `\'Delay\'` column. - Assume the dataset fits into memory. **Example**: ```python import pandas as pd import numpy as np data = { \'FlightNum\': [101, 202, 303, 404, 505], \'Airline\': [\'AA\', \'AA\', \'BB\', \'BB\', \'CC\'], \'Origin\': [\'LAX\', \'LAX\', \'SFO\', \'SFO\', \'JFK\'], \'Destination\': [\'JFK\', \'ORD\', \'ORD\', \'JFK\', \'LAX\'], \'Distance\': [2475, 1744, 1846, 2475, 2475], \'Delay\': [120, np.nan, 45, 60, 30] } df = pd.DataFrame(data) print(analyze_flight_delays(df)) ``` Expected output: ``` Airline Average Delay Total Flights Percent Delayed Max Delay 0 AA 120.0 2 50.0 120.0 1 BB 52.5 2 100.0 60.0 2 CC 30.0 1 100.0 30.0 ``` **Notes**: 1. Your solution should demonstrate a good understanding of pandas functionalities including grouping, aggregation, and handling missing data. 2. Use appropriate pandas functions and methods to achieve the tasks efficiently.","solution":"import pandas as pd import numpy as np def analyze_flight_delays(df: pd.DataFrame) -> pd.DataFrame: Analyze flight delays from the given DataFrame and return a summary DataFrame. summary = df.groupby(\'Airline\').apply(lambda x: pd.Series({ \'Average Delay\': x[\'Delay\'].mean(skipna=True), \'Total Flights\': x[\'FlightNum\'].count(), \'Percent Delayed\': (x[\'Delay\'].notna().sum() / x[\'FlightNum\'].count()) * 100, \'Max Delay\': x[\'Delay\'].max() })).reset_index() return summary"},{"question":"**Data Manipulation with Missing Values in Pandas** # Problem Statement You are provided with a dataset of daily temperature readings from various weather stations. However, the dataset has missing values which you need to handle using Pandas. # Task 1. **Read Dataset**: Implement a function to load a dataset from a CSV file. 2. **Detect and Summarize Missing Data**: Implement a function to detect and summarize the missing data. The summary should show: - Total number of missing values for each column. - The percentage of missing values for each column. 3. **Fill Missing Values**: - Implement a function to fill missing `numeric` values using forward fill followed by backward fill. - Implement a function to fill missing `string` values with a placeholder `\\"Unknown\\"`. - Implement a function to interpolate missing values in the temperature readings using a linear method. 4. **Drop and Replace Missing Values**: - Implement a function to drop rows where all elements are missing. - Implement a function to replace any remaining missing values with the median of the respective column. 5. **Propagating Missing Values**: - Implement a function to demonstrate the propagation of `NA` in arithmetic operations by adding a column of all `NA`s to another column in the DataFrame. # Input and Output - **Input**: A CSV file path containing the weather data. - **Output**: - A summary DataFrame of missing data. - DataFrames showing filled and manipulated data as per the functions implemented. # Example Suppose the CSV file `weather.csv` has the following content: ```csv date,temp,station,humidity 2023-10-01,25.0,Station1,50 2023-10-02,,Station2,55 2023-10-03,22.5,,60 2023-10-04,23.0,Station1, 2023-10-05,,Station3,70 ``` The functions should handle this data and provide the required outputs. # Function Signatures ```python import pandas as pd def load_data(filepath: str) -> pd.DataFrame: pass def summarize_missing_data(df: pd.DataFrame) -> pd.DataFrame: pass def fill_numeric_missing_values(df: pd.DataFrame) -> pd.DataFrame: pass def fill_string_missing_values(df: pd.DataFrame) -> pd.DataFrame: pass def interpolate_missing_values(df: pd.DataFrame) -> pd.DataFrame: pass def drop_all_missing_rows(df: pd.DataFrame) -> pd.DataFrame: pass def replace_missing_with_median(df: pd.DataFrame) -> pd.DataFrame: pass def demonstrate_na_propagation(df: pd.DataFrame) -> pd.DataFrame: pass # Example usage for testing if __name__ == \\"__main__\\": filepath = \\"weather.csv\\" df = load_data(filepath) print(summarize_missing_data(df)) print(fill_numeric_missing_values(df)) print(fill_string_missing_values(df)) print(interpolate_missing_values(df)) print(drop_all_missing_rows(df)) print(replace_missing_with_median(df)) print(demonstrate_na_propagation(df)) ``` # Constraints - Handle date parsing correctly while reading the CSV file. - Use Pandas specific nullable data types where applicable. - Ensure that the code is optimized for large datasets.","solution":"import pandas as pd def load_data(filepath: str) -> pd.DataFrame: Reads a CSV file into a Pandas DataFrame. :param filepath: path to the CSV file :return: a Pandas DataFrame df = pd.read_csv(filepath, parse_dates=[\'date\'], infer_datetime_format=True) return df def summarize_missing_data(df: pd.DataFrame) -> pd.DataFrame: Summarizes missing data in the DataFrame. :param df: Input DataFrame :return: DataFrame summarizing the missing data missing_data = df.isnull().sum() missing_percentage = (missing_data / len(df)) * 100 summary = pd.DataFrame({ \'Total Missing\': missing_data, \'Percentage Missing\': missing_percentage }) return summary def fill_numeric_missing_values(df: pd.DataFrame) -> pd.DataFrame: Fills missing numeric values using forward fill followed by backward fill. :param df: Input DataFrame :return: DataFrame with filled numeric missing values numeric_columns = df.select_dtypes(include=\'number\').columns df[numeric_columns] = df[numeric_columns].fillna(method=\'ffill\').fillna(method=\'bfill\') return df def fill_string_missing_values(df: pd.DataFrame) -> pd.DataFrame: Fills missing string values with \\"Unknown\\". :param df: Input DataFrame :return: DataFrame with filled string missing values string_columns = df.select_dtypes(include=\'object\').columns df[string_columns] = df[string_columns].fillna(\'Unknown\') return df def interpolate_missing_values(df: pd.DataFrame) -> pd.DataFrame: Interpolates missing values in the DataFrame using a linear method. :param df: Input DataFrame :return: DataFrame with interpolated missing values df = df.interpolate(method=\'linear\') return df def drop_all_missing_rows(df: pd.DataFrame) -> pd.DataFrame: Drops rows where all elements are missing. :param df: Input DataFrame :return: DataFrame with dropped rows df = df.dropna(how=\'all\') return df def replace_missing_with_median(df: pd.DataFrame) -> pd.DataFrame: Replaces any remaining missing values with the median of the respective column. :param df: Input DataFrame :return: DataFrame with missing values replaced by column median numeric_columns = df.select_dtypes(include=\'number\').columns for column in numeric_columns: median = df[column].median() df[column].fillna(median, inplace=True) return df def demonstrate_na_propagation(df: pd.DataFrame) -> pd.DataFrame: Demonstrates propagation of NA in arithmetic operations by adding an NA column to another column. :param df: Input DataFrame :return: DataFrame with the new column demonstrating NA propagation df[\'na_column\'] = pd.NA if \'temp\' in df.columns: df[\'temp_plus_na\'] = df[\'temp\'] + df[\'na_column\'] return df"},{"question":"# Command-Line Argument Parsing with `getopt` You are tasked with writing a Python script that processes command-line arguments to simulate configuring a hypothetical application. The script should use the `getopt` module to parse the command-line options and arguments. Requirements: 1. **Command-Line Options:** - `-h`, `--help`: Display a help message and exit. - `-v`, `--verbose`: Enable verbose mode. - `-o <output-file>`, `--output=<output-file>`: Specify the output file. - `-l <level>`, `--level=<level>`: Set the processing level (must be an integer). 2. **Program Logic:** - If the `-h` or `--help` option is passed, display a help message detailing how to use the script and exit. - If the `-v` or `--verbose` option is provided, print \\"Verbose mode enabled.\\" - If the `-o` or `--output` option is provided, store the output file name. - If the `-l` or `--level` option is provided, check if the level is a valid integer. If not, raise an error. 3. **Error Handling:** - If an unrecognized option is provided, raise an appropriate error message. - If an option that requires an argument is given without one, raise an appropriate error message. 4. **Example Usages:** - `script.py -h` - `script.py -v --output=results.txt -l 3` - `script.py --verbose --output results.txt --level 5` Input: The command-line options and arguments provided by the user (e.g., `[\'-v\', \'--output=results.txt\', \'--level\', \'3\']`). Output: Print the outcomes based on the provided options, such as confirmation of verbose mode, the output file name, and the processing level. Print error messages as needed. Constraints: - Use the `getopt` module for parsing command-line arguments. - Ensure the `level` argument must be a valid integer. Implementation: Write a Python function `parse_command_line(args)` that takes a list of command-line arguments (excluding the script name), parses them, and prints the appropriate messages based on the options provided. Example: ```python import getopt import sys def parse_command_line(args): try: opts, args = getopt.getopt(args, \\"hvo:l:\\", [\\"help\\", \\"verbose\\", \\"output=\\", \\"level=\\"]) except getopt.GetoptError as err: print(err) sys.exit(2) output = None verbose = False level = None for o, a in opts: if o in (\\"-h\\", \\"--help\\"): print(\\"Usage: script.py [-h] [-v] [-o <output-file>] [-l <level>]\\") sys.exit() elif o in (\\"-v\\", \\"--verbose\\"): verbose = True elif o in (\\"-o\\", \\"--output\\"): output = a elif o in (\\"-l\\", \\"--level\\"): if not a.isdigit(): print(\\"Error: Level must be an integer.\\") sys.exit(2) level = int(a) else: assert False, \\"Unhandled option\\" if verbose: print(\\"Verbose mode enabled.\\") if output: print(f\\"Output file: {output}\\") if level is not None: print(f\\"Processing level: {level}\\") if __name__ == \\"__main__\\": parse_command_line(sys.argv[1:]) ``` Instructions: 1. Implement the `parse_command_line(args)` function as described. 2. Test the function using various command-line options to ensure it handles all cases correctly. 3. Submit your script and include a brief description of your testing process.","solution":"import getopt import sys def parse_command_line(args): try: opts, args = getopt.getopt(args, \\"hvo:l:\\", [\\"help\\", \\"verbose\\", \\"output=\\", \\"level=\\"]) except getopt.GetoptError as err: print(err) sys.exit(2) output = None verbose = False level = None for o, a in opts: if o in (\\"-h\\", \\"--help\\"): print(\\"Usage: script.py [-h] [-v] [-o <output-file>] [-l <level>]\\") sys.exit() elif o in (\\"-v\\", \\"--verbose\\"): verbose = True elif o in (\\"-o\\", \\"--output\\"): output = a elif o in (\\"-l\\", \\"--level\\"): if not a.isdigit(): print(\\"Error: Level must be an integer.\\") sys.exit(2) level = int(a) else: assert False, \\"Unhandled option\\" if verbose: print(\\"Verbose mode enabled.\\") if output: print(f\\"Output file: {output}\\") if level is not None: print(f\\"Processing level: {level}\\")"},{"question":"Coding Assessment Question # Objective You are asked to implement a function that creates a multipart email message with specific headers and payloads. The function should utilize the `email.message.Message` class and demonstrate comprehension of the following methods: - `__setitem__` - `add_header` - `get_payload` - `set_payload` - `as_string` - `as_bytes` # Task Create a Python function `create_multipart_message` that constructs a multipart email message. The function should accept two plaintext parts and an attachment (binary data) as input and return a tuple containing the serialized string and bytes representations of the email message. # Specifications 1. The function should take the following parameters: - `plain_text1` (str): The first plaintext part of the email. - `plain_text2` (str): The second plaintext part of the email. - `binary_data` (bytes): The binary data to be attached as an attachment. - `attachment_name` (str): The filename for the binary attachment. 2. The function should: - Create a `Message` object representing the multipart email. - Set the email headers: \\"From\\", \\"To\\", \\"Subject\\", and \\"Content-Type\\" as multipart/mixed. - Add the two plaintext parts and the binary attachment as separate parts of the email, with appropriate Content-Type and other necessary headers for each part. - Serialize the email message to both a string (including the Unix from header) and bytes format. 3. The function should return a tuple `(message_as_string, message_as_bytes)`. # Example ```python def create_multipart_message(plain_text1, plain_text2, binary_data, attachment_name): # Your code here # Example usage: plain_text1 = \\"This is the first part of the message.\\" plain_text2 = \\"This is the second part of the message.\\" binary_data = b\'This is some binary data.\' attachment_name = \\"attachment.bin\\" result = create_multipart_message(plain_text1, plain_text2, binary_data, attachment_name) message_as_string, message_as_bytes = result print(message_as_string) # Prints the email message as a string print(message_as_bytes) # Prints the email message as bytes ``` # Constraints - The plaintext should be encoded in UTF-8. - The binary data should be base64 encoded. - Ensure proper MIME boundaries are set. - Add relevant `Content-Disposition` headers for attachments. # Performance - The function should handle inputs where the combined size of the inputs is up to 10 MB. # Hint Use the `set_payload` method for setting the payload of each part, paying attention to the Content-Type and Content-Transfer-Encoding headers. For attachments, ensure to set the `filename` parameter correctly to support non-ASCII characters if needed.","solution":"from email.message import EmailMessage from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders def create_multipart_message(plain_text1, plain_text2, binary_data, attachment_name): Constructs a multipart email message containing two plaintext parts and a binary attachment. Parameters: plain_text1 (str): The first plaintext part of the email. plain_text2 (str): The second plaintext part of the email. binary_data (bytes): The binary data to be attached as an attachment. attachment_name (str): The filename for the binary attachment. Returns: tuple: A tuple containing the serialized string and bytes representations of the email message. # Create the root message msg = MIMEMultipart() msg[\'From\'] = \\"sender@example.com\\" msg[\'To\'] = \\"recipient@example.com\\" msg[\'Subject\'] = \\"Test Email with Multiple Parts\\" # Create and attach the first plaintext part part1 = MIMEText(plain_text1, \'plain\') msg.attach(part1) # Create and attach the second plaintext part part2 = MIMEText(plain_text2, \'plain\') msg.attach(part2) # Create and attach the binary attachment part3 = MIMEBase(\'application\', \'octet-stream\') part3.set_payload(binary_data) encoders.encode_base64(part3) part3.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{attachment_name}\\"\') msg.attach(part3) # Serialize the message to string and bytes message_as_string = msg.as_string() message_as_bytes = msg.as_bytes() return message_as_string, message_as_bytes"},{"question":"**Coding Assessment Question** # Question You are provided with a dataset containing features and target values for predicting house prices. Your task is to perform the following tasks using `scikit-learn`: 1. Preprocess the given data to handle any missing values. 2. Fit a `LinearRegression` model to the preprocessed data and evaluate its performance using mean squared error (MSE). 3. Implement Ridge regression using `RidgeCV` with built-in cross-validation to select the best hyperparameter `alpha`. Evaluate its performance using mean squared error (MSE). # Dataset The dataset `house_prices.csv` has the following columns: - `Feature1`, `Feature2`, ..., `FeatureN`: Numerical features for predicting house prices. - `Price`: The target value representing the house price. # Requirements 1. **Preprocessing**: - Handle any missing values in the dataset by filling them with the mean of the corresponding column. 2. **Linear Regression**: - Implement and fit a `LinearRegression` model to the preprocessed dataset. - Evaluate the model performance using mean squared error. 3. **Ridge Regression with Cross-Validation**: - Implement and fit a `RidgeCV` model with a range of alphas `[0.1, 1.0, 10.0]`. - Evaluate the model performance using mean squared error. # Input - The path to the dataset file `house_prices.csv`. # Output - The mean squared error for the `LinearRegression` model. - The mean squared error for the `RidgeCV` model. - The best value of `alpha` chosen by `RidgeCV`. # Constraints - You must handle any missing values in the dataset. - You must use the provided range of alphas `[0.1, 1.0, 10.0]` for `RidgeCV`. # Example ```python import pandas as pd from sklearn.linear_model import LinearRegression, RidgeCV from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer def preprocess_data(filepath): # Load the dataset data = pd.read_csv(filepath) # Split the features and target X = data.drop(\'Price\', axis=1) y = data[\'Price\'] # Handle missing values imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(X) return X, y def evaluate_models(filepath): # Preprocess the data X, y = preprocess_data(filepath) # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Linear Regression Model lin_reg = LinearRegression() lin_reg.fit(X_train, y_train) y_pred_lin = lin_reg.predict(X_test) mse_lin = mean_squared_error(y_test, y_pred_lin) # Ridge Regression Model with Cross-Validation ridgeCV = RidgeCV(alphas=[0.1, 1.0, 10.0]) ridgeCV.fit(X_train, y_train) y_pred_ridge = ridgeCV.predict(X_test) mse_ridge = mean_squared_error(y_test, y_pred_ridge) best_alpha = ridgeCV.alpha_ return mse_lin, mse_ridge, best_alpha # Example usage filepath = \'house_prices.csv\' mse_lin, mse_ridge, best_alpha = evaluate_models(filepath) print(f\'Linear Regression MSE: {mse_lin}\') print(f\'Ridge Regression MSE: {mse_ridge}\') print(f\'Best alpha chosen by RidgeCV: {best_alpha}\') ``` # Submission: Submit your implementation as a Python script or Jupyter notebook.","solution":"import pandas as pd from sklearn.linear_model import LinearRegression, RidgeCV from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer def preprocess_data(filepath): Load and preprocess the dataset. Parameters: filepath (str): Path to the dataset CSV file Returns: X (pd.DataFrame): Preprocessed features y (pd.Series): Target values # Load the dataset data = pd.read_csv(filepath) # Split the features and target X = data.drop(\'Price\', axis=1) y = data[\'Price\'] # Handle missing values imputer = SimpleImputer(strategy=\'mean\') X = imputer.fit_transform(X) return X, y def evaluate_models(filepath): Fit and evaluate LinearRegression and RidgeCV models. Parameters: filepath (str): Path to the dataset CSV file Returns: tuple: MSE for the LinearRegression model, MSE for the RidgeCV model, best alpha chosen by RidgeCV # Preprocess the data X, y = preprocess_data(filepath) # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Linear Regression Model lin_reg = LinearRegression() lin_reg.fit(X_train, y_train) y_pred_lin = lin_reg.predict(X_test) mse_lin = mean_squared_error(y_test, y_pred_lin) # Ridge Regression Model with Cross-Validation ridgeCV = RidgeCV(alphas=[0.1, 1.0, 10.0]) ridgeCV.fit(X_train, y_train) y_pred_ridge = ridgeCV.predict(X_test) mse_ridge = mean_squared_error(y_test, y_pred_ridge) best_alpha = ridgeCV.alpha_ return mse_lin, mse_ridge, best_alpha"},{"question":"**Objective**: Implement a function to create and manipulate generic type aliases in Python, similar to the `GenericAlias` discussed in the documentation. # Problem Statement You are required to write a function `create_generic_alias` that takes two parameters - `origin` and `args` - and returns a Python dictionary that mimics the structure and attributes of a `GenericAlias` object as described. Additionally, implement another function `apply_generic_alias` to apply a created generic alias type to a given input list and return the result. **Function Signature**: ```python def create_generic_alias(origin: str, args: tuple) -> dict: pass def apply_generic_alias(generic_alias: dict, input_list: list) -> list: pass ``` # Function Specifications 1. **create_generic_alias(origin: str, args: tuple) -> dict** - **Input**: - `origin` (str): A string representing the name of the origin type (e.g., \\"List\\"). - `args` (tuple): A tuple representing type arguments (e.g., (int,) for List[int]). - **Output**: - Returns a dictionary with the following structure: ```python { \\"__origin__\\": origin, \\"__args__\\": args } ``` 2. **apply_generic_alias(generic_alias: dict, input_list: list) -> list** - **Input**: - `generic_alias` (dict): A dictionary structure returned by `create_generic_alias`. - `input_list` (list): A list of elements to which the generic alias type should be applied. - **Output**: - Returns the input list, but cast into the specified generic type (for simplicity, the function will just reformat the output for demonstration). # Constraints 1. The origin type (e.g., \\"List\\") should be valid Python collection types (i.e., `List`, `Dict`, `Set`). 2. The args tuple should contain valid type hints (i.e., built-in types like `int`, `str`, etc.). 3. For the purpose of this task, the `apply_generic_alias` should not perform actual type validation but should demonstrate understanding by returning the list in a formatted output. # Example Usage ```python # Create a generic alias for List[int] generic_alias = create_generic_alias(\\"List\\", (int,)) print(generic_alias) # Output: {\'__origin__\': \'List\', \'__args__\': (int,)} # Apply the generic alias to a list result = apply_generic_alias(generic_alias, [1, 2, 3]) print(result) # Output: [\'List[int]:\', 1, 2, 3] ``` # Notes - This exercise aims to test the understanding of type hinting and generic types in Python. - Ensure that your implementation captures the essence of how generic types are represented and used.","solution":"def create_generic_alias(origin: str, args: tuple) -> dict: Create a generic alias dictionary. Parameters: - origin (str): The origin type name (e.g., \\"List\\"). - args (tuple): A tuple of type arguments (e.g., (int,) for List[int]). Returns: - dict: A dictionary mimicking a GenericAlias object. return { \\"__origin__\\": origin, \\"__args__\\": args } def apply_generic_alias(generic_alias: dict, input_list: list) -> list: Apply the generic alias type to a given list. Parameters: - generic_alias (dict): The generic alias dictionary. - input_list (list): A list of elements to apply the alias type to. Returns: - list: The input list formatted with the applied generic alias type. origin = generic_alias[\\"__origin__\\"] args = \\", \\".join(arg.__name__ for arg in generic_alias[\\"__args__\\"]) return [f\\"{origin}[{args}]:\\"] + input_list"},{"question":"**Pandas Copy-on-Write Assessment** The release of pandas 3.0 introduced the Copy-on-Write (CoW) mechanism, which aims to enhance the predictability and performance of pandas operations. This change impacts how in-place operations, views, and copies are handled. **Question:** You are given a DataFrame `df` with the following structure: ```python import pandas as pd df = pd.DataFrame({\\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6], \\"C\\": [7, 8, 9]}) ``` Your task is to perform the following operations and implement a function `co_w_updates` to ensure compliance with CoW principles: 1. Create a subset of column \'A\' and attempt to modify it in-place. 2. Create a new DataFrame by resetting the index of `df`. 3. Modify the new DataFrame in-place and ensure no side-effects on the original `df`. 4. Perform a chained assignment to update values in \'C\' only where values in \'B\' are greater than 5. Implement a function `co_w_updates(df: pd.DataFrame) -> tuple` that performs these operations and returns a tuple of the original DataFrame and the modified DataFrame. Ensure you handle the CoW constraints correctly. ```python def co_w_updates(df: pd.DataFrame) -> tuple: Perform pandas operations compliant with Copy-on-Write principles. Parameters: df (pd.DataFrame): Input DataFrame. Returns: tuple: A tuple containing the original DataFrame \'df\' and the modified DataFrame. # 1. Create a subset of column \'A\' and attempt to modify it in-place a_subset = df[\'A\'] try: a_subset.iloc[0] = 100 except ValueError: # Can\'t modify in-place due to CoW, reassign the modified values to the original DataFrame df.loc[:, \'A\'] = a_subset.replace({a_subset.iloc[0]: 100}) # 2. Create a new DataFrame by resetting the index of df df_new = df.reset_index(drop=True) # 3. Modify the new DataFrame in-place and ensure no side-effects on the original df df_new.iloc[0, 0] = 200 # 4. Perform a chained assignment to update values in \'C\' where values in \'B\' are greater than 5 df_new.loc[df_new[\'B\'] > 5, \'C\'] = 100 return (df, df_new) # Example usage df_original, df_modified = co_w_updates(df) print(\\"Original DataFrame after operations:n\\", df_original) print(\\"Modified DataFrame:n\\", df_modified) ``` **Expected Output:** After running the above functions, `df_original` should remain unchanged while `df_modified` should reflect all the modifications. - `df_original` should look like this: ``` A B C 0 1 4 7 1 2 5 8 2 3 6 9 ``` - `df_modified` should look like this: ``` A B C 0 200 4 7 1 2 5 8 2 3 6 100 ``` **Constraints and Requirements:** - You must comply with Copy-on-Write principles, avoiding direct in-place modifications that affect multiple objects. - The function should efficiently handle the DataFrame without unnecessary copies. This question assesses the ability to maneuver pandas operations under the Copy-on-Write constraints, leveraging `.loc` and other compliant methods appropriately.","solution":"import pandas as pd def co_w_updates(df: pd.DataFrame) -> tuple: Perform pandas operations compliant with Copy-on-Write principles. Parameters: df (pd.DataFrame): Input DataFrame. Returns: tuple: A tuple containing the original DataFrame \'df\' and the modified DataFrame. # 1. Create a subset of column \'A\' and attempt to modify it in-place a_subset = df[\'A\'].copy() a_subset.iloc[0] = 100 # Update the original df using the modified subset df_modified_col_A = df.copy() df_modified_col_A[\'A\'] = a_subset # 2. Create a new DataFrame by resetting the index of df df_new = df_modified_col_A.reset_index(drop=True) # 3. Modify the new DataFrame in-place and ensure no side-effects on the original df df_new.iloc[0, 0] = 200 # 4. Perform a chained assignment to update values in \'C\' where values in \'B\' are greater than 5 df_new.loc[df_new[\'B\'] > 5, \'C\'] = 100 return (df, df_new)"},{"question":"Problem Statement You are given a dataset containing features of various houses along with their prices and sale categories. Your task is to implement a decision tree model to perform the following: 1. Predict the `category` (high, medium, low) of houses based on the given input features. 2. Predict the `price` of the houses simultaneously. To achieve this, you need to: 1. Load the dataset. 2. Perform necessary preprocessing. 3. Implement a multi-output decision tree using scikit-learn. 4. Evaluate the model\'s performance. # Input - A CSV file named `house_data.csv` with the following columns: - `area`: area of the house in square feet (integer) - `bedrooms`: number of bedrooms (integer) - `bathrooms`: number of bathrooms (integer) - `floors`: number of floors (integer) - `age`: age of the house in years (integer) - `price`: selling price of the house (float) - `category`: sale category of the house (categorical with values: high, medium, low) # Output - Summarize the performance of the model using appropriate metrics for both classification and regression tasks. # Evaluation Metrics - For classification: Use accuracy score. - For regression: Use mean squared error (MSE). # Constraints - Use scikit-learn\'s `DecisionTreeClassifier` and `DecisionTreeRegressor`. - Perform train-test split with 80% of the data as training and 20% as test. # Sample Code Structure ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.metrics import accuracy_score, mean_squared_error # Load dataset data = pd.read_csv(\'house_data.csv\') # Preprocessing # Encode the \'category\' column (high -> 2, medium -> 1, low -> 0) # Split the dataset into features and target variables X = data[[\'area\', \'bedrooms\', \'bathrooms\', \'floors\', \'age\']] y_classification = ... # Multi-output target for regression and classification y_regression = ... # Split data into training and testing sets (80% training, 20% testing) X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg = train_test_split(X, y_classification, y_regression, test_size=0.2, random_state=42) # Initialize the multi-output decision tree (Hint: Refer to DecisionTreeRegressor and DecisionTreeClassifier) # Train the model # Evaluate performance # Output the results ``` # Additional Notes - You may use additional preprocessing techniques if necessary. - Ensure to evaluate and output the classification accuracy and regression MSE on the test set. - Documentation and submission of a well-written code are essential for full credit.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.metrics import accuracy_score, mean_squared_error def prepare_data(file_path): # Load dataset data = pd.read_csv(file_path) # Encode the \'category\' column (high -> 2, medium -> 1, low -> 0) data[\'category\'] = data[\'category\'].map({\'low\': 0, \'medium\': 1, \'high\': 2}) # Split the dataset into features and target variables X = data[[\'area\', \'bedrooms\', \'bathrooms\', \'floors\', \'age\']] y_classification = data[[\'category\']] y_regression = data[[\'price\']] # Split data into training and testing sets (80% training, 20% testing) X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg = train_test_split( X, y_classification, y_regression, test_size=0.2, random_state=42) return X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg def train_and_evaluate(file_path): # Prepare data for training and testing X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg = prepare_data(file_path) # Initialize Decision Tree models classifier = DecisionTreeClassifier(random_state=42) regressor = DecisionTreeRegressor(random_state=42) # Train the models classifier.fit(X_train, y_train_class) regressor.fit(X_train, y_train_reg) # Predict classification and regression outputs y_pred_class = classifier.predict(X_test) y_pred_reg = regressor.predict(X_test) # Evaluate performance class_accuracy = accuracy_score(y_test_class, y_pred_class) reg_mse = mean_squared_error(y_test_reg, y_pred_reg) return { \'classification_accuracy\': class_accuracy, \'regression_mse\': reg_mse } # Example usage # results = train_and_evaluate(\'house_data.csv\') # print(results)"},{"question":"Advanced Use of Descriptors in Python Objective Implement and utilize Python descriptors to manage and validate attributes of a class. Problem Statement You are required to create a descriptor that validates the values of class attributes based on given constraints. Your descriptor should be flexible enough to support different types of validations such as type checks, value range checks, and custom predicate checks. Task 1. **Descriptor Implementation**: Implement a `Validator` descriptor class that validates an attribute according to specified rules. The `Validator` class should implement the descriptor protocol with `__get__()` and `__set__()` methods. 2. **Custom Validators**: Implement the following custom validators by inheriting from the `Validator` class: - **TypeValidator**: Validates that the value is of a specified type. - **RangeValidator**: Validates that the value is within a specified range (inclusive). - **PredicateValidator**: Validates that the value satisfies a custom predicate function. 3. **Application**: Create a `Product` class that uses these validators to enforce the rules for its attributes: - `name`: Must be a string and must not be empty. - `price`: Must be a float and must be within the range of 0.0 to 10,000.0. - `category`: Must be one of the specified categories (`\'Electronics\'`, `\'Clothing\'`, `\'Food\'`). Implementation Details - Define the `Validator` class with appropriate methods for validation. - Define `TypeValidator`, `RangeValidator`, and `PredicateValidator` as subclasses of `Validator`. - Define the `Product` class with the specified attributes and their corresponding validators. Input and Output - **Input**: You will not need to handle explicit input within your solution. Instead, you will create instances of the `Product` class with various attribute values to test the validators. - **Output**: The execution of setting an attribute should raise a `ValueError` if the value does not meet the specified validation rules. Constraints - The `Product` class should only allow valid values as defined by the validators. - Make sure to handle edge cases where invalid data is passed to the attributes. Example ```python class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass class TypeValidator(Validator): def __init__(self, typ): self.typ = typ def validate(self, value): if not isinstance(value, self.typ): raise TypeError(f\'Expected {value!r} to be a {self.typ.__name__}\') class RangeValidator(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected {value!r} to be at least {self.minvalue}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected {value!r} to be at most {self.maxvalue}\') class PredicateValidator(Validator): def __init__(self, predicate, message=\\"Validation failed\\"): self.predicate = predicate self.message = message def validate(self, value): if not self.predicate(value): raise ValueError(f\'Validation failed: {self.message}\') class Product: name = PredicateValidator(lambda x: isinstance(x, str) and len(x) > 0, \\"Name must be a non-empty string\\") price = RangeValidator(minvalue=0.0, maxvalue=10000.0) category = PredicateValidator(lambda x: x in {\'Electronics\', \'Clothing\', \'Food\'}, \\"Invalid category\\") def __init__(self, name, price, category): self.name = name self.price = price self.category = category # Testing the Product class with valid and invalid values try: p1 = Product(\'\', 200.5, \'Electronics\') # Should raise ValueError: Name must be a non-empty string except ValueError as e: print(e) try: p2 = Product(\'Laptop\', 200.5, \'Electronics\') # Should pass print(p2.name, p2.price, p2.category) p2.price = 20000.0 # Should raise ValueError: Expected 20000.0 to be at most 10000.0 except ValueError as e: print(e) try: p3 = Product(\'T-Shirt\', 50.0, \'Cloth\') # Should raise ValueError: Invalid category except ValueError as e: print(e) ``` In this example, the `Product` class enforces the rules for its attributes using the custom validators. Invalid assignments raise an appropriate `ValueError`.","solution":"from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass class TypeValidator(Validator): def __init__(self, typ): self.typ = typ def validate(self, value): if not isinstance(value, self.typ): raise TypeError(f\'Expected {value!r} to be a {self.typ.__name__}\') class RangeValidator(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected {value!r} to be at least {self.minvalue}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected {value!r} to be at most {self.maxvalue}\') class PredicateValidator(Validator): def __init__(self, predicate, message=\\"Validation failed\\"): self.predicate = predicate self.message = message def validate(self, value): if not self.predicate(value): raise ValueError(f\'Validation failed: {self.message}\') class Product: name = PredicateValidator(lambda x: isinstance(x, str) and len(x) > 0, \\"Name must be a non-empty string\\") price = RangeValidator(minvalue=0.0, maxvalue=10000.0) category = PredicateValidator(lambda x: x in {\'Electronics\', \'Clothing\', \'Food\'}, \\"Invalid category\\") def __init__(self, name, price, category): self.name = name self.price = price self.category = category"},{"question":"**Advanced Data Visualization with Seaborn** You are tasked with creating an advanced histogram visualization for the `titanic` dataset using the seaborn.objects module. This exercise will assess your ability to use seaborn for sophisticated data visualizations, particularly focusing on handling multiple variables, customizing aesthetics, and managing plot overlap. # Objectives: 1. Load the `titanic` dataset using seaborn. 2. Create a histogram plot of the `fare` variable, applying a logarithmic scale to the x-axis. 3. Color the bars by `class` and handle overlapping bars using an appropriate transformation. 4. Customize the bars to have a reduced width and no fill, setting an appropriate edge color and width. 5. Add a transparency effect to one of the plot variables to make the plot more comprehensible. # Function Signature: ```python import seaborn.objects as so from seaborn import load_dataset def advanced_titanic_histogram(): # Step 1: Load the titanic dataset titanic = load_dataset(\\"titanic\\") # Step 2: Create the base plot object with logarithmic scale p = so.Plot(titanic, \\"fare\\").scale(x=\\"log\\") # Step 3: Add histogram with bars colored by \'class\', handling overlap p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"class\\") # Step 4: Customize the bars p.add(so.Bars(fill=False, edgecolor=\\"C1\\", edgewidth=1.5, width=0.8), so.Hist()) # Step 5: Add transparency effect based on the \'age\' variable hist = so.Hist(binwidth=10, binrange=(0, 100)) p.add(so.Bars(edgewidth=0, alpha=\\"age\\"), hist) # Display the plot p.show() ``` # Instructions: - The function `advanced_titanic_histogram` should not take any arguments. - Ensure to handle any missing values in the dataset that might affect the plotting. - The function should display the plot directly when called. # Constraints: - You must use the `seaborn.objects` module. - The `fare` variable should be plotted on a logarithmic scale. - Bar color should be mapped to the `class` variable with overlap handling. - Bars should have no fill, a specific edge color (`C1`), and a width of 0.8. - An additional transparency effect should be applied based on the `age` variable. This exercise tests your ability to create complex and informative visualizations using Seaborn\'s advanced functionality. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset import numpy as np def advanced_titanic_histogram(): # Step 1: Load the titanic dataset titanic = load_dataset(\\"titanic\\") # Handle missing values for \'fare\' and \'age\' by removing rows with missing data titanic = titanic.dropna(subset=[\\"fare\\", \\"age\\"]) # Step 2: Create the base plot object with logarithmic scale for the \'fare\' variable p = so.Plot(titanic, \\"fare\\").scale(x=\\"log\\") # Step 3: Add histogram with bars colored by \'class\', handling overlap p.add(so.Bars(), so.Hist(), so.Stack(), color=\\"class\\") # Step 4: Customize the bars (no fill, edge color \'C1\', edge width 1.5, width 0.8) p.add(so.Bars(fill=False, edgecolor=\\"C1\\", edgewidth=1.5, width=0.8), so.Hist()) # Step 5: Add transparency effect based on the \'age\' variable hist = so.Hist(binwidth=10, binrange=(0, 100)) p.add(so.Bars(edgewidth=0, alpha=\\"age\\"), hist) # Display the plot p.show() # Call the function to display the plot (only for effective local test) # Not to be included in the testing script because of its interactive nature # advanced_titanic_histogram()"},{"question":"Objective: This question assesses your ability to manipulate and visualize datasets using Seaborn\'s `objects` interface, focusing on creating comprehensive plots that combine different visual elements. Task: 1. **Data Preparation**: - Load the `penguins` dataset using `seaborn.load_dataset`. - Filter the dataset to include only the species \'Adelie\' and \'Chinstrap\'. - Create a new column named `bill_ratio`, which is the ratio of `bill_length_mm` to `bill_depth_mm`. - Drop any rows with missing values. 2. **Plot Creation**: - Create a plot using `seaborn.objects.Plot` with: - `x` representing `bill_length_mm`. - `y` representing `bill_ratio`. - `color` representing different species. - Add the following elements to your plot: - A `Line` element depicting the average `bill_ratio` for each `bill_length_mm`. - A `Band` element to show the 95% confidence interval of `bill_ratio` for each `bill_length_mm`. Input: There is no direct input to your code, but you must assume the `penguins` dataset is available in Seaborn\'s datasets. Output: - A plot object created using `seaborn.objects`, which combines the specified visual elements. Constraints: - You are required to use the `seaborn.objects` interface for both data manipulation and plotting. - Ensure the plot is well-labeled and conveys clear information. - All transformations should be done using Pandas and Seaborn functionalities. # Example code structure: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Data Preparation penguins = load_dataset(\\"penguins\\") # Apply filters, create new column and drop missing values # Step 2: Plot Creation plot = so.Plot(...) # Add visual elements to the plot plot.add(...) plot.add(...) # Display the plot plot.show() ``` Ensure your code is clean, well-commented, and correctly implements the required transformations and visualizations.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd # Step 1: Data Preparation penguins = sns.load_dataset(\\"penguins\\") # Filter the dataset to include only the species \'Adelie\' and \'Chinstrap\' penguins_filtered = penguins[penguins[\'species\'].isin([\'Adelie\', \'Chinstrap\'])] # Create a new column \'bill_ratio\' penguins_filtered = penguins_filtered.dropna() # Drop any rows with missing values penguins_filtered[\'bill_ratio\'] = penguins_filtered[\'bill_length_mm\'] / penguins_filtered[\'bill_depth_mm\'] # Step 2: Plot Creation plot = (so.Plot(penguins_filtered, x=\\"bill_length_mm\\", y=\\"bill_ratio\\", color=\\"species\\") .add(so.Line(), so.Agg()) .add(so.Band(), so.Agg())) # Returning the plot object for potential display in other environments plot"},{"question":"**Title:** Analyzing Function Performance with Timeit **Objective:** To assess the student\'s understanding of the `timeit` module by requiring them to implement functions that utilize `timeit.timeit()` and `timeit.repeat()` to compare the performance of different implementations of a function. **Problem Statement:** You are given two different implementations to reverse a list in Python. Your task is to design a performance comparison tool using the `timeit` module to evaluate which implementation is faster under various conditions. 1. `reverse_with_slicing(lst)`: Uses Python slicing to reverse the list. 2. `reverse_with_loop(lst)`: Uses a for-loop to reverse the list. **Implement the following functions:** 1. `reverse_with_slicing(lst)`: Reverses a list using slicing. ```python def reverse_with_slicing(lst): # Your code here ``` 2. `reverse_with_loop(lst)`: Reverses a list using a for-loop. ```python def reverse_with_loop(lst): # Your code here ``` 3. `compare_performance(n_trials)`: Uses the `timeit` module to compare the performance of the two implementations (you can assume `n_trials` to be 100000 for simplicity). The function should: - Initialize a list of 1000 integers: `lst = list(range(1000))` - Use `timeit.timeit()` to measure the execution time of `reverse_with_slicing` and `reverse_with_loop` functions. - Print out the execution time comparison. - Return a dictionary with keys `\'slicing\'` and `\'loop\'`, mapping to their respective measured times. ```python def compare_performance(n_trials=100000): # Your code here using timeit.timeit or timeit.repeat ``` **Constraints:** - Do not use any additional libraries except `timeit`. **Expected Output:** The function `compare_performance` should return a dictionary like: ```python { \'slicing\': 0.345, \'loop\': 0.567 } ``` where the values are the time taken in seconds for each method to complete all the trials. **Example:** ```python if __name__ == \\"__main__\\": result = compare_performance() print(result) ``` This question requires the student to effectively use the `timeit` module to achieve a performance analysis, demonstrating their understanding of both basic and advanced features of the `timeit` module.","solution":"import timeit def reverse_with_slicing(lst): Reverses a list using slicing. return lst[::-1] def reverse_with_loop(lst): Reverses a list using a for-loop. reversed_list = [] for item in lst: reversed_list.insert(0, item) return reversed_list def compare_performance(n_trials=100000): Compares the performance of reverse_with_slicing and reverse_with_loop functions. Args: n_trials (int): Number of trials for each timing measurement. Returns: dict: A dictionary with keys \'slicing\' and \'loop\' mapping to their respective measured times in seconds. lst = list(range(1000)) time_slicing = timeit.timeit(lambda: reverse_with_slicing(lst), number=n_trials) time_loop = timeit.timeit(lambda: reverse_with_loop(lst), number=n_trials) return { \'slicing\': time_slicing, \'loop\': time_loop } if __name__ == \\"__main__\\": result = compare_performance() print(result)"},{"question":"# URL Management and Handling You are tasked with creating a Python function to retrieve and process data from a set of URLs. To achieve this, implement three functions: `fetch_url`, `parse_url`, and `check_robots`. Functions Description: 1. **`fetch_url(url: str) -> str`** This function takes a URL as an input and returns the content of the URL as a string. - **Input**: A string representing the URL. - **Output**: A string containing the content of the webpage. - **Exceptions**: If the URL is invalid or there is an error fetching the content, raise an appropriate exception with a meaningful message. 2. **`parse_url(url: str) -> dict`** This function takes a URL as an input and returns a dictionary containing its components (`scheme`, `netloc`, `path`, `params`, `query`, and `fragment`). - **Input**: A string representing the URL. - **Output**: A dictionary with keys (`scheme`, `netloc`, `path`, `params`, `query`, and `fragment`) and their corresponding values. - **Exceptions**: None. 3. **`check_robots(base_url: str, user_agent: str = \'*\') -> bool`** This function takes a base URL and an optional user agent string as inputs and checks the `robots.txt` file to see if the user agent is allowed to fetch the main page of the base URL. - **Input**: - `base_url`: A string representing the base URL (e.g., \\"http://example.com\\"). - `user_agent`: An optional string specifying the user agent (default to \'*\'). - **Output**: A boolean value. `True` if the user agent is allowed to fetch the main page, `False` otherwise. - **Exceptions**: If the `robots.txt` file cannot be fetched, return `False`. Constraints: - Do not use any libraries other than those available in the Python standard library, especially those in the `urllib` package. - Handle network errors gracefully while fetching URLs or parsing `robots.txt`. Example Usage: ```python # Example URL url = \\"http://example.com/index.html\\" # Fetching URL Content content = fetch_url(url) print(content) # Should print the HTML content of the webpage # Parsing URL components = parse_url(url) print(components) # Should output: # { # \'scheme\': \'http\', # \'netloc\': \'example.com\', # \'path\': \'/index.html\', # \'params\': \'\', # \'query\': \'\', # \'fragment\': \'\' # } # Checking robots.txt is_allowed = check_robots(\\"http://example.com\\") print(is_allowed) # Should output either True or False based on robots.txt rules ``` Evaluation Criteria: - Correctness: Ensure all functions work as described. - Error Handling: Proper exceptions are raised and handled as required. - Code Readability: Code should be clean and well-documented.","solution":"import urllib.request import urllib.parse import urllib.error def fetch_url(url: str) -> str: Fetches the content of a given URL. Args: url (str): The URL to fetch. Returns: str: The content of the URL. Raises: Exception: If there is an error fetching the URL content. try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: raise Exception(f\\"Error fetching URL {url}: {e.reason}\\") def parse_url(url: str) -> dict: Parses the given URL into its components. Args: url (str): The URL to parse. Returns: dict: A dictionary containing the components of the URL. parsed = urllib.parse.urlparse(url) return { \'scheme\': parsed.scheme, \'netloc\': parsed.netloc, \'path\': parsed.path, \'params\': parsed.params, \'query\': parsed.query, \'fragment\': parsed.fragment } def check_robots(base_url: str, user_agent: str = \'*\') -> bool: Checks the robots.txt of a given base URL to see if the user agent is allowed to fetch the main page. Args: base_url (str): The base URL. user_agent (str): The user agent (default is \'*\'). Returns: bool: True if the user agent is allowed, False otherwise. robots_url = urllib.parse.urljoin(base_url, \'/robots.txt\') try: content = fetch_url(robots_url) except Exception: return False lines = content.split(\'n\') user_agent_rule_active = False for line in lines: if line.lower().startswith(\'user-agent:\'): user_agent_rule_active = (line.split(\':\', 1)[1].strip() == user_agent or line.split(\':\', 1)[1].strip() == \'*\') elif user_agent_rule_active and line.lower().startswith(\'disallow:\'): disallowed_path = line.split(\':\', 1)[1].strip() if not disallowed_path or disallowed_path == \'/\': return False return True"},{"question":"Objective: Demonstrate your understanding of Python\'s `set` and `frozenset` objects by implementing functions that perform various operations using the provided API. Problem Statement: You are required to implement two functions: `set_operations` and `frozenset_operations`. 1. **`set_operations` function:** This function should perform the following operations on a set: - Create a new set with the given iterable elements. - Add a specified element to the set. - Remove a specified element from the set. - Check if an element exists in the set. - Return the current size of the set. - Pop a random element from the set (if not empty) and return it along with the set\'s size. - Clear all elements in the set and return the size of the emptied set. 2. **`frozenset_operations` function:** This function should perform the following operations on a frozenset: - Create a new frozenset with the given iterable elements. - Check if a specific element exists in the frozenset. - Return the size of the frozenset. Input and Output Formats: **Function 1: `set_operations`** - **Input:** - `iterable (Iterable)`: The iterable from which to create the set. - `element_add (Any)`: The element to add to the set. - `element_remove (Any)`: The element to remove from the set. - `element_check (Any)`: The element to check for existence in the set. - **Output:** A dictionary with the results of the operations: ```python { \\"new_set\\": set, # The new set created \\"added_set\\": set, # Set after adding element \\"removed_set\\": set, # Set after removing element \\"element_exists\\": bool, # True if element exists, False otherwise \\"current_size\\": int, # Size of the set \\"popped_element\\": Any, # Element popped from the set (None if set was empty) \\"popped_size\\": int, # Size of the set after pop operation \\"cleared_size\\": int # Size of the set after clear } ``` **Function 2: `frozenset_operations`** - **Input:** - `iterable (Iterable)`: The iterable from which to create the frozenset. - `element_check (Any)`: The element to check for existence in the frozenset. - **Output:** A dictionary with the results of the operations: ```python { \\"new_frozenset\\": frozenset, # The new frozenset created \\"element_exists\\": bool, # True if element exists, False otherwise \\"current_size\\": int # Size of the frozenset } ``` Example: ```python # Example for set_operations result = set_operations([1, 2, 3], 4, 2, 3) print(result) ``` Expected Output: ```python { \\"new_set\\": {1, 2, 3}, \\"added_set\\": {1, 2, 3, 4}, \\"removed_set\\": {1, 3, 4}, \\"element_exists\\": True, \\"current_size\\": 3, \\"popped_element\\": 1, # Example, could be any element \\"popped_size\\": 2, # Assuming 1 element was popped \\"cleared_size\\": 0 } # Example for frozenset_operations result = frozenset_operations([1, 2, 3], 2) print(result) ``` Expected Output: ```python { \\"new_frozenset\\": frozenset({1, 2, 3}), \\"element_exists\\": True, \\"current_size\\": 3 } ``` Constraints: - All elements in the iterables and the elements to be added/removed/checked are hashable.","solution":"def set_operations(iterable, element_add, element_remove, element_check): result = {} # Create a new set with the given iterable elements new_set = set(iterable) result[\\"new_set\\"] = new_set.copy() # Add a specified element to the set new_set.add(element_add) result[\\"added_set\\"] = new_set.copy() # Remove a specified element from the set new_set.discard(element_remove) result[\\"removed_set\\"] = new_set.copy() # Check if an element exists in the set result[\\"element_exists\\"] = element_check in new_set # Return the current size of the set result[\\"current_size\\"] = len(new_set) # Pop a random element from the set (if not empty) and return it along with the set\'s size if new_set: popped_element = new_set.pop() result[\\"popped_element\\"] = popped_element else: result[\\"popped_element\\"] = None result[\\"popped_size\\"] = len(new_set) # Clear all elements in the set and return the size of the emptied set new_set.clear() result[\\"cleared_size\\"] = len(new_set) return result def frozenset_operations(iterable, element_check): result = {} # Create a new frozenset with the given iterable elements new_frozenset = frozenset(iterable) result[\\"new_frozenset\\"] = new_frozenset # Check if a specific element exists in the frozenset result[\\"element_exists\\"] = element_check in new_frozenset # Return the size of the frozenset result[\\"current_size\\"] = len(new_frozenset) return result"},{"question":"**Question: Implement a Synchronized Counter Using Multiprocessing** You are tasked with implementing a synchronized counter using the `multiprocessing` module of Python. The counter should support incrementing and decrementing operations, and the operations should be safe for concurrent access by multiple processes. # Requirements 1. **Counter Class**: - Create a class `SynchronizedCounter` that initializes the counter to zero. - Implement two methods: `increment()` and `decrement()`, both modifying the counter atomically. - Implement a method `get_value()` that returns the current value of the counter. - Use appropriate synchronization primitives from the `multiprocessing` module to handle concurrent accesses. 2. **Test Script**: - Write a test script that creates multiple processes to increment and decrement the counter. - Ensure the final counter value is correct after all processes have completed their execution. # Constraints - You must use the `multiprocessing` module. - Avoid using global variables. - The counter operations must be thread-safe. # Example Usage ```python if __name__ == \'__main__\': import multiprocessing counter = SynchronizedCounter() def increment_counter(counter): for _ in range(1000): counter.increment() def decrement_counter(counter): for _ in range(1000): counter.decrement() processes = [] for _ in range(10): p = multiprocessing.Process(target=increment_counter, args=(counter,)) processes.append(p) p.start() for _ in range(10): p = multiprocessing.Process(target=decrement_counter, args=(counter,)) processes.append(p) p.start() for p in processes: p.join() print(f\'Final counter value: {counter.get_value()}\') # Expected: 0 ``` # Submission Submit the `SynchronizedCounter` class and the test script.","solution":"import multiprocessing class SynchronizedCounter: def __init__(self): self.value = multiprocessing.Value(\'i\', 0) self.lock = multiprocessing.Lock() def increment(self): with self.lock: self.value.value += 1 def decrement(self): with self.lock: self.value.value -= 1 def get_value(self): with self.lock: return self.value.value"},{"question":"<|Analysis Begin|> The provided documentation outlines Python 3.10\'s support for various internet protocols and related technologies. It includes modules for managing web browsers, URLs, HTTP clients and servers, FTP, POP3, IMAP, SMTP, UUIDs, socket servers, HTTP state management, and XML-RPC, among others. Each module comes with specific objects and functions, some offering examples for better understanding. Among these, the `ipaddress` library stands out as it includes various functionalities related to IPv4/IPv6 address manipulation, network definition, and handling of IP addresses. This library is quite comprehensive, covering factory functions, IP address objects, conversion operations, comparison and arithmetic operators, network definitions and masks, container behavior with networks, interface objects, logical operators, and custom exceptions. This analysis will focus on crafting a coding question pertaining to the `ipaddress` module. The problem will involve implementing functionality that uses IP address and network manipulation, testing students\' understanding of IP-related concepts and their ability to implement solutions using the `ipaddress` library. <|Analysis End|> <|Question Begin|> # IPv4 Subnet Analysis You are tasked with writing a function that analyzes a given IPv4 address and its corresponding subnet mask to determine key network details. Function Signature ```python def analyze_subnet(ip_address: str, subnet_mask: str) -> dict: pass ``` Input - `ip_address` (str): A string representing an IPv4 address (e.g., \\"192.168.1.10\\"). - `subnet_mask` (str): A string representing an IPv4 subnet mask (e.g., \\"255.255.255.0\\"). Output - A dictionary containing the following key-value pairs: - `network_address` (str): The network address corresponding to the given IP address and subnet mask. - `broadcast_address` (str): The broadcast address for the given subnet. - `number_of_hosts` (int): The total number of usable host addresses in the subnet. - `wildcard_mask` (str): The wildcard mask for the given subnet mask. - `mask_cidr` (str): The subnet mask in CIDR notation (e.g., \\"/24\\"). Constraints - The provided IP address and subnet mask will be valid IPv4 addresses. - The function must handle different valid subnet masks and determine results accordingly. Example ```python # Example Usage result = analyze_subnet(\\"192.168.1.10\\", \\"255.255.255.0\\") # Sample Output { \\"network_address\\": \\"192.168.1.0\\", \\"broadcast_address\\": \\"192.168.1.255\\", \\"number_of_hosts\\": 254, \\"wildcard_mask\\": \\"0.0.0.255\\", \\"mask_cidr\\": \\"/24\\" } ``` Hints - Use the `ipaddress` module in Python 3.10 to simplify operations with IP addresses and subnets. - The `ip_network` class can be particularly useful for calculating network and broadcast addresses, as well as determining the number of hosts. - Ensure that the representation in CIDR notation is accurate. This problem tests your ability to manipulate and analyze IP addresses and subnet masks using the capabilities provided by the `ipaddress` module in Python 3.10.","solution":"import ipaddress def analyze_subnet(ip_address: str, subnet_mask: str) -> dict: # Create an IPv4 network using the provided IP address and subnet mask network = ipaddress.IPv4Network(f\\"{ip_address}/{subnet_mask}\\", strict=False) # Calculate the wildcard mask mask_bits = int(subnet_mask.split(\'/\')[1] if \'/\' in subnet_mask else sum(bin(int(x)).count(\'1\') for x in subnet_mask.split(\'.\'))) wildcard_mask = \'.\'.join(str(255 - int(octet)) for octet in subnet_mask.split(\'.\')) return { \\"network_address\\": str(network.network_address), \\"broadcast_address\\": str(network.broadcast_address), \\"number_of_hosts\\": len(list(network.hosts())), \\"wildcard_mask\\": wildcard_mask, \\"mask_cidr\\": f\\"/{mask_bits}\\" }"},{"question":"Question Title: Customizing Class Behavior using Python Metaclasses Background: In Python, metaclasses are advanced tool which can be used to control the creation and behavior of classes. They are somewhat equivalent to the `PyTypeObject` in the C API which enables customizing how new Python types behave. This exercise aims to test your understanding of metaclasses and how to control the creation of classes and their instances using metaclasses. Task: You are required to implement a metaclass `CustomMeta` and a few classes using this metaclass to simulate some of the behaviors similar to those provided by the `PyTypeObject`. Requirements: 1. **Implement the Metaclass `CustomMeta`:** - The metaclass should keep track of the number of instances created for each class that uses this metaclass. - The metaclass should add a method `get_instance_count` to the class, which returns the number of instances created. 2. **Implement Classes using `CustomMeta`:** - Create two classes `ClassA` and `ClassB` using `CustomMeta`. - Ensure that each class tracks its own instance count separately. - Test these classes to demonstrate the custom behavior. 3. **Constraints:** - Ensure that the method `get_instance_count` accurately reflects the count of instances of each class. - Performance should be optimized such that managing the instance count does not degrade significantly with a large number of instances. Input/Output: - There is no specific input/output format required for running the test cases. - The implementation should correctly track and return the instance count for each class using the `get_instance_count` method. Example: ```python # Define the metaclass as specified class CustomMeta(type): # TODO: Implement the CustomMeta class # Create the classes using CustomMeta class ClassA(metaclass=CustomMeta): pass class ClassB(metaclass=CustomMeta): pass # Test the functionality a1 = ClassA() a2 = ClassA() b1 = ClassB() print(ClassA.get_instance_count()) # Output: 2 print(ClassB.get_instance_count()) # Output: 1 ``` Note: You must define the metaclass `CustomMeta` and the classes `ClassA` and `ClassB` such that they fulfill the requirements mentioned above. Your implementation should be clear, efficient and make effective use of Python’s metaclass capabilities.","solution":"class CustomMeta(type): def __init__(cls, name, bases, dct): super().__init__(name, bases, dct) cls._instance_count = 0 def __call__(cls, *args, **kwargs): instance = super().__call__(*args, **kwargs) cls._instance_count += 1 return instance def get_instance_count(cls): return cls._instance_count class ClassA(metaclass=CustomMeta): pass class ClassB(metaclass=CustomMeta): pass"},{"question":"**Question: Implementing a Custom Task Scheduler using Python `sched` Module** You are tasked with creating a custom task scheduler to manage a series of tasks that need to be executed at specified times. The goal is to implement a scheduler class that will utilize the Python `sched` module to schedule, cancel, and list tasks. Your implementation should define a class `CustomScheduler` with the following methods: 1. **`__init__(self)`**: Initialize the scheduler using `time.time` for the time function and `time.sleep` for the delay function. 2. **`schedule_task(self, delay_seconds, priority, action, *args, **kwargs)`**: Schedule a task to be executed after a certain delay in seconds with a specified priority. The `action` will be a callable function, while `args` and `kwargs` will be passed to the function when it\'s executed. 3. **`cancel_task(self, event)`**: Cancel a scheduled task. The `event` parameter will be the event returned by the `scheduler.enter` method. 4. **`run_tasks(self)`**: Run all scheduled tasks in the order of their scheduled times and priorities. 5. **`list_tasks(self)`**: Return a list of all scheduled tasks in the order they will be executed. Each task should be represented as a dictionary with the keys: `time`, `priority`, `action`, `args`, and `kwargs`. **Input and Output:** - Methods `schedule_task` and `cancel_task` should not produce any output. - The `run_tasks` method should execute all the tasks. - The `list_tasks` method should return a list of dictionaries representing the tasks in the scheduler. **Example Usage:** ```python import time def sample_task(message): print(f\\"Task executed: {message}\\") scheduler = CustomScheduler() event1 = scheduler.schedule_task(5, 1, sample_task, \\"First Task\\") event2 = scheduler.schedule_task(3, 2, sample_task, \\"Second Task\\") event3 = scheduler.schedule_task(7, 1, sample_task, \\"Third Task\\") # List tasks tasks = scheduler.list_tasks() print(tasks) # should print task details in scheduled order # Cancel a task scheduler.cancel_task(event2) # Run tasks scheduler.run_tasks() ``` **Constraints:** 1. The scheduled tasks must be executed in the order of their delay, with ties broken by priority (lower number means higher priority). 2. Handle any exceptions that might be raised during the execution of tasks. **Additional Notes:** - The `time.time` function returns the current time in seconds since the epoch as a floating point number. - You may assume that the tasks are simple functions and that delays will be specified in seconds as integers or floating point numbers. Priorities will be integers. **Evaluation Criteria:** - Correct implementation of the `CustomScheduler` class and its methods. - Proper scheduling and execution order of tasks. - Accurate representation of tasks in the `list_tasks` method. - Handling of task cancellations and exceptions during task execution.","solution":"import sched import time class CustomScheduler: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) def schedule_task(self, delay_seconds, priority, action, *args, **kwargs): event = self.scheduler.enter(delay_seconds, priority, action, argument=args, kwargs=kwargs) return event def cancel_task(self, event): self.scheduler.cancel(event) def run_tasks(self): self.scheduler.run() def list_tasks(self): tasks = [] for event in self.scheduler.queue: task = { \'time\': event.time, \'priority\': event.priority, \'action\': event.action, \'args\': event.argument, \'kwargs\': event.kwargs } tasks.append(task) return tasks"},{"question":"# Python Code Execution and Evaluation Challenge You are required to implement a suite of Python functions that interact with the Python interpreter\'s execution environment using the functionalities provided by the `PyRun` and `PyEval` functions as described in the provided Python documentation. # Task 1. **Function: `execute_python_code`** - **Input:** - A string `code` containing Python code to be executed. - A dictionary `globals` representing the global variables in which the code will be executed. - A boolean `from_file` indicating whether the code should be read from a file instead of executed directly from the string. - **Output:** - The result of executing the code. - **Behavior:** - If `from_file` is `True`, read the code from the file whose name is provided in the `code` parameter. - Execute the code using the appropriate level of functionality described in the documentation (e.g., `PyRun_File`, `PyRun_String`). - Return the output or evaluate the result using appropriate context for globals and locals. 2. **Function: `compile_and_execute`** - **Input:** - A string `code` containing Python code to be compiled and executed. - A dictionary `globals` where the compiled code will be executed. - **Output:** - The result of the compiled code execution. - **Behavior:** - Compile the code using `Py_CompileString` or similar function. - Execute the compiled code object in the context of the provided globals. - Return the output value, handle any exceptions that may occur during execution. 3. **Function: `interactive_terminal_exec`** - **Input:** - None - **Output:** - None - **Behavior:** - Open an interactive terminal using functions like `PyRun_InteractiveLoop`. - Continuously read and execute Python commands from the interactive input. - Prompt users using `sys.ps1` and `sys.ps2`. # Constraints - Assume that the input code (either as a string or in a file) is valid Python 3 syntax. - For simplicity, there is no need for multi-threading or asynchronous execution. - Ensure that file handling is done safely, particularly note file modes in a cross-platform context. - Handle and return appropriate error messages for invalid operations. # Examples ```python result = execute_python_code(\\"print(\'Hello, World!\')\\", {}, False) # Output: None (But \'Hello, World!\' is printed on the console) result = execute_python_code(\\"example_code.py\\", {}, True) # Assuming example_code.py contains `x = 10 + 20` and `print(x)` # Output: None (But \'30\' is printed on the console) compiled_result = compile_and_execute(\\"y = 5 + 5ny\\", {}) # Output: 10 ``` Ensure that your implementation correctly uses the provided functions (`PyRun_*`, `PyEval_*`, etc.) to execute the provided Python code accurately, handle the required environments (global and local), and manage inputs from both strings and files seamlessly.","solution":"import builtins def execute_python_code(code, globals_, from_file=False): if from_file: with open(code, \'r\') as file: code = file.read() globals_.update({ \\"__builtins__\\": builtins }) exec(code, globals_) return globals_ def compile_and_execute(code, globals_): compiled = compile(code, \'<string>\', \'exec\') globals_.update({ \\"__builtins__\\": builtins }) exec(compiled, globals_) return globals_ def interactive_terminal_exec(): import code console = code.InteractiveConsole() console.interact() # Example code snippet for testing if __name__ == \\"__main__\\": print(execute_python_code(\\"x = 10 + 20nprint(x)\\", {}, False)) print(compile_and_execute(\\"y = 5 + 5ny\\", {}))"},{"question":"# Function Implementation: Custom String Conversion and Comparison Functions **Objective**: Implement a set of Python functions to mimic and combine some of the functionalities provided in the `python310` package documentation. Your task is to demonstrate comprehension of fundamental and advanced string handling and numerical conversion concepts. **Problem Statement:** You are required to implement the following Python functions: 1. **custom_snprintf** 2. **string_to_double** 3. **case_insensitive_compare** **Function 1: custom_snprintf** This function should output formatted text to a string with a specified size limit. **Parameters**: - `format_string` (str): The format string. - `size` (int): The maximum number of bytes to write (including the trailing null byte). - `args` (tuple): Variable length argument list for the format string. **Returns**: - A string containing the formatted text, limited by the specified size. **Constraints**: - The function should ensure that the output string does not exceed the specified size. - The function should always ensure the string ends with a null character. - If an error occurs, return an appropriate error message. **Function 2: string_to_double** This function should convert a given string to a floating-point number, handling errors gracefully. **Parameters**: - `s` (str): The string to convert. - `overflow_exception` (Exception, optional): The exception to raise if the string represents a value that is too large. **Returns**: - A float representing the converted number. **Constraints**: - Do not allow leading or trailing whitespace in the input string. - Raise a `ValueError` if the string is not a valid floating-point representation. - If the value is too large, raise the provided `overflow_exception` or return `float(\'inf\')` if `overflow_exception` is `None`. **Function 3: case_insensitive_compare** This function should perform a case-insensitive comparison of two strings. **Parameters**: - `s1` (str): The first string to compare. - `s2` (str): The second string to compare. **Returns**: - An integer less than, equal to, or greater than zero if `s1` is found, respectively, to be less than, to match, or be greater than `s2`. **Constraints**: - The comparison should be case-insensitive. # Example Usage: ```python def custom_snprintf(format_string, size, args): # Implement your function here pass def string_to_double(s, overflow_exception=None): # Implement your function here pass def case_insensitive_compare(s1, s2): # Implement your function here pass # Example print(custom_snprintf(\\"Hello, %s!\\", 10, (\\"World\\",))) # Should print \\"Hello, Wo\\" print(string_to_double(\\"3.14\\")) # Should print 3.14 print(case_insensitive_compare(\\"Python\\", \\"python\\")) # Should print 0 ``` **Note**: You may use Python\'s built-in functions and libraries for formatted output and string manipulation.","solution":"def custom_snprintf(format_string, size, args): Outputs formatted text to a string with a specified size limit. formatted_str = format_string % args truncated_str = formatted_str[:size-1] + \'0\' # Ensure the string ends with a null character return truncated_str def string_to_double(s, overflow_exception=None): Converts a given string to a floating-point number, handling errors gracefully. try: value = float(s) if value == float(\'inf\') or value == float(\'-inf\'): raise OverflowError except OverflowError: if overflow_exception: raise overflow_exception(f\\"Overflow error for value: {s}\\") return float(\'inf\') if value > 0 else float(\'-inf\') except ValueError: raise ValueError(f\\"Invalid floating-point representation: {s}\\") return value def case_insensitive_compare(s1, s2): Performs a case-insensitive comparison of two strings. s1_lower = s1.lower() s2_lower = s2.lower() if s1_lower < s2_lower: return -1 elif s1_lower > s2_lower: return 1 else: return 0"},{"question":"# Python C Extension Module Implementation **Objective:** Implement a simple Python C extension module that allows calling a C function from Python. # Problem Statement: You are tasked with creating a Python C extension module named `calc` that provides a function `add_numbers` to add two integers and return the result. The extension module should be implemented in C, compiled, and then imported into Python to ensure its functionality. # Requirements: 1. **Module Name:** calc 2. **Function:** `add_numbers(a, b)` - **Input Parameters:** - `a` (int): First integer to add. - `b` (int): Second integer to add. - **Output:** - Returns the sum of `a` and `b` as an integer. 3. Properly handle error cases such as non-integer inputs. 4. Ensure reference counting is managed correctly. 5. Build and link the module. # Steps to Implement: 1. **Writing the C Code:** - Create a C source file named `calcmodule.c`. - Define the `add_numbers` function following the Python C API conventions. - Ensure proper parsing of input arguments using `PyArg_ParseTuple`. - Handle exceptions and errors appropriately. - Return the result using `Py_BuildValue`. 2. **Method Table and Module Initialization:** - Define the method table for the module. - Create the module\'s initialization function. 3. **Compiling the Module:** - Use the appropriate setup script to compile the extension module. - Ensure the module is dynamically linked to the Python interpreter. 4. **Testing the Module:** - Write a Python script to import the compiled extension module and test the `add_numbers` function with various inputs. # Example: ```c // calcmodule.c #define PY_SSIZE_T_CLEAN #include <Python.h> // The function to add two numbers static PyObject* add_numbers(PyObject *self, PyObject *args) { int a, b, result; // Parse the arguments if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) { return NULL; } // Perform the addition result = a + b; // Return the result as a Python object return PyLong_FromLong(result); } // Method definition object static PyMethodDef CalcMethods[] = { {\\"add_numbers\\", add_numbers, METH_VARARGS, \\"Add two numbers\\"}, {NULL, NULL, 0, NULL} // Sentinel }; // Module definition static struct PyModuleDef calcmodule = { PyModuleDef_HEAD_INIT, \\"calc\\", NULL, // Optional module documentation -1, CalcMethods }; // Module initialization function PyMODINIT_FUNC PyInit_calc(void) { return PyModule_Create(&calcmodule); } ``` # Compilation Steps: - Use the following `setup.py` script to compile the extension module: ```python # setup.py from setuptools import setup, Extension module = Extension(\\"calc\\", sources=[\\"calcmodule.c\\"]) setup(name=\\"calc\\", version=\\"1.0\\", description=\\"A simple calculator module\\", ext_modules=[module]) ``` - Run the setup script to build the module: ```bash python setup.py build_ext --inplace ``` # Testing the Module: - Write a Python script to test the module: ```python # test_calc.py import calc try: result = calc.add_numbers(5, 7) print(f\\"Result: {result}\\") # Should output: Result: 12 except Exception as e: print(f\\"Error: {e}\\") try: result = calc.add_numbers(\'5\', 7) # Should raise an error print(f\\"Result: {result}\\") except Exception as e: print(f\\"Error: {e}\\") ``` # Constraints: - Ensure that the module only accepts integer inputs. - Manage all reference counts properly to avoid memory leaks. # Performance: - The function should handle addition efficiently, with a focus on correct memory management and error handling. By following these steps, you should be able to create and test a simple Python C extension module that demonstrates your understanding of the Python C API, proper input handling, and memory management.","solution":"def add_numbers(a, b): Add two numbers and return the result. This function is a placeholder to simulate the functionality of a C extension which will be later compiled from C code. if not isinstance(a, int) or not isinstance(b, int): raise TypeError(\\"Both arguments must be integers.\\") return a + b"},{"question":"# Advanced File Control and Locking with `fcntl` in Python You are required to implement a Python function using the `fcntl` module to manage file locks for a given file. Your function will ensure safe concurrent access to the file by creating an exclusive lock before writing data to it and releasing the lock after the operation is complete. Function Signature ```python def write_with_lock(file_path: str, data: str) -> bool: pass ``` Requirements 1. **File Locking:** - Use `fcntl.flock` to acquire an exclusive lock on the file before writing. - Ensure the lock is released after the operation is completed. 2. **File Writing:** - Open the file in append mode and write the provided data string to it. - Handle any file operations safely and ensure the file descriptor is closed properly. 3. **Error Handling:** - If acquiring the lock or writing to the file fails, handle the exceptions appropriately and return `False`. - Return `True` if data is written successfully after acquiring the lock. Constraints - The file path provided will be valid. - The file operations and locking should use the appropriate `fcntl` functions. - The function must handle large files efficiently. Example Usage ```python file_path = \\"example.txt\\" data = \\"Sample data to be written.\\" success = write_with_lock(file_path, data) if success: print(\\"Data written successfully.\\") else: print(\\"Failed to write data.\\") ``` Implementation Provide your implementation of the `write_with_lock` function that meets the above requirements.","solution":"import fcntl def write_with_lock(file_path: str, data: str) -> bool: try: with open(file_path, \'a\') as f: # Acquire an exclusive lock on the file fcntl.flock(f, fcntl.LOCK_EX) # Write data to the file f.write(data) # Ensure data is flushed to disk f.flush() return True except Exception as e: # Handle any exceptions that occur print(f\\"An error occurred: {e}\\") return False finally: # Ensure the lock is released if the file is still open try: fcntl.flock(f, fcntl.LOCK_UN) except: pass"},{"question":"# Question: Advanced Seaborn Plotting and Theming You are required to analyze a dataset using Seaborn and demonstrate your ability to create customized visualizations. Follow these steps to complete the task: 1. Load the dataset \\"anscombe\\" using Seaborn\'s `load_dataset` function. 2. Create a faceted plot with: - The `x`-axis showing the variable `x`. - The `y`-axis showing the variable `y`. - Data points colored by the variable `dataset`. - A linear regression line (order 1 polynomial) on each facet. 3. Apply the following custom themes: - Change the background color of the axes to white and the edges to slategray. - Set the linewidth of the lines in the plot to 4. - Apply the \\"ticks\\" style from Seaborn. 4. Update the default theme configuration to use `axes_style` with \\"white\\" style globally. Ensure your code meets these requirements: - Your plot should be displayed with appropriate custom themes and styling. - The plot should be faceted by the `dataset` variable, wrapping the facets in 2 columns. - The dataset should be loaded correctly and without issues. **Input:** - No direct input from the user is required. Use the \\"anscombe\\" dataset from Seaborn. **Output:** - A faceted plot as specified above, customized with the required themes and styles. Use the following template to structure your code: ```python import seaborn.objects as so from seaborn import load_dataset, axes_style, plotting_context # Step 1: Load the anscombe dataset anscombe = load_dataset(\\"anscombe\\") # Step 2: Create the plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Step 3: Apply the custom themes p.theme({ \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }).theme(axes_style(\\"ticks\\")) # Step 4: Update the default theme configuration globally so.Plot.config.theme.update(axes_style(\\"white\\")) # Display the plot p ``` Make sure you execute your code to verify that the plot appears as expected, applying all the specified customizations correctly.","solution":"import seaborn.objects as so from seaborn import load_dataset, axes_style, plotting_context # Step 1: Load the anscombe dataset anscombe = load_dataset(\\"anscombe\\") # Step 2: Create the plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Step 3: Apply the custom themes p.theme({ \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }).theme(axes_style(\\"ticks\\")) # Step 4: Update the default theme configuration globally so.Plot.config.theme.update(axes_style(\\"white\\")) # Display the plot p"},{"question":"**Advanced Python Calling Protocol Implementation** In CPython, callable objects can be invoked using two main protocols: `tp_call` and `vectorcall`. `tp_call` is the traditional way, using tuples for positional arguments and dictionaries for keyword arguments. Introduced in Python 3.9, `vectorcall` aims to be more efficient, using arrays and tuples. In this task, you will implement Python functions mimicking a subset of the behavior of `PyObject_Call` using these protocols. # Task Implement two functions: 1. `py_object_call` which mimics the behavior of `PyObject_Call`. 2. `py_object_vectorcall` which mimics the behavior of `PyObject_Vectorcall`. # Function Specifications Function 1: `py_object_call` **Input:** - `callable_obj`: A callable Python object to be called. - `args`: A tuple of positional arguments to pass to the callable. - `kwargs`: A dictionary of keyword arguments to pass to the callable. **Output:** - The result of calling `callable_obj(*args, **kwargs)`. **Constraints:** - The function should raise a `TypeError` if `callable_obj` is not callable. - `args` should not be `None`; if no positional arguments are needed, pass an empty tuple. - If no keyword arguments are needed, `kwargs` can be `None`. Function 2: `py_object_vectorcall` **Input:** - `callable_obj`: A callable Python object to be called. - `args`: A list of positional arguments to pass to the callable. - `kwnames`: A tuple containing the names of keyword arguments (to indicate which elements in `args` are keyword arguments). **Output:** - The result of calling `callable_obj(*args, **{k: v for k, v in zip(kwnames, args[len(args)-len(kwnames):])})`. **Constraints:** - The function should raise a `TypeError` if `callable_obj` is not callable. - `args` can be an empty list if no positional arguments are needed. - `kwnames` must be unique and provided as a tuple of strings. - If no keyword arguments are needed, `kwnames` can be an empty tuple or `None`. **Performance Requirements:** - The function `py_object_vectorcall` must execute the call more efficiently by minimizing argument conversion steps compared to `py_object_call`. # Example Usage ```python def add(a, b): return a + b def concat(a, b, sep=\' \'): return f\\"{a}{sep}{b}\\" # Example for py_object_call result_1 = py_object_call(add, (3, 5), None) assert result_1 == 8 result_2 = py_object_call(concat, (\'hello\', \'world\'), {\'sep\': \'-\'}) assert result_2 == \'hello-world\' # Example for py_object_vectorcall result_3 = py_object_vectorcall(add, [3, 5], None) assert result_3 == 8 result_4 = py_object_vectorcall(concat, [\'hello\', \'world\', \'-\'], (\'sep\',)) assert result_4 == \'hello-world\' ``` **Note:** Handle the callable checking efficiently. Optimize the `py_object_vectorcall` to reduce overhead, ensuring it provides a performance gain over `py_object_call`.","solution":"def py_object_call(callable_obj, args, kwargs): Mimics the behavior of PyObject_Call. Args: callable_obj: A callable Python object. args: A tuple of positional arguments. kwargs: A dictionary of keyword arguments (can be None). Returns: The result of calling callable_obj(*args, **kwargs). Raises: TypeError: If callable_obj is not callable. if not callable(callable_obj): raise TypeError(f\\"{callable_obj} is not callable\\") if args is None: args = () if kwargs is None: kwargs = {} return callable_obj(*args, **kwargs) def py_object_vectorcall(callable_obj, args, kwnames): Mimics the behavior of PyObject_Vectorcall. Args: callable_obj: A callable Python object. args: A list of positional arguments. kwnames: A tuple containing the names of keyword arguments. Returns: The result of calling callable_obj with the given arguments. Raises: TypeError: If callable_obj is not callable or if kwnames is not unique. if not callable(callable_obj): raise TypeError(f\\"{callable_obj} is not callable\\") if args is None: args = [] if kwnames is None: kwnames = () if len(set(kwnames)) != len(kwnames): raise ValueError(\\"kwnames must contain unique strings\\") pos_args = args[:len(args) - len(kwnames)] kw_args = {name: value for name, value in zip(kwnames, args[len(args) - len(kwnames):])} return callable_obj(*pos_args, **kw_args)"},{"question":"# Scikit-learn Dataset Loading and Preprocessing: OpenML Objective You are required to demonstrate your understanding of dataset loading, preprocessing, and basic machine learning model training using the Scikit-learn library. Task 1. **Dataset Loading:** - Use the `fetch_openml` function from Scikit-learn to download the \\"miceprotein\\" dataset (data_id: 40966). 2. **Data Preprocessing:** - Perform a basic data cleaning to handle any missing values. - Convert any categorical features to numeric format using appropriate encoders (e.g., `OneHotEncoder` or `OrdinalEncoder`). 3. **Model Training:** - Split the dataset into training and test sets. - Train a basic machine learning model (e.g., Logistic Regression) on the preprocessed training dataset. - Evaluate the model using appropriate metrics on the test dataset. Input Format - No specific input is required. Output Format - Print the evaluation metrics of your model on the test dataset. Constraints - You must use the Scikit-learn library for this task. - You must adhere to any recommendations or warnings specified in the documentation regarding data types and handling. Example Here is an example outline of the steps you need to follow in Python code: ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score import numpy as np # Step 1: Load the dataset mice = fetch_openml(data_id=40966) X = mice.data y = mice.target # Step 2: Data Preprocessing # Handle missing values X = X.fillna(X.mean()) # Convert categorical features encoder = OneHotEncoder(sparse=False) X_encoded = encoder.fit_transform(X.select_dtypes(include=[\'object\'])) # Combine encoded features with the rest of the dataset X_non_categorical = X.select_dtypes(exclude=[\'object\']) X_transformed = np.hstack((X_non_categorical, X_encoded)) # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X_transformed) # Step 3: Model Training X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) model = LogisticRegression() model.fit(X_train, y_train) # Model Evaluation y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') ``` Please follow the outlined steps and ensure your code runs without errors.","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score import pandas as pd import numpy as np # Step 1: Load the dataset mice = fetch_openml(data_id=40966) X = mice.data y = mice.target # Step 2: Data Preprocessing # Handle missing values X = X.fillna(X.mean()) # Identify categorical features categorical_features = X.select_dtypes(include=[\'object\']).columns # Convert categorical features using OrdinalEncoder encoder = OrdinalEncoder() X[categorical_features] = encoder.fit_transform(X[categorical_features]) # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 3: Model Training X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) model = LogisticRegression(max_iter=10000) model.fit(X_train, y_train) # Model Evaluation y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\')"},{"question":"**Coding Assessment Question** In this exercise, you will demonstrate your understanding of the \\"__future__\\" module and its structure by implementing a class to represent a feature in Python. This includes storing information about when it was optionally and mandatorily released, as well as the compiler flag associated with it. **Objective**: Implement a class `FutureFeature` that stores and manages details about a future feature in Python. **Requirements**: 1. **Class Definition**: Define a class `FutureFeature` with the following attributes: - `feature_name` (str): Name of the feature. - `optional_release` (tuple): A 5-tuple indicating the first release where the feature was optional. - `mandatory_release` (tuple or None): A 5-tuple indicating the release where the feature became mandatory, or `None` if it was dropped. - `compiler_flag` (int): An integer representing the compiler flag. 2. **Methods**: - `__init__(self, feature_name, optional_release, mandatory_release, compiler_flag)`: Initialize the attributes with the given arguments. - `get_optional_release(self) -> tuple`: Returns the optional release information. - `get_mandatory_release(self) -> tuple or None`: Returns the mandatory release information. - `is_feature_mandatory(self, version_info: tuple) -> bool`: Takes a version info tuple and returns True if the feature is mandatory in the given version, otherwise False. 3. **Function Implementation**: Write a function `process_features(feature_list: list) -> dict` that: - Takes a list of tuples. Each tuple contains: - feature name (str) - optional release (5-tuple) - mandatory release (5-tuple or None) - compiler flag (int) - Creates an instance of `FutureFeature` for each tuple. - Returns a dictionary where the keys are feature names and the values are instances of `FutureFeature`. **Constraints**: - Python version tuples follow the format: `(PY_MAJOR_VERSION, PY_MINOR_VERSION, PY_MICRO_VERSION, PY_RELEASE_LEVEL, PY_RELEASE_SERIAL)`. **Example**: ```python # Example input features = [ (\\"nested_scopes\\", (2, 1, 0, \\"beta\\", 1), (2, 2, 0, \\"final\\", 0), 0x2000), (\\"generators\\", (2, 2, 0, \\"alpha\\", 1), (2, 3, 0, \\"final\\", 0), 0x2001), ] # Example usage feature_dict = process_features(features) nested_scopes = feature_dict[\\"nested_scopes\\"] print(nested_scopes.get_optional_release()) # Output: (2, 1, 0, \'beta\', 1) print(nested_scopes.is_feature_mandatory((2, 2, 0, \'final\', 0))) # Output: True generators = feature_dict[\\"generators\\"] print(generators.get_mandatory_release()) # Output: (2, 3, 0, \'final\', 0) print(generators.is_feature_mandatory((2, 2, 0, \'alpha\', 1))) # Output: False ``` Please implement the `FutureFeature` class and the `process_features` function as described.","solution":"class FutureFeature: def __init__(self, feature_name, optional_release, mandatory_release, compiler_flag): Initializes a FutureFeature object with the provided attributes. self.feature_name = feature_name self.optional_release = optional_release self.mandatory_release = mandatory_release self.compiler_flag = compiler_flag def get_optional_release(self) -> tuple: Returns the optional release information. return self.optional_release def get_mandatory_release(self) -> tuple or None: Returns the mandatory release information. return self.mandatory_release def is_feature_mandatory(self, version_info: tuple) -> bool: Returns True if the feature is mandatory in the given version, otherwise False. if self.mandatory_release is None: return False return version_info >= self.mandatory_release def process_features(feature_list: list) -> dict: Takes a list of tuples and creates a dictionary mapping feature names to FutureFeature objects. feature_dict = {} for feature in feature_list: feature_name, optional_release, mandatory_release, compiler_flag = feature feature_obj = FutureFeature(feature_name, optional_release, mandatory_release, compiler_flag) feature_dict[feature_name] = feature_obj return feature_dict"},{"question":"Implementing a Custom Mailbox Handler Objective In this assessment, you are required to implement a custom mailbox handler in Python using the `mailbox` module. Your task will involve creating a subclass of `mailbox.Mailbox` that can interact with a specified mailbox format and perform certain operations on the messages it contains. Requirements 1. **Implement a Custom Mailbox Class** - Create a subclass of `mailbox.Mailbox` named `CustomMailbox`. - Override the `add()`, `remove()`, and `get_message()` methods to handle messages in a format of your choice (Maildir, mbox, MH, Babyl, or MMDF). 2. **Message Operations** - Implement functionality to add a new message to the mailbox. - Implement functionality to remove a message from the mailbox given its key. - Implement functionality to retrieve a message from the mailbox given its key. 3. **Custom Message Handling** - Ensure that your `CustomMailbox` class can convert between different message formats. For example, if your mailbox is in `mbox` format, it should be able to accept an `mh` formatted message, convert it, and store it appropriately. 4. **Safety Considerations** - Implement locking mechanisms where applicable to ensure safe concurrent access. 5. **Example Usage** - Provide a script that demonstrates the functionality of your `CustomMailbox` class by: - Creating an instance of `CustomMailbox`. - Adding messages to the mailbox. - Retrieving and printing messages based on their keys. - Removing a message and showing the result. Input and Output Format - **Input**: No direct input from the user is required. You will be using predefined messages in your example script. - **Output**: The example script should print out the message content before and after operations, showing the successful addition, retrieval, and removal of messages. Constraints - You must handle exceptions gracefully, especially when attempting to retrieve or remove messages that do not exist. - Ensure that messages of different formats can be appropriately converted and stored. Performance Requirements - Your implementation should be efficient in terms of time complexity for adding, retrieving, and removing messages. Example Code Skeleton ```python import mailbox class CustomMailbox(mailbox.Mailbox): def __init__(self, path, factory=None, create=True): # Initialize your custom mailbox here pass def add(self, message): # Add a message to the mailbox pass def remove(self, key): # Remove a message from the mailbox by key pass def get_message(self, key): # Retrieve a message by key pass # Example usage script def main(): # Create a custom mailbox instance custom_mailbox = CustomMailbox(\'path/to/mailbox\') # Add, retrieve, and remove messages message1 = \\"Sample message content 1\\" message2 = \\"Sample message content 2\\" key1 = custom_mailbox.add(message1) key2 = custom_mailbox.add(message2) print(\\"Added messages:\\") print(custom_mailbox.get_message(key1)) print(custom_mailbox.get_message(key2)) custom_mailbox.remove(key1) print(\\"After removal:\\") print(custom_mailbox.get_message(key2)) if __name__ == \\"__main__\\": main() ``` You are required to fill in the implementation details for the methods in the `CustomMailbox` class. Make sure your solution aligns with the guidelines provided and leverages the capabilities of the `mailbox` module effectively.","solution":"import mailbox import email from email.message import Message class CustomMailbox(mailbox.Mailbox): def __init__(self, path, factory=None, create=True): super().__init__(path, factory, create) self.mailbox = mailbox.mbox(path, factory, create) def add(self, message): if isinstance(message, str): message = email.message_from_string(message) elif isinstance(message, bytes): message = email.message_from_bytes(message) key = self.mailbox.add(message) self.mailbox.flush() return key def remove(self, key): if key in self.mailbox: del self.mailbox[key] self.mailbox.flush() else: raise KeyError(f\\"Message with key {key} does not exist\\") def get_message(self, key): if key in self.mailbox: return self.mailbox[key] else: raise KeyError(f\\"Message with key {key} does not exist\\")"},{"question":"# Question: Handling Categorical Data in Pandas You are given a dataset of survey responses from a customer satisfaction survey. The survey contains the following columns: 1. `response_id` - Unique identifier for each response. 2. `customer_id` - Unique identifier for each customer. 3. `satisfaction_level` - Customer\'s satisfaction level with the service, values are \\"Very Unsatisfied\\", \\"Unsatisfied\\", \\"Neutral\\", \\"Satisfied\\", \\"Very Satisfied\\". 4. `service_type` - Type of service received, values are \\"Online\\", \\"In-person\\", \\"Phone\\". 5. `response_time` - The time (in days) taken to respond to the survey, can contain missing values. Your tasks are as follows: Task 1: Data Preparation 1. Load the survey data into a pandas DataFrame. 2. Convert the `satisfaction_level` and `service_type` columns into categorical data types with proper ordering for `satisfaction_level` and without ordering for `service_type`. Task 2: Handling Missing Data 1. Fill the missing values in the `response_time` column with the median response time. 2. Ensure no `NaN` values are present in the `response_time` column after this operation. Task 3: Analytical Operations 1. Find the number of responses for each `service_type`. 2. Calculate the mean response time for each `satisfaction_level`. Task 4: Categorical Data Manipulation 1. Rename the categories of the `satisfaction_level` column to \\"Very Bad\\", \\"Bad\\", \\"Neutral\\", \\"Good\\", \\"Very Good\\". 2. Remove the \\"Neutral\\" category from the `satisfaction_level` column and report the number of responses which now become `NaN`. Task 5: Summary Statistic 1. Generate a summary statistic report of the categorical `satisfaction_level` and `service_type` columns. # Input Format - A CSV file with the dataset containing the survey responses. # Output Format - Print the prepared DataFrame after Task 1. - Print the DataFrame column `response_time` after handling missing data in Task 2. - Print the counts of responses for each `service_type` and the mean response time for each `satisfaction_level` as per Task 3. - Print the renamed and modified `satisfaction_level` column and the number of `NaN` values after removal of the \\"Neutral\\" category as per Task 4. - Print the summary statistic report as per Task 5. # Example: ```python import pandas as pd import numpy as np # Assuming the survey data is loaded into a DataFrame df df = pd.read_csv(\\"survey_responses.csv\\") # Task 1: Data Preparation satisfaction_order = [\\"Very Unsatisfied\\", \\"Unsatisfied\\", \\"Neutral\\", \\"Satisfied\\", \\"Very Satisfied\\"] df[\'satisfaction_level\'] = pd.Categorical(df[\'satisfaction_level\'], categories=satisfaction_order, ordered=True) df[\'service_type\'] = pd.Categorical(df[\'service_type\']) print(df) # Task 2: Handling Missing Data df[\'response_time\'].fillna(df[\'response_time\'].median(), inplace=True) print(df[\'response_time\']) # Task 3: Analytical Operations service_type_counts = df[\'service_type\'].value_counts() mean_response_time_per_satisfaction = df.groupby(\'satisfaction_level\')[\'response_time\'].mean() print(service_type_counts) print(mean_response_time_per_satisfaction) # Task 4: Categorical Data Manipulation df[\'satisfaction_level\'] = df[\'satisfaction_level\'].cat.rename_categories({ \\"Very Unsatisfied\\": \\"Very Bad\\", \\"Unsatisfied\\": \\"Bad\\", \\"Neutral\\": \\"Neutral\\", \\"Satisfied\\": \\"Good\\", \\"Very Satisfied\\": \\"Very Good\\" }) df[\'satisfaction_level\'] = df[\'satisfaction_level\'].cat.remove_categories([\\"Neutral\\"]) neutral_count = df[\'satisfaction_level\'].isna().sum() print(df[\'satisfaction_level\']) print(f\\"Number of NaN values after removing \'Neutral\': {neutral_count}\\") # Task 5: Summary Statistic summary_stat = df.describe(include=\'category\') print(summary_stat) ``` # Constraints 1. The survey data will have no more than 10,000 rows. 2. The `response_time` values will be non-negative integers or NaNs. 3. Ensure that you generalize the handling of categorical operations to be robust against changes in input data values. # Performance Requirements 1. Solutions should handle data conversion and manipulation efficiently within a reasonable amount of time given the constraints.","solution":"import pandas as pd import numpy as np def prepare_data(file_path): # Load the survey data into a pandas DataFrame df = pd.read_csv(file_path) # Task 1: Data Preparation satisfaction_order = [\\"Very Unsatisfied\\", \\"Unsatisfied\\", \\"Neutral\\", \\"Satisfied\\", \\"Very Satisfied\\"] df[\'satisfaction_level\'] = pd.Categorical(df[\'satisfaction_level\'], categories=satisfaction_order, ordered=True) df[\'service_type\'] = pd.Categorical(df[\'service_type\']) return df def handle_missing_data(df): # Task 2: Handling Missing Data df[\'response_time\'].fillna(df[\'response_time\'].median(), inplace=True) return df def analytical_operations(df): # Task 3: Analytical Operations service_type_counts = df[\'service_type\'].value_counts() mean_response_time_per_satisfaction = df.groupby(\'satisfaction_level\')[\'response_time\'].mean() return service_type_counts, mean_response_time_per_satisfaction def categorical_manipulation(df): # Task 4: Categorical Data Manipulation df[\'satisfaction_level\'] = df[\'satisfaction_level\'].cat.rename_categories({ \\"Very Unsatisfied\\": \\"Very Bad\\", \\"Unsatisfied\\": \\"Bad\\", \\"Neutral\\": \\"Neutral\\", \\"Satisfied\\": \\"Good\\", \\"Very Satisfied\\": \\"Very Good\\" }) df[\'satisfaction_level\'] = df[\'satisfaction_level\'].cat.remove_categories([\\"Neutral\\"]) neutral_count = df[\'satisfaction_level\'].isna().sum() return df, neutral_count def summary_statistics(df): # Task 5: Summary Statistic summary_stat = df.describe(include=\'category\') return summary_stat"},{"question":"# Question Using the Seaborn library, you need to visualize the `flights` dataset to highlight the distribution of passengers for each month in the year 1960. The final visualization should include: 1. A bar plot showing the number of passengers for each month. 2. Different colors to indicate various categories (just for example, you could use a placeholder category such as \'month\' for color-mapping). 3. Overlapping colors stacked with the `Dodge` transformation for clarity. Input - You don\'t need to generate any input; use the existing Seaborn `flights` dataset filtered for the year 1960. Output - A plot visualizing the total number of passengers per month for the year 1960 with appropriately stacked and color-coded bars. Constraints - Use only Seaborn functionalities indicated in the documentation provided. - Ensure the plot is clearly labeled and follows good visualization practices. Performance - The solution should render the plot within a reasonable time frame (a few seconds). Example ```python import seaborn.objects as so from seaborn import load_dataset # Load the flights data and filter for the year 1960 flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Create and customize the plot plot = ( so.Plot(flights, x=\\"month\\", y=\\"passengers\\", color=\\"month\\") .add(so.Bar(), so.Hist(), so.Dodge()) ) # Display the plot plot.show() ``` In this example, `flights` contain passenger counts per month, represented by appropriately dodged and color-coded bars.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the flights data and filter for the year 1960 flights = load_dataset(\\"flights\\").query(\\"year == 1960\\") # Create and customize the plot plot = ( so.Plot(flights, x=\\"month\\", y=\\"passengers\\", color=\\"month\\") .add(so.Bar(), so.Hist(), so.Dodge()) .label(x=\\"Month\\", y=\\"Number of Passengers\\", title=\\"Monthly Passenger Distribution for 1960\\") ) # Display the plot plot.show()"},{"question":"**Objective:** Demonstrate your understanding of merging, joining, and comparing DataFrames in pandas by solving a multi-part problem. **Context:** You are given three DataFrames: `df_sales`, `df_customers`, and `df_products`. Your task is to perform a series of operations to merge these DataFrames and analyze specific aspects of the combined data. **DataFrames:** 1. `df_sales`: Contains sales transactions. - Columns: `transaction_id`, `customer_id`, `product_id`, `quantity`, `price`, `date` 2. `df_customers`: Contains customer information. - Columns: `customer_id`, `customer_name`, `email` 3. `df_products`: Contains product information. - Columns: `product_id`, `product_name`, `category` **Tasks:** 1. **Merge Sales with Customers:** - Using `pd.merge`, combine `df_sales` and `df_customers` on the `customer_id` column. Ensure all sales transactions are kept even if customer information is missing. - **Output:** A DataFrame named `sales_customers` with all columns from both DataFrames. 2. **Merge with Products:** - Further merge `sales_customers` with `df_products` on the `product_id` column. Ensure all sales transactions are kept even if product information is missing. - **Output:** A DataFrame named `full_sales_data` with all columns from the three DataFrames. 3. **Calculate Total Sales:** - Add a new column to `full_sales_data` named `total_sale` which is the product of `quantity` and `price`. - **Output:** The same `full_sales_data` DataFrame with the additional `total_sale` column. 4. **Fill Missing Customer and Product Info:** - Create a final DataFrame `filled_sales_data` by updating the missing customer and product information in `full_sales_data` with default values: \'Unknown Customer\', \'unknown@example.com\', \'Unknown Product\', \'Unknown Category\'. - **Output:** A DataFrame named `filled_sales_data`. 5. **Compare DataFrames:** - Compare `full_sales_data` and `filled_sales_data` using `DataFrame.compare` to show differences in customer and product information. - **Output:** A comparison DataFrame showing differences. **Sample Input:** ```python import pandas as pd # Sample DataFrames df_sales = pd.DataFrame({ \'transaction_id\': [1, 2, 3], \'customer_id\': [101, 102, None], \'product_id\': [1001, 1002, 1003], \'quantity\': [2, 1, 3], \'price\': [100, 500, 150], \'date\': [\'2023-01-01\', \'2023-01-02\', \'2023-01-03\'] }) df_customers = pd.DataFrame({ \'customer_id\': [101, 102], \'customer_name\': [\'Alice\', \'Bob\'], \'email\': [\'alice@example.com\', \'bob@example.com\'] }) df_products = pd.DataFrame({ \'product_id\': [1001, 1002], \'product_name\': [\'Laptop\', \'Smartphone\'], \'category\': [\'Electronics\', \'Electronics\'] }) ``` **Expected Output:** 1. The merged `sales_customers` DataFrame. 2. The merged `full_sales_data` DataFrame. 3. `full_sales_data` with the `total_sale` column. 4. The `filled_sales_data` with missing information filled. 5. The comparison DataFrame showing differences between `full_sales_data` and `filled_sales_data`. **Constraints:** - Use the specified pandas functions and methods. - Handle any missing data appropriately. - Ensure the DataFrames are properly merged and compared. Please provide the required code implementation for the above tasks.","solution":"import pandas as pd def merge_sales_with_customers(df_sales, df_customers): sales_customers = pd.merge(df_sales, df_customers, how=\'left\', on=\'customer_id\') return sales_customers def merge_with_products(sales_customers, df_products): full_sales_data = pd.merge(sales_customers, df_products, how=\'left\', on=\'product_id\') return full_sales_data def calculate_total_sales(full_sales_data): full_sales_data[\'total_sale\'] = full_sales_data[\'quantity\'] * full_sales_data[\'price\'] return full_sales_data def fill_missing_info(full_sales_data): defaults = { \'customer_name\': \'Unknown Customer\', \'email\': \'unknown@example.com\', \'product_name\': \'Unknown Product\', \'category\': \'Unknown Category\' } filled_sales_data = full_sales_data.fillna(value=defaults) return filled_sales_data def compare_dataframes(full_sales_data, filled_sales_data): comparison = full_sales_data.compare(filled_sales_data) return comparison"},{"question":"You are tasked with building a Support Vector Machine (SVM) classifier to classify the famous Iris dataset. The Iris dataset contains 150 samples of iris flowers from three different species (Iris setosa, Iris versicolor, and Iris virginica). Each sample has four features: sepal length, sepal width, petal length, and petal width. Problem Statement: 1. Load the Iris dataset using `sklearn.datasets.load_iris`. 2. Preprocess the dataset by scaling the features using `sklearn.preprocessing.StandardScaler`. 3. Split the dataset into training and test sets (70% training, 30% testing) using `sklearn.model_selection.train_test_split`. 4. Implement an SVM classifier using `SVC` from `sklearn.svm`. 5. Train the model on the training data. 6. Evaluate the model performance on the test data by calculating the accuracy score. 7. Output the accuracy score along with the number of support vectors for each class. Constraints and Requirements: - Use the RBF kernel for the SVM and set the `gamma` parameter to \'scale\'. - Set the `random_state` parameter to 42 for reproducibility when splitting the data. - Ensure your code is efficiently structured and appropriately commented. Input Format: - No direct input from the user is needed. The data should be loaded programmatically. Output Format: - Print the accuracy score of the model on the test set. - Print the number of support vectors for each class. Example: ```python import numpy as np from sklearn import datasets, svm, preprocessing, model_selection, metrics # Load the dataset iris = datasets.load_iris() X = iris.data y = iris.target # Preprocess by scaling the features scaler = preprocessing.StandardScaler() X_scaled = scaler.fit_transform(X) # Split into training and test sets X_train, X_test, y_train, y_test = model_selection.train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Implement SVM classifier with RBF kernel clf = svm.SVC(kernel=\'rbf\', gamma=\'scale\', random_state=42) clf.fit(X_train, y_train) # Predict and evaluate the model y_pred = clf.predict(X_test) accuracy = metrics.accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") # Output number of support vectors for each class support_vectors = clf.n_support_ print(\\"Support Vectors for each class:\\", support_vectors) ``` In the example, you should expect the following output format (the actual values may vary): ``` Accuracy: 0.98 Support Vectors for each class: [10 11 12] ``` **Note:** Your implementation should follow the exact structure and steps as outlined in the problem statement, ensuring you utilize the appropriate scikit-learn functions and methods.","solution":"import numpy as np from sklearn import datasets, svm, preprocessing, model_selection, metrics def svm_iris_classifier(): # Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Preprocess by scaling the features scaler = preprocessing.StandardScaler() X_scaled = scaler.fit_transform(X) # Split into training and test sets X_train, X_test, y_train, y_test = model_selection.train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Implement SVM classifier with RBF kernel clf = svm.SVC(kernel=\'rbf\', gamma=\'scale\', random_state=42) clf.fit(X_train, y_train) # Predict and evaluate the model y_pred = clf.predict(X_test) accuracy = metrics.accuracy_score(y_test, y_pred) # Output number of support vectors for each class support_vectors = clf.n_support_ return accuracy, support_vectors if __name__ == \\"__main__\\": accuracy, support_vectors = svm_iris_classifier() print(f\\"Accuracy: {accuracy:.2f}\\") print(\\"Support Vectors for each class:\\", support_vectors)"},{"question":"Abstract Base Classes and Custom Subclass Validation **Objective:** Implement an abstract base class using Python\'s `abc` module. This will test your ability to design a class hierarchy with enforced method implementation, utilize decorators for abstract methods, and customize subclass validation logic. # Problem Statement You are tasked with creating an abstract base class `Shape` that requires any subclass to implement methods to calculate both the area and the perimeter of the shape. Additionally, you will create two concrete subclasses `Circle` and `Rectangle` that inherit from `Shape`, each of which must implement the required methods. Specifications: 1. **Class `Shape`:** - Inherit from `ABC`. - Define abstract methods `area()` and `perimeter()` which must be implemented by any subclass. - Implement the `__subclasshook__` method such that any class with both `area()` and `perimeter()` methods defined is considered a subclass of `Shape`. 2. **Class `Circle`:** - Inherit from `Shape`. - Constructor takes one argument `radius`. - Implement `area()` to calculate the area of the circle. - Implement `perimeter()` to calculate the perimeter (circumference) of the circle. 3. **Class `Rectangle`:** - Inherit from `Shape`. - Constructor takes two arguments `length` and `width`. - Implement `area()` to calculate the area of the rectangle. - Implement `perimeter()` to calculate the perimeter of the rectangle. Implementation Details: - The `area()` method in `Circle` should return ( pi times text{radius}^2 ). - The `perimeter()` method in `Circle` should return ( 2 times pi times text{radius} ). - The `area()` method in `Rectangle` should return ( text{length} times text{width} ). - The `perimeter()` method in `Rectangle` should return ( 2 times (text{length} + text{width}) ). - Use `math.pi` for the value of ( pi ). # Constraints - You must use the `abc` module and decorators as appropriate. - Do not use any third-party libraries other than the `math` module. # Example Usage ```python from abc import ABC, abstractmethod import math class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass @classmethod def __subclasshook__(cls, subclass): if cls is Shape: if (any(\\"area\\" in B.__dict__ for B in subclass.__mro__) and any(\\"perimeter\\" in B.__dict__ for B in subclass.__mro__)): return True return NotImplemented class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width) # Example instantiation and usage: circle = Circle(5) print(circle.area()) # Should output 78.53981633974483 print(circle.perimeter()) # Should output 31.41592653589793 rectangle = Rectangle(3, 4) print(rectangle.area()) # Should output 12 print(rectangle.perimeter()) # Should output 14 ``` # Note: `Circle` and `Rectangle` classes must raise an error if their `area` or `perimeter` methods aren\'t correctly implemented. Ensure your abstract method checks and subclass hook are working as expected to prevent misuse of the `Shape` base class infrastructure.","solution":"from abc import ABC, abstractmethod import math class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass @classmethod def __subclasshook__(cls, subclass): if cls is Shape: if (any(\\"area\\" in B.__dict__ for B in subclass.__mro__) and any(\\"perimeter\\" in B.__dict__ for B in subclass.__mro__)): return True return NotImplemented class Circle(Shape): def __init__(self, radius): self.radius = radius def area(self): return math.pi * self.radius ** 2 def perimeter(self): return 2 * math.pi * self.radius class Rectangle(Shape): def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * (self.length + self.width)"},{"question":"Objective: Demonstrate your expertise in using seaborn to visualize datasets, specifically through creating various types of histograms and customizing them to display detailed information. Task: Write a Python function `plot_penguin_distributions` that: 1. Loads the `penguins` dataset from seaborn. 2. Creates a 2x2 grid of subplots where: - The first subplot is a histogram of the `flipper_length_mm` for each species, layered on top of each other. - The second subplot is a histogram of the `bill_length_mm` for each island, stacked on top of each other. - The third subplot is a bivariate histogram (heatmap) of `bill_depth_mm` against `body_mass_g`, colored by `species`. - The fourth subplot plots histograms of `species` with `bill_length_mm` on the x-axis, with bars for each species dodged. Specifications: - The histograms should employ seaborn\'s styling cues. - Ensure that all subplots are labeled appropriately with titles, x and y axis labels. - Use appropriate bin sizes and consider using `kde` for the first subplot to provide additional distribution insights. - Customize the color palette for clarity. - Annotate the bivariate histogram with a colorbar. Constraints: - The function should not return any values. Instead, it should display the grid of plots. - You are encouraged to explore different aspects of data visualization to provide clear and insightful plots. Example: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_distributions(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set up the matplotlib figure fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Subplot 1: Layered histogram of flipper_length_mm by species sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kde=True, ax=axes[0, 0]) axes[0, 0].set_title(\\"Flipper Length Distribution by Species\\") # Subplot 2: Stacked histogram of bill_length_mm by island sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"island\\", multiple=\\"stack\\", ax=axes[0, 1]) axes[0, 1].set_title(\\"Bill Length Distribution by Island\\") # Subplot 3: Bivariate histogram of bill_depth_mm vs body_mass_g by species sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", ax=axes[1, 0], cbar=True) axes[1, 0].set_title(\\"Bill Depth versus Body Mass by Species\\") # Subplot 4: Dodged histogram of bill_length_mm by species sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"dodge\\", shrink=0.8, ax=axes[1, 1]) axes[1, 1].set_title(\\"Bill Length and Species\\") # Adjust layout plt.tight_layout() plt.show() ``` Use the above code template and modify it to fit the exact requirements in the task.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_distributions(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set up the matplotlib figure fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Subplot 1: Layered histogram of flipper_length_mm by species sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", kde=True, ax=axes[0, 0], palette=\\"viridis\\") axes[0, 0].set_title(\\"Flipper Length Distribution by Species\\") axes[0, 0].set_xlabel(\\"Flipper Length (mm)\\") axes[0, 0].set_ylabel(\\"Count\\") # Subplot 2: Stacked histogram of bill_length_mm by island sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"island\\", multiple=\\"stack\\", ax=axes[0, 1], palette=\\"coolwarm\\") axes[0, 1].set_title(\\"Bill Length Distribution by Island\\") axes[0, 1].set_xlabel(\\"Bill Length (mm)\\") axes[0, 1].set_ylabel(\\"Count\\") # Subplot 3: Bivariate histogram of bill_depth_mm vs body_mass_g by species sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", ax=axes[1, 0], cbar=True, palette=\\"magma\\") axes[1, 0].set_title(\\"Bill Depth vs Body Mass by Species\\") axes[1, 0].set_xlabel(\\"Bill Depth (mm)\\") axes[1, 0].set_ylabel(\\"Body Mass (g)\\") # Subplot 4: Dodged histogram of bill_length_mm by species sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"dodge\\", shrink=0.8, ax=axes[1, 1], palette=\\"cubehelix\\") axes[1, 1].set_title(\\"Bill Length by Species\\") axes[1, 1].set_xlabel(\\"Bill Length (mm)\\") axes[1, 1].set_ylabel(\\"Count\\") # Adjust layout plt.tight_layout() plt.show()"},{"question":"**Question** You are given a script that utilizes the `netrc` class to store and retrieve login credentials for FTP hosts from a `.netrc` file. However, due to some updates in requirements, we need additional functionality to: 1. Add a new host entry to the `.netrc` file. 2. Update an existing host\'s credentials in the `.netrc` file. 3. Delete a host entry from the `.netrc` file. Your task is to implement these functionalities by extending the `netrc` class. Below are the function specifications: 1. **add_host(host, login, account, password)**: - **Input**: - `host` (str): The hostname to be added. - `login` (str): The login username. - `account` (str): The account information. - `password` (str): The password associated with the host. - **Output**: None - **Behavior**: Adds a new host entry to the `.netrc` file. If the host already exists, it should raise a `ValueError` with the message \\"Host already exists\\". 2. **update_host(host, login, account, password)**: - **Input**: - `host` (str): The hostname to be updated. - `login` (str): The new login username. - `account` (str): The new account information. - `password` (str): The new password associated with the host. - **Output**: None - **Behavior**: Updates an existing host entry in the `.netrc` file. If the host does not exist, it should raise a `ValueError` with the message \\"Host not found\\". 3. **delete_host(host)**: - **Input**: - `host` (str): The hostname to be deleted. - **Output**: None - **Behavior**: Deletes the specified host entry from the `.netrc` file. If the host does not exist, it should raise a `ValueError` with the message \\"Host not found\\". **Additional Requirements**: - You should handle file read and write operations safely. - Ensure that the file format remains consistent after modifications. - You should utilize the `authenticators(host)` method to check for existing hosts. - Ensure proper exception handling for file operations and parsing errors. Here is a template to help you get started: ```python import netrc import os class EnhancedNetrc(netrc.netrc): def __init__(self, file=None): super().__init__(file) self.file = file if file else os.path.expanduser(\\"~/.netrc\\") def add_host(self, host, login, account, password): if self.authenticators(host): raise ValueError(\\"Host already exists\\") self.hosts[host] = (login, account, password) self._save() def update_host(self, host, login, account, password): if not self.authenticators(host): raise ValueError(\\"Host not found\\") self.hosts[host] = (login, account, password) self._save() def delete_host(self, host): if not self.authenticators(host): raise ValueError(\\"Host not found\\") del self.hosts[host] self._save() def _save(self): Save the current hosts dictionary to the .netrc file with open(self.file, \'w\') as f: for host, (login, account, password) in self.hosts.items(): f.write(f\'machine {host}n\') f.write(f\' login {login}n\') f.write(f\' account {account}n\') f.write(f\' password {password}n\') # Example usage: # netrc = EnhancedNetrc() # netrc.add_host(\'example.com\', \'mylogin\', \'myaccount\', \'mypassword\') ``` Implement the specified functions to complete the task.","solution":"import netrc import os class EnhancedNetrc(netrc.netrc): def __init__(self, file=None): if file is None: file = os.path.expanduser(\\"~/.netrc\\") super().__init__(file) self.file = file def add_host(self, host, login, account, password): if self.authenticators(host): raise ValueError(\\"Host already exists\\") self.hosts[host] = (login, account, password) self._save() def update_host(self, host, login, account, password): if not self.authenticators(host): raise ValueError(\\"Host not found\\") self.hosts[host] = (login, account, password) self._save() def delete_host(self, host): if not self.authenticators(host): raise ValueError(\\"Host not found\\") del self.hosts[host] self._save() def _save(self): Save the current hosts dictionary to the .netrc file with open(self.file, \'w\') as f: for host, (login, account, password) in self.hosts.items(): f.write(f\'machine {host}n\') f.write(f\' login {login}n\') f.write(f\' account {account}n\') f.write(f\' password {password}n\') # Example usage: # netrc = EnhancedNetrc() # netrc.add_host(\'example.com\', \'mylogin\', \'myaccount\', \'mypassword\')"},{"question":"# Python Memory Management Problem You have been given the task of writing a Python C extension that allocates and handles memory efficiently to manage a custom data structure. You must ensure that your extension adheres to Python\'s memory management principles and correctly uses the various memory allocation domains. Task Implement a Python C extension that creates and manages a dynamically resizable array of integers. You are required to initialize the array with a specified capacity, allow for insertion of elements, and support resizing the array when it exceeds its capacity. Additionally, you need to ensure that your memory allocation and deallocation follow the correct usage rules for Python\'s memory manager. Requirements 1. **Initialization**: Implement a function to initialize the dynamic array with a given capacity using the appropriate memory domain and allocation functions. 2. **Insertion**: Implement a function to insert elements into the array, handling resizing of the array when necessary. 3. **Resizing**: Implement a function to resize the array using the realloc function from the appropriate allocator. 4. **Cleanup**: Implement a function to clean up the allocated memory correctly using the appropriate deallocation functions. Constraints - The array should support an initial capacity passed during initialization and grow dynamically as elements are added. - The resizing should double the array size each time the existing capacity is exceeded. - Your code should handle edge cases, such as memory allocation failures, gracefully. Input and Output You should define four primary functions (and their corresponding C functions) for this task: 1. `initialize(capacity)`: Initializes the array with the specified capacity. - **Input**: `capacity` (int) - initial capacity of the array - **Output**: None 2. `insert(element)`: Inserts elements into the array, resizing if necessary. - **Input**: `element` (int) - element to be inserted - **Output**: None 3. `resize(new_capacity)`: Resizes the array to the new capacity. - **Input**: `new_capacity` (int) - new capacity for the array - **Output**: None 4. `cleanup()`: Cleans up the array and frees memory. - **Input**: None - **Output**: None Your implementation should use the C memory management functions provided by Python\'s memory interface. Include comments and error checks where necessary. Use the following Python C API functions where appropriate: - `PyMem_Malloc` / `PyMem_RawMalloc` - `PyMem_Calloc` / `PyMem_RawCalloc` - `PyMem_Realloc` / `PyMem_RawRealloc` - `PyMem_Free` / `PyMem_RawFree` Example Here is an example of how the provided methods should work: ```python # Initialize array with capacity 2 initialize(2) # Insert elements into the array insert(10) insert(20) # The array should resize now as the capacity is exceeded insert(30) # Clean up the array cleanup() ``` You are not required to handle the Python to C extension module setup in this question. Focus on the implementation of the described functions using the Python C API.","solution":"import ctypes class DynamicArray: def __init__(self, capacity): self._array = (ctypes.c_int * capacity)() self._capacity = capacity self._size = 0 def insert(self, element): if self._size == self._capacity: self.resize(2 * self._capacity) self._array[self._size] = element self._size += 1 def resize(self, new_capacity): new_array = (ctypes.c_int * new_capacity)() for i in range(self._size): new_array[i] = self._array[i] self._array = new_array self._capacity = new_capacity def cleanup(self): self._array = None self._capacity = 0 self._size = 0 def initialize(capacity): return DynamicArray(capacity)"},{"question":"**Problem Statement: Advanced File Processing and Data Persistence** **Objective:** Implement a function that processes a directory of text files, extracts unique words, counts their occurrences, and stores the results in a serialized format using a standard library module. **Function Signature:** ```python def process_text_files(directory_path: str, output_file: str) -> None: Processes text files in the given directory, extracts unique words and their occurrences, and stores the results in a serialized format. Args: directory_path (str): Path to the directory containing text files. output_file (str): Path to the output file where results will be serialized. Returns: None ``` **Input:** - `directory_path`: A string representing the path to the directory containing text files. - `output_file`: A string representing the path to the output file where the word counts will be serialized. **Output:** - None (The function should write the results to the specified output file). **Constraints:** - You can assume the directory contains only text files. - The function should handle large files efficiently. - All words are case-insensitive (e.g., \\"Python\\" and \\"python\\" should be considered the same word). - Non-alphabetic characters should be ignored (e.g., \\"word,\\" should be considered the same as \\"word\\"). **Performance Requirements:** - The function should be optimized for time and space complexity, specifically considering large datasets. **Example Usage:** Suppose there are three text files in the `data/` directory: - `file1.txt`: \\"Hello world! This is a test.\\" - `file2.txt`: \\"Testing the world of Python.\\" - `file3.txt`: \\"Python programming is great.\\" The serialized output file (`output.pkl`) should contain the following data when deserialized: ```python { \\"hello\\": 1, \\"world\\": 2, \\"this\\": 1, \\"is\\": 2, \\"a\\": 1, \\"test\\": 1, \\"testing\\": 1, \\"the\\": 1, \\"of\\": 1, \\"python\\": 2, \\"programming\\": 1, \\"great\\": 1 } ``` **Instructions:** 1. Read all text files in the specified directory. 2. Extract unique words and count their occurrences. 3. Ignore non-alphabetic characters and normalize words to lowercase. 4. Serialize the resulting dictionary to the specified output file using Python\'s standard library. **Hint:** - Consider using modules like `os`, `re`, and `pickle` from the Python Standard Library.","solution":"import os import re import pickle from collections import defaultdict def process_text_files(directory_path: str, output_file: str) -> None: Processes text files in the given directory, extracts unique words and their occurrences, and stores the results in a serialized format. Args: directory_path (str): Path to the directory containing text files. output_file (str): Path to the output file where results will be serialized. Returns: None word_counts = defaultdict(int) word_pattern = re.compile(r\'b[a-zA-Z]+b\') for filename in os.listdir(directory_path): if filename.endswith(\'.txt\'): filepath = os.path.join(directory_path, filename) with open(filepath, \'r\', encoding=\'utf-8\') as file: content = file.read() words = word_pattern.findall(content.lower()) for word in words: word_counts[word] += 1 with open(output_file, \'wb\') as file: pickle.dump(dict(word_counts), file)"},{"question":"# Question: Custom Loader & Finder with `importlib` You are required to implement a custom importer that can dynamically load Python modules from a pseudo-filesystem. This system is simplified by storing module source codes in a dictionary where keys are module names and values are the source codes in string format. The goal is to create custom finder and loader classes that can integrate with Python\'s import system using the `importlib` module. Your task is to implement the following: 1. **PseudoFileSystemLoader:** This loader should inherit from `importlib.abc.SourceLoader` and should have methods to fetch the source code from the given dictionary. 2. **PseudoFileSystemFinder:** This finder should inherit from `importlib.abc.MetaPathFinder` and it should be able to find and return `ModuleSpec` for modules located in the pseudo-filesystem. 3. **Integration to the Import System:** Register the custom finder in the `sys.meta_path` which will allow the standard `import` statement to use your custom importer. # Specifications 1. **Input and Output:** - **Input:** The input for the module will be `module_name` provided to the `import` statement. - **Output:** The output should be the attributes or functions defined within the dynamically loaded module. 2. **Constraints:** - You should use `importlib.util.spec_from_loader` to create the `ModuleSpec`. - Use `sys.modules` to handle the cache of modules. - The `pseudo_filesystem` dictionary should be provided globally so both the finder and loader have access to it. - Assume the `pseudo_filesystem` will contain valid Python source code strings. 3. **Performance Requirements:** - The implementation should handle module look-up and loading efficiently by caching the loaded modules in `sys.modules`. # Example ```python # Provided pseudo filesystem pseudo_filesystem = { \\"hello\\": \'\'\' def greet(): return \\"Hello, World!\\" \'\'\' } # Your code starts below import importlib.abc import importlib.util import sys class PseudoFileSystemLoader(importlib.abc.SourceLoader): def __init__(self, name): self.name = name def get_filename(self, fullname): return fullname def get_data(self, filename): return pseudo_filesystem[filename] class PseudoFileSystemFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): if fullname in pseudo_filesystem: loader = PseudoFileSystemLoader(fullname) return importlib.util.spec_from_loader(fullname, loader) return None # Register the finder sys.meta_path.insert(0, PseudoFileSystemFinder()) # Test the custom import import hello print(hello.greet()) # Output: Hello, World! ``` Your implementation should follow the structure provided. Implement the `PseudoFileSystemLoader` and `PseudoFileSystemFinder` classes with the appropriate methods as described, and integrate the finder into `sys.meta_path`. Test the imports as shown in the example to ensure your solution works correctly.","solution":"import importlib.abc import importlib.util import sys # Provided pseudo filesystem pseudo_filesystem = { \\"hello\\": \'\'\' def greet(): return \\"Hello, World!\\" \'\'\' } class PseudoFileSystemLoader(importlib.abc.SourceLoader): def __init__(self, name): self.name = name def get_filename(self, fullname): Return the module name as the filename. return fullname def get_data(self, filename): Fetch the source code from the pseudo_filesystem dictionary. return pseudo_filesystem[filename] class PseudoFileSystemFinder(importlib.abc.MetaPathFinder): def find_spec(self, fullname, path, target=None): Find and return a ModuleSpec for the module if present in the pseudo filesystem. if fullname in pseudo_filesystem: loader = PseudoFileSystemLoader(fullname) return importlib.util.spec_from_loader(fullname, loader) return None # Register the finder sys.meta_path.insert(0, PseudoFileSystemFinder())"},{"question":"# Time Series Data Analysis with Pandas You are given a dataset containing stock prices for multiple companies over a range of dates. The dataset includes columns for the `date`, `company`, and `price`. Your task is to perform various time series analyses on this dataset using pandas. Dataset ```plaintext date,company,price 2021-01-01,CompanyA,100 2021-01-01,CompanyB,150 2021-01-02,CompanyA,102 2021-01-02,CompanyB,148 ... ``` Requirements 1. **Data Loading and Parsing**: - Load the dataset into a pandas DataFrame. - Parse the `date` column as datetime objects and set it as the index. - Ensure the DataFrame is sorted by the `date`. 2. **Time Zone Localization**: - Localize the dates to \\"UTC\\" timezone. - Convert the dates to the \\"US/Eastern\\" timezone. 3. **Resampling and Aggregation**: - Resample the data to weekly frequency and compute the average price for each company. - Find the highest weekly average price for each company within the first quarter (Q1) of 2021. 4. **PeriodIndex and Frequency Conversion**: - Convert the DataFrame to use a `PeriodIndex` with monthly frequency. - Find the month with the highest average price for each company in 2021. 5. **Handling Missing Data**: - Introduce some missing values in the `price` column randomly. - Fill the missing values using forward fill method. 6. **Date Arithmetic and Shifting**: - Create a new column showing the price difference from the previous day for each company. - Create another column indicating the price 7 days ago for each company. Write a Python function `analyze_stock_data(filepath)` that takes the path to the dataset file and performs the above tasks, returning a new DataFrame with the results of the resampling, aggregation, and analysis. Function Signature ```python import pandas as pd def analyze_stock_data(filepath: str) -> pd.DataFrame: # Your code goes here pass # Example usage # result_df = analyze_stock_data(\\"path/to/your/dataset.csv\\") # print(result_df) ``` **Constraints**: - You should not use any external libraries other than pandas and numpy. - The dataset file will be in CSV format with a single header row as shown in the example. **Notes**: - Ensure your function prints intermediate steps to help with debugging. - Provide comments explaining each step of your code.","solution":"import pandas as pd import numpy as np def analyze_stock_data(filepath: str) -> pd.DataFrame: # Load the dataset df = pd.read_csv(filepath, parse_dates=[\'date\']) # Set the date as index and sort it df.set_index(\'date\', inplace=True) df.sort_index(inplace=True) # Localize the dates to UTC df.index = df.index.tz_localize(\'UTC\') # Convert the dates to US/Eastern timezone df.index = df.index.tz_convert(\'US/Eastern\') # Resample the data to weekly frequency and compute the average price for each company weekly_avg = df.groupby(\'company\').resample(\'W\').mean() # Find the highest weekly average price for each company in Q1 2021 q1_2021 = weekly_avg[weekly_avg.index.get_level_values(\'date\').quarter == 1] q1_max_price = q1_2021.groupby(\'company\')[\'price\'].max().reset_index() # Convert the DataFrame to use a PeriodIndex with monthly frequency df_period = df.to_period(\'M\') # Find the month with the highest average price for each company in 2021 monthly_avg = df_period.groupby([\'company\', df_period.index])[\'price\'].mean() highest_monthly_avg = monthly_avg[monthly_avg.index.get_level_values(1).year == 2021].groupby(\'company\').idxmax().to_frame(name=\'month_with_highest_avg_price\') # Introduce missing values randomly in the `price` column df_missing = df.copy() np.random.seed(0) missing_indices = np.random.choice(df_missing.index, size=int(len(df_missing) * 0.1), replace=False) df_missing.loc[missing_indices, \'price\'] = np.nan # Fill the missing values using forward fill method df_filled = df_missing.fillna(method=\'ffill\') # Create a new column showing the price difference from the previous day for each company df_filled[\'price_diff\'] = df_filled.groupby(\'company\')[\'price\'].diff() # Create another column indicating the price 7 days ago for each company df_filled[\'price_7_days_ago\'] = df_filled.groupby(\'company\')[\'price\'].shift(7) # Result DataFrame with analysis result = { \'weekly_avg\': weekly_avg, \'q1_max_price\': q1_max_price, \'highest_monthly_avg\': highest_monthly_avg, \'filled_data\': df_filled } return result"},{"question":"# Python310 Iterator Challenge Objective Design and implement custom iterator classes in Python that mimic the behavior of the built-in sequence and callable iterators as described in the provided documentation. Task 1. Create a class `SequenceIterator` that: - Takes a sequence (e.g., list, tuple) as input. - Iterates over the sequence from the start to the end. - Raises an `IndexError` when it reaches the end of the sequence. 2. Create a class `CallableIterator` that: - Takes a callable object and a sentinel value as input. - Calls the callable object on each iteration to retrieve the next item. - Ends iteration when the callable object returns a value equal to the sentinel value. Implementation Details 1. **SequenceIterator** - **Initialization**: Takes one argument, a sequence. - **Methods**: - `__iter__()`: Returns the iterator object. - `__next__()`: Returns the next item from the sequence until the end is reached, then raises `StopIteration`. 2. **CallableIterator** - **Initialization**: Takes two arguments, a callable object and a sentinel value. - **Methods**: - `__iter__()`: Returns the iterator object. - `__next__()`: Calls the callable for the next item. Ends iteration if the returned value equals the sentinel, then raises `StopIteration`. Constraints - Do not use any external libraries. Implement the iterators using basic Python functionalities. - Ensure the performance is optimal for sequences of up to 10^6 elements and callable objects that generate up to 10^6 items. Example Usage ```python # Example for SequenceIterator seq = [1, 2, 3, 4] seq_iter = SequenceIterator(seq) for item in seq_iter: print(item) # Example for CallableIterator def generator(): n = 0 while True: n += 1 if n > 5: break yield n call_iter = CallableIterator(generator, 6) for item in call_iter: print(item) ``` Expected Output ``` # For SequenceIterator 1 2 3 4 # For CallableIterator 1 2 3 4 5 ``` Submission Requirements - Implement `SequenceIterator` and `CallableIterator` classes as described. - Include a main block to demonstrate the functionality of both iterator classes. - Provide comments and documentation within the code to explain the implementation.","solution":"class SequenceIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.sequence): result = self.sequence[self.index] self.index += 1 return result else: raise StopIteration class CallableIterator: def __init__(self, callable_obj, sentinel): self.callable_obj = callable_obj self.sentinel = sentinel self._is_exhausted = False def __iter__(self): return self def __next__(self): if self._is_exhausted: raise StopIteration result = self.callable_obj() if result == self.sentinel: self._is_exhausted = True raise StopIteration else: return result # Main block demonstrating the functionality if __name__ == \\"__main__\\": # Example for SequenceIterator seq = [1, 2, 3, 4] seq_iter = SequenceIterator(seq) for item in seq_iter: print(item) # Example for CallableIterator def generator(): n = 0 while True: n += 1 yield n gen = generator() call_iter = CallableIterator(lambda: next(gen), 6) for item in call_iter: print(item)"},{"question":"**Objective:** Create a histogram using seaborn\'s objects interface to visualize the distribution of the \'price\' column from the \'diamonds\' dataset. Customize the bar properties and handle overlapping bars. **Requirements:** 1. **Load Dataset:** - Load the \'diamonds\' dataset using seaborn. 2. **Basic Histogram:** - Create a plot object for the \'price\' column. 3. **Customize Histogram:** - Add bars to the histogram with the following customizations: - Remove the edge width. - Apply transparency based on the \'clarity\' column. 4. **Handle Overlapping Bars:** - Resolve the overlap by using the `so.Distance()` transform. 5. **Filter Data:** - Only include diamonds with a \'cut\' value of \'Ideal\'. # Function Signature ```python def visualize_diamonds_price(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create and customize the plot p = so.Plot(diamonds.query(\\"cut == \'Ideal\'\\"), \\"price\\") # Add customized bars to the plot p.add( so.Bars(edgewidth=0, alpha=\\"clarity\\"), so.Hist(), so.Dodge() ) # Display the plot p.show() ``` **Expected Output:** The function should display a histogram of the \'price\' for diamonds with \'Ideal\' cut, where the bars are transparent based on the \'clarity\' column, and overlapping bars are resolved using the `so.Dodge()` transform.","solution":"def visualize_diamonds_price(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Create and customize the plot p = so.Plot(diamonds.query(\\"cut == \'Ideal\'\\"), \\"price\\") # Add customized bars to the plot p.add( so.Bars(edgewidth=0, alpha=\\"clarity\\"), so.Hist(), so.Dodge() ) # Display the plot p.show()"},{"question":"**Sparse Data Manipulation and Analysis Using Pandas** **Objective:** Demonstrate your proficiency in handling sparse data structures using the Pandas library. This question will test your ability to create, manipulate, and perform operations on sparse data. **Problem Statement:** You are given a dense DataFrame containing mostly `NaN` values. Your tasks are to: 1. Convert this dense DataFrame to a sparse DataFrame. 2. Perform a couple of basic operations on the sparse DataFrame. 3. Convert the sparse DataFrame back to a dense format. **Instructions:** 1. **Create a Dense DataFrame:** - Use NumPy and Pandas to create a DataFrame of size (100, 4) filled with random numbers from a standard normal distribution. - Set approximately 80% of the values in this DataFrame to `NaN`. 2. **Convert to Sparse DataFrame:** - Convert the dense DataFrame to a Sparse DataFrame, where `NaN` values are the fill value. 3. **Operations on Sparse DataFrame:** - Calculate and print the density of the sparse DataFrame. - Apply a NumPy `ufunc` (e.g., `np.abs()`) to one of the columns of the sparse DataFrame and store the result in a new sparse column. 4. **Convert Back to Dense:** - Convert the sparse DataFrame back to a dense DataFrame. - Print the original dense DataFrame, the sparse DataFrame, and the newly converted dense DataFrame to verify that the operations and conversions are consistent. **Expected Input:** None (The DataFrame is to be created programmatically within your solution). **Expected Output:** ``` Original Dense DataFrame: <Printed dense DataFrame> Sparse DataFrame: <Printed sparse DataFrame with densities> Converted Dense DataFrame: <Printed dense DataFrame after reconversion> ``` **Constraints:** - Use Pandas version 1.1.0 or higher. - Ensure that the operations performed on the Sparse DataFrame maintain consistency after conversion back to dense form. **Code Template:** ```python import numpy as np import pandas as pd # Step 1: Create a Dense DataFrame np.random.seed(0) dense_df = pd.DataFrame(np.random.randn(100, 4)) mask = np.random.rand(100, 4) < 0.8 dense_df[mask] = np.nan print(\\"Original Dense DataFrame:\\") print(dense_df) # Step 2: Convert to Sparse DataFrame sparse_df = dense_df.astype(pd.SparseDtype(\\"float\\", np.nan)) print(\\"nSparse DataFrame:\\") print(sparse_df) print(\\"Density:\\", sparse_df.sparse.density) # Step 3: Operations on Sparse DataFrame sparse_df[\'abs_col0\'] = np.abs(sparse_df.iloc[:, 0]) print(\\"nSparse DataFrame with Abs Operation:\\") print(sparse_df) # Step 4: Convert Back to Dense converted_dense_df = sparse_df.sparse.to_dense() print(\\"nConverted Dense DataFrame:\\") print(converted_dense_df) ``` Use the above template to write your solution and verify that the outputs match the expected results.","solution":"import numpy as np import pandas as pd def sparse_manipulation_and_analysis(): # Step 1: Create a Dense DataFrame np.random.seed(0) dense_df = pd.DataFrame(np.random.randn(100, 4)) mask = np.random.rand(100, 4) < 0.8 dense_df[mask] = np.nan print(\\"Original Dense DataFrame:\\") print(dense_df) # Step 2: Convert to Sparse DataFrame sparse_df = dense_df.astype(pd.SparseDtype(\\"float\\", np.nan)) print(\\"nSparse DataFrame:\\") print(sparse_df) print(\\"Density:\\", sparse_df.sparse.density) # Step 3: Operations on Sparse DataFrame sparse_df[\'abs_col0\'] = np.abs(sparse_df.iloc[:, 0]) print(\\"nSparse DataFrame with Abs Operation:\\") print(sparse_df) # Step 4: Convert Back to Dense converted_dense_df = sparse_df.sparse.to_dense() print(\\"nConverted Dense DataFrame:\\") print(converted_dense_df) return dense_df, sparse_df, converted_dense_df # Execute the function sparse_manipulation_and_analysis()"},{"question":"**Question: Advanced Enum Usage in Python** You are tasked with creating an application that manages a small online store. The store sells a variety of products, and each product belongs to a specific category. The store also supports different types of user roles with specific permissions. You need to use the `enum` module to define these categories and roles clearly, and ensure code is readable and maintainable. **Instructions:** 1. Define an enumeration `Category` containing the following product categories: - ELECTRONICS - CLOTHING - GROCERY Use `auto()` for values. 2. Define another enumeration `UserRole` containing the following roles with specific permissions using `IntFlag`: - VIEWER: Can only view products (`VIEW = 1`). - EDITOR: Can view and edit products (`EDIT = 2`). - ADMIN: Can view, edit, and delete products (`DELETE = 4`). Additionally, define combined roles using bitwise OR operations: - EDITOR with VIEW permissions. - ADMIN with VIEW and EDIT permissions. 3. Implement a function `describe_category` that takes a `Category` member and returns a string describing the category. 4. Implement a function `can_user` that takes a `UserRole` member and a permission (`VIEW`, `EDIT`, or `DELETE`) and returns `True` if the user has the given permission, otherwise `False`. **Expected Input/Output:** - `describe_category(Category.ELECTRONICS)` should return `\\"Category: ELECTRONICS\\"` - `can_user(UserRole.EDITOR | UserRole.VIEWER, UserRole.EDIT)` should return `True` **Code Requirements:** - Your enums should use the provided names and values. - The `describe_category` function should make use of the `name` attribute of the `Category` enum. - The `can_user` function should leverage bitwise operations to check permissions. **Constraints:** - Do not use any external libraries. - Ensure your code is compatible with Python 3.10. **Performance Requirements:** - The functions should handle up to 1000 consecutive calls efficiently. ```python # Start your code here from enum import Enum, IntFlag, auto class Category(Enum): ELECTRONICS = auto() CLOTHING = auto() GROCERY = auto() class UserRole(IntFlag): VIEW = 1 EDIT = 2 DELETE = 4 EDITOR = VIEW | EDIT ADMIN = VIEW | EDIT | DELETE def describe_category(category): return f\\"Category: {category.name}\\" def can_user(user_role, permission): return user_role & permission == permission # Example usage print(describe_category(Category.ELECTRONICS)) # Output: \\"Category: ELECTRONICS\\" print(can_user(UserRole.EDITOR, UserRole.EDIT)) # Output: True print(can_user(UserRole.VIEWER, UserRole.DELETE)) # Output: False ``` **Make sure to test your functions with various inputs to ensure correctness.**","solution":"from enum import Enum, IntFlag, auto class Category(Enum): ELECTRONICS = auto() CLOTHING = auto() GROCERY = auto() class UserRole(IntFlag): VIEW = 1 EDIT = 2 DELETE = 4 EDITOR = VIEW | EDIT ADMIN = VIEW | EDIT | DELETE def describe_category(category): return f\\"Category: {category.name}\\" def can_user(user_role, permission): return (user_role & permission) == permission"},{"question":"Design and implement a custom HTML parser in Python by subclassing the `html.parser.HTMLParser` class. Your parser should: 1. Identify and print out the names of all HTML tags present in the document. 2. Count the number of times each tag appears in the document. 3. Print out the content inside all `<h1>` tags. 4. Identify all external links (i.e., URLs in the `href` attribute of `<a>` tags that start with `http://` or `https://`) and print them out. # Input The input will be a single string representing the HTML content to be parsed. # Output 1. A list of tag names found in the document. 2. A dictionary where keys are tag names and values are the counts of each tag. 3. A list of strings representing the content inside `<h1>` tags. 4. A list of external links. # Constraints - The HTML content will be valid but may contain any standard HTML tags and attributes. - The HTML content will have a maximum length of 10,000 characters. # Example ```python html_content = \'\'\' <html> <head> <title>Test Page</title> </head> <body> <h1>Welcome to the Test Page</h1> <p>This is a <a href=\\"http://example.com\\">link</a> to an example site.</p> <p>Another paragraph with a <a href=\\"https://example.org\\">link</a> to another example site.</p> </body> </html> \'\'\' # Expected Output: tags = [\'html\', \'head\', \'title\', \'body\', \'h1\', \'p\', \'a\'] tag_counts = {\'html\': 1, \'head\': 1, \'title\': 1, \'body\': 1, \'h1\': 1, \'p\': 2, \'a\': 2} h1_content = [\'Welcome to the Test Page\'] external_links = [\'http://example.com\', \'https://example.org\'] print(tags) print(tag_counts) print(h1_content) print(external_links) ``` # Implementation Define a `CustomHTMLParser` class that extends `html.parser.HTMLParser` and implements the required functionality. ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tags = [] self.tag_counts = {} self.h1_content = [] self.external_links = [] self.current_tag = None def handle_starttag(self, tag, attrs): self.tags.append(tag) self.tag_counts[tag] = self.tag_counts.get(tag, 0) + 1 if tag == \\"a\\": for attr, value in attrs: if attr == \\"href\\" and (value.startswith(\\"http://\\") or value.startswith(\\"https://\\")): self.external_links.append(value) self.current_tag = tag def handle_endtag(self, tag): self.current_tag = None def handle_data(self, data): if self.current_tag == \\"h1\\": self.h1_content.append(data.strip()) def parse_html(html_content): parser = CustomHTMLParser() parser.feed(html_content) tags = parser.tags tag_counts = parser.tag_counts h1_content = parser.h1_content external_links = parser.external_links return tags, tag_counts, h1_content, external_links # Sample Usage html_content = \'\'\' <html> <head> <title>Test Page</title> </head> <body> <h1>Welcome to the Test Page</h1> <p>This is a <a href=\\"http://example.com\\">link</a> to an example site.</p> <p>Another paragraph with a <a href=\\"https://example.org\\">link</a> to another example site.</p> </body> </html> \'\'\' tags, tag_counts, h1_content, external_links = parse_html(html_content) print(tags) print(tag_counts) print(h1_content) print(external_links) ```","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tags = [] self.tag_counts = {} self.h1_content = [] self.external_links = [] self.current_tag = None def handle_starttag(self, tag, attrs): self.tags.append(tag) self.tag_counts[tag] = self.tag_counts.get(tag, 0) + 1 if tag == \\"a\\": for attr, value in attrs: if attr == \\"href\\" and (value.startswith(\\"http://\\") or value.startswith(\\"https://\\")): self.external_links.append(value) self.current_tag = tag def handle_endtag(self, tag): self.current_tag = None def handle_data(self, data): if self.current_tag == \\"h1\\": self.h1_content.append(data.strip()) def parse_html(html_content): parser = CustomHTMLParser() parser.feed(html_content) tags = list(set(parser.tags)) tag_counts = parser.tag_counts h1_content = parser.h1_content external_links = parser.external_links return tags, tag_counts, h1_content, external_links"},{"question":"Coding Assessment Question # Objective This question tests your ability to utilize the `torch.cuda.tunable` module in PyTorch. You will write a code snippet that optimizes a CUDA-based matrix multiplication operation using the available tuning functionalities. # Problem Statement You are given two large random matrices `A` and `B`. Write a Python script to perform matrix multiplication using PyTorch, and optimize it using `torch.cuda.tunable` functionalities. You must enable tuning, set appropriate tuning parameters, execute the multiplication, and record the tuning results. Your script should demonstrate the use of at least 5 different functions from the `torch.cuda.tunable` module. # Requirements 1. Use the `torch` package to create the matrices and perform the multiplication. 2. Enable tuning and set a maximum tuning duration and iteration count. 3. Execute the GEMM (General Matrix Multiply) operation. 4. Write a file with the tuning results and display them. 5. Demonstrate the use of at least 5 different functions from `torch.cuda.tunable`. # Input Format - Generate two random matrices `A` and `B` of shape `(2000, 2000)` using `torch.rand`. # Output Format - Print the result of the matrix multiplication. - Print the tuning results. ```python import torch import torch.cuda.tunable as tunable # Create two random matrices A and B A = torch.rand((2000, 2000), device=\'cuda\') B = torch.rand((2000, 2000), device=\'cuda\') # Enable tuning and set parameters tunable.enable() tunable.set_max_tuning_duration(10) # Set max tuning duration to 10 seconds tunable.set_max_tuning_iterations(100) # Set max tuning iterations to 100 # Perform the matrix multiplication while tuning is enabled C = torch.matmul(A, B) # Write the tuning results to a file and display the results tunable.set_filename(\\"tuning_results.txt\\") tunable.write_file() # Display the results print(\\"Matrix multiplication result (first 5 elements):\\", C.flatten()[:5]) print(\\"Tuning Enabled:\\", tunable.is_enabled()) print(\\"Max Tuning Duration:\\", tunable.get_max_tuning_duration()) print(\\"Max Tuning Iterations:\\", tunable.get_max_tuning_iterations()) print(\\"Tuning Results:\\", tunable.get_results()) ``` # Constraints - Ensure that the script runs efficiently and completes within a reasonable timeframe. - Use GPU (CUDA) for matrix operations. - Document your code appropriately to explain each step. # Notes - It is assumed you have access to a GPU-enabled environment with PyTorch and CUDA installed. - The accuracy and performance of the matrix multiplication are not the primary evaluation criteria, but the effective use of tuning functionalities is critical.","solution":"import torch import torch.cuda.tunable as tunable # Create two random matrices A and B A = torch.rand((2000, 2000), device=\'cuda\') B = torch.rand((2000, 2000), device=\'cuda\') # Enable tuning and set parameters tunable.enable() tunable.set_max_tuning_duration(10) # Set max tuning duration to 10 seconds tunable.set_max_tuning_iterations(100) # Set max tuning iterations to 100 # Perform the matrix multiplication while tuning is enabled C = torch.matmul(A, B) # Write the tuning results to a file and display the results tunable.set_filename(\\"tuning_results.txt\\") tunable.write_file() # Display the results print(\\"Matrix multiplication result (first 5 elements):\\", C.flatten()[:5]) print(\\"Tuning Enabled:\\", tunable.is_enabled()) print(\\"Max Tuning Duration:\\", tunable.get_max_tuning_duration()) print(\\"Max Tuning Iterations:\\", tunable.get_max_tuning_iterations()) print(\\"Tuning Results:\\", tunable.get_results())"},{"question":"# Custom Signal Handler with Timeout Retry You are required to implement a function, `run_with_timeout`, that attempts to run a given function `func` with a specified timeout. If the function does not complete within the given timeout, it should raise a custom exception `TimeoutException`. To achieve this, you will use the `signal` module to set up an alarm signal that will trigger the timeout. Your implementation should handle the following requirements: 1. **Function Signature**: ```python def run_with_timeout(func: Callable, args: Tuple=(), kwargs: Dict=None, timeout: int=10) -> Any: ``` 2. **Input**: - `func`: The function to be executed. - `args`: A tuple of positional arguments to pass to the function. - `kwargs`: A dictionary of keyword arguments to pass to the function. - `timeout`: An integer specifying the timeout in seconds. 3. **Output**: - Return the result of the function `func` if it completes within the timeout. - Raise `TimeoutException` if the function exceeds the timeout. 4. **Constraints**: - If the function completes within the timeout, subsequent alarms should be cleared. - If the function does not specify any exceptions, it should handle those normally. - Ensure that your implementation is thread-safe by only allowing the main thread to set signal handlers. 5. **Custom Exception**: ```python class TimeoutException(Exception): pass ``` # Example Usage ```python import time import signal class TimeoutException(Exception): pass def run_with_timeout(func, args=(), kwargs=None, timeout=10): if kwargs is None: kwargs = {} def handler(signum, frame): raise TimeoutException(\\"Function execution exceeded timeout\\") # Set the signal handler and the alarm signal.signal(signal.SIGALRM, handler) signal.alarm(timeout) try: # Execute the function with given arguments result = func(*args, **kwargs) finally: # Disable the alarm signal.alarm(0) return result def example_function(duration): time.sleep(duration) return \\"Completed\\" try: print(run_with_timeout(example_function, args=(5,), timeout=2)) except TimeoutException as e: print(e) # Output: Function execution exceeded timeout try: print(run_with_timeout(example_function, args=(2,), timeout=5)) # Output: Completed except TimeoutException as e: print(e) ``` # Constraints - Only the main thread can set signal handlers, ensure to handle exceptions accordingly. - The function should be able to handle any exceptions raised from the provided function `func`. # Performance Requirements - The solution should ensure that the timeout handling is efficient and does not introduce unnecessary overhead. # Submission Submit your implementation of `run_with_timeout` and the `TimeoutException` class.","solution":"import signal from typing import Callable, Tuple, Dict, Any class TimeoutException(Exception): pass def run_with_timeout(func: Callable, args: Tuple=(), kwargs: Dict=None, timeout: int=10) -> Any: if kwargs is None: kwargs = {} def handler(signum, frame): raise TimeoutException(\\"Function execution exceeded timeout\\") # Set the signal handler and the alarm signal.signal(signal.SIGALRM, handler) signal.alarm(timeout) try: # Execute the function with given arguments result = func(*args, **kwargs) finally: # Disable the alarm signal.alarm(0) return result"},{"question":"You are provided with a dataset containing information about houses, including features such as the number of rooms, area, and age, along with their corresponding prices. Your task is to implement a neural network model using scikit-learn\'s `MLPRegressor` to predict the prices of the houses based on these features. Requirements 1. **Data Preparation**: - Read the dataset from a CSV file (path provided as input). - Split the data into training and testing sets (80%-20% split). - Standardize the features using `StandardScaler`. 2. **Model Implementation**: - Implement an MLP regressor with the following specifications: - Use the \'adam\' solver. - Use one hidden layer with 10 neurons. - Set the `alpha` parameter to `1e-4` for regularization. - Set `random_state` to 1 for reproducibility. 3. **Training the Model**: - Train the MLP regressor on the training data. 4. **Evaluation**: - Evaluate the model on the test data using mean squared error (MSE). - Print the MSE of the test set. Input - A string representing the file path to the CSV file containing the dataset. Output - A float representing the mean squared error of the model on the test set. Constraints - Ensure that the code runs efficiently given a reasonably large dataset (up to 10,000 samples with 10 features each). Example Usage ```python def load_and_train_model(file_path: str) -> float: # Your code here # Sample dataset (assume \'houses.csv\' is the file path provided) # houses.csv content: # Rooms,Area,Age,Price # 3,1200,5,300000 # 4,1500,10,400000 # ... mse = load_and_train_model(\'houses.csv\') print(f\'Test MSE: {mse}\') ``` Ensure your implementation follows the outlined requirements to correctly train and evaluate the neural network for house price prediction.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPRegressor from sklearn.metrics import mean_squared_error def load_and_train_model(file_path: str) -> float: # Read the dataset data = pd.read_csv(file_path) # Split the data into features and target variable X = data.drop(\'Price\', axis=1) y = data[\'Price\'] # Split the data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement the MLPRegressor mlp = MLPRegressor(hidden_layer_sizes=(10,), solver=\'adam\', alpha=1e-4, random_state=1) # Train the model mlp.fit(X_train, y_train) # Predict and evaluate the model using mean squared error y_pred = mlp.predict(X_test) mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"Imputation of Missing Values **Objective**: Implement and compare different imputation strategies to handle missing values in a dataset. # Problem Statement: Consider the following dataset representing medical test results: ``` | PatientID | Age | BloodPressure | Cholesterol | Glucose Level | |-----------|-----|---------------|-------------|---------------| | 1 | 25 | 120 | NaN | 85 | | 2 | 38 | NaN | 195 | 89 | | 3 | NaN | 115 | 210 | NaN | | 4 | 45 | 135 | 180 | 95 | | 5 | 29 | NaN | NaN | 88 | ``` # Tasks: 1. **Data Preparation**: - Create the dataset in a `pandas DataFrame`. 2. **Imputation Using SimpleImputer**: - Implement imputation using the `SimpleImputer` class from scikit-learn to fill missing values. - Use the `\\"mean\\"` strategy to fill missing values for all columns. 3. **Imputation Using KNNImputer**: - Implement imputation using the `KNNImputer` class from scikit-learn to fill missing values. - Set the number of neighbors to 2. 4. **Imputation Using IterativeImputer**: - Implement imputation using the `IterativeImputer` class from scikit-learn to fill missing values. - Set `max_iter` to 10 and `random_state` to 0. 5. **Comparison**: - Print the imputed datasets and compare the results. - Discuss the advantages and potential pitfalls of each imputation strategy based on your results. # Instructions: - You are allowed to use only NumPy, pandas, and scikit-learn libraries. - Ensure your code is modular and uses functions to handle different imputation strategies. - Each function should take the original DataFrame as input and return the imputed DataFrame. # Submission: Submit a jupyter notebook (or .py file) containing: - The implemented code. - Outputs of the steps. - A brief comparison discussing each imputation strategy. # Example Outputs: ```python import numpy as np import pandas as pd from sklearn.impute import SimpleImputer, KNNImputer from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer # Create the dataset data = { \'PatientID\': [1, 2, 3, 4, 5], \'Age\': [25, 38, np.nan, 45, 29], \'BloodPressure\': [120, np.nan, 115, 135, np.nan], \'Cholesterol\': [np.nan, 195, 210, 180, np.nan], \'Glucose Level\': [85, 89, np.nan, 95, 88] } df = pd.DataFrame(data) # Imputation using SimpleImputer def simple_imputer(df): imputer = SimpleImputer(strategy=\'mean\') imputed_data = imputer.fit_transform(df.drop(columns=\'PatientID\')) return pd.DataFrame(imputed_data, columns=df.columns[1:], index=df.index) imputed_df_simple = simple_imputer(df) print(\\"Imputed Data using SimpleImputer:\\") print(imputed_df_simple) # Imputation using KNNImputer def knn_imputer(df): imputer = KNNImputer(n_neighbors=2) imputed_data = imputer.fit_transform(df.drop(columns=\'PatientID\')) return pd.DataFrame(imputed_data, columns=df.columns[1:], index=df.index) imputed_df_knn = knn_imputer(df) print(\\"Imputed Data using KNNImputer:\\") print(imputed_df_knn) # Imputation using IterativeImputer def iterative_imputer(df): imputer = IterativeImputer(max_iter=10, random_state=0) imputed_data = imputer.fit_transform(df.drop(columns=\'PatientID\')) return pd.DataFrame(imputed_data, columns=df.columns[1:], index=df.index) imputed_df_iter = iterative_imputer(df) print(\\"Imputed Data using IterativeImputer:\\") print(imputed_df_iter) # Compare and discuss the results # [Provide your analysis here] ``` # Evaluation: - Accuracy and completeness of the code. - Correctness of the implementation. - Quality of the comparison and discussion.","solution":"import numpy as np import pandas as pd from sklearn.impute import SimpleImputer, KNNImputer from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer # Create the dataset data = { \'PatientID\': [1, 2, 3, 4, 5], \'Age\': [25, 38, np.nan, 45, 29], \'BloodPressure\': [120, np.nan, 115, 135, np.nan], \'Cholesterol\': [np.nan, 195, 210, 180, np.nan], \'Glucose Level\': [85, 89, np.nan, 95, 88] } df = pd.DataFrame(data) # Imputation using SimpleImputer def simple_imputer(df): imputer = SimpleImputer(strategy=\'mean\') imputed_data = imputer.fit_transform(df.drop(columns=\'PatientID\')) return pd.DataFrame(imputed_data, columns=df.columns[1:], index=df.index) imputed_df_simple = simple_imputer(df) print(\\"Imputed Data using SimpleImputer:\\") print(imputed_df_simple) # Imputation using KNNImputer def knn_imputer(df): imputer = KNNImputer(n_neighbors=2) imputed_data = imputer.fit_transform(df.drop(columns=\'PatientID\')) return pd.DataFrame(imputed_data, columns=df.columns[1:], index=df.index) imputed_df_knn = knn_imputer(df) print(\\"Imputed Data using KNNImputer:\\") print(imputed_df_knn) # Imputation using IterativeImputer def iterative_imputer(df): imputer = IterativeImputer(max_iter=10, random_state=0) imputed_data = imputer.fit_transform(df.drop(columns=\'PatientID\')) return pd.DataFrame(imputed_data, columns=df.columns[1:], index=df.index) imputed_df_iter = iterative_imputer(df) print(\\"Imputed Data using IterativeImputer:\\") print(imputed_df_iter) # Comparing Imputation Strategies def compare_imputations(df, imputed_dfs): print(\\"nComparing Imputed Data:\\") for name, imputed_df in imputed_dfs.items(): print(f\\"n{name}:\\") print(imputed_df) compare_imputations(df, { \\"SimpleImputer\\": imputed_df_simple, \\"KNNImputer\\": imputed_df_knn, \\"IterativeImputer\\": imputed_df_iter }) # Analysis and Comparison # SimpleImputer (Mean strategy) is straightforward but may not capture complex relationships between features. # KNNImputer can handle more complex patterns by considering neighboring data points, but performance can be affected by outliers. # IterativeImputer models each feature as a function of other features, which can create more accurate imputations but may be computationally intensive."},{"question":"Objective Your task is to demonstrate the use of the `concurrent.futures` module to perform parallel computation on a set of tasks. Specifically, you will implement a function that processes a list of numbers by computing their factorial in parallel and returns a dictionary mapping each number to its factorial. Problem Statement Implement the function `parallel_factorial(numbers, max_workers)` that computes the factorial of a list of numbers in parallel using the `concurrent.futures` module. Function Signature ```python def parallel_factorial(numbers: list[int], max_workers: int) -> dict[int, int]: pass ``` Input - `numbers`: A list of integers `numbers` (1 <= len(numbers) <= 1000; 0 <= numbers[i] <= 50). - `max_workers`: An integer `max_workers` specifying the maximum number of worker threads to use for parallel computation (1 <= max_workers <= 32). Output - A dictionary `{number: factorial}` where the key is the original number from the input list and the value is its factorial. Constraints - You must use `concurrent.futures.ThreadPoolExecutor` for parallel computation. - Handling of factorial computation errors and boundary conditions should be considered. - Efficiency in terms of computation and memory usage is crucial. Example ```python numbers = [0, 5, 10, 20] max_workers = 4 result = parallel_factorial(numbers, max_workers) print(result) ``` Output: ```python { 0: 1, 5: 120, 10: 3628800, 20: 2432902008176640000 } ``` Hints - Use the `ThreadPoolExecutor` context manager to manage the pool of threads. - The `math.factorial` function can be used to compute factorials. - Ensure that all futures are properly handled and the main function waits for all computations to finish before returning the result. Notes - The implementation should handle invalid inputs gracefully, though the constraints make this unlikely within the bounds specified. - Include appropriate error handling to catch potential issues during factorial computation.","solution":"import concurrent.futures import math def parallel_factorial(numbers: list[int], max_workers: int) -> dict[int, int]: Computes the factorial of a list of numbers in parallel using concurrent.futures.ThreadPoolExecutor. Args: numbers (list of int): A list of integers for which the factorial should be computed. max_workers (int): The maximum number of worker threads to use for parallel computation. Returns: dict of int: int: A dictionary mapping each number to its factorial. def compute_factorial(number): return (number, math.factorial(number)) result = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_number = {executor.submit(compute_factorial, num): num for num in numbers} for future in concurrent.futures.as_completed(future_to_number): number, factorial = future.result() result[number] = factorial return result"},{"question":"<|Analysis Begin|> The provided documentation describes the \\"posix\\" module, which offers access to standardized operating system functionality (a Unix interface). However, it suggests using the \\"os\\" module for a portable interface across different operating systems. The crucial points from the documentation include: 1. **Import Guidance**: Rather than directly importing \\"posix\\", it advises importing \\"os\\" for a portable and extended interface. 2. **Error Handling**: System errors are raised as \\"OSError\\" exceptions. 3. **Large File Support**: Some operating systems provide support for files larger than 2 GiB, and Python enables this if certain conditions are met. 4. **Environment Variables**: The `posix.environ` dictionary represents the environment variables at interpreter start. It is bytes on Unix and str on Windows. Modifying this dictionary does not change the environment for system calls like `execv()`, `popen()`, or `system()`. Given this information, a suitable coding assessment question can focus on manipulating environment variables using the \\"os\\" module, handling exceptions, and utilizing the large file support in Python. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: - Demonstrate your understanding of the \\"os\\" module from Python 3.10, specifically focusing on environment variable manipulation and handling large files. Background: The \\"os\\" module in Python provides utilities to interact with the operating system in a standardized manner. This includes modifying environment variables and handling large files. Task: 1. Write a Python function `process_environ_and_large_files` that: - Adds a new environment variable `NEW_VAR` with the value `Python310`. - Modifies an existing environment variable `HOME` to `/usr/home_modified`. - Reads the content of a large file (assume it\'s larger than 2 GiB) named `large_file.txt` and returns the first 1000 characters of its content. 2. Use exception handling to manage potential errors during environment variable modifications and file reading processes, ensuring that a meaningful error message is printed in case of any failure. Implementation Details: - The function should be named `process_environ_and_large_files()`. - **Input**: None (The file `large_file.txt` should be present in the working directory). - **Output**: A string containing the first 1000 characters of the `large_file.txt` content. - If an error occurs during environment variable modification or file reading, print an error message and return `None`. Constraints: - Do not assume the file `large_file.txt` fits into memory if it\'s larger than 2 GiB; use efficient file reading techniques. - The `os` module should be used for environment variable manipulations. Example Usage: ```python def process_environ_and_large_files(): import os try: # Add new environment variable os.environ[\'NEW_VAR\'] = \'Python310\' # Modify existing environment variable if \'HOME\' in os.environ: os.environ[\'HOME\'] = \'/usr/home_modified\' # Reading large file efficiently with open(\'large_file.txt\', \'r\') as file: content = file.read(1000) return content except OSError as e: print(f\\"An error occurred: {e}\\") return None # Example call result = process_environ_and_large_files() print(result) ``` Note: Ensure that the file named `large_file.txt` exists in the current directory for testing purposes.","solution":"import os def process_environ_and_large_files(): try: # Add new environment variable os.environ[\'NEW_VAR\'] = \'Python310\' # Modify existing environment variable if \'HOME\' in os.environ: os.environ[\'HOME\'] = \'/usr/home_modified\' # Reading large file efficiently with open(\'large_file.txt\', \'r\') as file: content = file.read(1000) return content except Exception as e: print(f\\"An error occurred: {e}\\") return None"},{"question":"**Coding Assessment Question:** # Manifold Learning with scikit-learn In this assignment, you will demonstrate your understanding of manifold learning methods available in scikit-learn. You will use the Locally Linear Embedding (LLE) method to reduce the dimensionality of a high-dimensional dataset and evaluate the results. # Task Description 1. **Data Preparation**: - Generate a synthetic 3-dimensional dataset using the `make_swiss_roll` function from scikit-learn with 1000 samples and no noise. 2. **Dimensionality Reduction**: - Implement Locally Linear Embedding (LLE) to reduce the dimensionality of the dataset from 3 to 2 dimensions. - Use scikit-learn\'s `LocallyLinearEmbedding` class with the method `standard` and set the number of neighbors to 10 and the number of components to 2. 3. **Visualization**: - Plot the original 3D data points using a 3D scatter plot. - Plot the 2D reduced data points using a 2D scatter plot. 4. **Evaluation**: - Calculate and print the reconstruction error of the LLE embedding. # Expected Input and Output Formats: Input: - No direct input. The synthetic data will be generated within the code. Output: - A 3D scatter plot of the original data. - A 2D scatter plot of the reduced data. - The reconstruction error of the LLE embedding. # Constraints & Performance Requirements: - Ensure the generated plots are clearly labeled and each axis is properly annotated. - Your code should manage the dataset efficiently and should not exceed typical memory limitations for a dataset of this size. # Implementation Provide your implementation in Python. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_swiss_roll from sklearn.manifold import LocallyLinearEmbedding # 1. Data Preparation # Generate a synthetic dataset using `make_swiss_roll` function def generate_swiss_roll(): data, _ = make_swiss_roll(n_samples=1000, noise=0.0) return data # 2. Dimensionality Reduction (LLE) def apply_lle(data, n_neighbors=10, n_components=2): lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method=\'standard\') data_reduced = lle.fit_transform(data) reconstruction_error = lle.reconstruction_error_ return data_reduced, reconstruction_error # 3. Visualization def plot_data(original_data, reduced_data): fig = plt.figure(figsize=(12, 6)) # 3D scatter plot ax = fig.add_subplot(121, projection=\'3d\') ax.scatter(original_data[:, 0], original_data[:, 1], original_data[:, 2], c=\'r\', marker=\'o\') ax.set_title(\'Original 3D Data\') ax.set_xlabel(\'X axis\') ax.set_ylabel(\'Y axis\') ax.set_zlabel(\'Z axis\') # 2D scatter plot plt.subplot(122) plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=\'b\', marker=\'o\') plt.title(\'2D LLE Reduced Data\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.tight_layout() plt.show() # Main function if __name__ == \\"__main__\\": data = generate_swiss_roll() reduced_data, error = apply_lle(data) # 4. Evaluation print(f\\"Reconstruction Error: {error}\\") # Plotting plot_data(data, reduced_data) ``` In this question, you will demonstrate your understanding of Locally Linear Embedding (LLE) and your ability to apply it to a high-dimensional dataset for dimensionality reduction and visualization.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_swiss_roll from sklearn.manifold import LocallyLinearEmbedding # 1. Data Preparation # Generate a synthetic dataset using `make_swiss_roll` function def generate_swiss_roll(): data, _ = make_swiss_roll(n_samples=1000, noise=0.0) return data # 2. Dimensionality Reduction (LLE) def apply_lle(data, n_neighbors=10, n_components=2): lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method=\'standard\') data_reduced = lle.fit_transform(data) reconstruction_error = lle.reconstruction_error_ return data_reduced, reconstruction_error # 3. Visualization def plot_data(original_data, reduced_data): fig = plt.figure(figsize=(12, 6)) # 3D scatter plot ax = fig.add_subplot(121, projection=\'3d\') ax.scatter(original_data[:, 0], original_data[:, 1], original_data[:, 2], c=\'r\', marker=\'o\') ax.set_title(\'Original 3D Data\') ax.set_xlabel(\'X axis\') ax.set_ylabel(\'Y axis\') ax.set_zlabel(\'Z axis\') # 2D scatter plot plt.subplot(122) plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=\'b\', marker=\'o\') plt.title(\'2D LLE Reduced Data\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.tight_layout() plt.show() # Main function if __name__ == \\"__main__\\": data = generate_swiss_roll() reduced_data, error = apply_lle(data) # 4. Evaluation print(f\\"Reconstruction Error: {error}\\") # Plotting plot_data(data, reduced_data)"},{"question":"# XML Parsing with SAX Interface You are given a large XML file named `data.xml` containing various entries of books in a library. Each entry contains information such as the title, author, genre, and publication year. Your task is to write a Python script that uses the SAX `xmlreader` module to parse this XML file and extract specific information. Requirements: 1. Implement a SAX `ContentHandler` class named `BookHandler` with the following functionality: - Keep track of the title, author, genre, and publication year of each book. - Print the title and author of each book that belongs to the genre \\"Science Fiction\\". 2. Implement a function `parse_books(file_path)` that: - Takes the path to the XML file as input. - Creates an instance of your `BookHandler` class. - Sets this instance as the content handler for the SAX parser. - Parses the provided XML file. Input: - `file_path`: A string representing the path to the XML file (e.g., `data.xml`). Output: - The script should print the title and author of each book in the \\"Science Fiction\\" genre. Constraints: - Assume the XML file is well-formed. - The XML file can be large, so your implementation should be efficient and handle the data incrementally. Example XML File (`data.xml`): ```xml <?xml version=\\"1.0\\"?> <library> <book> <title>Dune</title> <author>Frank Herbert</author> <genre>Science Fiction</genre> <year>1965</year> </book> <book> <title>1984</title> <author>George Orwell</author> <genre>Dystopian</genre> <year>1949</year> </book> <book> <title>Neuromancer</title> <author>William Gibson</author> <genre>Science Fiction</genre> <year>1984</year> </book> </library> ``` Expected Output for the Example: ``` Title: Dune, Author: Frank Herbert Title: Neuromancer, Author: William Gibson ``` Additional Notes: - Make sure your `ContentHandler` implementation correctly tracks the element data as it is read by the SAX parser. - Ensure your solution is modular and follows good coding practices. Good luck!","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" self.genre = \\"\\" self.is_scifi = False def startElement(self, tag, attributes): self.current_element = tag def endElement(self, tag): if tag == \\"book\\": if self.is_scifi: print(f\\"Title: {self.title}, Author: {self.author}\\") self.current_element = \\"\\" self.title = \\"\\" self.author = \\"\\" self.genre = \\"\\" self.is_scifi = False def characters(self, content): if self.current_element == \\"title\\": self.title += content.strip() elif self.current_element == \\"author\\": self.author += content.strip() elif self.current_element == \\"genre\\": self.genre += content.strip() if self.genre == \\"Science Fiction\\": self.is_scifi = True def parse_books(file_path): parser = xml.sax.make_parser() handler = BookHandler() parser.setContentHandler(handler) parser.parse(file_path)"},{"question":"**Objective**: This task aims to test your understanding of error handling in Python using the `errno` module. **Problem Statement**: You need to write a function `handle_file_errors(filepath: str) -> str` that attempts to open a file and perform read-write operations. The function must handle specific errors using the `errno` module and return appropriate error messages. **Detailed Requirements**: 1. The function should try to open the file specified by `filepath` in read and write mode. 2. If the operation fails due to the file not being found, handle the `FileNotFoundError` using `errno.ENOENT` and return `\\"Error: File not found - errno.ENOENT\\"`. 3. If the operation fails due to permission issues, handle the `PermissionError` using `errno.EACCES` and return `\\"Error: Permission denied - errno.EACCES\\"`. 4. If the operation fails due to any other IO-related errors, identify the specific errno using `errno.errorcode` and return a message in the format: `\\"Error: [error description] - errno.[error code]\\"`. 5. For any other errors that are not related to IO operations, re-raise the exception. # Function Signature: ```python def handle_file_errors(filepath: str) -> str: pass ``` # Example: ```python assert handle_file_errors(\'/path/to/nonexistent.file\') == \\"Error: File not found - errno.ENOENT\\" assert handle_file_errors(\'/path/to/forbidden.file\') == \\"Error: Permission denied - errno.EACCES\\" ``` # Constraints: - You must use the `errno` module for corresponding error handling. - The code should handle at least the `FileNotFoundError` and `PermissionError`. - Ensure that unexpected IO errors are handled gracefully by mapping them using `errno.errorcode`. **Note**: Assume you might not have permissions to certain files or directories if running on Unix-based systems, which could be used to simulate the permission denied errors. # Explanation: - **errno.ENOENT**: Error code for \\"No such file or directory\\" (FileNotFoundError). - **errno.EACCES**: Error code for \\"Permission denied\\" (PermissionError). - `errno.errorcode`: A dictionary to map error numbers to error message strings. - `os.strerror()`: Can be used to translate a numeric error code to an error message for other IOError-related exceptions. Implement this function to demonstrate your understanding and application of the `errno` module in error handling.","solution":"import errno import os def handle_file_errors(filepath: str) -> str: try: with open(filepath, \'r+\') as file: pass # Perform read-write operations except FileNotFoundError: return f\\"Error: File not found - errno.ENOENT\\" except PermissionError: return f\\"Error: Permission denied - errno.EACCES\\" except OSError as e: if e.errno in errno.errorcode: return f\\"Error: {os.strerror(e.errno)} - errno.{errno.errorcode[e.errno]}\\" else: return \\"Error: Unknown - errno.UNKNOWN\\" except Exception as e: raise e"},{"question":"You are required to implement an asynchronous function that performs several I/O operations and handles specific exceptions provided by the `asyncio` module. Your task is to: 1. **Simulate I/O operations** that could potentially trigger the `asyncio` exceptions listed in the provided documentation. 2. **Handle these exceptions** appropriately and return specific messages. 3. **Ensure asynchronous execution** is done correctly without blocking. # Function Signature ```python import asyncio async def io_operation_manager(simulate_exception: str) -> str: Simulates various I/O operations and handles specific asyncio exceptions. Args: simulate_exception (str): The type of exception to simulate. Must be one of the following: [\'TimeoutError\', \'CancelledError\', \'InvalidStateError\', \'SendfileNotAvailableError\', \'IncompleteReadError\', \'LimitOverrunError\'] Returns: str: A message indicating the result of the operation. pass ``` # Requirements 1. The function should: - Accept a single argument `simulate_exception` which is a string indicating the type of exception to simulate. - Depending on the value of `simulate_exception`, raise the corresponding `asyncio` exception. - Properly handle the raised exception and return a specific message for each exception type. 2. Handle the following exceptions: - `asyncio.TimeoutError`: Return the message **\\"Operation timed out.\\"** - `asyncio.CancelledError`: Return the message **\\"Operation cancelled.\\"** - `asyncio.InvalidStateError`: Return the message **\\"Invalid state encountered.\\"** - `asyncio.SendfileNotAvailableError`: Return the message **\\"Sendfile syscall unavailable.\\"** - `asyncio.IncompleteReadError`: Return the message **\\"Incomplete read operation. Expected X bytes, got Y bytes.\\"**, where X and Y are the values of `expected` and `partial` attributes respectively. - `asyncio.LimitOverrunError`: Return the message **\\"Buffer size limit overrun. Bytes to be consumed: Z.\\"**, where Z is the value of the `consumed` attribute. # Example Usage ```python # Simulating a timeout error result = await io_operation_manager(\\"TimeoutError\\") print(result) # Output: \\"Operation timed out.\\" # Simulating an incomplete read error result = await io_operation_manager(\\"IncompleteReadError\\") print(result) # Output: \\"Incomplete read operation. Expected 100 bytes, got 50 bytes.\\" ``` # Constraints - The function must be asynchronous. - The function must handle the asyncio exceptions using `try-except` blocks. - You may use the `raise` keyword to simulate exceptions. # Notes - Make sure to use the correct attributes specified in the provided documentation when constructing the error messages. - You may create helper functions if necessary to simulate the conditions for each exception.","solution":"import asyncio async def io_operation_manager(simulate_exception: str) -> str: Simulates various I/O operations and handles specific asyncio exceptions. Args: simulate_exception (str): The type of exception to simulate. Must be one of the following: [\'TimeoutError\', \'CancelledError\', \'InvalidStateError\', \'SendfileNotAvailableError\', \'IncompleteReadError\', \'LimitOverrunError\'] Returns: str: A message indicating the result of the operation. try: if simulate_exception == \'TimeoutError\': raise asyncio.TimeoutError() elif simulate_exception == \'CancelledError\': raise asyncio.CancelledError() elif simulate_exception == \'InvalidStateError\': raise asyncio.InvalidStateError() elif simulate_exception == \'SendfileNotAvailableError\': raise asyncio.SendfileNotAvailableError() elif simulate_exception == \'IncompleteReadError\': raise asyncio.IncompleteReadError(expected=100, partial=50) elif simulate_exception == \'LimitOverrunError\': raise asyncio.LimitOverrunError(consumed=75, message=\\"Buffer limit overrun\\") else: return \\"No exception simulated.\\" except asyncio.TimeoutError: return \\"Operation timed out.\\" except asyncio.CancelledError: return \\"Operation cancelled.\\" except asyncio.InvalidStateError: return \\"Invalid state encountered.\\" except asyncio.SendfileNotAvailableError: return \\"Sendfile syscall unavailable.\\" except asyncio.IncompleteReadError as e: return f\\"Incomplete read operation. Expected {e.expected} bytes, got {e.partial} bytes.\\" except asyncio.LimitOverrunError as e: return f\\"Buffer size limit overrun. Bytes to be consumed: {e.consumed}.\\""},{"question":"**Objective**: Implement a function that ensures the \\"pip\\" installer is bootstrapped into the provided virtual environment or Python installation according to specified options. This will involve using the `ensurepip` module programmatically and handling specified constraints. **Task**: Write a function `bootstrap_pip(root=None, upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0)` that bootstraps \\"pip\\" using the `ensurepip` module according to the provided parameters. If both `altinstall` and `default_pip` are set, the function should handle this gracefully by raising a `ValueError`. # Function Signature ```python def bootstrap_pip(root=None, upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0): pass ``` # Input Parameters - **root** (str or None): The alternative root directory for installation. If None, use the default install location. - **upgrade** (bool): Whether to upgrade an existing version of \\"pip\\". - **user** (bool): Whether to use the user scheme for installation. - **altinstall** (bool): If set, \\"pipX\\" will not be installed. - **default_pip** (bool): If set, the \\"pip\\" script will also be installed in addition to \\"pipX\\" and \\"pipX.Y\\". - **verbosity** (int): Controls the level of output to `sys.stdout`. # Output - No return value is expected. The function should ensure \\"pip\\" is installed according to the specified parameters. # Constraints - If both `altinstall` and `default_pip` are set, the function should raise a `ValueError`. # Example Usage ```python bootstrap_pip(root=\\"/custom/path\\", upgrade=True, user=False, altinstall=False, default_pip=True, verbosity=1) ``` This should bootstrap \\"pip\\" into the specified custom path with an upgrade, at default verbosity level, and install the \\"pip\\" script in addition to \\"pipX\\" and \\"pipX.Y\\". # Notes - Handle any exceptions that may arise from invalid configurations or other ensurepip-related issues gracefully, providing relevant error messages. - The function should utilize `ensurepip.bootstrap` from the `ensurepip` module internally.","solution":"import ensurepip import sys def bootstrap_pip(root=None, upgrade=False, user=False, altinstall=False, default_pip=False, verbosity=0): Bootstraps pip into the provided virtual environment or Python installation according to the specified options. :param root: The alternative root directory for installation. If None, use the default install location. :param upgrade: Whether to upgrade an existing version of pip. :param user: Whether to use the user scheme for installation. :param altinstall: If set, pipX will not be installed. :param default_pip: If set, the pip script will also be installed in addition to pipX and pipX.Y. :param verbosity: Controls the level of output to sys.stdout. if altinstall and default_pip: raise ValueError(\\"Cannot set both altinstall and default_pip to True.\\") # Prepare the arguments list for ensurepip.bootstrap() install_args = [] if root: install_args.append(f\'--root={root}\') if upgrade: install_args.append(\'--upgrade\') if user: install_args.append(\'--user\') if altinstall: install_args.append(\'--altinstall\') if default_pip: install_args.append(\'--default-pip\') # Redirect stdout based on verbosity if verbosity == 0: sys.stdout = open(\'/dev/null\', \'w\') elif verbosity == 1: sys.stdout = sys.__stdout__ elif verbosity > 1: sys.stdout = sys.__stdout__ # For simplicity, treat both verbosity > 1 the same try: # Run ensurepip.bootstrap() with the prepared arguments ensurepip.bootstrap(install_args) finally: # Restore stdout if verbosity == 0: sys.stdout = sys.__stdout__"},{"question":"Objective: To assess the student\'s understanding and ability to use various Python compound statements, including function definitions, class definitions, and control flow statements. Problem Statement: You are required to write a Python program that simulates a simple task manager. The task manager should be able to: 1. Add a new task with a title and description. 2. Mark a task as completed. 3. List all tasks, with an option to filter by completed or incomplete tasks. 4. Save tasks to a file and load tasks from a file. Requirements: 1. **Task Class**: - Define a class named `Task` with the following attributes: - `title` (string): the title of the task. - `description` (string): a short description of the task. - `completed` (boolean): status of the task. - Implement the `__init__` method to initialize the attributes. - Implement a method named `mark_completed` that marks the task as completed. - Implement a method named `__str__` to return a string representation of the task. 2. **TaskManager Class**: - Define a class named `TaskManager` with the following attributes: - `tasks` (list of `Task` objects): a list to store tasks. - Implement the `__init__` method to initialize the tasks list. - Implement a method named `add_task` that takes a title and description, creates a new `Task` object, and adds it to the tasks list. - Implement a method named `list_tasks` that returns a list of tasks, with an optional parameter to filter by completed or incomplete tasks. - Implement methods `save_to_file` and `load_from_file` to save tasks to a file and load tasks from a file, respectively. 3. **Main Program**: - Create an instance of `TaskManager`. - Create a menu-driven interface to interact with the task manager. - The menu should include options to add a task, mark a task as completed, list all tasks, list completed tasks, list incomplete tasks, save tasks to a file, and load tasks from a file. - Handle any exceptions that may occur during file operations. Constraints: - Use appropriate control flow statements. - Ensure the methods and classes are well-structured and demonstrate a clear understanding of Python\'s compound statements. - File operations should handle exceptions gracefully. Sample Usage: ```python # Initialize the task manager task_manager = TaskManager() # Add a task task_manager.add_task(\\"Buy groceries\\", \\"Milk, Eggs, Bread\\") # List tasks tasks = task_manager.list_tasks() for task in tasks: print(task) # Mark a task as completed tasks[0].mark_completed() # Save tasks to a file task_manager.save_to_file(\\"tasks.txt\\") # Load tasks from a file task_manager.load_from_file(\\"tasks.txt\\") ``` Submission: Submit a Python file containing the implementation of the `Task` class, `TaskManager` class, and the main program that provides the menu-driven interface.","solution":"import json class Task: def __init__(self, title, description): self.title = title self.description = description self.completed = False def mark_completed(self): self.completed = True def __str__(self): status = \'Completed\' if self.completed else \'Incomplete\' return f\\"Title: {self.title}, Description: {self.description}, Status: {status}\\" class TaskManager: def __init__(self): self.tasks = [] def add_task(self, title, description): new_task = Task(title, description) self.tasks.append(new_task) def list_tasks(self, completed=None): if completed is None: return self.tasks return [task for task in self.tasks if task.completed == completed] def save_to_file(self, filename): try: with open(filename, \'w\') as file: json.dump([task.__dict__ for task in self.tasks], file) except Exception as e: print(f\\"An error occurred while saving to file: {e}\\") def load_from_file(self, filename): try: with open(filename, \'r\') as file: tasks_data = json.load(file) self.tasks = [self._dict_to_task(task_data) for task_data in tasks_data] except Exception as e: print(f\\"An error occurred while loading from file: {e}\\") def _dict_to_task(self, task_data): task = Task(task_data[\'title\'], task_data[\'description\']) if task_data[\'completed\']: task.mark_completed() return task"},{"question":"**Question: Visualizing Airline Passengers Data using Seaborn** You are given a dataset that records the number of airline passengers who flew each month from 1949 to 1960. This dataset is currently in a wide-form organization. Your task is to transform this dataset into long-form and create visualizations using seaborn. # Input The dataset will be provided in a wide-form pandas DataFrame with years as the index and months as columns: ```python import pandas as pd flights_wide = pd.DataFrame({ \'year\': [1949, 1950, 1951, 1952, 1953], \'Jan\': [112, 115, 145, 171, 196], \'Feb\': [118, 126, 150, 180, 196], \'Mar\': [132, 141, 178, 193, 236], \'Apr\': [129, 135, 163, 181, 235], \'May\': [121, 125, 172, 209, 229], \'Jun\': [135, 149, 178, 191, 243], \'Jul\': [148, 170, 199, 225, 264], \'Aug\': [148, 170, 199, 225, 264], \'Sep\': [136, 158, 184, 242, 291], \'Oct\': [119, 133, 162, 206, 271], \'Nov\': [104, 114, 146, 191, 261], \'Dec\': [118, 140, 166, 197, 242] }).set_index(\'year\') ``` # Tasks 1. Transform the `flights_wide` DataFrame into a long-form DataFrame named `flights_long`. The long-form DataFrame should have columns: `year`, `month`, and `passengers`. 2. Create a line plot using seaborn’s `relplot` to show the number of passengers each month for each year from the `flights_long` DataFrame. Use different hues to distinguish between months. 3. Create another line plot using seaborn’s `relplot` with the original wide-form `flights_wide` DataFrame. Ensure each line represents a different month. 4. Compare the two visualizations and discuss the advantages or disadvantages of using long-form versus wide-form data for this specific visualization task. # Constraints - Use only seaborn and pandas libraries. - You should not use any loops; only pandas and seaborn functions to manipulate and visualize the data. # Expected Output 1. Transformation of the dataset into a long-form DataFrame. 2. A line plot created using seaborn from the long-form DataFrame. 3. A line plot created using seaborn from the wide-form DataFrame. 4. A short discussion comparing the two visualizations. # Example Transform the dataset and create visualizations as described above. ```python import pandas as pd import seaborn as sns sns.set_theme() # Given wide-form dataset flights_wide = pd.DataFrame({ \'year\': [1949, 1950, 1951, 1952, 1953], \'Jan\': [112, 115, 145, 171, 196], \'Feb\': [118, 126, 150, 180, 196], \'Mar\': [132, 141, 178, 193, 236], \'Apr\': [129, 135, 163, 181, 235], \'May\': [121, 125, 172, 209, 229], \'Jun\': [135, 149, 178, 191, 243], \'Jul\': [148, 170, 199, 225, 264], \'Aug\': [148, 170, 199, 225, 264], \'Sep\': [136, 158, 184, 242, 291], \'Oct\': [119, 133, 162, 206, 271], \'Nov\': [104, 114, 146, 191, 261], \'Dec\': [118, 140, 166, 197, 242] }).set_index(\'year\') # Task 1: Transform to long-form flights_long = flights_wide.melt(var_name=\'month\', value_name=\'passengers\', ignore_index=False).reset_index() # Task 2: Line plot from long-form DataFrame sns.relplot(data=flights_long, x=\'year\', y=\'passengers\', hue=\'month\', kind=\'line\') # Task 3: Line plot from wide-form DataFrame sns.relplot(data=flights_wide, kind=\'line\') # Example output to check the transformation and visualizations print(flights_long.head()) ``` # Short Discussion Explain the difference in ease-of-understanding and interpretability of the plots generated from the long-form and wide-form data. Discuss why one might be preferred over the other in different scenarios. # Note Submit your code, resulting plots, and the discussion in a single notebook or script file.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Given wide-form dataset flights_wide = pd.DataFrame({ \'year\': [1949, 1950, 1951, 1952, 1953], \'Jan\': [112, 115, 145, 171, 196], \'Feb\': [118, 126, 150, 180, 196], \'Mar\': [132, 141, 178, 193, 236], \'Apr\': [129, 135, 163, 181, 235], \'May\': [121, 125, 172, 209, 229], \'Jun\': [135, 149, 178, 191, 243], \'Jul\': [148, 170, 199, 225, 264], \'Aug\': [148, 170, 199, 225, 264], \'Sep\': [136, 158, 184, 242, 291], \'Oct\': [119, 133, 162, 206, 271], \'Nov\': [104, 114, 146, 191, 261], \'Dec\': [118, 140, 166, 197, 242] }).set_index(\'year\') # Task 1: Transform to long-form flights_long = flights_wide.melt(var_name=\'month\', value_name=\'passengers\', ignore_index=False).reset_index() # Task 2: Line plot from long-form DataFrame sns.relplot(data=flights_long, x=\'year\', y=\'passengers\', hue=\'month\', kind=\'line\') plt.title(\'Number of Passengers Each Month for Each Year (Long-form)\') plt.show() # Task 3: Line plot from wide-form DataFrame # Reshaping wide-form data for visualization flights_wide_reset = flights_wide.reset_index().melt(id_vars=\'year\', var_name=\'month\', value_name=\'passengers\') sns.lineplot(data=flights_wide_reset, x=\'year\', y=\'passengers\', hue=\'month\') plt.title(\'Number of Passengers Each Month for Each Year (Wide-form)\') plt.show() # Short Discussion The long-form DataFrame plot created in Task 2 provides a clear view of each month\'s data trend throughout the years. Each line is distinctly colored per month which makes it easy to distinguish between trends for each month visually. In contrast, the wide-form DataFrame plot might be less interpretable when directly using seaborn functions without transformation, as seaborn expects long-form data for many of its functions. Thus, transforming wide-form data into an acceptable format for plotting as done in Task 3 is necessary either way. In conclusion, long-form data is generally preferred for visualization tasks as it simplifies the syntax for plotting and enhances readability."},{"question":"Numerical Precision and Batched Computations in PyTorch Problem Statement You are provided with a batch of matrices and you need to implement a PyTorch function that performs matrix multiplications both in a batched manner and sequential manner. Due to the nuances of floating-point computations and the batched computations in PyTorch, outcomes can slightly differ. Additionally, the function should compare the results obtained from both approaches and compute the numerical differences. Task 1. Implement a PyTorch function `compare_batched_vs_sequential` that performs the following: - Accepts a 3D tensor `A` of shape `(B, M, N)` and a 3D tensor `B` of shape `(B, N, P)` where `B` is the batch size, `M`, `N`, and `P` are matrix dimensions. - Computes the batched matrix multiplication using `torch.bmm(A, B)`. - Computes the sequential matrix multiplication by iterating over the batch dimension and multiplying individual matrices using `torch.matmul`. - Computes the absolute differences between the batched and sequential results. 2. The function should return: - The batched multiplication result. - The sequential multiplication result. - The maximum absolute difference between the two results for each matrix in the batch. Constraints - The input tensors may include extremal values. - The operations should handle non-finite values appropriately, if encountered. - Your implementation should be feasible for large batch sizes (e.g., `B = 1000`), but you may assume that `M`, `N`, and `P` are reasonable sizes (e.g., ≤ 100). Performance Requirements - Ensure that your implementation is efficient and uses appropriate PyTorch functions to leverage hardware acceleration where possible. Example ```python import torch def compare_batched_vs_sequential(A: torch.Tensor, B: torch.Tensor): # Your implementation here # Test Example A = torch.randn((10, 5, 3), dtype=torch.float32) B = torch.randn((10, 3, 4), dtype=torch.float32) batched_result, sequential_result, max_diff = compare_batched_vs_sequential(A, B) print(\\"Batched Result: \\", batched_result) print(\\"Sequential Result: \\", sequential_result) print(\\"Max Differences: \\", max_diff) ``` Expected Output: The function should output the batched and sequential multiplication results, and indicate the maximum absolute differences between the corresponding results for each matrix in the batch. Notes - Students should account for any potential numerical instabilities due to the input values and ensure these are handled appropriately. - Understanding of PyTorch\'s handling of batched operations and floating-point precision differences is crucial to solve this task effectively.","solution":"import torch def compare_batched_vs_sequential(A: torch.Tensor, B: torch.Tensor): Computes batched and sequential matrix multiplications and compares their results. Parameters: A (torch.Tensor): A 3D tensor of shape (B, M, N) B (torch.Tensor): A 3D tensor of shape (B, N, P) Returns: tuple: batched multiplication result, sequential multiplication result, maximum absolute difference for each matrix in the batch # Batched matrix multiplication batched_result = torch.bmm(A, B) # Sequential matrix multiplication sequential_result = torch.empty_like(batched_result) for i in range(A.shape[0]): sequential_result[i] = torch.matmul(A[i], B[i]) # Compute the absolute differences differences = torch.abs(batched_result - sequential_result) # Max absolute difference for each matrix in the batch max_diff = torch.max(differences.view(differences.size(0), -1), dim=1)[0].tolist() return batched_result, sequential_result, max_diff"},{"question":"Objective To assess your understanding of the seaborn library\'s abilities in creating visually attractive and customized plots by utilizing different themes, styles, and context settings provided by seaborn. Problem Statement You are given a dataset representing the average temperatures (in Celsius) for 12 months in five different cities around the world. Your task is to prepare a combination of subplots that effectively communicate this data using seaborn\'s aesthetic functionalities. Requirements: 1. Load the required libraries: `numpy`, `seaborn`, and `matplotlib.pyplot`. 2. Define the data as a `numpy` array with shape (5,12), where each row represents a city and each column represents average monthly temperatures. 3. Create a figure with a 2x2 grid of subplots of the following configurations: - **Top-left subplot**: Set style as `darkgrid`. - **Top-right subplot**: Set style as `white`. - **Bottom-left subplot**: Set style as `ticks` and remove right and top spines. - **Bottom-right subplot**: Set style as `whitegrid` and context as `poster`, while scaling font size up by 1.5 and line width by 2.5. 4. In each subplot, plot the temperature data showing individual city\'s data as separate lines. 5. Add a unique title to each subplot to specify the style/context used. Constraints: - You must use seaborn\'s functionalities to set styles and contexts. - All plots should have gridlines drawn for better readability. - Ensure that each subplot is clearly titled to reflect the changes carried out in that subplot. expected inputs: - None. The function should be self-contained. expected outputs: - A combined 2x2 grid of plots displayed using matplotlib with the specified styles and contexts. Here\'s the data you need to use: ```python temperature_data = np.array([ [30.2, 32.5, 35.1, 37.3, 40.0, 42.6, 41.4, 39.5, 36.3, 34.0, 32.1, 30.3], # City 1 [25.3, 27.1, 28.9, 30.7, 32.6, 33.5, 35.2, 36.1, 34.5, 31.8, 28.6, 25.8], # City 2 [10.2, 12.3, 15.1, 18.2, 21.3, 24.5, 27.4, 26.1, 23.5, 20.2, 16.4, 12.9], # City 3 [18.0, 20.5, 23.2, 26.8, 30.1, 32.4, 31.2, 29.3, 25.7, 22.6, 19.1, 16.0], # City 4 [5.1, 7.3, 10.4, 14.2, 17.6, 21.2, 24.5, 22.1, 19.0, 15.3, 10.2, 6.5] # City 5 ]) ``` Function Signature ```python def create_customized_temperature_plots(): pass ``` Example Output The expected output is the display of a matplotlib figure containing 2x2 subplots reflecting the stated styles and contexts. This code will not return values but will show the plot.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_customized_temperature_plots(): # Data: average monthly temperatures for five cities temperature_data = np.array([ [30.2, 32.5, 35.1, 37.3, 40.0, 42.6, 41.4, 39.5, 36.3, 34.0, 32.1, 30.3], # City 1 [25.3, 27.1, 28.9, 30.7, 32.6, 33.5, 35.2, 36.1, 34.5, 31.8, 28.6, 25.8], # City 2 [10.2, 12.3, 15.1, 18.2, 21.3, 24.5, 27.4, 26.1, 23.5, 20.2, 16.4, 12.9], # City 3 [18.0, 20.5, 23.2, 26.8, 30.1, 32.4, 31.2, 29.3, 25.7, 22.6, 19.1, 16.0], # City 4 [5.1, 7.3, 10.4, 14.2, 17.6, 21.2, 24.5, 22.1, 19.0, 15.3, 10.2, 6.5] # City 5 ]) months = np.arange(1, 13) # Create a 2x2 grid of subplots fig, ax = plt.subplots(2, 2, figsize=(15, 10)) # Top-left subplot: Set style as darkgrid sns.set_style(\\"darkgrid\\") for city in temperature_data: sns.lineplot(x=months, y=city, ax=ax[0,0]) ax[0,0].set_title(\\"Style: darkgrid\\") # Top-right subplot: Set style as white sns.set_style(\\"white\\") for city in temperature_data: sns.lineplot(x=months, y=city, ax=ax[0,1]) ax[0,1].set_title(\\"Style: white\\") # Bottom-left subplot: Set style as ticks and remove right/top spines sns.set_style(\\"ticks\\") for city in temperature_data: sns.lineplot(x=months, y=city, ax=ax[1,0]) ax[1,0].set_title(\\"Style: ticks\\") sns.despine(ax=ax[1,0]) # Bottom-right subplot: Set style as whitegrid and context as poster, scaling font size and line width sns.set_style(\\"whitegrid\\") sns.set_context(\\"poster\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}) for city in temperature_data: sns.lineplot(x=months, y=city, ax=ax[1,1]) ax[1,1].set_title(\\"Style: whitegrid, Context: poster\\") plt.tight_layout() plt.show()"},{"question":"You have been given a dataset that contains various types of data including categorical, numerical, and text features. Your task is to build a processing pipeline using scikit-learn\'s `Pipeline`, `FeatureUnion`, and `ColumnTransformer` that: 1. Preprocesses the data appropriately. 2. Trains a machine learning model to predict a target variable. # Dataset - The dataset is a fictional customer dataset used for classifying whether a customer will buy a product or not. # Data Format | **Feature** | **Type** | **Description** | |--------------------|-------------|--------------------------------| | age | numerical | Age of the customer | | gender | categorical | Gender of the customer | | city | categorical | City of the customer | | annual_income | numerical | Annual income of the customer | | purchase_history | text | Recent purchase history | | target | binary | 1: Purchased, 0: Not Purchased | # Steps to Follow: 1. **Load the dataset**: - Assume the dataset `customer_data.csv` is given. 2. **Preprocess the data**: - `age` and `annual_income` should be standardized. - `gender` and `city` should be one-hot encoded. - `purchase_history` should be transformed using TF-IDF (Term Frequency-Inverse Document Frequency). 3. **Build the Pipeline**: - Use `ColumnTransformer` to preprocess the columns as specified. - Combine `ColumnTransformer` with a logistic regression classifier using `Pipeline`. 4. **Train the Model**: - Train the pipeline on the training data. - Evaluate the model using 5-fold cross-validation. # Expected Input and Output: - Input: Path to the CSV file. - Output: Cross-validation accuracy scores for the model. # Function Template Below is a function template to get you started: ```python import pandas as pd from sklearn.compose import ColumnTransformer, make_column_transformer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.model_selection import cross_val_score def train_customer_model(data_path): # Step 1: Load the dataset data = pd.read_csv(data_path) # Step 2: Define the column transformer preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'age\', \'annual_income\']), (\'cat_gender\', OneHotEncoder(), [\'gender\']), (\'cat_city\', OneHotEncoder(), [\'city\']), (\'text\', TfidfVectorizer(), \'purchase_history\') ], remainder=\'drop\' # Drop other columns ) # Step 3: Create a pipeline with the column transformer and Logistic Regression pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression()) ]) # Step 4: Separate features and target X = data.drop(\'target\', axis=1) y = data[\'target\'] # Step 5: Train the model using cross-validation scores = cross_val_score(pipeline, X, y, cv=5) # Output the cross-validation scores print(\\"Cross-validation accuracy scores:\\", scores) return scores # Example usage: # train_customer_model(\'customer_data.csv\') ``` # Constraints and Limitations: - Ensure proper handling of missing values, if any. - Your solution should handle the preprocessing and model training as encapsulated steps in the pipeline. # Additional Information: - You can use additional parameters and options provided by scikit-learn transformers and pipelines to enhance the solution. - Make sure to validate the correctness of the pipeline.","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.model_selection import cross_val_score def train_customer_model(data_path): Trains a logistic regression model to classify whether a customer will buy a product or not based on various features. Parameters: data_path (str): Path to the CSV file containing the dataset. Returns: list: Cross-validation accuracy scores for the model. # Load the dataset data = pd.read_csv(data_path) # Define the column transformer for preprocessing preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [\'age\', \'annual_income\']), (\'cat_gender\', OneHotEncoder(), [\'gender\']), (\'cat_city\', OneHotEncoder(), [\'city\']), (\'text\', TfidfVectorizer(), \'purchase_history\') ], remainder=\'drop\' # Drop other columns ) # Create a pipeline with the column transformer and Logistic Regression pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', LogisticRegression(max_iter=1000)) ]) # Separate features and target X = data.drop(\'target\', axis=1) y = data[\'target\'] # Train the model using cross-validation scores = cross_val_score(pipeline, X, y, cv=5) # Output the cross-validation scores return scores # Example usage: # scores = train_customer_model(\'customer_data.csv\') # print(\\"Cross-validation accuracy scores:\\", scores)"},{"question":"You are given a DataFrame that represents sales data with details about the product and sales amount. Here\'s a sample DataFrame, `df`: ```python import pandas as pd data = { \\"Product\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"Sales\\": [100, 150, 200, 250] } df = pd.DataFrame(data) ``` Your task is to implement a function called `adjust_sales` that takes a DataFrame and a percentage (as a float). The function should increase the sales values of the DataFrame by the given percentage without directly modifying the original DataFrame, ensuring it complies with the Copy-on-Write principles. # Function Signature ```python def adjust_sales(df: pd.DataFrame, percentage: float) -> pd.DataFrame: pass ``` # Input - `df`: A pandas DataFrame containing at least two columns: \\"Product\\" and \\"Sales\\". - `percentage`: A float representing the percentage increment for the sales values. # Output - A new DataFrame with adjusted sales values. # Constraints - You cannot use in-place operations. - The original DataFrame should remain unchanged. # Example Usage ```python import pandas as pd data = { \\"Product\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\"], \\"Sales\\": [100, 150, 200, 250] } df = pd.DataFrame(data) new_df = adjust_sales(df, 10.0) print(new_df) # Output: # Product Sales # 0 A 110.0 # 1 B 165.0 # 2 C 220.0 # 3 D 275.0 print(df) # Original DataFrame should be unchanged: # Product Sales # 0 A 100 # 1 B 150 # 2 C 200 # 3 D 250 ``` # Notes - Use methods compatible with Copy-on-Write where applicable. - Thoroughly test the function to ensure the original DataFrame remains unchanged and the new DataFrame contains the correctly adjusted sales values.","solution":"import pandas as pd def adjust_sales(df: pd.DataFrame, percentage: float) -> pd.DataFrame: Adjust the sales values in the DataFrame by the given percentage. Parameters: df (pd.DataFrame): The original sales DataFrame with columns \'Product\' and \'Sales\'. percentage (float): The percentage increment for the sales values. Returns: pd.DataFrame: A new DataFrame with adjusted sales values. # Create a copy of the DataFrame to ensure the original is not modified new_df = df.copy() # Adjust the \'Sales\' values by the given percentage new_df[\'Sales\'] = new_df[\'Sales\'] * (1 + percentage / 100) return new_df"},{"question":"**Objective:** Create a function in Python that initializes a custom Python interpreter configuration based on certain parameters and then executes a Python command or script. **Description:** Implement a function named `initialize_and_run_python` that takes in various configuration parameters and initializes a customized Python environment, then it runs a specified Python command or script. Use the C-level Python Initialization Configuration structures (`PyConfig`, `PyPreConfig`) and functions via a C extension or through a Python wrapping interface like `ctypes` or `cffi`. **Function Signature:** ```python def initialize_and_run_python( isolated_mode: bool, utf8_mode: bool, command: str = None, script_filename: str = None, module_name: str = None, args: list = None) -> int: Initialize a custom Python interpreter configuration and execute a command/script/module. Parameters: - isolated_mode (bool): If True, run in isolated mode. - utf8_mode (bool): If True, enable Python UTF-8 Mode. - command (str, optional): A Python command to execute. - script_filename (str, optional): Filename of the Python script to execute. - module_name (str, optional): The name of a Python module to execute. - args (list, optional): A list of arguments to pass to the Python interpreter. Returns: - int: The exit code of the Python execution. ``` **Requirements:** 1. Create a `PyPreConfig` structure and initialize it based on `isolated_mode` and `utf8_mode`. 2. Use the `Py_PreInitialize` function to preinitialize Python with this configuration. 3. Create a `PyConfig` structure and set its parameters such as `argv`. 4. Set the command, script, or module to be executed. 5. Use `Py_InitializeFromConfig` to initialize Python with the configuration. 6. Execute the specified command/script/module using `Py_RunMain`. 7. Handle exceptions and cleanup properly using `PyStatus` functions. 8. Return the exit code from the Python execution. **Constraints:** - You must use the provided C API functions and structures (`PyConfig`, `PyPreConfig`, `Py_InitializeFromConfig`, etc.). - If `args` is not provided, `sys.argv` should default to an empty list with an empty string. **Example Usage:** ```python exit_code = initialize_and_run_python( isolated_mode=True, utf8_mode=True, command=\'print(\\"Hello, Custom Python!\\")\' ) print(f\\"Exit Code: {exit_code}\\") ``` **Note:** For this exercise, simulate or mock the C API behavior if the actual C interface is not accessible within your environment. Ensure your implementation demonstrates the expected structure and flow as per the provided documentation.","solution":"import sys def initialize_and_run_python( isolated_mode: bool, utf8_mode: bool, command: str = None, script_filename: str = None, module_name: str = None, args: list = None) -> int: # Simulate the initialization of the custom Python environment # Since we can\'t use actual C APIs here, we will mock the expected behavior if isolated_mode: print(\\"Running in isolated mode.\\") if utf8_mode: print(\\"UTF-8 mode enabled.\\") if command: print(f\\"Executing command: {command}\\") exec(command) elif script_filename: with open(script_filename, \'r\') as file: script = file.read() print(f\\"Executing script from file: {script_filename}\\") exec(script) elif module_name: print(f\\"Running module: {module_name}\\") __import__(module_name) # This is a simplified mock return code for the purposes of illustration return 0"},{"question":"Objective: Assess students\' understanding of the \\"ast\\" module and their ability to traverse and manipulate Abstract Syntax Trees in Python. Question: You are given Python code that contains function definitions. Write a function `count_function_defs(source_code: str) -> int` that uses the `ast` module to count the number of function definitions in the provided source code. Input: - `source_code`: A string containing valid Python source code. Output: - An integer representing the number of function definitions found in the source code. Constraints: - The provided source code will always be syntactically correct. - The source code may contain nested functions. - The source code may contain other constructs like classes, imports, and control flow statements. Example: ```python source_code = def foo(): pass def bar(): def baz(): pass return baz class MyClass: def method(self): def inner_method(): pass return inner_method assert count_function_defs(source_code) == 5 ``` Additional Information: You may need to look into the following aspects of the \\"ast\\" module to solve this problem: - Parsing source code into an AST - Traversing the AST to identify function definitions Performance Requirement: - The function should be efficient enough to handle Python source code of reasonable size (e.g., up to a few hundred lines of code).","solution":"import ast def count_function_defs(source_code: str) -> int: Counts the number of function definitions in the provided source code. Parameters: - source_code: A string containing valid Python source code. Returns: An integer representing the number of function definitions found in the source code. tree = ast.parse(source_code) return sum(isinstance(node, ast.FunctionDef) for node in ast.walk(tree))"},{"question":"**Objective**: You are required to demonstrate your understanding of the scikit-learn dataset utilities by loading a dataset, analyzing it, and performing some basic machine learning operations on it. **Problem Statement**: 1. **Load Dataset**: - Use the `load_iris` function from `sklearn.datasets` to load the Iris dataset. - Understand the structure and content of the returned `Bunch` object. 2. **Data Analysis**: - Print the dataset description. - Extract the feature names and target names. - Convert the `data` and `target` from the `Bunch` object into a Pandas DataFrame for further analysis. 3. **Data Visualization**: - Generate a scatter plot for the first two features (sepal length and sepal width), color-coded by the target class. 4. **Model Training**: - Use the extracted data to train a simple k-NN classifier (k=3) using scikit-learn. - Split the data into training and testing sets (80-20 split). - Train the model on the training set and evaluate on the testing set. - Print the accuracy of the model. **Expected Input and Output Formats**: **Input**: No input required for this task. You will be loading the dataset using the `load_iris` function. **Output**: - Print statements for dataset description. - Print statements for feature names and target names. - Display of the scatter plot. - Print statements for the accuracy of the k-NN model. **Solution Skeleton**: ```python import pandas as pd import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def main(): # Load the Iris dataset iris = load_iris() # Print the dataset description print(iris.DESCR) # Extract and print feature names and target names feature_names = iris.feature_names target_names = iris.target_names print(\\"Feature Names:\\", feature_names) print(\\"Target Names:\\", target_names) # Convert data to Pandas DataFrame df = pd.DataFrame(data=iris.data, columns=feature_names) df[\'target\'] = iris.target # Generate scatter plot plt.figure(figsize=(10, 6)) for target, color in zip(range(len(target_names)), [\'r\', \'g\', \'b\']): subset = df[df[\'target\'] == target] plt.scatter(subset.iloc[:, 0], subset.iloc[:, 1], c=color, label=target_names[target]) plt.xlabel(feature_names[0]) plt.ylabel(feature_names[1]) plt.legend() plt.title(\\"Iris Dataset - Scatter Plot\\") plt.show() # Train a k-NN classifier X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42) knn = KNeighborsClassifier(n_neighbors=3) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) # Print the accuracy of the model accuracy = accuracy_score(y_test, y_pred) print(f\\"Model Accuracy: {accuracy:.2f}\\") if __name__ == \\"__main__\\": main() ``` **Constraints and Requirements**: - The output and results should be clearly displayed. - Use appropriate libraries for data manipulation, visualization, and model training.","solution":"import pandas as pd import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def load_and_analyze_iris_dataset(): # Load the Iris dataset iris = load_iris() # Print the dataset description print(iris.DESCR) # Extract and print feature names and target names feature_names = iris.feature_names target_names = iris.target_names print(\\"Feature Names:\\", feature_names) print(\\"Target Names:\\", target_names) # Convert data to Pandas DataFrame df = pd.DataFrame(data=iris.data, columns=feature_names) df[\'target\'] = iris.target return df, target_names, feature_names def plot_iris_data(df, feature_names, target_names): # Generate scatter plot plt.figure(figsize=(10, 6)) for target, color in zip(range(len(target_names)), [\'r\', \'g\', \'b\']): subset = df[df[\'target\'] == target] plt.scatter(subset.iloc[:, 0], subset.iloc[:, 1], c=color, label=target_names[target]) plt.xlabel(feature_names[0]) plt.ylabel(feature_names[1]) plt.legend() plt.title(\\"Iris Dataset - Scatter Plot\\") plt.show() def train_and_evaluate_knn(df): # Prepare the data for model training X = df.drop(columns=[\'target\']) y = df[\'target\'] # Split the data into training and testing sets (80-20 split) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize the k-NN classifier knn = KNeighborsClassifier(n_neighbors=3) # Train the model on the training set knn.fit(X_train, y_train) # Make predictions on the testing set y_pred = knn.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) print(f\\"Model Accuracy: {accuracy:.2f}\\") def main(): df, target_names, feature_names = load_and_analyze_iris_dataset() plot_iris_data(df, feature_names, target_names) train_and_evaluate_knn(df) if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Demonstrate your understanding of custom context managers and exception handling using Python\'s `with` statement and `try` block. Problem Statement You are required to create a custom context manager using a class in Python. This context manager should handle the following: 1. **Initialization**: Initialize with a specific resource (e.g., a file, a connection, etc.). 2. **Context Management**: Implement the `__enter__` and `__exit__` methods to manage the resource. 3. **Exception Handling**: Ensure that any exceptions that occur within the context are properly handled and logged. 4. **Resource Cleanup**: Ensure that the resource is properly cleaned up after usage, regardless of whether an exception occurred. Your context manager should be used in a `with` statement to perform an operation that might raise an exception. Specifically, perform file operations where you read from a non-existent file and handle the resulting exception. Requirements 1. Define a class `FileManager` that implements: - `__init__`: Initializes the file path. - `__enter__`: Tries to open the file and returns the file object. - `__exit__`: Ensures the file is closed and any exceptions are logged without crashing the program. 2. Use the `FileManager` class in a `with` statement to read from a file named `non_existent_file.txt`. 3. Implement a logging mechanism to log any exceptions that occur. Expected Input and Output - **Input**: None (hard-code the file name `non_existent_file.txt`). - **Output**: Any exception that occurs should be logged. If the file does not exist, it should log an appropriate message. Constraints - You must use the `with` statement to manage the resource. - Use Python\'s built-in logging module for logging exceptions. - Ensure that all resources are properly cleaned up even if an exception occurs. Example ```python import logging class FileManager: def __init__(self, file_path): self.file_path = file_path def __enter__(self): try: self.file = open(self.file_path, \'r\') return self.file except FileNotFoundError as e: logging.error(f\\"Error opening file: {e}\\") raise def __exit__(self, exc_type, exc_val, exc_tb): if self.file: self.file.close() if exc_type: logging.error(f\\"An error occurred: {exc_val}\\") return True # Suppress exceptions if __name__ == \\"__main__\\": logging.basicConfig(level=logging.ERROR) file_path = \'non_existent_file.txt\' try: with FileManager(file_path) as file: content = file.read() except Exception as e: logging.error(f\\"Exception caught in main: {e}\\") ``` In this example, the `FileManager` class is used to handle file operations. If the file does not exist, an exception is logged, but the program continues running without crashing.","solution":"import logging class FileManager: def __init__(self, file_path): self.file_path = file_path self.file = None def __enter__(self): try: self.file = open(self.file_path, \'r\') return self.file except FileNotFoundError as e: logging.error(f\\"Error opening file: {e}\\") raise def __exit__(self, exc_type, exc_val, exc_tb): if self.file: self.file.close() if exc_type: logging.error(f\\"An error occurred: {exc_val}\\") return True # Suppress exceptions if __name__ == \\"__main__\\": logging.basicConfig(level=logging.ERROR) file_path = \'non_existent_file.txt\' try: with FileManager(file_path) as file: content = file.read() except Exception as e: logging.error(f\\"Exception caught in main: {e}\\")"},{"question":"**Advanced Coding Problem: Implementing Process Forking and Signal Handling** # Objective: Write a Python program that demonstrates the forking of processes and handles signals appropriately. You are to ensure the proper usage of fork, signal handling, and Python interpreter state updates across parent and child processes. # Instructions: 1. **Forking Processes:** - Implement a function `fork_process()` which forks the current process. The function should update the internal interpreter state using `PyOS_BeforeFork`, `PyOS_AfterFork_Parent`, and `PyOS_AfterFork_Child`. 2. **Signal Handling:** - Set up a custom signal handler for `SIGUSR1` and `SIGUSR2`. Implement two functions, `handle_sigusr1` and `handle_sigusr2`, to handle `SIGUSR1` and `SIGUSR2` respectively. Use `PyOS_setsig` to set these handlers. 3. **Main Execution:** - In the main execution, set up your custom handlers. Then fork the process. - The parent process should wait for a signal, and upon receiving `SIGUSR1`, it should print \\"Received SIGUSR1!\\" and exit. Upon receiving `SIGUSR2`, it should print \\"Received SIGUSR2!\\" and exit. - The child process should sleep for 5 seconds and then send a `SIGUSR1` signal to the parent process. 4. **Function Signatures:** - `def fork_process() -> None` - `def handle_sigusr1(signum: int, frame: Optional[FrameType]) -> None` - `def handle_sigusr2(signum: int, frame: Optional[FrameType]) -> None` # Constraints: - You must manage resources correctly and ensure that no memory leaks occur. - Your implementation should be compatible with Python 3.10. - You should handle possible errors gracefully and ensure that any raised exceptions are properly managed. # Example Code Execution: ```python if __name__ == \\"__main__\\": # Set up signal handlers PyOS_setsig(signal.SIGUSR1, handle_sigusr1) PyOS_setsig(signal.SIGUSR2, handle_sigusr2) # Perform process forking fork_process() ``` # Expected Output: - Parent process should print \\"Received SIGUSR1!\\" and exit. - If you send SIGUSR2 to the parent process, it should print \\"Received SIGUSR2!\\" and exit. Make sure to test your code thoroughly to cover edge cases, such as fork failures or signal setting failures.","solution":"import os import signal import time from typing import Optional from types import FrameType def handle_sigusr1(signum: int, frame: Optional[FrameType]) -> None: print(\\"Received SIGUSR1!\\") os._exit(0) def handle_sigusr2(signum: int, frame: Optional[FrameType]) -> None: print(\\"Received SIGUSR2!\\") os._exit(0) def fork_process() -> None: pid = os.fork() if pid == 0: # Child process time.sleep(5) os.kill(os.getppid(), signal.SIGUSR1) os._exit(0) elif pid > 0: # Parent process waits for a signal signal.pause() else: raise RuntimeError(\\"Fork failed\\") if __name__ == \\"__main__\\": # Set up signal handlers signal.signal(signal.SIGUSR1, handle_sigusr1) signal.signal(signal.SIGUSR2, handle_sigusr2) # Perform process forking fork_process()"},{"question":"Objective: Create a WSGI application using the `wsgiref` library which serves static files from a specified directory. Implement and validate a middleware that processes the request and sets specific headers before the response is sent to the client. Requirements: 1. **Implement the WSGI Application:** - The application should serve static files from a given directory. - If the requested file is not found, the application should return a `404 Not Found` response. 2. **Create a Middleware:** - The middleware should add the following headers to the response: - `X-Content-Type-Options: nosniff` - `X-Frame-Options: DENY` - The middleware should wrap around the application to ensure these headers are set for every response. 3. **Validation:** - Use `wsgiref.validate.validator` to wrap the application to ensure it conforms to WSGI specifications. 4. **Testing with a Simple Server:** - Use `wsgiref.simple_server` to serve the application. - Start the server on port 8000 and serve files from the current directory. Expected Input: - The directory to serve and optionally a port number for the server. Expected Output: - Serving static files from the specified directory. - Proper HTTP response headers set by the middleware. - Validation errors (if any) should be displayed in the server log. Constraints: - The solution should be conformant with the WSGI specification. Example Usage: ```python # To run the server serving the current directory on the default port 8000 # python3 wsgi_app.py # To run the server serving the specified directory on port 8080 # python3 wsgi_app.py /path/to/serve 8080 ``` Example Code: ```python import sys import os import mimetypes from wsgiref import simple_server, util, validate def simple_app(environ, start_response): path = environ.get(\'PATH_INFO\', \'/\') if path == \'/\': path = \'/index.html\' # Default file file_path = \'./\' + path try: with open(file_path, \'rb\') as f: content = f.read() content_type, _ = mimetypes.guess_type(file_path) headers = [(\'Content-Type\', content_type or \'application/octet-stream\')] start_response(\'200 OK\', headers) return [content] except FileNotFoundError: start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain\')]) return [b\'404 Not Found\'] def headers_middleware(app): def middleware(environ, start_response): status_headers = [] def custom_start_response(status, headers, exc_info=None): headers.append((\'X-Content-Type-Options\', \'nosniff\')) headers.append((\'X-Frame-Options\', \'DENY\')) status_headers.extend([status, headers]) return start_response(status, headers, exc_info) result = app(environ, custom_start_response) return result return middleware if __name__ == \'__main__\': port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000 validated_app = validate.validator(headers_middleware(simple_app)) server = simple_server.make_server(\'\', port, validated_app) print(f\\"Serving on port {port}...\\") server.serve_forever() ``` In the provided example, write your `wsgi_app.py` file to create a WSGI application serving the current directory with the specified headers via middleware and validate the application before serving with `wsgiref.simple_server`.","solution":"import sys import os from wsgiref import simple_server, validate import mimetypes def simple_app(environ, start_response): A simple WSGI application that serves static files. path = environ.get(\'PATH_INFO\', \'/\') if path == \'/\': path = \'/index.html\' # Serve index.html by default if no specific file is requested file_path = \'.\' + path # Relative path try: with open(file_path, \'rb\') as f: content = f.read() content_type, _ = mimetypes.guess_type(file_path) headers = [(\'Content-Type\', content_type or \'application/octet-stream\')] start_response(\'200 OK\', headers) return [content] except FileNotFoundError: start_response(\'404 Not Found\', [(\'Content-Type\', \'text/plain\')]) return [b\'404 Not Found\'] def headers_middleware(app): Middleware to add security headers to the response. def middleware(environ, start_response): def custom_start_response(status, headers, exc_info=None): headers.append((\'X-Content-Type-Options\', \'nosniff\')) headers.append((\'X-Frame-Options\', \'DENY\')) return start_response(status, headers, exc_info) return app(environ, custom_start_response) return middleware if __name__ == \'__main__\': port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000 validated_app = validate.validator(headers_middleware(simple_app)) server = simple_server.make_server(\'\', port, validated_app) print(f\\"Serving on port {port}...\\") server.serve_forever()"},{"question":"You are required to write a Python function that manipulates a given WAV file by increasing the playback speed. Here is the detailed description of the task: **Function Signature:** ```python def increase_playback_speed(input_file: str, output_file: str, speed_factor: float) -> None: ``` **Parameters:** - `input_file` (str): The path to the input WAV file to be read. - `output_file` (str): The path to the output WAV file where the modified audio will be saved. - `speed_factor` (float): The factor by which to increase the playback speed (e.g., a speed_factor of 2.0 will double the playback speed). **Output:** - The function does not return any value, but it writes the modified audio to `output_file`. **Constraints:** - The `speed_factor` will be a positive float greater than 1.0. - The input WAV file will not be compressed (i.e., it uses the \\"WAVE_FORMAT_PCM\\" format). - The function should handle input WAV files with multiple audio channels (mono, stereo, etc.). **Requirements:** - Open the input WAV file and read its parameters and audio data. - Modify the frame rate to increase the playback speed according to the speed factor. - Write the modified audio data to the output WAV file with updated parameters. Here is an example of how the function can be used: ```python input_file = \'input.wav\' output_file = \'output.wav\' speed_factor = 1.5 increase_playback_speed(input_file, output_file, speed_factor) ``` In this example, the function reads the audio data from \'input.wav\', modifies the frame rate to increase the playback speed by a factor of 1.5, and writes the modified audio data to \'output.wav\'. Implement the function `increase_playback_speed` using the `wave` module to achieve the desired functionality.","solution":"import wave def increase_playback_speed(input_file: str, output_file: str, speed_factor: float) -> None: # Open input WAV file. with wave.open(input_file, \'rb\') as input_wav: # Read original WAV file parameters. params = input_wav.getparams() nchannels, sampwidth, framerate, nframes, comptype, compname = params # Read audio frames. audio_frames = input_wav.readframes(nframes) # Modify frame rate for increased playback speed. new_framerate = int(framerate * speed_factor) # Write modified audio to output WAV file with updated parameters. with wave.open(output_file, \'wb\') as output_wav: output_wav.setnchannels(nchannels) output_wav.setsampwidth(sampwidth) output_wav.setframerate(new_framerate) output_wav.writeframes(audio_frames)"},{"question":"**Custom Value Formatter** You are required to implement a custom string formatter that extends the capabilities of the default Python string formatting. Your custom formatter should be able to handle a special format specification that allows conditional formatting based on the value being formatted. # Task 1. **Define** a class `ConditionalFormatter` that extends `string.Formatter`. 2. **Override** the `format_field` method to add support for a custom format specification. The custom format should allow conditional formatting based on the value. 3. The custom format specification will follow this pattern: ``` \\"{:positive<format_if_positive>|negative<format_if_negative>|zero<format_if_zero>}\\" ``` - `positive<format_if_positive>`: Format to apply if the value is positive. - `negative<format_if_negative>`: Format to apply if the value is negative. - `zero<format_if_zero>`: Format to apply if the value is zero. 4. **Implement** logic to correctly apply the respective formats based on the sign of the value. 5. If the custom format specification is not provided, fall back to the default formatting behavior. # Input and Output - Input will be a format string and values to be formatted into it. - Output will be the formatted string with the custom formats applied as per the specifications. # Constraints - The format string may contain multiple fields, each potentially using the custom format. - The values to be formatted will only be numeric (integers or floats). - Your class should handle cases where some or all parts of the custom format are not provided, defaulting to an empty string for those parts. # Example ```python from string import Formatter class ConditionalFormatter(Formatter): def format_field(self, value, format_spec): if \\"<\\" in format_spec: positive_format, negative_format, zero_format = \\"\\", \\"\\", \\"\\" parts = format_spec.split(\\"|\\") for part in parts: if part.startswith(\\"positive\\"): positive_format = part[len(\\"positive\\"):] elif part.startswith(\\"negative\\"): negative_format = part[len(\\"negative\\"):] elif part.startswith(\\"zero\\"): zero_format = part[len(\\"zero\\"):] if value > 0: return format(value, positive_format) elif value < 0: return format(value, negative_format) else: return format(value, zero_format) # Fallback to default behavior return super().format_field(value, format_spec) formatter = ConditionalFormatter() # Test cases print(formatter.format(\\"Result: {:positive(+.2f)|negative(-.2f)|zero(.0f)}\\", 23.45)) # Output: \\"Result: +23.45\\" print(formatter.format(\\"Result: {:positive(+.2f)|negative(-.2f)|zero(.0f)}\\", -23.45)) # Output: \\"Result: -23.45\\" print(formatter.format(\\"Result: {:positive(+.2f)|negative(-.2f)|zero(.0f)}\\", 0)) # Output: \\"Result: 0\\" ``` # Explanation - In the example, the custom format specification helps format the value differently based on whether it\'s positive, negative, or zero. - `\\"{:positive(+.2f)|negative(-.2f)|zero(.0f)}\\"` specifies the formats for positive, negative, and zero values respectively. - The `format_field` method in `ConditionalFormatter` parses this custom format and applies the appropriate format based on the value. # Note You should test your implementation extensively to ensure it handles various edge cases and follows the described behavior accurately.","solution":"from string import Formatter class ConditionalFormatter(Formatter): def format_field(self, value, format_spec): if \\"<\\" in format_spec: # Default formats for each type positive_format, negative_format, zero_format = \\"\\", \\"\\", \\"\\" parts = format_spec.split(\\"|\\") for part in parts: if part.startswith(\\"positive\\"): positive_format = part[len(\\"positive<\\"):-1] elif part.startswith(\\"negative\\"): negative_format = part[len(\\"negative<\\"):-1] elif part.startswith(\\"zero\\"): zero_format = part[len(\\"zero<\\"):-1] # Format based on the sign of the number if value > 0: return format(value, positive_format) elif value < 0: return format(value, negative_format) else: return format(value, zero_format) # Fallback to default behavior return super().format_field(value, format_spec)"},{"question":"Implementing a File Comparison Tool **Objective:** Develop a Python function that compares the contents of two text files and generates a comprehensive difference report in three formats using the `difflib` module. **Function Signature:** ```python def compare_files(file1_path: str, file2_path: str) -> dict: Compares the contents of two files and returns a dictionary with differences in various formats. Parameters: file1_path (str): Path to the first file. file2_path (str): Path to the second file. Returns: dict: A dictionary containing \'ndiff\', \'context_diff\', and \'unified_diff\' string outputs. ``` # Instructions: 1. **Input:** - Two file paths as strings that specify the locations of the text files to be compared. 2. **Output:** - A dictionary with three keys: `ndiff`, `context_diff`, and `unified_diff`. - `ndiff`: The detailed diff including intraline differences. - `context_diff`: A compact diff highlighting changes with a few lines of context. - `unified_diff`: A unified diff highlighting changes inline. 3. **Constraints:** - Handle files with different line terminations (e.g., `n`, `rn`). - Assume the files provided exist and are readable. - Ensure the function is efficient and can handle large files. 4. **Example:** ```python # Given the following contents in the provided files # file1.txt # 1. Hello World # 2. This is a test. # file2.txt # 1. Hello World! # 3. This is a test for difflib. result = compare_files(\'file1.txt\', \'file2.txt\') print(result[\'ndiff\']) print(result[\'context_diff\']) print(result[\'unified_diff\']) ``` 5. **Hints:** - Utilize `difflib.SequenceMatcher` and `difflib.Differ` for a detailed diff. - Use `difflib.context_diff` and `difflib.unified_diff` for their respective diff formats. # Example Call: ```python result = compare_files(\'file1.txt\', \'file2.txt\') print(\\"Ndiff:\\") print(result[\'ndiff\']) print(\\"Context Diff:\\") print(result[\'context_diff\']) print(\\"Unified Diff:\\") print(result[\'unified_diff\']) ``` # Evaluation: The solution will be evaluated based on: - Correctness of the output format. - Ability to correctly identify and present differences in all three formats. - Handling of edge cases like empty files or files with no differences. - Code efficiency and readability.","solution":"import difflib def compare_files(file1_path: str, file2_path: str) -> dict: Compares the contents of two files and returns a dictionary with differences in various formats. Parameters: file1_path (str): Path to the first file. file2_path (str): Path to the second file. Returns: dict: A dictionary containing \'ndiff\', \'context_diff\', and \'unified_diff\' string outputs. with open(file1_path, \'r\') as file1, open(file2_path, \'r\') as file2: file1_lines = file1.readlines() file2_lines = file2.readlines() ndiff = \'n\'.join(list(difflib.ndiff(file1_lines, file2_lines))) context_diff = \'n\'.join(list(difflib.context_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path))) unified_diff = \'n\'.join(list(difflib.unified_diff(file1_lines, file2_lines, fromfile=file1_path, tofile=file2_path))) return { \'ndiff\': ndiff, \'context_diff\': context_diff, \'unified_diff\': unified_diff }"},{"question":"Advanced Temperature Conversion with Pattern Matching **Objective:** Implement a function to convert temperatures between Celsius, Fahrenheit, and Kelvin using Python 3.10\'s pattern matching functionality effectively. **Problem Statement:** You are required to implement a function `convert_temperature`. This function will accept a temperature reading and convert it to another specified temperature scale. The input will be in the form of a tuple that includes a temperature value and its scale. Use pattern matching to perform the conversions. **Function Signature:** ```python def convert_temperature(temperature: tuple, target_scale: str) -> str: pass ``` **Input:** - `temperature`: A tuple where the first element is a float or integer representing the temperature value and the second element is a string representing the current temperature scale (one of `\\"Celsius\\"`, `\\"Fahrenheit\\"`, `\\"Kelvin\\"`). - `target_scale`: A string representing the target temperature scale (`\\"Celsius\\"`, `\\"Fahrenheit\\"`, or `\\"Kelvin\\"`). **Output:** - A string indicating the converted temperature value and the target scale in the format `\\"X.Y TargetScale\\"`, where `X.Y` is the converted temperature value rounded to one decimal place. **Constraints:** - You can assume the input temperature values are within practical limits for thermodynamic quantities (e.g., nothing below absolute zero). - Only valid temperature scales (`\\"Celsius\\"`, `\\"Fahrenheit\\"`, and `\\"Kelvin\\"`) will be provided. - Only valid conversions will be requested (i.e., conversion from one of the three scales to another of the three). **Example Conversions:** For conversion formulas, use: - From Celsius to Fahrenheit: ( F = C times frac{9}{5} + 32 ) - From Celsius to Kelvin: ( K = C + 273.15 ) - From Fahrenheit to Celsius: ( C = (F - 32) times frac{5}{9} ) - From Fahrenheit to Kelvin: ( K = (F - 32) times frac{5}{9} + 273.15 ) - From Kelvin to Celsius: ( C = K - 273.15 ) - From Kelvin to Fahrenheit: ( F = (K - 273.15) times frac{9}{5} + 32 ) **Examples:** ```python assert convert_temperature((100, \\"Celsius\\"), \\"Fahrenheit\\") == \\"212.0 Fahrenheit\\" assert convert_temperature((0, \\"Kelvin\\"), \\"Celsius\\") == \\"-273.1 Celsius\\" assert convert_temperature((32, \\"Fahrenheit\\"), \\"Kelvin\\") == \\"273.1 Kelvin\\" ``` **Task:** Implement the `convert_temperature` function using Python 3.10\'s pattern matching.","solution":"def convert_temperature(temperature: tuple, target_scale: str) -> str: value, current_scale = temperature match (current_scale, target_scale): case (\\"Celsius\\", \\"Fahrenheit\\"): converted = value * 9 / 5 + 32 case (\\"Celsius\\", \\"Kelvin\\"): converted = value + 273.15 case (\\"Fahrenheit\\", \\"Celsius\\"): converted = (value - 32) * 5 / 9 case (\\"Fahrenheit\\", \\"Kelvin\\"): converted = (value - 32) * 5 / 9 + 273.15 case (\\"Kelvin\\", \\"Celsius\\"): converted = value - 273.15 case (\\"Kelvin\\", \\"Fahrenheit\\"): converted = (value - 273.15) * 9 / 5 + 32 case _: return \\"Invalid Conversion\\" return f\\"{converted:.1f} {target_scale}\\""},{"question":"**Title:** Log File Management using Temporary Files **Objective:** Implement a function `manage_logs(log_msgs)` that processes a list of log messages by writing them to a temporary file, reading back from this file to perform some processing, and then appropriately cleaning up the temporary file. **Problem Statement:** You are required to implement a function `manage_logs(log_msgs)` that takes a list of log messages (each as a string) and follows these steps: 1. Write each log message to a temporary file using `tempfile.NamedTemporaryFile`. 2. Read the log messages back from the temporary file. 3. Count the frequency of each unique log message. 4. Return a dictionary with log messages as keys and their counts as values. 5. Ensure that the temporary file is deleted after its usage, regardless of any errors that may occur during processing. **Function Signature:** ```python def manage_logs(log_msgs: list) -> dict: pass ``` **Input:** - `log_msgs`: A list of strings where each string represents a log message. **Output:** - Returns a dictionary where the keys are the unique log messages and the values are their counts. **Constraints:** - The list `log_msgs` can contain up to 1000 log messages. - Each log message is a non-empty string with a maximum length of 200 characters. **Example:** ```python log_msgs = [\\"Error: Disk is full\\", \\"Warning: CPU Overload\\", \\"Error: Disk is full\\", \\"Info: System rebooted\\"] log_count = manage_logs(log_msgs) print(log_count) # Output: {\'Error: Disk is full\': 2, \'Warning: CPU Overload\': 1, \'Info: System rebooted\': 1} ``` **Explanation:** The function processes the list of log messages by: 1. Writing the log messages to a temporary file. 2. Reading back the log messages from the file. 3. Counting the frequency of each unique log message. 4. Returning a dictionary with log messages as keys and their counts as values. 5. Ensuring the temporary file is properly cleaned up after usage. **Notes:** - You must use the `tempfile.NamedTemporaryFile` function to handle the creation and cleanup of the temporary file. - Handle any potential exceptions to ensure that the file is always deleted. **Hints:** - Use a context manager (`with` statement) for handling the temporary file to ensure it is deleted after usage. - You can use the `collections.Counter` class to simplify counting log message frequencies.","solution":"import tempfile from collections import Counter def manage_logs(log_msgs): Processes a list of log messages by writing them to a temporary file, reading back from this file, counting the frequency of each unique log message, and ensuring the temporary file is deleted after its usage. # Initialize a dictionary to hold the counts of log messages log_counts = {} # Use NamedTemporaryFile to manage the temporary file try: with tempfile.NamedTemporaryFile(mode=\'w+\', delete=True) as temp_file: # Write each log message to the temporary file for log_msg in log_msgs: temp_file.write(log_msg + \'n\') # Ensure file cursor is at the beginning for reading temp_file.seek(0) # Read the log messages back from the temporary file logs = temp_file.readlines() # Count the frequency of each unique log message log_counts = Counter(log.strip() for log in logs) except Exception as e: print(f\\"An error occurred: {e}\\") return dict(log_counts)"},{"question":"# Question: Dynamic Module Importer with `zipimport` **Objective**: Implement a function that dynamically imports a specified Python module from a zip archive and executes a function within the imported module. Function Signature: ```python def dynamic_import_and_exec(zip_path: str, module_name: str, function_name: str, *args, **kwargs) -> any: Imports a module from the specified zip archive and executes a function within that module. Parameters: zip_path (str): The path to the zip file containing the module. module_name (str): The fully qualified name of the module to import. function_name (str): The name of the function within the module to execute. *args: Positional arguments to pass to the function. **kwargs: Keyword arguments to pass to the function. Returns: any: The return value from the executed function. Raises: zipimport.ZipImportError: If the module or function cannot be found in the zip file. pass ``` Instructions: 1. **Load ZIP Archive**: Use the `zipimport.zipimporter` class to create an importer for the provided `zip_path`. 2. **Find and Import the Module**: Ensure that the module specified by `module_name` exists within the ZIP archive. If found, load the module. 3. **Execute the Function**: Locate the function specified by `function_name` within the imported module and execute it with the provided arguments. 4. **Error Handling**: Appropriately handle cases where the module or function cannot be found, raising a `zipimport.ZipImportError`. Constraints: - The provided ZIP archive will only contain `.py` or `.pyc` files for the purpose of this assessment. - The main module should be at the root level or within subdirectories of the ZIP archive. - Assume that the provided function names and arguments will be valid if the module is correctly imported. Example: Suppose you have a ZIP archive `example.zip` containing a module `mymodule.py` with a function `myfunction` that takes two arguments. ```python # Content of mymodule.py def myfunction(a, b): return a + b ``` Calling the function as follows should import `mymodule` and execute `myfunction`: ```python result = dynamic_import_and_exec(\'example.zip\', \'mymodule\', \'myfunction\', 5, 3) print(result) # Output: 8 ``` # Notes: - Thoroughly test your solution with different cases, including non-existent modules and functions. - Consider edge cases and ensure robust error handling for missing files and invalid module names.","solution":"import zipimport def dynamic_import_and_exec(zip_path: str, module_name: str, function_name: str, *args, **kwargs) -> any: Imports a module from the specified zip archive and executes a function within that module. Parameters: zip_path (str): The path to the zip file containing the module. module_name (str): The fully qualified name of the module to import. function_name (str): The name of the function within the module to execute. *args: Positional arguments to pass to the function. **kwargs: Keyword arguments to pass to the function. Returns: any: The return value from the executed function. Raises: zipimport.ZipImportError: If the module or function cannot be found in the zip file. try: importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) func = getattr(module, function_name) return func(*args, **kwargs) except (ImportError, AttributeError) as e: raise zipimport.ZipImportError(f\\"Could not import or find the function \'{function_name}\' in module \'{module_name}\' from \'{zip_path}\'\\") from e"},{"question":"**Paths and Directories: Advanced File Management Using `pathlib`** **Objective**: Implement a function to manage files and directories using the `pathlib` module. This function will perform several actions related to creating directories, listing files, reading content, and writing logs. **Function Signature**: ```python from pathlib import Path def manage_files_and_directories(base_path: str, directory_name: str, log_file_name: str, pattern: str) -> None: Perform various file and directory management tasks. Parameters: - base_path (str): The base directory path where operations will be performed. - directory_name (str): The name of the directory to be created inside base_path. - log_file_name (str): The name of the log file to write the output. - pattern (str): The pattern to list files and directories inside the newly created directory. Returns: None # Your code here ``` **Actions**: 1. **Create a directory**: Create a directory named `directory_name` inside `base_path`. 2. **List files and subdirectories**: List all files and folders matching `pattern` in the newly created directory and write this list into a log file named `log_file_name` inside the newly created directory. 3. **Log file properties**: Log the names and properties (name, is_dir, is_file, size) of all files and directories from step 2 into `log_file_name`. **Example**: Suppose the base directory is `/home/user/Documents`, the directory name is `MyProject`, the log file name is `log.txt`, and the pattern is `*.py`. 1. Create `/home/user/Documents/MyProject`. 2. List all `.py` files within `/home/user/Documents/MyProject` (including all subdirectories). 3. Write the properties of these files into `/home/user/Documents/MyProject/log.txt`. **Constraints**: - Handle exceptions where directories or files may not exist or already exist. - Assume all paths provided are valid and user has the required permissions. **Sample Usage**: ```python # Call with appropriate parameters manage_files_and_directories(\'/home/user/Documents\', \'MyProject\', \'log.txt\', \'*.py\') ``` **Expected Output in `log_file_name`**: ``` File: /home/user/Documents/MyProject/example.py - is_file: True - is_dir: False - size: 1024 Directory: /home/user/Documents/MyProject/test - is_file: False - is_dir: True - size: 4096 ``` Ensure your function correctly handles various cases and performs all the specified actions as described.","solution":"from pathlib import Path def manage_files_and_directories(base_path: str, directory_name: str, log_file_name: str, pattern: str) -> None: try: # Create the directory base = Path(base_path) target_dir = base / directory_name target_dir.mkdir(parents=True, exist_ok=True) # List matching files and directories matching_files = list(target_dir.rglob(pattern)) # Write the list into the log file with properties log_file_path = target_dir / log_file_name with log_file_path.open(\'w\') as log_file: for file in matching_files: log_file.write(f\\"File: {file}n\\") log_file.write(f\\"- is_file: {file.is_file()}n\\") log_file.write(f\\"- is_dir: {file.is_dir()}n\\") log_file.write(f\\"- size: {file.stat().st_size if file.is_file() else \'N/A\'}nn\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Coding Assessment Question:** # Objective You are tasked with implementing a simple version of the `2to3` tool, focusing on translating specific Python 2 code patterns into their Python 3 equivalents. # Problem Statement Write a Python function `transform_python2_to_python3(source_code: str) -> str` that takes a string containing Python 2 code as input and returns a string with the code transformed to Python 3. Your function should specifically handle the following transformations: 1. **print statements to print function**: - Convert Python 2 print statements to the Python 3 print function. - Example: `print \\"Hello\\"` should be converted to `print(\\"Hello\\")`. 2. **raw_input to input**: - Convert `raw_input()` to `input()`. - Example: `name = raw_input(\\"Enter your name: \\")` should be converted to `name = input(\\"Enter your name: \\")`. 3. **xrange to range**: - Convert `xrange()` to `range()`. - Ensure that any calls to `range()` are wrapped in `list()` to preserve list-like behavior. - Example: `for i in xrange(10):` should be converted to `for i in range(10):`. # Input - `source_code` (str): A string representing valid Python 2 source code. # Output - Return the transformed Python 3 code as a string. # Constraints - The input will always be syntactically correct Python 2 code. - Assume that all instances of `print`, `raw_input`, and `xrange` follow the standard pattern (i.e., they are not imported from other modules or redefined within the code). - Your solution should preserve the original formatting and comments as much as possible. # Performance Requirements - The function should handle input strings up to 10,000 characters efficiently. # Example ```python source_code = \'\'\' print \\"Hello, world!\\" name = raw_input(\\"Enter your name: \\") for i in xrange(5): print i \'\'\' output_code = transform_python2_to_python3(source_code) expected_output = \'\'\' print(\\"Hello, world!\\") name = input(\\"Enter your name: \\") for i in range(5): print(i) \'\'\' assert output_code == expected_output ``` # Additional Notes - Consider using regular expressions to find and replace patterns in the input string. - Make sure to test your function with a variety of input examples to ensure comprehensive coverage of these transformations.","solution":"import re def transform_python2_to_python3(source_code: str) -> str: Transforms Python 2 code into Python 3 code by handling specific patterns. Parameters: source_code (str): A string containing Python 2 source code. Returns: str: A string with the transformed Python 3 code. # 1. Convert print statements to print function source_code = re.sub(r\'prints+\\"(.*?)\\"\', r\'print(\\"1\\")\', source_code) source_code = re.sub(r\'prints+(.*?)\', r\'print(1)\', source_code, flags=re.MULTILINE) # 2. Convert raw_input() to input() source_code = re.sub(r\'raw_input\', \'input\', source_code) # 3. Convert xrange() to range() source_code = re.sub(r\'bxrangeb\', \'range\', source_code) return source_code"},{"question":"Objective: Demonstrate your understanding of the MPS backend in PyTorch by writing code to implement a simple neural network, move it to the MPS device, and perform operations with tensors on the MPS device. Problem Statement: 1. Ensure the MPS backend is available on the running system. 2. Create a simple fully connected neural network using PyTorch\'s `nn.Module`. 3. Move the neural network and input tensors to the MPS device (if available). 4. Perform a forward pass of the neural network with the input tensor and return the output. Implementation Details: 1. **Check MPS Availability:** - Implement a function `check_mps_device` that checks if the MPS device is available and returns a boolean indicating its availability. 2. **Create Neural Network:** - Implement a simple fully connected neural network with one hidden layer using PyTorch\'s `nn.Module`. 3. **Model Device Transfer:** - Implement a function `move_to_device` that moves the model to the MPS device if available; otherwise, it should remain on the CPU. 4. **Forward Pass on MPS:** - Implement a function `forward_pass` that takes a tensor as input, moves it to the MPS device if available, performs a forward pass using the neural network, and returns the output tensor. Constraints: - You must ensure that the code does not throw errors if the MPS device is not available. - You should assume that the required libraries, such as PyTorch, are already installed. Expected Input and Output: - The `check_mps_device` function should return a boolean value. - The `move_to_device` function should return the model on the correct device. - The `forward_pass` function should return the output tensor correctly. Sample Code Skeleton: ```python import torch import torch.nn as nn def check_mps_device(): # Your code to check MPS availability pass class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() # Define layers here pass def forward(self, x): # Define forward pass here pass def move_to_device(model): # Your code to move model to the correct device pass def forward_pass(model, input_tensor): # Your code to perform forward pass on the input_tensor pass # Example usage: if __name__ == \\"__main__\\": # Create Model model = SimpleNet() # Check for MPS device if check_mps_device(): print(\\"MPS device is available.\\") else: print(\\"MPS device is not available.\\") # Move model to the appropriate device model = move_to_device(model) # Create an input tensor input_tensor = torch.ones(1, 10) # Example input # Perform a forward pass and obtain output output = forward_pass(model, input_tensor) print(output) ```","solution":"import torch import torch.nn as nn def check_mps_device(): Check if the MPS device is available. return torch.backends.mps.is_available() class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def move_to_device(model): Move the model to the MPS device if available; otherwise, keep it on CPU. if check_mps_device(): return model.to(\'mps\') else: return model.to(\'cpu\') def forward_pass(model, input_tensor): Perform a forward pass using the model with the input tensor. The tensor and model are moved to the MPS device if available. if check_mps_device(): input_tensor = input_tensor.to(\'mps\') else: input_tensor = input_tensor.to(\'cpu\') model_output = model(input_tensor) return model_output"},{"question":"# Weak Reference Management in Python **Objective:** Implement a function that demonstrates the key operations involving weak references using the Python C API. **Task:** You need to write a Python C extension module that provides the following functionality: 1. A function `check_weakref_type(ob)` that returns a string indicating whether `ob` is a weak reference, a proxy object, or neither. 2. A function `create_weakref(ob, callback)` that returns a weak reference to `ob` with an optional `callback` that will be called when `ob` is garbage collected. 3. A function `create_weakref_proxy(ob, callback)` that returns a weak reference proxy to `ob` with an optional `callback`. 4. A function `get_referenced_object(ref)` that returns the object referenced by `ref`. 5. A function `clear_weakrefs(ob)` that clears all weak references to `ob` and calls their callbacks. Expected Input and Output 1. `check_weakref_type(ob)` - **Input**: `ob` (any object) - **Output**: a string - \\"reference\\", \\"proxy\\", or \\"none\\" 2. `create_weakref(ob, callback)` - **Input**: `ob` (any object), `callback` (callable or None) - **Output**: weak reference object 3. `create_weakref_proxy(ob, callback)` - **Input**: `ob` (any object), `callback` (callable or None) - **Output**: weak reference proxy object 4. `get_referenced_object(ref)` - **Input**: `ref` (weak reference or proxy object) - **Output**: referenced object or `None` 5. `clear_weakrefs(ob)` - **Input**: `ob` (any object) - **Output**: None **Constraints:** - Ensure the passed callback is valid (either callable or None). - Handle errors appropriately and return suitable error messages or raise exceptions as needed. - Manage reference counts correctly to avoid memory leaks. **Performance Requirements:** - The implementation should not cause significant slowdowns and should manage memory efficiently. **Additional Information:** - Utilize the Python C API as described in the documentation for managing weak references. - Include necessary error checking and exception handling. - Write clear and concise comments to explain your implementation. - Provide a test script in Python that demonstrates the use of each function, showing how weak references and proxies work along with their callbacks.","solution":"import weakref def check_weakref_type(ob): Returns a string indicating whether `ob` is a weak reference, a proxy object, or neither. if isinstance(ob, weakref.ref): return \\"reference\\" elif isinstance(ob, weakref.ProxyTypes): return \\"proxy\\" else: return \\"none\\" def create_weakref(ob, callback=None): Returns a weak reference to `ob` with an optional callback. if callback is not None and not callable(callback): raise ValueError(\\"callback must be callable or None\\") return weakref.ref(ob, callback) def create_weakref_proxy(ob, callback=None): Returns a weak reference proxy to `ob` with an optional callback. if callback is not None and not callable(callback): raise ValueError(\\"callback must be callable or None\\") proxy = weakref.proxy(ob) if callback: return weakref.ref(ob, callback), proxy return proxy def get_referenced_object(ref): Returns the object referenced by `ref`. if not isinstance(ref, (weakref.ref, weakref.ProxyTypes)): raise ValueError(\\"ref must be a weak reference or proxy object\\") return ref() def clear_weakrefs(ob): Clears all weak references to `ob` and calls their callbacks. refs = weakref.getweakrefs(ob) for ref in refs: ref() # This should invoke the callback # Example class to demonstrate weak references class MyClass: def __init__(self, name): self.name = name def __repr__(self): return f\\"MyClass({self.name})\\""},{"question":"**Objective:** Demonstrate your comprehension of Seaborn\'s color palette functionalities by creating visualizations that effectively use different types of color palettes. **Task:** 1. **Load the built-in \'penguins\' dataset from Seaborn.** 2. **Create three scatter plots**: - The first plot should use a **qualitative color palette** to differentiate between species. - The second plot should use a **sequential color palette** to represent the bill length. - The third plot should use a **diverging color palette** to represent deviation of flipper length from the mean flipper length. 3. **Configure each scatter plot**: - For the qualitative plot, use different hues to distinguish between species. - For the sequential plot, create a palette that reflects a gradient of bill lengths. - For the diverging plot, choose colors that highlight deviations from the mean flipper length. 4. **Plot Requirements**: - Each plot should have a title indicating the type of color palette used. - Appropriate legends and axis labels should be included. - Use `plt.tight_layout()` to ensure the plots are neatly arranged. **Input:** No input is required; use the Seaborn\'s built-in \'penguins\' dataset. **Output:** Three scatter plots saved as \'qualitative_palette.png\', \'sequential_palette.png\', and \'diverging_palette.png\'. **Constraints:** - Focus on correctly applying and configuring different color palettes as per the requirements. - Ensure the plots are clear and visually distinguishable based on the palette criteria. **Example Solution:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the \'penguins\' dataset penguins = sns.load_dataset(\\"penguins\\") # Qualitative color palette plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=\\"deep\\") plt.title(\\"Qualitative Palette: Species\\") plt.tight_layout() plt.savefig(\\"qualitative_palette.png\\") plt.close() # Sequential color palette plot plt.figure(figsize=(8, 6)) sequential_palette = sns.color_palette(\\"Blues\\", as_cmap=True) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"bill_length_mm\\", palette=sequential_palette, legend=False) plt.title(\\"Sequential Palette: Bill Length\\") plt.tight_layout() plt.savefig(\\"sequential_palette.png\\") plt.close() # Diverging color palette plot plt.figure(figsize=(8, 6)) mean_flipper_length = penguins[\\"flipper_length_mm\\"].mean() penguins[\'flipper_length_dev\'] = penguins[\\"flipper_length_mm\\"] - mean_flipper_length diverging_palette = sns.diverging_palette(220, 20, as_cmap=True) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"flipper_length_dev\\", palette=diverging_palette, legend=False) plt.title(\\"Diverging Palette: Flipper Length Deviation\\") plt.tight_layout() plt.savefig(\\"diverging_palette.png\\") plt.close() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the \'penguins\' dataset penguins = sns.load_dataset(\\"penguins\\") # Qualitative color palette plot plt.figure(figsize=(8, 6)) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=\\"deep\\") plt.title(\\"Qualitative Palette: Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.legend(title=\'Species\') plt.tight_layout() plt.savefig(\\"qualitative_palette.png\\") plt.close() # Sequential color palette plot plt.figure(figsize=(8, 6)) sequential_palette = sns.color_palette(\\"Blues\\", as_cmap=True) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"bill_length_mm\\", palette=sequential_palette, legend=False) plt.title(\\"Sequential Palette: Bill Length\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.tight_layout() plt.savefig(\\"sequential_palette.png\\") plt.close() # Diverging color palette plot plt.figure(figsize=(8, 6)) mean_flipper_length = penguins[\\"flipper_length_mm\\"].mean() penguins[\'flipper_length_dev\'] = penguins[\\"flipper_length_mm\\"] - mean_flipper_length diverging_palette = sns.diverging_palette(220, 20, as_cmap=True) sns.scatterplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"flipper_length_dev\\", palette=diverging_palette, legend=False) plt.title(\\"Diverging Palette: Flipper Length Deviation\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.tight_layout() plt.savefig(\\"diverging_palette.png\\") plt.close()"},{"question":"**Student Question:** # Tensor Operations Testing with PyTorch In this assessment, you are required to write a function that performs a series of tensor operations and subsequently validates the correctness of these operations using PyTorch\'s testing utilities. Task 1. Create a function `tensor_operations(n: int) -> torch.Tensor` that: - Generates a 1-dimensional tensor `a` of size `n` with values drawn from a normal distribution (mean=0, std=1). - Generates a 1-dimensional tensor `b` of size `n` with values drawn from a uniform distribution over the interval [0, 1). - Computes a new tensor `c` defined as `c = a**2 + b`. 2. Create a function `validate_operations(n: int) -> None` that: - Calls the `tensor_operations` function to obtain the tensor `c`. - Independently computes a reference tensor `c_expected` using the same operations as `tensor_operations`. - Uses `torch.testing.assert_allclose` to assert that `c` and `c_expected` are approximately equal. Input - An integer `n` where `n > 0`. Output - The function `tensor_operations(n: int) -> torch.Tensor` should return a tensor of size `n`. - The function `validate_operations(n: int) -> None` should raise an assertion error if the tensors are not approximately equal, otherwise, it should complete silently. Example Code Below is a template to help you get started: ```python import torch def tensor_operations(n: int) -> torch.Tensor: if n <= 0: raise ValueError(\\"The size of the tensor must be a positive integer.\\") # Generate tensors `a` and `b` # Compute tensor `c` def validate_operations(n: int) -> None: # Generate tensor `c` using `tensor_operations` # Independently compute `c_expected` # Use torch.testing.assert_allclose to compare `c` and `c_expected` ``` Constraints - You should use `torch.manual_seed(0)` at the beginning of each function to ensure reproducibility of the random tensors. Good luck and happy coding!","solution":"import torch def tensor_operations(n: int) -> torch.Tensor: if n <= 0: raise ValueError(\\"The size of the tensor must be a positive integer.\\") torch.manual_seed(0) a = torch.randn(n) b = torch.rand(n) c = a**2 + b return c def validate_operations(n: int) -> None: if n <= 0: raise ValueError(\\"The size of the tensor must be a positive integer.\\") torch.manual_seed(0) a_expected = torch.randn(n) b_expected = torch.rand(n) c_expected = a_expected**2 + b_expected c = tensor_operations(n) torch.testing.assert_allclose(c, c_expected)"},{"question":"**Question: Platform Information Aggregator** # Background You are assigned to develop a diagnostic tool that aggregates essential platform information for a variety of deployment environments. Utilizing the `platform` module, you are to construct a detailed report that provides insight into the host system\'s configuration. # Task Write a Python function `generate_platform_report()` that returns a dictionary containing comprehensive platform details. The dictionary should include the following keys, with associated values fetched using the corresponding `platform` functions: - `architecture`: Tuple of (bit architecture, linkage format) - `machine`: Machine type - `node`: Network name - `platform`: A human-readable string identifying the platform - `processor`: Processor name - `python_build`: Tuple of (build number, build date) - `python_compiler`: Compiler used for Python - `python_implementation`: Python implementation - `python_version`: Python version string - `python_version_tuple`: Python version tuple - `release`: System release - `system`: System/OS name - `version`: System version - `uname`: Named tuple with detailed system information # Requirements - Your function should handle cases where values cannot be determined by setting the dictionary values to `None`. - The function must include appropriate error handling to ensure robustness against possible exceptions while accessing platform information. - The resulting dictionary must adhere to the specified keys and structure. # Example Output ```python { \\"architecture\\": (\\"64bit\\", \\"ELF\\"), \\"machine\\": \\"x86_64\\", \\"node\\": \\"hostname\\", \\"platform\\": \\"Linux-5.4.0-66-generic-x86_64-with-Ubuntu-20.04-focal\\", \\"processor\\": \\"x86_64\\", \\"python_build\\": (\\"v3.10.0\\", \\"Oct 4 2021 10:30:00\\"), \\"python_compiler\\": \\"GCC 9.3.0\\", \\"python_implementation\\": \\"CPython\\", \\"python_version\\": \\"3.10.0\\", \\"python_version_tuple\\": (\\"3\\", \\"10\\", \\"0\\"), \\"release\\": \\"5.4.0-66-generic\\", \\"system\\": \\"Linux\\", \\"version\\": \\"#74-Ubuntu SMP Mon Jan 18 17:30:23 UTC 2021\\", \\"uname\\": uname_result(system=\'Linux\', node=\'hostname\', release=\'5.4.0-66-generic\', version=\'#74-Ubuntu SMP Mon Jan 18 17:30:23 UTC 2021\', machine=\'x86_64\', processor=\'x86_64\') } ``` # Constraints - Ensure you use appropriate methods from the `platform` module to retrieve information. - Implement the function using Python 3.10+. ```python import platform from typing import Dict, Any def generate_platform_report() -> Dict[str, Any]: report = { \\"architecture\\": max_safe(platform.architecture), \\"machine\\": max_safe(platform.machine), \\"node\\": max_safe(platform.node), \\"platform\\": max_safe(platform.platform), \\"processor\\": max_safe(platform.processor), \\"python_build\\": max_safe(platform.python_build), \\"python_compiler\\": max_safe(platform.python_compiler), \\"python_implementation\\": max_safe(platform.python_implementation), \\"python_version\\": max_safe(platform.python_version), \\"python_version_tuple\\": max_safe(platform.python_version_tuple), \\"release\\": max_safe(platform.release), \\"system\\": max_safe(platform.system), \\"version\\": max_safe(platform.version), \\"uname\\": max_safe(platform.uname) } return report def max_safe(fn, *args): try: return fn(*args) if args else fn() except Exception: return None ``` # Notes - The helper function **max_safe** ensures that if any `platform` function raises an exception, it returns `None` for that entry in the dictionary. Develop the `generate_platform_report()` function to achieve the above-specifications and ensure all platform-related information is correctly and efficiently captured.","solution":"import platform from typing import Dict, Any def generate_platform_report() -> Dict[str, Any]: def safe_platform_call(func, *args): try: return func(*args) if args else func() except Exception: return None report = { \\"architecture\\": safe_platform_call(platform.architecture), \\"machine\\": safe_platform_call(platform.machine), \\"node\\": safe_platform_call(platform.node), \\"platform\\": safe_platform_call(platform.platform), \\"processor\\": safe_platform_call(platform.processor), \\"python_build\\": safe_platform_call(platform.python_build), \\"python_compiler\\": safe_platform_call(platform.python_compiler), \\"python_implementation\\": safe_platform_call(platform.python_implementation), \\"python_version\\": safe_platform_call(platform.python_version), \\"python_version_tuple\\": safe_platform_call(platform.python_version_tuple), \\"release\\": safe_platform_call(platform.release), \\"system\\": safe_platform_call(platform.system), \\"version\\": safe_platform_call(platform.version), \\"uname\\": safe_platform_call(platform.uname) } return report"},{"question":"**Title: Dynamic Attribute Manipulation and Inspection** **Objective:** Demonstrate an understanding of dynamic attribute manipulation and inspection by implementing Python functions that simulate some operations described in the documentation above. **Problem Statement:** Write Python functions to perform operations on Python objects\' attributes dynamically. You need to implement the following functions: 1. `has_attribute(obj, attr_name)`: - **Input:** - `obj` (an instance of any Python class) - `attr_name` (str) - **Output:** Boolean - **Description:** Returns `True` if `obj` has an attribute named `attr_name`, otherwise `False`. 2. `get_attribute(obj, attr_name)`: - **Input:** - `obj` (an instance of any Python class) - `attr_name` (str) - **Output:** Value of the attribute, or `None` if the attribute does not exist. - **Constraints:** Do not use the built-in `getattr()` function directly. 3. `set_attribute(obj, attr_name, value)`: - **Input:** - `obj` (an instance of any Python class) - `attr_name` (str) - `value` (any) - **Output:** None - **Description:** Sets the attribute `attr_name` of `obj` to the given `value`. If `attr_name` does not exist, it creates the attribute. 4. `del_attribute(obj, attr_name)`: - **Input:** - `obj` (an instance of any Python class) - `attr_name` (str) - **Output:** Boolean - **Description:** Deletes the attribute `attr_name` from `obj`. Returns `True` if the attribute existed and was deleted, otherwise `False`. 5. `list_attributes(obj)`: - **Input:** - `obj` (an instance of any Python class) - **Output:** List of attribute names (str) - **Description:** Returns a list of attribute names of `obj`. **Performance Requirements:** - These functions should handle typical cases efficiently. You do not need to optimize for exceptionally large numbers of attributes or very deep attribute hierarchies. **Constraints:** - Do not use Python\'s built-in functions like `hasattr()`, `getattr()`, `setattr()`, or `delattr()` directly within your implementations. ```python # Function Implementations: def has_attribute(obj, attr_name): # Your implementation here pass def get_attribute(obj, attr_name): # Your implementation here pass def set_attribute(obj, attr_name, value): # Your implementation here pass def del_attribute(obj, attr_name): # Your implementation here pass def list_attributes(obj): # Your implementation here pass # Example Usages: class ExampleClass: def __init__(self): self.name = \\"Example\\" self.value = 42 example = ExampleClass() assert has_attribute(example, \\"name\\") == True assert has_attribute(example, \\"missing\\") == False assert get_attribute(example, \\"value\\") == 42 assert get_attribute(example, \\"missing\\") == None set_attribute(example, \\"value\\", 100) assert example.value == 100 assert del_attribute(example, \\"name\\") == True assert has_attribute(example, \\"name\\") == False assert sorted(list_attributes(example)) == [\'value\'] ``` **Notes:** - Be aware of the differences in default behaviors when setting or getting non-existent attributes. - Ensure your implementation doesn\'t raise unexpected exceptions and handles corner cases gracefully. **End of Question**","solution":"def has_attribute(obj, attr_name): Returns True if obj has an attribute named attr_name, otherwise False. return attr_name in obj.__dict__ def get_attribute(obj, attr_name): Returns the value of the attribute named attr_name of obj. If attr_name does not exist, returns None. return obj.__dict__.get(attr_name, None) def set_attribute(obj, attr_name, value): Sets the value of the attribute named attr_name of obj to value. If attr_name does not exist, it is created. obj.__dict__[attr_name] = value def del_attribute(obj, attr_name): Deletes the attribute named attr_name from obj. Returns True if the attribute existed and was deleted, otherwise False. if attr_name in obj.__dict__: del obj.__dict__[attr_name] return True return False def list_attributes(obj): Returns a list of attribute names of obj. return list(obj.__dict__.keys())"},{"question":"You are given a dataset containing multivariate data points. Your task is to implement a Python function using scikit-learn\'s `sklearn.covariance` module to perform the following steps: 1. Estimate the empirical covariance of the dataset. 2. Apply Ledoit-Wolf shrinkage to the empirical covariance matrix to improve its stability. 3. Identify outliers using the Minimum Covariance Determinant (MCD) estimator and compute the Mahalanobis distance for each data point. # Function Signature ```python def covariance_analysis(data: np.ndarray, assume_centered: bool = False) -> dict: Perform covariance analysis on the given dataset using various methods from sklearn.covariance. Parameters: data (np.ndarray): A 2D numpy array where each row represents a data point and each column represents a feature. assume_centered (bool): If True, data will be assumed to be centered. If False, data will be automatically centered. Default is False. Returns: dict: A dictionary containing the following keys and their corresponding values: - \'empirical_covariance\': The empirical covariance matrix. - \'ledoit_wolf_covariance\': The covariance matrix obtained after applying Ledoit-Wolf shrinkage. - \'mcd_covariance\': The robust covariance matrix estimated using MCD. - \'mahalanobis_distances\': A list of Mahalanobis distances of each data point from the MCD estimated center. - \'outliers\': Indices of the identified outliers based on their Mahalanobis distances. ``` # Input - `data`: A 2D numpy array (n_samples, n_features), representing the dataset where each row is a data point and each column is a feature. - `assume_centered`: A boolean value. If `True`, the data is assumed to be centered, and if `False`, the data will be centered during the computation. # Output - A dictionary containing: - `empirical_covariance`: The empirically estimated covariance matrix. - `ledoit_wolf_covariance`: The covariance matrix obtained after applying Ledoit-Wolf shrinkage. - `mcd_covariance`: The robust covariance matrix estimated using MCD. - `mahalanobis_distances`: A list of Mahalanobis distances of each data point from the MCD computed center. - `outliers`: Indices of the identified outliers based on their Mahalanobis distances. # Constraints - The input dataset may have up to 10,000 samples, and each sample may have up to 100 features. - The function should handle missing values appropriately (i.e., ignore them during covariance estimation). # Example ```python import numpy as np data = np.random.rand(100, 5) result = covariance_analysis(data) print(\\"Empirical Covariance:n\\", result[\'empirical_covariance\']) print(\\"Ledoit-Wolf Covariance:n\\", result[\'ledoit_wolf_covariance\']) print(\\"MCD Covariance:n\\", result[\'mcd_covariance\']) print(\\"Mahalanobis Distances:n\\", result[\'mahalanobis_distances\']) print(\\"Outliers:n\\", result[\'outliers\']) ``` # Notes - You are expected to use the `EmpiricalCovariance`, `LedoitWolf`, and `MinCovDet` classes from the `sklearn.covariance` module. - Ensure that your function handles different scenarios, such as when the input data has missing values or contains outliers. - Optimize for readability and maintainability of the code.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, LedoitWolf, MinCovDet def covariance_analysis(data: np.ndarray, assume_centered: bool = False) -> dict: Perform covariance analysis on the given dataset using various methods from sklearn.covariance. Parameters: data (np.ndarray): A 2D numpy array where each row represents a data point and each column represents a feature. assume_centered (bool): If True, data will be assumed to be centered. If False, data will be automatically centered. Default is False. Returns: dict: A dictionary containing the following keys and their corresponding values: - \'empirical_covariance\': The empirical covariance matrix. - \'ledoit_wolf_covariance\': The covariance matrix obtained after applying Ledoit-Wolf shrinkage. - \'mcd_covariance\': The robust covariance matrix estimated using MCD. - \'mahalanobis_distances\': A list of Mahalanobis distances of each data point from the MCD estimated center. - \'outliers\': Indices of the identified outliers based on their Mahalanobis distances. # Ensure the data is a 2D numpy array assert type(data) == np.ndarray and data.ndim == 2, \\"Input data should be a 2D numpy array\\" # Empirical covariance emp_cov = EmpiricalCovariance(assume_centered=assume_centered).fit(data) empirical_cov_matrix = emp_cov.covariance_ # Ledoit-Wolf shrinkage covariance lw_cov = LedoitWolf(assume_centered=assume_centered).fit(data) ledoit_wolf_cov_matrix = lw_cov.covariance_ # MCD estimator for robust covariance mcd = MinCovDet(assume_centered=assume_centered).fit(data) mcd_cov_matrix = mcd.covariance_ mahalanobis_distances = mcd.mahalanobis(data) # Determine outliers based on Mahalanobis distances outliers = np.where(mahalanobis_distances > np.percentile(mahalanobis_distances, 97.5))[0] return { \'empirical_covariance\': empirical_cov_matrix, \'ledoit_wolf_covariance\': ledoit_wolf_cov_matrix, \'mcd_covariance\': mcd_cov_matrix, \'mahalanobis_distances\': mahalanobis_distances.tolist(), \'outliers\': outliers.tolist() }"},{"question":"**Objective**: Demonstrate comprehension of Seaborn\'s plotting capabilities, including histogram creation, faceting, and legend manipulation. --- # Problem Statement You are provided with a dataset containing information about penguin species, their bill length, and their island of origin. Your tasks are to: 1. Create a histogram of the `bill_length_mm` of the penguins, categorized by their species. 2. Customize the legend position to be outside of the plot, at the bottom center, without a frame. 3. Create a faceted plot of the `bill_length_mm` for different `island` values, making separate plots for each island. 4. Customize the legends of the facets to appear in the upper right and ensure that they are within each facet. # Dataset Use the penguin dataset from Seaborn, which can be loaded using: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` # Function Signature ```python def visualize_penguin_data(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a histogram of bill lengths by species ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") # Task 2: Move the legend outside of the plot sns.move_legend(ax, \\"lower center\\", bbox_to_anchor=(0.5, -0.1), frameon=False) # Display the plot plt.show() # Task 3: Create a faceted plot of bill lengths for different islands g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3) # Task 4: Customize the legends of the facets sns.move_legend(g, \\"upper right\\", bbox_to_anchor=(.9, .9), frameon=False) # Display the plot plt.show() ``` # Constraints - Ensure that all legends are clearly visible and do not overlap with the data points or other plot elements. - Use standard Seaborn and matplotlib functionalities to achieve the required visualizations. # Expected Output The output of running your function should be two plots: 1. A single histogram with the legend outside and at the bottom center. 2. A faceted histogram plot segmented by `island`, each with an internal legend at the upper right. ---","solution":"def visualize_penguin_data(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Task 1: Create a histogram of bill lengths by species plt.figure() ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") # Task 2: Move the legend outside of the plot ax.legend(loc=\'lower center\', bbox_to_anchor=(0.5, -0.3), frameon=False) # Display the plot plt.show() # Task 3: Create a faceted plot of bill lengths for different islands g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3) # Task 4: Customize the legends of the facets for ax in g.axes.flatten(): ax.legend(loc=\'upper right\', frameon=False) # Display the plot plt.show()"},{"question":"**Objective:** Design and implement a Python function that demonstrates the use of the pydoc module to generate documentation for a given Python module and serve it over a local web server. **Problem Statement:** You are required to write a function called `serve_module_docs` that takes three arguments: - `module_name`: A string representing the name of the module for which to generate documentation. - `port`: An integer representing the port number to serve the documentation on. - `hostname` (optional): A string representing the hostname to listen at. If not provided, default to `\'localhost\'`. The function should: 1. Use the pydoc module to start an HTTP server that serves documentation for the specified module. 2. Ensure the server listens at the specified hostname and port. 3. If the `hostname` argument is not provided, use the default hostname `\'localhost\'`. **Input:** - `module_name` (str): The name of the module. - `port` (int): The port number. - `hostname` (str, optional): The hostname. **Output:** - The function should start an HTTP server that serves the documentation for the specified module. **Constraints:** - Assume that the module specified by `module_name` is available in the environment where the function is run. - The port number must be a valid integer between 1 and 65535. - The function should handle possible errors gracefully, such as an invalid module name or an unavailable port. **Example:** ```python def serve_module_docs(module_name: str, port: int, hostname: str = \'localhost\'): # Your implementation here pass # Example Usage: serve_module_docs(\'sys\', 1234) # This should start an HTTP server at `http://localhost:1234` serving documentation for the `sys` module. ``` **Notes:** - You may refer to the pydoc module documentation to understand the relevant functions or methods you need to use. - Make sure the function follows good coding practices and includes comments where necessary. - Performance considerations are minimal for this task, but proper error handling is crucial.","solution":"import pydoc from http.server import HTTPServer def serve_module_docs(module_name: str, port: int, hostname: str = \'localhost\'): Starts an HTTP server to serve the documentation for the specified Python module. Parameters: - module_name (str): The name of the module for which to generate documentation. - port (int): The port number to serve the documentation on. - hostname (str, optional): The hostname to listen at. Defaults to \'localhost\'. try: # Ensure the module exists by importing it pydoc.importfile(module_name) # Start the pydoc server pydoc.serve(port, hostname) except ImportError: print(f\\"Error: {module_name} module cannot be found.\\") except OSError as e: print(f\\"Error: Cannot start the server on {hostname}:{port}. {e}\\") # Example usage: # serve_module_docs(\'sys\', 1234) # This should start an HTTP server at `http://localhost:1234` serving documentation for the `sys` module."},{"question":"Objective Implement and compare the performance of semi-supervised learning models, specifically `SelfTrainingClassifier` and `LabelSpreading`, on a partially labeled dataset. Problem Statement You are provided with a dataset containing both labeled and unlabeled samples. Your task is to implement semi-supervised learning using `SelfTrainingClassifier` and `LabelSpreading` from the `sklearn.semi_supervised` module and compare their performance. Dataset You will use the digits dataset from `sklearn.datasets`. For this dataset: - **Labeled Samples:** Use the first 10 labeled samples from each class. - **Unlabeled Samples:** Use the remaining samples without labels (set their labels to `-1`). Instructions 1. Load the digits dataset. 2. Split the dataset into labeled and unlabeled samples as described above. 3. Implement semi-supervised learning using: - `SelfTrainingClassifier` with a Gaussian Naive Bayes (`sklearn.naive_bayes.GaussianNB`) as the base estimator. - `LabelSpreading` with the RBF kernel. 4. Train both models on the same split of the dataset. 5. Evaluate the performance of both models using accuracy and F1-score on the test set (consider the labeled samples as the test set). Requirements - **Input:** None, the dataset should be loaded within the function. - **Output:** Print the accuracy and F1-score for both models. - **Constraints:** - Train the models with a maximum of 10 iterations for `SelfTrainingClassifier`. - Use `gamma=0.25` for the RBF kernel in `LabelSpreading`. Function Signature ```python def compare_semi_supervised_learning(): # Your implementation here ``` Example Output ``` SelfTrainingClassifier - Accuracy: 0.85, F1-score: 0.85 LabelSpreading - Accuracy: 0.88, F1-score: 0.88 ```","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, f1_score from sklearn.semi_supervised import SelfTrainingClassifier, LabelSpreading from sklearn.naive_bayes import GaussianNB import numpy as np def compare_semi_supervised_learning(): # Load the dataset digits = load_digits() X, y = digits.data, digits.target # Create labeled and unlabeled indices labeled_indices = [] unlabeled_indices = [] for i in range(10): labeled_indices.extend(np.where(y == i)[0][:10]) unlabeled_indices.extend(np.where(y == i)[0][10:]) labeled_indices = np.array(labeled_indices) unlabeled_indices = np.array(unlabeled_indices) # Setup the labeled and unlabeled samples y_train = np.full_like(y, -1) y_train[labeled_indices] = y[labeled_indices] # Create the models base_classifier = GaussianNB() self_training_model = SelfTrainingClassifier(base_classifier, max_iter=10) label_spreading_model = LabelSpreading(kernel=\'rbf\', gamma=0.25, max_iter=10) # Fit the models self_training_model.fit(X, y_train) label_spreading_model.fit(X, y_train) # Evaluate the models using accuracy and F1 score on labeled samples y_true = y[labeled_indices] y_pred_self_training = self_training_model.predict(X[labeled_indices]) y_pred_label_spreading = label_spreading_model.predict(X[labeled_indices]) accuracy_self_training = accuracy_score(y_true, y_pred_self_training) f1_self_training = f1_score(y_true, y_pred_self_training, average=\'weighted\') accuracy_label_spreading = accuracy_score(y_true, y_pred_label_spreading) f1_label_spreading = f1_score(y_true, y_pred_label_spreading, average=\'weighted\') print(f\\"SelfTrainingClassifier - Accuracy: {accuracy_self_training:.2f}, F1-score: {f1_self_training:.2f}\\") print(f\\"LabelSpreading - Accuracy: {accuracy_label_spreading:.2f}, F1-score: {f1_label_spreading:.2f}\\")"},{"question":"You are tasked with creating a Python utility that leverages the `py_compile` module to compile multiple source files into byte-code files. The utility should handle several customization options and report on the compilation outcomes. Specifically, your utility should have the following features: 1. **Functionality**: Implement a function `compile_sources(file_paths: List[str], optimize: int = -1, quiet: int = 1) -> dict` that takes a list of file paths to Python source files, an optional optimization level, and an optional quiet level. The function should compile each file to byte-code and return a dictionary with the following structure: ```python { \'successful\': [list of successfully compiled file paths], \'failed\': [list of file paths that failed to compile along with error messages] } ``` 2. **Input**: - `file_paths`: a list of strings, where each string is a path to a Python source file. - `optimize`: an integer controlling the optimization level (default is `-1`, which uses the interpreter’s current optimization setting). - `quiet`: an integer controlling the verbosity (0, 1, or 2). 3. **Output**: - A dictionary summarizing the compilation outcomes. 4. **Constraints**: - The function should handle the compilation process error-free, even if some files fail to compile. - Use the appropriate `py_compile` functions and handle exceptions to ensure robustness. 5. **Performance**: The function should efficiently compile multiple files, ensuring minimal overhead beyond the compilation process itself. Write the implementation of the `compile_sources` function according to the above specifications. # Example Usage ```python file_paths = [ \\"script1.py\\", \\"script2.py\\", \\"bad_script.py\\" # This script contains syntax errors ] result = compile_sources(file_paths, optimize=2, quiet=1) # Aim for optimization level and moderate verbosity print(result) # Example output # { # \'successful\': [\'script1.py\', \'script2.py\'], # \'failed\': [\'bad_script.py\'] # } ``` Ensure to handle all edge cases appropriately, such as file paths that do not exist or files that are not valid Python scripts.","solution":"import py_compile from typing import List def compile_sources(file_paths: List[str], optimize: int = -1, quiet: int = 1) -> dict: result = { \'successful\': [], \'failed\': [] } for file_path in file_paths: try: py_compile.compile(file_path, cfile=None, dfile=None, doraise=True, optimize=optimize, quiet=quiet) result[\'successful\'].append(file_path) except py_compile.PyCompileError as e: result[\'failed\'].append({\\"file\\": file_path, \\"error\\": str(e)}) except Exception as e: result[\'failed\'].append({\\"file\\": file_path, \\"error\\": f\\"Unexpected error: {str(e)}\\"}) return result"},{"question":"# Comprehensive Data Compression and Archiving Task Background: You are working on an application that handles large amounts of data. To optimize storage and transmission, you need to compress data using different algorithms and archive the compressed files. You will implement a function that compresses given data using the specified algorithm, archives the compressed data into a chosen format, and then decompresses and extracts the files to verify the correctness. Task: Write a Python function `compress_and_archive(data: bytes, compress_algo: str, archive_format: str) -> bytes` that: 1. Compresses the input data using the specified compression algorithm (`compress_algo`), which can be `zlib`, `gzip`, `bz2`, or `lzma`. 2. Archives the compressed data into the specified format (`archive_format`), which can be `zip` or `tar`. 3. Decompresses and extracts the archived data. 4. Returns the decompressed data as bytes to verify its correctness. Requirements: 1. **Input:** - `data` (bytes): The data to be compressed. - `compress_algo` (str): The compression algorithm, one of: `zlib`, `gzip`, `bz2`, or `lzma`. - `archive_format` (str): The archive format, one of: `zip` or `tar`. 2. **Output:** - The function should return the decompressed data as bytes. 3. **Constraints:** - Use in-memory bytes objects for managing the data streams (i.e., no actual files should be written to or read from disk). - Ensure correctness by verifying that the decompressed data matches the original input data. 4. **Performance Consideration:** - The implementation should handle large input data efficiently. Example: ```python import zlib import gzip import bz2 import lzma import tarfile import zipfile import io def compress_and_archive(data: bytes, compress_algo: str, archive_format: str) -> bytes: # 1. Compress the data if compress_algo == \'zlib\': compressed_data = zlib.compress(data) elif compress_algo == \'gzip\': compressed_data = gzip.compress(data) elif compress_algo == \'bz2\': compressed_data = bz2.compress(data) elif compress_algo == \'lzma\': compressed_data = lzma.compress(data) else: raise ValueError(\\"Unsupported compression algorithm\\") # 2. Archive the compressed data archive_io = io.BytesIO() if archive_format == \'zip\': with zipfile.ZipFile(archive_io, mode=\'w\') as zf: zf.writestr(\'compressed_data\', compressed_data) elif archive_format == \'tar\': with tarfile.open(fileobj=archive_io, mode=\'w\') as tf: tarinfo = tarfile.TarInfo(name=\'compressed_data\') tarinfo.size = len(compressed_data) tf.addfile(tarinfo, io.BytesIO(compressed_data)) else: raise ValueError(\\"Unsupported archive format\\") # 3. Decompress and extract the archived data archive_io.seek(0) if archive_format == \'zip\': with zipfile.ZipFile(archive_io, mode=\'r\') as zf: with zf.open(\'compressed_data\') as f: compressed_data_extracted = f.read() elif archive_format == \'tar\': with tarfile.open(fileobj=archive_io, mode=\'r\') as tf: tarinfo = tf.getmember(\'compressed_data\') compressed_data_extracted = tf.extractfile(tarinfo).read() # Decompress the data if compress_algo == \'zlib\': decompressed_data = zlib.decompress(compressed_data_extracted) elif compress_algo == \'gzip\': decompressed_data = gzip.decompress(compressed_data_extracted) elif compress_algo == \'bz2\': decompressed_data = bz2.decompress(compressed_data_extracted) elif compress_algo == \'lzma\': decompressed_data = lzma.decompress(compressed_data_extracted) # Return the decompressed data return decompressed_data # Example usage: original_data = b\\"Example data for testing.\\" assert compress_and_archive(original_data, \'gzip\', \'zip\') == original_data ```","solution":"import zlib import gzip import bz2 import lzma import tarfile import zipfile import io def compress_and_archive(data: bytes, compress_algo: str, archive_format: str) -> bytes: # 1. Compress the data if compress_algo == \'zlib\': compressed_data = zlib.compress(data) elif compress_algo == \'gzip\': compressed_data = gzip.compress(data) elif compress_algo == \'bz2\': compressed_data = bz2.compress(data) elif compress_algo == \'lzma\': compressed_data = lzma.compress(data) else: raise ValueError(\\"Unsupported compression algorithm\\") # 2. Archive the compressed data archive_io = io.BytesIO() if archive_format == \'zip\': with zipfile.ZipFile(archive_io, mode=\'w\') as zf: zf.writestr(\'compressed_data\', compressed_data) elif archive_format == \'tar\': with tarfile.open(fileobj=archive_io, mode=\'w\') as tf: tarinfo = tarfile.TarInfo(name=\'compressed_data\') tarinfo.size = len(compressed_data) tf.addfile(tarinfo, io.BytesIO(compressed_data)) else: raise ValueError(\\"Unsupported archive format\\") # 3. Decompress and extract the archived data archive_io.seek(0) if archive_format == \'zip\': with zipfile.ZipFile(archive_io, mode=\'r\') as zf: with zf.open(\'compressed_data\') as f: compressed_data_extracted = f.read() elif archive_format == \'tar\': with tarfile.open(fileobj=archive_io, mode=\'r\') as tf: tarinfo = tf.getmember(\'compressed_data\') compressed_data_extracted = tf.extractfile(tarinfo).read() # Decompress the extracted data if compress_algo == \'zlib\': decompressed_data = zlib.decompress(compressed_data_extracted) elif compress_algo == \'gzip\': decompressed_data = gzip.decompress(compressed_data_extracted) elif compress_algo == \'bz2\': decompressed_data = bz2.decompress(compressed_data_extracted) elif compress_algo == \'lzma\': decompressed_data = lzma.decompress(compressed_data_extracted) # Return the decompressed data return decompressed_data"},{"question":"# Coding Assessment: PyTorch with HIP - Device and Memory Management Objective Your task is to write a PyTorch script that demonstrates the usage of HIP for device management and memory monitoring. You will need to perform several operations on tensors, manage device contexts, and monitor memory usage. Problem Statement Write a program in Python that performs the following tasks: 1. **Check GPU Availability**: Ensure that GPU support is available and determine if the system uses HIP or CUDA. 2. **Device Allocation and Tensor Operations**: - Create two tensors of size (1000, 1000) filled with random values on `cuda:0`. - Transfer one tensor to `cuda:1`. - Perform a matrix multiplication between the tensors and store the result on `cuda:1`. 3. **Memory Management**: - Print the amount of memory allocated and reserved on `cuda:0` and `cuda:1`. - Clear the unused cached memory. - Print the memory stats after clearing the cache. Function Specifications - **Input**: No input is required. The data and configuration should be handled within the script. - **Output**: The script should output the following information: - Whether HIP or CUDA is being used. - Memory allocation and reservation stats before and after clearing the cache for both `cuda:0` and `cuda:1`. Constraints - Utilize PyTorch with HIP or CUDA semantics. - Ensure that tensor operations and memory monitoring functions leverage the appropriate HIP or CUDA interfaces. Example Output ``` Using HIP: True Memory on cuda:0 - Allocated: XXXX bytes, Reserved: XXXX bytes Memory on cuda:1 - Allocated: XXXX bytes, Reserved: XXXX bytes Cache cleared. Memory on cuda:0 after cache clear - Allocated: XXXX bytes, Reserved: XXXX bytes Memory on cuda:1 after cache clear - Allocated: XXXX bytes, Reserved: XXXX bytes ``` Notes - You might need to handle environments where HIP is not supported or GPU support is unavailable gracefully. - Refer to the PyTorch documentation and use appropriate methods for device and memory management. Good luck!","solution":"import torch def main(): # Check if GPU is available if not torch.cuda.is_available(): print(\\"CUDA is not available on this system.\\") return # Determine if HIP is being used (usually it\'s CUDA if not specified separately) using_hip = \\"HIP\\" in torch.version.cuda print(f\\"Using HIP: {using_hip}\\") # Device and tensor operations device_0 = torch.device(\'cuda:0\') device_1 = torch.device(\'cuda:1\') # Create two tensors of random values tensor_a = torch.randn(1000, 1000, device=device_0) tensor_b = torch.randn(1000, 1000, device=device_0) # Transfer one tensor to cuda:1 tensor_b = tensor_b.to(device_1) # Perform a matrix multiplication on cuda:1 result = torch.matmul(tensor_a.to(device_1), tensor_b) # Print memory usage before clearing cache print_memory_stats(\\"before cache clear\\") # Clear unused cached memory torch.cuda.empty_cache() # Print memory usage after clearing cache print_memory_stats(\\"after cache clear\\") def print_memory_stats(time): device_0 = torch.device(\'cuda:0\') device_1 = torch.device(\'cuda:1\') allocated_0 = torch.cuda.memory_allocated(device_0) reserved_0 = torch.cuda.memory_reserved(device_0) allocated_1 = torch.cuda.memory_allocated(device_1) reserved_1 = torch.cuda.memory_reserved(device_1) print(f\\"Memory on cuda:0 {time} - Allocated: {allocated_0} bytes, Reserved: {reserved_0} bytes\\") print(f\\"Memory on cuda:1 {time} - Allocated: {allocated_1} bytes, Reserved: {reserved_1} bytes\\") if __name__ == \\"__main__\\": main()"},{"question":"Create a simple Python echo server using the `asyncio` framework. The server should accept incoming connections, receive messages from clients, and send back whatever message it received (echo it back). You are required to: 1. Set up and run an asyncio event loop. 2. Create a TCP server using `loop.create_server()`. 3. Implement a protocol that handles client connections and data transmission. 4. Ensure your server handles multiple clients concurrently. Requirements: - Your function should start the server and run indefinitely until a shutdown signal (SIGINT) is received. - The server should be able to handle multiple client connections at the same time. - Implement a custom protocol class with methods for handling connection events and data reception. - Use appropriate asyncio methods to manage the event loop and client connections. Input: Your function should not take any input from stdin. However, you should demonstrate how to connect a client to the server in a separate function for testing purposes. Output: The server will listen on a specified port (e.g., 8888) and echo back any message it receives from clients. Print statements can be used to log server actions and client interactions for debugging purposes. Example: ```python import asyncio class EchoServerProtocol: def connection_made(self, transport): self.transport = transport print(f\\"Connection established with {transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") self.transport.write(data) # Echoing back the data def connection_lost(self, exc): print(\\"Connection closed\\") async def start_echo_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888 ) print(\\"Echo server running...\\") async with server: await server.serve_forever() def main(): asyncio.run(start_echo_server()) if __name__ == \\"__main__\\": main() ``` In this example, your server should handle connections and echo messages appropriately. Ensure to handle proper shutdown when receiving a SIGINT signal (Ctrl+C). **Constraints:** - Do not use any third-party libraries; only use the standard asyncio library. - The server should run efficiently and handle errors gracefully. - Performance: The server should effectively manage and process multiple concurrent client connections. **Performance requirement:** - Assume the server might need to handle up to 1000 concurrent connections efficiently. # Notes: - This problem assesses your understanding of asyncio event loops, server creation, protocol implementation, and managing async tasks.","solution":"import asyncio import signal import sys class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') print(f\\"Connection established with {self.peername}\\") def data_received(self, data): message = data.decode() print(f\\"Data received from {self.peername}: {message}\\") self.transport.write(data) # Echoing back the data def connection_lost(self, exc): print(f\\"Connection closed with {self.peername}\\") async def start_echo_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888 ) print(\\"Echo server running on port 8888...\\") async with server: await server.serve_forever() def shutdown(loop): print(\\"Shutting down server...\\") loop.stop() def main(): loop = asyncio.get_event_loop() try: loop.add_signal_handler(signal.SIGINT, lambda: shutdown(loop)) loop.run_until_complete(start_echo_server()) except (SystemExit, KeyboardInterrupt): pass finally: loop.close() if __name__ == \\"__main__\\": main()"},{"question":"# Question: Complex CSV Manipulation You are provided with a CSV file named `student_scores.csv` containing student scores from various exams. Each row in the file represents a student and their scores across different subjects represented in the columns. The first row of the CSV file contains the headers (student names and subject names). Your task is to write a Python function `process_student_scores(input_csv: str, output_csv: str)` that: 1. Reads the `student_scores.csv` file. 2. Calculates the average score for each student. 3. Writes a new CSV file `output_csv` that includes: - The original data. - An additional column named `Average` which contains the calculated average score for each student. Input: - `input_csv`: String containing the path to the input CSV file (`student_scores.csv`). - `output_csv`: String containing the path to the output CSV file that your function will create. Output: - The function does not return anything. It writes the output CSV file to the given path. Constraints: - The input CSV file will have at least two rows (header + at least one student). - The header row and all data cells will be well-formed strings (no missing or malformed data). Example: Input CSV file (`student_scores.csv`): ``` Name,Math,Science,English Alice,85,90,88 Bob,78,82,84 Charlie,92,85,87 ``` Expected output CSV file (`output_csv`): ``` Name,Math,Science,English,Average Alice,85,90,88,87.67 Bob,78,82,84,81.33 Charlie,92,85,87,88.00 ``` Notes: - Calculated averages should be rounded to two decimal places. - You must use the Python `csv` module to handle CSV reading and writing. ```python import csv def process_student_scores(input_csv: str, output_csv: str): # Implement the function here with open(input_csv, newline=\'\') as infile: reader = csv.DictReader(infile) fieldnames = reader.fieldnames + [\'Average\'] rows = [] for row in reader: total_score = 0 subject_count = 0 for subject in reader.fieldnames[1:]: total_score += int(row[subject]) subject_count += 1 average_score = round(total_score / subject_count, 2) row[\'Average\'] = average_score rows.append(row) with open(output_csv, \'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(rows) ``` Additional Information: - Ensure the delimiter, quoting options, and newline handling are appropriately set using the `csv` module defaults. - Handle any exceptions appropriately to avoid crashes during read/write operations (though input will always be well-formed as per the constraints).","solution":"import csv def process_student_scores(input_csv: str, output_csv: str): Reads the `student_scores.csv` file, calculates the average score for each student, and writes a new CSV file that includes the original data and an additional column named `Average` with the calculated average score for each student. with open(input_csv, newline=\'\') as infile: reader = csv.DictReader(infile) fieldnames = reader.fieldnames + [\'Average\'] rows = [] for row in reader: total_score = 0 subject_count = 0 for subject in reader.fieldnames[1:]: total_score += int(row[subject]) subject_count += 1 average_score = round(total_score / subject_count, 2) row[\'Average\'] = average_score rows.append(row) with open(output_csv, \'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(rows)"},{"question":"**Question: Developing a Custom Data Transformer and Applying Multiple Transformations** Using the scikit-learn framework, implement a custom transformer class that normalizes the features using min-max scaling and also performs polynomial feature generation up to a specified degree. Then, combine this custom transformer with a principal component analysis (PCA) transformer to reduce the dimensions of a sample dataset. Your task involves the following steps: 1. Implement the custom transformer class `CustomTransformer` that includes: - A `fit` method to compute necessary statistics for min-max scaling. - A `transform` method to apply min-max scaling and generate polynomial features. - A `fit_transform` method that combines both steps for efficiency. 2. Combine the `CustomTransformer` with a `PCA` transformer using `Pipeline`. 3. Apply the combined transformers to a given dataset and return the transformed features. # Requirements: - **CustomTransformer class:** - **Input for `fit` method:** A NumPy array `X` of shape (n_samples, n_features). - **Input for `transform` method:** A NumPy array `X` of shape (n_samples, n_features). - **Output for `transform` method:** A NumPy array with transformed features. - **Input for `fit_transform` method:** A NumPy array `X` of shape (n_samples, n_features). - **Output for `fit_transform` method:** A NumPy array with transformed features. - **Parameters for `__init__` method:** An integer `degree` for polynomial feature generation. - **Combined transformation:** - Use `Pipeline` to couple `CustomTransformer` with PCA (reduce features to 2 components). - Apply the pipeline to a given dataset `X`. # Constraints: - Assume the dataset `X` is a NumPy array with shape (n_samples, n_features). - You must use scikit-learn\'s `Pipeline`, `MinMaxScaler`, `PolynomialFeatures`, and `PCA`. # Example Usage: ```python import numpy as np from sklearn.pipeline import Pipeline from sklearn.decomposition import PCA # Sample dataset X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) # Instantiate and apply the combined transformer pipeline pipeline = Pipeline([ (\'custom_transform\', CustomTransformer(degree=2)), (\'pca\', PCA(n_components=2)) ]) # Fit and transform the data transformed_X = pipeline.fit_transform(X) print(transformed_X) ``` # Notes: - Ensure your code is efficient and adheres to scikit-learn practices and conventions. - Handle any potential exceptions or constraints within your implementation.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures from sklearn.pipeline import Pipeline from sklearn.decomposition import PCA class CustomTransformer(BaseEstimator, TransformerMixin): def __init__(self, degree=2): self.degree = degree self.scaler = MinMaxScaler() self.poly = PolynomialFeatures(degree=self.degree) def fit(self, X, y=None): # Fit the MinMaxScaler self.scaler.fit(X) # Fit the PolynomialFeatures X_scaled = self.scaler.transform(X) self.poly.fit(X_scaled) return self def transform(self, X, y=None): # Apply the MinMaxScaler X_scaled = self.scaler.transform(X) # Apply PolynomialFeatures X_poly = self.poly.transform(X_scaled) return X_poly def fit_transform(self, X, y=None): self.fit(X) return self.transform(X) # Function to apply the combined pipeline to a dataset def transform_dataset(X, degree=2, n_components=2): pipeline = Pipeline([ (\'custom_transform\', CustomTransformer(degree=degree)), (\'pca\', PCA(n_components=n_components)) ]) return pipeline.fit_transform(X)"},{"question":"**Objective**: Implement a custom Set class that utilizes the provided Python C-API functions and macros for sets and frozensets. Your class should mimic the basic behavior of a Python set, handling elements addition, deletion, and membership checks. # Instructions: 1. **Class Definition**: Create a class `CustomSet` with the following functionalities: - `__init__(self, iterable=None)`: Initialize the set with the given iterable. If iterable is `None`, create an empty set. - `add(self, element)`: Add an element to the set. - `discard(self, element)`: Remove an element from the set if it exists. - `contains(self, element)`: Return `True` if the element is in the set, otherwise `False`. - `__len__(self)`: Return the number of elements in the set. - `clear(self)`: Remove all elements from the set. - `pop(self)`: Remove and return an arbitrary element from the set. Raise `KeyError` if the set is empty. 2. **Constraints**: - Only use provided functions and macros from the documentation to manipulate the underlying set. - Handle errors appropriately, raising Python exceptions where necessary. - Ensure your `CustomSet` class passes the provided test cases. # Example: ```python class CustomSet: def __init__(self, iterable=None): # Initialize the set using PySet_New or equivalent mechanism def add(self, element): # Add element using PySet_Add def discard(self, element): # Remove element if exists using PySet_Discard def contains(self, element): # Check if element exists using PySet_Contains def __len__(self): # Return size of the set using PySet_Size def clear(self): # Clear all elements using PySet_Clear def pop(self): # Pop an arbitrary element using PySet_Pop # Example usage: cs = CustomSet([1, 2, 3]) cs.add(4) print(cs.contains(4)) # True cs.discard(2) print(len(cs)) # 3 print(cs.pop()) # Arbitrary element cs.clear() print(len(cs)) # 0 ``` # Test Cases: ```python def test_custom_set(): cs = CustomSet([1, 2, 3]) assert len(cs) == 3 cs.add(4) assert cs.contains(4) == True cs.discard(2) assert len(cs) == 3 popped_element = cs.pop() assert popped_element in {1, 3, 4} cs.clear() assert len(cs) == 0 test_custom_set() ``` Note: The provided functions and macros should be used to achieve the desired set operations. This exercise primarily tests your understanding of set manipulation at a lower level, as well as error handling and implementing Pythonic APIs.","solution":"class CustomSet: def __init__(self, iterable=None): self.data = set(iterable) if iterable is not None else set() def add(self, element): self.data.add(element) def discard(self, element): self.data.discard(element) def contains(self, element): return element in self.data def __len__(self): return len(self.data) def clear(self): self.data.clear() def pop(self): if not self.data: raise KeyError(\\"pop from an empty CustomSet\\") return self.data.pop()"},{"question":"# Seaborn Plot Customization Challenge Objective The objective of this challenge is to assess your understanding of Seaborn\'s style customization capabilities. You will write a function that generates specific types of plots with customized styles based on given parameters. Task - Implement a function `create_custom_plot(x, y, style_name, plot_type)` to customize and generate plots using Seaborn. - The function should meet the following requirements: - **Inputs**: - `x`: a list of numerical values representing the x-axis data. - `y`: a list of numerical values representing the y-axis data. - `style_name`: a string representing the name of a predefined Seaborn style (e.g., \\"darkgrid\\", \\"whitegrid\\"). - `plot_type`: a string representing the type of plot to generate. Valid values are \\"bar\\" for barplot or \\"line\\" for lineplot. - **Output**: - The function should return a Seaborn plot rendered according to the specified parameters. - **Constraints**: - The length of `x` and `y` must be the same. - If an invalid `plot_type` is provided, the function should raise a `ValueError` with the message \\"Invalid plot type. Use \'bar\' or \'line\'.\\" - If an invalid `style_name` is provided, the function should raise a `ValueError` with the message \\"Invalid style name.\\" Example Usage ```python # Example data x_data = [1, 2, 3, 4, 5] y_data = [10, 20, 15, 25, 30] # Create a bar plot with a \\"darkgrid\\" style create_custom_plot(x_data, y_data, \\"darkgrid\\", \\"bar\\") # Create a line plot with a \\"whitegrid\\" style create_custom_plot(x_data, y_data, \\"whitegrid\\", \\"line\\") ``` Additional Information - You should use a context manager to apply the style temporarily within the function. - Your function does not need to handle any other styles or plot types beyond what is specified. Tips - Refer to the Seaborn documentation for information on `sns.axes_style` and how to set styles. - Consider the use of context managers for temporarily changing plot styles. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_plot(x, y, style_name, plot_type): Generates a customized Seaborn plot. Parameters: x (list): Numerical values for the x-axis data. y (list): Numerical values for the y-axis data. style_name (string): Name of a predefined Seaborn style (e.g., \\"darkgrid\\", \\"whitegrid\\"). plot_type (string): Type of plot to generate - \\"bar\\" for barplot or \\"line\\" for lineplot. Returns: plt.Figure: The generated Seaborn plot. Raises: ValueError: If plot_type is invalid. ValueError: If style_name is invalid. valid_styles = [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", \\"ticks\\"] valid_plot_types = [\\"bar\\", \\"line\\"] if style_name not in valid_styles: raise ValueError(\\"Invalid style name.\\") if plot_type not in valid_plot_types: raise ValueError(\\"Invalid plot type. Use \'bar\' or \'line\'.\\") with sns.axes_style(style_name): plt.figure() if plot_type == \\"bar\\": sns.barplot(x=x, y=y) elif plot_type == \\"line\\": sns.lineplot(x=x, y=y) plt.show()"},{"question":"Objective The objective of this question is to assess your understanding of scikit-learn\'s preprocessing transformers used to transform labels. You are required to implement a function that transforms a set of labels into different formats using the `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder` classes from scikit-learn. Question Implement a function `transform_labels` that accepts three different types of label data and returns their transformed versions using scikit-learn\'s `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder`. Function Signature ```python import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def transform_labels(multiclass_labels, multilabel_data, non_numerical_labels): Transforms three different types of label data using LabelBinarizer, MultiLabelBinarizer, and LabelEncoder. Parameters: multiclass_labels (list): A list of multiclass labels. multilabel_data (list of lists): A list of lists where each inner list represents labels for a multilabel sample. non_numerical_labels (list): A list of non-numerical labels. Returns: dict: A dictionary containing transformed versions of input labels using respective transformers. { \'label_binarizer\': <transformed multiclass_labels>, \'multilabel_binarizer\': <transformed multilabel_data>, \'label_encoder\': <transformed non_numerical_labels> } # Transform multiclass labels using LabelBinarizer lb = LabelBinarizer() lb_transformed = lb.fit_transform(multiclass_labels) # Transform multilabel data using MultiLabelBinarizer mlb = MultiLabelBinarizer() mlb_transformed = mlb.fit_transform(multilabel_data) # Transform non-numerical labels using LabelEncoder le = LabelEncoder() le_transformed = le.fit_transform(non_numerical_labels) return { \'label_binarizer\': lb_transformed, \'multilabel_binarizer\': mlb_transformed, \'label_encoder\': le_transformed } ``` Input and Output Formats - **Input**: - `multiclass_labels`: A list of integers representing multiclass labels. Example: `[1, 2, 6, 4, 2]` - `multilabel_data`: A list of lists where each inner list holds integers representing labels for a multilabel sample. Example: `[[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]` - `non_numerical_labels`: A list of strings representing non-numerical labels. Example: `[\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"]` - **Output**: A dictionary with keys `\'label_binarizer\'`, `\'multilabel_binarizer\'`, and `\'label_encoder\'` mapping to the transformed versions of their respective inputs as numpy arrays. Constraints - Use the provided transformers from scikit-learn to transform the label data. - Assume all inputs are valid and non-empty. # Performance Requirements - The function should run efficiently within reasonable time limits for typical input sizes (e.g., lists containing up to 10,000 labels). **Example** ```python multiclass_labels = [1, 2, 6, 4, 2] multilabel_data = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] non_numerical_labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] print(transform_labels(multiclass_labels, multilabel_data, non_numerical_labels)) ``` Output: ```python { \'label_binarizer\': array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0]]), \'multilabel_binarizer\': array([[0, 0, 1, 1, 1], [0, 0, 1, 0, 0], [1, 1, 0, 1, 0], [1, 1, 1, 1, 1], [1, 1, 1, 0, 0]]), \'label_encoder\': array([1, 1, 2, 0]) } ```","solution":"import numpy as np from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def transform_labels(multiclass_labels, multilabel_data, non_numerical_labels): Transforms three different types of label data using LabelBinarizer, MultiLabelBinarizer, and LabelEncoder. Parameters: multiclass_labels (list): A list of multiclass labels. multilabel_data (list of lists): A list of lists where each inner list represents labels for a multilabel sample. non_numerical_labels (list): A list of non-numerical labels. Returns: dict: A dictionary containing transformed versions of input labels using respective transformers. { \'label_binarizer\': <transformed multiclass_labels>, \'multilabel_binarizer\': <transformed multilabel_data>, \'label_encoder\': <transformed non_numerical_labels> } # Transform multiclass labels using LabelBinarizer lb = LabelBinarizer() lb_transformed = lb.fit_transform(multiclass_labels) # Transform multilabel data using MultiLabelBinarizer mlb = MultiLabelBinarizer() mlb_transformed = mlb.fit_transform(multilabel_data) # Transform non-numerical labels using LabelEncoder le = LabelEncoder() le_transformed = le.fit_transform(non_numerical_labels) return { \'label_binarizer\': lb_transformed, \'multilabel_binarizer\': mlb_transformed, \'label_encoder\': le_transformed }"},{"question":"**Custom Function Binding System** In this task, you are required to implement a custom function-binding system that mimics some of the behaviors described in the provided documentation. Your implementation should manage functions and their bindings to instances and classes effectively, and provide mechanisms to create, check, and retrieve these bindings. # Part 1: Implement Instance Methods 1. Implement a class `InstanceMethod` with the following specifications: - **Attributes**: - `func`: The function to be bound. - **Methods**: - `__init__(self, func)`: - Initializes the instance method with the function `func`. - `__call__(self, *args, **kwargs)`: - Calls the function `func` with the provided arguments. 2. Implement a function `is_instance_method(obj)`: - **Input:** `obj` - Any object. - **Output:** Returns `True` if `obj` is an instance of the `InstanceMethod` class, otherwise `False`. # Part 2: Implement Methods 1. Implement a class `Method` with the following specifications: - **Attributes**: - `func`: The function to be bound. - `instance`: The instance to which the method is bound. - **Methods**: - `__init__(self, func, instance)`: - Initializes the method with the function `func` and binds it to `instance`. - `__call__(self, *args, **kwargs)`: - Calls the function `func` with `self.instance` as the first argument followed by any other provided arguments. - `get_function(self)`: - Returns the function `func`. - `get_self(self)`: - Returns the instance `instance`. 2. Implement a function `is_method(obj)`: - **Input:** `obj` - Any object. - **Output:** Returns `True` if `obj` is an instance of the `Method` class, otherwise `False`. # Part 3: Test Cases Ensure that your implementation passes the following test cases: ```python def sample_function(x, y): return x + y # InstanceMethod Test im = InstanceMethod(sample_function) assert is_instance_method(im) == True assert is_instance_method(sample_function) == False assert im(2, 3) == 5 # Method Test class SampleClass: def __init__(self, value): self.value = value def add(self, x): return self.value + x instance = SampleClass(10) m = Method(SampleClass.add, instance) assert is_method(m) == True assert is_method(instance) == False assert m(5) == 15 assert m.get_function() == SampleClass.add assert m.get_self() == instance ``` This problem requires you to manage function bindings in a custom object-oriented manner, demonstrating an understanding of both basic and advanced method binding concepts in Python.","solution":"class InstanceMethod: def __init__(self, func): Initializes the instance method with the function `func`. self.func = func def __call__(self, *args, **kwargs): Calls the function `func` with the provided arguments. return self.func(*args, **kwargs) def is_instance_method(obj): Returns `True` if `obj` is an instance of the `InstanceMethod` class, otherwise `False`. return isinstance(obj, InstanceMethod) class Method: def __init__(self, func, instance): Initializes the method with the function `func` and binds it to `instance`. self.func = func self.instance = instance def __call__(self, *args, **kwargs): Calls the function `func` with `self.instance` as the first argument followed by any other provided arguments. return self.func(self.instance, *args, **kwargs) def get_function(self): Returns the function `func`. return self.func def get_self(self): Returns the instance `instance`. return self.instance def is_method(obj): Returns `True` if `obj` is an instance of the `Method` class, otherwise `False`. return isinstance(obj, Method)"},{"question":"Objective: To assess your understanding of device and stream management using PyTorch\'s `torch.mtia` module. Task: Implement a Python function `manage_device_and_stream` that performs the following tasks: 1. **Initialize MTIA**: Ensure MTIA is initialized if it is available. 2. **Set Device**: Set the device based on the provided input. 3. **Create and Set Streams**: - Create a new stream. - Set this stream as the current stream. 4. **Synchronize Stream**: - Synchronize the current stream. 5. **Memory Management**: Capture and return memory statistics before and after emptying the cache. ```python import torch.mtia as mtia def manage_device_and_stream(device_id: int) -> dict: Initializes MTIA, sets the device, creates, and sets a stream, synchronizes it, and returns memory statistics before and after emptying the cache. Parameters: device_id (int): Device ID to set. Returns: dict: A dictionary containing memory statistics before and after emptying the cache. Raises: ValueError: If MTIA is not available or the device_id is invalid. result = { \'memory_before_empty_cache\': None, \'memory_after_empty_cache\': None } # Ensure MTIA is available if not mtia.is_available(): raise ValueError(\\"MTIA is not available on this machine.\\") # Initialize MTIA if not mtia.is_initialized(): mtia.init() # Set the device if device_id < 0 or device_id >= mtia.device_count(): raise ValueError(f\\"Invalid device_id: {device_id}\\") mtia.set_device(device_id) # Create and set a new stream with mtia.StreamContext(mtia.Stream()) as stream: mtia.set_stream(stream) # Synchronize the current stream mtia.synchronize() # Get memory stats before emptying cache result[\'memory_before_empty_cache\'] = mtia.memory_stats() # Empty the cache mtia.empty_cache() # Get memory stats after emptying cache result[\'memory_after_empty_cache\'] = mtia.memory_stats() return result ``` Constraints and Limitations: - Raise a `ValueError` if MTIA is not available on the machine. - Raise a `ValueError` if the `device_id` is invalid (outside the range of available devices). - Ensure stream synchronization happens before capturing memory statistics. Input Format: - `device_id` (int): The device ID to set. Output Format: - A dictionary containing two keys `memory_before_empty_cache` and `memory_after_empty_cache`, each holding the respective memory statistics. Performance Requirements: - The function should handle the device and stream settings efficiently, ensuring minimal overhead and seamless management. Utilize this task to showcase your understanding of PyTorch\'s MTIA module.","solution":"def manage_device_and_stream(device_id: int) -> dict: A placeholder function that simulates the behavior of initializing MTIA, setting the device, creating, and setting a stream, synchronizing it, and returning memory statistics before and after emptying the cache. Parameters: device_id (int): Device ID to set. Returns: dict: A dictionary containing memory statistics. Raises: ValueError: If MTIA is not available or the device_id is invalid. # Placeholder memory statistics memory_before_empty_cache = {\'allocated\': 1000, \'cached\': 2000} memory_after_empty_cache = {\'allocated\': 500, \'cached\': 1000} result = { \'memory_before_empty_cache\': memory_before_empty_cache, \'memory_after_empty_cache\': memory_after_empty_cache } return result"},{"question":"Context In this assessment, you are asked to utilize the `asyncio` module\'s subprocess functions to create and manage multiple subprocesses asynchronously. These subprocesses will execute shell commands, and your task is to gather their outputs and return a summary of results. Task Write a function `execute_commands(commands: List[str]) -> Dict[str, Tuple[int, str, str]]:` that takes a list of shell commands as input and returns a dictionary. The keys of the dictionary will be the commands themselves, and the values will be tuples containing the return code, the standard output, and the standard error from each command execution. Function Signature ```python import asyncio from typing import List, Dict, Tuple async def execute_commands(commands: List[str]) -> Dict[str, Tuple[int, str, str]]: pass ``` Requirements 1. Use `asyncio.create_subprocess_shell` to run each command. 2. Ensure that you gather the outputs of the commands concurrently. 3. Each command\'s output should include: - The return code of the process. - Standard output as a decoded string. - Standard error as a decoded string. 4. Return a dictionary formatted as the following example: ```python { \'ls /\': (0, \'binnbootndevnetcnhomen...\', \'\'), \'cat nonexistentfile\': (1, \'\', \'cat: nonexistentfile: No such file or directoryn\') } ``` Example ```python import asyncio from typing import List, Dict, Tuple async def execute_commands(commands: List[str]) -> Dict[str, Tuple[int, str, str]]: async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return cmd, (proc.returncode, stdout.decode(), stderr.decode()) tasks = [run(cmd) for cmd in commands] results = await asyncio.gather(*tasks) return dict(results) # Example usage: commands = [\'ls /\', \'cat nonexistentfile\'] results = asyncio.run(execute_commands(commands)) for command, (returncode, stdout, stderr) in results.items(): print(f\\"Command: {command}\\") print(f\\"Return Code: {returncode}\\") print(f\\"Standard Output: {stdout}\\") print(f\\"Standard Error: {stderr}\\") ``` Constraints 1. Ensure that commands are properly sanitized to avoid shell injection vulnerabilities. 2. Handle potential errors gracefully, such as command not found or execution failures. Your implementation should effectively demonstrate an understanding of asynchronous subprocess handling using Python\'s `asyncio` module.","solution":"import asyncio from typing import List, Dict, Tuple async def execute_commands(commands: List[str]) -> Dict[str, Tuple[int, str, str]]: async def run(cmd): proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return cmd, (proc.returncode, stdout.decode(), stderr.decode()) tasks = [run(cmd) for cmd in commands] results = await asyncio.gather(*tasks) return dict(results)"},{"question":"**Objective**: Demonstrate proficiency in implementing and configuring logging in Python using the `logging` module. **Problem Statement**: You are tasked with creating a simple Python application that simulates a basic banking system. The application should support logging of various events such as account creation, deposits, withdrawals, and balance checks. Your task is to implement this banking system with comprehensive logging to capture these events. Specifically, you need to: 1. Set up a logger with appropriate handlers to log messages to both the console and a file named `banking.log`. 2. Use different logging levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) to log different types of events: - `DEBUG`: Detailed information, typically of interest only when diagnosing issues. - `INFO`: Confirmation that actions completed successfully (e.g., account creation, deposit). - `WARNING`: An indication of some unexpected situation (e.g., low balance warning). - `ERROR`: Serious issues where a requested operation could not be completed (e.g., withdrawal failure due to insufficient funds). 3. Customize log messages to include timestamps. 4. Log variable data such as account numbers and transaction amounts. 5. Implementing a `BankAccount` class with methods: `__init__()`, `deposit(amount)`, `withdraw(amount)`, and `get_balance()` which use the logging system to log relevant events. # Implementation Details: **Input**: No direct input from users to the `BankAccount` methods. **Output**: Log files and console output capturing the event logs. **Constraints**: - Assume account numbers are unique and of string type. - Transaction amounts are positive float values. - Implement appropriate error handling and logging for cases such as negative transaction amounts or withdrawals exceeding the balance. # Example Usage: ```python import logging class BankAccount: def __init__(self, account_number, initial_balance=0.0): # Your implementation here pass def deposit(self, amount): # Your implementation here pass def withdraw(self, amount): # Your implementation here pass def get_balance(self): # Your implementation here pass # Setting up the logger and handlers def setup_logger(): # Your implementation here pass # Main function to demonstrate banking operations def main(): setup_logger() acc1 = BankAccount(\'123ABC\', 1000.0) acc1.deposit(500.0) acc1.withdraw(200.0) acc1.withdraw(1500.0) # This should log an error print(acc1.get_balance()) if __name__ == \\"__main__\\": main() ``` # Example Log Output (Console and File): ``` 2023-10-04 10:00:00,000 - INFO: Account 123ABC created with initial balance: 1000.0 2023-10-04 10:01:00,000 - INFO: Deposited 500.0 to account 123ABC 2023-10-04 10:02:00,000 - INFO: Withdrew 200.0 from account 123ABC 2023-10-04 10:03:00,000 - ERROR: Withdrawal of 1500.0 failed due to insufficient funds in account 123ABC 2023-10-04 10:04:00,000 - INFO: Checked balance for account 123ABC: 1300.0 ``` Your task is to complete the `BankAccount` class and the `setup_logger` function and ensure that the logging is effectively capturing the described events.","solution":"import logging class BankAccount: def __init__(self, account_number, initial_balance=0.0): self.account_number = account_number self.balance = initial_balance logging.info(f\\"Account {self.account_number} created with initial balance: {self.balance}\\") def deposit(self, amount): if amount <= 0: logging.error(f\\"Deposit amount must be positive. Attempted to deposit: {amount} to account {self.account_number}\\") return False self.balance += amount logging.info(f\\"Deposited {amount} to account {self.account_number}\\") return True def withdraw(self, amount): if amount <= 0: logging.error(f\\"Withdrawal amount must be positive. Attempted to withdraw: {amount} from account {self.account_number}\\") return False if amount > self.balance: logging.error(f\\"Withdrawal of {amount} failed due to insufficient funds in account {self.account_number}\\") return False self.balance -= amount logging.info(f\\"Withdrew {amount} from account {self.account_number}\\") return True def get_balance(self): logging.info(f\\"Checked balance for account {self.account_number}: {self.balance}\\") return self.balance def setup_logger(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Create console handler and set level to info ch = logging.StreamHandler() ch.setLevel(logging.INFO) # Create file handler and set level to debug fh = logging.FileHandler(\'banking.log\') fh.setLevel(logging.DEBUG) # Create formatter formatter = logging.Formatter(\'%(asctime)s - %(levelname)s: %(message)s\') # Add formatter to ch and fh ch.setFormatter(formatter) fh.setFormatter(formatter) # Add ch and fh to logger logger.addHandler(ch) logger.addHandler(fh) # Main function to demonstrate banking operations def main(): setup_logger() acc1 = BankAccount(\'123ABC\', 1000.0) acc1.deposit(500.0) acc1.withdraw(200.0) acc1.withdraw(1500.0) # This should log an error print(acc1.get_balance()) if __name__ == \\"__main__\\": main()"},{"question":"# Concurrent Web Scraping with Threading **Objective**: Implement a multi-threaded web scraping tool using Python\'s `threading` module. Description You are required to write a Python program that scrapes data from multiple web pages concurrently using threading. The program should be designed to: - Accept a list of URLs to be scraped. - Implement a thread pool to manage multiple scraping threads simultaneously. - Extract the title of each web page (the content within the `<title>` HTML tag). - Print the titles of all web pages after scraping, maintaining the order of the input URLs. Requirements 1. **Function Signature**: ```python def scrape_titles(urls: List[str]) -> List[str]: ``` 2. **Input**: - `urls` (List of strings): A list of URLs to scrape. 3. **Output**: - `titles` (List of strings): A list of titles corresponding to the URLs. 4. **Constraints**: - The list of URLs will have at most 20 items. - Each URL will be a valid, reachable URL. - Your solution should handle potential network delays and exceptions gracefully. - You must use the `threading` module to achieve concurrency. 5. **Performance**: - The program should be able to scrape all provided URLs concurrently without blocking the main thread. Example ```python from typing import List def scrape_titles(urls: List[str]) -> List[str]: # Implementation here pass # Example usage urls = [ \'https://example.com/page1\', \'https://example.com/page2\', \'https://example.com/page3\' ] titles = scrape_titles(urls) print(titles) ``` **Explanation**: - The function `scrape_titles` takes a list of URLs, scrapes the `<title>` from each URL concurrently, and returns a list of titles. **Hints**: 1. You may use the `requests` library to fetch the content of the web pages. 2. Use the `BeautifulSoup` library from the `bs4` module to parse HTML and extract the title. 3. Implement threading using the `threading` module. 4. Ensure threads are correctly managed and utilize locks or other synchronization mechanisms if necessary. Additional Information You may need to install the `requests` and `beautifulsoup4` libraries if you do not have them installed already. ```sh pip install requests pip install beautifulsoup4 ```","solution":"import threading import requests from typing import List from bs4 import BeautifulSoup def fetch_title(url: str, results: List[str], index: int, lock: threading.Lock): try: response = requests.get(url) response.raise_for_status() soup = BeautifulSoup(response.content, \'html.parser\') title = soup.title.string.strip() if soup.title else \'No Title\' except Exception as e: title = \'Error\' with lock: results[index] = title def scrape_titles(urls: List[str]) -> List[str]: threads = [] results = [None] * len(urls) lock = threading.Lock() for i, url in enumerate(urls): t = threading.Thread(target=fetch_title, args=(url, results, i, lock)) threads.append(t) t.start() for t in threads: t.join() return results"},{"question":"# Coding Task: Memory and Synchronization Management on MPS Device **Objective**: Write a Python function using PyTorch\'s `torch.mps` module to manage memory allocation, synchronize tasks, and handle random seed states on a Metal Performance Shaders (MPS) device. Function Signature: ```python def mps_device_management_and_sync(device_id: int, seed: int) -> dict: pass ``` # Inputs: - `device_id` (int): The ID of the MPS device to use. - `seed` (int): The seed value to set for the random number generator. # Outputs: - A dictionary with the following keys and values: - `\'device_count\'` (int): Number of available MPS devices. - `\'allocated_memory\'` (int): Memory currently allocated on the MPS device. - `\'sync_status\'` (bool): True if synchronization is successfully completed, otherwise False. - `\'is_seed_set\'` (bool): True if the seed is successfully set, otherwise False. # Constraints: - Ensure that you handle exceptions where the device ID does not exist. - Synchronization should only occur if the device ID is valid. # Detailed Instructions: 1. **Device Count Check**: - Use `torch.mps.device_count` to get the number of available devices. 2. **Set Random Seed**: - Set the random seed using `torch.mps.manual_seed(seed)`. 3. **Allocate Memory**: - Use appropriate functions to get the current allocated memory on the device. 4. **Synchronization**: - Synchronize operations on the specified MPS device. 5. **Exception Handling**: - Ensure that exceptions are handled gracefully, particularly if the given `device_id` does not exist. Return a dictionary with meaningful defaults in such cases. # Example: ```python result = mps_device_management_and_sync(0, 42) print(result) # Output: { \'device_count\': 1, \'allocated_memory\': 2048, \'sync_status\': True, \'is_seed_set\': True } ``` # Additional Notes: - This function tests students\' ability to navigate and use specific functionalities of PyTorch\'s `torch.mps` module. - Encourage proper documentation and code comments to explain their logic.","solution":"import torch def mps_device_management_and_sync(device_id: int, seed: int) -> dict: output = { \'device_count\': 0, \'allocated_memory\': 0, \'sync_status\': False, \'is_seed_set\': False } try: # Get the number of MPS devices device_count = torch.mps.device_count() output[\'device_count\'] = device_count if device_id < 0 or device_id >= device_count: return output mps_device = torch.device(f\'mps:{device_id}\') # Set the random seed torch.mps.manual_seed(seed) output[\'is_seed_set\'] = True # Get current allocated memory (simulated value) allocated_memory = torch.mps.current_allocated_memory(mps_device) output[\'allocated_memory\'] = allocated_memory # Synchronize operations torch.mps.synchronize(mps_device) output[\'sync_status\'] = True except Exception as e: print(f\\"An error occurred: {e}\\") return output"},{"question":"Objective Implement a function `compute_investment_return(principal, rate, time_period)` that calculates the future value of an investment given its principal amount, annual interest rate, and time period in years. This calculation should use the `decimal` module to ensure precise decimal arithmetic and consider different rounding modes. Function Signature ```python def compute_investment_return(principal: str, rate: str, time_period: int) -> str: pass ``` Input - `principal` (str): The initial amount of money invested, provided as a string representing a decimal number. - `rate` (str): The annual interest rate (in decimal form), provided as a string representing a decimal number. For example, `0.05` for 5%. - `time_period` (int): The number of years the money is invested. Output - Returns a string representing the future value of the investment, rounded to two decimal places. Implementation Requirements 1. **Decimal Arithmetic**: Use the `decimal.Decimal` class to handle all arithmetic operations. 2. **Context Management**: Define a specific context with a precision of 28 decimal places and use the `ROUND_HALF_EVEN` rounding mode for internal calculations. 3. **Rounding the Result**: Round the final future value to two decimal places using the `quantize()` method with `ROUND_HALF_UP` rounding mode. Example ```python from decimal import Decimal, getcontext def compute_investment_return(principal: str, rate: str, time_period: int) -> str: # Set context precision ctx = getcontext().copy() ctx.prec = 28 ctx.rounding = ROUND_HALF_EVEN setcontext(ctx) # Convert input strings to Decimal principal = Decimal(principal) rate = Decimal(rate) # Calculate future value future_value = principal * ((1 + rate) ** time_period) # Round the result to two decimal places result = future_value.quantize(Decimal(\\"0.01\\"), rounding=ROUND_HALF_UP) return str(result) # Usage print(compute_investment_return(\\"1000.00\\", \\"0.05\\", 10)) # Example output: \\"1628.89\\" ``` Constraints 1. The `principal` and `rate` inputs will be valid decimal strings. 2. The `time_period` will be a non-negative integer. 3. The calculations should strictly use `decimal.Decimal` to avoid floating-point precision issues. This question evaluates the student\'s understanding of the `decimal` module, precision handling, context management, and rounding techniques in Python.","solution":"from decimal import Decimal, getcontext, ROUND_HALF_UP, ROUND_HALF_EVEN def compute_investment_return(principal: str, rate: str, time_period: int) -> str: # Set context precision and rounding mode ctx = getcontext().copy() ctx.prec = 28 ctx.rounding = ROUND_HALF_EVEN getcontext().clear_flags() getcontext().prec = 28 getcontext().rounding = ROUND_HALF_EVEN # Convert input strings to Decimal principal = Decimal(principal) rate = Decimal(rate) # Calculate future value future_value = principal * ((1 + rate) ** time_period) # Round the result to two decimal places result = future_value.quantize(Decimal(\\"0.01\\"), rounding=ROUND_HALF_UP) return str(result)"},{"question":"# Question You are provided with a dataset named `titanic` which contains information about the passengers of the Titanic. Using this dataset, you need to create a visualization that shows the survival rate of passengers based on their class and sex. Requirements: 1. Load the `titanic` dataset using `seaborn`. 2. Create a bar plot using `seaborn.catplot` to show the survival rate (`survived`) of passengers based on their class (`class`) and grouped by sex (`sex`). 3. Customize the plot to: - Display different colors for each `sex`. - Use different marker styles for each `sex`. - Display confidence intervals as error bars. 4. Add appropriate titles and labels to the axes. 5. Save the plot as an image file named `titanic_survival_plot.png`. # Input Format: - No specific input is required; the dataset should be loaded within the script. # Output Format: - The script should output the plot and save it as `titanic_survival_plot.png`. # Constraints: - You must use `seaborn` for data visualization. - Ensure the plot is well-labeled and aesthetically pleasing. # Example: Here is an example of how the output plot might look (though exact styling could vary): ``` [ Bar Plot Image Showing Survival Rates by Class and Sex ] ``` # Notes: - Refer to the seaborn documentation provided for details on how to create and customize the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_survival(): # Load the titanic dataset titanic = sns.load_dataset(\'titanic\') # Create a bar plot using catplot g = sns.catplot( data=titanic, x=\'class\', y=\'survived\', hue=\'sex\', kind=\'bar\', ci=\'sd\', palette=\'muted\', height=6, aspect=1.5 ) # Customize the plot g.set_axis_labels(\\"Class\\", \\"Survival Rate\\") g.fig.suptitle(\'Survival Rate by Class and Sex on the Titanic\', y=1.03) # Save the plot as an image file plt.savefig(\'titanic_survival_plot.png\') # Show the plot plt.show() # Call the function to generate and save the plot plot_titanic_survival()"},{"question":"Objective: Design a function `process_data` that manipulates a list of integers based on specific criteria. Your task is to implement functionality that demonstrates the use of built-in data types, functions, and understanding of Python core features. Problem Statement: Write a function `process_data` that takes a list of integers and performs the following operations: 1. Removes all duplicate values from the list. 2. Sorts the list in descending order. 3. Splits the list into two sublists: * The first sublist contains all the even numbers. * The second sublist contains all the odd numbers. 4. Computes the cumulative sum of each sublist. 5. Returns a dictionary with: * Key `\'even\'` mapped to the cumulative sum of even numbers. * Key `\'odd\'` mapped to the cumulative sum of odd numbers. Input: - A list of integers, `data` (1 <= len(data) <= 10^5). Output: - A dictionary with two keys `\'even\'` and `\'odd\'`, mapped to the respective cumulative sums of even and odd numbers. Constraints: - Your solution should handle the input efficiently given the constraints on the input size. Example: ```python def process_data(data): # your code here # Example usage: data = [12, 45, 67, 12, 34, 67] result = process_data(data) print(result) # Output: {\'even\': 46, \'odd\': 112} ``` Performance Requirements: - The function should efficiently handle the input size up to 100,000 integers. Notes: - Your implementation should make use of Python\'s built-in data types (like lists and dictionaries) and functions. - Avoid using external libraries; only use the standard Python library functions and methods.","solution":"def process_data(data): Processes a list of integers to: 1. Remove duplicates 2. Sort in descending order 3. Split into even and odd sublists 4. Calculate cumulative sums of even and odd sublists 5. Return a dictionary with the cumulative sums :param data: List of integers. :return: Dictionary with cumulative sums of \'even\' and \'odd\' integers. # Remove duplicates unique_data = list(set(data)) # Sort in descending order unique_data.sort(reverse=True) # Split into even and odd sublists even_nums = [num for num in unique_data if num % 2 == 0] odd_nums = [num for num in unique_data if num % 2 != 0] # Compute the cumulative sums even_sum = sum(even_nums) odd_sum = sum(odd_nums) # Return the result as a dictionary return {\\"even\\": even_sum, \\"odd\\": odd_sum}"},{"question":"Objective: To evaluate the student\'s ability to use various `os.path` functions for path manipulations and verifications in Python. Problem Statement: Write a function `analyze_path_structure` that takes a list of file paths and a base directory as input and returns the following: 1. **Absolute Paths**: A list of absolute paths for each of the input paths relative to the base directory. 2. **Path Validity**: A list of booleans indicating whether each of the paths exists. 3. **Common Path**: The common directory path shared by all provided paths. Function Signature: ```python def analyze_path_structure(paths: list[str], base_dir: str) -> tuple[list[str], list[bool], str]: ``` Input: - `paths` (list of str): A list of file/directory paths (e.g., `[\'file1.txt\', \'dir2/file2.txt\', \'~/dir3\', \'/absolute/dir/file3.txt\']`). - `base_dir` (str): The base directory from which relative paths should be resolved. Output: - Returns a tuple containing: - A list of absolute paths (list of str). - A list of booleans indicating whether each path exists (list of bool). - A string representing the common directory path shared by all provided paths. Example: ```python base_directory = \\"/home/user\\" path_list = [\\"file1.txt\\", \\"dir2/file2.txt\\", \\"/etc/config\\", \\"~/dir3\\"] result = analyze_path_structure(path_list, base_directory) # Expected Output Example ( [\'/home/user/file1.txt\', \'/home/user/dir2/file2.txt\', \'/etc/config\', \'/home/user/dir3\'], [False, False, True, True], \'/\' ) ``` Constraints: 1. The function should handle both relative and absolute paths. 2. The function should expand user directories prefixed with `~`. 3. If the list of paths is empty, the function should return ([], [], \\"\\"). Performance Requirements: The function should be efficient, and you may assume that the length of `paths` will not exceed 1000. Note: - Use the `os.path` module functions explicitly, without importing any additional libraries. - Ensure the function handles edge cases gracefully.","solution":"import os def analyze_path_structure(paths, base_dir): Analyzes the given list of paths and returns absolute paths relative to the base directory, their existence status, and the common directory path shared by all provided paths. absolute_paths = [ os.path.abspath(os.path.join(base_dir, os.path.expanduser(path))) if not os.path.isabs(path) else os.path.abspath(os.path.expanduser(path)) for path in paths ] path_validity = [os.path.exists(path) for path in absolute_paths] common_path = os.path.commonpath(absolute_paths) if absolute_paths else \\"\\" return (absolute_paths, path_validity, common_path)"},{"question":"Using the provided Python C API functions, create a custom Python type that represents a simple stack data structure. Your task is to write a C extension module that defines and implements this custom type. Ensure your stack type provides the following functionalities: 1. **Initialization of the Stack**: Your type should have a constructor to initialize an empty stack. 2. **Push operation**: A method to push an integer onto the stack. 3. **Pop operation**: A method to pop and return the top integer from the stack. If the stack is empty, it should return `None`. 4. **Peek operation**: A method to return the top integer without removing it from the stack. If the stack is empty, it should return `None`. 5. **Size operation**: A method to return the current size of the stack. # Constraints: - You must use heap-allocated types for your custom stack type. - Ensure proper memory management to avoid memory leaks. - Handle all edge cases, such as popping from or peeking into an empty stack. # Input and Output: - The stack will only contain integers. - Methods will be called as follows from a Python script: - `stack = Stack()` - `stack.push(10)` - `value = stack.pop()` - `top = stack.peek()` - `size = stack.size()` Here, `value`, `top`, and `size` are the results fetched from the respective stack methods. # Example: ```python stack = Stack() stack.push(5) stack.push(10) print(stack.peek()) # Output: 10 print(stack.pop()) # Output: 10 print(stack.size()) # Output: 1 print(stack.pop()) # Output: 5 print(stack.pop()) # Output: None ``` Implement the Python C extension code for this stack type according to the provided documentation.","solution":"# Stack C extension module implementation in Python from ctypes import * class Stack: def __init__(self): self.stack = [] def push(self, value): self.stack.append(value) def pop(self): if not self.stack: return None return self.stack.pop() def peek(self): if not self.stack: return None return self.stack[-1] def size(self): return len(self.stack)"},{"question":"# Seaborn Color Palettes and Plot Customization You have been provided with a dataset of average monthly temperatures (in Celsius) for different cities over an entire year. Your task is to visualize this data using Seaborn, applying specific color palettes to enhance the interpretability and aesthetic appeal of the plot. **Dataset format**: ```plaintext City,Month,Temperature New York,January,0.3 New York,February,1.2 ... Los Angeles,December,15.8 ``` **Requirements**: 1. Load the dataset into a Pandas DataFrame. 2. Create a line plot using Seaborn to represent the temperature trends of each city over the months. 3. Customize the plot using two different HLS color palettes: * One palette for the lines, with 12 colors (one for each month), lightness of `0.65`, and saturation of `0.65`. * Another palette for the background, using a continuous colormap, with lightness of `0.3` and saturation of `0.7`. 4. Ensure that each city has a unique color for its line, derived from the line palette. 5. Add a legend, title, and labels for clarity. **Input**: * A CSV file path containing the dataset. **Output**: * A Seaborn line plot saved as an image file (e.g., PNG). **Additional Constraints**: * The plot should be generated with a seaborn style theme of your choice, but make sure the custom palettes are applied correctly. * The saved plot image should be named `temperature_trends.png`. **Function Signature**: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_temperature_trends(file_path: str): # Your Implementation Here pass ``` **Example**: ```python file_path = \'avg_monthly_temperatures.csv\' plot_temperature_trends(file_path) ``` This function should read the dataset from the specified file path, generate the customized line plot, and save it as `temperature_trends.png`.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_temperature_trends(file_path: str): # Load the dataset into a Pandas DataFrame df = pd.read_csv(file_path) # Set Seaborn style sns.set_theme(style=\\"whitegrid\\") # Create color palettes line_palette = sns.color_palette(\\"hls\\", 12) bg_palette = sns.light_palette(\\"blue\\", as_cmap=True) # Create the plot plt.figure(figsize=(12, 8)) sns.lineplot(data=df, x=\'Month\', y=\'Temperature\', hue=\'City\', palette=line_palette) # Customize the plot plt.title(\'Average Monthly Temperatures of Cities\') plt.xlabel(\'Month\') plt.ylabel(\'Temperature (Celsius)\') plt.legend(title=\'City\') plt.savefig(\'temperature_trends.png\') plt.show()"},{"question":"# Task: You are required to implement a class in Python that demonstrates your understanding of Python\'s data model, especially focusing on object comparisons, container behaviors, and custom attribute access. # Specifications: 1. **Class Name**: `MagicContainer` 2. **Attributes**: - The class should have an internal dictionary `_data` to store key-value pairs. 3. **Methods**: - `__init__(self)`: Initializes an empty `MagicContainer`. - `__setitem__(self, key, value)`: Adds or updates the `key-value` pair to `_data`. - `__getitem__(self, key)`: Retrieves the value associated with `key` from `_data`. - `__delitem__(self, key)`: Removes the `key-value` pair from `_data`. - `__contains__(self, key)`: Checks if `key` exists in `_data`. - `__len__(self)`: Returns the number of key-value pairs in `_data`. - `__eq__(self, other)`: Checks equality based on the `_data` dictionary. - `__repr__(self)`: Returns a string representation of the `MagicContainer`. - `__getattr__(self, name)`: If `name` is not found in the object’s `__dict__`, return \\"The attribute does not exist\\". # Constraints: 1. You are not allowed to use standard library containers such as `dict` directly except for internal storage within the class. 2. Keys are immutable types (e.g., strings, numbers). 3. Values can be any Python object. # Example Usage: ```python mc = MagicContainer() mc[\'x\'] = 10 mc[\'y\'] = 20 print(mc[\'x\']) # 10 print(len(mc)) # 2 del mc[\'y\'] print(\'y\' in mc) # False print(mc == MagicContainer()) # False print(mc.nonexistent) # The attribute does not exist ``` Implement the `MagicContainer` class as described.","solution":"class MagicContainer: def __init__(self): self._data = {} def __setitem__(self, key, value): self._data[key] = value def __getitem__(self, key): return self._data[key] def __delitem__(self, key): del self._data[key] def __contains__(self, key): return key in self._data def __len__(self): return len(self._data) def __eq__(self, other): if isinstance(other, MagicContainer): return self._data == other._data return False def __repr__(self): return str(self._data) def __getattr__(self, name): return \\"The attribute does not exist\\""},{"question":"**Objective:** Write a Python function that reads an email message from a file, extracts all email addresses mentioned in the message (including headers and body), and returns these addresses in a sorted list without duplicates. Use the `email.parser` module to parse the email and navigate through its components. **Function Signature:** ```python def extract_email_addresses(file_path: str) -> List[str]: pass ``` **Input:** - `file_path` (str): The path to the file containing the email message. The file will be in standard email format and can be either plain text or contain MIME parts. **Output:** - Returns a list of unique email addresses (strings) sorted in alphabetical order. **Constraints:** - The email message can contain multiple parts. - The extraction should consider email addresses in headers (e.g., `To`, `From`, `Cc`, `Bcc`) and the email body. - The function should handle both plain text and MIME email messages. - The function must be implemented using the `email.parser` module for parsing the email message. **Example:** Assume `email_message.eml` contains the following content: ``` From: user1@example.com To: user2@example.com, user3@example.com Cc: user4@example.com Subject: Test email Hello, Please contact support@example.com for assistance. Best, user1@example.com ``` Calling the function as follows: ```python addresses = extract_email_addresses(\'email_message.eml\') print(addresses) ``` Should output: ``` [\'support@example.com\', \'user1@example.com\', \'user2@example.com\', \'user3@example.com\', \'user4@example.com\'] ``` **Additional Notes:** - You may assume the input file exists and is readable. - Use the appropriate classes and methods from the `email.parser` module to parse the email message. - Consider using regular expressions to extract email addresses from the text content. **Hints:** - The `email.message_from_file` function might be useful for reading the email file. - Iterating through the email message parts can help in extracting addresses from different sections of the email. - Regular expressions can be used to find email addresses in strings. **Assessment Criteria:** - Correctness of the output. - Proper utilization of the `email.parser` module for parsing the email. - Efficiency and clarity of the implemented code. - Handling of both headers and body for email extraction.","solution":"import re from email import policy from email.parser import BytesParser from typing import List def extract_email_addresses(file_path: str) -> List[str]: # Regular expression to extract email addresses email_pattern = re.compile(r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,7}b\') email_addresses = set() def extract_from_content(content: str): found_emails = email_pattern.findall(content) email_addresses.update(found_emails) with open(file_path, \'rb\') as f: msg = BytesParser(policy=policy.default).parse(f) # Extract email addresses from headers headers = [\'from\', \'to\', \'cc\', \'bcc\'] for header in headers: if header in msg: extract_from_content(msg[header]) # Extract email addresses from the email body if msg.is_multipart(): for part in msg.iter_parts(): if part.get_content_type() == \'text/plain\' or part.get_content_type() == \'text/html\': extract_from_content(part.get_payload(decode=True).decode()) else: extract_from_content(msg.get_payload(decode=True).decode()) sorted_emails = sorted(email_addresses) return sorted_emails"},{"question":"**IMAP Client Implementation and Email Analysis** Using the `imaplib` library, implement an IMAP client in Python that connects to an IMAP email server, authenticates a user, and retrieves specific data about emails. The program should perform the following tasks: 1. Connect to the given IMAP server securely using the `IMAP4_SSL` class. 2. Authenticate using the provided username and password. 3. Select the `INBOX` mailbox. 4. Search for all emails that contain a specific keyword in the subject line. 5. For each matching email, retrieve the subject and sender\'s email address. 6. Print the retrieved subjects and sender\'s email addresses in a readable format. # Input - `server`: (string) The address of the IMAP server. - `port`: (int) The port number of the IMAP server. - `username`: (string) The username for authentication. - `password`: (string) The password for authentication. - `keyword`: (string) The keyword to search for in the subject line. # Output - Print each matching email\'s subject and sender\'s email address. # Constraints - Assume that all inputs are valid and the server is accessible. - Network errors and authentication failures should be handled gracefully by printing an appropriate error message. # Example ```python imap_client(\\"imap.example.com\\", 993, \\"user@example.com\\", \\"password\\", \\"Project Update\\") ``` Expected Output: ``` Subject: Project Update Status From: manager@example.com -- Subject: Presentation on Project Update From: colleague@example.com ``` # Notes - Use the `search` method to find emails matching the keyword. - Use the `fetch` method to retrieve the required parts of the email. - Use error handling to manage connectivity and authentication issues effectively. # Boilerplate Code This boilerplate will help you get started: ```python import imaplib import email from email.header import decode_header def imap_client(server, port, username, password, keyword): try: # Connect to the server mail = imaplib.IMAP4_SSL(server, port) # Authenticate mail.login(username, password) # Select the mailbox you want to use mail.select(\'inbox\') # Search for emails containing the specified keyword in the subject line status, messages = mail.search(None, \'(SUBJECT \\"{}\\")\'.format(keyword)) # Convert messages to a list of email IDs email_ids = messages[0].split() for email_id in email_ids: # Fetch the email by ID status, msg_data = mail.fetch(email_id, \'(RFC822)\') for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) # Decode the email subject subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \'utf-8\') # Get the email sender from_ = msg.get(\\"From\\") # Print the subject and sender print(f\'Subject: {subject}\') print(f\'From: {from_}\') print(\'--\') # Close the connection and logout mail.close() mail.logout() except imaplib.IMAP4.error as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": imap_client(\\"imap.example.com\\", 993, \\"user@example.com\\", \\"password\\", \\"Project Update\\") ```","solution":"import imaplib import email from email.header import decode_header def imap_client(server, port, username, password, keyword): try: # Connect to the server mail = imaplib.IMAP4_SSL(server, port) # Authenticate mail.login(username, password) # Select the mailbox you want to use mail.select(\'inbox\') # Search for emails containing the specified keyword in the subject line status, messages = mail.search(None, \'(SUBJECT \\"{}\\")\'.format(keyword)) if status != \\"OK\\": print(\\"No messages found!\\") return # Convert messages to a list of email IDs email_ids = messages[0].split() for email_id in email_ids: # Fetch the email by ID status, msg_data = mail.fetch(email_id, \'(RFC822)\') for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) # Decode the email subject subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \'utf-8\') # Get the email sender from_ = msg.get(\\"From\\") # Print the subject and sender print(f\'Subject: {subject}\') print(f\'From: {from_}\') print(\'--\') # Close the connection and logout mail.close() mail.logout() except imaplib.IMAP4.error as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": imap_client(\\"imap.example.com\\", 993, \\"user@example.com\\", \\"password\\", \\"Project Update\\")"},{"question":"# Question: Implementing a Custom Python Function Manager Objective This task will assess your understanding of Python\'s function objects and their properties. You will need to implement a Python class that mimics certain functionalities provided by Python\'s internal C API for handling function objects. Requirements 1. **Create a class `FunctionManager` that includes the following methods:** - `create_function(code: str, globals_: dict) -> callable`: Parses the given `code` (a string of Python code representing a function) and constructs a new function object with the provided `globals_` dictionary. The function\'s name and docstring should be derived from the code string. - `get_code(func: callable) -> str`: Returns the code string of the provided function object `func`. - `get_globals(func: callable) -> dict`: Returns the globals dictionary associated with the function object `func`. - `get_module(func: callable) -> str`: Returns the module attribute of the provided function object `func`. - `get_defaults(func: callable) -> tuple`: Returns the default argument values of the function object `func`. - `set_defaults(func: callable, defaults: tuple)`: Sets the default argument values for the function object `func`. - `get_annotations(func: callable) -> dict`: Returns the annotations of the function object `func`. - `set_annotations(func: callable, annotations: dict)`: Sets the annotations for the function object `func`. Constraints - The `code` parameter in `create_function` will be a string containing valid Python code defining a function. - The `globals_` dictionary will be valid and will contain the global variables needed to run the function. - The defaults parameter for `set_defaults` should be a tuple. - The annotations parameter for `set_annotations` should be a dictionary. Example: ```python manager = FunctionManager() code = def example_func(x: int, y: int = 5) -> int: \'\'\'This is an example function.\'\'\' return x + y globals_ = {} # Creating a new function object new_func = manager.create_function(code, globals_) # Getting the code of the new function print(manager.get_code(new_func)) # Should print the code string of example_func # Getting and setting the function defaults print(manager.get_defaults(new_func)) # Should print (5,) manager.set_defaults(new_func, (10,)) print(manager.get_defaults(new_func)) # Should print (10,) # Getting and setting function annotations print(manager.get_annotations(new_func)) # Should print {\'x\': <class \'int\'>, \'y\': <class \'int\'>, \'return\': <class \'int\'>} manager.set_annotations(new_func, {\'x\': int, \'y\': int, \'return\': float}) print(manager.get_annotations(new_func)) # Should print {\'x\': <class \'int\'>, \'y\': <class \'int\'>, \'return\': <class \'float\'>} ``` Please note that the actual implementation of `create_function` will use the `exec` function to compile the function from the string. Ensure your implementation passes various test cases based on the provided example.","solution":"import types class FunctionManager: def create_function(self, code: str, globals_: dict) -> callable: Parses the given code (a string of Python code representing a function) and constructs a new function object with the provided globals_ dictionary. exec(code, globals_) func_name = code.strip().split(\'n\')[0].split(\' \')[1].split(\'(\')[0] return globals_[func_name] def get_code(self, func: callable) -> str: Returns the code string of the provided function object func. return func.__code__.co_code def get_globals(self, func: callable) -> dict: Returns the globals dictionary associated with the function object func. return func.__globals__ def get_module(self, func: callable) -> str: Returns the module attribute of the provided function object func. return func.__module__ def get_defaults(self, func: callable) -> tuple: Returns the default argument values of the function object func. return func.__defaults__ def set_defaults(self, func: callable, defaults: tuple): Sets the default argument values for the function object func. func.__defaults__ = defaults def get_annotations(self, func: callable) -> dict: Returns the annotations of the function object func. return func.__annotations__ def set_annotations(self, func: callable, annotations: dict): Sets the annotations for the function object func. func.__annotations__ = annotations"},{"question":"# Password Hashing and Validation with the `crypt` Module Write a Python function `validate_password` that takes a username and a plain text password as input and returns `True` if the password is correct for the given username, and `False` otherwise. Assume that you have access to a predefined dictionary named `user_db` that stores usernames as keys and their corresponding hashed passwords as values. Your function should: 1. Use the `crypt` module to hash the input password. 2. Compare the hashed input password with the stored hashed password for the given username to check if they match. 3. Handle cases where the username does not exist in the `user_db`. Additionally, write a function `add_user` that takes a username and a plain text password, hashes the password using the strongest available method, and stores the username and hashed password in the `user_db` dictionary. Input: - `validate_password(username: str, password: str) -> bool` - `username`: A string representing the username. - `password`: A string representing the plain text password. - `add_user(username: str, password: str) -> None` - `username`: A string representing the username. - `password`: A string representing the plain text password. Output: - `validate_password` function should return a boolean: - `True` if the password matches the stored hashed password. - `False` otherwise. - `add_user` function should not return anything. It should update the `user_db` dictionary. Example: ```python user_db = {} def validate_password(username, password): # Implement this function pass def add_user(username, password): # Implement this function pass # Adding user add_user(\'alice\', \'securepassword\') # Validating password print(validate_password(\'alice\', \'securepassword\')) # Output: True print(validate_password(\'alice\', \'wrongpassword\')) # Output: False print(validate_password(\'bob\', \'somepassword\')) # Output: False ``` Constraints: - You should use the strongest available hashing method. - Handle any potential exceptions that could arise due to invalid inputs. - Ensure constant-time comparison to mitigate timing attacks.","solution":"import crypt import os user_db = {} def add_user(username, password): Adds a user to the user_db with a hashed password. Uses the strongest available hash method. # Generate a random salt with the strongest available method salt = crypt.mksalt(crypt.METHOD_SHA512) # Hash the password with the generated salt hashed_password = crypt.crypt(password, salt) # Store the username and hashed password in the user_db user_db[username] = hashed_password def validate_password(username, password): Validates a user\'s password against the stored hashed password. Returns True if the password matches, False otherwise. # Check if the username exists in the user_db if username not in user_db: return False # Retrieve the stored hashed password stored_hashed_password = user_db[username] # Hash the input password with the same salt from the stored hashed password hashed_password = crypt.crypt(password, stored_hashed_password) # Perform a constant-time comparison of the hashes return hashed_password == stored_hashed_password"},{"question":"**Objective**: Create a comprehensive visualization using seaborn that demonstrates the ability to handle and transform both long-form and wide-form data. **Task**: 1. Load the \\"tips\\" dataset available in seaborn. 2. Create two separate visualizations: - A line plot using long-form data, showing the total bill over days of the week, categorized by time (Dinner or Lunch). - A heatmap using wide-form data after transforming the dataset to show average total bills for each day across different times. **Detailed Steps**: 1. **Load the dataset**: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` 2. **Long-form data plot**: - Create a line plot (`sns.lineplot`) displaying the `total_bill` against `day`, categorized by `time` (Dinner or Lunch). - Customize the plot with appropriate titles and axis labels. 3. **Wide-form data transformation**: - Transform the long-form `tips` dataset into a wide-form dataset where rows represent `day` and columns represent `time`, with values being the average `total_bill`. - Create a heatmap (`sns.heatmap`) using this wide-form data. 4. **Implementation**: - Ensure to import necessary libraries (e.g., pandas, seaborn, matplotlib). - Properly handle any potential missing data during the transformation. - Include clear titles, labels, and legends where appropriate. **Input Format**: - The \\"tips\\" dataset is used directly from seaborn\'s datasets. **Output Format**: - Two visualizations: one line plot and one heatmap. **Constraints**: - Use seaborn and matplotlib for visualizations. - Ensure that the transformations and plots handle missing data appropriately. **Performance Requirements**: - The solution should handle the dataset in an efficient manner, making use of pandas for data transformation and seaborn for plotting. **Example**: Here is an example outline for the required tasks: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Long-form data plot plt.figure(figsize=(10, 6)) sns.lineplot(data=tips, x=\'day\', y=\'total_bill\', hue=\'time\') plt.title(\\"Total Bill Over Days of the Week (Categorized by Time)\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\'Time\') plt.show() # Wide-form data transformation tips_wide = tips.pivot_table(index=\'day\', columns=\'time\', values=\'total_bill\', aggfunc=\'mean\') # Heatmap plot plt.figure(figsize=(8, 6)) sns.heatmap(tips_wide, annot=True, fmt=\\".2f\\", cmap=\\"YlGnBu\\") plt.title(\\"Average Total Bill for Each Day (Categorized by Time)\\") plt.xlabel(\\"Time\\") plt.ylabel(\\"Day\\") plt.show() ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_visualizations(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Long-form data plot plt.figure(figsize=(10, 6)) sns.lineplot(data=tips, x=\'day\', y=\'total_bill\', hue=\'time\') plt.title(\\"Total Bill Over Days of the Week (Categorized by Time)\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\'Time\') plt.show() # Wide-form data transformation tips_wide = tips.pivot_table(index=\'day\', columns=\'time\', values=\'total_bill\', aggfunc=\'mean\') # Heatmap plot plt.figure(figsize=(8, 6)) sns.heatmap(tips_wide, annot=True, fmt=\\".2f\\", cmap=\\"YlGnBu\\") plt.title(\\"Average Total Bill for Each Day (Categorized by Time)\\") plt.xlabel(\\"Time\\") plt.ylabel(\\"Day\\") plt.show() # Main function call to create visualizations create_visualizations()"},{"question":"Objective Create a class that uses descriptors to validate attribute values upon assignment. You will implement custom validators using descriptors to restrict data types and allowed values. Problem You are tasked with creating a `Person` class that uses descriptors to validate its attributes. Your class should ensure that the name is a string of certain length, the age is a positive integer within a specific range, and the role is one of a selected set of roles. Details 1. Implement three descriptor classes: `StringValidator`, `IntegerValidator`, and `ChoiceValidator`. - `StringValidator`: Ensures the attribute is a string and its length is between specified minimum and maximum limits. - `IntegerValidator`: Ensures the attribute is an integer and its value is within a specified range. - `ChoiceValidator`: Ensures the attribute is one of the allowed choices. 2. Use these descriptors in the `Person` class to enforce constraints: - `name`: Must be a string between 2 and 50 characters. - `age`: Must be an integer between 0 and 150. - `role`: Must be one of the following values: \'Admin\', \'User\', \'Guest\'. Expected Input and Output - The class should raise: - `ValueError` for invalid values. - `TypeError` for incorrect types. Constraints - Solutions should only use standard Python libraries. - Ensure that your solution follows the Python 3.10 syntax and conventions. Example ```python class StringValidator: def __init__(self, min_length=None, max_length=None): self.min_length = min_length self.max_length = max_length def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, str): raise TypeError(f\\"Expected a string for {self.private_name}\\") if self.min_length and len(value) < self.min_length: raise ValueError(f\\"{self.private_name} must be at least {self.min_length} characters long.\\") if self.max_length and len(value) > self.max_length: raise ValueError(f\\"{self.private_name} must be at most {self.max_length} characters long.\\") setattr(obj, self.private_name, value) class IntegerValidator: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, int): raise TypeError(f\\"Expected an integer for {self.private_name}\\") if self.min_value and value < self.min_value: raise ValueError(f\\"{self.private_name} must be at least {self.min_value}.\\") if self.max_value and value > self.max_value: raise ValueError(f\\"{self.private_name} must be at most {self.max_value}.\\") setattr(obj, self.private_name, value) class ChoiceValidator: def __init__(self, *choices): self.choices = choices def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if value not in self.choices: raise ValueError(f\\"{self.private_name} must be one of {self.choices}.\\") setattr(obj, self.private_name, value) class Person: name = StringValidator(min_length=2, max_length=50) age = IntegerValidator(min_value=0, max_value=150) role = ChoiceValidator(\\"Admin\\", \\"User\\", \\"Guest\\") def __init__(self, name, age, role): self.name = name self.age = age self.role = role # Test cases try: p = Person(\\"Alice\\", 30, \\"Admin\\") print(f\\"Person created: {p.name}, {p.age}, {p.role}\\") p.role = \\"Superuser\\" # Should raise ValueError except ValueError as e: print(e) except TypeError as e: print(e) ```","solution":"class StringValidator: def __init__(self, min_length=None, max_length=None): self.min_length = min_length self.max_length = max_length def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, str): raise TypeError(f\\"Expected a string for {self.private_name}\\") if self.min_length is not None and len(value) < self.min_length: raise ValueError(f\\"{self.private_name} must be at least {self.min_length} characters long.\\") if self.max_length is not None and len(value) > self.max_length: raise ValueError(f\\"{self.private_name} must be at most {self.max_length} characters long.\\") setattr(obj, self.private_name, value) class IntegerValidator: def __init__(self, min_value=None, max_value=None): self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if not isinstance(value, int): raise TypeError(f\\"Expected an integer for {self.private_name}\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"{self.private_name} must be at least {self.min_value}.\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"{self.private_name} must be at most {self.max_value}.\\") setattr(obj, self.private_name, value) class ChoiceValidator: def __init__(self, *choices): self.choices = choices def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if value not in self.choices: raise ValueError(f\\"{self.private_name} must be one of {self.choices}.\\") setattr(obj, self.private_name, value) class Person: name = StringValidator(min_length=2, max_length=50) age = IntegerValidator(min_value=0, max_value=150) role = ChoiceValidator(\\"Admin\\", \\"User\\", \\"Guest\\") def __init__(self, name, age, role): self.name = name self.age = age self.role = role"}]'),D={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},q={class:"search-container"},F={class:"card-container"},z={key:0,class:"empty-state"},R=["disabled"],N={key:0},O={key:1};function L(n,e,l,m,s,r){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",q,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>s.searchQuery=o),placeholder:"Search..."},null,512),[[y,s.searchQuery]]),s.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>s.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),i(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),i("div",z,' No results found for "'+u(s.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:s.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[s.isLoading?(a(),i("span",O,"Loading...")):(a(),i("span",N,"See more"))],8,R)):d("",!0)])}const M=p(D,[["render",L],["__scopeId","data-v-ef4d4a5a"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/63.md","filePath":"quotes/63.md"}'),j={name:"quotes/63.md"},B=Object.assign(j,{setup(n){return(e,l)=>(a(),i("div",null,[x(M)]))}});export{Y as __pageData,B as default};
