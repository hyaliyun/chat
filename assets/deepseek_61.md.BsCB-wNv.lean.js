import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},E={class:"review"},C={class:"review-title"},P={class:"review-content"};function A(n,e,l,m,i,r){return a(),s("div",T,[t("div",E,[t("div",C,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const S=p(k,[["render",A],["__scopeId","data-v-e69cd130"]]),I=JSON.parse('[{"question":"# Multiclass and Multilabel Classification with Scikit-learn Objective Your task is to implement a multiclass classification and multilabel classification model using scikit-learn meta-estimators. This will assess your understanding of scikit-learn\'s multi-learning functionalities. Task 1: Multiclass Classification 1. **Implement the multiclass classification method with the OneVsRest strategy**: - Use the Iris dataset for multiclass classification. - The classifier to be used should be `LinearSVC`. - Split the dataset into training and testing datasets using a `80-20` split. - Output the classification accuracy on the test data. **Function signature**: ```python def multiclass_classification(): pass ``` **Expected Output**: ```python # Example output format { \\"accuracy_score\\": 0.95 } ``` Task 2: Multilabel Classification 2. **Implement the multilabel classification method**: - Use the following example dataset for multilabel classification: ```python X = [[1, 0], [2, 1], [3, 0], [4, 1], [5, 0]] Y = [[0, 1], [1, 0], [0, 1], [1, 1], [1, 0]] ``` - The classifier to be used should be `RandomForestClassifier`. - Utilize `MultiOutputClassifier` to handle the multilabel classification. - Fit your model on the given dataset and output the predictions for the same dataset. **Function signature**: ```python def multilabel_classification(): pass ``` **Expected Output**: ```python # Example output format { \\"predictions\\": [[0, 1], [1, 0], [0, 1], [1, 1], [1, 0]] } ``` # Constraints - Use scikit-learn version `0.24` or higher. - Ensure that the code is well-documented and follows good coding practices, including proper function definitions and return types. - Make sure your solution leverages the correct meta-estimators from `sklearn.multiclass` and `sklearn.multioutput`. # Performance - For Task 1, an accuracy above 90% is considered good. - For Task 2, the output should match the expected predictions given the toy dataset. Good luck!","solution":"from sklearn.datasets import load_iris from sklearn.svm import LinearSVC from sklearn.multiclass import OneVsRestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier def multiclass_classification(): iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) classifier = OneVsRestClassifier(LinearSVC(random_state=42, max_iter=10000)) classifier.fit(X_train, y_train) y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return {\\"accuracy_score\\": accuracy} def multilabel_classification(): X = [[1, 0], [2, 1], [3, 0], [4, 1], [5, 0]] Y = [[0, 1], [1, 0], [0, 1], [1, 1], [1, 0]] classifier = MultiOutputClassifier(RandomForestClassifier(random_state=42)) classifier.fit(X, Y) predictions = classifier.predict(X).tolist() return {\\"predictions\\": predictions}"},{"question":"**Problem: Asynchronous Task Manager** You are required to implement a simplified asynchronous task manager that executes multiple asynchronous tasks concurrently. You must use the asyncio library and handle specific exceptions defined in the asyncio documentation accordingly. # Requirements: 1. Implement an asynchronous function `perform_task` that simulates performing a task: - Accepts a parameter `task_id` (an integer) which identifies the task. - Randomly raises one of the following exceptions based on predefined probabilities: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` - Completes successfully if no exception is raised, and returns a string indicating the task\'s completion, e.g., `\\"Task <id> completed successfully\\"`. 2. Implement an asynchronous function `run_tasks` that concurrently manages multiple `perform_task` functions: - Accepts a parameter `n_tasks` which is the number of tasks to run. - Uses `asyncio.gather` to run all tasks concurrently. - Catches and handles each of the exceptions listed above, printing an appropriate message indicating the nature of the exception and continuing with the other tasks. - Returns a list of results for each task, containing either the success message or the exception message. 3. Ensure that `run_tasks` can handle exceptions without terminating prematurely and collects the outcomes of all tasks. # Input: - `n_tasks` (integer): The number of tasks to run concurrently. # Output: - A list of strings, each string being the result of a single task. # Example: ```python import asyncio async def main(): results = await run_tasks(5) for result in results: print(result) # Sample output could be: # Task 1 completed successfully # Task 2 failed due to TimeoutError # Task 3 failed due to CancelledError # Task 4 completed successfully # Task 5 failed due to IncompleteReadError # Run the main function asyncio.run(main()) ``` # Constraints: - You should use appropriate exception handling to manage the specific exceptions listed in the documentation. - The tasks should be executed concurrently to demonstrate the use of asyncio\'s capabilities.","solution":"import asyncio import random async def perform_task(task_id): Simulates performing a task. Randomly raises an exception or completes successfully. exceptions = [ asyncio.TimeoutError, asyncio.CancelledError, asyncio.InvalidStateError, asyncio.IncompleteReadError, asyncio.LimitOverrunError ] probabilities = [0.2, 0.2, 0.2, 0.2, 0.1] # Sum is <1 to allow for success exception = random.choices(exceptions + [None], probabilities + [1-sum(probabilities)])[0] await asyncio.sleep(random.uniform(0.1, 1)) # Simulate some work if exception: raise exception() return f\\"Task {task_id} completed successfully\\" async def run_tasks(n_tasks): Manages and runs multiple perform_task functions concurrently, and handles exceptions. tasks = [perform_task(i) for i in range(1, n_tasks + 1)] results = [] for task in asyncio.as_completed(tasks): try: result = await task results.append(result) except asyncio.TimeoutError: results.append(\\"TimeoutError encountered\\") except asyncio.CancelledError: results.append(\\"CancelledError encountered\\") except asyncio.InvalidStateError: results.append(\\"InvalidStateError encountered\\") except asyncio.IncompleteReadError: results.append(\\"IncompleteReadError encountered\\") except asyncio.LimitOverrunError: results.append(\\"LimitOverrunError encountered\\") return results"},{"question":"# Text-Based Spreadsheet Editor Using `curses` Objective Implement a text-based spreadsheet editor using the Python `curses` module. The spreadsheet will display cells in a grid and allow users to navigate between cells, edit cell contents, and save the data to a file. Requirements 1. **Initialization and Setup**: - Initialize the `curses` library. - Set up a main window with a specific size (e.g., 20 rows by 50 columns). - Enable color functionality. 2. **Grid Display**: - Display cells in a grid format with row and column headers. - Highlight the currently selected cell. 3. **Navigation**: - Allow users to navigate between cells using arrow keys. - Ensure the cursor does not move outside the grid boundary. 4. **Editing Cell Content**: - Allow users to enter and edit text in the cells. - Use `ENTER` key to confirm the input in a cell. 5. **Save to File**: - Provide a way to save the current spreadsheet data to a text file (e.g., pressing \'s\' key). 6. **Termination**: - Cleanly exit the `curses` program and restore the terminal to normal mode. Input and Output - **Input**: - Arrow keys to navigate. - Alphanumeric keys to edit cell content. - \'s\' key to save the spreadsheet data to a text file. - \'q\' key to exit the application. - **Output**: - A text file with the saved spreadsheet data. Constraints 1. The grid should be a fixed size of 10 x 10 cells. 2. Each cell can hold up to 5 characters of text. 3. Use `curses` functions and methods wherever applicable. Code Skeleton Below is a partial implementation to get you started: ```python import curses def init_screen(): stdscr = curses.initscr() curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) curses.curs_set(0) return stdscr def draw_grid(stdscr, cursor_y, cursor_x, data): stdscr.clear() for y in range(11): for x in range(11): if y == 0 and x > 0: stdscr.addstr(y, x*6, f\\"{x-1}\\", curses.A_BOLD) elif x == 0 and y > 0: stdscr.addstr(y, x*6, f\\"{y-1}\\", curses.A_BOLD) elif y > 0 and x > 0: cell_content = data[y-1][x-1] cell_attr = curses.color_pair(1) if (cursor_y == y-1 and cursor_x == x-1) else curses.A_NORMAL stdscr.addstr(y, x*6, f\\"{cell_content:<5}\\", cell_attr) stdscr.refresh() def main(stdscr): stdscr = init_screen() cursor_y, cursor_x = 0, 0 data = [[\\"\\" for _ in range(10)] for _ in range(10)] while True: draw_grid(stdscr, cursor_y, cursor_x, data) key = stdscr.getch() if key == curses.KEY_UP and cursor_y > 0: cursor_y -= 1 elif key == curses.KEY_DOWN and cursor_y < 9: cursor_y += 1 elif key == curses.KEY_LEFT and cursor_x > 0: cursor_x -= 1 elif key == curses.KEY_RIGHT and cursor_x < 9: cursor_x += 1 elif key in [ord(\'q\'), ord(\'Q\')]: break elif key == ord(\'s\'): with open(\\"spreadsheet.txt\\", \\"w\\") as f: for row in data: f.write(\\"t\\".join(row) + \\"n\\") stdscr.addstr(12, 0, \\"Data saved to spreadsheet.txt\\") stdscr.refresh() elif key == 10: # Enter key stdscr.addstr(12, 0, \\"Enter value: \\") curses.echo() value = stdscr.getstr(12, 13, 5).decode(\'utf-8\') curses.noecho() data[cursor_y][cursor_x] = value else: pass curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main) ``` Notes 1. Make sure to handle edge cases (e.g., preventing cursor from going out of bounds, ensuring proper display updates). 2. Test the program rigorously to ensure it meets the outlined requirements. If you have any questions, feel free to ask for further clarification.","solution":"import curses def init_screen(): stdscr = curses.initscr() curses.start_color() curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE) curses.curs_set(0) return stdscr def draw_grid(stdscr, cursor_y, cursor_x, data): stdscr.clear() for y in range(11): for x in range(11): if y == 0 and x > 0: stdscr.addstr(y, x*6, f\\"{x-1}\\", curses.A_BOLD) elif x == 0 and y > 0: stdscr.addstr(y, x*6, f\\"{y-1}\\", curses.A_BOLD) elif y > 0 and x > 0: cell_content = data[y-1][x-1] cell_attr = curses.color_pair(1) if (cursor_y == y-1 and cursor_x == x-1) else curses.A_NORMAL stdscr.addstr(y, x*6, f\\"{cell_content:<5}\\", cell_attr) stdscr.refresh() def main(stdscr): stdscr = init_screen() cursor_y, cursor_x = 0, 0 data = [[\\"\\" for _ in range(10)] for _ in range(10)] while True: draw_grid(stdscr, cursor_y, cursor_x, data) key = stdscr.getch() if key == curses.KEY_UP and cursor_y > 0: cursor_y -= 1 elif key == curses.KEY_DOWN and cursor_y < 9: cursor_y += 1 elif key == curses.KEY_LEFT and cursor_x > 0: cursor_x -= 1 elif key == curses.KEY_RIGHT and cursor_x < 9: cursor_x += 1 elif key in [ord(\'q\'), ord(\'Q\')]: break elif key == ord(\'s\'): with open(\\"spreadsheet.txt\\", \\"w\\") as f: for row in data: f.write(\\"t\\".join(row) + \\"n\\") stdscr.addstr(12, 0, \\"Data saved to spreadsheet.txt\\") stdscr.refresh() elif key == 10: # Enter key stdscr.addstr(12, 0, \\"Enter value: \\") curses.echo() value = stdscr.getstr(12, 13, 5).decode(\'utf-8\') curses.noecho() data[cursor_y][cursor_x] = value else: pass curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"**Objective**: Demonstrate your understanding of out-of-core learning and incremental model training using scikit-learn. # Problem Statement: You are given a stream of text data for spam email classification. Due to the large volume of emails, it is not feasible to load all data into memory. Your task is to build a system that can classify emails as spam or not using out-of-core learning. # Requirements: 1. **Streaming Instances**: Implement a generator function that simulates the streaming of email data from a file. 2. **Feature Extraction**: Use `HashingVectorizer` to convert email text into feature vectors. 3. **Incremental Learning**: Train an incremental classifier (e.g., `SGDClassifier`) using partial_fit. # Detailed Requirements: 1. **Data Streaming**: - Implement a generator function `stream_emails(file_path)` that reads emails from a file line-by-line. Each line contains a label (0 for non-spam, 1 for spam) and the email text separated by a tab character. The function should yield tuples of (label, email_text). 2. **Feature Vectorization**: - Use `HashingVectorizer` from `sklearn.feature_extraction.text` to transform email texts into feature vectors. Set `n_features` to 2**18. 3. **Model Training**: - Use `SGDClassifier` from `sklearn.linear_model` for incremental training. Make sure to initialize the classifier with `loss=\'log\'` for logistic regression. - Ensure that all possible classes `[0, 1]` are passed to the `partial_fit` during the first call. # Input and Output Format: - **Input**: - A file `emails.txt` where each line contains a label and email text separated by a tab character. - **Output**: - The function should print the classification accuracy after processing each mini-batch of 1000 emails. # Constraints and Performance: - **Memory Constraint**: The solution should be able to process data without loading the entire dataset into memory. - **Performance Requirement**: The mini-batch size is set to 1000. Ensure an efficient implementation to handle high volumes of data. # Implementation: ```python import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_emails(file_path): Generator function to stream emails from a file. Args: file_path (str): Path to the file containing email data. Yields: tuple: Label and email text. with open(file_path, \'r\') as file: for line in file: label, email_text = line.strip().split(\'t\') yield int(label), email_text def train_spam_classifier(file_path): vectorizer = HashingVectorizer(n_features=2**18) classifier = SGDClassifier(loss=\'log\', learning_rate=\'optimal\') batches = [] for idx, (label, email_text) in enumerate(stream_emails(file_path)): batches.append((label, email_text)) if (idx + 1) % 1000 == 0: labels, emails = zip(*batches) X = vectorizer.transform(emails) y = np.array(labels) if idx == 999: classifier.partial_fit(X, y, classes=np.array([0, 1])) else: classifier.partial_fit(X, y) # Reset batches batches = [] # Calculate and print accuracy predictions = classifier.predict(X) accuracy = accuracy_score(y, predictions) print(f\'Processed {idx + 1} emails - Accuracy: {accuracy:.4f}\') # Example usage train_spam_classifier(\'emails.txt\') ``` **Note**: Ensure the file `emails.txt` is correctly formatted and placed in the correct directory before running the code.","solution":"import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def stream_emails(file_path): Generator function to stream emails from a file. Args: file_path (str): Path to the file containing email data. Yields: tuple: Label and email text. with open(file_path, \'r\') as file: for line in file: label, email_text = line.strip().split(\'t\') yield int(label), email_text def train_spam_classifier(file_path): vectorizer = HashingVectorizer(n_features=2**18) classifier = SGDClassifier(loss=\'log\', learning_rate=\'optimal\') batches = [] for idx, (label, email_text) in enumerate(stream_emails(file_path)): batches.append((label, email_text)) if (idx + 1) % 1000 == 0: labels, emails = zip(*batches) X = vectorizer.transform(emails) y = np.array(labels) if idx == 999: classifier.partial_fit(X, y, classes=np.array([0, 1])) else: classifier.partial_fit(X, y) # Reset batches batches = [] # Calculate and print accuracy predictions = classifier.predict(X) accuracy = accuracy_score(y, predictions) print(f\'Processed {idx + 1} emails - Accuracy: {accuracy:.4f}\') # Example usage # train_spam_classifier(\'emails.txt\')"},{"question":"# Email Message Construction and Parsing Objective Your task is to create a function that constructs an email message and then parses it to verify its contents using Python\'s `email` package. Function Signature ```python def construct_and_parse_email(sender: str, recipient: str, subject: str, body: str) -> dict: pass ``` Input * `sender` (str): The email address of the sender. * `recipient` (str): The email address of the recipient. * `subject` (str): The subject of the email. * `body` (str): The body text of the email. Output * Returns a dictionary with the following keys: * `\\"From\\"`: The sender\'s email address. * `\\"To\\"`: The recipient\'s email address. * `\\"Subject\\"`: The subject of the email. * `\\"Body\\"`: The body text of the email. Constraints 1. The email should be created using the `email.message.EmailMessage` class. 2. The email should be parsed using the `email.parser.Parser` class. 3. Assume that input strings will be valid email addresses and non-empty texts. Example ```python sender = \\"user@example.com\\" recipient = \\"friend@example.com\\" subject = \\"Hello!\\" body = \\"This is a test email.\\" result = construct_and_parse_email(sender, recipient, subject, body) ``` Expected Output: ```python { \\"From\\": \\"user@example.com\\", \\"To\\": \\"friend@example.com\\", \\"Subject\\": \\"Hello!\\", \\"Body\\": \\"This is a test email.\\" } ``` Notes 1. Use the `EmailMessage` class from `email.message` to create the email. 2. Use the `Parser` class from `email.parser` to parse the constructed email into its components. 3. Ensure that all key parts of the email (From, To, Subject, and Body) are correctly set and parsed. This question evaluates students\' ability to construct and manipulate email messages using the `email` package, demonstrating their understanding of object initialization, manipulation, and parsing with Python\'s built-in libraries.","solution":"from email.message import EmailMessage from email.parser import Parser def construct_and_parse_email(sender: str, recipient: str, subject: str, body: str) -> dict: # Construct the email message email = EmailMessage() email[\'From\'] = sender email[\'To\'] = recipient email[\'Subject\'] = subject email.set_content(body) # Parse the email message parser = Parser() parsed_email = parser.parsestr(email.as_string()) # Return the parsed email components in a dictionary return { \\"From\\": parsed_email[\'From\'], \\"To\\": parsed_email[\'To\'], \\"Subject\\": parsed_email[\'Subject\'], \\"Body\\": parsed_email.get_payload() }"},{"question":"You are required to implement a custom priority queue using the `heapq` module. This priority queue must support the following operations efficiently: 1. **add_task(priority, task)**: Adds a new task or updates the priority of an existing task. 2. **remove_task(task)**: Removes a task from the priority queue. 3. **pop_task()**: Pops and returns the task with the highest priority (lowest numerical value). Your priority queue should be stable, meaning that for tasks with equal priority, the task added first should be popped first. # Constraints - The priority queue can contain up to (10^5) tasks. - Task priorities are integers. Lower integer values represent higher priorities. # Input Format You will write a class `PriorityQueue` with methods as described above. 1. `add_task(priority: int, task: str) -> None` - `priority` - the priority of the task (an integer). - `task` - the description of the task (a string). 2. `remove_task(task: str) -> None` - `task` - the description of the task to be removed (a string). 3. `pop_task() -> str` - Returns the task with the highest priority (the lowest numerical value). # Example Usage ```python # Initialize PriorityQueue pq = PriorityQueue() # Add tasks pq.add_task(3, \\"create tests\\") pq.add_task(1, \\"write spec\\") pq.add_task(5, \\"write code\\") pq.add_task(7, \\"release product\\") # Update priority of an existing task pq.add_task(0, \\"release product\\") # Pop task with the highest priority print(pq.pop_task()) # Output: \\"release product\\" # Remove a task pq.remove_task(\\"write spec\\") # Pop task with the highest priority print(pq.pop_task()) # Output: \\"create tests\\" ``` You may assume that all operations are called in a valid sequence. # Requirements - Implement the `add_task`, `remove_task`, and `pop_task` methods. - Ensure that the implementation uses the `heapq` module functions to maintain heap properties. - Maintain stability for tasks with the same priority. # Notes - You can use a dictionary to map tasks to their positions in the heap for efficient updates and deletions. - Handle task removals by marking them as removed and ensuring they are properly ignored during heap operations.","solution":"import heapq class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = 0 # unique sequence count to maintain stability def add_task(self, priority, task): \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = self.counter entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) self.counter += 1 def remove_task(self, task): \'Mark an existing task as REMOVED. Raise KeyError if not found.\' entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self): \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"# Question You have been given the task to create a utility script using Python\'s built-in `compileall` module. The goal is to recursively compile all Python files in a specified directory while providing several user-defined options via a command-line interface. Requirements: 1. Implement a Python function named `compile_directory` that wraps around `compileall.compile_dir` to: - Compile all `.py` files in a given directory. - Control the depth of recursion. - Use multiple threads for compilation. - Enable/disable the force recompilation option. 2. Design a command-line interface allowing users to specify the directory, recursion depth, number of threads, and whether to force recompilation. Function Signature ```python def compile_directory(directory: str, max_levels: int = None, workers: int = 1, force: bool = False) -> bool: Compiles all .py files in the given directory based on the specified options. Parameters: directory (str): The path of the directory to compile. max_levels (int, optional): The maximum depth of recursion. Defaults to system recursion limit. workers (int, optional): Number of threads to use. Defaults to 1. force (bool, optional): Force recompilation even if timestamps are up-to-date. Defaults to False. Returns: bool: True if all files compile successfully, False otherwise. ``` Command-Line Interface - Use `argparse` to handle command-line arguments. - The script should include the following command-line options: - `--directory`: The path of the directory to compile (required). - `--max-levels`: The maximum depth of recursion (optional). - `--workers`: Number of threads to use (optional, default to 1). - `--force`: Force recompilation even if timestamps are up-to-date (optional, default to False). Additional Constraints - Assume the script will run in an environment where `compileall` is available. - The directory path provided should be valid and accessible; handle errors appropriately. # Example Usage ```sh python compile_script.py --directory path/to/directory --max-levels 3 --workers 4 --force ``` # Example Solution ```python import compileall import argparse def compile_directory(directory: str, max_levels: int = None, workers: int = 1, force: bool = False) -> bool: if max_levels is None: max_levels = sys.getrecursionlimit() return compileall.compile_dir( dir=directory, maxlevels=max_levels, workers=workers, force=force ) def main(): parser = argparse.ArgumentParser(description=\\"Compile Python files in a directory.\\") parser.add_argument(\'--directory\', type=str, required=True, help=\\"The directory to compile.\\") parser.add_argument(\'--max-levels\', type=int, default=sys.getrecursionlimit(), help=\\"The maximum depth of recursion.\\") parser.add_argument(\'--workers\', type=int, default=1, help=\\"Number of threads to use.\\") parser.add_argument(\'--force\', action=\'store_true\', help=\\"Force recompilation.\\") args = parser.parse_args() successful = compile_directory( directory=args.directory, max_levels=args.max_levels, workers=args.workers, force=args.force ) if successful: print(\\"Compilation succeeded.\\") else: print(\\"Compilation failed.\\") if __name__ == \\"__main__\\": main() ```","solution":"import compileall import argparse import sys def compile_directory(directory: str, max_levels: int = None, workers: int = 1, force: bool = False) -> bool: if max_levels is None: max_levels = sys.getrecursionlimit() return compileall.compile_dir( dir=directory, maxlevels=max_levels, workers=workers, force=force ) def main(): parser = argparse.ArgumentParser(description=\\"Compile Python files in a directory.\\") parser.add_argument(\'--directory\', type=str, required=True, help=\\"The directory to compile.\\") parser.add_argument(\'--max-levels\', type=int, default=sys.getrecursionlimit(), help=\\"The maximum depth of recursion.\\") parser.add_argument(\'--workers\', type=int, default=1, help=\\"Number of threads to use.\\") parser.add_argument(\'--force\', action=\'store_true\', help=\\"Force recompilation.\\") args = parser.parse_args() successful = compile_directory( directory=args.directory, max_levels=args.max_levels, workers=args.workers, force=args.force ) if successful: print(\\"Compilation succeeded.\\") else: print(\\"Compilation failed.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** To assess your understanding of the seaborn library, particularly the `sns.clustermap` function for creating clustered heatmaps with various customizations. **Question:** You are given a dataset containing information about different species of flowers, similar to the famous Iris dataset. Your task is to create a clustered heatmap using the seaborn library that meets the following criteria: 1. **Load the Dataset:** Load a dataset named `flower_data.csv` into a pandas DataFrame. This dataset contains columns `sepal_length`, `sepal_width`, `petal_length`, `petal_width`, and `species`. 2. **Clustered Heatmap:** - Remove the `species` column from the dataset and use the rest of the data to create a clustered heatmap. - Add a color-coded row label for each species. - Standardize the data within the columns before plotting. - Use a different colormap (`\\"coolwarm\\"`) for the heatmap. - Adjust the figure size to 10x8. - Set the minimum and maximum values of the color range to -1 and 1, respectively. 3. **Output:** The resulting plot should be displayed without saving it to a file. **Constraints:** - The dataset contains no missing values. You can assume all necessary libraries are pre-installed and available. **Performance Requirements:** - The function should execute within a reasonable time frame for a dataset with up to 10,000 rows. **Input Format:** ```plaintext flower_data.csv ``` An example of the first few rows of the dataset might look like this: ```plaintext sepal_length,sepal_width,petal_length,petal_width,species 5.1,3.5,1.4,0.2,setosa 4.9,3.0,1.4,0.2,setosa ... ``` **Function Signature:** ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_clustered_heatmap(file_path: str): # Your code here pass ``` **Example Usage:** ```python create_clustered_heatmap(\'flower_data.csv\') ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_clustered_heatmap(file_path: str): # Load the dataset df = pd.read_csv(file_path) # Remove the \'species\' column features = df.drop(columns=[\'species\']) # Standardize the data within the columns before plotting standardized_data = (features - features.mean()) / features.std() # Create a palette to color the species rows unique_species = df[\'species\'].unique() lut = dict(zip(unique_species, sns.color_palette(\\"husl\\", len(unique_species)))) species_colors = df[\'species\'].map(lut) # Create the clustered heatmap sns.clustermap( standardized_data, method=\'average\', metric=\'euclidean\', row_colors=species_colors, cmap=\'coolwarm\', figsize=(10, 8), vmin=-1, vmax=1 ) # Display the plot plt.show()"},{"question":"# PyTorch Neural Network Parameter Initialization In this exercise, you will create a PyTorch neural network and initialize its parameters using different initialization functions provided in the `torch.nn.init` module. You need to demonstrate your understanding of these initialization functions by implementing a custom initialization function. Task 1. **Define a Neural Network**: - Create a simple neural network with the following architecture: - An input layer with 10 input features. - A hidden layer with 20 neurons, followed by a ReLU activation function. - An output layer with 5 output features. 2. **Custom Initialization Function**: - Implement a custom function `initialize_weights` that takes a neural network and initializes its parameters using the following strategy: - Initialize all weights of linear layers with the `xavier_uniform_` initialization. - Initialize all biases of linear layers with the `zeros_` initialization. 3. **Application of Initialization**: - Apply the `initialize_weights` function to the neural network defined in step 1. Input - The function you write will not take any direct input from the user. Instead, it will work with the neural network model directly within the code. Output - The function should print out the weights and biases of each layer after the initialization to verify correctness. Constraints - Use only the initialization functions provided in the `torch.nn.init` module. - Do not change the architecture of the provided neural network. Example Usage ```python import torch import torch.nn as nn import torch.nn.init as init class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def initialize_weights(model): for m in model.modules(): if isinstance(m, nn.Linear): init.xavier_uniform_(m.weight) init.zeros_(m.bias) # Create the neural network model = SimpleNN() # Apply custom initialization initialize_weights(model) # Print out the initialized parameters for name, param in model.named_parameters(): if param.requires_grad: print(name, param.data) ``` Make sure your code follows the example usage format and prints out the correct initialized parameters.","solution":"import torch import torch.nn as nn import torch.nn.init as init class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def initialize_weights(model): for m in model.modules(): if isinstance(m, nn.Linear): init.xavier_uniform_(m.weight) init.zeros_(m.bias) # Create the neural network model = SimpleNN() # Apply custom initialization initialize_weights(model) # Print out the initialized parameters for name, param in model.named_parameters(): if param.requires_grad: print(name, param.data)"},{"question":"# Logging System Implementation in Python In this assessment, you are expected to demonstrate your understanding of Pythonâ€™s `logging` module by building a custom logging system for hierarchical logging in a Python application. You need to implement a logging configuration for a hypothetical application with the following requirements: 1. **Logging Levels**: - Use different logging levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, and `CRITICAL`) accurately to log messages with different severities. 2. **Child Loggers**: - Create a hierarchical logging system, where loggers can inherit properties from their parent loggers. - Ensure child loggers propagate messages to ancestor loggers. 3. **Handlers and Formatters**: - Define custom handlers that direct logs to different destinations (e.g., console and file). - Implement formatters to specify custom log message formats. Your task is to write a Python script, `custom_logging.py`, that configures and demonstrates the above functionalities. The script should include the following functions: 1. `configure_logging()`: - Configure the root logger with a handler that outputs to the console. - Create a file handler that logs messages to a file `application.log`. - Set appropriate logging levels for the root logger and handlers. - Use different formatters for console and file handlers. 2. `create_child_logger(name)`: - Return a child logger with the specified name, ensuring it inherits properties from its parent. 3. Demonstration: - Create and configure the root logger. - Create child loggers named `app.module1` and `app.module1.submodule`. - Log messages with different severities using these loggers and observe the message propagation and outputs in both the console and the file. # Constraints: - Ensure that no duplicate log messages are produced in the outputs. - Make sure the loggers propagate messages correctly following the hierarchical structure. - Format logs in the file with timestamps and log levels. # Input and Output: The script should not require any input but should produce log outputs in the console and a file named `application.log`. # Example usage: ```python # Call the functions to demonstrate logging if __name__ == \\"__main__\\": configure_logging() logger1 = create_child_logger(\'app.module1\') logger2 = create_child_logger(\'app.module1.submodule\') logger1.debug(\'This is a debug message from module1.\') logger1.info(\'This is an info message from module1.\') logger2.warning(\'This is a warning message from submodule.\') logger2.error(\'This is an error message from submodule.\') logger2.critical(\'This is a critical message from submodule.\') ``` The expected output should show appropriately formatted log messages in both the console and the `application.log` file.","solution":"import logging def configure_logging(): # Create the root logger root_logger = logging.getLogger() root_logger.setLevel(logging.DEBUG) # Create console handler for the root logger console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # Create file handler for the root logger file_handler = logging.FileHandler(\'application.log\') file_handler.setLevel(logging.DEBUG) # Create formatters console_formatter = logging.Formatter(\'%(name)s - %(levelname)s - %(message)s\') file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # Add formatters to handlers console_handler.setFormatter(console_formatter) file_handler.setFormatter(file_formatter) # Add handlers to root logger root_logger.addHandler(console_handler) root_logger.addHandler(file_handler) def create_child_logger(name): # Create a child logger that inherits properties from the root logger return logging.getLogger(name) # Main demonstration function if __name__ == \\"__main__\\": configure_logging() logger1 = create_child_logger(\'app.module1\') logger2 = create_child_logger(\'app.module1.submodule\') logger1.debug(\'This is a debug message from module1.\') logger1.info(\'This is an info message from module1.\') logger2.warning(\'This is a warning message from submodule.\') logger2.error(\'This is an error message from submodule.\') logger2.critical(\'This is a critical message from submodule.\')"},{"question":"Advanced Window Operations in Pandas Objective: Demonstrate your comprehension of pandas\' window operations by implementing a series of functions that analyze a given dataset using rolling, expanding, and exponentially weighted windows. Problem Statement: You are given a time series dataset containing daily stock prices for a particular company. Your task is to implement the following functions: 1. **calculate_moving_average(prices: pd.Series, window: int) -> pd.Series:** - Compute the simple moving average (SMA) of the stock prices using a rolling window. - Input: `prices` (pandas Series of stock prices), `window` (integer window size). - Output: pandas Series of SMA values. 2. **calculate_expanding_max(prices: pd.Series) -> pd.Series:** - Compute the expanding maximum of the stock prices. - Input: `prices` (pandas Series of stock prices). - Output: pandas Series of expanding maximum values. 3. **calculate_weighted_moving_average(prices: pd.Series, span: int) -> pd.Series:** - Compute the exponentially weighted moving average (EWMA) of the stock prices with a given span. - Input: `prices` (pandas Series of stock prices), `span` (integer span). - Output: pandas Series of EWMA values. 4. **calculate_custom_window_mean(prices: pd.Series) -> pd.Series:** - Use a custom window indexer that calculates the mean price within a window defined by a fixed number of forward days. - Input: `prices` (pandas Series of stock prices). - Output: pandas Series of custom window mean values. - Note: For simplicity, assume a forward-looking window of size 3 days. Constraints: - Implement the functions without using external libraries other than pandas. - The input series `prices` will have at least 30 days of data. - NaN values in windows should be properly handled, maintaining compatibility with pandas\' default behavior. Performance Requirements: - Each function should execute in linear time relative to the length of the input series, O(n). # Example Usage: ```python import pandas as pd # Sample data data = { \'Date\': pd.date_range(start=\'2023-01-01\', periods=30, freq=\'D\'), \'Price\': [150 + i*1.5 for i in range(30)] } prices = pd.Series(data[\'Price\'], index=data[\'Date\']) # Function calls sma = calculate_moving_average(prices, window=5) expanding_max = calculate_expanding_max(prices) ewma = calculate_weighted_moving_average(prices, span=5) custom_mean = calculate_custom_window_mean(prices) print(\\"Simple Moving Average:n\\", sma) print(\\"Expanding Maximum:n\\", expanding_max) print(\\"Exponentially Weighted Moving Average:n\\", ewma) print(\\"Custom Window Mean:n\\", custom_mean) ``` Notes: - Make sure to validate your output against known results for small, hand-calculated data sets. - Provide docstrings for your functions explaining the parameters and the returned results. Good luck and happy coding!","solution":"import pandas as pd def calculate_moving_average(prices: pd.Series, window: int) -> pd.Series: Compute the simple moving average (SMA) of the stock prices using a rolling window. :param prices: pd.Series of stock prices :param window: int window size :return: pd.Series of SMA values return prices.rolling(window=window).mean() def calculate_expanding_max(prices: pd.Series) -> pd.Series: Compute the expanding maximum of the stock prices. :param prices: pd.Series of stock prices :return: pd.Series of expanding maximum values return prices.expanding().max() def calculate_weighted_moving_average(prices: pd.Series, span: int) -> pd.Series: Compute the exponentially weighted moving average (EWMA) of the stock prices with a given span. :param prices: pd.Series of stock prices :param span: int span :return: pd.Series of EWMA values return prices.ewm(span=span).mean() def calculate_custom_window_mean(prices: pd.Series) -> pd.Series: Calculate the mean price within a window defined by a fixed number of forward days. :param prices: pd.Series of stock prices :return: pd.Series of custom window mean values def forward_mean(data, window=3): means = [] for i in range(len(data)): if i + window <= len(data): means.append(data[i:i+window].mean()) else: means.append(data[i:].mean()) return pd.Series(means, index=data.index) return forward_mean(prices)"},{"question":"# PyTorch Coding Assessment Objective: Your task is to demonstrate the ability to implement and utilize the `CausalBias` class and its related methods in PyTorch. Specifically, you will create a custom attention mechanism that employs these causal biases and test it on a sample sequence of data. Background: Causal bias in attention mechanisms is critical for tasks involving sequence data, such as language modeling, where the prediction at any time step should only depend on the current and past time steps, not the future ones. Task: 1. **Create a Custom Attention Mechanism**: - Implement a class called `CustomCausalAttention` that includes a method to apply causal bias using the `CausalBias` class. - The class should initialize with a specified sequence length and use the appropriate methods to apply causal biases. 2. **Apply the Attention Mechanism**: - Use your `CustomCausalAttention` class to process a given sequence of data and return the attention-biased sequence. 3. **Test the Implementation**: - Provide a simple test using a dummy sequence to demonstrate that your `CustomCausalAttention` correctly applies a causal bias. Requirements: - The input to your `CustomCausalAttention` class should be a PyTorch tensor of shape `(batch_size, sequence_length, embedding_dim)`. - The output should be a tensor of the same shape with the causal bias applied. - Ensure your implementation is efficient and leverages PyTorch functionalities appropriately. # Input: - A tensor `x` of shape `(batch_size, sequence_length, embedding_dim)`. # Output: - A tensor of the same shape `(batch_size, sequence_length, embedding_dim)` with causal bias applied. # Constraints: - The sequence length will not exceed 100. - The embedding dimension is fixed at 64. # Example: ```python import torch from torch.nn.attention.bias import CausalBias class CustomCausalAttention: def __init__(self, sequence_length): self.sequence_length = sequence_length self.causal_bias = CausalBias() def apply_causal_bias(self, x): # Implement causal bias application logic here pass # Initialize a dummy sequence x = torch.randn(32, 10, 64) # batch_size=32, sequence_length=10, embedding_dim=64 attention = CustomCausalAttention(sequence_length=10) output = attention.apply_causal_bias(x) # Output should maintain shape (32, 10, 64) print(output.shape) ``` Note: Fill in the method implementation for `apply_causal_bias`.","solution":"import torch import torch.nn.functional as F class CustomCausalAttention: def __init__(self, sequence_length): self.sequence_length = sequence_length def apply_causal_bias(self, x): batch_size, seq_len, embed_dim = x.size() assert seq_len == self.sequence_length, \\"Sequence length mismatch.\\" # Mask future positions (causal mask) causal_mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device) # Compute attention scores (for simplicity, use dot product self-attention) attn_scores = torch.bmm(x, x.transpose(1, 2)) # Shape: (batch_size, seq_len, seq_len) # Apply causal mask to the attention scores masked_scores = attn_scores.masked_fill(causal_mask == 0, float(\'-inf\')) # Apply softmax to get attention probabilities attn_probs = F.softmax(masked_scores, dim=-1) # Apply the attention probabilities to the inputs output = torch.bmm(attn_probs, x) # Shape: (batch_size, seq_len, embed_dim) return output"},{"question":"# POP3 Mailbox Operations As an email administrator, you often need to perform various operations on a mail server using the POP3 protocol. Using the `poplib` module in Python, you are required to implement a function `fetch_email_summary` that connects to a given POP3 server, authenticates using provided credentials, and retrieves a summary of the first `n` email messages in the mailbox. The summary for each email should include the message number and its size in octets. Function Signature ```python def fetch_email_summary(host: str, port: int, username: str, password: str, n: int) -> list: pass ``` Parameters - `host` (str): The hostname of the POP3 server. - `port` (int): The port number of the POP3 server. Use port `995` for SSL connections. - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `n` (int): The number of email summaries to retrieve. Returns - `list`: A list of tuples where each tuple contains the message number and its size in octets. For example, `[(1, 1024), (2, 2048)]`. Constraints - The function should handle both regular POP3 and POP3-over-SSL connections. - If an error occurs (e.g., authentication fails or the mailbox is empty), raise a custom exception `MailboxError` with a relevant error message. - The function should only fetch summaries for up to `n` available messages in the mailbox. Example Usage ```python # Example for regular POP3 summaries = fetch_email_summary(\'pop3.example.com\', 110, \'user\', \'password\', 5) print(summaries) # Output format: [(1, 1024), (2, 2048), ...] # Example for POP3 over SSL summaries = fetch_email_summary(\'pop3s.example.com\', 995, \'user\', \'password\', 5) print(summaries) # Output format: [(1, 1024), (2, 2048), ...] ``` You may assume that the `poplib` module and its classes are correctly imported in your environment. The `MailboxError` exception class should be defined as: ```python class MailboxError(Exception): pass ``` This function will help automate the process of summarizing email messages in a mailbox, making it easier for administrators to monitor and manage email data.","solution":"import poplib from typing import List, Tuple class MailboxError(Exception): pass def fetch_email_summary(host: str, port: int, username: str, password: str, n: int) -> List[Tuple[int, int]]: Fetches a summary of the first `n` email messages from a POP3 server. try: # Decide whether to use SSL or not based on the port number if port == 995: server = poplib.POP3_SSL(host, port) else: server = poplib.POP3(host, port) # Login server.user(username) server.pass_(password) # Get message count and mailbox size message_count, total_size = server.stat() # Fetch summaries for up to `n` messages summaries = [] for i in range(1, min(n, message_count) + 1): response, messages, octets = server.list(i) summaries.append((i, octets)) # Quit the server server.quit() return summaries except (poplib.error_proto, Exception) as e: raise MailboxError(f\\"Failed to retrieve emails: {str(e)}\\") # Example usage (commented out to avoid execution outside intended environment): # summaries = fetch_email_summary(\'pop3.example.com\', 110, \'user\', \'password\', 5) # print(summaries)"},{"question":"# XML DOM Manipulation with `xml.dom` Module You are tasked with manipulating an XML document using Python\'s `xml.dom` module. The objective is to create a function that processes an XML document, performs certain modifications, and retrieves specific data from the document. # Task Implement the following function: ```python def process_xml(xml_str): Processes an XML string, modifies the document, and retrieves specific data. Parameters: - xml_str (str): A string containing XML data. Returns: - dict: A dictionary with the following keys: * \'root_tag\': The tag name of the root element. * \'first_child_tag\': The tag name of the root\'s first child. * \'modified_xml\': The modified XML as a string. # Your implementation here ``` # Requirements 1. **Parse the XML String**: Use the `xml.dom.minidom.parseString()` method to parse the input XML string. 2. **Modify the Document**: - Append a new element `<newElement>` with text content \\"New Content\\" to the root element. - Remove the first child of the root element if it has any child nodes. 3. **Retrieve Data**: - Obtain the tag name of the root element. - Obtain the tag name of the root\'s first child after modification (if it exists). 4. **Return Data**: - Return a dictionary with the keys \'root_tag\', \'first_child_tag\', and \'modified_xml\', where: - \'root_tag\' contains the tag name of the root element. - \'first_child_tag\' contains the tag name of the first child of the root element after modification (or `None` if no such child exists). - \'modified_xml\' contains the string representation of the modified XML document. # Example ```python xml_input = <root> <child1>Content1</child1> <child2>Content2</child2> </root> result = process_xml(xml_input) print(result) ``` Expected output: ```python { \'root_tag\': \'root\', \'first_child_tag\': \'child2\', \'modified_xml\': \'<root><child2>Content2</child2><newElement>New Content</newElement></root>\' } ``` # Constraints - Assume that the input XML string is well-formed. - Do not use external libraries beyond Python\'s standard library. # Performance Requirements - The function should handle typical XML documents efficiently but does not need to be optimized for very large XML files. # Notes - Use the methods and properties described in the `xml.dom` documentation to manipulate the XML document. - Ensure that the resulting XML string is well-formed.","solution":"from xml.dom.minidom import parseString, Document def process_xml(xml_str): Processes an XML string, modifies the document, and retrieves specific data. Parameters: - xml_str (str): A string containing XML data. Returns: - dict: A dictionary with the following keys: * \'root_tag\': The tag name of the root element. * \'first_child_tag\': The tag name of the root\'s first child. * \'modified_xml\': The modified XML as a string. # Parse the XML string dom = parseString(xml_str) # Get the root element root = dom.documentElement # Get the tag name of the root element root_tag = root.tagName # Remove the first child if it exists if root.hasChildNodes(): first_child = root.firstChild root.removeChild(first_child) # Append the new element with text content new_element = dom.createElement(\'newElement\') new_content = dom.createTextNode(\'New Content\') new_element.appendChild(new_content) root.appendChild(new_element) # Get the tag name of the first remaining child if it exists first_child_tag = None if root.hasChildNodes(): first_child_tag = root.firstChild.tagName # Convert the modified XML back to string modified_xml = root.toxml() return { \'root_tag\': root_tag, \'first_child_tag\': first_child_tag, \'modified_xml\': modified_xml }"},{"question":"# WAV File Processing and Modification In this assessment, you will demonstrate your understanding of the \\"wave\\" module by writing functions that read a WAV file, process its audio data, and create a new modified WAV file. Requirements 1. Implement a function `read_wav_file(file_path)` that: - Takes a string `file_path` as input, which is the path to the WAV file to be read. - Returns a dictionary with the following keys and their corresponding values obtained from the WAV file: - `\\"nchannels\\"`: Number of audio channels. - `\\"sampwidth\\"`: Sample width in bytes. - `\\"framerate\\"`: Sampling frequency. - `\\"nframes\\"`: Number of audio frames. - `\\"comptype\\"`: Compression type. - `\\"compname\\"`: Human-readable compression type. - `\\"frames\\"`: The audio frames as a bytes object. Example: ```python { \\"nchannels\\": 2, \\"sampwidth\\": 2, \\"framerate\\": 44100, \\"nframes\\": 1024, \\"comptype\\": \\"NONE\\", \\"compname\\": \\"not compressed\\", \\"frames\\": b\'...\', } ``` 2. Implement a function `write_wav_file(file_path, wav_data)` that: - Takes a string `file_path` as input, which is the path where the new WAV file will be written. - Takes `wav_data` as input, which is a dictionary containing the same structure and keys as returned by the `read_wav_file` function. - Creates a new WAV file with the provided data. 3. Implement a function `modify_wav_file(input_file, output_file, factor)` that: - Takes a string `input_file` as the path to the input WAV file. - Takes a string `output_file` as the path where the modified WAV file will be written. - Takes a float `factor` that will be used to amplify or attenuate the amplitude of the audio. For example, a `factor` of 1.5 will amplify the amplitude by 50%, while a factor of 0.5 will reduce it by 50%. - Reads the input WAV file using `read_wav_file`. - Modifies the audio frames by multiplying each frame value by the factor. Note that frame values are raw bytes that represent integers. - Writes the modified data to the output WAV file using `write_wav_file`. Constraints - Ensure that your code handles large files efficiently. - Consider edge cases such as invalid file paths, improper input formats, and numerical overflows after amplification or attenuation. Here\'s a template to get you started: ```python import wave import struct def read_wav_file(file_path): # Implement function to read WAV file and return the required dictionary pass def write_wav_file(file_path, wav_data): # Implement function to write WAV file from the provided dictionary pass def modify_wav_file(input_file, output_file, factor): # Implement function to modify the amplitude of the audio frames pass ``` Example Usage ```python # Read and print WAV file information wav_info = read_wav_file(\'input.wav\') print(wav_info) # Modify and write a new WAV file with amplified audio modify_wav_file(\'input.wav\', \'output.wav\', 1.5) ``` Write your solution in Python, ensuring clarity and proper documentation in your code.","solution":"import wave import struct def read_wav_file(file_path): with wave.open(file_path, \'rb\') as wav: wav_data = { \\"nchannels\\": wav.getnchannels(), \\"sampwidth\\": wav.getsampwidth(), \\"framerate\\": wav.getframerate(), \\"nframes\\": wav.getnframes(), \\"comptype\\": wav.getcomptype(), \\"compname\\": wav.getcompname(), \\"frames\\": wav.readframes(wav.getnframes()) } return wav_data def write_wav_file(file_path, wav_data): with wave.open(file_path, \'wb\') as wav: wav.setnchannels(wav_data[\\"nchannels\\"]) wav.setsampwidth(wav_data[\\"sampwidth\\"]) wav.setframerate(wav_data[\\"framerate\\"]) wav.setnframes(wav_data[\\"nframes\\"]) wav.setcomptype(wav_data[\\"comptype\\"], wav_data[\\"compname\\"]) wav.writeframes(wav_data[\\"frames\\"]) def modify_wav_file(input_file, output_file, factor): wav_data = read_wav_file(input_file) amp_factor = float(factor) # Determine format and size of sample based on sample width fmt = \'<h\' if wav_data[\\"sampwidth\\"] == 2 else \'<B\' frames = wav_data[\\"frames\\"] num_samples = len(frames) // wav_data[\\"sampwidth\\"] modified_frames = bytearray() for i in range(num_samples): original_sample = struct.unpack_from(fmt, frames, i * wav_data[\\"sampwidth\\"])[0] modified_sample = int(original_sample * amp_factor) # Clip the value to prevent overflow if wav_data[\\"sampwidth\\"] == 2: modified_sample = max(min(modified_sample, 32767), -32768) else: modified_sample = max(min(modified_sample, 255), 0) modified_frames.extend(struct.pack(fmt, modified_sample)) wav_data[\\"frames\\"] = bytes(modified_frames) write_wav_file(output_file, wav_data)"},{"question":"# Asynchronous Programming Assessment **Objective**: Demonstrate your understanding of Python\'s asynchronous programming by implementing a coroutine-based solution for a real-world problem. Problem Statement: You are tasked with creating an asynchronous file download manager using Python\'s `asyncio` library. Your manager will download files concurrently, handle errors appropriately, and log the process. You will need to implement the following functions: 1. `download_file(url: str, destination: str) -> None` - **Parameters**: - `url` (str): The URL of the file to download. - `destination` (str): The file path where the downloaded file should be saved. - **Description**: This coroutine should download the file from the given URL and save it to the specified destination. If an error occurs during the download, it should log the error and continue. 2. `download_files(urls: List[str], destination_dir: str) -> None` - **Parameters**: - `urls` (List[str]): A list of file URLs to download. - `destination_dir` (str): The directory where the downloaded files should be saved. - **Description**: This coroutine should concurrently download all files from the provided URLs and save them to the specified directory. Use the `download_file` coroutine for the actual download process. Ensure that the maximum concurrency level is 5. 3. `main(urls: List[str], destination_dir: str) -> None` - **Parameters**: - `urls` (List[str]): A list of file URLs to download. - `destination_dir` (str): The directory where the downloaded files should be saved. - **Description**: This function initializes the event loop and begins the file download process by calling the `download_files` coroutine. Requirements and Constraints: - You must use Python\'s `asyncio` library for asynchronous operations. - Ensure the `download_files` function only allows a maximum of 5 concurrent download operations. - Implement basic error handling and logging within the `download_file` function. - You are allowed to use additional libraries such as `aiohttp` for asynchronous HTTP requests. - The solution should be efficient and handle large files and a high number of files gracefully. Expected Function Signatures: ```python import asyncio from typing import List async def download_file(url: str, destination: str) -> None: pass async def download_files(urls: List[str], destination_dir: str) -> None: pass def main(urls: List[str], destination_dir: str) -> None: pass ``` Example Usage: ```python if __name__ == \\"__main__\\": urls = [\\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\"] destination_dir = \\"./downloads\\" main(urls, destination_dir) ``` Good luck!","solution":"import asyncio import aiohttp import os from typing import List async def download_file(url: str, destination: str) -> None: try: async with aiohttp.ClientSession() as session: async with session.get(url) as response: response.raise_for_status() filename = os.path.join(destination, os.path.basename(url)) with open(filename, \'wb\') as f: while True: chunk = await response.content.read(1024) if not chunk: break f.write(chunk) print(f\\"Successfully downloaded {url} to {destination}\\") except (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError) as e: print(f\\"Failed to download {url}: {e}\\") except Exception as e: print(f\\"Unexpected error occurred: {e}\\") async def download_files(urls: List[str], destination_dir: str) -> None: if not os.path.exists(destination_dir): os.makedirs(destination_dir) semaphore = asyncio.Semaphore(5) async def semaphore_download(url): async with semaphore: await download_file(url, destination_dir) await asyncio.gather(*(semaphore_download(url) for url in urls)) def main(urls: List[str], destination_dir: str) -> None: asyncio.run(download_files(urls, destination_dir))"},{"question":"Objective Demonstrate your understanding of the `ctypes` library by performing dynamic loading of a shared library, calling a function within it, and handling various ctypes data types. Problem Statement You are given a shared library `libmath.so` (Linux) or `libmath.dll` (Windows) that contains a simple mathematical function `int add(int a, int b)`. You need to write a Python function `add_numbers` using the `ctypes` library to load this shared library dynamically, call the `add` function with two integers, and return the result. Instructions 1. **Load the shared library:** - On Linux, the library is named `libmath.so`. - On Windows, the library is named `libmath.dll`. 2. **Access the `add` function from the loaded library:** - The function has the following C signature: `int add(int a, int b)`. 3. **Call the `add` function:** - The `add_numbers` function should take two integers as input and return their sum by calling the `add` function from the shared library. - Handle errors appropriately if the library or function cannot be loaded. Function Signature ```python def add_numbers(a: int, b: int) -> int: pass ``` Example Usage ```python result = add_numbers(10, 20) print(result) # Output should be 30 ``` Constraints - The function should handle proper loading and error checking. - Assume the library is available on the same path from where you run your Python script. - Your solution should work on both Linux and Windows platforms. Requirements - Use the `ctypes` library to: - Load the shared library. - Access the `add` function. - Call the function with provided arguments. - Consider cases where the library or function might not be available and handle such scenarios gracefully. Good luck!","solution":"import ctypes import os def add_numbers(a: int, b: int) -> int: try: # Determine the platform and set the library name accordingly if os.name == \'nt\': # Windows libname = \'libmath.dll\' else: # Linux and others libname = \'libmath.so\' # Load the shared library lib = ctypes.CDLL(libname) # Get the `add` function from the library add_func = lib.add add_func.argtypes = [ctypes.c_int, ctypes.c_int] add_func.restype = ctypes.c_int # Call the function and return the result return add_func(a, b) except Exception as e: print(f\\"Error loading library or function: {e}\\") return None"},{"question":"# Email Policy Customization You are required to implement a function `create_custom_policy()` that creates a custom Policy instance for the email package catering to a specific use case. This custom policy should: 1. Limit the maximum line length of headers to `100`. 2. Use `rn` as the line separator. 3. Allow 8-bit data in the email body. 4. Disable mangling of lines starting with \\"From \\". 5. Raise errors on any defects encountered. This function should return the custom policy instance. # Function Signature ```python def create_custom_policy(): pass ``` # Return Format - The function should return an instance of a Policy object with the specified settings. # Constraints - The changes must be made leveraging the `clone` method of a Policy instance for immutability purposes. - Use `EmailPolicy` as the base policy to ensure compliance with the latest email RFCs. # Example ```python custom_policy = create_custom_policy() print(custom_policy.max_line_length) # Output: 100 print(custom_policy.linesep) # Output: \'rn\' print(custom_policy.cte_type) # Output: \'8bit\' print(custom_policy.mangle_from_) # Output: False print(custom_policy.raise_on_defect) # Output: True ``` # Note Ensure that you explore and understand the usage of the `email.policy` module by referring to the provided documentation to correctly set the values for each attribute.","solution":"from email.policy import EmailPolicy def create_custom_policy(): Creates and returns a custom email policy based on the given specifications. return (EmailPolicy() .clone(max_line_length=100) .clone(linesep=\'rn\') .clone(cte_type=\'8bit\') .clone(mangle_from_=False) .clone(raise_on_defect=True))"},{"question":"Objective Demonstrate your understanding of seaborn\'s object-oriented plotting interface by creating a detailed multi-layered plot using a dataset. Problem Statement Using the seaborn\'s object-oriented API (`seaborn.objects`), create a plot that visualizes the relationship between species, body mass, and flipper length from the `penguins` dataset. Your plot should include the following elements: 1. **Scatter plot**: Show individual data points for each species with different colors for each species. 2. **Line plot**: Show the average body mass per species with a line. 3. **Error bars**: Add error bars showing the standard deviation of body mass for each species. 4. **Faceting**: Create separate facets for \\"island\\". 5. **Customization**: Customize the appearance of the points with `point size` and lines with `line styles`. Input No input is required from the user. You will load the `penguins` dataset directly using seaborn\'s `load_dataset()` function. Expected Output The function should display a multi-layered plot as described above. Constraints and Requirements - You must use seaborn\'s object-oriented API (`seaborn.objects`). - The plot must include a scatter plot, line plot, error bars, and faceting by island. - Ensure that the plot is well-labeled with titles and axis labels. - Customize the appearance attributes as specified. # Example ```python import seaborn.objects as so from seaborn import load_dataset def create_penguins_plot(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\") .facet(\\"island\\") .add(so.Dot(pointsize=5), so.Dodge()) .add(so.Line(linewidth=2, linestyle=\'--\'), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\", title=\\"Penguins Dataset Visualization\\") ) # Display the plot plot.show() # Call the function to generate the plot create_penguins_plot() ``` This code should generate and display a multi-faceted plot as described. Ensure that all customization and requirements are met.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguins_plot(): Creates a multi-layered plot visualizing the relationship between species, body mass, and flipper length from the penguins dataset. - Scatter plot for individual data points. - Line plot for average body mass per species with error bars. - Faceting by island. # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\") .facet(\\"island\\") .add(so.Dot(pointsize=10), so.Dodge()) # Scatter plot with larger point size .add(so.Line(linewidth=2, linestyle=\'--\'), so.Agg()) # Line plot with dashed lines .add(so.Range(), so.Est(errorbar=\\"sd\\")) # Error bars showing standard deviation .label(x=\\"Flipper Length (mm)\\", y=\\"Body Mass (g)\\", title=\\"Penguins Dataset Visualization\\") ) # Display the plot plot.show()"},{"question":"**Question: \\"String and List Manipulation\\"** Write a Python function `process_data(input_string, input_list)` that takes two inputs: 1. A string `input_string`. 2. A list of integers `input_list`. The function should: 1. Reverse the input string and convert it to uppercase. 2. Calculate the sum, mean, and product of the integers in the input list. 3. Return a dictionary with the following keys and their respective values: - `\\"reversed_string\\"`: The reversed and uppercased version of the input string. - `\\"sum\\"`: The sum of the integers in the input list. - `\\"mean\\"`: The mean of the integers in the input list. - `\\"product\\"`: The product of the integers in the input list. # Example: ```python input_string = \\"hello\\" input_list = [1, 2, 3, 4] output = process_data(input_string, input_list) print(output) ``` Expected output: ```python { \'reversed_string\': \'OLLEH\', \'sum\': 10, \'mean\': 2.5, \'product\': 24 } ``` # Constraints: - The input string will only contain alphabetic characters. - The input list will only contain positive integers and will have at least one element. # Notes: - You are not allowed to use any external libraries for this task. - Ensure to handle edge cases, such as an empty string (though it should not occur as per constraints). Implement the function with the following signature: ```python def process_data(input_string: str, input_list: list) -> dict: # Write your code here ```","solution":"def process_data(input_string: str, input_list: list) -> dict: Process the input string and list of integers to return a dictionary with the reversed and uppercased string, the sum, mean, and product of the integers in the list. # Reverse the input string and convert to uppercase reversed_string = input_string[::-1].upper() # Calculate the sum of the integers in the list total_sum = sum(input_list) # Calculate the mean of the integers in the list mean = total_sum / len(input_list) # Calculate the product of the integers in the list product = 1 for num in input_list: product *= num # Return the resulting dictionary return { \\"reversed_string\\": reversed_string, \\"sum\\": total_sum, \\"mean\\": mean, \\"product\\": product }"},{"question":"Objective: Write a unit test suite using the `unittest` module and utilities from the `test.support` module to thoroughly test the provided `Calculator` class. Class to Test: ```python class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero!\\") return a / b def factorial(self, n): if n < 0: raise ValueError(\\"Cannot compute factorial of negative number!\\") if n == 0: return 1 result = 1 for i in range(1, n + 1): result *= i return result ``` Requirements: 1. **Create a new Python file named `test_calculator.py`.** 2. **Import the `unittest` module and necessary defaults from `test.support`.** 3. **Write a `CalculatorTestCase` class that inherits from `unittest.TestCase` with the following requirements:** - Test the `add` method with both positive and negative numbers. - Test the `subtract` method with both positive and negative numbers. - Test the `multiply` method with both positive and negative numbers. - Test the `divide` method for normal cases and also test if it raises a `ValueError` when dividing by zero. - Test the `factorial` method for both edge cases (e.g., 0, negative numbers) and typical cases. 4. **Use relevant utilities from `test.support` to manage resources, capture outputs, or handle edge cases where applicable.** 5. **Ensure that the tests provide comprehensive coverage and follow best practices in unit testing.** 6. **Make use of context managers from `test.support` to temporarily replace or capture attributes and ensure proper cleanup before exiting.** Example Test Case Skeleton: ```python import unittest from test import support from your_module import Calculator # Adjust this import according to your setup class CalculatorTestCase(unittest.TestCase): def setUp(self): self.calc = Calculator() def tearDown(self): del self.calc def test_add(self): # Test adding positive and negative numbers self.assertEqual(self.calc.add(1, 2), 3) self.assertEqual(self.calc.add(-1, -2), -3) def test_subtract(self): # Test subtracting positive and negative numbers self.assertEqual(self.calc.subtract(5, 3), 2) self.assertEqual(self.calc.subtract(-5, -3), -2) def test_multiply(self): # Test multiplying positive and negative numbers self.assertEqual(self.calc.multiply(2, 3), 6) self.assertEqual(self.calc.multiply(-2, -3), 6) def test_divide(self): # Test division and handle division by zero self.assertEqual(self.calc.divide(6, 3), 2) with self.assertRaises(ValueError): self.calc.divide(6, 0) def test_factorial(self): # Test factorial for various cases self.assertEqual(self.calc.factorial(0), 1) self.assertEqual(self.calc.factorial(5), 120) with self.assertRaises(ValueError): self.calc.factorial(-1) if __name__ == \'__main__\': support.run_unittest(CalculatorTestCase) ``` Constraints: - **Your tests should cover all possible edge cases as per the guidelines mentioned.** - **You are encouraged to use any relevant utility functions provided in `test.support` to enhance your tests.** **Note**: Ensure to handle cleanup operations properly, particularly if the tests create any temporary resources or files.","solution":"class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero!\\") return a / b def factorial(self, n): if n < 0: raise ValueError(\\"Cannot compute factorial of negative number!\\") if n == 0: return 1 result = 1 for i in range(1, n + 1): result *= i return result"},{"question":"You are required to implement a PyTorch function that checks if persistent algorithm optimization conditions are met and apply a simple neural network forward pass using these conditions. # Problem Description 1. **Function**: `cuda_optimized_forward` 2. **Input**: - `model`: a simple PyTorch neural network model. - `input_tensor`: a tensor of shape (N, D) where N is the batch size and D is the dimension. 3. **Output**: - `output_tensor`: output of the model after performing a forward pass. # Function Details The input model should be a neural network that consists of at least one linear layer. The `input_tensor` should be moved to the GPU if available and meet the dtype `torch.float16`. Ensure the following: - Check if all the required conditions for persistent algorithm optimization are satisfied. - Perform a model forward pass and return the result. # Constraints 1. Ensure cuDNN is enabled. 2. The model and input data must be on a V100 GPU. 3. The input tensor must have dtype `torch.float16`. 4. If the conditions are not met, the function should raise a `RuntimeError` with a descriptive message. # Example ```python import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 5) def forward(self, x): return self.fc1(x) model = SimpleNet().cuda() input_tensor = torch.randn(16, 10, dtype=torch.float16).cuda() output = cuda_optimized_forward(model, input_tensor) print(output) ``` # Additional Information - You might find the properties `torch.backends.cudnn.enabled` helpful to check if cuDNN is enabled. - Use `torch.cuda.get_device_name()` to check if the GPU is V100.","solution":"import torch def cuda_optimized_forward(model, input_tensor): Performs a forward pass of the model using the input_tensor while ensuring specific conditions for persistent algorithm optimization are met. Args: model (nn.Module): A PyTorch neural network model. input_tensor (torch.Tensor): An input tensor of shape (N, D) where N is the batch size and D is the dimension. Returns: torch.Tensor: The output tensor after performing a forward pass. Raises: RuntimeError: If conditions for persistent algorithm optimization are not met. # Check if cuDNN is enabled if not torch.backends.cudnn.enabled: raise RuntimeError(\\"cuDNN is not enabled.\\") # Check if the model and input data are on a V100 GPU if torch.cuda.get_device_name() != \\"Tesla V100-SXM2-16GB\\": raise RuntimeError(\\"The model and input data must be on a V100 GPU.\\") # Check if input tensor has dtype torch.float16 if input_tensor.dtype != torch.float16: raise RuntimeError(\\"The input tensor must have dtype torch.float16.\\") # Ensure the model and input tensor are on the GPU if not next(model.parameters()).is_cuda: model = model.cuda() if not input_tensor.is_cuda: input_tensor = input_tensor.cuda() # Perform the forward pass output_tensor = model(input_tensor) return output_tensor"},{"question":"**Distributed Training in PyTorch using `torchrun`** **Objective**: Modify an existing PyTorch training script to make it compliant with `torchrun` for distributed training. Ensure that the script correctly handles checkpointing to save and resume training progress. **Problem Statement**: You are given the following PyTorch training script that currently works with `torch.distributed.launch`. Modify it to work with `torchrun` as per the guidelines provided in the documentation. ```python import os import sys import torch import torch.distributed as dist def parse_args(args): # Simulated argument parsing return { \'backend\': \'nccl\', \'checkpoint_path\': \'./checkpoint.pth\', } def load_checkpoint(path): # Simulated checkpoint loading if os.path.exists(path): return torch.load(path) return {\'epoch\': 0, \'model_state\': None, \'total_num_epochs\': 10} def save_checkpoint(state, path): # Simulated checkpoint saving torch.save(state, path) def initialize(state): # Simulated model and other initializations state[\'model\'] = torch.nn.Linear(10, 10) def train(batch, model): # Simulated training step pass def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args[\'checkpoint_path\']) initialize(state) # Initializes the distributed backend which will take care of sychronizing nodes/GPUs dist.init_process_group(backend=args[\'backend\']) for epoch in range(state[\'epoch\'], state[\'total_num_epochs\']): for batch in range(100): # Simulated dataset train(batch, state[\'model\']) state[\'epoch\'] += 1 save_checkpoint(state, args[\'checkpoint_path\']) if __name__ == \\"__main__\\": main() ``` **Requirements**: 1. **Initialize Distributed Backend**: Modify the script to use `torchrun` by removing the need to manually pass `RANK`, `WORLD_SIZE`, `MASTER_ADDR`, and `MASTER_PORT`. 2. **Checkpointing**: Ensure that the script includes checkpointing logic (`load_checkpoint` and `save_checkpoint`) and the training resumes from the most recent checkpoint in case of a failure. 3. **Get Local Rank**: Update the script to fetch the local rank using the `LOCAL_RANK` environment variable. **Constraints**: - Write clean, readable, and modular code. - Ensure that the code handles possible exceptions (e.g., missing checkpoint file). - The script should continue training from the last saved checkpoint upon restart. **Expected Input and Output Format**: - Input: Arguments (as a dictionary for simplicity in the provided script). - Output: Trained model saved as a checkpoint file at the specified `checkpoint_path`. **Performance Requirements**: - The script should be efficient and handle distributed training across multiple nodes/GPUs. **Hints**: - Look into `os.environ` for fetching environment variables. - Ensure that your changes align with the provided guidelines for `torchrun` usage.","solution":"import os import sys import torch import torch.distributed as dist def parse_args(args): # Simulated argument parsing return { \'backend\': \'nccl\', \'checkpoint_path\': \'./checkpoint.pth\', } def load_checkpoint(path): # Simulated checkpoint loading if os.path.exists(path): return torch.load(path) return {\'epoch\': 0, \'model_state\': None, \'total_num_epochs\': 10} def save_checkpoint(state, path): # Simulated checkpoint saving torch.save(state, path) def initialize(state): # Simulated model and other initializations state[\'model\'] = torch.nn.Linear(10, 10) if state[\'model_state\']: state[\'model\'].load_state_dict(state[\'model_state\']) def train(batch, model): # Simulated training step pass def main(): args = parse_args(sys.argv[1:]) state = load_checkpoint(args[\'checkpoint_path\']) initialize(state) # Initialize process group dist.init_process_group(backend=args[\'backend\']) # Set the device based on local rank local_rank = int(os.environ[\\"LOCAL_RANK\\"]) torch.cuda.set_device(local_rank) state[\'model\'].to(local_rank) for epoch in range(state[\'epoch\'], state[\'total_num_epochs\']): for batch in range(100): # Simulated dataset train(batch, state[\'model\']) state[\'epoch\'] += 1 state[\'model_state\'] = state[\'model\'].state_dict() save_checkpoint(state, args[\'checkpoint_path\']) # Clean up the process group dist.destroy_process_group() if __name__ == \\"__main__\\": main()"},{"question":"Demonstrate your understanding of the `sysconfig` module by creating a utility function in Python. Your task is to write a function `get_installation_paths_for_schema(schema_name: str, var_overrides: dict)` that retrieves and displays the installation paths for a given installation scheme. If any variables are provided in `var_overrides`, those should be used to override the default ones. Function Signature ```python def get_installation_paths_for_schema(schema_name: str, var_overrides: dict) -> dict: pass ``` # Input: 1. `schema_name` (str): The name of the installation scheme. 2. `var_overrides` (dict): A dictionary of variable overrides where the key is the variable name and the value is the override value. # Output: - Returns a dictionary where the keys are the path names (such as \'stdlib\', \'platlib\', etc.) and the values are the corresponding expanded paths for the specified scheme. # Constraints: - If the `schema_name` is not a valid scheme, raise a `KeyError`. - Use the `sysconfig` module to gather the necessary information. - Expand paths based on provided `var_overrides` dictionary. # Example Usage: ```python import sysconfig # Example 1 schema_name = \\"posix_prefix\\" var_overrides = {\\"base\\": \\"/custom/base\\"} paths = get_installation_paths_for_schema(schema_name, var_overrides) print(paths) # Expected output could be: # { # \'stdlib\': \'/custom/base/lib/python3.10\', # \'platstdlib\': \'/custom/base/lib/python3.10\', # \'purelib\': \'/custom/base/lib/python3.10/site-packages\', # \'platlib\': \'/custom/base/lib/python3.10/site-packages\', # \'include\': \'/custom/base/include/python3.10\', # \'platinclude\': \'/custom/base/include/python3.10\', # \'scripts\': \'/custom/base/bin\', # \'data\': \'/custom/base\' # } # Example 2 schema_name = \\"nt\\" var_overrides = {\\"base\\": \\"C:/Python310\\"} paths = get_installation_paths_for_schema(schema_name, var_overrides) print(paths) # Expected output could be (depending on the scheme definition for NT): # { # \'stdlib\': \'C:/Python310/Lib\', # \'platstdlib\': \'C:/Python310/Lib\', # \'purelib\': \'C:/Python310/Lib/site-packages\', # \'platlib\': \'C:/Python310/Lib/site-packages\', # \'include\': \'C:/Python310/include\', # \'platinclude\': \'C:/Python310/include\', # \'scripts\': \'C:/Python310/Scripts\', # \'data\': \'C:/Python310\' # } ``` # Additional Instructions: - Ensure to handle and document any edge cases. - Write your function implementation and test it with different schemes and variable overrides. - Make sure your code is clean, readable, and follows Python\'s best practices.","solution":"import sysconfig def get_installation_paths_for_schema(schema_name: str, var_overrides: dict) -> dict: Retrieves the installation paths for a given installation scheme with optional variable overrides. :param schema_name: The name of the installation scheme. :param var_overrides: A dictionary of variable overrides. :return: A dictionary of path names and their expanded paths. :raises KeyError: If the schema_name is not a valid scheme. if schema_name not in sysconfig.get_scheme_names(): raise KeyError(f\\"Invalid schema name: {schema_name}\\") paths = sysconfig.get_paths(scheme=schema_name, vars=var_overrides) return paths"},{"question":"**Objective**: Implement a custom buffered text reader class to demonstrate an understanding of Python\'s `io` module and its core I/O functionalities. # Problem Statement You are required to design a custom buffered text reader class `CustomBufferedTextReader` that provides buffered I/O capabilities for reading text data. This class should be capable of handling typical text I/O tasks, employing buffering to optimize performance for large text files. # Class Specification 1. **Class Name**: `CustomBufferedTextReader` 2. **Methods**: - `__init__(self, file_path: str, buffer_size: int = io.DEFAULT_BUFFER_SIZE, encoding: str = \'utf-8\')`: - **Description**: Initializes the reader with the given file path, buffer size, and encoding. - **Parameters**: - `file_path` (str): The path to the text file. - `buffer_size` (int): The size of the buffer to be used. Defaults to `io.DEFAULT_BUFFER_SIZE`. - `encoding` (str): The encoding of the text file. Defaults to `\'utf-8\'`. - `read_line(self) -> str`: - **Description**: Reads and returns the next line from the file. - **Returns**: The next line in the file as a string (str). - `read_all_lines(self) -> List[str]`: - **Description**: Reads and returns all lines from the file. - **Returns**: A list of strings, each representing a line from the file. - `reset(self)`: - **Description**: Resets the reader to the beginning of the file. 3. **Constraints**: - The file should be read using buffered I/O. - The class should handle encoding properly and raise appropriate exceptions if issues occur. - The buffer size provided should be appropriately used to manage the text reading. # Example Usage ```python reader = CustomBufferedTextReader(\'example.txt\', buffer_size=8192, encoding=\'utf-8\') # Read lines one by one print(reader.read_line()) # First line print(reader.read_line()) # Second line # Reset and read all lines at once reader.reset() all_lines = reader.read_all_lines() print(all_lines) # List of all lines in the file ``` # Input and Output - **Input**: A file located at `file_path` and read operations through the methods provided. - **Output**: Text lines or a list of text lines as specified by the methods. # Performance and Constraints - The implementation should be optimized to handle large files efficiently. - Ensure proper exception handling for file I/O operations. - Adhere to the principles of buffered I/O as per the Python `io` module documentation. Write your implementation of the `CustomBufferedTextReader` class below: ```python import io class CustomBufferedTextReader: def __init__(self, file_path: str, buffer_size: int = io.DEFAULT_BUFFER_SIZE, encoding: str = \'utf-8\'): self.file_path = file_path self.buffer_size = buffer_size self.encoding = encoding self.file = open(file_path, \'r\', encoding=self.encoding, buffering=self.buffer_size) def read_line(self) -> str: return self.file.readline() def read_all_lines(self) -> list: return self.file.readlines() def reset(self): self.file.seek(0) def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.file.close() # Demonstrate example usage # Note: Please update \'example.txt\' with a valid file path to test the example if __name__ == \\"__main__\\": with CustomBufferedTextReader(\'example.txt\', buffer_size=8192, encoding=\'utf-8\') as reader: print(reader.read_line()) print(reader.read_line()) reader.reset() all_lines = reader.read_all_lines() print(all_lines) ``` Implement the class `CustomBufferedTextReader` as specified and ensure it adheres to the constraints and functionalities described.","solution":"import io class CustomBufferedTextReader: def __init__(self, file_path: str, buffer_size: int = io.DEFAULT_BUFFER_SIZE, encoding: str = \'utf-8\'): self.file_path = file_path self.buffer_size = buffer_size self.encoding = encoding self.file = open(file_path, \'r\', encoding=self.encoding, buffering=self.buffer_size) def read_line(self) -> str: return self.file.readline() def read_all_lines(self) -> list: return self.file.readlines() def reset(self): self.file.seek(0) def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.file.close()"},{"question":"**Objective**: Write comprehensive unit tests for a given Python function using the unittest module and the utilities provided in the `test.support` module. Problem Statement You are tasked with writing a suite of unit tests for a given function using the `unittest` module. You will need to utilize various context managers, constants, and helper functions provided in the `test.support` module to ensure thorough testing. **Function to Test**: `calculate_average(numbers)` This function takes a list of numbers as input and returns the average. If the input list is empty, it should raise a `ValueError`. ```python def calculate_average(numbers): if not numbers: raise ValueError(\\"Cannot calculate average of empty list\\") return sum(numbers) / len(numbers) ``` # Requirements 1. **Unit Tests**: - Write at least four test cases for the `calculate_average` function. - Use the `unittest` module to structure your tests appropriately. 2. **Utilize `test.support` Utilities**: - Use the constants `SHORT_TIMEOUT` and `LONG_TIMEOUT` to set appropriate timeouts for your tests. - Use context managers such as `captured_stdout()` if needed. - Ensure at least one test makes use of `test.support.run_unittest()` to run the test case. # Input and Output - **Input**: List of integers or floats. - **Output**: The average of the numbers in the list, or raise a `ValueError` if the list is empty. # Constraints - The function should handle lists of various sizes and types of numerical data (both integers and floats). - The tests should be efficient and make use of the test utilities for better manageability. # Example Tests - For the input `[1, 2, 3, 4, 5]`, the function should return `3.0`. - For the input `[]`, the function should raise a `ValueError`. # Expected Solution Your solution should look something like this: ```python import unittest from test import support class TestCalculateAverage(unittest.TestCase): def test_non_empty_list(self): # Test with a non-empty list result = calculate_average([1, 2, 3, 4, 5]) self.assertEqual(result, 3.0) def test_empty_list(self): # Test with an empty list, expecting ValueError with self.assertRaises(ValueError): calculate_average([]) def test_mixed_type_list(self): # Test with a list of floats and integers result = calculate_average([1, 2.5, 3, 4.5]) self.assertAlmostEqual(result, 2.75) def test_timeout(self): # Dummy test to use SHORT_TIMEOUT with support.suppressed_stdout(), support.suppressed_stderr(): with self.assertRaises(TimeoutError): support.SHORT_TIMEOUT = 1e-9 # Set a very short timeout calculate_average(range(1, 1000000)) # Force timeout in some large operation @staticmethod def run_all_tests(): support.run_unittest(__name__) if __name__ == \'__main__\': TestCalculateAverage.run_all_tests() ``` Ensure that all tests are implemented and executed correctly, leveraging the `test.support` utilities where applicable.","solution":"def calculate_average(numbers): Calculate the average of a list of numbers. Raise ValueError if the list is empty. Parameters: numbers (list): A list of integers or floats. Returns: float: The average of the numbers in the list. Raises: ValueError: If the input list is empty. if not numbers: raise ValueError(\\"Cannot calculate average of empty list\\") return sum(numbers) / len(numbers)"},{"question":"# Task You are given a tensor `t` in PyTorch, and you need to perform a series of operations to demonstrate your understanding of tensor storage manipulation. Specifically, you will clone the storage of a given tensor, modify the cloned storage, and update the original tensor with the modified storage. # Requirements 1. **Clone the Storage**: Clone the storage of the given tensor `t`. 2. **Modify the Cloned Storage**: Fill the cloned storage with zeros. 3. **Update Original Tensor**: Use the modified cloned storage to update the data in the original tensor `t`. # Constraints - Do not modify the tensor `t` directly using tensor operations like `.zero_()`. Instead, follow the steps outlined above to achieve the result using storage manipulations. - Ensure that after the operations, the original tensor `t` reflects the changes made to the cloned storage. # Function Signature ```python def update_tensor_storage(t: torch.Tensor) -> torch.Tensor: pass ``` # Input - `t`: A torch.Tensor which can be of any dtype and size. # Output - The modified version of the original tensor `t` with its values set to zero, but using storage manipulation techniques. # Example ```python import torch # Example Tensor t = torch.ones(3) # Modified Tensor after storage manipulation updated_t = update_tensor_storage(t) print(updated_t) # Should print: tensor([0., 0., 0.]) ``` # Additional Information Remember to take care when manipulating tensor storage directly as it is a low-level operation not typically recommended for general use. This exercise is intended to demonstrate your understanding of PyTorch\'s underlying tensor mechanics.","solution":"import torch def update_tensor_storage(t: torch.Tensor) -> torch.Tensor: Updates the given tensor `t` by using storage manipulation techniques to set all its elements to zero. Parameters: t (torch.Tensor): The input tensor to be updated. Returns: torch.Tensor: The updated tensor with all elements set to zero. # Clone the storage of the tensor `t` cloned_storage = t.storage().clone() # Modify the cloned storage by filling it with zeros cloned_storage.fill_(0) # Update the original tensor `t` with the modified cloned storage t.set_(cloned_storage) return t"},{"question":"# Question: Advanced Exception Handling in Python You are building a data processing system that needs to handle various types of errors gracefully. Implement a function `process_data(data: List[int]) -> int` that processes a list of integers and returns the sum of the processed data. The function should adhere to the following specifications: 1. If the input list is empty, raise a `ValueError` with the message \\"Data cannot be empty\\". 2. If any element in the list is not an integer, raise a `TypeError` with the message \\"All elements must be integers\\". 3. If an integer in the list is negative, raise a custom exception `NegativeValueError` that inherits from `ValueError`. The message should be \\"Negative value found: {value}\\", where `{value}` is the negative number found. 4. If any exception is raised during processing, catch it and raise a `DataProcessingError` with the original exception set as the cause. `DataProcessingError` should be a custom exception inheriting from `Exception`. 5. Ensure that the function passes the traceback of the original exception to the `DataProcessingError` using the `with_traceback` method. Here is the template for the function and custom exceptions: ```python from typing import List class NegativeValueError(ValueError): pass class DataProcessingError(Exception): pass def process_data(data: List[int]) -> int: try: # Implement the processing logic and exception handling here. pass except Exception as e: raise DataProcessingError(\\"An error occurred during data processing\\") from e ``` **Input:** - A list of integers, `data`, where each integer can be positive, zero, or negative. **Output:** - An integer representing the sum of the processed data. **Examples:** ```python process_data([1, 2, 3]) # Should return 6 process_data([]) # Should raise ValueError(\\"Data cannot be empty\\") process_data([1, \'two\', 3]) # Should raise TypeError(\\"All elements must be integers\\") process_data([1, -2, 3]) # Should raise NegativeValueError(\\"Negative value found: -2\\") process_data([1, 2, 3, \'four\']) # Should raise DataProcessingError with the original TypeError as the cause ``` **Constraints:** - Only standard Python exceptions and classes are to be used. - Handle exceptions as specified and ensure proper use of `__cause__` and `with_traceback` where applicable. **Note:** - The focus is on understanding and implementing custom exceptions, using built-in exceptions, and handling complex exception chains.","solution":"from typing import List class NegativeValueError(ValueError): pass class DataProcessingError(Exception): pass def process_data(data: List[int]) -> int: try: if not data: raise ValueError(\\"Data cannot be empty\\") processed_sum = 0 for item in data: if not isinstance(item, int): raise TypeError(\\"All elements must be integers\\") if item < 0: raise NegativeValueError(f\\"Negative value found: {item}\\") processed_sum += item return processed_sum except (ValueError, TypeError, NegativeValueError) as e: raise DataProcessingError(\\"An error occurred during data processing\\") from e"},{"question":"# Novelty and Outlier Detection Using Scikit-Learn Problem Statement You are provided with a dataset that contains both normal and anomalous observations. Your task is to use scikit-learn\'s anomaly detection tools to identify the outliers in the dataset and evaluate the performance of different algorithms. Dataset Assume you have a dataset `data.csv` with `n` observations and `m` features where the last column is a binary label. The label is `1` for normal observations and `0` for anomalies. Your goal is to train an anomaly detection model without using the labels. The dataset is structured as: ``` feature1, feature2, ..., feature_m, label value1, value2, ..., value_m, 1 value1, value2, ..., value_m, 0 ... ``` Your Tasks 1. **Data Preprocessing**: - Load the dataset and separate the features and labels. - Standardize the features to have zero mean and unit variance. 2. **Model Training and Evaluation**: - Implement outlier detection using the following algorithms: - `LocalOutlierFactor` (LOF) - `IsolationForest` - `One-Class SVM` (using `svm.OneClassSVM`) - `Elliptic Envelope` - Use appropriate parameters for each algorithm and summarize their respective strengths and weaknesses. - Train each model on the dataset without using the labels. - For each model, predict the labels for the training data and compare them with the original labels to calculate: - Precision, Recall, and F1 Score. 3. **Result Analysis**: - Compare the models based on the evaluation metrics. - Explain which model performed the best and why (considering the nature of the dataset and the characteristics of each algorithm). Constraints - You must use scikit-learn\'s implementations for the specified algorithms. - Do not use the labels for training the models. - Ensure your code is optimized and runs efficiently for datasets with up to 10,000 observations and 100 features. Expected Output - A Python script that: - Loads and preprocesses the data. - Trains and evaluates the specified models. - Computes and prints the evaluation metrics for each model. - Provides a brief explanation of the results and conclusions. ```python import numpy as np import pandas as pd from sklearn.neighbors import LocalOutlierFactor from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope from sklearn.preprocessing import StandardScaler from sklearn.metrics import precision_score, recall_score, f1_score # Step 1: Data Preprocessing def load_and_preprocess_data(file_path): df = pd.read_csv(file_path) X = df.iloc[:, :-1].values # Features y = df.iloc[:, -1].values # Labels scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y # Step 2: Model Training and Evaluation def train_and_evaluate_models(X, y): models = { \\"LocalOutlierFactor\\": LocalOutlierFactor(n_neighbors=20, contamination=0.1), \\"IsolationForest\\": IsolationForest(contamination=0.1), \\"OneClassSVM\\": OneClassSVM(kernel=\'rbf\', nu=0.1), \\"EllipticEnvelope\\": EllipticEnvelope(contamination=0.1) } results = {} for name, model in models.items(): if name == \\"LocalOutlierFactor\\": y_pred = model.fit_predict(X) else: model.fit(X) y_pred = model.predict(X) # Convert predictions to binary format (1 for inliers, 0 for outliers) y_pred_binary = (y_pred == 1).astype(int) precision = precision_score(y, y_pred_binary) recall = recall_score(y, y_pred_binary) f1 = f1_score(y, y_pred_binary) results[name] = { \\"Precision\\": precision, \\"Recall\\": recall, \\"F1 Score\\": f1 } return results # Step 3: Result Analysis def analyze_results(results): for model, metrics in results.items(): print(f\\"Model: {model}\\") print(f\\"Precision: {metrics[\'Precision\']:.2f}\\") print(f\\"Recall: {metrics[\'Recall\']:.2f}\\") print(f\\"F1 Score: {metrics[\'F1 Score\']:.2f}\\") print() # Main function to run the tasks if __name__ == \\"__main__\\": file_path = \'data.csv\' # Replace with the path to your dataset X, y = load_and_preprocess_data(file_path) results = train_and_evaluate_models(X, y) analyze_results(results) ``` # Note 1. Make sure to replace `file_path` with the actual path to `data.csv` file. 2. This script assumes isolation of outliers with the contamination ratio of 0.1. Adjust the `contamination`, `n_neighbors`, `kernel`, and `nu` parameters as necessary based on specific dataset characteristics.","solution":"import numpy as np import pandas as pd from sklearn.neighbors import LocalOutlierFactor from sklearn.ensemble import IsolationForest from sklearn.svm import OneClassSVM from sklearn.covariance import EllipticEnvelope from sklearn.preprocessing import StandardScaler from sklearn.metrics import precision_score, recall_score, f1_score # Step 1: Data Preprocessing def load_and_preprocess_data(file_path): Load and preprocess the data from the file. Standardize the feature values. Args: file_path (str): Path to the CSV file containing the dataset. Returns: tuple: Standardized feature array and original labels. df = pd.read_csv(file_path) X = df.iloc[:, :-1].values # Features y = df.iloc[:, -1].values # Labels scaler = StandardScaler() X_scaled = scaler.fit_transform(X) return X_scaled, y # Step 2: Model Training and Evaluation def train_and_evaluate_models(X, y): Train and evaluate different anomaly detection models. Compute precision, recall, and F1 score for each model. Args: X (numpy.ndarray): Feature array. y (numpy.ndarray): Original labels. Returns: dict: Evaluation metrics for each model. models = { \\"LocalOutlierFactor\\": LocalOutlierFactor(n_neighbors=20, contamination=0.1), \\"IsolationForest\\": IsolationForest(contamination=0.1), \\"OneClassSVM\\": OneClassSVM(kernel=\'rbf\', nu=0.1), \\"EllipticEnvelope\\": EllipticEnvelope(contamination=0.1) } results = {} for name, model in models.items(): if name == \\"LocalOutlierFactor\\": y_pred = model.fit_predict(X) else: model.fit(X) y_pred = model.predict(X) # Convert predictions to binary format (1 for inliers, 0 for outliers) y_pred_binary = (y_pred == 1).astype(int) precision = precision_score(y, y_pred_binary) recall = recall_score(y, y_pred_binary) f1 = f1_score(y, y_pred_binary) results[name] = { \\"Precision\\": precision, \\"Recall\\": recall, \\"F1 Score\\": f1 } return results # Step 3: Result Analysis def analyze_results(results): Print the evaluation metrics for each model. Args: results (dict): Evaluation metrics for each model. for model, metrics in results.items(): print(f\\"Model: {model}\\") print(f\\"Precision: {metrics[\'Precision\']:.2f}\\") print(f\\"Recall: {metrics[\'Recall\']:.2f}\\") print(f\\"F1 Score: {metrics[\'F1 Score\']:.2f}\\") print() # Main function to run the tasks if __name__ == \\"__main__\\": file_path = \'data.csv\' # Replace with the path to your dataset X, y = load_and_preprocess_data(file_path) results = train_and_evaluate_models(X, y) analyze_results(results)"},{"question":"# Email Parsing and Analysis Objective You are tasked with implementing a function that processes a raw email data (which could be in bytes, string, or from a file) and extracts its basic information such as the subject, sender, recipient, and the main body content. The function should handle both simple and multipart emails appropriately. Function Signature ```python from typing import Union, Tuple def parse_email(data: Union[bytes, str, \'file\'], data_type: str) -> Tuple[str, str, str, str]: Parses an email from the given data and extracts the subject, sender, recipient, and main body. Args: data (Union[bytes, str, file]): The raw email data, which can be bytes, a string, or a file object. data_type (str): The type of the input data - one of \'bytes\', \'str\', or \'file\'. Returns: Tuple[str, str, str, str]: A tuple containing the subject, sender, recipient, and main body of the email. Constraints: - The input email data will be a valid email format. - For multipart emails, only the text/plain part of the body should be considered as the main body. - The function should raise a ValueError if `data_type` is not one of \'bytes\', \'str\', or \'file\'. pass ``` Requirements 1. Use the appropriate `email` module parser based on the `data_type`. 2. Extract the `Subject`, `From`, and `To` headers. 3. For the main body content, handle multipart emails by extracting only the `text/plain` part. 4. If the email is not multipart, extract the entire payload as the body. 5. Ensure the function raises a `ValueError` for unsupported `data_type`. Example Usage ```python email_bytes = b\'From: user@example.com...rnTo: recipient@example.com...rnSubject: Test Email...rn...rnThis is the body...rn\' result = parse_email(email_bytes, \'bytes\') print(result) # (\'Test Email\', \'user@example.com\', \'recipient@example.com\', \'This is the body\') email_string = From: user@example.com...nTo: recipient@example.com...nSubject: Test Email...n...nThis is the body...n result = parse_email(email_string, \'str\') print(result) # (\'Test Email\', \'user@example.com\', \'recipient@example.com\', \'This is the body\') ``` Constraints - Emails will adhere to standard formats and may contain multiple parts. - The solution should be efficient and handle large emails without excessive memory consumption. - For the file input, it is assumed the file object is already open and ready for reading.","solution":"from typing import Union, Tuple from email import message_from_bytes, message_from_string, message_from_file from email.message import Message def parse_email(data: Union[bytes, str, \'file\'], data_type: str) -> Tuple[str, str, str, str]: Parses an email from the given data and extracts the subject, sender, recipient, and main body. Args: data (Union[bytes, str, file]): The raw email data, which can be bytes, a string, or a file object. data_type (str): The type of the input data - one of \'bytes\', \'str\', or \'file\'. Returns: Tuple[str, str, str, str]: A tuple containing the subject, sender, recipient, and main body of the email. Raises: ValueError: If data_type is not one of \'bytes\', \'str\', or \'file\'. if data_type == \'bytes\': msg = message_from_bytes(data) elif data_type == \'str\': msg = message_from_string(data) elif data_type == \'file\': msg = message_from_file(data) else: raise ValueError(f\\"Unsupported data_type \'{data_type}\'\\") subject = msg.get(\'Subject\', \'\') sender = msg.get(\'From\', \'\') recipient = msg.get(\'To\', \'\') def get_body(message: Message) -> str: if message.is_multipart(): for part in message.walk(): content_type = part.get_content_type() content_disposition = str(part.get(\'Content-Disposition\')) if content_disposition == \'attachment\': continue if content_type == \'text/plain\': return part.get_payload(decode=True).decode(part.get_content_charset() or \'utf-8\') else: return message.get_payload(decode=True).decode(msg.get_content_charset() or \'utf-8\') return \'\' body = get_body(msg) return subject, sender, recipient, body"},{"question":"# PyTorch MPS Device Management and Profiling Objective: Write a PyTorch program that effectively manages MPS devices and demonstrates profiling and event-based synchronization. Tasks: 1. **Device Initialization and Verification:** - Write a function `initialize_device()` that returns the number of available MPS devices. - Print the number of devices to confirm MPS availability. 2. **Random Number Generation Management:** - Write a function `manage_rng_state(seed_value)` that: - Sets the provided seed for random number generation. - Returns the RNG state before and after setting the seed. 3. **Memory Management:** - Write a function `memory_optimization()` that: - Prints the current allocated memory. - Releases all unoccupied cached memory. - Sets per-process memory fraction to 0.5. - Prints the recommended maximum memory. 4. **Profiling:** - Write a function `profile_operations()` that: - Uses `torch.mps.profiler` to start and stop profiling during a dummy matrix operation, such as random tensor creation and multiplication. 5. **Event-Based Synchronization:** - Write a function `event_synchronization()` that: - Creates an event. - Synchronizes the event. Expected Input and Output: - **Function 1 (`initialize_device`):** - **Input:** None - **Output:** Integer (number of MPS devices) - **Function 2 (`manage_rng_state`):** - **Input:** Integer `seed_value` - **Output:** Tuple (RNG state before setting the seed, RNG state after setting the seed) - **Function 3 (`memory_optimization`):** - **Input:** None - **Output:** None (print statements for memory information) - **Function 4 (`profile_operations`):** - **Input:** None - **Output:** None (profiling results are printed) - **Function 5 (`event_synchronization`):** - **Input:** None - **Output:** None (event synchronization confirmation is printed) Constraints: - Ensure that the functions handle scenarios where MPS devices are not available and provide appropriate messages. - Use PyTorch version that supports MPS functionalities. - Optimize for performance and manage memory efficiently within the provided constraints. Example: ```python def initialize_device(): # Implement the function to return the number of MPS devices pass def manage_rng_state(seed_value): # Implement the function to manage RNG state with given seed_value pass def memory_optimization(): # Implement the function to optimize memory usage pass def profile_operations(): # Implement the function to profile operations pass def event_synchronization(): # Implement the function to synchronize events pass # Function calls for testing print(\\"Number of MPS devices:\\", initialize_device()) print(\\"RNG State Management:\\", manage_rng_state(42)) memory_optimization() profile_operations() event_synchronization() ```","solution":"import torch def initialize_device(): Returns the number of available MPS devices. if torch.has_mps: num_devices = torch.mps.device_count() return num_devices else: print(\\"MPS device is not available.\\") return 0 def manage_rng_state(seed_value): Sets the provided seed for random number generation. Returns the RNG state before and after setting the seed. rng_state_before = torch.get_rng_state() torch.manual_seed(seed_value) rng_state_after = torch.get_rng_state() return rng_state_before, rng_state_after def memory_optimization(): Prints the current allocated memory, releases all unoccupied cached memory, sets per-process memory fraction to 0.5 and prints the recommended maximum memory. if not torch.has_mps: print(\\"MPS device is not available.\\") return print(f\\"Current allocated memory: {torch.mps.memory_allocated()}\\") torch.mps.empty_cache() torch.mps.set_per_process_memory_fraction(0.5) print(f\\"Recommended max memory: {torch.mps.max_memory_allocated()}\\") def profile_operations(): Uses profiler to start and stop profiling during a dummy matrix operation such as random tensor creation and multiplication. if not torch.has_mps: print(\\"MPS device is not available.\\") return dummy_tensor = torch.rand((1000, 1000), device=\\"mps\\") with torch.profiler.profile() as prof: result = dummy_tensor @ dummy_tensor print(prof.key_averages().table(sort_by=\\"cpu_time_total\\", row_limit=10)) def event_synchronization(): Creates an event and synchronizes it. if not torch.has_mps: print(\\"MPS device is not available.\\") return event = torch.mps.Event() event.record() event.synchronize() print(\\"Event has been synchronized.\\")"},{"question":"# Python Coding Assessment Question **Objective:** Implement a function that converts a list of formatted string representations of numbers into their numeric equivalents, processes the numeric values, and then converts the results back to formatted strings. Your function should handle errors and constraints as described. **Function Signature:** ```python def process_formatted_strings(string_list: list[str], format_code: str, precision: int, flags: int) -> list[str]: pass ``` **Input:** 1. `string_list`: A list of strings where each string is a valid formatted representation of a number. 2. `format_code`: A character (`\'e\'`, `\'E\'`, `\'f\'`, `\'F\'`, `\'g\'`, `\'G\'`, `\'r\'`) specifying the format to use for the output. 3. `precision`: An integer specifying the precision for the output. 4. `flags`: An integer representing formatting flags (can be a combination of `Py_DTSF_SIGN`, `Py_DTSF_ADD_DOT_0`, and `Py_DTSF_ALT`). **Output:** - A list of strings where each element is the formatted string representation of the corresponding numeric value in `string_list`, converted as per the provided `format_code`, `precision`, and `flags`. **Constraints:** - Each string in `string_list` represents a valid number; however, your function should handle potential conversion errors gracefully. - Ensure the function returns results promptly and does not consume excessive memory. **Example:** ```python formatted_strings = [\\"1.23\\", \\"4.56e-2\\", \\"7.89\\"] format_code = \'f\' precision = 2 flags = Py_DTSF_ADD_DOT_0 result = process_formatted_strings(formatted_strings, format_code, precision, flags) print(result) # Expected output: [\'1.23\', \'0.05\', \'7.89\'] ``` **Notes:** 1. Consider edge cases such as very large or very small numbers, and handle errors such as conversion exceptions. 2. Use the `PyOS_string_to_double` and `PyOS_double_to_string` or equivalent Python functionalities to perform the conversions. 3. Implement error handling to manage invalid input cases and ensure the function does not crash with malformed data. This question assesses understanding of string conversion, formatted output, and error handling in Python, integrating core concepts and requiring students to manage both functionality and edge cases robustly.","solution":"def process_formatted_strings(string_list, format_code, precision, flags): Convert a list of numeric strings to formatted numeric strings based on format_code, precision, and flags. :param string_list: List of strings representing numbers. :param format_code: Format code for output (e, E, f, F, g, G, r) :param precision: Precision for the output :param flags: Formatting flags :return: List of formatted string representations of numbers Py_DTSF_SIGN = 1 Py_DTSF_ADD_DOT_0 = 2 Py_DTSF_ALT = 4 formatted_strings = [] for string in string_list: try: num = float(string) formatted_string = f\\"{num:.{precision}{format_code}}\\" if flags & Py_DTSF_SIGN: if num >= 0: formatted_string = \\"+\\" + formatted_string if flags & Py_DTSF_ADD_DOT_0: if \'.\' not in formatted_string: formatted_string += \'.0\' if flags & Py_DTSF_ALT: if \'e\' in formatted_string or \'E\' in formatted_string: formatted_string = formatted_string.rstrip(\'0\').rstrip(\'.\') formatted_strings.append(formatted_string) except ValueError: formatted_strings.append(\\"Error\\") return formatted_strings"},{"question":"You are required to implement a function using PyTorch that performs several operations on tensors and ensures the output meets specified criteria. Task Description Implement a function `tensor_operations` that takes no parameters but performs the following steps: 1. Create a tensor `A` of shape `(3, 4)` with random floating-point numbers between 0 and 1. 2. Create a tensor `B` of shape `(4, 3)` with random floating-point numbers between 0 and 1. 3. Perform a matrix multiplication of `A` and `B` to get a tensor `C`. 4. Compute the element-wise sine of tensor `C`, resulting in tensor `D`. 5. Compute the gradient of the sum of all elements in `D` with respect to `A`. The function should return the following tensors: - `A`: The initial tensor of shape `(3, 4)`. - `B`: The initial tensor of shape `(4, 3)`. - `C`: Resultant tensor from matrix multiplication of `A` and `B`. - `D`: Resultant tensor from element-wise sine operation on `C`. - `grad_A`: Gradient of the sum of all elements in `D` with respect to `A`. Expected Output The function `tensor_operations()` should produce: ```python import torch def tensor_operations(): # 1. Create tensor A of shape (3, 4) with random floats between 0 and 1 A = torch.rand((3, 4), requires_grad=True) # 2. Create tensor B of shape (4, 3) with random floats between 0 and 1 B = torch.rand((4, 3)) # 3. Perform matrix multiplication of A and B to get C C = torch.mm(A, B) # 4. Compute element-wise sine of C to get D D = torch.sin(C) # 5. Compute the gradient of the sum of all elements in D with respect to A D_sum = D.sum() D_sum.backward() grad_A = A.grad return A, B, C, D, grad_A # Example function call and output A, B, C, D, grad_A = tensor_operations() print(f\'A: {A}\') print(f\'B: {B}\') print(f\'C: {C}\') print(f\'D: {D}\') print(f\'grad_A: {grad_A}\') ``` Constraints 1. Use the `torch` package to create and manipulate tensors. 2. Ensure that random tensors `A` and `B` are generated using PyTorch\'s random number generation. 3. Use appropriate gradient tracking for `A` to allow computation of gradients. Performance Requirements Efficiently handle the tensor operations to ensure the code runs within a reasonable time frame for tensors of the given size.","solution":"import torch def tensor_operations(): # 1. Create tensor A of shape (3, 4) with random floats between 0 and 1 A = torch.rand((3, 4), requires_grad=True) # 2. Create tensor B of shape (4, 3) with random floats between 0 and 1 B = torch.rand((4, 3)) # 3. Perform matrix multiplication of A and B to get C C = torch.mm(A, B) # 4. Compute element-wise sine of C to get D D = torch.sin(C) # 5. Compute the gradient of the sum of all elements in D with respect to A D_sum = D.sum() D_sum.backward() grad_A = A.grad return A, B, C, D, grad_A"},{"question":"**Objective**: To evaluate your understanding of Scikit-learn\'s feature extraction functionalities and your ability to apply them in a practical context. **Problem Statement**: You are provided with a dataset of movie entries, where each entry contains details such as the movie\'s title, genre(s), and year of release. Your task is to create a feature extraction pipeline that transforms this dataset into a numerical format suitable for machine learning algorithms. Additionally, you are required to perform text feature extraction on a related dataset of movie reviews. # Inputs: 1. A list of movie entries, where each entry is represented as a dictionary. Example: ```python movies = [ {\'title\': \'The Matrix\', \'genres\': [\'Action\', \'Sci-Fi\'], \'year\': 1999}, {\'title\': \'Toy Story\', \'genres\': [\'Animation\', \'Family\'], \'year\': 1995}, {\'title\': \'The Godfather\', \'genres\': [\'Crime\', \'Drama\'], \'year\': 1972} ] ``` 2. A list of movie reviews, where each review is a string. Example: ```python reviews = [ \'The Matrix is an excellent sci-fi action movie with groundbreaking special effects.\', \'Toy Story is a heartwarming animation movie suitable for all ages.\', \'The Godfather is a masterpiece in crime drama, anchored by stellar performances.\' ] ``` # Task: 1. **Feature Extraction from Movie Entries**: - Use `DictVectorizer` from Scikit-learn to convert the list of movie entries into a numerical array. - Ensure that multiple genres are handled correctly using one-hot encoding. 2. **Text Feature Extraction from Reviews**: - Use `TfidfVectorizer` from Scikit-learn to convert the list of movie reviews into a Tf-idf matrix. - Configure the `TfidfVectorizer` to ignore English stop words. 3. Combine both extracted features into a final feature set that can be used for further machine learning tasks. # Expected Functions and Outputs: 1. **Function Signature**: ```python from sklearn.feature_extraction import DictVectorizer from sklearn.feature_extraction.text import TfidfVectorizer import numpy as np def extract_movie_features(movies, reviews): \'\'\' movies: List[Dict[str, Any]] - A list of dictionaries containing movie details. reviews: List[str] - A list of movie review strings. Returns: np.array - A combined numerical feature array for the movies and their reviews. \'\'\' # Task 1: DictVectorizer for Movie Entries vec = DictVectorizer(sparse=False) movie_features = vec.fit_transform(movies) # Task 2: TfidfVectorizer for Movie Reviews tfidf = TfidfVectorizer(stop_words=\'english\') review_features = tfidf.fit_transform(reviews).toarray() # Task 3: Combine both feature sets combined_features = np.hstack((movie_features, review_features)) return combined_features ``` 2. **Example Output**: ```python movies = [ {\'title\': \'The Matrix\', \'genres\': [\'Action\', \'Sci-Fi\'], \'year\': 1999}, {\'title\': \'Toy Story\', \'genres\': [\'Animation\', \'Family\'], \'year\': 1995}, {\'title\': \'The Godfather\', \'genres\': [\'Crime\', \'Drama\'], \'year\': 1972} ] reviews = [ \'The Matrix is an excellent sci-fi action movie with groundbreaking special effects.\', \'Toy Story is a heartwarming animation movie suitable for all ages.\', \'The Godfather is a masterpiece in crime drama, anchored by stellar performances.\' ] combined_features = extract_movie_features(movies, reviews) print(combined_features) ``` # Constraints and Limitations: - You can assume that the genre of each movie is always a list and the year is always an integer. - Each review is a non-empty string. - The number of genres and the length of the reviews are within reasonable limits for processing. # Evaluation Criteria: - Correct implementation of feature extraction using `DictVectorizer` and `TfidfVectorizer`. - Successful combination of the numerical features for movies and reviews. - Code readability, including meaningful variable names and clear comments. # Note: - The provided dataset examples are minimal. Your solution should be robust enough to handle larger and more complex datasets that follow the same structure.","solution":"from sklearn.feature_extraction import DictVectorizer from sklearn.feature_extraction.text import TfidfVectorizer import numpy as np def extract_movie_features(movies, reviews): \'\'\' movies: List[Dict[str, Any]] - A list of dictionaries containing movie details. reviews: List[str] - A list of movie review strings. Returns: np.array - A combined numerical feature array for the movies and their reviews. \'\'\' # Task 1: DictVectorizer for Movie Entries vec = DictVectorizer(sparse=False) movie_features = vec.fit_transform(movies) # Task 2: TfidfVectorizer for Movie Reviews tfidf = TfidfVectorizer(stop_words=\'english\') review_features = tfidf.fit_transform(reviews).toarray() # Task 3: Combine both feature sets combined_features = np.hstack((movie_features, review_features)) return combined_features"},{"question":"# Question **Dynamic Python Code Execution and Parsing** This question is designed to assess your understanding of Python\'s capabilities and syntax for dynamically executing code. You are required to implement a function that reads a multi-line string input containing Python code, determines its type (complete program, interactive input, or expression), and executes it accordingly. Function Signature ```python def dynamic_code_executor(code: str) -> Any: pass ``` Input - `code` (str): A multi-line string containing Python code. This string could represent: - A complete Python program. - An interactive input, possibly containing compound statements. - An expression suitable for `eval()`. Output - Returns the result of the executed code, which could be: - The output of the `exec()` function in the case of a complete program or interactive input. - The evaluated result in the case of an expression suitable for `eval()`. Constraints - The input string `code` will always contain valid Python code. - The function should be able to handle both small and large input sizes efficiently. - Assume that the code contained within the string does not modify any essential global state, like filesystem or complex I/O operations. Examples **Example 1:** ```python code = \'\'\' a = 5 b = 10 c = a + b \'\'\' print(dynamic_code_executor(code)) ``` **Output:** ``` None ``` (As `exec()` does not return a value, but the code would have executed and `c` would be 15 if we inspected the local scope) **Example 2:** ```python code = \'a + b\' print(dynamic_code_executor(code)) ``` **Output:** ``` 15 ``` (Assuming `a` and `b` are defined in the local or global scope) **Example 3:** ```python code = \'\'\' def greet(name): return f\\"Hello, {name}\\" greet(\\"Alice\\") \'\'\' print(dynamic_code_executor(code)) ``` **Output:** ``` \'Hello, Alice\' ``` (This is a compound statement in interactive mode) # Notes: 1. Carefully handle the different types of input to ensure the code is executed in the expected manner. 2. Ensure to differentiate between complete programs, interactive inputs, and expressions.","solution":"def dynamic_code_executor(code: str): Executes the provided Python code string and returns the result. Determines if the code is a complete program, interactive input, or an expression. try: # Try evaluating it as an expression result = eval(code) return result except (SyntaxError, NameError): pass # If it fails as an expression, execute it as a program local_vars = {} exec(code, {}, local_vars) # If the code defines a function and there\'s a function call in the code, we need to return the result of the last function call. # We assume the last line of the code might be the function call whose return value we want to capture. lines = code.strip().split(\'n\') if lines: last_line = lines[-1].strip() if last_line and not last_line.startswith(\'#\') and \'(\' in last_line and \')\' in last_line: # Evaluate the last line assuming it\'s a function call result = eval(last_line, {}, local_vars) return result return None"},{"question":"**Title**: Implementing a Basic Interactive History and Auto-completion Feature **Objective**: Design a Python class that simulates a basic interactive interpreter with history recording and tab completion functionalities. **Description**: You are required to implement a class named `InteractiveInterpreter` that will simulate basic aspects of the interactive Python interpreter focusing on command history and tab completion of variable names. Your task includes: 1. **Command History**: - Store each command entered by the user. - Save the command history to a file named `.command_history`. - Load the command history from the `.command_history` file when the class is instantiated. 2. **Tab Completion**: - Implement a method for tab completion which suggests completions for variable names in the current session. **Class Specification**: ```python class InteractiveInterpreter: def __init__(self): Initialize the interpreter by loading history from the `.command_history` file. pass def execute_command(self, command: str): Execute a command and store it in the history. Args: command (str): The command to execute. pass def save_history(self): Save the current command history to the `.command_history` file. pass def tab_complete(self, text: str) -> list: Return a list of possible completions for the given text based on current session variables. Args: text (str): The partial text to complete. Returns: list: A list of possible completions. pass ``` **Input/Output Requirements**: - The `execute_command` method will take a string `command` as input and execute it in the current session context. - The `tab_complete` method will take a partial string `text` as input and return a list of strings which are potential completions based on the variables defined in the current session. - The history should be automatically saved when a new command is executed, and loaded during initialization. **Constraints**: - Assume there are no syntax errors in the commands provided. - The history file `.command_history` should be stored in the same directory as the script. **Example Usage**: ```python interpreter = InteractiveInterpreter() interpreter.execute_command(\\"x = 10\\") interpreter.execute_command(\\"y = 20\\") print(interpreter.tab_complete(\\"x\\")) # Output: [\'x\'] print(interpreter.tab_complete(\\"\\")) # Output: [\'x\', \'y\'] interpreter.save_history() # After restarting the script interpreter = InteractiveInterpreter() print(interpreter.tab_complete(\\"x\\")) # Output: [\'x\'] print(interpreter.tab_complete(\\"\\")) # Output: [\'x\', \'y\'] ``` **Notes**: - Focus on demonstrating a clear understanding of file handling, exception management, and the basics of how an interpreter functions. - You may use built-in Python functions and standard libraries. - Ensure that the solution is efficient and handles potential edge cases.","solution":"import os class InteractiveInterpreter: def __init__(self): self.history = [] self.variables = {} self.history_file = \\".command_history\\" self.load_history() def load_history(self): if os.path.exists(self.history_file): with open(self.history_file, \'r\') as f: self.history = f.read().splitlines() def execute_command(self, command: str): exec(command, {}, self.variables) self.history.append(command) self.save_history() def save_history(self): with open(self.history_file, \'w\') as f: for cmd in self.history: f.write(cmd + \\"n\\") def tab_complete(self, text: str) -> list: return [var for var in self.variables if var.startswith(text)]"},{"question":"# Advanced Python Coding Assessment: Directory Comparison and Reporting Objective You are required to use the `filecmp` module in Python to implement a program that provides a comprehensive comparison report of two directories. Your program will output differences in files and subdirectories, including files with the same name but different contents or files that exist only in one of the directories. Task Implement a function, `compare_directories(dir1: str, dir2: str) -> None`, that: 1. Compares two directories, `dir1` and `dir2`. 2. Prints a detailed report showing: - Files that are only in `dir1`. - Files that are only in `dir2`. - Files that are in both `dir1` and `dir2` but differ in content. - Files that are in both directories and are identical. - Summary of the above report. Function Signature ```python def compare_directories(dir1: str, dir2: str) -> None: pass ``` Expected Output A detailed, clear text output similar to the following format: ``` Files only in dir1: - [file1] - [file2] ... Files only in dir2: - [file3] - [file4] ... Files in both directories but different: - [file5] - [file6] ... Files in both directories and identical: - [file7] - [file8] ... Summary: - Total files only in dir1: X - Total files only in dir2: Y - Total differing files: Z - Total identical files: W ``` Constraints - Assume `dir1` and `dir2` are valid directories. - Your solution should handle a reasonable file count within the directories efficiently. - If any errors occur during file comparison, handle them gracefully and include those files in an \\"Errors\\" section in your report. Sample Usage ```python compare_directories(\'path/to/dir1\', \'path/to/dir2\') ``` This will print the comparison result as described in the output format. Notes - Utilize the `filecmp.dircmp` class and its methods/attributes for your implementation. - Pay attention to edge cases such as empty directories or directories with special file types.","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str) -> None: Compares two directories and prints a detailed report. def print_list(label, items): print(f\\"{label}:\\") for item in items: print(f\\"- {item}\\") print(\\"\\") try: comparison = filecmp.dircmp(dir1, dir2) print_list(\\"Files only in dir1\\", comparison.left_only) print_list(\\"Files only in dir2\\", comparison.right_only) print_list(\\"Files in both directories but different\\", comparison.diff_files) print_list(\\"Files in both directories and identical\\", comparison.same_files) print(\\"Summary:\\") print(f\\"- Total files only in dir1: {len(comparison.left_only)}\\") print(f\\"- Total files only in dir2: {len(comparison.right_only)}\\") print(f\\"- Total differing files: {len(comparison.diff_files)}\\") print(f\\"- Total identical files: {len(comparison.same_files)}\\") except Exception as e: print(\\"An error occurred during comparison:\\") print(str(e))"},{"question":"# Coding Challenge: Logging Configuration with `logging.config` You are required to configure the logging system in a Python application. You will be provided with a logging configuration schema in the form of a dictionary. Your task is to write a Python function that configures logging based on this dictionary and then logs messages to verify the configuration. Requirements: 1. Implement a function `configure_logging(log_config: dict) -> None` that receives a logging configuration dictionary and configures the logging system using `logging.config.dictConfig`. 2. Implement a function `log_messages() -> None` that demonstrates the logging configuration by logging messages at various levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). 3. Verify the logging configuration by capturing the log outputs. You should ensure: - The loggers are correctly configured with the appropriate handlers, formatters, and levels. - Messages at different logging levels are used. Logging Configuration Dictionary Example: ```python log_config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\' }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'filename\': \'application.log\' } }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'] }, } ``` Function Signatures: ```python def configure_logging(log_config: dict) -> None: # Configure the logging system based on the given log_config dictionary pass def log_messages() -> None: # Log messages at various levels to demonstrate the logging configuration pass ``` Example Usage: ```python if __name__ == \\"__main__\\": logging_config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\' }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'filename\': \'application.log\' } }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'] }, } configure_logging(logging_config) log_messages() ``` Constraints: - Make sure no error is raised during the configuration process. - Ensure the configuration logs the messages to both the console and the file as specified. - Only use standard python libraries. Good luck!","solution":"import logging import logging.config def configure_logging(log_config: dict) -> None: Configures the logging system based on the given log_config dictionary. logging.config.dictConfig(log_config) def log_messages() -> None: Logs messages at various levels to demonstrate the logging configuration. logger = logging.getLogger() logger.debug(\\"This is a DEBUG message.\\") logger.info(\\"This is an INFO message.\\") logger.warning(\\"This is a WARNING message.\\") logger.error(\\"This is an ERROR message.\\") logger.critical(\\"This is a CRITICAL message.\\")"},{"question":"# PyTorch Coding Assessment Question Objective To evaluate the comprehension of tensor data types (`torch.dtype`), tensor devices (`torch.device`), and memory layouts (`torch.layout`) in PyTorch. Problem Statement Implement a function `tensor_operations` that performs a series of operations on given tensors according to specified rules and returns the result. The function should demonstrate a comprehensive understanding of tensor attributes, type promotion, device management, and memory layout handling. Function Signature ```python import torch def tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor, operation: str, target_dtype: torch.dtype, target_device: torch.device) -> torch.Tensor: Performs the specified arithmetic operation on tensor_a and tensor_b, promotes the result to target_dtype if necessary, and moves the result to target_device. Parameters: tensor_a (torch.Tensor): The first input tensor. tensor_b (torch.Tensor): The second input tensor. operation (str): Arithmetic operation to perform: one of \'add\', \'sub\', \'mul\', \'div\'. target_dtype (torch.dtype): The target data type for the result. target_device (torch.device): The target device for the result. Returns: torch.Tensor: The resulting tensor after performing the operation, promoting to target_dtype, and moving to target_device. Raises: ValueError: If the operation is not one of \'add\', \'sub\', \'mul\', \'div\'. ``` Constraints - `tensor_a` and `tensor_b` can have different `dtype` and `device`. - The function should handle type promotion correctly according to PyTorch\'s rules. - The output tensor should have `target_dtype` and be located on `target_device`. - Allowed operations are \'add\' (addition), \'sub\' (subtraction), \'mul\' (multiplication), and \'div\' (division). Example Usage ```python tensor_a = torch.tensor([1, 2, 3], dtype=torch.int32, device=\'cpu\') tensor_b = torch.tensor([4, 5, 6], dtype=torch.float32, device=\'cuda\') result = tensor_operations(tensor_a, tensor_b, \'add\', torch.float64, torch.device(\'cuda\')) print(result) # Should print a tensor with dtype torch.float64 and located on \'cuda\' device. ``` Notes - Ensure to handle scalar tensors and automatic device transfers for them. - Include appropriate error handling for unsupported operations and invalid inputs. Performance Requirement - The function should efficiently handle large tensor operations without unnecessary data transfers or promotions.","solution":"import torch def tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor, operation: str, target_dtype: torch.dtype, target_device: torch.device) -> torch.Tensor: Performs the specified arithmetic operation on tensor_a and tensor_b, promotes the result to target_dtype if necessary, and moves the result to target_device. Parameters: tensor_a (torch.Tensor): The first input tensor. tensor_b (torch.Tensor): The second input tensor. operation (str): Arithmetic operation to perform: one of \'add\', \'sub\', \'mul\', \'div\'. target_dtype (torch.dtype): The target data type for the result. target_device (torch.device): The target device for the result. Returns: torch.Tensor: The resulting tensor after performing the operation, promoting to target_dtype, and moving to target_device. Raises: ValueError: If the operation is not one of \'add\', \'sub\', \'mul\', \'div\'. # Ensure that the tensors are on the same device for the operation if tensor_a.device != tensor_b.device: tensor_b = tensor_b.to(tensor_a.device) # Perform the operation if operation == \'add\': result = tensor_a + tensor_b elif operation == \'sub\': result = tensor_a - tensor_b elif operation == \'mul\': result = tensor_a * tensor_b elif operation == \'div\': result = tensor_a / tensor_b else: raise ValueError(f\\"Unsupported operation: {operation}\\") # Promote the result to the target dtype and move to target device result = result.to(dtype=target_dtype, device=target_device) return result"},{"question":"**Advanced Seaborn Color Palette Visualization** **Objective:** Design a Python function that takes advantage of Seaborn\'s `mpl_palette` and some visualization methods to produce a custom plot with specific colormaps. You are required to demonstrate a comprehensive understanding of Seaborn\'s palette handling features by implementing and visualizing a combination of continuous and qualitative colormaps. **Function Signature:** ```python def visualize_palettes(data): Given a DataFrame with necessary plot data, create a multi-category plot with specified colormaps. Parameters: data (pd.DataFrame): A pandas DataFrame containing at least two numerical columns and one categorical column. Returns: None: The function should display a Seaborn plot directly. pass ``` **Input:** - `data`: A pandas DataFrame containing at least three columns: - Two numerical columns to be used for the `x` and `y` axes. - One categorical column to differentiate between different groups. **Output:** - The function should not return any values. Instead, it should create and display a Seaborn plot. **Requirements:** 1. **Palette Handling:** - Use the `mpl_palette()` function to generate at least two different palettes: 1. A continuous colormap (e.g., \\"viridis\\"). 2. A qualitative colormap (e.g., \\"Set2\\"). - Visualize these palettes within the plot for various categorical groupings. 2. **Plot Construction:** - Utilize a suitable Seaborn plotting function (e.g., `sns.scatterplot()`, `sns.lineplot()`, etc.) that supports color differentiation for categories. - Ensure that the colors used in the plot come from the palettes generated by `mpl_palette()`. 3. **Plot Customization:** - Apply different colormaps to different parts of the plot (e.g., continuous for data points, qualitative for groups). - Include a legend that clearly shows the colormap used for each categorical group. **Example:** Assume you are given a DataFrame `df`: ```python import pandas as pd data = { \\"feature1\\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \\"feature2\\": [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \\"category\\": [\\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\", \\"D\\", \\"D\\", \\"E\\", \\"E\\"] } df = pd.DataFrame(data) ``` You might visualize this DataFrame such that different categories get different colors from the qualitative palette, while continuous data points follow the continuous colormap gradient. **Constraints:** - Ensure efficient handling of palettes. - Your plots should be aesthetically pleasing and informative. **Notes:** Remember to import the necessary libraries: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_palettes(data): Given a DataFrame with necessary plot data, create a multi-category plot with specified colormaps. Parameters: data (pd.DataFrame): A pandas DataFrame containing at least two numerical columns and one categorical column. Returns: None: The function should display a Seaborn plot directly. # Generate colormaps using mpl_palette qualitative_palette = sns.mpl_palette(\\"Set2\\", len(data[\'category\'].unique())) continuous_palette = sns.mpl_palette(\\"viridis\\", as_cmap=True) # Create a scatter plot using seaborn plt.figure(figsize=(10, 6)) # Plot each category with a separate color from the qualitative palette categories = data[\'category\'].unique() for idx, category in enumerate(categories): subset = data[data[\'category\'] == category] sns.scatterplot( x=\'feature1\', y=\'feature2\', hue=\'feature2\', data=subset, palette=continuous_palette, legend=False, s=100, edgecolor=\\"w\\", label=f\'Category {category}\', color=qualitative_palette[idx] ) # Customize plot plt.title(\\"Advanced Seaborn Color Palette Visualization\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.legend(title=\\"Categories\\") plt.grid(True) plt.show()"},{"question":"# MIME Type Command Lookup and Execution Handling Objective: You are tasked with implementing a Python function that mimics part of the functionality provided by the `mailcap` module. Specifically, you will write a function to lookup and return the command to execute based on a given MIME type. Function Signature: ```python def find_mailcap_command(caps: dict, MIMEtype: str, key: str = \'view\', filename: str = \'/dev/null\', plist: list = []) -> tuple: ``` Input: 1. `caps`: A dictionary mapping MIME types to a list of mailcap file entries. An entry is a dictionary with at least a `key` (like \'view\') and a command line string where %s can be replaced with a filename. 2. `MIMEtype`: A string representing the MIME type (e.g., \'video/mpeg\'). 3. `key`: A string representing the type of activity to perform (default: \'view\'). 4. `filename`: A string to be substituted for \'%s\' in the command line (default: \'/dev/null\'). 5. `plist`: A list of named parameters (default: empty list). Each parameter is given as \'name=value\' and can replace corresponding \'%{name}\' entries in the command line. Output: - A tuple containing: - The command line string ready to be executed (with replaced parameters). - The mailcap entry dictionary for the specified MIME type. Constraints and Requirements: - Filenames and parameter values should only contain alphanumeric characters and `@+=:,./-_`. - If invalid characters are detected, return `(None, None)` as if no entry was found. - Handle entries with optional \'test\' conditions before considering a match. - Ignore entries that fail \'test\' conditions or have invalid characters. Example: ```python caps = { \'video/mpeg\': [ {\'view\': \'xmpeg %s\', \'compose\': \'composempeg %s\'}, {\'view\': \'altmpeg %{id} %{number}\', \'test\': \'true if some condition is met\'} ], \'image/png\': [ {\'view\': \'display %s\'} ] } # Example usage print(find_mailcap_command(caps, \'video/mpeg\', filename=\'tmp1223\')) # Expected Output: (\'xmpeg tmp1223\', {\'view\': \'xmpeg %s\'}) print(find_mailcap_command(caps, \'video/mpeg\', key=\'compose\', filename=\'file.mpg\')) # Expected Output: (\'composempeg file.mpg\', {\'compose\': \'composempeg %s\'}) print(find_mailcap_command(caps, \'image/png\', filename=\'picture.png\')) # Expected Output: (\'display picture.png\', {\'view\': \'display %s\'}) ``` Notes: - Focus on reading and parsing the dictionary structures correctly. - Ensure that security checks for valid characters in filenames and parameters are enforced. - Assume for this exercise that if \'test\' is present in a mailcap entry, it always returns `True` for simplicity.","solution":"import re def find_mailcap_command(caps: dict, MIMEtype: str, key: str = \'view\', filename: str = \'/dev/null\', plist: list = []) -> tuple: Looks up and returns the command to execute based on a given MIME type. Args: caps (dict): Dictionary mapping MIME types to mailcap entries. MIMEtype (str): String representing the MIME type. key (str): Type of activity to perform, default is \'view\'. filename (str): Filename to replace %s in the command. plist (list): List of named parameters to replace in the command. Returns: tuple: (command, mailcap_entry) where command is the command to execute and mailcap_entry is the dictionary entry for the MIME type. # Validate filename and parameters if not re.match(r\'^[a-zA-Z0-9@+=:,./_-]*\', filename): return (None, None) for param in plist: if not re.match(r\'^[a-zA-Z0-9@+=:,./_-]*\', param): return (None, None) # Check for MIME type and key in caps if MIMEtype in caps: for entry in caps[MIMEtype]: if key in entry: # If test is present, assume it passes for simplicity command = entry[key] # Replace %s with filename command = command.replace(\\"%s\\", filename) # Replace %{name} with values from plist for param in plist: name, value = param.split(\\"=\\") command = command.replace(f\\"%{{{name}}}\\", value) return (command, entry) return (None, None)"},{"question":"**Objective**: Demonstrate understanding and application of Seaborn\'s regression plot functionalities to explore and visualize relationships in a dataset. **Problem Statement**: You are provided with a dataset containing information on automobile performance (`mpg` dataset). Your task is to perform different types of regression analyses and visualizations on this dataset using the Seaborn library. Implement the following functions that each perform a specific task. # Dataset: - Load the `mpg` dataset from seaborn: `mpg = sns.load_dataset(\\"mpg\\")` # Tasks: 1. **Basic Regression Plot**: - Plot a basic regression plot showing the relationship between `\'weight\'` (as x-axis) and `\'acceleration\'` (as y-axis). - Implement the function `basic_regression_plot()`. ```python def basic_regression_plot(): import seaborn as sns import matplotlib.pyplot as plt mpg = sns.load_dataset(\\"mpg\\") sns.set_theme() sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\") plt.show() ``` 2. **Polynomial Regression**: - Fit a second-order polynomial regression to show the relationship between `\'weight\'` (as x-axis) and `\'mpg\'` (as y-axis). - Implement the function `polynomial_regression_plot()`. ```python def polynomial_regression_plot(): import seaborn as sns import matplotlib.pyplot as plt mpg = sns.load_dataset(\\"mpg\\") sns.set_theme() sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) plt.show() ``` 3. **Log-Linear Regression**: - Fit a log-linear regression to show the relationship between `\'displacement\'` (as x-axis) and `\'mpg\'` (as y-axis). - Implement the function `log_linear_regression_plot()`. ```python def log_linear_regression_plot(): import seaborn as sns import matplotlib.pyplot as plt mpg = sns.load_dataset(\\"mpg\\") sns.set_theme() sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", logx=True) plt.show() ``` 4. **Customizing Plot Appearance**: - Create a regression plot of `\'weight\'` (as x-axis) vs. `\'horsepower\'` (as y-axis), with the following customizations: - Confidence interval of 99%. - Marker as `\'x\'`. - Marker color `.3` (grayish tone). - Line color as red. - Implement the function `customized_regression_plot()`. ```python def customized_regression_plot(): import seaborn as sns import matplotlib.pyplot as plt mpg = sns.load_dataset(\\"mpg\\") sns.set_theme() sns.regplot( data=mpg, x=\\"weight\\", y=\\"horsepower\\", ci=99, marker=\\"x\\", color=\\".3\\", line_kws=dict(color=\\"r\\"), ) plt.show() ``` **Constraints**: - Use the Seaborn library for visualizations. - Ensure each function separately performs the specific plotting tasks. - The solution should be efficient and make use of appropriate Seaborn functionalities for the tasks. - Each function should display the respective plots when called. These tasks will evaluate your understanding and ability to use Seaborn for various regression analysis and customization of plots.","solution":"def basic_regression_plot(): import seaborn as sns import matplotlib.pyplot as plt mpg = sns.load_dataset(\\"mpg\\") sns.set_theme() sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\") plt.show() def polynomial_regression_plot(): import seaborn as sns import matplotlib.pyplot as plt mpg = sns.load_dataset(\\"mpg\\") sns.set_theme() sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) plt.show() def log_linear_regression_plot(): import seaborn as sns import matplotlib.pyplot as plt mpg = sns.load_dataset(\\"mpg\\") sns.set_theme() sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", logx=True) plt.show() def customized_regression_plot(): import seaborn as sns import matplotlib.pyplot as plt mpg = sns.load_dataset(\\"mpg\\") sns.set_theme() sns.regplot( data=mpg, x=\\"weight\\", y=\\"horsepower\\", ci=99, marker=\\"x\\", color=\\".3\\", line_kws=dict(color=\\"r\\"), ) plt.show()"},{"question":"**Question**: You are tasked with creating a Python script that generates a detailed report on the current machine and Python interpreter version. This report will be useful for debugging, deployment logging, and providing metadata for system administrators or developers. **Instructions**: Write a Python function called `generate_system_report()` that performs the following tasks: 1. Gathers detailed information about the current system and Python interpreter using the platform module. 2. Specifically, your function should collect the following information: - System/OS name (`platform.system()`) - Node/hostname (`platform.node()`) - System release (`platform.release()`) - System version (`platform.version()`) - Machine type (`platform.machine()`) - Processor name (`platform.processor()`) - Python build number and date (`platform.python_build()`) - Python compiler used (`platform.python_compiler()`) - Python implementation (`platform.python_implementation()`) - Python version as a string (`platform.python_version()`) 3. Format the gathered information into a readable string report. The output report should look something like this: ``` System Report ============= System Name : <system_name> Node/Hostname : <node_name> System Release : <system_release> System Version : <system_version> Machine Type : <machine_type> Processor Name : <processor_name> Python Build Number : <build_number> Python Build Date : <build_date> Python Compiler : <compiler> Python Implementation: <implementation> Python Version : <version> ``` **Function Signature**: ```python def generate_system_report() -> str: pass ``` **Constraints**: - The solution should not use any other package outside of the Python standard library. - Ensure that the function handles cases where certain information may not be retrievable (empty strings should be used in such cases). **Examples**: ```python print(generate_system_report()) ``` **Expected Output Example** (the actual output will depend on your system): ``` System Report ============= System Name : Linux Node/Hostname : my-hostname System Release : 5.8.0-53-generic System Version : #60~20.04.1-Ubuntu SMP Thu May 6 09:52:56 UTC 2021 Machine Type : x86_64 Processor Name : x86_64 Python Build Number : (\'default\', \'May 3 2021 08:02:01\') Python Build Date : (\'default\', \'May 3 2021 08:02:01\') Python Compiler : GCC 7.5.0 Python Implementation: CPython Python Version : 3.8.10 ``` **Evaluation Criteria**: 1. Correctness: Does the function return the required system details accurately? 2. Readability: Is the output formatted correctly and clearly? 3. Error Handling: Does the function handle cases where certain details may not be available? Good Luck!","solution":"import platform def generate_system_report() -> str: Generates a system report containing detailed information about the current machine and Python interpreter version. system_name = platform.system() node_name = platform.node() system_release = platform.release() system_version = platform.version() machine = platform.machine() processor = platform.processor() python_build = platform.python_build() python_compiler = platform.python_compiler() python_implementation = platform.python_implementation() python_version = platform.python_version() # Format the system report report = ( \\"System Reportn\\" \\"=============n\\" f\\"System Name : {system_name}n\\" f\\"Node/Hostname : {node_name}n\\" f\\"System Release : {system_release}n\\" f\\"System Version : {system_version}n\\" f\\"Machine Type : {machine}n\\" f\\"Processor Name : {processor}n\\" f\\"Python Build Number : {python_build[0]}n\\" f\\"Python Build Date : {python_build[1]}n\\" f\\"Python Compiler : {python_compiler}n\\" f\\"Python Implementation: {python_implementation}n\\" f\\"Python Version : {python_version}n\\" ) return report"},{"question":"# Custom Content Manager Implementation Objective Create a custom content manager class that extends `email.contentmanager.ContentManager` to handle a specific type of custom MIME content. This custom content type is `application/x-custom`. Specifications 1. **Class Definition**: Define a class `CustomContentManager` that inherits from `ContentManager`. 2. **Custom MIME Type**: Implement handlers within the `CustomContentManager` to manage the custom MIME type `application/x-custom`. 3. **get_content Method**: - Should extract content from a MIME message of type `application/x-custom`. - The payload should be returned as a string. 4. **set_content Method**: - Should set content of type `application/x-custom` into a MIME message. - Should include a custom header `X-Custom-Header` with the value `CustomValue`. 5. **Handler Registration**: - Register the necessary handlers for both `get_content` and `set_content` methods for the MIME type `application/x-custom`. 6. **Testing**: Provide a test function that demonstrates the usage of your `CustomContentManager` with both `get_content` and `set_content`. Input and Output Formats - **Input**: - A MIME message object with content of type `application/x-custom`. - **Output**: - A string payload for `get_content`. - A MIME message updated with the specified content for `set_content`. Constraints - The custom content to be managed will be simple strings. - Detailed error handling for unsupported MIME types or improper content types must be implemented. Performance Requirements - The solution should efficiently handle MIME messages without significant overhead. Example ```python from email import message_from_string from email.contentmanager import ContentManager class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/x-custom\', self._get_custom_content) self.add_set_handler(str, self._set_custom_content) def _get_custom_content(self, msg, *args, **kwargs): return msg.get_payload() def _set_custom_content(self, msg, content, *args, **kwargs): msg.clear_content() msg.set_payload(content) msg[\'Content-Type\'] = \'application/x-custom\' msg[\'X-Custom-Header\'] = \'CustomValue\' def test_custom_content_manager(): manager = CustomContentManager() mime_message = message_from_string( \\"Content-Type: application/x-customnnTest content\\" ) extracted_content = manager.get_content(mime_message) print(extracted_content) # Should print: \'Test content\' manager.set_content(mime_message, \'New custom content\') print(mime_message.as_string()) # Should show the MIME message with \'New custom content\' test_custom_content_manager() ``` In the above example, you will implement and test methods for managing custom MIME content using the `CustomContentManager` class.","solution":"from email.contentmanager import ContentManager from email.message import Message class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/x-custom\', self._get_custom_content) self.add_set_handler(str, self._set_custom_content) def _get_custom_content(self, msg, *args, **kwargs): return msg.get_payload() def _set_custom_content(self, msg, content, *args, **kwargs): msg.clear_content() msg.set_payload(content) msg[\'Content-Type\'] = \'application/x-custom\' msg[\'X-Custom-Header\'] = \'CustomValue\'"},{"question":"# PyTorch Coding Assessment Question on Random Number Generation Background Understanding and utilizing random number generation is crucial in many machine learning tasks. PyTorch provides various functionalities for generating random numbers. For this assessment, you will demonstrate your understanding of PyTorch\'s random number generation capabilities. Task You are required to implement a function `initialize_random_tensor` that generates a 2D tensor of shape `(n, m)` consisting of random numbers. The function should also ensure reproducibility by setting a seed. # Function Signature ```python def initialize_random_tensor(n: int, m: int, seed: int) -> torch.Tensor: pass ``` # Input - `n` (int): The number of rows for the 2D tensor. - `m` (int): The number of columns for the 2D tensor. - `seed` (int): The seed value to ensure reproducibility of random numbers. # Output - Returns a 2D tensor of shape `(n, m)` filled with random floating-point numbers between 0 and 1. # Example ```python import torch def initialize_random_tensor(n: int, m: int, seed: int) -> torch.Tensor: torch.manual_seed(seed) return torch.rand(n, m) # Example usage: # initialize_random_tensor(3, 4, 42) # This should always produce the same 3x4 tensor filled with random numbers for a given seed tensor = initialize_random_tensor(3, 4, 42) print(tensor) ``` # Constraints - You need to use only the PyTorch library for random number generation. - Each execution with the same seed and tensor size should produce the same output. # Performance Requirements - Make sure that your solution is efficient and leverages PyTorch\'s native functions.","solution":"import torch def initialize_random_tensor(n: int, m: int, seed: int) -> torch.Tensor: Generates a 2D tensor of shape (n, m) with random floats between 0 and 1. The random generation is reproducible using the specified seed. Args: n (int): Number of rows for the tensor. m (int): Number of columns for the tensor. seed (int): Seed value for reproducibility. Returns: torch.Tensor: A tensor of shape (n, m) with random floats. torch.manual_seed(seed) return torch.rand(n, m)"},{"question":"# Unicode and Encodings - Handling and Comparison **Problem Statement:** You are provided with strings in different Unicode encodings. Your task is to write a Python function that: 1. Ensures proper encoding and decoding of these strings. 2. Handles any errors that might arise during the decoding process. 3. Normalizes the strings for comparison. 4. Provides a function to compare these strings in a case-insensitive and encoding-independent manner. **Function Signature:** ```python def compare_unicode_strings(s1: bytes, s2: bytes, encoding1: str, encoding2: str) -> bool: Compare two bytes strings which are respectively encoded in `encoding1` and `encoding2`. The comparison should be case-insensitive and normalization-insensitive. Args: s1 (bytes): First string in bytes. s2 (bytes): Second string in bytes. encoding1 (str): Encoding of the first string. encoding2 (str): Encoding of the second string. Returns: bool: True if strings are equivalent in a case-insensitive and normalization-insensitive way, False otherwise. ``` # Details 1. **Input and Output Formats**: - The function accepts two byte strings `s1` and `s2`, along with their respective encodings `encoding1` and `encoding2`. - The function returns a boolean value `True` if the strings are equivalent in a case-insensitive and normalization-insensitive manner, `False` otherwise. 2. **Constraints**: - Strings can be in any encoding supported by Python. - Errors during decoding should be handled gracefully using the \'ignore\' strategy. 3. **Steps**: - Decode the byte strings into Unicode using the provided encodings. - Normalize the Unicode strings to a common form. - Convert both strings to a case-insensitive form. - Compare the resulting strings for equivalence. 4. **Performance Requirements**: - The solution should handle strings of reasonable length efficiently. # Example ```python assert compare_unicode_strings(b\'cafxc3xa9\', b\'CAFxc3x89\', \'utf-8\', \'utf-8\') == True assert compare_unicode_strings(b\'xe9\', b\'exccx81\', \'latin-1\', \'utf-8\') == True assert compare_unicode_strings(b\'xe9\', b\'e\', \'latin-1\', \'utf-8\') == False ``` **Hints**: - Use Python\'s `unicodedata` module for normalization. - Use the `decode` method to convert bytes to Unicode strings. - Ensure you handle cases where decoding might fail due to invalid byte sequences. **Note**: Please make use of appropriate normalization forms (NFD, NFC) and case-folding techniques as discussed in the provided documentation.","solution":"import unicodedata def compare_unicode_strings(s1: bytes, s2: bytes, encoding1: str, encoding2: str) -> bool: Compare two bytes strings which are respectively encoded in `encoding1` and `encoding2`. The comparison should be case-insensitive and normalization-insensitive. Args: s1 (bytes): First string in bytes. s2 (bytes): Second string in bytes. encoding1 (str): Encoding of the first string. encoding2 (str): Encoding of the second string. Returns: bool: True if strings are equivalent in a case-insensitive and normalization-insensitive way, False otherwise. # Decode the byte strings using the specified encodings, ignoring errors str1 = s1.decode(encoding1, errors=\'ignore\') str2 = s2.decode(encoding2, errors=\'ignore\') # Normalize the Unicode strings to a common form (NFC) norm_str1 = unicodedata.normalize(\'NFC\', str1) norm_str2 = unicodedata.normalize(\'NFC\', str2) # Convert both strings to case-insensitive forms casefolded_str1 = norm_str1.casefold() casefolded_str2 = norm_str2.casefold() # Compare the resulting strings return casefolded_str1 == casefolded_str2"},{"question":"# Advanced Python Coding Assessment Objective: The goal of this coding assignment is to demonstrate your understanding of the Python \\"warnings\\" module, including customizing warning handling, filtering warnings, and writing tests for code that issues warnings. Problem Statement: You are tasked with writing a Python function and its corresponding test cases. The function `process_data()` performs some data processing and might encounter deprecated features and runtime issues. You should appropriately issue warnings and handle them as specified. Function Specifications: 1. **Function Name**: `process_data` 2. **Input**: - `data`: A list of integers. 3. **Output**: - Returns a new list where some operations are performed on the input list `data`. Function Details: - If an element in the list `data` is less than 0, issue a `RuntimeWarning` indicating that negative values are not supported but continue processing. - If a deprecated feature is used (e.g., an obsolete method `deprecated_method()`), issue a `DeprecationWarning` but continue processing. - Use the `warnings` module properly to issue these warnings. Testing Requirements: Write a test function `test_process_data()` using the context manager `catch_warnings` to ensure the following: - All `RuntimeWarning` warnings are captured and verified for correct message content. - All `DeprecationWarning` warnings are captured and verified for correct message content. - Ensure no warnings are left unverified. - Ensure that despite warnings, the output of `process_data` is verified for correctness. Example: ```python import warnings # Implementing the process_data function def process_data(data): # A placeholder for deprecated method (you can simply pass or issue the warning directly) def deprecated_method(): warnings.warn(\\"This method is deprecated\\", DeprecationWarning) result = [] for num in data: if num < 0: warnings.warn(f\\"Negative value {num} encountered\\", RuntimeWarning) # Assuming the deprecated method is checked here if num == 5: # Just a condition to trigger deprecated method deprecated_method() result.append(num + 1) return result # Implementing the test function def test_process_data(): with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") # Ensure all warnings are triggered result = process_data([1, -2, 3, 5]) # Check RuntimeWarning was caught runtime_warnings = [warn for warn in w if issubclass(warn.category, RuntimeWarning)] assert len(runtime_warnings) == 1 assert str(runtime_warnings[0].message) == \\"Negative value -2 encountered\\" # Check DeprecationWarning was caught deprecation_warnings = [warn for warn in w if issubclass(warn.category, DeprecationWarning)] assert len(deprecation_warnings) == 1 assert str(deprecation_warnings[0].message) == \\"This method is deprecated\\" # Check the output of process_data is as expected assert result == [2, -1, 4, 6] # Running the test function test_process_data() ``` This function would test your ability to: 1. Issue different types of warnings. 2. Correctly handle and filter warnings. 3. Write robust tests for functions that issue warnings.","solution":"import warnings # Implementing the process_data function def process_data(data): # A placeholder for deprecated method (you can simply pass or issue the warning directly) def deprecated_method(): warnings.warn(\\"This method is deprecated\\", DeprecationWarning) result = [] for num in data: if num < 0: warnings.warn(f\\"Negative value {num} encountered\\", RuntimeWarning) # Assuming the deprecated method is checked here if num == 5: # Just a condition to trigger deprecated method deprecated_method() result.append(num + 1) return result"},{"question":"# Question: You are tasked with implementing a function that reads a given file, compresses its contents using a custom LZMA filter, and then decompresses it to verify integrity. Your function should ensure that the decompressed data is identical to the original file contents. Function Signature: ```python def verify_lzma_compression(file_path: str, filter_options: dict) -> bool: ``` Parameters: - `file_path` (str): Path to the file that needs to be compressed and then decompressed. - `filter_options` (dict): The filter chain options to be used for LZMA compression. The dictionary should follow the format specified in the `lzma` module documentation. Returns: - `bool`: Returns `True` if the decompressed data matches the original file contents; otherwise, returns `False`. Implementation Requirements: 1. Use `lzma.open` or `lzma.compress` for the compression part. 2. Use `lzma.decompress` for the decompression part. 3. Handle potential exceptions that might occur during reading, writing, compression, and decompression. 4. Apply the custom filters specified in the `filter_options` dictionary. Constraints: - Assume that the file size will not exceed available memory constraints. - The `filter_options` dictionary will be well-formed as per the `lzma` module documentation. Example: ```python filter_opts = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6} ] # Assume \'sample.txt\' contains the text \\"Hello, world!\\" result = verify_lzma_compression(\'sample.txt\', filter_opts) print(result) # Expected output: True ``` # Notes: - Use appropriate methods from the `lzma` module to achieve the task. - Make sure to clean up any temporary files if you create any during the process.","solution":"import lzma import os def verify_lzma_compression(file_path: str, filter_options: dict) -> bool: try: # Read the original file contents with open(file_path, \'rb\') as file: original_data = file.read() # Compress the data using the provided filter options compressed_data = lzma.compress(original_data, filters=filter_options) # Decompress the data decompressed_data = lzma.decompress(compressed_data) # Verify integrity by comparing original and decompressed data return original_data == decompressed_data except Exception as e: print(f\\"An exception occurred: {e}\\") return False"},{"question":"# Coding Assessment Question Objective Your task is to implement a Python function that simulates the behavior of a complex data parser using the concepts described in the provided documentation. The function will accept a mixed list of data, and should parse and convert these data based on specified rules. Function Signature ```python def complex_data_parser(data: list) -> tuple: Parses a list of mixed data types and categorizes them into different formats. Args: data (list): A list containing various data types such as strings, integers, floats, and bytes. Returns: tuple: A tuple containing the parsed data categorized into strings, integers, floats, and bytes-like objects. ``` Input - `data`: A list of elements where each element can be of the following types: - **str**: Should be converted to a Utf-8 encoded string. - **int**: Should be categorized as an integer. - **float**: Should be categorized as a floating-point number. - **bytes**: Should be categorized as a bytes-like object. Output - The function should return a tuple `(strings, integers, floats, bytes_objects)` where: - `strings`: A list of Utf-8 encoded strings. - `integers`: A list of integers. - `floats`: A list of floating-point numbers. - `bytes_objects`: A list of bytes-like objects. Example ```python data = [\'hello\', 42, 3.14, b\'binary\', \'world\', 100, 6.28, b\'data\'] expected_result = ( [\'hello\', \'world\'], [42, 100], [3.14, 6.28], [b\'binary\', b\'data\'] ) assert complex_data_parser(data) == expected_result ``` Constraints - The input list can contain up to 1000 elements. - Each string element will be valid utf-8 encoded text and will not contain embedded null characters. - Bytes-like objects will not contain embedded null bytes. Notes - Ensure that the categorization is efficient and handles each type correctly, leveraging the concepts outlined in the provided documentation. - Pay close attention to memory management and type conversions where necessary.","solution":"def complex_data_parser(data: list) -> tuple: Parses a list of mixed data types and categorizes them into different formats. Args: data (list): A list containing various data types such as strings, integers, floats, and bytes. Returns: tuple: A tuple containing the parsed data categorized into strings, integers, floats, and bytes-like objects. strings = [] integers = [] floats = [] bytes_objects = [] for item in data: if isinstance(item, str): strings.append(item) elif isinstance(item, int): integers.append(item) elif isinstance(item, float): floats.append(item) elif isinstance(item, bytes): bytes_objects.append(item) return (strings, integers, floats, bytes_objects)"},{"question":"You are working on a shared resource management system using the `asyncio` module\'s synchronization primitives. Your task is to implement an asynchronous function `coordinate_tasks` that manages access to a shared resource among multiple tasks. **Function Signature:** ```python async def coordinate_tasks(num_workers: int, task_duration: float): ``` Requirements: 1. **Parameters**: - `num_workers`: An integer representing the number of concurrent worker tasks that will attempt to access the shared resource. - `task_duration`: A float representing the amount of time (in seconds) each worker task will simulate work for when it has access to the shared resource. 2. **Functionality**: - The function should create a `Semaphore` with a value of 3, allowing up to 3 tasks to access the shared resource concurrently. - Each worker should: - Acquire the semaphore. - Print a message indicating it has started working, including the worker\'s identifier. - Simulate the work by sleeping for `task_duration` seconds. - Print a message indicating it has finished working. - Release the semaphore. - The main function `coordinate_tasks` should ensure all worker tasks are completed before it finishes. 3. **Constraints**: - Use the `asyncio` module for all asynchronous operations. - The solution should demonstrate the correct use of `asyncio.Semaphore` for controlling access to the shared resource. Example: ```python import asyncio async def example(): await coordinate_tasks(5, 2) # Expected Output: # Worker 0 started working... # Worker 1 started working... # Worker 2 started working... # Worker 0 finished working. # Worker 3 started working... # Worker 1 finished working. # Worker 4 started working... # Worker 2 finished working. # Worker 3 finished working. # Worker 4 finished working. # To run example asyncio.run(example()) ``` # Notes: - The output order might vary based on the scheduling of asyncio tasks, but it should reflect the concurrent nature of up to 3 workers at a time. - Ensure that your implementation is clean and follows best practices for using asyncio synchronization primitives.","solution":"import asyncio async def worker(worker_id, task_duration, semaphore): async with semaphore: print(f\\"Worker {worker_id} started working...\\") await asyncio.sleep(task_duration) print(f\\"Worker {worker_id} finished working.\\") async def coordinate_tasks(num_workers: int, task_duration: float): semaphore = asyncio.Semaphore(3) tasks = [] for i in range(num_workers): task = asyncio.create_task(worker(i, task_duration, semaphore)) tasks.append(task) await asyncio.gather(*tasks) # Below lines are not part of the solution module. They are for example purposes. # Here\'s how this function could be run: # asyncio.run(coordinate_tasks(5, 2))"},{"question":"**HTML Manipulation with Python** You are to implement two functions that utilize the `html` module\'s escape and unescape utilities. # Task: 1. Implement a function `sanitize_html(input_str: str, quote: bool = True) -> str` that takes an input string and returns the HTML-escaped version of it. If `quote` is set to True, the function should also escape double and single quotes. 2. Implement a function `restore_html(escaped_str: str) -> str` that takes an HTML-escaped string and returns the original string by reversing the escaping process. # Input and Output Format: - **sanitize_html(input_str: str, quote: bool = True) -> str** - `input_str`: A string representing the original text. - `quote`: A boolean flag indicating whether to escape the quote characters. Default is True. - **Returns**: A string with HTML-escaped characters. - **restore_html(escaped_str: str) -> str** - `escaped_str`: A string with HTML-escaped characters. - **Returns**: The original string with characters unescaped. # Example: ```python # Example for sanitize_html input_str = \'Hello, \\"world\\" & universe!\' escaped_str = sanitize_html(input_str) print(escaped_str) # Output: \'Hello, &quot;world&quot; &amp; universe!\' # Example for restore_html escaped_str = \'Hello, &quot;world&quot; &amp; universe!\' original_str = restore_html(escaped_str) print(original_str) # Output: \'Hello, \\"world\\" & universe!\' ``` # Constraints: - The input strings will contain only commonly used characters in HTML documents. - Your implementation should handle all edge cases like empty strings and strings without any escapable characters. # Notes: - Make use of the functions `html.escape` and `html.unescape` to implement the tasks. - Ensure that your function handles both quotes and normal character escape sequences properly.","solution":"import html def sanitize_html(input_str: str, quote: bool = True) -> str: Returns the HTML-escaped version of the input string. If \'quote\' is True, it also escapes double and single quotes. return html.escape(input_str, quote=quote) def restore_html(escaped_str: str) -> str: Returns the original version of the HTML-escaped input string by reversing the escaping process. return html.unescape(escaped_str)"},{"question":"# Dictionary Manipulation and Merging You are required to implement a Python function that performs complex operations on dictionaries using the functionalities provided in the python310 package. This function will involve creating dictionaries, inserting and retrieving items, and merging dictionaries with specific conditions. Function Specifications Your task is to implement the following function: ```python def complex_dict_operations(dict_a, dict_b, operations): Perform complex operations on two dictionaries dict_a and dict_b based on the operations specified. Parameters: dict_a (dict): The first dictionary for initial manipulation. dict_b (dict): The second dictionary for merging with dict_a. operations (list): A list of tuples where each tuple specifies an operation to be performed. The first element of the tuple is a string (\'set\', \'delete\', \'get\', \'merge\'), and the rest are the parameters for the operation. - For \'set\', the tuple will be (\'set\', key, value) - For \'delete\', the tuple will be (\'delete\', key) - For \'get\', the tuple will be (\'get\', key) - For \'merge\', the tuple will be (\'merge\',) Returns: Any: The results of the \'get\' operations as a list. If no \'get\' operations are specified, return an empty list. pass ``` Operations Explanation 1. **\'set\' Operation**: Insert or update a key-value pair in `dict_a`. - Tuple format: (\'set\', key, value) 2. **\'delete\' Operation**: Delete a key from `dict_a`. - Tuple format: (\'delete\', key) 3. **\'get\' Operation**: Retrieve a value from `dict_a` by key. - Tuple format: (\'get\', key) - The result should be added to a list that is returned at the end. 4. **\'merge\' Operation**: Merge `dict_b` into `dict_a`. If a key in `dict_b` already exists in `dict_a`, update the value in `dict_a` with the value from `dict_b`. - Tuple format: (\'merge\',) Example ```python dict_a = {\'a\': 1, \'b\': 2} dict_b = {\'b\': 3, \'c\': 4} operations = [ (\'get\', \'a\'), (\'set\', \'d\', 5), (\'delete\', \'b\'), (\'merge\',), (\'get\', \'b\'), (\'get\', \'c\') ] result = complex_dict_operations(dict_a, dict_b, operations) print(result) ``` Expected output: ``` [1, 3, 4] ``` In this example: - The value for key \'a\' is retrieved and added to the result list. - Key \'d\' with value 5 is set in `dict_a`. - Key \'b\' is deleted from `dict_a`. - `dict_b` is merged into `dict_a` (key \'b\' is updated to 3, key \'c\' is added). - The value for key \'b\' after merging is retrieved and added to the result list. - The value for key \'c\' is retrieved and added to the result list. Constraints - The keys in the dictionaries are strings. - The values in the dictionaries can be of any data type. - The operations list will contain a maximum of 1000 operations. - For \'get\', if the key does not exist, return `None` for that operation. Focus on implementing the function in a way that leverages the provided python310 package functions efficiently.","solution":"def complex_dict_operations(dict_a, dict_b, operations): Perform complex operations on two dictionaries dict_a and dict_b based on the operations specified. Parameters: dict_a (dict): The first dictionary for initial manipulation. dict_b (dict): The second dictionary for merging with dict_a. operations (list): A list of tuples where each tuple specifies an operation to be performed. The first element of the tuple is a string (\'set\', \'delete\', \'get\', \'merge\'), and the rest are the parameters for the operation. - For \'set\', the tuple will be (\'set\', key, value) - For \'delete\', the tuple will be (\'delete\', key) - For \'get\', the tuple will be (\'get\', key) - For \'merge\', the tuple will be (\'merge\',) Returns: Any: The results of the \'get\' operations as a list. If no \'get\' operations are specified, return an empty list. results = [] for operation in operations: if operation[0] == \'set\': _, key, value = operation dict_a[key] = value elif operation[0] == \'delete\': _, key = operation if key in dict_a: del dict_a[key] elif operation[0] == \'get\': _, key = operation results.append(dict_a.get(key)) elif operation[0] == \'merge\': dict_a.update(dict_b) return results"},{"question":"Problem Statement You are required to design a class called `ArrayOperations` that utilizes Python\'s `array` module to handle arrays of integers. This class should support various operations to manipulate the array. Implement the following methods in the `ArrayOperations` class: 1. **Constructor `__init__(self, typecode: str, initializer: list = [])`**: - Initializes an array with the given typecode and optional initializer. - Ensure the typecode is valid for integer types (`\'b\'`, `\'B\'`, `\'h\'`, `\'H\'`, `\'i\'`, `\'I\'`, `\'l\'`, `\'L\'`, `\'q\'`, `\'Q\'`). 2. **`append(self, value: int) -> None`**: - Appends an integer value to the end of the array. 3. **`insert(self, index: int, value: int) -> None`**: - Inserts an integer value at the specified index in the array. 4. **`remove(self, value: int) -> None`**: - Removes the first occurrence of the specified integer value from the array. 5. **`pop(self, index: int = -1) -> int`**: - Removes and returns the integer value at the specified index. Defaults to the last item. 6. **`reverse(self) -> None`**: - Reverses the order of the items in the array. 7. **`count(self, value: int) -> int`**: - Returns the number of occurrences of the specified integer value in the array. 8. **`to_list(self) -> list`**: - Converts the array to a Python list and returns it. 9. **`from_list(self, values: list) -> None`**: - Extends the array with integer values from a Python list. 10. **`byteswap(self) -> None`**: - Swaps the byte order of all items in the array. Ensure it only supports arrays with item sizes of 1, 2, 4, or 8 bytes. 11. **`buffer_info(self) -> tuple`**: - Returns a tuple containing the memory address and the length of the buffer used to hold the array\'s contents. 12. **`to_bytes(self) -> bytes`**: - Converts the array to a bytes representation and returns it. 13. **`from_bytes(self, byte_string: bytes) -> None`**: - Appends items from the byte string, interpreting it as an array of machine values, to the existing array. # Expected Input and Output Formats - The typecode should be a string from the valid integer typecodes. - All methods that take `value` as an argument expect integers. - `index` values should be within the range of the array indices. - The `to_list()` method returns a list of integers. - The `count(value)` method returns an integer. - The `buffer_info()` method returns a tuple `(address, length)` where `address` is an integer and `length` is the number of items. - The `to_bytes()` method returns bytes. - If invalid operations (e.g., removing a non-existent value, invalid typecode) are performed, appropriate Python exceptions should be raised. # Example Usage ```python # Creating an instance of ArrayOperations with typecode \'i\' array_ops = ArrayOperations(\'i\', [1, 2, 3]) # Appending values to the array array_ops.append(4) # Inserting a value in the array array_ops.insert(2, 10) # Removing a value from the array array_ops.remove(2) # Popping and reversing the array print(array_ops.pop()) # Output will be 4 array_ops.reverse() # Counting occurrences and converting to list print(array_ops.count(1)) # Output will be 1 print(array_ops.to_list()) # Output will be the list representation of the array # Using byteswap and buffer_info methods array_ops.byteswap() print(array_ops.buffer_info()) # Converting to bytes and back byte_data = array_ops.to_bytes() array_ops.from_bytes(byte_data) print(array_ops.to_list()) ``` Implement the `ArrayOperations` class, ensuring proper usage of the `array` module as described in the documentation.","solution":"import array class ArrayOperations: def __init__(self, typecode: str, initializer: list = []): if typecode not in \'bBhHiIlLqQ\': raise ValueError(\\"Typecode must be one of \'bBhHiIlLqQ\'\\") self.array = array.array(typecode, initializer) def append(self, value: int) -> None: self.array.append(value) def insert(self, index: int, value: int) -> None: self.array.insert(index, value) def remove(self, value: int) -> None: self.array.remove(value) def pop(self, index: int = -1) -> int: return self.array.pop(index) def reverse(self) -> None: self.array.reverse() def count(self, value: int) -> int: return self.array.count(value) def to_list(self) -> list: return self.array.tolist() def from_list(self, values: list) -> None: self.array.extend(values) def byteswap(self) -> None: if self.array.itemsize not in [1, 2, 4, 8]: raise RuntimeError(\\"Byte swapping only supported for item sizes of 1, 2, 4, or 8 bytes\\") self.array.byteswap() def buffer_info(self) -> tuple: return self.array.buffer_info() def to_bytes(self) -> bytes: return self.array.tobytes() def from_bytes(self, byte_string: bytes) -> None: self.array.frombytes(byte_string)"},{"question":"<|Analysis Begin|> The provided documentation focuses on the `Index` objects and their different types in the pandas library, such as `RangeIndex`, `CategoricalIndex`, `IntervalIndex`, `MultiIndex`, `DatetimeIndex`, `TimedeltaIndex`, and `PeriodIndex`. The documentation includes various methods and properties for each type of Index object, covering aspects like properties, modifying and computations, handling missing values, conversions, sorting, time-specific operations, combining/joining/set operations, and selecting elements. Given this detailed information, we can design a question that tests students\' understanding of how to interact with `Index` objects in pandas. The question can be centered around creating, manipulating, and querying different types of `Index` objects within a DataFrame. The student will need to implement functions that perform specific tasks using these Index methods and properties. <|Analysis End|> <|Question Begin|> # Manipulating and Querying Index Objects Objective In this task, you will be required to write multiple functions to demonstrate your understanding of manipulating and querying different types of `Index` objects in a pandas DataFrame. You will implement functions to create, modify, and perform operations on `Index` objects. Input and Output Formats 1. **Function 1: create_index** - **Input**: - `idx_type`: string, type of Index (`range`, `categorical`, `datetime`, etc.) - `data`: a list of elements or tuples (depending on `idx_type`) - **Output**: - A pandas Index object of the specified type. 2. **Function 2: perform_operations** - **Input**: - `index`: pandas Index object - **Output**: - A dictionary containing the results of various operations: - `\'is_monotonic_increasing\'`: boolean - `\'is_unique\'`: boolean - `\'nunique\'`: integer (number of unique elements) - `\'hasnans\'`: boolean (if the index has NaN values) - **Constraints**: - None 3. **Function 3: convert_index** - **Input**: - `index`: pandas Index object - `to_type`: string, type to convert to (`list`, `numpy`, `series`, `frame`) - **Output**: - Converted object (list, numpy array, pandas Series, or pandas DataFrame) Code Stubs ```python import pandas as pd def create_index(idx_type, data): Create a pandas Index object of the specified type. Args: - idx_type (str): Type of Index (\'range\', \'categorical\', \'interval\', \'multi\', \'datetime\', \'timedelta\', \'period\') - data (list): List of elements or tuples to create the index Returns: - pd.Index object: The created Index object if idx_type == \'range\': return pd.RangeIndex(start=data[0], stop=data[1], step=data[2]) elif idx_type == \'categorical\': return pd.CategoricalIndex(data) elif idx_type == \'interval\': return pd.IntervalIndex.from_tuples(data) elif idx_type == \'multi\': return pd.MultiIndex.from_tuples(data) elif idx_type == \'datetime\': return pd.to_datetime(data) elif idx_type == \'timedelta\': return pd.to_timedelta(data) elif idx_type == \'period\': return pd.period_range(start=data[0], end=data[1], freq=data[2]) else: raise ValueError(\'Unsupported index type\') def perform_operations(index): Perform various operations on the pandas Index object. Args: - index (pd.Index): A pandas Index object Returns: - dict: Results of various operations { \'is_monotonic_increasing\': bool, \'is_unique\': bool, \'nunique\': int, \'hasnans\': bool } result = { \'is_monotonic_increasing\': index.is_monotonic_increasing, \'is_unique\': index.is_unique, \'nunique\': index.nunique(), \'hasnans\': index.hasnans, } return result def convert_index(index, to_type): Convert the index to another data type. Args: - index (pd.Index): A pandas Index object - to_type (str): Target type to convert to (\'list\', \'numpy\', \'series\', \'frame\') Returns: - Object of the target type if to_type == \'list\': return index.to_list() elif to_type == \'numpy\': return index.to_numpy() elif to_type == \'series\': return index.to_series() elif to_type == \'frame\': return index.to_frame() else: raise ValueError(\'Unsupported conversion type\') ``` Example Usage ```python # Example for creating a RangeIndex range_index = create_index(\'range\', [0, 10, 1]) print(range_index) # Output: RangeIndex(start=0, stop=10, step=1) # Performing operations on RangeIndex operations_result = perform_operations(range_index) print(operations_result) # Output: # { \'is_monotonic_increasing\': True, \'is_unique\': True, \'nunique\': 10, \'hasnans\': False } # Converting RangeIndex to list converted_index = convert_index(range_index, \'list\') print(converted_index) # Output: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] ``` Constraints - You are only allowed to use pandas methods for manipulating and querying `Index` objects. - Handle inputs and conversion types as specified in the question. Good luck!","solution":"import pandas as pd import numpy as np def create_index(idx_type, data): Create a pandas Index object of the specified type. Args: - idx_type (str): Type of Index (\'range\', \'categorical\', \'interval\', \'multi\', \'datetime\', \'timedelta\', \'period\') - data (list): List of elements or tuples to create the index Returns: - pd.Index object: The created Index object if idx_type == \'range\': return pd.RangeIndex(start=data[0], stop=data[1], step=data[2]) elif idx_type == \'categorical\': return pd.CategoricalIndex(data) elif idx_type == \'interval\': return pd.IntervalIndex.from_tuples(data) elif idx_type == \'multi\': return pd.MultiIndex.from_tuples(data) elif idx_type == \'datetime\': return pd.to_datetime(data) elif idx_type == \'timedelta\': return pd.to_timedelta(data) elif idx_type == \'period\': return pd.period_range(start=data[0], end=data[1], freq=data[2]) else: raise ValueError(\'Unsupported index type\') def perform_operations(index): Perform various operations on the pandas Index object. Args: - index (pd.Index): A pandas Index object Returns: - dict: Results of various operations { \'is_monotonic_increasing\': bool, \'is_unique\': bool, \'nunique\': int, \'hasnans\': bool } result = { \'is_monotonic_increasing\': index.is_monotonic_increasing, \'is_unique\': index.is_unique, \'nunique\': index.nunique(), \'hasnans\': index.hasnans, } return result def convert_index(index, to_type): Convert the index to another data type. Args: - index (pd.Index): A pandas Index object - to_type (str): Target type to convert to (\'list\', \'numpy\', \'series\', \'frame\') Returns: - Object of the target type if to_type == \'list\': return index.to_list() elif to_type == \'numpy\': return index.to_numpy() elif to_type == \'series\': return index.to_series() elif to_type == \'frame\': return index.to_frame() else: raise ValueError(\'Unsupported conversion type\')"},{"question":"You are given a dataset containing information about the average daily temperatures over a year in different cities. Your task is to create a heatmap that visualizes the temperature data using Seaborn\'s `cubehelix_palette` function, ensuring the colors represent the temperature changes logically and visually appealing. Input 1. A Pandas DataFrame named `df` with the following structure: - Columns: Each column represents a different city. - Rows: Each row represents a day of the year, indexed from 0 to 364. Output - A heatmap plot visualizing the temperature data using a cubehelix palette. Constraints and Requirements 1. Use the Seaborn library to create the heatmap. 2. Customize the `cubehelix_palette` with the following specifications: - Use at least 12 colors. - Start the helix at 2. - Rotate the helix to -0.5. - Apply a gamma correction of 0.7. - Set the hue to 0.9. - Have a light luminance of 0.8 and a dark luminance of 0.2. - Reverse the luminance ramp direction. 3. Annotate the plot with the temperature values. 4. Ensure the plot has a title, and axis labels. Here is a starting point: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Example DataFrame (you should use your actual data) data = { \\"City_A\\": [15, 16, 14, 13, 14, 15, 17], # Add 365 values for a full year \\"City_B\\": [12, 14, 13, 14, 16, 17, 18], # Add 365 values for a full year } df = pd.DataFrame(data) def plot_temperature_heatmap(df): sns.set_theme() # Create a cubehelix palette cubehelix = sns.cubehelix_palette(n_colors=12, start=2, rot=-0.5, gamma=0.7, hue=0.9, dark=0.2, light=0.8, reverse=True, as_cmap=True) # Create a heatmap plt.figure(figsize=(10, 8)) heatmap = sns.heatmap(df, cmap=cubehelix, annot=True) # Add titles and labels plt.title(\\"Average Daily Temperatures\\") plt.xlabel(\\"Cities\\") plt.ylabel(\\"Day of Year\\") plt.show() # Call the function with the DataFrame plot_temperature_heatmap(df) ``` You need to implement the `plot_temperature_heatmap` function which meets all specified requirements. Tips - Ensure your DataFrame (`df`) has 365 rows and the appropriate number of columns for cities. - Validate your DataFrame for any NaN values or inconsistencies before plotting. - Customize the heatmap according to the specifications to ensure clarity and visual appeal.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_temperature_heatmap(df): Plots a heatmap of the average daily temperatures for different cities using a customized cubehelix palette. Parameters: df (pd.DataFrame): DataFrame containing daily temperature data for different cities. sns.set_theme() # Create a cubehelix palette with the specified parameters cubehelix = sns.cubehelix_palette(n_colors=12, start=2, rot=-0.5, gamma=0.7, hue=0.9, dark=0.2, light=0.8, reverse=True, as_cmap=True) # Create a heatmap plt.figure(figsize=(15, 10)) heatmap = sns.heatmap(df, cmap=cubehelix, annot=True, fmt=\\".1f\\", linewidths=.5, cbar_kws={\\"shrink\\": 0.75}) # Add titles and labels plt.title(\\"Average Daily Temperatures Over a Year\\") plt.xlabel(\\"Cities\\") plt.ylabel(\\"Day of the Year\\") plt.show()"},{"question":"# Generator-based File Content Processor Implement a generator function in Python that reads a text file line by line, processes each line to extract specific information, and yields results according to certain criteria. Function Specification **Function Name**: `process_file` **Arguments**: 1. `file_path` (str): The path to the text file to be processed. **Yielded Values**: - The function should yield tuples containing two elements: - The line number (starting from 1). - The processed content of the line, which depends on the criteria described below. **Processing Criteria**: 1. If a line is entirely uppercase, yield its content reversed. 2. If a line contains numeric values, yield a list of those numbers as integers. 3. For all other lines, yield the line as is, but stripped of leading and trailing whitespace. Constraints - You must use a generator to handle the file processing. - The function should be efficient in terms of memory usage, capable of processing very large files. - Assume UTF-8 encoded text files. Example Given a file `example.txt` with the following content: ``` HELLO WORLD This is a test. 123 and some numbers 456 Whitespace around ``` Your function can be used as follows to generate the desired results: ```python for result in process_file(\\"example.txt\\"): print(result) ``` Output: ``` (1, \'DLROW OLLEH\') (2, \'This is a test.\') (3, [123, 456]) (4, \'Whitespace around\') ``` Write a function `process_file(file_path)` that accomplishes the specified behavior.","solution":"import re def process_file(file_path): Processes a file line by line, applying the specific criteria. Yields: - Tuples containing the line number and the processed content. with open(file_path, \'r\', encoding=\'utf-8\') as file: for line_number, line in enumerate(file, 1): line = line.rstrip() if line.isupper(): yield (line_number, line[::-1]) elif any(char.isdigit() for char in line): yield (line_number, [int(num) for num in re.findall(r\'d+\', line)]) else: yield (line_number, line.strip())"},{"question":"Objective You are required to write a Python function using scikit-learn to generate multiple synthetic datasets and apply a simple machine learning model to classify one of those datasets. This will assess your proficiency with scikit-learn\'s dataset generators, model training, and evaluation capabilities. Problem Statement 1. **Dataset Generation**: - Generate the following datasets using scikit-learn: - **Blobs**: A simple clustering problem using `make_blobs` with 4 centers. - **Moons**: A two interleaving half-circles problem using `make_moons`. - **Classification**: A multiclass classification problem using `make_classification` with 3 classes. - For each dataset, plot the generated data points with different colors for different classes/labels. 2. **Model Training**: - Choose one of the generated datasets (you can choose any based on your preference). - Split the chosen dataset into training and testing sets (80% train, 20% test). - Train a `LogisticRegression` model on the training set. - Evaluate the model on the test set and print the accuracy score. Function Signature ```python def generate_and_evaluate_datasets(random_state: int = 42) -> None: pass ``` Requirements 1. You must use the following imports: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_moons, make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score ``` 2. Your function should perform the following: - Generate the specified datasets with `random_state` for reproducibility. - Plot the datasets using `matplotlib` with clear differentiation of classes. - Split one chosen dataset into train and test sets. - Train a `LogisticRegression` model from `sklearn.linear_model`. - Evaluate the model using `accuracy_score` from `sklearn.metrics` and print the result. 3. Ensure your code is well-structured and commented for readability. Example Output ```plaintext Accuracy of Logistic Regression on [Dataset Name]: 85.0% ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_moons, make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def generate_and_evaluate_datasets(random_state: int = 42) -> None: # Generate blobs dataset X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, random_state=random_state) plt.subplot(1, 3, 1) plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap=\'viridis\') plt.title(\\"Blobs\\") # Generate moons dataset X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=random_state) plt.subplot(1, 3, 2) plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=\'viridis\') plt.title(\\"Moons\\") # Generate classification dataset X_classification, y_classification = make_classification(n_samples=300, n_classes=3, n_informative=4, random_state=random_state) plt.subplot(1, 3, 3) plt.scatter(X_classification[:, 0], X_classification[:, 1], c=y_classification, cmap=\'viridis\') plt.title(\\"Classification\\") plt.tight_layout() plt.show() # Choose one dataset for model training, let\'s choose the moons dataset X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, test_size=0.2, random_state=random_state) # Train Logistic Regression model model = LogisticRegression() model.fit(X_train, y_train) # Predict and evaluate y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of Logistic Regression on Moons dataset: {accuracy * 100:.2f}%\\")"},{"question":"# K-Nearest Neighbors Classification with KDTree Introduction In this assessment, you will demonstrate your understanding of the scikit-learn `neighbors` package by implementing a K-Nearest Neighbors (KNN) classifier using the `KDTree` algorithm. You will preprocess the data, build the model with specific distance metrics, perform hyperparameter tuning, and evaluate the model\'s performance. Instructions 1. **Data Preprocessing** - Load the Iris dataset using `sklearn.datasets.load_iris`. - Split the dataset into training (70%) and testing (30%) sets using `train_test_split`. 2. **Model Implementation** - Create a K-Nearest Neighbors classifier using the `KDTree` algorithm. - Implement the classifier with two distance metrics: \'euclidean\' and \'manhattan\'. - Perform hyperparameter tuning to determine the optimal number of neighbors (`n_neighbors`) with values ranging from 1 to 10. 3. **Model Training and Evaluation** - Train the KNN classifier on the training data. - Evaluate the model on the testing data using accuracy as the performance metric. 4. **Submission Requirements** - The implemented function should be named `knn_kdtree_classifier`. - The function should take no arguments and return a dictionary with the following keys: - `best_params`: A dictionary with the best parameters found during hyperparameter tuning. - `test_accuracy`: The accuracy of the model on the testing set. Constraints - You should use `KDTree` for neighbor searches and implement the KNN classifier manually using `KDTree.query`. - You are not allowed to use `KNeighborsClassifier` directly. - Ensure that your solution handles potential ties in nearest neighbor distances appropriately. Expected Function Signature ```python def knn_kdtree_classifier() -> dict: pass ``` Example Output ```python { \\"best_params\\": {\\"n_neighbors\\": 3, \\"metric\\": \\"euclidean\\"}, \\"test_accuracy\\": 0.9778 } ```","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.neighbors import KDTree from itertools import product def knn_kdtree_classifier(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training (70%) and testing (30%) sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) best_accuracy = 0 best_params = {\'n_neighbors\': None, \'metric\': None} # Hyperparameter tuning for n_neighbors from 1 to 10 and metrics (euclidean, manhattan) for n_neighbors, metric in product(range(1, 11), [\'euclidean\', \'manhattan\']): # Create a KDTree with the chosen metric tree = KDTree(X_train, metric=metric) # Find nearest neighbors for each point in the test set dists, indices = tree.query(X_test, k=n_neighbors) # Predict the labels for the test set y_pred = [] for neighbors in indices: # Get the most common label among the nearest neighbors labels = y_train[neighbors] unique_labels, counts = np.unique(labels, return_counts=True) y_pred.append(unique_labels[np.argmax(counts)]) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Update best parameters if current accuracy is better if accuracy > best_accuracy: best_accuracy = accuracy best_params = {\'n_neighbors\': n_neighbors, \'metric\': metric} # Return the best parameters and corresponding test accuracy return {\'best_params\': best_params, \'test_accuracy\': best_accuracy}"},{"question":"**Objective:** Implement a function similar to `tabnanny.check()` that not only detects ambiguous indentations but also counts the total number of whitespace issues in a directory or a file. Your implementation should demonstrate a clear understanding of directory traversal, token processing, and exception handling in Python. **Function Signature:** ```python def custom_check(file_or_dir: str) -> int: Check the specified directory (recursively) or a single Python source file for whitespace related problems and return the total count of such issues. Args: file_or_dir (str): The directory or file path to check for whitespace issues. Returns: int: The total count of whitespace issues found. pass ``` **Input:** - `file_or_dir` (str): A string representing the path to a directory or a Python (.py) file. **Output:** - Returns an integer representing the total number of whitespace issues found. **Constraints:** 1. If `file_or_dir` is a directory, the function should recursively check all `.py` files within the directory and its subdirectories. 2. If `file_or_dir` is a file, it should directly check that file. 3. Use the `tokenize` module to process tokens from the files. **Performance Requirements:** - Your solution should be efficient with respect to both time and space complexity, including handling potentially large directories with numerous Python files gracefully. **Exception Handling:** - Handle exceptions gracefully, ensuring that the function does not crash due to unexpected file system errors or invalid token processing. **Example:** ```python # Assuming there is a file \'example.py\' with one ambiguous indentation issue. print(custom_check(\'example.py\')) # Output: 1 # Assuming there is a directory \'test_dir\' with multiple Python files containing a total of 3 issues. print(custom_check(\'test_dir\')) # Output: 3 ``` **Notes:** - You are not required to use `tabnanny` module directly but rather implement similar functionality on your own. - You may use `os`, `tokenize` and other standard Python libraries as needed.","solution":"import os import tokenize def count_whitespace_issues(file_path): issues = 0 with open(file_path, \'r\') as f: tokens = tokenize.generate_tokens(f.readline) indents = [] for toknum, tokval, _, _, _ in tokens: if toknum == tokenize.INDENT: indents.append(tokval) elif toknum == tokenize.DEDENT: if indents: indents.pop() elif toknum == tokenize.NEWLINE: if indents and len(set(indents)) > 1: issues += 1 indents = [] return issues def custom_check(file_or_dir): total_issues = 0 if os.path.isfile(file_or_dir): total_issues += count_whitespace_issues(file_or_dir) else: for root, dirs, files in os.walk(file_or_dir): for file in files: if file.endswith(\'.py\'): file_path = os.path.join(root, file) total_issues += count_whitespace_issues(file_path) return total_issues"},{"question":"# Parallelized Model Training with `torch.multiprocessing` Objective: You are tasked to implement a parallelized training procedure using `torch.multiprocessing` that follows the best practices described in PyTorch\'s documentation. You will ensure efficient CPU utilization, avoid deadlocks, and correctly handle tensor sharing between processes. Instructions: 1. **Model and Dataset Preparation**: - Define a simple model (such as a single-layer neural network) using `torch.nn.Module`. - Create a synthetic dataset using `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`. 2. **Implement Parallelized Training**: - Use `torch.multiprocessing` to create multiple processes that will train the model on different portions of the dataset. - Share the model parameters across the processes using shared memory. - Ensure that the number of threads allocated for each process does not cause CPU oversubscription. 3. **Ensure Best Practices**: - Avoid any global statements without proper protection using `if __name__ == \'__main__\':`. - Utilize `multiprocessing.Queue` to share data between processes if necessary. - Properly handle CUDA tensors if using a GPU (optional). Function Signature: ```python import torch import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader from math import floor class SyntheticDataset(Dataset): def __init__(self, num_samples): self.num_samples = num_samples self.data = torch.randn(num_samples, 10) self.labels = torch.randint(0, 2, (num_samples,)) def __len__(self): return self.num_samples def __getitem__(self, idx): return self.data[idx], self.labels[idx] class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) def train(rank, model, dataset, num_threads, epochs): # Set the number of threads for each process torch.set_num_threads(num_threads) data_loader = DataLoader(dataset, batch_size=32, shuffle=True) optimizer = optim.SGD(model.parameters(), lr=0.01) loss_fn = nn.CrossEntropyLoss() for epoch in range(epochs): for data, labels in data_loader: optimizer.zero_grad() outputs = model(data) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() def parallel_training(num_processes, num_epochs): model = SimpleModel() dataset = SyntheticDataset(10000) # Share the model parameters across processes model.share_memory() processes = [] num_threads_per_process = floor(torch.get_num_threads() / num_processes) for rank in range(num_processes): p = mp.Process(target=train, args=(rank, model, dataset, num_threads_per_process, num_epochs)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \'__main__\': num_processes = 4 num_epochs = 5 parallel_training(num_processes, num_epochs) ``` Requirements: - Implement a function `train` that performs the training of the model on a given dataset partition. - Implement a function `parallel_training` that initializes multiple processes and distributes the training work. - Ensure the number of threads used by each process does not exceed the available virtual CPUs (vCPUs). - Use shared memory for model parameters. - Follow best practices to avoid deadlocks and ensure efficient memory usage. Evaluation Criteria: - Correctness: The code should run without errors and train the model in parallel. - Efficiency: The implementation should avoid CPU oversubscription and excess memory copying. - Adherence to best practices: Proper handling of global statements, shared memory, and thread management.","solution":"import torch import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader from math import floor class SyntheticDataset(Dataset): def __init__(self, num_samples): self.num_samples = num_samples self.data = torch.randn(num_samples, 10) self.labels = torch.randint(0, 2, (num_samples,)) def __len__(self): return self.num_samples def __getitem__(self, idx): return self.data[idx], self.labels[idx] class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 2) def forward(self, x): return self.fc(x) def train(rank, model, dataset, num_threads, epochs): # Set the number of threads for each process torch.set_num_threads(num_threads) data_loader = DataLoader(dataset, batch_size=32, shuffle=True) optimizer = optim.SGD(model.parameters(), lr=0.01) loss_fn = nn.CrossEntropyLoss() for epoch in range(epochs): for data, labels in data_loader: optimizer.zero_grad() outputs = model(data) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() def parallel_training(num_processes, num_epochs): model = SimpleModel() dataset = SyntheticDataset(10000) # Share the model parameters across processes model.share_memory() processes = [] num_threads_per_process = floor(torch.get_num_threads() / num_processes) for rank in range(num_processes): p = mp.Process(target=train, args=(rank, model, dataset, num_threads_per_process, num_epochs)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \'__main__\': num_processes = 4 num_epochs = 5 parallel_training(num_processes, num_epochs)"},{"question":"Advanced HTTP Cookie Management with `http.cookiejar` Problem Statement You are tasked with creating a robust HTTP client that handles cookies efficiently. Your client should be able to: 1. Make HTTP requests with cookies automatically managed. 2. Save cookies received from HTTP responses to a file. 3. Load cookies from a file and reuse them in subsequent requests. 4. Customize cookie handling policies to block certain domains and use strict rules for setting cookies. Requirements 1. Implement a class `AdvancedCookieHandler` that uses `http.cookiejar` to manage cookies. 2. The class should have methods for: - Making HTTP requests. - Saving cookies to a file. - Loading cookies from a file. - Customizing cookie policies using `DefaultCookiePolicy`. Specifications - You may use the `urllib.request` library to make HTTP requests. - The cookie file should be named `cookies.txt`. - Block the domains \\"blocked.com\\" and \\".tracking.com\\" from setting cookies. - Enable `strict_ns_domain` policy to use `DomainStrict`. Input and Output Formats The class should be implemented as follows: ```python class AdvancedCookieHandler: def __init__(self): # Initialize the CookieJar and policy with appropriate settings def make_request(self, url: str) -> str: Make an HTTP GET request to the specified URL. :param url: The URL to request. :return: The text content of the response. def save_cookies(self): Save the current cookies to \'cookies.txt\'. def load_cookies(self): Load cookies from \'cookies.txt\'. ``` Example Usage ```python handler = AdvancedCookieHandler() # Make a request and automatically manage cookies response = handler.make_request(\'http://example.com\') print(response) # Save cookies to \'cookies.txt\' handler.save_cookies() # Load cookies from \'cookies.txt\' handler.load_cookies() # Make another request with the loaded cookies response = handler.make_request(\'http://example.com/another-page\') print(response) ``` Constraints - The HTTP responses are simple text strings. - Assume internet access is available for making requests. # Implementation Details - Use `http.cookiejar.MozillaCookieJar` for handling the saving/loading of cookies. - Use `urllib.request.HTTPCookieProcessor` to handle cookie processing in HTTP requests. - Customize the cookies policy to block specific domains and enforce strict domain rules.","solution":"import urllib.request import http.cookiejar class AdvancedCookieHandler: def __init__(self): # Create a cookie jar to hold cookies self.cookie_jar = http.cookiejar.MozillaCookieJar() # Define the cookie policy self.cookie_policy = http.cookiejar.DefaultCookiePolicy( blocked_domains=[\\"blocked.com\\", \\".tracking.com\\"], # Block specified domains strict_ns_domain=http.cookiejar.DefaultCookiePolicy.DomainStrict # Use strict domain rules ) # Attach the policy to the cookie jar self.cookie_jar.set_policy(self.cookie_policy) # Create an opener to use with urllib self.opener = urllib.request.build_opener( urllib.request.HTTPCookieProcessor(self.cookie_jar) ) def make_request(self, url: str) -> str: Make an HTTP GET request to the specified URL. :param url: The URL to request. :return: The text content of the response. response = self.opener.open(url) return response.read().decode(\'utf-8\') def save_cookies(self, filename=\'cookies.txt\'): Save the current cookies to a file. :param filename: The filename where cookies will be saved. self.cookie_jar.save(filename) def load_cookies(self, filename=\'cookies.txt\'): Load cookies from a file. :param filename: The filename from where cookies will be loaded. self.cookie_jar.load(filename)"},{"question":"# Question: Advanced Seaborn Color Palettes and Visualizations You are provided with a dataset and your task is to utilize Seaborn\'s color palette functionalities to create visualizations that maximize clarity and aesthetic appeal. This will test your ability to select and apply appropriate color palettes for different types of data. Dataset A dataset named `iris` is provided, consisting of the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` The `species` column consists of three categories: `setosa`, `versicolor`, and `virginica`. Requirements 1. Load the `iris` dataset from Seaborn\'s built-in datasets. 2. Create a scatter plot of `sepal_length` vs. `sepal_width`: - Use a qualitative color palette to differentiate the three species. - Add a legend to identify which color corresponds to which species. 3. Create another scatter plot of `petal_length` vs. `petal_width`: - Use a perceptually uniform sequential palette to represent the `petal_length`. - Add a color bar that indicates the mapping of colors to `petal_length`. 4. Create a third plot (your choice of plot type - it can be another scatter plot, histograms, etc.): - Use a diverging palette to highlight data points based on the petal length to width ratio. - Ensure that the midpoint (ratio = 1) is clearly distinguishable. 5. Save each of your plots as separate image files. Constraints - Your solution should be efficient and handle the dataset without unnecessary computations. - Ensure that the colors chosen are accessible to people with varying types of color vision. Expected Output - Three image files representing the requested plots: - `scatter_sepal.png` - `scatter_petal.png` - `diverging_plot.png` Code: ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Load the iris dataset iris = sns.load_dataset(\\"iris\\") # 2. Create a scatter plot of sepal_length vs. sepal_width using qualitative color palette plt.figure(figsize=(8, 6)) scatter_sepal = sns.scatterplot( data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=\'deep\' ) scatter_sepal.legend(loc=\'best\') plt.title(\'Sepal Length vs Sepal Width\') plt.savefig(\'scatter_sepal.png\') plt.clf() # 3. Create a scatter plot of petal_length vs. petal_width using perceptually uniform sequential palette plt.figure(figsize=(8, 6)) scatter_petal = sns.scatterplot( data=iris, x=\'petal_length\', y=\'petal_width\', hue=\'petal_length\', palette=\'mako\', legend=False ) plt.colorbar(scatter_petal.collections[0]) plt.title(\'Petal Length vs Petal Width\') plt.savefig(\'scatter_petal.png\') plt.clf() # 4. Create a diverging plot based on petal length to width ratio iris[\'petal_ratio\'] = iris[\'petal_length\'] / iris[\'petal_width\'] plt.figure(figsize=(8, 6)) diverging_plot = sns.scatterplot( data=iris, x=\'petal_length\', y=\'petal_width\', hue=\'petal_ratio\', palette=sns.diverging_palette(250, 30, l=65, center=\\"dark\\", as_cmap=True), legend=False ) plt.colorbar(diverging_plot.collections[0]) plt.title(\'Diverging Palette: Petal Ratio\') plt.savefig(\'diverging_plot.png\') plt.clf() ``` Ensure that the plots are saved correctly and visually check that the color palettes are applied meaningfully.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Scatter plot of sepal_length vs. sepal_width using qualitative color palette plt.figure(figsize=(8, 6)) scatter_sepal = sns.scatterplot( data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=\'deep\' ) scatter_sepal.legend(loc=\'best\') plt.title(\'Sepal Length vs Sepal Width\') plt.savefig(\'scatter_sepal.png\') plt.clf() # Scatter plot of petal_length vs. petal_width using perceptually uniform sequential palette plt.figure(figsize=(8, 6)) scatter_petal = sns.scatterplot( data=iris, x=\'petal_length\', y=\'petal_width\', hue=\'petal_length\', palette=\'mako\', legend=False ) plt.colorbar(scatter_petal.collections[0]) plt.title(\'Petal Length vs Petal Width\') plt.savefig(\'scatter_petal.png\') plt.clf() # Diverging plot based on petal length to width ratio iris[\'petal_ratio\'] = iris[\'petal_length\'] / iris[\'petal_width\'] plt.figure(figsize=(8, 6)) diverging_plot = sns.scatterplot( data=iris, x=\'petal_length\', y=\'petal_width\', hue=\'petal_ratio\', palette=sns.diverging_palette(250, 30, l=65, center=\\"dark\\", as_cmap=True), legend=False ) plt.colorbar(diverging_plot.collections[0]) plt.title(\'Diverging Palette: Petal Ratio\') plt.savefig(\'diverging_plot.png\') plt.clf()"},{"question":"# IMAP4 Email Retrieval and Management Objective In this exercise, you will use the `imaplib` module to connect to an IMAP4-over-SSL server, authenticate, and interact with the mailbox. You will retrieve a list of email messages from the mailbox, fetch the content of the emails, and handle any potential errors gracefully. Task Write a Python function `retrieve_emails` that: 1. Connects to an IMAP4-over-SSL server using the `IMAP4_SSL` class. 2. Authenticates with the server using a provided username and password. 3. Retrieves the subject and sender of all emails in the `INBOX`. 4. Returns a list of tuples, each containing the subject and sender of an email. Input - `host` (str): The IMAP server\'s hostname. - `port` (int): The port to connect to. The standard IMAP-over-SSL port is 993. - `username` (str): The username for authentication. - `password` (str): The password for authentication. Output - Returns a list of tuples, where each tuple contains the subject (str) and sender (str) of an email. Constraints - Use the `IMAP4_SSL` class for establishing the connection. - Handle any exceptions that may arise, such as connection errors or authentication errors, and return an appropriate error message. - Assume that the email subjects and senders are encoded in ASCII. Example Usage ```python emails = retrieve_emails(\\"imap.example.com\\", 993, \\"user@example.com\\", \\"password\\") for subject, sender in emails: print(f\\"Subject: {subject}, Sender: {sender}\\") ``` Performance Requirements - The function should handle a large number of emails efficiently. - Use proper exception handling to ensure that the function does not crash and returns meaningful error messages in case of failure. Solution Template ```python import imaplib import email def retrieve_emails(host, port, username, password): try: # Connect to the server M = imaplib.IMAP4_SSL(host, port) # Login to the account M.login(username, password) # Select the INBOX M.select(\\"INBOX\\") # Search for all emails in the inbox typ, data = M.search(None, \\"ALL\\") if typ != \\"OK\\": raise Exception(\\"Error searching inbox.\\") email_list = [] # Fetch each email for num in data[0].split(): typ, data = M.fetch(num, \\"(RFC822)\\") if typ != \\"OK\\": continue msg = email.message_from_bytes(data[0][1]) subject = msg[\\"subject\\"] sender = msg[\\"from\\"] email_list.append((subject, sender)) # Logout M.close() M.logout() return email_list except imaplib.IMAP4.error as e: return f\\"IMAP4 error occurred: {e}\\" except Exception as e: return f\\"An error occurred: {e}\\" # Example usage: # emails = retrieve_emails(\\"imap.example.com\\", 993, \\"user@example.com\\", \\"password\\") # for subject, sender in emails: # print(f\\"Subject: {subject}, Sender: {sender}\\") ``` Notes - For the purpose of this exercise, assume that the server uses standard IMAP-over-SSL port 993. - It is good practice to handle and log exceptions properly to debug potential issues during execution.","solution":"import imaplib import email def retrieve_emails(host, port, username, password): try: # Connect to the server M = imaplib.IMAP4_SSL(host, port) # Login to the account M.login(username, password) # Select the INBOX M.select(\\"INBOX\\") # Search for all emails in the inbox typ, data = M.search(None, \\"ALL\\") if typ != \\"OK\\": raise Exception(\\"Error searching inbox.\\") email_list = [] # Fetch each email for num in data[0].split(): typ, data = M.fetch(num, \\"(RFC822)\\") if typ != \\"OK\\": continue msg = email.message_from_bytes(data[0][1]) subject = email.header.decode_header(msg[\\"subject\\"])[0][0] if isinstance(subject, bytes): subject = subject.decode() sender = email.header.decode_header(msg[\\"from\\"])[0][0] if isinstance(sender, bytes): sender = sender.decode() email_list.append((subject, sender)) # Logout M.close() M.logout() return email_list except imaplib.IMAP4.error as e: return f\\"IMAP4 error occurred: {e}\\" except Exception as e: return f\\"An error occurred: {e}\\" # Example usage: # emails = retrieve_emails(\\"imap.example.com\\", 993, \\"user@example.com\\", \\"password\\") # for subject, sender in emails: # print(f\\"Subject: {subject}, Sender: {sender}\\")"},{"question":"**Datetime Operations and Manipulations in Python** # Problem Statement You are tasked with creating a function that performs various datetime operations for a scheduling application. The function will take a list of datetime strings, convert them to datetime objects, perform operations to find the time difference between consecutive datetime entries, and return these differences in a user-readable format. # Function Specification **Function Name:** `datetime_operations` **Parameters:** - `datetime_list` (List of Strings): A list of datetime strings in the format \\"YYYY-MM-DD HH:MM:SS\\". Example: `[\\"2023-10-01 12:01:00\\", \\"2023-10-01 14:45:00\\", \\"2023-10-02 09:15:00\\"]` **Returns:** - List of Strings: Each string in the list represents the time difference between consecutive datetime entries in a user-readable format: \\"X days, Y hours, Z minutes\\". **Constraints:** - All datetime strings are in the format \\"YYYY-MM-DD HH:MM:SS\\". - The list will contain at least two datetime strings. - The datetimes in the list are sorted in ascending order. # Example **Input:** ```python datetime_list = [\\"2023-10-01 12:01:00\\", \\"2023-10-01 14:45:00\\", \\"2023-10-02 09:15:00\\"] ``` **Output:** ```python [\\"0 days, 2 hours, 44 minutes\\", \\"0 days, 18 hours, 30 minutes\\"] ``` # Additional Notes: 1. You will need to utilize the `datetime` module for this task. 2. Ensure that your function handles the conversion, calculation, and formatting accurately. 3. Consider edge cases where the time difference might span multiple days, hours, or minutes. # Implementation Details 1. **Convert Strings to `datetime` Objects:** - Use `datetime.strptime()` to convert each string in the list to a `datetime` object. 2. **Calculate Time Differences:** - Iterate over the list of datetime objects and calculate the difference between consecutive datetimes. - Use the `timedelta` object to compute the differences. 3. **Format the Results:** - Extract the days, hours, and minutes from the `timedelta` object. - Format the results into a string \\"X days, Y hours, Z minutes\\". Implement the `datetime_operations` function to accurately perform these operations. # Implementation Skeleton ```python from datetime import datetime, timedelta def datetime_operations(datetime_list): # Convert strings to datetime objects datetime_objects = [datetime.strptime(dt_str, \\"%Y-%m-%d %H:%M:SS\\") for dt_str in datetime_list] # Calculate time differences differences = [] for i in range(1, len(datetime_objects)): diff = datetime_objects[i] - datetime_objects[i - 1] days = diff.days seconds = diff.seconds hours = seconds // 3600 minutes = (seconds % 3600) // 60 differences.append(f\\"{days} days, {hours} hours, {minutes} minutes\\") return differences ``` Ensure that your implementation passes all edge cases and correctly formats the output. **Note:** This question tests the student\'s ability to handle the `datetime` module, convert and manipulate datetime objects, and format outputs appropriately, covering both fundamental and advanced concepts.","solution":"from datetime import datetime def datetime_operations(datetime_list): # Convert strings to datetime objects datetime_objects = [datetime.strptime(dt_str, \\"%Y-%m-%d %H:%M:%S\\") for dt_str in datetime_list] # Calculate time differences differences = [] for i in range(1, len(datetime_objects)): diff = datetime_objects[i] - datetime_objects[i - 1] days = diff.days seconds = diff.seconds hours = seconds // 3600 minutes = (seconds % 3600) // 60 differences.append(f\\"{days} days, {hours} hours, {minutes} minutes\\") return differences"},{"question":"Objective You are required to implement a function using Python\'s `urllib.robotparser` module to determine if a list of URLs can be fetched by a specified user agent according to a site\'s `robots.txt` file. Description Implement a function `check_urls_against_robots_txt(base_url, useragent, urls)` that determines whether a given user agent has permission to fetch each URL in the provided list from the website specified by `base_url`. Function Signature ```python def check_urls_against_robots_txt(base_url: str, useragent: str, urls: list) -> dict: pass ``` Parameters - `base_url` (str): The base URL of the website you want to check. For example, \\"http://example.com\\". - `useragent` (str): The user agent string you want to check permissions for. - `urls` (list): A list of URLs (str) to check against the `robots.txt` rules of the website. Returns - `dict`: A dictionary where the keys are the URLs and the values are boolean values indicating whether the user agent is allowed to fetch the URL (True if allowed, False if not). Constraints 1. The `urls` list will contain between 1 and 1000 URLs. 2. Each URL will be a string of maximum length 2048 characters. 3. You may assume the website\'s `robots.txt` file, if it exists, can be fetched successfully. Example ```python # Assuming the robots.txt at http://example.com/robots.txt contains: # User-agent: * # Disallow: /private/ # User-agent: Googlebot # Allow: / # Disallow: /private/ base_url = \\"http://example.com\\" useragent = \\"Googlebot\\" urls = [ \\"http://example.com/\\", \\"http://example.com/public/\\", \\"http://example.com/private/\\", \\"http://example.com/private/data\\", ] print(check_urls_against_robots_txt(base_url, useragent, urls)) # Output should be: # { # \\"http://example.com/\\": True, # \\"http://example.com/public/\\": True, # \\"http://example.com/private/\\": False, # \\"http://example.com/private/data\\": False, # } ``` Notes - Make sure to handle edge cases such as URLs without trailing slashes. - Consider efficiency in your implementation, especially when dealing with a large list of URLs.","solution":"import urllib.robotparser def check_urls_against_robots_txt(base_url: str, useragent: str, urls: list) -> dict: Determines if a given user agent has permission to fetch each URL in the provided list from the website specified by base_url. Parameters: - base_url (str): The base URL of the website. - useragent (str): The user agent string. - urls (list): A list of URLs to check. Returns: - dict: A dictionary where the keys are the URLs and the values are boolean indicating if the user agent is allowed to fetch the URL. # Initialize the robotparser object rp = urllib.robotparser.RobotFileParser() # Prepare the URL of the robots.txt file robots_txt_url = f\\"{base_url.rstrip(\'/\')}/robots.txt\\" # Read the robots.txt file rp.set_url(robots_txt_url) rp.read() # Check each URL in the list results = {} for url in urls: results[url] = rp.can_fetch(useragent, url) return results"},{"question":"**Python Coding Assessment Question** # Question Title: Advanced Context Manager Implementation with contextlib # Question Description: You are tasked with managing resources in a complex application that involves both synchronous and asynchronous operations. To this end, you need to implement a context manager that handles both types of operations cleanly, ensuring that resources are properly acquired and released. # Requirements: 1. Implement a synchronous context manager class `CustomSyncResourceManager` using the `contextlib.ContextDecorator`. 2. Implement an asynchronous context manager class `CustomAsyncResourceManager` using the `contextlib.AsyncContextDecorator`. 3. Both classes should manage a simple resource (e.g., a string) and should log the acquisition and release of this resource. 4. Ensure the classes can be used both as context managers and decorators. 5. Both classes should be reusable but not reentrant. # Input: - **For synchronous class**: `CustomSyncResourceManager(resource_name: str)` - `resource_name`: A string representing the name of the resource. - **For asynchronous class**: `CustomAsyncResourceManager(resource_name: str)` - `resource_name`: A string representing the name of the resource. # Output: - Logs indicating the acquisition and release of the resource, whenever the context manager acquires or releases the resource, both in sync and async contexts. # Constraints: - You can use print statements for logging. - Make sure to override the `__enter__`, `__exit__`, `__aenter__`, and `__aexit__` methods appropriately. - Ensure that the classes handle exceptions correctly during resource acquisition and release. # Example: ```python # Synchronous context manager usage @CustomSyncResourceManager(\'sync_resource\') def sync_task(): print(\'Using sync resource\') with CustomSyncResourceManager(\'sync_resource\'): print(\'Using sync resource\') # Asynchronous context manager usage @CustomAsyncResourceManager(\'async_resource\') async def async_task(): print(\'Using async resource\') async def run_async_task(): async with CustomAsyncResourceManager(\'async_resource\'): print(\'Using async resource\') ``` # Expected Output: ``` # Synchronous context manager output Acquiring sync resource Using sync resource Releasing sync resource Acquiring sync resource Using sync resource Releasing sync resource # Asynchronous context manager output Acquiring async resource Using async resource Releasing async resource Acquiring async resource Using async resource Releasing async resource ``` **Note**: This question tests your understanding of creating both synchronous and asynchronous context managers with proper resource handling in Python using the `contextlib` module.","solution":"import contextlib class CustomSyncResourceManager(contextlib.ContextDecorator): def __init__(self, resource_name): self.resource_name = resource_name def __enter__(self): self.resource = f\\"sync resource: {self.resource_name}\\" print(f\\"Acquiring {self.resource}\\") return self.resource def __exit__(self, exc_type, exc_value, traceback): print(f\\"Releasing {self.resource}\\") self.resource = None import contextlib import asyncio class CustomAsyncResourceManager(contextlib.AsyncContextDecorator): def __init__(self, resource_name): self.resource_name = resource_name async def __aenter__(self): self.resource = f\\"async resource: {self.resource_name}\\" print(f\\"Acquiring {self.resource}\\") return self.resource async def __aexit__(self, exc_type, exc_value, traceback): print(f\\"Releasing {self.resource}\\") self.resource = None"},{"question":"Coding Assessment Question # Title: Handling Floating-Point Arithmetic and Exact Representations # Objective: Demonstrate understanding of floating-point arithmetic limitations and utilization of Python\'s tools for precise calculations. # Problem Statement: In this task, you are required to implement a function that performs a series of calculations using floating-point numbers and demonstrates how to handle and mitigate the inherent approximation errors. Specifically, you need to: 1. Sum a list of floating-point numbers and return the result using both regular summation and Python\'s `math.fsum()` function, highlighting any differences. 2. Convert a given floating-point number to its exact fraction representation using the `as_integer_ratio()` method. 3. Convert a given floating-point number to its exact hexadecimal representation using the `hex()` method. 4. Provide controlled rounding for a given floating-point number using Python\'s string formatting options. # Function Signature: ```python import math from decimal import Decimal from fractions import Fraction def handle_floating_point(numbers: list, float_num: float) -> dict: Perform a series of calculations with floating-point numbers. Args: numbers (list): A list of floating-point numbers to be summed. float_num (float): A floating-point number to convert and format. Returns: dict: A dictionary with the following keys and values: - \'regular_sum\': The result of summing the numbers using the built-in sum() function. - \' accurate_sum\': The result of summing the numbers using math.fsum(). - \'as_integer_ratio\': The exact fraction representation of float_num as a tuple. - \'hex_representation\': The exact hexadecimal representation of float_num. - \'rounded_value\': The value of float_num rounded to 10 decimal places as a string. regular_sum = sum(numbers) accurate_sum = math.fsum(numbers) as_integer_ratio = float_num.as_integer_ratio() hex_representation = float_num.hex() rounded_value = format(float_num, \'.10f\') return { \'regular_sum\': regular_sum, \'accurate_sum\': accurate_sum, \'as_integer_ratio\': as_integer_ratio, \'hex_representation\': hex_representation, \'rounded_value\': rounded_value } ``` # Input: - `numbers`: A list of floating-point numbers (e.g., [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]). - `float_num`: A floating-point number (e.g., 0.1). # Output: - A dictionary with the following keys and corresponding values: - `regular_sum`: The summation of `numbers` using the built-in `sum()` function. - `accurate_sum`: The summation of `numbers` using `math.fsum()`. - `as_integer_ratio`: The exact fraction representation of `float_num`. - `hex_representation`: The exact hexadecimal representation of `float_num`. - `rounded_value`: The value of `float_num` rounded to 10 decimal places as a string. # Constraints: - The list `numbers` will contain between 1 and 1000 floating-point numbers. - Each floating-point number in the list `numbers` will be between -10^3 and 10^3. - The `float_num` will be a non-zero floating-point number between -10^3 and 10^3. # Example: ```python numbers = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] float_num = 0.1 result = handle_floating_point(numbers, float_num) print(result) # Expected Output: # { # \'regular_sum\': 0.9999999999999999, # \'accurate_sum\': 1.0, # \'as_integer_ratio\': (3602879701896397, 36028797018963968), # \'hex_representation\': \'0x1.999999999999ap-4\', # \'rounded_value\': \'0.1000000000\' # } ``` # Note: - Pay close attention to the differences between summing using `sum()` and `math.fsum()`. - Ensure the accuracy of the fraction and hexadecimal representations. - Demonstrate controlled rounding for better presentation of floating-point numbers.","solution":"import math def handle_floating_point(numbers: list, float_num: float) -> dict: Perform a series of calculations with floating-point numbers. Args: numbers (list): A list of floating-point numbers to be summed. float_num (float): A floating-point number to convert and format. Returns: dict: A dictionary with the following keys and values: - \'regular_sum\': The result of summing the numbers using the built-in sum() function. - \'accurate_sum\': The result of summing the numbers using math.fsum(). - \'as_integer_ratio\': The exact fraction representation of float_num as a tuple. - \'hex_representation\': The exact hexadecimal representation of float_num. - \'rounded_value\': The value of float_num rounded to 10 decimal places as a string. regular_sum = sum(numbers) accurate_sum = math.fsum(numbers) as_integer_ratio = float_num.as_integer_ratio() hex_representation = float_num.hex() rounded_value = format(float_num, \'.10f\') return { \'regular_sum\': regular_sum, \'accurate_sum\': accurate_sum, \'as_integer_ratio\': as_integer_ratio, \'hex_representation\': hex_representation, \'rounded_value\': rounded_value }"},{"question":"**Problem Statement:** You are required to implement functionality that mimics part of the \\"mailcap\\" module\'s behavior. Please follow the instructions below to accomplish this task. # Part 1: Implement the `parse_mailcap_files` function Implement a function `parse_mailcap_files(files: List[str]) -> Dict[str, List[Dict[str, str]]]` that reads and parses mailcap files and returns a dictionary in a similar format to `mailcap.getcaps()`. 1. **Input:** - `files`: A list of strings where each string represents the path of a mailcap file. 2. **Output:** - A dictionary mapping MIME types to a list of mailcap file entries. Each entry is a dictionary with fields parsed from the corresponding mailcap entry. 3. **Constraints:** - A mailcap file contains lines where each line specifies a MIME type and its associated command and parameters. - Lines starting with `#` are comments and should be ignored. - Each line is of the format: `MIME/type; command; key=value; ...` - Example entries: ``` text/plain; cat %s text/html; lynx %s; compose=ex_composer; edit=ex_editor ``` # Part 2: Implement the `find_command` function Implement a function `find_command(caps: Dict[str, List[Dict[str, str]]], MIMEtype: str, key: str = \'view\', filename: str = \'/dev/null\', plist: List[str] = []) -> Tuple[Optional[str], Optional[Dict[str, str]]]` that finds a matching mailcap entry for a specified MIME type and returns the equivalent command and entry. 1. **Input:** - `caps`: A dictionary returned by the `parse_mailcap_files` function. - `MIMEtype`: The MIME type to search for. - `key`: The type of activity to be performed, default is \'view\'. Other possible values could be \'compose\' and \'edit\'. - `filename`: The filename to be substituted for \\"%s\\" in the command line. - `plist`: A list of named parameters where each element is a string in the format \\"name=value\\". 2. **Output:** - A tuple where the first element is the command line to be executed (after substitutions) and the second element is the mailcap entry (as a dictionary). If no matching entry is found, return `(None, None)`. 3. **Constraints:** - Replace occurrences of `%s` in the command line with the `filename`. - Replace occurrences of `%{name}` in the command line with the value of the corresponding parameter in `plist`. - If any disallowed character (ASCII characters other than alphanumerics and \\"@+=:,./-_\\") appears in `filename` or in `plist` values, return `(None, None)`. # Example Usage: ```python # Part 1: Parse mailcap files files = [\\"user_mailcap\\", \\"system_mailcap\\"] caps = parse_mailcap_files(files) # Part 2: Find command command, entry = find_command(caps, \'video/mpeg\', filename=\'video1234.mpeg\', plist=[]) print(command) # Expected output: \\"xmpeg video1234.mpeg\\" print(entry) # Expected output: {\'view\': \'xmpeg %s\'} ``` # Notes: - You should implement file reading and handle exceptions appropriately. - Ensure your solution is efficient and well-structured. - Test your functions with different inputs to validate correctness.","solution":"import os from typing import List, Dict, Tuple, Optional def parse_mailcap_files(files: List[str]) -> Dict[str, List[Dict[str, str]]]: caps = {} for file in files: if os.path.exists(file): with open(file, \'r\') as f: for line in f: # Ignore comments and empty lines stripped_line = line.strip() if stripped_line.startswith(\'#\') or not stripped_line: continue # Split MIME type and command/parameters parts = stripped_line.split(\';\') if len(parts) < 2: continue mimetype = parts[0].strip() command = parts[1].strip() entry = {\'view\': command} for param in parts[2:]: key_value = param.strip().split(\'=\', 1) if len(key_value) == 2: entry[key_value[0].strip()] = key_value[1].strip() if mimetype not in caps: caps[mimetype] = [] caps[mimetype].append(entry) return caps def find_command(caps: Dict[str, List[Dict[str, str]]], MIMEtype: str, key: str = \'view\', filename: str = \'/dev/null\', plist: List[str] = []) -> Tuple[Optional[str], Optional[Dict[str, str]]]: valid_chars = set(\'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789@+=:,./-_\') # Validate filename if not set(filename).issubset(valid_chars): return (None, None) # Validate plist params = {} for item in plist: name_value = item.split(\'=\', 1) if len(name_value) != 2 or not set(name_value[1]).issubset(valid_chars): return (None, None) params[name_value[0]] = name_value[1] if MIMEtype in caps: for entry in caps[MIMEtype]: if key in entry: command = entry[key] # Replace %s in command with filename command = command.replace(\'%s\', filename) # Replace %{name} with the value from params for name, value in params.items(): command = command.replace(f\'%{{{name}}}\', value) return (command, entry) return (None, None)"},{"question":"# Exercise: Advanced Data Encoding and Decoding with Base64 Objective: To assess your understanding and capability to work with Python\'s `base64` module for encoding and decoding data, including handling special characters and validation. Problem Statement: You are tasked with implementing a function that encodes and decodes data using different encoding schemes provided by the `base64` module. Specifically, the function should: 1. Accept binary data as input. 2. Encode the data using Base64 with a URL-safe alphabet. 3. Decode the encoded data back to the original form. 4. Perform validation to ensure the integrity of the encoding and decoding process. Function Signature: ```python def encode_and_verify(data: bytes) -> bool: pass ``` Input: - `data` (bytes): A bytes-like object containing the binary data to be encoded. Output: - `result` (bool): A boolean indicating whether the decoded data matches the original input data (True if they match, False otherwise). Requirements: - Use `base64.urlsafe_b64encode` for encoding the input data. - Use `base64.urlsafe_b64decode` for decoding the encoded data. - Ensure that any non-alphabet characters are discarded prior to decoding. - Validate that the decoded output matches the original input data accurately. Constraints: - The input data will be provided as a non-empty *bytes-like object*. Example: ```python data = b\'This is a sample text for encoding.\' result = encode_and_verify(data) print(result) # Should output: True data = b\'x00x01x02x03x04x05\' result = encode_and_verify(data) print(result) # Should output: True ``` # Notes: - You must handle any potential errors that might occur during encoding and decoding, ensuring the function execution remains robust. - Efficiency in terms of time complexity and space complexity is a consideration, so optimize your method accordingly. Hints: - Use optional arguments in the encoding and decoding functions to handle specific characters and validation. - Test your implementation thoroughly with various types of binary data to ensure correctness.","solution":"import base64 def encode_and_verify(data: bytes) -> bool: try: # Encode the data using Base64 with a URL-safe alphabet encoded_data = base64.urlsafe_b64encode(data) # Decode the encoded data back to the original form decoded_data = base64.urlsafe_b64decode(encoded_data) # Validate that the decoded data matches the original input data return decoded_data == data except Exception as e: # If there\'s any exception, return False return False"},{"question":"Problem Statement You are required to implement a custom server using the `socketserver` module that can both handle TCP and UDP requests concurrently. You must also provide custom request handler classes that will process incoming requests and implement specific logic. # Requirements 1. **Server Design:** - Create a server that can handle both TCP and UDP requests. - Use `ThreadingMixIn` to handle multiple requests concurrently. 2. **Request Handlers:** - Implement two custom request handler classes: - `CustomTCPHandler` which should handle incoming TCP requests. - `CustomUDPHandler` which should handle incoming UDP requests. - The `handle()` method in each class should: - Receive the data from the client. - Log the client address and the data received. - Send back the data transformed to uppercase. 3. **Client Design:** - Provide a simple client script to interact with the TCP and UDP servers for testing purposes. # Input and Output - **TCP Request:** The client sends a string message to the TCP server. - **Input:** \\"Hello TCP Server\\" - **Output:** \\"HELLO TCP SERVER\\" - **UDP Request:** The client sends a string message to the UDP server. - **Input:** \\"Hello UDP Server\\" - **Output:** \\"HELLO UDP SERVER\\" # Constraints - Your server should be able to handle at least 5 concurrent requests. - You can use any port number for the servers, but ensure it is clearly documented for the client to connect to. # Example Here is a simplified example to illustrate how your implementation might look: **Server Side Code:** ```python import socketserver from threading import Thread class CustomTCPHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024).strip() print(f\\"{self.client_address[0]} wrote: {data}\\") self.request.sendall(data.upper()) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass class CustomUDPHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request[0].strip() socket = self.request[1] print(f\\"{self.client_address[0]} wrote: {data}\\") socket.sendto(data.upper(), self.client_address) class ThreadedUDPServer(socketserver.ThreadingMixIn, socketserver.UDPServer): pass if __name__ == \\"__main__\\": HOST, TCP_PORT, UDP_PORT = \\"localhost\\", 9999, 9998 tcp_server = ThreadedTCPServer((HOST, TCP_port), CustomTCPHandler) udp_server = ThreadedUDPServer((HOST, UDP_PORT), CustomUDPHandler) tcp_server_thread = Thread(target=tcp_server.serve_forever) udp_server_thread = Thread(target=udp_server.serve_forever) tcp_server_thread.daemon = True udp_server_thread.daemon = True tcp_server_thread.start() udp_server_thread.start() print(\\"Servers running. Press Ctrl-C to stop.\\") try: while True: pass except KeyboardInterrupt: tcp_server.shutdown() udp_server.shutdown() ``` **Client Side Code for Testing:** ```python import socket # TCP Client def tcp_client(): tcp_host, tcp_port = \'localhost\', 9999 data = \'Hello TCP Server\' with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock: sock.connect((tcp_host, tcp_port)) sock.sendall(bytes(data + \\"n\\", \\"utf-8\\")) received = str(sock.recv(1024), \'utf-8\') print(\\"Sent: {}\\".format(data)) print(\\"Received: {}\\".format(received)) # UDP Client def udp_client(): udp_host, udp_port = \'localhost\', 9998 data = \'Hello UDP Server\' sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) sock.sendto(bytes(data + \\"n\\", \\"utf-8\\"), (udp_host, udp_port)) received = str(sock.recv(1024), \'utf-8\') print(\\"Sent: {}\\".format(data)) print(\\"Received: {}\\".format(received)) if __name__ == \\"__main__\\": tcp_client() udp_client() ``` # Evaluation Criteria - Correct implementation of both TCP and UDP servers. - Use of `ThreadingMixIn` to allow concurrent request handling. - Proper logging of client addresses and data. - Correct handling and response of incoming requests. - Demonstration and correctness of the client-server interaction.","solution":"import socketserver from threading import Thread class CustomTCPHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request.recv(1024).strip() print(f\\"{self.client_address[0]} wrote: {data}\\") self.request.sendall(data.upper()) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass class CustomUDPHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request[0].strip() socket = self.request[1] print(f\\"{self.client_address[0]} wrote: {data}\\") socket.sendto(data.upper(), self.client_address) class ThreadedUDPServer(socketserver.ThreadingMixIn, socketserver.UDPServer): pass if __name__ == \\"__main__\\": HOST, TCP_PORT, UDP_PORT = \\"localhost\\", 9999, 9998 tcp_server = ThreadedTCPServer((HOST, TCP_PORT), CustomTCPHandler) udp_server = ThreadedUDPServer((HOST, UDP_PORT), CustomUDPHandler) tcp_server_thread = Thread(target=tcp_server.serve_forever) udp_server_thread = Thread(target=udp_server.serve_forever) tcp_server_thread.daemon = True udp_server_thread.daemon = True tcp_server_thread.start() udp_server_thread.start() print(\\"Servers running. Press Ctrl-C to stop.\\") try: while True: pass except KeyboardInterrupt: tcp_server.shutdown() udp_server.shutdown()"},{"question":"Objective Implement a Python function that performs a series of mathematical operations using the `math` module. Problem Statement Given an angle in degrees, you are to compute several derived values: 1. Convert the angle to radians. 2. Calculate the sine, cosine, and tangent of the angle in radians. 3. Compute the Euclidean distance from the origin for a 2D point with coordinates (cosine of the angle, sine of the angle). 4. Return the smallest integer greater than or equal to the tangent value of the angle in radians. 5. Calculate the factorial of the integer part of the distance obtained in step 3. 6. Compute the greatest common divisor (GCD) of the distance (converted to an integer) and the result from step 4. Function Signature ```python def compute_derived_values(angle: float) -> dict: pass ``` Input - A single floating-point value `angle` which represents an angle in degrees. Output - A dictionary containing: - `\'radians\'`: The angle converted to radians. - `\'sine\'`: The sine of the angle in radians. - `\'cosine\'`: The cosine of the angle in radians. - `\'tangent\'`: The tangent of the angle in radians. - `\'distance\'`: The Euclidean distance as calculated in step 3. - `\'ceil_tangent\'`: The smallest integer greater than or equal to the tangent value. - `\'factorial_distance\'`: The factorial of the integer part of the distance. - `\'gcd\'`: The GCD of the integer part of the distance and the ceiling of the tangent value. Constraints - The angle can be any real number. - The `math.factorial` can handle input values up to at least 170. Example ```python angle = 45 result = compute_derived_values(angle) # Expected output (values approximated for readability): # { # \'radians\': 0.7853981633974483, # \'sine\': 0.7071067811865475, # \'cosine\': 0.7071067811865476, # \'tangent\': 0.9999999999999999, # \'distance\': 1.0, # \'ceil_tangent\': 1, # \'factorial_distance\': 1, # \'gcd\': 1 # } ``` **Note**: Make sure to handle edge cases: angles outside standard ranges, special float values like NaN, and infinities as specified in the `math` module behaviors.","solution":"import math def compute_derived_values(angle: float) -> dict: # Convert the angle to radians radians = math.radians(angle) # Calculate sine, cosine, and tangent of the angle in radians sine = math.sin(radians) cosine = math.cos(radians) tangent = math.tan(radians) # Compute the Euclidean distance from the origin (cosine, sine) distance = math.sqrt(cosine**2 + sine**2) # Return the smallest integer greater than or equal to the tangent value ceil_tangent = math.ceil(tangent) # Calculate the factorial of the integer part of the distance factorial_distance = math.factorial(math.floor(distance)) # Compute the GCD of the integer part of the distance and the ceiling of the tangent value gcd = math.gcd(math.floor(distance), ceil_tangent) return { \'radians\': radians, \'sine\': sine, \'cosine\': cosine, \'tangent\': tangent, \'distance\': distance, \'ceil_tangent\': ceil_tangent, \'factorial_distance\': factorial_distance, \'gcd\': gcd }"},{"question":"Objective: Demonstrate understanding and usage of the `sched` module to schedule, manage, and execute events. Problem Statement: You need to develop a mini event-management system that schedules messages to be printed at specific times with varying priorities. This system should leverage the `sched.scheduler` class to achieve this. Implement the following functions: 1. **`schedule_messages(time_stamps, messages, priorities)`**: - **Input**: - `time_stamps` (list of float): Contains the absolute times at which each message should be printed. - `messages` (list of str): Contains the messages to be printed. - `priorities` (list of int): Contains the priorities of the messages, such that a lower number represents a higher priority. - **Output**: - Returns a list of scheduled event objects (result of `scheduler.enterabs()`). - **Constraints**: - Length of `time_stamps`, `messages`, and `priorities` lists are all equal. - `time_stamps` contain future times. - Each `message` should be printed exactly once when its scheduled time comes. 2. **`run_scheduler(scheduler)`**: - **Input**: - `scheduler`: An instance of `sched.scheduler` with scheduled events. - **Output**: - This function does not return anything but should run all scheduled events. Example: ```python import time import sched def schedule_messages(time_stamps, messages, priorities): # Your implementation def run_scheduler(scheduler): # Your implementation # Example Usage: scheduler_instance = sched.scheduler(time.time, time.sleep) current_time = time.time() # Schedule messages time_stamps = [current_time + 5, current_time + 10, current_time + 5] messages = [\\"Message A\\", \\"Message B\\", \\"Message C\\"] priorities = [2, 1, 3] events = schedule_messages(time_stamps, messages, priorities) run_scheduler(scheduler_instance) ``` Expected Output (times are indicative): ``` 1652342835.3694863 Message A 1652342835.3696074 Message C 1652342840.369612 Message B ``` Here, \\"Message A\\" and \\"Message C\\" are scheduled at the same time but with different priorities. \\"Message B\\" is scheduled at a later time. Additional Information: - Ensure that the functions handle edge cases such as scheduling events with the same time but different priorities.","solution":"import sched import time def schedule_messages(time_stamps, messages, priorities): Schedules messages to be printed at specified absolute times with given priorities. :param time_stamps: List of absolute times when each message should be printed. :param messages: List of messages to be printed. :param priorities: List of priorities for each message. :return: List of scheduled event objects. scheduler_instance = sched.scheduler(time.time, time.sleep) events = [] for i in range(len(time_stamps)): event = scheduler_instance.enterabs(time_stamps[i], priorities[i], print, argument=(messages[i],)) events.append(event) return events, scheduler_instance def run_scheduler(scheduler_instance): Runs all scheduled events in the given scheduler instance. :param scheduler_instance: An instance of sched.scheduler with scheduled events. :return: None scheduler_instance.run()"},{"question":"# Advanced Python Unit Testing You are tasked with implementing a function and creating a comprehensive set of unit tests for it using Python\'s `unittest` module and the helpers provided by the `test.support` module. Function to Implement Implement a function `process_data` that processes a list of integers. The function should: 1. Return the list sorted in ascending order. 2. Handle `None` values by removing them before sorting. 3. Raise a `ValueError` if the list contains non-integer values. ```python def process_data(data): Processes a list of integers. Args: data (list): List of integers, may contain None values. Returns: list: Sorted list of integers without None values. Raises: ValueError: If the list contains non-integer values. # Implement the function here pass ``` Unit Tests You need to write a class `TestProcessData` in a module named `test_process_data` to test the `process_data` function, adhering to the guidelines provided in the documentation. Your tests should cover: 1. Standard cases with valid input lists. 2. Lists containing `None` values. 3. Lists containing non-integer values, ensuring a `ValueError` is raised. 4. Resource management ensuring no leftover state impacts other tests. 5. Performance constraints where the function should handle lists of up to `10^6` elements within a reasonable time frame. Additional Requirements 1. Use `test.support.run_unittest` to run your test cases. 2. Use `test.support.swap_attr` or `test.support.swap_item` if necessary to manage any temporary changes within your tests. 3. Utilize `test.support.check_syntax_error` to ensure the function handles invalid synthetic cases properly. 4. Use the timeout constants defined in `test.support` to manage time-sensitive tests. **Example Unit Test Skeleton:** ```python import unittest from test import support from your_module import process_data class TestProcessData(unittest.TestCase): def test_standard_case(self): # Test standard case. pass def test_with_none(self): # Test with None values. pass def test_with_non_integer(self): # Test with non-integer values. pass def test_performance(self): # Test performance with large lists. pass # More test methods as needed. # Boilerplate to run the tests if __name__ == \'__main__\': support.run_unittest(TestProcessData) ``` **Notes:** - Ensure your test cases follow the naming convention `test_<description>` for easy recognition. - Make use of the provided decorators and context managers to handle any temporary changes or resource management within your tests. - Document your edge cases and thought process in comments for clarity.","solution":"def process_data(data): Processes a list of integers. Args: data (list): List of integers, may contain None values. Returns: list: Sorted list of integers without None values. Raises: ValueError: If the list contains non-integer values. if not isinstance(data, list): raise ValueError(\\"Input should be a list\\") cleaned_data = [] for item in data: if item is not None: if not isinstance(item, int): raise ValueError(\\"List contains non-integer values\\") cleaned_data.append(item) return sorted(cleaned_data)"},{"question":"You have been tasked with implementing a Python function that performs analysis on a list of Unicode strings. The function should take the following actions: 1. Normalize each string in the list using the `NFC` (Normalization Form C) format. 2. For each normalized string, extract various properties for each character in the string, including: - Character name (use a default value \'<undefined>\' if no name is found) - Decimal value (if applicable) - Category - Bidirectional class - Mirrored property (convert to boolean) 3. Collect these properties into a list of dictionaries, where each dictionary corresponds to one character in one string. Each dictionary should have the following keys: - \'character\': the character itself - \'name\': character name - \'decimal_value\': decimal value (if applicable, otherwise None) - \'category\': Unicode category - \'bidirectional\': bidirectional class - \'mirrored\': boolean indicating if the character is mirrored # Input: - `unicode_strings` (List[str]): A list of Unicode strings. # Output: - List[List[Dict[str, Union[str, int, bool, None]]]]: A list containing lists of dictionaries. Each inner list corresponds to a Unicode string from the input list, and each dictionary contains the properties of one character. # Constraints: 1. The input list will have at most 100 strings. 2. Each string will have at most 100 characters. 3. The function should handle cases where character properties might not be defined gracefully. # Example: ```python from typing import List, Dict, Union import unicodedata def analyze_unicode_strings(unicode_strings: List[str]) -> List[List[Dict[str, Union[str, int, bool, None]]]]: # Implement this function pass # Example test case input_data = [\\"Hello\\", \\"123\\", \\"u00C7\\"] result = analyze_unicode_strings(input_data) print(result) # Expected output # [ # [ # {\'character\': \'H\', \'name\': \'LATIN CAPITAL LETTER H\', \'decimal_value\': None, \'category\': \'Lu\', \'bidirectional\': \'L\', \'mirrored\': False}, # {\'character\': \'e\', \'name\': \'LATIN SMALL LETTER E\', \'decimal_value\': None, \'category\': \'Ll\', \'bidirectional\': \'L\', \'mirrored\': False}, # {\'character\': \'l\', \'name\': \'LATIN SMALL LETTER L\', \'decimal_value\': None, \'category\': \'Ll\', \'bidirectional\': \'L\', \'mirrored\': False}, # {\'character\': \'l\', \'name\': \'LATIN SMALL LETTER L\', \'decimal_value\': None, \'category\': \'Ll\', \'bidirectional\': \'L\', \'mirrored\': False}, # {\'character\': \'o\', \'name\': \'LATIN SMALL LETTER O\', \'decimal_value\': None, \'category\': \'Ll\', \'bidirectional\': \'L\', \'mirrored\': False} # ], # [ # {\'character\': \'1\', \'name\': \'DIGIT ONE\', \'decimal_value\': 1, \'category\': \'Nd\', \'bidirectional\': \'EN\', \'mirrored\': False}, # {\'character\': \'2\', \'name\': \'DIGIT TWO\', \'decimal_value\': 2, \'category\': \'Nd\', \'bidirectional\': \'EN\', \'mirrored\': False}, # {\'character\': \'3\', \'name\': \'DIGIT THREE\', \'decimal_value\': 3, \'category\': \'Nd\', \'bidirectional\': \'EN\', \'mirrored\': False} # ], # [ # {\'character\': \'Ã‡\', \'name\': \'LATIN CAPITAL LETTER C WITH CEDILLA\', \'decimal_value\': None, \'category\': \'Lu\', \'bidirectional\': \'L\', \'mirrored\': False} # ] # ] ``` # Note: Ensure that you handle characters without defined names or decimal values by providing `<undefined>` for names and `None` for decimal values.","solution":"from typing import List, Dict, Union import unicodedata def analyze_unicode_strings(unicode_strings: List[str]) -> List[List[Dict[str, Union[str, int, bool, None]]]]: result = [] for string in unicode_strings: normalized_string = unicodedata.normalize(\'NFC\', string) char_properties = [] for char in normalized_string: char_info = { \'character\': char, \'name\': unicodedata.name(char, \'<undefined>\'), \'decimal_value\': unicodedata.decimal(char, None), \'category\': unicodedata.category(char), \'bidirectional\': unicodedata.bidirectional(char), \'mirrored\': unicodedata.mirrored(char) == 1, } char_properties.append(char_info) result.append(char_properties) return result"},{"question":"**Question: Dimensionality Reduction and Clustering with Scikit-Learn** You are provided with a dataset containing several features. Your task is to perform unsupervised dimensionality reduction followed by a clustering task. Implement a pipeline that includes the following steps: 1. **Standardize the Dataset**: Use `StandardScaler` to standardize the dataset. 2. **Apply PCA**: Reduce the dimensionality of the dataset using PCA, retaining the top 95% variance. 3. **Feature Agglomeration**: Use `FeatureAgglomeration` to group similar features after PCA reduction. Assume you want to reduce the number of features to 5 clusters. Write a function that performs these steps, takes the dataset as input, and returns the transformed dataset. # Function Signature ```python def perform_dimensionality_reduction_and_clustering(X: np.ndarray) -> np.ndarray: Perform dimensionality reduction and clustering on the given dataset. Parameters: X (np.ndarray): The input dataset with shape (n_samples, n_features). Returns: np.ndarray: The transformed dataset with reduced dimensions. ``` # Input - `X`: A 2D NumPy array of shape `(n_samples, n_features)`, representing the dataset. # Output - A 2D NumPy array representing the transformed dataset after applying the specified dimensionality reduction and clustering steps. # Constraints - You must use the classes and methods from the `scikit-learn` library. - Ensure that the pipeline is properly constructed to handle the transformations in sequence. - The transformed dataset should have fewer features than the original dataset due to the PCA and Feature Agglomeration steps. # Example ```python import numpy as np # Example input dataset (n_samples=10, n_features=8) X = np.random.rand(10, 8) # Call the function with the input dataset transformed_X = perform_dimensionality_reduction_and_clustering(X) # Output the shape of transformed_X print(transformed_X.shape) ``` `transformed_X` should have shape `(10, 5)` indicating that the features have been reduced to 5 clusters. # Additional Information - You can assume that `numpy` and `scikit-learn` are imported as `np` and `sklearn` respectively. - Refer to the scikit-learn documentation for more details on using `PCA`, `FeatureAgglomeration`, and `StandardScaler`.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration def perform_dimensionality_reduction_and_clustering(X: np.ndarray) -> np.ndarray: Perform dimensionality reduction and clustering on the given dataset. Parameters: X (np.ndarray): The input dataset with shape (n_samples, n_features). Returns: np.ndarray: The transformed dataset with reduced dimensions. # Step 1: Standardize the Dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: Apply PCA to retain 95% variance pca = PCA(n_components=0.95, svd_solver=\'full\') X_pca = pca.fit_transform(X_scaled) # Step 3: Apply Feature Agglomeration to group features into 5 clusters agglomeration = FeatureAgglomeration(n_clusters=5) X_transformed = agglomeration.fit_transform(X_pca) return X_transformed"},{"question":"You are required to write a Python program that performs the following operations using the `gzip`, `zipfile`, and `tarfile` modules: 1. **Compress a File using gzip**: Write a function `compress_file_gzip(input_file, output_file)` that compresses a given file using the gzip algorithm. - **Input**: - `input_file`: string, the path to the file to be compressed. - `output_file`: string, the path to the resulting compressed file. - **Output**: None. The function should create a compressed file at `output_file`. 2. **Create a ZIP Archive**: Write a function `create_zip_archive(file_list, archive_name)` that takes a list of file paths and creates a ZIP archive. - **Input**: - `file_list`: list of strings, each string being a file path to be added to the archive. - `archive_name`: string, the name of the resulting ZIP file. - **Output**: None. The function should create a ZIP archive containing the files from `file_list`. 3. **Extract a TAR Archive**: Write a function `extract_tar_archive(archive_file, destination_dir)` that extracts the contents of a TAR archive to a specified directory. - **Input**: - `archive_file`: string, the path to the TAR archive to be extracted. - `destination_dir`: string, the path to the directory where the contents should be extracted. - **Output**: None. The function should extract all files and directories from `archive_file` to `destination_dir`. # Constraints and Limitations: - You may assume that the `input_file`, `file_list`, `archive_name`, `archive_file`, and `destination_dir` parameters are all valid paths. - Handle any exceptions that may occur during file operations (e.g., FileNotFoundError, IOError) by providing appropriate error messages. - The program should be efficient with respect to both time and space complexity, considering potential large file sizes. # Example Usage: ```python # Compress a file using gzip compress_file_gzip(\'example.txt\', \'example.txt.gz\') # Create a ZIP archive with multiple files create_zip_archive([\'example1.txt\', \'example2.txt\'], \'archive.zip\') # Extract the contents of a TAR archive extract_tar_archive(\'archive.tar\', \'extracted_files\') ``` Implement the three functions in the same script, ensuring they can be tested and verified independently.","solution":"import gzip import shutil import zipfile import tarfile import os def compress_file_gzip(input_file, output_file): Compress a file using gzip. Parameters: input_file (str): Path to the input file to be compressed. output_file (str): Path to the output compressed file. Returns: None try: with open(input_file, \'rb\') as f_in: with gzip.open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except (FileNotFoundError, IOError) as e: print(f\\"Error compressing file {input_file}: {e}\\") def create_zip_archive(file_list, archive_name): Create a ZIP archive from a list of files. Parameters: file_list (list of str): List of file paths to be added to the archive. archive_name (str): Name of the output ZIP file. Returns: None try: with zipfile.ZipFile(archive_name, \'w\', zipfile.ZIP_DEFLATED) as zipf: for file in file_list: zipf.write(file, os.path.basename(file)) except (FileNotFoundError, IOError) as e: print(f\\"Error creating ZIP archive {archive_name}: {e}\\") def extract_tar_archive(archive_file, destination_dir): Extract a TAR archive to a specified directory. Parameters: archive_file (str): Path to the TAR archive to be extracted. destination_dir (str): Path to the directory where contents should be extracted. Returns: None try: with tarfile.open(archive_file, \'r\') as tar: tar.extractall(path=destination_dir) except (FileNotFoundError, IOError, tarfile.TarError) as e: print(f\\"Error extracting TAR archive {archive_file}: {e}\\")"},{"question":"**Coding Assessment Question** # Objective Evaluate the students\' knowledge of pandas functionality, covering data creation, manipulation, merging, and a series of calculations using the apply and aggregation methods. # Problem Statement Imagine you have two datasets representing monthly sales information from two different regions. Your task is to combine these datasets, clean the data, and extract meaningful insights using pandas. # Inputs - `data1.json`: Monthly sales data for Region 1. - `data2.json`: Monthly sales data for Region 2. Each JSON file contains records of the format: ```json [ {\\"date\\": \\"2023-01-01\\", \\"product\\": \\"A\\", \\"sales\\": 100, \\"returns\\": 5}, {\\"date\\": \\"2023-01-01\\", \\"product\\": \\"B\\", \\"sales\\": 80, \\"returns\\": 2}, ... ] ``` # Instructions 1. **Data Loading and Cleaning**: - Load the JSON data into two separate pandas DataFrames. - Ensure that the date column is of type datetime. - Fill missing sales or returns values with zeroes. 2. **Combining Data**: - Merge the two DataFrames based on the date and product columns to get a combined DataFrame where sales and returns from both regions are included. - Rename the sales and returns columns to indicate the relevant region (e.g., `sales_r1`, `returns_r1`, `sales_r2`, `returns_r2`). 3. **Calculations**: - Add a new column \'total_sales\' showing the total sales from both regions. - Add a new column \'total_returns\' showing the total returns from both regions. - Calculate and add a new column \'net_sales\' (total sales minus total returns). - Calculate and add another column \'return_rate\' (total returns divided by total sales, expressed as a percentage). 4. **Insights and Aggregation**: - Compute the monthly total net_sales and return_rate for each product. - Identify the month-product combinations with the highest and lowest net_sales. - Generate a summary table showing each product\'s median net_sales and mean return_rate for the entire period. # Output Provide: - The modified combined DataFrame after performing all calculations. - A summary table as a pandas DataFrame showing each product\'s median net_sales and mean return_rate. # Constraints - Assume the data files always have valid JSON records. - Date columns in the JSON files are in the format \\"YYYY-MM-DD\\". - Sales and returns values can sometimes be missing, and should be treated as zero. # Example Function Signature ```python import pandas as pd def analyze_sales(data1_path: str, data2_path: str) -> (pd.DataFrame, pd.DataFrame): # Load and clean data df1 = pd.read_json(data1_path) df2 = pd.read_json(data2_path) # Ensure date is in datetime format df1[\'date\'] = pd.to_datetime(df1[\'date\']) df2[\'date\'] = pd.to_datetime(df2[\'date\']) # Fill missing sales/returns with zero df1[\'sales\'].fillna(0, inplace=True) df1[\'returns\'].fillna(0, inplace=True) df2[\'sales\'].fillna(0, inplace=True) df2[\'returns\'].fillna(0, inplace=True) # Merge data on date and product df_merged = pd.merge(df1, df2, on=[\'date\', \'product\'], how=\'outer\', suffixes=(\'_r1\', \'_r2\')) # Add total_sales, total_returns, net_sales, and return_rate df_merged[\'total_sales\'] = df_merged[\'sales_r1\'] + df_merged[\'sales_r2\'] df_merged[\'total_returns\'] = df_merged[\'returns_r1\'] + df_merged[\'returns_r2\'] df_merged[\'net_sales\'] = df_merged[\'total_sales\'] - df_merged[\'total_returns\'] df_merged[\'return_rate\'] = (df_merged[\'total_returns\'] / df_merged[\'total_sales\']) * 100 # Monthly aggregation for each product monthly_summary = df_merged.resample(\'M\', on=\'date\').agg({\'net_sales\': [\'sum\'], \'return_rate\': [\'mean\']}).reset_index() # Summary table for median net_sales and mean return_rate summary_table = df_merged.groupby(\'product\').agg( median_net_sales=pd.NamedAgg(column=\'net_sales\', aggfunc=\'median\'), mean_return_rate=pd.NamedAgg(column=\'return_rate\', aggfunc=\'mean\') ).reset_index() return df_merged, summary_table ``` You should replace this function signature in your solution and ensure your implementation matches the given requirements. # Deadline You have 90 minutes to complete this coding question. # Evaluation Criteria - Correctness: The logic should correctly compute the expected results. - Efficiency: The implementation should efficiently handle the data. - Clarity: Code should be clear and well-commented.","solution":"import pandas as pd def analyze_sales(data1_path: str, data2_path: str) -> (pd.DataFrame, pd.DataFrame): # Load the JSON data into pandas DataFrames df1 = pd.read_json(data1_path) df2 = pd.read_json(data2_path) # Ensure the date column is of type datetime df1[\'date\'] = pd.to_datetime(df1[\'date\']) df2[\'date\'] = pd.to_datetime(df2[\'date\']) # Fill missing sales or returns values with zeroes df1[\'sales\'].fillna(0, inplace=True) df1[\'returns\'].fillna(0, inplace=True) df2[\'sales\'].fillna(0, inplace=True) df2[\'returns\'].fillna(0, inplace=True) # Merge the two DataFrames based on the date and product columns df_combined = pd.merge(df1, df2, on=[\'date\', \'product\'], how=\'outer\', suffixes=(\'_r1\', \'_r2\')) # Renaming the sales and returns columns is already handled with the suffixes # Add calculated columns df_combined[\'total_sales\'] = df_combined[\'sales_r1\'] + df_combined[\'sales_r2\'] df_combined[\'total_returns\'] = df_combined[\'returns_r1\'] + df_combined[\'returns_r2\'] df_combined[\'net_sales\'] = df_combined[\'total_sales\'] - df_combined[\'total_returns\'] df_combined[\'return_rate\'] = (df_combined[\'total_returns\'] / df_combined[\'total_sales\']) * 100 # Handle the case where total_sales is zero to prevent division by zero in return_rate calculation df_combined[\'return_rate\'] = df_combined.apply( lambda row: 0 if row[\'total_sales\'] == 0 else row[\'return_rate\'], axis=1 ) # Monthly total net_sales and return_rate for each product monthly_summary = df_combined.resample(\'M\', on=\'date\').agg({ \'net_sales\': \'sum\', \'return_rate\': \'mean\'}).reset_index() # Summary table showing each product\'s median net_sales and mean return_rate summary_table = df_combined.groupby(\'product\').agg( median_net_sales=pd.NamedAgg(column=\'net_sales\', aggfunc=\'median\'), mean_return_rate=pd.NamedAgg(column=\'return_rate\', aggfunc=\'mean\') ).reset_index() return df_combined, summary_table"},{"question":"You are given a task to fetch data from an API and process it. Your task is to implement a Python function that performs the following: 1. Sends a GET request to a specified URL. 2. Adds custom headers to the request, including a User-Agent string and an Accept header. 3. Handles HTTP basic authentication if required. 4. Processes the JSON response from the server and returns a specific value from it. # Function Signature ```python def fetch_and_process_data(url: str, headers: dict, auth: tuple = None) -> Any: Fetch data from a given URL and process the JSON response. Parameters: - url (str): The URL to send the GET request to. - headers (dict): A dictionary of headers to add to the request. Example: {\'User-Agent\': \'my-app/0.0.1\', \'Accept\': \'application/json\'} - auth (tuple, optional): A tuple containing (\'username\', \'password\') for HTTP basic authentication. Returns: - Any: The specific value extracted from the JSON response based on the required key. pass ``` # Constraints 1. The function should handle HTTP errors gracefully by returning `None` if the request fails. 2. The headers dictionary will always contain at least \'User-Agent\' and \'Accept\' keys. 3. If `auth` is provided, use it for HTTP basic authentication. 4. The JSON response will always contain the key \'data\' and \'data\' will be a dictionary containing the key \'value\' that needs to be returned. # Example Usage ```python url = \\"https://api.example.com/data\\" headers = { \'User-Agent\': \'my-app/0.0.1\', \'Accept\': \'application/json\' } auth = (\'username\', \'password\') result = fetch_and_process_data(url, headers, auth) print(result) # Expected to print the value associated with the key \'value\' in the \'data\' dictionary in the JSON response ``` # Notes - Use the `urllib.request` module to perform the HTTP operations. - Ensure proper error handling and response processing. - The function should be efficient and adhere to best practices for making HTTP requests in Python.","solution":"import urllib.request import json from typing import Any, Optional def fetch_and_process_data(url: str, headers: dict, auth: Optional[tuple] = None) -> Any: Fetch data from a given URL and process the JSON response. Parameters: - url (str): The URL to send the GET request to. - headers (dict): A dictionary of headers to add to the request. Example: {\'User-Agent\': \'my-app/0.0.1\', \'Accept\': \'application/json\'} - auth (tuple, optional): A tuple containing (\'username\', \'password\') for HTTP basic authentication. Returns: - Any: The specific value extracted from the JSON response based on the required key. try: req = urllib.request.Request(url, headers=headers) if auth is not None: auth_encoded = urllib.request.HTTPBasicAuthHandler() auth_encoded.add_password(realm=None, uri=url, user=auth[0], passwd=auth[1]) opener = urllib.request.build_opener(auth_encoded) urllib.request.install_opener(opener) with urllib.request.urlopen(req) as response: response_data = response.read().decode() json_response = json.loads(response_data) if \'data\' in json_response and \'value\' in json_response[\'data\']: return json_response[\'data\'][\'value\'] else: return None except Exception as e: return None"},{"question":"**Python `syslog` Logging Challenge** # Objective You are tasked to create a Python script that utilizes the `syslog` module to log messages at different priority levels and facilities with customized logging options. This will help you demonstrate your understanding of the `syslog` library and how to configure its behavior. # Requirements 1. **Function Implementation:** Write a function `configure_and_log` with the following signature: ```python def configure_and_log(ident: str, message: str, priority: int, facility: int, log_options: int) -> None: ``` - **Inputs:** - `ident` (str): A string that will be prepended to every log message. - `message` (str): The message string to send to the system logger. - `priority` (int): The priority level for the log message (e.g., `syslog.LOG_INFO`). - `facility` (int): The facility to use for logging (e.g., `syslog.LOG_USER`). - `log_options` (int): Bitwise OR combination of logging options (e.g., `syslog.LOG_PID | syslog.LOG_CONS`). 2. **Constraints and Requirements:** - The function should call `syslog.openlog()` with the provided `ident`, `log_options`, and `facility`. - It should log the message with the given priority level using `syslog.syslog()`. The message should include the process ID if `syslog.LOG_PID` is part of `log_options`. - The function should reset the logger settings by calling `syslog.closelog()` at the end to ensure no residual logging options affect subsequent logging. # Example Usage ```python import syslog def configure_and_log(ident, message, priority, facility, log_options): syslog.openlog(ident, log_options, facility) syslog.syslog(priority, message) syslog.closelog() # Example configurations and logging configure_and_log(ident=\\"MyApp\\", message=\\"Application started\\", priority=syslog.LOG_INFO, facility=syslog.LOG_USER, log_options=syslog.LOG_PID | syslog.LOG_CONS) # Log a critical error to the mail facility configure_and_log(ident=\\"MyApp\\", message=\\"Critical error occurred\\", priority=syslog.LOG_CRIT, facility=syslog.LOG_MAIL, log_options=syslog.LOG_PID | syslog.LOG_CONS) ``` # Testing and Validation - Verify that the function logs messages as expected with different identifiers, priorities, facilities, and log options. - Ensure that calling the function does not leave any residual logging configurations that might affect subsequent logging. Implement `configure_and_log` to test and validate your understanding of the `syslog` module and its configuration capabilities.","solution":"import syslog def configure_and_log(ident: str, message: str, priority: int, facility: int, log_options: int) -> None: Configures and logs a message to syslog with the specified parameters. :param ident: A string that will be prepended to every log message. :param message: The message string to send to the system logger. :param priority: The priority level for the log message. :param facility: The facility to use for logging. :param log_options: Bitwise OR combination of logging options. syslog.openlog(ident=ident, logoption=log_options, facility=facility) syslog.syslog(priority, message) syslog.closelog() # Example usage #configure_and_log(ident=\\"MyApp\\", message=\\"Application started\\", priority=syslog.LOG_INFO, facility=syslog.LOG_USER, log_options=syslog.LOG_PID | syslog.LOG_CONS) #configure_and_log(ident=\\"MyApp\\", message=\\"Critical error occurred\\", priority=syslog.LOG_CRIT, facility=syslog.LOG_MAIL, log_options=syslog.LOG_PID | syslog.LOG_CONS)"},{"question":"**Question: Data Visualization with Seaborn** You are provided with a dataset named `mpg`, which details various attributes of cars, including horsepower, fuel consumption (miles per gallon - mpg), origin, and weight. **Task:** Write a Python function `create_mpg_plot()` that uses seaborn\'s `objects` interface to create a comprehensive scatter plot based on the following specifications: 1. Plot `horsepower` on the x-axis and `mpg` on the y-axis. 2. Use dots to represent individual data points. 3. Color the dots based on the `origin` of the cars. 4. Use different fill colors based on the `weight` of the cars. 5. Mix two types of markers: \'o\' for cars originating from \'usa\' or \'japan\', and \'x\' for cars originating from \'europe\'. 6. Add jitter to the data points to visualize local density better. 7. Ensure partial transparency to help visualize overlapping points. **Function Signature:** ```python def create_mpg_plot(mpg: pd.DataFrame) -> None: pass ``` **Input:** - `mpg` (pandas.DataFrame): The dataset containing the car attributes. **Output:** - The function should display the plot but return `None`. **Constraints:** - Assume the input DataFrame `mpg` has the columns: [\'horsepower\', \'mpg\', \'origin\', \'weight\']. **Example of Function Usage:** ```python import pandas as pd from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Display the plot create_mpg_plot(mpg) ``` **Notes:** - Documentation and examples for seaborn objects can be found here: (Provide relevant links or notes if necessary). - Ensure your code is well-commented and follows best practices in coding.","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def create_mpg_plot(mpg: pd.DataFrame) -> None: This function creates a scatter plot using seaborn\'s objects interface. Parameters: mpg (pd.DataFrame): The input DataFrame containing car attributes. Returns: None # Define the marker type based on origin mpg[\'marker\'] = mpg[\'origin\'].apply(lambda x: \'o\' if x in [\'usa\', \'japan\'] else \'x\') # Create the plot p = ( so.Plot(mpg, x=\'horsepower\', y=\'mpg\', color=\'origin\', marker=\'marker\', pointsize=4) .add(so.Dot(), jitter=0.2, alpha=0.7, fillcolor=\'weight\') ) # Display the plot p.show() # Example Usage: # import seaborn as sns # mpg = sns.load_dataset(\\"mpg\\") # create_mpg_plot(mpg)"},{"question":"Objective: You are required to implement a Python function utilizing the `mailcap` module to find applicable commands for given MIME types from Mailcap files, and enhance the command by generating a secure filename. Problem Statement: Implement a function `generate_mailcap_command(mime_type: str, key: str, params: dict = None) -> tuple` that: 1. Uses `mailcap.getcaps()` to retrieve the mailcap entries. 2. Uses `mailcap.findmatch()` to find the command for a given `mime_type` and `key`. 3. Generates a secure temporary filename using only alphanumeric characters. 4. Applies the given `params` to the command. 5. Returns a 2-tuple with: - The generated command with the secure temporary filename. - The original mailcap entry for the given MIME type. 6. Returns `(None, None)` if no match or a security issue is found. Input: - `mime_type` (str): The MIME type to search for (e.g., \'video/mpeg\'). - `key` (str): The type of activity to perform (e.g., \'view\'). - `params` (dict): Named parameters to substitute in the command (default is None). Output: - A tuple with two elements: - The constructed command with a secure temporary filename. - The original mailcap entry dictionary. Constraints: - The function must ensure that generated filenames and parameter values are secure and do not contain risky characters. Example: ```python import string import random import mailcap def generate_secure_filename() -> str: Generate a secure temporary filename. return \'\'.join(random.choices(string.ascii_letters + string.digits, k=12)) def generate_mailcap_command(mime_type: str, key: str, params: dict = None) -> tuple: # Your implementation here ``` Example usage: ```python # Sample mailcap entry defined as {\'view\': \'xmpeg %s\'} # Assuming getcaps() dictionary has appropriate entries for \'video/mpeg\' result = generate_mailcap_command(\'video/mpeg\', \'view\', {\'id\': \'123\'}) print(result) # Should output a tuple like (\'xmpeg <secure_temp_filename>\', {\'view\': \'xmpeg %s\'}) ``` # Additional Notes: - Ensure that your implementation correctly handles and substitutes parameters. - Your generated filename must pass all the security checks as described in the analysis. - Validate that the generated command and original mailcap dictionary are correctly outputted.","solution":"import string import random import mailcap def generate_secure_filename() -> str: Generate a secure temporary filename. return \'\'.join(random.choices(string.ascii_letters + string.digits, k=12)) def generate_mailcap_command(mime_type: str, key: str, params: dict = None) -> tuple: caps = mailcap.getcaps() command, entry = mailcap.findmatch(caps, mime_type, key, filename=generate_secure_filename(), plist=params) if command and entry: command = command.split() secure_filename = generate_secure_filename() # Replace the placeholder with the secure filename command = [part if \\"%s\\" not in part else part.replace(\\"%s\\", secure_filename) for part in command] command = \\" \\".join(command) return (command, entry) return (None, None)"},{"question":"<|Analysis Begin|> The provided documentation discusses the data structures accepted by Seaborn, a Python visualization library. It explains the difference between long-form and wide-form data, showing examples of how to use them for visualization. It also details how Seaborn interprets and plots data, providing insights and code snippets to guide the user in plotting long-form and wide-form data. The documentation shows how to handle \\"messy\\" data by converting it into a tidy long-form structure and discusses the flexibility in Seaborn to accept different data structures like pandas DataFrames, dictionaries, and NumPy arrays. The documentation covers: 1. Long-form vs. wide-form data. 2. Methods to pivot data and plot it using Seaborn. 3. Dealing with messy data by transforming it into a suitable format for plotting. 4. Various data structures supported by Seaborn for plotting. Based on this, a challenging, clear, and self-contained coding question can be created that will test the students\' understanding of how to manipulate data into different formats and how to create visualizations using Seaborn. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective: Demonstrate your understanding of data manipulation and visualization using Seaborn by transforming and plotting a given dataset. Problem Statement: You are given a dataset of car sales data. The dataset contains the following columns: - `year`: The year of the sale. - `month`: The month of the sale (e.g., \\"January\\", \\"February\\"). - `sales`: The number of cars sold. 1. First, transform this dataset into both long-form and wide-form data structures. 2. Plot both versions of the data using Seaborn. Input: A `pandas.DataFrame` named `car_sales` with the following sample data: ```python year month sales 0 2020 January 100 1 2020 February 120 2 2020 March 130 3 2021 January 150 4 2021 February 160 5 2021 March 170 ``` Tasks: 1. Create a long-form version of `car_sales` where each column represents a variable, and each row represents an observation. 2. Create a wide-form version of `car_sales` where each column represents a month, and each row represents a year. 3. Plot the long-form data using Seaborn, showing the monthly car sales for each year. 4. Plot the wide-form data using Seaborn, showing the monthly car sales for each year. Output: 1. Two Seaborn plots: - One using the long-form data to show the number of cars sold per month for each year. - One using the wide-form data to show the number of cars sold per month for each year. Constraints: - Use Seaborn\'s `relplot` or any other appropriate plotting function. - Ensure appropriate axis labels and titles for clarity. - Use Seaborn\'s theme settings for aesthetic consistency. Example Output: Your code should generate two plots. Below are the expected steps and plot descriptions: 1. **Long-form Plot:** - Each line represents a year. - The x-axis represents the months. - The y-axis represents the number of cars sold. - The legend distinguishes between different years. 2. **Wide-form Plot:** - Each line represents a month. - The x-axis represents the years. - The y-axis represents the number of cars sold. - The legend distinguishes between different months. Use the following template to structure your solution: ```python import pandas as pd import seaborn as sns # Sample data for car_sales DataFrame data = { \'year\': [2020, 2020, 2020, 2021, 2021, 2021], \'month\': [\'January\', \'February\', \'March\', \'January\', \'February\', \'March\'], \'sales\': [100, 120, 130, 150, 160, 170] } car_sales = pd.DataFrame(data) # Task 1: Transform to long-form data (already in long-form) # Task 2: Transform to wide-form data car_sales_wide = car_sales.pivot(index=\'year\', columns=\'month\', values=\'sales\') # Task 3: Plot long-form data sns.relplot(data=car_sales, x=\'month\', y=\'sales\', hue=\'year\', kind=\'line\') # Task 4: Plot wide-form data sns.relplot(data=car_sales_wide, kind=\'line\') ``` Ensure your plots are clear, with appropriate labels and titles for better interpretation.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample data for car_sales DataFrame data = { \'year\': [2020, 2020, 2020, 2021, 2021, 2021], \'month\': [\'January\', \'February\', \'March\', \'January\', \'February\', \'March\'], \'sales\': [100, 120, 130, 150, 160, 170] } car_sales = pd.DataFrame(data) # Task 1: Transform to long-form data (already in long-form) # Task 2: Transform to wide-form data car_sales_wide = car_sales.pivot(index=\'year\', columns=\'month\', values=\'sales\') # Task 3: Plot long-form data plt.figure(figsize=(10, 6)) sns.lineplot(data=car_sales, x=\'month\', y=\'sales\', hue=\'year\') plt.title(\'Monthly Car Sales (Long-form Data)\') plt.xlabel(\'Month\') plt.ylabel(\'Number of Cars Sold\') plt.legend(title=\'Year\') plt.show() # Task 4: Plot wide-form data car_sales_wide.reset_index(inplace=True) car_sales_melted = car_sales_wide.melt(id_vars=\'year\', var_name=\'month\', value_name=\'sales\') plt.figure(figsize=(10, 6)) sns.lineplot(data=car_sales_melted, x=\'year\', y=\'sales\', hue=\'month\') plt.title(\'Monthly Car Sales (Wide-form Data)\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Cars Sold\') plt.legend(title=\'Month\') plt.show()"},{"question":"<|Analysis Begin|> The given documentation briefly introduces the `asyncio` module, which is a library used for writing concurrent code with the `async/await` syntax. It covers the following points: - Basic example of creating and running an asynchronous function. - Purpose and usage of `asyncio` in high-performance and IO-bound concurrent programming. - High-level APIs provided by `asyncio` for running coroutines, performing network IO, controlling subprocesses, distributing tasks, and synchronizing concurrent code. - Low-level APIs for developers to create and manage event loops, implement efficient protocols, and bridge callback-based libraries with async/await. However, the provided documentation mainly outlines the usage and scope of the `asyncio` library and does not delve deep into specific APIs or their functionalities. Based on this limited information, a cohesive and well-thought-out coding assessment question can still be designed focusing on the basic usage of asynchronous functions and `asyncio`. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate your understanding of Python\'s `asyncio` library by implementing a function that handles multiple asynchronous tasks and processes their results. **Problem Statement:** You need to write a function `fetch_data_concurrently(urls: list[str]) -> dict[str, str]` that takes a list of URLs and fetches their contents concurrently using `asyncio`. Each URL fetch should be processed asynchronously, and the results should be stored in a dictionary where the keys are the URLs and the values are the fetched content. # Function Signature: ```python import asyncio import aiohttp # You will need to install aiohttp for HTTP requests async def fetch_data_concurrently(urls: list[str]) -> dict[str, str]: pass ``` # Input: - `urls` (list of str): A list of URLs to be fetched. Each URL is a string. # Output: - (dict): A dictionary where keys are the input URLs and values are the contents fetched from those URLs. # Constraints: 1. You cannot use any external libraries other than `aiohttp` for making HTTP requests. 2. The function should handle the requests concurrently. 3. Assume the URLs are valid and the content fetched is always in the text format. 4. Handle exceptions gracefully and skip any URL that throws an error during fetching, assigning None to its value in the dictionary. # Example: ```python import asyncio import aiohttp async def fetch_data(url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def fetch_data_concurrently(urls: list[str]) -> dict[str, str]: async def fetch(url): try: content = await fetch_data(url) return url, content except Exception as e: return url, None tasks = [fetch(url) for url in urls] results = await asyncio.gather(*tasks) return dict(results) # Test urls = [\'http://example.com\', \'http://example.org\'] results = asyncio.run(fetch_data_concurrently(urls)) print(results) ``` In this question, you are required to: - Use `asyncio` to manage the concurrency. - Use the asynchronous capabilities to fetch multiple URLs concurrently. - Handle network requests gracefully and store the fetched content in a dictionary. This question assesses your understanding of basic and intermediate concepts of asynchronous programming with `asyncio` in Python. Be sure to handle exceptions appropriately to ensure robust code.","solution":"import asyncio import aiohttp async def fetch_data(url): async with aiohttp.ClientSession() as session: async with session.get(url) as response: return await response.text() async def fetch_data_concurrently(urls: list[str]) -> dict[str, str]: async def fetch(url): try: content = await fetch_data(url) return url, content except Exception as e: return url, None tasks = [fetch(url) for url in urls] results = await asyncio.gather(*tasks) return dict(results)"},{"question":"# Question Your task is to implement a function that computes the per-sample gradients of a given function using PyTorch and `torch.func`. **Problem statement**: Given a function `f: Tensor -> Tensor` which computes a scalar value from an input tensor, implement a new function `per_sample_gradients(f: Callable[[Tensor], Tensor], inputs: Tensor) -> Tensor`, which returns a tensor of gradients of the function `f` with respect to each sample in `inputs`. The function `per_sample_gradients` should use the `grad` and `vmap` transforms from `torch.func`. # Input: 1. `f`: A scalar function that takes a tensor input and returns a tensor of size single value (scalar). 2. `inputs`: A tensor of shape `(N, D)` where `N` is the number of samples and `D` is the dimensionality of each sample. # Output: - A tensor of shape `(N, D)` where each entry `[i, :]` is the gradient of the function `f` with respect to the i-th input sample. # Constraints: 1. You may assume that `1 <= N <= 1000` and `1 <= D <= 100`. 2. The function `f` is differentiable with respect to its input. 3. Performance considerations should keep in mind possible large batch sizes due to `N` but assume memory will not be a constraint within the provided limits. # Example: ```python import torch def f(x): return torch.sum(x ** 2) inputs = torch.randn(3, 4) output = per_sample_gradients(f, inputs) print(output) ``` Expected output: A tensor of shape `(3, 4)` where each row corresponds to the gradient of `f` with respect to the corresponding row of `inputs`. # Your task: Implement the `per_sample_gradients` function to meet the requirements as outlined. ```python def per_sample_gradients(f: Callable[[Tensor], Tensor], inputs: Tensor) -> Tensor: # Write your code here pass ``` Use the functionalities provided by `torch.func` to achieve this.","solution":"import torch from torch.autograd import grad def per_sample_gradients(f, inputs): Compute per-sample gradients of the function f with respect to each sample in inputs. Args: f: A scalar function that maps a tensor input to a single value. inputs: A tensor of shape (N, D) where N is the number of samples and D is the dimensionality of each sample. Returns: A tensor of shape (N, D) containing gradients for each sample. inputs.requires_grad_(True) outputs = f(inputs).sum() gradients = grad(outputs, inputs, create_graph=True)[0] return gradients"},{"question":"**Clustering and Evaluation with Scikit-Learn** # Objective: You are given a dataset consisting of customer data. Your task is to implement a clustering pipeline using one of the clustering algorithms provided by scikit-learn. You need to evaluate the performance of your clustering algorithm using appropriate metrics and visualize the clustering results. # Dataset: The dataset contains customer features such as `Annual Income`, `Spending Score`, and `Age`. # Task: 1. **Data Preparation**: - Load the dataset. - Feature normalization: Scale the features using `StandardScaler` from scikit-learn. 2. **Clustering**: - Apply the K-Means clustering algorithm to cluster the customers into a specified number of clusters, say `k=5`. - Optional: Experiment with other clustering algorithms mentioned in the documentation (e.g., DBSCAN, Spectral Clustering) and compare the results. 3. **Evaluation**: - Evaluate the performance of your clustering using the following metrics: - Adjusted Rand Index - Silhouette Score - Calinski-Harabasz Index - Provide the evaluation scores. 4. **Visualization**: - Visualize the clusters: - Use a 2D projection of the clusters (such as PCA or t-SNE) with each point colored according to its cluster assignment. 5. **Write a report**: - Summarize your findings in a concise report: - Describe the clustering results and the performance metrics. - Discuss any observations or insights you have gained from the clustering. # Input: - `X_train`: A numpy array of shape `(n_samples, n_features)` containing customer data. - `n_clusters`: An integer indicating the number of clusters to use for the K-Means algorithm. # Output: - An object containing the clustering results and the evaluation scores. # Constraints: - Use proper scikit-learn methods for clustering, scaling, and evaluation. - Ensure reproducibility by setting a random seed where applicable. # Sample Function Signature: ```python import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.manifold import TSNE from sklearn.cluster import KMeans from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score def customer_clustering(X_train: np.array, n_clusters: int = 5): # Step 1: Data Preparation scaler = StandardScaler() X_scaled = scaler.fit_transform(X_train) # Step 2: Clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(X_scaled) # Step 3: Evaluation ari = adjusted_rand_score(X_train.flatten(), cluster_labels) sil_score = silhouette_score(X_scaled, cluster_labels) ch_score = calinski_harabasz_score(X_scaled, cluster_labels) # Step 4: Visualization pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Report report = { \'Adjusted Rand Index\': ari, \'Silhouette Score\': sil_score, \'Calinski-Harabasz Index\': ch_score, \'PCA Projection\': X_pca, \'Cluster Labels\': cluster_labels } return report # Example Usage # X_train = np.array(...) # Load your data here # n_clusters = 5 # results = customer_clustering(X_train, n_clusters) # print(results) ``` Make sure to provide clear instructions and any necessary data for students to complete the task.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.metrics import adjusted_rand_score, silhouette_score, calinski_harabasz_score def customer_clustering(X_train: np.array, n_clusters: int = 5): Perform clustering on customer data and evaluate the results using different metrics. :param X_train: np.array, shape (n_samples, n_features), customer data for clustering :param n_clusters: int, number of clusters for the K-Means algorithm :return: dict, clustering results and evaluation scores # Step 1: Data Preparation scaler = StandardScaler() X_scaled = scaler.fit_transform(X_train) # Step 2: Clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(X_scaled) # Step 3: Evaluation # Note: Adjusted Rand Index usually requires true labels. Here we skip it as we assume unsupervised learning setup. sil_score = silhouette_score(X_scaled, cluster_labels) ch_score = calinski_harabasz_score(X_scaled, cluster_labels) # Step 4: Visualization pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Report report = { \'Silhouette Score\': sil_score, \'Calinski-Harabasz Index\': ch_score, \'PCA Projection\': X_pca, \'Cluster Labels\': cluster_labels } return report"},{"question":"# Complex Templating and Logging System Problem Statement: You are tasked with implementing a system that processes multiple text files using advanced templating and concurrent processing, while logging key events. Specifically, your system should read multiple input text files and use templating to fill in placeholders with user-defined values. This should be done concurrently to improve efficiency, and all actions should be logged for debugging and tracking purposes. Requirements: 1. **Input and Output:** - The input will consist of N text files, each containing placeholders in the form of `{placeholder_name}`. - You will be provided with a dictionary containing the placeholder values. - The output should be N new processed text files with the same names suffixed by `_processed`. 2. **Concurrency:** - Use the `threading` module to process the text files concurrently. 3. **Templating:** - Use the `string.Template` class to substitute placeholders with the provided values. 4. **Logging:** - Use the `logging` module to log the following events: - Start and end of processing each file. - Any errors encountered during processing. 5. **Performance Requirements:** - Ensure your code processes the files concurrently to maximize efficiency. Constraints: - Assume there are at most 1000 text files to process, and each file size is up to 1 MB. - Placeholders follow the convention `{name}`, with no nested placeholders. - The provided dictionary contains values for all placeholders used in the text files. Function Signature: ```python import logging from string import Template import threading from pathlib import Path def process_files_concurrently(file_paths: list, placeholders: dict): # Implement this function pass ``` Example: Suppose you have the following text files with the given contents and a dictionary of placeholder values: `file1.txt`: ``` Hello {name}, welcome to {place}. ``` `file2.txt`: ``` The current date is {date}. ``` Placeholder Dictionary: ```python placeholders = { \'name\': \'Alice\', \'place\': \'Wonderland\', \'date\': \'2023-10-21\' } ``` After processing, the contents of the output files would be: `file1_processed.txt`: ``` Hello Alice, welcome to Wonderland. ``` `file2_processed.txt`: ``` The current date is 2023-10-21. ``` Log entries would include: ``` INFO: Starting processing of file1.txt INFO: Finished processing of file1.txt INFO: Starting processing of file2.txt INFO: Finished processing of file2.txt ``` Hints: - Use threading.Thread for concurrency. - Use logging.basicConfig for basic logging setup. - Utilize Template.safe_substitute for templating to avoid KeyError.","solution":"import logging from string import Template import threading from pathlib import Path # Configure logging logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') def process_file(file_path: Path, placeholders: dict): try: logging.info(f\'Starting processing of {file_path}\') # Read the file content with file_path.open(\'r\') as file: content = file.read() # Substitute placeholders template = Template(content) new_content = template.safe_substitute(placeholders) # Write the new content to a new file new_file_path = file_path.with_name(f\'{file_path.stem}_processed{file_path.suffix}\') with new_file_path.open(\'w\') as new_file: new_file.write(new_content) logging.info(f\'Finished processing of {file_path}\') except Exception as e: logging.error(f\'Error processing file {file_path}: {e}\') def process_files_concurrently(file_paths: list, placeholders: dict): threads = [] for file_path in file_paths: thread = threading.Thread(target=process_file, args=(Path(file_path), placeholders)) threads.append(thread) thread.start() # Wait for all threads to complete for thread in threads: thread.join()"},{"question":"# Question: Implementing and Utilizing Static Markers in CPython for Tracing Objective: The goal of this question is to assess your understanding of embedding static markers in CPython and utilizing them with DTrace or SystemTap to monitor and report specific activities. You will implement a Python function with embedded static markers and write a SystemTap script to trace the activity. Task: 1. Implement a Python function called `calculate_factorial(n)` that calculates the factorial of a given number `n`. Embed static markers to denote the entry and exit of the function. 2. Write a SystemTap script to trace the `calculate_factorial` function and generate a report showing: - Entry and exit times for the `calculate_factorial` function. - The name of the function, filename, and line number of the entry/exit points. Requirements: 1. The `calculate_factorial(n)` function should: - Use recursion to calculate the factorial of `n`. - Include static markers `function__entry` and `function__return` at the appropriate points inside the function. 2. The SystemTap script should: - Use the embedded markers to trace the function\'s activity. - Output the entry and exit times, function name, filename, and line number. Implementation Details: 1. **Python Function Implementation** ```python def calculate_factorial(n): # Embed function entry static marker # systemtap: function__entry(\\"filename\\", \\"calculate_factorial\\", lineno) if n == 1: return 1 else: result = n * calculate_factorial(n - 1) # Embed function return static marker # systemtap: function__return(\\"filename\\", \\"calculate_factorial\\", lineno) return result # Example usage: result = calculate_factorial(5) print(f\\"Factorial: {result}\\") ``` 2. **SystemTap Script Implementation** ```systemtap global fn_calls; probe process(\\"python\\").mark(\\"function__entry\\") { filename = user_string(arg1); funcname = user_string(arg2); lineno = arg3; printf(\\"%s => %s in %s:%dn\\", ctime(gettimeofday_s()), funcname, filename, lineno); fn_calls[pid(), filename, funcname, lineno, \\"entry\\"] = gettimeofday_ns(); } probe process(\\"python\\").mark(\\"function__return\\") { filename = user_string(arg1); funcname = user_string(arg2); lineno = arg3; printf(\\"%s <= %s in %s:%dn\\", ctime(gettimeofday_s()), funcname, filename, lineno); fn_calls[pid(), filename, funcname, lineno, \\"return\\"] = gettimeofday_ns(); } probe end { foreach ([pid, filename, funcname, lineno, event] in fn_calls) { if (event == \\"entry\\") { printf(\\"Function %s in file %s at line %d entered at %d nsn\\", funcname, filename, lineno, fn_calls[pid, filename, funcname, lineno, event]); } else if (event == \\"return\\") { printf(\\"Function %s in file %s at line %d exited at %d nsn\\", funcname, filename, lineno, fn_calls[pid, filename, funcname, lineno, event]); } } } ``` Constraints: - You need to configure your CPython with DTrace/SystemTap support to embed and use the static markers correctly. - Ensure that your SystemTap development tools are correctly installed and configured in your system. - Validate your implementation with a sample script and SystemTap output. Evaluation Criteria: - Correct implementation of the `calculate_factorial` function with static markers. - Functional SystemTap script that accurately traces the function\'s entry and returns. - Clear and correct output of the function\'s activity details as specified. Good luck!","solution":"def calculate_factorial(n): Calculate the factorial of a given number n. Embedded static markers for entry and exit using dtrace probes. import os import ctypes if os.name == \'posix\' and hasattr(ctypes, \'dlsym\'): libc = ctypes.CDLL(None) dtrace_entry = ctypes.cast(libc.__dtrace___FUNC_ENTRY, ctypes.c_void_p) dtrace_return = ctypes.cast(libc.__dtrace___FUNC_RETURN, ctypes.c_void_p) if dtrace_entry: dtrace_entry(b\\"calculate_factorial.entry\\", b\\"calculate_factorial\\", 0, 0, ctypes.c_void_p()) if n == 1: result = 1 else: result = n * calculate_factorial(n - 1) if os.name == \'posix\' and hasattr(ctypes, \'dlsym\'): if dtrace_return: dtrace_return(b\\"calculate_factorial.exit\\", b\\"calculate_factorial\\", 0, 0, ctypes.c_void_p()) return result # Example usage: result = calculate_factorial(5) print(f\\"Factorial: {result}\\")"},{"question":"# Question: Implement a Function to Inspect `__future__` Feature Information You are required to implement a function `future_info(feature_name)` that retrieves and returns specific details about a given feature from the `__future__` module in Python. The function should accept the name of the feature as a string and return a dictionary with the following information: - The first version in which the feature was optional. - The version in which the feature became or is expected to become mandatory. - The compiler flag associated with the feature. If the feature name provided is not part of the `__future__` module, the function should raise a `ValueError` with an appropriate error message. Function Signature: ```python def future_info(feature_name: str) -> dict: pass ``` Input: - `feature_name` (str): The name of the feature to look up in the `__future__` module. Output: - A dictionary with keys: - `\\"optional\\"`: A tuple representing the first release in which the feature was optional. - `\\"mandatory\\"`: A tuple representing the version in which the feature became or will become mandatory. - `\\"compiler_flag\\"`: An integer representing the compiler flag for the feature. Constraints: - You may assume that the `feature_name` provided will be a string. Example: ```python feature_name = \\"division\\" info = future_info(feature_name) print(info) ``` Expected output (these are illustrative values): ```python { \\"optional\\": (2, 2, 0, \\"alpha\\", 2), \\"mandatory\\": (3, 0, 0, \\"final\\", 0), \\"compiler_flag\\": 8192 } ``` Notes: - Utilize the `__future__` module to obtain the required details. - Handle cases where the feature does not exist by raising an appropriate exception. Performance Requirements: - Ensure that the function performs well for lookups and handles invalid feature names efficiently.","solution":"import __future__ def future_info(feature_name: str) -> dict: Retrieves details about a given feature from the __future__ module. Parameters: feature_name (str): The name of the feature to look up in the __future__ module. Returns: dict: A dictionary with the following keys: - \\"optional\\": A tuple representing the first release in which the feature was optional. - \\"mandatory\\": A tuple representing the version in which the feature became or will become mandatory. - \\"compiler_flag\\": An integer representing the compiler flag for the feature. Raises: ValueError: If the feature name does not exist in the __future__ module. if not hasattr(__future__, feature_name): raise ValueError(f\\"Feature \'{feature_name}\' not found in __future__ module.\\") feature = getattr(__future__, feature_name) return { \\"optional\\": feature.optional, \\"mandatory\\": feature.mandatory, \\"compiler_flag\\": feature.compiler_flag }"},{"question":"# POP3 Email Client with Unread Email Processing You are required to implement a function in Python that connects to a POP3 email server, authenticates using a provided username and password, retrieves all unread emails, and processes them by extracting and printing the subject line of each unread email. # Function Signature ```python def fetch_unread_emails(host: str, username: str, password: str, use_ssl: bool = False) -> None: pass ``` # Input Parameters - `host` (str): The hostname of the POP3 server. - `username` (str): The username for authentication. - `password` (str): The password for authentication. - `use_ssl` (bool): A flag indicating whether to use SSL for the connection. Default is `False`. # Output - The function should print the subject lines of all unread emails. # Constraints 1. Only fetch emails that are marked as unread. For simplicity, assume that any email retrieved using the `retr` method is unread if it hasn\'t been marked for deletion. 2. Handle exceptions gracefully by printing appropriate error messages. 3. Ensure the connection is properly terminated whether the operation is successful or not. 4. Use the `poplib` module to interact with the server. # Example Here is a usage example of how your function might be called: ```python fetch_unread_emails(\'pop.example.com\', \'username\', \'password\', use_ssl=True) ``` # Notes 1. **Subject Extraction:** The subject of an email can be found in the header information typically prefixed with `Subject:`. You need to parse this information from the email headers. 2. Consider case-insensitivity when searching for the `Subject:` header. 3. The function must use the `poplib.POP3` class for non-SSL connections and `poplib.POP3_SSL` for SSL connections. # Hints - Look into the `retr` method for fetching emails and the `quit` method for signing off. - Use the `getwelcome` method to understand the server\'s initial response. - Exception handling is crucial â€“ particularly catching `poplib.error_proto`. Implement this function to demonstrate your understanding of network connections, exception handling, and data parsing in Python.","solution":"import poplib from email.parser import Parser from email.policy import default import ssl def fetch_unread_emails(host: str, username: str, password: str, use_ssl: bool = False) -> None: try: if use_ssl: pop_conn = poplib.POP3_SSL(host) else: pop_conn = poplib.POP3(host) pop_conn.user(username) pop_conn.pass_(password) # Get the number of messages in the mailbox num_msgs = len(pop_conn.list()[1]) for i in range(num_msgs): response, lines, octets = pop_conn.retr(i + 1) msg_content = b\'n\'.join(lines).decode(\'utf-8\', errors=\'ignore\') msg = Parser(policy=default).parsestr(msg_content) # Extract subject subject = msg[\'subject\'] if subject: print(subject) except poplib.error_proto as e: print(f\\"POP3 Protocol Error: {e}\\") except ssl.SSLError as e: print(f\\"SSL Error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: try: pop_conn.quit() except: pass"},{"question":"Objective: Demonstrate your understanding of the \\"atexit\\" module in Python by creating a Python class that employs this module to manage resource cleanup and data persistence. Task: Write a Python class called `CounterManager` that performs the following: 1. **Initialization (`__init__` Method)**: - Upon instantiation, `CounterManager` should attempt to read an integer from a file named `counterfile.txt`. If the file does not exist, the counter should be initialized to zero. - The `__init__` method should register a cleanup function using the \\"atexit\\" module to save the current state of the counter to `counterfile.txt` when the program terminates. 2. **Increment Counter (`increment` Method)**: - This method should accept one argument, an integer `n`, and add it to the counter. 3. **Get Counter (`get_counter` Method)**: - This method should return the current value of the counter. 4. **Reset Counter (`reset` Method)**: - This method should reset the counter to zero. 5. **Unregister Cleanup (`unregister_cleanup` Method)**: - This method should unregister the previously registered cleanup function from the \\"atexit\\" module. 6. **Cleanup Function**: - This function should save the current value of the counter to `counterfile.txt`. It should be registered in `__init__` and unregistered in `unregister_cleanup`. Input: - The class does not take any direct input. Output: - The task does not directly produce output; however, the class methods return values (like the counter value) and interact with the file system. Example Usage: ```python # Example usage of the CounterManager class # Create an instance of CounterManager counter_manager = CounterManager() # Increment the counter by 5 counter_manager.increment(5) # Get the current value of the counter print(counter_manager.get_counter()) # Output: 5 # Reset the counter counter_manager.reset() # Get the current value of the counter print(counter_manager.get_counter()) # Output: 0 # Increment the counter by 10 counter_manager.increment(10) # Unregister the cleanup function counter_manager.unregister_cleanup() ``` Constraints: - Ensure that the counter file `counterfile.txt` is properly read and written using appropriate file handling techniques in Python. **Notes:** - Your solution should be robust and handle potential exceptions that may arise during file operations gracefully. - Make sure to include appropriate docstrings in your code for better readability. Good luck!","solution":"import atexit import os class CounterManager: def __init__(self): self.counter = 0 self.counter_file = \'counterfile.txt\' self.cleanup_registered = False # Try to read the counter from file try: if os.path.exists(self.counter_file): with open(self.counter_file, \'r\') as file: self.counter = int(file.read().strip()) except Exception: self.counter = 0 # Register the cleanup function to save counter to file self.cleanup_func = self.cleanup atexit.register(self.cleanup_func) self.cleanup_registered = True def increment(self, n): Increment the counter by n. self.counter += n def get_counter(self): Get the current value of the counter. return self.counter def reset(self): Reset the counter to zero. self.counter = 0 def cleanup(self): Save the current value of the counter to counterfile.txt. try: with open(self.counter_file, \'w\') as file: file.write(str(self.counter)) except Exception as e: print(\\"Error writing counter to file:\\", e) def unregister_cleanup(self): Unregister the cleanup function from the atexit module. if self.cleanup_registered: atexit.unregister(self.cleanup_func) self.cleanup_registered = False"},{"question":"**Problem Description:** You are tasked with analyzing the \'planets\' dataset from seaborn to reveal insights about the distribution of exoplanets discovered by different detection methods over the years. Your objective is to create a comprehensive visualization that includes both univariate and bivariate histograms. Specifically: 1. **Load the \'planets\' dataset using seaborn.** 2. **Create a univariate histogram to visualize the distribution of planets\' distance. Customize the histogram using log-scaled bins and add a KDE.** 3. **Create a bivariate histogram to visualize the relationship between the year of discovery and the distance of the planet. Use log scaling for the \'distance\' axis, and overlay a KDE plot for clarity.** 4. **Generate a separate bivariate histogram with hue variation based on the \'method\' of discovery. Use cumulative histograms with independent density normalization.** **Requirements:** - **Input/Output:** No specific function input and output are required. Ensure your code plot the graphs correctly using seaborn. - **Constraints:** Use seaborn and matplotlib libraries only. - **Performance:** Ensure the histograms are rendered efficiently and correctly. ``` import seaborn as sns import matplotlib.pyplot as plt # 1. Load the \'planets\' dataset planets = sns.load_dataset(\\"planets\\") # 2. Create a univariate histogram plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"distance\\", log_scale=True, kde=True) plt.title(\\"Univariate Histogram with KDE: Distance of Planets\\") plt.show() # 3. Create a bivariate histogram plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), kde=True) plt.title(\\"Bivariate Histogram with KDE: Year vs Distance of Planets\\") plt.show() # 4. Create a cumulative histogram with hue variation based on the \'method\' plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", hue=\\"method\\", element=\\"step\\", cumulative=True, stat=\\"density\\", common_norm=False, log_scale=(False, True)) plt.title(\\"Cumulative Bivariate Histogram: Year vs Distance by Method\\") plt.show() ``` Ensure that your plots are well-labeled with titles, and axes labels, and include legends where appropriate.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_planet_data(): # 1. Load the \'planets\' dataset planets = sns.load_dataset(\\"planets\\") # 2. Create a univariate histogram plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"distance\\", log_scale=True, kde=True) plt.title(\\"Univariate Histogram with KDE: Distance of Planets\\") plt.xlabel(\\"Distance (log scale)\\") plt.ylabel(\\"Number of Planets\\") plt.show() # 3. Create a bivariate histogram plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", log_scale=(False, True), kde=True) plt.title(\\"Bivariate Histogram with KDE: Year vs Distance of Planets\\") plt.xlabel(\\"Year of Discovery\\") plt.ylabel(\\"Distance (log scale)\\") plt.show() # 4. Create a cumulative histogram with hue variation based on the \'method\' plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"year\\", y=\\"distance\\", hue=\\"method\\", element=\\"step\\", cumulative=True, stat=\\"density\\", common_norm=False, log_scale=(False, True)) plt.title(\\"Cumulative Bivariate Histogram: Year vs Distance by Method\\") plt.xlabel(\\"Year of Discovery\\") plt.ylabel(\\"Distance (log scale)\\") plt.legend(title=\'Detection Method\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Running the function to visualize the data visualize_planet_data()"},{"question":"**Custom Interactive Python Interpreter using the `code` Module** **Objective:** You are required to create a custom interactive Python interpreter that emulates the behavior of the standard Python REPL using the `code` module. Your implementation should provide basic functionalities like executing valid Python commands, handling incomplete inputs, and showing appropriate error messages for syntactical issues. **Task:** 1. Implement a class `CustomREPL` that extends the `code.InteractiveConsole` class in the `code` module. 2. Add the following functionalities to this custom REPL: - A command history feature that stores the last 10 commands input by the user. - An exit message \\"Goodbye!\\" when the user exits the REPL. - Displaying the current line number at each prompt. 3. Overload the `write()` method to handle error messages by prefixing them with \\"Error: \\". 4. Implement a `main()` function that: - Creates an instance of `CustomREPL`. - Runs the interactive console with a custom welcome banner \\"Welcome to Custom REPL!\\". **Constraints:** - You must use the `code` module and extend the `code.InteractiveConsole` class. - The REPL should handle normal Python commands, detect incomplete commands, and handle syntax errors appropriately. - You are not allowed to use any third-party libraries. **Input/Output Requirements:** - The REPL should read lines of Python commands from standard input. - Display the result of evaluated Python expressions. - Display \\"Goodbye!\\" on exit. - Prefix error messages with \\"Error: \\". - Show the command history when the user types `history`. **Example:** ``` Welcome to Custom REPL! 1 >>> a = 10 2 >>> print(a) 10 3 >>> def foo(): 4 ... print(\\"foo\\") 5 >>> foo() foo 6 >>> invalid python Error: invalid python 7 >>> history 1: a = 10 2: print(a) 3: def foo(): 4: print(\\"foo\\") 5: foo() ``` **Notes:** - Ensure encapsulation and proper class design. - Use built-in exceptions for handling errors. ```python import code class CustomREPL(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.history = [] self.line_number = 1 def push(self, line): prompt = f\\"{self.line_number} >>> \\" if not self.iswaiting else f\\"{self.line_number} ... \\" source = input(prompt) self.line_number += 1 if source.strip() == \\"history\\": self.show_history() else: self.history.append(source.strip()) self.history = self.history[-10:] super().push(source) def write(self, data): print(f\\"Error: {data}\\") def show_history(self): for idx, command in enumerate(self.history): print(f\\"{idx + 1}: {command}\\") def main(): banner = \\"Welcome to Custom REPL!\\" exitmsg = \\"Goodbye!\\" repl = CustomREPL() repl.interact(banner=banner, exitmsg=exitmsg) if __name__ == \\"__main__\\": main() ```","solution":"import code class CustomREPL(code.InteractiveConsole): def __init__(self, locals=None, filename=\\"<console>\\"): super().__init__(locals, filename) self.history = [] self.line_number = 1 self.iswaiting = False def push(self, line): if line.strip() == \\"history\\": self.show_history() return False self.history.append(line.strip()) self.history = self.history[-10:] self.line_number += 1 self.iswaiting = super().push(line) == False return self.iswaiting def raw_input(self, prompt=None): if not prompt: prompt = f\\"{self.line_number} >>> \\" if not self.iswaiting else f\\"{self.line_number} ... \\" return input(prompt) def write(self, data): print(f\\"Error: {data}\\") def show_history(self): for idx, command in enumerate(self.history): print(f\\"{idx + 1}: {command}\\") def main(): banner = \\"Welcome to Custom REPL!\\" exitmsg = \\"Goodbye!\\" repl = CustomREPL() repl.interact(banner=banner, exitmsg=exitmsg) if __name__ == \\"__main__\\": main()"},{"question":"# Email Parsing and Analysis You are provided with a series of raw email messages in a list format. Your task is to implement a function `parse_email_messages` that reads these emails, identifies whether they are multipart or not, and extracts basic header information (like subject, sender, and receiver). If the message is multipart, provide a summary of its MIME parts. You should use the `email.parser` module available in Python\'s standard library to achieve this. Function Signature ```python def parse_email_messages(emails: list[str]) -> list[dict]: pass ``` Input - `emails`: A list of strings, where each string represents a raw email message. Output - A list of dictionaries, where each dictionary contains: - `subject`: Subject of the email as a string. - `from`: Sender of the email as a string. - `to`: Receiver of the email as a string. - `is_multipart`: Boolean indicating whether the email is a multipart message or not. - `parts`: If `is_multipart` is `True`, a list of mime types of the parts; otherwise, an empty list. Example ```python emails = [ \\"From: john@example.comnTo: jane@example.comnSubject: HellonnThis is a test email.\\", \\"From: mark@example.comnTo: luke@example.comnSubject: Multipart ExamplenContent-Type: multipart/mixed; boundary=\\"XYZ\\"nn--XYZnContent-Type: text/plainnnThis is the body part.nn--XYZnContent-Type: text/htmlnn<html><body>This is the HTML part.</body></html>nn--XYZ--\\" ] output = parse_email_messages(emails) ``` The `output` should be: ```python [ { \'subject\': \'Hello\', \'from\': \'john@example.com\', \'to\': \'jane@example.com\', \'is_multipart\': False, \'parts\': [] }, { \'subject\': \'Multipart Example\', \'from\': \'mark@example.com\', \'to\': \'luke@example.com\', \'is_multipart\': True, \'parts\': [\'text/plain\', \'text/html\'] } ] ``` Constraints - Assume all emails are well-formed and compliant with relevant email standards. Performance Requirements - The function should operate efficiently even if the number of emails is large (e.g., in the order of thousands). Use the standard `email.parser` module\'s capabilities effectively to solve this problem. Make sure to handle both single-part and multi-part emails correctly and extract the necessary header information.","solution":"import email from email.parser import Parser def parse_email_messages(emails: list[str]) -> list[dict]: parsed_emails = [] for raw_email in emails: message = Parser().parsestr(raw_email) email_dict = { \'subject\': message[\'subject\'], \'from\': message[\'from\'], \'to\': message[\'to\'], \'is_multipart\': message.is_multipart(), \'parts\': [] } if message.is_multipart(): for part in message.get_payload(): email_dict[\'parts\'].append(part.get_content_type()) parsed_emails.append(email_dict) return parsed_emails"},{"question":"# Web Spider Permission Checker In this task, you are required to implement a utility that helps a web spider determine if it can fetch URLs from a given website as per the `robots.txt` rules. You will use the `urllib.robotparser` module to parse the `robots.txt` file and implement functions to answer specific queries. Task Details 1. Write a function `can_fetch_url(url, useragent)`: - **Input**: - `url` (string): The URL of the website to be processed. - `useragent` (string): The user agent string for the spider. - **Output**: - A boolean value `True` if the useragent is allowed to fetch the URL as per `robots.txt`. Otherwise, `False`. 2. Write a function `crawl_delay(url, useragent)`: - **Input**: - `url` (string): The URL of the website to be processed. - `useragent` (string): The user agent string for the spider. - **Output**: - An integer representing the crawl delay in seconds if specified. If there is no crawl delay or if the syntax is invalid, return `None`. 3. Write a function `site_maps(url)`: - **Input**: - `url` (string): The URL of the website to be processed. - **Output**: - A list of sitemap URLs specified in `robots.txt`. If there are no sitemaps specified or the syntax is invalid, return `None`. # Constraints & Notes - Consider only the `robots.txt` files that are accessible without authentication. - Assume URLs passed are valid and well-formed. # Example Usage Here is an example of how the functions should behave: ```python # Example usage url = \\"http://www.example.com\\" useragent = \\"my_spider\\" # Function calls print(can_fetch_url(url, useragent)) # Example output: True or False print(crawl_delay(url, useragent)) # Example output: 10 or None print(site_maps(url)) # Example output: [\\"http://www.example.com/sitemap.xml\\"] or None ``` Ensure your implementation correctly utilizes the `urllib.robotparser` module to extract the necessary information from the `robots.txt` file. # Implementation Write your implementation below: ```python import urllib.robotparser def can_fetch_url(url, useragent): rp = urllib.robotparser.RobotFileParser() rp.set_url(url + \'/robots.txt\') rp.read() return rp.can_fetch(useragent, url) def crawl_delay(url, useragent): rp = urllib.robotparser.RobotFileParser() rp.set_url(url + \'/robots.txt\') rp.read() return rp.crawl_delay(useragent) def site_maps(url): rp = urllib.robotparser.RobotFileParser() rp.set_url(url + \'/robots.txt\') rp.read() return rp.site_maps() ``` Make sure to thoroughly test your implementation with different `robots.txt` configurations to ensure accuracy.","solution":"import urllib.robotparser def can_fetch_url(url, useragent): Determines if the useragent is allowed to fetch the URL as per robots.txt Parameters: - url (string): The URL of the website to be processed - useragent (string): The user agent string for the spider Returns: - bool: True if the useragent is allowed to fetch the URL, otherwise False rp = urllib.robotparser.RobotFileParser() rp.set_url(url + \'/robots.txt\') rp.read() return rp.can_fetch(useragent, url) def crawl_delay(url, useragent): Retrieves the crawl delay in seconds from robots.txt for the useragent Parameters: - url (string): The URL of the website to be processed - useragent (string): The user agent string for the spider Returns: - int or None: The crawl delay in seconds if specified, otherwise None rp = urllib.robotparser.RobotFileParser() rp.set_url(url + \'/robots.txt\') rp.read() return rp.crawl_delay(useragent) def site_maps(url): Retrieves a list of sitemap URLs specified in robots.txt Parameters: - url (string): The URL of the website to be processed Returns: - list or None: A list of sitemap URLs if specified, otherwise None rp = urllib.robotparser.RobotFileParser() rp.set_url(url + \'/robots.txt\') rp.read() sitemaps = rp.site_maps() return sitemaps if sitemaps else None"},{"question":"**Objective:** Demonstrate your understanding of the `curses.panel` module by creating and manipulating a stack of panels. **Task:** You are required to create a function `manage_panels(screen)` that will be integrated into a larger `curses` application. This function should: 1. Create three panels with distinct content. 2. Move the panels to specific locations on the screen. 3. Manipulate the stack order: push one panel to the top, another to the bottom. 4. Hide and then show one of the panels. 5. Update the virtual screen after making changes to the panel stack. 6. Include `doupdate()` to ensure the actual screen display is updated. **Function Implementation:** ```python import curses import curses.panel def manage_panels(screen): # Your implementation here pass def main(screen): curses.curs_set(0) # Hide the cursor manage_panels(screen) screen.getch() # Wait for user input if __name__ == \'__main__\': curses.wrapper(main) ``` **Steps and Requirements:** 1. **Create Panels and Windows**: - Create three windows of size 3x10 at different positions. - Create three panels using these windows. - Fill each window with some distinct content (e.g., different letters or numbers). 2. **Move Panels**: - Move the panels to specific coordinates on the screen. 3. **Manipulate Stack**: - Push one panel to the top of the stack. - Push another panel to the bottom of the stack. 4. **Visibility**: - Hide one of the panels and then show it again. 5. **Update Screen**: - Use `curses.panel.update_panels()` to update the virtual screen. - Use `curses.doupdate()` to refresh the actual screen display. **Expected Output**: Ensure that the panels are displayed correctly on the screen with the specified stack order and visibility changes. ```text < Expected screen output after running the function > ``` **Hints**: - You may use methods such as `move()`, `top()`, `bottom()`, `hide()`, `show()`, `update_panels()`, and `doupdate()`. - Ensure you handle the screen refresh correctly to see the changes. **Constraints**: - Do not use global variables; pass information through function parameters or use object methods as appropriate. Happy coding! We look forward to seeing your unique solution to managing panels using the `curses.panel` module.","solution":"import curses import curses.panel def manage_panels(screen): # Create three windows win1 = curses.newwin(3, 10, 5, 5) win2 = curses.newwin(3, 10, 8, 8) win3 = curses.newwin(3, 10, 11, 11) # Add content to the windows win1.addstr(0, 0, \\"Panel 1\\") win2.addstr(0, 0, \\"Panel 2\\") win3.addstr(0, 0, \\"Panel 3\\") # Create panels from windows panel1 = curses.panel.new_panel(win1) panel2 = curses.panel.new_panel(win2) panel3 = curses.panel.new_panel(win3) # Manipulate the panels panel1.move(5, 5) panel2.move(6, 5) panel3.move(7, 5) # Modify stack order panel2.top() panel1.bottom() # Hide and then show panel3 panel3.hide() curses.panel.update_panels() curses.doupdate() curses.napms(1000) # Pause for user to see effect panel3.show() # Commit changes and refresh the screen curses.panel.update_panels() curses.doupdate() curses.napms(1000) # Pause for user to see final effect def main(screen): curses.curs_set(0) # Hide the cursor manage_panels(screen) screen.getch() # Wait for user input if __name__ == \'__main__\': curses.wrapper(main)"},{"question":"**Objective:** Your task is to load a dataset, preprocess it, and create a customized box plot using the seaborn library. This will evaluate your understanding of seaborn\'s `boxenplot` function along with various customization parameters. **Question:** 1. Load the `titanic` dataset from seaborn\'s inbuilt datasets. 2. Preprocess the dataset to only include rows where the age is not null. 3. Create a boxen plot of the `age` of the passengers grouped by their `class` and further segmented by their `sex`. 4. Customize the plot by: - Using linear width method for the boxes. - Setting the box width to 0.6. - Control the outlines of the boxes with a linewidth of 0.7 and line color `0.5`. - Set the appearance of the median line with linewidth of 1.2 and color `black`. - Control the appearance of outliers with face color `red` and linewidth 0.2. - Ensure that the boxes are not filled. **Constraints:** - You should use seaborn library\'s functions and methods for visualization (`sns.boxenplot`). - Use the `titanic` dataset and ensure that any missing value handling is done efficiently. - Performance constraints are minor due to the relatively small size of the `titanic` dataset. **Expected Output:** - A properly labeled and customized boxen plot as per the specifications mentioned. ```python # Your implementation starts here import seaborn as sns # Load the \'titanic\' dataset titanic = sns.load_dataset(\\"titanic\\") # Preprocess the dataset to exclude null age values titanic = titanic.dropna(subset=[\\"age\\"]) # Create the customized boxen plot sns.boxenplot( data=titanic, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", width_method=\\"linear\\", width=0.6, linewidth=0.7, linecolor=\\"0.5\\", line_kws=dict(linewidth=1.2, color=\\"black\\"), flier_kws=dict(facecolor=\\"red\\", linewidth=0.2), fill=False ) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_boxenplot(): # Load the \'titanic\' dataset titanic = sns.load_dataset(\\"titanic\\") # Preprocess the dataset to exclude rows where the age is null titanic = titanic.dropna(subset=[\\"age\\"]) # Create the customized boxen plot plt.figure(figsize=(10, 6)) sns.boxenplot( data=titanic, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", width_method=\\"linear\\", width=0.6, linewidth=0.7, linecolor=\\"0.5\\", line_kws={\\"linewidth\\": 1.2, \\"color\\": \\"black\\"}, flier_kws={\\"facecolor\\": \\"red\\", \\"linewidth\\": 0.2}, fill=False ) # Additional plot settings plt.title(\\"Boxen Plot of Age by Class and Sex\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Class\\") plt.legend(title=\\"Sex\\") plt.show() # Call the function to create the plot create_custom_boxenplot()"},{"question":"# Question: Advanced Visualizations with Seaborn You are tasked with analyzing a dataset using the seaborn library. The dataset contains information about model performance on different natural language processing tasks. You need to load the dataset, process it, and create a bunch of visualizations with customized annotations as specified in the steps below. Follow the instructions carefully: 1. **Dataset Loading and Transformation** - Load the dataset `glue` using seaborn\'s `load_dataset` function. - Pivot the dataset to have `Model` and `Encoder` as the index, `Task` as columns, and `Score` as values. - Compute an additional column \'Average\' which represents the mean of all task scores for each model, rounded to one decimal place. - Sort the dataset based on the \'Average\' score in descending order. 2. **Scatter Plot with Annotations** - Create a scatter plot with \'SST-2\' scores on the x-axis and \'MRPC\' scores on the y-axis. - Annotate each point with the corresponding `Model` name. 3. **Bar Plot with Annotations** - Create a bar plot with `Model` names on the y-axis and their `Average` scores on the x-axis. - Add the `Average` score as an annotation inside the bars. - Customize the text color to white and align it to the right. 4. **Custom Text Alignment** - Create a scatter plot with `RTE` scores on the x-axis, `MRPC` scores on the y-axis, and color the dots based on the `Encoder`. - Annotate each point with the corresponding `Model` name. - Customize the vertical alignment to \'bottom\' for the text. - Implement specific horizontal text alignments for different encoders: align text to the left for the \'LSTM\' encoder and to the right for the \'Transformer\' encoder. Provide the implementation for each of the steps mentioned above in a function called `seaborn_visualizations()`. The function should not take any parameters and should not return anything. It should simply execute the required visualization steps when called. # Constraints - You must use the seaborn.objects interface as shown in the documentation provided. - Make sure to import all required libraries at the beginning of your function. - The function should be efficient and avoid unnecessary computations. Expected Input and Output Formats - **Input**: No input is taken as the function operates solely within its scope. - **Output**: Matplotlib plots are generated and displayed as specified in the question. Sample Function Signature ```python def seaborn_visualizations(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Implementation of required visualizations ``` Ensure your visualizations are clear, well-labeled, and appropriately customized according to the instructions.","solution":"def seaborn_visualizations(): import seaborn as sns from seaborn import load_dataset import matplotlib.pyplot as plt import pandas as pd # Load the dataset dataset = sns.load_dataset(\'glue\').dropna() # Pivot the dataset pivot_df = dataset.pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") # Compute \'Average\' column pivot_df[\'Average\'] = pivot_df.mean(axis=1).round(1) # Sort the dataset based on \'Average\' sorted_df = pivot_df.sort_values(by=\'Average\', ascending=False) # Print the sorted DataFrame for verification print(sorted_df) # Scatter plot with annotations: SST-2 vs MRPC plt.figure(figsize=(10, 6)) ax = sns.scatterplot(data=sorted_df, x=\'SST-2\', y=\'MRPC\', hue=\'Encoder\') for idx, row in sorted_df.iterrows(): ax.text(row[\'SST-2\'], row[\'MRPC\'], idx[0], ha=\'right\') plt.title(\\"SST-2 vs MRPC Scores with Model Annotations\\") plt.show() # Bar plot with annotations: Average scores plt.figure(figsize=(8, 10)) ax = sns.barplot(y=sorted_df.index.get_level_values(0), x=sorted_df[\'Average\'], palette=\'viridis\') for idx, row in enumerate(sorted_df[\'Average\']): ax.text(row, idx, f\'{row:.1f}\', color=\'white\', ha=\'right\', va=\'center\') plt.title(\\"Average Score by Model\\") plt.show() # Scatter plot with custom text alignments: RTE vs MRPC plt.figure(figsize=(10, 6)) ax = sns.scatterplot(data=sorted_df, x=\'RTE\', y=\'MRPC\', hue=\'Encoder\') for idx, row in sorted_df.iterrows(): ha = \'left\' if idx[1] == \'LSTM\' else \'right\' ax.text(row[\'RTE\'], row[\'MRPC\'], idx[0], ha=ha, va=\'bottom\') plt.title(\\"RTE vs MRPC Scores with Custom Text Alignment Based on Encoder\\") plt.show()"},{"question":"# Objective You are tasked with creating a preprocessing pipeline that includes dimensionality reduction techniques and evaluating a supervised learning algorithm\'s performance on the reduced data. # Problem Statement Write a Python function `evaluate_dimensionality_reduction` that takes in a dataset and performs the following steps: 1. Standardizes the dataset using `StandardScaler`. 2. Applies Principal Component Analysis (PCA) to reduce the dataset to a specified number of components. 3. Trains a supervised classifier (e.g., Logistic Regression) on the reduced dataset. 4. Evaluates the classifier using cross-validation and returns the mean accuracy score. # Input - `X_train` (numpy.ndarray): Training features with shape `(n_samples, n_features)`. - `y_train` (numpy.ndarray): Training labels with shape `(n_samples,)`. - `n_components` (int): Number of components for PCA. # Output - `mean_accuracy` (float): The mean accuracy score of the classifier evaluated using cross-validation. # Constraints - `X_train` should be a 2-dimensional array with more than 1 feature and more than 10 samples. - `y_train` should be a 1-dimensional array with the same number of samples as `X_train`. # Example ```python import numpy as np # Example data X_train = np.array([[0.87, 1.34, -0.33], [1.24, 0.23, 0.53], [0.45, 0.75, -0.14], ...]) y_train = np.array([0, 1, 0, ...]) # Number of components for PCA n_components = 2 # Evaluate the pipeline mean_accuracy = evaluate_dimensionality_reduction(X_train, y_train, n_components) print(mean_accuracy) ``` # Notes - Use `StandardScaler` from `sklearn.preprocessing` to standardize the dataset. - Use `PCA` from `sklearn.decomposition` to reduce dimensions. - Use `LogisticRegression` from `sklearn.linear_model` as the supervised learning classifier. - Use `cross_val_score` from `sklearn.model_selection` for cross-validation, set `cv=5` for 5-fold cross-validation. ```python def evaluate_dimensionality_reduction(X_train: np.ndarray, y_train: np.ndarray, n_components: int) -> float: from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score # Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X_train) # Apply PCA pca = PCA(n_components=n_components) X_reduced = pca.fit_transform(X_scaled) # Initialize the classifier classifier = LogisticRegression() # Evaluate using cross-validation scores = cross_val_score(classifier, X_reduced, y_train, cv=5) # Calculate and return the mean accuracy mean_accuracy = scores.mean() return mean_accuracy ```","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score def evaluate_dimensionality_reduction(X_train: np.ndarray, y_train: np.ndarray, n_components: int) -> float: Evaluates dimensionality reduction using PCA and logistic regression. Parameters: - X_train: Training features (numpy.ndarray), shape (n_samples, n_features) - y_train: Training labels (numpy.ndarray), shape (n_samples,) - n_components: Number of components for PCA (int) Returns: - mean_accuracy: Mean accuracy score of the classifier (float) # Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X_train) # Apply PCA pca = PCA(n_components=n_components) X_reduced = pca.fit_transform(X_scaled) # Initialize the classifier classifier = LogisticRegression() # Evaluate using cross-validation scores = cross_val_score(classifier, X_reduced, y_train, cv=5) # Calculate and return the mean accuracy mean_accuracy = scores.mean() return mean_accuracy"},{"question":"Advanced Visualization with Pandas **Objective:** Evaluate the student\'s ability to create and customize data visualizations using pandas and Matplotlib integration. **Problem Statement:** You are given a dataset containing weather data, including daily temperatures, precipitation, and humidity levels over a year. Your task is to create multiple visualizations to analyze this dataset. **Dataset:** The dataset is a CSV file named `weather.csv` with the following columns: - `Date`: Date of the record in \\"YYYY-MM-DD\\" format. - `Temperature`: Average daily temperature in Celsius. - `Precipitation`: Amount of daily precipitation in mm. - `Humidity`: Average daily humidity in percentage. # Tasks: 1. **Load the Data:** - Read the dataset from `weather.csv`. - Ensure the `Date` column is parsed as a datetime object and set it as the index of the DataFrame. 2. **Time Series Plot of Temperature:** - Create a line plot of the `Temperature` over time. Ensure the x-axis is labeled as \\"Date\\" and the y-axis as \\"Temperature (C)\\". Add a title \\"Daily Temperature Over Time\\". 3. **Histogram of Precipitation:** - Create a histogram of the `Precipitation` data with 20 bins, setting the x-axis as \\"Precipitation (mm)\\" and the y-axis as \\"Frequency\\". Add a title \\"Precipitation Distribution\\". 4. **Box Plot of Humidity:** - Generate a box plot to visualize the distribution of `Humidity`. Ensure that the x-axis is labeled as \\"Humidity (%)\\". Add a title \\"Humidity Distribution\\". 5. **Scatter Plot of Temperature vs. Humidity:** - Create a scatter plot of `Temperature` versus `Humidity`. Set the x-axis as \\"Temperature (C)\\" and the y-axis as \\"Humidity (%)\\". Add a title \\"Temperature vs. Humidity\\". 6. **Subplots:** - Generate subplots to display the above visualizations (tasks 2 to 5) in a single figure with a 2x2 grid layout. # Constraints: - Assume the dataset is correctly formatted. - Use the pandas and matplotlib packages for your visualizations. - Ensure each plot is appropriately labeled and includes a title. # Input Format: - A CSV file named `weather.csv` containing the dataset. # Output Format: - Plots generated as specified in the tasks. # Example: Assuming `weather.csv` looks like the following: ``` Date,Temperature,Precipitation,Humidity 2022-01-01,5.2,0.0,80 2022-01-02,6.3,1.0,78 ... ``` Your output should be a collection of plots as detailed in the tasks. --- **Note:** Make sure to submit a single script that performs all the tasks and produces the required plots.","solution":"import pandas as pd import matplotlib.pyplot as plt def visualize_weather_data(): # 1. Load the Data df = pd.read_csv(\'weather.csv\', parse_dates=[\'Date\']) df.set_index(\'Date\', inplace=True) # 2. Time Series Plot of Temperature plt.figure(figsize=(10, 6)) df[\'Temperature\'].plot() plt.xlabel(\'Date\') plt.ylabel(\'Temperature (C)\') plt.title(\'Daily Temperature Over Time\') plt.savefig(\'temperature_time_series.png\') plt.close() # 3. Histogram of Precipitation plt.figure(figsize=(10, 6)) df[\'Precipitation\'].plot.hist(bins=20) plt.xlabel(\'Precipitation (mm)\') plt.ylabel(\'Frequency\') plt.title(\'Precipitation Distribution\') plt.savefig(\'precipitation_histogram.png\') plt.close() # 4. Box Plot of Humidity plt.figure(figsize=(10, 6)) df[\'Humidity\'].plot.box() plt.xlabel(\'Humidity (%)\') plt.title(\'Humidity Distribution\') plt.savefig(\'humidity_boxplot.png\') plt.close() # 5. Scatter Plot of Temperature vs. Humidity plt.figure(figsize=(10, 6)) plt.scatter(df[\'Temperature\'], df[\'Humidity\']) plt.xlabel(\'Temperature (C)\') plt.ylabel(\'Humidity (%)\') plt.title(\'Temperature vs. Humidity\') plt.savefig(\'temperature_vs_humidity.png\') plt.close() # 6. Subplots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) df[\'Temperature\'].plot(ax=axes[0, 0]) axes[0, 0].set_title(\'Daily Temperature Over Time\') axes[0, 0].set_xlabel(\'Date\') axes[0, 0].set_ylabel(\'Temperature (C)\') df[\'Precipitation\'].plot.hist(bins=20, ax=axes[0, 1]) axes[0, 1].set_title(\'Precipitation Distribution\') axes[0, 1].set_xlabel(\'Precipitation (mm)\') axes[0, 1].set_ylabel(\'Frequency\') df[\'Humidity\'].plot.box(ax=axes[1, 0]) axes[1, 0].set_title(\'Humidity Distribution\') axes[1, 0].set_xlabel(\'Humidity (%)\') axes[1, 1].scatter(df[\'Temperature\'], df[\'Humidity\']) axes[1, 1].set_title(\'Temperature vs. Humidity\') axes[1, 1].set_xlabel(\'Temperature (C)\') axes[1, 1].set_ylabel(\'Humidity (%)\') plt.tight_layout() plt.savefig(\'weather_subplots.png\') plt.close()"},{"question":"# Custom Python Interpreter With Enhanced Features Implement an enhanced Python interactive console using the `code` module in Python. Your task is to create a custom interactive interpreter that supports the following additional features: 1. **Command History**: Maintain a history of commands entered during the session. The user should be able to retrieve past commands using a special command `!history`. 2. **Custom Commands**: - Implement a command `!vars` to display all the current variables and their values defined in the interactive session so far. - Implement a command `!exit` to gracefully exit the interactive session. 3. **Enhanced Error Messages**: Customize the error messages to be more descriptive, indicating the type of error and a possible solution, if applicable. # Specifications: 1. **Class Definitions**: - Create a `CustomInterpreter` class derived from `code.InteractiveConsole`. - Define methods for handling command history and custom commands within this class. 2. **Expected Methods**: - `push(line)`: This method will process a single line of code, handle custom commands, and store the command history. - `show_history()`: This method will print out the history of commands. - `show_variables()`: This method will print out all variables and their values in the current namespace. - `error_handler(exctype, value, tb)`: Override the default error handler to provide enhanced error messages. 3. **Execution**: - Instantiate the `CustomInterpreter` class and use it to start an interactive session. - Test the interactive session to ensure all custom features work correctly. # Example Usage: ```python interpreter = CustomInterpreter() interpreter.interact() # During the session >>> x = 42 >>> !vars \'x\': 42 >>> !history 1: x = 42 2: !vars >>> raise ValueError(\\"Sample error\\") Error: ValueError occurred. Possible Solution: Check the values being processed. >>> !exit ``` **Constraints**: - Your solution should handle edge cases, such as attempting to access `!history` or `!vars` before any commands are run. - Ensure minimal performance overhead when managing command history and handling custom commands. **Note**: You are not required to implement the full interactive session but ensure your class definitions and methods are complete and can be instantiated as shown.","solution":"import code import traceback class CustomInterpreter(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.command_history = [] def push(self, line): self.command_history.append(line) if line.strip() == \'!history\': self.show_history() elif line.strip() == \'!vars\': self.show_variables() elif line.strip() == \'!exit\': return True # signal to exit else: try: super().push(line) except Exception as e: self.error_handler(e.__class__, e, e.__traceback__) return False return False def show_history(self): for idx, command in enumerate(self.command_history, 1): print(f\\"{idx}: {command}\\") def show_variables(self): variables = {k: v for k, v in self.locals.items() if not k.startswith(\'__\')} for name, val in variables.items(): print(f\\"{name}: {val}\\") def error_handler(self, exctype, value, tb): print(\\"Error: {} occurred. Possible Solution: {}\\".format(exctype.__name__, value)) # Instantiate the interpreter; in real usage, call interpreter.interact() # interpreter = CustomInterpreter() # interpreter.interact()"},{"question":"Objective To evaluate your understanding of seaborn\'s style parameters and your ability to create plots using these styles, you will need to create a composite plot with multiple styles. Task 1. **Define a Function `composite_plot`**: - This function should take two arguments: `data` (a list of tuples where each tuple contains x and y data) and `styles` (a list of seaborn style names). - Generate one subplot for each tuple in `data`, using the corresponding style in `styles`. - If the number of styles is less than the number of data elements, cycle through the styles. 2. **Input**: - `data`: A list of tuples where each tuple contains two lists `[x, y]`. E.g., `data = [([1, 2, 3], [2, 5, 3]), ([4, 5, 6], [1, 2, 7])]` - `styles`: A list of seaborn style names as strings. E.g., `styles = [\\"darkgrid\\", \\"whitegrid\\"]` 3. **Output**: - The function should create a composite plot with one subplot for each `(x, y)` pair in `data`, applying the corresponding style from `styles`. Constraints - You can assume that all style names provided in `styles` are valid seaborn style names. - Ensure that all subplots are clearly labeled with the applied style name. Example ```python import seaborn as sns import matplotlib.pyplot as plt def composite_plot(data, styles): num_plots = len(data) num_styles = len(styles) fig, axs = plt.subplots(1, num_plots, figsize=(15, 5)) for i, (x, y) in enumerate(data): style = styles[i % num_styles] with sns.axes_style(style): sns.barplot(x=x, y=y, ax=axs[i]) axs[i].set_title(f\'Style: {style}\') plt.show() # Example use data = [([1, 2, 3], [2, 5, 3]), ([4, 5, 6], [1, 2, 7])] styles = [\\"darkgrid\\", \\"whitegrid\\"] composite_plot(data, styles) ``` In this task, you need to demonstrate the ability to handle seaborn\'s style settings, iterate through data and style lists, and manage plotting with subplots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def composite_plot(data, styles): Creates a composite plot with subplots, each using a specific seaborn style. Args: data (list of tuples): List of (x, y) data pairs for plotting. styles (list of str): List of seaborn style names. num_plots = len(data) num_styles = len(styles) fig, axs = plt.subplots(1, num_plots, figsize=(15, 5)) if num_plots == 1: axs = [axs] # Ensure axs is iterable if there\'s only one subplot for i, (x, y) in enumerate(data): style = styles[i % num_styles] with sns.axes_style(style): sns.barplot(x=x, y=y, ax=axs[i]) axs[i].set_title(f\'Style: {style}\') plt.tight_layout() plt.show()"},{"question":"Objective The goal of this assignment is to test your understanding of PyTorch\'s configurable logging system and to demonstrate your ability to manipulate logging settings via both environment variables and the Python API provided by PyTorch. Problem Statement Implement a Python function `configure_logging` that will accept configuration details about log levels and artifacts through a dictionary and apply those settings using the PyTorch Python API. # Function Signature ```python def configure_logging(config: dict) -> None: Configure logging levels in PyTorch based on the provided configuration dictionary. Args: config (dict): A dictionary where keys are component names or artifact names and values are the log level settings. The log level can be specified as follows: - \\"default\\": the default logging level - \\"debug\\": for maximum verbosity - \\"info\\": for informational messages - \\"warn\\": for warning messages - \\"error\\": for error messages - \\"critical\\": for critical error messages - \\"enable\\": to display the artifact - \\"disable\\": to hide the artifact Returns: None ``` Input Format - The input `config` is a dictionary where keys are the names of logging components (e.g., `dynamo`, `aot`, `inductor`, `your.custom.module`) or artifact names (e.g., `bytecode`, `aot_graphs`), and values are the log levels or enable/disable statuses. Example Inputs and Outputs ```python # Example 1 config1 = { \\"dynamo\\": \\"debug\\", \\"aot\\": \\"warn\\", \\"graph\\": \\"enable\\", \\"schedule\\": \\"disable\\" } configure_logging(config1) # Example 2 config2 = { \\"all\\": \\"error\\", \\"your.custom.module\\": \\"info\\" } configure_logging(config2) ``` Constraints and Limitations 1. The function should handle unknown components gracefully by ignoring them. 2. The artifacts should only be enabled or disabled, not affecting their verbosity. 3. The default configuration should be `logging.WARN` for unspecified components. 4. Handle invalid configuration values gracefully with a warning message; invalid configurations should not break the function. Performance Requirements - The function should be efficient with a time complexity linear to the number of components and artifacts provided in the configuration dictionary. # Note You might find the `torch._logging.set_logs` function helpful for implementing this function.","solution":"import torch._logging as torch_logging import logging LOG_LEVELS = { \\"default\\": logging.WARNING, \\"debug\\": logging.DEBUG, \\"info\\": logging.INFO, \\"warn\\": logging.WARNING, \\"error\\": logging.ERROR, \\"critical\\": logging.CRITICAL } def configure_logging(config: dict) -> None: Configure logging levels in PyTorch based on the provided configuration dictionary. Args: config (dict): A dictionary where keys are component names or artifact names and values are the log level settings. Returns: None for component, level in config.items(): if level in LOG_LEVELS: torch_logging.set_logs(component, LOG_LEVELS[level]) elif level == \\"enable\\": torch_logging.set_logs(component, True) elif level == \\"disable\\": torch_logging.set_logs(component, False) else: print(f\\"Warning: Invalid log level \'{level}\' for component \'{component}\'.\\")"},{"question":"**Objective:** Validate the indentation and logical structure of a Python code string. **Description:** You are given a string containing Python code. Your task is to write a function `validate_python_code` that checks if the provided code has valid indentation and logical structure. Specifically, your function should: - Detect and report if there are any indentation errors. - Ensure all types of brackets (parentheses `()`, square brackets `[]`, and curly braces `{}`) are properly matched. - Ensure comments (lines starting with `#`) are correctly identified and do not interfere with the logical structure of the code. - Detect and report if there are any syntax errors related to improper line joining or blank lines in critical places. **Function Signature:** ```python def validate_python_code(code: str) -> bool: pass ``` **Input:** - `code`: A string containing Python code. The code may span multiple lines. **Output:** - Return `True` if the code is valid, otherwise return `False`. **Constraints:** - The code string can contain any valid ASCII characters. - The length of the code string will not exceed 10,000 characters. **Hints:** - Use a stack to keep track of indentation levels and unmatched brackets. - Pay special attention to lines starting with `#` (comments) and lines that end with `` (explicit line joining). - Ignore blank lines or lines that only contain spaces or comments when checking indentation. **Example:** ```python code_snippet = \'\'\' def perm(l): # Compute the list of all permutations of l if len(l) <= 1: return [l] r = [] for i in range(len(l)): s = l[:i] + l[i+1:] p = perm(s) for x in p: r.append(l[i:i+1] + x) return r \'\'\' print(validate_python_code(code_snippet)) # Output: True code_snippet_with_errors = \'\'\' def perm(l): # Compute the list of all permutations of l if len(l) <= 1: return [l] r = [] for i in range(len(l)): s = l[:i] + l[i + 1:] p = perm(s) for x in p: r.append(l[i:i + 1] + x) return r \'\'\' print(validate_python_code(code_snippet_with_errors)) # Output: False ``` **Note:** In the second example, there is an indentation error where the line `r.append(l[i:i+1] + x)` has incorrect indentation.","solution":"def validate_python_code(code: str) -> bool: Validates the indentation and logical structure of a Python code string. :param code: A string containing the Python code. :return: True if the code is valid, otherwise False. lines = code.split(\'n\') indentation_stack = [] bracket_stack = [] lines_to_ignore = {\'\', \' \', \'#\'} # Lines to ignore (either blank or only spaces or comments) for line in lines: stripped_line = line.strip() if stripped_line == \'\': # Ignore blank lines continue if stripped_line.startswith(\'#\'): # Ignore comment lines continue # Check indentation leading_spaces = len(line) - len(line.lstrip()) if stripped_line and leading_spaces % 4 != 0: return False # Maintain the stack for indentations if len(indentation_stack) == 0 or leading_spaces > indentation_stack[-1]: indentation_stack.append(leading_spaces) else: while indentation_stack and indentation_stack[-1] > leading_spaces: indentation_stack.pop() # Check for balanced brackets for char in stripped_line: if char in \\"([{\\": bracket_stack.append(char) elif char in \\")]}\\": if not bracket_stack: return False top = bracket_stack.pop() if char == \')\' and top != \'(\': return False if char == \']\' and top != \'[\': return False if char == \'}\' and top != \'{\': return False if bracket_stack: return False return True"},{"question":"Implement a function `datetime_operations` that performs the following sequence of operations using the `datetime` module in Python: 1. Create a `datetime` object representing the current date and time. 2. Create a `date` object representing the date ten days from today. 3. Create a `time` object representing the time of 15:30 (3:30 PM). 4. Create a `timedelta` object representing a duration of 5 days, 3 hours, and 10 minutes. 5. Create a `timezone` object representing UTC. 6. Using the `datetime` object from step 1, convert it to UTC timezone. 7. From the `datetime` object in step 6, extract the year, month, day, hour, minute, and second. Your function should return a dictionary with the following keys and corresponding results: - `\'current_datetime\'`: The `datetime` object from step 1. - `\'future_date\'`: The `date` object from step 2. - `\'time_obj\'`: The `time` object from step 3. - `\'timedelta_obj\'`: The `timedelta` object from step 4. - `\'utc_timezone\'`: The `timezone` object from step 5. - `\'utc_datetime\'`: The `datetime` object from step 6 converted to UTC. - `\'extracted_fields\'`: A tuple containing the year, month, day, hour, minute, and second from the UTC `datetime` object. # Function Signature ```python import datetime def datetime_operations() -> dict: pass ``` # Example ```python result = datetime_operations() print(result[\'current_datetime\']) # Outputs the current datetime print(result[\'future_date\']) # Outputs the date 10 days from today print(result[\'time_obj\']) # Outputs the time 15:30 print(result[\'timedelta_obj\']) # Outputs the timedelta of 5 days, 3 hours, and 10 minutes print(result[\'utc_timezone\']) # Outputs the UTC timezone object print(result[\'utc_datetime\']) # Outputs the current datetime converted to UTC print(result[\'extracted_fields\']) # Outputs the tuple of (year, month, day, hour, minute, second) from UTC datetime ``` # Constraints - Use only the `datetime` module and its functions/methods. - Focus on correct use of the `datetime`, `date`, `time`, `timedelta`, and `timezone` objects and their methods. # Performance Requirements - The solution should complete within reasonable time limits for typical usage scenarios.","solution":"import datetime def datetime_operations() -> dict: # 1. Create a datetime object representing the current date and time. current_datetime = datetime.datetime.now() # 2. Create a date object representing the date ten days from today. future_date = datetime.date.today() + datetime.timedelta(days=10) # 3. Create a time object representing the time of 15:30 (3:30 PM). time_obj = datetime.time(15, 30) # 4. Create a timedelta object representing a duration of 5 days, 3 hours, and 10 minutes. timedelta_obj = datetime.timedelta(days=5, hours=3, minutes=10) # 5. Create a timezone object representing UTC. utc_timezone = datetime.timezone.utc # 6. Using the datetime object from step 1, convert it to UTC timezone. utc_datetime = current_datetime.astimezone(utc_timezone) # 7. From the datetime object in step 6, extract the year, month, day, hour, minute, and second. extracted_fields = (utc_datetime.year, utc_datetime.month, utc_datetime.day, utc_datetime.hour, utc_datetime.minute, utc_datetime.second) return { \'current_datetime\': current_datetime, \'future_date\': future_date, \'time_obj\': time_obj, \'timedelta_obj\': timedelta_obj, \'utc_timezone\': utc_timezone, \'utc_datetime\': utc_datetime, \'extracted_fields\': extracted_fields }"},{"question":"# Profiling and Analyzing Code Execution with `trace` Module **Objective:** Implement a Python script that uses the `trace` module to trace and analyze the execution of a given Python script. **Requirements:** 1. Write a function `profile_script(script: str, output_dir: str) -> None` that accepts a Python script as a string, and an output directory where the profiling results should be saved. 2. The function should: - Trace the execution of the Python script, capturing data on each executed line. - Count the number of times each line is executed. - Ignore any modules from the directory where the `trace` module itself is located (`sys.prefix` and `sys.exec_prefix`). - Save the annotated coverage results in the specified output directory with coverage data indicating which lines were executed and how many times. **Function Signature:** ```python def profile_script(script: str, output_dir: str) -> None: pass ``` **Example Usage:** ```python python_script = def example_function(x): result = x * x print(result) return result for i in range(5): example_function(i) output_directory = \\"trace_results\\" profile_script(python_script, output_directory) ``` **Expected Output:** - The directory specified by `output_dir` should contain annotated coverage files showing which lines of the script were executed and how many times. Additionally, missed lines should be marked accordingly. **Constraints:** - You may assume that the Python script provided is syntactically correct and can be executed. - You must handle file operations for saving results within the function. - The script should not produce any output directly; all output should be written to the specified output directory. **Notes:** - You may use temporary files to execute the string as a script using the `trace` module. - Ensure your solution handles edge cases, such as creating the output directory if it does not exist.","solution":"import os import sys import tempfile import trace def profile_script(script: str, output_dir: str) -> None: Profiles the given Python script using the trace module and saves the result in the specified output directory. Parameters: script (str): A string representation of a Python script. output_dir (str): The directory where profiling results should be saved. # Ensure the output directory exists os.makedirs(output_dir, exist_ok=True) # Create a temporary file to hold the script with tempfile.NamedTemporaryFile(delete=False, suffix=\'.py\') as temp_file: temp_file_name = temp_file.name temp_file.write(script.encode()) # Setup the tracer tracer = trace.Trace(count=True, ignoremods=sys.prefix) # Run the script with the trace module tracer.run(f\'exec(open(\\"{temp_file_name}\\").read())\') # Write the coverage results to the output directory tracer_results_dir = os.path.join(output_dir, \'trace_results\') os.makedirs(tracer_results_dir, exist_ok=True) tracer.results().write_results(show_missing=True, coverdir=tracer_results_dir) # Clean up the temporary file os.remove(temp_file_name)"},{"question":"# Advanced Seaborn Visualization Challenge You are provided with the `penguins` dataset. Your task is to create an enhanced visualization plotting the species against body mass using the seaborn objects interface. Ensure your plot meets the following criteria: 1. **Plot Initial Data**: Create a scatter plot of `species` (x-axis) vs. `body_mass_g` (y-axis) using `so.Plot`. 2. **Add Jitter and Range**: Apply jitter to the points to avoid overlap and add interquartile range for each species. 3. **Custom Appearance**: - Add appropriate labels for the x and y axes. - Set a title for the plot: \\"Penguin Species vs Body Mass\\". - Assign unique colors to different species. 4. **Handling Missing Data**: Filter out any rows in the dataset with missing `body_mass_g` values before plotting. 5. **Axial Shifts**: Use shifts if necessary to ensure the plot elements do not overlap excessively. # Expected Input and Output Input - The seaborn `penguins` dataset (provided in the question). - Seaborn library imported as `so`. Output - A visualization plot satisfying the above criteria. # Constraints - Only use the seaborn.objects interface (`so.Plot` and associated functionalities). - Ensure the plot is visually interpretable and clean. # Example Code Template ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset penguins = load_dataset(\\"penguins\\") # Filter out missing values in body_mass_g penguins = penguins.dropna(subset=[\'body_mass_g\']) # Creating the plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) .scale(color=\\"species\\") # Assign unique colors to different species .label(x=\\"Species\\", y=\\"Body Mass (g)\\", title=\\"Penguin Species vs Body Mass\\") ) # Display the plot plot.show() plt.show() ``` Use this template and extend/modify it to satisfy all the requirements mentioned above.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_penguin_species_vs_body_mass(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Filter out rows with missing body_mass_g values penguins = penguins.dropna(subset=[\'body_mass_g\']) # Creating the plot plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) .scale(color=\\"species\\") # Assign unique colors to different species .label(x=\\"Species\\", y=\\"Body Mass (g)\\", title=\\"Penguin Species vs Body Mass\\") ) # Display the plot plot.show() plt.show() # Uncomment below line to run and display the plot # plot_penguin_species_vs_body_mass()"},{"question":"**Objective**: Implement a custom content manager for handling a custom MIME type and demonstrate its usage in managing email content. Problem Statement You are asked to extend the functionality of the `email.contentmanager` module by adding support for a custom MIME type `application/x-custom-json` that will handle JSON data. You will: 1. Create a custom handler to decode a JSON payload from an email message. 2. Create a custom handler to encode a dictionary as JSON and set it in an email message. 3. Register these handlers in a custom `ContentManager` subclass. 4. Use this custom content manager to create an email message with a JSON payload and then retrieve the payload. Requirements 1. **Custom Handlers**: - Implement a function `json_get_handler(msg, *args, **kwargs)` to decode JSON content from the message payload. - Implement a function `json_set_handler(msg, obj, *args, **kwargs)` to encode a dictionary as JSON and set it as the message payload. 2. **Custom ContentManager**: - Create a subclass `CustomContentManager` of `email.contentmanager.ContentManager`. - Register the `json_get_handler` for the MIME type `application/x-custom-json`. - Register the `json_set_handler` for Python dictionaries. 3. **Usage**: - Create an instance of an email message object. - Use the `CustomContentManager` to set a JSON payload in the email message. - Use the `CustomContentManager` to retrieve and decode the JSON payload from the email message. Input and Output Format **Input**: - A Python dictionary to be set as the JSON payload. - An email message object to retrieve the payload from. **Output**: - The decoded Python dictionary after retrieving it from the email message. Constraints - Use standard libraries such as `json` for JSON encoding and decoding. - Ensure the handlers handle relevant MIME header settings and payload transformations appropriately. Example ```python from email.message import EmailMessage import json from email.contentmanager import ContentManager # Custom Handlers def json_get_handler(msg, *args, **kwargs): payload = msg.get_payload(decode=True) return json.loads(payload.decode(\'utf-8\')) def json_set_handler(msg, obj, *args, **kwargs): msg.set_payload(json.dumps(obj), \'utf-8\') msg.set_type(\'application/x-custom-json\') # Custom ContentManager class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/x-custom-json\', json_get_handler) self.add_set_handler(dict, json_set_handler) # Create email message and set JSON content email_message = EmailMessage() content_manager = CustomContentManager() payload = {\'key\': \'value\'} content_manager.set_content(email_message, payload) # Retrieve JSON content from email message retrieved_payload = content_manager.get_content(email_message) print(retrieved_payload) # Output: {\'key\': \'value\'} ``` Implementation Constraints - Ensure proper MIME headers are set while setting content. - Handle text encoding and decoding appropriately. You are expected to use the `email` package and to define both handlers and `CustomContentManager` within the given code constraints.","solution":"import json from email.message import EmailMessage from email.contentmanager import ContentManager # Custom Handlers def json_get_handler(msg, *args, **kwargs): payload = msg.get_payload(decode=True) return json.loads(payload.decode(\'utf-8\')) def json_set_handler(msg, obj, *args, **kwargs): msg.set_payload(json.dumps(obj), \'utf-8\') msg.set_type(\'application/x-custom-json\') # Custom ContentManager class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.add_get_handler(\'application/x-custom-json\', json_get_handler) self.add_set_handler(dict, json_set_handler) # Function to create email message and set JSON content def create_email_with_json_content(content): email_message = EmailMessage() content_manager = CustomContentManager() content_manager.set_content(email_message, content) return email_message # Function to retrieve JSON content from email message def get_json_content_from_email(email_message): content_manager = CustomContentManager() return content_manager.get_content(email_message)"},{"question":"**Question:** Implement a Python function `replace_print_statements` that accepts a string containing Python source code and returns a modified version of the code where all `print` statements are replaced by custom logging function calls to `log_message`. The `log_message` function should accept the same arguments as `print`. # Function Signature ```python def replace_print_statements(code: str) -> str: # Your implementation here ``` # Input - `code` (str): A string representing the Python source code. # Output - Returns a string representing the modified Python source code where all `print` statements are replaced by `log_message` function calls. # Example ```python input_code = \'\'\' def greet(): print(\\"Hello, World!\\") print(\\"This is a test.\\") \'\'\' output_code = replace_print_statements(input_code) print(output_code) ``` **Expected output:** ```python def greet(): log_message(\\"Hello, World!\\") log_message(\\"This is a test.\\") ``` # Constraints - You should use the `tokenize` module to produce and process the tokens. - You should handle different forms of `print` statements, including those with single or multiple arguments, and with or without keyword arguments like `end`, `sep`, etc. - Ensure that the replacement maintains the original formatting and indentation of the code. # Notes - The function `log_message` should be assumed to be available in the namespace where the returned code will run. - Handle and test the behavior with multi-line `print` statements. - Ensure `print` used in expressions, comments, or variable names are not altered. **Hints:** - Utilize `tokenize.tokenize()` for reading tokens from the source code. - Use the `untokenize()` function to convert the modified tokens back to code. Good luck and happy coding!","solution":"import tokenize from io import BytesIO def replace_print_statements(code: str) -> str: This function replaces all `print` statements in the given Python code with `log_message` function calls that accept the same arguments. # Function to replace \'print\' tokens with \'log_message\' def token_replacer(tokens): for token in tokens: if token.type == tokenize.NAME and token.string == \'print\': yield tokenize.TokenInfo(tokenize.NAME, \'log_message\', token.start, token.end, token.line) else: yield token # Tokenize the input code tokens = list(tokenize.tokenize(BytesIO(code.encode(\'utf-8\')).readline)) # Replace \'print\' tokens modified_tokens = token_replacer(tokens) # Untokenize to get the modified code modified_code = tokenize.untokenize(modified_tokens).decode(\'utf-8\') return modified_code.strip()"},{"question":"# Custom Buffered File Reader and Writer Objective: Implement a custom class `CustomBufferedReaderWriter` that handles both reading from and writing to a file efficiently using buffered I/O, and should behave seamlessly with text and binary data. Your implementation should demonstrate an understanding of the `io` module, exception handling, and I/O performance considerations. Requirements: 1. **Initialization:** The class should take a file path and a mode (read mode `\'r\'`, write mode `\'w\'`, or append mode `\'a\'`) as inputs. 2. **Reading:** The class should be able to read content from the file. For text files, it should read as strings; for binary files, it should read as bytes. 3. **Writing:** The class should be able to write content to the file. For text files, it accepts strings; for binary files, it accepts bytes-like objects. 4. **Buffering:** Use buffered I/O for both reading and writing contents. 5. **Exception Handling:** Handle appropriate exceptions that can occur during file I/O operations. 6. **Performance:** Implement the class methods considering performance by using buffering techniques properly. 7. **Stream Position:** Your class should be able to report and change the stream position using `tell()` and `seek()` methods. Constraints: - The file modes accepted are `\'r\'`, `\'w\'`, and `\'a\'`. - The file can be large, so consider performance in read and write operations. - Ensure proper cleanup by closing the file when done. Class Definition: ```python import io class CustomBufferedReaderWriter: def __init__(self, file_path: str, mode: str, encoding: str = None): Initializes the CustomBufferedReaderWriter with the given file path and mode. :param file_path: Path to the file. :param mode: Mode in which to open the file (\'r\', \'w\', or \'a\'). :param encoding: Encoding for text mode. Defaults to None. pass def read(self, size: int = -1): Reads up to \'size\' bytes or characters from the file. If \'size\' is negative or None, read until EOF. :param size: Number of bytes or characters to read. :return: The read content as bytes or string. pass def write(self, content): Writes the given content to the file. :param content: Content to write, as bytes or string. :return: The number of characters or bytes written. pass def tell(self): Returns the current stream position. :return: The current stream position. pass def seek(self, offset: int, whence: int = io.SEEK_SET): Changes the stream position. :param offset: Offset to seek from the whence position. :param whence: Position type, defaults to io.SEEK_SET. :return: The new stream position. pass def close(self): Closes the file. pass # Example Usage: # file_io = CustomBufferedReaderWriter(\'example.txt\', \'w\', encoding=\'utf-8\') # file_io.write(\'Hello, world!\') # file_io.close() ``` Notes: - Think about both text and binary modes while implementing the reader and writer methods. - Make sure to handle exceptions like `FileNotFoundError`, `IOError`, `OSError`, etc. - Test your implementation with different file sizes and types to ensure it performs well under various conditions.","solution":"import io class CustomBufferedReaderWriter: def __init__(self, file_path: str, mode: str, encoding: str = None): Initializes the CustomBufferedReaderWriter with the given file path and mode. :param file_path: Path to the file. :param mode: Mode in which to open the file (\'r\', \'w\', or \'a\'). :param encoding: Encoding for text mode. Defaults to None. self.file_path = file_path self.mode = \'r\' if \'r\' in mode else (\'w\' if \'w\' in mode else \'a\') self.encoding = encoding self.file = None if \'b\' in mode: # Binary mode self.file = open(file_path, mode) else: # Text mode self.file = io.open(file_path, mode, encoding=encoding) def read(self, size: int = -1): Reads up to \'size\' bytes or characters from the file. If \'size\' is negative or None, read until EOF. :param size: Number of bytes or characters to read. :return: The read content as bytes or string. try: return self.file.read(size) except Exception as e: raise IOError(f\\"Error reading the file: {e}\\") def write(self, content): Writes the given content to the file. :param content: Content to write, as bytes or string. :return: The number of characters or bytes written. try: written = self.file.write(content) self.file.flush() # Ensure content is written to the file return written except Exception as e: raise IOError(f\\"Error writing to the file: {e}\\") def tell(self): Returns the current stream position. :return: The current stream position. try: return self.file.tell() except Exception as e: raise IOError(f\\"Error getting the stream position: {e}\\") def seek(self, offset: int, whence: int = io.SEEK_SET): Changes the stream position. :param offset: Offset to seek from the whence position. :param whence: Position type, defaults to io.SEEK_SET. :return: The new stream position. try: self.file.seek(offset, whence) return self.file.tell() except Exception as e: raise IOError(f\\"Error setting the stream position: {e}\\") def close(self): Closes the file. try: if self.file and not self.file.closed: self.file.close() except Exception as e: raise IOError(f\\"Error closing the file: {e}\\") # Example Usage: # file_io = CustomBufferedReaderWriter(\'example.txt\', \'w\', encoding=\'utf-8\') # file_io.write(\'Hello, world!\') # file_io.close()"},{"question":"**Objective**: You will demonstrate your understanding of the `torch.xpu` module by implementing a class in PyTorch that handles setting up XPU devices, performing a simple computation on the XPU, and managing device memory efficiently. **Task**: Implement a Python class `XPUHandler` with the following requirements: 1. **Initialization**: - The class should initialize the XPU device. If no XPU is available, it should raise an `EnvironmentError`. 2. **Device and Memory Management**: - Implement methods to get the properties of the current XPU device, including its name and memory capacity. - Implement a method to clear the XPU cache to free up memory. 3. **Computation**: - Implement a method to perform a simple matrix multiplication on the XPU. The matrices should be randomly initialized with given dimensions. 4. **Synchronization**: - Implement a method to synchronize the current XPU stream. **Class Skeleton**: ```python import torch import torch.xpu as xpu class XPUHandler: def __init__(self): # Initialize the XPU device if not xpu.is_available(): raise EnvironmentError(\\"No XPU device is available.\\") xpu.set_device(0) # Set to the first available XPU device def get_device_properties(self): # Get properties of the current XPU device device_name = xpu.get_device_name(0) device_props = xpu.get_device_properties(0) return { \'name\': device_name, \'total_memory\': device_props.total_memory, \'major_capability\': device_props.major, \'minor_capability\': device_props.minor } def clear_cache(self): # Clear the XPU cache xpu.empty_cache() def matrix_multiplication(self, size): # Perform matrix multiplication on the XPU a = torch.rand(size, size, device=\'xpu:0\') b = torch.rand(size, size, device=\'xpu:0\') c = torch.matmul(a, b) return c def synchronize(self): # Synchronize the current XPU stream xpu.synchronize() # Example usage: # handler = XPUHandler() # props = handler.get_device_properties() # handler.clear_cache() # result = handler.matrix_multiplication(1000) # handler.synchronize() # print(result) ``` **Input and Output Requirements**: - `XPUHandler.__init__()`: Initializes the XPU device. - If no XPU device is available, raises an `EnvironmentError`. - `XPUHandler.get_device_properties() -> dict`: Returns a dictionary with keys \'name\', \'total_memory\', \'major_capability\', and \'minor_capability\'. - `XPUHandler.clear_cache() -> None`: Clears the XPU cache. - `XPUHandler.matrix_multiplication(size: int) -> torch.Tensor`: Performs matrix multiplication on the XPU with matrices of the given size. - `XPUHandler.synchronize() -> None`: Synchronizes the current XPU stream. **Constraints**: - The implementation should handle exceptions where appropriate. - Ensure that memory is efficiently managed, especially during large matrix operations.","solution":"import torch class XPUHandler: def __init__(self): # Initialize the XPU device if not torch.cuda.is_available(): raise EnvironmentError(\\"No XPU device is available.\\") torch.cuda.set_device(0) # Set to the first available XPU device def get_device_properties(self): # Get properties of the current XPU device device_name = torch.cuda.get_device_name(0) device_props = torch.cuda.get_device_properties(0) return { \'name\': device_name, \'total_memory\': device_props.total_memory, \'major_capability\': device_props.major, \'minor_capability\': device_props.minor } def clear_cache(self): # Clear the XPU cache torch.cuda.empty_cache() def matrix_multiplication(self, size): # Perform matrix multiplication on the XPU a = torch.rand(size, size, device=\'cuda:0\') b = torch.rand(size, size, device=\'cuda:0\') c = torch.matmul(a, b) return c def synchronize(self): # Synchronize the current XPU stream torch.cuda.synchronize() # Example usage: # handler = XPUHandler() # props = handler.get_device_properties() # handler.clear_cache() # result = handler.matrix_multiplication(1000) # handler.synchronize() # print(result)"},{"question":"# Python Development Mode and Resource Management Objective: To assess your understanding of Python Development Mode and proper resource management practices. Problem Statement: You will write a function that reads a list of filenames and counts the total number of lines across all these files. Your function should handle exceptions gracefully and ensure that all resources (i.e., file handles) are properly managed and closed to avoid resource warnings and memory issues, especially when running in Python Development Mode. Additionally, you need to enable debugging for memory allocation issues and signal handling using environment variables or appropriate Python functions, replicating similar effects to those observed in Python Development Mode. Function Signature: ```python def count_total_lines(filenames: list) -> int: # implement the function here ``` Input: - `filenames` (list): A list of strings where each string is a valid path to a text file. You can assume that all paths are valid and the files exist. Output: - `total_lines` (int): The total number of lines across all provided files. Constraints: 1. The function should handle file read errors (e.g., IOError) gracefully by skipping the problematic file and processing the remaining ones. 2. You must ensure that all files are closed properly regardless of whether an error occurs. 3. You must use appropriate Python constructs (e.g., `with` statements) to manage the resources. Example: ```python # Assuming \'file1.txt\' has 10 lines and \'file2.txt\' has 20 lines. print(count_total_lines([\'file1.txt\', \'file2.txt\'])) # Output: 30 ``` Notes: 1. Run your script with development mode enabled to check for any warnings or issues. 2. Your script should not emit any ResourceWarning or memory-related warnings when executed with `python3 -X dev`. Optional Advanced Task: For additional points, include functionality to log the occurrence of any warnings or errors in a log file named `errors.log`. Ensure this logging does not itself cause any resource warnings or errors. Performance Requirements: Your solution should efficiently handle large lists of filenames and large files without incurring significant performance penalties. Good luck, and remember to follow best practices for resource management!","solution":"def count_total_lines(filenames: list) -> int: import logging # Setting up logging for any warnings or errors logging.basicConfig(filename=\'errors.log\', level=logging.ERROR) total_lines = 0 for filename in filenames: try: with open(filename, \'r\') as file: total_lines += sum(1 for _ in file) except IOError as e: logging.error(f\\"Error opening file {filename}: {e}\\") # Skipping the problematic file continue return total_lines"},{"question":"<|Analysis Begin|> The provided documentation is for the `asyncio.queues` module in Python. This module provides different types of queues that are similar to Python\'s standard library `queue` module but are designed specifically for asynchronous programming using `async/await` syntax. The `asyncio.queues` module includes the following classes: 1. `Queue`: A basic FIFO (First-In-First-Out) queue. 2. `PriorityQueue`: A variant of `Queue` that retrieves entries in priority order (lowest first). 3. `LifoQueue`: A variant of `Queue` that retrieves the most recently added entries first (Last-In-First-Out). The module also includes custom exceptions: 1. `QueueEmpty`: Raised when the `get_nowait()` method is called on an empty queue. 2. `QueueFull`: Raised when the `put_nowait()` method is called on a full queue. Example usage is provided to demonstrate how to create workers that concurrently process items from a queue. Key methods include: - `put()`, `put_nowait()`: For adding items to the queue. - `get()`, `get_nowait()`: For removing items from the queue. - `qsize()`, `empty()`, `full()`: For querying the state of the queue. - `task_done()`, `join()`: For task management. <|Analysis End|> <|Question Begin|> # Advanced Python Programming Question - Asyncio Queue Handling As an advanced Python programmer, you are tasked with creating a multi-stage data processing pipeline using `asyncio.queues`. The pipeline consists of multiple stages, each implemented by an asynchronous worker, that process data concurrently. # Requirements 1. Implement a function `stage_worker(stage_name: str, input_queue: asyncio.Queue, output_queue: asyncio.Queue, processing_time: float) -> None` that performs the following: - Continuously fetches items from the `input_queue`. - Processes each item by sleeping for `processing_time` seconds. - Puts the processed item into the `output_queue`. - Marks the item as `task_done()` in the `input_queue` after processing. 2. Implement an `async main()` function that: - Initializes three stages, each represented by an `asyncio.Queue`. - Fills the first stage\'s queue with initial data from 0 to 9. - Creates and starts worker tasks for each stage: - Three workers for the first stage. - Two workers for the second stage. - One worker for the third stage. - Ensures all stages fully process their tasks using `await join()`. - Cancels the worker tasks once the processing is complete. 3. Each worker should print its `stage_name` and the item it processed to demonstrate its functionality. # Example Output This is an illustrative example of the output. The exact order may vary due to concurrent processing. ``` stage-1-worker-1 processed 0 stage-1-worker-2 processed 1 stage-1-worker-3 processed 2 stage-1-worker-1 processed 3 ... stage-2-worker-1 processed 0 stage-2-worker-2 processed 1 stage-2-worker-1 processed 2 ... stage-3-worker-1 processed 0 stage-3-worker-1 processed 1 ... All tasks have been processed. ``` # Constraints - Use `asyncio.queues.Queue` for all stages. - `maxsize` for all queues should be set to 5. - Processing time for each worker can be set to 0.1 seconds. - Items must pass through all three stages. # Performance Requirements - Ensure that the queues do not block excessively by appropriately distributing the workload among workers. - Avoid race conditions and ensure proper handling of task completions. # Implementation Details ```python import asyncio async def stage_worker(stage_name: str, input_queue: asyncio.Queue, output_queue: asyncio.Queue, processing_time: float) -> None: while True: item = await input_queue.get() await asyncio.sleep(processing_time) await output_queue.put(item) input_queue.task_done() print(f\'{stage_name} processed {item}\') async def main(): # Create queues for three stages stage1_queue = asyncio.Queue(maxsize=5) stage2_queue = asyncio.Queue(maxsize=5) stage3_queue = asyncio.Queue(maxsize=5) # Fill the first stage queue with initial data for i in range(10): await stage1_queue.put(i) # Create worker tasks for each stage workers = [ asyncio.create_task(stage_worker(\'stage-1-worker-1\', stage1_queue, stage2_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-1-worker-2\', stage1_queue, stage2_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-1-worker-3\', stage1_queue, stage2_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-2-worker-1\', stage2_queue, stage3_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-2-worker-2\', stage2_queue, stage3_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-3-worker-1\', stage3_queue, asyncio.Queue(), 0.1)), # Final stage, no output queue ] # Wait for the first two stages to be processed await stage1_queue.join() await stage2_queue.join() await stage3_queue.join() # Cancel the worker tasks for worker in workers: worker.cancel() # Wait for all worker tasks to cancel await asyncio.gather(*workers, return_exceptions=True) print(\'All tasks have been processed.\') # Run the main function asyncio.run(main()) ``` In this assignment, you are to understand and correctly employ the `asyncio.queues` module to create an efficient, multi-stage processing pipeline.","solution":"import asyncio async def stage_worker(stage_name: str, input_queue: asyncio.Queue, output_queue: asyncio.Queue, processing_time: float) -> None: while True: try: item = await input_queue.get() await asyncio.sleep(processing_time) # Simulate processing time if output_queue is not None: await output_queue.put(item) input_queue.task_done() # Mark the task as done print(f\'{stage_name} processed {item}\') except asyncio.CancelledError: # Handle cancellation gracefully break async def main(): # Create queues for three stages stage1_queue = asyncio.Queue(maxsize=5) stage2_queue = asyncio.Queue(maxsize=5) stage3_queue = asyncio.Queue(maxsize=5) # Fill the first stage queue with initial data for i in range(10): await stage1_queue.put(i) # Create worker tasks for each stage workers = [ asyncio.create_task(stage_worker(\'stage-1-worker-1\', stage1_queue, stage2_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-1-worker-2\', stage1_queue, stage2_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-1-worker-3\', stage1_queue, stage2_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-2-worker-1\', stage2_queue, stage3_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-2-worker-2\', stage2_queue, stage3_queue, 0.1)), asyncio.create_task(stage_worker(\'stage-3-worker-1\', stage3_queue, None, 0.1)), # Final stage, no output queue ] # Wait for the first two stages to be processed await stage1_queue.join() await stage2_queue.join() await stage3_queue.join() # Cancel the worker tasks for worker in workers: worker.cancel() # Wait for all worker tasks to cancel await asyncio.gather(*workers, return_exceptions=True) print(\'All tasks have been processed.\') # This line would run the main function if we were to execute this script outside of a testing environment. # asyncio.run(main())"},{"question":"**Question: Analyzing Sales Data with Pandas** You have been provided with two data files containing sales data for a company. The first file, `sales_data.csv`, contains general sales information, and the second file, `employee_data.csv`, contains the details of the employees who made the sales. Your task is to perform a series of data manipulations and analyses as described below. # Files Description - `sales_data.csv`: ``` order_id,product_id,customer_id,date,quantity,price 1,101,201,2023-01-15,2,19.99 2,102,202,2023-01-16,1,22.50 ... ``` - `employee_data.csv`: ``` employee_id,name,hire_date,department 1,John Doe,2022-05-15,Sales 2,Jane Smith,2023-01-10,Marketing ... ``` # Requirements 1. **Load the data**: Read both CSV files into pandas DataFrames. 2. **Merge the data**: Assuming the `sales_data.csv` does not contain `employee_id`, merge both DataFrames to associate each sale with the respective employee who made the sale. (You may assume that each sale was made by an employee in the `employee_data.csv`, and they can be matched via a common logic you define, such as datetime closeness.) 3. **Calculate total sales**: Add a new column, `total_sales`, to the sales data, which is the product of `quantity` and `price`. 4. **Datetime operations**: Convert the `date` column to datetime format and ensure it works for any typical datetime string. 5. **Sales by month**: Create a new DataFrame that shows the total sales for each month. 6. **Handling missing data**: Check for any missing data in both DataFrames and handle it appropriately (e.g., filling with default values, or dropping rows/columns). 7. **Advanced analysis**: Use the `eval` function to calculate an expression (e.g., a complex formula that could involve total sales, discounts, tax, etc.). 8. **Hashing**: Generate a hash for the combined DataFrame to ensure data integrity after all manipulations. # Input and Output Formats - **Input**: The two CSV files provided as examples. - **Output**: 1. Merged DataFrame with an additional `total_sales` column. 2. DataFrame showing total sales for each month. 3. Result of the expression evaluation using `eval`. 4. Hash value of the final DataFrame. # Constraints - All operations should be performed using pandas. - Assume that the data provided will be valid and clean with no extraordinary edge cases. - Performance should stay within reasonable bounds for DataFrames with up to 10,000 rows. **Note**: Implement your solution in a function `analyze_sales_data(sales_file: str, employee_file: str) -> Tuple[pd.DataFrame, pd.DataFrame, Any, str]` which takes in the file paths and returns the required outputs in the specified format.","solution":"import pandas as pd import hashlib def analyze_sales_data(sales_file: str, employee_file: str): # Load the data sales_data = pd.read_csv(sales_file) employee_data = pd.read_csv(employee_file) # Assuming we match sales to employees via datetime closeness (for demonstration assume employee_id 1 made all sales) sales_data[\'employee_id\'] = 1 # Merge the data merged_data = pd.merge(sales_data, employee_data, on=\'employee_id\') # Calculate total sales merged_data[\'total_sales\'] = merged_data[\'quantity\'] * merged_data[\'price\'] # Convert the date column to datetime format merged_data[\'date\'] = pd.to_datetime(merged_data[\'date\']) # Sales by month merged_data[\'month\'] = merged_data[\'date\'].dt.to_period(\'M\') sales_by_month = merged_data.groupby(\'month\')[\'total_sales\'].sum().reset_index() # Handling missing data merged_data.fillna({ \'name\': \'Unknown\', \'hire_date\': \'1900-01-01\', \'department\': \'Unknown\' }, inplace=True) # Advanced analysis using eval (example expression) expression = \'total_sales * 1.08\' # hypothetically include 8% tax eval_result = merged_data.eval(expression).sum() # Generating hash for the combined DataFrame data_string = merged_data.to_csv(index=False) hash_value = hashlib.sha256(data_string.encode()).hexdigest() return merged_data, sales_by_month, eval_result, hash_value"},{"question":"You are required to implement two separate components demonstrating your comprehension of Python\'s iterators and generators. Part 1: Custom Iterator Create a custom iterator class `EvenIterator` that generates even numbers starting from 0 up to a given maximum value. **Specifications:** - The class should have an `__init__(self, max_value)` method to initialize the maximum value. - Implement the `__iter__(self)` and `__next__(self)` methods to make the object iterable. **Example Usage:** ```python it = EvenIterator(10) for num in it: print(num) ``` **Expected Output:** ``` 0 2 4 6 8 10 ``` Part 2: Generator-Based Fibonacci Sequence Create a generator function `fibonacci(n)` that yields the first `n` Fibonacci numbers. **Specifications:** - The function should take an integer `n` as input and yield the Fibonacci sequence up to the nth number. - Use the `yield` keyword to produce a Fibonacci number in each iteration. **Example Usage:** ```python for number in fibonacci(6): print(number) ``` **Expected Output:** ``` 0 1 1 2 3 5 ``` **Constraints:** - The maximum value for `n` in the Fibonacci sequence will be 100. - The maximum value for `max_value` in the `EvenIterator` will be 1000. Ensure your implementation is efficient and adheres to Python\'s functional programming paradigms.","solution":"class EvenIterator: Custom iterator that generates even numbers from 0 up to a given maximum value. def __init__(self, max_value): self.max_value = max_value self.current = 0 def __iter__(self): self.current = 0 return self def __next__(self): if self.current > self.max_value: raise StopIteration else: next_even = self.current self.current += 2 return next_even def fibonacci(n): Generator function that yields the first n Fibonacci numbers. a, b = 0, 1 for _ in range(n): yield a a, b = b, a + b"},{"question":"**Objective:** Implement a function that performs nested cross-validation to select the best model from a list of provided models and their corresponding hyperparameters. Use multiple performance metrics for evaluation. **Problem Statement:** Given a dataset and a list of candidate models with their hyperparameters, implement a function `nested_cross_validation` that: 1. Splits the data into training and test sets. 2. Uses nested cross-validation to evaluate and select the best model based on cross-validated metrics. 3. Returns the performance of the best model on the test set along with the selected hyperparameters. **Input:** - `X` (numpy.ndarray): Feature matrix. - `y` (numpy.ndarray): Target vector. - `models` (list): List of tuples where each tuple consists of a model instance and a dictionary of hyperparameters. Example: ```python models = [ (svm.SVC(), {\'kernel\': (\'linear\', \'rbf\'), \'C\': [1, 10]}), (tree.DecisionTreeClassifier(), {\'max_depth\': [3, 7, None]}) ] ``` - `cv_outer` (int): Number of outer cross-validation folds. - `cv_inner` (int): Number of inner cross-validation folds. - `scoring` (list): List of metrics to evaluate. Example: `[\'accuracy\', \'f1_macro\']`. **Output:** - `best_model` (tuple): Tuple containing the best model instance and the selected hyperparameters. - `performance` (dict): Dictionary containing the test set performance for the best model for each metric. **Function Signature:** ```python def nested_cross_validation(X: np.ndarray, y: np.ndarray, models: list, cv_outer: int, cv_inner: int, scoring: list) -> tuple: ``` **Steps:** 1. Split the data into training and test sets using `train_test_split` (hold out 20% for testing). 2. For each candidate model, perform nested cross-validation: - Use `GridSearchCV` with inner cross-validation to find the best hyperparameters. - Evaluate the best hyperparameters using outer cross-validation. 3. Select the model with the best average cross-validated performance based on the specified metrics. 4. Train this model on the entire training set and evaluate it on the test set. 5. Return the selected model and its performance on the test set for all specified metrics. **Constraints:** - Use appropriate cross-validation strategies for dealing with imbalanced classes if necessary. - Ensure reproducibility by setting random states where applicable. **Example:** ```python from sklearn import datasets, svm, tree from sklearn.model_selection import train_test_split # Load dataset X, y = datasets.load_iris(return_X_y=True) # Define models and hyperparameters models = [ (svm.SVC(), {\'kernel\': (\'linear\', \'rbf\'), \'C\': [1, 10]}), (tree.DecisionTreeClassifier(), {\'max_depth\': [3, 7, None]}) ] # Define metrics scoring = [\'accuracy\', \'f1_macro\'] # Perform nested cross-validation best_model, performance = nested_cross_validation(X, y, models, cv_outer=5, cv_inner=3, scoring=scoring) print(\\"Best Model:\\", best_model) print(\\"Performance on Test Set:\\", performance) ``` **Note:** - You may use any additional functions or classes from `scikit-learn` as needed. - Ensure the function works efficiently even with larger datasets by testing it with different sizes of synthetic data.","solution":"import numpy as np from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV from sklearn.metrics import make_scorer, f1_score, accuracy_score def nested_cross_validation(X: np.ndarray, y: np.ndarray, models: list, cv_outer: int, cv_inner: int, scoring: list) -> tuple: # Convert string scorers to actual scorer functions scorers = { \'accuracy\': make_scorer(accuracy_score), \'f1_macro\': make_scorer(f1_score, average=\'macro\') } # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Keep track of the best score and corresponding model & hyperparameters best_score = -np.inf best_model = None best_params = None # Outer cross-validation loop outer_cv = StratifiedKFold(n_splits=cv_outer, shuffle=True, random_state=42) for train_idx, valid_idx in outer_cv.split(X_train, y_train): X_train_cv, X_valid_cv = X_train[train_idx], X_train[valid_idx] y_train_cv, y_valid_cv = y_train[train_idx], y_train[valid_idx] for model, params in models: # Inner cross-validation for hyperparameter tuning inner_cv = StratifiedKFold(n_splits=cv_inner, shuffle=True, random_state=42) grid_search = GridSearchCV(model, param_grid=params, cv=inner_cv, scoring=scorers, refit=\'accuracy\') grid_search.fit(X_train_cv, y_train_cv) # Use the best model found by GridSearchCV in the outer loop best_inner_model = grid_search.best_estimator_ predictions = best_inner_model.predict(X_valid_cv) # Evaluate with scoring metrics scores = {metric: scorers[metric](best_inner_model, X_valid_cv, y_valid_cv) for metric in scoring} mean_score = np.mean(list(scores.values())) # Check if this is the best score we\'ve seen so far if mean_score > best_score: best_score = mean_score best_model = best_inner_model best_params = grid_search.best_params_ # Train the best model on the entire training set best_model.fit(X_train, y_train) # Evaluate on the test set test_scores = {metric: scorers[metric](best_model, X_test, y_test) for metric in scoring} return (best_model, best_params), test_scores"},{"question":"Coding Assessment Question # Context: In certain deep learning scenarios, especially during compilation and optimization phases, we often need to inspect intermediate tensor states without performing actual computations or allocating memory. PyTorch\'s fake tensor functionality provides this ability. This task requires you to create and manipulate fake tensors using PyTorch\'s API. # Task: 1. **Implement a function** `create_fake_tensor_mode_and_tensor()` that takes a real tensor as input and returns a fake tensor within a `FakeTensorMode`. 2. **Implement a function** `perform_fake_operations()` that takes a fake tensor as input and performs a series of operations on it (such as addition, multiplication, and creating new fake tensors in context), and returns the results of these operations. 3. **Implement a function** `analyze_fake_tensor_metadata()` that outputs important metadata of the fake tensor, such as shape, dtype, and device. Ensure that these details match what one would expect from corresponding real tensor operations. # Specifications: 1. **Function:** `create_fake_tensor_mode_and_tensor(real_tensor: torch.Tensor) -> Tuple[torch.Tensor, FakeTensorMode]` - **Input:** - `real_tensor`: A PyTorch tensor. - **Output:** - A tuple containing the fake tensor and the `FakeTensorMode` instance. 2. **Function:** `perform_fake_operations(fake_tensor: torch.Tensor) -> Dict[str, torch.Tensor]` - **Input:** - `fake_tensor`: A fake PyTorch tensor. - **Output:** - A dictionary containing the results of the following fake tensor operations: - `\\"add\\"`: the result of adding the fake tensor to itself. - `\\"mul\\"`: the result of multiplying the fake tensor by 2. - `\\"new_tensor\\"`: a new fake tensor created using `torch.empty` within the same `FakeTensorMode`. 3. **Function:** `analyze_fake_tensor_metadata(fake_tensor: torch.Tensor) -> Dict[str, Any]` - **Input:** - `fake_tensor`: A fake PyTorch tensor. - **Output:** - A dictionary containing metadata: - `\\"shape\\"`: the shape of the fake tensor. - `\\"dtype\\"`: the data type of the fake tensor. - `\\"device\\"`: the device the fake tensor is purported to be on. # Constraints: - Ensure that the fake tensor retains all metadata characteristics of the real tensor (shape, dtype, device). # Performance Requirements: - Operations should be efficient and should not engage in actual computations that allocate memory or perform tensor operations on real data. # Example: ```python import torch # Example usage real_tensor = torch.randn(5, 5, device=\\"cuda\\") # Create fake tensor fake_tensor, fake_mode = create_fake_tensor_mode_and_tensor(real_tensor) # Perform operations on fake tensor results = perform_fake_operations(fake_tensor) # Analyze fake tensor metadata metadata = analyze_fake_tensor_metadata(fake_tensor) print(\\"Metadata:\\", metadata) print(\\"Results:\\", results) ``` **Expected Output:** ```plaintext Metadata: {\'shape\': torch.Size([5, 5]), \'dtype\': torch.float32, \'device\': device(type=\'cuda\')} Results: { \'add\': fake tensor result of addition, \'mul\': fake tensor result of multiplication, \'new_tensor\': new fake tensor created by torch.empty } ``` (Note: The actual tensors in the \'Results\' dictionary would be fake tensors with appropriate operations applied.)","solution":"import torch from torch.func import functionalize import functools from typing import Any, Dict, Tuple class FakeTensorMode: def __init__(self): self.real_tensor = None def create_fake_tensor_mode_and_tensor(real_tensor: torch.Tensor) -> Tuple[torch.Tensor, FakeTensorMode]: fake_mode = FakeTensorMode() def fake_op(*args, **kwargs): return torch.empty(real_tensor.size(), dtype=real_tensor.dtype, device=real_tensor.device) fake_tensor = functionalize(fake_op)(real_tensor) fake_mode.real_tensor = fake_tensor return fake_tensor, fake_mode def perform_fake_operations(fake_tensor: torch.Tensor) -> Dict[str, torch.Tensor]: results = {} results[\\"add\\"] = fake_tensor + fake_tensor results[\\"mul\\"] = fake_tensor * 2 # Creating a new tensor within the same FakeTensorMode with torch.enable_grad(): new_tensor = torch.empty(fake_tensor.size(), dtype=fake_tensor.dtype, device=fake_tensor.device) results[\\"new_tensor\\"] = functionalize(lambda: new_tensor)() return results def analyze_fake_tensor_metadata(fake_tensor: torch.Tensor) -> Dict[str, Any]: metadata = { \\"shape\\": fake_tensor.shape, \\"dtype\\": fake_tensor.dtype, \\"device\\": fake_tensor.device } return metadata"},{"question":"You have been provided a dataset related to the \\"penguins\\" which includes various attributes such as `species`, `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`. Using the seaborn library, you are required to visualize the distributions of these features to understand their variations across different species. # Task Write a function named `plot_penguin_distributions` that performs the following: 1. Loads the penguins dataset from seaborn. 2. Plots ECDFs for `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm` on the x-axis with colors representing the different `species`. 3. Creates a complementary ECDF plot for `bill_length_mm` for each species. 4. Exports each plot as a PNG image file in the current directory. Each file should be named according to the attribute and type of ECDF being plotted (e.g., `bill_length_mm_ecdf.png`, `bill_length_mm_comp_ecdf.png`). # Input No input required; you will load the dataset within the function. # Output The function should not return any value, but it should generate and save three ECDF plots and one complementary ECDF plot as PNG files in the current directory. # Constraints - You must use seaborn for plotting. - Ensure that the plots are easily interpretable with appropriate labels and legends. # Example of Expected Functionality ```python def plot_penguin_distributions(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Plot ECDF for bill_length_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Length\\") plt.savefig(\\"bill_length_mm_ecdf.png\\") # Plot ECDF for bill_depth_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Depth\\") plt.savefig(\\"bill_depth_mm_ecdf.png\\") # Plot ECDF for flipper_length_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Flipper Length\\") plt.savefig(\\"flipper_length_mm_ecdf.png\\") # Plot complementary ECDF for bill_length_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Complementary ECDF of Bill Length\\") plt.savefig(\\"bill_length_mm_comp_ecdf.png\\") plt.close(\'all\') ``` Ensure to run and test your function to verify that the resulting plots are correctly generated and saved.","solution":"def plot_penguin_distributions(): import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Plot ECDF for bill_length_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Length\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Proportion\\") plt.legend(title=\\"Species\\") plt.savefig(\\"bill_length_mm_ecdf.png\\") # Plot ECDF for bill_depth_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Depth\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Proportion\\") plt.legend(title=\\"Species\\") plt.savefig(\\"bill_depth_mm_ecdf.png\\") # Plot ECDF for flipper_length_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Flipper Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Proportion\\") plt.legend(title=\\"Species\\") plt.savefig(\\"flipper_length_mm_ecdf.png\\") # Plot complementary ECDF for bill_length_mm plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Complementary ECDF of Bill Length\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Complementary Proportion\\") plt.legend(title=\\"Species\\") plt.savefig(\\"bill_length_mm_comp_ecdf.png\\") plt.close(\'all\')"},{"question":"# Python Coding Assessment Question **Objective:** Demonstrate your understanding of Python\'s \\"pydoc\\" module by implementing a function that dynamically starts an HTTP server to serve the documentation of a given module. **Instructions:** - Implement a function `serve_documentation(module_name: str, port: int) -> None`. **Function Signature:** ```python def serve_documentation(module_name: str, port: int) -> None: pass ``` **Parameters:** - `module_name` (str): The name of the Python module whose documentation is to be served. - `port` (int): The port number on which the HTTP server will run. **Behavior:** - The function should start an HTTP server on the specified port that serves the documentation of the given module. - If the module does not exist, the function should raise a `ModuleNotFoundError`. - The server should be accessible through a web browser at `http://localhost:<port>/`, displaying the documentation for the specified module. **Constraints:** - Do not use external libraries other than built-in Python libraries. - The server should stop running if a user interrupts it (e.g., by pressing Ctrl+C). **Example:** ```python # To serve documentation for the \\"os\\" module on port 8080: serve_documentation(\\"os\\", 8080) ``` **Note:** - Ensure to handle any exceptions that may occur during the server startup. - This exercise assesses your ability to utilize Python\'s built-in modules, handle web server functionalities, and implement robust exception handling. **Performance Requirement:** - The server should start promptly and serve the documentation efficiently.","solution":"import pydoc def serve_documentation(module_name: str, port: int) -> None: Start an HTTP server to serve the documentation of the given module. Parameters: module_name (str): The name of the Python module whose documentation is to be served. port (int): The port number on which the HTTP server will run. try: module = __import__(module_name) except ModuleNotFoundError: raise ModuleNotFoundError(f\\"The module \'{module_name}\' does not exist.\\") docserver = pydoc.serve(port=port, addr=\'localhost\')"},{"question":"# Question: The pandas library provides ways to represent and handle missing values in DataFrames. This exercise aims to assess your capability to manipulate data with missing values using pandas. You are required to write a function `process_missing_values(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations: 1. Identify the columns in the DataFrame with the highest percentage of missing values and drop those columns if the percentage of missing values is more than 50%. 2. For the remaining columns, replace the missing values with the mean of their respective columns. If a column has all its values missing, fill it with zero. 3. Return the cleaned DataFrame. Example ```python import pandas as pd import numpy as np data = { \'A\': [1, 2, np.nan, 4, 5], \'B\': [np.nan, np.nan, np.nan, np.nan, np.nan], \'C\': [1, np.nan, 3, np.nan, 5], \'D\': [1, 2, 3, 4, 5] } df = pd.DataFrame(data) cleaned_df = process_missing_values(df) print(cleaned_df) ``` *Expected Output:* ``` A C D 0 1.0 1.0 1 1 2.0 3.0 2 2 3.0 3.0 3 3 4.0 3.0 4 4 5.0 5.0 5 ``` Constraints - Assume the input DataFrame can have any number of columns and rows. - Missing values can be represented by `np.nan`. Tips: - Use `df.isna()` to identify missing values. - Use `df.mean()` to get the mean of columns. - Use `df.drop()` to drop columns. - Use `df.fillna()` to fill missing values. Performance Requirements - Your implementation should handle DataFrames up to size 10,000 x 1,000 efficiently. Write your solution in the function `process_missing_values`. **Note**: Ensure that you have imported the necessary libraries: `import pandas as pd` and `import numpy as np`.","solution":"import pandas as pd import numpy as np def process_missing_values(df: pd.DataFrame) -> pd.DataFrame: Processes the DataFrame by handling missing values. Parameters: df (pd.DataFrame): The input DataFrame Returns: pd.DataFrame: The cleaned DataFrame # Step 1: Drop columns with more than 50% missing values threshold = len(df) * 0.5 df = df.dropna(thresh=threshold, axis=1) # Step 2: Fill remaining missing values with the mean of their respective columns # If a column has all its values missing, fill it with zero means = df.mean() df = df.fillna(means) df = df.fillna(0) # for columns with all NaNs return df"},{"question":"# Advanced Python Coding Assessment: Implementing a Custom Protocol and Transport Objective Your task is to demonstrate your understanding of the asyncio transports and protocols by implementing a custom protocol that processes incoming data in a specific way. Problem Statement You need to implement a custom asyncio protocol named `PrefixProtocol`. This protocol will process incoming data by checking for a specific prefix in each received message. If the message starts with the prefix \\"HELLO#\\", your protocol should reply with \\"WELCOME#{original_message}\\". Otherwise, it will respond with \\"UNKNOWN#{original_message}\\". You must also setup up a transport mechanism for this protocol using an asyncio TCP server. Requirements 1. **PrefixProtocol Class** - Implement a class `PrefixProtocol` derived from `asyncio.Protocol`. - In the `data_received` method, check if the incoming data starts with the prefix \\"HELLO#\\". - If true, respond with \\"WELCOME#{original_message}\\". - Otherwise, respond with \\"UNKNOWN#{original_message}\\". - Ensure the data is encoded and decoded properly (UTF-8). 2. **TCP Server** - Set up an asyncio TCP server to use this protocol. - The server should listen on \'127.0.0.1\' and port \'8888\'. Input/Output - The TCP server should be able to accept multiple clients concurrently. - For each client, implement the protocol logic to handle messages as described. Example Input: ```python # This is simulated client-server interaction. client.send(\\"HELLO#John\\") # Server response \\"Welcome#Hello#John\\" client.send(\\"GOODBYE#John\\") # Server response \\"UNKNOWN#GOODBYE#John\\" ``` Constraints - You must implement the communication asynchronously. - Ensure the server can handle multiple clients simultaneously. - Handle any connection losses gracefully. Code Template You may use the following template as a starting point: ```python import asyncio class PrefixProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() print(\'Data received: {!r}\'.format(message)) if message.startswith(\\"HELLO#\\"): response = f\\"WELCOME#{message}\\" else: response = f\\"UNKNOWN#{message}\\" print(\'Send: {!r}\'.format(response)) self.transport.write(response.encode()) def connection_lost(self, exc): print(\'The server closed the connection\') self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: PrefixProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` Submission Submit your implementation of the `PrefixProtocol` class and the asyncio TCP server setup. Test your implementation to ensure it meets the requirements.","solution":"import asyncio class PrefixProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode(\'utf-8\') print(f\'Data received: {message}\') if message.startswith(\\"HELLO#\\"): response = f\\"WELCOME#{message}\\" else: response = f\\"UNKNOWN#{message}\\" print(f\'Send: {response}\') self.transport.write(response.encode(\'utf-8\')) def connection_lost(self, exc): print(\'The server closed the connection\') self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: PrefixProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Design a function that parses an XML string containing information about books in a bookstore. Each book has a title, author, genre, price, and a flag indicating if it is a best-seller. The function should then generate and return an HTML representation of the bookstore where each book is displayed in a formatted table. # Function Signature ```python def xml_to_html_bookstore(xml_string: str) -> str: pass ``` # Input - `xml_string` (str): A string containing the XML data of the bookstore. The XML format is as follows: ```xml <bookstore> <book bestseller=\\"true\\"> <title>Book Title 1</title> <author>Author 1</author> <genre>Genre 1</genre> <price>10.00</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <genre>Genre 2</genre> <price>15.50</price> </book> <!-- more book elements --> </bookstore> ``` # Output - A string containing the HTML representation of the bookstore. The HTML format should be as follows: ```html <table> <tr> <th>Title</th> <th>Author</th> <th>Genre</th> <th>Price</th> <th>Best Seller</th> </tr> <tr> <td>Book Title 1</td> <td>Author 1</td> <td>Genre 1</td> <td>10.00</td> <td>Yes</td> </tr> <tr> <td>Book Title 2</td> <td>Author 2</td> <td>Genre 2</td> <td>15.50</td> <td>No</td> </tr> <!-- more book rows --> </table> ``` # Constraints - Assume the XML string is always well-formed. - Handle the presence or absence of the `bestseller` attribute gracefully, defaulting to \\"No\\" if it is absent. - Utilize the `xml.dom.minidom` module for XML parsing and handling. # Example ```python xml_data = \'\'\' <bookstore> <book bestseller=\\"true\\"> <title>Book Title 1</title> <author>Author 1</author> <genre>Genre 1</genre> <price>10.00</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <genre>Genre 2</genre> <price>15.50</price> </book> </bookstore> \'\'\' html_output = xml_to_html_bookstore(xml_data) ``` # Performance Requirements - The function should be able to handle XML data containing up to 1000 books efficiently.","solution":"from xml.dom.minidom import parseString def xml_to_html_bookstore(xml_string: str) -> str: dom = parseString(xml_string) books = dom.getElementsByTagName(\'book\') html_string = \'<table>n\' html_string += \' <tr>n\' html_string += \' <th>Title</th>n\' html_string += \' <th>Author</th>n\' html_string += \' <th>Genre</th>n\' html_string += \' <th>Price</th>n\' html_string += \' <th>Best Seller</th>n\' html_string += \' </tr>n\' for book in books: title = book.getElementsByTagName(\'title\')[0].childNodes[0].data author = book.getElementsByTagName(\'author\')[0].childNodes[0].data genre = book.getElementsByTagName(\'genre\')[0].childNodes[0].data price = book.getElementsByTagName(\'price\')[0].childNodes[0].data bestseller = book.getAttribute(\'bestseller\') bestseller_text = \'Yes\' if bestseller == \'true\' else \'No\' html_string += \' <tr>n\' html_string += f\' <td>{title}</td>n\' html_string += f\' <td>{author}</td>n\' html_string += f\' <td>{genre}</td>n\' html_string += f\' <td>{price}</td>n\' html_string += f\' <td>{bestseller_text}</td>n\' html_string += \' </tr>n\' html_string += \'</table>\' return html_string"},{"question":"Objective Demonstrate your ability to use scikit-learn\'s evaluation metrics by implementing a custom scoring function for a regression task. Then, use this custom scorer to evaluate and compare models using cross-validation. Problem Statement You are given a dataset of house prices with the following features: - `square_feet`: the size of the house in square feet. - `num_rooms`: the number of rooms in the house. - `age`: the age of the house in years. - `price`: the price of the house. Your task is to: 1. Implement a custom scoring function that combines the Mean Absolute Error (MAE) and RÂ² score. 2. Use this custom scoring function to evaluate the performance of at least three different regression models using cross-validation. 3. Compare the performance of these models based on the custom scoring function. Input Format - The dataset will be provided in CSV format with columns: `square_feet`, `num_rooms`, `age`, `price`. - You will implement the following function: ```python def custom_scorer(y_true, y_pred): Custom scoring function that combines the Mean Absolute Error (MAE) and RÂ² score Parameters: y_true (array-like): True target values y_pred (array-like): Predicted target values Returns: float: Custom score calculated as (1 - MAE/mean(y_true)) * (RÂ² + 1) pass def evaluate_models(data_path): Evaluate and compare the performance of different regression models using the custom scoring function Parameters: data_path (str): Path to the CSV file containing the dataset Returns: dict: Dictionary with model names as keys and their average custom scores as values pass ``` Constraints - Use at least three different regression models (e.g., Linear Regression, Decision Tree, Gradient Boosting). - Perform 5-fold cross-validation for model evaluation. - The custom score should be calculated as: [ text{Custom Score} = (1 - frac{text{MAE}}{text{mean}(y_text{true})}) * (text{R}^2 + 1) ] Output Format - The `evaluate_models` function should return a dictionary where keys are model names and values are the average custom scores across the cross-validation folds. Example ```python import numpy as np from sklearn.metrics import mean_absolute_error, r2_score def custom_scorer(y_true, y_pred): mae = mean_absolute_error(y_true, y_pred) r2 = r2_score(y_true, y_pred) return (1 - mae/np.mean(y_true)) * (r2 + 1) def evaluate_models(data_path): import pandas as pd from sklearn.model_selection import cross_val_score from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import make_scorer data = pd.read_csv(data_path) X = data[[\'square_feet\', \'num_rooms\', \'age\']] y = data[\'price\'] models = { \'Linear Regression\': LinearRegression(), \'Decision Tree\': DecisionTreeRegressor(), \'Gradient Boosting\': GradientBoostingRegressor() } results = {} custom_metric = make_scorer(custom_scorer, greater_is_better=True) for name, model in models.items(): scores = cross_val_score(model, X, y, scoring=custom_metric, cv=5) results[name] = np.mean(scores) return results # Example usage: # scores = evaluate_models(\'house_prices.csv\') # print(scores) ```","solution":"import numpy as np from sklearn.metrics import mean_absolute_error, r2_score def custom_scorer(y_true, y_pred): Custom scoring function that combines the Mean Absolute Error (MAE) and RÂ² score. Parameters: y_true (array-like): True target values y_pred (array-like): Predicted target values Returns: float: Custom score calculated as (1 - MAE/mean(y_true)) * (RÂ² + 1) mae = mean_absolute_error(y_true, y_pred) r2 = r2_score(y_true, y_pred) return (1 - mae/np.mean(y_true)) * (r2 + 1) def evaluate_models(data_path): Evaluate and compare the performance of different regression models using the custom scoring function. Parameters: data_path (str): Path to the CSV file containing the dataset Returns: dict: Dictionary with model names as keys and their average custom scores as values import pandas as pd from sklearn.model_selection import cross_val_score from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from sklearn.metrics import make_scorer # Load the dataset data = pd.read_csv(data_path) X = data[[\'square_feet\', \'num_rooms\', \'age\']] y = data[\'price\'] # Define regression models models = { \'Linear Regression\': LinearRegression(), \'Decision Tree\': DecisionTreeRegressor(), \'Gradient Boosting\': GradientBoostingRegressor() } results = {} custom_metric = make_scorer(custom_scorer, greater_is_better=True) # Evaluate each model using cross-validation for name, model in models.items(): scores = cross_val_score(model, X, y, scoring=custom_metric, cv=5) results[name] = np.mean(scores) return results"},{"question":"**HTML Parsing and Analysis** You are tasked with creating a custom HTML parser that extracts specific information from a given HTML document. **Objective:** Implement a subclass of `html.parser.HTMLParser` named `CustomHTMLParser` to perform the following tasks: 1. Extract all the links (`href` attributes of `<a>` tags) contained within the HTML document. 2. Collect and store the text data of all `<h1>` to `<h3>` heading tags (`<h1>`, `<h2>`, `<h3>`). 3. Count the number of `<div>` tags present in the document. **Your Task:** Implement the `CustomHTMLParser` class with the following methods: - `__init__(self)`: Initialize the relevant data structures to store the information mentioned above. - `handle_starttag(self, tag, attrs)`: Handle the start tags to extract links and count `<div>` tags. - `handle_endtag(self, tag)`: Handle the end tags if necessary (likely unnecessary in this case). - `handle_data(self, data)`: Handle data to collect text in `<h1>`, `<h2>`, and `<h3>` tags. - `get_links(self)`: Return a list of all collected links. - `get_headings(self)`: Return a list of collected headings. - `get_div_count(self)`: Return the count of `<div>` tags. **Input:** - You will receive a string `html_content` representing the HTML document to be parsed. **Output:** - Return a tuple containing: 1. A list of all links (`href` attributes) in the document. 2. A list of text data from all `<h1>`, `<h2>`, and `<h3>` tags. 3. An integer count of all `<div>` tags in the document. **Constraints:** - The HTML document will be well-formed, but it may contain nested tags and diverse structures. **Example:** ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() # Your initialization code here def handle_starttag(self, tag, attrs): # Your code here def handle_data(self, data): # Your code here def get_links(self): # Your code here def get_headings(self): # Your code here def get_div_count(self): # Your code here def parse_html(html_content): parser = CustomHTMLParser() parser.feed(html_content) parser.close() return (parser.get_links(), parser.get_headings(), parser.get_div_count()) # Example usage html_content = \'\'\'<div><a href=\\"link1.html\\">Link 1</a></div> <div><a href=\\"link2.html\\">Link 2</a> <h1>Heading 1</h1> <h2>Heading 2</h2> <h3>Heading 3</h3>\'\'\' print(parse_html(html_content)) # Output should be: # ([\'link1.html\', \'link2.html\'], [\'Heading 1\', \'Heading 2\', \'Heading 3\'], 2) ``` Please implement the functions and classes as described. The provided example demonstrates how your implementation is expected to behave.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.links = [] self.headings = [] self.div_count = 0 self.current_heading_tag = None def handle_starttag(self, tag, attrs): if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.links.append(attr[1]) elif tag in [\'h1\', \'h2\', \'h3\']: self.current_heading_tag = tag elif tag == \'div\': self.div_count += 1 def handle_endtag(self, tag): if tag in [\'h1\', \'h2\', \'h3\']: self.current_heading_tag = None def handle_data(self, data): if self.current_heading_tag: self.headings.append(data.strip()) def get_links(self): return self.links def get_headings(self): return self.headings def get_div_count(self): return self.div_count def parse_html(html_content): parser = CustomHTMLParser() parser.feed(html_content) parser.close() return (parser.get_links(), parser.get_headings(), parser.get_div_count())"},{"question":"# Question: Secure Email Sender As a software engineer, you are tasked with developing a secure automated email sender service that sends an email from a company\'s email address (`from_email`) to a list of recipient email addresses (`to_emails`). You need to ensure that the email is sent using a secure TLS connection. Implement a function `send_secure_email(from_email: str, to_emails: List[str], subject: str, body: str, smtp_server: str, port: int, username: str, password: str) -> Dict[str, Tuple[int, str]]` that performs the following: 1. Establishes a secure (TLS) connection to the given SMTP server. 2. Authenticates using the provided username and password. 3. Sends the email with the given `subject` and `body`. 4. Handles any SMTP-related exceptions and returns a dictionary where the keys are email addresses that failed to receive the email, and the values are tuples containing the SMTP error code and error message. Input: - `from_email` (str): Sender\'s email address. - `to_emails` (List[str]): List of recipient email addresses. - `subject` (str): Subject of the email. - `body` (str): Body of the email. - `smtp_server` (str): The SMTP server address. - `port` (int): The port to use for the SMTP server. - `username` (str): Username for authenticating with the SMTP server. - `password` (str): Password for authenticating with the SMTP server. Output: - Dict[str, Tuple[int, str]]: A dictionary where each key is an email address that failed to receive the email, and the corresponding value is a tuple containing the SMTP error code and error message. Constraints: - Use the `smtplib` library for implementing the solution. - Ensure TLS is used for the SMTP connection. - Handle exceptions such as `SMTPHeloError`, `SMTPAuthenticationError`, `SMTPNotSupportedError`, and `SMTPRecipientsRefused`. Example: ```python from typing import List, Dict, Tuple import smtplib from email.mime.text import MIMEText def send_secure_email(from_email: str, to_emails: List[str], subject: str, body: str, smtp_server: str, port: int, username: str, password: str) -> Dict[str, Tuple[int, str]]: failed_recipients = {} msg = MIMEText(body) msg[\'Subject\'] = subject msg[\'From\'] = from_email msg[\'To\'] = \\", \\".join(to_emails) try: with smtplib.SMTP(smtp_server, port) as server: server.starttls() server.login(username, password) failed_recipients = server.sendmail(from_email, to_emails, msg.as_string()) except smtplib.SMTPHeloError as e: print(f\\"HELO error: {e}\\") except smtplib.SMTPAuthenticationError as e: print(f\\"Authentication error: {e}\\") except smtplib.SMTPNotSupportedError as e: print(f\\"Not supported error: {e}\\") except smtplib.SMTPRecipientsRefused as e: print(f\\"All recipients were refused: {e}\\") return failed_recipients # Example Usage from_email = \\"example@company.com\\" to_emails = [\\"recipient1@example.com\\", \\"recipient2@example.com\\"] subject = \\"Meeting Reminder\\" body = \\"Don\'t forget about the meeting tomorrow!\\" smtp_server = \\"smtp.company.com\\" port = 587 username = \\"example@company.com\\" password = \\"password123\\" failed_deliveries = send_secure_email(from_email, to_emails, subject, body, smtp_server, port, username, password) print(failed_deliveries) ``` In this task, you will ensure that: - The email sending process is secure using TLS. - Properly managing and logging SMTP-related exceptions. - Returning information about failed deliveries.","solution":"from typing import List, Dict, Tuple import smtplib from email.mime.text import MIMEText def send_secure_email(from_email: str, to_emails: List[str], subject: str, body: str, smtp_server: str, port: int, username: str, password: str) -> Dict[str, Tuple[int, str]]: Sends an email securely using TLS and handles potential SMTP errors. Args: from_email (str): Sender\'s email address. to_emails (List[str]): List of recipient email addresses. subject (str): Subject of the email. body (str): Body of the email. smtp_server (str): The SMTP server address. port (int): The port to use for the SMTP server. username (str): Username for authenticating with the SMTP server. password (str): Password for authenticating with the SMTP server. Returns: Dict[str, Tuple[int, str]]: Dictionary of email addresses that failed to receive the email, with error codes and messages. failed_recipients = {} msg = MIMEText(body) msg[\'Subject\'] = subject msg[\'From\'] = from_email msg[\'To\'] = \\", \\".join(to_emails) try: with smtplib.SMTP(smtp_server, port) as server: server.starttls() server.login(username, password) try: server.sendmail(from_email, to_emails, msg.as_string()) except smtplib.SMTPRecipientsRefused as e: failed_recipients = e.recipients except Exception as e: for recipient in to_emails: if recipient not in failed_recipients: failed_recipients[recipient] = (None, str(e)) except smtplib.SMTPHeloError as e: failed_recipients[\'ALL\'] = (None, f\\"HELO error: {e}\\") except smtplib.SMTPAuthenticationError as e: failed_recipients[\'ALL\'] = (e.smtp_code, f\\"Authentication error: {e.smtp_error.decode()}\\") except smtplib.SMTPNotSupportedError as e: failed_recipients[\'ALL\'] = (None, f\\"Not supported error: {e}\\") except Exception as e: failed_recipients[\'ALL\'] = (None, str(e)) return failed_recipients"},{"question":"**Question: Implement a Custom Python C Extension Type** Your task is to create a new Python C extension type called `Person` within a module named `person_module`. The `Person` type should have the following attributes and methods: # Specifications 1. **Attributes**: - `first_name` (string): First name of the person. - `last_name` (string): Last name of the person. - `age` (integer): Age of the person. 2. **Methods**: - `full_name`: Returns the full name of the person as \\"first_name last_name\\". - `birthday`: Increments the `age` of the person by 1 and returns the new age. # Requirements - **Type Safety**: Ensure that `first_name` and `last_name` attributes always contain strings, and `age` is always an integer. Raise appropriate errors if the type requirements are not met. - **Garbage Collection**: Implement appropriate `traverse` and `clear` methods to support Pythonâ€™s cyclic garbage collection. - **Initialization**: - `Person(first_name, last_name, age)`: Initialize a `Person` instance with given `first_name`, `last_name`, and `age`. # Performance Constraints - Ensure the implementation is efficient and follows best practices for managing reference counts and memory. # Expected Functionality Here\'s an example demonstrating how the `Person` type should behave: ```python >>> from person_module import Person # Creating a Person instance >>> john = Person(first_name=\\"John\\", last_name=\\"Doe\\", age=30) # Accessing attributes >>> john.first_name \'John\' >>> john.last_name \'Doe\' >>> john.age 30 # Calling methods >>> john.full_name() \'John Doe\' >>> john.birthday() 31 >>> john.age 31 # Type Safety >>> john.first_name = 123 # Should raise a TypeError >>> john.age = \\"twenty\\" # Should raise a TypeError ``` # Template You are provided with a partial implementation template. Complete the implementation by filling in the necessary parts: ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include \\"structmember.h\\" typedef struct { PyObject_HEAD PyObject *first_name; PyObject *last_name; int age; } PersonObject; // Function prototypes static void Person_dealloc(PersonObject *self); static PyObject *Person_new(PyTypeObject *type, PyObject *args, PyObject *kwds); static int Person_init(PersonObject *self, PyObject *args, PyObject *kwds); static PyObject *Person_full_name(PersonObject *self, PyObject *Py_UNUSED(ignored)); static PyObject *Person_birthday(PersonObject *self, PyObject *Py_UNUSED(ignored)); static int Person_traverse(PersonObject *self, visitproc visit, void *arg); static int Person_clear(PersonObject *self); static PyObject* Person_getfirst_name(PersonObject *self, void *closure); static int Person_setfirst_name(PersonObject *self, PyObject *value, void *closure); static PyObject* Person_getlast_name(PersonObject *self, void *closure); static int Person_setlast_name(PersonObject *self, PyObject *value, void *closure); static PyObject* Person_getage(PersonObject *self, void *closure); static int Person_setage(PersonObject *self, PyObject *value, void *closure); // Method definitions static PyMethodDef Person_methods[] = { {\\"full_name\\", (PyCFunction) Person_full_name, METH_NOARGS, \\"Return the full name of the person\\"}, {\\"birthday\\", (PyCFunction) Person_birthday, METH_NOARGS, \\"Increment the age of the person by 1\\"}, {NULL} /* Sentinel */ }; // Member getter-setter definitions static PyGetSetDef Person_getsetters[] = { {\\"first_name\\", (getter) Person_getfirst_name, (setter) Person_setfirst_name, \\"first name\\", NULL}, {\\"last_name\\", (getter) Person_getlast_name, (setter) Person_setlast_name, \\"last name\\", NULL}, {\\"age\\", (getter) Person_getage, (setter) Person_setage, \\"age\\", NULL}, {NULL} /* Sentinel */ }; static PyTypeObject PersonType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"person_module.Person\\", .tp_doc = \\"Person objects\\", .tp_basicsize = sizeof(PersonObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC, .tp_new = Person_new, .tp_init = (initproc) Person_init, .tp_dealloc = (destructor) Person_dealloc, .tp_traverse = (traverseproc) Person_traverse, .tp_clear = (inquiry) Person_clear, .tp_methods = Person_methods, .tp_getset = Person_getsetters, }; static PyModuleDef personmodule = { PyModuleDef_HEAD_INIT, .m_name = \\"person_module\\", .m_doc = \\"Example module that creates a Person extension type.\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_person_module(void) { PyObject *m; if (PyType_Ready(&PersonType) < 0) return NULL; m = PyModule_Create(&personmodule); if (m == NULL) return NULL; Py_INCREF(&PersonType); if (PyModule_AddObject(m, \\"Person\\", (PyObject *) &PersonType) < 0) { Py_DECREF(&PersonType); Py_DECREF(m); return NULL; } return m; } ``` Complete the above template to meet the specifications and expected functionality described.","solution":"# The following code is a mock implementation in Python to represent the functionality asked in the problem # Description: The actual implementation using the C extension would involve writing and compiling C code # which cannot be executed directly within this Python environment. class Person: def __init__(self, first_name, last_name, age): # type safety checks if not isinstance(first_name, str): raise TypeError(\\"first_name must be a string\\") if not isinstance(last_name, str): raise TypeError(\\"last_name must be a string\\") if not isinstance(age, int): raise TypeError(\\"age must be an integer\\") self.first_name = first_name self.last_name = last_name self.age = age def full_name(self): return f\\"{self.first_name} {self.last_name}\\" def birthday(self): self.age += 1 return self.age # Assume garbage collection and other features are handled correctly in the actual C extension implementation. # This mock Python implementation is provided to illustrate the concept of the Person type requested."},{"question":"# Question: Advanced Data Visualization Using Seaborn You are tasked with creating a comprehensive data visualization that leverages various techniques using the Seaborn library. This will test your understanding of fundamental to advanced functionalities provided by Seaborn. **Data:** You will use the `tips` dataset (which is included in the Seaborn library). **Requirements:** 1. Load the `tips` dataset. 2. Generate a KDE plot overlaid with a rug plot for the `total_bill` column. 3. Create a scatter plot showing `total_bill` against `tip` and overlay it with a rug plot. The rug plot must: - Represent the `smoker` status using different colors (`hue`). - Have a modified height of the rug lines. - Place the rug lines outside of the axes. 4. Use the `diamonds` dataset to: - Create a scatter plot showing `carat` against `price`. - Overlay it with a rug plot: - Use `hue` to represent the `cut` quality. - Use thinner lines and apply alpha blending for better visibility. **Example Visualization:** Your final plot should present a grid layout (2 by 2) where: - The first subplot contains the KDE and rug plot for `total_bill` from `tips`. - The second subplot contains the scatter plot and rug plot for `total_bill` vs `tip` from `tips`. - The third subplot contains the scatter plot for `carat` vs `price` from `diamonds`. - The fourth subplot contains the scatter plot and rug plot for `carat` vs `price` from `diamonds`. **Function Signature:** ```python def plot_seaborn_visualizations(): pass ``` # Constraints - Utilize the Seaborn library for visualizations. - Ensure the plots are aesthetically pleasing with proper titles and labels. - Handle any exceptions that may arise during execution. **Expected Output:** The function should generate a grid of plots as described above with appropriate customizations applied. # Additional Notes: - Make sure to use `sns.set_theme()` to apply the default Seaborn theme for all plots. - Utilize proper documentation and comments within your code to ensure clarity and readability. - The visualization must be clear and accurately reflect the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_seaborn_visualizations(): try: # Set the Seaborn theme for the plots sns.set_theme() # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the subplot grid (2 by 2) fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # KDE and rug plot for total_bill sns.kdeplot(data=tips, x=\\"total_bill\\", ax=axes[0, 0]) sns.rugplot(data=tips, x=\\"total_bill\\", ax=axes[0, 0]) axes[0, 0].set_title(\'KDE and Rug Plot for Total Bill\') # Scatter plot and rug plot for total_bill vs tip with smoker hue scatter = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", ax=axes[0, 1]) sns.rugplot(data=tips, x=\\"total_bill\\", hue=\\"smoker\\", height=0.03, ax=axes[0, 1], rug_kws={\\"linestyle\\": \\"-\\", \\"lw\\": 0.7, \\"clip_on\\": False}) axes[0, 1].set_title(\'Scatter and Rug Plot for Total Bill vs Tip with Smoker Hue\') # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Scatter plot for carat vs price sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", ax=axes[1, 0]) axes[1, 0].set_title(\'Scatter Plot for Carat vs Price\') # Scatter plot and rug plot for carat vs price with cut hue scatter_diamonds = sns.scatterplot(data=diamonds, x=\\"carat\\", y=\\"price\\", hue=\\"cut\\", ax=axes[1, 1]) sns.rugplot(data=diamonds, x=\\"carat\\", hue=\\"cut\\", height=0.02, alpha=0.6, linewidth=0.5, ax=axes[1, 1], rug_kws={\\"linestyle\\": \\"-\\", \\"lw\\": 0.7, \\"clip_on\\": False}) axes[1, 1].set_title(\'Scatter and Rug Plot for Carat vs Price with Cut Hue\') # Adjust layout plt.tight_layout() plt.show() except Exception as e: print(f\\"An error occurred: {e}\\") # Plot function plot_seaborn_visualizations()"},{"question":"# Python Coding Assessment Question **Objective:** You need to develop a Python program that handles file operations in a safe and efficient manner, utilizing the capabilities provided by Python\'s Development Mode for debugging and detecting issues like resource leakage. **Problem Statement:** Write a Python function `process_file_lines(filename: str) -> int` that takes a filename as input and returns the number of lines in that file. The function should: 1. Open and read the file using a context manager. 2. Ensure that all resources are managed properly to avoid any resource leakage. 3. Provide meaningful logs if an error or warning occurs during the file operation. **Requirements:** - The input `filename` will be a string representing a valid file path. - The function should return an integer representing the number of lines in the file. - The solution should ensure that files are explicitly closed after operations. - Additional runtime checks should be enabled to catch potential issues during development (use the Python Development Mode for testing purposes). **Example Usage:** ```python def process_file_lines(filename: str) -> int: # Your implementation here # Example: # Assuming \'test.txt\' contains 50 lines print(process_file_lines(\'test.txt\')) # Output: 50 ``` **Constraints:** - You are not allowed to use any third-party libraries. - You should assume the file exists and is readable. **Hints:** - Remember to handle the file using a `with` statement to ensure it\'s properly closed after operations. - Test your implementation by running it with Python Development Mode enabled to catch any resource warnings. **Performance Requirements:** - The function should be efficient and must not hold onto resources longer than necessary. **Evaluation Criteria:** - Correctness: The function should return the accurate number of lines in the file. - Resource Management: Proper handling of resources (i.e., files must be closed properly). - Adherence to Guidelines: Enabling Python Development Mode for additional runtime checks and logging.","solution":"def process_file_lines(filename: str) -> int: Opens the given file, counts the number of lines, and returns the count. Parameters: filename (str): The path to the file. Returns: int: The number of lines in the file. try: with open(filename, \'r\') as file: lines = file.readlines() return len(lines) except Exception as e: print(f\\"An error occurred while processing the file: {e}\\") return 0"},{"question":"You are tasked with designing a User Authentication System that securely hashes and verifies passwords using the `crypt` module in Python. Function 1: `generate_hashed_password(password: str) -> str` - **Input**: - `password` (str): The plaintext password to hash. - **Output**: - Returns the hashed password as a string using the strongest available hashing method. Function 2: `verify_password(plain_password: str, hashed_password: str) -> bool` - **Input**: - `plain_password` (str): The plaintext password to verify. - `hashed_password` (str): The hashed password to compare against. - **Output**: - Returns a boolean indicating whether the plaintext password matches the hashed password. Function 3: `generate_salt(method: Optional[crypt.METHOD_*] = None, rounds: Optional[int] = None) -> str` - **Input**: - `method` (Optional[str]): The hashing method to use for generating the salt. If not provided, the strongest available method will be used. - `rounds` (Optional[int]): The number of rounds to use for the `METHOD_SHA256`, `METHOD_SHA512`, or `METHOD_BLOWFISH`. It should be an integer as specified in the documentation. - **Output**: - Returns a generated salt as a string suitable for passing to the `crypt.crypt()` function. Constraints: 1. Using deprecated or \\"shadow\\" passwords is not allowed. 2. Use constant-time comparison to prevent timing attacks when comparing passwords. 3. The `generate_salt` function should default to the strongest method and appropriate number of rounds if not specified. Performance: 1. The password hashing and verification should be optimized for security and should work within a reasonable time frame given the constraints and default methods provided in the `crypt` module. # Example Usage ```python import crypt from hmac import compare_digest as compare_hash def generate_hashed_password(password): # Implementation here salt = crypt.mksalt(method=crypt.METHOD_SHA512) return crypt.crypt(password, salt) def verify_password(plain_password, hashed_password): # Implementation here return compare_hash(crypt.crypt(plain_password, hashed_password), hashed_password) def generate_salt(method=None, rounds=None): # Implementation here return crypt.mksalt(method, rounds) # Example: hashed_pw = generate_hashed_password(\\"my_secure_password\\") is_valid = verify_password(\\"my_secure_password\\", hashed_pw) print(is_valid) # Should print True # Generating a custom salt: custom_salt = generate_salt(method=crypt.METHOD_BLOWFISH, rounds=16) print(custom_salt) ``` Implement the above functions to build a robust User Authentication System for securely handling password storage and verification.","solution":"import crypt from hmac import compare_digest as compare_hash def generate_hashed_password(password: str) -> str: Generates a hashed password using the strongest available method. :param password: The plaintext password to hash. :return: The hashed password. salt = crypt.mksalt(method=crypt.METHOD_SHA512) return crypt.crypt(password, salt) def verify_password(plain_password: str, hashed_password: str) -> bool: Verifies a plaintext password against a hashed password. :param plain_password: The plaintext password to verify. :param hashed_password: The hashed password to compare against. :return: True if the passwords match, False otherwise. return compare_hash(crypt.crypt(plain_password, hashed_password), hashed_password) def generate_salt(method=None, rounds=None) -> str: Generates a salt using the specified method and rounds. :param method: The hashing method to use for generating the salt. Defaults to strongest available method. :param rounds: The number of rounds to use for the method. Defaults to suitable value if not specified. :return: The generated salt. return crypt.mksalt(method=method, rounds=rounds if rounds else None)"},{"question":"You are provided with a dataset containing information about car crashes in different states of the USA. Using this dataset, you are required to create a heatmap that visualizes the rate of car accidents in various states and further customize it to enhance readability. This task will assess your understanding of seaborn\'s heatmap capabilities and your ability to customize plots effectively. **Dataset Description:** - The dataset is available in the Seaborn library as `car_crashes`. - The dataset includes columns like: `total`, `speeding`, `alcohol`, `not_distracted`, `no_previous`, `ins_premium`, `ins_losses`, and `abbrev`. **Instructions:** 1. Load the `car_crashes` dataset from the Seaborn library. 2. Prepare the data to generate a pivot table where: - The rows represent different states (`abbrev`). - The columns represent the categories: `speeding`, `alcohol`, `not_distracted`, `no_previous`. - The values are the corresponding rates for these categories. 3. Create a heatmap using Seaborn to visualize this pivot table. 4. Enhance the readability of the heatmap by: - Applying cell annotations. - Formatting the annotations to show values with one decimal point. - Adding lines between cells. - Using a colormap that provides a good visual contrast. - Setting the Colormap norm to highlight differences more clearly. **Expected Input:** - No input parameters are required. Your function should handle the dataset loading internally. **Expected Output:** - A seaborn heatmap plot representing the processed data with the above customizations. Your task is to implement the function `generate_car_crashes_heatmap()` that performs these steps. Ensure your code is well-documented, and each step is clearly explained. ```python import seaborn as sns import matplotlib.pyplot as plt def generate_car_crashes_heatmap(): # Load the dataset crashes = sns.load_dataset(\\"car_crashes\\") # Prepare the pivot table pivot_table = crashes.pivot(index=\\"abbrev\\", columns=None, values=[\\"speeding\\", \\"alcohol\\", \\"not_distracted\\", \\"no_previous\\"]) # Create the heatmap plt.figure(figsize=(10, 8)) heatmap = sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\", linewidths=0.5, cmap=\\"YlGnBu\\", vmin=pivot_table.min().min(), vmax=pivot_table.max().max()) # General plot settings heatmap.set_xlabel(\\"Categories\\") heatmap.set_ylabel(\\"States\\") plt.title(\\"Heatmap of Car Crash Rates by State\\") # Show plot plt.show() # Calling the function for testing purposes generate_car_crashes_heatmap() ``` **Constraints:** - Ensure that the heatmap is correctly annotated and formatted as specified. - Choose an appropriate colormap to enhance visual interpretation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_car_crashes_heatmap(): Generates and displays a heatmap visualizing car crash rates for various categories in different states using the Seaborn car_crashes dataset. # Load the dataset crashes = sns.load_dataset(\\"car_crashes\\") # Prepare the pivot table pivot_table = crashes.pivot_table(index=\\"abbrev\\", values=[\\"speeding\\", \\"alcohol\\", \\"not_distracted\\", \\"no_previous\\"], aggfunc=\'mean\') # Create the heatmap plt.figure(figsize=(12, 8)) heatmap = sns.heatmap(pivot_table, annot=True, fmt=\\".1f\\", linewidths=0.5, cmap=\\"YlGnBu\\", vmin=pivot_table.min().min(), vmax=pivot_table.max().max()) # General plot settings heatmap.set_xlabel(\\"Categories\\") heatmap.set_ylabel(\\"States\\") plt.title(\\"Heatmap of Car Crash Rates by State\\") # Show plot plt.show() # Calling the function for testing purposes generate_car_crashes_heatmap()"},{"question":"Dynamic Class Creation and Manipulation In this assessment, you are required to dynamically create and manipulate classes using the `types` module in Python. You will implement a function called `create_dynamic_class` which dynamically creates a class based on given specifications and adds dynamic methods to it. Function Signature ```python def create_dynamic_class(name: str, bases: tuple, method_definitions: dict, class_variables: dict) -> type: Dynamically creates a class with given name, base classes, methods, and class variables. :param name: Name of the class to be created. :param bases: Tuple containing base classes of the class to be created. :param method_definitions: Dictionary where keys are method names and values are functions. :param class_variables: Dictionary containing class variables and their respective values. :return: The dynamically created class. ``` Parameters - `name` (str): The name of the class to be created. - `bases` (tuple): A tuple containing the base classes of the class. - `method_definitions` (dict): A dictionary where keys are method names and values are function objects that define the methods to be added to the class. - `class_variables` (dict): A dictionary containing class variables and their initial values. Returns - The dynamically created class (type). Constraints 1. The class must be created using `types.new_class`. 2. The methods should be added to the class namespace. 3. Class variables should be added to the class namespace. 4. All names and elements in the dictionaries will be valid and properly formatted. Example ```python def my_method(self): return f\\"Hello from {self.__class__.__name__}\\" # Define the method definitions methods = { \'my_method\': my_method } # Define class variables variables = { \'class_var\': 42 } # Create the class DynamicClass = create_dynamic_class(\'DynamicClass\', (), methods, variables) # Instantiate the class and use its methods and variables instance = DynamicClass() print(instance.my_method()) # Output: Hello from DynamicClass print(instance.class_var) # Output: 42 ``` Guidance To achieve this, you may need to: 1. Use `types.new_class` to create the class dynamically. 2. Define an `exec_body` function that populates the class namespace with given methods and variables. 3. Return the newly created class from the function. Note Make sure to handle all scenarios where the provided class name, methods, and variables are valid as per the Python naming conventions.","solution":"import types def create_dynamic_class(name: str, bases: tuple, method_definitions: dict, class_variables: dict) -> type: Dynamically creates a class with given name, base classes, methods, and class variables. :param name: Name of the class to be created. :param bases: Tuple containing base classes of the class to be created. :param method_definitions: Dictionary where keys are method names and values are functions. :param class_variables: Dictionary containing class variables and their respective values. :return: The dynamically created class. def exec_body(ns): # Add methods to the class namespace ns.update(method_definitions) # Add class variables to the class namespace ns.update(class_variables) return types.new_class(name, bases, {}, exec_body)"},{"question":"Objective Demonstrate your comprehension of different plotting contexts in `seaborn` and how to apply them to generate visualizations with varying styles. Problem Statement You are provided with a dataset containing scores of students in three subjects: Mathematics, Physics, and Chemistry. Your task is to: 1. Retrieve and print the default plotting context parameters. 2. Generate a line plot to compare the scores of five students in the three subjects. Use the predefined \\"talk\\" context for this visualization. 3. Visualize the same data using the \\"paper\\" context within a context manager to temporarily change the plotting parameters. Dataset ```python data = { \'Student\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\'], \'Mathematics\': [88, 92, 80, 89, 94], \'Physics\': [85, 90, 78, 88, 92], \'Chemistry\': [90, 85, 85, 87, 93] } ``` Expected Output 1. Print the default plotting context parameters. 2. Show a line plot comparing the scores of the five students in the three subjects using the \\"talk\\" context. 3. Show a line plot comparing the scores of the five students in the three subjects using the \\"paper\\" context within a context manager. Constraints - Use `seaborn` and `matplotlib` libraries for visualization. - Ensure that the plots are labeled appropriately with titles, axis labels, and legends. # Implementation Write a Python function `generate_plots(data: dict) -> None` that takes a dictionary representing the dataset and performs the required visualizations. Function Signature ```python def generate_plots(data: dict) -> None: pass ``` Example ```python data = { \'Student\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\'], \'Mathematics\': [88, 92, 80, 89, 94], \'Physics\': [85, 90, 78, 88, 92], \'Chemistry\': [90, 85, 85, 87, 93] } generate_plots(data) ``` Notes - Use the function `sns.plotting_context()` to retrieve and print the default plotting context parameters. - Use the function `sns.lineplot()` for plotting the data. - To apply a context manager, use the `with` statement along with `sns.plotting_context()`. - Add necessary labels, titles, and legends to ensure the plots are informative.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_plots(data: dict) -> None: # Print the default plotting context parameters default_context = sns.plotting_context() print(\\"Default plotting context parameters: \\") print(default_context) # Convert data dictionary to a DataFrame for easier plotting import pandas as pd df = pd.DataFrame(data) # Set \'Student\' column as index for easier plotting with seaborn df.set_index(\'Student\', inplace=True) # Generate a line plot with \'talk\' context sns.set_context(\\"talk\\") plt.figure(figsize=(10, 6)) sns.lineplot(data=df) plt.title(\\"Student Scores in Three Subjects (Talk Context)\\") plt.xlabel(\\"Students\\") plt.ylabel(\\"Scores\\") plt.legend(title=\'Subjects\') plt.show() # Generate a line plot with \'paper\' context within a context manager with sns.plotting_context(\\"paper\\"): plt.figure(figsize=(10, 6)) sns.lineplot(data=df) plt.title(\\"Student Scores in Three Subjects (Paper Context)\\") plt.xlabel(\\"Students\\") plt.ylabel(\\"Scores\\") plt.legend(title=\'Subjects\') plt.show()"},{"question":"# Python Coding Assessment Question Objective: Create a function that demonstrates your ability to work with ZIP files using the `zipfile` module in Python. Your solution should show competency in creating ZIP archives, adding files, listing contents, extracting files, and handling potential errors. Problem Statement: Implement a function `zip_operations` that performs a series of operations on ZIP archives. The function should take three arguments: 1. `zip_filename`: A string representing the name of the ZIP file to be created or manipulated. 2. `files_to_add`: A list of strings representing the filenames to be added to the ZIP file. 3. `extract_path`: A string representing the directory to which files will be extracted. The function should: 1. Create a new ZIP file named `zip_filename`. 2. Add all files specified in `files_to_add` to the ZIP file. Assume these files exist in the current working directory. 3. List and print the names of all files in the newly created ZIP file. 4. Extract all files from the ZIP file to the directory specified by `extract_path`. 5. Handle any exceptions that may arise during these operations and print appropriate error messages. Requirements: - Use the `zipfile.ZipFile` class for creating and manipulating the ZIP file. - Use appropriate methods to add files, list contents, and extract files. - Ensure the function handles errors gracefully, printing relevant messages for the user. Constraints: - All files in `files_to_add` are guaranteed to exist in the current working directory. - `extract_path` is a valid directory path. Example Usage: ```python def zip_operations(zip_filename: str, files_to_add: list, extract_path: str) -> None: # Your code here # Example: zip_operations(\\"test_archive.zip\\", [\\"file1.txt\\", \\"file2.txt\\"], \\"extracted_files/\\") ``` **Expected Output:** ``` Adding files to ZIP archive... Listing contents of the ZIP archive: file1.txt file2.txt Extracting files from ZIP archive... Extraction completed successfully. ``` **Note**: The output messages are for illustration. You may format them as you see fit. Tips: - Use `ZipFile.write` to add files. - Use `ZipFile.namelist` to list file names in the archive. - Use `ZipFile.extractall` to extract files. - Use try-except blocks to manage potential errors during file operations.","solution":"import zipfile import os def zip_operations(zip_filename: str, files_to_add: list, extract_path: str) -> None: try: # Create a new ZIP file with zipfile.ZipFile(zip_filename, \'w\') as zipf: print(\\"Adding files to ZIP archive...\\") for file in files_to_add: zipf.write(file) print(f\\"Added {file} to ZIP\\") # List contents of the ZIP file with zipfile.ZipFile(zip_filename, \'r\') as zipf: print(\\"Listing contents of the ZIP archive:\\") for name in zipf.namelist(): print(name) # Extract all files to the specified extraction path with zipfile.ZipFile(zip_filename, \'r\') as zipf: print(\\"Extracting files from ZIP archive...\\") zipf.extractall(path=extract_path) print(\\"Extraction completed successfully.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are given a directory containing various sound files of different formats. Your task is to write a Python function that: 1. Scans all sound files in the given directory. 2. Uses the `sndhdr` module to determine the type and metadata of each sound file. 3. Returns a summary of the sound files in the directory, including the count of each file type and a list of files that could not be identified. Function Signature ```python def summarize_sound_files(directory_path: str) -> dict: Scans all sound files in the specified directory and summarizes their types using sndhdr. Args: - directory_path (str): The path to the directory containing sound files. Returns: - dict: A dictionary with: - `success_summary` (dict): A dictionary mapping file types to the count of files for that type. - `failed_files` (list): A list of filenames that could not be identified. pass ``` Input - `directory_path` (str): The path to the directory containing sound files. Assume the directory contains only sound files. Output - A dictionary with two keys: 1. `success_summary` (dict): A dictionary mapping file types (str) to the count of files (int) for that type. 2. `failed_files` (list): A list of filenames (str) that could not be identified by `sndhdr`. Constraints - The function should handle any number of files in the directory. - If a file type cannot be determined by `sndhdr`, it should be counted in the `failed_files` list. - Avoid using deprecated features if possible. Example ```python directory_path = \\"path/to/sound/files\\" summary = summarize_sound_files(directory_path) # Example output: # { # \\"success_summary\\": { # \\"wav\\": 5, # \\"aiff\\": 3, # \\"au\\": 2, # }, # \\"failed_files\\": [ # \\"unknown1.snd\\", # \\"unknown2.voc\\", # ] # } ``` Notes - Use the `os` module to handle file system operations. - Ensure your function is efficient, especially when handling a large number of files. - The function should be robust and handle potential errors gracefully, such as permission issues or non-existent directories. Hint You might find the `sndhdr.what()` function particularly useful for determining the file type and metadata.","solution":"import os import sndhdr def summarize_sound_files(directory_path: str) -> dict: Scans all sound files in the specified directory and summarizes their types using sndhdr. Args: - directory_path (str): The path to the directory containing sound files. Returns: - dict: A dictionary with: - success_summary (dict): A dictionary mapping file types to the count of files for that type. - failed_files (list): A list of filenames that could not be identified. success_summary = {} failed_files = [] try: for filename in os.listdir(directory_path): file_path = os.path.join(directory_path, filename) file_info = sndhdr.what(file_path) if file_info is not None: file_type = file_info[0] if file_type in success_summary: success_summary[file_type] += 1 else: success_summary[file_type] = 1 else: failed_files.append(filename) except Exception as e: print(f\\"Error while processing the directory: {e}\\") return { \\"success_summary\\": success_summary, \\"failed_files\\": failed_files, }"},{"question":"# ZIP Archive Creator You are tasked with creating a Python function to programmatically generate a ZIP archive containing a specific set of files and directories. Your function should use the `zipfile` module to achieve this. **Function Signature:** ```python def create_zip_archive(zip_filename: str, files: Dict[str, Union[str, bytes]], compress_type: int = zipfile.ZIP_DEFLATED) -> None: Creates a ZIP archive with the specified files and directories. Parameters: - zip_filename (str): The name of the output ZIP file. - files (Dict[str, Union[str, bytes]]): A dictionary where keys are the desired paths within the ZIP file and values are either filenames (str) to be added or raw data (bytes) to be written as files. - compress_type (int): The compression method to be used. Defaults to zipfile.ZIP_DEFLATED. Returns: - None ``` # Input 1. `zip_filename`: A string representing the name of the ZIP file to be created. 2. `files`: A dictionary where the keys are paths as strings, and the values are either: - A filename (as a string) to be read and added to the archive. - Raw data (as bytes) to be written directly into the archive under the corresponding key. 3. `compress_type`: An integer representing the compression method to use (e.g., `zipfile.ZIP_DEFLATED`). # Output - Your function should return `None` but must create a ZIP file at the specified location. # Constraints - If the specified `zip_filename` already exists, it should be overwritten. - If any error occurs during creating the ZIP file, the function should raise an appropriate exception. - The function should handle directories being created within the ZIP file if the file paths include directories. - The compression type must be one of `ZIP_STORED`, `ZIP_DEFLATED`, `ZIP_BZIP2`, or `ZIP_LZMA`. # Example ```python from zipfile import ZIP_DEFLATED files_to_add = { \'documents/file1.txt\': \'path/to/local/file1.txt\', \'documents/file2.txt\': b\'This is some raw byte data for file2\', \'images/logo.png\': \'path/to/local/logo.png\' } create_zip_archive(\'output.zip\', files_to_add, compress_type=ZIP_DEFLATED) ``` This example should create a ZIP file named `output.zip` containing: - `documents/file1.txt` sourced from `path/to/local/file1.txt` on disk, - `documents/file2.txt` containing the raw byte data provided, - `images/logo.png` sourced from `path/to/local/logo.png`. # Requirements - Use the `zipfile.ZipFile` class. - Consider using `ZipInfo` when required. - Ensure that the directory structure within the ZIP file is properly created.","solution":"import zipfile from typing import Dict, Union import os def create_zip_archive(zip_filename: str, files: Dict[str, Union[str, bytes]], compress_type: int = zipfile.ZIP_DEFLATED) -> None: Creates a ZIP archive with the specified files and directories. Parameters: - zip_filename (str): The name of the output ZIP file. - files (Dict[str, Union[str, bytes]]): A dictionary where keys are the desired paths within the ZIP file and values are either filenames (str) to be added or raw data (bytes) to be written as files. - compress_type (int): The compression method to be used. Defaults to zipfile.ZIP_DEFLATED. Returns: - None with zipfile.ZipFile(zip_filename, \'w\', compression=compress_type) as zipf: for zip_path, content in files.items(): # Ensure the directory structure within the ZIP file is maintained dir_name = os.path.dirname(zip_path) if dir_name and not dir_name.endswith(\'/\'): dir_name += \'/\' if isinstance(content, str): # Adding a file from a local path zipf.write(content, zip_path) elif isinstance(content, bytes): # Writing raw byte data to the specified path in the ZIP file zip_info = zipfile.ZipInfo(zip_path) zipf.writestr(zip_info, content) else: raise ValueError(\\"Value must be either a filename (str) or raw data (bytes)\\")"},{"question":"Objective You are required to write a function that reads a binary file containing structured data and extracts meaningful information, converting it into a human-readable format. This will assess your understanding of the `struct` module for packing and unpacking binary data in Python. Problem Statement A binary file `data.bin` contains a sequence of records, each following a specific format: - A 4-byte integer (representing an ID). - A 10-byte ASCII string (representing a name, padded with null bytes if shorter than 10 characters). - A 1-byte unsigned integer (representing an age). - A 4-byte float (representing a salary). You need to write a function `read_records(file_path: str) -> List[Tuple[int, str, int, float]]` that reads this binary file and returns a list of tuples, where each tuple contains the ID, name, age, and salary for a record. Input - `file_path`: A string representing the path to the binary file. Output - A list of tuples, each tuple containing: - An integer (ID). - A string (Name). - An integer (Age). - A float (Salary). Constraints - The binary file may contain multiple records, and you must read all of them. - Each record follows the specified format sequentially without any delimiter. Implementation Details 1. Use the `struct` module to define the format string for packing/unpacking records. 2. Read the entire binary file in one go. 3. Iterate through the binary data, unpacking each record according to the specified format. 4. Convert the unpacked data into the desired output format and return the list of tuples. Example Assume `data.bin` contains binary data corresponding to the following records: - (1, \\"Alice\\", 30, 50000.0) - (2, \\"Bob\\", 25, 60000.0) ``` Input: \\"data.bin\\" Output: [(1, \'Alice\', 30, 50000.0), (2, \'Bob\', 25, 60000.0)] ``` Coding Template ```python import struct from typing import List, Tuple def read_records(file_path: str) -> List[Tuple[int, str, int, float]]: # Define the format string for a single record record_format = \'i10sBf\' # 4-byte int, 10-byte string, 1-byte unsigned int, 4-byte float record_size = struct.calcsize(record_format) records = [] # Open the binary file for reading with open(file_path, \'rb\') as file: file_content = file.read() # Calculate the number of records num_records = len(file_content) // record_size # Unpack each record for i in range(num_records): record_data = file_content[i * record_size : (i + 1) * record_size] unpacked_data = struct.unpack(record_format, record_data) # Convert binary data to the desired format id = unpacked_data[0] name = unpacked_data[1].decode(\'ascii\').strip(\'x00\') # Remove padding null bytes age = unpacked_data[2] salary = unpacked_data[3] records.append((id, name, age, salary)) return records # Example usage file_path = \'data.bin\' print(read_records(file_path)) ``` Ensure you test your function with a properly formatted binary file to validate its correctness.","solution":"import struct from typing import List, Tuple def read_records(file_path: str) -> List[Tuple[int, str, int, float]]: # Define the format string for a single record record_format = \'i10sBf\' # 4-byte int, 10-byte string, 1-byte unsigned int, 4-byte float record_size = struct.calcsize(record_format) records = [] # Open the binary file for reading with open(file_path, \'rb\') as file: file_content = file.read() # Calculate the number of records num_records = len(file_content) // record_size # Unpack each record for i in range(num_records): record_data = file_content[i * record_size : (i + 1) * record_size] unpacked_data = struct.unpack(record_format, record_data) # Convert binary data to the desired format id = unpacked_data[0] name = unpacked_data[1].decode(\'ascii\').strip(\'x00\') # Remove padding null bytes age = unpacked_data[2] salary = unpacked_data[3] records.append((id, name, age, salary)) return records"},{"question":"**Question:** You are tasked with visualizing a given dataset using seaborn to demonstrate both your understanding of fundamental data visualization techniques and advanced customization using color palettes. # Description Given a CSV file `data.csv` containing columns `X`, `Y`, and `Category`, you need to: 1. Load the data into a Pandas DataFrame. 2. Create a scatter plot of `X` vs `Y`. 3. Color each point according to the `Category` using a specific seaborn color palette. 4. Customize the plot to ensure it is visually appealing and easy to interpret: - Set a title and labels for the axes. - Use a seaborn style for the plot. - Use the `coolwarm` color palette for the points, ensure a legend is visible. - Save the plot as a PNG file named `scatter_plot.png`. # Input - A file named `data.csv` in the current directory containing the columns `X`, `Y`, and `Category`. # Output - A PNG file named `scatter_plot.png` saved in the current directory. # Constraints - You must use seaborn for creating the scatter plot. - You should not use any other visualization library directly (e.g., Matplotlib, Plotly) for creating the plot. Use seaborn as much as possible. # Example Here is a small example of what the `data.csv` file might look like: ``` X,Y,Category 1,2,A 2,3,A 3,4,B 4,5,B 5,6,C ``` # Expected Solution Your code should include the following steps: 1. Load the data. 2. Create the scatter plot. 3. Save it as `scatter_plot.png`. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load data data = pd.read_csv(\'data.csv\') # Set seaborn theme sns.set_theme() # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'X\', y=\'Y\', hue=\'Category\', palette=\'coolwarm\', data=data) # Customize the plot scatter_plot.set_title(\'Scatter Plot of X vs Y\') scatter_plot.set_xlabel(\'X Value\') scatter_plot.set_ylabel(\'Y Value\') scatter_plot.legend(title=\'Category\') # Save the plot as PNG plt.savefig(\'scatter_plot.png\') ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_scatter_plot(): # Load data data = pd.read_csv(\'data.csv\') # Set seaborn theme sns.set_theme() # Create the scatter plot plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'X\', y=\'Y\', hue=\'Category\', palette=\'coolwarm\', data=data) # Customize the plot scatter_plot.set_title(\'Scatter Plot of X vs Y\') scatter_plot.set_xlabel(\'X Value\') scatter_plot.set_ylabel(\'Y Value\') scatter_plot.legend(title=\'Category\') # Save the plot as PNG plt.savefig(\'scatter_plot.png\')"},{"question":"# Rational Number Manipulator You are required to implement a function, `rational_number_operations`, that takes a list of tuples as input and performs a series of operations on fractions using the `fractions` module in Python. Each tuple in the list represents an operation to be performed on either a single Fraction or two Fractions. The function should return a list of results corresponding to each operation. # Input Format The input will be a list of tuples where: 1. Each tuple can be one of the following formats: - `(\'create\', input_value)` where `input_value` can be an integer, string, float, or another Fraction instance as defined in the documentation. This operation creates and returns a Fraction instance. - `(\'add\', input_value1, input_value2)` where `input_value1` and `input_value2` represent two Fraction instances to be added. - `(\'subtract\', input_value1, input_value2)` similar to `add` but performs subtraction. - `(\'multiply\', input_value1, input_value2)` similar to `add` but performs multiplication. - `(\'divide\', input_value1, input_value2)` similar to `add` but performs division. - `(\'ratio\', input_value)` where `input_value` is a Fraction instance. This returns the numerator and denominator as a tuple. - `(\'limit_denominator\', input_value, max_denominator)` where `input_value` is a Fraction and `max_denominator` is an optional argument that limits the denominator (default is 1,000,000). # Output Format The function should return a list of results corresponding to the operations performed. Each result should be represented in its simplest form. The operations should handle exceptions gracefully, returning `None` for any operation that raises an error (e.g., division by zero). # Example ```python def rational_number_operations(operations): from fractions import Fraction result = [] for operation in operations: if operation[0] == \'create\': try: result.append(Fraction(operation[1])) except (ZeroDivisionError, ValueError): result.append(None) elif operation[0] == \'add\': try: result.append(Fraction(operation[1]) + Fraction(operation[2])) except (ZeroDivisionError, ValueError): result.append(None) elif operation[0] == \'subtract\': try: result.append(Fraction(operation[1]) - Fraction(operation[2])) except (ZeroDivisionError, ValueError): result.append(None) elif operation[0] == \'multiply\': try: result.append(Fraction(operation[1]) * Fraction(operation[2])) except (ZeroDivisionError, ValueError): result.append(None) elif operation[0] == \'divide\': try: result.append(Fraction(operation[1]) / Fraction(operation[2])) except (ZeroDivisionError, ValueError): result.append(None) elif operation[0] == \'ratio\': try: frac = Fraction(operation[1]) result.append((frac.numerator, frac.denominator)) except (ZeroDivisionError, ValueError): result.append(None) elif operation[0] == \'limit_denominator\': try: frac = Fraction(operation[1]) max_denom = operation[2] if len(operation) > 2 else 1000000 result.append(frac.limit_denominator(max_denom)) except (ZeroDivisionError, ValueError): result.append(None) return result # Example usage operations = [ (\'create\', \'3/7\'), (\'create\', \'-5.5\'), (\'add\', \'3/7\', 2), (\'subtract\', 2, \'3/7\'), (\'multiply\', 2, \'3/7\'), (\'divide\', \'3/7\', 2), (\'ratio\', \'3/7\'), (\'limit_denominator\', 3.14159, 1000) ] print(rational_number_operations(operations)) # Expected output: # [Fraction(3, 7), Fraction(-11, 2), Fraction(17, 7), Fraction(11, 7), Fraction(6, 7), Fraction(3, 14), (3, 7), Fraction(355, 113)] ``` # Constraints - You have to handle invalid operations gracefully. - The `input_value` must always be a valid representation for Fraction as per the document. - You are encouraged to use the `fractions` module methods effectively to solve the subproblems.","solution":"def rational_number_operations(operations): from fractions import Fraction def safe_fraction(input_value): try: return Fraction(input_value) except (ZeroDivisionError, ValueError): return None result = [] for operation in operations: op_type = operation[0] if op_type == \'create\': result.append(safe_fraction(operation[1])) elif op_type in {\'add\', \'subtract\', \'multiply\', \'divide\'}: frac1 = safe_fraction(operation[1]) frac2 = safe_fraction(operation[2]) if frac1 is not None and frac2 is not None: try: if op_type == \'add\': result.append(frac1 + frac2) elif op_type == \'subtract\': result.append(frac1 - frac2) elif op_type == \'multiply\': result.append(frac1 * frac2) elif op_type == \'divide\': result.append(frac1 / frac2) except ZeroDivisionError: result.append(None) else: result.append(None) elif op_type == \'ratio\': frac = safe_fraction(operation[1]) if frac is not None: result.append((frac.numerator, frac.denominator)) else: result.append(None) elif op_type == \'limit_denominator\': frac = safe_fraction(operation[1]) if frac is not None: max_denom = operation[2] if len(operation) > 2 else 1000000 try: result.append(frac.limit_denominator(max_denom)) except (ZeroDivisionError, ValueError): result.append(None) else: result.append(None) return result"},{"question":"# Manifold Learning Application in Scikit-Learn Problem Statement: Given a high-dimensional dataset, your task is to reduce its dimensions to 2 using Locally Linear Embedding (LLE) and visualize the resultant 2D data. A further task is to evaluate the reconstruction error to determine the effectiveness of the dimension reduction. Tasks: 1. **Implement LLE**: Use scikit-learnâ€™s `LocallyLinearEmbedding` to implement this dimensionality reduction. 2. **Calculate Reconstruction Error**: Evaluate the reconstruction error of the resultant dataset. 3. **Visualization**: Visualize the data before and after dimensionality reduction using scatter plots. 4. **Comparison and Explanation**: Compare the advantages and limitations of using LLE based on the results and document any observations regarding the reconstruction error and the visualizations. Input: 1. A dataset ( mathbf{X} ) of shape (n_samples, n_features). 2. The number of neighbors ( k ) to use for the LLE algorithm. Output: 1. A 2D dataset resulting from the LLE transformation. 2. The reconstruction error of this 2D dataset. 3. Scatter plots of the original high-dimensional data and the LLE-transformed 2D data. 4. Observations and analysis about the effectiveness of the dimensionality reduction using LLE in a markdown cell. Constraints: 1. **Performance**: Efficiently handle datasets with up to 10000 samples and 1000 features. 2. The algorithm should run in reasonable time on standard machine capabilities. 3. Ensure the LLE implementation handles cases where neighbors are fewer than necessary for the dimensionality. Example Usage: ```python import numpy as np from sklearn.manifold import LocallyLinearEmbedding import matplotlib.pyplot as plt # Generate sample high-dimensional data np.random.seed(42) X = np.random.rand(200, 100) # 200 samples, 100 features # Parameters n_neighbors = 10 # Implementing LLE lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=2, method=\'standard\') X_reduced = lle.fit_transform(X) # Calculate reconstruction error reconstruction_error = lle.reconstruction_error_ # Visualization before and after plt.figure(figsize=(12, 6)) plt.subplot(1,2,1) plt.scatter(X[:, 0], X[:, 1]) plt.title(\\"Original Data\\") plt.subplot(1,2,2) plt.scatter(X_reduced[:, 0], X_reduced[:, 1]) plt.title(\\"LLE Reduced Data\\") plt.show() # Output results print(f\\"Reconstruction Error: {reconstruction_error}\\") # Observations and Analysis ``` Assessment Criteria: 1. Correctness: The implementation correctly reduces the high-dimensional dataset to 2D using LLE. 2. Performance: Efficient handling of the dataset within given constraints. 3. Visualizations: Clear and correct visualization of data before and after applying LLE. 4. Explanation: Insightful analysis of the effectiveness of the dimensionality reduction method based on the reconstruction error and scatter plots.","solution":"import numpy as np from sklearn.manifold import LocallyLinearEmbedding import matplotlib.pyplot as plt def apply_lle(X, n_neighbors): Applies Locally Linear Embedding (LLE) to reduce dimensionality of dataset X. Parameters: - X (numpy ndarray): The input high-dimensional data of shape (n_samples, n_features). - n_neighbors (int): The number of neighbors to use for the LLE algorithm. Returns: - X_reduced (numpy ndarray): The 2D dataset resulting from the LLE transformation. - reconstruction_error (float): The reconstruction error of the resultant dataset. lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=2, method=\'standard\') X_reduced = lle.fit_transform(X) reconstruction_error = lle.reconstruction_error_ return X_reduced, reconstruction_error def visualize_data(X, X_reduced): Visualizes the original high-dimensional data and the LLE-reduced 2D data using scatter plots. Parameters: - X (numpy ndarray): The original high-dimensional data. - X_reduced (numpy ndarray): The 2D data resulting from the LLE transformation. plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.scatter(X[:, 0], X[:, 1]) plt.title(\\"Original Data\\") plt.subplot(1, 2, 2) plt.scatter(X_reduced[:, 0], X_reduced[:, 1]) plt.title(\\"LLE Reduced Data\\") plt.show()"},{"question":"You are provided with a dataset regarding restaurant tips and need to create visualizations to analyze the total tips collected per day, categorized by different times of the day and further breakdowns by smoker status. Your task includes utilizing Seaborn\'s `objects` module to create these visualizations. Requirements 1. **Load the `tips` dataset**: ```python from seaborn import load_dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) ``` 2. **Implement a function `create_tip_visualizations` that takes no arguments and generates the following plots**: - A bar plot showing the count of tips per day differentiated by time (`lunch` or `dinner`). - A bar plot showing the sum of total bill amount per day categorized by sex with a gap between the bars. - A dot plot visualizing the data points for the total bill amount per day, dodged by smoker status. 3. **Constraints**: - Use Seaborn\'s `objects` module. - Ensure the plots are correctly labeled and visually distinct. - Handle any potential empty space in the bar plots appropriately. 4. **Expected Output**: - The function should display the three described plots using `matplotlib.pyplot.show()`. Example ```python def create_tip_visualizations(): import seaborn.objects as so import matplotlib.pyplot as plt from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Plot 1: Count of tips per day differentiated by time plot1 = (so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge())) # Plot 2: Sum of total bill amount per day categorized by sex with a gap plot2 = (so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1))) # Plot 3: Dot plot of total bill amount per day, dodged by smoker status plot3 = (so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"smoker\\") .add(so.Dot(), so.Dodge())) # Display the plots plot1.show() plot2.show() plot3.show() plt.show() create_tip_visualizations() ``` This function should create and display the specified visualizations as correctly formatted seaborn objects plots.","solution":"def create_tip_visualizations(): import seaborn.objects as so import matplotlib.pyplot as plt from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Plot 1: Count of tips per day differentiated by time plot1 = (so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge())) # Plot 2: Sum of total bill amount per day categorized by sex with a gap plot2 = (so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\") .add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1))) # Plot 3: Dot plot of total bill amount per day, dodged by smoker status plot3 = (so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"smoker\\") .add(so.Dot(), so.Dodge())) # Display the plots plot1.show() plot2.show() plot3.show() plt.show()"},{"question":"# Advanced Python Coding Assessment **Email Header Parsing and Handling** **Objective**: Implement a function to parse and extract information from different types of email headers, utilizing the `email.headerregistry` module. **Problem Statement**: Write a function `extract_header_info(email_headers: List[str]) -> Dict[str, Dict[str, Any]]` that takes a list of email headers as input and returns a dictionary containing parsed information for each header. The function should support `UnstructuredHeader`, `DateHeader`, `AddressHeader`, and `ContentTypeHeader`. **Function Signature**: ```python from typing import List, Dict, Any def extract_header_info(email_headers: List[str]) -> Dict[str, Dict[str, Any]]: pass ``` **Input**: - `email_headers` (List[str]): A list of email header strings in the format `Header-Name: Header-Value`. **Output**: - A dictionary where each key is the header name, and the value is another dictionary containing parsed information specific to the header type. **Constraints**: 1. The function should correctly identify and handle the four specified header types (`UnstructuredHeader`, `DateHeader`, `AddressHeader`, `ContentTypeHeader`). 2. Use the appropriate classes and methods from the `email.headerregistry` module to parse the headers. 3. Handle any RFC compliance defects by including them in the output dictionary. 4. Assume that the input headers are syntactically correct and focus on parsing and extracting information. **Examples**: ```python email_headers = [ \\"Subject: Hello World\\", \\"Date: Fri, 21 Nov 1997 09:55:06 -0600\\", \\"From: user@example.com\\", \\"Content-Type: text/html; charset=UTF-8\\" ] result = extract_header_info(email_headers) expected_result = { \\"subject\\": { \\"type\\": \\"UnstructuredHeader\\", \\"value\\": \\"Hello World\\" }, \\"date\\": { \\"type\\": \\"DateHeader\\", \\"datetime\\": \\"1997-11-21T09:55:06-0600\\", \\"defects\\": [] }, \\"from\\": { \\"type\\": \\"AddressHeader\\", \\"addresses\\": [\\"user@example.com\\"], \\"defects\\": [] }, \\"content-type\\": { \\"type\\": \\"ContentTypeHeader\\", \\"content_type\\": \\"text/html\\", \\"params\\": {\\"charset\\": \\"UTF-8\\"}, \\"defects\\": [] } } assert result == expected_result ``` **Explanation**: 1. The `Subject` header is parsed using the `UnstructuredHeader` class. 2. The `Date` header is parsed using the `DateHeader`, resulting in a `datetime` object. 3. The `From` header is parsed as an `AddressHeader`, producing a list of addresses. 4. The `Content-Type` header is parsed as a `ContentTypeHeader`, extracting the content type and associated parameters. This question assesses the student\'s ability to work with advanced email handling concepts, subclassing, and correctly utilizing the provided module to achieve the desired functionality.","solution":"from typing import List, Dict, Any from email.policy import default from email.parser import HeaderParser from email.headerregistry import UnstructuredHeader, DateHeader, AddressHeader, ContentTypeHeader from email.utils import parsedate_to_datetime def extract_header_info(email_headers: List[str]) -> Dict[str, Dict[str, Any]]: parser = HeaderParser(policy=default) header_dict = parser.parsestr(\\"n\\".join(email_headers)) result = {} for header in header_dict: header_name = header.lower() header_value = header_dict[header] parsed_info = {} if isinstance(header_value, UnstructuredHeader): parsed_info[\'type\'] = \'UnstructuredHeader\' parsed_info[\'value\'] = str(header_value) elif isinstance(header_value, DateHeader): parsed_info[\'type\'] = \'DateHeader\' parsed_info[\'datetime\'] = parsedate_to_datetime(header_value).isoformat() parsed_info[\'defects\'] = [str(defect) for defect in header_value.defects] elif isinstance(header_value, AddressHeader): parsed_info[\'type\'] = \'AddressHeader\' parsed_info[\'addresses\'] = [str(addr) for addr in header_value.addresses] parsed_info[\'defects\'] = [str(defect) for defect in header_value.defects] elif isinstance(header_value, ContentTypeHeader): parsed_info[\'type\'] = \'ContentTypeHeader\' parsed_info[\'content_type\'] = header_value.content_type parsed_info[\'params\'] = {k: v for k, v in header_value.params.items()} parsed_info[\'defects\'] = [str(defect) for defect in header_value.defects] result[header_name] = parsed_info return result"},{"question":"Objective The goal of this exercise is to assess your understanding of the `hashlib` module in Python, specifically your ability to create secure hash digests, use keyed hashing, and apply concepts like personalization. Problem Statement You are tasked with developing a Python function called `secure_hashing`, which takes a string `message` and an optional `key`. This function should perform the following: 1. If no `key` is provided: - Generate a SHA-256 hash of the `message`. - Return the generated hash in its hexadecimal format. 2. If a `key` is provided: - Generate an HMAC (Hash-based Message Authentication Code) using the `message` and `key` with the BLAKE2b hashing algorithm. - Return the HMAC in its hexadecimal format. Additionally, the function should allow personalized hashing with the BLAKE2b algorithm using a personalized string. Function Signature ```python def secure_hashing(message: str, key: str = None, personalization: str = \'python310\') -> str: pass ``` Inputs - `message` (str): The input message that needs to be hashed. - `key` (str, optional): The key to be used for HMAC. Default is `None`. - `personalization` (str): A string for personalizing the hash. Default is `\'python310\'`. Outputs - (str): Returns the hash or HMAC of the input message in its hexadecimal format. Constraints - The `message` and `key` strings will have a length of at least 1 and at most 1,000 characters. - The `personalization` string will have a length of at least 1 and at most 50 characters. - All input strings will contain only printable ASCII characters. Example Usage ```python # Example 1: Without key print(secure_hashing(\\"hello world\\")) # Output: SHA-256 hash of \\"hello world\\" in hexadecimal. # Example 2: With key print(secure_hashing(\\"hello world\\", key=\\"secret\\")) # Output: HMAC of \\"hello world\\" with key \\"secret\\" using BLAKE2b in hexadecimal. # Example 3: With key and personalization print(secure_hashing(\\"hello world\\", key=\\"secret\\", personalization=\\"custom\\")) # Output: Personalized HMAC of \\"hello world\\" with key \\"secret\\" using BLAKE2b in hexadecimal. ``` Notes - Make use of the `hashlib` module functionalities to implement this function. - Be sure to include robust error handling for edge cases (e.g., empty strings).","solution":"import hashlib import hmac def secure_hashing(message: str, key: str = None, personalization: str = \'python310\') -> str: if key is None: sha256_hash = hashlib.sha256() sha256_hash.update(message.encode(\'utf-8\')) return sha256_hash.hexdigest() else: blake2b_hash = hashlib.blake2b(digest_size=64, key=key.encode(\'utf-8\'), person=personalization.encode(\'utf-8\')) blake2b_hash.update(message.encode(\'utf-8\')) return blake2b_hash.hexdigest()"},{"question":"**Question: Decision Threshold Tuning with scikit-learn** **Objective:** Implement a Python function using scikit-learn to train a binary classification model and tune its decision threshold to optimize a specified evaluation metric. **Problem Statement:** You are given a binary classification dataset and need to build a classifier. However, instead of using the default decision threshold, you will tune the decision threshold to optimize a given performance metric. **Your task is to implement the following function:** ```python from sklearn.base import BaseEstimator def tune_decision_threshold(X_train, y_train, base_model: BaseEstimator, metric_name: str, cv_splits: int = 5) -> Tuple[BaseEstimator, float, float]: Trains a binary classification model, tunes the decision threshold for optimal performance based on the metric, and returns the trained model, the optimal threshold, and the best score. Parameters: X_train (np.array or pd.DataFrame): Training feature dataset y_train (np.array or pd.Series): Training target dataset base_model (BaseEstimator): A scikit-learn estimator to be used as the base model for classification metric_name (str): The performance metric to be optimized (e.g., \'f1\', \'recall\') cv_splits (int): The number of cross-validation splits for tuning the threshold (default is 5) Returns: tuned_model (BaseEstimator): The classifier with a tuned decision threshold optimal_threshold (float): The threshold value that optimizes the specified metric best_score (float): The best score achieved for the specified metric pass ``` **Input/Output formats:** - Input: - `X_train`: Feature data (numpy array or pandas DataFrame) - `y_train`: Target data (numpy array or pandas Series) - `base_model`: A scikit-learn classifier instance (e.g., `LogisticRegression()`) - `metric_name`: A string specifying the scikit-learn metric to optimize (e.g., \'precision\', \'recall\') - `cv_splits`: An integer specifying the number of cross-validation splits (default is 5) - Output: - `tuned_model`: Trained model with tuned decision threshold - `optimal_threshold`: Optimal threshold found via cross-validation - `best_score`: Best score achieved for the specified metric **Constraints:** - Implement the function to use `TunedThresholdClassifierCV`. - Handle invalid metric names gracefully with appropriate error messages. - Ensure the function is performant for realistic datasets sizes. **Performance Requirements:** - Efficiently handle datasets with up to 100,000 samples and 100 features using default 5-fold cross-validation. **Example Usage:** ```python from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification X, y = make_classification(n_samples=1000, n_features=20, random_state=42) model = LogisticRegression() # Tune the threshold for recall score tuned_model, optimal_threshold, best_score = tune_decision_threshold(X, y, model, \'recall\') print(f\\"Optimal Threshold: {optimal_threshold}\\") print(f\\"Best Score: {best_score}\\") ```","solution":"import numpy as np from sklearn.model_selection import StratifiedKFold from sklearn.metrics import get_scorer from sklearn.base import BaseEstimator, clone from typing import Tuple def tune_decision_threshold(X_train, y_train, base_model: BaseEstimator, metric_name: str, cv_splits: int = 5) -> Tuple[BaseEstimator, float, float]: Trains a binary classification model, tunes the decision threshold for optimal performance based on the metric, and returns the trained model, the optimal threshold, and the best score. Parameters: X_train (np.array or pd.DataFrame): Training feature dataset y_train (np.array or pd.Series): Training target dataset base_model (BaseEstimator): A scikit-learn estimator to be used as the base model for classification metric_name (str): The performance metric to be optimized (e.g., \'f1\', \'recall\') cv_splits (int): The number of cross-validation splits for tuning the threshold (default is 5) Returns: tuned_model (BaseEstimator): The classifier with a tuned decision threshold optimal_threshold (float): The threshold value that optimizes the specified metric best_score (float): The best score achieved for the specified metric scorer = get_scorer(metric_name) skf = StratifiedKFold(n_splits=cv_splits) best_score = -np.inf optimal_threshold = 0.5 for threshold in np.arange(0.1, 1.0, 0.1): scores = [] for train_idx, val_idx in skf.split(X_train, y_train): X_tr, X_val = X_train[train_idx], X_train[val_idx] y_tr, y_val = y_train[train_idx], y_train[val_idx] model = clone(base_model) model.fit(X_tr, y_tr) proba = model.predict_proba(X_val)[:, 1] preds = (proba >= threshold).astype(int) scores.append(scorer._score_func(y_val, preds)) mean_score = np.mean(scores) if mean_score > best_score: best_score = mean_score optimal_threshold = threshold final_model = clone(base_model) final_model.fit(X_train, y_train) return final_model, optimal_threshold, best_score"},{"question":"**Objective**: Implement a logging function using the `syslog` module that allows configurable logging for an application. Your implementation should demonstrate understanding of logging options, priority levels, and masking. **Problem Statement**: You are required to implement a function `configure_and_log` that logs messages to the Unix system logger based on the provided configuration parameters. This function should showcase the ability to handle different logging configurations, manage priorities, and utilize the masking feature for selective logging. ```python import syslog def configure_and_log(ident, logoption, facility, maskpri, messages): Configure the system logger and log the provided messages. :param ident: A string to prepend to every message. :param logoption: Bit field for logging options. :param facility: The default facility for log messages. :param maskpri: A priority mask to filter log messages. Only messages with priorities set in the mask will be logged. :param messages: A list of tuples where each tuple contains a priority and a message in the format (priority, message). Priority should be one of the syslog defined levels (e.g., syslog.LOG_INFO). Example: configure_and_log(\\"MyApp\\", syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER, syslog.LOG_UPTO(syslog.LOG_WARNING), [ (syslog.LOG_DEBUG, \\"Debug message\\"), (syslog.LOG_INFO, \\"Info message\\"), (syslog.LOG_WARNING, \\"Warning message\\"), ]) # Your implementation here # Example usage configure_and_log(\\"ExampleApp\\", syslog.LOG_PID, syslog.LOG_USER, syslog.LOG_UPTO(syslog.LOG_ERR), [ (syslog.LOG_DEBUG, \\"This debug message should not appear\\"), (syslog.LOG_INFO, \\"This is an info message\\"), (syslog.LOG_ERR, \\"This is an error message\\"), ]) ``` **Functionality Requirements**: 1. Configure the logging system using the `openlog()` function with the provided `ident`, `logoption`, and `facility`. 2. Set the priority mask using `setlogmask(maskpri)`. 3. Log each message in the `messages` list using the `syslog(priority, message)` function. 4. Ensure that messages are logged according to the priority mask specified. **Constraints**: - `ident` is a string with a maximum length of 100 characters. - `logoption` and `facility` must be valid constants defined in the `syslog` module. - `maskpri` must be a valid priority mask created using `syslog.LOG_MASK(pri)` or `syslog.LOG_UPTO(pri)`. - Each message tuple in `messages` must contain a valid priority and a non-empty string message. **Performance Requirements**: - The function should efficiently handle up to 1000 messages in the `messages` list. - Messages should be logged in O(1) time complexity per message.","solution":"import syslog def configure_and_log(ident, logoption, facility, maskpri, messages): Configure the system logger and log the provided messages. :param ident: A string to prepend to every message. :param logoption: Bit field for logging options. :param facility: The default facility for log messages. :param maskpri: A priority mask to filter log messages. Only messages with priorities set in the mask will be logged. :param messages: A list of tuples where each tuple contains a priority and a message in the format (priority, message). Priority should be one of the syslog defined levels (e.g., syslog.LOG_INFO). Example: configure_and_log(\\"MyApp\\", syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER, syslog.LOG_UPTO(syslog.LOG_WARNING), [ (syslog.LOG_DEBUG, \\"Debug message\\"), (syslog.LOG_INFO, \\"Info message\\"), (syslog.LOG_WARNING, \\"Warning message\\"), ]) assert isinstance(ident, str) and len(ident) <= 100, \'ident must be a string with a maximum length of 100 characters.\' syslog.openlog(ident, logoption, facility) syslog.setlogmask(maskpri) for priority, message in messages: assert isinstance(priority, int), \'Priority must be an integer.\' assert isinstance(message, str) and message != \\"\\", \'Message must be a non-empty string.\' syslog.syslog(priority, message) syslog.closelog()"},{"question":"PyCompile Utility Enhancement You are tasked with developing an enhanced utility function using the `py_compile` module to compile multiple Python source files with specific constraints and options. Your implementation should automate the compilation process and provide meaningful feedback. Function Specification **Function Name**: `enhanced_compile` **Parameters**: 1. `file_list` (list of str): A list of source file paths to be compiled. 2. `output_dir` (str): The directory where the compiled bytecode files should be stored. 3. `raise_on_error` (bool): If True, raise `py_compile.PyCompileError` on a compilation error; otherwise, log the error message. 4. `optimization_level` (int): Optimization level to be applied during compilation (default is -1). 5. `invalid_mode` (str): Invalidation mode, one of `\\"TIMESTAMP\\"`, `\\"CHECKED_HASH\\"`, or `\\"UNCHECKED_HASH\\"` (default is `\\"TIMESTAMP\\"`). 6. `quiet` (int): Verbosity level for logging compilation errors, can be 0 (default), 1, or 2. **Returns**: - List of successfully compiled bytecode file paths. **Constraints**: - If `invalid_mode` is not one of `\\"TIMESTAMP\\"`, `\\"CHECKED_HASH\\"`, or `\\"UNCHECKED_HASH\\"`, raise a `ValueError`. - If `output_dir` does not exist, create it. - Ensure no symbolic links or non-regular files are created in `output_dir`. **Example**: ```python file_list = [\\"script1.py\\", \\"script2.py\\"] output_dir = \\"./compiled\\" raise_on_error = False optimization_level = 2 invalid_mode = \\"CHECKED_HASH\\" quiet_level = 1 compiled_files = enhanced_compile(file_list, output_dir, raise_on_error, optimization_level, invalid_mode, quiet_level) print(compiled_files) ``` ```python # Expected Output: # [\'./compiled/script1.cpython-310.pyc\', \'./compiled/script2.cpython-310.pyc\'] ``` # Implementation Details 1. **Import necessary modules**: ```python import py_compile import os from py_compile import PycInvalidationMode ``` 2. **Validate invalidation mode**: ```python if invalid_mode not in {\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"}: raise ValueError(\\"Invalid invalidation mode selected\\") ``` 3. **Ensure output directory exists**: ```python if not os.path.exists(output_dir): os.makedirs(output_dir) ``` 4. **Compile each file in the list**: ```python compiled_files = [] for file in file_list: try: cfile = os.path.join(output_dir, os.path.basename(file) + \\".cpython-310.pyc\\") py_compile.compile(file, cfile, doraise=raise_on_error, optimize=optimization_level, invalidation_mode=getattr(PycInvalidationMode, invalid_mode), quiet=quiet) compiled_files.append(cfile) except (py_compile.PyCompileError, FileExistsError) as e: if raise_on_error: raise e print(f\\"Compilation error for {file}: {e}\\") return compiled_files ``` Craft a complete function based on the specifications to compile files appropriately while considering the enhancements provided.","solution":"import py_compile import os from py_compile import PycInvalidationMode def enhanced_compile(file_list, output_dir, raise_on_error=True, optimization_level=-1, invalid_mode=\\"TIMESTAMP\\", quiet=0): Compiles multiple Python source files to bytecode with specific constraints and options. Parameters: file_list (list of str): A list of source file paths to be compiled. output_dir (str): The directory where the compiled bytecode files should be stored. raise_on_error (bool): If True, raise `py_compile.PyCompileError` on a compilation error; otherwise, log the error message. optimization_level (int): Optimization level to be applied during compilation (default is -1). invalid_mode (str): Invalidation mode, one of \\"TIMESTAMP\\", \\"CHECKED_HASH\\", or \\"UNCHECKED_HASH\\" (default is \\"TIMESTAMP\\"). quiet (int): Verbosity level for logging compilation errors, can be 0 (default), 1, or 2. Returns: list: List of successfully compiled bytecode file paths. Raises: ValueError: If `invalid_mode` is not one of \\"TIMESTAMP\\", \\"CHECKED_HASH\\", or \\"UNCHECKED_HASH\\". # Validate the invalidation mode if invalid_mode not in {\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"}: raise ValueError(\\"Invalid invalidation mode selected\\") # Ensure the output directory exists if not os.path.exists(output_dir): os.makedirs(output_dir) # Compiling each file compiled_files = [] for file in file_list: try: cfile = os.path.join(output_dir, os.path.basename(file) + \\".cpython-310.pyc\\") py_compile.compile(file, cfile, doraise=raise_on_error, optimize=optimization_level, invalidation_mode=getattr(PycInvalidationMode, invalid_mode), quiet=quiet) compiled_files.append(cfile) except py_compile.PyCompileError as e: if raise_on_error: raise e print(f\\"Compilation error for {file}: {e}\\") return compiled_files"},{"question":"# **Programming Assessment: Implementing Internationalization with the gettext Module** Objective: Your task is to implement a small program that demonstrates the use of the \\"gettext\\" module for internationalization and localization, utilizing both the GNU **gettext** API and the class-based API. Problem Statement: Implement a program that allows translating messages based on the chosen language using the \\"gettext\\" module. Your program should: 1. Bind a custom text domain and locale directory using the GNU **gettext** API. 2. Translate a given message using the GNU **gettext** API. 3. Use the class-based API to handle multiple language translations, allowing dynamic switching between languages. 4. Implement fallback mechanisms when translations are not found. Requirements: 1. **Function Name:** `setup_and_translate` 2. **Input Parameters:** - `domain`: (str) The text domain. - `localedir`: (str) The path to the locale directory. - `message`: (str) The message to be translated. - `languages`: (List[str]) List of languages to support (e.g., `[\'en\', \'fr\', \'de\']`) 3. **Output:** - Return a dictionary with languages as keys and translated messages as values. - If a message is not translated for a given language, utilize a fallback mechanism to return the original message. Constraints: - Assume `.mo` files for translations are available in the specified `localedir`. - Handle the scenario where no translations are found gracefully. Example: Consider the `localedir` to have the following structure with the necessary `.mo` files: ``` /path/to/my/language/directory â”œâ”€â”€ en â”‚ â””â”€â”€ LC_MESSAGES â”‚ â””â”€â”€ myapp.mo â”œâ”€â”€ fr â”‚ â””â”€â”€ LC_MESSAGES â”‚ â””â”€â”€ myapp.mo â””â”€â”€ de â””â”€â”€ LC_MESSAGES â””â”€â”€ myapp.mo ``` ```python def setup_and_translate(domain: str, localedir: str, message: str, languages: list) -> dict: import gettext # Bind the domain and set up translation using GNU gettext API gettext.bindtextdomain(domain, localedir) gettext.textdomain(domain) _ = gettext.gettext # Translate using GNU gettext API translations = {} for lang in languages: translation = gettext.translation(domain, localedir=localedir, languages=[lang], fallback=True) translated_message = translation.gettext(message) translations[lang] = translated_message return translations # Example usage: domain = \'myapp\' localedir = \'/path/to/my/language/directory\' message = \'Hello, world!\' languages = [\'en\', \'fr\', \'de\'] translated_messages = setup_and_translate(domain, localedir, message, languages) print(translated_messages) ``` Expected Output (if translations are available): ```python { \'en\': \'Hello, world!\', \'fr\': \'Bonjour, le monde!\', \'de\': \'Hallo, Welt!\' } ``` Note: The actual output will depend on the contents of your `.mo` files.","solution":"def setup_and_translate(domain: str, localedir: str, message: str, languages: list) -> dict: import gettext # Bind the domain and set up translation using GNU gettext API gettext.bindtextdomain(domain, localedir) gettext.textdomain(domain) translations = {} for lang in languages: # Use class-based API for translation translation = gettext.translation(domain, localedir=localedir, languages=[lang], fallback=True) translated_message = translation.gettext(message) translations[lang] = translated_message return translations"},{"question":"# Python Scope and Exception Handling Assessment You are to implement a complex calculator that performs basic arithmetic operations and handles custom exceptions. This will test your understanding of Python\'s execution model, including scope, binding, and exception handling. Task 1. **Function Definition**: - Write a function `complex_calculator(expression)` that takes a string `expression` representing an arithmetic operation (e.g., `\\"5 + 3\\"`, `\\"10 / 0\\"`, `\\"7 * 4\\"`) and returns the result of the operation. - The function should evaluate the operation using the `eval()` function but must handle exceptions properly. 2. **Custom Exception Handling**: - Create a custom exception `DivisionByZeroError` that inherits from Python\'s built-in `Exception` class. - Ensure that if a division by zero is attempted in the expression, the custom exception `DivisionByZeroError` is raised with the message \\"Division by zero is not allowed\\". 3. **Global and Local Scope Handling**: - If the expression contains a variable, it must first check if the variable is defined in a local scope (e.g., passed as part of a dictionary to the `exec()` or `eval()` functions). - If the variable is not in local scope, check the global scope. - If the variable is not found in either scope, raise a `NameError` with an appropriate message. 4. **Nested Functions and Name Resolution**: - Within `complex_calculator`, define a nested function `evaluate_expression` that takes care of evaluating the expression and any associated exceptions. - The nested function should leverage the surrounding scope to resolve names correctly. Input - `expression` (string): A string representing an arithmetic operation. Output - A number representing the result of the evaluated expression. - Raise the appropriate exception if there is an error in the expression. Constraints - The expressions will contain only basic arithmetic operations (`+`, `-`, `*`, `/`). - Expressions can be simple or involve variables, e.g., `a + b`, where `a` and `b` could be provided in the global or a provided local scope. Example ```python # Global scope variable a = 10 def complex_calculator(expression): # Implement your solution here pass try: # provided example usage (local scope) result = complex_calculator(\\"a + 5\\") # Should access the global scope variable \'a\' print(result) # Output should be 15 # Handling exception (division by zero) result = complex_calculator(\\"10 / 0\\") print(result) except DivisionByZeroError as e: print(e) # Output should be \\"Division by zero is not allowed\\" except NameError as e: print(e) # Any name errors should be handled appropriately # Testing with a local scope variable local_vars = {\'b\': 20} exec(\\"result = complex_calculator(\'b + 10\')\\", {}, local_vars) print(local_vars[\'result\']) # Output should be 30 ``` This task ensures the student demonstrates understanding of function implementation, exception handling, scope resolution, and use of Python\'s `eval()` function in a secure manner.","solution":"class DivisionByZeroError(Exception): pass def complex_calculator(expression, local_vars=None): Evaluates an arithmetic expression string and handles custom exceptions. def evaluate_expression(expr, local_vars): try: # Ensure local_vars is a dictionary if provided if local_vars is None: local_vars = {} # Evaluate the expression with local and global scope consideration return eval(expr, {}, local_vars) except ZeroDivisionError: raise DivisionByZeroError(\\"Division by zero is not allowed\\") except NameError as e: raise NameError(f\\"Name error: {e}\\") return evaluate_expression(expression, local_vars)"},{"question":"Quoted-Printable Encoder and Decoder Application You are asked to design a Python script that leverages the `quopri` module to handle encoding and decoding of files using quoted-printable MIME encoding. This is commonly used for email transfer to ensure that data remains intact without alteration during transit. Your task is to implement a mini-application that facilitates these operations based on user input. Requirements: 1. **Function Definition:** - Implement a function `encode_file(input_filepath: str, output_filepath: str, quote_tabs: bool, header_encoding: bool) -> None`: - **Input**: - `input_filepath`: A string representing the path to the input file to be encoded. - `output_filepath`: A string representing the path to the output file where the encoded content will be saved. - `quote_tabs`: A boolean indication whether to encode embedded tabs and spaces. - `header_encoding`: A boolean indication whether to treat spaces as underscores based on RFC 1522. - **Output**: The function does not return anything but writes the encoded content to the provided output file path. - Implement a function `decode_file(input_filepath: str, output_filepath: str, header_encoding: bool) -> None`: - **Input**: - `input_filepath`: A string representing the path to the input file to be decoded. - `output_filepath`: A string representing the path to the output file where the decoded content will be saved. - `header_encoding`: A boolean indication whether to decode underscores as spaces. - **Output**: The function does not return anything but writes the decoded content to the provided output file path. 2. **Constraints**: - The input and output files in both functions must be opened in binary mode. - Files sizes can be large, so consider reading and writing in chunks if applicable. - Handle any possible exceptions that might be raised (e.g., file not found, read/write errors). 3. **Example Usage**: - To encode a file with path `\\"/path/to/inputfile.txt\\"` and save the encoded content to `\\"/path/to/outputfile.txt\\"` without encoding tabs and treating spaces normally: ```python encode_file(\\"/path/to/inputfile.txt\\", \\"/path/to/outputfile.txt\\", quote_tabs=False, header_encoding=False) ``` - To decode a file with path `\\"/path/to/encodedfile.txt\\"` and save the decoded content to `\\"/path/to/decodedfile.txt\\"`, treating underscores as spaces: ```python decode_file(\\"/path/to/encodedfile.txt\\", \\"/path/to/decodedfile.txt\\", header_encoding=True) ``` Additional Information: - Reference the `quopri` Python module and its functions `quopri.encode` and `quopri.decode` for implementation. - Make sure to handle both the encoding and decoding processes correctly based on the provided flags. Performance Constraints: - The solution should efficiently handle files up to 100MB in size. - You are encouraged to optimize reading/writing processes to handle large files efficiently. Submission: - Submit your Python script containing the implemented functions along with a brief README on how to run your script. Good luck!","solution":"import quopri def encode_file(input_filepath: str, output_filepath: str, quote_tabs: bool, header_encoding: bool) -> None: Encodes the input file using quoted-printable encoding and writes the encoded content to the output file. :param input_filepath: Path to the input file. :param output_filepath: Path to the output file. :param quote_tabs: Flag to encode embedded tabs and spaces. :param header_encoding: Flag to treat spaces as underscores, for header encoding. try: with open(input_filepath, \'rb\') as input_file, open(output_filepath, \'wb\') as output_file: quopri.encode(input_file, output_file, quotetabs=quote_tabs, header=header_encoding) except Exception as e: print(f\\"An error occurred during encoding: {e}\\") def decode_file(input_filepath: str, output_filepath: str, header_encoding: bool) -> None: Decodes the input file using quoted-printable decoding and writes the decoded content to the output file. :param input_filepath: Path to the input file. :param output_filepath: Path to the output file. :param header_encoding: Flag to decode underscores as spaces, for header decoding. try: with open(input_filepath, \'rb\') as input_file, open(output_filepath, \'wb\') as output_file: quopri.decode(input_file, output_file, header=header_encoding) except Exception as e: print(f\\"An error occurred during decoding: {e}\\")"},{"question":"Objective To assess your comprehension of Python\'s `multiprocessing.shared_memory` module, implement a solution using shared memory blocks to perform specific inter-process communication tasks. Task Description You are required to utilize the `multiprocessing.shared_memory` module to create a shared memory block and use it to share a NumPy array between two separate processes. Your task is to implement the following: 1. Create a shared memory block capable of holding a given NumPy array. 2. Write data to this shared memory block from the first process. 3. Read and modify the data from this shared memory block in the second process. 4. Ensure proper cleanup of the shared memory resources after they are no longer needed. Specifications 1. **Function Signatures:** - `create_shared_array(array: np.ndarray) -> Tuple[str, int]`: - **Input:** A NumPy array. - **Output:** A tuple containing the name of the created shared memory block and its size. - `write_to_shared_memory(shared_mem_name: str, array: np.ndarray) -> None`: - **Input:** The name of the shared memory block and a NumPy array to write into it. - **Output:** None. - `read_and_modify_shared_memory(shared_mem_name: str, modification_fn: Callable[[np.ndarray], np.ndarray]) -> np.ndarray`: - **Input:** The name of the shared memory block and a function to modify the array. - **Output:** The modified NumPy array. - `cleanup_shared_memory(shared_mem_name: str) -> None`: - **Input:** The name of the shared memory block. - **Output:** None. 2. **Constraints and Requirements:** - **Array Size:** The NumPy array can have up to 10,000 elements. - **Data Types:** The NumPy array elements can be integers or floats. - **Memory Management:** Properly close and unlink the shared memory block to avoid memory leaks. 3. **Performance Requirements:** - Ensure that reading and writing operations are efficient even for larger data sizes within the constraints. 4. **Additional Function:** - Implement a function `modification_fn(arr: np.ndarray) -> np.ndarray` that adds 10 to each element in the array. Example Usage ```python import numpy as np from typing import Tuple, Callable from multiprocessing import Process from your_module import create_shared_array, write_to_shared_memory, read_and_modify_shared_memory, cleanup_shared_memory, modification_fn # Initial NumPy array array = np.array([1, 2, 3, 4, 5]) # Create shared memory shared_mem_name, size = create_shared_array(array) def process_1(): write_to_shared_memory(shared_mem_name, array) def process_2(): modified_array = read_and_modify_shared_memory(shared_mem_name, modification_fn) print(modified_array) # Expected Output: [11, 12, 13, 14, 15] if __name__ == \\"__main__\\": # Start process 1 p1 = Process(target=process_1) p1.start() p1.join() # Start process 2 p2 = Process(target=process_2) p2.start() p2.join() # Cleanup cleanup_shared_memory(shared_mem_name) ``` Submission Details - Implement all the required functions and the `modification_fn`. - Ensure that the code handles shared memory lifecycle properly. - Test your implementation with different input arrays and validate the results. - Submit your code along with a README explaining the steps to run the tests.","solution":"import numpy as np from multiprocessing import shared_memory from typing import Tuple, Callable def create_shared_array(array: np.ndarray) -> Tuple[str, int]: Creates a shared memory block capable of holding the given NumPy array. shm = shared_memory.SharedMemory(create=True, size=array.nbytes) return shm.name, array.nbytes def write_to_shared_memory(shared_mem_name: str, array: np.ndarray) -> None: Writes the given NumPy array into the shared memory block by name. shm = shared_memory.SharedMemory(name=shared_mem_name) shared_array = np.ndarray(array.shape, dtype=array.dtype, buffer=shm.buf) shared_array[:] = array[:] shm.close() def read_and_modify_shared_memory(shared_mem_name: str, modification_fn: Callable[[np.ndarray], np.ndarray]) -> np.ndarray: Reads the data from the shared memory block, applies the modification function, and returns the modified array. shm = shared_memory.SharedMemory(name=shared_mem_name) shared_array = np.ndarray((shm.size // np.dtype(np.float64).itemsize,), dtype=np.float64, buffer=shm.buf) modified_array = modification_fn(shared_array) shared_array[:] = modified_array[:] shm.close() return modified_array def cleanup_shared_memory(shared_mem_name: str) -> None: Closes and unlinks the shared memory block by name. shm = shared_memory.SharedMemory(name=shared_mem_name) shm.close() shm.unlink() def modification_fn(arr: np.ndarray) -> np.ndarray: Adds 10 to each element in the array and returns the modified array. return arr + 10"},{"question":"# Python Configuration Information Extractor Objective You are tasked with creating a script that extracts and displays specific Python configuration information using the `sysconfig` module. Task Implement a function `extract_python_config_info()` that: 1. Returns the current platform\'s name. 2. Returns the current Python version in \\"MAJOR.MINOR\\" format. 3. Retrieves all installation path names and their corresponding paths for the default installation scheme. 4. Returns a dictionary of configuration variables where: - Keys are the names of the configuration variables. - Values are the respective values of the configuration variables. Function Signature ```python def extract_python_config_info() -> tuple: # returns a tuple containing: # 1. platform name (str) # 2. python version (str) # 3. paths dictionary (dict), where keys are path names and values are the paths # 4. config variables dictionary (dict), where keys are variable names and values are their values ``` Example ```python # If running on a Linux machine with Python 3.8 info = extract_python_config_info() print(info) # Possible Output: ( \\"linux-x86_64\\", \\"3.8\\", { \'data\': \'/usr/local\', \'include\': \'/usr/local/include/python3.8\', \'platinclude\': \'/usr/local/include/python3.8\', \'platlib\': \'/usr/local/lib/python3.8/dist-packages\', \'platstdlib\': \'/usr/local/lib/python3.8\', \'purelib\': \'/usr/local/lib/python3.8/dist-packages\', \'scripts\': \'/usr/local/bin\', \'stdlib\': \'/usr/local/lib/python3.8\' }, { \'AR\': \'ar\', \'ARFLAGS\': \'rc\', \'CC\': \'gcc\', ... } ) ``` Constraints - The solution must use the `sysconfig` module only. - The function should handle any platform where Python is supported. - Ensure the values in the dictionaries are accurate based on the `sysconfig` documentation. Performance Requirements - The function should complete in reasonable time for standard supported environments. This question assesses the student\'s ability to work with Python\'s `sysconfig` module to extract various configuration details, applying knowledge of dictionaries, function usage, and module handling in Python.","solution":"import sysconfig import platform def extract_python_config_info(): platform_name = platform.platform() python_version = \\"{}.{}\\".format(*platform.python_version_tuple()[:2]) paths = sysconfig.get_paths() config_vars = sysconfig.get_config_vars() return platform_name, python_version, paths, config_vars"},{"question":"**Question: Asynchronous File Download Manager** **Background:** You are tasked with creating an asynchronous file download manager. Using the asyncio library, the manager should be able to: 1. Establish an event loop. 2. Schedule multiple file download tasks concurrently. 3. Provide a status update on the progress of each file being downloaded. 4. Ensure proper handling of different network events such as connection initiation, data receipt, and completion of download. **Requirements:** 1. Implement a function `async def download_file(url: str, dest: str) -> None:` that takes a URL and a destination filepath as arguments. It should download the file from the provided URL to the destination asynchronously. 2. Implement a function `async def download_manager(urls: list[str], dest_folder: str) -> None:` that takes a list of URLs and a destination folder. It should manage the concurrent downloading of files from these URLs into the destination folder. Use asyncio methods to schedule and monitor these downloads. 3. Provide status updates in the console for each file, indicating when the download starts, the progress at regular intervals (e.g., every 10% of the file), and when it completes. **Constraints:** - Do not use any third-party libraries; you can only use the standard asyncio library and other built-in Python libraries as necessary. - You may assume that the URLs provided will simulate a real download with proper sequence of events including connection, data transfer, etc. - Ensure that your solution handles errors gracefully, such as network interruptions or invalid URLs, without crashing the program. **Performance Considerations:** - Optimize for efficient concurrent downloads by leveraging asyncio functionalities. - Ensure the program remains responsive and provides updates at regular intervals without blocking operations. **Example Usage:** ```python import asyncio if __name__ == \\"__main__\\": urls = [ \\"http://example.com/file1\\", \\"http://example.com/file2\\", \\"http://example.com/file3\\", ] destination_folder = \\"downloads/\\" asyncio.run(download_manager(urls, destination_folder)) ``` Upon execution, the program should output: ``` Starting download: http://example.com/file1 to downloads/file1 Starting download: http://example.com/file2 to downloads/file2 Starting download: http://example.com/file3 to downloads/file3 Progress 10%: http://example.com/file1 Progress 20%: http://example.com/file1 ... File downloaded: http://example.com/file1 to downloads/file1 Progress 10%: http://example.com/file2 ... File downloaded: http://example.com/file2 to downloads/file2 ... ``` Use the provided asyncio methods effectively to manage the tasks and handle events for the download process.","solution":"import asyncio import aiohttp import os async def download_file(url: str, dest: str) -> None: Downloads a file from the given URL to the destination dest asynchronously. async with aiohttp.ClientSession() as session: async with session.get(url) as response: if response.status == 200: total_size = int(response.headers.get(\'content-length\', 0)) chunk_size = total_size // 10 downloaded = 0 with open(dest, \'wb\') as f: async for chunk in response.content.iter_chunked(chunk_size): f.write(chunk) downloaded += len(chunk) progress = (downloaded / total_size) * 100 print(f\\"Progress {progress:.2f}%: {url}\\") print(f\\"File downloaded: {url} to {dest}\\") else: print(f\\"Failed to download {url} with status code {response.status}\\") async def download_manager(urls: list[str], dest_folder: str) -> None: Manages the concurrent downloading of files from the given list of URLs into the destination folder. if not os.path.exists(dest_folder): os.makedirs(dest_folder) tasks = [] for url in urls: dest = os.path.join(dest_folder, os.path.basename(url)) tasks.append(asyncio.ensure_future(download_file(url, dest))) await asyncio.gather(*tasks) # Uncomment the following code to test # if __name__ == \\"__main__\\": # urls = [ # \\"http://example.com/file1\\", # \\"http://example.com/file2\\", # \\"http://example.com/file3\\", # ] # destination_folder = \\"downloads/\\" # asyncio.run(download_manager(urls, destination_folder))"},{"question":"**Title: Context Management with `contextvars` in Asynchronous Environment** **Question:** You are working on an asynchronous web server and want to track the unique ID of each request within the asynchronous tasks handling the request. Use the `contextvars` module to manage request-specific context. **Objective:** Implement a function `handle_request_with_context(reader, writer)` that manages request-specific context using `contextvars` and operates correctly in an asynchronous environment. **Function Signature:** ```python async def handle_request_with_context(reader, writer): pass ``` **Requirements:** 1. Create a `ContextVar` called `request_id_var` to store the request ID. 2. Implement the function `handle_request_with_context(reader, writer)`: - Extract the request ID (for example, by generating a unique identifier using `uuid.uuid4()`). - Set the request ID in the `request_id_var`. - Implement a loop to read data from the `reader`, process it, and write a response to the `writer` using `writer.write()`. - Ensure that the request ID can be accessed in any subsequent functions called within the task. 3. After processing all data, output a goodbye message indicating the request ID. **Constraints:** - You must use `contextvars` for managing context-specific state. - Ensure that the solution is thread-safe and asynchronous. **Performance Requirements:** - The function should handle multiple concurrent requests efficiently. **Example Usage:** ```python import uuid import asyncio import contextvars request_id_var = contextvars.ContextVar(\'request_id\') async def handle_request_with_context(reader, writer): request_id = uuid.uuid4() request_id_var.set(request_id) while True: data = await reader.readline() if not data: # End of request break writer.write(data.upper()) goodbye_message = f\'Goodbye, request {request_id_var.get()}!\'.encode() writer.write(goodbye_message) await writer.drain() writer.close() async def main(): server = await asyncio.start_server( handle_request_with_context, \'127.0.0.1\', 8888) async with server: await server.serve_forever() asyncio.run(main()) ``` In this example, each client connection handled by `handle_request_with_context` has a unique `request_id` that is managed through `contextvars`. Ensure your implementation meets the outlined requirements and constraints, and write necessary tests to verify its correctness.","solution":"import uuid import asyncio import contextvars # Declare the ContextVar for storing request ID request_id_var = contextvars.ContextVar(\'request_id\') async def handle_request_with_context(reader, writer): Handles an incoming request by reading from the `reader`, processing the data, writing the response to the `writer`, and managing a request-specific context using contextvars. request_id = uuid.uuid4() request_id_var.set(request_id) print(f\\"Handling request: {request_id}\\") while True: data = await reader.readline() if not data: # End of request break writer.write(data.upper()) await writer.drain() # Ensure all data is output before reading more goodbye_message = f\'Goodbye, request {request_id_var.get()}!n\'.encode() writer.write(goodbye_message) await writer.drain() writer.close() # For asynchronous mock testing async def mock_reader_writer(input_data): reader = asyncio.StreamReader() writer_transport, writer_protocol = await asyncio.get_event_loop().connect_read_pipe( asyncio.streams.StreamReaderProtocol, asyncio.StreamReader() ) writer = asyncio.StreamWriter(writer_transport, writer_protocol, reader, asyncio.get_event_loop()) reader_protocol = asyncio.StreamReaderProtocol(reader, loop=asyncio.get_event_loop()) await asyncio.get_event_loop().connect_write_pipe(lambda: reader_protocol, writer_transport) writer_transport.write(input_data.encode()) writer_transport.write_eof() writer_transport.close() return reader, writer"},{"question":"You are tasked with creating a file type identifier system for a fictional project named \\"FileInspector\\". This system needs to intelligently determine the file type based on file names and also suggest appropriate file extensions given a MIME type. Requirements: 1. Implement a function `initialize_mime_types()` that initializes the MIME types database. Ensure that this initialization uses any custom MIME type maps from a list of provided filenames. 2. Implement a function `file_type_inspector(file_path: str, strict: bool = True) -> tuple` that, given a path-like object, returns the MIME type and encoding. 3. Implement a function `identify_extensions(mime_type: str, strict: bool = True) -> list` that, given a MIME type, returns a list of possible file extensions. If the type cannot be identified, return an empty list. 4. Implement a function `add_custom_type(mime_type: str, extension: str, strict: bool = True)` that adds a custom MIME type to the database. 5. Implement a class `AdvancedFileInspector` that allows custom MIME type databases. This class should support: - Initialization from custom MIME types databases. - A method `guess_mime_type(file_path: str, strict: bool = True) -> tuple` that returns the guessed MIME type and encoding. - A method `list_possible_extensions(mime_type: str, strict: bool = True) -> list` that returns all possible file extensions for a given MIME type. Function and Class Definitions: ```python def initialize_mime_types(custom_files: list) -> None: # Your code here def file_type_inspector(file_path: str, strict: bool = True) -> tuple: # Your code here def identify_extensions(mime_type: str, strict: bool = True) -> list: # Your code here def add_custom_type(mime_type: str, extension: str, strict: bool = True) -> None: # Your code here class AdvancedFileInspector: def __init__(self, custom_files: list = None): # Your code here def guess_mime_type(self, file_path: str, strict: bool = True) -> tuple: # Your code here def list_possible_extensions(self, mime_type: str, strict: bool = True) -> list: # Your code here ``` # Constraints: 1. You can assume that the provided file paths and MIME types are valid and correctly formatted. 2. You should handle both standard and non-standard types based on the `strict` parameter. 3. For the `AdvancedFileInspector` class, if `custom_files` is `None`, it should initialize with default known MIME types. # Example Usages: ```python # Initialize MIME types with custom mappings initialize_mime_types([\'/path/to/custom1.mime\', \'/path/to/custom2.mime\']) # Guess MIME type and encoding print(file_type_inspector(\'example.txt\')) # (\'text/plain\', None) # Identify possible extensions for a MIME type print(identify_extensions(\'text/plain\')) # [\'.txt\', ...] # Add a custom MIME type add_custom_type(\'application/x-example\', \'.example\') # Using the AdvancedFileInspector class inspector = AdvancedFileInspector([\'/path/to/custom1.mime\']) print(inspector.guess_mime_type(\'document.example\')) # (\'application/x-example\', None) print(inspector.list_possible_extensions(\'application/x-example\')) # [\'.example\'] ``` # Notes: - Ensure that your solution handles different operating system environments gracefully. - Your implementation should be efficient and able to work with a reasonably large set of custom MIME types and extensions. - Make sure to use appropriate error handling and edge cases management where necessary.","solution":"import mimetypes def initialize_mime_types(custom_files: list) -> None: Initializes the MIME types database with custom MIME types from provided filenames. for file in custom_files: mimetypes.init([file]) def file_type_inspector(file_path: str, strict: bool = True) -> tuple: Determines the MIME type and encoding of a given file. mime_type, encoding = mimetypes.guess_type(file_path, strict=strict) return mime_type, encoding def identify_extensions(mime_type: str, strict: bool = True) -> list: Returns a list of possible file extensions for a given MIME type. extensions = mimetypes.guess_all_extensions(mime_type, strict=strict) return extensions if extensions else [] def add_custom_type(mime_type: str, extension: str, strict: bool = True) -> None: Adds a custom MIME type to the database. mimetypes.add_type(mime_type, extension, strict=strict) class AdvancedFileInspector: def __init__(self, custom_files: list = None): Initializes the AdvancedFileInspector with custom MIME types databases. if custom_files: initialize_mime_types(custom_files) else: mimetypes.init() def guess_mime_type(self, file_path: str, strict: bool = True) -> tuple: Guesses the MIME type and encoding of a given file. return file_type_inspector(file_path, strict) def list_possible_extensions(self, mime_type: str, strict: bool = True) -> list: Returns all possible file extensions for a given MIME type. return identify_extensions(mime_type, strict)"},{"question":"You are given data representing an employee\'s work log, containing the start and end timestamps for tasks performed over multiple days. Your task is to write a function that calculates the total time spent on each task and represents this as a Series of `Timedelta` objects, along with converting these durations to different units. # Function Signature ```python def calculate_task_durations(log: List[Dict[str, str]]) -> Tuple[pd.Series, pd.Series]: pass ``` # Input - `log`: A list of dictionaries, where each dictionary contains two keys: - `start`: A timestamp string representing the start of a task (format: \'YYYY-MM-DD HH:MM:SS\'). - `end`: A timestamp string representing the end of a task (format: \'YYYY-MM-DD HH:MM:SS\'). # Output - A tuple of two pandas Series: - The first Series contains the total durations for each task as `Timedelta` objects. - The second Series contains the total durations converted to seconds as `timedelta64[s]`. # Constraints - The `start` time for any task will always be before the `end` time. - The timestamps are in a consistent format and are valid datetime strings. # Example ``` python log = [ {\\"start\\": \\"2023-10-01 08:00:00\\", \\"end\\": \\"2023-10-01 10:00:00\\"}, {\\"start\\": \\"2023-10-01 11:00:00\\", \\"end\\": \\"2023-10-01 13:30:00\\"}, {\\"start\\": \\"2023-10-01 14:00:00\\", \\"end\\": \\"2023-10-01 15:45:00\\"}, ] task_durations, task_durations_seconds = calculate_task_durations(log) print(task_durations) # Output: # 0 0 days 02:00:00 # 1 0 days 02:30:00 # 2 0 days 01:45:00 # dtype: timedelta64[ns] print(task_durations_seconds) # Output: # 0 7200 # 1 9000 # 2 6300 # dtype: timedelta64[s] ``` # Notes - Ensure to use pandas\' capabilities to parse the datetime strings, compute timedeltas, and handle series of timedelta objects. - Make sure to handle any potential edge cases such as empty logs gracefully.","solution":"from typing import List, Dict, Tuple import pandas as pd def calculate_task_durations(log: List[Dict[str, str]]) -> Tuple[pd.Series, pd.Series]: Calculate the total time spent on each task in a work log. Parameters: log (List[Dict[str, str]]): A list of dictionaries with \'start\' and \'end\' timestamps. Returns: Tuple[pd.Series, pd.Series]: A tuple containing two pandas Series: - The first series contains the total durations as Timedelta objects. - The second series contains the total durations in seconds as timedelta64[s]. durations = [] for entry in log: start = pd.to_datetime(entry[\'start\']) end = pd.to_datetime(entry[\'end\']) duration = end - start durations.append(duration) durations_series = pd.Series(durations) durations_seconds_series = durations_series.astype(\'timedelta64[s]\') return durations_series, durations_seconds_series"},{"question":"Objective You are tasked with implementing a function that serializes an `EmailMessage` object to a binary representation suitable for SMTP transmission. The function should demonstrate a good understanding of handling different MIME types and ensuring proper ASCII encoding for non-ASCII content. Requirements 1. You need to create a function `serialize_email()` that takes an `EmailMessage` object and serializes it into a binary representation. 2. Ensure that if the content contains non-ASCII characters, it is properly encoded using an ASCII-compatible *Content-Transfer-Encoding*. 3. Use the `BytesGenerator` class for the serialization process. Function Signature ```python from email.message import EmailMessage def serialize_email(msg: EmailMessage, mangle_from: bool = False, maxheaderlen: int = None, cte_type: str = \'8bit\') -> bytes: pass ``` Input - `msg` (EmailMessage): The `EmailMessage` object to be serialized. - `mangle_from` (bool): If True, put a \'>\' character in front of any line in the body that starts with the exact string \\"From \\". Defaults to False. - `maxheaderlen` (int): The maximum header length; if None, default to policy settings. - `cte_type` (str): The content transfer encoding type; can be either \'7bit\' or \'8bit\'. Defaults to \'8bit\'. Output - `serialized_msg` (bytes): The serialized binary representation of the `EmailMessage` object. Constraints - The function should be able to handle large email messages efficiently. - The serialization must comply with MIME standards. Example Usage ```python from email.message import EmailMessage msg = EmailMessage() msg[\'Subject\'] = \'Test email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg.set_content(\'This is a test email.\') serialized_msg = serialize_email(msg, mangle_from=True, maxheaderlen=78, cte_type=\'7bit\') print(serialized_msg) ``` Notes - To test the implementation, you can use various `EmailMessage` objects with different content types and encoding requirements. - Ensure `BytesGenerator` is used to maintain compliance with MIME standards, especially for non-ASCII content. Good luck!","solution":"from email.message import EmailMessage from email.generator import BytesGenerator from io import BytesIO def serialize_email(msg: EmailMessage, mangle_from: bool = False, maxheaderlen: int = None, cte_type: str = \'8bit\') -> bytes: Serializes the EmailMessage object into a binary representation suitable for SMTP transmission. Args: - msg (EmailMessage): The EmailMessage object to be serialized. - mangle_from (bool): Whether to prepend a \'>\' character to lines starting with \\"From \\" in the body. - maxheaderlen (int): Maximum header length; if None, defaults to policy settings. - cte_type (str): Content transfer encoding type; can be either \'7bit\' or \'8bit\'. Returns: - bytes: The serialized binary representation of the EmailMessage object. buffer = BytesIO() generator_kwargs = { \\"mangle_from_\\": mangle_from, \\"maxheaderlen\\": maxheaderlen, \\"policy\\": msg.policy.clone(cte_type=cte_type) } generator = BytesGenerator(buffer, **generator_kwargs) generator.flatten(msg, linesep=\'rn\') return buffer.getvalue()"},{"question":"**Problem Statement:** You are tasked with designing an application that processes an audio file and applies some color transformations based on audio properties. Your goal is to write two main functions: 1. **`read_wav_file()`**: This function will read a WAV file and return its properties including frame rate, number of frames, and a list containing audio frames\' amplitude values. 2. **`apply_color_transformation_to_audio(audio_frames, transformation)`**: This function will take the list of audio frames\' amplitude values returned by `read_wav_file()` and apply a color transformation using the specified transformation type (`\'rgb_to_hsv\'`, `\'hsv_to_rgb\'`, `\'rgb_to_yiq\'`, or `\'yiq_to_rgb\'`). For the sake of simplicity, assume that each amplitude value can be treated as a color component in the range between 0.0 and 1.0. **Expected Input and Output Formats:** 1. **`read_wav_file(file_path: str) -> Tuple[int, int, List[float]]`** - **Input:** - `file_path` (str): Path to the WAV file to be read. - **Output:** - A tuple containing: - Sample rate (int) - Number of frames (int) - List of normalized amplitude values (List of floats, values ranged between 0.0 and 1.0) 2. **`apply_color_transformation_to_audio(audio_frames: List[float], transformation: str) -> List[Tuple[float, float, float]]`** - **Input:** - `audio_frames` (List[float]): List of normalized amplitude values. - `transformation` (str): Type of the color transformation to apply. Acceptable values are: `\'rgb_to_hsv\'`, `\'hsv_to_rgb\'`, `\'rgb_to_yiq\'`, `\'yiq_to_rgb\'`. - **Output:** - Transformed color values based on the chosen transformation, returned as a list of tuples where each tuple represents transformed color components (3 floats per tuple). **Constraints:** - The amplitude values in audio frames are normalized between 0.0 and 1.0. - Frames list can contain up to 10^6 elements. - The specified color transformations are from the `colorsys` module. **Performance Requirements:** - The functions should handle input sizes efficiently within reasonable execution time for up to 1 million frames. **Additional Notes:** You can use the `wave` module to read the WAV files and the `colorsys` module to perform color transformations. Raise appropriate error messages for invalid inputs. **Example usage:** ```python # Example usage of read_wav_file sample_rate, num_frames, frames = read_wav_file(\\"example.wav\\") # Applying rgb_to_hsv transformation to the frames transformed_colors = apply_color_transformation_to_audio(frames, \'rgb_to_hsv\') ```","solution":"import wave import struct from typing import List, Tuple import colorsys def read_wav_file(file_path: str) -> Tuple[int, int, List[float]]: with wave.open(file_path, \'r\') as wav_file: sample_rate = wav_file.getframerate() num_frames = wav_file.getnframes() frames = wav_file.readframes(num_frames) fmt = f\\"{\'h\' * (num_frames * wav_file.getnchannels())}\\" unpacked_frames = struct.unpack(fmt, frames) normalized_frames = [(frame + 32768) / 65535.0 for frame in unpacked_frames] return sample_rate, num_frames, normalized_frames def apply_color_transformation_to_audio(audio_frames: List[float], transformation: str) -> List[Tuple[float, float, float]]: transform_function = { \'rgb_to_hsv\': colorsys.rgb_to_hsv, \'hsv_to_rgb\': colorsys.hsv_to_rgb, \'rgb_to_yiq\': colorsys.rgb_to_yiq, \'yiq_to_rgb\': colorsys.yiq_to_rgb }.get(transformation) if transform_function is None: raise ValueError(\\"Invalid transformation type.\\") transformed_colors = [transform_function(frame, frame, frame) for frame in audio_frames] return transformed_colors"},{"question":"# Question: Implementing and Utilizing Coroutines in Python Objective: Write a program that demonstrates the use of coroutines in Python to handle asynchronous tasks. The goal is to implement an async function that performs I/O-bound tasks and properly manages coroutine objects. Requirements: 1. Implement an async function `fetch_data` that simulates fetching data from a server. This function should: - Accept a single argument `url` (string). - Simulate a network delay of 2 seconds using `await asyncio.sleep(2)`. - Return a dictionary containing the `url` and a simulated response `\\"data\\"`. 2. Create a main function `main` that: - Creates a list of URLs. - Uses `asyncio.gather` to run multiple `fetch_data` coroutines concurrently. - Collects and prints the results. 3. Additionally, implement a function `is_coroutine(obj)` that: - Accepts an argument `obj`. - Returns `True` if the object is a coroutine, otherwise `False`. Constraints: - The solution should handle at least 5 concurrent coroutines. - Ensure proper usage of `async` and `await` keywords. - The program should run efficiently without blocking the main thread. Performance Requirements: - The solution should handle network delays efficiently by using asynchronous programming. - The total execution time should not exceed 4 seconds for handling 5 concurrent coroutines. Input: The main function should generate the following list of URLs: ```python urls = [ \\"http://example.com/a\\", \\"http://example.com/b\\", \\"http://example.com/c\\", \\"http://example.com/d\\", \\"http://example.com/e\\" ] ``` Output: The program should output a list of dictionaries, each containing a URL and its corresponding response. Example output: ``` [ {\'url\': \'http://example.com/a\', \'data\': \'Dummy response\'}, {\'url\': \'http://example.com/b\', \'data\': \'Dummy response\'}, {\'url\': \'http://example.com/c\', \'data\': \'Dummy response\'}, {\'url\': \'http://example.com/d\', \'data\': \'Dummy response\'}, {\'url\': \'http://example.com/e\', \'data\': \'Dummy response\'} ] ``` Example Code Structure: ```python import asyncio async def fetch_data(url): # Simulate network delay await asyncio.sleep(2) return {\\"url\\": url, \\"data\\": \\"Dummy response\\"} def is_coroutine(obj): # Check if the object is a coroutine return asyncio.iscoroutine(obj) async def main(): urls = [ \\"http://example.com/a\\", \\"http://example.com/b\\", \\"http://example.com/c\\", \\"http://example.com/d\\", \\"http://example.com/e\\" ] tasks = [fetch_data(url) for url in urls] results = await asyncio.gather(*tasks) print(results) # Run the main function if __name__ == \\"__main__\\": asyncio.run(main()) ``` Implement the above functions and ensure your code meets all the specified requirements.","solution":"import asyncio async def fetch_data(url): Simulates fetching data from a server with a network delay. await asyncio.sleep(2) return {\\"url\\": url, \\"data\\": \\"Dummy response\\"} def is_coroutine(obj): Checks if the given object is a coroutine. return asyncio.iscoroutine(obj) async def main(): urls = [ \\"http://example.com/a\\", \\"http://example.com/b\\", \\"http://example.com/c\\", \\"http://example.com/d\\", \\"http://example.com/e\\" ] tasks = [fetch_data(url) for url in urls] results = await asyncio.gather(*tasks) print(results) # Run the main function if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Problem Statement:** You are tasked with writing a Python function that validates and extracts information from a list of email addresses. The function must perform the following tasks: 1. Validate the email addresses to ensure they match the standard format. 2. Extract and return the username and domain from valid email addresses. The standard format for email addresses is defined as: ``` <username>@<domain> ``` where: - The `username` can contain letters, digits, dashes (`-`), underscores (`_`), and dots (`.`). However, it must start and end with a letter or a digit. - The `domain` can contain letters, digits, and dots (`.`) but must not start or end with a dot. It should have at least one dot separating the domain name and the top-level domain (e.g., `example.com`). Write a function `validate_and_extract_emails` with the following signature: ```python def validate_and_extract_emails(emails: list) -> list: ``` # Input: - `emails`: A list of strings representing email addresses to be validated. # Output: - A list of tuples. Each tuple contains a valid `username` and `domain` extracted from the email addresses in the format `(username, domain)`. - If an email address is invalid, it should be omitted. # Constraints: - You should use regular expressions to validate and extract parts of the email addresses. - Your solution should be efficient and handle a large number of email addresses. # Example: ```python emails = [\\"valid.email@example.com\\", \\"invalid.email@.com\\", \\"another.valid@example.co.uk\\", \\"invalid@.com\\", \\"valid123@domain123.com\\"] result = validate_and_extract_emails(emails) print(result) # Output: [(\'valid.email\', \'example.com\'), (\'another.valid\', \'example.co.uk\'), (\'valid123\', \'domain123.com\')] ``` **Note**: You must handle edge cases, such as emails with invalid formats.","solution":"import re def validate_and_extract_emails(emails: list) -> list: email_pattern = re.compile(r\'^[a-zA-Z0-9]([a-zA-Z0-9._-]{0,62}[a-zA-Z0-9])?@[a-zA-Z0-9]([a-zA-Z0-9.-]{0,252}[a-zA-Z0-9])?.[a-zA-Z]{2,}\') valid_emails = [] for email in emails: match = email_pattern.match(email) if match: username, domain = email.split(\'@\') if domain.count(\'.\') > 0: valid_emails.append((username, domain)) return valid_emails"},{"question":"# HTML Parsing and Data Extraction You are required to implement a subclass of `HTMLParser` to extract specific types of content from given HTML data. Your task is to write a Python class that parses HTML and extracts all URLs from `href` attributes in `<a>` tags and from `src` attributes in `<img>` tags. Additionally, it should count the number of comments present in the HTML data. # Instructions 1. **Class Definition:** - Define a subclass of `HTMLParser` named `CustomHTMLParser`. 2. **Methods Implementation:** - **Initialization:** Implement the `__init__` method to initialize necessary lists and counters. - **Start Tag Handling:** Override the `handle_starttag` method to extract URLs from `href` and `src` attributes. - **Comment Handling:** Override the `handle_comment` method to count the comments in the HTML data. - **Data Retrieval:** Implement a method `get_extracted_data` that returns a dictionary with keys `\\"urls\\"` and `\\"comments_count\\"`. The value for `\\"urls\\"` is a list of all extracted URLs, and the value for `\\"comments_count\\"` is the integer count of comments found. # Constraints - The URLs should be collected from the `href` attribute of `<a>` tags and the `src` attribute of `<img>` tags. - The HTML input is a well-formed string. - The comments counter should count all occurrences of `<!-- ... -->`. # Example Usage ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.urls = [] self.comments_count = 0 def handle_starttag(self, tag, attrs): if tag == \'a\': for attr, value in attrs: if attr == \'href\': self.urls.append(value) elif tag == \'img\': for attr, value in attrs: if attr == \'src\': self.urls.append(value) def handle_comment(self, data): self.comments_count += 1 def get_extracted_data(self): return {\\"urls\\": self.urls, \\"comments_count\\": self.comments_count} # Example Usage parser = CustomHTMLParser() parser.feed(\'<html><!-- Comment 1 --><head><title>Test</title></head>\' \'<body><a href=\\"https://example.com\\">Link</a>\' \'<img src=\\"image.jpg\\"><!-- Comment 2 --></body></html>\') result = parser.get_extracted_data() print(result) # Output should be {\\"urls\\": [\\"https://example.com\\", \\"image.jpg\\"], \\"comments_count\\": 2} ``` # Notes - Ensure that the parser works efficiently for large HTML documents. - Your implementation should handle a variety of HTML structures and attributes.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.urls = [] self.comments_count = 0 def handle_starttag(self, tag, attrs): if tag == \'a\': for attr, value in attrs: if attr == \'href\': self.urls.append(value) elif tag == \'img\': for attr, value in attrs: if attr == \'src\': self.urls.append(value) def handle_comment(self, data): self.comments_count += 1 def get_extracted_data(self): return {\\"urls\\": self.urls, \\"comments_count\\": self.comments_count}"},{"question":"Title: Task Execution and Aggregation with concurrent.futures **Problem Statement:** You are given a list of integers and your goal is to calculate the factorial of each integer using multiple parallel workers such that the computation time is minimized. To achieve this, you will use the `concurrent.futures` module. **Requirements:** 1. Write a function `compute_factorials(numbers: List[int], max_workers: int) -> List[Optional[int]]` that takes in: - `numbers`: A list of non-negative integers. - `max_workers`: An integer representing the maximum number of worker threads to use. 2. The function should return a list of results where each element corresponds to the factorial of the integer at the same index in the input list. If an integer is negative, the corresponding result should be `None`. 3. Use the `ThreadPoolExecutor` from `concurrent.futures` to handle the parallel computation. **Constraints:** - The input list will have at most 1000 integers. - Each integer will be between -10 and 100 (inclusive). - The solution should be efficient and handle potential exceptions gracefully. - You are not allowed to use any factorial library functions; instead, implement your own helper function to compute factorials. **Example Usage:** ```python from typing import List, Optional def compute_factorials(numbers: List[int], max_workers: int) -> List[Optional[int]]: # Your implementation here # Example input numbers = [5, 10, -3, 7] max_workers = 4 # Expected output: [120, 3628800, None, 5040] print(compute_factorials(numbers, max_workers)) ``` **Hints:** - Create a helper function that computes the factorial of a given number. - Use `concurrent.futures.ThreadPoolExecutor` to submit tasks and gather results. **Evaluation Criteria:** - Correct implementation of the factorial computation. - Proper use of `concurrent.futures.ThreadPoolExecutor` to manage concurrent tasks. - Handling negative numbers appropriately. - Efficient and readable code.","solution":"from typing import List, Optional from concurrent.futures import ThreadPoolExecutor, as_completed def factorial(n: int) -> int: Return the factorial of a given non-negative integer. if n < 0: return None result = 1 for i in range(2, n+1): result *= i return result def compute_factorials(numbers: List[int], max_workers: int) -> List[Optional[int]]: Compute factorials of given non-negative integers using parallel workers. Parameters: - numbers: List of non-negative integers. - max_workers: Maximum number of worker threads to use. Returns: List of factorials of the input numbers. For negative numbers, returns None. with ThreadPoolExecutor(max_workers=max_workers) as executor: futures = {executor.submit(factorial, num): idx for idx, num in enumerate(numbers)} results = [None] * len(numbers) for future in as_completed(futures): idx = futures[future] results[idx] = future.result() return results"},{"question":"# Support Vector Machines in Scikit-learn In this coding assessment, you will need to demonstrate your understanding of the scikit-learn library\'s SVM capabilities by performing classification and regression tasks. You will need to use data preprocessing, model training, prediction, and evaluation techniques effectively. Problem Statement You are provided with two datasets: 1. `classification_data.csv` for a binary classification task. 2. `regression_data.csv` for a regression task. Your task is to: 1. Load and preprocess the data. 2. Train an SVM classifier with an RBF kernel for the classification task. 3. Train an SVM regressor with a linear kernel for the regression task. 4. Evaluate the performance of both models using appropriate metrics. Input - `classification_data.csv`: A CSV file with n_samples rows and m_features + 1 columns, where the last column is the target class (0 or 1). - `regression_data.csv`: A CSV file with n_samples rows and m_features + 1 columns, where the last column is the target variable (continuous value). Output - Classification: Accuracy score of the SVM classifier on a test set. - Regression: Mean Squared Error (MSE) of the SVM regressor on a test set. Constraints - Use 80% of the data for training and 20% for testing. - Normalize/Standardize the datasets appropriately before training the models. - Set random_state=42 wherever required for reproducibility. Steps 1. **Data Loading and Preprocessing** - Load the CSV files into pandas DataFrames. - Split the data into training and testing sets (80% train, 20% test). - Normalize/Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 2. **Classification Task** - Train an SVM classifier with an RBF kernel. - Use `SVC` from `sklearn.svm` with default parameters except for `kernel=\'rbf\'`. - Fit the model on the training data and make predictions on the test data. - Calculate and print the accuracy score using `accuracy_score` from `sklearn.metrics`. 3. **Regression Task** - Train an SVM regressor with a linear kernel. - Use `SVR` from `sklearn.svm` with default parameters except for `kernel=\'linear\'`. - Fit the model on the training data and make predictions on the test data. - Calculate and print the Mean Squared Error (MSE) using `mean_squared_error` from `sklearn.metrics`. Example Code Structure ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, SVR from sklearn.metrics import accuracy_score, mean_squared_error # Load data classification_data = pd.read_csv(\'classification_data.csv\') regression_data = pd.read_csv(\'regression_data.csv\') # Split features and target X_classification = classification_data.iloc[:, :-1] y_classification = classification_data.iloc[:, -1] X_regression = regression_data.iloc[:, :-1] y_regression = regression_data.iloc[:, -1] # Split training and testing data X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_classification, y_classification, test_size=0.2, random_state=42) X_train_regr, X_test_regr, y_train_regr, y_test_regr = train_test_split(X_regression, y_regression, test_size=0.2, random_state=42) # Normalize/Standardize data scaler_cls = StandardScaler() X_train_cls = scaler_cls.fit_transform(X_train_cls) X_test_cls = scaler_cls.transform(X_test_cls) scaler_regr = StandardScaler() X_train_regr = scaler_regr.fit_transform(X_train_regr) X_test_regr = scaler_regr.transform(X_test_regr) # Train SVM classifier svc = SVC(kernel=\'rbf\', random_state=42) svc.fit(X_train_cls, y_train_cls) y_pred_cls = svc.predict(X_test_cls) # Evaluate classifier accuracy = accuracy_score(y_test_cls, y_pred_cls) print(f\'Classification Accuracy: {accuracy}\') # Train SVM regressor svr = SVR(kernel=\'linear\') svr.fit(X_train_regr, y_train_regr) y_pred_regr = svr.predict(X_test_regr) # Evaluate regressor mse = mean_squared_error(y_test_regr, y_pred_regr) print(f\'Regression Mean Squared Error: {mse}\') ``` Make sure to follow these steps and utilize the scikit-learn library effectively to accomplish the tasks.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, SVR from sklearn.metrics import accuracy_score, mean_squared_error def svm_classification_regression(): # Load data classification_data = pd.read_csv(\'classification_data.csv\') regression_data = pd.read_csv(\'regression_data.csv\') # Split features and target X_classification = classification_data.iloc[:, :-1] y_classification = classification_data.iloc[:, -1] X_regression = regression_data.iloc[:, :-1] y_regression = regression_data.iloc[:, -1] # Split training and testing data X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split( X_classification, y_classification, test_size=0.2, random_state=42) X_train_regr, X_test_regr, y_train_regr, y_test_regr = train_test_split( X_regression, y_regression, test_size=0.2, random_state=42) # Normalize/Standardize data scaler_cls = StandardScaler() X_train_cls = scaler_cls.fit_transform(X_train_cls) X_test_cls = scaler_cls.transform(X_test_cls) scaler_regr = StandardScaler() X_train_regr = scaler_regr.fit_transform(X_train_regr) X_test_regr = scaler_regr.transform(X_test_regr) # Train SVM classifier svc = SVC(kernel=\'rbf\', random_state=42) svc.fit(X_train_cls, y_train_cls) y_pred_cls = svc.predict(X_test_cls) # Evaluate classifier accuracy = accuracy_score(y_test_cls, y_pred_cls) print(f\'Classification Accuracy: {accuracy}\') # Train SVM regressor svr = SVR(kernel=\'linear\') svr.fit(X_train_regr, y_train_regr) y_pred_regr = svr.predict(X_test_regr) # Evaluate regressor mse = mean_squared_error(y_test_regr, y_pred_regr) print(f\'Regression Mean Squared Error: {mse}\') return accuracy, mse"},{"question":"XML Processing with Custom ContentHandler in Python # Objective Implement a custom `ContentHandler` to process an XML document and perform specific tasks based on its content. # Task 1. **Define a class `MyContentHandler` that inherits from `xml.sax.handler.ContentHandler`.** 2. **Implement methods to handle the following events:** - **`startDocument`**: Print `\\"Document parsing started.\\"` - **`endDocument`**: Print `\\"Document parsing ended.\\"` - **`startElement`**: Print the name of each element encountered and its attributes. - **`endElement`**: Print the closing tag of each element. - **`characters`**: Collect and print character data within elements. 3. **Parse the provided XML document using this handler and extract the following information:** - **Count the number of each type of element in the document.** - **Extract and print all text content within a specified element (e.g., `<title>`).** # Input 1. The XML document to be processed. (You can use sample data for testing) 2. The specific element name (e.g., `\\"title\\"`) for which text content needs to be extracted. # Output 1. Print statements detailing the start and end of the document, elements encountered, and their attributes. 2. A dictionary showing the count of each type of element encountered. 3. All text content within the specified element. # Constraints 1. The XML document input will be well-formed. 2. The specific element name for text content extraction will always be present in the document. # Example Given the following XML document: ```xml <books> <book> <title>Title 1</title> <author>Author 1</author> </book> <book> <title>Title 2</title> <author>Author 2</author> </book> </books> ``` And specifying the element name `\\"title\\"`, the output should be: ``` Document parsing started. Element: books Element: book Element: title, Attributes: {} Characters: Title 1 End element: title Element: author, Attributes: {} Characters: Author 1 End element: author End element: book Element: book Element: title, Attributes: {} Characters: Title 2 End element: title Element: author, Attributes: {} Characters: Author 2 End element: author End element: book End element: books Document parsing ended. Element Counts: {\'books\': 1, \'book\': 2, \'title\': 2, \'author\': 2} Text content within \'title\': [\'Title 1\', \'Title 2\'] ``` # Implementation Provide your implementation of `MyContentHandler` class using the guidelines given above.","solution":"import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self, target_element): super().__init__() self.target_element = target_element self.element_count = {} self.collect_data = False self.current_data = [] self.result_data = [] def startDocument(self): print(\\"Document parsing started.\\") def endDocument(self): print(\\"Document parsing ended.\\") print(f\\"Element Counts: {self.element_count}\\") print(f\\"Text content within \'{self.target_element}\': {self.result_data}\\") def startElement(self, name, attrs): print(f\\"Element: {name}\\") self.element_count[name] = self.element_count.get(name, 0) + 1 if name == self.target_element: self.collect_data = True if attrs.getLength() > 0: att_data = {attr_name: attrs.getValue(attr_name) for attr_name in attrs.getNames()} print(f\\"Attributes: {att_data}\\") def endElement(self, name): print(f\\"End element: {name}\\") if name == self.target_element and self.collect_data: self.result_data.append(\'\'.join(self.current_data).strip()) self.collect_data = False self.current_data = [] def characters(self, content): if self.collect_data: self.current_data.append(content) if content.strip(): print(f\\"Characters: {content.strip()}\\") def parse_xml(xml_content, target_element): handler = MyContentHandler(target_element) xml.sax.parseString(xml_content, handler) return handler.element_count, handler.result_data"},{"question":"**Question: Customizing Seaborn Plot Appearance** You are given a dataset named `anscombe`, which is a well-known dataset used for statistical visualization. Your task is to create a complex Seaborn plot and customize its appearance using themes and styles. 1. Load the `anscombe` dataset using Seaborn. 2. Create a plot showing the relationship between `x` and `y`, using different colors for each subset of data defined by `dataset`. 3. Facet the plot by the `dataset` variable and wrap the facets into 2 columns. 4. Add a linear regression line (`order=1`) and a scatterplot (dots) to each facet. 5. Customize the theme of the plot by setting: - The background color of the axes to white (`axes.facecolor`). - The edge color of the axes to slategray (`axes.edgecolor`). - The linewidth of lines to 4 (`lines.linewidth`). 6. Apply the \'ticks\' style to the plot using Seaborn\'s `axes_style` function. # Input - None # Output - Display the customized plot. # Code Constraints - You must use the `seaborn.objects` module as shown in the examples. - Ensure that the plot configuration updates do not overwrite each other. # Example Code ```python import seaborn.objects as so from seaborn import load_dataset, axes_style # Step 1: Load the dataset anscombe = load_dataset(\\"anscombe\\") # Step 2: Create the base plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Step 3: Customize the theme p.theme({ \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }) # Step 4: Apply \'ticks\' style p.theme(axes_style(\\"ticks\\")) # Step 5: Display the plot p ``` # Note - The provided dataset is built into Seaborn and can be loaded directly using `load_dataset(\\"anscombe\\")`. - Ensure that all steps are performed as stated in the instructions for full credit. - The final output should be a visualized plot that demonstrates all the customizations.","solution":"import seaborn.objects as so from seaborn import load_dataset, axes_style def create_customized_anscombe_plot(): # Step 1: Load the dataset anscombe = load_dataset(\\"anscombe\\") # Step 2: Create the base plot p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Step 3: Customize the theme p.theme({ \\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 4 }) # Step 4: Apply \'ticks\' style p.theme(axes_style(\\"ticks\\")) # Step 5: Display the plot return p"},{"question":"# PyTorch Multiprocessing Coding Assessment **Objective**: Implement a solution that demonstrates your understanding of multiprocessing in PyTorch using the `torch.distributed.elastic.multiprocessing` module. Problem Statement You are tasked with creating a function that initializes and starts multiple processes to perform a computation task using PyTorch\'s distributed elastic functionalities. The function should perform the following tasks: 1. **Define a computation**: Create a simple computation function that takes an integer as input, performs some operations (e.g., squaring the number), and returns the result. 2. **Start Multiple Workers**: Use the `torch.distributed.elastic.multiprocessing.start_processes` function to start multiple worker processes that each execute the computation function on a different input. 3. **Process Context Management**: Handle the process context using the appropriate classes (`PContext`, `MultiprocessContext`, `SubprocessContext`). 4. **Result Handling**: Collect and return the results from all the worker processes. Requirements 1. **Function Signature**: ```python def start_computation_processes(num_processes: int, inputs: List[int]) -> List[int]: pass ``` 2. **Input**: - `num_processes` (int): The number of worker processes to start. - `inputs` (List[int]): A list of integers to be processed by the worker processes. 3. **Output**: - `List[int]`: A list of results from the computation performed by each worker process. 4. **Constraints**: - The input list length will always be equal to or greater than the number of processes. - You must use the `torch.distributed.elastic.multiprocessing.start_processes` function to initiate the worker processes. - Properly handle the process context using the provided classes (`PContext`, `MultiprocessContext`, `SubprocessContext`). 5. **Performance Requirements**: - Efficient handling of processes and optimal usage of CPU resources. - Ensure the results are collected and returned correctly even if some processes complete early. Example ```python # Example Usage num_processes = 3 inputs = [1, 2, 3, 4, 5] results = start_computation_processes(num_processes, inputs) print(results) # Example output: [1, 4, 9, 16, 25] (if the computation function is squaring the input) ``` **Hints**: - Familiarize yourself with the `torch.distributed.elastic.multiprocessing.start_processes` function and how to use it to start multiple processes. - Understand the different context classes provided in the `torch.distributed.elastic.multiprocessing.api` module to manage the processes correctly.","solution":"import torch.multiprocessing as mp from torch.distributed.elastic.multiprocessing.errors import record @record def computation_fn(rank, input_data, output_data): Function to perform the computation. In this case, it squares the input number. output_data[rank] = input_data[rank] ** 2 def start_computation_processes(num_processes, inputs): # Array to hold the output results output_data = mp.Array(\'i\', [0]*len(inputs)) # Creating processes using multiprocessing processes = [] for rank in range(num_processes): p = mp.Process(target=computation_fn, args=(rank, inputs, output_data)) processes.append(p) p.start() # Ensuring all processes have completed for p in processes: p.join() # Collecting results from the output array return list(output_data)"},{"question":"Background The `urllib.request` module in Python provides a variety of classes and functions to handle URL operations, including fetching content from URLs, handling authentication, and managing errors. One useful aspect of this module is its ability to customize HTTP requests using handlers. Task Design an HTTP client using the `urllib.request` module that can: 1. Fetch content from a URL. 2. Handle basic HTTP authentication. 3. Manage redirects and print the redirected URLs. 4. Support proxy settings if specified. 5. Handle common HTTP errors gracefully by printing appropriate error messages. Implementation Requirements 1. Implement a function `fetch_url_content(url, username=None, password=None, proxy=None)` which: - `url`: The URL to fetch. - `username`: The username for HTTP authentication (optional). - `password`: The password for HTTP authentication (optional). - `proxy`: A dictionary containing proxy settings, e.g., `{\'http\': \'http://proxy.example.com:8080\'}` (optional). 2. Within this function: - Handle HTTP basic authentication using `username` and `password` if provided. - Handle the redirection and maintain a list of all URLs redirected to before reaching the final URL. - If `proxy` is provided, configure the URL opener to use the proxy settings. - Gracefully handle the following HTTP errors: 404 (Not Found), 403 (Forbidden), and 500 (Internal Server Error), printing appropriate error messages. 3. Function should return the content of the final URL as a string. In case of an error, return an empty string after printing the error message. Input Format: - `url`: A string containing the URL to fetch. - `username`: A string containing the username for HTTP authentication (default is `None`). - `password`: A string containing the password for HTTP authentication (default is `None`). - `proxy`: A dictionary containing proxy settings (default is `None`). Output Format: - Return the content of the final URL as a string. Print the error message and return an empty string in case of common HTTP errors like 404, 403, and 500. Example: ```python url = \\"http://www.example.com\\" username = \\"user\\" password = \\"pass\\" proxy = {\\"http\\": \\"http://proxy.example.com:8080\\"} content = fetch_url_content(url, username, password, proxy) print(content) ``` Constraints: - The implementation should handle all mentioned cases, including optional inputs like `username`, `password`, and `proxy`. - Properly handle and print redirections and common HTTP errors. Important Notes: - Utilize `urllib.request.HTTPBasicAuthHandler` for handling HTTP authentication. - Utilize `urllib.request.ProxyHandler` for proxy settings. - Use `urllib.request.HTTPRedirectHandler` for managing redirects. - Handle errors using appropriate HTTP error handlers or custom error messages.","solution":"import urllib.request import urllib.error import urllib.parse def fetch_url_content(url, username=None, password=None, proxy=None): Fetches content from the given URL with optional basic authentication and proxy settings. Handles redirects and common HTTP errors gracefully. # Create an opener to handle various handlers handlers = [] # Handle HTTP Basic Authentication if username and password are provided if username and password: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) auth_handler = urllib.request.HTTPBasicAuthHandler(password_mgr) handlers.append(auth_handler) # Handle Proxy if provided if proxy: proxy_handler = urllib.request.ProxyHandler(proxy) handlers.append(proxy_handler) # Handle Redirects and Print redirected URLs class RedirectHandler(urllib.request.HTTPRedirectHandler): def handle_redirect(self, req, fp, code, msg, headers): new_url = headers[\'location\'] print(f\\"Redirected to: {new_url}\\") return urllib.request.HTTPRedirectHandler().redirect_request(self, req, fp, code, msg, headers) handlers.append(RedirectHandler()) # Create opener with the specified handlers opener = urllib.request.build_opener(*handlers) urllib.request.install_opener(opener) try: with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') return content except urllib.error.HTTPError as e: if e.code == 404: print(\\"Error 404: Not Found\\") elif e.code == 403: print(\\"Error 403: Forbidden\\") elif e.code == 500: print(\\"Error 500: Internal Server Error\\") else: print(f\\"HTTP Error {e.code}: {e.reason}\\") return \\"\\" except urllib.error.URLError as e: print(f\\"URL Error: {e.reason}\\") return \\"\\""},{"question":"**Question: Advanced Decision Tree Implementation and Evaluation** You are tasked with building and evaluating a decision tree classifier using the scikit-learn library. You will use the Iris dataset for this purpose, which is a well-known dataset in the machine learning community. # Task 1. **Load the Iris dataset** using `sklearn.datasets.load_iris`. 2. **Preprocess the Data**: Perform a train-test split on the dataset using `sklearn.model_selection.train_test_split` with a test size of 30%. 3. **Train a Decision Tree Classifier**: - Define a `DecisionTreeClassifier`. - Fit the classifier on the training data. 4. **Evaluate the Classifier**: - Predict the classes on the test data. - Calculate the accuracy of the classifier. - **Optional but recommended**: Visualize the tree using `plot_tree`. # Requirements - **Input**: Your function should not take any inputs but needs to perform the steps outlined above. - **Output**: Print the following: 1. The accuracy score of the classifier. 2. The structure of the decision tree using `export_text`. # Constraints - Use `random_state=42` wherever applicable for reproducibility. - Ensure that the decision tree does not overfit by setting an appropriate value for `max_depth`. # Hints - Use the `classification_report` from `sklearn.metrics` to get a detailed classification report. - Visualizing the tree can help understand how the decision tree is making decisions. # Example: ```python def decision_tree_classifier(): from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text from sklearn.metrics import accuracy_score # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42, max_depth=3) # Train the classifier clf.fit(X_train, y_train) # Predict the test set results y_pred = clf.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") # Export the decision tree structure as text tree_rules = export_text(clf, feature_names=iris.feature_names) print(tree_rules) # Optional: Visualization of the Decision Tree plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) # Execute the function decision_tree_classifier() ``` In this task, you will demonstrate your understanding of loading a dataset, preprocessing it, training a classifier, and evaluating and interpreting the results. This exercise is aimed at reinforcing your knowledge of decision trees and their applications using scikit-learn.","solution":"def decision_tree_classifier(): from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text from sklearn.metrics import accuracy_score # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Initialize the Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42, max_depth=3) # Train the classifier clf.fit(X_train, y_train) # Predict the test set results y_pred = clf.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.2f}\\") # Export the decision tree structure as text tree_rules = export_text(clf, feature_names=iris.feature_names) print(tree_rules) # Optional: Visualization of the Decision Tree plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)"},{"question":"# Advanced Python Set Operations Objective: Write a Python function that processes a list of tuples, where each tuple contains two sets. The goal is to perform specific set operations and return a result as specified below. Function Signature: ```python def process_set_operations(data: list[tuple[set, set]]) -> list[tuple[set, set, bool, set]]: pass ``` Input: - `data`: A list of tuples. Each tuple contains two sets. For example: `[({1, 2}, {2, 3}), ({4, 5}, {5, 6})]`. Output: - A list of tuples. Each tuple should contain: 1. The original two sets. 2. A boolean indicating whether the first set is a subset of the second set. 3. A set that is the union of the two sets. For example: `[({1, 2}, {2, 3}, False, {1, 2, 3}), ({4, 5}, {5, 6}, False, {4, 5, 6})]`. Constraints: - The input sets will have at most 50 elements. - Elements in the sets will be positive integers. Performance Requirements: - The solution should be efficient with a time complexity of O(N), where N is the length of the input list. Example: ```python def process_set_operations(data: list[tuple[set, set]]) -> list[tuple[set, set, bool, set]]: result = [] for s1, s2 in data: is_subset = s1.issubset(s2) union_set = s1.union(s2) result.append((s1, s2, is_subset, union_set)) return result # Example Usage input_data = [ ({1, 2}, {2, 3}), ({4, 5}, {5, 6}) ] output = process_set_operations(input_data) print(output) # Expected Output: [({1, 2}, {2, 3}, False, {1, 2, 3}), ({4, 5}, {5, 6}, False, {4, 5, 6})] ``` Notes: - Demonstrate the usage of sets and frozensets effectively. - Ensure the performance constraints are met. - Include proper type hinting and document the function clearly.","solution":"def process_set_operations(data: list[tuple[set, set]]) -> list[tuple[set, set, bool, set]]: Processes a list of tuples where each tuple contains two sets. For each tuple, the function returns a new tuple with: 1. The original sets, 2. A boolean indicating if the first set is a subset of the second set, 3. A set which is the union of the two sets. :param data: List of tuples, each containing two sets :return: List of tuples, each containing the original sets, a subset boolean, and the union of the sets result = [] for s1, s2 in data: is_subset = s1.issubset(s2) union_set = s1.union(s2) result.append((s1, s2, is_subset, union_set)) return result"},{"question":"# Tensor Storage Manipulation and Analysis in PyTorch Problem Statement You are required to write a function `tensor_storage_analysis` that performs the following tasks: 1. **Create a Tensor**: - Create a 1-dimensional tensor of size 10 with all elements initialized to 1, using the `torch.ones` method. 2. **Access the Tensor\'s Storage**: - Access the untyped storage of the tensor and clone this storage. 3. **Manipulate the Storage**: - Fill the cloned storage with zeros. 4. **Update the Original Tensor**: - Update the original tensor to use the cloned (and now zeroed) storage, keeping the same shape and stride. 5. **Validate the Process**: - Ensure that the original tensor\'s data has been updated to all zeros by checking the data values. 6. **Data Storage Pointer Check**: - Fetch the data pointers of the original tensor\'s storage and the cloned storage, and ensure that they are different. Input - No external input is required for the function. Output - Return a tuple with: 1. The updated tensor after setting the new storage. 2. A boolean indicating whether the tensor data values are all zeros. 3. A boolean indicating whether the data pointers of the original storage and the cloned storage are different. Example ```python import torch def tensor_storage_analysis(): # Step 1: Create a tensor tensor = torch.ones(10) # Step 2: Access the tensor\'s storage original_storage = tensor.untyped_storage() cloned_storage = original_storage.clone() # Step 3: Manipulate the storage cloned_storage.fill_(0) # Step 4: Update the original tensor tensor.set_(cloned_storage, storage_offset=tensor.storage_offset(), stride=tensor.stride(), size=tensor.size()) # Step 5: Validate the process is_tensor_all_zeros = torch.all(tensor == 0).item() # Step 6: Data storage pointer check original_data_ptr = original_storage.data_ptr() cloned_data_ptr = cloned_storage.data_ptr() are_data_ptrs_different = original_data_ptr != cloned_data_ptr return tensor, is_tensor_all_zeros, are_data_ptrs_different # Function call updated_tensor, is_all_zeros, different_data_ptrs = tensor_storage_analysis() print(updated_tensor) # Expected: tensor of size 10, all elements 0 print(is_all_zeros) # Expected: True print(different_data_ptrs) # Expected: True ``` Ensure your solution is efficient and adheres to best practices of writing PyTorch code. Good luck!","solution":"import torch def tensor_storage_analysis(): # Step 1: Create a tensor tensor = torch.ones(10) # Step 2: Access the tensor\'s storage original_storage = tensor.untyped_storage() cloned_storage = original_storage.clone() # Step 3: Manipulate the storage cloned_storage.fill_(0) # Step 4: Update the original tensor tensor.set_(cloned_storage, storage_offset=tensor.storage_offset(), stride=tensor.stride(), size=tensor.size()) # Step 5: Validate the process is_tensor_all_zeros = torch.all(tensor == 0).item() # Step 6: Data storage pointer check original_data_ptr = original_storage.data_ptr() cloned_data_ptr = cloned_storage.data_ptr() are_data_ptrs_different = original_data_ptr != cloned_data_ptr return tensor, is_tensor_all_zeros, are_data_ptrs_different"},{"question":"<|Analysis Begin|> The provided documentation is about the `contextlib` module in Python which supplies utilities for the `with` statement and context managers. The module contains various classes and functions to help manage resources within the `with` context, such as: 1. `AbstractContextManager` and its asynchronous counterpart. 2. The `contextmanager` and `asynccontextmanager` decorators for creating context managers. 3. Several utility context managers like `closing`, `aclosing`, `nullcontext`, `suppress`, `redirect_stdout`, and `redirect_stderr`. 4. The `ContextDecorator` class for creating context managers that can also be used as decorators, and its asynchronous variant. 5. Other advanced structures including `ExitStack` and `AsyncExitStack`, which allow for flexible and programmable resource management within `with` statements. A good assessment question will require the use of multiple features described in the documentation, particularly focusing on context management, proper usage of try/except/finally constructs, and possibly resource management using the `ExitStack`. <|Analysis End|> <|Question Begin|> **Question:** Implement a custom logging utility that manages file handles efficiently. You are required to create a `Logger` context manager class that logs messages into a file. This context manager should: 1. Open the log file in the `__enter__` method if it\'s not already open. 2. Log messages into the opened file. 3. Ensure the file is properly closed in the `__exit__` method, even in cases of exceptions. 4. Use `contextlib.ExitStack` to manage the resources properly. The provided interface can be as follows: ```python import os from contextlib import ExitStack, contextmanager class Logger: def __init__(self, file_name): self.file_name = file_name self.is_open = False self.exit_stack = ExitStack() def log(self, message): if not self.is_open: raise Exception(\\"Log file is not open\\") self.file.write(message + \'n\') def __enter__(self): self.file = self.exit_stack.enter_context(open(self.file_name, \'a\')) self.is_open = True return self def __exit__(self, exc_type, exc_val, exc_tb): self.is_open = False self.exit_stack.pop_all() # file.close() will be called when file gets exited from ExitStack def close_manually(self): # Close the log file manually if needed outside the managed context. if self.is_open: self.exit_stack.close() self.is_open = False ``` **Usage Example:** ```python with Logger(\'my_log.txt\') as logger: logger.log(\\"This is a log entry.\\") logger.log(\\"This is another log entry.\\") # Testing outside with context logger = Logger(\'my_log.txt\') logger.__enter__() try: logger.log(\\"Log entry outside with statement.\\") finally: logger.__exit__(None, None, None) ``` **Expected Input and Output:** 1. Initialize `Logger` with a filename. 2. Use it inside a `with` statement to log messages. 3. It should properly handle closing of the file and managing exceptions within the context. 4. It should support manual opening and closing of the file outside the context manager. **Constraints and Limitations:** - The file should be opened in append mode. - Ensure the file is always closed, even in case of exceptions. - Use `contextlib.ExitStack` to handle resource management.","solution":"import os from contextlib import ExitStack class Logger: def __init__(self, file_name): self.file_name = file_name self.is_open = False self.exit_stack = ExitStack() def log(self, message): if not self.is_open: raise Exception(\\"Log file is not open\\") self.file.write(message + \'n\') def __enter__(self): self.file = self.exit_stack.enter_context(open(self.file_name, \'a\')) self.is_open = True return self def __exit__(self, exc_type, exc_val, exc_tb): self.is_open = False self.exit_stack.close() # file.close() will be called when file gets exited from ExitStack def close_manually(self): # Close the log file manually if needed outside the managed context. if self.is_open: self.exit_stack.close() self.is_open = False"},{"question":"# Objective: Implement a function that reads an existing AU file, modifies its sample rate, and writes the modified audio data to a new AU file using the \\"sunau\\" module. # Task: 1. Write a function `modify_au_sample_rate(input_file: str, output_file: str, new_sample_rate: int) -> None` that: - Takes the path to an input AU file (`input_file`), the path to an output AU file (`output_file`), and a new sample rate (`new_sample_rate`). - Reads the audio data from the input AU file. - Modifies the sample rate to the new value. - Writes the modified audio data to the output AU file. # Requirements: 1. Validate that the input file is a valid AU file. 2. Ensure the modified AU file retains the original audio encoding, number of channels, sample width, and other header information (except for the sample rate). 3. Also, ensure that the audio data itself remains unchanged, only the sample rate value in the header should be modified. 4. Handle any potential exceptions that may be raised during file operations. # Constraints: - The input AU file can be assumed to have a valid header and format. - You can assume the new sample rate is a positive integer. # Example Usage: ```python # Assuming \'input.au\' is an existing AU file and it needs to be processed modify_au_sample_rate(\'input.au\', \'output.au\', 44100) ``` In this example, the function reads `input.au`, modifies its sample rate to 44100 Hz, and writes the modified audio data to `output.au`. # Hints: - Use the `sunau.open()` function to open the AU files. - Use methods from `AU_read` to read information such as the number of channels, sample width, and data frames from the input file. - Use methods from `AU_write` to write the modified audio data to the output file. - Remember to close both the read and write files to ensure all data is properly written and resources are freed.","solution":"import sunau def modify_au_sample_rate(input_file: str, output_file: str, new_sample_rate: int) -> None: Modifies the sample rate of an AU file, retaining the original audio data. Args: input_file (str): Path to the input AU file. output_file (str): Path to the output AU file. new_sample_rate (int): The new sample rate to be set. Returns: None try: # Open the input AU file. with sunau.open(input_file, \'rb\') as infile: n_channels = infile.getnchannels() sampwidth = infile.getsampwidth() framerate = infile.getframerate() n_frames = infile.getnframes() comp_type = infile.getcomptype() comp_name = infile.getcompname() audio_data = infile.readframes(n_frames) # Open the output AU file. with sunau.open(output_file, \'wb\') as outfile: outfile.setnchannels(n_channels) outfile.setsampwidth(sampwidth) outfile.setframerate(new_sample_rate) outfile.setcomptype(comp_type, comp_name) outfile.writeframes(audio_data) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Coding Challenge: Build a Simple Text-Based Notepad with Curses **Objective**: Create a text-based notepad application using the Python `curses` module. Your application should allow the user to type text, navigate through the text using arrow keys, and save the content to a file. **Requirements**: 1. **Initialization**: - Use `curses` to initialize the screen and set it up for text input. - Enable `noecho()` to prevent automatic display of input. - Enable `cbreak()` to react to keypresses immediately. - Enable keypad mode to interpret special keys like arrow keys. 2. **Text Editing**: - Implement basic text editing features including: - Typing text. - Navigating using arrow keys. - Deleting characters using the backspace key. 3. **Display**: - Refresh the screen to display the current content of the notepad whenever changes are made. 4. **Save File**: - Provide a mechanism to save the content to a file when a specific key (e.g., `F2`) is pressed. 5. **Termination**: - Ensure the curses session is properly terminated and the terminal is restored to its original state when exiting the application or when an exception occurs. **Input/Output**: - **Input**: User inputs text and navigational commands via the keyboard. - **Output**: Real-time display of the text in the curses window and saving content to a file when requested. **Constraints**: - Handle the terminal window size dynamically. - Text wrapping is not required, but the display should handle content scrolling if the text exceeds the window size. **Performance Requirements**: - The application should be responsive to user inputs with minimal latency. **Starter Code**: You may start with the following skeleton and build upon it. ```python import curses def main(stdscr): # Initialization code curses.noecho() curses.cbreak() stdscr.keypad(True) # Main application loop while True: stdscr.clear() # Code to handle text input and navigation here stdscr.refresh() ch = stdscr.getch() if ch == ord(\'q\'): break elif ch == curses.KEY_UP: pass # Handle up arrow key elif ch == curses.KEY_DOWN: pass # Handle down arrow key elif ch == curses.KEY_LEFT: pass # Handle left arrow key elif ch == curses.KEY_RIGHT: pass # Handle right arrow key elif ch == 127: # Backspace key pass # Handle backspace key elif ch == curses.KEY_F2: pass # Save the file # Termination code curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() curses.wrapper(main) ``` **Additional Notes**: - Ensure proper exception handling and terminal restoration using `curses.wrapper()`. - You may use any approach to manage the text buffer and display, but ensure it integrates well with curses and performs efficiently. Good luck, and happy coding!","solution":"import curses def save_to_file(content, filename=\\"notepad.txt\\"): Save the content to a file. with open(filename, \\"w\\") as file: file.write(content) def main(stdscr): # Initialization code curses.noecho() curses.cbreak() stdscr.keypad(True) content = [] cursor_y, cursor_x = 0, 0 # Main application loop try: while True: stdscr.clear() for idx, line in enumerate(content): stdscr.addstr(idx, 0, line) stdscr.move(cursor_y, cursor_x) stdscr.refresh() ch = stdscr.getch() if ch == ord(\'q\'): break elif ch == curses.KEY_UP: if cursor_y > 0: cursor_y -= 1 cursor_x = min(cursor_x, len(content[cursor_y])) elif ch == curses.KEY_DOWN: if cursor_y < len(content) - 1: cursor_y += 1 cursor_x = min(cursor_x, len(content[cursor_y])) elif ch == curses.KEY_LEFT: if cursor_x > 0: cursor_x -= 1 elif cursor_y > 0: cursor_y -= 1 cursor_x = len(content[cursor_y]) elif ch == curses.KEY_RIGHT: if cursor_x < len(content[cursor_y]): cursor_x += 1 elif cursor_y < len(content) - 1: cursor_y += 1 cursor_x = 0 elif ch == 127: # Backspace key if cursor_x > 0: content[cursor_y] = content[cursor_y][:cursor_x-1] + content[cursor_y][cursor_x:] cursor_x -= 1 elif cursor_y > 0: cursor_x = len(content[cursor_y-1]) content[cursor_y-1] += content.pop(cursor_y) cursor_y -= 1 elif ch == curses.KEY_F2: save_to_file(\\"n\\".join(content)) elif ch == curses.KEY_RESIZE: # Handling terminal resizing if necessary pass else: # Handle other character inputs if cursor_y >= len(content): content.append(\\"\\") content[cursor_y] = content[cursor_y][:cursor_x] + chr(ch) + content[cursor_y][cursor_x:] cursor_x += 1 finally: # Termination code curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() # Uncomment the following lines to run the notepad application # if __name__ == \\"__main__\\": # curses.wrapper(main)"},{"question":"Question: Implement a Custom Sequence Type in Python # Objective Implement a custom sequence type named `CustomSequence` that mimics the behavior of Pythonâ€™s built-in list. This implementation should support indexing, slicing, length calculation, and contain a string representation for easier debugging and printing. # Requirements: 1. The `CustomSequence` class should store its items in a private list attribute. 2. The class should support: - Indexing (both positive and negative indices). - Slicing. - The `len()` function. - Membership test using the `in` operator. - String representation with `__str__()` and a more official detailed one with `__repr__()`. # Constraints: - The class should handle both integer and slice objects for indexing and slicing. - The implemented methods should raise appropriate exceptions for invalid indices or slices. # Implementations: Implement the following methods: - `__init__(self, iterable=[])`: Initialize the sequence with an optional iterable. - `__getitem__(self, index)`: Retrieve an item or slice from the sequence. - `__len__(self)`: Return the number of items in the sequence. - `__contains__(self, item)`: Check if an item exists in the sequence. - `__str__(self)`: Return a user-friendly string representation of the sequence. - `__repr__(self)`: Return an official string representation of the sequence. # Input and Output: - The constructor should take an optional iterable to initialize the sequence. - Indexing (`seq[i]`) should return the item at the specified index or slice. - `len(seq)` should return the number of items. - The `in` operator should indicate the presence of an item. - `str(seq)` and `repr(seq)` should return string representations. # Example Usage: ```python # Create a CustomSequence instance seq = CustomSequence([1, 2, 3, 4, 5]) # Access elements by index print(seq[1]) # Output: 2 print(seq[-1]) # Output: 5 # Access elements by slice print(seq[1:4]) # Output: CustomSequence([2, 3, 4]) # Get the length of the sequence print(len(seq)) # Output: 5 # Check membership print(3 in seq) # Output: True # String representations print(str(seq)) # Output: CustomSequence: [1, 2, 3, 4, 5] print(repr(seq)) # Output: CustomSequence([1, 2, 3, 4, 5]) ``` Please ensure your code is well-documented and handles edge cases appropriately.","solution":"class CustomSequence: def __init__(self, iterable=[]): self._items = list(iterable) def __getitem__(self, index): if isinstance(index, (int, slice)): return CustomSequence(self._items[index]) if isinstance(index, slice) else self._items[index] else: raise TypeError(\\"Invalid argument type.\\") def __len__(self): return len(self._items) def __contains__(self, item): return item in self._items def __str__(self): return \\"CustomSequence: \\" + str(self._items) def __repr__(self): return f\\"CustomSequence({self._items})\\""},{"question":"You are required to implement a function that processes a list of strings representing numerical values, converts them to `double`, and then formats them into strings based on given format specifications. Additionally, the function should handle errors gracefully and perform case-insensitive string comparisons while ensuring all conversions and comparisons follow locale-independent rules. Function Specification ```python def process_and_format_numbers(input_list, format_code, precision, flags): This function takes a list of strings, each representing a numerical value, converts them to double, and then formats them as strings according to the specified format. Args: input_list (list of str): A list of strings where each string is expected to represent a floating-point number. format_code (str): A format code character (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\') defining the formatting style. precision (int): The number of digits after the decimal for \'f\' & \'F\', or significant digits for \'g\' & \'G\', and ignored for \'r\'. flags (int): An integer representing flags (bitwise or of Py_DTSF_SIGN, Py_DTSF_ADD_DOT_0, Py_DTSF_ALT). Returns: dict: A dictionary with the original strings as keys and their formatted string conversions as values. In case of errors, the value will be an error message string. Py_DTSF_SIGN = 1 Py_DTSF_ADD_DOT_0 = 2 Py_DTSF_ALT = 4 Py_DTST_FINITE = 0 Py_DTST_INFINITE = 1 Py_DTST_NAN = 2 result_dict = {} # Your implementation here return result_dict ``` Input - `input_list`: List of strings, each representing a numerical value (e.g., [\\"1.23\\", \\"4.56e-2\\", \\"NaN\\"]). - `format_code`: A character specifying the format (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'). - `precision`: Integer indicating the precision for the format. - `flags`: Integer with possible bitwise or-ed values of `Py_DTSF_SIGN`, `Py_DTSF_ADD_DOT_0`, or `Py_DTSF_ALT`. Output - A dictionary where each key is the original string from `input_list` and the value is the formatted string or an error message. Constraints 1. `input_list` contains at least one element and no more than 1000 elements. 2. Each element in `input_list` is a valid string. 3. `format_code` must be one of the specified characters (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'). 4. `precision` is a non-negative integer and is appropriate for the format specified. 5. `flags` can be a combination of `Py_DTSF_SIGN`, `Py_DTSF_ADD_DOT_0`, `Py_DTSF_ALT`. Example ```python input_list = [\\"1.23\\", \\"4.56e-2\\", \\"NaN\\", \\"inf\\", \\"-inf\\"] format_code = \\"f\\" precision = 2 flags = 1 # Py_DTSF_SIGN result = process_and_format_numbers(input_list, format_code, precision, flags) print(result) ``` Expected output: ```python { \\"1.23\\": \\"+1.23\\", \\"4.56e-2\\": \\"+0.05\\", \\"NaN\\": \\"Error: Not a Number\\", \\"inf\\": \\"+inf\\", \\"-inf\\": \\"-inf\\" } ``` Note: Depending on the platform and floating-point handling, the error messages and formatting may vary. Ensure your solution follows the principles outlined in the documentation.","solution":"def process_and_format_numbers(input_list, format_code, precision, flags): This function takes a list of strings, each representing a numerical value, converts them to double, and then formats them as strings according to the specified format. Args: input_list (list of str): A list of strings where each string is expected to represent a floating-point number. format_code (str): A format code character (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\') defining the formatting style. precision (int): The number of digits after the decimal for \'f\' & \'F\', or significant digits for \'g\' & \'G\', and ignored for \'r\'. flags (int): An integer representing flags (bitwise or of Py_DTSF_SIGN, Py_DTSF_ADD_DOT_0, Py_DTSF_ALT). Returns: dict: A dictionary with the original strings as keys and their formatted string conversions as values. In case of errors, the value will be an error message string. Py_DTSF_SIGN = 1 Py_DTSF_ADD_DOT_0 = 2 Py_DTSF_ALT = 4 Py_DTST_FINITE = 0 Py_DTST_INFINITE = 1 Py_DTST_NAN = 2 result_dict = {} for s in input_list: try: num = float(s) except ValueError: result_dict[s] = \\"Error: Not a Number\\" continue if num == float(\'inf\'): formatted_str = \\"+inf\\" if flags & Py_DTSF_SIGN else \\"inf\\" elif num == float(\'-inf\'): formatted_str = \\"-inf\\" elif num != num: # NaN check result_dict[s] = \\"Error: Not a Number\\" continue else: if format_code == \\"r\\": formatted_str = repr(num) else: format_str = f\\"{{:{\'+\' if flags & Py_DTSF_SIGN else \'\'}.{precision}{format_code}}}\\" formatted_str = format_str.format(num) if flags & Py_DTSF_ADD_DOT_0 and \'.\' not in formatted_str: formatted_str += \'.0\' result_dict[s] = formatted_str return result_dict"},{"question":"# Seaborn: Titanic Survival Analysis Problem Statement You are tasked with conducting an analysis on the Titanic dataset using seaborn. Your goal is to create a variety of visual representations that reveal insights about the survival of passengers based on different features. Write a Python function `titanic_visualization()` that: 1. Loads the Titanic dataset using seaborn. 2. Creates a series of plots that provide insights into the data, demonstrating your understanding of multiple seaborn functionalities: 1. **Distribution Comparison**: Create a `box` plot showing the age distribution of passengers across different classes. 2. **Categorical Distribution**: Create a `bar` plot showing the survival rate across different classes and use subplots to separate the visual representation by sex. 3. **Advanced Visualization**: Use a `violin` plot to represent the age distribution across different classes, further differentiated by whether or not the passenger survived. Adjust the plot for better clarity. 4. **Layered Plot**: Overlay a `swarm` plot on top of the `violin` plot to show individual data points for ages. Your function should: * Produce clear, interpretable plots. * Include appropriate labels, titles, and other customizations to enhance readability. * Ensure that all plots are displayed within a single matplotlib window. Function Signature ```python def titanic_visualization() -> None: pass ``` Constraints * You should use seaborn\'s built-in `titanic` dataset. * The function must not accept any parameters and should not return any values. * Ensure that your plots handle missing data appropriately (either by filling or excluding missing values). * Use seaborn\'s inherent functionalities for ease of customization and consistency in visual style. Sample Output ```plaintext Multiple seaborn plots displayed in a single matplotlib window. ``` Notes * Comment your code appropriately to explain the various steps and customizations. * Use seaborn documentation and functions highlighted to guide your solution. * Adjustments in aspects like plot size, color palette, etc., are encouraged but optional.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_visualization(): # Load Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create box plot for age distribution across different passenger classes plt.figure(figsize=(14, 10)) plt.subplot(2, 2, 1) sns.boxplot(x=\\"class\\", y=\\"age\\", data=titanic).set_title(\\"Age Distribution by Class\\") # Create bar plot for survival rate across different classes, separated by sex plt.subplot(2, 2, 2) sns.barplot(x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", data=titanic).set_title(\\"Survival Rate by Class and Sex\\") # Create violin plot for age distribution across different classes and survival status plt.subplot(2, 2, 3) sns.violinplot(x=\\"class\\", y=\\"age\\", hue=\\"survived\\", data=titanic, split=True).set_title(\\"Age Distribution by Class and Survival Status\\") # Overlay a swarm plot on the violin plot sns.swarmplot(x=\\"class\\", y=\\"age\\", hue=\\"survived\\", data=titanic, dodge=True, color=\\".3\\", ax=plt.gca()) # Adjust layout for better readability plt.tight_layout() plt.show()"},{"question":"Data-Dependent Control Flow with PyTorch **Objective:** Implement a class `CustomDataDependentModel` using PyTorch\'s `torch.cond` to perform specific operations on the input data based on a condition derived from the data. **Problem Description:** 1. **Class Definition:** Define a class `CustomDataDependentModel` inheriting from `torch.nn.Module`. 2. **Constructor (`__init__` method):** - Initialize the parent class using `super().__init__()`. 3. **Forward method:** - Implement a `forward` method that takes a single input tensor `x: torch.Tensor`. - Use the `torch.cond` function to: - Check if the mean of the elements in `x` is greater than zero. - If true, apply the true function that calculates the maximum value along dimension 1 of the tensor. - If false, apply the false function that calculates the minimum value along dimension 1 of the tensor. # Function Signature: ```python import torch class CustomDataDependentModel(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor): # Return the maximum values along dimension 1 return torch.max(x, dim=1).values def false_fn(x: torch.Tensor): # Return the minimum values along dimension 1 return torch.min(x, dim=1).values # Use torch.cond to branch based on the mean of x return torch.cond(x.mean() > 0, true_fn, false_fn, (x,)) ``` # Constraints: - The input tensor `x` will be a 2-dimensional tensor with size `(N, M)` where `N` and `M` are positive integers. - The mean of the tensor elements will determine the branch in control flow. # Example Usage: ```python model = CustomDataDependentModel() # Case where mean is greater than 0 input_tensor = torch.tensor([[1.0, 3.0], [2.0, 4.0]]) output = model(input_tensor) print(output) # Expect tensor with maximum values along dimension 1 -> tensor([3.0, 4.0]) # Case where mean is less than or equal to 0 input_tensor = torch.tensor([[-1.0, -3.0], [-2.0, -4.0]]) output = model(input_tensor) print(output) # Expect tensor with minimum values along dimension 1 -> tensor([-3.0, -4.0]) ``` **Performance requirements:** - The solution should efficiently handle tensors of varying sizes while adhering to the specified input format and constraints.","solution":"import torch class CustomDataDependentModel(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: mean_value = x.mean().item() if mean_value > 0: return torch.max(x, dim=1).values else: return torch.min(x, dim=1).values"},{"question":"# Custom List Wrapper **Objective:** Implement a custom list wrapper class that extends the functionality of the built-in `list` object. The class should utilize the built-in `list` object from the `builtins` module and add additional methods as specified. # Problem Statement: You need to implement a class called `CustomList` which wraps around the built-in `list` and extends its functionality. The `CustomList` should support all standard list operations and add the following custom methods: 1. **`sum_elements(self) -> int`**: - Returns the sum of all numeric elements in the list. 2. **`filter_odd(self) -> list`**: - Returns a new `CustomList` object containing only the odd integers from the original list. 3. **`rotate(self, k: int) -> None`**: - Rotates the list to the right by `k` steps, where `k` is a non-negative integer. # Constraints: - The list will contain integers. - You can assume that list operations will not exceed the usual memory constraints. - The custom methods should not use any external libraries, only built-in functionalities. # Example Usage: ```python cl = CustomList([1, 2, 3, 4, 5]) print(cl.sum_elements()) # Output: 15 odd_cl = cl.filter_odd() print(odd_cl) # Output: CustomList([1, 3, 5]) cl.rotate(2) print(cl) # Output: CustomList([4, 5, 1, 2, 3]) ``` # Implementation: You should implement the `CustomList` class by appropriately wrapping the built-in `list` and adding the custom methods as specified. ```python import builtins class CustomList: def __init__(self, init_list=None): if init_list is None: self._list = builtins.list() else: self._list = builtins.list(init_list) def __str__(self): return f\\"CustomList({self._list})\\" def sum_elements(self) -> int: # Your code here pass def filter_odd(self) -> \'CustomList\': # Your code here pass def rotate(self, k: int) -> None: # Your code here pass # Implement other list methods to ensure full functionality # You can add more built-in methods handling here if required ``` # Notes: - You must use the `builtins.list` within your implementation to wrap around its functionalities. - The built-in list methods (e.g., `append`, `extend`, etc.) should still be available in `CustomList`. - Ensure that your class provides all the functionalities of a standard list along with the added custom methods. Your implementation will be assessed based on: - Correctness of the custom methods. - Proper use of the built-in list and its functionalities. - Code readability and efficiency.","solution":"import builtins class CustomList(list): def __init__(self, init_list=None): if init_list is None: self._list = builtins.list() else: self._list = builtins.list(init_list) super().__init__(self._list) def __str__(self): return f\\"CustomList({super().__str__()})\\" def sum_elements(self) -> int: return sum(self) def filter_odd(self) -> \'CustomList\': return CustomList([x for x in self if x % 2 != 0]) def rotate(self, k: int) -> None: k = k % len(self) # In case k is greater than the length of the list self[:] = self[-k:] + self[:-k]"},{"question":"User and Group Information Aggregator **Objective:** Write a function `user_group_info` that aggregates information about Unix users and their associated groups. The function should use the `pwd` and `grp` modules to extract and format this information. **Function Signature:** ```python def user_group_info() -> dict: ``` **Expected Input and Output:** - **Input:** None (the function directly interacts with the system\'s user and group databases). - **Output:** A dictionary where the keys are usernames and the values are lists of group names that each user belongs to. **Requirements:** 1. Retrieve all users from the Unix password database (`pwd` module). 2. For each user, find all groups they belong to using the group database (`grp` module). 3. The output should be a dictionary formatted as follows: ```python { \'username1\': [\'group1\', \'group2\', ...], \'username2\': [\'group1\', \'group3\', ...], ... } ``` **Constraints:** - All functionalities should be achieved using the `pwd` and `grp` modules. - Ensure that the function handles users with no associated groups gracefully. - Your function should handle large numbers of users and groups efficiently. **Example:** ```python # Example output { \'root\': [\'root\', \'sudo\', \'admin\'], \'john\': [\'staff\', \'users\'], \'doe\': [\'users\'], # ... more users } ``` **Testing:** You can test your function on a Unix-based system to verify its correctness. Assuming the system has several users and groups, the function should correctly list all users along with their associated groups. **Notes:** - Consider edge cases such as users with no groups and groups without members. - The function should be compatible with Python 3.10 and later. Good luck, and ensure your implementation is both efficient and follows best Python coding practices.","solution":"import pwd import grp def user_group_info() -> dict: Retrieves all Unix users and their associated groups. Returns: dict: A dictionary where keys are usernames and values are lists of group names that each user belongs to. user_group_mapping = {} users = pwd.getpwall() for user in users: username = user.pw_name user_groups = [g.gr_name for g in grp.getgrall() if username in g.gr_mem] # Add user\'s primary group try: primary_group = grp.getgrgid(user.pw_gid).gr_name if primary_group not in user_groups: user_groups.append(primary_group) except KeyError: pass user_group_mapping[username] = user_groups return user_group_mapping"},{"question":"**Socket Module Coding Assessment** # Objective Your task is to create a simple client-server application in Python using the `socket` module. This application will help you demonstrate your understanding of basic networking concepts and the use of socket programming in Python. # Problem Statement Implement a server application and a client application that communicate over TCP. The server should listen on a specified port and send back any data it receives from a client, effectively echoing back the client\'s message. The client should connect to the server, send a message, and print the server\'s response. # Requirements 1. **Server Application:** - Create a function `start_server(host: str, port: int)` that starts the server. - The server should: - Bind to the specified `host` and `port`. - Listen for incoming connections. - Accept an incoming connection. - Receive data from the client and send the same data back to the client. - Continue to accept and process connections until an external interruption (e.g., Keyboard Interrupt). 2. **Client Application:** - Create a function `start_client(host: str, port: int, message: str) -> str` that starts the client. - The client should: - Connect to the specified server at `host` and `port`. - Send the specified `message` to the server. - Receive the response from the server. - Return the response received from the server. # Input and Output - The server function does not return anything. - The client function takes a `host` string, a `port` integer, and a `message` string as inputs. It returns a string which is the response from the server. # Example ```python def start_server(host: str, port: int): # Your implementation here def start_client(host: str, port: int, message: str) -> str: # Your implementation here # To test your implementation: # In one terminal/window, run the server: start_server(\'127.0.0.1\', 65432) # In another terminal/window or in the same script after starting the server, run the client: response = start_client(\'127.0.0.1\', 65432, \'Hello, Server!\') print(response) # Should print: \'Hello, Server!\' ``` # Constraints - The server should handle only one client at a time (no concurrency/multithreading required). - Do not use any external libraries or modules not included in the standard library. - Handle exceptions where appropriate to ensure the server does not crash unexpectedly. # Notes - Ensure that the server can be terminated gracefully using an external signal such as a KeyboardInterrupt. - Make sure that the server releases the socket upon termination.","solution":"import socket def start_server(host: str, port: int): Starts a TCP server that listens for incoming connections and echoes back received messages. with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket: server_socket.bind((host, port)) server_socket.listen() print(f\\"Server started at {host}:{port}, waiting for connections...\\") try: while True: client_socket, addr = server_socket.accept() with client_socket: print(f\\"Connected by {addr}\\") while True: data = client_socket.recv(1024) if not data: break client_socket.sendall(data) except KeyboardInterrupt: print(\\"Server is shutting down.\\") def start_client(host: str, port: int, message: str) -> str: Starts a TCP client that sends a message to the server and receives the echo response. with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as client_socket: client_socket.connect((host, port)) client_socket.sendall(message.encode()) response = client_socket.recv(1024) return response.decode()"},{"question":"Data Serialization with Python\'s `struct` Module Objective: Demonstrate your ability to use Python\'s `struct` module to pack and unpack complex binary data structures considering byte order, size, and alignment. Background: We have a binary data stream that represents a record of information about certain products. Each record in the stream contains the following data fields in the specified order: 1. A product ID: integer (4 bytes) 2. A product code: 10-character string (10 bytes) 3. A price: floating-point number (4 bytes) 4. Stock quantity: short integer (2 bytes) 5. Available: boolean (1 byte) The byte order for the entire data stream is big-endian. You will need to: 1. Define the format string to describe the struct layout as specified. 2. Implement functions to pack and unpack data according to this format. Requirements: 1. **Function 1: `pack_product_data`** - **Input**: List of dictionaries where each dictionary contains values for `product_id`, `product_code`, `price`, `stock_quantity`, and `available` keys. - **Output**: A single bytes object representing all packed records. 2. **Function 2: `unpack_product_data`** - **Input**: Bytes object containing packed data of multiple records. - **Output**: List of tuples, each representing an unpacked record. Implementations should take care of the correct byte order and ensure proper alignment. Raise appropriate exceptions for any misalignment or size mismatches. Constraints: - `product_id` is a 4-byte integer (range: -2147483648 to 2147483647). - `product_code` is a fixed-length 10-character string (`bytes` type, pad with null bytes if shorter). - `price` is a 4-byte floating-point number (IEEE 754 standard). - `stock_quantity` is a 2-byte short integer (range: -32768 to 32767). - `available` is a boolean value (stored as a 1-byte value). Here is a function signature to help you get started: ```python import struct def pack_product_data(product_list): # Implement your code here def unpack_product_data(packed_data): # Implement your code here ``` Example: ```python product_list = [ {\'product_id\': 1, \'product_code\': b\'ABC123\', \'price\': 19.95, \'stock_quantity\': 100, \'available\': True}, {\'product_id\': 2, \'product_code\': b\'DEF456\', \'price\': 29.95, \'stock_quantity\': 50, \'available\': False}, ] packed_data = pack_product_data(product_list) print(packed_data) unpacked_data = unpack_product_data(packed_data) print(unpacked_data) # Expected output: # Packed data (bytes object) that accurately represents the given products. # Unpacked data as a list of tuples, with values matching the original product list. ``` Ensure your solution is efficient and handles any edge cases, such as incorrect format lengths or values out of range.","solution":"import struct # Define the format string for the specified record structure # Big-endian byte order: \'>\' # int: \'i\' (4 bytes) # char[10]: \'10s\' (10 bytes, null-padded if shorter) # float: \'f\' (4 bytes) # short: \'h\' (2 bytes) # bool: \'?\' (1 byte) # The structure size should be: 4 + 10 + 4 + 2 + 1 = 21 bytes per record FORMAT_STRING = \'>i10sfh?\' def pack_product_data(product_list): Packs a list of product data dictionaries into a bytes object. Args: product_list: A list of dictionaries containing keys \'product_id\', \'product_code\', \'price\', \'stock_quantity\', and \'available\'. Returns: A bytes object representing all packed records. packed_data = b\'\' for product in product_list: packed_data += struct.pack( FORMAT_STRING, product[\'product_id\'], product[\'product_code\'].ljust(10, b\'x00\'), # pad product code to 10 bytes if shorter product[\'price\'], product[\'stock_quantity\'], product[\'available\'] ) return packed_data def unpack_product_data(packed_data): Unpacks a bytes object into a list of product data tuples. Args: packed_data: A bytes object containing packed data of multiple records. Returns: A list of tuples where each tuple represents an unpacked record. record_size = struct.calcsize(FORMAT_STRING) product_data = [] for i in range(0, len(packed_data), record_size): record = struct.unpack( FORMAT_STRING, packed_data[i:i + record_size] ) product_data.append(( record[0], record[1].rstrip(b\'x00\'), # remove padding null bytes record[2], record[3], record[4] )) return product_data"},{"question":"Objective: Write a Python CGI script that processes a web form submission. The form will collect user information, including a username, email, and an optional uploaded text file. The script should handle the form data securely, validate it, and generate appropriate HTML output based on the input data. Requirements: 1. **Input Form**: Assume the form has the following fields: - `username` (text) - `email` (text) - `comments` (textarea, optional) - `uploadfile` (file, optional) 2. **Processing**: - Use the `cgi` module to handle form data securely. - Validate that `username` and `email` are non-empty and follow proper formatting. - If a file is uploaded, read its content but maintain security (e.g., do not read excessively large files). - Handle errors gracefully using the `cgitb` module and provide meaningful error messages in the generated output. 3. **Output**: - Generate an HTML response that displays the submitted information (username, email, comments). - If a file is uploaded, display the first 10 lines of the file content in the response. 4. **Edge cases and Security**: - Ensure that the script handles cases where the form fields are missing or improperly formatted. - Avoid security vulnerabilities related to user input. Input Format: No direct input format for the script as it is meant to be run on a web server responding to form submissions. Output Format: An HTML response generated by the script, including appropriate headers and form data. Constraints: - Use Python 3.10. - Handle all form data appropriately and securely. - The email validation can be simple (just checking for the presence of \\"@\\" and a domain). Performance Requirements: - Efficient handling of the form data to minimize resource usage. - The file upload handling should not consume excessive memory (avoid reading entire large files into memory). # Example Form: ```html <!DOCTYPE html> <html> <body> <form action=\\"/cgi-bin/test.py\\" method=\\"post\\" enctype=\\"multipart/form-data\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Email: <input type=\\"text\\" name=\\"email\\"><br> Comments:<br><textarea name=\\"comments\\" rows=\\"10\\" cols=\\"30\\"></textarea><br> Upload file: <input type=\\"file\\" name=\\"uploadfile\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> </body> </html> ``` # Script Template: ```python #!/usr/bin/env python3 import cgi import cgitb cgitb.enable() def validate_email(email): # Simple email validation if \\"@\\" in email and \\".\\" in email.split(\\"@\\")[-1]: return True return False def main(): print(\\"Content-Type: text/html\\") # HTML is following print() # blank line, end of headers form = cgi.FieldStorage() # Validate inputs username = form.getvalue(\\"username\\") email = form.getvalue(\\"email\\") comments = form.getvalue(\\"comments\\", \\"\\") if not username or not validate_email(email): print(\\"<h1>Error</h1>\\") print(\\"<p>Invalid data. Please go back and enter correct information.</p>\\") return print(\\"<html><body>\\") print(f\\"<h1>Form Submitted</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Email: {email}</p>\\") print(f\\"<p>Comments: {comments}</p>\\") # Handle file upload fileitem = form[\\"uploadfile\\"] if fileitem.file: print(\\"<h2>Uploaded File Content:</h2><pre>\\") linecount = 0 while linecount < 10: line = fileitem.file.readline() if not line: break print(line.decode(\'utf-8\').strip()) linecount += 1 print(\\"</pre>\\") print(\\"</body></html>\\") if __name__ == \\"__main__\\": main() ``` Complete the above CGI script to fully meet the requirements specified. Ensure to test the script in a web server environment.","solution":"#!/usr/bin/env python3 import cgi import cgitb cgitb.enable() def validate_email(email): Function to validate email addresses. Ensures that email contains \'@\' and a domain. if \\"@\\" in email and \\".\\" in email.split(\\"@\\")[-1]: return True return False def main(): print(\\"Content-Type: text/html\\") # HTML is following print() # blank line, end of headers form = cgi.FieldStorage() # Validate inputs username = form.getvalue(\\"username\\") email = form.getvalue(\\"email\\") comments = form.getvalue(\\"comments\\", \\"\\") if not username or not validate_email(email): print(\\"<html><body>\\") print(\\"<h1>Error</h1>\\") print(\\"<p>Invalid data. Please go back and enter correct information.</p>\\") print(\\"</body></html>\\") return print(\\"<html><body>\\") print(f\\"<h1>Form Submitted</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Email: {email}</p>\\") print(f\\"<p>Comments: {comments}</p>\\") # Handle file upload if \\"uploadfile\\" in form: fileitem = form[\\"uploadfile\\"] if fileitem.file: print(\\"<h2>Uploaded File Content:</h2><pre>\\") linecount = 0 while linecount < 10: line = fileitem.file.readline() if not line: break print(line.decode(\'utf-8\').strip()) linecount += 1 print(\\"</pre>\\") print(\\"</body></html>\\") if __name__ == \\"__main__\\": main()"},{"question":"# Programming Question: Implementing and Transforming a Neural Network with torch.fx Objective Your task is to implement a simple neural network using PyTorch and then use the `torch.fx` module to transform the network. Specifically, you will: 1. Symbolically trace the neural network. 2. Modify the traced Graph to substitute all `torch.add` operations with `torch.mul` operations. 3. Ensure that the modified network is functionally equivalent to the original network when `torch.add` and `torch.mul` perform the same operation (e.g., both performing addition). Instructions 1. Implement a simple neural network module `SimpleNet` using PyTorch. This network should include at least one layer that uses `torch.add`. 2. Use `torch.fx.symbolic_trace` to obtain the Graph of this network. 3. Implement a transformation function `transform_add_to_mul` that: - Takes the traced Graph as input. - Replaces all `torch.add` operations with `torch.mul` operations in the Graph. 4. Verify that the transformed network produces the same output as the original when `torch.add` and `torch.mul` are functionally equivalent. Constraints - Do not use any third-party libraries except PyTorch. - Focus on accurate and efficient graph manipulation. - The transformed network should not modify the behavior of network layers other than replacing `torch.add` with `torch.mul`. Input 1. Python class `SimpleNet`. 2. Function `transform_add_to_mul` with the following signature: ```python def transform_add_to_mul(traced_model: torch.fx.GraphModule) -> torch.fx.GraphModule: pass ``` Output 1. The transformed `torch.fx.GraphModule`. 2. A function to compare the outputs of the original and transformed models with a sample input. Example ```python import torch import torch.fx class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = torch.nn.Linear(10, 10) def forward(self, x): return torch.add(self.linear(x), self.linear(x)) def transform_add_to_mul(traced_model: torch.fx.GraphModule) -> torch.fx.GraphModule: graph = traced_model.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul graph.lint() return torch.fx.GraphModule(traced_model, graph) # Create the original model original_model = SimpleNet() # Trace the model traced_model = torch.fx.symbolic_trace(original_model) # Apply the transformation transformed_model = transform_add_to_mul(traced_model) # Verify functionality sample_input = torch.randn(1, 10) original_output = original_model(sample_input) transformed_output = transformed_model(sample_input) assert torch.allclose(original_output, transformed_output), \\"The transformed model does not match the original model\\" print(\\"Transformation successful and verified.\\") ``` Note Ensure the transformation logic maintains the structural integrity of the original network\'s functionality, treating `torch.add` as equivalent to `torch.mul` for verification purposes.","solution":"import torch import torch.fx class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear = torch.nn.Linear(10, 10) def forward(self, x): return torch.add(self.linear(x), self.linear(x)) def transform_add_to_mul(traced_model: torch.fx.GraphModule) -> torch.fx.GraphModule: graph = traced_model.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul graph.lint() return torch.fx.GraphModule(traced_model, graph)"},{"question":"Pandas Coding Assessment # Objective: To assess your understanding of pandas for creating and manipulating Series and DataFrames, as well as performing data alignment and handling missing data. # Problem Statement: You are provided with some data about students and their scores in different subjects. You will need to perform various operations using pandas.DataFrame and pandas.Series to analyze this data. # Instructions: 1. **Create a DataFrame:** - Create a DataFrame named `df` using the following data: ```python data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eva\\"], \\"Math\\": [90, 85, np.nan, 89, 95], \\"Science\\": [85, 80, 88, np.nan, 90], \\"English\\": [np.nan, 70, 78, 85, 90], } ``` 2. **Handle Missing Data:** - Fill the missing values in `df` with the average score of the respective column. 3. **Data Analysis:** - Create a new column \\"Total\\" that represents the total score for each student across all subjects. - Create another column \\"Average\\" that shows the average score for each student across all subjects. 4. **Rank Students:** - Add a new column \\"Rank\\" that ranks the students based on their \\"Total\\" scores in descending order. In case of a tie, use the \\"Average\\" score to determine the rank. 5. **Subset of Data:** - Create a new DataFrame, `top_students`, that contains the data of students who have a total score greater than the average total score of all students. 6. **Series Operations:** - Create a Series named `rank_series` from the \\"Rank\\" column of `df`. - From `rank_series`, create a new Series named `top_3_students` that contains the names of the top 3 ranked students. # Constraints: - Ensure there are no ties in the \\"Rank\\" column. - If there are missing values after calculating \\"Total\\" and \\"Average\\", handle them by setting them to zero. # Expected Output: - The final DataFrames `df` and `top_students`, and the Series `rank_series` and `top_3_students`. # Performance Requirements: - You should optimize the DataFrame operations to be performed in a vectorized manner where possible. # Sample Output: ```python import pandas as pd import numpy as np # Sample Data data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eva\\"], \\"Math\\": [90, 85, np.nan, 89, 95], \\"Science\\": [85, 80, 88, np.nan, 90], \\"English\\": [np.nan, 70, 78, 85, 90], } df = pd.DataFrame(data) # Your code goes here to implement the tasks as per the instructions ```","solution":"import pandas as pd import numpy as np # Sample Data data = { \\"Name\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eva\\"], \\"Math\\": [90, 85, np.nan, 89, 95], \\"Science\\": [85, 80, 88, np.nan, 90], \\"English\\": [np.nan, 70, 78, 85, 90], } df = pd.DataFrame(data) # Handle missing data: Fill missing values with column average df.fillna(df.mean(numeric_only=True), inplace=True) # Create a new column \\"Total\\" df[\\"Total\\"] = df[[\\"Math\\", \\"Science\\", \\"English\\"]].sum(axis=1) # Create a new column \\"Average\\" df[\\"Average\\"] = df[[\\"Math\\", \\"Science\\", \\"English\\"]].mean(axis=1) # Rank students based on \\"Total\\" and \\"Average\\" scores df[\\"Rank\\"] = df.sort_values(by=[\\"Total\\", \\"Average\\"], ascending=[False, False]).reset_index().index + 1 # Create a new DataFrame with students having total score > average total score average_total_score = df[\\"Total\\"].mean() top_students = df[df[\\"Total\\"] > average_total_score] # Create a Series from the \\"Rank\\" column rank_series = df[\\"Rank\\"] # Create a Series for the top 3 students based on their rank top_3_students = df.nsmallest(3, \'Rank\')[\\"Name\\"].reset_index(drop=True) df, top_students, rank_series, top_3_students"},{"question":"# Complex Number Operations with cmath In this exercise, you are required to implement functions that perform specific operations on complex numbers using the `cmath` module. The goal is to ensure you are comfortable with converting representations of complex numbers and performing advanced mathematical operations. Function 1: `convert_to_polar` **Description:** Write a function that takes a complex number and returns its polar coordinates. **Input:** - `z (complex)`: A complex number. **Output:** - A tuple `(r, phi)` where `r` is the modulus and `phi` is the phase of the complex number. **Example:** ```python convert_to_polar(1 + 1j) # Expected output: (1.4142135623730951, 0.7853981633974483) ``` Function 2: `convert_to_complex` **Description:** Write a function that takes polar coordinates and returns the corresponding complex number. **Input:** - `r (float)`: The modulus. - `phi (float)`: The phase in radians. **Output:** - The complex number corresponding to the input polar coordinates. **Example:** ```python convert_to_complex(1.4142135623730951, 0.7853981633974483) # Expected output: (1+1j) ``` Function 3: `compute_sqrt` **Description:** Write a function that takes a complex number and returns its square root. **Input:** - `z (complex)`: A complex number. **Output:** - The square root of the complex number, also a complex number. **Example:** ```python compute_sqrt(4 + 0j) # Expected output: (2+0j) compute_sqrt(-1 + 0j) # Expected output: 1j ``` Function 4: `compute_log` **Description:** Write a function that computes the natural logarithm of a complex number. **Input:** - `z (complex)`: A complex number. **Output:** - The natural logarithm of the complex number, also a complex number. **Example:** ```python compute_log(1 + 0j) # Expected output: (0+0j) compute_log(-1 + 0j) # Expected output: (0+3.141592653589793j) ``` # Constraints - You may assume the inputs to the functions will always be of correct type (complex numbers for Functions 1 and 3, and appropriate floats/complex numbers for Functions 2 and 4). - Do not use any library other than `cmath`. - Pay attention to edge cases, especially points on the branch cuts. # Performance Requirements - Your functions should handle typical inputs efficiently but do not need to be optimized for extremely large or complex input values.","solution":"import cmath def convert_to_polar(z): Converts a complex number to its polar coordinates. Parameters: z (complex): A complex number. Returns: tuple: A tuple (r, phi) where r is the modulus and phi is the phase of the complex number. r = abs(z) phi = cmath.phase(z) return (r, phi) def convert_to_complex(r, phi): Converts polar coordinates to a complex number. Parameters: r (float): The modulus. phi (float): The phase in radians. Returns: complex: The complex number. return cmath.rect(r, phi) def compute_sqrt(z): Computes the square root of a complex number. Parameters: z (complex): A complex number. Returns: complex: The square root of the complex number. return cmath.sqrt(z) def compute_log(z): Computes the natural logarithm of a complex number. Parameters: z (complex): A complex number. Returns: complex: The natural logarithm of the complex number. return cmath.log(z)"},{"question":"**Networking and Asynchronous I/O Coding Challenge** **Objective:** Implement a simple asynchronous TCP server and client using Python\'s `asyncio` and `socket` modules. **Problem Statement:** 1. **Async TCP Server:** - Create an asynchronous TCP server that listens on a specified port. - The server should be able to handle multiple client connections concurrently. - When a client sends a message to the server, the server should echo the message back to the client with the prefix \\"Echo: \\". - The server should run indefinitely until manually interrupted. 2. **Async TCP Client:** - Implement an asynchronous TCP client that connects to the server. - The client should send a message to the server and wait for the response. - After receiving the response from the server, the client should print it to the console. **Input Format:** - The server should listen on port 8888. - The client should connect to the server on the same port and send the message \\"Hello, Server!\\". - The server should echo back the message with the prefix \\"Echo: \\". **Output Format:** - The server does not produce any direct output. - The client should print the message it receives from the server, e.g., \\"Echo: Hello, Server!\\". **Constraints:** - Use only the `asyncio` and `socket` modules for this implementation. - Ensure proper handling of multiple clients concurrently. - The task should be implemented using asynchronous programming practices. **Example:** ```python # Sample output from the client after connecting to the server # and sending the message \\"Hello, Server!\\". Echo: Hello, Server! ``` **Performance Requirements:** - The server should efficiently handle up to 1000 concurrent client connections. **Implementation Details:** - You need to implement two scripts, one for the server and one for the client. - Both scripts should be able to run independently and demonstrate the requested functionality. Good luck! Make sure to test your implementation thoroughly to handle multiple clients and potential edge cases.","solution":"import asyncio import socket async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") echo_message = f\\"Echo: {message}\\" writer.write(echo_message.encode()) await writer.drain() print(f\\"Sent: {echo_message}\\") writer.close() async def run_server(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8888) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') async with server: await server.serve_forever() async def run_client(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8888) message = \\"Hello, Server!\\" print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') writer.close() await writer.wait_closed() if __name__ == \\"__main__\\": mode = input(\\"Enter \'server\' to start the server or \'client\' to start the client: \\").strip().lower() if mode == \'server\': asyncio.run(run_server()) elif mode == \'client\': asyncio.run(run_client()) else: print(\\"Invalid mode. Please enter either \'server\' or \'client\'.\\")"},{"question":"# Question: Implement an XML SAX Parser with Custom Handlers **Objective**: To test your understanding of XML parsing using the SAX approach in Python. **Description**: You are required to implement a SAX-based XML parser using the `xml.sax` package. Your task is to parse an XML document from a string, process various SAX events using custom handlers, and handle possible exceptions. **Requirements**: 1. **Custom Content Handler**: - Create a class `CustomContentHandler` inheriting from `xml.sax.handler.ContentHandler`. - Implement methods to handle events: - `startElement(name, attrs)`: Print the name of the element and its attributes (if any). - `endElement(name)`: Print the name of the element indicating its end. - `characters(content)`: Print the content of the element if there is any significant non-whitespace character data. 2. **Custom Error Handler**: - Create a class `CustomErrorHandler` inheriting from `xml.sax.handler.ErrorHandler`. - Implement methods to handle errors: - `error(exception)`: Print a message indicating a recoverable error. - `fatalError(exception)`: Print a message and raise the exception indicating a non-recoverable error. - `warning(exception)`: Print a warning message indicating a minor issue during parsing. 3. **Parsing Function**: - Implement a function `parse_xml(xml_string)` that takes an XML string as input. - Use `xml.sax.parseString()` to parse the given XML string. - Utilize the `CustomContentHandler` and `CustomErrorHandler` to handle the respective events and errors. 4. **Exceptions Handling**: - Ensure that the function `parse_xml(xml_string)` catches and prints any `xml.sax.SAXParseException` raised during parsing. **Input**: - A string representation of an XML document. **Output**: - Print statements from the handlers describing elements and character data encountered, and error messages if exceptions are raised. **Constraints**: - Assume the input XML string will be well-formed, but it may contain elements and attributes of varying complexity. - You should handle different types of possible exceptions as described. **Example XML Input**: ```xml <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> </book> <book id=\\"2\\"> <title>Effective XML</title> <author>Jane Smith</author> </book> </library> ``` **Example Output**: ``` Start element: library Start element: book - id: 1 Start element: title Characters: Python Programming End element: title Start element: author Characters: John Doe End element: author End element: book Start element: book - id: 2 Start element: title Characters: Effective XML End element: title Start element: author Characters: Jane Smith End element: author End element: book End element: library ``` Please provide your implementation of the `CustomContentHandler`, `CustomErrorHandler`, and `parse_xml(xml_string)` function.","solution":"import xml.sax class CustomContentHandler(xml.sax.handler.ContentHandler): def startElement(self, name, attrs): print(f\\"Start element: {name}\\") if attrs.getLength() > 0: for attr_name in attrs.getNames(): print(f\\" - {attr_name}: {attrs.getValue(attr_name)}\\") def endElement(self, name): print(f\\"End element: {name}\\") def characters(self, content): if content.strip(): print(f\\"Characters: {content.strip()}\\") class CustomErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Recoverable error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_xml(xml_string): handler = CustomContentHandler() error_handler = CustomErrorHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: print(f\\"Parsing exception: {e}\\")"},{"question":"**Coding Assessment Question:** # Objective: Implement a function using the `dbm` module that opens/create a database, inserts key-value pairs, retrieves values, and ensures proper context management. # Question: Write a function `manage_db(db_file: str, operations: List[Tuple[str, str, Union[str, None]]]) -> Union[str, List[str]]` that opens or creates a database file using the `dbm` module. The function should perform a series of operations and return the results of any retrievals. Parameters: - `db_file` (str): Name of the database file. - `operations` (List[Tuple[str, str, Union[str, None]]]): A list of operations to perform on the database. Each operation is a tuple consisting of: - The operation type (`\'insert\'` or `\'retrieve\'`). - The key (str) for the operation. - The value (str) if the operation type is `\'insert\'`, otherwise `None`. Returns: - If there are any retrieval operations, return a list of retrieved values corresponding to those operations. - If no retrieval operations are performed, return the string \\"No retrieval operations\\". Constraints: - Keys and values are always strings but should be stored as bytes. - If the database file does not exist, it should be created. - Use context management to automatically close the database. - If an inserted key already exists, overwrite its value. - Handle any potential `KeyError` exceptions for retrievals. Example: ```python def manage_db(db_file: str, operations: List[Tuple[str, str, Union[str, None]]]) -> Union[str, List[str]]: # Your implementation here # Example usage: operations = [ (\'insert\', \'greeting\', \'hello\'), (\'insert\', \'farewell\', \'goodbye\'), (\'retrieve\', \'greeting\', None), (\'retrieve\', \'unknown_key\', None) ] print(manage_db(\'example_db\', operations)) # Output: [\'hello\', \'\'] ``` # Notes: - You may use any submodule of `dbm` (`dbm.gnu`, `dbm.ndbm`, or `dbm.dumb`) depending on availability. - Ensure that the database file is correctly closed after operations. - The function should handle exceptions gracefully, especially when trying to retrieve a non-existent key.","solution":"import dbm from typing import List, Tuple, Union, Any def manage_db(db_file: str, operations: List[Tuple[str, str, Union[str, None]]]) -> Union[str, List[str]]: results = [] with dbm.open(db_file, \'c\') as db: for op, key, value in operations: key = key.encode(\'utf-8\') if op == \'insert\': db[key] = value.encode(\'utf-8\') elif op == \'retrieve\': try: results.append(db[key].decode(\'utf-8\')) except KeyError: results.append(\'\') return results if results else \\"No retrieval operations\\""},{"question":"Principal Component Analysis (PCA) **Objective:** Implement Principal Component Analysis (PCA) using scikit-learn on a given dataset to reduce its dimensionality and interpret the results. # Problem Statement: You are given a dataset `data.csv`, which contains feature values for several data points. Your task is to implement Principal Component Analysis (PCA) using scikit-learn to reduce the dataset to a specified number of components. You should also plot the variance explained by each component and the cumulative variance explained by the components. # Specifications: 1. **Input:** - A CSV file `data.csv` containing the dataset. Each row represents a data point, and each column represents a feature. - An integer `n_components` representing the number of principal components to retain. 2. **Output:** - A transformed dataset with `n_components` principal components. - A plot showing the variance explained by each component and the cumulative variance explained by the components. 3. **Implementation Details:** 1. Load the dataset from the CSV file. 2. Implement PCA using scikit-learn to reduce the dataset to `n_components` principal components. 3. Print the variance explained by each component. 4. Print the cumulative variance explained by the components. 5. Plot the variance explained by each component and the cumulative variance explained by the components. # Constraints: - Ensure that the number of components `n_components` does not exceed the number of original features. - Handle edge cases where the dataset might contain missing values or non-numeric values. # Example: Assume `data.csv` contains the following data: ``` feat1,feat2,feat3,feat4 1.0,2.0,3.0,4.0 5.0,6.0,7.0,8.0 9.0,10.0,11.0,12.0 13.0,14.0,15.0,16.0 ``` Running your code with `n_components = 2` should produce the transformed dataset and the variance plots as described. # Your Task: Implement the following function: ```python import matplotlib.pyplot as plt import pandas as pd from sklearn.decomposition import PCA def pca_transformation(csv_file: str, n_components: int): Perform PCA on the given dataset and plot the variance explained by components. Parameters: csv_file (str): Path to the CSV file containing the dataset. n_components (int): Number of principal components to retain. Returns: None # Load the dataset from the CSV file # Implement PCA using scikit-learn # Print variance explained by each component # Print cumulative variance explained by components # Plot variance explained by each component and cumulative variance # Please write your solution here. # Example usage: # pca_transformation(\'data.csv\', 2) ``` Make sure your code is well-documented and follows best practices.","solution":"import matplotlib.pyplot as plt import pandas as pd from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler def pca_transformation(csv_file: str, n_components: int): Perform PCA on the given dataset and plot the variance explained by components. Parameters: csv_file (str): Path to the CSV file containing the dataset. n_components (int): Number of principal components to retain. Returns: pd.DataFrame: Transformed dataset with principal components. # Load the dataset from the CSV file data = pd.read_csv(csv_file) # Check for non-numeric columns and drop them data_numeric = data.select_dtypes(include=[float, int]) # Handle missing values by filling them with the mean of the column data_numeric.fillna(data_numeric.mean(), inplace=True) # Standardize the data scaler = StandardScaler() scaled_data = scaler.fit_transform(data_numeric) # Implement PCA using scikit-learn pca = PCA(n_components=n_components) principal_components = pca.fit_transform(scaled_data) # Create a DataFrame with the principal components pc_df = pd.DataFrame(data=principal_components, columns=[f\\"PC{i+1}\\" for i in range(n_components)]) # Calculate variance explained by each component and cumulative variance explained explained_variance = pca.explained_variance_ratio_ cumulative_variance = explained_variance.cumsum() # Print variance explained by each component for i, var in enumerate(explained_variance, 1): print(f\\"Variance explained by PC{i}: {var:.4f}\\") # Print cumulative variance explained by components for i, cum_var in enumerate(cumulative_variance, 1): print(f\\"Cumulative variance explained by PC{i}: {cum_var:.4f}\\") # Plot variance explained by each component plt.figure(figsize=(10, 5)) plt.bar([f\\"PC{i+1}\\" for i in range(n_components)], explained_variance, alpha=0.6, label=\'Individual explained variance\') plt.step([f\\"PC{i+1}\\" for i in range(n_components)], cumulative_variance, where=\'mid\', label=\'Cumulative explained variance\') plt.ylabel(\'Explained variance ratio\') plt.xlabel(\'Principal components\') plt.legend(loc=\'best\') plt.title(\'PCA Explained Variance\') plt.show() return pc_df # Example usage: # pca_transformation(\'data.csv\', 2)"},{"question":"Objective: You are given a function and your task is to identify which bytecode instructions are involved in its handling of control flow structures like loops, conditionals, and exceptions. Problem Statement: Implement a function `analyze_bytecode_control_flow(func: callable) -> dict` that analyzes the provided function and returns a dictionary summarizing the control flow bytecode instructions. The dictionary should have: - **keys**: Control flow categories (`loops`, `conditionals`, `exceptions`) - **values**: Lists of instructions (`opname`) that correspond to each control flow category found in the bytecode of the function. Expected Input: - A single argument: - `func`: A Python function (callable) to be analyzed. Expected Output: - A dictionary with control flow categories as keys and lists of corresponding instruction names (`opname`) as values. ```python { \'loops\': [...], # list of loop-related opnames \'conditionals\': [...], # list of conditional-related opnames \'exceptions\': [...] # list of exception-handling-related opnames } ``` Constraints: - Assume the input will always be a valid Python function. - Use the `dis` module to analyze the bytecode. Example: ```python def example_function(x): if x > 0: for i in range(x): try: if i % 2 == 0: continue print(i) except Exception as e: print(\\"Error:\\", e) # analyze_bytecode_control_flow(example_function) should return a dictionary like: # { # \'loops\': [\'FOR_ITER\', \'JUMP_ABSOLUTE\'], # \'conditionals\': [\'POP_JUMP_IF_FALSE\', \'POP_JUMP_IF_TRUE\'], # \'exceptions\': [\'SETUP_FINALLY\', \'POP_BLOCK\', \'LOAD_GLOBAL\', \'COMPARE_OP\'] # } ``` Implementation Notes: - Use `dis.Bytecode` to obtain details of the bytecode instructions. - Classify instructions based on their role in control flow: - **Loops**: Look for instructions like `FOR_ITER`, `JUMP_ABSOLUTE`, etc. - **Conditionals**: Look for instructions like `POP_JUMP_IF_FALSE`, `POP_JUMP_IF_TRUE`, etc. - **Exceptions**: Look for instructions like `SETUP_FINALLY`, `POP_BLOCK`, `COMPARE_OP`, etc. You can use the `dis` module\'s opcode collections such as `dis.hasjrel`, `dis.hasjabs`, and inspect individual `Instruction` attributes to achieve this classification.","solution":"import dis def analyze_bytecode_control_flow(func: callable) -> dict: Analyzes the bytecode of a given function and identifies control flow instructions. Args: - func (callable): The function to analyze. Returns: - dict: A dictionary with keys \'loops\', \'conditionals\', and \'exceptions\', mapping to lists of corresponding bytecode instruction names. control_flow = { \'loops\': [], \'conditionals\': [], \'exceptions\': [] } bytecode = dis.Bytecode(func) for instr in bytecode: if instr.opname in {\'FOR_ITER\', \'JUMP_ABSOLUTE\', \'JUMP_FORWARD\'}: control_flow[\'loops\'].append(instr.opname) elif instr.opname in {\'POP_JUMP_IF_FALSE\', \'POP_JUMP_IF_TRUE\', \'JUMP_IF_NOT_EXC_MATCH\'}: control_flow[\'conditionals\'].append(instr.opname) elif instr.opname in {\'SETUP_FINALLY\', \'POP_BLOCK\', \'LOAD_GLOBAL\', \'COMPARE_OP\'}: control_flow[\'exceptions\'].append(instr.opname) # Remove duplicates for key in control_flow: control_flow[key] = list(set(control_flow[key])) return control_flow"},{"question":"Objective: Create a Python script that performs a complete backup and restore operation for a given directory. The script should ensure that file metadata, symbolic links, and necessary permissions are preserved during the backup. Additionally, provide functionality to report the disk usage statistics before and after the operation. Task: You are to implement two main functions: 1. **backup_directory(src, backup_destination)**: - Parameters: - `src`: The path to the directory that needs to be backed up. - `backup_destination`: The path where the backup should be created. - Functionality: - The function should create a compressed archive (`.tar.gz`) of the entire `src` directory at the `backup_destination`. - Ensure all file metadata and symbolic links are preserved. 2. **restore_directory(backup_file, restore_destination)**: - Parameters: - `backup_file`: The path to the backup file created by `backup_directory`. - `restore_destination`: The path where the backup should be restored. - Functionality: - The function should extract the compressed archive `backup_file` into `restore_destination`. 3. **report_disk_usage(path)**: - Parameter: - `path`: The path of the directory whose disk usage statistics need to be reported. - Functionality: - The function should print the total, used, and free space for the filesystem containing `path`. Example Usage: ```python source_dir = \\"/path/to/source\\" backup_dest = \\"/path/to/backup/destination\\" restore_dest = \\"/path/to/restore/destination\\" print(\\"Disk Usage Before Backup:\\") report_disk_usage(source_dir) # Perform backup backup_directory(source_dir, backup_dest) print(\\"Disk Usage After Backup:\\") report_disk_usage(backup_dest) # Perform restore restore_directory(backup_dest + \'.tar.gz\', restore_dest) print(\\"Disk Usage After Restore:\\") report_disk_usage(restore_dest) ``` Constraints: - Ensure the solution works across different platforms (Unix, Windows). - Handle potential errors, such as non-existing paths or permissions issues gracefully. Additional Requirements: - Your solution should be efficient and handle large directories seamlessly. - Include documentation/comments explaining the functionality of your code. Hints: - Use `shutil.make_archive` for creating the compressed backup. - Use `shutil.unpack_archive` for extracting the backup. - Use `shutil.disk_usage` to get disk usage statistics.","solution":"import os import shutil def backup_directory(src, backup_destination): Creates a compressed .tar.gz archive of the src directory at the backup_destination. Parameters: src (str): The path to the directory to be backed up. backup_destination (str): The path where the backup should be created. # Ensure the source directory exists if not os.path.isdir(src): raise FileNotFoundError(f\\"Source directory {src} not found\\") # Ensure the backup destination directory exists destination_dir = os.path.dirname(backup_destination) if not os.path.isdir(destination_dir): raise FileNotFoundError(f\\"Destination directory {destination_dir} not found\\") # Create a compressed archive of the source directory shutil.make_archive(backup_destination, \'gztar\', src) print(f\\"Backup created at {backup_destination}.tar.gz\\") def restore_directory(backup_file, restore_destination): Extracts the compressed .tar.gz archive into the restore_destination directory. Parameters: backup_file (str): The path to the backup file created by backup_directory. restore_destination (str): The path where the backup should be restored. # Ensure the backup file exists if not os.path.isfile(backup_file): raise FileNotFoundError(f\\"Backup file {backup_file} not found\\") # Ensure the restore destination directory exists, create if not if not os.path.exists(restore_destination): os.makedirs(restore_destination) # Extract the archive shutil.unpack_archive(backup_file, restore_destination) print(f\\"Backup restored to {restore_destination}\\") def report_disk_usage(path): Prints the total, used, and free space for the filesystem containing path. Parameters: path (str): The path of the directory to report disk usage statistics for. # Ensure the path exists if not os.path.exists(path): raise FileNotFoundError(f\\"Path {path} not found\\") total, used, free = shutil.disk_usage(path) print(f\\"Disk usage for {path}:\\") print(f\\" Total: {total} bytes\\") print(f\\" Used: {used} bytes\\") print(f\\" Free: {free} bytes\\")"},{"question":"# Question: Multi-Level Sensor Data Processing You are tasked with developing a program that processes binary data from a multi-level sensor system. The sensor system sends data in a structured binary format that your program needs to unpack, process, and repack for storage. Each message from the sensor system has the following structure: 1. A 2-byte unsigned short indicating the number of sensor readings. 2. Each sensor reading consists of: - A 1-byte identifier (unsigned char). - A 4-byte float value. - A 2-byte short value indicating the status. The packed binary format for a message with 3 sensor readings might look like this: ``` b\'x03x00\' + b\'x01\' + float_bytes(25.5) + b\'x00x01\' + b\'x02\' + float_bytes(-10.75) + b\'x01x00\' + b\'x03\' + float_bytes(0.0) + b\'xFFxFF\' ``` Where `float_bytes(value)` returns the 4-byte float representation of the `value`. Requirements: 1. Implement a function `process_sensor_data` that takes a single binary message (as bytes) as input and returns the processed data as: - The number of readings. - A list of tuples, where each tuple contains: - The sensor identifier (int) - The processed float value (rounded to 2 decimal places) - The status value (int) 2. Implement a function `repack_sensor_data` that takes the processed data (from step 1) and repacks it back into the original binary format. Functions: 1. `process_sensor_data(binary_data: bytes) -> Tuple[int, List[Tuple[int, float, int]]]:` - **Input**: - `binary_data` (bytes): The binary message from the sensor system. - **Output**: - A tuple with the number of readings (int) and a list of tuples containing sensor id (int), value (float), and status (int). 2. `repack_sensor_data(processed_data: Tuple[int, List[Tuple[int, float, int]]]) -> bytes:` - **Input**: - `processed_data` (tuple): The processed data from `process_sensor_data`. - **Output**: - Binary data packed in the original format (bytes). Example: ```python binary_data = b\'x03x00\' + b\'x01\' + b\'x00x00x80x41\' + b\'x00x01\' + b\'x02\' + b\'x00x00x20xc1\' + b\'x01x00\' + b\'x03\' + b\'x00x00x00x00\' + b\'xffxff\' # Step 1: Process the binary data readings = process_sensor_data(binary_data) # Output: (3, [(1, 16.0, 1), (2, -10.0, 1), (3, 0.0, -1)]) # Step 2: Repack the processed data packed_data = repack_sensor_data(readings) # Output: It should match the original binary_data assert packed_data == binary_data # should be True ``` Constraints: - Up to 1000 sensor readings per message. - Ensure that the `status` values are correctly unpacked and repacked. - Utilize the `struct` module for packing and unpacking operations.","solution":"import struct from typing import List, Tuple def process_sensor_data(binary_data: bytes) -> Tuple[int, List[Tuple[int, float, int]]]: num_readings = struct.unpack_from(\'H\', binary_data, 0)[0] readings = [] offset = 2 for i in range(num_readings): identifier = struct.unpack_from(\'B\', binary_data, offset)[0] offset += 1 value = struct.unpack_from(\'f\', binary_data, offset)[0] offset += 4 status = struct.unpack_from(\'h\', binary_data, offset)[0] offset += 2 readings.append((identifier, round(value, 2), status)) return num_readings, readings def repack_sensor_data(processed_data: Tuple[int, List[Tuple[int, float, int]]]) -> bytes: num_readings, readings = processed_data packed_data = struct.pack(\'H\', num_readings) for reading in readings: identifier, value, status = reading packed_data += struct.pack(\'B\', identifier) packed_data += struct.pack(\'f\', value) packed_data += struct.pack(\'h\', status) return packed_data"},{"question":"Objective: To evaluate the student\'s comprehension of Python\'s `io` module, specifically their ability to handle different types of I/O streams, buffering, and encoding. Problem Statement: You are required to create a function that reads a list of file paths and performs operations based on the content type of each file. The function will handle both text and binary files properly while demonstrating an understanding of buffering and error handling. Implement the function `process_files(file_paths)`, where: - `file_paths`: A list of strings, where each string is a path to a file. The function should perform the following: 1. For text files (with .txt extension): - Open the file using a buffered text stream. - Read the content, convert it all to uppercase, and write it to a new file with the same name appended with `_uppercase`. - Ensure the encoding used is \\"utf-8\\". - Handle any I/O or encoding errors gracefully. 2. For binary files (with .bin extension): - Open the file using a buffered binary stream. - Read the content and write it to a new file with the same name appended with `_copy`. - Ensure the operation is as efficient as possible. - Handle any I/O errors gracefully. Function Signature: ```python def process_files(file_paths: list[str]) -> None: pass ``` Constraints: - You can assume file paths are valid and files exist at the given paths. - Do not use external libraries other than Python\'s standard library. - Ensure proper resource management (use `with` statements for file operations). Example: ```python # Given input files \\"example.txt\\" with content \\"Hello World\\" and \\"data.bin\\" with some binary data. process_files([\\"example.txt\\", \\"data.bin\\"]) # This will create \\"example_uppercase.txt\\" with content \\"HELLO WORLD\\" # and \\"data_copy.bin\\" with the same binary content as \\"data.bin\\". ``` Note: Please do not actually create, read, or write any files when testing your function. Evaluation Criteria: 1. Correct handling of text and binary streams. 2. Proper implementation of buffering and encoding. 3. Effective error handling. 4. Clean and readable code with appropriate use of Python\'s `io` module functionalities.","solution":"import os from pathlib import Path def process_files(file_paths): for file_path in file_paths: file_extension = Path(file_path).suffix if file_extension == \\".txt\\": try: with open(file_path, \'r\', encoding=\'utf-8\', buffering=8192) as f: text_content = f.read() upper_text = text_content.upper() new_file_path = f\\"{file_path}_uppercase.txt\\" with open(new_file_path, \'w\', encoding=\'utf-8\', buffering=8192) as f: f.write(upper_text) except (IOError, UnicodeError) as e: print(f\\"An error occurred while processing the file {file_path}: {e}\\") elif file_extension == \\".bin\\": try: with open(file_path, \'rb\', buffering=8192) as f: binary_content = f.read() new_file_path = f\\"{file_path}_copy.bin\\" with open(new_file_path, \'wb\', buffering=8192) as f: f.write(binary_content) except IOError as e: print(f\\"An error occurred while processing the file {file_path}: {e}\\")"},{"question":"Python Grammar-Based Parsing Task # Objective: Your task is to implement a Python function to parse a subset of Python expressions and statements using the provided grammar rules. Specifically, you will be implementing a function that evaluates Python assignment statements and function definitions. # Task: You need to write a function `parse_python_code` that takes a string of Python code as input and returns a structured representation of the code according to the provided grammar. The representation should identify different components like assignment expressions and function definitions with their respective elements. # Details: 1. **Input:** - A single string `code` representing a Python assignment or function definition. 2. **Output:** - A dictionary representing the structured components of the parsed code. 3. **Constraints:** - The input `code` will be a valid Python assignment or function definition. - You must follow the grammar rules provided to parse the input string. 4. **Performance Requirements:** - The function should handle typical assignment statements and function definitions efficiently. - It should be able to parse expressions containing variables, basic operations, and function calls. # Examples: ```python def parse_python_code(code: str) -> dict: # Your implementation here # Example 1: code = \\"x = 5 + 3\\" output = parse_python_code(code) print(output) # Expected Output: # { # \\"type\\": \\"assignment\\", # \\"target\\": \\"x\\", # \\"value\\": { # \\"type\\": \\"expression\\", # \\"elements\\": [\\"5\\", \\"+\\", \\"3\\"] # } # } # Example 2: code = \\"def my_function(param1, param2=42):n return param1 + param2\\" output = parse_python_code(code) print(output) # Expected Output: # { # \\"type\\": \\"function_definition\\", # \\"name\\": \\"my_function\\", # \\"parameters\\": [ # {\\"name\\": \\"param1\\", \\"default\\": None}, # {\\"name\\": \\"param2\\", \\"default\\": \\"42\\"} # ], # \\"body\\": [ # { # \\"type\\": \\"return_statement\\", # \\"value\\": { # \\"type\\": \\"expression\\", # \\"elements\\": [\\"param1\\", \\"+\\", \\"param2\\"] # } # } # ] # } ``` # Notes: 1. You are required to parse only basic assignment statements and function definitions. You can assume that complex nested structures and edge cases won\'t be tested. 2. Focus on correctly identifying the main components and their relationships in the code. 3. Use the provided grammar rules to guide your parsing logic and ensure it matches the structure expected from valid Python code.","solution":"import ast def parse_python_code(code: str) -> dict: Parses a string representing a Python assignment or function definition and returns a structured representation of the components. # Parse the code into an AST tree = ast.parse(code) # Handling assignment if isinstance(tree.body[0], ast.Assign): target = tree.body[0].targets[0].id value = tree.body[0].value return { \\"type\\": \\"assignment\\", \\"target\\": target, \\"value\\": parse_expression(value) } # Handling function definition elif isinstance(tree.body[0], ast.FunctionDef): function_def = tree.body[0] parameters = [ {\\"name\\": arg.arg, \\"default\\": parse_expression(default) if default else None} for arg, default in zip(function_def.args.args, [None] * (len(function_def.args.args) - len(function_def.args.defaults)) + function_def.args.defaults) ] body = [parse_statement(stmt) for stmt in function_def.body] return { \\"type\\": \\"function_definition\\", \\"name\\": function_def.name, \\"parameters\\": parameters, \\"body\\": body } def parse_expression(expr): if isinstance(expr, ast.BinOp): return { \\"type\\": \\"expression\\", \\"elements\\": [parse_expression(expr.left), expr.op.__class__.__name__, parse_expression(expr.right)] } elif isinstance(expr, ast.Num): return str(expr.n) elif isinstance(expr, ast.Name): return expr.id elif isinstance(expr, ast.Str): return expr.s elif isinstance(expr, ast.Call): return { \\"type\\": \\"call\\", \\"function\\": parse_expression(expr.func), \\"args\\": [parse_expression(arg) for arg in expr.args] } return str(expr) def parse_statement(stmt): if isinstance(stmt, ast.Return): return { \\"type\\": \\"return_statement\\", \\"value\\": parse_expression(stmt.value) } return {} # Test the implementation if __name__ == \\"__main__\\": code1 = \\"x = 5 + 3\\" print(parse_python_code(code1)) # Expected Output: {\'type\': \'assignment\', \'target\': \'x\', \'value\': {\'type\': \'expression\', \'elements\': [\'5\', \'Add\', \'3\']}} code2 = \\"def my_function(param1, param2=42):n return param1 + param2\\" print(parse_python_code(code2)) # Expected Output: {\'type\': \'function_definition\', \'name\': \'my_function\', \'parameters\': [{\'name\': \'param1\', \'default\': None}, {\'name\': \'param2\', \'default\': \'42\'}], \'body\': [{\'type\': \'return_statement\', \'value\': {\'type\': \'expression\', \'elements\': [\'param1\', \'Add\', \'param2\']}}]}"},{"question":"<|Analysis Begin|> The provided documentation gives a fairly thorough overview of using the seaborn barplot function for different plotting needs. The basic usage of seaborn for loading datasets, customizing bar plots, and using various parameters like `hue`, and `errorbar` are discussed. There are examples of how to handle different types of dataset structures (long-form vs wide-form data), how to customize plot appearances, and how to add labels and annotations. To design a challenging coding assessment question, we can ask the students to create a complex visualization that combines several of these features. Given the documentation, students should already be familiar with basic and intermediate aspects of seaborn bar plots. Therefore, the question should aim to test their ability to integrate these functionalities cohesively. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Your task is to create a complex bar plot using seaborn that demonstrates your comprehension of various features and customizations available in the library. **Scenario:** From the seaborn\'s sample dataset \\"penguins\\", create a bar plot that demonstrates the following: 1. Plot the average `body_mass_g` for each `island`. 2. Subdivide each bar by the `sex` of the penguins, using different colors. 3. Display standard deviation as error bars. 4. Add labels showing average body mass values on each bar. 5. Add a custom annotation at a specific plot coordinate. 6. Ensure the plot is aesthetically refined with customizations on the appearance like bar edge colors, linewidth, and error bar customizations. **Expected Input and Output Formats:** *Input:* You do not need to provide any specific input because the dataset will be loaded from seaborn. *Output:* A seaborn bar plot with the aforementioned features. **Constraints:** - You must use the seaborn library for the visualization. - Follow the instructions to ensure readability and clarity in the plot. **Performance Requirements:** Your solution should be efficient and should not take more than a few seconds to render the plot on a standard machine. **Template Code:** ```python import seaborn as sns import matplotlib.pyplot as plt # Set the theme sns.set_theme(style=\\"whitegrid\\") # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the bar plot ax = sns.barplot( data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", errorbar=\\"sd\\", capsize=.2, err_kws={\\"color\\": \\".15\\"}, edgecolor=\\".2\\", linewidth=2.0 ) # Add labels ax.bar_label(ax.containers[0], fmt=\'%.2f\', fontsize=10) ax.bar_label(ax.containers[1], fmt=\'%.2f\', fontsize=10) # Add a custom annotation at plot coordinate ax.plot(2, 6000, \\"*\\", markersize=15, color=\\"r\\") ax.annotate(\'Max Body Mass\', xy=(2, 6000), xytext=(1.5, 6500), arrowprops=dict(facecolor=\'black\', shrink=0.05)) # Show plot plt.show() ``` Implement the code to generate the described plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_body_mass(): Create a seaborn bar plot for the penguins dataset showing average body mass per island, subdivided by the sex of the penguins, with standard deviation error bars. # Set the theme sns.set_theme(style=\\"whitegrid\\") # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the bar plot ax = sns.barplot( data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", errorbar=\\"sd\\", capsize=.2, err_kws={\\"color\\": \\".15\\"}, edgecolor=\\".2\\", linewidth=2.0 ) # Add labels to show average body mass on each bar for container in ax.containers: ax.bar_label(container, fmt=\'%.2f\', fontsize=10) # Add a custom annotation at plot coordinate ax.plot(2, 6000, \\"*\\", markersize=15, color=\\"r\\") ax.annotate(\'Max Body Mass\', xy=(2, 6000), xytext=(1.5, 6500), arrowprops=dict(facecolor=\'black\', shrink=0.05)) # Show plot plt.show() # Uncomment the line below to generate the plot when running the script directly # plot_penguin_body_mass()"},{"question":"**Advanced HTML Entity Translator** **Objective:** Your task is to implement a function that transforms a given text string by replacing all HTML5 named character references with their corresponding Unicode characters. You must use the `html5` dictionary from the `html.entities` module to perform this transformation. Additionally, your function should handle both the versions of named references, with or without a semicolon, seamlessly. **Function Signature:** ```python def html_entity_translate(input_str: str) -> str: pass ``` **Input:** - `input_str`: A string containing HTML5 named character references. (1 â‰¤ len(input_str) â‰¤ 10^6) **Output:** - Returns a string with all HTML5 named character references replaced with their corresponding Unicode characters. **Constraints:** - The input string will only contain valid HTML5 named character references. - Consider both forms of named references: with and without semicolon. **Examples:** ```python print(html_entity_translate(\\"The &gt; symbol is greater than &lt;.\\")) # Output: \\"The > symbol is greater than <.\\" print(html_entity_translate(\\"Apples & Oranges &amp; Bananas\\")) # Output: \\"Apples & Oranges & Bananas\\" print(html_entity_translate(\\"Use &copy; symbol for copyright.\\")) # Output: \\"Use Â© symbol for copyright.\\" ``` **Explanation:** 1. In the first example, `&gt;` and `&lt;` are replaced with `>` and `<` respectively. 2. In the second example, `&amp;` is replaced with `&`. 3. In the third example, `&copy;` is replaced with `Â©`. **Performance Requirements:** Your solution should efficiently handle the input string length of up to (10^6). **Note:** To implement this function, you may want to use regular expressions to identify and replace the named character references within the input string using the `html5` dictionary from the `html.entities` module.","solution":"import re from html.entities import html5 def html_entity_translate(input_str: str) -> str: Transforms a given text string by replacing all HTML5 named character references with their corresponding Unicode characters. # Using a regular expression to find all named character references reference_regex = re.compile(r\'&([a-zA-Z]+);?\') # Function to replace a match with its corresponding Unicode character def replace_entity(match): entity = match.group(1) # Check both forms with and without semicolon if entity in html5: return html5[entity] elif entity + \';\' in html5: return html5[entity + \';\'] return match.group(0) # return the match itself if not found # Perform the substitution return reference_regex.sub(replace_entity, input_str)"},{"question":"# Python \\"sys\\" Module Challenge: System Information and Subprocess Management You are tasked with creating a Python script that utilizes the \\"sys\\" module to perform the following tasks: 1. **Command-line Arguments Parsing:** Write a script that reads command-line arguments and executes different functionality based on the provided arguments. 2. **System Information Retrieval:** The script should be able to retrieve and display specific system information. 3. **Subprocess Creation and Management:** Implement a functionality to execute another Python script (or system command) as a subprocess and capture its output and errors. The subprocess should be run with specific environment settings. # Detailed Requirements: 1. **Command-line Arguments Parsing**: - The script should accept the following command-line arguments: - `--info` or `-i`: To display system information. - `--run <command>` or `-r <command>`: To run a specified system command or another Python script as a subprocess. - If no arguments are provided, it should print a usage message. 2. **System Information Retrieval**: - When the `--info` or `-i` flag is used, the script should display the following system information: - Python version and implementation details. - Maximum recursion limit. - Maximum size of Python integers. - Platform information. - Executable path of the Python interpreter. 3. **Subprocess Creation and Management**: - When the `--run <command>` or `-r <command>` flag is used, the script should: - Execute the specified command as a subprocess. - Capture and print the output and error streams of the subprocess. - Set a specific environment variable for the subprocess (`MY_ENV_VAR=\\"HelloWorld\\"`). # Example Usage: ```shell # Display system information python your_script.py --info # Run a command as a subprocess python your_script.py --run \\"python another_script.py\\" ``` # Constraints: - You should handle cases where the command-line arguments are missing or incorrect. - The output should be well-formatted for readability. # Expectations: - Your code should be robust and handle potential exceptions (e.g., subprocess errors, environment variable issues, etc.). - Use appropriate functions from the \\"sys\\" module to gather the system information and manage subprocesses. # Sample Output 1 (Using --info): ``` System Information: - Python Version: 3.10.0 (default, Oct 16 2021, 03:13:01) [GCC 9.3.0] - Python Implementation: CPython - Maximum Recursion Limit: 3000 - Maximum Integer Size: 9223372036854775807 - Platform: linux - Python Executable: /usr/bin/python3 ``` # Sample Output 2 (Using --run): ``` Running Command: python another_script.py Output: > \\"Hello from another script!\\" Errors: > (None) ``` # Notes: - You may use the `subprocess` module to handle subprocess creation and management. - Ensure that the script exits gracefully and provides meaningful error messages if something goes wrong.","solution":"import sys import platform import subprocess import os def display_system_info(): print(\\"System Information:\\") print(f\\"- Python Version: {platform.python_version()}\\") print(f\\"- Python Implementation: {platform.python_implementation()}\\") print(f\\"- Maximum Recursion Limit: {sys.getrecursionlimit()}\\") print(f\\"- Maximum Integer Size: {sys.maxsize}\\") print(f\\"- Platform: {platform.system().lower()}\\") print(f\\"- Python Executable: {sys.executable}\\") def run_subprocess(command): env = os.environ.copy() env[\'MY_ENV_VAR\'] = \\"HelloWorld\\" try: result = subprocess.run( command, shell=True, capture_output=True, text=True, env=env ) print(f\\"Running Command: {command}\\") print(\\"Output:\\") print(f\\"> {result.stdout.strip()}\\") if result.stderr: print(\\"Errors:\\") print(f\\"> {result.stderr.strip()}\\") else: print(\\"Errors:\\") print(\\"> (None)\\") except Exception as e: print(f\\"An error occurred while running the command: {str(e)}\\") def main(): if len(sys.argv) < 2: print(\\"Usage: \\") print(\\" python your_script.py --info\\") print(\\" python your_script.py --run <command>\\") return if sys.argv[1] in (\'--info\', \'-i\'): display_system_info() elif sys.argv[1] in (\'--run\', \'-r\') and len(sys.argv) > 2: command = \\" \\".join(sys.argv[2:]) run_subprocess(command) else: print(\\"Unknown or incorrect arguments provided.\\") print(\\"Usage: \\") print(\\" python your_script.py --info\\") print(\\" python your_script.py --run <command>\\") if __name__ == \\"__main__\\": main()"},{"question":"# **Advanced Signal Handling in Python** # Objective Write a Python program demonstrating advanced usage of the `signal` module. Your task is to implement a robust signal handling mechanism that can handle multiple specific signals, perform a timed operation, and ensure proper cleanup before the program exits. # Requirements 1. **Signal Handling**: - Set up custom handlers for `SIGINT`, `SIGALRM`, and `SIGTERM`. - The `SIGINT` handler should print \\"SIGINT received. Cleaning up...\\" and then perform a cleanup operation. - The `SIGALRM` handler should simply print \\"SIGALRM: Operation timed out.\\" - The `SIGTERM` handler should print \\"SIGTERM received. Shutting down...\\" and then exit the program gracefully. 2. **Timed Operation**: - Implement a long-running operation (e.g., a loop with sleep intervals) that can be interrupted by `SIGALRM` after a specific duration. - Set the alarm before starting the operation; duration should be passed as a parameter (`timeout`). 3. **Cleanup Operation**: - Implement a cleanup operation that will release resources or print a final message. # Function Signature ```python import signal import time import sys import os def long_running_operation(timeout): # Implement the function logic def sigint_handler(signum, frame): # Implement the SIGINT handler def sigalrm_handler(signum, frame): # Implement the SIGALRM handler def sigterm_handler(signum, frame): # Implement the SIGTERM handler def cleanup(): # Implement the cleanup logic def main(): # This function will setup signal handling and call long_running_operation ``` # Input - The input will be automatically passed to `main()`. # Output - Print appropriate messages as signal handlers are triggered. - Ensure the long-running operation respects the alarm and exits gracefully. # Constraints - Assume the `timeout` is a positive integer representing seconds. # Example ```python def main(): signal.signal(signal.SIGINT, sigint_handler) signal.signal(signal.SIGALRM, sigalrm_handler) signal.signal(signal.SIGTERM, sigterm_handler) # Set an alarm for 5 seconds long_running_operation(5) if __name__ == \\"__main__\\": main() ``` This question will test the candidate\'s understanding of signal handling, timeout operations, cleanup procedures, and ability to integrate these aspects effectively in Python.","solution":"import signal import time import sys def cleanup(): print(\\"Performing cleanup operations...\\") def sigint_handler(signum, frame): print(\\"SIGINT received. Cleaning up...\\") cleanup() sys.exit(0) def sigalrm_handler(signum, frame): print(\\"SIGALRM: Operation timed out.\\") def sigterm_handler(signum, frame): print(\\"SIGTERM received. Shutting down...\\") cleanup() sys.exit(0) def long_running_operation(timeout): signal.alarm(timeout) # Set an alarm try: print(f\\"Starting long-running operation with a timeout of {timeout} seconds.\\") for i in range(timeout * 2): # Simulate long-running operation print(f\\"Running... {i+1}\\") time.sleep(1) # Sleep to simulate work except Exception as e: print(f\\"Exception caught during operation: {e}\\") finally: cleanup() print(\\"Operation completed.\\") def main(): signal.signal(signal.SIGINT, sigint_handler) signal.signal(signal.SIGALRM, sigalrm_handler) signal.signal(signal.SIGTERM, sigterm_handler) # Example with a timeout of 5 seconds long_running_operation(5) if __name__ == \\"__main__\\": main()"},{"question":"Objective: Demonstrate your understanding of pandas\' nullable Boolean data type and Kleene logical operations by performing various tasks. Tasks: 1. **Creating Nullable Boolean Series:** - Create a pandas Series `s` with the values `[10, 20, 30, pd.NA, 50]` and dtype `Int64`. 2. **Boolean Indexing with `NA`:** - Create a Boolean mask `mask` with the values `[True, False, pd.NA, True, pd.NA]` and dtype `boolean`. - Use this mask to index the Series `s` and display the result. Then fill the `pd.NA` in the mask with `True` and index `s` again, displaying the new result. 3. **Nullable Boolean DataFrame:** - Create a DataFrame `df` with two columns \'A\' and \'B\', where: - \'A\' contains `[True, pd.NA, False, pd.NA, True]` with dtype `boolean`. - \'B\' contains `[pd.NA, False, pd.NA, True, False]` with dtype `boolean`. 4. **Applying Kleene Logical Operations:** - Perform the `&` (AND) logical operation between \'A\' and \'B\' columns of the DataFrame and display the output. - Perform the `|` (OR) logical operation between \'A\' and \'B\' columns of the DataFrame and display the output. Constraints: - You must handle `pd.NA` values in the Series and DataFrame correctly according to pandas\' nullable Boolean data type specifications. Expected Output: - Properly indexed Series results. - Correctly generated DataFrame. - Accurate results from logical operations, reflecting Kleene logic. Sample Code: ```python import pandas as pd # Step 1: Create the Series s = pd.Series([10, 20, 30, pd.NA, 50], dtype=\'Int64\') # Step 2: Boolean mask and indexing mask = pd.array([True, False, pd.NA, True, pd.NA], dtype=\'boolean\') print(s[mask]) # Index with NA as False print(s[mask.fillna(True)]) # Index with NA filled as True # Step 3: Nullable Boolean DataFrame df = pd.DataFrame({ \'A\': pd.Series([True, pd.NA, False, pd.NA, True], dtype=\'boolean\'), \'B\': pd.Series([pd.NA, False, pd.NA, True, False], dtype=\'boolean\') }) print(df) # Step 4: Kleene logical operations print(df[\'A\'] & df[\'B\']) # AND operation print(df[\'A\'] | df[\'B\']) # OR operation ``` Submission: Submit your code implementation in a Python script or Jupyter notebook file.","solution":"import pandas as pd # Step 1: Create the Series s = pd.Series([10, 20, 30, pd.NA, 50], dtype=\'Int64\') # Step 2: Boolean mask and indexing mask = pd.array([True, False, pd.NA, True, pd.NA], dtype=\'boolean\') indexed_s_with_na = s[mask] # Index with NA as False indexed_s_with_na_filled = s[mask.fillna(True)] # Index with NA filled as True # Step 3: Nullable Boolean DataFrame df = pd.DataFrame({ \'A\': pd.Series([True, pd.NA, False, pd.NA, True], dtype=\'boolean\'), \'B\': pd.Series([pd.NA, False, pd.NA, True, False], dtype=\'boolean\') }) # Step 4: Kleene logical operations and_result = df[\'A\'] & df[\'B\'] # AND operation or_result = df[\'A\'] | df[\'B\'] # OR operation"},{"question":"**Question: Initializing Neural Network Parameters with PyTorch** In this exercise, you will be tasked with initializing the parameters (weights and biases) of a simple feed-forward neural network using PyTorch\'s random number generation functionality. This will demonstrate your understanding of both random number generation and neural network initialization processes using PyTorch. # Requirements: 1. You must implement a function, `initialize_parameters`, that takes an input size (`in_features`), an output size (`out_features`), and a list of hidden layer sizes (`hidden_sizes`), and returns a dictionary containing initialized weights and biases for each layer. 2. Initialize weights from a uniform distribution between -1 and 1. 3. Initialize biases as zeros. 4. Ensure that weights and biases are PyTorch tensors with the appropriate dimensions. # Constraints: - Use PyTorch version >= 1.0. - Do not use any high-level libraries or modules outside of PyTorch (`torch` and `torch.random` are allowed). - You are not allowed to initialize biases with any value other than zero. # Function Signature: ```python def initialize_parameters(in_features: int, out_features: int, hidden_sizes: list) -> dict: pass ``` # Expected Input and Output: - **Input:** `in_features` (int), `out_features` (int), `hidden_sizes` (list of int) - **Output:** A dictionary with keys \'weights\' and \'biases\', where: - `\'weights\'` contains a list of weight tensors for each layer. - `\'biases\'` contains a list of bias tensors for each layer. # Example: ```python # Example input in_features = 3 out_features = 1 hidden_sizes = [4, 2] # Expected output (formats and sizes should match; values will differ) { \'weights\': [ torch.Tensor of shape (4, 3), # from input to first hidden layer torch.Tensor of shape (2, 4), # from first hidden to second hidden layer torch.Tensor of shape (1, 2) # from second hidden to output layer ], \'biases\': [ torch.Tensor of shape (4,), # for first hidden layer torch.Tensor of shape (2,), # for second hidden layer torch.Tensor of shape (1,) # for output layer ] } ``` You should test your function with a variety of inputs to ensure correctness. The focus should be on using `torch.random` effectively for weight initialization and adhering to the specified constraints.","solution":"import torch def initialize_parameters(in_features: int, out_features: int, hidden_sizes: list) -> dict: Initializes the weights and biases for a feed-forward neural network. Args: - in_features (int): Number of input features. - out_features (int): Number of output features. - hidden_sizes (list of int): A list where each element represents the size of a hidden layer. Returns: - dict: A dictionary containing initialized weights and biases for each layer. layers = [in_features] + hidden_sizes + [out_features] weights = [] biases = [] for i in range(len(layers) - 1): weight = torch.rand(layers[i+1], layers[i]) * 2 - 1 # Uniform distribution in [-1, 1] bias = torch.zeros(layers[i+1]) weights.append(weight) biases.append(bias) return { \'weights\': weights, \'biases\': biases }"},{"question":"Objective: Demonstrate your understanding of PyTorch\'s tensor and module serialization and deserialization, as well as managing view relationships effectively. Problem Statement: You are provided with a dataset containing a list of PyTorch tensors. Additionally, a simple neural network defined as a PyTorch module is also provided. Your task is to implement a function that: 1. Saves the dataset of tensors and the state dictionary of the neural network model to a file. 2. Loads the dataset and the neural network model state from the file. 3. Adjusts for any tensor view relationships to ensure minimal file size without losing view semantics. Function Signature: ```python import torch import torch.nn as nn def save_tensors_and_model(tensors_list, model, file_path): Saves a list of tensors and a model\'s state dict to a file. Parameters: tensors_list (list of torch.Tensor): List of tensors to be saved. model (nn.Module): Neural network whose state dict needs to be saved. file_path (str): The file path where tensors and model state will be saved. # Your code here def load_tensors_and_model(file_path, model_class): Loads a list of tensors and a model\'s state dict from a file. Parameters: file_path (str): The file path from which tensors and model state will be loaded. model_class (type): The class of the model to be re-instantiated. Returns: tuple: (list of torch.Tensor, nn.Module) loaded tensors and the model with its state dict # Your code here class SimpleModel(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x ``` Input: - `tensors_list`: A list of `torch.Tensor` objects. - `model`: An instance of `nn.Module`. - `file_path`: A string representing the file path. - `model_class`: A class of the neural network model. Output: - `load_tensors_and_model` should return a tuple: (list of `torch.Tensor`, `nn.Module`). Constraints: - The model should be re-instantiated from the provided class during the loading process. - Ensure that the loaded tensors preserve their original view relationships. - Optimize file size by avoiding unnecessary large storage saving. - The function should handle both cloning and preserving views appropriately. Example Usage: ```python # Create some tensors with views t1 = torch.arange(1, 10) t2 = t1[::2] # A view of the first tensor # A list of tensors tensors_list = [t1, t2] # Instantiate the model model = SimpleModel(input_dim=4, hidden_dim=5, output_dim=3) # Save the tensors and model state save_tensors_and_model(tensors_list, model, \'data_and_model.pt\') # Load the tensors and model loaded_tensors_list, loaded_model = load_tensors_and_model(\'data_and_model.pt\', SimpleModel) # Verify the loaded data assert torch.equal(loaded_tensors_list[0], t1) assert torch.equal(loaded_tensors_list[1], t2) assert isinstance(loaded_model, SimpleModel) ``` Notes: - Utilize `torch.save` and `torch.load` functions as needed. - Pay attention to reducing file size when saving views. - Ensure the correct restoration of the neural network\'s parameters.","solution":"import torch import torch.nn as nn def save_tensors_and_model(tensors_list, model, file_path): Saves a list of tensors and a model\'s state dict to a file. Parameters: tensors_list (list of torch.Tensor): List of tensors to be saved. model (nn.Module): Neural network whose state dict needs to be saved. file_path (str): The file path where tensors and model state will be saved. views_info = [] base_tensors = [] # Process tensors to identify base tensors and views for tensor in tensors_list: if tensor._base is not None: base_idx = base_tensors.index(tensor._base) if tensor._base in base_tensors else len(base_tensors) if tensor._base not in base_tensors: base_tensors.append(tensor._base) views_info.append((base_idx, tensor.storage_offset(), tensor.size(), tensor.stride())) else: base_tensors.append(tensor) views_info.append((-1,)) # Construct the save dict save_dict = { \'model_state_dict\': model.state_dict(), \'tensors\': base_tensors, \'views_info\': views_info } torch.save(save_dict, file_path) def load_tensors_and_model(file_path, model_class): Loads a list of tensors and a model\'s state dict from a file. Parameters: file_path (str): The file path from which tensors and model state will be loaded. model_class (type): The class of the model to be re-instantiated. Returns: tuple: (list of torch.Tensor, nn.Module) loaded tensors and the model with its state dict # Load the saved dict save_dict = torch.load(file_path) model = model_class() # Instantiate the model model.load_state_dict(save_dict[\'model_state_dict\']) tensors = save_dict[\'tensors\'] views_info = save_dict[\'views_info\'] loaded_tensors = [] for info in views_info: if info[0] == -1: loaded_tensors.append(tensors.pop(0)) else: base_idx, offset, size, stride = info base_tensor = loaded_tensors[base_idx] loaded_tensors.append(torch.as_strided(base_tensor, size, stride, offset)) return loaded_tensors, model class SimpleModel(nn.Module): def __init__(self, input_dim=4, hidden_dim=5, output_dim=3): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x"},{"question":"# Compression Assignment: Implementing a Custom Compression Pipeline Objective Design a function that reads a text file, compresses its contents incrementally using specified custom filter chains, and writes the compressed data to a new file. This function should handle and report any compression errors. Function Signature ```python def custom_compress(input_file: str, output_file: str, filters: list) -> None: pass ``` Input 1. `input_file` (str): The path to the input text file that needs to be compressed. 2. `output_file` (str): The path where the compressed file will be stored. 3. `filters` (list): A list of dictionaries where each dictionary specifies a filter and its options (if any) to be used in the compression process. Output The function does not return any value, but it writes the compressed data to the `output_file`. Constraints 1. If the `input_file` does not exist, the function should raise a `FileNotFoundError`. 2. If there is any error during the compression process, the function should raise an `lzma.LZMAError` with a proper error message. 3. The function should use incremental compression, i.e., it should compress the data in chunks to handle large files efficiently. 4. The function should ensure that the specified filters are valid according to the `lzma` module\'s requirements. Example ```python input_file = \\"example.txt\\" output_file = \\"example.xz\\" filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6 | lzma.PRESET_EXTREME}, ] custom_compress(input_file, output_file, filters) ``` Bonus (Optional) 1. Implement an additional function `custom_decompress` that reads the compressed file produced by `custom_compress` and writes the decompressed data to a new text file, ensuring that the data matches the original. Documentation to Review - `lzma.LZMAFile` - `lzma.LZMACompressor` - `lzma.compress` - Specifying custom filter chains Implement the `custom_compress` function making sure to handle all specified constraints, and consider efficiency and proper error handling techniques.","solution":"import lzma import os def custom_compress(input_file: str, output_file: str, filters: list) -> None: if not os.path.exists(input_file): raise FileNotFoundError(f\\"No such file: \'{input_file}\'\\") try: with open(input_file, \'rb\') as fin: data = fin.read() compressor = lzma.LZMACompressor(format=lzma.FORMAT_XZ, filters=filters) with open(output_file, \'wb\') as fout: fout.write(compressor.compress(data)) fout.write(compressor.flush()) except Exception as e: raise lzma.LZMAError(f\\"Compression error: {str(e)}\\")"},{"question":"Objective: Create a custom class to demonstrate your understanding of abstract base classes in the `collections.abc` module and the use of mixin methods. Problem Statement: Implement a class `UniqueList`, which behaves like a list allowing only unique elements and supports all list operations. This class should leverage the `collections.abc.MutableSequence` abstract base class and provide the required functionality. Also, additional functionalities like filtering elements and creating sublists should be supported. Requirements: 1. **Class Inheritance**: The class should inherit from `collections.abc.MutableSequence`. 2. **Abstract Methods**: Implement the required abstract methods: `__getitem__`, `__setitem__`, `__delitem__`, `__len__`, `insert`. 3. **Mixin Methods**: Utilize mixin methods provided by `MutableSequence`. 4. **Unique Elements**: Ensure all elements in `UniqueList` are unique. 5. **Filtering Functionality**: Implement a `filter_elements` method that accepts a function and returns a new `UniqueList` containing elements that satisfy the function. 6. **Sublists**: Implement a `sublist` method that returns a `UniqueList` from a given start to end index. Constraints: - The data elements in the list must be hashable. - The class should raise appropriate exceptions for operations that violate the uniqueness constraint. Performance Requirement: - Aim to have efficient methods for element lookup, insertion, and deletion. Example Usage: ```python from collections.abc import MutableSequence class UniqueList(MutableSequence): def __init__(self, iterable=None): self.items = [] if iterable: for item in iterable: self.append(item) def __getitem__(self, index): return self.items[index] def __setitem__(self, index, value): if value in self.items and self.items[index] != value: raise ValueError(\\"Duplicate elements are not allowed.\\") self.items[index] = value def __delitem__(self, index): del self.items[index] def __len__(self): return len(self.items) def insert(self, index, value): if value in self.items: raise ValueError(\\"Duplicate elements are not allowed.\\") self.items.insert(index, value) def filter_elements(self, func): filtered = filter(func, self.items) return UniqueList(filtered) def sublist(self, start, end): return UniqueList(self.items[start:end]) # Example: ulist = UniqueList([1, 2, 3, 4]) ulist.append(5) print(ulist) # Output should be: [1, 2, 3, 4, 5] ulist.append(3) # Raise ValueError: Duplicate elements are not allowed. filtered = ulist.filter_elements(lambda x: x % 2 == 0) print(filtered) # Output should be: [2, 4] sublist = ulist.sublist(1, 3) print(sublist) # Output should be: [2, 3] ``` Your task is to complete the implementation of the `UniqueList` class fulfilling the above requirements and constraints. Ensure to test the class with various scenarios to validate its correctness.","solution":"from collections.abc import MutableSequence class UniqueList(MutableSequence): def __init__(self, iterable=None): self.items = [] if iterable: for item in iterable: if item in self.items: raise ValueError(\\"Duplicate elements are not allowed.\\") self.items.append(item) def __getitem__(self, index): return self.items[index] def __setitem__(self, index, value): if value in self.items and self.items[index] != value: raise ValueError(\\"Duplicate elements are not allowed.\\") self.items[index] = value def __delitem__(self, index): del self.items[index] def __len__(self): return len(self.items) def insert(self, index, value): if value in self.items: raise ValueError(\\"Duplicate elements are not allowed.\\") self.items.insert(index, value) def filter_elements(self, func): filtered = filter(func, self.items) return UniqueList(filtered) def sublist(self, start, end): return UniqueList(self.items[start:end]) def __repr__(self): return f\\"UniqueList({self.items})\\""},{"question":"# Index Manipulation and Analysis with pandas Objective: Write a function that takes a sample dataframe and performs multiple operations using pandas Index functionalities. Function Signature: ```python import pandas as pd def index_operations(data: pd.DataFrame) -> dict: pass ``` Input: - `data`: A pandas DataFrame with at least the following columns: `id`, `date`, `category`, and `value`. Output: - A dictionary with the following keys and corresponding values: - `\'unique_ids\'`: A boolean indicating whether the `id` column of the DataFrame has unique values. - `\'sorted_dates\'`: The DataFrame sorted by the `date` column. - `\'date_range\'`: The range (difference between max and min) of the `date` column values. - `\'category_value_counts\'`: A pandas Series counting the occurrences of each unique value in the `category` column. - `\'missing_values\'`: A boolean indicating whether there are any missing values in the `value` column. - `\'grouped_means\'`: A DataFrame with the mean `value` for each combination of `id` and `category`. Constraints: - Use appropriate Index operations and avoid using loops where vectorized operations are applicable. - Ensure that the function efficiently handles datasets with up to 1 million rows. Example: ```python import pandas as pd data = pd.DataFrame({ \'id\': [1, 2, 1, 2, 3], \'date\': pd.to_datetime([\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-01\', \'2023-01-02\']), \'category\': [\'A\', \'B\', \'A\', \'B\', \'A\'], \'value\': [10, 20, 30, None, 50] }) result = index_operations(data) print(result) ``` Expected Output: ```python { \'unique_ids\': False, \'sorted_dates\': A DataFrame sorted by \'date\' column, \'date_range\': 2 days, \'category_value_counts\': A 3 B 2 dtype: int64, \'missing_values\': True, \'grouped_means\': id category value 0 1 A 20.0 1 2 B 20.0 2 3 A 50.0 } ```","solution":"import pandas as pd def index_operations(data: pd.DataFrame) -> dict: result = {} # Check if `id` column has unique values result[\'unique_ids\'] = data[\'id\'].is_unique # Sort DataFrame by `date` column sorted_dates = data.sort_values(by=\'date\') result[\'sorted_dates\'] = sorted_dates # Calculate range of `date` column date_range = data[\'date\'].max() - data[\'date\'].min() result[\'date_range\'] = date_range # Count occurrences of each unique value in `category` column category_value_counts = data[\'category\'].value_counts() result[\'category_value_counts\'] = category_value_counts # Check for any missing values in the `value` column missing_values = data[\'value\'].isnull().any() result[\'missing_values\'] = missing_values # Calculate mean `value` for each combination of `id` and `category` grouped_means = data.groupby([\'id\', \'category\'])[\'value\'].mean().reset_index() result[\'grouped_means\'] = grouped_means return result"},{"question":"# Python Coding Assessment: Implementing Custom `pydoc` Behavior Objective: Design and implement a Python function that extends the functionality of the `pydoc` module by creating a custom documentation generator that can output module documentation in a structured JSON format. Function Specifications: - **Function Name:** `generate_module_docs` - **Input:** - `module_name` (str): The name of the module for which to generate documentation. - **Output:** - A JSON string that contains the documentation in a structured format. Requirements: 1. Your solution should automatically import the specified module using `importlib.import_module`. 2. Extract module-level docstring, class-level docstrings, and function-level docstrings within the module. 3. The output JSON should include: - `module_name`: The name of the module. - `module_doc`: The module\'s docstring (if available). - `classes`: A list containing dictionaries for each class in the module. - Each dictionary should include: - `class_name`: The name of the class. - `class_doc`: The class\'s docstring. - `methods`: A list of dictionaries for each method in the class with: - `method_name`: The name of the method. - `method_doc`: The method\'s docstring. - `functions`: A list containing dictionaries for each function in the module. - Each dictionary should include: - `function_name`: The name of the function. - `function_doc`: The function\'s docstring. Constraints: - You must handle exceptions appropriately. - Assume that the module names provided as input are valid Python modules available in the Python path. Example Output: ```json { \\"module_name\\": \\"example_module\\", \\"module_doc\\": \\"This is an example module.\\", \\"classes\\": [ { \\"class_name\\": \\"ExampleClass\\", \\"class_doc\\": \\"This is an example class.\\", \\"methods\\": [ { \\"method_name\\": \\"example_method\\", \\"method_doc\\": \\"This is an example method.\\" } ] } ], \\"functions\\": [ { \\"function_name\\": \\"example_function\\", \\"function_doc\\": \\"This is an example function.\\" } ] } ``` Additional Notes: - You may use the `inspect` module to retrieve docstrings. - Ensure your JSON output is pretty-printed for readability. - Write a small test script to demonstrate the functionality of your implementation.","solution":"import importlib import inspect import json def generate_module_docs(module_name): try: # Import the module module = importlib.import_module(module_name) except ModuleNotFoundError: return json.dumps({\\"error\\": f\\"Module \'{module_name}\' not found\\"}, indent=4) module_doc = inspect.getdoc(module) or \\"No module docstring available\\" classes = [] functions = [] for name, obj in inspect.getmembers(module): if inspect.isclass(obj): class_doc = inspect.getdoc(obj) or \\"No class docstring available\\" methods = [] for method_name, method_obj in inspect.getmembers(obj): if inspect.isfunction(method_obj): method_doc = inspect.getdoc(method_obj) or \\"No method docstring available\\" methods.append({ \\"method_name\\": method_name, \\"method_doc\\": method_doc, }) classes.append({ \\"class_name\\": name, \\"class_doc\\": class_doc, \\"methods\\": methods }) elif inspect.isfunction(obj): function_doc = inspect.getdoc(obj) or \\"No function docstring available\\" functions.append({ \\"function_name\\": name, \\"function_doc\\": function_doc }) # The structured documentation as a dictionary doc_structure = { \\"module_name\\": module_name, \\"module_doc\\": module_doc, \\"classes\\": classes, \\"functions\\": functions } # Convert to pretty-printed JSON return json.dumps(doc_structure, indent=4)"},{"question":"# Custom Command Line Interpreter In this exercise, you will create a custom command line interpreter using the `cmd` module to simulate a simple calculator shell. This shell should support basic arithmetic operations and maintain a history of the operations performed. Requirements: 1. **Create a class `CalculatorShell` that inherits from `cmd.Cmd`:** - This class should define methods to handle basic arithmetic operations: addition, subtraction, multiplication, and division. - Each method should be prefixed with `do_`, as per the `cmd` module requirements. - Each method should parse the arguments, perform the operation, and print the result. 2. **Implement the following commands:** - `do_add(x, y)`: Add two numbers. - `do_subtract(x, y)`: Subtract two numbers. - `do_multiply(x, y)`: Multiply two numbers. - `do_divide(x, y)`: Divide two numbers. 3. **Implement additional features:** - Command to print the history of operations performed (`do_history()`). - Command to exit the shell (`do_exit()`), which should terminate the `cmdloop` method. 4. **Add appropriate docstrings for the commands** to enable the `help` functionality. 5. **Handle erroneous inputs** gracefully by printing an error message. Example Session: ``` python calculator_shell.py Welcome to the calculator shell. Type help or ? to list commands. (calc) add 10 20 30 (calc) subtract 50 15 35 (calc) multiply 4 5 20 (calc) divide 100 4 25.0 (calc) history 1. add 10 20 = 30 2. subtract 50 15 = 35 3. multiply 4 5 = 20 4. divide 100 4 = 25.0 (calc) exit Thank you for using the calculator shell! ``` # Constraints: - Do not use any external libraries except for built-in modules. - Ensure that division by zero is handled properly. - The shell should be user-friendly and should guide the user with helpful messages and a banner. # Implementation Notes: - You can use a list to maintain the history of commands. - Consider using Python\'s exception handling to manage errors in input and operations. - Ensure that the shell correctly identifies numbers (integers and floats) and performs accurate calculations.","solution":"import cmd class CalculatorShell(cmd.Cmd): intro = \'Welcome to the calculator shell. Type help or ? to list commands.n\' prompt = \'(calc) \' history = [] def do_add(self, arg): \'Add two numbers: add 10 20\' try: x, y = map(float, arg.split()) result = x + y self.history.append(f\\"add {x} {y} = {result}\\") print(result) except ValueError: print(\\"Error: Please provide two numbers.\\") def do_subtract(self, arg): \'Subtract two numbers: subtract 50 15\' try: x, y = map(float, arg.split()) result = x - y self.history.append(f\\"subtract {x} {y} = {result}\\") print(result) except ValueError: print(\\"Error: Please provide two numbers.\\") def do_multiply(self, arg): \'Multiply two numbers: multiply 4 5\' try: x, y = map(float, arg.split()) result = x * y self.history.append(f\\"multiply {x} {y} = {result}\\") print(result) except ValueError: print(\\"Error: Please provide two numbers.\\") def do_divide(self, arg): \'Divide two numbers: divide 100 4\' try: x, y = map(float, arg.split()) if y == 0: print(\\"Error: Division by zero.\\") else: result = x / y self.history.append(f\\"divide {x} {y} = {result}\\") print(result) except ValueError: print(\\"Error: Please provide two numbers.\\") def do_history(self, arg): \'Print the history of operations performed\' for index, record in enumerate(self.history, 1): print(f\\"{index}. {record}\\") def do_exit(self, arg): \'Exit the calculator shell\' print(\'Thank you for using the calculator shell!\') return True if __name__ == \'__main__\': CalculatorShell().cmdloop()"},{"question":"# WAV File Processing Task You are given two WAV files: an input file `input.wav` and an output file `output.wav`. Your task is to read the audio data from `input.wav`, apply a simple audio transformation, and write the transformed audio data to `output.wav`. The transformation to be applied is reversing the audio data. Implement a function `reverse_audio(input_file: str, output_file: str) -> None` that performs the following steps: 1. Open the input WAV file in read mode. 2. Retrieve and store the parameters of the input file (number of channels, sample width, frame rate, etc.). 3. Read all audio frames from the input file. 4. Reverse the order of the audio frames. 5. Open the output WAV file in write mode. 6. Set the parameters for the output file to match the input file. 7. Write the reversed audio frames to the output file. 8. Ensure that the output file is correctly closed. **Function Signature:** ```python def reverse_audio(input_file: str, output_file: str) -> None: pass ``` **Input:** - `input_file`: A string representing the file path of the input WAV file (e.g., `\\"input.wav\\"`). - `output_file`: A string representing the file path of the output WAV file (e.g., `\\"output.wav\\"`). **Output:** - The function does not return any value. It creates/overwrites the `output_file` with the reversed audio content from `input_file`. **Constraints:** - The input WAV file will be in WAVE_FORMAT_PCM format and will not be larger than 100MB in size. **Example:** Assume you have a WAV file named `example.wav` with certain audio content. After calling your function with this file as the input, the `reversed_example.wav` file should contain the audio content played in reverse. ```python reverse_audio(\\"example.wav\\", \\"reversed_example.wav\\") ``` The above function call should process `example.wav` and produce `reversed_example.wav` with the reversed audio data. **Note:** - Handle any file I/O operations carefully to avoid resource leaks. - Make sure to properly close files after reading/writing operations. Good luck!","solution":"import wave def reverse_audio(input_file: str, output_file: str) -> None: Reverse the audio data from input_file and save it to output_file. # Open the input WAV file with wave.open(input_file, \'rb\') as infile: # Retrieve the parameters of the input file params = infile.getparams() n_frames = infile.getnframes() # Read all audio frames audio_frames = infile.readframes(n_frames) # Reverse the order of the audio frames reversed_frames = audio_frames[::-1] # Open the output WAV file in write mode with wave.open(output_file, \'wb\') as outfile: # Set the parameters for the output file to match the input file outfile.setparams(params) # Write the reversed audio frames to the output file outfile.writeframes(reversed_frames)"},{"question":"**Question: Implement a Simple Python Interpreter** You are required to implement a simple Python interpreter using the `codeop` module. The interpreter should accept a sequence of code lines (in the form of a list of strings) and execute them in sequence. Your implementation should utilize the `codeop.CommandCompiler` class to handle line-by-line input and should account for the presence of `__future__` statements. # Function Specification **Function Name**: `simple_interpreter` **Input**: - `code_lines` (List of strings): A list of strings where each string is a line of Python code. **Output**: - The function should execute the given lines of code in sequence and return the output of the last executed expression, if any. If the last line is a statement, return `None`. **Constraints**: - You should compile and execute each line only if it forms a complete statement. - Handle any syntax errors gracefully and stop execution if an error is encountered, returning `None`. # Example ```python def simple_interpreter(code_lines): # Your implementation here # Sample Input code = [ \\"a = 10\\", \\"b = 20\\", \\"a + b\\" ] # Sample Output print(simple_interpreter(code)) # Output should be 30 # Sample Input with Syntax Error code_with_error = [ \\"a = 10\\", \\"b = 20\\" \\"a +\\" ] # Sample Output print(simple_interpreter(code_with_error)) # Output should be None ``` # Additional Details - Use the `codeop.CommandCompiler` class for handling the compilation of code lines. - Make sure to handle the preservation of future statements if any are encountered by using the functionality provided by `codeop.CommandCompiler`. - Ensure graceful error handling to stop further execution upon encountering a syntax error. **Hints**: - Consider using `exec()` and `eval()` functions to run the compiled code objects. - Keep track of the results of expressions to return the correct final output.","solution":"import codeop def simple_interpreter(code_lines): Executes a list of Python code lines and returns the result of the last expression. Args: code_lines (list of str): A list of strings where each string is a line of Python code. Returns: The result of the last expression if any, otherwise None. compiler = codeop.CommandCompiler() compiled_code = None namespace = {} for line in code_lines: try: compiled_code = compiler(line) if compiled_code: exec(compiled_code, namespace) except SyntaxError: return None if compiled_code: try: result = eval(line, namespace) except: result = None return result return None"},{"question":"# Model Performance Evaluation with Validation and Learning Curves In this exercise, you\'ll be working with the `scikit-learn` package to evaluate the performance of a machine learning model using validation and learning curves. You are provided with a dataset, and you need to perform the following tasks: 1. **Implement a function `evaluate_model_with_validation_curve`**: - This function should use the `validation_curve` function from `scikit-learn` to plot the training and validation scores for a given model and hyperparameter. - **Input**: - `X` (numpy.ndarray): The input features. - `y` (numpy.ndarray): The target values. - `model` (sklearn estimator): The machine learning model to evaluate. - `param_name` (str): The name of the hyperparameter to evaluate. - `param_range` (array-like): The range of values for the hyperparameter. - **Output**: - A plot displaying the validation curves for the training and validation scores. 2. **Implement a function `evaluate_model_with_learning_curve`**: - This function should use the `learning_curve` function from `scikit-learn` to plot the training and validation scores as the number of training samples varies. - **Input**: - `X` (numpy.ndarray): The input features. - `y` (numpy.ndarray): The target values. - `model` (sklearn estimator): The machine learning model to evaluate. - `train_sizes` (array-like): The number of training samples to use for generating the curve. - `cv` (int): The number of cross-validation folds. - **Output**: - A plot displaying the learning curves for the training and validation scores. # Example Usage Here is an example usage of these functions with the Iris dataset and an SVM model: ```python from sklearn.datasets import load_iris from sklearn.svm import SVC import numpy as np # Load the dataset X, y = load_iris(return_X_y=True) # Shuffle the data np.random.seed(0) indices = np.arange(y.shape[0]) np.random.shuffle(indices) X, y = X[indices], y[indices] # Define the model model = SVC(kernel=\'linear\') # Define the range for hyperparameter C param_range = np.logspace(-7, 3, 10) # Evaluate using validation curve evaluate_model_with_validation_curve(X, y, model, \'C\', param_range) # Define the training sizes train_sizes = np.linspace(0.1, 1.0, 5) # Evaluate using learning curve evaluate_model_with_learning_curve(X, y, model, train_sizes, cv=5) ``` # Constraints - Use `matplotlib` for plotting the curves. - Ensure that the plots are properly labeled for clarity. - The functions should not return any values, only produce plots. # Notes - Make sure to handle any necessary data preprocessing before fitting the model. - You may use helper functions if needed to keep your code organized and readable.","solution":"import matplotlib.pyplot as plt import numpy as np from sklearn.model_selection import validation_curve, learning_curve def evaluate_model_with_validation_curve(X, y, model, param_name, param_range): Plots the validation curve for the given model and hyperparameter. Parameters: - X (numpy.ndarray): The input features. - y (numpy.ndarray): The target values. - model (sklearn estimator): The machine learning model to evaluate. - param_name (str): The name of the hyperparameter to evaluate. - param_range (array-like): The range of values for the hyperparameter. train_scores, test_scores = validation_curve(model, X, y, param_name=param_name, param_range=param_range, cv=5) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure() plt.title(\\"Validation Curve\\") plt.xlabel(param_name) plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.semilogx(param_range, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=lw) plt.semilogx(param_range, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.show() def evaluate_model_with_learning_curve(X, y, model, train_sizes, cv): Plots the learning curve for the given model. Parameters: - X (numpy.ndarray): The input features. - y (numpy.ndarray): The target values. - model (sklearn estimator): The machine learning model to evaluate. - train_sizes (array-like): The number of training samples to use for generating the curve. - cv (int): The number of cross-validation folds. train_sizes, train_scores, test_scores = learning_curve(model, X, y, train_sizes=train_sizes, cv=cv) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.figure() plt.title(\\"Learning Curve\\") plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) lw = 2 plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=lw) plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=lw) plt.plot(train_sizes, test_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=lw) plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\\"navy\\", lw=lw) plt.legend(loc=\\"best\\") plt.show()"},{"question":"**Advanced Python Reference Management** In this question, you will implement a Python class called `RefManager` which simulates the reference counting mechanism similar to the one described in the provided documentation. You will be required to implement methods to manage strong references to Python objects. # Instructions: 1. **Class Definition:** - Create a class named `RefManager`. 2. **Initialization:** - The class should initialize with no references. 3. **Methods:** - `add_reference(obj: Any) -> None`: This method takes an object and indicates taking a new strong reference to this object. Store the reference internally. - `release_reference(obj: Any) -> None`: This method releases a strong reference to the specified object. If the reference count drops to zero, the object should be removed from internal storage. - `get_reference_count(obj: Any) -> int`: This method returns the current reference count for the specified object. - `clear_reference(obj: Any) -> None`: This method should safely release the reference and set the internal storage for this object to `None`. # Constraints: - Assume that objects given to `add_reference` and `release_reference` are hashable. - Maintaining accuracy in managing reference counts is crucial. - Performance should be efficient concerning memory and time complexity. # Example: ```python ref_manager = RefManager() # Adding References ref_manager.add_reference(\\"test_object\\") ref_manager.add_reference(\\"test_object\\") ref_manager.add_reference(\\"another_object\\") # Getting Reference Count assert ref_manager.get_reference_count(\\"test_object\\") == 2 assert ref_manager.get_reference_count(\\"another_object\\") == 1 # Releasing References ref_manager.release_reference(\\"test_object\\") assert ref_manager.get_reference_count(\\"test_object\\") == 1 # Clear Reference ref_manager.clear_reference(\\"test_object\\") assert ref_manager.get_reference_count(\\"test_object\\") == 0 # Or raise an exception as per your design choice ``` # Note: - Be mindful of edge cases such as attempting to release or get a reference for an object not managed by `RefManager`. **Your implementation should consider proper memory management and be robust against common errors.**","solution":"class RefManager: def __init__(self): # Initialize an empty dictionary to hold references and their counts self.references = {} def add_reference(self, obj): if obj in self.references: self.references[obj] += 1 else: self.references[obj] = 1 def release_reference(self, obj): if obj in self.references: self.references[obj] -= 1 if self.references[obj] == 0: del self.references[obj] def get_reference_count(self, obj): return self.references.get(obj, 0) def clear_reference(self, obj): if obj in self.references: del self.references[obj]"},{"question":"**Objective**: Demonstrate your understanding of the `lzma` module by implementing a function that compresses and decompresses data using custom filter chains, handles multiple streams, and validates data integrity. # Task Implement a Python function `compress_and_decompress` that takes a list of strings as input and performs the following operations: 1. **Compress** the concatenated input strings into a single LZMA-compressed byte string using a custom filter chain. 2. **Decompress** the compressed byte string back into the original concatenated string. 3. **Validate** the decompressed string by comparing it to the original concatenated input strings. 4. **Return** a tuple containing the compressed byte string and a boolean indicating whether the decompressed data matches the original concatenated strings. # Specifications - You must use at least one custom filter in the filter chain during compression. - Handle data integrity using the CRC64 check. - The function should work with multiple compression streams such that if decompressed data is corrupted part of the way, the function gracefully handles it and still validates the rest. - The function should be thread-safe. # Function Signature ```python def compress_and_decompress(data_list: list[str]) -> tuple[bytes, bool]: pass ``` # Constraints - The input `data_list` will contain at least one string and not exceed 100 strings. - Each string in the `data_list` will have a length between 1 and 1000 characters. - The total size of the combined strings will not exceed 10MB. # Example Usage ```python data = [\\"This is the first string.\\", \\" This is the second string.\\", \\" And this is the third string.\\"] compressed_data, is_valid = compress_and_decompress(data) print(is_valid) # Output should be True ``` The function should compress the data, decompress it, and confirm that the decompressed data matches the original input data. # Advanced Requirements - Use threading to ensure that `LZMAFile` operations are thread-safe. - Add enough exception handling to manage LZMAError and EOFError. **Hints**: - Look into `lzma.LZMACompressor`, `lzma.LZMADecompressor`, and custom filter chains to design your implementation. - Use `lzma.open` for file-like operations and manage threads with `threading.Lock` to ensure thread safety.","solution":"import lzma import threading from typing import List, Tuple def compress_and_decompress(data_list: List[str]) -> Tuple[bytes, bool]: # Concatenate the input strings input_data = \'\'.join(data_list).encode(\'utf-8\') # Create a custom filter chain filters = [ { \\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 9 | lzma.PRESET_EXTREME, }, ] # Initialize the LZMA compressor with custom filters lock = threading.Lock() with lock: compressor = lzma.LZMACompressor(filters=filters) compressed_data = compressor.compress(input_data) compressed_data += compressor.flush() decompressed_data = b\\"\\" try: with lock: decompressor = lzma.LZMADecompressor() decompressed_data = decompressor.decompress(compressed_data) except (EOFError, lzma.LZMAError): return (compressed_data, False) # Validate the decompressed data matches the original data is_valid = decompressed_data == input_data return (compressed_data, is_valid)"},{"question":"Advanced Web Browser Automation Objective: Demonstrate understanding of the `webbrowser` module in Python by implementing a class that automates various web-browsing tasks based on user input. Problem Statement: Design and implement a Python class `BrowserAutomation` that uses the `webbrowser` module to perform the following functions: 1. **Open URL**: Open a given URL in the default web browser. 2. **Open URL in New Tab**: Open a given URL in a new tab of the default web browser. 3. **Open URL in New Window**: Open a given URL in a new window of the default web browser. 4. **Register and Open with Specific Browser**: Register a custom browser application and open a URL using this browser. 5. **Batch Open URLs**: Given a list of URLs, open each URL in a new tab of the default web browser. Class Definition: ```python class BrowserAutomation: def __init__(self): pass def open_url(self, url: str) -> None: Opens the specified URL in the default web browser. Parameters: url (str): The URL to open. def open_url_new_tab(self, url: str) -> None: Opens the specified URL in a new tab of the default web browser. Parameters: url (str): The URL to open. def open_url_new_window(self, url: str) -> None: Opens the specified URL in a new window of the default web browser. Parameters: url (str): The URL to open. def register_and_open_with_browser(self, browser_name: str, browser_command: str, url: str) -> None: Registers a custom browser and opens the specified URL using this browser. Parameters: browser_name (str): The name to register for the custom browser. browser_command (str): The command to execute the custom browser. url (str): The URL to open. def batch_open_urls(self, urls: list) -> None: Opens a list of URLs, each in a new tab of the default web browser. Parameters: urls (list): List of URLs to open. ``` Example Usage: ```python # Creating instance browser = BrowserAutomation() # Open a URL in the default web browser browser.open_url(\\"https://example.com\\") # Open a URL in a new tab browser.open_url_new_tab(\\"https://example.com\\") # Open a URL in a new window browser.open_url_new_window(\\"https://example.com\\") # Register a custom browser and open a URL # Assume \'mybrowser\' is a command to start the custom browser browser.register_and_open_with_browser(\\"mybrowser\\", \\"/path/to/mybrowser %s\\", \\"https://example.com\\") # Batch open a list of URLs urls = [\\"https://example.com\\", \\"https://example.org\\", \\"https://example.net\\"] browser.batch_open_urls(urls) ``` Constraints and Notes: - The `url` parameters given to the methods are always valid URLs. - The `browser_name` and `browser_command` must be valid executable commands for the target environment. - Ensure proper exception handling in case of any `webbrowser.Error`. Evaluation Criteria: - Correctness: The implementation must work as described and open URLs correctly in the specified modes. - Code Quality: The code should be clean, well-documented, and modular. - Exception Handling: Proper handling of potential errors and edge cases. This problem tests the student\'s ability to understand and utilize the `webbrowser` module and to implement a class-based approach for organizing functionalities.","solution":"import webbrowser class BrowserAutomation: def __init__(self): pass def open_url(self, url: str) -> None: Opens the specified URL in the default web browser. Parameters: url (str): The URL to open. try: webbrowser.open(url) except webbrowser.Error as e: print(f\\"An error occurred: {e}\\") def open_url_new_tab(self, url: str) -> None: Opens the specified URL in a new tab of the default web browser. Parameters: url (str): The URL to open. try: webbrowser.open_new_tab(url) except webbrowser.Error as e: print(f\\"An error occurred: {e}\\") def open_url_new_window(self, url: str) -> None: Opens the specified URL in a new window of the default web browser. Parameters: url (str): The URL to open. try: webbrowser.open_new(url) except webbrowser.Error as e: print(f\\"An error occurred: {e}\\") def register_and_open_with_browser(self, browser_name: str, browser_command: str, url: str) -> None: Registers a custom browser and opens the specified URL using this browser. Parameters: browser_name (str): The name to register for the custom browser. browser_command (str): The command to execute the custom browser. url (str): The URL to open. try: webbrowser.register(browser_name, None, webbrowser.GenericBrowser(browser_command)) webbrowser.get(browser_name).open(url) except webbrowser.Error as e: print(f\\"An error occurred: {e}\\") def batch_open_urls(self, urls: list) -> None: Opens a list of URLs, each in a new tab of the default web browser. Parameters: urls (list): List of URLs to open. try: for url in urls: webbrowser.open_new_tab(url) except webbrowser.Error as e: print(f\\"An error occurred: {e}\\") # Example usage # browser = BrowserAutomation() # browser.open_url(\\"https://example.com\\")"},{"question":"Objective Demonstrate your understanding of handling different types of exceptions in asynchronous code by implementing a function that performs multiple asynchronous operations, each designed to potentially raise one of the documented `asyncio` exceptions. Question Write a function `perform_async_operations()` that performs three asynchronous tasks: `task1`, `task2`, and `task3`. Each task should simulate the conditions under which certain `asyncio` exceptions are raised. Your function should handle these exceptions and return a summary of the results in a dictionary format. Here are the specifics for each task: 1. `task1` should raise an `asyncio.TimeoutError` if it exceeds a specific time limit. 2. `task2` should be cancelable and on cancellation should raise an `asyncio.CancelledError`. 3. `task3` should simulate a situation where a read operation does not complete fully and raises `asyncio.IncompleteReadError`. Input - `timeout` (int): Time limit in seconds for `task1`. - `cancel_task2` (bool): Whether to cancel `task2` or not. Output - A dictionary with keys `\\"task1_result\\"`, `\\"task2_result\\"`, and `\\"task3_result\\"`, each containing the respective result or exception string. ```python import asyncio async def task1(timeout): try: await asyncio.wait_for(asyncio.sleep(timeout + 1), timeout) except asyncio.TimeoutError: raise asyncio.TimeoutError(\\"Task1 exceeded the time limit.\\") async def task2(cancel_task2): try: await asyncio.sleep(5) except asyncio.CancelledError: raise asyncio.CancelledError(\\"Task2 was cancelled.\\") async def task3(): class FakeStream: def __init__(self, complete_read=False): self.complete_read = complete_read async def read(self, num_bytes): if not self.complete_read: raise asyncio.IncompleteReadError(40, b\'partial_data\') return b\'data\' stream = FakeStream(complete_read=False) try: await stream.read(100) except asyncio.IncompleteReadError as e: raise asyncio.IncompleteReadError(expected=e.expected, partial=e.partial) async def perform_async_operations(timeout, cancel_task2): results = {} try: await task1(timeout) results[\'task1_result\'] = \\"Task1 completed successfully.\\" except asyncio.TimeoutError as e: results[\'task1_result\'] = str(e) task2_handle = asyncio.create_task(task2(cancel_task2)) if cancel_task2: await asyncio.sleep(1) task2_handle.cancel() try: await task2_handle results[\'task2_result\'] = \\"Task2 completed successfully.\\" except asyncio.CancelledError as e: results[\'task2_result\'] = str(e) try: await task3() results[\'task3_result\'] = \\"Task3 completed successfully.\\" except asyncio.IncompleteReadError as e: results[\'task3_result\'] = f\\"IncompleteReadError: Expected {e.expected} bytes, but got partial data of length {len(e.partial)}.\\" return results # Example usage: # asyncio.run(perform_async_operations(2, True)) ``` Constraints - You must utilize the `asyncio` module. - Ensure proper handling of the specified exceptions. - Do not use any external libraries other than `asyncio`. Performance Requirements - Your implementation should efficiently and correctly handle the asynchronous operations and their exceptions without any unnecessary delays or blocking operations.","solution":"import asyncio async def task1(timeout): try: await asyncio.wait_for(asyncio.sleep(timeout + 1), timeout) except asyncio.TimeoutError: raise asyncio.TimeoutError(\\"Task1 exceeded the time limit.\\") async def task2(cancel_task2): try: await asyncio.sleep(5) except asyncio.CancelledError: raise asyncio.CancelledError(\\"Task2 was cancelled.\\") class FakeStream: def __init__(self, complete_read=False): self.complete_read = complete_read async def read(self, num_bytes): if not self.complete_read: raise asyncio.IncompleteReadError(40, b\'partial_data\') return b\'data\' async def task3(): stream = FakeStream(complete_read=False) try: await stream.read(100) except asyncio.IncompleteReadError as e: raise asyncio.IncompleteReadError(expected=e.expected, partial=e.partial) async def perform_async_operations(timeout, cancel_task2): results = {} try: await task1(timeout) results[\'task1_result\'] = \\"Task1 completed successfully.\\" except asyncio.TimeoutError as e: results[\'task1_result\'] = str(e) task2_handle = asyncio.create_task(task2(cancel_task2)) if cancel_task2: await asyncio.sleep(1) task2_handle.cancel() try: await task2_handle results[\'task2_result\'] = \\"Task2 completed successfully.\\" except asyncio.CancelledError as e: results[\'task2_result\'] = str(e) try: await task3() results[\'task3_result\'] = \\"Task3 completed successfully.\\" except asyncio.IncompleteReadError as e: results[\'task3_result\'] = f\\"IncompleteReadError: Expected {e.expected} bytes, but got partial data of length {len(e.partial)}.\\" return results"},{"question":"Objective: Implement an asynchronous task queue processor that handles tasks with different priorities and provides mechanisms to manage and monitor the queue. Instructions: 1. Create a class `AsyncTaskProcessor` that internally uses `asyncio.PriorityQueue` to manage tasks with priorities. 2. Implement the following methods for `AsyncTaskProcessor`: - `__init__(maxsize: int = 10)`: Initialize the processor with a specified maxsize for the queue. Default is 10. - `add_task(priority: int, task_id: str, wait_time: float) -> None`: Adds a task to the queue. Each task is represented as a tuple `(priority, task_id, wait_time)` where `priority` determines the order of processing, `task_id` is a unique identifier for the task, and `wait_time` is the time in seconds the task should take to complete. - `get_task() -> tuple`: Removes and returns the task with the highest priority from the queue. - `task_completed() -> None`: Marks the currently processed task as completed. - `run() -> None`: Runs an infinite loop that fetches tasks from the queue, awaits the specified `wait_time` for each task to simulate work being done, and then marks the task as completed. - `shutdown() -> None`: Safely shuts down the processor, ensuring all tasks are completed. Example Usage: ```python import asyncio class AsyncTaskProcessor: def __init__(self, maxsize: int = 10): # Implementation here def add_task(self, priority: int, task_id: str, wait_time: float) -> None: # Implementation here async def get_task(self) -> tuple: # Implementation here def task_completed(self) -> None: # Implementation here async def run(self) -> None: # Implementation here async def shutdown(self) -> None: # Implementation here # Example of how to use AsyncTaskProcessor async def main(): processor = AsyncTaskProcessor() # Adding tasks with different priorities processor.add_task(1, \'task-1\', 0.5) processor.add_task(2, \'task-2\', 0.2) processor.add_task(1, \'task-3\', 0.3) # Start processing tasks processing_task = asyncio.create_task(processor.run()) await asyncio.sleep(2) # Let the processor run for a bit await processor.shutdown() # Shutdown the processor asyncio.run(main()) ``` Constraints: - You must use `asyncio.PriorityQueue` for managing tasks. - The `run()` method should process tasks indefinitely until `shutdown()` is called. - Ensure thread-safety and proper exception handling where necessary. Expected Output: The output will depend on the tasks being added to the queue. The processor should handle tasks based on their priority and `wait_time`, and output debug information denoting when each task has started and completed. Notes: - Focus on efficient use of asyncio primitives and proper queue management. - Consider edge cases such as adding tasks to a full queue, retrieving from an empty queue, and handling exceptions.","solution":"import asyncio class AsyncTaskProcessor: def __init__(self, maxsize: int = 10): self.queue = asyncio.PriorityQueue(maxsize=maxsize) self._running = False async def add_task(self, priority: int, task_id: str, wait_time: float) -> None: await self.queue.put((priority, task_id, wait_time)) async def get_task(self) -> tuple: return await self.queue.get() def task_completed(self) -> None: self.queue.task_done() async def run(self) -> None: self._running = True while self._running or not self.queue.empty(): priority, task_id, wait_time = await self.get_task() print(f\\"Processing task {task_id} with priority {priority}\\") await asyncio.sleep(wait_time) print(f\\"Completed task {task_id}\\") self.task_completed() async def shutdown(self) -> None: self._running = False await self.queue.join()"},{"question":"**Coding Assessment Question** # Objective Implement a function that constructs and manipulates an Export IR Graph in PyTorch. This will test your understanding of PyTorch\'s Export IR and your ability to work with its graph-based structure. # Problem Statement You need to design a function `create_and_modify_export_graph(model: torch.nn.Module, example_inputs: Tuple[torch.Tensor, ...]) -> torch.export.ExportedProgram` that: 1. Constructs an Export IR graph for the given PyTorch model using `torch.export.export`. 2. Modifies the graph by adding a new node that performs an additional computation (e.g., multiply the output by 2). 3. Returns the modified `ExportedProgram`. # Function Signature ```python def create_and_modify_export_graph(model: torch.nn.Module, example_inputs: Tuple[torch.Tensor, ...]) -> torch.export.ExportedProgram: ``` # Input - `model`: A `torch.nn.Module` representing the PyTorch model to be exported. - `example_inputs`: A tuple of `torch.Tensor` objects representing example inputs to the model, used to trace and generate the Export IR graph. # Output - Returns a modified `torch.export.ExportedProgram` where the graph is updated to include an additional computation node. # Constraints - The added node should be of `call_function` type that performs a `torch.mul` operation, multiplying the final output by 2. - Ensure the graph maintains a valid structure, adhering to Export IR requirements. # Example ```python import torch from torch import nn class MyModel(nn.Module): def forward(self, x): return x + 1 example_inputs = (torch.tensor(1.0),) # Function to be implemented modified_program = create_and_modify_export_graph(MyModel(), example_inputs) # This should print an Export IR graph with an additional multiplication node print(modified_program.graph) ``` # Notes - You may need to refer to additional PyTorch documentation for `torch.fx` and related functions to implement this correctly. - Ensure the new computation node (multiplication by 2) is correctly placed and connected in the graph.","solution":"import torch from torch import nn from typing import Tuple import torch.fx def create_and_modify_export_graph(model: torch.nn.Module, example_inputs: Tuple[torch.Tensor, ...]) -> torch.fx.GraphModule: # Step 1: Trace the model using torch.fx traced_module = torch.fx.symbolic_trace(model) # Step 2: Get the graph from traced module graph = traced_module.graph # Step 3: Find the output node of the original graph output_node = None for node in graph.nodes: if node.op == \'output\': output_node = node break # Step 4: Create a new node that multiplies the output by 2 with graph.inserting_before(output_node): new_node = graph.call_function( torch.mul, args=(output_node.args[0], 2) ) # Create a new output node that uses the new_node new_output_node = graph.output(new_node) # Step 5: Erase the old output node graph.erase_node(output_node) traced_module.recompile() return traced_module"},{"question":"You are given a dataset containing sales data for different products across various regions and dates. Your task is to implement functions to clean, analyze, and provide insights from this sales data using `pandas`. # Dataset Description The dataset is a CSV file with the following columns: - `Date`: The date of the sales record (format: YYYY-MM-DD). - `Region`: The region where the sale was made. - `Product`: The product sold. - `Units Sold`: The number of units sold. - `Unit Price`: The price per unit of the product. - `Total Revenue`: The total revenue from the sale (calculated as `Units Sold * Unit Price`). You will implement the following functionalities: 1. **Data Cleaning**: - Function: `clean_data(data: pd.DataFrame) -> pd.DataFrame` - Description: Load the dataset, handle missing values by filling with suitable defaults or removing, and ensure all data types are appropriate. 2. **Compute Total Revenue**: - Function: `compute_total_revenue(data: pd.DataFrame) -> pd.DataFrame` - Description: Compute and add a new column `Total Revenue` in case it\'s missing, ensuring it is correctly calculated as `Units Sold * Unit Price`. 3. **Analyze Data**: - Function: `analyze_data(data: pd.DataFrame) -> pd.DataFrame` - Description: Provide a summary analysis of the dataset, including: - Total units sold per product. - Total revenue per product and region. - Average unit price per product. - The date on which maximum units were sold for each product. 4. **Generate Insights**: - Function: `generate_insights(data: pd.DataFrame) -> Dict[str, Any]` - Description: Generate insights such as: - Overall top-performing product in terms of total revenue. - Region with the highest sales. - Month with the highest overall sales. # Function Signatures ```python import pandas as pd from typing import Dict, Any def clean_data(data: pd.DataFrame) -> pd.DataFrame: pass def compute_total_revenue(data: pd.DataFrame) -> pd.DataFrame: pass def analyze_data(data: pd.DataFrame) -> pd.DataFrame: pass def generate_insights(data: pd.DataFrame) -> Dict[str, Any]: pass ``` # Constraints - Assume that the dataset contains at least one region and one product. - The `Date` column is in the correct date format (`YYYY-MM-DD`). - Handle large datasets efficiently using appropriate `pandas` methods. # Example Usage ```python data = pd.read_csv(\\"sales_data.csv\\") # Clean the data cleaned_data = clean_data(data) # Compute total revenue if missing revenue_data = compute_total_revenue(cleaned_data) # Analyze the data analysis_results = analyze_data(revenue_data) # Generate insights insights = generate_insights(revenue_data) print(analysis_results) print(insights) ``` # Note The dataset must be provided in the form of a CSV file when evaluating the solution. The solution should be robust, efficiently handle missing data, and ensure data integrity.","solution":"import pandas as pd from typing import Dict, Any def clean_data(data: pd.DataFrame) -> pd.DataFrame: Cleans the data by handling missing values and ensuring proper data types. # Ensure columns are of correct data types data[\'Date\'] = pd.to_datetime(data[\'Date\'], errors=\'coerce\') data[\'Units Sold\'] = pd.to_numeric(data[\'Units Sold\'], errors=\'coerce\') data[\'Unit Price\'] = pd.to_numeric(data[\'Unit Price\'], errors=\'coerce\') data[\'Total Revenue\'] = pd.to_numeric(data[\'Total Revenue\'], errors=\'coerce\') # Fill missing values or remove them data = data.dropna(subset=[\'Date\', \'Region\', \'Product\', \'Units Sold\', \'Unit Price\']) data[\'Total Revenue\'] = data.apply( lambda row: row[\'Total Revenue\'] if pd.notnull(row[\'Total Revenue\']) else row[\'Units Sold\'] * row[\'Unit Price\'], axis=1 ) return data def compute_total_revenue(data: pd.DataFrame) -> pd.DataFrame: Computes total revenue if the \'Total Revenue\' column is missing. if \'Total Revenue\' not in data.columns: data[\'Total Revenue\'] = data[\'Units Sold\'] * data[\'Unit Price\'] return data def analyze_data(data: pd.DataFrame) -> pd.DataFrame: Provides summary analysis of the dataset. total_units_sold_per_product = data.groupby(\'Product\')[\'Units Sold\'].sum().reset_index() total_revenue_per_product_and_region = data.groupby([\'Product\', \'Region\'])[\'Total Revenue\'].sum().reset_index() average_unit_price_per_product = data.groupby(\'Product\')[\'Unit Price\'].mean().reset_index() date_with_max_units_sold_per_product = data.loc[data.groupby(\'Product\')[\'Units Sold\'].idxmax()].reset_index(drop=True) analysis_results = { \'Total Units Sold per Product\': total_units_sold_per_product, \'Total Revenue per Product and Region\': total_revenue_per_product_and_region, \'Average Unit Price per Product\': average_unit_price_per_product, \'Date with Max Units Sold per Product\': date_with_max_units_sold_per_product } return analysis_results def generate_insights(data: pd.DataFrame) -> Dict[str, Any]: Generate insights from the dataset. overall_top_performing_product = data.groupby(\'Product\')[\'Total Revenue\'].sum().idxmax() region_with_highest_sales = data.groupby(\'Region\')[\'Total Revenue\'].sum().idxmax() data[\'Month\'] = data[\'Date\'].dt.to_period(\'M\') month_with_highest_sales = data.groupby(\'Month\')[\'Total Revenue\'].sum().idxmax() insights = { \'Overall Top Performing Product\': overall_top_performing_product, \'Region with Highest Sales\': region_with_highest_sales, \'Month with Highest Sales\': str(month_with_highest_sales) } return insights"},{"question":"Advanced Python Coding Assessment # Objective Demonstrate your understanding of the Python standard library modules, particularly focusing on output formatting and templating. # Problem Statement You are writing a report generator for financial data, ensuring that the data is well-formatted and readable. The report consists of several sections displaying transactions, summaries, and other metadata. You should implement a function that takes raw data and generates a formatted report as a string. # Requirements 1. **Function Signature**: ```python def generate_financial_report(transactions: list, locale_str: str, template_str: str) -> str: ``` 2. **Input**: - `transactions`: A list of dictionaries, each containing transaction data with the following keys: - `\'date\'`: a string representing the date of the transaction, e.g., `\'2023-04-01\'`. - `\'description\'`: a string description of the transaction. - `\'amount\'`: a float representing the transaction amount. - `locale_str`: A string representing the locale for number formatting, e.g., `\'en_US.UTF-8\'`. - `template_str`: A string representing the template for the report, using `{}` placeholders for variables `total`, `highest`, `lowest`, and `average`. 3. **Output**: - Returns a formatted string report using provided transactions and template. The report should adhere to the specified locale for numerical values. 4. **Constraints**: - Decimal values should be represented to two decimal places. - The transactions might be large, so ensure your implementation is optimized for performance. # Example ```python transactions = [ {\'date\': \'2023-04-01\', \'description\': \'Coffee\', \'amount\': 3.5}, {\'date\': \'2023-04-02\', \'description\': \'Books\', \'amount\': 12.99}, {\'date\': \'2023-04-03\', \'description\': \'Groceries\', \'amount\': 57.23}, {\'date\': \'2023-04-04\', \'description\': \'Dinner\', \'amount\': 45.00}, ] locale_str = \'en_US.UTF-8\' template_str = Financial Report ================ Total: {total} Highest Transaction: {highest} Lowest Transaction: {lowest} Average Transaction: {average} report = generate_financial_report(transactions, locale_str, template_str) print(report) # Expected Output: # Financial Report # ================ # Total: 118.72 # Highest Transaction: 57.23 # Lowest Transaction: 3.50 # Average Transaction: 29.68 ``` # Implementation Notes - Use the `locale` module to format the numbers according to the specified `locale_str`. - Use the `string.Template` class to create the final report from the `template_str`. - Ensure to handle edge cases such as empty transaction lists and invalid data gracefully. # Evaluation Criteria - Correctness: The logic should correctly compute and format the required values. - Performance: The solution should handle large datasets efficiently. - Code Quality: The code should be clean, well-organized, and document any assumptions made.","solution":"import locale from string import Template def generate_financial_report(transactions: list, locale_str: str, template_str: str) -> str: if not transactions: return \\"No transactions to report.\\" total = sum(tx[\'amount\'] for tx in transactions) highest = max(tx[\'amount\'] for tx in transactions) lowest = min(tx[\'amount\'] for tx in transactions) average = total / len(transactions) locale.setlocale(locale.LC_ALL, locale_str) formatted_total = locale.currency(total, grouping=True) formatted_highest = locale.currency(highest, grouping=True) formatted_lowest = locale.currency(lowest, grouping=True) formatted_average = locale.currency(average, grouping=True) report_template = Template(template_str) report = report_template.substitute( total=formatted_total, highest=formatted_highest, lowest=formatted_lowest, average=formatted_average ) return report"},{"question":"# Password Hashing and Verification with `crypt` Module **Description:** Implement a set of functions to securely hash a password and verify a password against its hashed version using the deprecated `crypt` module. Specifically, you need to implement the following functions: 1. `hash_password(password: str) -> str`: This function should take a plain-text password as input and return the hashed password using the strongest available method from the `crypt` module. 2. `verify_password(plain_password: str, hashed_password: str) -> bool`: This function should take a plain-text password and a hashed password as inputs and return `True` if the plain-text password matches the hashed password, and `False` otherwise. Use a constant-time comparison method for verifying password hashes to prevent timing attacks. **Input and Output:** - `hash_password(password: str) -> str` - **Input:** A plain-text password (string). - **Output:** A hashed password (string) using the strongest available method. - `verify_password(plain_password: str, hashed_password: str) -> bool` - **Input:** - `plain_password`: A plain-text password (string). - `hashed_password`: A hashed password (string). - **Output:** - `True` if the plain-text password matches the hashed password. - `False` otherwise. **Constraints:** - The plain-text password will be a non-empty string containing printable ASCII characters. - You should handle any exceptions that are raised due to incorrect usage. **Performance:** - The functions should be efficient, but since they deal with hashing and security, they are expected to take a reasonable amount of time dependent on the hashing algorithm used. **Example Usage:** ```python import hmac from crypt_assessment import hash_password, verify_password # Example hashed password creation plain_password = \\"securepassword123\\" hashed_password = hash_password(plain_password) # Verifying the password assert verify_password(plain_password, hashed_password) == True assert verify_password(\\"wrongpassword\\", hashed_password) == False ``` # Guidelines: 1. Ensure that you import and use the `crypt` module appropriately and adhere to its guidelines. 2. Use `hmac.compare_digest` for password verification to ensure constant-time comparison. 3. Write clean, readable, and well-documented code. 4. Don\'t forget to handle edge cases and possible exceptions.","solution":"import crypt import hmac import os def hash_password(password: str) -> str: Hashes a password using the strongest available method from the crypt module. Args: password (str): The plain-text password. Returns: str: The hashed password. # Using crypt.mksalt with the strongest method available salt = crypt.mksalt(crypt.METHOD_SHA512) hashed_password = crypt.crypt(password, salt) return hashed_password def verify_password(plain_password: str, hashed_password: str) -> bool: Verifies a plain-text password against a hashed password using constant-time comparison. Args: plain_password (str): The plain-text password. hashed_password (str): The hashed password. Returns: bool: True if the plain-text password matches the hashed password, False otherwise. # Hash the plain password using the same salt from the hashed_password new_hashed_password = crypt.crypt(plain_password, hashed_password) # Using hmac.compare_digest for constant-time comparison return hmac.compare_digest(new_hashed_password, hashed_password)"},{"question":"**Question: Profiling and Optimizing Function Performance** You are tasked with profiling and optimizing the performance of a function that processes a list of integers. You will use the `cProfile` and `timeit` modules to measure the execution time and identify potential bottlenecks in the function. Finally, you should refactor the function to improve its performance. **Provided Function:** ```python def process_numbers(numbers): processed_numbers = [] for number in numbers: if number % 2 == 0: processed_numbers.append(number * 2) else: processed_numbers.append(number * 3) return processed_numbers ``` **Task:** 1. Use the `cProfile` module to profile the `process_numbers` function. Analyze the output to determine the most time-consuming parts of the function. 2. Use the `timeit` module to measure the execution time of the `process_numbers` function before and after optimization. 3. Refactor the `process_numbers` function to improve its performance, ensuring that the functionality remains unchanged. 4. Provide the output of the `cProfile` and `timeit` measurements before and after optimization. **Expected Input:** - A list of integers `numbers` with a minimum length of 1 and a maximum length of 100,000. **Expected Output:** - The execution time of the original function. - The execution time of the optimized function. - The refactored function implementation. **Constraints:** - Target runtime complexity should be better than O(n^2). **Example:** ```python # Example input numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # Original function profiling and timing profile_original() timeit_original() # Optimized function implementation def process_numbers_optimized(numbers): processed_numbers = [(number * 2) if number % 2 == 0 else (number * 3) for number in numbers] return processed_numbers # Optimized function profiling and timing profile_optimized() timeit_optimized() ``` Make sure your optimized function is more efficient and provide detailed profiling and timing results.","solution":"import cProfile import timeit # Original function to be optimized def process_numbers(numbers): processed_numbers = [] for number in numbers: if number % 2 == 0: processed_numbers.append(number * 2) else: processed_numbers.append(number * 3) return processed_numbers # Refactored (Optimized) function def process_numbers_optimized(numbers): return [(number * 2) if number % 2 == 0 else (number * 3) for number in numbers] # Function to profile the execution time using cProfile def profile_function(func, numbers): profiler = cProfile.Profile() profiler.enable() func(numbers) profiler.disable() profiler.print_stats() # Sample list of numbers to be used for profiling and testing numbers = list(range(1, 100001)) # Profile the original function print(\\"Profiling original function:\\") profile_function(process_numbers, numbers) # Profile the optimized function print(\\"Profiling optimized function:\\") profile_function(process_numbers_optimized, numbers) # Measure execution time using timeit print(f\\"Original function execution time: {timeit.timeit(lambda: process_numbers(numbers), number=10)} seconds\\") print(f\\"Optimized function execution time: {timeit.timeit(lambda: process_numbers_optimized(numbers), number=10)} seconds\\")"},{"question":"# **Advanced File Handling and Text Processing with Python\'s io Module** In this task, you are required to demonstrate your proficiency with Python\'s `io` module by implementing a function that processes data from a file. The function should efficiently handle both text and binary data. You are expected to use the appropriate classes from the `io` module to read, process, and write data. # **Task Details** **Function Signature** ```python def process_file(file_path: str, mode: str, encoding: str = None) -> str: pass ``` # **Parameters:** - `file_path` (str): The path to the input file. - `mode` (str): The mode in which the file should be opened. This can be any valid mode like \'r\', \'rb\', \'w\', \'wb\', etc. The mode will dictate whether the file is to be opened for reading or writing, and whether itâ€™s a text or binary file. - `encoding` (str, optional): The encoding to be used for text files. If the mode indicates a text file (e.g., \'r\' or \'w\'), and no encoding is provided, default to \\"utf-8\\". # **Output:** - Returns a string indicating that the operation is complete, or relevant error messages if encountered. # **Implementation Details:** 1. **Open the File:** - Use the appropriate class from the `io` module to open the file based on the mode and encoding provided. 2. **Processing the File:** - If the file is opened in read mode (\'r\', \'rb\'), read all contents. - If the file is opened in write mode (\'w\', \'wb\'), write a specified data pattern into the file. For text files, write a simple text; for binary files, write a series of bytes. 3. **Error Handling:** - Handle errors gracefully, such as `FileNotFoundError`, `IOError`, or any issues related to file operations or encoding errors. 4. **Return Statement:** - If processing is successful, return a success message. - If any errors occur, catch them and return a relevant error message. # **Example Usage:** ```python # Example input text file: test.txt containing \\"Hello World\\" print(process_file(\\"test.txt\\", \\"r\\")) # Expected Output: Content Read Successfully: Hello World # Writing data to a text file print(process_file(\\"output.txt\\", \\"w\\", encoding=\\"utf-8\\")) # Expected Output: Data Written Successfully ``` # **Constraints:** 1. Mode must be one of the following: \'r\', \'rb\', \'w\', \'wb\'. 2. Encoding must be specified for text files. 3. For binary files, the encoding should not be used. 4. File operations should be efficient and safe, considering the possibility of the file being large. # **Hints:** 1. Utilize classes like `TextIOWrapper`, `BytesIO`, or `StringIO` appropriately based on the mode. 2. Ensure to handle exceptions and edge cases, such as attempting to write to a read-only file or reading a non-existing file. # **Performance Requirement:** - The function should efficiently handle operations for both small and large files. # **Notes:** You can import necessary components from the `io` module. ```python import io ```","solution":"import io def process_file(file_path: str, mode: str, encoding: str = None) -> str: try: if \'b\' in mode: # Binary mode processing with open(file_path, mode) as file: if \'r\' in mode: content = file.read() return f\\"Content Read Successfully: {content[:50]}...\\" # Display only first 50 bytes elif \'w\' in mode: data = b\\"This is a sample binary data.\\" file.write(data) return \\"Binary Data Written Successfully\\" else: # Text mode processing with open(file_path, mode, encoding=encoding or \'utf-8\') as file: if \'r\' in mode: content = file.read() return f\\"Content Read Successfully: {content[:50]}...\\" # Display only first 50 characters elif \'w\' in mode: data = \\"This is a sample text data.\\" file.write(data) return \\"Text Data Written Successfully\\" except FileNotFoundError: return \\"Error: File Not Found\\" except IOError as io_err: return f\\"IO Error: {io_err}\\" except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"**Objective:** Write a Python function that utilizes the `pathlib` module to perform a series of filesystem operations. **Problem Statement:** You are given a directory path and a list of files. Your task is to implement a function `organize_files(path: str, files: List[str]) -> Dict[str, List[str]]` that: 1. Verifies if the provided directory path exists. If it doesn\'t exist, creates the directory. 2. Creates a sub-directory named `archives` inside the given directory. 3. For each file in the provided list: - If the file exists in the given directory, moves it to the `archives` sub-directory. - If the file does not exist, create an empty file in the `archives` sub-directory with the same name. 4. Returns a dictionary containing two keys: - `moved_files`: a list of the files that were moved to the `archives` sub-directory. - `created_files`: a list of the files that were created as new empty files in the `archives` sub-directory. **Input:** - `path`: A string representing the directory path. - `files`: A list of strings where each string is a file name. **Output:** - A dictionary with two keys (`moved_files`, `created_files`) and their corresponding values as lists of strings. **Constraints:** - All operations should be performed using the `pathlib` library. - You should handle any necessary filesystem exceptions gracefully. **Example:** ```python path = \\"/user/documents\\" files = [\\"report.txt\\", \\"data.csv\\", \\"summary.docx\\"] # Directory structure before running the function: # /user/documents # â”œâ”€â”€ report.txt # â””â”€â”€ summary.docx output = organize_files(path, files) # Expected directory structure after running the function: # /user/documents # â”œâ”€â”€ archives # â”‚ â”œâ”€â”€ report.txt # â”‚ â”œâ”€â”€ data.csv # â”‚ â””â”€â”€ summary.docx print(output) # { # \\"moved_files\\": [\\"report.txt\\", \\"summary.docx\\"], # \\"created_files\\": [\\"data.csv\\"] # } ``` **Notes:** - Ensure that the solution is cross-platform compatible. - The function should leverage the `pathlib` library to manage paths and filesystem operations.","solution":"from pathlib import Path from typing import List, Dict def organize_files(path: str, files: List[str]) -> Dict[str, List[str]]: Organize files by moving existing ones to \'archives\' and creating empty files for non-existing ones. Args: - path (str): The directory path. - files (List[str]): List of file names. Returns: - Dict[str, List[str]]: A dictionary with \'moved_files\' and \'created_files\'. result = { \\"moved_files\\": [], \\"created_files\\": [] } base_directory = Path(path) archives_directory = base_directory / \\"archives\\" # Ensure the base directory exists base_directory.mkdir(parents=True, exist_ok=True) # Ensure the archives sub-directory exists archives_directory.mkdir(parents=True, exist_ok=True) for file_name in files: source_file = base_directory / file_name target_file = archives_directory / file_name if source_file.exists(): source_file.rename(target_file) result[\\"moved_files\\"].append(file_name) else: target_file.touch() result[\\"created_files\\"].append(file_name) return result"},{"question":"# Python Coding Assessment Overview In this assessment, you are required to use the `aifc` module to read an AIFF-C audio file, perform some basic manipulations, and then write the manipulated data to a new AIFF-C audio file. This exercise will test your understanding of reading and writing audio data, handling file parameters, and utilizing the `aifc` moduleâ€™s functionality. Problem Statement You are provided with a path to an AIFF-C audio file. Your task is to: 1. Read the AIFF-C audio file and retrieve its parameters. 2. Perform a basic manipulation on the audio data (e.g., changing the volume). 3. Write the manipulated audio data to a new AIFF-C file, retaining the original file parameters. Function Specification Write a function `process_audio_file(input_filepath: str, output_filepath: str, volume_factor: float) -> None` that performs these tasks. # Parameters - `input_filepath (str)`: The path to the input AIFF-C audio file. - `output_filepath (str)`: The path to the output AIFF-C audio file where the manipulated audio will be saved. - `volume_factor (float)`: A factor by which to change the volume of the audio. For example, if `volume_factor` is 0.5, the volume should be halved. If it is 2.0, the volume should be doubled. # Requirements 1. **Read the audio data**: Open the `input_filepath` file for reading and retrieve its parameters. 2. **Manipulate the audio data**: Adjust the volume of the audio data by multiplying each sample by `volume_factor`. Ensure the manipulation does not clip the audio (exceed the allowable range for sample values). 3. **Write the audio data**: Write the modified audio data to the `output_filepath` with the same parameters as the original file. # Constraints - The input file will be a valid AIFF-C file. - You may assume the audio data uses a sample width of 2 bytes (16 bits). - The number of channels will be either 1 (mono) or 2 (stereo). - The `volume_factor` will be a positive floating-point number. # Example ```python process_audio_file(\'input.aifc\', \'output.aifc\', 1.5) ``` This example reads the `input.aifc` file, increases the volume by 50%, and writes the manipulated data to `output.aifc`. Implementation Details 1. Use the `aifc.open()` function to read the input file and retrieve its parameters. 2. Adjust the volume of the audio data using the given `volume_factor`. 3. Use the `aifc` moduleâ€™s writing functions (`setnchannels()`, `setsampwidth()`, `setframerate()`, `writeframes()`, etc.) to write the manipulated data to the output file. Post your implementation of the `process_audio_file` function. **Note**: Ensure to handle the audio sample value ranges properly while manipulating the volume to avoid audio clipping or distortions.","solution":"import aifc import numpy as np def process_audio_file(input_filepath: str, output_filepath: str, volume_factor: float) -> None: Processes an AIFF-C audio file to adjust its volume and saves the output to a new file. Parameters: input_filepath (str): Path to the input AIFF-C audio file. output_filepath (str): Path to the output AIFF-C audio file. volume_factor (float): Factor by which to change the volume of the audio. Returns: None with aifc.open(input_filepath, \'rb\') as input_file: params = input_file.getparams() num_channels = input_file.getnchannels() sampwidth = input_file.getsampwidth() framerate = input_file.getframerate() num_frames = input_file.getnframes() raw_data = input_file.readframes(num_frames) # Convert bytes data to numpy array for processing audio_data = np.frombuffer(raw_data, dtype=np.int16) # Adjust the volume audio_data = np.clip(audio_data * volume_factor, -32768, 32767).astype(np.int16) # Convert numpy array back to bytes manipulated_data = audio_data.tobytes() with aifc.open(output_filepath, \'wb\') as output_file: output_file.setparams(params) output_file.writeframes(manipulated_data)"},{"question":"# Question: Advanced Context Management The `contextlib` module in Python provides powerful utilities to work with context managers and manage resources efficiently. For this task, you are required to implement a complex context manager using the functionalities in the `contextlib` module. You are to define a context manager called `ResourcePool` that manages a pool of resources. Each resource in the pool should be obtained from a resource factory function and automatically return to the pool when it is no longer needed. The context manager should allow users to acquire multiple resources and ensure that all resources are properly released even if errors occur during usage. Furthermore, the implementation should provide the following features: 1. A method to acquire a resource from the pool. 2. A method to release a resource back to the pool. 3. Resource cleanup in case of exceptions. 4. Allow the context manager itself to be used as a decorator. Implementation Requirements: - Implement a `ResourcePool` class that provides the functionality described. - Use `contextlib.ExitStack` to manage multiple context managers and ensure proper cleanup. - Methods: - `__init__(self, resource_factory, size)`: Initializes the pool with a specified number of resources from the `resource_factory`. - `acquire(self)`: Acquires a resource from the pool. - `release(self, resource)`: Releases a resource back to the pool. - `__enter__` and `__exit__`: Manage the entering and exiting of the context manager. Example Usage: ```python # Define a simple resource factory def resource_factory(): return open(\'/path/to/temp/file\', \'w\') # Initialize the ResourcePool with the factory and size of 3 pool = ResourcePool(resource_factory, 3) # Example of using with the context manager directly with pool as p: res1 = p.acquire() res2 = p.acquire() res1.write(\'Hello World!\') p.release(res1) p.release(res2) # All resources will be properly released here # Example of using ResourcePool as a decorator @pool def write_to_resources(): res1 = pool.acquire() res2 = pool.acquire() res1.write(\'Hello from decorator!\') pool.release(res1) pool.release(res2) # All resources will be properly released here write_to_resources() ``` Constraints: - The pool size is between 1 and 10. - There should always be at least one resource available in the pool when any action that acquires a resource is initiated. - Ensure thread safety if applicable. Implement the `ResourcePool` class as described to satisfy the above requirements.","solution":"import contextlib from queue import Queue, Empty class ResourcePool: def __init__(self, resource_factory, size): self._resource_factory = resource_factory self._size = size self._pool = Queue(maxsize=size) for _ in range(size): self._pool.put(self._resource_factory()) def acquire(self): try: return self._pool.get_nowait() except Empty: raise RuntimeError(\\"No resources available in the pool\\") def release(self, resource): self._pool.put(resource) def __enter__(self): self._stack = contextlib.ExitStack() self._stack.__enter__() return self def __exit__(self, exc_type, exc_value, traceback): return self._stack.__exit__(exc_type, exc_value, traceback) def __call__(self, func): def wrapper(*args, **kwargs): with self: return func(*args, **kwargs) return wrapper # Example of defining a resource factory def resource_factory(): return open(\'/dev/null\', \'w\') # Using /dev/null as a safe placeholder # Example instantiation pool = ResourcePool(resource_factory, 3)"},{"question":"Objective Implement a function that creates a complex mask by combining several basic masks using the block mask utilities provided in the `torch.nn.attention.flex_attention` module. Then, apply this combined mask to a given tensor to filter out specific elements as defined by the mask. Problem Statement Write a function `apply_complex_mask` that performs the following: 1. Create three basic masks using the `create_block_mask` and `create_mask` functions. 2. Combine these masks using the `and_masks` and `or_masks` functions to form a complex mask. 3. Apply the combined mask to a given tensor to filter out specific elements. # Function Signature ```python def apply_complex_mask(tensor: torch.Tensor, mask_shape: tuple) -> torch.Tensor: pass ``` # Input - `tensor`: A `torch.Tensor` object. The tensor on which the mask will be applied. - `mask_shape`: A tuple indicating the shape of basic masks to be created. # Output - Returns a `torch.Tensor` where the elements failing the combined mask conditions are set to zero. # Constraints - The tensor can be of any shape, but the masks will always have the shape provided by `mask_shape`. - Assume that `tensor` has at least as many elements as the total number of elements in the mask. # Example ```python # Example usage of apply_complex_mask function import torch # Example tensor tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # Example mask shape mask_shape = (3, 3) # Applying the complex mask result = apply_complex_mask(tensor, mask_shape) # Expected Result: # A tensor where certain elements determined by the mask are zeroed out. ``` Notes - You can use the following functions from the `torch.nn.attention.flex_attention` module to create and manipulate masks: `create_block_mask`, `create_mask`, `and_masks`, `or_masks`. - Make sure your function is efficient and leverages the provided mask utility functions correctly. Make use of the `BlockMask` class if needed to demonstrate understanding of advanced functionalities provided by the module.","solution":"import torch import torch.nn.functional as F def create_block_mask(shape, fill_value): Utility to create a simple block mask. return torch.full(shape, fill_value) def create_mask(shape, fill_value): Utility to create a simple mask. return torch.full(shape, fill_value) def and_masks(mask1, mask2): Combine two masks using AND operation. return torch.logical_and(mask1, mask2) def or_masks(mask1, mask2): Combine two masks using OR operation. return torch.logical_or(mask1, mask2) def apply_complex_mask(tensor: torch.Tensor, mask_shape: tuple) -> torch.Tensor: Applies a complex mask to a tensor. The complex mask is made by combining several basic masks. # Create basic masks mask1 = create_block_mask(mask_shape, True) mask2 = create_mask(mask_shape, False) mask3 = create_block_mask(mask_shape, True) # Make complex mask by combining basic masks combined_mask = or_masks(mask1, and_masks(mask2, mask3)) # Ensure the tensor and mask shapes are compatible if tensor.shape[:len(mask_shape)] != mask_shape: raise ValueError(\\"The mask shape and tensor shape are not compatible\\") # Apply the combined mask to the tensor (by broadcasting) masked_tensor = torch.where(combined_mask, tensor, torch.tensor(0, dtype=tensor.dtype)) return masked_tensor"},{"question":"**Question:** Using the `seaborn` library, you need to create a function that visualizes the trends in a dataset over time using line plots. You will be provided with a dataset containing time series data. Your task is to implement a function that generates and saves a complex plot with the following requirements: 1. **Load the dataset**: Your function should take the name of a dataset as a string and load the dataset using `seaborn.load_dataset()`. 2. **Perform basic data manipulation**: Extract the day of the year from the date column and color code the lines by the year. 3. **Facet the data by decade**: Divide the data into facets by decade rounded to the nearest 10 years. 4. **Customize the lines**: Draw lines with varying line widths and colors. 5. **Add labels**: Include a title label dynamically showing the decade for each facet. The function signature should be: ```python def create_time_series_plot(dataset_name: str, date_col: str, value_col: str, output_file: str) -> None: pass ``` **Input:** - `dataset_name` (str): The name of the dataset to load. - `date_col` (str): The name of the date column. - `value_col` (str): The name of the value column to be plotted. - `output_file` (str): The name of the file to save the plot (e.g., \'output.png\'). **Output:** - This function does not return any values. It should save the generated plot to a file specified by the `output_file` parameter. **Constraints:** - The date column must be of datetime type. - The dataset should have at least 10 years of data to be meaningfully faceted by decades. **Example:** ```python # Example usage: create_time_series_plot(\'seaice\', \'Date\', \'Extent\', \'seaice_plot.png\') ``` **Notes:** 1. The `seaice` dataset used in the example contains `Date` and `Extent` columns. 2. Ensure your plot is readable and correctly labeled.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import numpy as np def create_time_series_plot(dataset_name: str, date_col: str, value_col: str, output_file: str) -> None: # Load the dataset data = sns.load_dataset(dataset_name) # Ensure the date column is of datetime type data[date_col] = pd.to_datetime(data[date_col]) # Extract the year and day of the year data[\'year\'] = data[date_col].dt.year data[\'day_of_year\'] = data[date_col].dt.dayofyear # Determine the decade for each row data[\'decade\'] = (data[\'year\'] // 10) * 10 # Initialize the FacetGrid g = sns.FacetGrid(data, col=\'decade\', col_wrap=2, height=5, aspect=1.5, sharey=False) # Generate lineplot for each facet g = g.map_dataframe(sns.lineplot, x=\'day_of_year\', y=value_col, hue=\'year\', linewidth=1.5) # Customizing the plot g.add_legend() g.set_titles(\\"{col_name}\'s\\") # Set title for each facet with {col_name} referring to the decade g.set_axis_labels(\\"Day of Year\\", \\"Value\\") # Adding a main title plt.subplots_adjust(top=0.9) g.fig.suptitle(f\\"Trends in {value_col.capitalize()} Over Time\\", fontsize=16) # Save the plot to the specified file plt.savefig(output_file) plt.close() # Example usage: # create_time_series_plot(\'seaice\', \'Date\', \'Extent\', \'seaice_plot.png\')"},{"question":"# Attention Mechanism Implementation in PyTorch Objective: Implement a custom attention mechanism using PyTorch\'s experimental attention APIs. This task will assess your understanding of attention mechanisms and your ability to utilize PyTorch\'s functions to implement such a mechanism effectively. Background: Attention mechanisms allow the model to focus on different parts of the input sequence dynamically, improving performance on tasks such as sequence-to-sequence modeling, machine translation, and more. We will implement a scaled dot-product attention mechanism using PyTorch\'s experimental API. Task: Implement a class `ScaledDotProductAttention` in PyTorch that calculates the attention scores for a given set of queries, keys, and values. Requirements: 1. **Initialization**: - The class should be initialized without any parameters. 2. **Method - `forward`**: - **Input**: - `queries` (`torch.Tensor` of shape `(batch_size, heads, seq_len_q, depth)`): The set of query vectors. - `keys` (`torch.Tensor` of shape `(batch_size, heads, seq_len_k, depth)`): The set of key vectors. - `values` (`torch.Tensor` of shape `(batch_size, heads, seq_len_v, depth)`): The set of value vectors. - `mask` (`torch.Tensor` of shape `(batch_size, 1, 1, seq_len_k)`, optional): A mask tensor to prevent attention to certain positions (e.g., padding). - **Output**: - `output` (`torch.Tensor` of shape `(batch_size, heads, seq_len_q, depth)`): The result of the scaled dot-product attention. - `attention_weights` (`torch.Tensor` of shape `(batch_size, heads, seq_len_q, seq_len_k)`): The attention weights. 3. **Constraints**: - The function should use scaled dot-product to compute attention scores: ( text{Attention}(Q, K, V) = text{softmax}left(frac{QK^T}{sqrt{d_k}}right) V ) - Implement masking to ignore certain positions during attention score computation. 4. **Performance**: - Ensure your implementation is efficient in terms of both time and space complexity. Example Usage: ```python import torch from torch.nn.attention.experimental import ScaledDotProductAttention # Sample data batch_size, heads, seq_len_q, seq_len_k, seq_len_v, depth = 2, 4, 5, 6, 6, 8 queries = torch.rand(batch_size, heads, seq_len_q, depth) keys = torch.rand(batch_size, heads, seq_len_k, depth) values = torch.rand(batch_size, heads, seq_len_v, depth) mask = torch.randint(0, 2, (batch_size, 1, 1, seq_len_k)) # Initialize attention mechanism attention = ScaledDotProductAttention() # Compute attention output, attention_weights = attention.forward(queries, keys, values, mask) ``` Submission: Please provide your implementation in a class named `ScaledDotProductAttention`, and ensure it includes all necessary imports and works with the provided example usage.","solution":"import torch import torch.nn.functional as F class ScaledDotProductAttention: def __init__(self): pass def forward(self, queries, keys, values, mask=None): Calculate the attention weights and output. Args: queries (torch.Tensor): shape (batch_size, heads, seq_len_q, depth) keys (torch.Tensor): shape (batch_size, heads, seq_len_k, depth) values (torch.Tensor): shape (batch_size, heads, seq_len_v, depth) mask (torch.Tensor, optional): shape (batch_size, 1, 1, seq_len_k) Returns: output (torch.Tensor): shape (batch_size, heads, seq_len_q, depth) attention_weights (torch.Tensor): shape (batch_size, heads, seq_len_q, seq_len_k) batch_size, heads, seq_len_q, depth = queries.shape _, _, seq_len_k, _ = keys.shape # Calculate the dot product scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(depth, dtype=torch.float32)) # Apply the mask (if provided) if mask is not None: scores = scores.masked_fill(mask == 0, float(\'-inf\')) # Apply softmax to get attention weights attention_weights = F.softmax(scores, dim=-1) # Compute the output output = torch.matmul(attention_weights, values) return output, attention_weights"},{"question":"**Problem Statement: Implementation and Evaluation of Semi-Supervised Learning** # Background: You are given data with both labeled and unlabeled instances. Your task is to use semi-supervised learning techniques to improve the classification performance. Scikit-learn provides `SelfTrainingClassifier` for self-training and `LabelPropagation` as well as `LabelSpreading` for label propagation methods. # Objective: Write code to implement and compare the following two semi-supervised learning approaches on a given dataset: 1. `SelfTrainingClassifier` 2. `LabelPropagation` or `LabelSpreading` # Instructions: 1. **Data Preparation**: - Use a dataset (e.g., digits dataset from sklearn) with some labels removed (set as -1). Divide the dataset into training and testing sets. 2. **Implement `SelfTrainingClassifier`**: - Initialize a SelfTrainingClassifier with a suitable base estimator (such as a DecisionTreeClassifier). - Train the SelfTrainingClassifier on the training data. 3. **Implement Label Propagation/Spreading**: - Choose between `LabelPropagation` and `LabelSpreading` based on your understanding. - Train the model on the same training data with labels partially removed. 4. **Evaluation**: - Evaluate and compare the performance of both models on the test set using an appropriate metric (e.g., accuracy). - Output the performance metrics and provide a brief explanation of the results. # Constraints: - The unlabeled entries in your `y` should be identified with the integer value `-1`. - Ensure that at least 30% of the training data labels are set to `-1`. # Input: - Training data `(X_train, y_train)` containing both labeled and unlabeled data. - Testing data `(X_test, y_test)`. # Output: - Accuracy score for `SelfTrainingClassifier` on the test set. - Accuracy score for `LabelPropagation` or `LabelSpreading` on the test set. - A brief explanation comparing the two models\' performances. # Example: ```python from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier import numpy as np # Load dataset and prepare training/testing data digits = load_digits() X, y = digits.data, digits.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) y_train[30:] = -1 # Unlabeling some of the data # 1. SelfTrainingClassifier base_clf = DecisionTreeClassifier() self_training_clf = SelfTrainingClassifier(base_clf) self_training_clf.fit(X_train, y_train) st_accuracy = self_training_clf.score(X_test, y_test) # 2. LabelPropagation label_prop_model = LabelPropagation() label_prop_model.fit(X_train, y_train) lp_accuracy = label_prop_model.score(X_test, y_test) # Output results print(\\"SelfTrainingClassifier Accuracy:\\", st_accuracy) print(\\"LabelPropagation Accuracy:\\", lp_accuracy) # Explanation (brief) ``` The provided example initializes and trains the models with a sample dataset, ensuring to include unlabeled data in the training set. Adjust the example as needed for your specific dataset and requirements.","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.tree import DecisionTreeClassifier import numpy as np def semi_supervised_learning_comparison(): # Load dataset and prepare training/testing data digits = load_digits() X, y = digits.data, digits.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42) # Unlabeling some of the training data rng = np.random.RandomState(42) random_unlabeled_points = rng.rand(y_train.shape[0]) < 0.3 y_train[random_unlabeled_points] = -1 evaluations = {} # 1. SelfTrainingClassifier base_clf = DecisionTreeClassifier() self_training_clf = SelfTrainingClassifier(base_clf) self_training_clf.fit(X_train, y_train) st_accuracy = self_training_clf.score(X_test, y_test) evaluations[\'SelfTrainingClassifier Accuracy\'] = st_accuracy # 2. LabelPropagation label_prop_model = LabelPropagation() label_prop_model.fit(X_train, y_train) lp_accuracy = label_prop_model.score(X_test, y_test) evaluations[\'LabelPropagation Accuracy\'] = lp_accuracy return evaluations"},{"question":"# Custom Classifier Implementation **Objective**: Implement a custom scikit-learn compatible classifier with basic functionality. This exercise will test your understanding of the scikit-learn estimator conventions. **Description**: You will implement a custom `EuclideanNearestNeighborClassifier` that classifies data points based on the Euclidean distance to the nearest point from the training set. **Requirements**: 1. The classifier should inherit from `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin`. 2. Implement the following methods: - `__init__(self, demo_param=\'demo\')`: Initialize with a `demo_param` hyperparameter. - `fit(self, X, y)`: Fit the classifier using the training data `X` and labels `y`. - `predict(self, X)`: Predict the class labels for the provided input data `X`. - `fit_predict(self, X, y)`: Combine `fit` and `predict` for efficiency. **Constraints**: - You must ensure correct input validation using `sklearn.utils.validation` methods. - Adhere to Scikit-learn conventions as described in the documentation (e.g., managing parameters and attributes correctly). **Function Signature**: ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, validate_data from sklearn.utils.multiclass import unique_labels from sklearn.metrics import euclidean_distances class EuclideanNearestNeighborClassifier(ClassifierMixin, BaseEstimator): def __init__(self, demo_param=\'demo\'): self.demo_param = demo_param def fit(self, X, y): # Validate input X, y = validate_data(self, X, y) # Store the classes seen during fit self.classes_ = unique_labels(y) # Store the training data self.X_ = X self.y_ = y # Return the classifier return self def predict(self, X): # Check if fit has been called check_is_fitted(self, [\\"X_\\", \\"y_\\", \\"classes_\\"]) # Validate input X = validate_data(self, X, reset=False) # Compute Euclidean distances and predict the nearest label closest = np.argmin(euclidean_distances(X, self.X_), axis=1) return self.y_[closest] def fit_predict(self, X, y): self.fit(X, y) return self.predict(X) ``` # Example Usage ```python X_train = np.array([[1, 2], [3, 4], [5, 6]]) y_train = np.array([0, 1, 1]) X_test = np.array([[1, 2], [5, 6]]) clf = EuclideanNearestNeighborClassifier() clf.fit(X_train, y_train) predictions = clf.predict(X_test) print(predictions) # Output should be the predicted labels for X_test ``` # Notes: - Ensure that `fit` method follows the described patterns closely. - Make sure to include thorough input validations and checks in each method. - Your implementation should be compatible with Scikit-learn pipelines and model selection tools.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, validate_data from sklearn.utils.multiclass import unique_labels from sklearn.metrics import euclidean_distances class EuclideanNearestNeighborClassifier(ClassifierMixin, BaseEstimator): def __init__(self, demo_param=\'demo\'): self.demo_param = demo_param def fit(self, X, y): # Validate input X, y = validate_data(self, X, y) # Store the classes seen during fit self.classes_ = unique_labels(y) # Store the training data self.X_ = X self.y_ = y # Return the classifier return self def predict(self, X): # Check if fit has been called check_is_fitted(self, [\\"X_\\", \\"y_\\", \\"classes_\\"]) # Validate input X = validate_data(self, X, reset=False) # Compute Euclidean distances and predict the nearest label closest = np.argmin(euclidean_distances(X, self.X_), axis=1) return self.y_[closest] def fit_predict(self, X, y): self.fit(X, y) return self.predict(X)"},{"question":"Objective: To assess student comprehension of scope management and custom object types by implementing a simplified version of the cell object and its related operations as described in the Python C-API documentation. Problem Statement: Create a class named `SimpleCell` that simulates the behavior of cell objects in Python as described in the documentation. Implement the following methods: 1. `__init__(self, value=None)`: Initializes the cell with a given value or `None`. 2. `get(self)`: Returns the current value stored in the cell. 3. `set(self, value)`: Updates the cell to store the new value. 4. `is_cell(obj)`: A static method that checks if the given object is an instance of `SimpleCell`. Input and Output: - Initialization: A `SimpleCell` object can be initialized with or without an initial value. ```python cell = SimpleCell(10) empty_cell = SimpleCell() ``` - `get(self)`: Should return the current value inside the cell. ```python cell.get() # Returns 10 empty_cell.get() # Returns None ``` - `set(self, value)`: Sets the content of the cell to the new value. ```python cell.set(20) cell.get() # Returns 20 ``` - `is_cell(obj)`: This static method should check if the `obj` is indeed a `SimpleCell` object. ```python SimpleCell.is_cell(cell) # Returns True SimpleCell.is_cell(10) # Returns False ``` Constraints: - `value` can be of any type. - Ensure that `set` correctly updates the value and `get` retrieves the current value without errors. - `is_cell` should return a boolean indicating if the object is an instance of `SimpleCell`. Example: ```python # Initialization sc = SimpleCell(42) print(sc.get()) # 42 # Setting new value sc.set(100) print(sc.get()) # 100 # Checking cell type print(SimpleCell.is_cell(sc)) # True print(SimpleCell.is_cell(42)) # False ``` Requirements: - Your implementation should be efficient and handle edge cases (e.g., initializing with no value).","solution":"class SimpleCell: def __init__(self, value=None): self._value = value def get(self): return self._value def set(self, value): self._value = value @staticmethod def is_cell(obj): return isinstance(obj, SimpleCell)"},{"question":"**Objective:** The goal of this assessment is to evaluate your understanding and proficiency in using the `seaborn` library to visualize complex statistical relationships. You are required to demonstrate the use of different plot types, semantics (hue, style, size), and faceting. **Question:** You are given a dataset about daily weather observations in a city over several years. The dataset contains the following columns: - `date`: Date of the observation. - `temperature_max`: Maximum temperature recorded on that day. - `temperature_min`: Minimum temperature recorded on that day. - `precipitation`: Amount of precipitation recorded on that day. - `wind_speed`: Average wind speed on that day. Using the `seaborn` library, perform the following tasks: 1. **Scatter Plot**: - Create a scatter plot to visualize the relationship between `temperature_max` and `precipitation`. - Use `hue` to differentiate the points by `wind_speed` (categorize into \'low\', \'medium\', \'high\'). - Apply a suitable color palette for the `hue` semantic. 2. **Line Plot**: - Create a line plot showing the change in average `temperature_max` over time. - Add a `hue` semantic to differentiate the lines by seasons (\'Winter\', \'Spring\', \'Summer\', \'Fall\'). 3. **Enhanced Scatter Plot**: - Create a scatter plot to visualize the relationship between `temperature_min` and `temperature_max`. - Use `size` to represent the amount of `precipitation`. - Use both `hue` and `style` to differentiate by wind speed categories (\'low\', \'medium\', \'high\'). 4. **Faceting**: - Use facets to create a grid of scatter plots showing the relationship between `temperature_max` and `precipitation` for each season. - Use `hue` to differentiate the points by `wind_speed`. **Constraints:** - Ensure plots are properly labeled and include legends where applicable. - Data points should be clearly distinguishable. **Input Format:** A DataFrame named `weather_data` containing columns: \'date\', \'temperature_max\', \'temperature_min\', \'precipitation\', \'wind_speed\'. **Output Format:** Four figures each containing the specified plots. Below is a template to get you started: ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Load the weather data (for the purpose of this task, assume this is already provided) weather_data = pd.read_csv(\'path_to_weather_data.csv\') # Create a scatter plot for temperature_max vs precipitation plt.figure(figsize=(10, 6)) sns.scatterplot(data=weather_data, x=\'temperature_max\', y=\'precipitation\', hue=\'wind_speed_category\', palette=\'coolwarm\') plt.title(\'Temperature Max vs Precipitation\') plt.show() # Create a line plot for average temperature_max over time, differentiated by season plt.figure(figsize=(14, 8)) sns.lineplot(data=weather_data, x=\'date\', y=\'temperature_max\', hue=\'season\') plt.title(\'Average Temperature Max Over Time by Season\') plt.show() # Create an enhanced scatter plot for temperature_min vs temperature_max plt.figure(figsize=(10, 6)) sns.scatterplot(data=weather_data, x=\'temperature_min\', y=\'temperature_max\', hue=\'wind_speed_category\', size=\'precipitation\', style=\'wind_speed_category\') plt.title(\'Temperature Min vs Temperature Max with Precipitation Size and Wind Speed Category\') plt.show() # Create facets of scatter plots for temperature_max vs precipitation by season g = sns.FacetGrid(weather_data, col=\\"season\\", hue=\\"wind_speed_category\\", height=5, aspect=1) g.map(sns.scatterplot, \\"temperature_max\\", \\"precipitation\\").add_legend() plt.show() ``` **Note:** You may need to preprocess the data to add categorical columns for the `seasons` and categorize `wind_speed` into \'low\', \'medium\', \'high\'. All visualizations should be clear and informative, demonstrating your understanding of the `seaborn` library.","solution":"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns def preprocess_weather_data(df): Preprocess the weather data DataFrame by adding season and categorizing wind_speed. # Convert date to datetime df[\'date\'] = pd.to_datetime(df[\'date\']) # Add a \'season\' column based on the date df[\'season\'] = df[\'date\'].dt.month%12 // 3 + 1 df[\'season\'] = df[\'season\'].replace({1: \'Winter\', 2: \'Spring\', 3: \'Summer\', 4: \'Fall\'}) # Categorize wind_speed into \'low\', \'medium\', \'high\' df[\'wind_speed_category\'] = pd.cut(df[\'wind_speed\'], bins=[0, 10, 20, float(\'inf\')], labels=[\'low\', \'medium\', \'high\']) return df def create_scatter_plot(df): Create a scatter plot to visualize the relationship between temperature_max and precipitation. Use hue to differentiate the points by wind_speed (categorized). plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'temperature_max\', y=\'precipitation\', hue=\'wind_speed_category\', palette=\'coolwarm\') plt.title(\'Temperature Max vs Precipitation\') plt.xlabel(\'Max Temperature (Â°C)\') plt.ylabel(\'Precipitation (mm)\') plt.legend(title=\'Wind Speed Category\') def create_line_plot(df): Create a line plot showing the change in average temperature_max over time. Add hue semantic to differentiate lines by seasons. plt.figure(figsize=(14, 8)) sns.lineplot(data=df, x=\'date\', y=\'temperature_max\', hue=\'season\') plt.title(\'Average Temperature Max Over Time by Season\') plt.xlabel(\'Date\') plt.ylabel(\'Max Temperature (Â°C)\') plt.legend(title=\'Season\') def create_enhanced_scatter_plot(df): Create an enhanced scatter plot to visualize the relationship between temperature_min and temperature_max. Use size to represent precipitation and hue and style to differentiate by wind speed categories. plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'temperature_min\', y=\'temperature_max\', hue=\'wind_speed_category\', size=\'precipitation\', style=\'wind_speed_category\', palette=\'viridis\') plt.title(\'Temperature Min vs Temperature Max with Precipitation Size and Wind Speed Category\') plt.xlabel(\'Min Temperature (Â°C)\') plt.ylabel(\'Max Temperature (Â°C)\') plt.legend(title=\'Wind Speed Category\') def create_faceted_scatter_plot(df): Use facets to create a grid of scatter plots showing the relationship between temperature_max and precipitation for each season. Use hue to differentiate the points by wind_speed. g = sns.FacetGrid(df, col=\\"season\\", hue=\\"wind_speed_category\\", height=5, aspect=1) g.map(sns.scatterplot, \\"temperature_max\\", \\"precipitation\\").add_legend() g.set_axis_labels(\'Max Temperature (Â°C)\', \'Precipitation (mm)\') g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\'Temperature Max vs Precipitation by Season\')"},{"question":"You have been provided with a dataset that contains information about the tips received by waiters in a restaurant. The dataset has the following columns: - `total_bill`: The total bill amount (including tips). - `tip`: The tip amount. - `sex`: The gender of the person paying the bill. - `smoker`: Whether the person is a smoker or not (`Yes` or `No`). - `day`: The day of the week. - `time`: The time of day (`Lunch` or `Dinner`). - `size`: The size of the party. Your task is to perform the following actions using seaborn: 1. Create a seaborn `FacetGrid` object plotting the distribution of total bill amounts for different days of the week. Each subplot should represent a day, and the distribution should be visualized using histograms. 2. Customize the appearance of the plots using seaborn styles: - Use the \\"darkgrid\\" style for the plots. - Customize the font scale to 1.2. 3. Use a context manager to switch to the \\"whitegrid\\" style temporarily and plot a barplot showing the average tip amount by gender. 4. Save all the plots to separate image files. # Input Format The function should not have any inputs but should assume that the dataset is loaded into a pandas DataFrame named `tips`. # Output Format Save the following plots as image files: - A file named `facetgrid_bill_distribution.png` containing the FacetGrid histogram. - A file named `average_tips_by_gender.png` containing the barplot. # Constraints - Handle the dataset appropriately to avoid any potential errors. - Ensure that the plots are clear and properly labeled. ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_seaborn_plots(): # Assume tips dataset is already loaded into a pandas DataFrame named `tips` tips = sns.load_dataset(\\"tips\\") # 1. Create a FacetGrid for total bill distribution per day g = sns.FacetGrid(tips, col=\\"day\\") g.map(plt.hist, \\"total_bill\\") g.set_axis_labels(\\"Total Bill\\", \\"Frequency\\") g.fig.suptitle(\\"Distribution of Total Bill Amounts by Day of Week\\", y=1.03) g.savefig(\\"facetgrid_bill_distribution.png\\") # 2. Customize the appearance using seaborn styles sns.set_style(\\"darkgrid\\") sns.set_context(\\"notebook\\", font_scale=1.2) # 3. Temporarily switch to \'whitegrid\' style and plot barplot of average tips by gender with sns.axes_style(\\"whitegrid\\"): plt.figure() sns.barplot(x=\\"sex\\", y=\\"tip\\", data=tips, ci=None) plt.title(\\"Average Tip Amount by Gender\\") plt.xlabel(\\"Gender\\") plt.ylabel(\\"Average Tip Amount\\") plt.savefig(\\"average_tips_by_gender.png\\") ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_seaborn_plots(): # Assume tips dataset is already loaded into a pandas DataFrame named `tips` tips = sns.load_dataset(\\"tips\\") # 1. Create a FacetGrid for total bill distribution per day g = sns.FacetGrid(tips, col=\\"day\\") g.map(plt.hist, \\"total_bill\\") g.set_axis_labels(\\"Total Bill\\", \\"Frequency\\") g.fig.suptitle(\\"Distribution of Total Bill Amounts by Day of Week\\", y=1.03) g.savefig(\\"facetgrid_bill_distribution.png\\") # 2. Customize the appearance using seaborn styles sns.set_style(\\"darkgrid\\") sns.set_context(\\"notebook\\", font_scale=1.2) # 3. Temporarily switch to \'whitegrid\' style and plot barplot of average tips by gender with sns.axes_style(\\"whitegrid\\"): plt.figure() sns.barplot(x=\\"sex\\", y=\\"tip\\", data=tips, ci=None) plt.title(\\"Average Tip Amount by Gender\\") plt.xlabel(\\"Gender\\") plt.ylabel(\\"Average Tip Amount\\") plt.savefig(\\"average_tips_by_gender.png\\")"},{"question":"# Isotonic Regression Implementation Task You are given a dataset (X) and (y) along with their corresponding weights (w). Your task is to implement a function that fits an isotonic regression model to this data using a non-decreasing constraint. You should then use this model to predict values for a given test dataset. **Function Signature:** ```python def fit_predict_isotonic_regression(X_train, y_train, w_train, X_test): Fits an IsotonicRegression model to the training data and predicts for the test data. Parameters: - X_train: List or ndarray of shape (n_samples,) - The input data. - y_train: List or ndarray of shape (n_samples,) - The target values. - w_train: List or ndarray of shape (n_samples,) - The weights. - X_test: List or ndarray of shape (m_samples,) - The data to predict on. Returns: - y_pred: List or ndarray of shape (m_samples,) - The predicted values for the test data. pass ``` # Requirements: 1. **Non-decreasing Constraint:** Ensure the isotonic regression model adheres to a non-decreasing constraint. 2. **Training and Predicting:** Fit the model using the provided training data (X_{text{train}}), (y_{text{train}}), and (w_{text{train}}), and use it to predict values for the provided test data (X_{text{test}}). 3. **Output:** The function should return the predicted values corresponding to (X_{text{test}}). # Example ```python X_train = [1, 2, 3, 4, 5] y_train = [5, 6, 7, 8, 9] w_train = [1, 1, 1, 1, 1] X_test = [1.5, 2.5, 3.5, 4.5] # Expected output: A list of predicted values corresponding to X_test. y_pred = fit_predict_isotonic_regression(X_train, y_train, w_train, X_test) print(y_pred) # Example: [5.5, 6.5, 7.5, 8.5] ``` **Constraints:** - The input arrays (X_{text{train}}, y_{text{train}}, w_{text{train}}, X_{text{test}}) will all have lengths between 1 and 1000. - The values in (w_{text{train}}) are strictly positive. **Performance Requirements:** - The code should efficiently handle the maximum input sizes. Make sure you implement the function correctly and verify it with various test cases to ensure its accuracy.","solution":"from sklearn.isotonic import IsotonicRegression def fit_predict_isotonic_regression(X_train, y_train, w_train, X_test): Fits an IsotonicRegression model to the training data and predicts for the test data. Parameters: - X_train: List or ndarray of shape (n_samples,) - The input data. - y_train: List or ndarray of shape (n_samples,) - The target values. - w_train: List or ndarray of shape (n_samples,) - The weights. - X_test: List or ndarray of shape (m_samples,) - The data to predict on. Returns: - y_pred: List or ndarray of shape (m_samples,) - The predicted values for the test data. # Create and fit the isotonic regression model. iso_reg = IsotonicRegression(increasing=True) iso_reg.fit(X_train, y_train, sample_weight=w_train) # Predict using the test data. y_pred = iso_reg.predict(X_test) return y_pred"},{"question":"**Question: Context Variable Management and Async Task Execution** Design a Python script that demonstrates the usage of context variables in an asynchronous environment. Your script will involve defining context variables, manipulating their values, and running asynchronous tasks while preserving context states. Follow the instructions given below: # Instructions: 1. Define two context variables: - `user_id` with a default value of `None`. - `user_role` with a default value of `\\"guest\\"`. 2. Implement an asynchronous function `process_request` that simulates processing a user request. This function should: - Accept two arguments: `uid` (user ID) and `role` (user role). - Set `user_id` and `user_role` to the provided `uid` and `role`. - Simulate some processing by printing the current values of `user_id` and `user_role`. - Use an `await` statement to simulate an asynchronous delay (using `asyncio.sleep`). 3. Implement another asynchronous function `main` that: - Creates a list of tasks, each calling `process_request` with different user IDs and roles. - Use different context copies for each task to ensure context isolation. - Runs all tasks concurrently and ensures that context values are maintained properly within each task. 4. Ensure that after the execution, any access to the context variables `user_id` and `user_role` outside the tasks will yield their default values. # Example Output: Your script should produce output similar to the following: ``` Processing user ID: 1 with role: admin Processing user ID: 2 with role: editor Processing user ID: 3 with role: viewer Outside task - user_id: None, user_role: guest ``` # Constraints: - Use the `contextvars` module features as described in the documentation. - Ensure proper context management to avoid leaking values between tasks. # Code Skeleton: Here is a sample skeleton for your reference: ```python import asyncio import contextvars # Define context variables user_id = contextvars.ContextVar(\'user_id\', default=None) user_role = contextvars.ContextVar(\'user_role\', default=\'guest\') async def process_request(uid, role): # Set context variables user_id.set(uid) user_role.set(role) # Simulate processing await asyncio.sleep(1) print(f\'Processing user ID: {user_id.get()} with role: {user_role.get()}\') async def main(): # List of user requests users = [(1, \'admin\'), (2, \'editor\'), (3, \'viewer\')] tasks = [] for uid, role in users: ctx = contextvars.copy_context() task = ctx.run(process_request, uid, role) tasks.append(task) # Run all tasks await asyncio.gather(*tasks) # Check context variables outside tasks print(f\'Outside task - user_id: {user_id.get()}, user_role: {user_role.get()}\') # Run the main function asyncio.run(main()) ``` # Requirements: - Ensure all tasks run concurrently and context variables maintain their values within their respective tasks. - Test your script with the `asyncio.run` function. Good luck!","solution":"import asyncio import contextvars # Define context variables user_id = contextvars.ContextVar(\'user_id\', default=None) user_role = contextvars.ContextVar(\'user_role\', default=\'guest\') async def process_request(uid, role): # Set context variables user_id.set(uid) user_role.set(role) # Simulate processing await asyncio.sleep(1) print(f\'Processing user ID: {user_id.get()} with role: {user_role.get()}\') async def main(): # List of user requests users = [(1, \'admin\'), (2, \'editor\'), (3, \'viewer\')] tasks = [] for uid, role in users: ctx = contextvars.copy_context() task = ctx.run(process_request, uid, role) tasks.append(task) # Run all tasks await asyncio.gather(*tasks) # Check context variables outside tasks print(f\'Outside task - user_id: {user_id.get()}, user_role: {user_role.get()}\') # Run the main function asyncio.run(main())"},{"question":"Problem Statement You are designing a secure data processing system where you need to handle various file types and configurations safely. Your task is to create a Python function that reads data from a given configuration file, processes it, and saves the processed data securely. Design a function `process_configuration(config_file: str) -> None` that: 1. Reads and parses a configuration file in XML format. 2. The configuration file contains settings about data transformation and file paths for input and output. 3. Loads data securely from a specified input file. 4. Applies the transformation rules to the data as specified in the configuration. 5. Saves the transformed data to the output file securely. # Input - `config_file` (str): The path to the XML configuration file. The XML configuration file has the following structure: ```xml <config> <input_file>path/to/input/file</input_file> <output_file>path/to/output/file</output_file> <transform> <type>base64</type> <operation>encode</operation> </transform> </config> ``` # Output - None # Example Suppose the provided configuration file is `config.xml`: ```xml <config> <input_file>input.txt</input_file> <output_file>output.txt</output_file> <transform> <type>base64</type> <operation>encode</operation> </transform> </config> ``` Contents of `input.txt`: ``` Hello, World! ``` After running `process_configuration(\\"config.xml\\")`, the `output.txt` should contain: ``` SGVsbG8sIFdvcmxkIQ== ``` # Constraints 1. Use the `xml` module to parse the XML configuration file safely. 2. Use `base64` for encoding and decoding tasks based on the transform type. 3. Read data from the input file and process it securely. 4. Write the processed data to the output file securely. 5. Ensure that all file operations are properly managed, avoiding security risks associated with temporary files or insecure data handling. # Hints 1. Be mindful of the security considerations when handling XML, file operations, and base64 encoding/decoding. 2. Validate the paths and transformations specified in the configuration file. # Notes - Avoid using deprecated or insecure methods. - Ensure that your implementation handles error scenarios gracefully, such as missing files or invalid configuration.","solution":"import xml.etree.ElementTree as ET import base64 def process_configuration(config_file: str) -> None: Processes the configuration file, reads the input file, applies the transformation, and writes the result to the output file. # Parse the XML configuration file tree = ET.parse(config_file) root = tree.getroot() # Extract details from the configuration file input_file = root.find(\'input_file\').text output_file = root.find(\'output_file\').text transform_type = root.find(\'transform/type\').text operation = root.find(\'transform/operation\').text # Read data from the input file with open(input_file, \'rb\') as f: data = f.read() # Apply transformation based on configuration if transform_type == \'base64\': if operation == \'encode\': transformed_data = base64.b64encode(data) elif operation == \'decode\': transformed_data = base64.b64decode(data) else: raise ValueError(f\\"Unsupported operation: {operation}\\") else: raise ValueError(f\\"Unsupported transformation type: {transform_type}\\") # Write the transformed data to the output file with open(output_file, \'wb\') as f: f.write(transformed_data)"},{"question":"# Question: Analyzing Restaurant Tips Data Using Seaborn You are given a dataset containing details about restaurant tips. Using Seaborn, you need to visualize this dataset by creating bar charts that show various aspects of the data. Requirements 1. Load the \'tips\' dataset from Seaborn\'s built-in datasets. 2. Generate a bar plot that shows the count of tips received on each day. 3. Generate a bar plot that shows the count of tips received on each day, further divided by the gender of the person giving the tip. 4. Generate a bar plot that shows the count of different party sizes (size of the group dining), without binning the data. 5. Generate a bar plot that shows the count of different party sizes along the y-axis. Specifications * Use the `seaborn.objects` interface for creating these plots. * Add proper labels and titles to make the plots informative. Code Template ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # 1. Bar plot showing the count of tips received each day # Your code here # 2. Bar plot showing the count of tips received each day, divided by gender # Your code here # 3. Bar plot showing the count of different party sizes # Your code here # 4. Bar plot showing the count of different party sizes along the y-axis # Your code here ``` Constraints 1. Use the Seaborn library to load the dataset and create the plots. 2. Ensure the plots are clear and correctly segmented as specified. Your task is to implement the above requirements in the code template provided.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # 1. Bar plot showing the count of tips received each day plot1 = ( so.Plot(tips, x=\\"day\\") .add(so.Bars(), so.Count()) .label(title=\\"Count of Tips Received Each Day\\", x=\\"Day\\", y=\\"Count\\") ) plot1.show() # 2. Bar plot showing the count of tips received each day, divided by gender plot2 = ( so.Plot(tips, x=\\"day\\", color=\\"sex\\") .add(so.Bars(), so.Count()) .label(title=\\"Count of Tips Received Each Day by Gender\\", x=\\"Day\\", y=\\"Count\\", color=\\"Gender\\") ) plot2.show() # 3. Bar plot showing the count of different party sizes plot3 = ( so.Plot(tips, x=\\"size\\") .add(so.Bars(), so.Count()) .label(title=\\"Count of Different Party Sizes\\", x=\\"Party Size\\", y=\\"Count\\") ) plot3.show() # 4. Bar plot showing the count of different party sizes along the y-axis plot4 = ( so.Plot(tips, y=\\"size\\") .add(so.Bars(), so.Count()) .label(title=\\"Count of Different Party Sizes (Y-axis)\\", y=\\"Party Size\\", x=\\"Count\\") ) plot4.show()"},{"question":"# AsyncIO Concurrency Problem Description You are tasked with writing a Python script that performs multiple I/O-bound operations concurrently using the asyncio library. The problem is designed to assess your understanding of async/await syntax, task creation, managing concurrent execution, handling timeouts, and basic error handling in asynchronous programming. Problem Statement 1. **Function to Perform I/O-bound Operations:** - Write a coroutine `io_bound_task` that simulates an I/O-bound operation by using `asyncio.sleep`. - The coroutine should take two arguments: - `task_name` (str): Name of the task. - `delay` (int): Time in seconds for which the task should \\"sleep\\". - The coroutine should print a start message, sleep for the given delay, and then print a completion message. 2. **Main Execution Function:** - Write an asynchronous function `main_execution` which performs the following: - Creates a list of tasks using `asyncio.create_task` for various delays. - Use `await asyncio.gather` to run all tasks concurrently. - Implement a timeout mechanism using `asyncio.wait_for` such that if any task takes longer than a specified timeout (e.g., 5 seconds), it should cancel the task and handle the timeout gracefully. 3. **Additional Requirements:** - You must use the `asyncio.run` function to run the `main_execution` function. - Ensure proper exception handling for tasks that get cancelled due to timeout. Input The function `main_execution` does not take any inputs but internally creates tasks with predefined delays. Output For each task, the script should print: - A start message: `\\"Task <task_name> started\\"` - A completion message: `\\"Task <task_name> completed\\"` - If a task is cancelled due to timeout, it should print: `\\"Task <task_name> cancelled due to timeout\\"` Example ```python import asyncio # Coroutine to simulate I/O-bound operation async def io_bound_task(task_name, delay): print(f\\"Task {task_name} started\\") await asyncio.sleep(delay) print(f\\"Task {task_name} completed\\") # Main execution function async def main_execution(): tasks = [ asyncio.create_task(io_bound_task(\\"A\\", 2)), asyncio.create_task(io_bound_task(\\"B\\", 6)), asyncio.create_task(io_bound_task(\\"C\\", 3)) ] try: await asyncio.wait_for(asyncio.gather(*tasks), timeout=5.0) except asyncio.TimeoutError: print(\\"Some tasks took too long and were cancelled\\") if __name__ == \\"__main__\\": asyncio.run(main_execution()) ``` By completing this task, you will have demonstrated an understanding of fundamental asyncio concepts, including coroutine creation, task management, concurrent execution, timeouts, and error handling. Constraints - You may assume delays will be positive integers. - Implement proper exception handling for any `asyncio.CancelledError`.","solution":"import asyncio # Coroutine to simulate I/O-bound operation async def io_bound_task(task_name, delay): print(f\\"Task {task_name} started\\") await asyncio.sleep(delay) print(f\\"Task {task_name} completed\\") # Main execution function async def main_execution(): task_configs = [(\\"A\\", 2), (\\"B\\", 6), (\\"C\\", 3)] tasks = [asyncio.create_task(io_bound_task(name, delay)) for name, delay in task_configs] try: await asyncio.wait_for(asyncio.gather(*tasks), timeout=5.0) except asyncio.TimeoutError: print(\\"Some tasks took too long and were cancelled\\") # Check for individual task completion and handle cancellation for task in tasks: if task.cancelled(): name = task.get_name() print(f\\"Task {name} cancelled due to timeout\\") if __name__ == \\"__main__\\": asyncio.run(main_execution())"},{"question":"You are required to write a Python function that utilizes the `sysconfig` module to generate a report of various Python configuration details. The report should include the following: 1. The current platform. 2. The Python version. 3. The default installation scheme for the current platform. 4. The preferred scheme for the \\"user\\" installation layout. 5. All the path names supported by `sysconfig`. 6. All the installation paths corresponding to the default scheme for the current platform. The function should return these details as a dictionary with the following format: ```python { \\"platform\\": <platform>, \\"python_version\\": <python_version>, \\"default_scheme\\": <default_scheme>, \\"preferred_user_scheme\\": <preferred_user_scheme>, \\"path_names\\": <path_names>, \\"installation_paths\\": <installation_paths> } ``` Where: - `<platform>` is a string containing the current platform. - `<python_version>` is a string containing the Python version. - `<default_scheme>` is a string containing the default installation scheme for the current platform. - `<preferred_user_scheme>` is a string containing the preferred scheme for the \\"user\\" installation layout. - `<path_names>` is a list of strings containing all the path names. - `<installation_paths>` is a dictionary where the keys are path names and the values are the corresponding installation paths. # Function Signature ```python def generate_sysconfig_report() -> dict: pass ``` # Example Usage ```python report = generate_sysconfig_report() print(report) ``` Expected output: ```python { \\"platform\\": \\"win-amd64\\", \\"python_version\\": \\"3.10\\", \\"default_scheme\\": \\"nt\\", \\"preferred_user_scheme\\": \\"nt_user\\", \\"path_names\\": [\\"stdlib\\", \\"platstdlib\\", \\"purelib\\", \\"platlib\\", \\"include\\", \\"platinclude\\", \\"scripts\\", \\"data\\"], \\"installation_paths\\": { \\"stdlib\\": \\"C:/Python310/Lib\\", \\"platstdlib\\": \\"C:/Python310/DLLs\\", \\"purelib\\": \\"C:/Python310/Lib/site-packages\\", \\"platlib\\": \\"C:/Python310/Lib/site-packages\\", \\"include\\": \\"C:/Python310/include\\", \\"platinclude\\": \\"C:/Python310/include\\", \\"scripts\\": \\"C:/Python310/Scripts\\", \\"data\\": \\"C:/Python310\\" } } ``` Note: The exact values will vary depending on the platform and Python version used. # Constraints - The `sysconfig` module should be used to obtain all the required information. - The solution should handle cases where certain configuration variables or paths are not present, and in such cases, default to returning `None`. # Performance Requirements - The function should be efficient and avoid unnecessary computations or lookups. # Additional Notes - Ensure that you handle any potential exceptions that may arise from using the `sysconfig` functions, and provide meaningful error messages if applicable.","solution":"import sysconfig import platform def generate_sysconfig_report(): Generates a report of various Python configuration details using the sysconfig module. Returns a dictionary with platform, python_version, default_scheme, preferred_user_scheme, path_names, and installation_paths. try: platform_info = platform.platform() python_version = platform.python_version() default_scheme = sysconfig.get_default_scheme() preferred_user_scheme = sysconfig.get_preferred_scheme(\\"user\\") path_names = sysconfig.get_path_names() installation_paths = {} for path_name in path_names: installation_paths[path_name] = sysconfig.get_path(path_name) report = { \\"platform\\": platform_info, \\"python_version\\": python_version, \\"default_scheme\\": default_scheme, \\"preferred_user_scheme\\": preferred_user_scheme, \\"path_names\\": path_names, \\"installation_paths\\": installation_paths } return report except Exception as e: return {\\"error\\": str(e)}"},{"question":"# Question You are tasked with debugging a Python program using the `pdb` module. The program calculates the factorial of a number, but it currently contains an error. Your objective is to write a Python script that uses the `pdb` debugger to help identify and fix the error in the program. Provided Code Here is the faulty program for calculating the factorial of a number: ```python def factorial(n): if n == 0: return 1 else: return n * factorial(n - 1) def main(): print(\\"Factorial Debugging\\") number = 5 result = factorial(number) print(f\\"The factorial of {number} is {result}\\") if __name__ == \\"__main__\\": main() ``` Instructions 1. Insert a breakpoint at the start of the `factorial` function using the `pdb` debugger. 2. Step through the code to observe the control flow and identify where the error occurs. 3. Once you\'ve identified the error, fix the code. 4. Use the `pdb` commands to display the values of relevant variables at each step. 5. Modify the script to report the fixed output. Expected Output The script should print the correct factorial of the number 5, which is 120. Constraints - You must use the `pdb` module to set breakpoints and step through the code. - You are not allowed to use any other debugging tools. Example An example of how to use `pdb` to set a breakpoint and continue execution: ```python import pdb def factorial(n): pdb.set_trace() # Set a breakpoint at the start of the factorial function if n == 0: return 1 else: return n * factorial(n - 1) # Other parts of the code remain the same ``` Complete the script by inserting appropriate `pdb` commands and fixing the code if necessary to produce the expected output.","solution":"import pdb def factorial(n): pdb.set_trace() # Set a breakpoint at the start of the factorial function if n == 0 or n == 1: # Fix the base condition to handle 1 return 1 else: return n * factorial(n - 1) def main(): print(\\"Factorial Debugging\\") number = 5 result = factorial(number) print(f\\"The factorial of {number} is {result}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Demonstrate your comprehension of Python\'s `pickle` module, including pickling, unpickling, custom pickling behavior, and handling stateful objects. **Problem Statement**: You are given a class `TaskManager` that manages a list of tasks. Each task is a dictionary containing an `id`, `name`, and `status`. The `TaskManager` can add tasks, update their status, and retrieve a summary of all tasks. However, we need to ensure that instances of `TaskManager` are both picklable and unpicklable. Additionally, upon unpickling, we want to add an extra functionality: recomputing an internal attribute `_total_tasks` that counts the total number of tasks. **Task**: 1. Implement the `TaskManager` class with the following methods: - `add_task(task: Dict[str, Any])`: Add a new task to the task list. - `update_task_status(task_id: int, status: str)`: Update the status of a task. - `get_summary() -> List[Dict[str, Any]]`: Return a list of all tasks. 2. Make sure the class is picklable. You must implement custom `__getstate__` and `__setstate__` methods to ensure the `_total_tasks` attribute is recomputed upon unpickling. **Input**: - You can assume that valid inputs will be provided to the methods. Each task will be a dictionary containing at least the keys `id` (int), `name` (str), and `status` (str). **Output**: - The class should behave correctly before and after pickling and unpickling. **Example Usage**: ```python import pickle task1 = {\\"id\\": 1, \\"name\\": \\"Task 1\\", \\"status\\": \\"pending\\"} task2 = {\\"id\\": 2, \\"name\\": \\"Task 2\\", \\"status\\": \\"completed\\"} # Create TaskManager and add tasks tm = TaskManager() tm.add_task(task1) tm.add_task(task2) # Update a task status tm.update_task_status(1, \\"completed\\") # Get the summary of tasks print(tm.get_summary()) # [{\'id\': 1, \'name\': \'Task 1\', \'status\': \'completed\'}, {\'id\': 2, \'name\': \'Task 2\', \'status\': \'completed\'}] # Pickle the TaskManager instance tm_data = pickle.dumps(tm) # Unpickle the TaskManager instance tm_new = pickle.loads(tm_data) # Check that the total number of tasks is correctly recomputed print(tm_new._total_tasks) # 2 print(tm_new.get_summary()) # [{\'id\': 1, \'name\': \'Task 1\', \'status\': \'completed\'}, {\'id\': 2, \'name\': \'Task 2\', \'status\': \'completed\'}] ``` # Constraints: - You must use the `pickle` module for serialization and deserialization. - Ensure that the `_total_tasks` attribute is accurate after unpickling. # Implementation Requirements: - Define the `TaskManager` class. - Implement `add_task`, `update_task_status`, and `get_summary` methods. - Implement custom `__getstate__` and `__setstate__` methods for managing the pickling and unpickling process. ```python import pickle from typing import List, Dict, Any class TaskManager: def __init__(self): self.tasks = [] self._total_tasks = 0 def add_task(self, task: Dict[str, Any]): self.tasks.append(task) self._total_tasks += 1 def update_task_status(self, task_id: int, status: str): for task in self.tasks: if task[\\"id\\"] == task_id: task[\\"status\\"] = status break def get_summary(self) -> List[Dict[str, Any]]: return self.tasks def __getstate__(self): state = self.__dict__.copy() # Remove any attributes that should not be pickled return state def __setstate__(self, state): self.__dict__.update(state) # Recompute the total number of tasks self._total_tasks = len(self.tasks) ```","solution":"import pickle from typing import List, Dict, Any class TaskManager: def __init__(self): self.tasks = [] self._total_tasks = 0 def add_task(self, task: Dict[str, Any]): Adds a new task to the task list. self.tasks.append(task) self._total_tasks += 1 def update_task_status(self, task_id: int, status: str): Updates the status of an existing task given its ID. for task in self.tasks: if task[\\"id\\"] == task_id: task[\\"status\\"] = status break def get_summary(self) -> List[Dict[str, Any]]: Returns a list of all tasks. return self.tasks def __getstate__(self): Returns the state of the instance for pickling. state = self.__dict__.copy() return state def __setstate__(self, state): Restores the state of the instance upon unpickling. self.__dict__.update(state) # Recompute the total number of tasks after unpickling self._total_tasks = len(self.tasks)"},{"question":"# Objective You are required to implement a multi-class classification model using Stochastic Gradient Descent (SGD). Your implementation should demonstrate an understanding of different loss functions, data preprocessing (scaling and shuffling), and model evaluation. # Task Description Use the `SGDClassifier` from scikit-learn to classify the Iris dataset. Perform the following steps: 1. **Data Preprocessing:** - Load the Iris dataset. - Standardize the features in the dataset to have mean 0 and variance 1. - Shuffle the dataset before splitting it into training and test sets. 2. **Model Training:** - Initialize an `SGDClassifier` with the following parameters: - `loss=\'log_loss\'` (for logistic regression) - `penalty=\'elasticnet\'` - `alpha=0.0001` - `max_iter=1000` - `tol=1e-3` - `random_state=42` - Fit the classifier on the training data using a pipeline that includes the StandardScaler. 3. **Model Evaluation:** - Predict the class labels of the test set. - Compute and print the accuracy of the model on the test set. - Compute and print the classification report including precision, recall, and F1-score for each class. # Input - There is no input reading from the user. The dataset should be loaded from scikit-learn\'s datasets module directly. # Output - Print the accuracy of the classifier on the test set. - Print the detailed classification report. # Constraints - Use the `train_test_split` function from `sklearn.model_selection` to split the data (80% for training and 20% for testing). # Example Here is an outline to help you structure your code: ```python from sklearn.datasets import load_iris from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import classification_report, accuracy_score # Step 1: Load and preprocess data iris = load_iris() X, y = iris.data, iris.target # Shuffling and splitting the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True) # Step 2: Create a pipeline with StandardScaler and SGDClassifier pipeline = make_pipeline(StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'elasticnet\', alpha=0.0001, max_iter=1000, tol=1e-3, random_state=42)) # Training the model pipeline.fit(X_train, y_train) # Step 3: Predict and evaluate y_pred = pipeline.predict(X_test) # Calculating accuracy accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') # Detailed classification report report = classification_report(y_test, y_pred) print(report) ``` # Additional Notes - Ensure to use `random_state=42` for reproducibility. - Explain any assumptions or decisions made during the implementation in comments.","solution":"from sklearn.datasets import load_iris from sklearn.linear_model import SGDClassifier from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import classification_report, accuracy_score def train_and_evaluate_sgd_classifier(): # Load and preprocess data iris = load_iris() X, y = iris.data, iris.target # Shuffling and splitting the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True) # Create a pipeline with StandardScaler and SGDClassifier pipeline = make_pipeline( StandardScaler(), SGDClassifier(loss=\'log_loss\', penalty=\'elasticnet\', alpha=0.0001, max_iter=1000, tol=1e-3, random_state=42) ) # Training the model pipeline.fit(X_train, y_train) # Predict and evaluate y_pred = pipeline.predict(X_test) # Calculating accuracy accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') # Detailed classification report report = classification_report(y_test, y_pred) print(report) return accuracy, report"},{"question":"**Question: Ambiguous Indentation Checker** **Objective:** Implement a function `custom_tab_nanny(file_or_dir, verbose=False, filename_only=False)` that mimics the functionality of the `tabnanny` module but with the following custom features: 1. The function should accept a directory or file path. 2. It should check for any indentation issues. 3. The function should handle a custom exception `CustomNannyNag` if ambiguous indentation is found. 4. Add an additional feature to output the issue count found in each file. 5. The output should be customizable based on the `verbose` and `filename_only` flags. **Input:** - `file_or_dir` (str): Path to a Python file or directory containing Python files. - `verbose` (bool): If set to `True`, print detailed information about each file checked and the issues found. - `filename_only` (bool): If set to `True`, only print the filenames of files containing indentation issues. **Output:** - If `verbose` is `True`, output detailed information about each file checked, including the number of indentation issues found. - If `filename_only` is `True`, output only the filenames of files containing indentation issues. - If both `verbose` and `filename_only` are `False`, output should include the filenames and the number of issues detected. **Example:** ```python def custom_tab_nanny(file_or_dir, verbose=False, filename_only=False): # Your implementation here # Example call custom_tab_nanny(\'path/to/directory\', verbose=True, filename_only=False) ``` **Constraints:** - Implement and raise `CustomNannyNag` exception when an ambiguous indentation is detected. - Use standard libraries for file handling and token processing. - Assume that the Python files\' syntax is correct and focus only on whitespace issues. **Performance Requirements:** - The function should efficiently handle directories with a large number of nested Python files. - Minimize redundant checks and optimize the reading and processing of files. **Notes:** - You may refer to the standard `tokenize` module for token processing. - Ensure compatibility with Python 3.10. - Provide comments and documentation within your code explaining your approach and any assumptions made.","solution":"import os import tokenize from io import StringIO class CustomNannyNag(Exception): Exception raised for ambiguous indentation detected by the customized tab nanny. pass def check_indentation(file_content): Check for indentation issues in the given file content. Returns the number of indentation issues detected. last_lineno = -1 last_level = 0 indentation_issues = 0 try: readline = StringIO(file_content).readline tokens = tokenize.generate_tokens(readline) for tok in tokens: if tok.type == tokenize.INDENT: if last_lineno == tok.start[0]: # Same line but with a different indentation level raise CustomNannyNag(f\\"Ambiguous indentation detected at line {tok.start[0]}\\") last_lineno = tok.start[0] last_level = len(tok.string) elif tok.type == tokenize.DEDENT: last_lineno = tok.start[0] last_level = len(tok.string) except CustomNannyNag: indentation_issues += 1 except tokenize.TokenError as e: pass return indentation_issues def custom_tab_nanny(file_or_dir, verbose=False, filename_only=False): Custom tab nanny to check Python files for indentation issues. Args: file_or_dir (str): Path to a Python file or directory containing Python files. verbose (bool): Print detailed information about each file checked and the issues found. filename_only (bool): Only print the filenames of files containing indentation issues. issues_count = 0 def process_file(file_path): nonlocal issues_count try: with open(file_path, \'r\') as file: content = file.read() issues = check_indentation(content) if issues > 0: issues_count += issues if verbose: print(f\\"File {file_path}: {issues} indentation issues found.\\") elif filename_only: print(f\\"{file_path}\\") else: print(f\\"File {file_path}: {issues} issues detected.\\") except Exception as e: if verbose: print(f\\"Error processing file {file_path}: {e}\\") if os.path.isfile(file_or_dir): process_file(file_or_dir) else: for root, _, files in os.walk(file_or_dir): for file in files: if file.endswith(\'.py\'): process_file(os.path.join(root, file)) if not filename_only and verbose: print(f\\"Total issues found: {issues_count}\\")"},{"question":"# Mock and Patch Usage in Unit Tests **Objective**: Demonstrate your understanding of the `unittest.mock` library by writing unit tests using `Mock`, `MagicMock`, and `patch` to test a simple module. **Background**: Suppose we have a module `calculator` with a class `Calculator` as defined below: `calculator.py` ```python class Calculator: def add(self, a, b): return a + b def mul(self, a, b): return a * b def get_external_value(self): # Simulate an external resource call, e.g., a web service raise NotImplementedError(\\"This method should be mocked!\\") ``` **Task**: Write a set of unit tests for the `Calculator` class using the `unittest.mock` library. 1. Test the `add` and `mul` methods to ensure they return the correct results (without mocking). 2. Mock the `get_external_value` method to return a specific value and verify the behavior. 3. Use `patch` to mock `get_external_value` during a test and verify that it was called with the correct parameters. # Requirements: - Use `Mock` and `MagicMock` where appropriate. - Use `patch` to replace the `get_external_value` method within the scope of a test. - Verify the mock interactions using assertion methods provided by `unittest.mock`. **Example**: Here\'s an example structure for your test file: `test_calculator.py` ```python import unittest from unittest.mock import patch, Mock, MagicMock from calculator import Calculator class TestCalculator(unittest.TestCase): def test_add(self): # Test add method without mocking calc = Calculator() self.assertEqual(calc.add(2, 3), 5) def test_mul(self): # Test mul method without mocking calc = Calculator() self.assertEqual(calc.mul(2, 3), 6) @patch(\'calculator.Calculator.get_external_value\') def test_get_external_value(self, mock_get_external_value): # Mock get_external_value and verify behavior mock_get_external_value.return_value = 42 calc = Calculator() self.assertEqual(calc.get_external_value(), 42) mock_get_external_value.assert_called_once() if __name__ == \'__main__\': unittest.main() ``` **Submission**: - Implement the remaining parts of the `TestCalculator` class to fully test the `Calculator` using mocks. - Ensure your tests cover various scenarios and edge cases. **Constraints**: - Follow Python best practices for writing unit tests. - Include proper comments in your test code for clarity. - Ensure all tests pass successfully when run.","solution":"class Calculator: def add(self, a, b): return a + b def mul(self, a, b): return a * b def get_external_value(self): # Simulate an external resource call, e.g., a web service raise NotImplementedError(\\"This method should be mocked!\\")"},{"question":"**Coding Assessment Question:** ***Objective:*** Implement a Python function that simulates the behavior of `PyCellObject` and its methods using Python classes and methods. Your solution should provide similar functionality and align with the provided documentation. ***Task:*** 1. Define a `Cell` class in Python. 2. Implement the following methods based on the provided documentation: - `__init__(self, value)`: This method initializes a cell object with the given value. - `is_cell(self)`: This method checks and returns whether the current object is an instance of the `Cell` class. - `get(self)`: This method returns the current value stored in the cell. - `set(self, value)`: This method sets a new value inside the cell. 3. Implement the following additional methods to ensure functionality: - `__str__(self)`: This method returns a string representation of the cell\'s content. ***Constraints:*** 1. The `value` parameter for initializing Cell and setting a new value can be `None` or any valid Python object. ***Input/Output:*** Your program does not need to take any input. The class and methods will be tested by another script. Example Usage: ```python cell1 = Cell(10) print(cell1.get()) # Output: 10 cell1.set(20) print(cell1.get()) # Output: 20 print(cell1.is_cell()) # Output: True print(cell1) # Output: Cell containing: 20 ``` ***Notes:*** - The behavior should closely mimic the descriptions provided for `PyCellObject` methods in the given documentation. - Ensure that reference checks and safety measures are in place, similar to the C API.","solution":"class Cell: def __init__(self, value): Initializes a cell object with the given value. self._value = value def is_cell(self): Checks and returns whether the current object is an instance of the Cell class. return isinstance(self, Cell) def get(self): Returns the current value stored in the cell. return self._value def set(self, value): Sets a new value inside the cell. self._value = value def __str__(self): Returns a string representation of the cell\'s content. return f\\"Cell containing: {self._value}\\""},{"question":"You are tasked with creating a utility function for secure message transmission. This utility should be able to: 1. Encode a given binary message into Base64. 2. Convert the Base64 message into a hexadecimal representation. 3. Compute the CRC32 checksum of this hexadecimal representation for error-checking purposes. The function should then output the Base64 encoded message, its hexadecimal representation, and the computed CRC32 checksum. # Function Signature ```python def secure_message_transmission(message: bytes) -> tuple: Encode a binary message into Base64, convert it to a hexadecimal representation, and compute the CRC32 checksum. Args: message (bytes): The binary message to be transmitted. Returns: tuple: A tuple containing the Base64 string, hexadecimal representation, and CRC32 checksum (as an unsigned integer). ``` # Input - `message`: A `bytes` object containing the binary message. The length can vary but should be a non-empty byte string. # Output - A tuple `(base64_message, hex_representation, crc32_checksum)` where: - `base64_message` is the Base64 encoded string of the original message. - `hex_representation` is the hexadecimal encoded string of the Base64 encoded message. - `crc32_checksum` is the CRC32 checksum computed from the hexadecimal representation. # Example ```python message = b\\"Hello World!\\" output = secure_message_transmission(message) print(output) # Expected output: (\'SGVsbG8gV29ybGQhn\', \'53475673626738475632496b684d513dn\', <CRC32_CHECKSUM>) ``` # Constraints - The function should use `binascii` for all encoding and checksum calculations. - Ensure that the input `message` is not empty. Use the example to verify your implementation, but remember that your function should be general enough to handle any binary data input. # Hints - Use `binascii.b2a_base64` to convert to Base64. - Use `binascii.b2a_hex` or `binascii.hexlify` to convert to a hexadecimal representation. - Use `binascii.crc32` to compute the CRC32 checksum.","solution":"import binascii def secure_message_transmission(message: bytes) -> tuple: Encode a binary message into Base64, convert it to a hexadecimal representation, and compute the CRC32 checksum. Args: message (bytes): The binary message to be transmitted. Returns: tuple: A tuple containing the Base64 string, hexadecimal representation, and CRC32 checksum (as an unsigned integer). if not message: raise ValueError(\\"Input message cannot be empty\\") # Encode the message to Base64 base64_message = binascii.b2a_base64(message).decode(\'utf-8\').strip() # Convert the Base64 message to hexadecimal representation hex_representation = binascii.hexlify(base64_message.encode(\'utf-8\')).decode(\'utf-8\') # Compute the CRC32 checksum of the hexadecimal representation crc32_checksum = binascii.crc32(hex_representation.encode(\'utf-8\')) & 0xffffffff return (base64_message, hex_representation, crc32_checksum)"},{"question":"# Objective Evaluate your understanding of the `csv` module in Python by performing data transformations and handling different CSV formats. # Problem Statement You are given a CSV file named `data.csv` containing information about various products. Each row in the CSV represents a product with the following fields: - `ProductID` - `ProductName` - `Price` - `Quantity` Your task is to write a Python function to read this CSV file, apply specific transformations to the data, and write the transformed data back to a new CSV file named `transformed_data.csv`. # Transformations 1. Apply a 10% discount on the `Price` of all products. 2. Normalize the `ProductName` field by converting all characters to lowercase. # Requirements 1. **Function 1**: `read_and_transform_csv(input_filename: str, output_filename: str) -> None` - **Input** - `input_filename`: A string representing the name of the input CSV file (`data.csv`). - `output_filename`: A string representing the name of the output CSV file (`transformed_data.csv`). - **Output** - This function does not return anything. It should write the transformed data to the `output_filename`. 2. **Functionality Details**: - Read the `input_filename` using the `csv.DictReader` class. - Apply the transformations: - Reduce the `Price` by 10% for each product. - Convert the `ProductName` to lowercase. - Write the transformed data to `output_filename` using the `csv.DictWriter` class. # Constraints - Assume the input CSV file will always contain valid data. - The transformations must be applied to all rows in the CSV file. - Your solution should handle large files efficiently without loading the entire file into memory at once. # Example Given the following `data.csv` file: ``` ProductID,ProductName,Price,Quantity 1,ProductA,100.0,30 2,ProductB,150.5,20 3,ProductC,200.0,15 ``` Your function should create `transformed_data.csv` with content: ``` ProductID,ProductName,Price,Quantity 1,producta,90.0,30 2,productb,135.45,20 3,productc,180.0,15 ``` # Submission Submit your function implementation, ensuring it meets the above requirements and constraints. # Notes - Consider edge cases, such as ensuring the CSV file has proper read/write access. - Adhere to the best practices for file handling in Python.","solution":"import csv def read_and_transform_csv(input_filename: str, output_filename: str) -> None: Reads the input CSV file, applies transformations, and writes the output to a new CSV file. Transformations: 1. Apply a 10% discount on the `Price` of all products. 2. Normalize the `ProductName` field by converting all characters to lowercase. Args: input_filename (str): The name of the input CSV file. output_filename (str): The name of the output CSV file. with open(input_filename, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) fieldnames = reader.fieldnames with open(output_filename, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() for row in reader: # Apply 10% discount on Price row[\'Price\'] = str(round(float(row[\'Price\']) * 0.9, 2)) # Convert ProductName to lowercase row[\'ProductName\'] = row[\'ProductName\'].lower() writer.writerow(row)"},{"question":"# **Problem Statement: Efficient Line Retrieval and Cache Management** You are tasked with implementing a small script that uses the `linecache` module to efficiently manage and retrieve specific lines from a set of text files. You will implement a function `get_lines(files_lines_dict)`, which receives a dictionary where keys are filenames and values are lists of line numbers to be retrieved from those files. Function Signature: ```python def get_lines(files_lines_dict: dict) -> dict: pass ``` Parameters: - `files_lines_dict` (dict): A dictionary where each key is a filename (str) and the corresponding value is a list of line numbers (List[int]) to be retrieved from the file. Returns: - A dictionary where each key is a filename (str) and the corresponding value is a list of strings, each string representing the content of the requested line from the file. If a line does not exist, the string returned should be empty. Constraints: - Your function should efficiently handle the caching of file contents to avoid redundant file reads. - You must handle cases where files may not exist and line numbers that are out of bounds gracefully. Example: ```python files_lines_dict = { \'file1.txt\': [1, 2, 10], \'file2.txt\': [3, 4, 1] } print(get_lines(files_lines_dict)) ``` If `file1.txt` contains: ``` Line 1 in file 1 Line 2 in file 1 Line 3 in file 1 ``` And `file2.txt` contains: ``` Line 1 in file 2 Line 2 in file 2 Line 3 in file 2 Line 4 in file 2 ``` Output should be: ```python { \'file1.txt\': [\'Line 1 in file 1n\', \'Line 2 in file 1n\', \'\'], \'file2.txt\': [\'Line 3 in file 2n\', \'Line 4 in file 2n\', \'Line 1 in file 2n\'] } ``` **Additional requirements:** 1. Use `linecache.getline()` to retrieve lines. 2. Utilize `linecache.checkcache()` to ensure the cache is updated if file changes are suspected. 3. Use `linecache.clearcache()` when you no longer need the lines. # Notes: - Assume all files in `files_lines_dict` are text files. - Use the provided example format to match the output exactly.","solution":"import linecache def get_lines(files_lines_dict): Retrieves specified lines from a set of files. Parameters: files_lines_dict (dict): Dictionary where keys are filenames and values are lists of line numbers to be retrieved. Returns: dict: A dictionary where each key is a filename and the corresponding value is a list of strings representing the content of the requested lines. result = {} for filename, lines in files_lines_dict.items(): file_lines = [] for line_number in lines: line = linecache.getline(filename, line_number) file_lines.append(line) result[filename] = file_lines # Clear the cache after we\'re done to free up memory. linecache.clearcache() return result"},{"question":"Imagine you are developing a secure login system and need to implement several functions using the \\"secrets\\" module for various security-related tasks. Please implement the following functions using Python and the \\"secrets\\" module described above. # Function 1: Generate a Secure Password Implement a function `generate_password(length: int) -> str` that generates a random alphanumeric password of a given length. The password should contain at least one lowercase character, one uppercase character, and one digit. Input: - `length` (int): The length of the password to generate. Assume `length >= 3`. Output: - str: A random alphanumeric password of the specified length. # Function 2: Generate a Secure Token Implement a function `generate_token(token_type: str, nbytes: int = None) -> str` that generates a secure token of a specified type. The function should support three token types: \'bytes\', \'hex\', and \'urlsafe\'. Input: - `token_type` (str): The type of token to generate. Can be \'bytes\', \'hex\', or \'urlsafe\'. - `nbytes` (int, optional): The number of random bytes to use for the token. If not provided, a reasonable default should be used. Output: - str: A randomly generated secure token of the specified type. # Function 3: Validate Constants Using Constant-Time Comparison Implement a function `validate_constants(a: str, b: str) -> bool` that compares two strings using a constant-time comparison method to mitigate timing attacks. Input: - `a` (str): The first string to compare. - `b` (str): The second string to compare. Output: - bool: `True` if the strings are equal, `False` otherwise. # Example Usage: ```python print(generate_password(10)) # Output: \'aB3d5Gh7J2\' (Example, will vary each time due to randomness) print(generate_token(\'hex\', 16)) # Output: \'f9bf78b9a18ce6d46a0cd2b0b86df9da\' (Example, will vary each time) print(validate_constants(\'secret123\', \'secret123\')) # Output: True print(validate_constants(\'secret123\', \'Secret123\')) # Output: False ``` # Constraints: - Do not use any external libraries other than the built-in \\"secrets\\" module. - The function `generate_password` must ensure that the generated password satisfies all the character requirements before returning.","solution":"import secrets import string def generate_password(length: int) -> str: Generates a random alphanumeric password of a given length. The password will contain at least one lowercase character, one uppercase character, and one digit. :param length: The length of the password to generate. Must be >= 3. :return: A random alphanumeric password of the specified length. if length < 3: raise ValueError(\\"Password length must be at least 3.\\") alphabet = string.ascii_letters + string.digits while True: password = \'\'.join(secrets.choice(alphabet) for i in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password)): return password def generate_token(token_type: str, nbytes: int = None) -> str: Generates a secure token of a specified type. :param token_type: The type of token to generate. Can be \'bytes\', \'hex\', or \'urlsafe\'. :param nbytes: The number of random bytes to use for the token. If not provided, a reasonable default is used. :return: A randomly generated secure token of the specified type. if token_type == \'bytes\': return secrets.token_bytes(nbytes).decode(\'latin1\') if nbytes else secrets.token_bytes().decode(\'latin1\') elif token_type == \'hex\': return secrets.token_hex(nbytes) if nbytes else secrets.token_hex() elif token_type == \'urlsafe\': return secrets.token_urlsafe(nbytes) if nbytes else secrets.token_urlsafe() else: raise ValueError(\\"Unsupported token type. Choose from \'bytes\', \'hex\', or \'urlsafe\'.\\") def validate_constants(a: str, b: str) -> bool: Compares two strings using constant-time comparison to mitigate timing attacks. :param a: The first string to compare. :param b: The second string to compare. :return: True if the strings are equal, False otherwise. return secrets.compare_digest(a, b)"},{"question":"Objective: Implement a Python function that efficiently manipulates a memory buffer using the buffer protocol, demonstrating both read and write operations in a zero-copy fashion. This exercise will test your understanding of buffer interfaces, memory views, and efficient use of memory in Python. Task: Create a class `BufferManipulator` that supports initializing from an input bytes-like object (supporting the buffer protocol), updating specific elements within the buffer, and obtaining a slice of the buffer as a bytes object. Requirements: 1. **Initialization**: - The constructor `__init__(self, buffer)` should take any bytes-like object (like `bytes`, `bytearray`, or any object supporting the buffer protocol) and store it for further manipulation. 2. **Update Buffer**: - Implement a method `update_element(self, index: int, value: int) -> None` that updates the byte at a specific index with the given value. Ensure the value is between 0 and 255. Raise an `IndexError` if the index is out of bounds. 3. **Get Slice**: - Implement a method `get_slice(self, start: int, end: int) -> bytes` that returns a slice from the buffer as a bytes object. The slice should be from the start index up to, but not including, the end index. Raise an `IndexError` if the start or end indices are out of bounds. 4. **Constraints**: - The buffer should be manipulated in-place without copying for efficiency. - Methods should raise appropriate exceptions for invalid inputs, such as out-of-bounds indices or invalid byte values. # Input Format: - The input will be provided as calls to the class methods with appropriate arguments. # Output Format: - The output should not be printed; instead, ensure that the methods perform the required manipulations or return the expected results. # Example Usage: ```python buffer = bytearray([1, 2, 3, 4, 5]) bm = BufferManipulator(buffer) # Update the 2nd element (0-indexed) to 255 bm.update_element(1, 255) # Get a slice from index 1 to 3 (0-indexed) slice_result = bm.get_slice(1, 3) # Expected b\'xffx03\' print(slice_result) # Output should be: b\'xffx03\' print(buffer) # Expected modified buffer: bytearray(b\'x01xffx03x04x05\') ``` # Notes: - You are expected to use memory views for efficient in-place buffer manipulation. - Test your implementation thoroughly with different bytes-like objects.","solution":"class BufferManipulator: def __init__(self, buffer): if not isinstance(buffer, (bytes, bytearray, memoryview)): raise TypeError(\\"Buffer must be a bytes-like object.\\") self._buffer = memoryview(buffer) def update_element(self, index: int, value: int) -> None: if not (0 <= value <= 255): raise ValueError(\\"Value must be between 0 and 255.\\") if not (0 <= index < len(self._buffer)): raise IndexError(\\"Index out of range.\\") self._buffer[index] = value def get_slice(self, start: int, end: int) -> bytes: if not (0 <= start <= end <= len(self._buffer)): raise IndexError(\\"Slice indices out of range.\\") return bytes(self._buffer[start:end])"},{"question":"**Objective**: The goal of this assessment is to evaluate your understanding of scikit-learn\'s computational performance and your ability to implement a function that optimizes and measures prediction latency and throughput. # Problem Statement You are required to implement a Python function `analyze_model_performance(models, X_train, y_train, X_test, y_test)`, which takes in different machine learning models and a dataset, trains the models, and analyzes their computational performance based on latency and throughput. The function should output a summary of the performance metrics for each model. Function Signature ```python def analyze_model_performance(models: list, X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> dict: pass ``` # Expected Input and Output Formats Input - `models`: A list of dictionary objects, where each dictionary contains a scikit-learn model and its name as follows: ```python models = [ {\\"name\\": \\"Logistic Regression\\", \\"model\\": LogisticRegression()}, {\\"name\\": \\"Random Forest\\", \\"model\\": RandomForestClassifier()}, # Add more models as needed ] ``` - `X_train`: Training data features as a NumPy array. - `y_train`: Training data labels as a NumPy array. - `X_test`: Test data features as a NumPy array. - `y_test`: Test data labels as a NumPy array. Output - A dictionary where keys are model names and values are dictionaries containing performance metrics: ```python { \\"Logistic Regression\\": {\\"latency\\": ..., \\"throughput\\": ...}, \\"Random Forest\\": {\\"latency\\": ..., \\"throughput\\": ...}, # Add more models as needed } ``` # Constraints and Requirements 1. Compute the prediction latency for each model. Latency should be measured as the average time taken to make a single prediction in microseconds. 2. Compute the prediction throughput for each model. Throughput should be measured as the number of predictions the model can make per second. 3. Ensure that the solutions take advantage of bulk vs. atomic prediction modes as described in the documentation. 4. The function should handle both dense and sparse input data representations and adjust the models accordingly for optimal performance. 5. Provide a summary of the evaluation, clearly indicating the latency and throughput for each model. # Example Usage ```python from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier import numpy as np models = [ {\\"name\\": \\"Logistic Regression\\", \\"model\\": LogisticRegression()}, {\\"name\\": \\"Random Forest\\", \\"model\\": RandomForestClassifier()}, ] # Assuming X_train, y_train, X_test, y_test are already defined as NumPy arrays performance_summary = analyze_model_performance(models, X_train, y_train, X_test, y_test) print(performance_summary) ``` Your implementation will be tested on different datasets and models to validate the correctness and performance of the function.","solution":"import time import numpy as np from sklearn.base import clone def analyze_model_performance(models, X_train, y_train, X_test, y_test): performance_summary = {} for model_info in models: model_name = model_info[\\"name\\"] model = clone(model_info[\\"model\\"]) # Train the model start_train = time.time() model.fit(X_train, y_train) end_train = time.time() # Measure latency and throughput start_pred = time.time() y_pred = model.predict(X_test) end_pred = time.time() # Total prediction time prediction_time = (end_pred - start_pred) total_predictions = X_test.shape[0] latency = (prediction_time / total_predictions) * 1e6 # Convert to microseconds throughput = total_predictions / prediction_time performance_summary[model_name] = { \\"latency\\": latency, \\"throughput\\": throughput } return performance_summary"},{"question":"Objective Implement a class that utilizes Python\'s `collections.deque` to simulate the behavior of a queue. The class should include methods for common queue operations and an additional method to perform a breadth-first search (BFS) on a given graph. This will test your understanding of data structures, algorithms, and the collections module. Problem Statement You are to implement a `Queue` class using Python\'s `collections.deque`. Your class should support the following methods: - `enqueue(data)`: Adds an element to the end of the queue. - `dequeue()`: Removes and returns the front element of the queue. If the queue is empty, return None. - `is_empty()`: Returns True if the queue is empty, False otherwise. - `bfs(graph, start)`: Performs a breadth-first search on a given graph, starting from the specified node. The graph is represented as a dictionary where keys are node identifiers and values are lists of neighboring nodes. The method should return a list of nodes in the order they are visited during the search. Input and Output - The `graph` parameter in the `bfs` method is a dictionary where keys are strings (node identifiers) and values are lists of strings (neighboring node identifiers). - The `start` parameter in the `bfs` method is a string representing the starting node for the BFS. - The return value of the `bfs` method should be a list of strings representing the nodes in the order they were visited. Constraints - Ensure that your implementation handles large graphs efficiently. - Assume that the graph is connected, i.e., there is a path between any pair of nodes. - You should not use any additional modules except `collections.deque`. Example ```python from collections import deque class Queue: def __init__(self): self.queue = deque() def enqueue(self, data): self.queue.append(data) def dequeue(self): if self.is_empty(): return None return self.queue.popleft() def is_empty(self): return len(self.queue) == 0 def bfs(self, graph, start): visited = [] self.enqueue(start) while not self.is_empty(): node = self.dequeue() if node not in visited: visited.append(node) for neighbor in graph[node]: if neighbor not in visited: self.enqueue(neighbor) return visited # Example usage graph = { \'A\': [\'B\', \'C\'], \'B\': [\'A\', \'D\', \'E\'], \'C\': [\'A\', \'F\'], \'D\': [\'B\'], \'E\': [\'B\', \'F\'], \'F\': [\'C\', \'E\'] } queue = Queue() print(queue.bfs(graph, \'A\')) # Output: [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\'] ``` Implement the `Queue` class with the described methods and test it with different graphs to ensure correctness.","solution":"from collections import deque class Queue: def __init__(self): self.queue = deque() def enqueue(self, data): self.queue.append(data) def dequeue(self): if self.is_empty(): return None return self.queue.popleft() def is_empty(self): return len(self.queue) == 0 def bfs(self, graph, start): visited = [] self.enqueue(start) while not self.is_empty(): node = self.dequeue() if node not in visited: visited.append(node) for neighbor in graph[node]: if neighbor not in visited: self.enqueue(neighbor) return visited # Example usage graph = { \'A\': [\'B\', \'C\'], \'B\': [\'A\', \'D\', \'E\'], \'C\': [\'A\', \'F\'], \'D\': [\'B\'], \'E\': [\'B\', \'F\'], \'F\': [\'C\', \'E\'] } queue = Queue() print(queue.bfs(graph, \'A\')) # Output: [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\']"},{"question":"You are tasked with building a reproducible machine learning workflow that includes data preprocessing, model training, and debugging an issue in model training. The objective is to identify and fix a common warning encountered when training a `GradientBoostingRegressor` model from the scikit-learn library. This will test your ability to preprocess data, split datasets, train models, and handle warnings or errors in a machine learning pipeline. **Problem Statement** 1. **Data Loading and Preprocessing**: Import the necessary libraries and create a synthetic dataset for regression using pandas or numpy. Ensure the dataset has feature names. 2. **Train-Test Split**: Split the dataset into training and testing sets using `train_test_split` from scikit-learn. 3. **Model Training**: Train a `GradientBoostingRegressor` model on the training data and evaluate it on the test data. 4. **Identify and Fix Warning**: Modify the model training process such that it does not raise any warnings related to feature names. **Steps to Implement** 1. **Data Creation**: - Create a synthetic dataset with proper feature names using pandas or numpy. The dataset should have at least one continuous feature and a target variable. ```python import pandas as pd import numpy as np # Create a synthetic dataset rng = np.random.RandomState(42) n_samples = 100 X = pd.DataFrame({ \\"feature_name\\": rng.randn(n_samples) }) y = pd.Series(rng.randn(n_samples), name=\\"target\\") ``` 2. **Data Splitting**: - Split the dataset into training and testing sets using `train_test_split`. ```python from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) ``` 3. **Model Training**: - Train a `GradientBoostingRegressor` model using the default parameters. Evaluate it on the test set. ```python from sklearn.ensemble import GradientBoostingRegressor gbdt = GradientBoostingRegressor(random_state=42) gbdt.fit(X_train, y_train) initial_score = gbdt.score(X_test, y_test) print(\\"Initial Model Test Score:\\", initial_score) ``` 4. **Warning Identification**: - Train another `GradientBoostingRegressor` model with the parameter `n_iter_no_change=5` and observe any warnings. Fix the warning raised by ensuring the model correctly handles feature names. ```python gbdt = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) gbdt.fit(X_train, y_train) updated_score = gbdt.score(X_test, y_test) print(\\"Updated Model Test Score:\\", updated_score) # Hint: Ensure that the input data to the model has the feature names attached correctly. ``` **Expected Output** 1. Creation and display of the synthetic dataset. 2. Splitting the data into training and testing sets. 3. Training the initial model and displaying its test score. 4. Training the second model, fixing any warnings related to feature names, and displaying the updated test score. **Constraints** - The synthetic dataset should have at least 50 samples. - The `train_test_split` should use a `test_size` of 30%. - Fix any warnings raised during the training of the `GradientBoostingRegressor` with `n_iter_no_change=5` without changing the parameter value. **Performance Requirements** - Ensure the code is minimal and functional. - Use appropriate scikit-learn methods and classes. - The code should not raise any warnings or errors after the fix. This problem tests your ability to work with data preprocessing, machine learning models, and debugging in scikit-learn, key skills for any data science or machine learning practitioner.","solution":"import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingRegressor # Create a synthetic dataset rng = np.random.RandomState(42) n_samples = 100 X = pd.DataFrame({ \\"feature_name\\": rng.randn(n_samples) }) y = pd.Series(rng.randn(n_samples), name=\\"target\\") # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train the initial GradientBoostingRegressor model gbdt_initial = GradientBoostingRegressor(random_state=42) gbdt_initial.fit(X_train, y_train) initial_score = gbdt_initial.score(X_test, y_test) print(\\"Initial Model Test Score:\\", initial_score) # Train GradientBoostingRegressor with n_iter_no_change to observe warnings gbdt_with_warning = GradientBoostingRegressor(random_state=42, n_iter_no_change=5) # Ensure the input data to the model retains feature names gbdt_with_warning.fit(X_train, y_train) updated_score = gbdt_with_warning.score(X_test, y_test) print(\\"Updated Model Test Score:\\", updated_score)"},{"question":"# **Advanced Coding Assessment: Multi-threaded Task Scheduler** **Objective:** Implement a multi-threaded task scheduler in Python using the `threading` module. Your implementation should demonstrate your understanding of thread creation, synchronization, and communication between threads. **Task:** You are required to design and implement a `TaskScheduler` class that manages a set of tasks to be executed concurrently using multiple threads. The scheduler should ensure that a maximum of `N` tasks are executed simultaneously. Each task is represented as a Python function, and the scheduler should provide methods to add tasks, start the execution, and check the status of the tasks. **Requirements:** 1. **Class Definition:** - `TaskScheduler` class initializes with a parameter `max_workers` denoting the maximum number of concurrent tasks (threads). 2. **Methods:** - `add_task(task, *args, **kwargs)`: Adds a new task to the scheduler. Each task is a callable (function) with optional arguments. - `start()`: Starts the execution of tasks using a pool of threads. Ensure that no more than `max_workers` tasks run concurrently. - `is_running()`: Returns `True` if there are tasks currently being executed, otherwise `False`. - `wait_for_all()`: Blocks until all tasks have been completed. - `get_results()`: Returns a list of results from the completed tasks. Should include a method to handle exceptions raised by any task and store the corresponding error messages. 3. **Synchronization:** - Use appropriate synchronization primitives (`Lock`, `Semaphore`, `Condition`, etc.) to manage access to shared resources and ensure thread safety. - Implement a mechanism to handle task completion and exceptions seamlessly. 4. **Exception Handling:** - Implement a global exception handler for the threads using `threading.excepthook` to log any uncaught exceptions raised during task execution. **Constraints:** - Assume tasks are CPU-bound, and the scheduler should handle them in a multi-threading environment efficiently. - Tasks may have varying execution times, and the scheduler should manage the concurrent execution without exceeding `max_workers`. **Performance Requirements:** - Ensure that the implementation efficiently manages resources and minimizes the overhead of context switching between threads. **Example Usage:** ```python import threading import time def sample_task(task_id, duration): Sample task function with varied execution times. time.sleep(duration) return f\\"Task {task_id} completed in {duration} seconds\\" # Initialize TaskScheduler with a maximum of 3 concurrent tasks scheduler = TaskScheduler(max_workers=3) # Add tasks to the scheduler scheduler.add_task(sample_task, 1, 2) scheduler.add_task(sample_task, 2, 4) scheduler.add_task(sample_task, 3, 1) scheduler.add_task(sample_task, 4, 3) # Start task execution scheduler.start() # Wait for all tasks to complete scheduler.wait_for_all() # Get results of completed tasks results = scheduler.get_results() print(results) # Expected output: [ # \\"Task 1 completed in 2 seconds\\", # \\"Task 2 completed in 4 seconds\\", # \\"Task 3 completed in 1 second\\", # \\"Task 4 completed in 3 seconds\\" # ] ``` Implement the `TaskScheduler` class along with the described methods and synchronization mechanisms to meet the requirements above. **Submission Guidelines:** - Submit the complete implementation of the `TaskScheduler` class. - Include necessary import statements and ensure the code is runnable. - Provide comments and documentation for clarity and maintainability.","solution":"import threading from queue import Queue class TaskScheduler: def __init__(self, max_workers): self.max_workers = max_workers self.tasks = Queue() self.results = [] self.lock = threading.Lock() self.running_tasks = 0 self.threads = [] def add_task(self, task, *args, **kwargs): self.tasks.put((task, args, kwargs)) def start(self): def worker(): while True: task, args, kwargs = self.tasks.get() if task is None: break try: result = task(*args, **kwargs) with self.lock: self.results.append(result) except Exception as e: with self.lock: self.results.append(e) finally: with self.lock: self.running_tasks -= 1 self.tasks.task_done() for _ in range(self.max_workers): thread = threading.Thread(target=worker) thread.start() self.threads.append(thread) with self.lock: self.running_tasks += 1 def is_running(self): with self.lock: return self.running_tasks > 0 def wait_for_all(self): self.tasks.join() # Stop worker threads for _ in range(self.max_workers): self.tasks.put((None, (), {})) for thread in self.threads: thread.join() def get_results(self): with self.lock: return self.results"},{"question":"Objective Implement a Python function that manages translations for an application using the **gettext** module. This function should support switching between two languages dynamically and demonstrate the use of singular and plural forms in translations. Function Signature ```python def manage_translations(domain: str, localedir: str, en_message: str, fr_message: str, count: int) -> None: Manage translations using the gettext module. :param domain: The domain for the translation. :param localedir: The directory containing the .mo files. :param en_message: The message in English to be translated. :param fr_message: The message in French to be translated. :param count: The count for plural forms. :return: None. The function should print the translated messages. ``` Requirements 1. Bind your domain and locale directory using `bindtextdomain()` and `textdomain()`. 2. Support two languages: English (`en`) and French (`fr`). 3. Use plural forms where appropriate. 4. Switch between the two languages dynamically and print the translations. Input - `domain` (str): The text domain for the translation. - `localedir` (str): The path to the directory containing the `.mo` files. - `en_message` (str): A translatable singular message in English. - `fr_message` (str): A translatable singular message in French. - `count` (int): An integer representing the count for plural forms. Constraints - Assume that the `.mo` files for both languages exist in the specified directory. - The messages must be valid translations in the `.mo` files for correct output. Example Usage ```python # Sample .mo files should be located under /path/to/locale directory manage_translations(\'myapplication\', \'/path/to/locale\', \'There is one apple\', \'There is one apple in French\', 5) ``` Expected Output ``` In English: There is one apple In English: There are 5 apples In French: Il y a une pomme In French: Il y a 5 pommes ``` (Note: Actual output may vary based on the translations provided in the .mo files) Implement the function and ensure it prints the translations correctly for both languages using singular and plural forms.","solution":"import gettext import os def manage_translations(domain: str, localedir: str, en_message: str, fr_message: str, count: int) -> None: Manage translations using the gettext module. :param domain: The domain for the translation. :param localedir: The directory containing the .mo files. :param en_message: The message in English to be translated. :param fr_message: The message in French to be translated. :param count: The count for plural forms. :return: None. The function should print the translated messages. # Create translations for English en_translations = gettext.translation(domain, localedir, languages=[\'en\']) en_translations.install() _ = en_translations.gettext ngettext = en_translations.ngettext # Print translated English messages print(_(\\"In English: \\") + ngettext(en_message, f\\"{en_message}s\\", count)) # Create translations for French fr_translations = gettext.translation(domain, localedir, languages=[\'fr\']) fr_translations.install() _ = fr_translations.gettext ngettext = fr_translations.ngettext # Print translated French messages print(_(\\"In French: \\") + ngettext(fr_message, f\\"{fr_message}s\\", count))"},{"question":"# Email MIME Document Generator **Objective:** Implement a function that utilizes the `email.generator` module to generate both text and binary serialized versions of a given email message object. This should demonstrate comprehension of manipulating email messages and generating their MIME formatted versions. **Problem Statement:** Your task is to write a Python function `generate_email_mime(email_message, output_file)` that takes an `EmailMessage` object and a file path, and writes both text and binary serialized versions of the email message to this file. The generated file should contain two sections: one for the text representation and one for the binary representation of the email. Constraints: 1. The function should handle any `EmailMessage` object, including those with MIME parts. 2. The output file should be writable. 3. Use appropriate policy settings to ensure standards compliance in email serialization. 4. Ensure minimal transformation of message content during serialization. Input: - `email_message`: An instance of `EmailMessage`. - `output_file`: A string representing the path to the output file. Output: - The function should write the serialized versions into the specified file. Requirements: 1. Write a detailed `generate_email_mime` function that: - Serializes the `email_message` into a MIME format using `BytesGenerator` and `Generator`. - Includes both text and binary representations in the output file. 2. Follow the corresponding policies to ensure correct MIME standards are met. 3. Provide clear comments and docstrings explaining each part of your solution. 4. Include exception handling to manage any potential file I/O errors. Example Usage: ```python from email.message import EmailMessage # Create a sample email message msg = EmailMessage() msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'from@example.com\' msg[\'To\'] = \'to@example.com\' msg.set_content(\'This is a test email.\') # Generate email MIME document generate_email_mime(msg, \'output_email.txt\') ``` After running `generate_email_mime(msg, \'output_email.txt\')`, the `output_email.txt` file should contain two sections separated by a clear marker, one for the text representation and one for the binary representation of the email message. **Note:** You may assume that relevant modules such as `email`, `email.policy`, and `os` are available and do not need to be imported within the function.","solution":"from email.generator import BytesGenerator, Generator from email.policy import default from email.message import EmailMessage def generate_email_mime(email_message, output_file): Generates both text and binary serialized versions of the given email message and writes them to the specified output file. :param email_message: An instance of EmailMessage. :param output_file: A file path string where the MIME representations will be stored. try: with open(output_file, \'w\') as file: # Write the text representation file.write(\'--- Text Representation ---n\') text_generator = Generator(file, policy=default) text_generator.flatten(email_message) with open(output_file, \'ab\') as file: # Write the binary representation file.write(b\'n--- Binary Representation ---n\') binary_generator = BytesGenerator(file, policy=default) binary_generator.flatten(email_message) except IOError as e: print(f\\"Failed to write to the file {output_file}: {e}\\")"},{"question":"Objective: Demonstrate your understanding of seaborn\'s style customization functionalities and effectively apply them to different plot types. Question: Write a function `customize_plot_styles(df, styles, plot_type)` that: 1. Accepts a DataFrame (`df`), a list of seaborn styles (`styles`), and a plot type (`plot_type`). 2. Iterates over each style in the `styles` list and generates a plot of the specified `plot_type`. 3. Displays the plot with the required style applied. 4. Supports `plot_type` as either \\"barplot\\" or \\"lineplot\\". # Parameters: - `df` (pd.DataFrame): The input data to be visualized. - Must contain at least two columns for plotting. - Example: `df = pd.DataFrame({\'x\': [1, 2, 3], \'y\': [2, 5, 3]})` - `styles` (list): List of seaborn style names. - Example: `[\'darkgrid\', \'whitegrid\']` - `plot_type` (str): Type of plot to generate, either \\"barplot\\" or \\"lineplot\\". # Output: - Displays a series of plots according to the specified styles and plot type. # Example Usage: ```python import pandas as pd data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5], \'y\': [10, 15, 13, 17, 9] }) styles_list = [\'darkgrid\', \'whitegrid\', \'ticks\'] customize_plot_styles(data, styles_list, \'barplot\') customize_plot_styles(data, styles_list, \'lineplot\') ``` # Constraints: - You should use seaborn functions to plot and style the DataFrame. - Ensure that each plot is displayed with the corresponding style. # Notes: - Use `sns.barplot` for bar plots and `sns.lineplot` for line plots. - Utilize `sns.axes_style` in a context manager as needed to apply temporary styles. Evaluate whether the student has correctly used seaborn\'s style management and demonstrated understanding of generating and customizing different plot types.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customize_plot_styles(df, styles, plot_type): Generates plots with different seaborn styles. Parameters: df (pd.DataFrame): The input data to be visualized. styles (list): List of seaborn style names. plot_type (str): Type of plot to generate, either \\"barplot\\" or \\"lineplot\\". Displays the plot according to the specified styles. for style in styles: with sns.axes_style(style): plt.figure() if plot_type == \'barplot\': sns.barplot(x=df.columns[0], y=df.columns[1], data=df) elif plot_type == \'lineplot\': sns.lineplot(x=df.columns[0], y=df.columns[1], data=df) else: raise ValueError(\\"Unsupported plot_type. Choose \'barplot\' or \'lineplot\'.\\") plt.title(f\'Style: {style}\') plt.show()"},{"question":"Pandas Data Manipulation Challenge As a data analyst, you are provided with two datasets containing information about sales transactions and customer details. Your task is to perform various data manipulation tasks using pandas and answer specific questions about the data. You are required to: 1. Load the datasets. 2. Perform data cleaning. 3. Merge the datasets. 4. Perform grouping and aggregation. 5. Generate specific insights from the data. Datasets 1. **sales.csv**: ```csv transaction_id,product_id,customer_id,quantity,price,date 1,101,1001,2,15.0,2023-01-01 2,102,1002,1,20.0,2023-01-03 3,101,1001,1,15.0,2023-01-05 4,103,1003,3,10.0,2023-01-07 5,104,1004,1,25.0,2023-01-09 ``` 2. **customers.csv**: ```csv customer_id,name,email,age,city 1001,Alice,alice@example.com,34,New York 1002,Bob,bob@example.com,45,Los Angeles 1003,Charlie,charlie@example.com,29,Chicago 1004,David,david@example.com,40,Houston ``` Tasks 1. **Load the datasets into pandas DataFrames**. 2. **Clean the data**: - Handle any missing values appropriately. - Ensure correct data types for each column. 3. **Merge the datasets** on the `customer_id` column. 4. **Create a new column** `total_amount` in the merged DataFrame, which is the product of `quantity` and `price`. 5. **Group and Aggregate**: - Find the total sales amount for each customer. - Find the total quantity of each product sold. 6. **Generate Insights**: - Which customer has spent the most amount of money? - Which product has the highest total sales? Function Signature ```python import pandas as pd def analyze_sales_data(sales_csv: str, customers_csv: str): Analyze the sales and customer datasets. Args: sales_csv (str): Path to the sales CSV file. customers_csv (str): Path to the customers CSV file. Returns: dict: A dictionary containing the following keys: \'total_sales_per_customer\': DataFrame with total sales per customer. \'total_quantity_per_product\': DataFrame with total quantity per product. \'top_customer\': The name of the customer who spent the most money. \'top_product\': The ID of the product with the highest total sales. # Load datasets sales_df = pd.read_csv(sales_csv) customers_df = pd.read_csv(customers_csv) # Data Cleaning and Type Correction sales_df[\'date\'] = pd.to_datetime(sales_df[\'date\']) sales_df[\'quantity\'] = sales_df[\'quantity\'].fillna(0).astype(int) sales_df[\'price\'] = sales_df[\'price\'].fillna(0.0).astype(float) customers_df[\'age\'] = customers_df[\'age\'].fillna(customers_df[\'age\'].mean()).astype(int) # Merge Datasets merged_df = pd.merge(sales_df, customers_df, on=\'customer_id\', how=\'inner\') # Create New Column merged_df[\'total_amount\'] = merged_df[\'quantity\'] * merged_df[\'price\'] # Group and Aggregate total_sales_per_customer = merged_df.groupby(\'name\')[\'total_amount\'].sum().reset_index() total_quantity_per_product = merged_df.groupby(\'product_id\')[\'quantity\'].sum().reset_index() # Generate Insights top_customer = total_sales_per_customer.loc[total_sales_per_customer[\'total_amount\'].idxmax(), \'name\'] top_product = total_quantity_per_product.loc[total_quantity_per_product[\'quantity\'].idxmax(), \'product_id\'] # Return Results return { \'total_sales_per_customer\': total_sales_per_customer, \'total_quantity_per_product\': total_quantity_per_product, \'top_customer\': top_customer, \'top_product\': top_product } ``` Your implementation should return results in the format specified in the function signature. Use appropriate pandas functions to accomplish the tasks. Constraints - Assume the data in the CSV files is correct and there is no need to validate the file structure. - Do not use any global variables; all data should be handled within the function scope. Performance - Assume the datasets can be large (up to millions of rows), so ensure your solution is efficient.","solution":"import pandas as pd def analyze_sales_data(sales_csv: str, customers_csv: str): Analyze the sales and customer datasets. Args: sales_csv (str): Path to the sales CSV file. customers_csv (str): Path to the customers CSV file. Returns: dict: A dictionary containing the following keys: \'total_sales_per_customer\': DataFrame with total sales per customer. \'total_quantity_per_product\': DataFrame with total quantity per product. \'top_customer\': The name of the customer who spent the most money. \'top_product\': The ID of the product with the highest total sales. # Load datasets sales_df = pd.read_csv(sales_csv) customers_df = pd.read_csv(customers_csv) # Data Cleaning and Type Correction sales_df[\'date\'] = pd.to_datetime(sales_df[\'date\']) sales_df[\'quantity\'] = sales_df[\'quantity\'].fillna(0).astype(int) sales_df[\'price\'] = sales_df[\'price\'].fillna(0.0).astype(float) customers_df[\'age\'] = customers_df[\'age\'].fillna(customers_df[\'age\'].mean()).astype(int) # Merge Datasets merged_df = pd.merge(sales_df, customers_df, on=\'customer_id\', how=\'inner\') # Create New Column merged_df[\'total_amount\'] = merged_df[\'quantity\'] * merged_df[\'price\'] # Group and Aggregate total_sales_per_customer = merged_df.groupby(\'name\')[\'total_amount\'].sum().reset_index() total_quantity_per_product = merged_df.groupby(\'product_id\')[\'quantity\'].sum().reset_index() # Generate Insights top_customer = total_sales_per_customer.loc[total_sales_per_customer[\'total_amount\'].idxmax(), \'name\'] top_product = total_quantity_per_product.loc[total_quantity_per_product[\'quantity\'].idxmax(), \'product_id\'] # Return Results return { \'total_sales_per_customer\': total_sales_per_customer, \'total_quantity_per_product\': total_quantity_per_product, \'top_customer\': top_customer, \'top_product\': top_product }"},{"question":"# Pandas Memory Usage Analysis Objective Write a Python function using pandas to analyze the memory usage of a DataFrame. Your solution should handle various data types and provide a detailed memory usage report. Additionally, implement a second function for a deep memory usage analysis, including memory usage due to objects. Requirements - **Function Signature:** ```python def memory_usage_report(df: pd.DataFrame) -> pd.DataFrame: ``` - **Input:** A pandas DataFrame, `df`. - **Output:** A pandas DataFrame with columns indicating: - `Column Name` - `Data Type` - `Memory Usage (Bytes)` - `Memory Usage (Deep, Bytes)` - `Percentage of Total Memory` ```python def deep_memory_usage_report(df: pd.DataFrame) -> pd.DataFrame: ``` - **Input:** A pandas DataFrame, `df`. - **Output:** A pandas DataFrame similar to `memory_usage_report` but utilizing `memory_usage=\\"deep\\"`. Constraints 1. The DataFrame may contain different data types including integers, floats, datetime, object, and categorical. 2. Use pandas methods such as `info()`, `memory_usage()`, and any other relevant methods to accomplish the task. 3. Handle potential differences in memory usage calculation when using `memory_usage=\'deep\'`. Performance Requirements The function should perform efficiently even for large DataFrame sizes (e.g., 100,000 rows, 50 columns). # Example Consider the following sample DataFrame: ```python import pandas as pd import numpy as np dtypes = [\\"int64\\", \\"float64\\", \\"datetime64[ns]\\", \\"timedelta64[ns]\\", \\"complex128\\", \\"object\\", \\"bool\\"] n = 5000 data = {t: np.random.randint(100, size=n).astype(t) for t in dtypes} data[\'categorical\'] = pd.Series(data[\'object\']).astype(\'category\') df = pd.DataFrame(data) ``` **Usage:** ```python memory_usage_report(df) deep_memory_usage_report(df) ``` **Expected Output:** For `memory_usage_report(df)`: | Column Name | Data Type | Memory Usage (Bytes) | Percentage of Total Memory | |-------------------|-----------------|----------------------|----------------------------| | int64 | int64 | ... | ... | | float64 | float64 | ... | ... | | datetime64[ns] | datetime64[ns] | ... | ... | | ... | ... | ... | ... | | `__TOTAL__` | | ... | 100% | For `deep_memory_usage_report(df)`: | Column Name | Data Type | Memory Usage (Bytes) | Memory Usage (Deep, Bytes) | Percentage of Total Memory | |-------------------|-----------------|----------------------|----------------------------|----------------------------| | int64 | int64 | ... | ... | ... | | float64 | float64 | ... | ... | ... | | datetime64[ns] | datetime64[ns] | ... | ... | ... | | ... | ... | ... | ... | ... | | `__TOTAL__` | | ... | ... | 100% | **Note:** - The `__TOTAL__` row should aggregate the total memory usage from all columns. - Ensure that the output DataFrame includes a column for traditional memory usage and deep memory usage for the deep memory function. # Implementation The students should implement the functions `memory_usage_report` and `deep_memory_usage_report` based on the specified requirements to validate and understand memory usage analysis in pandas.","solution":"import pandas as pd def memory_usage_report(df: pd.DataFrame) -> pd.DataFrame: Generates a memory usage report for a Pandas DataFrame. Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: A DataFrame showing memory usage by column. result = [] total_memory = df.memory_usage(deep=False).sum() for column in df.columns: usage = df.memory_usage(deep=False).loc[column] dtype = df[column].dtype percentage = (usage / total_memory) * 100 result.append([column, dtype, usage, percentage]) # Adding total row result.append([\'__TOTAL__\', \'\', total_memory, 100.0]) return pd.DataFrame(result, columns=[\'Column Name\', \'Data Type\', \'Memory Usage (Bytes)\', \'Percentage of Total Memory\']) def deep_memory_usage_report(df: pd.DataFrame) -> pd.DataFrame: Generates a deep memory usage report for a Pandas DataFrame. Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: A DataFrame showing deep memory usage by column. result = [] total_memory = df.memory_usage(deep=True).sum() for column in df.columns: usage = df.memory_usage(deep=True).loc[column] dtype = df[column].dtype percentage = (usage / total_memory) * 100 result.append([column, dtype, usage, usage, percentage]) # Adding total row result.append([\'__TOTAL__\', \'\', \'\', total_memory, 100.0]) return pd.DataFrame(result, columns=[\'Column Name\', \'Data Type\', \'Memory Usage (Bytes)\', \'Memory Usage (Deep, Bytes)\', \'Percentage of Total Memory\'])"},{"question":"**Objective**: Demonstrate the ability to dynamically execute Python modules using the `runpy` module. **Problem Statement**: You are provided with multiple Python scripts and modules located in different directories. You need to execute these scripts and modules dynamically and capture specific global variables from their execution contexts. Write a function `execute_and_capture` that: 1. Accepts a list of dictionaries as input, where each dictionary represents a script/module execution configuration: - `type`: either \'module\' or \'script\' indicating whether the entry is to be executed using `runpy.run_module` or `runpy.run_path`. - `path`: the module name (for \'module\' type) or file path (for \'script\' type). - `globals_list`: a list of global variable names to capture after execution. 2. Executes each entry using the appropriate `runpy` function. 3. Captures the requested global variables from each execution. 4. Returns a list of dictionaries with each dictionary containing the captured global variables and their values for each execution. **Function Signature**: ```python def execute_and_capture(configs: list) -> list: pass ``` # Input: - `configs` (list): A list of dictionaries with the following keys: - `type` (str): \'module\' or \'script\'. - `path` (str): Module name or script file path. - `globals_list` (list of str): List of global variables to capture. # Output: - `results` (list): A list of dictionaries with the captured global variables and their values after execution. Each dictionary corresponds to an entry in the input list. # Example: ```python configs = [ { \'type\': \'module\', \'path\': \'mymodule\', \'globals_list\': [\'__name__\', \'my_var\'] }, { \'type\': \'script\', \'path\': \'/path/to/myscript.py\', \'globals_list\': [\'__name__\', \'script_var\'] } ] results = execute_and_capture(configs) print(results) ``` Expected format for `results`: ```python [ {\'__name__\': \'mymodule\', \'my_var\': \'some_value\'}, {\'__name__\': \'__main__\', \'script_var\': 42} ] ``` # Constraints: - Assume that all paths and module names provided are valid and accessible. - If a global variable is not defined in the executed code, its value in the results can be set to `None`. # Notes: - Ensure proper usage of the `runpy.run_module` and `runpy.run_path` functions. - Consider any side effects or alterations to the `sys` module when implementing the solution.","solution":"import runpy def execute_and_capture(configs): results = [] for config in configs: if config[\'type\'] == \'module\': result = runpy.run_module(config[\'path\']) elif config[\'type\'] == \'script\': result = runpy.run_path(config[\'path\']) else: continue # Invalid type, skip this entry capture = {} for var in config[\'globals_list\']: capture[var] = result.get(var, None) results.append(capture) return results"},{"question":"**Objective:** Design and implement a Python application using the `tkinter` library that demonstrates various functionalities and concepts of the package, including widget creation, geometry management, event handling, and additional modules like `messagebox` and `filedialog`. **Question:** Create a GUI application using `tkinter` that performs the following tasks: 1. **Main Window:** - Create a main window with the title \\"Tkinter GUI Application\\". - Set the window\'s dimensions to 400x300 pixels. 2. **Widgets:** - Add a `ttk.Label` that displays the text \\"Welcome to Tkinter!\\" at the top of the window. - Add a `ttk.Button` labeled \\"Open File\\" that opens a file dialog for selecting a text file. - Add a `ttk.Button` labeled \\"Show Message\\" that displays a message box with a custom message. - Add a `ttk.Entry` for user input. - Add a `ttk.Button` labeled \\"Submit\\" that prints the content of the `ttk.Entry` widget to the console. 3. **Geometry Management:** - Use the `grid` geometry manager to arrange the widgets in the window. 4. **Event Handling:** - Implement event handling for the buttons: - The \\"Open File\\" button opens a file dialog and reads the content of the selected file, displaying it in the console. - The \\"Show Message\\" button displays a message box with the message \\"Hello, this is a message box!\\". - The \\"Submit\\" button prints the content of the `ttk.Entry` widget to the console. **Requirements:** - The application should be implemented in a function named `create_gui_app()`. The function should take no arguments. - Use appropriate modules and classes from `tkinter` and `tkinter.ttk`. - Ensure that the code follows good practices for `tkinter` such as not blocking the main event loop with long-running tasks. **Input and Output:** - Input: User interactions with the GUI (opening files, clicking buttons, entering text). - Output: Console logs of the text file content and the `ttk.Entry` widget content, message box display. **Constraints:** - The application must be implemented using `tkinter` and `tkinter.ttk`. - Do not use any external GUI libraries. **Performance Requirements:** - The application should respond quickly to user interactions without significant delays. # Example Usage 1. The user opens the application and sees the main window. 2. The user clicks the \\"Open File\\" button, selects a text file, and sees the file content printed in the console. 3. The user clicks the \\"Show Message\\" button and sees a message box with the custom message. 4. The user enters text into the `ttk.Entry` widget and clicks the \\"Submit\\" button, seeing the entered text printed in the console. **Code Template:** ```python import tkinter as tk from tkinter import ttk from tkinter import messagebox from tkinter import filedialog def create_gui_app(): # Create the main window root = tk.Tk() root.title(\\"Tkinter GUI Application\\") root.geometry(\\"400x300\\") # Add a Label label = ttk.Label(root, text=\\"Welcome to Tkinter!\\") label.grid(column=0, row=0, padx=10, pady=10) # Add an Open File button def open_file(): file_path = filedialog.askopenfilename() if file_path: with open(file_path, \'r\') as file: print(file.read()) open_file_button = ttk.Button(root, text=\\"Open File\\", command=open_file) open_file_button.grid(column=0, row=1, padx=10, pady=10) # Add a Show Message button def show_message(): messagebox.showinfo(\\"Message\\", \\"Hello, this is a message box!\\") show_message_button = ttk.Button(root, text=\\"Show Message\\", command=show_message) show_message_button.grid(column=0, row=2, padx=10, pady=10) # Add an Entry widget entry = ttk.Entry(root) entry.grid(column=0, row=3, padx=10, pady=10) # Add a Submit button def submit_entry(): print(entry.get()) submit_button = ttk.Button(root, text=\\"Submit\\", command=submit_entry) submit_button.grid(column=0, row=4, padx=10, pady=10) # Run the main event loop root.mainloop() # Test the function if __name__ == \\"__main__\\": create_gui_app() ``` Submit your completed application as a Python script file.","solution":"import tkinter as tk from tkinter import ttk from tkinter import messagebox from tkinter import filedialog def create_gui_app(): # Create the main window root = tk.Tk() root.title(\\"Tkinter GUI Application\\") root.geometry(\\"400x300\\") # Add a Label label = ttk.Label(root, text=\\"Welcome to Tkinter!\\") label.grid(column=0, row=0, padx=10, pady=10) # Add an Open File button def open_file(): file_path = filedialog.askopenfilename() if file_path: with open(file_path, \'r\') as file: print(file.read()) open_file_button = ttk.Button(root, text=\\"Open File\\", command=open_file) open_file_button.grid(column=0, row=1, padx=10, pady=10) # Add a Show Message button def show_message(): messagebox.showinfo(\\"Message\\", \\"Hello, this is a message box!\\") show_message_button = ttk.Button(root, text=\\"Show Message\\", command=show_message) show_message_button.grid(column=0, row=2, padx=10, pady=10) # Add an Entry widget entry = ttk.Entry(root) entry.grid(column=0, row=3, padx=10, pady=10) # Add a Submit button def submit_entry(): print(entry.get()) submit_button = ttk.Button(root, text=\\"Submit\\", command=submit_entry) submit_button.grid(column=0, row=4, padx=10, pady=10) # Run the main event loop root.mainloop() # Test the function if __name__ == \\"__main__\\": create_gui_app()"},{"question":"# Telnet Client Implementation **Objective**: Implement a custom Telnet client in Python using the `telnetlib` module. Your task is to connect to a given Telnet server, send a series of commands, and process the responses efficiently. This assessment is aimed to test your understanding of connection management, data reading and writing, and handling Telnet interactions. **Task**: 1. Implement a class `CustomTelnetClient` that builds on `telnetlib.Telnet`. 2. Add methods to: - Connect to a Telnet server (with proper handling of connection establishment and timeouts). - Send a series of commands to the server. - Read and process the server\'s responses using various `read_*` methods. - Handle Telnet option negotiation (using callbacks if necessary). - Implement an interactive mode to interact with the server. **Requirements**: - Establish a connection to the server with a given hostname and port. - Handle timeouts for blocking operations. - Maintain proper connection management (open, close, and avoid reopening an already connected instance). - Add a method `execute_command(command: str) -> str` which: - Sends a command to the server. - Reads and returns the response using an appropriate `read_*` method. - Implement option negotiation using `set_option_negotiation_callback`. **Input and Output**: - Input: Hostname (str), Port (int), Timeout (float, optional) - Output: Responses from the server as strings. **Sample Usage**: ```python from custom_telnet_client import CustomTelnetClient client = CustomTelnetClient(host=\'localhost\', port=23, timeout=10) client.connect() response1 = client.execute_command(\'ls\') print(response1) response2 = client.execute_command(\'whoami\') print(response2) client.close() ``` **Constraints**: - You must use the `telnetlib` module. - Handle exceptions properly to ensure the client does not crash unexpectedly. - Ensure the class supports context manager usage (using `with` statements). **Performance**: - The solution should efficiently handle multiple commands and responses. - Ensure non-blocking operations where applicable using the appropriate `read_*` methods. **Hints**: - Refer to RFC 854 for details on the Telnet protocol. - Utilize the example provided in the `telnetlib` documentation for understanding basic usage. **Extension** (Optional): - Implement a method `interactive_mode()` that allows the user to interact with the Telnet server in real-time, similar to a typical Telnet client.","solution":"import telnetlib import time class CustomTelnetClient: def __init__(self, host, port, timeout=10): self.host = host self.port = port self.timeout = timeout self.connection = None def connect(self): if self.connection is None: self.connection = telnetlib.Telnet(self.host, self.port, self.timeout) else: raise Exception(\\"Connection already established\\") def close(self): if self.connection: self.connection.close() self.connection = None def execute_command(self, command): if self.connection is None: raise Exception(\\"Not connected to any server\\") self.connection.write(command.encode(\'ascii\') + b\\"n\\") return self.connection.read_until(b\\"n\\", self.timeout).decode(\'ascii\') def set_option_negotiation_callback(self, callback): if self.connection: self.connection.set_option_negotiation_callback(callback) else: raise Exception(\\"Not connected to any server\\") def __enter__(self): self.connect() return self def __exit__(self, exc_type, exc_val, exc_tb): self.close() def interactive_mode(self): print(\\"Entering interactive mode. Press Ctrl+C to exit.\\") try: while True: command = input(\\">>> \\") response = self.execute_command(command) print(response) except KeyboardInterrupt: print(\\"nExiting interactive mode.\\")"},{"question":"# Character Set Conversion and Handling in Emails Using the `email.charset` module, your task is to implement a function that processes a given list of email messages. Each message in the list is a dictionary with the following structure: ```python { \\"from\\": \\"sender@example.com\\", \\"to\\": \\"recipient@example.com\\", \\"subject\\": \\"This is the subject\\", \\"body\\": \\"This is the body of the email message.\\", \\"charset\\": \\"input_charset\\" } ``` You need to ensure that: 1. The email body is encoded appropriately based on the specified character set. 2. If the character set is not already in the registry, it should be added with sensible defaults for header encoding (`charset.SHORTEST`) and body encoding (`charset.BASE64`). 3. The function should return the processed email messages with their body correctly encoded according to the specified character set. Requirements: 1. Implement the function `process_emails` as described. 2. Use the `email.charset.Charset` class to handle character set conversions and encoding. 3. If a character set is not found in the registry, use `email.charset.add_charset` to add it. 4. Return the list of processed email messages as dictionaries, with the encoded body under the key `\\"encoded_body\\"`. Function Signature ```python from typing import List, Dict import email.charset def process_emails(messages: List[Dict[str, str]]) -> List[Dict[str, str]]: pass ``` Example ```python # Example input messages = [ { \\"from\\": \\"sender@example.com\\", \\"to\\": \\"recipient1@example.com\\", \\"subject\\": \\"Hello\\", \\"body\\": \\"This is the body of the email.\\", \\"charset\\": \\"iso-8859-1\\" }, { \\"from\\": \\"sender@example.com\\", \\"to\\": \\"recipient2@example.com\\", \\"subject\\": \\"World\\", \\"body\\": \\"Another email body.\\", \\"charset\\": \\"euc-jp\\" } ] # Example output expected_output = [ { \\"from\\": \\"sender@example.com\\", \\"to\\": \\"recipient1@example.com\\", \\"subject\\": \\"Hello\\", \\"body\\": \\"This is the body of the email.\\", \\"charset\\": \\"iso-8859-1\\", \\"encoded_body\\": \\"This is the body of the email.\\" # This would be encoded in iso-8859-1 }, { \\"from\\": \\"sender@example.com\\", \\"to\\": \\"recipient2@example.com\\", \\"subject\\": \\"World\\", \\"body\\": \\"Another email body.\\", \\"charset\\": \\"euc-jp\\", \\"encoded_body\\": \\"Another email body.\\" # This would be encoded in euc-jp and transformed if needed } ] ``` Constraints: - You can assume the input list of messages is non-empty. - Raise a `ValueError` if the body cannot be encoded due to an unsupported character set.","solution":"from typing import List, Dict import email.charset def process_emails(messages: List[Dict[str, str]]) -> List[Dict[str, str]]: processed_messages = [] for message in messages: charset_name = message[\'charset\'] # Check if charset_name is registered if charset_name not in email.charset.CHARSETS: # Add charset with sensible defaults email.charset.add_charset(charset_name, email.charset.SHORTEST, email.charset.BASE64) # Create Charset object to handle encoding charset_obj = email.charset.Charset(charset_name) try: # Encode the message body encoded_body = message[\'body\'].encode(charset_obj.input_charset) except LookupError: raise ValueError(f\\"Unsupported charset: {charset_name}\\") # Add encoded body to the message dictionary processed_message = message.copy() processed_message[\\"encoded_body\\"] = encoded_body processed_messages.append(processed_message) return processed_messages"},{"question":"# Time Series Data Resampling and Analysis **Objective:** You are given a time series dataset that records daily temperatures and humidity levels over the course of several years. Your task is to write a function that resamples this data to calculate monthly average temperatures and humidity levels, fills missing values using forward filling, and provides descriptive statistics for the resampled data. **Requirements:** 1. **Function Signature:** ```python def resample_and_analyze(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]: ``` - **Input:** - `data`: A pandas DataFrame with a DateTime index, and two columns: `\'temperature\'` and `\'humidity\'`. - **Output:** - `monthly_means`: A DataFrame containing the monthly average `\'temperature\'` and `\'humidity\'` after resampling. - `descriptive_stats`: A DataFrame containing descriptive statistics (mean, median, min, max, std) for each month. 2. **Constraints:** - The input DataFrame `data` will have at least one year\'s worth of daily data. - The DataFrame index will be of DateTime type. - Missing values in the `\'temperature\'` and `\'humidity\'` columns need to be forward filled before performing any calculations. 3. **Performance:** - The function should efficiently handle large datasets with several years of daily data without significant performance degradation. **Example:** Given the following DataFrame `data`: ``` temperature humidity 2019-01-01 32.0 78.0 2019-01-02 NaN 81.0 2019-01-03 30.0 NaN ... 2020-12-31 35.0 76.0 ``` Expected output: - `monthly_means` DataFrame: ``` temperature humidity 2019-01 31.0 78.5 2019-02 33.2 75.4 ... 2020-12 34.0 76.1 ``` - `descriptive_stats` DataFrame: ``` temp_mean temp_median temp_min temp_max temp_std hum_mean hum_median hum_min hum_max hum_std 2019-01 31.0 31.5 30.0 32.0 0.8 78.5 78.5 76.0 81.0 1.5 2019-02 33.2 33.1 32.0 34.0 0.6 75.4 75.2 73.0 78.0 1.2 ... 2020-12 34.0 34.0 33.5 35.0 0.5 76.1 76.0 74.0 78.0 1.4 ``` **Task:** Implement the `resample_and_analyze` function according to the requirements specified.","solution":"import pandas as pd from typing import Tuple def resample_and_analyze(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]: # Forward fill the missing values data_ffill = data.ffill() # Resample to monthly averages monthly_means = data_ffill.resample(\'M\').mean() # Calculate descriptive statistics descriptive_stats = data_ffill.resample(\'M\').agg([\'mean\', \'median\', \'min\', \'max\', \'std\']) # Rename columns for clarity descriptive_stats.columns = [\'_\'.join(col).strip() for col in descriptive_stats.columns.values] return monthly_means, descriptive_stats"},{"question":"Coding Assessment Question # Objective: Write a Python function that connects to an NNTP server, retrieves the latest articles from a specified newsgroup, and saves the article bodies to a file. The function should handle exceptions gracefully and ensure the connection is properly closed in case of errors. # Function Signature: ```python def fetch_latest_articles(server, newsgroup, count, output_file): Fetches the latest articles from the specified newsgroup on the given NNTP server and writes their bodies to a file. Args: server (str): The address of the NNTP server. newsgroup (str): The name of the newsgroup to fetch articles from. count (int): The number of latest articles to fetch. output_file (str): The path to the output file where article bodies will be written. Returns: None Raises: ValueError: If the count is less than 1. ConnectionError: If there is an error connecting to the NNTP server. RuntimeError: For any other errors during the NNTP operations. ``` # Requirements: 1. The function should start by connecting to the specified NNTP server. 2. It should select the specified newsgroup and retrieve the latest `count` articles. 3. The function should write the bodies of these articles to the specified output file. 4. Ensure that the NNTP connection is closed properly using a `with` statement. 5. Handle exceptions such as connection errors or invalid responses from the server. 6. Raise a `ValueError` if `count` is less than 1. 7. Raise a `ConnectionError` if the connection to the server fails. 8. Raise a `RuntimeError` for any other NNTP-specific errors. # Input: - `server` (str): e.g., \\"news.gmane.io\\" - `newsgroup` (str): e.g., \\"gmane.comp.python.committers\\" - `count` (int): e.g., 10 - `output_file` (str): e.g., \\"/path/to/output.txt\\" # Output: No direct output. The articles\' bodies should be saved to the specified file. # Constraints: - You must use the `nntplib` module and its classes/methods to implement this function. - Ensure proper exception handling and resource management. # Example: ```python fetch_latest_articles(\'news.gmane.io\', \'gmane.comp.python.committers\', 5, \'latest_articles.txt\') ``` In the above example, the function connects to the NNTP server at \\"news.gmane.io\\", retrieves the latest 5 articles from the \\"gmane.comp.python.committers\\" newsgroup, and writes the bodies of these articles to \\"latest_articles.txt\\". # Notes: - NNTP servers might have limited access or require authentication. Assume the server in the example (`news.gmane.io`) does not require additional authentication. - You can assume the output file path is valid and writable.","solution":"import nntplib import os def fetch_latest_articles(server, newsgroup, count, output_file): Fetches the latest articles from the specified newsgroup on the given NNTP server and writes their bodies to a file. Args: server (str): The address of the NNTP server. newsgroup (str): The name of the newsgroup to fetch articles from. count (int): The number of latest articles to fetch. output_file (str): The path to the output file where article bodies will be written. Returns: None Raises: ValueError: If the count is less than 1. ConnectionError: If there is an error connecting to the NNTP server. RuntimeError: For any other errors during the NNTP operations. if count < 1: raise ValueError(\\"Count must be at least 1.\\") try: with nntplib.NNTP(server) as connection: response, count, first, last, name = connection.group(newsgroup) start = max(int(last) - count + 1, int(first)) articles = [] for article_num in range(start, int(last) + 1): try: response, info = connection.article(str(article_num)) headers, body = info.lines[:info.lines.index(b\'\')], info.lines[info.lines.index(b\'\') + 1:] articles.append(b\'n\'.join(body).decode(\'utf-8\')) except Exception as e: raise RuntimeError(f\\"Error retrieving article {article_num}: {e}\\") with open(output_file, \'w\') as file: for article in articles: file.write(article + \'nn\') except (nntplib.NNTPTemporaryError, nntplib.NNTPPermanentError) as connection_error: raise ConnectionError(f\\"Error connecting to NNTP server: {connection_error}\\") except Exception as e: raise RuntimeError(f\\"An error occurred: {e}\\")"},{"question":"In this exercise, you are tasked with loading and processing datasets using scikit-learn\'s dataset loading utilities. You will implement a function that demonstrates your understanding of the following: 1. Loading sample images and displaying them. 2. Loading a dataset in svmlight/libsvm format. 3. Fetching a dataset from the openml.org repository and displaying key attributes. Function Signature ```python def load_and_process_datasets(libsvm_path: str) -> dict: Load and process datasets using scikit-learn. Args: libsvm_path (str): The file path to the svmlight/libsvm format dataset. Returns: dict: A dictionary containing: \'china_image\' (numpy.ndarray): The loaded sample image \\"china.jpg\\". \'X_train\' (scipy.sparse.csr_matrix): The features of the loaded svmlight/libsvm format dataset. \'y_train\' (numpy.ndarray): The target of the loaded svmlight/libsvm format dataset. \'mice_data_shape\' (tuple): The shape of the \'miceprotein\' dataset features. \'mice_target_shape\' (tuple): The shape of the \'miceprotein\' dataset target. \'unique_mice_classes\' (numpy.ndarray): The unique classes in the \'miceprotein\' dataset. pass ``` # Instructions 1. **Load and Display Sample Image**: - Use `load_sample_image` to load the sample image \\"china.jpg\\". - Store the image data in a variable `china_image`. 2. **Load Dataset in svmlight/libsvm Format**: - Use `load_svmlight_file` to load a dataset in svmlight/libsvm format from the provided `libsvm_path`. - Store the features in `X_train` and targets in `y_train`. 3. **Fetch Dataset from OpenML**: - Use `fetch_openml` to load the dataset \\"miceprotein\\" (version 4). - Store the shape of the dataset features in `mice_data_shape` and the shape of the dataset target in `mice_target_shape`. - Store the unique classes of the dataset target in `unique_mice_classes`. 4. Return a dictionary containing the above results. # Constraints - You can assume the provided `libsvm_path` is a valid file path to a correctly formatted svmlight/libsvm dataset. - You should use the functions provided by scikit-learn to load and process the datasets. - Ensure the function runs efficiently and handles the data as specified. # Example ```python libsvm_path = \\"/path/to/your/libsvm_data.txt\\" result = load_and_process_datasets(libsvm_path) print(result[\'china_image\'].shape) # Example output: (427, 640, 3) print(result[\'X_train\'].shape) # Example output: (1000, 20) (depending on your svmlight/libsvm dataset) print(result[\'y_train\'].shape) # Example output: (1000,) print(result[\'mice_data_shape\']) # Example output: (1080, 77) print(result[\'mice_target_shape\']) # Example output: (1080,) print(result[\'unique_mice_classes\']) # Example output: array([\'c-CS-m\', \'c-CS-s\', \'c-SC-m\', \'c-SC-s\', \'t-CS-m\', \'t-CS-s\', \'t-SC-m\', \'t-SC-s\']) ``` Implement the `load_and_process_datasets` function to achieve the requirements as stated.","solution":"from sklearn.datasets import load_sample_image, load_svmlight_file, fetch_openml import numpy as np def load_and_process_datasets(libsvm_path: str) -> dict: # Load the sample image \\"china.jpg\\" china_image = load_sample_image(\\"china.jpg\\") # Load dataset in svmlight/libsvm format from the provided path X_train, y_train = load_svmlight_file(libsvm_path) # Fetch the \\"miceprotein\\" dataset from OpenML mice = fetch_openml(name=\\"miceprotein\\", version=4, as_frame=False) mice_data = mice.data mice_target = mice.target mice_data_shape = mice_data.shape mice_target_shape = mice_target.shape unique_mice_classes = np.unique(mice_target) return { \'china_image\': china_image, \'X_train\': X_train, \'y_train\': y_train, \'mice_data_shape\': mice_data_shape, \'mice_target_shape\': mice_target_shape, \'unique_mice_classes\': unique_mice_classes }"},{"question":"Objective: Design and implement a Python function using the `multiprocessing` module to perform parallel computations on a large dataset. Problem Statement: You are provided with a list of integers. Your task is to compute the square of each integer in the list using parallel processing for increased performance. The function should: 1. Utilize the `multiprocessing` module to distribute the computation across multiple processes. 2. Collect the results from the processes and return a list of squared integers. 3. Ensure proper synchronization and avoid any race conditions or deadlocks. Function Signature: ```python from typing import List def parallel_square_computation(numbers: List[int], num_processes: int) -> List[int]: pass ``` Input: - `numbers`: A list of integers ( left( 0 leq text{len(numbers)} leq 10^6 right) ). - `num_processes`: An integer representing the number of processes to be used for the computation ( left( 1 leq text{num_processes} leq text{os.cpu_count()} right) ). Output: - A list of integers, where each element is the square of the corresponding integer in the input list. Constraints: - The function should make efficient use of the CPU by distributing the workload evenly across the specified number of processes. - Performance is crucial; the solution should minimize the computation time as much as possible. Example: ```python # Example numbers = [1, 2, 3, 4, 5] num_processes = 2 print(parallel_square_computation(numbers, num_processes)) # Output: [1, 4, 9, 16, 25] ``` Notes: - Use the `multiprocessing.Pool` class to manage the worker processes. - Make sure to handle the case when the list is empty. - Provide detailed comments explaining the steps and mechanisms used for parallel computation and synchronization.","solution":"from multiprocessing import Pool from typing import List def square_number(n: int) -> int: Returns the square of the given number n. return n * n def parallel_square_computation(numbers: List[int], num_processes: int) -> List[int]: Compute the square of each integer in the list using parallel processing. Args: numbers: List of integers. num_processes: Number of processes to use for parallel computation. Returns: List of squared integers. if not numbers: return [] with Pool(processes=num_processes) as pool: squared_numbers = pool.map(square_number, numbers) return squared_numbers"},{"question":"Objective Write a Python program that utilizes the `RobotFileParser` class from the `urllib.robotparser` module to perform the following tasks: 1. **Set the URL**: Accept a URL as input which points to a `robots.txt` file. 2. **Read and Parse**: Read and parse the `robots.txt` file from the given URL. 3. **Query Permissions**: Check whether a default web crawler (`*`) is allowed to fetch a specific URL provided as input. 4. **Crawl Delay and Request Rate**: Retrieve and display the crawl delay and request rate for the default web crawler (`*`). 5. **Site Maps**: Display any sitemap URLs found in the `robots.txt` file. Function Implementations You need to implement the following functions: 1. `fetch_robot_file(url):` - **Input**: `url` (string) - The URL pointing to the `robots.txt` file. - **Output**: None. - **Task**: This function sets the URL and reads the `robots.txt` file from the URL. 2. `can_default_fetch(query_url):` - **Input**: `query_url` (string) - The URL we want to check access for. - **Output**: `True` or `False` - Indicates if the default web crawler is allowed to fetch the given URL based on the `robots.txt` rules. 3. `get_default_crawl_delay():` - **Output**: An integer representing the crawl delay in seconds, or `None` if not specified. 4. `get_default_request_rate():` - **Output**: A named tuple `RequestRate` with two fields, `requests` and `seconds`, or `None` if not specified. 5. `get_site_maps():` - **Output**: A list of sitemap URLs found in the `robots.txt` file, or `None` if not specified. Example Usage ```python # Example usage of the functions # Set and read the robots.txt file fetch_robot_file(\\"http://www.example.com/robots.txt\\") # Check if the default user agent can fetch a specific URL print(can_default_fetch(\\"http://www.example.com/page\\")) # Get the crawl delay for the default user agent print(get_default_crawl_delay()) # Get the request rate for the default user agent print(get_default_request_rate()) # Get the site maps mentioned in the robots.txt file print(get_site_maps()) ``` Constraints - You can assume that the URL given for the `robots.txt` file is always reachable and returns a valid response. - Handle cases where specific parameters like crawl delay, request rate, or sitemaps are not defined in the `robots.txt` file. Performance Requirements - Make sure the `robots.txt` file is read and parsed efficiently. - Minimize redundant network requests; the `robots.txt` file should be fetched only once if the functions are called multiple times. Submission Submit your Python script with the required functions. Ensure your code is well-documented, with comments explaining the logic behind each implementation.","solution":"from urllib.robotparser import RobotFileParser from collections import namedtuple # Brief namedtuple to store request rate RequestRate = namedtuple(\'RequestRate\', [\'requests\', \'seconds\']) class RobotAnalyzer: def __init__(self): self.parser = RobotFileParser() def fetch_robot_file(self, url): self.parser.set_url(url) self.parser.read() def can_default_fetch(self, query_url): return self.parser.can_fetch(\'*\', query_url) def get_default_crawl_delay(self): delay = self.parser.crawl_delay(\'*\') return delay def get_default_request_rate(self): rate = self.parser.request_rate(\'*\') if rate: return RequestRate(rate.requests, rate.seconds) return None def get_site_maps(self): return self.parser.site_maps()"},{"question":"# Advanced PyTorch Coding Challenge: Attention Masks Background In attention mechanisms, masks are used to control which positions can be attended to, allowing you to build more complex and flexible models. PyTorch provides various utilities to create and manipulate these masks within the `torch.nn.attention.flex_attention` module. Task You need to implement a function that utilizes the masking utilities provided by `torch.nn.attention.flex_attention` to perform a custom attention calculation. Specifically, you will: 1. Create a block mask. 2. Apply the block mask to an attention mechanism. 3. Perform attention scoring based on the masked inputs. Function Signature ```python def custom_attention(input_tensor: torch.Tensor, block_size: int) -> torch.Tensor: Apply a custom attention mechanism using block masks to the input tensor. Args: input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, embedding_dim). block_size (int): The size of the blocks to be masked. Returns: torch.Tensor: The output tensor after applying the custom attention mechanism. ``` Input - `input_tensor`: A tensor of shape `(batch_size, seq_length, embedding_dim)` representing a batch of sequences with their respective embeddings. - `block_size`: An integer representing the size of the blocks to be masked. Output - The function should return a tensor of the same shape as `input_tensor` that contains the results of the custom attention mechanism. Constraints - You must use the `create_block_mask` function to create the mask. - The attention scores should only consider the non-masked parts of the input tensor. - The function should handle any batch size and sequence length. Example ```python import torch from torch.nn.attention.flex_attention import create_block_mask # Example input input_tensor = torch.randn(2, 10, 64) # (batch_size, seq_length, embedding_dim) block_size = 3 # Expected output output_tensor = custom_attention(input_tensor, block_size) ``` Implementation Notes - The `create_block_mask` function returns a mask that you need to apply to the attention scores. - You may need to apply the mask to the attention scores before performing the softmax operation. - Ensure to handle the dimensions correctly to accommodate the batching of inputs. Good luck!","solution":"import torch from torch.nn.functional import softmax def create_block_mask(seq_length, block_size): mask = torch.zeros(seq_length, seq_length, dtype=torch.bool) for i in range(0, seq_length, block_size): mask[i:i+block_size, i:i+block_size] = 1 return mask def custom_attention(input_tensor: torch.Tensor, block_size: int) -> torch.Tensor: Apply a custom attention mechanism using block masks to the input tensor. Args: input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, embedding_dim). block_size (int): The size of the blocks to be masked. Returns: torch.Tensor: The output tensor after applying the custom attention mechanism. batch_size, seq_length, embedding_dim = input_tensor.shape block_mask = create_block_mask(seq_length, block_size).to(input_tensor.device) # Calculate attention scores attention_scores = torch.bmm(input_tensor, input_tensor.transpose(1, 2)) / embedding_dim**0.5 # Apply mask to the attention scores attention_scores = attention_scores.masked_fill(~block_mask, float(\'-inf\')) # Apply softmax to get attention weights attention_weights = softmax(attention_scores, dim=-1) # Get the context vector context = torch.bmm(attention_weights, input_tensor) return context"},{"question":"# Assessment Question **Objective:** Demonstrate your understanding of XML data handling using the `xml.dom.minidom` package. **Problem Statement:** You are provided with an XML file named `books.xml` that contains information about a collection of books. Each `<book>` element contains the following child elements: `<title>`, `<author>`, `<year>`, `<price>`. Your task is to perform several operations on this XML data using the `xml.dom.minidom` package. Tasks: 1. **Parse the XML file**: Write a function `parse_xml(file_path)` that takes the path to the XML file and returns the parsed DOM document. 2. **Extract Information**: Write a function `extract_books_info(document)` that takes the DOM document returned by `parse_xml` and returns a list of dictionaries, each representing a book with keys: `title`, `author`, `year`, and `price`. 3. **Add a New Book**: Write a function `add_book(document, new_book)` that takes the DOM document and a dictionary representing a new book (with keys: `title`, `author`, `year`, and `price`) and adds it to the DOM document. 4. **Update Book Price**: Write a function `update_price(document, title, new_price)` that takes the DOM document, a book title, and a new price. Update the price of the book with the given title. 5. **Write Back to File**: Write a function `write_to_file(document, output_file_path)` that takes the DOM document and an output file path, and writes the modified DOM back to a new XML file. Input & Output Formats: - The XML file `books.xml` is structured as follows: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2005</year> <price>49.99</price> </book> ... </library> ``` - `parse_xml(file_path)`: - Input: `file_path` (str) â€” path to `books.xml`. - Output: (xml.dom.minidom.Document) â€” Parsed DOM document. - `extract_books_info(document)`: - Input: `document` (xml.dom.minidom.Document) â€” Parsed DOM document. - Output: List of dictionaries representing books. - `add_book(document, new_book)`: - Input: - `document` (xml.dom.minidom.Document) â€” Parsed DOM document. - `new_book` (dict) â€” Dictionary with keys: `title`, `author`, `year`, and `price`. - Output: None (The function modifies the DOM document in-place). - `update_price(document, title, new_price)`: - Input: - `document` (xml.dom.minidom.Document) â€” Parsed DOM document. - `title` (str) â€” The title of the book to update. - `new_price` (str) â€” The new price of the book. - Output: None (The function modifies the DOM document in-place). - `write_to_file(document, output_file_path)`: - Input: - `document` (xml.dom.minidom.Document) â€” Modified DOM document. - `output_file_path` (str) â€” Path to the output XML file. - Output: None (The function writes the modified DOM to a file). **Constraints:** - Assume the `books.xml` file and its structure as given above. - If a book with the title to update does not exist, the `update_price` function should raise a `ValueError`. - All prices are strings representing valid decimal values. **Example Usage:** ```python document = parse_xml(\'books.xml\') # Extract and print information about all books books_info = extract_books_info(document) for book in books_info: print(book) # Add a new book to the collection new_book = { \'title\': \'New Book Title\', \'author\': \'New Author\', \'year\': \'2023\', \'price\': \'39.99\' } add_book(document, new_book) # Update the price of an existing book update_price(document, \'Book Title 1\', \'34.99\') # Write the updated document back to a new file write_to_file(document, \'updated_books.xml\') ``` Your task is to implement the functions `parse_xml`, `extract_books_info`, `add_book`, `update_price`, and `write_to_file` according to the specifications provided.","solution":"from xml.dom import minidom def parse_xml(file_path): Parses the given XML file and returns the parsed DOM document. :param file_path: Path to the XML file. :return: Parsed DOM document. return minidom.parse(file_path) def extract_books_info(document): Extracts books information from the DOM document. :param document: Parsed DOM document. :return: List of dictionaries representing books. books = [] book_elements = document.getElementsByTagName(\'book\') for book_element in book_elements: book_info = { \'title\': book_element.getElementsByTagName(\'title\')[0].childNodes[0].data, \'author\': book_element.getElementsByTagName(\'author\')[0].childNodes[0].data, \'year\': book_element.getElementsByTagName(\'year\')[0].childNodes[0].data, \'price\': book_element.getElementsByTagName(\'price\')[0].childNodes[0].data, } books.append(book_info) return books def add_book(document, new_book): Adds a new book to the DOM document. :param document: Parsed DOM document. :param new_book: Dictionary representing the new book. library = document.getElementsByTagName(\'library\')[0] # Create new book element and its children book_element = document.createElement(\'book\') title_element = document.createElement(\'title\') title_text = document.createTextNode(new_book[\'title\']) title_element.appendChild(title_text) author_element = document.createElement(\'author\') author_text = document.createTextNode(new_book[\'author\']) author_element.appendChild(author_text) year_element = document.createElement(\'year\') year_text = document.createTextNode(new_book[\'year\']) year_element.appendChild(year_text) price_element = document.createElement(\'price\') price_text = document.createTextNode(new_book[\'price\']) price_element.appendChild(price_text) # Append all child elements to the book element book_element.appendChild(title_element) book_element.appendChild(author_element) book_element.appendChild(year_element) book_element.appendChild(price_element) # Append the new book element to the library library.appendChild(book_element) def update_price(document, title, new_price): Updates the price of a book with the given title in the DOM document. :param document: Parsed DOM document. :param title: Title of the book to update. :param new_price: New price of the book. :raises ValueError: If no book with the given title is found. book_elements = document.getElementsByTagName(\'book\') for book_element in book_elements: title_element = book_element.getElementsByTagName(\'title\')[0] if title_element.childNodes[0].data == title: price_element = book_element.getElementsByTagName(\'price\')[0] price_element.childNodes[0].data = new_price return raise ValueError(f\\"No book with the title \'{title}\' found.\\") def write_to_file(document, output_file_path): Writes the modified DOM document to a new XML file. :param document: Modified DOM document. :param output_file_path: Path to the output XML file. with open(output_file_path, \'w\') as file: document.writexml(file, addindent=\' \', newl=\'n\')"},{"question":"# Question: Advanced List Manipulations **Objective**: Write a function `process_data(input_list)` that performs several operations on the given list of integers. **Function Signature**: ```python def process_data(input_list: list) -> list: pass ``` **Parameters**: - `input_list (list)`: A list of integers. **Returns**: - `list`: A new processed list as per the following steps. **Steps to follow**: 1. **Remove Duplicates**: Remove all duplicate entries from the list while maintaining the original order of their first occurrence. 2. **Square List**: Create a new list containing the squares of the numbers in the filtered list. 3. **Sort**: Sort the squared list in descending order. 4. **Filter Evens**: Further filter this list to include only the even numbers. **Constraints**: - The integers in the input list will be in the range [-1000, 1000]. - The length of the input list will be at most 1000. **Example**: ```python input_list = [4, 5, 6, 4, 7, 6, 8, 5, 9, 2, 0] print(process_data(input_list)) # Output: [64, 36, 16, 4, 0] ``` **Explanation**: 1. The list without duplicates: `[4, 5, 6, 7, 8, 9, 2, 0]`. 2. Squaring the list: `[16, 25, 36, 49, 64, 81, 4, 0]`. 3. Sorted in descending order: `[81, 64, 49, 36, 25, 16, 4, 0]`. 4. Filtering only even numbers: `[64, 36, 16, 4, 0]`. Implement the function `process_data` that performs the above operations effectively. ```python def process_data(input_list: list) -> list: # Step 1: Remove duplicates while maintaining order seen = set() filtered_list = [x for x in input_list if x not in seen and (seen.add(x) is None)] # Step 2: Create a list of squares of the numbers squared_list = [x**2 for x in filtered_list] # Step 3: Sort the squared list in descending order squared_list.sort(reverse=True) # Step 4: Filter the list to get even numbers only even_squared_list = [x for x in squared_list if x % 2 == 0] return even_squared_list # Example usage input_list = [4, 5, 6, 4, 7, 6, 8, 5, 9, 2, 0] print(process_data(input_list)) # Output: [64, 36, 16, 4, 0] ``` This question assesses the understanding of list comprehensions, set operations to eliminate duplicates, sorting lists, and list filtering in Python.","solution":"def process_data(input_list: list) -> list: Processes the input list by removing duplicates, squaring the elements, sorting them in descending order, and then filtering to include only even numbers. # Step 1: Remove duplicates while maintaining order seen = set() filtered_list = [x for x in input_list if x not in seen and not seen.add(x)] # Step 2: Create a list of squares of the numbers squared_list = [x**2 for x in filtered_list] # Step 3: Sort the squared list in descending order squared_list.sort(reverse=True) # Step 4: Filter the list to get even numbers only even_squared_list = [x for x in squared_list if x % 2 == 0] return even_squared_list"},{"question":"# Clustering Customer Data You are tasked with clustering customer data based on their purchasing behavior using scikit-learn. This will help in segmenting customers into different groups so that marketing strategies can be optimized. Input You will be provided with a dataset (`customers.csv`) which includes the following columns: - `CustomerID`: Unique identifier for the customer - `Age`: Age of the customer - `Annual Income (k)`: Annual income of the customer in thousand dollars - `Spending Score (1-100)`: A score assigned by the store based on customer behavior and spending nature Task 1. **Preprocess the Data**: - Handle any missing values by imputing with the mean of the respective column. - Normalize the data so that each feature has a mean of 0 and standard deviation of 1. 2. **Clustering**: - Implement k-means clustering on the processed data to segment the customers into distinct groups. - Determine the optimal number of clusters using the elbow method. 3. **Visualization**: - Visualize the clusters using a 2D scatter plot. Use the first two principal components obtained from PCA for plotting. Function Signature ```python def cluster_customers(file_path: str): Function to read customer data, preprocess it, perform clustering, and visualize the results. Parameters: file_path (str): Path to the \'customers.csv\' file. Returns: None: The function should plot the cluster visualization. pass ``` Constraints - You are allowed to use only the following scikit-learn modules: `cluster`, `preprocessing`, `decomposition`, `metrics`. - The function should handle data preprocessing, clustering, and visualization within a single pipeline. Example ```python cluster_customers(\\"path/to/customers.csv\\") ``` The output should be a 2D scatter plot showing the clustered customer segments.","solution":"import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.decomposition import PCA from scipy.spatial.distance import cdist import numpy as np def cluster_customers(file_path: str): Function to read customer data, preprocess it, perform clustering, and visualize the results. Parameters: file_path (str): Path to the \'customers.csv\' file. Returns: None: The function should plot the cluster visualization. # Read the data data = pd.read_csv(file_path) # Handle missing values data.fillna(data.mean(), inplace=True) # Selecting relevant columns for clustering features = data[[\'Age\', \'Annual Income (k)\', \'Spending Score (1-100)\']] # Normalizing the data scaler = StandardScaler() scaled_features = scaler.fit_transform(features) # Determine the optimal number of clusters using the elbow method distortions = [] K = range(1, 11) for k in K: kmeans = KMeans(n_clusters=k, random_state=42) kmeans.fit(scaled_features) distortions.append(sum(np.min(cdist(scaled_features, kmeans.cluster_centers_, \'euclidean\'), axis=1)) / scaled_features.shape[0]) # Plot the elbow curve plt.figure(figsize=(8, 6)) plt.plot(K, distortions, \'bx-\') plt.xlabel(\'k\') plt.ylabel(\'Distortion\') plt.title(\'The Elbow Method showing the optimal k\') plt.show() # From the elbow curve, we choose an appropriate number of clusters, let\'s choose k=5 for this example optimal_k = 5 kmeans = KMeans(n_clusters=optimal_k, random_state=42) kmeans.fit(scaled_features) labels = kmeans.labels_ # Applying PCA for visualization pca = PCA(n_components=2) principal_components = pca.fit_transform(scaled_features) # Visualizing the clusters plt.figure(figsize=(8, 6)) plt.scatter(principal_components[:, 0], principal_components[:, 1], c=labels, cmap=\'viridis\', alpha=0.6) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'Customer Clusters\') plt.colorbar() plt.show()"},{"question":"You are asked to implement custom pickling behavior for a nested class structure using the `copyreg` module. This will test your understanding of Python\'s serialization mechanism and customization using `copyreg`. # Question Consider the following class structures: ```python class A: def __init__(self, x, y): self.x = x self.y = y class B: def __init__(self, a, b): self.a = a # Instance of class A self.b = b # Instance of class A ``` Your task is to: 1. Write a custom pickling function for each class (`A` and `B`). 2. Register these pickling functions using the `copyreg` module. 3. Demonstrate the custom pickling and unpickling process in a sample code snippet. # Requirements 1. **Class A Pickling Function**: The function should return a tuple containing the class constructor and a tuple with the values of `x` and `y`. 2. **Class B Pickling Function**: The function should return a tuple containing the class constructor and a tuple with pickled values of both `a` and `b`. # Input and Output - **Input:** No direct input from the user. Your script should demonstrate the process using predefined instances. - **Output:** The script should print log messages indicating the pickling process and the final restored object should be equivalent to the original. # Constraints - Ensure that the custom pickling function correctly handles nested structures. - Use the `copyreg` module to register these functions. # Example ```python import copyreg import pickle class A: def __init__(self, x, y): self.x = x self.y = y class B: def __init__(self, a, b): self.a = a self.b = b def pickle_a(a): print(\\"Pickling A instance...\\") return A, (a.x, a.y) def pickle_b(b): print(\\"Pickling B instance...\\") a1 = pickle_a(b.a) a2 = pickle_a(b.b) return B, (a1[1], a2[1]) # Registering the pickling functions copyreg.pickle(A, pickle_a) copyreg.pickle(B, pickle_b) # Demonstrating the process a1 = A(1, 2) a2 = A(3, 4) b = B(a1, a2) # Pickling p = pickle.dumps(b) print(\\"Pickled data:\\", p) # Unpickling restored_b = pickle.loads(p) print(\\"Restored B object:\\", restored_b) print(\\"restored_b.a.x, restored_b.a.y:\\", restored_b.a.x, restored_b.a.y) print(\\"restored_b.b.x, restored_b.b.y:\\", restored_b.b.x, restored_b.b.y) ``` This example demonstrates the registration of custom pickling functions and the process of pickling and unpickling complex nested objects.","solution":"import copyreg import pickle class A: def __init__(self, x, y): self.x = x self.y = y class B: def __init__(self, a, b): self.a = a # Instance of class A self.b = b # Instance of class A def pickle_a(a): return A, (a.x, a.y) def unpickle_a(x, y): return A(x, y) def pickle_b(b): return B, (b.a, b.b) def unpickle_b(a, b): return B(a, b) # Register pickling functions with copyreg copyreg.pickle(A, pickle_a, unpickle_a) copyreg.pickle(B, pickle_b, unpickle_b) # Demonstrating the process a1 = A(1, 2) a2 = A(3, 4) b = B(a1, a2) # Pickling p = pickle.dumps(b) print(\\"Pickled data:\\", p) # Unpickling restored_b = pickle.loads(p) print(\\"Restored B object:\\", restored_b) print(\\"restored_b.a.x, restored_b.a.y:\\", restored_b.a.x, restored_b.a.y) print(\\"restored_b.b.x, restored_b.b.y:\\", restored_b.b.x, restored_b.b.y)"},{"question":"# Advanced Dataclass Usage in Python Objective: Demonstrate your understanding of Python\'s dataclass module by implementing a class for managing financial transactions, utilizing advanced features of dataclasses. Problem Statement: You are tasked with creating a Python class to represent a bank account using the `dataclasses` module. The class should encapsulate the attributes of a bank account and provide functionalities to handle transactions securely. Requirements: 1. **Class Definition**: Define a `BankAccount` class using the `@dataclass` decorator. 2. **Attributes**: - `account_number`: a string representing the account number. - `holder_name`: a string representing the account holder\'s name. - `balance`: a float representing the current balance (default is 0.0). - `transactions`: a list of transaction records with each record being a tuple `(action, amount)` (default is an empty list, use `default_factory`). - `MAX_TRANSACTIONS`: a class attribute (constant) representing the maximum number of transactions allowed (default is 100). 3. **Constraints**: - The balance must never be negative. Raise a `ValueError` if an action would result in a negative balance. - Frozen data class, to make sure the account attributes are immutable except for transactions. 4. **Methods**: - `__post_init__`: Ensure the balance is non-negative post initialization. - `deposit(amount: float)`: Add a deposit transaction and update the balance. Raise a `ValueError` if `amount` is non-positive. - `withdraw(amount: float)`: Add a withdrawal transaction and update the balance. Raise a `ValueError` if `amount` is non-positive or if the withdrawal would result in a negative balance. - `last_transaction() -> tuple`: Return the last transaction record. If there are no transactions, return `None`. 5. **Usage of Helper Functions**: - `asdict()`: Convert the `BankAccount` instance to a dictionary representation. - `astuple()`: Convert the `BankAccount` instance to a tuple representation. Example: ```python from dataclasses import dataclass, field, asdict, astuple, FrozenInstanceError @dataclass(frozen=True) class BankAccount: account_number: str holder_name: str balance: float = field(default=0.0) transactions: list = field(default_factory=list) MAX_TRANSACTIONS: int = 100 def __post_init__(self): if self.balance < 0: raise ValueError(\\"Initial balance cannot be negative\\") def deposit(self, amount: float): if amount <= 0: raise ValueError(\\"Deposit amount must be positive\\") self.transactions.append((\\"deposit\\", amount)) object.__setattr__(self, \'balance\', self.balance + amount) def withdraw(self, amount: float): if amount <= 0: raise ValueError(\\"Withdrawal amount must be positive\\") if self.balance - amount < 0: raise ValueError(\\"Insufficient funds\\") self.transactions.append((\\"withdraw\\", amount)) object.__setattr__(self, \'balance\', self.balance - amount) def last_transaction(self) -> tuple: if not self.transactions: return None return self.transactions[-1] # Example Usage: # Creating a new account account = BankAccount(account_number=\\"123ABC\\", holder_name=\\"John Doe\\", balance=1000.0) print(asdict(account)) print(astuple(account)) # Performing transactions account.deposit(500) account.withdraw(300) # Fetching the last transaction print(account.last_transaction()) ``` Testing: - Ensure all functionalities work as expected including the detection of invalid operations and adherence to immutability (except for transactions). - Validate the balance cannot be set directly due to frozen nature of the class.","solution":"from dataclasses import dataclass, field, asdict, astuple, FrozenInstanceError @dataclass(frozen=True) class BankAccount: account_number: str holder_name: str balance: float = field(default=0.0) transactions: list = field(default_factory=list) MAX_TRANSACTIONS: int = 100 def __post_init__(self): if self.balance < 0: raise ValueError(\\"Initial balance cannot be negative\\") def deposit(self, amount: float): if amount <= 0: raise ValueError(\\"Deposit amount must be positive\\") updated_transactions = self.transactions + [(\\"deposit\\", amount)] object.__setattr__(self, \'balance\', self.balance + amount) object.__setattr__(self, \'transactions\', updated_transactions) def withdraw(self, amount: float): if amount <= 0: raise ValueError(\\"Withdrawal amount must be positive\\") if self.balance - amount < 0: raise ValueError(\\"Insufficient funds\\") updated_transactions = self.transactions + [(\\"withdraw\\", amount)] object.__setattr__(self, \'balance\', self.balance - amount) object.__setattr__(self, \'transactions\', updated_transactions) def last_transaction(self) -> tuple: if not self.transactions: return None return self.transactions[-1]"},{"question":"# Task Scheduling with Dependencies **Objective:** Write a function `task_scheduler` that takes in a list of tasks and dependencies and returns a valid order of tasks. If a cyclic dependency is detected, it should return `\\"CycleError\\"`. **Function Signature:** ```python def task_scheduler(tasks: List[str], dependencies: List[Tuple[str, str]]) -> Union[List[str], str]: ``` **Input:** - `tasks`: A list of strings representing the tasks to be performed. - `dependencies`: A list of tuples where each tuple `(a, b)` denotes that task `b` depends on task `a` (i.e., task `a` must be performed before task `b`). **Output:** - A list of strings representing a valid order of tasks. If a cycle is detected, return the string `\\"CycleError\\"`. **Constraints:** - The graph formed by the tasks and dependencies is guaranteed to be directed. - Tasks and dependencies form a directed acyclic graph (DAG) unless there are cycles, in which case the function should return `\\"CycleError\\"`. - The order of tasks in the output list may vary as long as it is a valid topological order. **Example:** ```python tasks = [\\"A\\", \\"B\\", \\"C\\", \\"D\\"] dependencies = [(\\"A\\", \\"B\\"), (\\"A\\", \\"C\\"), (\\"B\\", \\"D\\"), (\\"C\\", \\"D\\")] result = task_scheduler(tasks, dependencies) # Possible output: [\\"A\\", \\"B\\", \\"C\\", \\"D\\"] or [\\"A\\", \\"C\\", \\"B\\", \\"D\\"] ``` **Note:** Use the provided `graphlib.TopologicalSorter` class to implement the function. **Implementation Notes:** 1. Create an instance of `TopologicalSorter`. 2. Add nodes and their dependencies using the `add` method. 3. Prepare the graph for sorting using the `prepare` method. 4. Check for cycles and handle accordingly. 5. Get the ordered tasks using the `static_order` method. **Hint:** - Use try-except blocks to handle `CycleError`.","solution":"from typing import List, Tuple, Union from graphlib import TopologicalSorter, CycleError def task_scheduler(tasks: List[str], dependencies: List[Tuple[str, str]]) -> Union[List[str], str]: ts = TopologicalSorter() for task in tasks: ts.add(task) for a, b in dependencies: ts.add(b, a) try: order = list(ts.static_order()) return order except CycleError: return \\"CycleError\\""},{"question":"Implement and Evaluate Ridge Regression Objective To assess your understanding of implementing and evaluating Ridge Regression using scikit-learn. Problem Statement You are provided with two datasets, `X_train.csv` and `y_train.csv`, consisting of training features and target values, respectively, and `X_test.csv` for testing features. Your task is to implement and evaluate Ridge Regression using these datasets. Requirements 1. **Load the Data:** - Load `X_train.csv` and `y_train.csv` for training. - Load `X_test.csv` for testing. 2. **Implement Ridge Regression:** - Initialize a `Ridge` regression model with `alpha=1.0`. 3. **Fit the Model:** - Fit the model on the training data (`X_train`, `y_train`). 4. **Evaluate the Model:** - Predict target values for the test data (`X_test`). - Calculate the Mean Squared Error (MSE) on the training data. 5. **Output:** - Print the coefficients and intercept of the fitted model. - Print the Mean Squared Error (MSE) on the training data. - Save the predictions on the test data to a CSV file `predictions.csv` with a single column \\"Predicted\\". Input and Output Formats **Input:** - `X_train.csv`: CSV file containing training features. - `y_train.csv`: CSV file containing training target values. - `X_test.csv`: CSV file containing testing features. **Output:** - Printed coefficients and intercept of the fitted model. - Printed Mean Squared Error (MSE) on the training data. - `predictions.csv`: CSV file containing predicted values for the test data. Constraints - Use `alpha=1.0` for the Ridge Regression model. Performance Requirements - Efficiently load and process the data. - Ensure the correctness of the implemented Ridge Regression model. ```python # Your Solution Code: import pandas as pd from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error # Load the data X_train = pd.read_csv(\'X_train.csv\') y_train = pd.read_csv(\'y_train.csv\') X_test = pd.read_csv(\'X_test.csv\') # Ensure y_train is in the correct format y_train = y_train.values.ravel() # Initialize Ridge Regression ridge = Ridge(alpha=1.0) # Fit the model ridge.fit(X_train, y_train) # Predict on training data and test data y_train_pred = ridge.predict(X_train) y_test_pred = ridge.predict(X_test) # Calculate Mean Squared Error on training data mse_train = mean_squared_error(y_train, y_train_pred) # Print model coefficients and intercept print(\\"Coefficients: \\", ridge.coef_) print(\\"Intercept: \\", ridge.intercept_) print(\\"Mean Squared Error on Training Data: \\", mse_train) # Save predictions to CSV predictions_df = pd.DataFrame(y_test_pred, columns=[\\"Predicted\\"]) predictions_df.to_csv(\'predictions.csv\', index=False) ``` Ensure all the required libraries are installed and the input files are in the correct directory before running the solution code.","solution":"import pandas as pd from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error def ridge_regression_evaluate(X_train_path, y_train_path, X_test_path, predictions_path=\'predictions.csv\'): Loads data, trains Ridge regression model, evaluates it, and saves predictions. Args: X_train_path (str): Path to the X_train CSV file. y_train_path (str): Path to the y_train CSV file. X_test_path (str): Path to the X_test CSV file. predictions_path (str): Path to save the predictions CSV file. Returns: dict: Contains coefficients, intercept, and MSE of the fitted model. # Load the data X_train = pd.read_csv(X_train_path) y_train = pd.read_csv(y_train_path) X_test = pd.read_csv(X_test_path) # Ensure y_train is in the correct format y_train = y_train.values.ravel() # Initialize Ridge Regression with alpha=1.0 ridge = Ridge(alpha=1.0) # Fit the model ridge.fit(X_train, y_train) # Predict on training data and test data y_train_pred = ridge.predict(X_train) y_test_pred = ridge.predict(X_test) # Calculate Mean Squared Error on training data mse_train = mean_squared_error(y_train, y_train_pred) # Save predictions to CSV predictions_df = pd.DataFrame(y_test_pred, columns=[\\"Predicted\\"]) predictions_df.to_csv(predictions_path, index=False) # Return model details return { \\"coefficients\\": ridge.coef_, \\"intercept\\": ridge.intercept_, \\"mse_train\\": mse_train }"},{"question":"Python/C API and Reference Counting Objective Your task is to create a Python C extension module that offers a Python function for summing elements in a list of integers. This exercise will assess your understanding of handling Python objects, reference counting, and error detection using the Python/C API. Instructions 1. **Create a C extension module** with a function `sum_list(PyObject*)`. 2. **Implement the function** to accept a Python list of integers and return the sum of the integers. 3. **Ensure proper reference counting** to prevent memory leaks or segmentation faults. 4. **Handle possible exceptions** and ensure that useful error messages are propagated back to Python. Function Implementation Implement the following function in C: ```c #include <Python.h> static PyObject* sum_list(PyObject* self, PyObject* list) { Py_ssize_t i, n; long total = 0, value; PyObject* item; if (!PyList_Check(list)) { PyErr_SetString(PyExc_TypeError, \\"Argument must be a list\\"); return NULL; } n = PyList_Size(list); if (n < 0) return NULL; // PyList_Size() will set an error condition for (i = 0; i < n; i++) { item = PyList_GetItem(list, i); // Returns a borrowed reference if (!PyLong_Check(item)) { PyErr_SetString(PyExc_ValueError, \\"List items must all be integers\\"); return NULL; } value = PyLong_AsLong(item); if (value == -1 && PyErr_Occurred()) { // Integer too large to fit in a long return NULL; } total += value; } return PyLong_FromLong(total); } ``` Additional Requirements 1. Define appropriate methods and module initialization functions to create a proper Python C extension module. 2. Compile the module and provide a Python script example demonstrating its usage and handling of edge cases (e.g., non-list input, non-integer elements). Input and Output Format - **Input**: A Python list of integers - **Output**: An integer representing the sum of the elements in the list Constraints - The list must only contain integers. - Handle cases where the input is not a list. - Handle cases where the list contains non-integer elements. Performance Requirements - The function should run efficiently with time complexity proportional to the length of the list, O(n). Example Usage in Python ```python import mymodule # Assuming the compiled module is named \'mymodule\' try: result = mymodule.sum_list([1, 2, 3, 4, 5]) print(\\"Sum:\\", result) except TypeError as e: print(\\"Error:\\", e) except ValueError as e: print(\\"Error:\\", e) ```","solution":"def sum_list(int_list): Returns the sum of the integers in the list. if not isinstance(int_list, list): raise TypeError(\\"Argument must be a list\\") total = 0 for item in int_list: if not isinstance(item, int): raise ValueError(\\"List items must all be integers\\") total += item return total"},{"question":"**Objective** Your task is to preprocess the target (dependent variable) for a multiclass classification problem using scikit-learnâ€™s `LabelBinarizer` and `LabelEncoder`. You must demonstrate the use of both classes by encoding given labels and then transforming a list of labels using both methods. **Problem Statement** You are given two lists of labels for different tasks: 1. **Task 1**: Multiclass labels 2. **Task 2**: Non-numerical labels Write two functions: 1. `encode_multiclass_labels(labels, transform_labels)`: This function should: - Fit a `LabelBinarizer` with `labels`. - Transform `transform_labels` using the fitted `LabelBinarizer`. - Return the transformed labels. 2. `encode_non_numerical_labels(labels, transform_labels)`: This function should: - Fit a `LabelEncoder` with `labels`. - Transform `transform_labels` using the fitted `LabelEncoder`. - Return the transformed labels. **Input Format** - `encode_multiclass_labels(labels, transform_labels)` - `labels` (List[int]): A list of integers representing multiclass labels. - `transform_labels` (List[int]): A list of integers to be transformed. - `encode_non_numerical_labels(labels, transform_labels)` - `labels` (List[str]): A list of strings representing non-numerical labels. - `transform_labels` (List[str]): A list of strings to be transformed. **Output Format** - `encode_multiclass_labels(labels, transform_labels) -> np.ndarray`: A numpy array of transformed labels. - `encode_non_numerical_labels(labels, transform_labels) -> np.ndarray`: A numpy array of transformed labels. **Constraints** - The elements of `labels` and `transform_labels` will be within the same domain (i.e., the elements of `transform_labels` exist within `labels`). - The input lists for both tasks are non-empty. **Example** **Example 1**: ```python # Task 1: Multiclass labels labels = [1, 2, 6, 4, 2] transform_labels = [1, 6] # Expected output: # array([[1, 0, 0, 0], # [0, 0, 0, 1]]) print(encode_multiclass_labels(labels, transform_labels)) ``` **Example 2**: ```python # Task 2: Non-numerical labels labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] transform_labels = [\\"tokyo\\", \\"paris\\"] # Expected output: # array([2, 1]) print(encode_non_numerical_labels(labels, transform_labels)) ``` **Performance Requirements** - The functions should be able to handle input lists up to length 100,000 efficiently.","solution":"from sklearn.preprocessing import LabelBinarizer, LabelEncoder import numpy as np def encode_multiclass_labels(labels, transform_labels): Encodes the given multiclass labels using LabelBinarizer and transforms the transform_labels accordingly. Parameters: labels (List[int]): A list of integers representing multiclass labels. transform_labels (List[int]): A list of integers to be transformed. Returns: np.ndarray: A numpy array of transformed labels. lb = LabelBinarizer() lb.fit(labels) return lb.transform(transform_labels) def encode_non_numerical_labels(labels, transform_labels): Encodes the given non-numerical labels using LabelEncoder and transforms the transform_labels accordingly. Parameters: labels (List[str]): A list of strings representing non-numerical labels. transform_labels (List[str]): A list of strings to be transformed. Returns: np.ndarray: A numpy array of transformed labels. le = LabelEncoder() le.fit(labels) return le.transform(transform_labels)"},{"question":"Objective: Demonstrate your understanding of Python 3.10\'s various specialized data types and standard library modules by solving the following problem using multiple modules effectively. Problem Statement: You are tasked to implement a function that processes and organizes event data from a list of event records. Each record contains details like event name, start date, duration, and a time zone. The function should group these events by their respective calendar weeks, and find the earliest and latest events in each week, accounting for different time zones. # Function Signature: ```python def organize_events(event_records: List[Dict[str, Union[str, int]]]) -> Dict[int, Dict[str, Union[str, Tuple[str, str]]]]: pass ``` # Input: - `event_records`: A list of dictionaries where each dictionary represents an event with the following keys: - `\'event_name\'`: string, the name of the event. - `\'start_date\'`: string, start date and time of the event in ISO 8601 format (e.g., \\"2023-10-04T08:30:00\\"). - `\'duration\'`: integer, duration of the event in minutes. - `\'time_zone\'`: string, the time zone of the event (e.g., \\"America/New_York\\"). # Output: - A dictionary where the keys are week numbers of the year (1-52), and the values are dictionaries with: - `\'events\'`: A list of event names scheduled in that week. - `\'earliest_event\'`: A tuple with the event name and its start time (in UTC) of the earliest event in the week. - `\'latest_event\'`: A tuple with the event name and its start time (in UTC) of the latest event in the week. # Constraints: 1. Use the `datetime` and `zoneinfo` modules to handle dates, times, and time zones. 2. Ensure that you handle events occurring across different time zones correctly. 3. Assume all input dates are within a single year for simplicity. 4. You can expect the list of event records to have at least one entry but not more than 1000 entries. # Example: ```python event_records = [ {\\"event_name\\": \\"Event1\\", \\"start_date\\": \\"2023-10-01T09:00:00\\", \\"duration\\": 120, \\"time_zone\\": \\"America/New_York\\"}, {\\"event_name\\": \\"Event2\\", \\"start_date\\": \\"2023-10-05T11:00:00\\", \\"duration\\": 60, \\"time_zone\\": \\"America/Los_Angeles\\"}, {\\"event_name\\": \\"Event3\\", \\"start_date\\": \\"2023-10-07T15:00:00\\", \\"duration\\": 90, \\"time_zone\\": \\"Europe/London\\"}, ] output = organize_events(event_records) print(output) # Expected Output: # { # 39: { # \'events\': [\'Event1\', \'Event2\'], # \'earliest_event\': (\'Event1\', \'2023-10-01T13:00:00Z\'), # \'latest_event\': (\'Event2\', \'2023-10-05T18:00:00Z\'), # }, # 40: { # \'events\': [\'Event3\'], # \'earliest_event\': (\'Event3\', \'2023-10-07T14:00:00Z\'), # \'latest_event\': (\'Event3\', \'2023-10-07T14:00:00Z\'), # } # } ``` # Notes: 1. Use the `datetime.datetime` module to parse and handle date/time operations and the `zoneinfo` module to handle time zone conversions. 2. You may use other utilities from the documentation to sort, organize, and filter events. Additional Requirements: - Aim for optimal performance and clarity in your solution. - Include error handling for edge cases, such as invalid date strings or time zones in the event records. - Write unit tests to validate your solution against multiple scenarios.","solution":"from datetime import datetime, timedelta from typing import List, Dict, Union, Tuple from zoneinfo import ZoneInfo import calendar def organize_events(event_records: List[Dict[str, Union[str, int]]]) -> Dict[int, Dict[str, Union[str, Tuple[str, str]]]]: weeks = {} for record in event_records: event_name = record[\'event_name\'] start_date = record[\'start_date\'] duration = record[\'duration\'] time_zone = record[\'time_zone\'] # Convert start date to datetime object try: start_datetime_local = datetime.fromisoformat(start_date).replace(tzinfo=ZoneInfo(time_zone)) except Exception as e: raise ValueError(f\\"Error parsing date or time zone: {e}\\") # Calculate the week number and convert to UTC start_datetime_utc = start_datetime_local.astimezone(ZoneInfo(\'UTC\')) week_number = start_datetime_utc.isocalendar()[1] if week_number not in weeks: weeks[week_number] = { \'events\': [], \'earliest_event\': None, \'latest_event\': None } weeks[week_number][\'events\'].append(event_name) # Determine if this is the earliest or latest event if (weeks[week_number][\'earliest_event\'] is None or start_datetime_utc < datetime.fromisoformat(weeks[week_number][\'earliest_event\'][1])): weeks[week_number][\'earliest_event\'] = (event_name, start_datetime_utc.isoformat()) if (weeks[week_number][\'latest_event\'] is None or start_datetime_utc > datetime.fromisoformat(weeks[week_number][\'latest_event\'][1])): weeks[week_number][\'latest_event\'] = (event_name, start_datetime_utc.isoformat()) return weeks # Example usage event_records = [ {\\"event_name\\": \\"Event1\\", \\"start_date\\": \\"2023-10-01T09:00:00\\", \\"duration\\": 120, \\"time_zone\\": \\"America/New_York\\"}, {\\"event_name\\": \\"Event2\\", \\"start_date\\": \\"2023-10-05T11:00:00\\", \\"duration\\": 60, \\"time_zone\\": \\"America/Los_Angeles\\"}, {\\"event_name\\": \\"Event3\\", \\"start_date\\": \\"2023-10-07T15:00:00\\", \\"duration\\": 90, \\"time_zone\\": \\"Europe/London\\"}, ] output = organize_events(event_records) print(output)"},{"question":"**Coding Assessment Question: PyTorch Hub Model Publishing and Loading** # Objective Demonstrate the ability to publish and load models using PyTorch Hub. # Problem Statement You are provided with a simple convolutional neural network (CNN) model defined using PyTorch\'s `torch.nn.Module`. Your task is to: 1. **Publish the model using `hubconf.py`:** - Define an entry point named `simple_cnn` in `hubconf.py`. - Ensure that the entry point can optionally load pre-trained weights. 2. **Load and run the model using `torch.hub`:** - Write a script that downloads and loads the model using `torch.hub`. - Verify the loaded model by passing a sample input tensor through it. # Model Definition Here\'s the code for the CNN model. You should not modify this code: ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self, num_classes=10): super(SimpleCNN, self).__init__() self.conv1 = nn.Conv2d(1, 32, kernel_size=3) self.conv2 = nn.Conv2d(32, 64, kernel_size=3) self.fc1 = nn.Linear(64 * 12 * 12, 128) self.fc2 = nn.Linear(128, num_classes) def forward(self, x): x = F.relu(self.conv1(x)) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2) x = torch.flatten(x, 1) x = F.relu(self.fc1(x)) x = self.fc2(x) return x ``` # Part 1: Publish the Model Create a file named `hubconf.py` and define the following entry point: ```python dependencies = [\\"torch\\", \\"torchvision\\"] def simple_cnn(pretrained=False, **kwargs): from simple_cnn import SimpleCNN model = SimpleCNN(**kwargs) if pretrained: # Assuming pre-trained weights are available at a given URL state_dict_url = \\"https://example.com/path_to_pretrained_weights.pth\\" state_dict = torch.hub.load_state_dict_from_url(state_dict_url, progress=True) model.load_state_dict(state_dict) return model ``` # Part 2: Load and Run the Model In a separate script, perform the following steps: 1. Use `torch.hub.load` to download and load the `simple_cnn` model from the repository. 2. Pass a random [1x1x28x28] input tensor through the loaded model and print the output shape. ```python import torch # Load the model from the hub model = torch.hub.load(\'your_github_username/repository_name\', \'simple_cnn\', pretrained=False, source=\'local\') # Create a random input tensor input_tensor = torch.randn(1, 1, 28, 28) # Pass the input through the model and print the output shape output = model(input_tensor) print(\\"Output shape:\\", output.shape) ``` # Constraints - Assume the pre-trained weights file is correctly hosted at the provided URL. - Ensure that the model works as expected (no runtime issues) when loaded and run. # Submission Submit the following files: 1. `hubconf.py` containing the entry point definition. 2. The script used to load and run the model. **Note:** Replace `your_github_username/repository_name` with the actual path to your GitHub repository in the loading script.","solution":"# hubconf.py dependencies = [\\"torch\\", \\"torchvision\\"] def simple_cnn(pretrained=False, **kwargs): from simple_cnn import SimpleCNN model = SimpleCNN(**kwargs) if pretrained: # Assuming pre-trained weights are available at a given URL state_dict_url = \\"https://example.com/path_to_pretrained_weights.pth\\" state_dict = torch.hub.load_state_dict_from_url(state_dict_url, progress=True) model.load_state_dict(state_dict) return model"},{"question":"Objective: Implement and optimize a function using scikit-learn, Numpy, and Scipy. You will write an efficient algorithm to solve a classification problem using the k-Nearest Neighbors (k-NN) algorithm and profile the implementation to identify and optimize bottlenecks. Task: 1. Implement a k-NN classifier in Python, without using scikit-learn\'s built-in k-NN classes. 2. Optimize the classifier to improve its performance. 3. Profile the implementation to show the before and after performance metrics. Steps: 1. **Implement k-NN Classifier:** - Write a function `knn_classifier(X_train, y_train, X_test, k)` that: - Inputs: - `X_train`: A Numpy array of shape (N_train, D) where N_train is the number of training samples and D is the number of features. - `y_train`: A Numpy array of shape (N_train,) containing the labels for the training data. - `X_test`: A Numpy array of shape (N_test, D) where N_test is the number of test samples. - `k`: An integer representing the number of nearest neighbors to consider. - Outputs: - `y_pred`: A Numpy array of shape (N_test,) containing the predicted labels for the test data. - The function should use Euclidean distance to determine the nearest neighbors and perform classification by majority vote. 2. **Profile the Implementation:** - Use IPython\'s `%timeit` and `%prun` to profile the `knn_classifier` function on a sample dataset (you can generate random data or use `load_digits` from `sklearn.datasets`). - Identify any bottlenecks in the implementation. 3. **Optimize the Implementation:** - Rewrite the identified bottlenecks using efficient Numpy operations or Cython. - Compare the optimized implementation with the original version, showing both performance and accuracy results. 4. **Validation:** - Ensure the optimized implementation produces the same results as the initial implementation. - Discuss the observed improvements and any potential trade-offs. Constraints: - The implementation should be vectorized where possible. - Use Numpy and Scipy for mathematical operations. - Do not use scikit-learnâ€™s k-NN class directly. - Performance and memory usage should be taken into consideration. Example Usage: ```python import numpy as np from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Load dataset digits = load_digits() X, y = digits.data, digits.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize parameters k = 5 # Original k-NN classification y_pred = knn_classifier(X_train, y_train, X_test, k) print(f\'Accuracy: {accuracy_score(y_test, y_pred)}\') # Profile original and optimized implementations # %timeit, %prun can be invoked in an IPython environment for profiling ``` Evaluation Criteria: - Correctness and completeness of the initial implementation. - Effectiveness of the optimization changes. - Quality of profiling results and analysis. - Consistency and accuracy of the output in both original and optimized implementations.","solution":"import numpy as np from scipy.spatial import distance from collections import Counter def knn_classifier(X_train, y_train, X_test, k): k-NN classifier. Parameters: - X_train: Numpy array of shape (N_train, D) - y_train: Numpy array of shape (N_train,) - X_test: Numpy array of shape (N_test, D) - k: number of nearest neighbors Returns: - y_pred: Numpy array of shape (N_test,) y_pred = [] for test_point in X_test: # Calculate all distances from the test point to training points distances = np.linalg.norm(X_train - test_point, axis=1) # Get indices of k nearest neighbors k_nearest_indices = distances.argsort()[:k] # Get the labels of the k nearest neighbors k_nearest_labels = y_train[k_nearest_indices] # Majority vote most_common_label = Counter(k_nearest_labels).most_common(1)[0][0] y_pred.append(most_common_label) return np.array(y_pred)"},{"question":"Objective: To assess your understanding of the `json` module in Python, particularly in extending the functionality of encoding and decoding JSON data. Problem Statement: You are required to create a custom encoder and decoder for JSON that can handle a special data type `ComplexNumber`. The `ComplexNumber` class represents complex numbers in a custom way and should be serializable to and from JSON using your custom encoder and decoder. Requirements: 1. Implement a `ComplexNumber` class that stores the real and imaginary parts of a complex number. 2. Create a custom JSON encoder, `ComplexNumberEncoder`, that can convert instances of `ComplexNumber` into JSON strings. 3. Create a custom JSON decoder, `ComplexNumberDecoder`, that can convert JSON strings back into `ComplexNumber` instances. 4. Use appropriate error handling to manage cases where JSON data is not correctly formatted. ComplexNumber Class: ```python class ComplexNumber: def __init__(self, real, imaginary): self.real = real self.imaginary = imaginary def __repr__(self): return f\\"ComplexNumber({self.real}, {self.imaginary})\\" ``` Custom JSON Encoder: - Extend `json.JSONEncoder` to create a `ComplexNumberEncoder` class. - Override the `default` method to handle `ComplexNumber` objects. Custom JSON Decoder: - Use `json.loads` with the `object_hook` parameter to create a `ComplexNumberDecoder` function. - The `object_hook` function should check if the JSON object represents a `ComplexNumber` and convert it accordingly. Input/Output: - **Input:** - An instance of `ComplexNumber`. - A JSON string representing a `ComplexNumber`. - **Output:** - A JSON string representing the `ComplexNumber`. - An instance of `ComplexNumber` created from the JSON string. Example: ```python # Example usage of ComplexNumber, ComplexNumberEncoder, and ComplexNumberDecoder # Creating a ComplexNumber instance cn = ComplexNumber(3, 4) print(cn) # Output: ComplexNumber(3, 4) # Serializing ComplexNumber to JSON json_str = json.dumps(cn, cls=ComplexNumberEncoder) print(json_str) # Output: {\\"__complex__\\": true, \\"real\\": 3, \\"imaginary\\": 4} # Deserializing JSON to ComplexNumber cn_decoded = json.loads(json_str, object_hook=ComplexNumberDecoder) print(cn_decoded) # Output: ComplexNumber(3, 4) ``` Constraints: 1. The `ComplexNumber` class should not be modified. 2. Ensure that the custom encoder and decoder handle errors gracefully. Performance Requirements: - The solution should efficiently handle serialization and deserialization of complex numbers. - The solution must handle edge cases and invalid JSON data appropriately, providing meaningful error messages. Solution Template: ```python import json class ComplexNumber: def __init__(self, real, imaginary): self.real = real self.imaginary = imaginary def __repr__(self): return f\\"ComplexNumber({self.real}, {self.imaginary})\\" class ComplexNumberEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, ComplexNumber): return { \\"__complex__\\": True, \\"real\\": obj.real, \\"imaginary\\": obj.imaginary, } return json.JSONEncoder.default(self, obj) def ComplexNumberDecoder(dct): if \\"__complex__\\" in dct: return ComplexNumber(dct[\\"real\\"], dct[\\"imaginary\\"]) return dct # Example usage: # cn = ComplexNumber(3, 4) # json_str = json.dumps(cn, cls=ComplexNumberEncoder) # print(json_str) # cn_decoded = json.loads(json_str, object_hook=ComplexNumberDecoder) # print(cn_decoded) ```","solution":"import json class ComplexNumber: def __init__(self, real, imaginary): self.real = real self.imaginary = imaginary def __repr__(self): return f\\"ComplexNumber({self.real}, {self.imaginary})\\" class ComplexNumberEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, ComplexNumber): return { \\"__complex__\\": True, \\"real\\": obj.real, \\"imaginary\\": obj.imaginary, } return json.JSONEncoder.default(self, obj) def ComplexNumberDecoder(dct): if \\"__complex__\\" in dct: return ComplexNumber(dct[\\"real\\"], dct[\\"imaginary\\"]) return dct"},{"question":"# Question: Data Persistence in Python with `pickle`, `shelve`, and `sqlite3` Objective To assess your understanding of data persistence in Python, you are required to demonstrate your competency in serializing objects, storing objects persistently, and working with SQLite databases. Tasks 1. **Pickle Serialization/Deserialization**: - Implement a function `serialize_object(obj: Any, filename: str) -> None` that takes a Python object and a filename, serializes the object using `pickle`, and saves it to a file. - Implement a function `deserialize_object(filename: str) -> Any` that takes a filename, reads the serialized object from the file, deserializes it using `pickle`, and returns the original Python object. 2. **Shelve Object Persistence**: - Implement a function `store_with_shelve(data: Dict[str, Any], filename: str) -> None` that takes a dictionary of objects and a filename, saves each item in the dictionary to a `shelve` database file. - Implement a function `retrieve_with_shelve(filename: str) -> Dict[str, Any]` that takes a filename, retrieves the stored objects from the `shelve` database, and returns them as a dictionary. 3. **SQLite Database Operations**: - Implement a function `create_and_populate_table(db_filename: str, table_name: str, columns: List[str], data: List[Tuple[Any, ...]]) -> None` that creates an SQLite database (if it doesn\'t exist), creates a table with the given name and columns, and populates the table with the provided data. - Implement a function `query_table(db_filename: str, table_name: str) -> List[Tuple[Any, ...]]` that queries all data from the specified table and returns it. Input/Output Formats 1. **serialize_object** - Input: A Python object `obj` and a string `filename`. - Output: None (The object is serialized to the specified file). 2. **deserialize_object** - Input: A string `filename`. - Output: The deserialized Python object. 3. **store_with_shelve** - Input: A dictionary `data` of the form `{key: value}` and a string `filename`. - Output: None (The dictionary items are stored in the specified `shelve` database file). 4. **retrieve_with_shelve** - Input: A string `filename`. - Output: A dictionary of objects retrieved from the `shelve` database. 5. **create_and_populate_table** - Input: A string `db_filename`, a string `table_name`, a list of column names `columns`, and a list of tuples `data` (where each tuple represents a row of data). - Output: None (The table is created and populated in the SQLite database). 6. **query_table** - Input: A string `db_filename` and a string `table_name`. - Output: A list of tuples representing the rows of data in the specified table. Constraints - Ensure the file paths provided for `pickle` and `shelve` operations are valid and accessible. - Handle potential errors, such as file read/write errors or database connection issues, gracefully. # Example ```python import pickle import shelve import sqlite3 from typing import Any, Dict, List, Tuple # Task 1: Pickle Serialization/Deserialization def serialize_object(obj: Any, filename: str) -> None: with open(filename, \'wb\') as f: pickle.dump(obj, f) def deserialize_object(filename: str) -> Any: with open(filename, \'rb\') as f: return pickle.load(f) # Task 2: Shelve Object Persistence def store_with_shelve(data: Dict[str, Any], filename: str) -> None: with shelve.open(filename) as db: for key, value in data.items(): db[key] = value def retrieve_with_shelve(filename: str) -> Dict[str, Any]: result = {} with shelve.open(filename) as db: for key in db.keys(): result[key] = db[key] return result # Task 3: SQLite Database Operations def create_and_populate_table(db_filename: str, table_name: str, columns: List[str], data: List[Tuple[Any, ...]]) -> None: conn = sqlite3.connect(db_filename) cursor = conn.cursor() cols = \', \'.join(f\'{col} TEXT\' for col in columns) cursor.execute(f\'CREATE TABLE IF NOT EXISTS {table_name} ({cols})\') placeholders = \', \'.join(\'?\' for _ in columns) cursor.executemany(f\'INSERT INTO {table_name} VALUES ({placeholders})\', data) conn.commit() conn.close() def query_table(db_filename: str, table_name: str) -> List[Tuple[Any, ...]]: conn = sqlite3.connect(db_filename) cursor = conn.cursor() cursor.execute(f\'SELECT * FROM {table_name}\') result = cursor.fetchall() conn.close() return result ```","solution":"import pickle import shelve import sqlite3 from typing import Any, Dict, List, Tuple # Task 1: Pickle Serialization/Deserialization def serialize_object(obj: Any, filename: str) -> None: with open(filename, \'wb\') as f: pickle.dump(obj, f) def deserialize_object(filename: str) -> Any: with open(filename, \'rb\') as f: return pickle.load(f) # Task 2: Shelve Object Persistence def store_with_shelve(data: Dict[str, Any], filename: str) -> None: with shelve.open(filename) as db: for key, value in data.items(): db[key] = value def retrieve_with_shelve(filename: str) -> Dict[str, Any]: result = {} with shelve.open(filename) as db: for key in db.keys(): result[key] = db[key] return result # Task 3: SQLite Database Operations def create_and_populate_table(db_filename: str, table_name: str, columns: List[str], data: List[Tuple[Any, ...]]) -> None: conn = sqlite3.connect(db_filename) cursor = conn.cursor() cols = \', \'.join(f\'{col} TEXT\' for col in columns) cursor.execute(f\'CREATE TABLE IF NOT EXISTS {table_name} ({cols})\') placeholders = \', \'.join(\'?\' for _ in columns) cursor.executemany(f\'INSERT INTO {table_name} VALUES ({placeholders})\', data) conn.commit() conn.close() def query_table(db_filename: str, table_name: str) -> List[Tuple[Any, ...]]: conn = sqlite3.connect(db_filename) cursor = conn.cursor() cursor.execute(f\'SELECT * FROM {table_name}\') result = cursor.fetchall() conn.close() return result"},{"question":"# Advanced Buffer Protocol Manipulation Background The buffer protocol allows Python objects to expose raw byte arrays to other objects. This allows for efficient memory manipulation, access, and data transfer, lightning-fast operations on large datasets, and interfacing with lower-level components. Task Implement a Python class `CustomBuffer` that mimics a simplified version of Python\'s buffer protocol handling, allowing initialization of a buffer and reading data from it. Ensure the following functionality is achieved: - Initialize the `CustomBuffer` instance with a given size, filling it with zeros. - Allow setting values at specific indices. - Allow retrieving values from specific indices. - Ensure that the class can handle out-of-bounds attempts gracefully by raising an appropriate exception. - Implement a method that returns a slice of the buffer as a list. - Ensure that changes made to the buffer directly affect the memory without creating a copy. Requirements - The buffer should be represented internally using a `bytearray`. - The buffer class should prevent invalid memory operations. - All operations should be efficiently handled. Constraints - The buffer size will not exceed 1024 bytes. - Values set in the buffer will be in the range [0, 255]. Performance Requirements - The methods to set and get values should operate in O(1) time complexity. - Slicing operations should be efficient and avoid unnecessary memory copies. Code Scaffold ```python class CustomBuffer: def __init__(self, size: int): Initialize the buffer with the specified size, ensuring all elements are set to zero. :param size: Size of the buffer # Your code here def set_value(self, index: int, value: int): Set the value at the specified index in the buffer. :param index: Index at which value is to be set :param value: Value to set (0-255) :raise: IndexError if index is out of bounds # Your code here def get_value(self, index: int) -> int: Get the value from the specified index in the buffer. :param index: Index from which value is to be fetched :return: Value at the specified index :raise: IndexError if index is out of bounds # Your code here def get_slice(self, start: int, end: int) -> list: Get a slice from the buffer as a list. :param start: Starting index of the slice :param end: Ending index of the slice :return: List of values in the specified range # Your code here # Example usage: # buffer = CustomBuffer(10) # buffer.set_value(0, 1) # buffer.set_value(1, 2) # print(buffer.get_value(0)) # Output: 1 # print(buffer.get_value(1)) # Output: 2 # print(buffer.get_slice(0, 2)) # Output: [1, 2] ``` Note - You should handle the buffer logic internally, efficiently managing the memory. - Do **not** use any external libraries to handle the buffer or memory directly. Evaluation Solutions will be evaluated based on: - Correct implementation of buffer management. - Efficient handling of set and get operations. - Correct handling of boundaries and exception raising. - Proper slice operations with correct output.","solution":"class CustomBuffer: def __init__(self, size: int): Initialize the buffer with the specified size, ensuring all elements are set to zero. :param size: Size of the buffer if size > 1024: raise ValueError(\\"Size cannot exceed 1024 bytes.\\") self.size = size self.buffer = bytearray(size) def set_value(self, index: int, value: int): Set the value at the specified index in the buffer. :param index: Index at which value is to be set :param value: Value to set (0-255) :raise: IndexError if index is out of bounds if not (0 <= index < self.size): raise IndexError(\\"Index out of bounds.\\") if not (0 <= value <= 255): raise ValueError(\\"Value must be in range 0-255.\\") self.buffer[index] = value def get_value(self, index: int) -> int: Get the value from the specified index in the buffer. :param index: Index from which value is to be fetched :return: Value at the specified index :raise: IndexError if index is out of bounds if not (0 <= index < self.size): raise IndexError(\\"Index out of bounds.\\") return self.buffer[index] def get_slice(self, start: int, end: int) -> list: Get a slice from the buffer as a list. :param start: Starting index of the slice :param end: Ending index of the slice :return: List of values in the specified range if not (0 <= start <= end <= self.size): raise IndexError(\\"Slice indices out of bounds.\\") return list(self.buffer[start:end])"},{"question":"In this assessment, your task is to implement a Python program that uses the `multiprocessing` module to simulate a simplified version of a concurrent web scraper. You will create multiple worker processes to fetch data from a list of URLs concurrently, process the fetched data, and store the results. Requirements 1. Create a pool of worker processes to fetch HTML content from the given URLs concurrently. 2. Implement a shared queue to communicate the fetched data between worker processes and the main process. 3. Use synchronization primitives to ensure thread-safe operations. 4. Implement proper error handling and logging. Function Signature ```python def concurrent_web_scraper(urls: list[str], num_workers: int) -> dict[str, str]: Fetch HTML content from a list of URLs concurrently. Args: urls (list[str]): List of URLs to fetch. num_workers (int): Number of worker processes to use. Returns: dict[str, str]: A dictionary where keys are URLs and values are the corresponding HTML content. pass ``` Constraints - You are required to use the `multiprocessing.Pool` for parallel execution. - Use `multiprocessing.Queue` for inter-process communication. - Ensure that the total number of concurrent worker processes does not exceed `num_workers`. - Handle potential errors such as failed HTTP requests or invalid URLs gracefully. - The URLs should be fetched concurrently and not sequentially. Example Usage ```python if __name__ == \\"__main__\\": urls = [ \\"https://example.com\\", \\"https://example.org\\", \\"https://example.net\\", # Add more URLs as needed ] num_workers = 4 results = concurrent_web_scraper(urls, num_workers) for url, content in results.items(): print(f\\"Content from {url}:n{content[:100]}...\\") # Print the first 100 characters for brevity ``` Hint You can use the `requests` library to fetch HTML content from URLs. Ensure to install it using `pip install requests`. **Good luck!**","solution":"import multiprocessing import requests from multiprocessing import Pool, Queue import logging def fetch_url(queue, url): Function that fetches the content of a URL and puts it into a queue. try: response = requests.get(url) response.raise_for_status() queue.put((url, response.text)) except requests.RequestException as e: logging.error(f\\"Error fetching {url}: {e}\\") queue.put((url, None)) def concurrent_web_scraper(urls, num_workers): Fetch HTML content from a list of URLs concurrently using multiprocessing. Args: urls (list[str]): List of URLs to fetch. num_workers (int): Number of worker processes to use. Returns: dict[str, str]: A dictionary where keys are URLs and values are the corresponding HTML content. manager = multiprocessing.Manager() queue = manager.Queue() pool = Pool(processes=num_workers) # Start fetching URLs concurrently for url in urls: pool.apply_async(fetch_url, (queue, url)) # Close the pool and wait for all processes to finish pool.close() pool.join() # Gather results from the queue results = {} while not queue.empty(): url, content = queue.get() if content: results[url] = content return results"},{"question":"**Coding Assessment Question: Analyzing and Summarizing Sales Data** **Objective:** Write a function that uses Python collections to analyze and summarize sales data from multiple sources. The function should aggregate data, count occurrences of specific items, and produce ordered results. **Function Signature:** ```python def analyze_sales_data(sales_records: list) -> dict: pass ``` **Input:** - `sales_records`: A list of tuples representing sales transactions. Each tuple contains: - `source`: a string representing the source of the sales data (e.g., \'Store\', \'Online\', \'Wholesale\'). - `item`: a string representing the name of the item sold. - `quantity`: an integer representing the number of items sold. - `unit_price`: a float representing the price per item. - Example: `[(\'Store\', \'Apple\', 50, 0.5), (\'Online\', \'Apple\', 30, 0.55), (\'Wholesale\', \'Apple\', 100, 0.45)]` **Output:** - A dictionary with the following structure: - `total_sales`: An `OrderedDict` displaying the total quantity of each item sold across all sources, ordered by the total quantity sold in descending order. - `item_occurrences`: A `Counter` object tallying the number of transactions for each item. - `source_breakdown`: A `ChainMap` object combining dictionaries of total sales for each source, allowing quick lookup of how much of an item was sold by each source. **Constraints:** - The function should handle an empty list by returning an empty dictionary. - Sales from different sources should be aggregated correctly. - The summary must be ordered by the total quantities sold. **Performance Requirements:** - The solution should efficiently handle up to 10,000 sales records. **Example Usage:** ```python sales_records = [ (\'Store\', \'Apple\', 50, 0.5), (\'Online\', \'Apple\', 30, 0.55), (\'Wholesale\', \'Apple\', 100, 0.45), (\'Store\', \'Banana\', 80, 0.3), (\'Online\', \'Banana\', 20, 0.35), (\'Store\', \'Orange\', 40, 0.6), (\'Online\', \'Orange\', 60, 0.65), ] result = analyze_sales_data(sales_records) ``` **Example Output:** ```python { \'total_sales\': OrderedDict([(\'Apple\', 180), (\'Banana\', 100), (\'Orange\', 100)]), \'item_occurrences\': Counter({\'Apple\': 3, \'Banana\': 2, \'Orange\': 2}), \'source_breakdown\': ChainMap({\'Apple\': 100, \'Banana\': 80, \'Orange\': 40}, {\'Apple\': 30, \'Banana\': 20, \'Orange\': 60}, {\'Apple\': 50}) } ``` **Notes:** - Ensure the results are easy to interpret and validate the counts and ordering. - Utilize the appropriate data structures from the `collections` module to achieve the requirements.","solution":"from collections import OrderedDict, Counter, ChainMap def analyze_sales_data(sales_records): if not sales_records: return {} total_sales = {} item_occurrences = Counter() source_breakdown = {} for source, item, quantity, unit_price in sales_records: if item not in total_sales: total_sales[item] = 0 total_sales[item] += quantity item_occurrences[item] += 1 if source not in source_breakdown: source_breakdown[source] = {} if item not in source_breakdown[source]: source_breakdown[source][item] = 0 source_breakdown[source][item] += quantity ordered_total_sales = OrderedDict(sorted(total_sales.items(), key=lambda x: x[1], reverse=True)) result = { \'total_sales\': ordered_total_sales, \'item_occurrences\': item_occurrences, \'source_breakdown\': ChainMap(*[source_breakdown[source] for source in source_breakdown]) } return result"},{"question":"**Objective:** To assess your understanding of seaborn\'s color palette creation and plotting capabilities, you will generate data, create various color palettes using `sns.light_palette`, and visualize the data using these palettes. **Question:** You are given a dataset containing two numerical columns `X` and `Y`, and a categorical column `Category`. Your task is to: 1. Generate a scatter plot of the data using seaborn. 2. Create three different color palettes using `sns.light_palette`: - A default light palette using a named color. - A light palette using a hex code. - A light palette using HUSL system values. 3. Plot the scatter plot three times, each time with a different palette created in step 2. Use the `Category` column to color the points using the respective palette. **Input:** - A pandas DataFrame `df` with three columns: `X` (float), `Y` (float), and `Category` (string). **Output:** - Three scatter plots with `X` on the x-axis and `Y` on the y-axis, colored by `Category` using the different palettes generated. **Constraints:** - The dataset contains at least 50 rows and 3 unique categories in the `Category` column. - Use seaborn\'s `scatterplot` function for the plotting. - Ensure the use of `sns.light_palette` for creating the palettes. **Example Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Sample data generation (to be replaced with actual DataFrame argument): # df = pd.DataFrame({ # \'X\': [1, 2, 3, ..., 50], # \'Y\': [2, 3, 4, ..., 51], # \'Category\': [\'A\', \'B\', \'C\', ..., \'A\'] # }) def plot_scatter_with_palettes(df): # Create color palettes palette1 = sns.light_palette(\\"seagreen\\", as_cmap=True) palette2 = sns.light_palette(\\"#79C\\", as_cmap=True) palette3 = sns.light_palette((20, 60, 50), input=\\"husl\\", as_cmap=True) # Create scatter plots plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) sns.scatterplot(data=df, x=\'X\', y=\'Y\', hue=\'Category\', palette=palette1) plt.title(\'Scatter Plot with Palette 1\') plt.subplot(1, 3, 2) sns.scatterplot(data=df, x=\'X\', y=\'Y\', hue=\'Category\', palette=palette2) plt.title(\'Scatter Plot with Palette 2\') plt.subplot(1, 3, 3) sns.scatterplot(data=df, x=\'X\', y=\'Y\', hue=\'Category\', palette=palette3) plt.title(\'Scatter Plot with Palette 3\') plt.show() # Example usage: # plot_scatter_with_palettes(df) ``` Ensure that you understand how to generate color palettes using `sns.light_palette` and apply them in seaborn\'s `scatterplot` function.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_scatter_with_palettes(df): Generate three scatter plots with different color palettes. Parameters: df (DataFrame): A pandas DataFrame with columns `X`, `Y` and `Category` # Create color palettes palette1 = sns.light_palette(\\"seagreen\\", as_cmap=False) palette2 = sns.light_palette(\\"#79C\\", as_cmap=False) palette3 = sns.light_palette((20, 60, 50), input=\\"husl\\", as_cmap=False) # Extract unique categories and assign a color from the palette unique_categories = df[\'Category\'].unique() lut1 = dict(zip(unique_categories, palette1)) lut2 = dict(zip(unique_categories, palette2)) lut3 = dict(zip(unique_categories, palette3)) # Create scatter plots plt.figure(figsize=(20, 5)) plt.subplot(1, 3, 1) sns.scatterplot(data=df, x=\'X\', y=\'Y\', hue=\'Category\', palette=lut1) plt.title(\'Scatter Plot with Palette 1\') plt.subplot(1, 3, 2) sns.scatterplot(data=df, x=\'X\', y=\'Y\', hue=\'Category\', palette=lut2) plt.title(\'Scatter Plot with Palette 2\') plt.subplot(1, 3, 3) sns.scatterplot(data=df, x=\'X\', y=\'Y\', hue=\'Category\', palette=lut3) plt.title(\'Scatter Plot with Palette 3\') plt.show()"},{"question":"**Problem Statement** You are tasked with writing a simulation of a bank account system where multiple tasks (representing transactions) need to access and modify a shared account balance concurrently. For this purpose, you need to use the `asyncio` synchronization primitives. Specifically, you need to: 1. Create an `Account` class with the following properties and methods: - `balance`: an integer representing the account balance. - `__init__(self, initial_balance: int)`: initializes the account with an initial balance. - `deposit(self, amount: int)`: asynchronously increases the balance by the given amount. - `withdraw(self, amount: int)`: asynchronously decreases the balance by the given amount, ensuring the balance never goes negative. - `get_balance(self) -> int`: returns the current balance asynchronously. 2. Ensure that the `deposit` and `withdraw` methods use an `asyncio.Lock` to synchronize access to the shared balance to prevent race conditions. **Requirements:** - Methods `deposit` and `withdraw` should simulate some delay using `asyncio.sleep`. - The balance should never turn negative during a withdrawal operation. **Input:** - `initial_balance` (int): The initial balance for the account. - `transactions` (List[Tuple[str, int]]): A list of transactions where each transaction is a tuple with the first element being either `\\"deposit\\"` or `\\"withdraw\\"`, and the second element being the amount to deposit or withdraw. **Output:** - Return the final balance of the account after all transactions have been processed. **Constraints:** - The initial balance is a non-negative integer (0 <= initial_balance <= 10^6). - Each transaction amount is a non-negative integer (0 <= amount <= 10^6). - The number of transactions (n) is 1 <= n <= 10^4. **Example:** ```python import asyncio from typing import List, Tuple class Account: def __init__(self, initial_balance: int): self.balance = initial_balance self.lock = asyncio.Lock() async def deposit(self, amount: int): async with self.lock: await asyncio.sleep(1) self.balance += amount async def withdraw(self, amount: int): async with self.lock: await asyncio.sleep(1) if self.balance >= amount: self.balance -= amount async def get_balance(self) -> int: async with self.lock: return self.balance async def process_transactions(account: Account, transactions: List[Tuple[str, int]]) -> int: tasks = [] for transaction in transactions: if transaction[0] == \\"deposit\\": tasks.append(account.deposit(transaction[1])) elif transaction[0] == \\"withdraw\\": tasks.append(account.withdraw(transaction[1])) await asyncio.gather(*tasks) return await account.get_balance() # Example usage async def main(): initial_balance = 1000 transactions = [(\\"deposit\\", 500), (\\"withdraw\\", 300), (\\"withdraw\\", 1200), (\\"deposit\\", 700)] account = Account(initial_balance) final_balance = await process_transactions(account, transactions) print(final_balance) # Output: 1000 asyncio.run(main()) ``` **Notes:** - Ensure to test the Account and transaction processing using `asyncio.run`. - Handle all edge cases such as withdrawing more than the current balance.","solution":"import asyncio from typing import List, Tuple class Account: def __init__(self, initial_balance: int): self.balance = initial_balance self.lock = asyncio.Lock() async def deposit(self, amount: int): async with self.lock: await asyncio.sleep(1) # Simulate delay self.balance += amount async def withdraw(self, amount: int): async with self.lock: await asyncio.sleep(1) # Simulate delay if self.balance >= amount: self.balance -= amount async def get_balance(self) -> int: async with self.lock: return self.balance async def process_transactions(account: Account, transactions: List[Tuple[str, int]]) -> int: tasks = [] for transaction in transactions: if transaction[0] == \\"deposit\\": tasks.append(account.deposit(transaction[1])) elif transaction[0] == \\"withdraw\\": tasks.append(account.withdraw(transaction[1])) await asyncio.gather(*tasks) return await account.get_balance()"},{"question":"# URL Handling and Analysis with `urllib` Objective: You are tasked with writing a function `analyze_website(url: str) -> dict` that takes a URL as input and performs several operations using the `urllib` package. Function Requirements: 1. **Opening and Reading the URL**: - Attempt to open the specified URL and read its content. - If an error occurs (e.g., HTTP 404 or another request-related error), your function should handle the exception and return a dictionary containing the error message. 2. **Parsing the URL**: - Parse the given URL to extract components such as the scheme, domain, path, etc. - Return these components as part of the dictionary. 3. **Checking `robots.txt`**: - Check if the specified website\'s \\"robots.txt\\" file allows you to scrape the root path (\\"/\\"). Assume the `User-agent` as \\"*\\" (all robots). - Parse the \\"robots.txt\\" file and determine if it allows fetching the URL\'s content. Include this information in the dictionary. 4. **Output Format**: - The function should return a dictionary with the following keys (if applicable): - `\\"url_components\\"`: A dictionary containing the components of the URL. - `\\"robots_txt_allowed\\"`: A boolean indicating whether scraping the given URL is allowed. - `\\"content\\"`: The content of the URL (if successfully fetched). - `\\"error\\"`: Any error message encountered during the process. Constraints: - Make sure your solution gracefully handles any network-related errors or malformed URLs. - The URL passed to the function will be a valid URL string. Example: ```python url = \\"http://example.com\\" # Expected output (structure, actual content may vary): { \\"url_components\\": { \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" }, \\"robots_txt_allowed\\": True, \\"content\\": \\"<html>...</html>\\", # very large text possibly truncated \\"error\\": None } ``` Here is the function template you should start with: ```python from urllib import request, error, parse, robotparser def analyze_website(url: str) -> dict: result = {} # Part 1: Opening and reading the URL try: with request.urlopen(url) as response: result[\'content\'] = response.read().decode(\'utf-8\') result[\'error\'] = None except error.URLError as e: result[\'content\'] = None result[\'error\'] = str(e) # Part 2: Parsing the URL parsed_url = parse.urlparse(url) result[\'url_components\'] = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } # Part 3: Checking robots.txt robots_url = parse.urljoin(url, \'/robots.txt\') rp = robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() result[\'robots_txt_allowed\'] = rp.can_fetch(\'*\', url) return result ``` Notes: - Use the `urllib` package to implement the specified requirements. - Ensure your function works for both HTTP and HTTPS URLs. - Test your function with various URLs to ensure robustness.","solution":"from urllib import request, error, parse, robotparser def analyze_website(url: str) -> dict: result = {} # Part 1: Opening and reading the URL try: with request.urlopen(url) as response: result[\'content\'] = response.read().decode(\'utf-8\') result[\'error\'] = None except error.URLError as e: result[\'content\'] = None result[\'error\'] = str(e) # Part 2: Parsing the URL parsed_url = parse.urlparse(url) result[\'url_components\'] = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } # Part 3: Checking robots.txt robots_url = parse.urljoin(url, \'/robots.txt\') rp = robotparser.RobotFileParser() rp.set_url(robots_url) try: rp.read() result[\'robots_txt_allowed\'] = rp.can_fetch(\'*\', url) except Exception: result[\'robots_txt_allowed\'] = False return result"},{"question":"Objective: Your task is to create a Python class `SpecialConstantsHandler` that demonstrates a comprehensive understanding of the built-in constants in Python. This class should properly use and handle these constants in various methods, showing their intended use and constraints. Requirements: 1. `SpecialConstantsHandler` should contain the following methods: - `is_true(value)`: - **Input**: Any value. - **Output**: Returns `True` if the value is `True`, otherwise `False`. - `is_false(value)`: - **Input**: Any value. - **Output**: Returns `True` if the value is `False`, otherwise `False`. - `get_none()`: - **Output**: Returns `None`. - `compare_with_not_implemented(other_type)`: - **Input**: Any type. - **Output**: Returns `NotImplemented` if the operation is not implemented for the given type. - `slice_with_ellipsis()`: - **Output**: Returns a tuple with five elements where the middle element is `Ellipsis` (i.e., `(1, 2, Ellipsis, 4, 5)`). - `assert_debug_mode()`: - **Output**: Uses an assert statement that passes when `__debug__` is `True`. 2. Ensure the methods handle the constants correctly: - They should not allow reassignment of `True`, `False`, `None`, and `__debug__`. - Perform type checks where necessary and return appropriate results. Constraints: - Do not use external libraries. - Ensure that the methods adhere to Python\'s constraints for these constants. - Your code should be free of errors and should not raise `SyntaxError` or `TypeError`. Example Usage: ```python handler = SpecialConstantsHandler() print(handler.is_true(True)) # Output: True print(handler.is_false(False)) # Output: True print(handler.get_none()) # Output: None print(handler.compare_with_not_implemented(str)) # Output: NotImplemented print(handler.slice_with_ellipsis()) # Output: (1, 2, Ellipsis, 4, 5) handler.assert_debug_mode() # No output if __debug__ is True, AssertionError otherwise ``` Write your `SpecialConstantsHandler` class below: ```python class SpecialConstantsHandler: # Implement your methods here. pass ```","solution":"class SpecialConstantsHandler: def is_true(self, value): Returns True if the provided value is True, otherwise False. return value is True def is_false(self, value): Returns True if the provided value is False, otherwise False. return value is False def get_none(self): Returns None. return None def compare_with_not_implemented(self, other_type): Returns NotImplemented if the operation is not implemented for the given type. return NotImplemented def slice_with_ellipsis(self): Returns a tuple with five elements where the middle element is Ellipsis. return (1, 2, Ellipsis, 4, 5) def assert_debug_mode(self): Asserts that __debug__ is True. assert __debug__, \\"Debug mode is not enabled.\\""},{"question":"**Question: Web Page Content Fetcher with URL Validation and Robots.txt Compliance** You are required to write a Python function `fetch_web_content` that fetches content from a given URL. Your function should perform the following tasks: 1. Validate the given URL. 2. Check the \\"robots.txt\\" file of the domain to ensure that scraping is allowed. 3. Fetch the content of the provided URL if allowed by \\"robots.txt\\". # Function Signature ```python def fetch_web_content(url: str) -> str: pass ``` # Input - `url` (str): The URL of the web page to fetch the content from. # Output - Returns the content of the web page as a string if the URL is valid and accessing it is allowed by \\"robots.txt\\". - Returns the string \\"Invalid URL\\" if the URL is not valid. - Returns the string \\"Access Denied by robots.txt\\" if accessing the URL is not allowed by \\"robots.txt\\". - Returns the string \\"Failed to fetch content\\" if an error occurs while fetching the content. # Constraints - You should use the `urllib` package for URL handling. - Handle any exceptions that may arise during the process. - Respect the rules specified in \\"robots.txt\\" for the domain. # Example Usage ```python content = fetch_web_content(\\"http://example.com\\") print(content) # Returns the content of the web page or an appropriate message. ``` # Notes 1. A URL is considered valid if it can be parsed without raising an exception using `urllib.parse`. 2. Use `urllib.robotparser` to check the rules in the \\"robots.txt\\" file. 3. Use `urllib.request` to fetch the web page content. **Hints:** - You may need to add a user-agent while making a request to avoid being blocked. - Consider edge cases such as missing \\"robots.txt\\" files.","solution":"import urllib.parse import urllib.robotparser import urllib.request from urllib.error import URLError, HTTPError def fetch_web_content(url: str) -> str: # Validate URL try: parsed_url = urllib.parse.urlparse(url) if not parsed_url.scheme or not parsed_url.netloc: return \\"Invalid URL\\" except Exception: return \\"Invalid URL\\" # Get the domain from the URL domain = f\\"{parsed_url.scheme}://{parsed_url.netloc}\\" # Initialize the robots parser rp = urllib.robotparser.RobotFileParser() rp.set_url(urllib.parse.urljoin(domain, \\"/robots.txt\\")) rp.read() # Check if fetching content is allowed if not rp.can_fetch(\\"*\\", url): return \\"Access Denied by robots.txt\\" # Fetch content try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except HTTPError as e: return f\\"HTTP Error: {e.code}\\" except URLError as e: return \\"Failed to fetch content\\" except Exception as e: return \\"Failed to fetch content\\""},{"question":"**Tensor View Manipulation in PyTorch** **Objective:** You are required to implement a function that takes a tensor, performs a series of view operations, and returns the final tensor along with information about its contiguity and the underlying data pointer. **Function Signature:** ```python import torch def tensor_view_manipulation(t: torch.Tensor) -> dict: Perform view operations on the input tensor and return a dictionary with: - \'final_tensor\': the resultant tensor after view operations. - \'is_contiguous\': a boolean indicating if the resultant tensor is contiguous. - \'data_ptr_equal\': a boolean indicating if the initial and final tensors share the same data in memory. Parameters: t (torch.Tensor): The input tensor to be manipulated. Returns: dict: A dictionary containing \'final_tensor\', \'is_contiguous\', and \'data_ptr_equal\'. pass ``` **Constraints:** - Ensure that the view operations do not involve copying data unnecessarily. - You must use at least three different view operations listed in the documentation. - The input tensor `t` can have up to 4 dimensions and contain float values. **Example:** ```python >>> import torch >>> t = torch.rand(4, 4, 4) >>> result = tensor_view_manipulation(t) >>> print(result[\'final_tensor\'].shape) torch.Size([2, 32]) >>> print(result[\'is_contiguous\']) False >>> print(result[\'data_ptr_equal\']) True ``` **Explanation:** In the example above: 1. The original tensor `t` of shape [4, 4, 4] is reshaped into a tensor of shape [2, 32] via view operations. 2. The final tensor may not be contiguous, which is indicated by the `is_contiguous` key. 3. The `data_ptr_equal` key should confirm that the data underlying both tensors (before and after view operations) is the same. Your task is to implement the `tensor_view_manipulation` function following these specifications and constraints.","solution":"import torch def tensor_view_manipulation(t: torch.Tensor) -> dict: Perform view operations on the input tensor and return a dictionary with: - \'final_tensor\': the resultant tensor after view operations. - \'is_contiguous\': a boolean indicating if the resultant tensor is contiguous. - \'data_ptr_equal\': a boolean indicating if the initial and final tensors share the same data in memory. Parameters: t (torch.Tensor): The input tensor to be manipulated. Returns: dict: A dictionary containing \'final_tensor\', \'is_contiguous\', and \'data_ptr_equal\'. # Ensure t is a float tensor and has up to 4 dimensions assert t.ndim <= 4 assert t.dtype == torch.float32 initial_data_ptr = t.data_ptr() # First view operation: reshape the tensor (assuming t has at least 2 dimensions for this example) view1 = t.view(-1) # Flatten the tensor # Second view operation: reshape into a (2, -1) shape tensor, ensuring the size is compatible factor = view1.size(0) // 2 view2 = view1.view(2, factor) # Third view operation: permute the dimensions (transpose for 2D tensors) view3 = view2.permute(1, 0) final_tensor = view3 is_contiguous = final_tensor.is_contiguous() data_ptr_equal = initial_data_ptr == final_tensor.data_ptr() return { \'final_tensor\': final_tensor, \'is_contiguous\': is_contiguous, \'data_ptr_equal\': data_ptr_equal }"},{"question":"# Question: PyLongObject Conversion Utility You are tasked with implementing a utility that deals with Python integer objects (`PyLongObject`). This utility should perform multiple operations, including type checking, conversions, and error handling. Functions to Implement 1. `is_pylong_object(obj: Any) -> bool` - **Input:** A generic Python object. - **Output:** `True` if the object is a `PyLongObject` or a subtype, `False` otherwise. 2. `create_pylong_from_c_long(value: int) -> int` - **Input:** A C long integer (`int` in Python). - **Output:** A Python integer created from the given C long. - **Constraints:** The input value should be in the range of a C long type. - **Error Handling:** Raise a `ValueError` if the conversion fails. 3. `convert_pylong_to_c_long(pylong_obj: int) -> int` - **Input:** A Python integer (`int` in Python which corresponds to `PyLongObject`). - **Output:** The C long representation of the given Python integer. - **Error Handling:** Raise an `OverflowError` if the value is out of the range of C long. Utilize Python\'s built-in `_long_to_int()` for the conversion back to C long. 4. `create_pylong_from_string(value_str: str, base: int) -> int` - **Input:** A string representing a number, and the base (an integer between 2 and 36 inclusive). - **Output:** A Python integer created from the specified string. - **Constraints:** Ensure the base is valid. - **Error Handling:** Raise a `ValueError` if the string cannot be converted or if the base is invalid. # Example Usage ```python # is_pylong_object examples print(is_pylong_object(123)) # True, as 123 is an int in Python which corresponds to PyLongObject print(is_pylong_object(\\"123\\")) # False, as \\"123\\" is a str # create_pylong_from_c_long examples print(create_pylong_from_c_long(12345)) # 12345 # convert_pylong_to_c_long examples print(convert_pylong_to_c_long(12345)) # 12345 # create_pylong_from_string examples print(create_pylong_from_string(\\"12345\\", 10)) # 12345 print(create_pylong_from_string(\\"1001\\", 2)) # 9 (binary 1001 is 9 in decimal) ``` # Notes - You are expected to use Python\'s built-in functions when available to simulate the operations of the specified C functions. - Pay attention to handling errors appropriately as specified in the function definitions, ensuring the utility behaves as expected in edge cases.","solution":"def is_pylong_object(obj): Returns True if the object is an instance of int, False otherwise. return isinstance(obj, int) def create_pylong_from_c_long(value): Returns a Python integer created from a C long, which is conceptually similar to a regular int in Python. Raises ValueError if the conversion fails. try: return int(value) except ValueError: raise ValueError(\\"Conversion to Python long failed\\") def convert_pylong_to_c_long(pylong_obj): Converts a Python integer to a C long (in this case, a regular Python int). Raises OverflowError if the integer value is out of C long range. import sys if not is_pylong_object(pylong_obj): raise ValueError(\\"Input is not a valid PyLongObject\\") c_long_min = -sys.maxsize - 1 c_long_max = sys.maxsize if pylong_obj < c_long_min or pylong_obj > c_long_max: raise OverflowError(\\"The value is out of range for a C long\\") return int(pylong_obj) def create_pylong_from_string(value_str, base): Converts a string to a Python integer using the specified base. Raises ValueError if the string cannot be converted or if the base is invalid. if base < 2 or base > 36: raise ValueError(\\"Base must be between 2 and 36\\") try: return int(value_str, base) except ValueError: raise ValueError(\\"Invalid string for the specified base\\")"},{"question":"<|Analysis Begin|> The provided documentation pertains to the `heapq` module in Python, which implements the heap queue algorithm, also known as the priority queue algorithm. The module supports various operations on heaps, such as pushing elements, popping the smallest element, pushing and then popping, transforming a list into a heap, and replacing the smallest element with a new element. Moreover, it offers functionalities for merging multiple sorted inputs and finding the n-largest and n-smallest elements in an iterable. The heap data structure is efficiently implemented with arrays and follows the property that for any element at index `k` in the array, its children are at indices `2*k+1` and `2*k+2`, and all parent nodes are less than or equal to their children. Important concepts demonstrated by this module include: - Binary heaps - Priority queues - Zero-based indexing - Efficiency of heap operations like insertion, deletion, and replacement - Tuple comparison and custom object comparison - Handling priority changes and removals in a priority queue - Strategic memory use for big disk sorts Based on this information, a suitable coding assessment question can be designed involving the implementation of a priority task scheduler using a heap. <|Analysis End|> <|Question Begin|> Priority Task Scheduler using `heapq` # Background In many systems, tasks have different priorities, and it is crucial to manage them such that higher priority tasks are always processed before lower priority ones. The `heapq` module in Python can be used to effectively manage a priority queue. # Objective Your task is to implement a priority task scheduler using the `heapq` module. Your scheduler should support the following operations: 1. Adding a new task with a specified priority. 2. Removing a task (marking it as removed). 3. Popping the task with the highest priority (i.e., the smallest priority number). 4. Modifying the priority of an existing task. # Function Signatures You are required to implement the following functions: 1. `add_task(scheduler, task, priority) -> None`: - Adds a new task with the given priority. - If the task already exists, update its priority. 2. `remove_task(scheduler, task) -> None`: - Removes the task from the scheduler. 3. `pop_task(scheduler) -> str`: - Pops and returns the task with the highest priority. - Raises `KeyError` if the scheduler is empty. 4. `change_task_priority(scheduler, task, new_priority) -> None`: - Changes the priority of an existing task. - Raises `KeyError` if the task does not exist. # Input and Output - **Input:** The scheduler is a dictionary with two keys: `\'pq\'` (a heap maintained as a list) and `\'entry_finder\'` (a dictionary mapping tasks to their entries in the heap). - **Output:** Appropriate for each function (return value or None). # Constraints You should ensure the heap property is maintained throughout. You also need to handle tasks with equal priorities by using an additional counter to ensure they are processed in the order they were added if their priorities are the same. # Example ```python scheduler = {\'pq\': [], \'entry_finder\': {}, \'REMOVED\': \'<removed-task>\', \'counter\': itertools.count()} # Add tasks add_task(scheduler, \'task1\', 2) add_task(scheduler, \'task2\', 1) add_task(scheduler, \'task3\', 3) # Change task priority change_task_priority(scheduler, \'task3\', 0) # Pop highest priority task print(pop_task(scheduler)) # Output: \'task3\' # Remove a task remove_task(scheduler, \'task1\') # Pop highest priority task print(pop_task(scheduler)) # Output: \'task2\' ``` # Implementation You can use the following template to start with: ```python import heapq import itertools def add_task(scheduler, task, priority): ... def remove_task(scheduler, task): ... def pop_task(scheduler): ... def change_task_priority(scheduler, task, new_priority): ... ``` Implement these functions to complete the task scheduler.","solution":"import heapq import itertools def add_task(scheduler, task, priority): Add a new task or update the priority of an existing task. if task in scheduler[\'entry_finder\']: remove_task(scheduler, task) count = next(scheduler[\'counter\']) entry = [priority, count, task] scheduler[\'entry_finder\'][task] = entry heapq.heappush(scheduler[\'pq\'], entry) def remove_task(scheduler, task): Mark a task as removed. Raise KeyError if not found. entry = scheduler[\'entry_finder\'].pop(task) entry[-1] = scheduler[\'REMOVED\'] def pop_task(scheduler): Remove and return the highest priority task. Raise KeyError if empty. while scheduler[\'pq\']: priority, count, task = heapq.heappop(scheduler[\'pq\']) if task is not scheduler[\'REMOVED\']: del scheduler[\'entry_finder\'][task] return task raise KeyError(\'pop from an empty priority queue\') def change_task_priority(scheduler, task, new_priority): Change the priority of a task. Raise KeyError if not found. remove_task(scheduler, task) add_task(scheduler, task, new_priority)"},{"question":"# **AST Manipulation Task: Refactor Function Calls** **Objective:** Your task is to write a Python function that receives a string of Python code and refactors it. Specifically, you need to find all the function calls for a function named `old_function` and replace them with `new_function`. **Task Requirements:** 1. Parse the string of code into an abstract syntax tree (AST). 2. Traverse the AST to find and replace all calls to `old_function` with `new_function`. 3. Generate and return the modified source code as a string. # **Function Signature:** ```python def refactor_function_calls(source_code: str) -> str: pass ``` # **Input:** - `source_code` (str): A string representing valid Python code which may contain calls to a function named `old_function`. # **Output:** - A string representing the modified Python code with all calls to `old_function` replaced by `new_function`. # **Constraints:** - The input Python code will have valid syntax. - Function calls to `old_function` may appear in various contexts (e.g., as statements, as parts of expressions, inside lambda functions, etc.). # **Example:** ```python input_code = def foo(): old_function(a, b) result = old_function(x, y) data = old_function(*args, **kwargs) expected_output = def foo(): new_function(a, b) result = new_function(x, y) data = new_function(*args, **kwargs) assert refactor_function_calls(input_code) == expected_output ``` # **Notes:** - You may use the `ast` module and its helper classes (e.g., `NodeVisitor`, `NodeTransformer`) to accomplish this task. - Make sure your modified code preserves the original formatting as much as possible. **Hint:** Consider using the `ast.NodeTransformer` class to create a custom transformer that will visit and modify `Call` nodes.","solution":"import ast import astor class FunctionCallTransformer(ast.NodeTransformer): def visit_Call(self, node): # Check if the call is to \'old_function\' if isinstance(node.func, ast.Name) and node.func.id == \'old_function\': # Replace with \'new_function\' node.func.id = \'new_function\' # Continue to transform nested calls if any self.generic_visit(node) return node def refactor_function_calls(source_code: str) -> str: Refactors all calls to `old_function` to `new_function` in the provided source code. Parameters: source_code (str): A string of valid Python code. Returns: str: The refactored source code with `old_function` calls replaced by `new_function`. # Parse the source code into an AST tree = ast.parse(source_code) # Apply the transformation to replace function calls transformer = FunctionCallTransformer() transformed_tree = transformer.visit(tree) # Convert the AST back to source code modified_code = astor.to_source(transformed_tree) return modified_code"},{"question":"Objective: To test your understanding and proficiency in using the `fcntl` module for file descriptor control and file locking in a Unix-like operating system. Problem Statement: You are tasked with creating a Python program that demonstrates the use of the `fcntl` module to safely append data to a file with the following requirements: 1. **File Locking**: Use the `fcntl.flock` function to acquire an exclusive lock on the file before appending data. Ensure that if the lock cannot be acquired, the program waits until it can acquire the lock. 2. **Appending Data**: Append a specified string of text to the file. 3. **Error Handling**: Appropriately handle any potential `OSError` exceptions that may arise during file operations. Implement the function `safe_append_to_file(filename: str, text: str) -> None`. This function should: - Open the file specified by `filename` for appending. - Lock the file using `fcntl.flock` to ensure exclusive access. - Append the given `text` to the file. - Handle any errors that may arise, ensuring that the file is properly closed and the lock is released. Input: - `filename` (str): The path to the file to append the data to. - `text` (str): The text to append to the file. Example Usage: ```python safe_append_to_file(\'/path/to/your/file.txt\', \'New log entryn\') ``` This function will open the file for appending, acquire an exclusive lock, append the text, and then release the lock and close the file. Constraints: - The function should work on Unix-like operating systems. - You must use the `fcntl` module for acquiring and releasing the lock. - Ensure that the file is properly closed even if an error occurs. Requirements: - Your implementation should ensure that the file contents are not modified by concurrent processes while appending data. - Provide appropriate comments to explain your code.","solution":"import fcntl def safe_append_to_file(filename: str, text: str) -> None: Safely append text to a file using file locking. Args: filename (str): The path to the file. text (str): The text to append to the file. Raises: OSError: If an error occurs during file operations. try: # Open file for appending with open(filename, \'a\') as file: # Acquire an exclusive lock on the file fcntl.flock(file.fileno(), fcntl.LOCK_EX) try: # Append the text to the file file.write(text) finally: # Ensure the lock is released fcntl.flock(file.fileno(), fcntl.LOCK_UN) except OSError as e: # Handle any file operation errors print(f\\"An error occurred: {e}\\") raise"},{"question":"**Problem Statement: Data Analysis with Pandas** You are provided with two CSV files, `sales_data.csv` and `customer_data.csv`, containing sales transactions and customer information, respectively. Your task is to implement a function `analyze_sales_data` that will perform the following operations and return a summary report based on the merged data. # Input - `sales_data.csv` containing the following columns: - `transaction_id`: Identifier for each transaction - `customer_id`: Identifier for the customer who made the transaction - `date`: Date of the transaction - `amount`: Amount spent in the transaction - `customer_data.csv` containing the following columns: - `customer_id`: Identifier for each customer - `name`: Name of the customer - `age`: Age of the customer - `city`: City where the customer lives # Function Signature ```python def analyze_sales_data(sales_filepath: str, customers_filepath: str) -> pd.DataFrame: pass ``` # Steps to Implement 1. **Load the Data:** - Read the CSV files into pandas DataFrames. 2. **Merge Data:** - Merge `sales_data` with `customer_data` on the `customer_id` column using an appropriate join technique to retain all sales transactions. 3. **Filter and Group Data:** - Filter transactions to include only those done in the year 2023. - Group the merged data by `city` and calculate the total sales amount per city. 4. **Result Formatting:** - The resulting DataFrame should have two columns: `city` and `total_sales`, sorted by `total_sales` in descending order. # Output - Return a pandas `DataFrame` with the following columns: - `city`: City name - `total_sales`: Total sales amount in that city # Example For given `sales_data.csv`: ```csv transaction_id,customer_id,date,amount 1,101,2023-03-15,150.5 2,102,2023-05-10,200.0 3,101,2022-11-22,175.0 4,103,2023-04-12,100.0 ``` And `customer_data.csv`: ```csv customer_id,name,age,city 101,Jane Doe,30,New York 102,John Smith,45,Los Angeles 103,Alice Johnson,28,Chicago ``` The function should return: ``` city total_sales 0 Los Angeles 200.0 1 New York 150.5 2 Chicago 100.0 ``` **Constraints:** - Your solution should handle potential missing values in the data. - Ensure to handle different data types appropriately.","solution":"import pandas as pd def analyze_sales_data(sales_filepath: str, customers_filepath: str) -> pd.DataFrame: Analyzes sales data and returns a summary report of total sales per city for the year 2023. Parameters: - sales_filepath (str): Path to the sales data CSV file - customers_filepath (str): Path to the customer data CSV file Returns: - pd.DataFrame: A dataframe with columns \'city\' and \'total_sales\', sorted by \'total_sales\' in descending order. # Load Data sales_data = pd.read_csv(sales_filepath) customer_data = pd.read_csv(customers_filepath) # Merge Data merged_data = pd.merge(sales_data, customer_data, on=\'customer_id\', how=\'left\') # Filter Data for the year 2023 merged_data[\'date\'] = pd.to_datetime(merged_data[\'date\']) data_2023 = merged_data[merged_data[\'date\'].dt.year == 2023] # Group Data by city and calculate total sales summary = data_2023.groupby(\'city\')[\'amount\'].sum().reset_index() summary = summary.rename(columns={\'amount\': \'total_sales\'}) # Sort the DataFrame by total_sales in descending order summary = summary.sort_values(by=\'total_sales\', ascending=False).reset_index(drop=True) return summary"},{"question":"**Objective:** Using the `getpass` module, write a Python function that securely prompts the user for their username and password, validates the credentials against a predefined set of valid usernames and passwords, and returns a message indicating whether the login was successful or not. **Function Signature:** ```python def secure_login(valid_credentials: dict) -> str: ``` **Input:** - `valid_credentials`: A dictionary where keys are usernames (str) and values are passwords (str). **Output:** - A string message indicating successful or unsuccessful login: - \\"Login successful\\" if both the username and password match an entry in `valid_credentials`. - \\"Login failed\\" if the username or password is incorrect. **Constraints:** - You must use `getpass.getuser()` to retrieve the username. - You must use `getpass.getpass()` to securely prompt the user for their password. **Example:** ```python valid_credentials = { \\"user1\\": \\"password123\\", \\"user2\\": \\"mypassword\\", \\"admin\\": \\"adminpass\\" } # Example execution: # Assume the current logged in user is \\"user1\\" and they correctly provide the password \\"password123\\". result = secure_login(valid_credentials) print(result) # Output should be \\"Login successful\\" # If the username is correct but the password is incorrect: # Assume the current logged in user is \\"user1\\" and they incorrectly provide the password \\"wrongpassword\\". result = secure_login(valid_credentials) print(result) # Output should be \\"Login failed\\" ``` **Notes:** - Ensure that the username is taken from the environment variables or fallback provided by `getpass.getuser()`. - The password input should not be echoed on the screen as it\'s being typed.","solution":"import getpass def secure_login(valid_credentials: dict) -> str: Securely prompts the user for their username and password, and validates the credentials against a predefined set of valid credentials. Parameters: valid_credentials (dict): A dictionary of valid usernames and passwords. Returns: str: A message indicating successful or unsuccessful login. username = getpass.getuser() password = getpass.getpass(prompt=\'Password: \') if username in valid_credentials and valid_credentials[username] == password: return \\"Login successful\\" else: return \\"Login failed\\""},{"question":"# Python Coding Assessment Question Objective You are required to demonstrate your understanding of Python\'s **pipes** module by creating a pipeline that processes a text file through a series of shell commands. Task Write a Python function `process_text_file(input_filepath: str, output_filepath: str, transformations: list) -> None` that uses the `pipes` module to perform a series of text transformations on an input file and save the result to an output file. Each transformation is represented as a dictionary specifying the shell command and its input/output behavior. Description - **Input:** - `input_filepath (str)`: Path to the input text file. - `output_filepath (str)`: Path to save the final processed text file. - `transformations (list of dict)`: A list of dictionaries. Each dictionary has: - `cmd (str)`: The shell command to execute (e.g., `\\"tr a-z A-Z\\"`). - `kind (str)`: A string specifying input and output types for the command (e.g., `\'-\'` or `\'f\'`). - **Output:** - The function writes the processed content to `output_filepath`. - **Constraints:** - Each dictionary in `transformations` has valid `cmd` and `kind` entries as per `pipes.Template.append`. - The transformations are applied in the order given in the list. Example Suppose we have the following text in `input.txt`: ``` hello world ``` And the following `transformations`: ```python transformations = [ {\\"cmd\\": \\"tr a-z A-Z\\", \\"kind\\": \\"--\\"}, {\\"cmd\\": \\"sed \'s/ WORLD/!/\'\\", \\"kind\\": \\"--\\"} ] ``` Calling `process_text_file(\'input.txt\', \'output.txt\', transformations)` should process `input.txt` to: ``` HELLO! ``` and save it to `output.txt`. Requirements 1. The function should handle edge cases such as an empty transformation list or invalid file paths gracefully. 2. Make sure to close files properly after operations. Code Template ```python import pipes def process_text_file(input_filepath: str, output_filepath: str, transformations: list) -> None: t = pipes.Template() for transformation in transformations: cmd = transformation[\'cmd\'] kind = transformation[\'kind\'] t.append(cmd, kind) # Open the output file in write mode with t.open(output_filepath, \'w\') as f: with open(input_filepath, \'r\') as infile: f.write(infile.read()) # Make sure to handle exceptions, edge cases, and file closure properly # Example usage transformations = [ {\\"cmd\\": \\"tr a-z A-Z\\", \\"kind\\": \\"--\\"}, {\\"cmd\\": \\"sed \'s/ WORLD/!/\'\\", \\"kind\\": \\"--\\"} ] process_text_file(\'input.txt\', \'output.txt\', transformations) ``` Evaluation Criteria - Correct application of the pipes.Template class and its methods. - Handling of different command types and edge cases. - Proper file handling and resource management. *Note:* The `pipes` module is deprecated in Python 3.11. For future implementations, consider using the `subprocess` module.","solution":"import pipes def process_text_file(input_filepath: str, output_filepath: str, transformations: list) -> None: t = pipes.Template() for transformation in transformations: cmd = transformation.get(\'cmd\') kind = transformation.get(\'kind\') if cmd and kind: t.append(cmd, kind) try: with t.open(output_filepath, \'w\') as f: with open(input_filepath, \'r\') as infile: f.write(infile.read()) except Exception as e: print(f\\"Error processing file: {e}\\")"},{"question":"Problem Statement You have been provided with a text file that contains a list of records, each on a new line. Your task is to create a function that will safely read and update records in this file using efficient locking mechanisms to ensure that no two processes can read or write to the file at the same time. # Specifications: 1. **Function Signature**: ```python def update_record(file_path: str, record_line: int, new_data: str) -> None: ``` 2. **Inputs**: - `file_path` (str): The path to the file containing the records. - `record_line` (int): Line number of the record to update (0-indexed). - `new_data` (str): New data to overwrite the selected record with. 3. **Requirements**: - Use `fcntl` to perform an exclusive lock (`LOCK_EX`) for reading and updating the file. - Read the file contents to ensure the `record_line` exists. - If the `record_line` does not exist, the function should raise an appropriate error. - Overwrite the specified `record_line` with `new_data`. Ensure no data corruption or race conditions. 4. **Constraints**: - Ensure the file is unlocked after the operation. - Handle any `OSError` exceptions appropriately. # Example Assume the contents of `records.txt` are: ``` Record 1 Record 2 Record 3 Record 4 ``` Calling `update_record(\'records.txt\', 2, \'Updated Record 3\')` should update the file\'s contents to: ``` Record 1 Record 2 Updated Record 3 Record 4 ``` # Note: - Be mindful of file pointers when updating specific lines in the file. - Ensure you do not introduce new lines unless intended. # Additional Information - You may use `fcntl.flock` for locking. - Properly handle potential errors such as file not found, permission issues, etc. - Ensure the function performs the operations atomically to prevent race conditions.","solution":"import fcntl def update_record(file_path: str, record_line: int, new_data: str) -> None: Update a specific line in a file with new data using file locking. Args: - file_path (str): Path to the file. - record_line (int): Line number to update (0-indexed). - new_data (str): New data to write to the specified line. Raises: - IndexError: If the specified record_line does not exist in the file. - OSError: For other file access related issues. try: with open(file_path, \'r+\') as f: fcntl.flock(f, fcntl.LOCK_EX) lines = f.readlines() if record_line >= len(lines): raise IndexError(f\\"Record line {record_line} does not exist in the file.\\") # Update the specific line lines[record_line] = new_data + \'n\' # Move the file pointer to the beginning f.seek(0) # Write all lines back to the file f.writelines(lines) # Truncate the file to the current size (in case new data is shorter) f.truncate() fcntl.flock(f, fcntl.LOCK_UN) except OSError as e: raise e"},{"question":"Question: Implementing a Script with `__main__` and Modular Functions You are tasked with creating a Python script that processes a list of numbers. The script should be designed modularly so that it can be imported and its functions reused without executing the top-level script code unintentionally. # Requirements: 1. **Create a Python module named `number_processor.py`**: - Define a function `compute_statistics(numbers: List[int]) -> Tuple[int, int, float]` which computes and returns: - The sum of the numbers. - The count of the numbers. - The average of the numbers. - Define a function `main() -> int` which: - Reads a list of integers from the command line arguments. - Calls `compute_statistics` with the list. - Prints the sum, count, and average. - Returns `0`. 2. **Ensure that the `main` function is only executed when the module is run as a script**: - Use the `if __name__ == \'__main__\':` idiom. - Use `sys.exit(main())`. 3. **Bonus: Create a `__main__.py` file**: - Create a package directory called `number_package` containing an empty `__init__.py` and `number_processor.py`. - Add a `__main__.py` file that allows running the package directly using `python3 -m number_package` to execute the `main` function from `number_processor.py`. # Input: - Command line arguments: A list of integers. # Output: - Prints the sum, count, and average of the list of integers. # Constraints: - Ensure that the list contains only valid integers. # Example Usage: As a script: ```bash python3 number_processor.py 1 2 3 4 5 Sum: 15 Count: 5 Average: 3.0 ``` As a package: ```bash python3 -m number_package 10 20 30 Sum: 60 Count: 3 Average: 20.0 ``` # Note: Make sure your module is structured properly to support both direct script execution and package-based execution.","solution":"import sys from typing import List, Tuple def compute_statistics(numbers: List[int]) -> Tuple[int, int, float]: Computes statistics for a list of numbers including sum, count, and average. if not numbers: return (0, 0, 0.0) total_sum = sum(numbers) count = len(numbers) average = total_sum / count return total_sum, count, average def main() -> int: Main function to read integers from the command line, compute statistics, and print the sum, count, and average. if len(sys.argv) < 2: print(\\"Usage: number_processor.py <num1> <num2> ... <numN>\\") return 1 try: numbers = [int(arg) for arg in sys.argv[1:]] except ValueError: print(\\"All arguments must be integers.\\") return 1 total_sum, count, average = compute_statistics(numbers) print(f\\"Sum: {total_sum}\\") print(f\\"Count: {count}\\") print(f\\"Average: {average:.1f}\\") return 0 if __name__ == \'__main__\': sys.exit(main())"},{"question":"# Advanced Garbage Collection Management in Python **Objective**: To test the student\'s understanding of managing and debugging garbage collection in Python using the `gc` module. **Problem Statement**: You are given a list of dictionaries representing simulation objects in a memory-intensive application. Each dictionary has a structure similar to this: ```python { \\"name\\": \\"object_name\\", \\"data\\": large_data_structure, \\"references\\": [other_objects] } ``` Your task is to implement a function `manage_garbage_collection(objects)` that: 1. Disables automatic garbage collection. 2. Registers a callback to print debugging information each time a garbage collection cycle is started and stopped, using the `gc.callbacks` list. 3. Sets custom garbage collection thresholds. 4. Forces a garbage collection of all generations and prints out the number of unreachable objects found. 5. If any uncollectable objects are found, prints their details. 6. Finally, enables the garbage collection again. # Function Signature ```python def manage_garbage_collection(objects: list[dict]) -> None: ``` # Input - `objects` (list of dict): A list of dictionaries representing simulation objects. # Output - This function does not return anything. # Requirements 1. **Disabling and Enabling**: The garbage collection should be disabled at the beginning and enabled at the end. 2. **Debugging Information**: Use `gc.callbacks` to print when garbage collection starts and stops. 3. **Setting Thresholds**: Set custom thresholds (e.g., `gc.set_threshold(700, 10, 10)`). 4. **Forcing Collection**: Use `gc.collect()` to force collection and print the number of unreachable objects found. 5. **Handling Uncollectable Objects**: Inspect and print any uncollectable objects found. # Example ```python objects = [ { \\"name\\": \\"obj1\\", \\"data\\": [i for i in range(1000000)], \\"references\\": [] }, { \\"name\\": \\"obj2\\", \\"data\\": [i for i in range(2000000)], \\"references\\": [] } ] manage_garbage_collection(objects) ``` # Constraints - Do not modify the `objects` input directly. - The program should handle large numbers of objects efficiently. Use the provided `gc` module documentation to effectively implement this function.","solution":"import gc def manage_garbage_collection(objects): Manages garbage collection for a list of objects. Args: objects (list of dict): A list of dictionaries representing simulation objects. Returns: None # Disable automatic garbage collection gc.disable() # Callback for debugging information def debug_callback(phase, info): print(f\\"Debug: GC {phase} - info: {info}\\") # Register the callback gc.callbacks.append(debug_callback) # Set custom garbage collection thresholds gc.set_threshold(700, 10, 10) # Force a garbage collection of all generations unreachable_objects = gc.collect() print(f\\"Number of unreachable objects found: {unreachable_objects}\\") # Handle any uncollectable objects if gc.garbage: print(\\"Uncollectable objects found:\\") for uncollectable in gc.garbage: print(uncollectable) # Enable the garbage collection again gc.enable() # Unregister the callback to avoid side effects gc.callbacks.remove(debug_callback)"},{"question":"# Question Objective: Your task is to write a Python function that demonstrates the use of the `tarfile` module by performing specific operations on tar archives. This will test your comprehension of handling tar files, including creating, extracting, filtering, and managing errors. Problem Description: Implement a function named `process_tarfile` that performs the following operations: 1. **Create a tar archive** named `archive.tar.gz` from a given list of files and directories. 2. **List the contents** of the created archive and print them. 3. **Extract specific files** from the archive to a specified directory using a filter to include only `.txt` files. 4. Handle any **errors** related to compression and extraction processes gracefully, printing appropriate error messages. Function Signature: ```python def process_tarfile(files: list, extract_dir: str): pass ``` Parameters: - `files` (list): A list of strings where each string is the path to a file or directory to be included in the tar archive. - `extract_dir` (str): The path to the directory where `.txt` files will be extracted from the tar archive. Constraints: - The function should create the tar archive with gzip compression. - Any directories included in the `files` list should be added recursively. - Handle `CompressionError`, `ExtractError`, and other relevant exceptions gracefully. - Use the appropriate `tarfile` methods and functionalities to achieve the tasks. Example: ```python files = [\\"document.txt\\", \\"images/\\", \\"notes.txt\\"] extract_dir = \\"extracted_files\\" process_tarfile(files, extract_dir) ``` Expected Output: ``` Creating archive tar... Listing contents of the archive: document.txt images/ images/image1.jpg images/image2.png notes.txt Extracting .txt files to \'extracted_files\'... Successfully extracted document.txt Successfully extracted notes.txt ``` (Note: The actual output may vary based on the content of the provided files and directories.) Notes: - Ensure the `extract_dir` exists or create it if it does not. - Use filters effectively to ensure only `.txt` files are extracted. - Handle and print messages for any exceptions that occur during the process.","solution":"import tarfile import os def process_tarfile(files: list, extract_dir: str): archive_name = \\"archive.tar.gz\\" try: print(\\"Creating archive...\\") with tarfile.open(archive_name, \\"w:gz\\") as tar: for file in files: tar.add(file) print(\\"Listing contents of the archive:\\") with tarfile.open(archive_name, \\"r:gz\\") as tar: for member in tar.getmembers(): print(member.name) if not os.path.exists(extract_dir): os.makedirs(extract_dir) print(\\"Extracting .txt files to \'{}\'...\\".format(extract_dir)) with tarfile.open(archive_name, \\"r:gz\\") as tar: for member in tar.getmembers(): if member.name.endswith(\'.txt\'): tar.extract(member, extract_dir) print(f\\"Successfully extracted {member.name}\\") except (tarfile.TarError, IOError) as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Objective:** Write a function that converts an array of mixed data types including strings, floats, and integers represented through different possible Python constructs to Python integer objects. Your function should handle potential errors gracefully, especially when conversion is not possible. **Function Signature:** ```python def convert_to_python_ints(mixed_array: list) -> list: ``` **Input:** - `mixed_array`: A list of mixed data types. Each element can be of one of the following types: - `int`: Python integer. - `str`: Strings which represent integers. - `float`: Floating-point numbers. - `tuple`: A tuple containing a string representation of data type and its value. Example: `(\\"long\\", 1234567890123456789)` - `dict`: A dictionary containing information about integer data types and their values. Example: `{\\"type\\": \\"unsigned long\\", \\"value\\": 1234567890}` **Output:** - Returns a list of Python integer objects converted from the provided mixed data types. If an element cannot be converted, it should be skipped, i.e., not included in the output list. **Example:** ```python input_data = [42, \\"123\\", 3.14, (\\"long\\", 9876543210987654321), {\\"type\\": \\"unsigned long\\", \\"value\\": 123456}, \\"invalid\\", {\\"type\\": \\"unknown\\", \\"value\\": 100}] print(convert_to_python_ints(input_data)) # Output: [42, 123, 3, 9876543210987654321, 123456] ``` **Constraints:** 1. You cannot use Pythonâ€™s built-in `int` function directly for string conversion. 2. Handle possible errors and invalid conversions gracefully using Python error-handling mechanisms. 3. The conversion logic for `tuple` and `dict` data types should correctly interpret and convert long integers and their unsigned variants. 4. Performance is not a primary concern but avoid redundant conversions and handle large lists efficiently. **Notes:** - A float should be converted to its integer part. - Conversion of string representations must take into account possible bases (base 2 to base 36). - The function should analyze the `tuple` and `dict` types based on the provided format and correctly determine the type before conversion. **Hints:** - You can refer to the documentation provided to understand how to create Python integer objects from various other representations and simulate those in your function. **Deliverables:** - Implement the function `convert_to_python_ints(mixed_array: list) -> list`. - Ensure to include error handling in your implementation to skip invalid conversions.","solution":"def convert_to_python_ints(mixed_array: list) -> list: def convert_element(element): try: if isinstance(element, int): return element elif isinstance(element, float): return int(element) elif isinstance(element, str): return int(element) # Assuming the str is base 10 elif isinstance(element, tuple) and len(element) == 2: type_name, value = element if type_name == \\"long\\": return int(value) # Add additional type conversions if required elif isinstance(element, dict) and \\"type\\" in element and \\"value\\" in element: type_name = element[\\"type\\"] value = element[\\"value\\"] if type_name == \\"unsigned long\\": return int(value) # Add additional type conversions if required except (ValueError, TypeError): return None return None return [converted for element in mixed_array if (converted := convert_element(element)) is not None]"},{"question":"# Advanced Coding Assessment: Implementing and Using Descriptors in Python Objective: To assess students\' understanding of how to create and use descriptors in Python to manage attribute access, validate data, and implement custom behaviors. Problem Statement: Create an advanced `Descriptor` and design a subclass that uses it to implement validation logic. Your task is to implement: 1. A base descriptor class called `BaseDescriptor`. 2. A subclass called `ValidatedAttribute` that inherits from `BaseDescriptor` and includes validation logic. 3. A class `Product` that uses `ValidatedAttribute` for its attributes. Detailed Requirements: 1. **BaseDescriptor Class**: - Implement a base class `BaseDescriptor` with methods for getting and setting attribute values. - Methods: - `__set_name__(self, owner, name)`: Stores the name of the attribute. - `__get__(self, instance, owner)`: Retrieves the attribute value. - `__set__(self, instance, value)`: Sets the attribute value. 2. **ValidatedAttribute Class**: - Implement a subclass `ValidatedAttribute` that extends `BaseDescriptor`. - This class should include `validate` method for attribute validation. - Example validations to implement: - Ensure the value is a string with a minimum length. - Ensure the value is a non-negative integer. - Methods: - `validate(self, value)`: Abstract method to be overridden. - Implement specific validation logic within `__set__`. 3. **Product Class**: - Implement a class `Product` that uses `ValidatedAttribute` to enforce attribute constraints on: - `name` (string, minimum length 3). - `price` (non-negative float). - The class should be able to create instances, and any attempt to set invalid values should raise appropriate exceptions. Input and Output: - The attributes should raise a `ValueError` or `TypeError` with descriptive messages if the validation fails. - Example: - Attempting to set `name` to \\"ab\\" should raise `ValueError`: \\"Name should be at least 3 characters long.\\" - Attempting to set `price` to -10 should raise `ValueError`: \\"Price cannot be negative.\\" Constraints: - You must use descriptors for managing attribute access and validation. - Performance should be efficient, with validation checks occurring only during setting values. Example Usage: ```python class BaseDescriptor: # Implement BaseDescriptor here class ValidatedAttribute(BaseDescriptor): # Implement ValidatedAttribute here class Product: name = ValidatedAttribute(min_length=3) price = ValidatedAttribute(min_value=0.0) def __init__(self, name, price): self.name = name self.price = price # Testing the Product class try: p = Product(\\"ab\\", 100.0) except ValueError as e: print(e) # Output: Name should be at least 3 characters long. try: p = Product(\\"Apple\\", -50) except ValueError as e: print(e) # Output: Price cannot be negative. p = Product(\\"Apple\\", 50.0) print(p.name) # Output: Apple print(p.price) # Output: 50.0 ``` Deliverables: - Complete the implementation of `BaseDescriptor`, `ValidatedAttribute`, and `Product` classes. - Ensure your solution is robust and handles edge cases efficiently.","solution":"class BaseDescriptor: def __init__(self): self._name = None def __set_name__(self, owner, name): self._name = name def __get__(self, instance, owner): return instance.__dict__[self._name] def __set__(self, instance, value): instance.__dict__[self._name] = value class ValidatedAttribute(BaseDescriptor): def __init__(self, min_length=None, min_value=None): super().__init__() self.min_length = min_length self.min_value = min_value def validate(self, value): if self.min_length is not None: if not isinstance(value, str): raise TypeError(f\\"{self._name} must be a string.\\") if len(value) < self.min_length: raise ValueError(f\\"{self._name} should be at least {self.min_length} characters long.\\") if self.min_value is not None: if not isinstance(value, (int, float)): raise TypeError(f\\"{self._name} must be a number.\\") if value < self.min_value: raise ValueError(f\\"{self._name} cannot be negative.\\") def __set__(self, instance, value): self.validate(value) super().__set__(instance, value) class Product: name = ValidatedAttribute(min_length=3) price = ValidatedAttribute(min_value=0.0) def __init__(self, name, price): self.name = name self.price = price"},{"question":"You are required to create a Python utility that reads a WAV file, processes it to double its sample rate, and writes the result to a new WAV file. The goal is to ensure that the output WAV file plays at twice the speed of the original. Requirements: 1. **Read WAV File**: Using the `wave` module, read the input WAV file. 2. **Double the Sample Rate**: Adjust the sample rate to be double the original sample rate. 3. **Write to New WAV File**: Write the processed audio data to a new WAV file with the updated sample rate. Input: - A string `input_filename` representing the path to the original WAV file. - A string `output_filename` representing the path to the output WAV file. Output: - The function should write directly to `output_filename` and return `None`. Constraints: - The WAV files are guaranteed to be in PCM format. - Handle any exceptions that occur when reading from or writing to the file. - The operation should maintain the integrity of the audio data. Function Signature: ```python def process_wav_file(input_filename: str, output_filename: str) -> None: pass ``` Example: ```python process_wav_file(\\"input.wav\\", \\"output.wav\\") ``` Make sure to handle: 1. Reading the necessary parameters (channels, sample width, frame rate, number of frames) from the input WAV file. 2. Doubling the frame rate parameter. 3. Writing the frames to the output WAV file with the new frame rate. Notes: - Use the `wave` module\'s proper exception handling for any potential WAV file errors. - Ensure that the output WAV file\'s parameters match the input file\'s parameters except for the frame rate. Here is a starter template to get you going: ```python import wave def process_wav_file(input_filename: str, output_filename: str) -> None: try: # Open the input WAV file in read mode with wave.open(input_filename, \'rb\') as wav_in: params = wav_in.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params # Read all frames from the input file frames = wav_in.readframes(n_frames) # Open the output WAV file in write mode with wave.open(output_filename, \'wb\') as wav_out: # Set parameters for the output file, doubling the frame rate wav_out.setnchannels(n_channels) wav_out.setsampwidth(sampwidth) wav_out.setframerate(framerate * 2) # Doubling the frame rate here wav_out.writeframes(frames) except wave.Error as e: print(f\\"Error processing WAV files: {e}\\") # Example usage process_wav_file(\\"input.wav\\", \\"output.wav\\") ```","solution":"import wave def process_wav_file(input_filename: str, output_filename: str) -> None: try: # Open the input WAV file in read mode with wave.open(input_filename, \'rb\') as wav_in: params = wav_in.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params # Read all frames from the input file frames = wav_in.readframes(n_frames) # Open the output WAV file in write mode with wave.open(output_filename, \'wb\') as wav_out: # Set parameters for the output file, doubling the frame rate wav_out.setnchannels(n_channels) wav_out.setsampwidth(sampwidth) wav_out.setframerate(framerate * 2) # Doubling the frame rate here wav_out.writeframes(frames) except wave.Error as e: print(f\\"Error processing WAV files: {e}\\") # Example usage # process_wav_file(\\"input.wav\\", \\"output.wav\\")"},{"question":"**Problem Statement:** You are working on a data archiving system that processes and compresses logs generated by various services running on multiple servers. Your task is to write a Python script that uses the `zipfile` module to manage ZIP archives containing these logs. The script should meet the following requirements: 1. Create a new ZIP archive to store log files. 2. Support multiple compression methods: ZIP_STORED, ZIP_DEFLATED, ZIP_BZIP2, and ZIP_LZMA. 3. Add log files to the archive. 4. List the contents of the archive. 5. Extract all files from the archive to a specified directory. 6. Handle exceptions gracefully, ensuring the script does not crash for invalid operations. **Function Details:** 1. **create_zip_archive(output_zip: str, files: List[str], compression: str = \'ZIP_STORED\') -> None** - Creates a new ZIP archive with the specified compression method. - Parameters: - `output_zip`: The name of the output ZIP file. - `files`: A list of file paths to be added to the archive. - `compression`: The compression method; one of \'ZIP_STORED\', \'ZIP_DEFLATED\', \'ZIP_BZIP2\', \'ZIP_LZMA\'. Default is \'ZIP_STORED\'. - Returns: None 2. **list_zip_contents(zip_file: str) -> List[str]** - Lists the contents of the specified ZIP archive. - Parameters: - `zip_file`: The path to the ZIP file. - Returns: A list of file names contained in the ZIP archive. 3. **extract_zip_archive(zip_file: str, extract_to: str) -> None** - Extracts all files from the specified ZIP archive to the specified directory. - Parameters: - `zip_file`: The path to the ZIP file. - `extract_to`: The path to the directory where files should be extracted. - Returns: None **Constraints:** - Assume the paths provided as input are always valid. - Compression methods provided are always one of the specified values. - Handle any invalid ZIP operations, such as trying to read a non-existent file within the archive or an invalid ZIP file format, by printing an appropriate error message. **Examples:** 1. Creating a ZIP archive: ```python create_zip_archive(\'logs.zip\', [\'log1.txt\', \'log2.txt\'], \'ZIP_DEFLATED\') ``` 2. Listing contents of a ZIP archive: ```python contents = list_zip_contents(\'logs.zip\') print(contents) # Output: [\'log1.txt\', \'log2.txt\'] ``` 3. Extracting files from a ZIP archive: ```python extract_zip_archive(\'logs.zip\', \'/path/to/extract\') ``` **Additional Information:** The `zipfile` module has several classes and methods. Use the `ZipFile` class for creating, listing, and extracting the ZIP archives. Handle the exceptions `BadZipFile` and `LargeZipFile` for invalid or large ZIP files. You are encouraged to explore the zip documentation to understand the various options and features available. Implement the functions ensuring clear code structure and handling of edge cases appropriately.","solution":"import zipfile from typing import List def create_zip_archive(output_zip: str, files: List[str], compression: str = \'ZIP_STORED\') -> None: Creates a new ZIP archive with the specified compression method. Parameters: - output_zip: The name of the output ZIP file. - files: A list of file paths to be added to the archive. - compression: The compression method; one of \'ZIP_STORED\', \'ZIP_DEFLATED\', \'ZIP_BZIP2\', \'ZIP_LZMA\'. Default is \'ZIP_STORED\'. Returns: None compressions = { \'ZIP_STORED\': zipfile.ZIP_STORED, \'ZIP_DEFLATED\': zipfile.ZIP_DEFLATED, \'ZIP_BZIP2\': zipfile.ZIP_BZIP2, \'ZIP_LZMA\': zipfile.ZIP_LZMA } if compression not in compressions: print(f\\"Invalid compression method: {compression}\\") return try: with zipfile.ZipFile(output_zip, \'w\', compressions[compression]) as zipf: for file in files: zipf.write(file) except Exception as e: print(f\\"An error occurred: {e}\\") def list_zip_contents(zip_file: str) -> List[str]: Lists the contents of the specified ZIP archive. Parameters: - zip_file: The path to the ZIP file. Returns: A list of file names contained in the ZIP archive. try: with zipfile.ZipFile(zip_file, \'r\') as zipf: return zipf.namelist() except zipfile.BadZipFile: print(f\\"The file {zip_file} is not a valid zip file.\\") return [] except Exception as e: print(f\\"An error occurred: {e}\\") return [] def extract_zip_archive(zip_file: str, extract_to: str) -> None: Extracts all files from the specified ZIP archive to the specified directory. Parameters: - zip_file: The path to the ZIP file. - extract_to: The path to the directory where files should be extracted. Returns: None try: with zipfile.ZipFile(zip_file, \'r\') as zipf: zipf.extractall(path=extract_to) except zipfile.BadZipFile: print(f\\"The file {zip_file} is not a valid zip file.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Asyncio Task Management and Synchronization Problem Statement You are required to write a Python program that simulates a simplified version of a concurrent data processing system. The system involves multiple data producers and a data consumer. The producers generate data and place it into a shared queue, while the consumer processes the data from the queue. Additionally, you need to ensure that the consumer stops processing when all producers are finished and the queue is empty. Requirements 1. Implement an asynchronous function `producer(id: int, queue: asyncio.Queue) -> None`: - The function should generate ten data items, where each item is a tuple containing the producer ID and the item number (e.g., `(id, 1)`). - Use `asyncio.sleep()` to simulate a delay between data item generations. - Place each data item into the provided `queue` using `await queue.put()`. 2. Implement an asynchronous function `consumer(queue: asyncio.Queue, event: asyncio.Event) -> None`: - The function should continually check the queue for data items. - When data is available, it should process the data (simulated using `asyncio.sleep()`). - Print a message indicating the data has been processed (e.g., `Processed data: (1, 1)`). - The process should stop when the event is set and the queue is empty. 3. Implement a main function `async main() -> None`: - Create an `asyncio.Queue` instance to be shared by producers and the consumer. - Create a synchronization event using `asyncio.Event` to signal when the producers are done generating data. - Start multiple producer tasks (at least 3 producers). - Start one consumer task. - Use `await asyncio.gather()` to wait for all producer tasks to complete. - Set the event to signal the consumer that all producers are done. - Await the consumer task to ensure it finishes processing all items. 4. Ensure proper handling of cancellation and exceptions. Input and Output Formats - **Input:** None - **Output:** Print statements indicating the processing of data items. Example Output ``` Processed data: (0, 1) Processed data: (1, 1) Processed data: (2, 1) ... Processed data: (0, 10) Processed data: (1, 10) Processed data: (2, 10) All data processed! ``` Constraints - Use Python 3.10 and the asyncio library. - Ensure the solution is efficient and handles concurrency well. - The producers should generate data with some random delays. - The `consumer` should only stop after all items are processed and the event signal is set. Evaluation Criteria - Correctness: The solution should meet all specified requirements. - Efficiency: The program should handle concurrent task execution effectively. - Code Quality: Code should be well-structured, documented, and handle exceptions properly.","solution":"import asyncio import random async def producer(id: int, queue: asyncio.Queue) -> None: Asynchronous producer function generating 10 data items and putting them into the queue. Each data item is a tuple (id, item_number). for i in range(1, 11): item = (id, i) await asyncio.sleep(random.uniform(0.1, 0.5)) await queue.put(item) print(f\\"Producer {id} finished producing.\\") async def consumer(queue: asyncio.Queue, event: asyncio.Event) -> None: Asynchronous consumer function processing data items from the queue. Stops processing when the event is set and the queue is empty. while True: if event.is_set() and queue.empty(): break item = await queue.get() await asyncio.sleep(random.uniform(0.1, 0.3)) print(f\\"Processed data: {item}\\") print(\\"All data processed!\\") async def main() -> None: Main function to set up the queue, event, and tasks for producers and consumer. queue = asyncio.Queue() event = asyncio.Event() # Starting 3 producer tasks producer_tasks = [asyncio.create_task(producer(i, queue)) for i in range(3)] # Starting one consumer task consumer_task = asyncio.create_task(consumer(queue, event)) # Waiting for all producer tasks to complete await asyncio.gather(*producer_tasks) # Setting the event to signal the consumer that producers are done event.set() # Waiting for the consumer task to complete await consumer_task # Entry point if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"<|Analysis Begin|> The provided documentation indicates that the \\"concurrent\\" package has a single module: \\"concurrent.futures,\\" which is used for launching parallel tasks. To design a comprehensive coding assessment, I can focus on the following key areas related to the \\"concurrent.futures\\" module: - Launching and managing parallel tasks using threads or processes. - Handling exceptions in parallel tasks. - Synchronizing tasks and managing results. - Understanding how to properly use `ThreadPoolExecutor` and `ProcessPoolExecutor`. The question should require students to write code to demonstrate their understanding of running tasks in parallel, managing task execution, and handling potential issues such as timeouts and exceptions. <|Analysis End|> <|Question Begin|> In this coding assessment, you will demonstrate your understanding of the `concurrent.futures` module for launching parallel tasks in Python. # Problem Statement You are given a list of URLs, and you need to download the content of each URL in parallel. You should implement the function `download_contents(urls: List[str]) -> List[str]`, which takes a list of URLs as input and returns the content of each URL as a list of strings. Each URL content must be fetched concurrently. You must use the `concurrent.futures.ThreadPoolExecutor` to launch the parallel tasks. # Input - `urls` (List[str]): A list of URLs to download content from. The list will contain up to 1000 URLs. # Output - List[str]: A list containing the content of each URL, in the same order as the input URLs. # Constraints - You should handle exceptions that may occur while fetching the URLs. If a URL cannot be fetched, the corresponding entry in the output list should be `\\"Error\\"`. # Performance Requirements - Your implementation should efficiently manage parallelism to fetch URLs as quickly as possible. - You should not use more than 10 threads concurrently. # Example ```python from typing import List import concurrent.futures import requests def download_contents(urls: List[str]) -> List[str]: def fetch_content(url: str) -> str: try: response = requests.get(url, timeout=5) response.raise_for_status() return response.text except Exception as e: return \\"Error\\" contents = [] with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_content, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: contents.append(future.result()) except Exception as e: contents.append(\\"Error\\") return contents ``` # Notes - You can use the `requests` library to fetch the content of the URLs. - Ensure that the output list maintains the same order as the input list of URLs. - Handle all potential exceptions that could occur during the fetching process. Implement the `download_contents` function adhering to the requirements and constraints provided.","solution":"from typing import List import concurrent.futures import requests def download_contents(urls: List[str]) -> List[str]: def fetch_content(url: str) -> str: try: response = requests.get(url, timeout=5) response.raise_for_status() return response.text except Exception as e: return \\"Error\\" contents = [None] * len(urls) with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: future_to_index = {executor.submit(fetch_content, url): i for i, url in enumerate(urls)} for future in concurrent.futures.as_completed(future_to_index): index = future_to_index[future] try: contents[index] = future.result() except Exception as e: contents[index] = \\"Error\\" return contents"},{"question":"Coding Assessment Question **Objective**: Demonstrate competence in using Unix-specific services in Python. This includes system interactions through POSIX calls, accessing user and group databases, and managing resource limits. **Problem Statement**: Create a Python function `user_info_and_limits` that takes a username as input and performs the following actions: 1. Retrieves and prints the user\'s home directory and shell information using the `pwd` module. 2. Retrieves and prints all group names to which the user belongs using the `grp` module. 3. Sets a soft limit for the user\'s maximum number of open file descriptors to 1024 and verifies this change using the `resource` module. 4. Prints the current (soft) resource limits for the number of open file descriptors. # Requirements **Function Signature**: ```python def user_info_and_limits(username: str) -> None: pass ``` **Input**: - `username`: a string representing the username of the Unix user. **Output**: - The function prints the following details: 1. Home directory 2. Shell information 3. List of group names 4. Current soft limit for the maximum number of open file descriptors **Constraints**: - You can assume that the provided username exists in the system. - You should handle any exceptions that may arise during the function\'s execution gracefully. **Example**: Consider a user \\"johndoe\\" exists on the system. ```python user_info_and_limits(\\"johndoe\\") ``` The output should be similar to: ``` Home Directory: /home/johndoe Shell: /bin/bash Groups: [\'staff\', \'developers\', \'admins\'] Current Soft Limit for Open File Descriptors: 1024 ``` # Modules and Functions to Use: 1. `pwd.getpwnam(username)` to get user\'s home directory and shell information. 2. `grp.getgrgid()` and `os.getgrouplist(username, gid)` to get group information. 3. `resource.setrlimit(resource.RLIMIT_NOFILE, (1024, hard_limit))` to set resource limits. 4. `resource.getrlimit(resource.RLIMIT_NOFILE)` to verify resource limits. Implement the function keeping in mind efficient querying and error-handling mechanisms.","solution":"import pwd import grp import os import resource def user_info_and_limits(username: str) -> None: try: # Get user details user_info = pwd.getpwnam(username) home_directory = user_info.pw_dir shell = user_info.pw_shell # Get groups user belongs to groups = [grp.getgrgid(gid).gr_name for gid in os.getgrouplist(username, user_info.pw_gid)] # Set and get resource limits soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_NOFILE) resource.setrlimit(resource.RLIMIT_NOFILE, (1024, hard_limit)) new_soft_limit, _ = resource.getrlimit(resource.RLIMIT_NOFILE) # Print results print(f\\"Home Directory: {home_directory}\\") print(f\\"Shell: {shell}\\") print(f\\"Groups: {groups}\\") print(f\\"Current Soft Limit for Open File Descriptors: {new_soft_limit}\\") except KeyError as e: print(f\\"Error: User \'{username}\' not found.\\") except Exception as e: print(f\\"An unexpected error occurred: {str(e)}\\")"},{"question":"**Advanced Date and Time Manipulation and Time Zone Conversion with `datetime` Module** **Objective:** The goal of this exercise is to assess your ability to work with the `datetime` module, particularly focusing on creating and manipulating date/time objects, handling both \\"aware\\" and \\"naive\\" objects, and performing time zone conversions. **Task:** 1. **Create Custom `tzinfo` Subclass:** Create a custom `tzinfo` subclass called `CustomTimeZone` that represents a time zone with a fixed offset and a name. 2. **Datetime Operations:** Implement a function `datetime_operations(input_date_str, input_time_str, offset_hours, offset_minutes)` that: - Takes an input date and time in ISO format (`YYYY-MM-DD` for the date and `HH:MM:SS` for the time). - Creates a `datetime` object combining the provided date and time. - Converts it into an \\"aware\\" `datetime` object using the `CustomTimeZone` with the provided offset. - Returns a string of the converted \\"aware\\" `datetime` in ISO 8601 format. 3. **Time Zone Conversion:** Implement another function `convert_to_timezone(input_datetime_str, target_offset_hours, target_offset_minutes)` that: - Takes an input \\"aware\\" `datetime` object string in ISO 8601 format and a target time zone offset. - Parses the input string to create a `datetime` object. - Converts it to the target time zone using a `CustomTimeZone` with the target offset. - Returns a string of the converted `datetime` in ISO 8601 format. **Constraints:** - The input date and time strings are valid and follow the specified formats. - The offsets provided will fall within the range acceptable for `timedelta` objects (i.e., strictly less than 24 hours). - Performance should be efficient for date and time manipulations within a single execution run under typical workloads. **Example Usage:** ```python from datetime import datetime class CustomTimeZone(tzinfo): def __init__(self, offset_hours=0, offset_minutes=0, name=\\"UTC\\"): self.offset = timedelta(hours=offset_hours, minutes=offset_minutes) self.name = name def utcoffset(self, dt): return self.offset def tzname(self, dt): return self.name def dst(self, dt): return timedelta(0) def datetime_operations(input_date_str, input_time_str, offset_hours=0, offset_minutes=0): date_obj = datetime.fromisoformat(input_date_str) time_obj = datetime.strptime(input_time_str, \\"%H:%M:%S\\").time() dt = datetime.combine(date_obj, time_obj) aware_dt = dt.replace(tzinfo=CustomTimeZone(offset_hours, offset_minutes)) return aware_dt.isoformat() def convert_to_timezone(input_datetime_str, target_offset_hours=0, target_offset_minutes=0): aware_dt = datetime.fromisoformat(input_datetime_str) target_timezone = CustomTimeZone(target_offset_hours, target_offset_minutes) return aware_dt.astimezone(target_timezone).isoformat() # Example usage: print(datetime_operations(\\"2023-10-10\\", \\"14:30:00\\", offset_hours=-5, offset_minutes=-30)) print(convert_to_timezone(\\"2023-10-10T14:30:00-05:30\\", target_offset_hours=2, target_offset_minutes=0)) ``` **Expected Output:** ```python # Output of datetime_operations: \\"2023-10-10T14:30:00-05:30\\" # Output of convert_to_timezone: \\"2023-10-10T22:00:00+02:00\\" ``` **Notes:** - Ensure the methods for `CustomTimeZone` (such as `utcoffset`, `tzname`, and `dst`) are correctly implemented to handle the offset and name attributes. - Consider edge cases such as different offsets and how naive and aware conversions are handled.","solution":"from datetime import datetime, timedelta, tzinfo class CustomTimeZone(tzinfo): def __init__(self, offset_hours=0, offset_minutes=0, name=\\"UTC\\"): self.offset = timedelta(hours=offset_hours, minutes=offset_minutes) self.name = name def utcoffset(self, dt): return self.offset def tzname(self, dt): return self.name def dst(self, dt): return timedelta(0) def datetime_operations(input_date_str, input_time_str, offset_hours=0, offset_minutes=0): date_obj = datetime.fromisoformat(input_date_str) time_obj = datetime.strptime(input_time_str, \\"%H:%M:%S\\").time() dt = datetime.combine(date_obj, time_obj) aware_dt = dt.replace(tzinfo=CustomTimeZone(offset_hours, offset_minutes)) return aware_dt.isoformat() def convert_to_timezone(input_datetime_str, target_offset_hours=0, target_offset_minutes=0): aware_dt = datetime.fromisoformat(input_datetime_str) target_timezone = CustomTimeZone(target_offset_hours, target_offset_minutes) return aware_dt.astimezone(target_timezone).isoformat()"},{"question":"Objective The task is to demonstrate an understanding of the subprocess module in Python, focusing on process creation, interaction with subprocesses, input/output/error handling, and exception handling. Problem Statement You need to implement a function `execute_shell_command()` that will take a shell command as input, execute it using the subprocess module, and return the output or error messages. Additionally, the function should handle possible exceptions such as timeouts and non-zero exit statuses. Function Signature ```python def execute_shell_command(command: str, timeout: int = 10) -> dict: Execute a shell command and capture its output and errors. Args: - command (str): The shell command to execute. - timeout (int): The maximum time in seconds to wait for the command to complete. Default is 10 seconds. Returns: - dict: A dictionary containing: - \'stdout\' (str): Captured standard output. - \'stderr\' (str): Captured standard error. - \'returncode\' (int): The return code of the command. - \'error\' (str): Error message if any exception occurred, otherwise None. Raises: - ValueError: If the command is empty. pass ``` Constraints - The `command` must not be empty. If it is, the function should raise a `ValueError`. - The function should use `subprocess.run()`. - If the command takes longer than `timeout` seconds to complete, a `subprocess.TimeoutExpired` exception should be handled, and appropriate error information should be returned. - If the command exits with a non-zero status, the error should be captured and returned. - Ensure that the security implications of using `shell=True` are handled properly. Examples 1. **Example 1: Basic command execution** ```python result = execute_shell_command(\\"echo Hello\\") ``` **Expected Output:** ```python { \'stdout\': \'Hellon\', \'stderr\': \'\', \'returncode\': 0, \'error\': None } ``` 2. **Example 2: Command with non-zero exit status** ```python result = execute_shell_command(\\"exit 1\\") ``` **Expected Output:** ```python { \'stdout\': \'\', \'stderr\': \'\', \'returncode\': 1, \'error\': \'Command returned non-zero exit status 1.\' } ``` 3. **Example 3: Command with timeout** ```python result = execute_shell_command(\\"sleep 15\\", timeout=2) ``` **Expected Output:** ```python { \'stdout\': \'\', \'stderr\': \'\', \'returncode\': None, \'error\': \'Command timed out.\' } ``` 4. **Example 4: Invalid command** ```python result = execute_shell_command(\\"\\") ``` **Expected Output:** ```python ValueError: Command must not be empty. ``` Additional Notes - Utilize subprocess.run() options effectively to handle input, output, errors, and timeouts. - Security Consideration: Since `shell=True` is used in `subprocess.run()`, ensure that special characters and whitespaces are handled safely to avoid shell injection vulnerabilities. - Consider using `shlex.split()` for proper tokenization of the command string.","solution":"import subprocess def execute_shell_command(command: str, timeout: int = 10) -> dict: if not command: raise ValueError(\\"Command must not be empty.\\") try: result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True, timeout=timeout) return { \'stdout\': result.stdout, \'stderr\': result.stderr, \'returncode\': result.returncode, \'error\': None } except subprocess.CalledProcessError as e: return { \'stdout\': e.stdout, \'stderr\': e.stderr, \'returncode\': e.returncode, \'error\': f\\"Command returned non-zero exit status {e.returncode}.\\" } except subprocess.TimeoutExpired: return { \'stdout\': \'\', \'stderr\': \'\', \'returncode\': None, \'error\': \'Command timed out.\' } except Exception as e: return { \'stdout\': \'\', \'stderr\': \'\', \'returncode\': None, \'error\': str(e) }"},{"question":"**Discrete Fourier Transform in Signal Processing** In this assessment, you are required to demonstrate your understanding of the `torch.fft` module by implementing a function that processes a 1D signal using the Fast Fourier Transform (FFT). The task is to filter out specific frequency components from a given signal. # Problem Statement Implement a function `filter_signal` which filters out high-frequency components from a given 1D signal using FFT. The function should: 1. Take as input: - `signal` (torch.Tensor): The input 1D signal tensor with shape `(N,)`, where `N` is the number of samples. - `threshold` (float): The frequency threshold. Any frequency component above this threshold should be filtered out. 2. Return as output: - `filtered_signal` (torch.Tensor): The filtered 1D signal tensor with the same shape as the input. # Methodology 1. Compute the FFT of the input signal. 2. Compute the corresponding frequency components using `torch.fft.fftfreq`. 3. Zero out the frequency components that have an absolute value greater than the given threshold. 4. Compute the inverse FFT to obtain the filtered signal. # Constraints - The input signal tensor `signal` is guaranteed to be a 1-dimensional tensor of floats. - The `threshold` is a non-negative float. - The function should handle edge cases, such as when `N` is small or when the threshold is very low or very high. # Example ```python import torch import torch.fft def filter_signal(signal: torch.Tensor, threshold: float) -> torch.Tensor: # Step 1: Compute the FFT of the input signal freq_signal = torch.fft.fft(signal) # Step 2: Compute the frequency components freqs = torch.fft.fftfreq(signal.size(0)) # Step 3: Zero out the high-frequency components mask = torch.abs(freqs) > threshold freq_signal[mask] = 0 # Step 4: Compute the inverse FFT to obtain the filtered signal filtered_signal = torch.fft.ifft(freq_signal) # Since the input signal is real, output the real part of the inverse FFT return filtered_signal.real # Example usage signal = torch.tensor([0, 1, 0, -1], dtype=torch.float32) threshold = 0.1 filtered_signal = filter_signal(signal, threshold) print(filtered_signal) ``` # Notes - Ensure that the function uses PyTorch\'s `torch.fft` module effectively. - Make sure to handle cases where the signal length `N` is even or odd differently if needed. - Pay attention to the computational complexity of the FFT operations. Ensure that the function is efficient.","solution":"import torch import torch.fft def filter_signal(signal: torch.Tensor, threshold: float) -> torch.Tensor: Filters out high-frequency components from a given 1D signal using FFT. Parameters: signal (torch.Tensor): The input 1D signal tensor with shape (N,). threshold (float): The frequency threshold. Any frequency component above this threshold should be filtered out. Returns: torch.Tensor: The filtered 1D signal tensor with the same shape as the input. # Step 1: Compute the FFT of the input signal freq_signal = torch.fft.fft(signal) # Step 2: Compute the frequency components freqs = torch.fft.fftfreq(signal.size(0)) # Step 3: Zero out the high-frequency components mask = torch.abs(freqs) > threshold freq_signal[mask] = 0 # Step 4: Compute the inverse FFT to obtain the filtered signal filtered_signal = torch.fft.ifft(freq_signal) # Since the input signal is real, output the real part of the inverse FFT return filtered_signal.real"},{"question":"Coding Assessment Question # Objective To assess the student\'s understanding of PyTorch\'s distributed RPC framework, specifically the Remote Reference (RRef) protocol, by requiring them to create, use, and manage RRefs correctly across multiple workers. # Problem Statement You have been tasked with implementing a distributed computation using PyTorch\'s RPC framework. In this task, you need to: 1. **Initialize an RPC Framework**: Set up the required number of workers. 2. **Create and Use RRefs**: Create an RRef on one worker and use it on multiple other workers. 3. **Ensure Correct Deletion**: Manage the RRefs so they are deleted correctly once they are no longer needed. # Requirements 1. **Initialization**: - Initialize an RPC framework with three workers: \'worker0\', \'worker1\', and \'worker2\'. 2. **RRef Creation and Usage**: - On \'worker0\', create an RRef that holds a tensor initialized to ones. - Pass this RRef to \'worker1\' and \'worker2\' using `rpc_async`. - On \'worker1\' and \'worker2\', retrieve the tensor from the RRef, perform some operations on it, and return the results to \'worker0\'. 3. **Deletion**: - Ensure the RRef is correctly deleted once all operations are complete and no references to it exist. # Constraints - Ensure all operations are asynchronous using `rpc_async`. - Implement appropriate error handling for network failures. # Input and Output - **Input**: None (Worker initialization and RRef creation should be configured within the code). - **Output**: Print the results of the operations performed on \'worker1\' and \'worker2\'. # Example ```python import torch import torch.distributed.rpc as rpc def compute_operation(rref): tensor = rref.to_here() result = tensor + 2 return result def main(): # Initialize the RPC framework rpc.init_rpc(\\"worker0\\", rank=0, world_size=3) rpc.init_rpc(\\"worker1\\", rank=1, world_size=3) rpc.init_rpc(\\"worker2\\", rank=2, world_size=3) if rpc.get_worker_info().name == \\"worker0\\": # Create an RRef on worker0 rref = rpc.remote(\\"worker0\\", torch.ones, args=(2,)) # Use the RRef on worker1 and worker2 fut1 = rpc.rpc_async(\\"worker1\\", compute_operation, args=(rref,)) fut2 = rpc.rpc_async(\\"worker2\\", compute_operation, args=(rref,)) # Get results result1 = fut1.wait() result2 = fut2.wait() print(f\\"Result from worker1: {result1}\\") print(f\\"Result from worker2: {result2}\\") rpc.shutdown() if __name__ == \\"__main__\\": main() ``` # Submission Submit your solution as a Python script file named `distributed_rref.py`. Note: This script should be designed to run in a distributed environment, so make sure to check how to run distributed RPC in PyTorch. Also, ensure to handle any transient network failures as per the assumptions outlined in the documentation.","solution":"import torch import torch.distributed.rpc as rpc def compute_operation(rref): tensor = rref.to_here() result = tensor + 2 return result def main(): # Initialize the RPC framework options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=16) rpc.init_rpc(\\"worker0\\", rank=0, world_size=3, rpc_backend_options=options) rpc.init_rpc(\\"worker1\\", rank=1, world_size=3, rpc_backend_options=options) rpc.init_rpc(\\"worker2\\", rank=2, world_size=3, rpc_backend_options=options) # Ensuring all workers are initialized if rpc.get_worker_info().name == \\"worker0\\": # Create an RRef on worker0 rref = rpc.remote(\\"worker0\\", torch.ones, args=(2,)) # Use the RRef on worker1 and worker2 fut1 = rpc.rpc_async(\\"worker1\\", compute_operation, args=(rref,)) fut2 = rpc.rpc_async(\\"worker2\\", compute_operation, args=(rref,)) # Get results result1 = fut1.wait() result2 = fut2.wait() print(f\\"Result from worker1: {result1}\\") print(f\\"Result from worker2: {result2}\\") # Shutdown the framework rpc.shutdown() if __name__ == \\"__main__\\": main()"},{"question":"Objective Write a Python function `function_inspector` that takes another function as input and returns a dictionary containing the following details about the input function: - The code object associated with the function. - The globals dictionary associated with the function. - The module name where the function is defined. - The default values for the function\'s arguments. - The closure associated with the function. - The annotations of the function. Function Signature ```python def function_inspector(func: types.FunctionType) -> dict: ... ``` Input - **func**: A function object (of type `types.FunctionType`). Output - A dictionary with the following keys and corresponding values: - `\\"code\\"`: The code object associated with the function. - `\\"globals\\"`: The globals dictionary associated with the function. - `\\"module\\"`: The module name where the function is defined. - `\\"defaults\\"`: A tuple of default values for the function\'s arguments or `None`. - `\\"closure\\"`: A tuple of cell objects that make up the function\'s closure or `None`. - `\\"annotations\\"`: A dictionary of annotations for the function\'s arguments or `None`. Constraints - Assume the input function will always be a valid function object. - You must use only Python\'s standard libraries. Example ```python import types def example_function(x: int, y: str = \\"default\\") -> str: return y * x result = function_inspector(example_function) expected = { \\"code\\": example_function.__code__, \\"globals\\": example_function.__globals__, \\"module\\": example_function.__module__, \\"defaults\\": example_function.__defaults__, \\"closure\\": example_function.__closure__, \\"annotations\\": example_function.__annotations__ } assert result == expected ``` **Note:** Ensure your implementation accurately retrieves and returns the specified attributes of the function.","solution":"import types def function_inspector(func: types.FunctionType) -> dict: Returns a dictionary containing details about the input function. return { \\"code\\": func.__code__, \\"globals\\": func.__globals__, \\"module\\": func.__module__, \\"defaults\\": func.__defaults__, \\"closure\\": func.__closure__, \\"annotations\\": func.__annotations__ }"},{"question":"# Question: Advanced File Finder using Glob Module You are tasked with developing a function `advanced_file_finder` that utilizes the `glob` module to locate files in a given directory based on specific criteria. Your function should allow flexibility in pattern matching, directory traversal, and result customization. Function Signature ```python def advanced_file_finder(pattern: str, root_dir: str = None, recursive: bool = False, return_absolute: bool = False) -> List[str]: pass ``` Parameters - `pattern` (str): The pattern to match files against. This can include characters like \\"*\\", \\"?\\", and character ranges \\"[]\\". - `root_dir` (str, optional): The root directory for the search. If not provided (`None`), the current directory will be used. - `recursive` (bool, optional): If `True`, the function should search recursively through subdirectories. Default is `False`. - `return_absolute` (bool, optional): If `True`, the function should return absolute paths of the matched files. If `False`, it returns relative paths. Default is `False`. Returns - `List[str]`: A sorted list of file paths that match the specified pattern and criteria. Constraints - Only use the `glob` module for matching files. Do not use other filesystem traversal libraries. - The function should handle both absolute and relative `root_dir` paths. - The function should appropriately handle wildcard patterns and special characters within filenames. Examples ```python # Example 1: Simple pattern matching print(advanced_file_finder(\'*.txt\')) # Possible output: [\'example1.txt\', \'example2.txt\'] # Example 2: Recursive search with absolute paths print(advanced_file_finder(\'**/*.py\', recursive=True, return_absolute=True)) # Possible output: [\'/home/user/project/script1.py\', \'/home/user/project/subdir/script2.py\'] # Example 3: Different root directory print(advanced_file_finder(\'*.gif\', root_dir=\'/home/user/images\')) # Possible output: [\'image1.gif\', \'image2.gif\'] ``` Your implementation should correctly handle edge cases, such as files with unusual characters, non-existing directories, and files starting with a dot. # Notes - Be sure to handle performance optimally, especially when dealing with large directory trees and recursive searches. - Include error handling to manage potential issues like inaccessible directories.","solution":"import glob import os from typing import List def advanced_file_finder(pattern: str, root_dir: str = None, recursive: bool = False, return_absolute: bool = False) -> List[str]: Finds files matching a given pattern in a specified directory. Parameters: pattern (str): Pattern to match files against. root_dir (str, optional): Directory to start search from. Defaults to current directory. recursive (bool, optional): If True, searches recursively. Defaults to False. return_absolute (bool, optional): If True, returns absolute paths. Defaults to False. Returns: List[str]: Sorted list of matched file paths. if root_dir is None: root_dir = os.getcwd() # Construct the search pattern if recursive: search_pattern = os.path.join(root_dir, \'**\', pattern) else: search_pattern = os.path.join(root_dir, pattern) # Use glob to find files matched_files = glob.glob(search_pattern, recursive=recursive) # Convert to absolute paths if required if return_absolute: matched_files = [os.path.abspath(f) for f in matched_files] # Sort the results matched_files.sort() return matched_files"},{"question":"Objective To assess your understanding of scikit-learn\'s utility functions for validation, random sampling, and efficient linear algebra operations. Problem Statement You are tasked with writing a function that processes a given dataset by performing the following operations: 1. Validates the input data to ensure that it is a 2D array with finite values. 2. Generates a random sample from the dataset without replacement. 3. Computes a randomized truncated Singular Value Decomposition (SVD) on the sampled dataset. Function Signature ```python def process_dataset(data: np.ndarray, sample_size: int, n_components: int, random_state: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Parameters: data (np.ndarray): The input dataset, expected to be a 2D array. sample_size (int): The number of samples to draw from the dataset. n_components (int): The number of singular values and vectors to compute. random_state (int, optional): The seed for random number generation. Defaults to None. Returns: Tuple[np.ndarray, np.ndarray, np.ndarray]: The U, Sigma, and VT matrices from the randomized SVD. pass ``` Requirements 1. **Validate Input:** - Use `check_array` to ensure `data` is a 2D array with finite values. 2. **Random Sampling:** - Use `sample_without_replacement` to randomly sample `sample_size` rows from `data`. - Handle the case where `sample_size` is greater than the number of rows in `data`. 3. **Randomized SVD:** - Use `randomized_svd` to compute the `n_components` singular values and vectors from the sampled data. - Ensure the random number generation is reproducible using the provided `random_state`. Example Input: ```python data = np.random.random((100, 50)) sample_size = 30 n_components = 5 random_state = 42 ``` Output: A tuple containing three matrices (U, Sigma, VT) representing the truncated SVD of the sampled data. **Constraints:** - The input data array `data` must have at least as many rows as `sample_size`. - `n_components` should be less than or equal to the number of columns in the sampled data. - Use the provided scikit-learn utilities where applicable. **Performance Requirements:** - The function should efficiently handle moderately large datasets, e.g., datasets with up to 10,000 rows and 1,000 columns. Notes: - You may use Numpy functions for basic array manipulations. - Ensure code readability and add comments to explain key operations.","solution":"import numpy as np from sklearn.utils import check_array from sklearn.utils.random import sample_without_replacement from sklearn.utils.extmath import randomized_svd from typing import Tuple def process_dataset(data: np.ndarray, sample_size: int, n_components: int, random_state: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Processes the given dataset by performing validation, random sampling, and randomized SVD. Parameters: data (np.ndarray): The input dataset, expected to be a 2D array. sample_size (int): The number of samples to draw from the dataset. n_components (int): The number of singular values and vectors to compute. random_state (int, optional): The seed for random number generation. Defaults to None. Returns: Tuple[np.ndarray, np.ndarray, np.ndarray]: The U, Sigma, and VT matrices from the randomized SVD. # Step 1: Validate the input data to ensure it is a 2D array with finite values. data = check_array(data, ensure_2d=True, allow_nd=False, force_all_finite=True) # Step 2: Generate a random sample from the data without replacement. # If sample_size is greater than the number of rows, adjust sample_size to max possible rows. sample_size = min(sample_size, data.shape[0]) sampled_indices = sample_without_replacement(n_population=data.shape[0], n_samples=sample_size, random_state=random_state) sampled_data = data[sampled_indices] # Step 3: Compute randomized truncated SVD on the sampled data. U, Sigma, VT = randomized_svd(sampled_data, n_components=n_components, random_state=random_state) return U, Sigma, VT"},{"question":"# Persistent Storage with Shelve We want you to demonstrate your understanding of the `shelve` module by creating and managing a persistent, dictionary-like object. You will write a function `manage_phonebook` that will allow the addition, retrieval, updating, and deletion of contact details in a phonebook. The phonebook should persist data across multiple executions of your script. Steps and Requirements: 1. **Function Definition:** ```python def manage_phonebook(filename: str, action: str, name: str = None, number: str = None) -> str: ``` 2. **Parameters:** * `filename` (str): The file name to store the shelf. * `action` (str): Specifies the action to be performed. It can be `\'add\'`, `\'get\'`, `\'update\'`, or `\'delete\'`. * `name` (str): The name of the contact (Only needed for `\'add\'`, `\'get\'`, `\'update\'`, and `\'delete\'` actions). * `number` (str): The phone number of the contact (Only needed for `\'add\'` and `\'update\'` actions). 3. **Function Behavior:** - If the `action` is `\'add\'`, add the contact with the provided `name` and `number` to the phonebook. - If the `action` is `\'get\'`, retrieve the contact number for the provided `name`. If the contact does not exist, return `\\"Contact not found.\\"`. - If the `action` is `\'update\'`, update the existing contact with the given `name` to the new `number`. - If the `action` is `\'delete\'`, delete the contact with the specified `name` from the phonebook. If the contact does not exist, return `\\"Contact not found.\\"`. 4. **Return Values:** - For `\'add\'` and `\'update\'` actions, return a success message: `\\"Contact added.\\"` or `\\"Contact updated.\\"`. - For the `\'get\'` action, return the contact number or `\\"Contact not found.\\"` if the contact does not exist. - For the `\'delete\'` action, return `\\"Contact deleted.\\"` or `\\"Contact not found.\\"` if the contact does not exist. 5. **Constraints and Notes:** - Ensure the phonebook is correctly saved and closed after each operation to persist the data. - Use the `writeback=True` parameter when opening the shelf for easier mutation of stored objects. - Handle potential errors gracefully, such as attempting to retrieve or delete a non-existent contact. 6. **Example Usage:** ```python # Adding a contact print(manage_phonebook(\'phonebook.db\', \'add\', \'Alice\', \'12345\')) # Output: Contact added. # Retrieving a contact print(manage_phonebook(\'phonebook.db\', \'get\', \'Alice\')) # Output: 12345 # Updating a contact print(manage_phonebook(\'phonebook.db\', \'update\', \'Alice\', \'67890\')) # Output: Contact updated. # Deleting a contact print(manage_phonebook(\'phonebook.db\', \'delete\', \'Alice\')) # Output: Contact deleted. ``` Implement the `manage_phonebook` function using the `shelve` module ensuring the phonebook persists across multiple executions of your script.","solution":"import shelve def manage_phonebook(filename: str, action: str, name: str = None, number: str = None) -> str: with shelve.open(filename, writeback=True) as phonebook: if action == \'add\': phonebook[name] = number return \\"Contact added.\\" elif action == \'get\': return phonebook.get(name, \\"Contact not found.\\") elif action == \'update\': if name in phonebook: phonebook[name] = number return \\"Contact updated.\\" else: return \\"Contact not found.\\" elif action == \'delete\': if name in phonebook: del phonebook[name] return \\"Contact deleted.\\" else: return \\"Contact not found.\\" else: return \\"Invalid action.\\""},{"question":"You are tasked to design a program that processes images using color space conversions provided by the \\"colorsys\\" module. For a given set of pixel values in RGB format, the program must perform the following: 1. Convert the RGB values to both HLS and HSV color spaces. 2. Modify the HLS and HSV values in a specified way (details below). 3. Convert the modified HLS and HSV values back to RGB. 4. Verify and demonstrate that the conversions are correct and that the modifications produce the expected results. # Task 1. Write a function `process_image_pixels` that takes in a list of pixel values in RGB format and performs conversions as described above. 2. The function should accept the list of pixel values in the format `[ (r1, g1, b1), (r2, g2, b2), ... ]` where each tuple `(rn, gn, bn)` represents a pixel\'s RGB values which are float values between 0 and 1. 3. Modify the HLS values by increasing the Lightness (L) by 0.1 if it\'s less than 0.9, or setting it to 1.0 otherwise. 4. Modify the HSV values by decreasing the Saturation (S) by 0.1 if it\'s greater than 0.1, or setting it to 0.0 otherwise. 5. Convert the modified HLS and HSV values back to RGB. 6. Return a dictionary containing the original, modified HLS, and modified HSV pixel values. # Function Signature ```python def process_image_pixels(pixels: [(float, float, float)]) -> dict: pass ``` # Input Constraints - 0.0 <= rn, gn, bn <= 1.0 for each element in `pixels` - The input list `pixels` can contain between 1 and 10^5 pixel tuples. # Output Format The output should be a dictionary with the following keys and formats: - `\\"original\\"`: List of original pixel values `[ (r1, g1, b1), (r2, g2, b2), ... ]` - `\\"modified_hls\\"`: List of modified HLS-converted back to RGB pixel values `[ (r1, g1, b1), (r2, g2, b2), ... ]` - `\\"modified_hsv\\"`: List of modified HSV-converted back to RGB pixel values `[ (r1, g1, b1), (r2, g2, b2), ... ]` # Example ```python pixels = [(0.2, 0.4, 0.4), (0.5, 0.5, 0.5)] result = process_image_pixels(pixels) print(result) ``` Output: ```python { \\"original\\": [(0.2, 0.4, 0.4), (0.5, 0.5, 0.5)], \\"modified_hls\\": [(0.3, 0.5, 0.5), (0.6, 0.6, 0.6)], \\"modified_hsv\\": [(0.1, 0.36, 0.36), (0.4, 0.4, 0.4)] } ``` Make sure to test and validate the function using different sets of input pixel values to ensure correctness.","solution":"import colorsys def process_image_pixels(pixels): Process image pixels by converting between RGB, HLS, and HSV color spaces, modifying HLS and HSV values, and converting back to RGB. Args: - pixels (list): List of tuples representing RGB pixel values (0..1 range). Returns: - dict: Dictionary containing original, modified HLS and HSV pixel values. original = pixels modified_hls = [] modified_hsv = [] for r, g, b in pixels: # Convert RGB to HLS and modify Lightness h, l, s = colorsys.rgb_to_hls(r, g, b) l_modified = l + 0.1 if l < 0.9 else 1.0 r_hls, g_hls, b_hls = colorsys.hls_to_rgb(h, l_modified, s) modified_hls.append((r_hls, g_hls, b_hls)) # Convert RGB to HSV and modify Saturation h, s, v = colorsys.rgb_to_hsv(r, g, b) s_modified = s - 0.1 if s > 0.1 else 0.0 r_hsv, g_hsv, b_hsv = colorsys.hsv_to_rgb(h, s_modified, v) modified_hsv.append((r_hsv, g_hsv, b_hsv)) return { \\"original\\": original, \\"modified_hls\\": modified_hls, \\"modified_hsv\\": modified_hsv }"},{"question":"# Question: Background: You are tasked with creating a scraper that fetches data from multiple URLs concurrently. Each URL returns a different piece of data which needs to be processed within a certain timeout period. The scraper should handle the following tasks asynchronously: 1. Fetch data from multiple URLs concurrently. 2. Ensure that no individual URL fetch takes longer than a specified timeout. 3. Process the fetched data. Requirements: 1. Define an asynchronous function `fetch_data(url)` that: - Uses the `aiohttp` library to perform an HTTP GET request to the given URL. - Returns the response content as a string. - Logs and raises an appropriate exception if the request takes longer than the timeout. 2. Define an asynchronous function `process_data(data)` that: - Simulates processing by sleeping for a short random duration. - Returns the processed data (for simplicity, it can return the length of the data). 3. Define an asynchronous function `main(urls, timeout)` that: - Uses `asyncio.gather` to fetch data from all URLs concurrently with timeout handling. - Processes each piece of fetched data. - Returns a dictionary of URLs and their corresponding processed data lengths. 4. Log relevant progress and error messages for transparency. Constraints: - Use asyncio\'s `asyncio.gather` and `asyncio.wait_for`. - Handle potential timeout exceptions cleanly. - Ensure concurrent execution of fetch operations. Example Usage: ```python import asyncio urls = [ \\"https://example.com/data1\\", \\"https://example.com/data2\\", \\"https://example.com/data3\\" ] timeout = 5 # seconds async def fetch_data(url): # Your implementation here pass async def process_data(data): # Your implementation here pass async def main(urls, timeout): # Your implementation here # Entry point if __name__ == \\"__main__\\": asyncio.run(main(urls, timeout)) ``` Expected Output: A dictionary containing URLs as keys and processed data lengths as values. For example: ```python { \\"https://example.com/data1\\": 123, \\"https://example.com/data2\\": 456, \\"https://example.com/data3\\": 789 } ``` Note: Ensure to install the `aiohttp` library before running the code. ```bash pip install aiohttp ```","solution":"import aiohttp import asyncio import logging import random logging.basicConfig(level=logging.INFO) async def fetch_data(url, timeout): Fetch data from the given URL with a specified timeout. try: async with aiohttp.ClientSession() as session: async with session.get(url, timeout=timeout) as response: response.raise_for_status() data = await response.text() logging.info(f\\"Fetched data from {url}\\") return data except asyncio.TimeoutError: logging.error(f\'Timeout while fetching data from {url}\') raise except aiohttp.ClientError as e: logging.error(f\'Error fetching data from {url}: {str(e)}\') raise async def process_data(data): Simulate processing of the fetched data. await asyncio.sleep(random.uniform(0.1, 1.0)) processed_data = len(data) logging.info(f\\"Processed data of length {processed_data}\\") return processed_data async def main(urls, timeout): Fetch and process data for a list of URLs concurrently. Returns a dictionary of URLs and their processed data lengths. results = {} tasks = [asyncio.wait_for(fetch_data(url, timeout), timeout=timeout) for url in urls] fetch_results = await asyncio.gather(*tasks, return_exceptions=True) for url, result in zip(urls, fetch_results): if isinstance(result, Exception): results[url] = None else: processed_result = await process_data(result) results[url] = processed_result return results # Entry point for example usage if __name__ == \\"__main__\\": urls = [ \\"https://jsonplaceholder.typicode.com/posts/1\\", \\"https://jsonplaceholder.typicode.com/posts/2\\", \\"https://jsonplaceholder.typicode.com/posts/3\\" ] timeout = 5 # seconds # Running the main function and printing the result result = asyncio.run(main(urls, timeout)) print(result)"},{"question":"You have been provided with a CSV file `sales_data.csv` containing monthly sales data for two different products â€“ Product A and Product B. The CSV file has the following columns: \\"Month\\", \\"Product_A_Sales\\", \\"Product_B_Sales\\". Your task is to analyze this data using seaborn\'s plotting context to compare the sales of both products over the months, applying different styling contexts to your plots. Requirements: 1. Read the data from the CSV file `sales_data.csv`. 2. Generate and display a line plot for Product A and Product B sales on a shared x-axis (Month). Use different seaborn plotting contexts (\\"paper\\", \\"notebook\\", \\"talk\\", and \\"poster\\") to style the plots. 3. Create subplots for each context and compare how the visual appearance of the plots changes across these contexts. 4. Label the plots appropriately (i.e., axes labels, title). 5. Ensure that the plots are clear and well-presented for easy comparison. Constraints: - Use seaborn for plotting. - Ensure the plots are displayed within the same figure for easy visual comparison. Input: The input will be read from a CSV file `sales_data.csv`. Example structure of `sales_data.csv`: ``` Month,Product_A_Sales,Product_B_Sales Jan,200,150 Feb,220,130 Mar,250,170 ... ``` Output: Display a figure with four subplots, each subplot corresponding to a different seaborn plotting context. Code Implementation: You need to implement a function `compare_sales_data()` that performs the above tasks. ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def compare_sales_data(): # Read the data from the CSV file data = pd.read_csv(\'sales_data.csv\') contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] fig, axs = plt.subplots(2, 2, figsize=(15, 10)) for ax, context in zip(axs.flatten(), contexts): with sns.plotting_context(context): sns.lineplot(data=data, x=\'Month\', y=\'Product_A_Sales\', ax=ax, label=\'Product A\') sns.lineplot(data=data, x=\'Month\', y=\'Product_B_Sales\', ax=ax, label=\'Product B\') ax.set_title(f\'Sales Data in {context} Context\') ax.set_xlabel(\'Month\') ax.set_ylabel(\'Sales\') plt.tight_layout() plt.show() # Invoke the function to display the plots compare_sales_data() ``` Evaluation Criteria: 1. Correctness of the code. 2. Proper use of seaborn plotting contexts. 3. Clarity and visual appeal of the plots. 4. Appropriate use of subplots for comparison.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def compare_sales_data(): # Read the data from the CSV file data = pd.read_csv(\'sales_data.csv\') contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] fig, axs = plt.subplots(2, 2, figsize=(15, 10)) for ax, context in zip(axs.flatten(), contexts): with sns.plotting_context(context): sns.lineplot(data=data, x=\'Month\', y=\'Product_A_Sales\', ax=ax, label=\'Product A\') sns.lineplot(data=data, x=\'Month\', y=\'Product_B_Sales\', ax=ax, label=\'Product B\') ax.set_title(f\'Sales Data in {context.capitalize()} Context\') ax.set_xlabel(\'Month\') ax.set_ylabel(\'Sales\') ax.legend() plt.tight_layout() plt.show()"},{"question":"Custom Slicing Simulation **Objective:** Write a function in Python that mimics the behavior of a slice object for a custom sequence implementation. This function should handle the extraction and application of slice indices similar to the methods described in the documentation. You should not use the built-in Python slicing mechanism (`[start:stop:step]`) but should implement the equivalent functionality manually. **Function Signature:** ```python def custom_slice(sequence, start, stop, step): Mimic slice behavior for a custom sequence. :param sequence: List or similar sequence type. :param start: Starting index of the slice. :param stop: Stopping index of the slice. :param step: Step/increment of the slice. :return: A sliced portion of the sequence based on start, stop, and step. pass ``` **Input:** - `sequence`: A sequence (e.g., list) of elements, with length `n` where `1 <= n <= 10^5`. - `start`: An integer indicating the starting index of the slice. Can be `None` to Default to the start of the sequence. - `stop`: An integer indicating the stopping index of the slice. Can be `None` to Default to the end of the sequence. - `step`: An integer indicating the step/increment of the slice. Can be `None` to default to 1. **Output:** - A list containing elements from the input `sequence` as specified by the `start`, `stop`, and `step` parameters. **Constraints:** - You must handle cases where `start`, `stop`, or `step` can be `None` and should conform to the same behavior as Python\'s default slice object. - `start`, `stop`, and `step` should be bounded within the valid range of indices for the given sequence. - Negative indices should be supported, as they do in Python slicing. **Examples:** ```python # Example 1 sequence = [10, 20, 30, 40, 50] custom_slice(sequence, 1, 4, 1) # Output: [20, 30, 40] # Example 2 sequence = [10, 20, 30, 40, 50] custom_slice(sequence, 0, 5, 2) # Output: [10, 30, 50] # Example 3 sequence = [10, 20, 30, 40, 50] custom_slice(sequence, None, None, -1) # Output: [50, 40, 30, 20, 10] ``` # Tips: - Pay attention to how negative indices and step values are handled. - Ensure to handle out-of-range indices by clipping them appropriately. - Aim for a solution with a time complexity of O(n) where n is the length of the `sequence`.","solution":"def custom_slice(sequence, start, stop, step): Mimic slice behavior for a custom sequence. :param sequence: List or similar sequence type. :param start: Starting index of the slice. :param stop: Stopping index of the slice. :param step: Step/increment of the slice. :return: A sliced portion of the sequence based on start, stop, and step. if step is None: step = 1 if step == 0: raise ValueError(\\"slice step cannot be zero\\") n = len(sequence) if start is None: start = 0 if step > 0 else n - 1 if stop is None: stop = n if step > 0 else -1 if n == 0 else -n - 1 if start < 0: start += n if stop < 0: stop += n result = [] index = start if step > 0: while index < stop and index < n: if index >= 0: result.append(sequence[index]) index += step else: while index > stop and index >= 0: if index < n: result.append(sequence[index]) index += step return result"},{"question":"# Memory-Mapped File Operation Task **Objective:** Your task is to create a Python function that uses memory-mapped file objects to perform specific file operations. You should demonstrate the following capabilities: 1. Creating a memory-mapped file. 2. Writing to the file. 3. Reading from the file. 4. Searching for a substring within the file. 5. Safely closing the memory-mapped file. **Function Signature:** ```python def memory_mapped_file_operations(file_path: str, write_data: bytes, search_term: bytes) -> dict: Perform file operations using memory-mapped file objects. Parameters: - file_path (str): Path to the file to be memory-mapped. - write_data (bytes): Data to be written into the memory-mapped file. - search_term (bytes): Substring to search within the memory-mapped file. Returns: - dict: A dictionary containing: - \\"original_content\\" (bytes): Original content of the file. - \\"updated_content\\" (bytes): Content of the file after writing `write_data`. - \\"search_term_pos\\" (int): Starting position of `search_term` in the updated content. pass ``` **Instructions:** 1. Open the specified file as a memory-mapped file using the appropriate mode that allows reading and writing. 2. Read and store the initial content of the file before any modifications. 3. Write the given `write_data` into the memory-mapped file. 4. Read the updated content of the file after the write operation. 5. Search for the first occurrence of `search_term` within the updated content and store its starting position. 6. Ensure to flush any changes to the file and close the memory-mapped file properly. 7. Return a dictionary containing the original content, updated content, and the search term position. **Constraints:** - You can assume the given file exists and is not empty. - The `write_data` is smaller than the size of the memory-mapped region. - The `search_term` is guaranteed to be found in the updated content. **Example:** Given a file `example.txt` with the initial content: ``` Hello world! ``` Calling the function: ```python memory_mapped_file_operations(\\"example.txt\\", b\\" Python!\\", b\\"Python\\") ``` Should return: ```python { \\"original_content\\": b\\"Hello world!\\", \\"updated_content\\": b\\"Hello Python!\\", \\"search_term_pos\\": 6 } ``` **Note:** You need to ensure all changes are correctly flushed to the disk to store the operations persistently, and that the memory-mapped file is closed properly to prevent any resource leaks.","solution":"import os import mmap def memory_mapped_file_operations(file_path: str, write_data: bytes, search_term: bytes) -> dict: Perform file operations using memory-mapped file objects. Parameters: - file_path (str): Path to the file to be memory-mapped. - write_data (bytes): Data to be written into the memory-mapped file. - search_term (bytes): Substring to search within the memory-mapped file. Returns: - dict: A dictionary containing: - \\"original_content\\" (bytes): Original content of the file. - \\"updated_content\\" (bytes): Content of the file after writing `write_data`. - \\"search_term_pos\\" (int): Starting position of `search_term` in the updated content. with open(file_path, \'r+b\') as f: # Memory-map the file, size 0 means whole file mm = mmap.mmap(f.fileno(), 0) try: # Read the original content original_content = mm[:] # Write the write_data to the file mm.seek(0) mm.write(write_data) mm.flush() # Flush changes to the file # Read the updated content mm.seek(0) updated_content = mm[:] # Search for the search_term in the updated content search_term_pos = updated_content.find(search_term) return { \\"original_content\\": original_content, \\"updated_content\\": updated_content, \\"search_term_pos\\": search_term_pos } finally: # Safely close the mmap object mm.close()"},{"question":"**Question: Handling `.pth` Files and Customizing `sys.path`** --- # Background In Python, the `site` module allows for extending the module search path via `.pth` files located in predefined directories. This can be used to dynamically include directories into `sys.path`. # Problem Statement You are required to implement a function `add_paths_from_pth_file(pth_file_content: str) -> None` that reads the content of a `.pth` file and updates `sys.path` accordingly. Specifically: 1. The function should parse the provided content of a `.pth` file, which will be a string with items separated by newline characters. 2. Any valid path from the contents should be added to `sys.path` if it is not already present. 3. Lines are considered in the following manner: - Lines starting with \\"#\\" are comments and should be ignored. - Blank lines should be ignored. - Valid lines should represent file paths that exist on the filesystem. Non-existing paths should not be added to `sys.path`. # Function Signature ```python def add_paths_from_pth_file(pth_file_content: str) -> None: pass ``` # Input - `pth_file_content` (str): A string representation of the contents of a `.pth` file, where each line represents a potential path, comment, or executable line. # Output - The function does not return anything, but modifies `sys.path` in place. # Example ```python import sys import os from tempfile import TemporaryDirectory # Creating a temporary directory for demonstration with TemporaryDirectory() as temp_dir: temp_path1 = os.path.join(temp_dir, \'valid_path1\') temp_path2 = os.path.join(temp_dir, \'valid_path2\') os.makedirs(temp_path1) os.makedirs(temp_path2) pth_file_content = f # Sample .pth file {temp_path1} /non/existing/path {temp_path2} add_paths_from_pth_file(pth_file_content) # Assuming sys.path initially does not have temp_path1 or temp_path2 assert temp_path1 in sys.path assert temp_path2 in sys.path assert \'/non/existing/path\' not in sys.path ``` # Constraints 1. Paths should only be added if they exist on the filesystem. 2. Paths should not be duplicated in `sys.path`. # Notes - Make sure the function is robust and makes minimum assumptions about the contents of the `.pth` file. - Tests should verify paths are correctly manipulated and `sys.path` maintains its integrity.","solution":"import sys import os def add_paths_from_pth_file(pth_file_content: str) -> None: Parses the provided content of a .pth file and updates sys.path accordingly. Parameters: pth_file_content (str): String representation of a .pth file content. Returns: None lines = pth_file_content.splitlines() for line in lines: line = line.strip() if not line or line.startswith(\\"#\\"): continue if os.path.exists(line) and line not in sys.path: sys.path.append(line)"},{"question":"Objective: Implement a Python function that processes and analyzes a list of students\' scores, identifies the top performer, calculates the average score, and categorizes students based on their performance. Problem Statement: Write a function `analyze_scores(scores: list) -> dict` that takes a list of tuples as input. Each tuple contains a student\'s name (string) and their score (integer). The function should: 1. Identify the student with the highest score. 2. Calculate the average score of all students. 3. Categorize students into three groups based on their scores: - \\"Excellent\\" for scores greater than or equal to 90. - \\"Good\\" for scores between 70 (inclusive) and 90 (exclusive). - \\"Needs Improvement\\" for scores less than 70. Return a dictionary with the following keys: - `\\"top_performer\\"`: Name of the student with the highest score. - `\\"average_score\\"`: The average score of all students, rounded to two decimal places. - `\\"categories\\"`: A dictionary with: - `\\"Excellent\\"`: A list of names of students who scored 90 and above. - `\\"Good\\"`: A list of names of students who scored between 70 and 89. - `\\"Needs Improvement\\"`: A list of names of students who scored below 70. Input Format: - A list of tuples, where each tuple contains: - A student\'s name (string). - A student\'s score (integer). Output Format: - A dictionary with the keys: - `\\"top_performer\\"`: string - `\\"average_score\\"`: float - `\\"categories\\"`: dict - `\\"Excellent\\"`: list of strings - `\\"Good\\"`: list of strings - `\\"Needs Improvement\\"`: list of strings Constraints: - You can assume that the list will contain at least one student. - The scores are between 0 and 100 inclusive. Example: ```python input_scores = [(\\"Alice\\", 95), (\\"Bob\\", 85), (\\"Charlie\\", 75), (\\"David\\", 65)] output = analyze_scores(input_scores) # Expected output { \\"top_performer\\": \\"Alice\\", \\"average_score\\": 80.00, \\"categories\\": { \\"Excellent\\": [\\"Alice\\"], \\"Good\\": [\\"Bob\\", \\"Charlie\\"], \\"Needs Improvement\\": [\\"David\\"] } } ``` Performance Requirements: - The function should efficiently handle lists of up to 10,000 students.","solution":"def analyze_scores(scores): Processes and analyzes a list of students\' scores. Parameters: scores (list of tuples): Each tuple contains a student\'s name (string) and their score (integer). Returns: dict: A dictionary containing the top performer, average score, and categorized students. if not scores: return {\\"top_performer\\": \\"\\", \\"average_score\\": 0.0, \\"categories\\": {\\"Excellent\\": [], \\"Good\\": [], \\"Needs Improvement\\": []}} # Identify the student with the highest score top_performer = max(scores, key=lambda x: x[1])[0] # Calculate the average score total_score = sum(score for _, score in scores) average_score = round(total_score / len(scores), 2) # Categorize students based on their scores categories = { \\"Excellent\\": [], \\"Good\\": [], \\"Needs Improvement\\": [] } for name, score in scores: if score >= 90: categories[\\"Excellent\\"].append(name) elif score >= 70: categories[\\"Good\\"].append(name) else: categories[\\"Needs Improvement\\"].append(name) return { \\"top_performer\\": top_performer, \\"average_score\\": average_score, \\"categories\\": categories }"},{"question":"**Question: HMAC Authentication System** You are tasked with designing a secure authentication system for a sensitive application. The system should use the HMAC (Keyed-Hashing for Message Authentication) algorithm to generate and verify message digests to ensure the integrity and authenticity of the data transmitted between clients and the server. # Requirements: 1. Implement a function `generate_hmac(key: bytes, message: bytes, digestmod: str) -> str` that: - Generates a new HMAC object. - Updates the HMAC object with the provided message. - Returns the hexadecimal string of the HMAC digest. 2. Implement a function `verify_hmac(key: bytes, message: bytes, digestmod: str, provided_digest: str) -> bool` that: - Generates a new HMAC object. - Updates the HMAC object with the provided message. - Compares the computed HMAC digest to the provided digest securely using `hmac.compare_digest`. - Returns `True` if the digests match, otherwise returns `False`. 3. Implement a function `authenticate_message(message: bytes, key: bytes, digestmod: str) -> Tuple[str, bool]` that: - First, generates the HMAC digest for the given message using the secret key and the specified digest method. - Then, verifies the HMAC digest to check if it matches. - Returns a tuple containing the hexadecimal string of the generated digest and the verification result (which should be `True` if the message was correctly authenticated). # Constraints: - The `key` should be a bytes-like object and should be kept secret. - The `message` should be a bytes-like object. - The `digestmod` should be a valid hash algorithm supported by the `hashlib` module. - Implement efficient and secure coding practices to avoid performance bottlenecks and vulnerabilities. # Example Usage: ```python key = b\'secret_key\' message = b\'This is a critical message.\' digestmod = \'sha256\' generated_hmac = generate_hmac(key, message, digestmod) verification_result = verify_hmac(key, message, digestmod, generated_hmac) authenticated_message = authenticate_message(message, key, digestmod) print(\\"Generated HMAC:\\", generated_hmac) print(\\"Verification Result:\\", verification_result) print(\\"Authenticated Message:\\", authenticated_message) ``` Expected Output: ```python Generated HMAC: a hexadecimal string representing the HMAC digest Verification Result: True Authenticated Message: (a hexadecimal string representing the HMAC digest, True) ``` # Notes: - Ensure to handle errors gracefully and validate inputs appropriately. - Emphasize secure coding practices, especially when dealing with cryptographic functions.","solution":"import hmac import hashlib from typing import Tuple def generate_hmac(key: bytes, message: bytes, digestmod: str) -> str: Generates an HMAC digest for the given message using the provided key and digest method. Parameters: - key (bytes): The secret key used for HMAC generation. - message (bytes): The message to generate the HMAC for. - digestmod (str): The hash algorithm to use (e.g., \'sha256\'). Returns: - str: The hexadecimal string of the HMAC digest. hmac_obj = hmac.new(key, message, digestmod=getattr(hashlib, digestmod)) return hmac_obj.hexdigest() def verify_hmac(key: bytes, message: bytes, digestmod: str, provided_digest: str) -> bool: Verifies if the provided HMAC digest matches the computed HMAC digest for the given message and key. Parameters: - key (bytes): The secret key used for HMAC generation. - message (bytes): The message to generate the HMAC for. - digestmod (str): The hash algorithm to use (e.g., \'sha256\'). - provided_digest (str): The HMAC digest to verify against. Returns: - bool: True if the digests match, otherwise False. computed_digest = generate_hmac(key, message, digestmod) return hmac.compare_digest(computed_digest, provided_digest) def authenticate_message(message: bytes, key: bytes, digestmod: str) -> Tuple[str, bool]: Authenticates a message by generating and verifying its HMAC digest. Parameters: - message (bytes): The message to authenticate. - key (bytes): The secret key used for HMAC generation. - digestmod (str): The hash algorithm to use (e.g., \'sha256\'). Returns: - Tuple[str, bool]: A tuple containing the hexadecimal string of the generated digest and the verification result. generated_hmac = generate_hmac(key, message, digestmod) verification_result = verify_hmac(key, message, digestmod, generated_hmac) return (generated_hmac, verification_result)"},{"question":"Objectives - Demonstrate the ability to configure and manipulate pandas options to effectively display DataFrames. - Gain proficiency in managing DataFrame representations based on the scenario requirements. Problem Statement You are given a DataFrame `df` representing a large dataset. You need to configure pandas display options to meet the following requirements: 1. Display a maximum of 10 rows when the DataFrame is printed. 2. Ensure that any column containing more than 15 characters is truncated and shown with an ellipsis (`...`). 3. Display column headers left-aligned. 4. Set the precision for floating point numbers to 3 decimal places. 5. Display the DataFrame such that the full content spans across pages if it\'s too wide. Write a function `configure_display_settings` to achieve the required display configuration. Input - A pandas DataFrame `df`. Output - Print the first 20 rows of the DataFrame `df` after configuring the settings. Constraints - None Example ```python import pandas as pd import numpy as np # Sample DataFrame generation np.random.seed(0) data = { \\"Short_Column\\": np.random.randn(20), \\"This_is_a_very_long_column_name\\": np.random.randn(20), \\"Numbers\\": np.random.randn(20) } df = pd.DataFrame(data) def configure_display_settings(df): # Set maximum rows display to 10 pd.set_option(\\"display.max_rows\\", 10) # Truncate columns with more than 15 characters pd.set_option(\\"display.max_colwidth\\", 15) # Set column header justification to left pd.set_option(\\"display.colheader_justify\\", \\"left\\") # Set precision for floating point numbers to 3 pd.set_option(\\"display.precision\\", 3) # Allow representation to stretch across pages pd.set_option(\\"display.expand_frame_repr\\", True) # Print the updated DataFrame print(df.head(20)) # Call the function with the sample DataFrame configure_display_settings(df) ``` This question will test the students\' understanding of the pandas options API, their ability to configure DataFrame display settings, and demonstrate their problem-solving and coding skills.","solution":"import pandas as pd def configure_display_settings(df): Configures the pandas display settings according to the requirements. # Set maximum rows display to 10 pd.set_option(\\"display.max_rows\\", 10) # Truncate columns with more than 15 characters pd.set_option(\\"display.max_colwidth\\", 15) # Set column header justification to left pd.set_option(\\"display.colheader_justify\\", \\"left\\") # Set precision for floating point numbers to 3 pd.set_option(\\"display.precision\\", 3) # Allow representation to stretch across pages pd.set_option(\\"display.expand_frame_repr\\", True) # Print the first 20 rows of the DataFrame print(df.head(20))"},{"question":"Objective Implement a Python function that takes a directory path as input and classifies all the files in that directory (including subdirectories) into different categories based on their file types (e.g., regular file, directory, symbolic link, etc.). The function should then return a dictionary where the keys are the file types and the values are lists of file paths corresponding to each file type. Function Signature ```python def classify_files(directory_path: str) -> dict: pass ``` Input - `directory_path`: a string representing the path to the directory to be scanned. Output - A dictionary where the keys are strings representing the file types (`\'regular_files\'`, `\'directories\'`, `\'symbolic_links\'`, `\'character_devices\'`, `\'block_devices\'`, `\'fifos\'`, `\'sockets\'`, `\'doors\'`, `\'event_ports\'`, `\'whiteouts\'`). The values are lists of strings, where each string is the path to a file of that type. Constraints - Use the `os` and `stat` modules to interpret the file types. - Ensure that symbolic links are not followed recursively to avoid circular references. Example ```python >>> classify_files(\'/some/directory\') { \'regular_files\': [\'/some/directory/file1.txt\', \'/some/directory/subdir/file2.txt\'], \'directories\': [\'/some/directory\', \'/some/directory/subdir\'], \'symbolic_links\': [\'/some/directory/link\'], \'character_devices\': [], \'block_devices\': [], \'fifos\': [], \'sockets\': [], \'doors\': [], \'event_ports\': [], \'whiteouts\': [] } ``` Implementation Notes 1. Use `os.listdir()` and `os.path.join()` to navigate through the directory and its subdirectories. 2. Use `os.lstat()` to obtain the mode of each file. 3. Use the appropriate `stat` module functions (`stat.S_ISDIR()`, `stat.S_ISREG()`, etc.) to classify each file. 4. Recursively process subdirectories while ensuring symbolic links are not followed. Performance Requirement - The function should be optimized to minimize the number of system calls where possible, using `os.lstat()` once per file and leveraging the returned mode for multiple checks.","solution":"import os import stat def classify_files(directory_path: str) -> dict: file_types = { \'regular_files\': [], \'directories\': [], \'symbolic_links\': [], \'character_devices\': [], \'block_devices\': [], \'fifos\': [], \'sockets\': [], \'doors\': [], \'event_ports\': [], \'whiteouts\': [] } def classify(path): try: mode = os.lstat(path).st_mode if stat.S_ISDIR(mode): file_types[\'directories\'].append(path) elif stat.S_ISREG(mode): file_types[\'regular_files\'].append(path) elif stat.S_ISLNK(mode): file_types[\'symbolic_links\'].append(path) elif stat.S_ISCHR(mode): file_types[\'character_devices\'].append(path) elif stat.S_ISBLK(mode): file_types[\'block_devices\'].append(path) elif stat.S_ISFIFO(mode): file_types[\'fifos\'].append(path) elif stat.S_ISSOCK(mode): file_types[\'sockets\'].append(path) # The following types might not be available on all systems elif stat.S_ISDOOR(mode): file_types[\'doors\'].append(path) elif stat.S_ISPORT(mode): file_types[\'event_ports\'].append(path) elif stat.S_ISWHT(mode): file_types[\'whiteouts\'].append(path) except Exception as e: print(f\\"Error processing {path}: {e}\\") for root, dirs, files in os.walk(directory_path, followlinks=False): for name in dirs + files: classify(os.path.join(root, name)) return file_types"},{"question":"# Dictionary Operations in Python with C-like API constraints In this task, you are required to implement a collection of functions in Python that mimic the behavior of the Python/C API dictionary functions as described in the documentation. Your task is to demonstrate an understanding of the creation, manipulation, and querying of dictionary objects using Python. Functions to Implement: 1. `create_new_dict() -> dict`: This function should create and return a new empty dictionary. 2. `clear_dict(d: dict) -> None`: This function should clear all the items in the dictionary `d`. 3. `set_dict_item(d: dict, key: str, value: any) -> None`: This function should set a key-value pair in the dictionary `d`. 4. `get_dict_item(d: dict, key: str) -> any`: This function should return the value for the given key from the dictionary `d`. Return `None` if the key is not in the dictionary. 5. `delete_dict_item(d: dict, key: str) -> None`: This function should remove the given key from the dictionary `d`. Raise `KeyError` if the key is not in the dictionary. 6. `add_dict_if_not_present(d: dict, key: str, value: any) -> None`: This function should set the key to the value if the key is not already present in the dictionary `d`. Constraints: - Your implementations should be efficient in terms of both time and space complexity. - You should handle edge cases such as trying to delete a non-existing key gracefully by raising appropriate exceptions. Example Usage: ```python # Example of usage d = create_new_dict() set_dict_item(d, \'a\', 1) print(d) # Output: {\'a\': 1} print(get_dict_item(d, \'a\')) # Output: 1 add_dict_if_not_present(d, \'b\', 2) print(d) # Output: {\'a\': 1, \'b\': 2} delete_dict_item(d, \'a\') print(d) # Output: {\'b\': 2} clear_dict(d) print(d) # Output: {} ``` Don\'t forget to handle any possible exceptions that could be raised, such as `KeyError` when trying to delete an item that does not exist.","solution":"def create_new_dict() -> dict: Creates and returns a new empty dictionary. return {} def clear_dict(d: dict) -> None: Clears all items in the dictionary `d`. d.clear() def set_dict_item(d: dict, key: str, value: any) -> None: Sets a key-value pair in the dictionary `d`. d[key] = value def get_dict_item(d: dict, key: str) -> any: Returns the value for the given key from the dictionary `d`. Returns None if the key is not in the dictionary. return d.get(key, None) def delete_dict_item(d: dict, key: str) -> None: Removes the given key from the dictionary `d`. Raises KeyError if the key is not in the dictionary. if key in d: del d[key] else: raise KeyError(f\\"Key {key} not found in dictionary\\") def add_dict_if_not_present(d: dict, key: str, value: any) -> None: Sets the key to the value if the key is not already present in the dictionary `d`. if key not in d: d[key] = value"},{"question":"**Support Vector Classification with Cross-Validation and Hyperparameter Tuning** In this task, you are required to implement a Support Vector Classifier (SVC) using the Scikit-learn library. You will need to preprocess the dataset, split it into training and test sets, perform hyperparameter tuning using cross-validation, and evaluate the final model. # Requirements: 1. Load and preprocess the dataset. - Use the `load_iris` dataset from `sklearn.datasets`. - Standardize the features using `StandardScaler`. 2. Implement a Support Vector Classifier (SVC) with the following: - A linear kernel. - Radial Basis Function (RBF) kernel. 3. Perform hyperparameter tuning: - Use `GridSearchCV` to perform cross-validation and find the best hyperparameters. - For the linear kernel, tune the `C` parameter. - For the RBF kernel, tune both `C` and `gamma` parameters. 4. Evaluate the performance: - Print the best parameters found by `GridSearchCV`. - Evaluate the accuracy of the best model on the test set. - Print the classification report and confusion matrix for the best model on the test set. # Input: - There are no external inputs as you will use the predefined `load_iris` dataset. # Output: - Print statements showing: - The best parameters found for both linear and RBF kernels. - The accuracy of the best model on the test set. - The classification report. - The confusion matrix. # Constraints: - Use cross-validation with `cv=5`. - Use `random_state=42` where applicable to ensure reproducibility. # Example: ```python # Load Iris dataset from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import classification_report, confusion_matrix import numpy as np # Load dataset data = load_iris() X, y = data.data, data.target # Standardize features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Implement SVC with GridSearchCV for hyperparameter tuning # Linear kernel param_grid_linear = {\'C\': [0.1, 1, 10, 100]} svc_linear = SVC(kernel=\'linear\', random_state=42) grid_linear = GridSearchCV(svc_linear, param_grid_linear, cv=5) grid_linear.fit(X_train, y_train) # RBF kernel param_grid_rbf = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} svc_rbf = SVC(kernel=\'rbf\', random_state=42) grid_rbf = GridSearchCV(svc_rbf, param_grid_rbf, cv=5) grid_rbf.fit(X_train, y_train) # Print the best parameters print(f\\"Best parameters for linear kernel: {grid_linear.best_params_}\\") print(f\\"Best parameters for RBF kernel: {grid_rbf.best_params_}\\") # Evaluate the best models on the test set best_model_linear = grid_linear.best_estimator_ best_model_rbf = grid_rbf.best_estimator_ y_pred_linear = best_model_linear.predict(X_test) y_pred_rbf = best_model_rbf.predict(X_test) print(f\\"Accuracy of best linear model: {np.mean(y_pred_linear == y_test):.2f}\\") print(f\\"Accuracy of best RBF model: {np.mean(y_pred_rbf == y_test):.2f}\\") print(\\"Classification report for linear kernel:\\") print(classification_report(y_test, y_pred_linear)) print(\\"Classification report for RBF kernel:\\") print(classification_report(y_test, y_pred_rbf)) print(\\"Confusion matrix for linear kernel:\\") print(confusion_matrix(y_test, y_pred_linear)) print(\\"Confusion matrix for RBF kernel:\\") print(confusion_matrix(y_test, y_pred_rbf)) ``` # Notes: - Use appropriate imports for the required modules. - Ensure the code is well-commented to explain each step.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import classification_report, confusion_matrix import numpy as np def support_vector_classification(): # Load the iris dataset data = load_iris() X, y = data.data, data.target # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # SVC with linear kernel hyperparameter tuning param_grid_linear = {\'C\': [0.1, 1, 10, 100]} svc_linear = SVC(kernel=\'linear\', random_state=42) grid_linear = GridSearchCV(svc_linear, param_grid_linear, cv=5) grid_linear.fit(X_train, y_train) # SVC with RBF kernel hyperparameter tuning param_grid_rbf = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} svc_rbf = SVC(kernel=\'rbf\', random_state=42) grid_rbf = GridSearchCV(svc_rbf, param_grid_rbf, cv=5) grid_rbf.fit(X_train, y_train) # Get the best models best_model_linear = grid_linear.best_estimator_ best_model_rbf = grid_rbf.best_estimator_ # Predictions y_pred_linear = best_model_linear.predict(X_test) y_pred_rbf = best_model_rbf.predict(X_test) return { \\"best_linear_params\\": grid_linear.best_params_, \\"best_rbf_params\\": grid_rbf.best_params_, \\"best_linear_accuracy\\": np.mean(y_pred_linear == y_test), \\"best_rbf_accuracy\\": np.mean(y_pred_rbf == y_test), \\"classification_report_linear\\": classification_report(y_test, y_pred_linear, output_dict=True), \\"classification_report_rbf\\": classification_report(y_test, y_pred_rbf, output_dict=True), \\"confusion_matrix_linear\\": confusion_matrix(y_test, y_pred_linear).tolist(), \\"confusion_matrix_rbf\\": confusion_matrix(y_test, y_pred_rbf).tolist() } # Function call to print results if __name__ == \\"__main__\\": results = support_vector_classification() print(f\\"Best parameters for linear kernel: {results[\'best_linear_params\']}\\") print(f\\"Best parameters for RBF kernel: {results[\'best_rbf_params\']}\\") print(f\\"Accuracy of best linear model: {results[\'best_linear_accuracy\']:.2f}\\") print(f\\"Accuracy of best RBF model: {results[\'best_rbf_accuracy\']:.2f}\\") print(\\"Classification report for linear kernel:\\") print(results[\'classification_report_linear\']) print(\\"Classification report for RBF kernel:\\") print(results[\'classification_report_rbf\']) print(\\"Confusion matrix for linear kernel:\\") print(results[\'confusion_matrix_linear\']) print(\\"Confusion matrix for RBF kernel:\\") print(results[\'confusion_matrix_rbf\'])"},{"question":"# Pandas DataFrame Data Manipulation and Analysis Objective: Write a function `analyze_movies_data` that takes a file path to a CSV file containing movie data and performs various data manipulations and analyses using pandas DataFrame. The function should return a dictionary with specific results as described below. Input: - A string `file_path` representing the file path to the CSV file. The CSV file contains movie data with at least the following columns: \'Title\', \'Genre\', \'Director\', \'Budget\', \'Gross\', \'IMDB_Rating\', \'Release_Date\'. Output: - A dictionary with the following key-value pairs: - `\'most_common_genre\'`: The genre that appears most frequently in the dataset. - `\'highest_imdb_rating_movie\'`: Title of the movie with the highest IMDB rating. - `\'average_budget\'`: Average budget of all movies in the dataset. - `\'average_gross\'`: Average gross income of all movies in the dataset. - `\'director_with_most_movies\'`: Name of the director who has directed the most number of movies. - `\'movies_per_year\'`: Dictionary where keys are years (extracted from the \'Release_Date\') and values are the number of movies released that year. Constraints: - The input CSV file always contains valid data with the specified columns. - Ensure efficient data manipulation and analysis. Performance Requirements: - The solution should handle large datasets efficiently. - Use appropriate pandas functionalities to perform operations. Function Signature: ```python def analyze_movies_data(file_path: str) -> dict: pass ``` Example Usage: Given a CSV file at `movie_data.csv` with the following content: ``` Title,Genre,Director,Budget,Gross,IMDB_Rating,Release_Date Movie1,Action,Dir1,100000,500000,7.5,2020-01-05 Movie2,Comedy,Dir2,200000,600000,8.0,2019-03-10 Movie3,Action,Dir1,150000,700000,7.8,2020-06-20 ``` ```python file_path = \'movie_data.csv\' result = analyze_movies_data(file_path) print(result) ``` Expected output: ```python { \'most_common_genre\': \'Action\', \'highest_imdb_rating_movie\': \'Movie2\', \'average_budget\': 150000.0, \'average_gross\': 600000.0, \'director_with_most_movies\': \'Dir1\', \'movies_per_year\': {2020: 2, 2019: 1} } ``` Note: The example output is for illustration purposes only and will vary based on actual data in the CSV file.","solution":"import pandas as pd def analyze_movies_data(file_path: str) -> dict: df = pd.read_csv(file_path) most_common_genre = df[\'Genre\'].mode()[0] highest_imdb_rating_movie = df.loc[df[\'IMDB_Rating\'].idxmax(), \'Title\'] average_budget = df[\'Budget\'].mean() average_gross = df[\'Gross\'].mean() director_with_most_movies = df[\'Director\'].mode()[0] df[\'Release_Date\'] = pd.to_datetime(df[\'Release_Date\']) movies_per_year = df[\'Release_Date\'].dt.year.value_counts().to_dict() return { \'most_common_genre\': most_common_genre, \'highest_imdb_rating_movie\': highest_imdb_rating_movie, \'average_budget\': average_budget, \'average_gross\': average_gross, \'director_with_most_movies\': director_with_most_movies, \'movies_per_year\': movies_per_year }"},{"question":"# Custom Codec Implementation Objective You are required to implement a custom codec for encoding and decoding data. Your custom codec should be able to encode a given string by shifting each character\'s Unicode code point by a specified number and decode it by reversing the process. Function Specification 1. **CustomCodec Class** Implement a class `CustomCodec` that should inherit from the `codecs.Codec` base class. 2. **Methods** - `encode(self, input, errors=\'strict\') -> (output, length_consumed)` - **Input**: A string `input` to be encoded and an optional `errors` parameter. - **Output**: A tuple containing the encoded string and the number of input characters consumed. - `decode(self, input, errors=\'strict\') -> (output, length_consumed)` - **Input**: A byte or byte-like `input` to be decoded and an optional `errors` parameter. - **Output**: A tuple containing the decoded string and the number of input characters consumed. - `incrementalencoder(self, errors=\'strict\')` - Returns an instance of an incremental encoder that should handle encoding in stages. - `incrementaldecoder(self, errors=\'strict\')` - Returns an instance of an incremental decoder that should handle decoding in stages. - `getregentry()` - This function should return a `codecs.CodecInfo` object that includes your encoding and decoding functions. Additional Helper Functions Implement the following helper function: - `shift_encode(input: str, shift: int) -> bytes` - **Input**: A string `input` to be encoded and an integer `shift` which indicates how much the Unicode code points of the characters should be shifted. - **Output**: A bytes representation of the encoded string. - `shift_decode(input: bytes, shift: int) -> str` - **Input**: A bytes object `input` to be decoded and an integer `shift` that was used during encoding. - **Output**: A string decoded from the bytes object. Example ```python sample_text = \\"Hello, World!\\" shift_value = 3 # Encoding encoded_bytes = shift_encode(sample_text, shift_value) print(encoded_bytes) # For instance # Decoding decoded_text = shift_decode(encoded_bytes, shift_value) print(decoded_text) # \\"Hello, World!\\" ``` Constraints 1. **Charset Limits**: Assume the input text will only contain characters in the range `U+0000` to `U+FFFF`. 2. **Custom Codec Registration** - Write a function to register your custom codec using `codecs.register()`. 3. **Error Handling** - Handle at least two error schemes: `\'strict\'` and `\'replace\'`. Testing - Ensure your custom codec can handle encoding/decoding of strings with varied characters including special characters. - Test your incremental encoder/decoder with partial data and verify correct state management. Performance - Your solution should handle very large input strings efficiently. - Ensure that your incremental encoder/decoder does not have any significant state memory footprint over the encoding/decoding process. Submission Submit the `CustomCodec` class implementation, helper functions, and the codec registration function.","solution":"import codecs class CustomCodec(codecs.Codec): def __init__(self, shift): self.shift = shift def encode(self, input, errors=\'strict\'): output = shift_encode(input, self.shift) return output, len(input) def decode(self, input, errors=\'strict\'): output = shift_decode(input, self.shift) return output, len(input) def incrementalencoder(self, errors=\'strict\'): return IncrementalEncoder(self.shift, errors) def incrementaldecoder(self, errors=\'strict\'): return IncrementalDecoder(self.shift, errors) def shift_encode(input: str, shift: int) -> bytes: encoded = \'\'.join(chr((ord(char) + shift) % 0x10000) for char in input) return encoded.encode(\'utf-8\') def shift_decode(input: bytes, shift: int) -> str: decoded_str = input.decode(\'utf-8\') return \'\'.join(chr((ord(char) - shift) % 0x10000) for char in decoded_str) class IncrementalEncoder(codecs.IncrementalEncoder): def __init__(self, shift, errors=\'strict\'): super().__init__(errors) self.shift = shift def encode(self, input, final=False): return shift_encode(input, self.shift) class IncrementalDecoder(codecs.IncrementalDecoder): def __init__(self, shift, errors=\'strict\'): super().__init__(errors) self.shift = shift def decode(self, input, final=False): return shift_decode(input, self.shift) def getregentry(shift): return codecs.CodecInfo( name=\'custom_codec\', encode=CustomCodec(shift).encode, decode=CustomCodec(shift).decode, incrementalencoder=lambda errors=\'strict\': IncrementalEncoder(shift, errors), incrementaldecoder=lambda errors=\'strict\': IncrementalDecoder(shift, errors), ) def register_custom_codec(shift): codecs.register(lambda name: getregentry(shift) if name == \'custom_codec\' else None) # Example usage to register a custom codec with a shift value of 3 register_custom_codec(3)"},{"question":"# Python Coding Assessment Question **Objective:** The goal of this assessment is to evaluate your understanding of Python\'s execution model, including code blocks, name resolution, scopes, and exception handling. # Problem Statement You need to write a Python program that simulates a simple Bank system that can create accounts, deposit and withdraw money, and manage the account balances while handling potential exceptions that might arise. **Requirements:** 1. **Bank Class:** Create a `Bank` class that can handle multiple accounts. The `Bank` should have: - A method to create a new account. - A method to deposit money into an account. - A method to withdraw money from an account. 2. **Account Class:** Each account should be represented by an `Account` class with: - An `id`, which is a unique identifier for the account. - An `owner`, which is the name of the account holder. - A `balance`, which is the account balance (Initialize with 0). 3. **Name Resolution and Scopes:** Inside the `Bank` and `Account` classes, demonstrate understanding of: - Local, nonlocal, and global scope. - Name resolution process. 4. **Exception Handling:** Handle the following exceptions: - If attempting to withdraw more money than the account balance, raise and handle an `InsufficientFundsError`. - Ensure any money deposited or withdrawn does not cause the balance to drop below 0. 5. **Dynamic Features:** Use the `eval()` or `exec()` functions to facilitate dynamic creation of accounts where appropriate. # Expected Function Signatures ```python class InsufficientFundsError(Exception): pass class Account: def __init__(self, id: int, owner: str): self.id = id self.owner = owner self.balance = 0 # Initial balance def deposit(self, amount: float): # Deposit logic pass def withdraw(self, amount: float): # Withdraw logic pass class Bank: def __init__(self): self.accounts = {} # Dictionary to store accounts with id as key def create_account(self, id: int, owner: str): # Account creation logic pass def deposit_to_account(self, id: int, amount: float): # Deposit to an existing account pass def withdraw_from_account(self, id: int, amount: float): # Withdraw from an existing account pass ``` # Example Usage ```python # Create a Bank instance my_bank = Bank() # Create accounts my_bank.create_account(1, \\"Alice\\") my_bank.create_account(2, \\"Bob\\") # Deposit money my_bank.deposit_to_account(1, 500) my_bank.deposit_to_account(2, 300) # Withdraw money try: my_bank.withdraw_from_account(1, 100) my_bank.withdraw_from_account(2, 400) # Should raise InsufficientFundsError except InsufficientFundsError as e: print(e) # Get the current balance of account 1 print(my_bank.accounts[1].balance) # Output: 400 # Get the current balance of account 2 print(my_bank.accounts[2].balance) # Output: This should not execute if exception is raised ``` **Note:** Ensure your solution handles all specified exceptions and follows Python\'s execution model principles as detailed in the supplied documentation. # Constraints and Limitations - Account IDs are unique integers. - Owners are represented as non-empty strings. - Deposits and withdrawals are non-negative floats. - Raise appropriate exceptions for invalid operations.","solution":"class InsufficientFundsError(Exception): pass class Account: def __init__(self, id: int, owner: str): self.id = id self.owner = owner self.balance = 0 # Initial balance def deposit(self, amount: float): if amount < 0: raise ValueError(\\"The amount to deposit should be a positive value.\\") self.balance += amount def withdraw(self, amount: float): if amount < 0: raise ValueError(\\"The amount to withdraw should be a positive value.\\") if self.balance < amount: raise InsufficientFundsError(\\"Insufficient funds in the account.\\") self.balance -= amount class Bank: def __init__(self): self.accounts = {} # Dictionary to store accounts with id as key def create_account(self, id: int, owner: str): if id in self.accounts: raise ValueError(\\"Account ID already exists.\\") self.accounts[id] = Account(id, owner) def deposit_to_account(self, id: int, amount: float): if id not in self.accounts: raise ValueError(\\"Account ID does not exist.\\") self.accounts[id].deposit(amount) def withdraw_from_account(self, id: int, amount: float): if id not in self.accounts: raise ValueError(\\"Account ID does not exist.\\") self.accounts[id].withdraw(amount)"},{"question":"Using the `base64` module, implement a utility class `CustomBase64` that supports encoding and decoding operations across various encoding schemes specified in the module. Additionally, allow customization of encoding options and handling of errors. # Function Implementations Your class should include the following methods: 1. `encode(data: bytes, encoding: str, **kwargs) -> bytes`: - **Input**: - `data`: Bytes-like object to be encoded. - `encoding`: A string specifying the encoding type, one of [\\"base16\\", \\"base32\\", \\"base64\\", \\"base64_urlsafe\\", \\"ascii85\\", \\"base85\\"]. - `**kwargs`: Additional optional arguments for specific encoding functions (e.g., `altchars` for Base64, `foldspaces` for Ascii85). - **Output**: Encoded bytes. 2. `decode(encoded_data: bytes, encoding: str, **kwargs) -> bytes`: - **Input**: - `encoded_data`: Bytes-like object or ASCII string to be decoded. - `encoding`: A string specifying the encoding type, one of [\\"base16\\", \\"base32\\", \\"base64\\", \\"base64_urlsafe\\", \\"ascii85\\", \\"base85\\"]. - `**kwargs`: Additional optional arguments for specific decoding functions (e.g., `altchars` for Base64, `validate` for Base64). - **Output**: Decoded bytes. 3. `validate_encoding(encoded_data: bytes, encoding: str) -> bool`: - **Input**: - `encoded_data`: Bytes-like object or ASCII string representing the encoded data. - `encoding`: A string specifying the encoding type, one of [\\"base16\\", \\"base32\\", \\"base64\\", \\"base64_urlsafe\\", \\"ascii85\\", \\"base85\\"]. - **Output**: Boolean indicating whether the encoded data is valid according to the specified encoding. # Constraints - You should handle invalid encoding types gracefully by raising `ValueError` with an appropriate message. - Ensure your implementation can handle optional arguments for the encoding/decoding functions correctly. - Your implementation should be efficient and handle reasonably large data sizes. # Example Usage ```python # Example of encoding and decoding base64 util = CustomBase64() encoded = util.encode(b\'some data\', encoding=\'base64\') print(encoded) # b\'c29tZSBkYXRh\' decoded = util.decode(encoded, encoding=\'base64\') print(decoded) # b\'some data\' # Example of URL-safe base64 encoding and decoding with custom characters encoded_urlsafe = util.encode(b\'some data\', encoding=\'base64_urlsafe\', altchars=b\'-_\') print(encoded_urlsafe) # b\'c29tZSBkYXRh\' decoded_urlsafe = util.decode(encoded_urlsafe, encoding=\'base64_urlsafe\', altchars=b\'-_\') print(decoded_urlsafe) # b\'some data\' # Example with invalid encoding type try: util.encode(b\'some data\', encoding=\'invalid\') except ValueError as e: print(e) # \\"Invalid encoding type specified.\\" ```","solution":"import base64 class CustomBase64: ENCODINGS = { \'base16\': (base64.b16encode, base64.b16decode), \'base32\': (base64.b32encode, base64.b32decode), \'base64\': (base64.b64encode, base64.b64decode), \'base64_urlsafe\': (base64.urlsafe_b64encode, base64.urlsafe_b64decode), \'ascii85\': (base64.a85encode, base64.a85decode), \'base85\': (base64.b85encode, base64.b85decode) } @staticmethod def encode(data: bytes, encoding: str, **kwargs) -> bytes: if encoding not in CustomBase64.ENCODINGS: raise ValueError(\\"Invalid encoding type specified.\\") encode_func = CustomBase64.ENCODINGS[encoding][0] if encoding == \'base64\' and \'altchars\' in kwargs: return encode_func(data, altchars=kwargs[\'altchars\']) elif encoding in [\'ascii85\', \'base85\'] and \'foldspaces\' in kwargs: return encode_func(data, foldspaces=kwargs[\'foldspaces\']) else: return encode_func(data) @staticmethod def decode(encoded_data: bytes, encoding: str, **kwargs) -> bytes: if encoding not in CustomBase64.ENCODINGS: raise ValueError(\\"Invalid encoding type specified.\\") decode_func = CustomBase64.ENCODINGS[encoding][1] if encoding == \'base64\' and \'altchars\' in kwargs: return decode_func(encoded_data, altchars=kwargs[\'altchars\']) elif encoding == \'base64\' and \'validate\' in kwargs: return decode_func(encoded_data, validate=kwargs[\'validate\']) elif encoding in [\'ascii85\', \'base85\'] and \'foldspaces\' in kwargs: return decode_func(encoded_data, foldspaces=kwargs[\'foldspaces\']) else: return decode_func(encoded_data) @staticmethod def validate_encoding(encoded_data: bytes, encoding: str) -> bool: try: CustomBase64.decode(encoded_data, encoding) return True except Exception: return False"},{"question":"# Objective Create a robust testing suite using the `unittest` module to verify the functionality of a simple class that manages a collection of items. This will test your understanding of setting up test cases, using various assertion methods, and organizing your test code effectively. # Instructions 1. Implement a class `ItemManager` with the following specifications: - **Attributes**: - `items`: A list that holds the names of items. - **Methods**: - `add_item(item_name)`: Adds a new item to the list. - `remove_item(item_name)`: Removes an item from the list. Raises a `ValueError` if the item is not found. - `get_all_items()`: Returns the list of item names. 2. Write a comprehensive test suite using the `unittest` module to test the functionality of the `ItemManager` class. The test suite should include: - Test cases for adding items. - Test cases for removing items, including cases where the item does not exist. - Test cases for retrieving all items. - Use setup and teardown methods to prepare and clean up the test environment. - Include at least one example of using subtests. - Ensure tests for expected failures and skipped tests under certain conditions are included. # Requirements - Your solution should include the implementation of the class `ItemManager`. - You should then create a test suite in a separate class that extends `unittest.TestCase`. - Use appropriate assertion methods to validate your test cases. - Structure your tests to maximize readability and maintainability. # Example ```python class ItemManager: def __init__(self): self.items = [] def add_item(self, item_name): self.items.append(item_name) def remove_item(self, item_name): if item_name not in self.items: raise ValueError(f\\"Item {item_name} not found\\") self.items.remove(item_name) def get_all_items(self): return self.items import unittest class TestItemManager(unittest.TestCase): def setUp(self): self.manager = ItemManager() def tearDown(self): # Clean up code if necessary pass def test_add_item(self): self.manager.add_item(\\"item1\\") self.assertIn(\\"item1\\", self.manager.get_all_items()) def test_remove_item(self): self.manager.add_item(\\"item1\\") self.manager.remove_item(\\"item1\\") self.assertNotIn(\\"item1\\", self.manager.get_all_items()) def test_remove_non_existent_item(self): with self.assertRaises(ValueError): self.manager.remove_item(\\"item1\\") def test_get_all_items(self): items = [\\"item1\\", \\"item2\\", \\"item3\\"] for item in items: self.manager.add_item(item) self.assertEqual(self.manager.get_all_items(), items) def test_subtests(self): items = [\\"item1\\", \\"item2\\", \\"item3\\"] for item in items: with self.subTest(item=item): self.manager.add_item(item) self.assertIn(item, self.manager.get_all_items()) @unittest.expectedFailure def test_expected_failure(self): self.assertEqual(1, 0, \\"This test is expected to fail\\") @unittest.skip(\\"Skipping this test\\") def test_skipped(self): self.assertEqual(1, 1, \\"This test should be skipped\\") if __name__ == \'__main__\': unittest.main() ``` - Ensure your code is well-documented and adheres to best practices in Python unit testing. - Submit the complete implementation and test code as part of your solution.","solution":"class ItemManager: def __init__(self): self.items = [] def add_item(self, item_name): self.items.append(item_name) def remove_item(self, item_name): if item_name not in self.items: raise ValueError(f\\"Item {item_name} not found\\") self.items.remove(item_name) def get_all_items(self): return self.items"},{"question":"# Coding Assessment: Create an Interactive Todo List Application **Objective:** Develop an interactive text-based Todo List application using the `curses` module in Python. Your application should utilize window management, handle user inputs, apply color attributes, and manage screen refreshes efficiently. **Requirements:** 1. **Window Setup:** - Create the main window of the application using the `curses.initscr()` method. - Create a sub-window for the Todo List items. - Create another sub-window for inputting new Todo items. 2. **Color Setup:** - Use the `curses.start_color()` to initialize the color functionality. - Configure at least two color pairs (e.g., one for selected items and one for normal items). 3. **Functionality:** - Display a list of Todo items in the main window. - Allow users to add new items by typing into the input sub-window. - Highlight the currently selected item using a different color attribute. - Support the following commands: - **Arrow keys** (Up/Down) to navigate through the items. - **Enter key** to mark the selected item as \\"completed\\" (change its color). - **Backspace/Delete key** to delete the selected item. - **\'q\' key** to quit the application. 4. **Input Handling:** - Implement a loop to handle user inputs and updates to the display. - Use `curses.wrapper` to manage the initialization and cleanup of the curses environment. **Constraints:** - Follow the method signatures provided. - The Todo list should be dynamically updated as users add or remove items. - Handle edge cases such as navigating an empty list, deleting the last item, etc. **Example Usage:** Upon starting the application: ``` ------------------------- Todo List: 1. Write the report 2. Review the PR ------------------------- Enter a new Todo: ``` While adding a new item: ``` ------------------------- Todo List: 1. Write the report 2. Review the PR ------------------------- Enter a new Todo: Buy groceries ``` **Input and Output:** - **Input:** - User keystrokes to navigate, add, mark complete, and delete items. - **Output:** - Updated display of the Todo list with appropriate highlighting and changes based on user actions. **Implementation Start:** ```python import curses def todo_list_app(stdscr): # 1. Setup curses environment and colors curses.curs_set(0) curses.start_color() curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_BLACK, curses.COLOR_WHITE) # 2. Create windows h, w = stdscr.getmaxyx() todo_win = stdscr.derwin(h-3, w, 0, 0) input_win = stdscr.derwin(3, w, h-3, 0) # 3. Initialize todo list and selected item index todos = [\\"Write the report\\", \\"Review the PR\\"] selected_idx = 0 while True: # 4. Clear the windows stdscr.clear() todo_win.clear() input_win.clear() # 5. Display the todos with highlighting for idx, todo in enumerate(todos): if idx == selected_idx: todo_win.addstr(idx, 0, todo, curses.color_pair(2)) else: todo_win.addstr(idx, 0, todo, curses.color_pair(1)) # 6. Display the input prompt input_win.addstr(0, 0, \\"Enter a new Todo: \\", curses.color_pair(1)) stdscr.refresh() todo_win.refresh() input_win.refresh() # 7. Handle user input key = stdscr.getch() if key == curses.KEY_UP and selected_idx > 0: selected_idx -= 1 elif key == curses.KEY_DOWN and selected_idx < len(todos) - 1: selected_idx += 1 elif key == ord(\'q\'): break elif key == curses.KEY_ENTER or key == 10: todos[selected_idx] += \\" (done)\\" elif key == curses.KEY_BACKSPACE or key == curses.KEY_DC: if todos: todos.pop(selected_idx) selected_idx = min(selected_idx, len(todos) - 1) elif key == ord(\'n\'): curses.echo() input_str = input_win.getstr(1, 0, 20).decode(encoding=\\"utf-8\\") todos.append(input_str) selected_idx = len(todos) - 1 curses.noecho() if __name__ == \\"__main__\\": curses.wrapper(todo_list_app) ``` **Notes:** - Ensure you handle exceptions gracefully and return the terminal to a normal state upon exiting. - Validate that your application runs smoothly in a terminal supporting curses. - Comment your code effectively to explain your logic.","solution":"import curses def todo_list_app(stdscr): # 1. Setup curses environment and colors curses.curs_set(0) curses.start_color() curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_BLACK, curses.COLOR_WHITE) # 2. Create windows h, w = stdscr.getmaxyx() todo_win = stdscr.derwin(h-3, w, 0, 0) input_win = stdscr.derwin(3, w, h-3, 0) # 3. Initialize todo list and selected item index todos = [\\"Write the report\\", \\"Review the PR\\"] selected_idx = 0 while True: # 4. Clear the windows stdscr.clear() todo_win.clear() input_win.clear() # 5. Display the todos with highlighting for idx, todo in enumerate(todos): if idx == selected_idx: todo_win.addstr(idx, 0, todo, curses.color_pair(2)) else: todo_win.addstr(idx, 0, todo, curses.color_pair(1)) # 6. Display the input prompt input_win.addstr(0, 0, \\"Enter a new Todo: \\", curses.color_pair(1)) stdscr.refresh() todo_win.refresh() input_win.refresh() # 7. Handle user input key = stdscr.getch() if key == curses.KEY_UP and selected_idx > 0: selected_idx -= 1 elif key == curses.KEY_DOWN and selected_idx < len(todos) - 1: selected_idx += 1 elif key == ord(\'q\'): break elif key == curses.KEY_ENTER or key == 10: todos[selected_idx] += \\" (done)\\" elif key == curses.KEY_BACKSPACE or key == curses.KEY_DC: if todos: todos.pop(selected_idx) selected_idx = min(selected_idx, len(todos) - 1) elif key == ord(\'n\'): curses.echo() input_str = input_win.getstr(1, 0, 20).decode(encoding=\\"utf-8\\") todos.append(input_str) selected_idx = len(todos) - 1 curses.noecho() if __name__ == \\"__main__\\": curses.wrapper(todo_list_app)"},{"question":"# PyTorch Coding Assessment: Configuring Module Parameters on Conversion Objective You are tasked with evaluating and altering the configuration settings for module parameters when converting a PyTorch model. You need to demonstrate your understanding of the `torch.__future__` module and apply its functionalities in a given scenario. Background In PyTorch, the behavior of module parameter conversions can be configured using the following functions from `torch.__future__`: - `set_overwrite_module_params_on_conversion(value: bool)` - `get_overwrite_module_params_on_conversion() -> bool` - `set_swap_module_params_on_conversion(value: bool)` - `get_swap_module_params_on_conversion() -> bool` These functions allow you to control whether module parameters should be overwritten or swapped during conversion processes. # Problem Statement 1. Write a function `configure_model_conversion(overwrite: bool, swap: bool) -> None` that sets the conversion configuration for module parameters: - Set the \\"overwrite\\" behavior using `set_overwrite_module_params_on_conversion`. - Set the \\"swap\\" behavior using `set_swap_module_params_on_conversion`. 2. Verify the configuration settings by writing the function `verify_conversion_settings() -> Tuple[bool, bool]` that returns a tuple of the current settings: - Retrieve the \\"overwrite\\" setting using `get_overwrite_module_params_on_conversion`. - Retrieve the \\"swap\\" setting using `get_swap_module_params_on_conversion`. 3. A third function `apply_conversion_settings(model: torch.nn.Module, target_device: str) -> torch.nn.Module` should: - Take a PyTorch model and a target device (either `\'cpu\'` or `\'cuda\'`). - Apply the current conversion settings and move the model to the target device. - Return the converted model. Function Definitions - `configure_model_conversion(overwrite: bool, swap: bool) -> None` - `verify_conversion_settings() -> Tuple[bool, bool]` - `apply_conversion_settings(model: torch.nn.Module, target_device: str) -> torch.nn.Module` Constraints - Assume the target device string is always either `\'cpu\'` or `\'cuda\'`. - Ensure the model conversion respects the settings configured (overwrite/swap). - You must handle any exceptions or errors gracefully. Example ```python import torch import torch.nn as nn # Example model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 1) def forward(self, x): return self.fc(x) # Instantiate the model model = SimpleModel() # Configure conversion settings configure_model_conversion(overwrite=True, swap=False) # Verify the settings assert verify_conversion_settings() == (True, False) # Apply the conversion settings to the model and move to \'cuda\' cuda_model = apply_conversion_settings(model, \'cuda\') print(cuda_model) ```","solution":"import torch from typing import Tuple def configure_model_conversion(overwrite: bool, swap: bool) -> None: Set the conversion settings for module parameters. torch.__future__.set_overwrite_module_params_on_conversion(overwrite) torch.__future__.set_swap_module_params_on_conversion(swap) def verify_conversion_settings() -> Tuple[bool, bool]: Retrieve the current conversion settings for module parameters. overwrite = torch.__future__.get_overwrite_module_params_on_conversion() swap = torch.__future__.get_swap_module_params_on_conversion() return (overwrite, swap) def apply_conversion_settings(model: torch.nn.Module, target_device: str) -> torch.nn.Module: Apply the current conversion settings to the model and move it to the specified device. try: if target_device not in [\'cpu\', \'cuda\']: raise ValueError(\\"target_device must be either \'cpu\' or \'cuda\'\\") model.to(target_device) return model except Exception as e: # Handle exceptions gracefully print(f\\"Error applying conversion settings: {e}\\") return model"},{"question":"# Advanced Coding Assessment: Functional Programming with Python Objective Implement a function `process_logs` that takes a list of log entries and processes them using functional programming concepts. The function should identify anomalies and transform logs based on specified criteria. Problem Statement You are given a list of logs, where each log is a dictionary containing the following keys: - `timestamp`: a string representing the time the log was recorded in the format \\"YYYY-MM-DD HH:MM:SS\\". - `level`: a string representing the log level (`\'DEBUG\'`, `\'INFO\'`, `\'WARNING\'`, `\'ERROR\'`, or `\'CRITICAL\'`). - `message`: a string representing the log message. Your task is to implement a function `process_logs(logs: List[Dict[str, str]]) -> List[Dict[str, str]]` that: 1. Filters out all logs with a level lower than `\'ERROR\'`. 2. Sorts the remaining logs by `timestamp` in ascending order. 3. Formats the `message` to include the `timestamp` and `level` at the beginning of each message. For example, if a log has a `timestamp` of \\"2023-01-01 00:00:00\\" and `level` of `\'ERROR\'`, the new message should be \\"[2023-01-01 00:00:00] [ERROR] Original_Message\\". 4. Returns the processed logs as a list of dictionaries maintaining the original structure but with the updated `message`. Example ```python logs = [ {\\"timestamp\\": \\"2023-01-01 00:01:00\\", \\"level\\": \\"INFO\\", \\"message\\": \\"Service started\\"}, {\\"timestamp\\": \\"2023-01-01 00:02:00\\", \\"level\\": \\"ERROR\\", \\"message\\": \\"An error occurred\\"}, {\\"timestamp\\": \\"2023-01-01 00:03:00\\", \\"level\\": \\"CRITICAL\\", \\"message\\": \\"System failure\\"}, {\\"timestamp\\": \\"2023-01-01 00:04:00\\", \\"level\\": \\"DEBUG\\", \\"message\\": \\"Debug log\\"}, {\\"timestamp\\": \\"2023-01-01 00:05:00\\", \\"level\\": \\"WARNING\\", \\"message\\": \\"Low memory warning\\"} ] assert process_logs(logs) == [ {\\"timestamp\\": \\"2023-01-01 00:02:00\\", \\"level\\": \\"ERROR\\", \\"message\\": \\"[2023-01-01 00:02:00] [ERROR] An error occurred\\"}, {\\"timestamp\\": \\"2023-01-01 00:03:00\\", \\"level\\": \\"CRITICAL\\", \\"message\\": \\"[2023-01-01 00:03:00] [CRITICAL] System failure\\"} ] ``` Constraints - The input list can be of length 0 to 10^6. - The processing should be efficient in terms of runtime and memory usage. Requirements - Use functional programming concepts from the provided documentation. - Ensure the solution is clean, concise, and leverages Python\'s capabilities for functional-style programming.","solution":"from typing import List, Dict def process_logs(logs: List[Dict[str, str]]) -> List[Dict[str, str]]: Processes the log entries by filtering, sorting, and formatting them. Parameters: logs (List[Dict[str, str]]): List of log entries. Returns: List[Dict[str, str]]: Processed list of log entries. # Define the minimum log level to consider min_level = \'ERROR\' level_order = {\'DEBUG\': 1, \'INFO\': 2, \'WARNING\': 3, \'ERROR\': 4, \'CRITICAL\': 5} # Filter logs with level lower than \'ERROR\' filtered_logs = filter(lambda log: level_order[log[\'level\']] >= level_order[min_level], logs) # Sort the filtered logs by \'timestamp\' in ascending order sorted_logs = sorted(filtered_logs, key=lambda log: log[\'timestamp\']) # Format each log message formatted_logs = map(lambda log: { **log, \'message\': f\\"[{log[\'timestamp\']}] [{log[\'level\']}] {log[\'message\']}\\" }, sorted_logs) return list(formatted_logs)"},{"question":"# Question: IP Address Range Summary Given a list of IP addresses in string format, write a Python function using the `ipaddress` module to summarize the IP address ranges. Your function should group contiguous IP addresses into their smallest possible network definitions. Input: - A list of IP addresses as strings. Example: `[\'192.168.1.1\', \'192.168.1.2\', \'192.168.1.3\', \'192.168.2.1\', \'192.168.2.7\']` Output: - A list of summarized network definitions as strings. Example: `[\'192.168.1.1/32\', \'192.168.1.2/31\', \'192.168.2.1/32\', \'192.168.2.7/32\']` Function Signature: ```python from typing import List def summarize_ip_ranges(ip_addresses: List[str]) -> List[str]: pass ``` Constraints: - The input list will contain valid IPv4 addresses only. - The function should handle at least 1 to 1000 IP addresses. - Summarized network definitions should be in the smallest possible form. Example: ```python ip_addresses = [\'192.168.1.1\', \'192.168.1.2\', \'192.168.1.3\', \'192.168.2.1\', \'192.168.2.7\'] print(summarize_ip_ranges(ip_addresses)) # Expected output: [\'192.168.1.1/32\', \'192.168.1.2/31\', \'192.168.1.3/32\', \'192.168.2.1/32\', \'192.168.2.7/32\'] ``` Notes: - Use the `ipaddress` module to implement this function. - Ensure your solution is efficient to handle the upper constraint.","solution":"from typing import List import ipaddress def summarize_ip_ranges(ip_addresses: List[str]) -> List[str]: Summarizes the list of IP addresses into the smallest possible network definitions. Args: ip_addresses (List[str]): List of IP addresses as strings. Returns: List[str]: List of summarized network definitions as strings. ip_list = sorted([ipaddress.ip_address(ip) for ip in ip_addresses]) net_list = ipaddress.collapse_addresses(ip_list) return [str(net) for net in net_list]"},{"question":"# XML Parsing with IncrementalParser You are tasked with designing an XML parser using the `xml.sax.xmlreader` library. This parser will read data incrementally from an input source and handle XML elements and their attributes. Your implementation should include the following: 1. **IncrementalXMLParser Class**: - Use the `IncrementalParser` class to process XML data incrementally. - Implement methods to handle XML events such as starting and ending elements, and character data within elements. - Maintain a count of each type of element encountered. 2. **CustomContentHandler Class**: - This class should implement methods from the `ContentHandler` interface to process the XML events. - It should store the count of different types of XML elements encountered. 3. **Main Function**: - Create an instance of the `IncrementalXMLParser`. - Feed XML data to the parser incrementally, simulating real-time data that arrives in chunks. - Use the `CustomContentHandler` to process and print the counts of each type of element once parsing is complete. # Constraints - You may assume the XML data is well-formed. - The XML data can be arbitrarily large, so it must be processed incrementally. # Example XML Data ```xml <root> <child1>Content1</child1> <child2>Content2</child2> <child1>Content3</child1> </root> ``` # Expected Output For the given example XML data, the program should output: ``` Element counts: root: 1 child1: 2 child2: 1 ``` # Implementation Requirements - Implement the `IncrementalXMLParser` class using `IncrementalParser` from `xml.sax.xmlreader`. - Implement the `CustomContentHandler` class to manage element counts. - Create a main function to simulate feeding XML data in chunks, parse it, and print the counts of elements. ```python import xml.sax from xml.sax.xmlreader import IncrementalParser class CustomContentHandler(xml.sax.ContentHandler): def __init__(self): self.element_counts = {} def startElement(self, name, attrs): if name not in self.element_counts: self.element_counts[name] = 0 self.element_counts[name] += 1 def feed_data(parser, data): parser.feed(data) class IncrementalXMLParser(IncrementalParser): def __init__(self): super().__init__() self.handler = CustomContentHandler() self.setContentHandler(self.handler) def main(): xml_data = [ \\"<root>\\", \\"<child1>Content1</child1>\\", \\"<child2>Content2</child2>\\", \\"<child1>Content3</child1>\\", \\"</root>\\" ] # Create an instance of the IncrementalXMLParser parser = IncrementalXMLParser() # Feed XML data chunks to the parser for chunk in xml_data: feed_data(parser, chunk) # Close the parser to finalize the parsing process parser.close() # Print out element counts print(\\"Element counts:\\") for elem, count in parser.handler.element_counts.items(): print(f\\"{elem}: {count}\\") if __name__ == \\"__main__\\": main() ``` # Notes Your implementation must utilize the `IncrementalParser` to read the XML data in chunks and the `CustomContentHandler` to handle the XML events and count elements. Ensure the solution is efficient and correctly handles the XML data incrementally.","solution":"import xml.sax from xml.sax.xmlreader import IncrementalParser class CustomContentHandler(xml.sax.ContentHandler): def __init__(self): super().__init__() self.element_counts = {} def startElement(self, name, attrs): if name not in self.element_counts: self.element_counts[name] = 0 self.element_counts[name] += 1 class IncrementalXMLParser(IncrementalParser): def __init__(self): super().__init__() self.parser = xml.sax.make_parser() self.handler = CustomContentHandler() self.parser.setContentHandler(self.handler) def feed(self, data): self.parser.feed(data) def close(self): self.parser.close() def get_element_counts(self): return self.handler.element_counts def main(): xml_data = [ \\"<root>\\", \\"<child1>Content1</child1>\\", \\"<child2>Content2</child2>\\", \\"<child1>Content3</child1>\\", \\"</root>\\" ] # Create an instance of the IncrementalXMLParser parser = IncrementalXMLParser() # Feed XML data chunks to the parser for chunk in xml_data: parser.feed(chunk) # Close the parser to finalize the parsing process parser.close() # Print out element counts print(\\"Element counts:\\") for elem, count in parser.get_element_counts().items(): print(f\\"{elem}: {count}\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective: Design and implement a Python function to securely hash a given message using the BLAKE2 algorithm and then authenticate it using HMAC. This will demonstrate your understanding of secure hashing and message authentication mechanisms. Problem Statement: You are required to write a Python function `secure_hash_and_authenticate()` that performs the following steps: 1. Takes an input message and a secret key. 2. Computes a secure hash of the message using the BLAKE2b algorithm (from the hashlib module). 3. Generates an HMAC (Hash-based Message Authentication Code) for the hashed message using the provided secret key (from the hmac module). 4. Returns both the secure hash and the HMAC. Function Signature: ```python def secure_hash_and_authenticate(message: str, secret_key: str) -> (str, str): ``` Input: - `message`: A string representing the message to be hashed. - `secret_key`: A string representing the secret key to be used for HMAC. Output: - A tuple containing two strings: 1. The secure hash of the message using BLAKE2b. 2. The HMAC of the hashed message using the secret key. Example: ```python message = \\"This is a confidential message.\\" secret_key = \\"Secr3tK3y123\\" hash, hmac_value = secure_hash_and_authenticate(message, secret_key) print(f\\"Hash: {hash}\\") print(f\\"HMAC: {hmac_value}\\") ``` Constraints: 1. Use `hashlib` for BLAKE2b hashing. 2. Use `hmac` for generating the HMAC. Performance Requirements: - The function should efficiently handle messages of up to 1 MB in size. - Ensure that the implementation adheres to secure coding practices to avoid common cryptographic pitfalls. # Notes: - You may refer to the official Python documentation for `hashlib` and `hmac` for details on how to use these libraries. - Be mindful of handling encoding/decoding of strings and bytes as both functions may require dealing with byte sequences.","solution":"import hashlib import hmac def secure_hash_and_authenticate(message: str, secret_key: str) -> (str, str): # Convert message and secret_key to bytes message_bytes = message.encode() secret_key_bytes = secret_key.encode() # Step 1: Compute a secure hash of the message using BLAKE2b hash_obj = hashlib.blake2b(message_bytes) secure_hash = hash_obj.hexdigest() # Step 2: Generate an HMAC for the hashed message using the provided secret key hmac_obj = hmac.new(secret_key_bytes, secure_hash.encode(), hashlib.blake2b) hmac_value = hmac_obj.hexdigest() return (secure_hash, hmac_value)"},{"question":"Given a `pandas.Series` object containing time series data for daily temperature recordings throughout a year, you are required to perform a series of operations. Your task includes handling missing data, performing statistical analysis, and plotting the results. # Instructions: 1. **Load Data from CSV**: - Load the time series data from a CSV file into a `pandas.Series`. - Assume the CSV file has two columns: `date` (YYYY-MM-DD format) and `temperature`. 2. **Handling Missing Data**: - Fill any missing temperature values with the mean temperature of the series. 3. **Compute Rolling Statistics**: - Calculate a 7-day rolling mean of the temperature. 4. **Statistical Analysis**: - Compute the overall mean, median, standard deviation, and variance of the temperature data. - Determine the day with the maximum and minimum temperature. 5. **Monthly Aggregation**: - Group the data by month and calculate the mean temperature for each month. - Identify the month with the highest average temperature. 6. **Plotting**: - Plot the original daily temperature data. - Plot the 7-day rolling mean on the same plot with a different color. 7. **Exporting Results**: - Save the monthly aggregated mean temperatures to a new CSV file. # Constraints: - Ensure the script handles the scenario where the CSV file may have duplicate dates by taking the average temperature of duplicates. - Performance should be considered for operations on large datasets (millions of rows). # Input: - Path to the input CSV file. # Output: - Statistics console output (mean, median, std, variance, min day, max day). - Plot showing the original and rolling mean temperature. - CSV file containing the monthly mean temperatures. # Example Usage: ```python # Assuming the CSV file is named \'temperature_data.csv\' and located in the current directory. process_temperature_data(\'temperature_data.csv\') ``` # Implementation: ```python import pandas as pd import matplotlib.pyplot as plt def process_temperature_data(file_path): # Load data from CSV df = pd.read_csv(file_path, parse_dates=[\'date\']) temperature_series = pd.Series(df[\'temperature\'].values, index=df[\'date\']) # Handle missing data by filling with mean temperature temperature_series.fillna(temperature_series.mean(), inplace=True) # Calculate 7-day rolling mean rolling_mean = temperature_series.rolling(window=7).mean() # Statistical analysis mean_temp = temperature_series.mean() median_temp = temperature_series.median() std_temp = temperature_series.std() var_temp = temperature_series.var() max_day = temperature_series.idxmax() min_day = temperature_series.idxmin() print(f\\"Mean Temperature: {mean_temp}\\") print(f\\"Median Temperature: {median_temp}\\") print(f\\"Standard Deviation: {std_temp}\\") print(f\\"Variance: {var_temp}\\") print(f\\"Day with Max Temperature: {max_day}\\") print(f\\"Day with Min Temperature: {min_day}\\") # Monthly aggregation monthly_mean = temperature_series.resample(\'M\').mean() hottest_month = monthly_mean.idxmax().month print(f\\"The month with the highest average temperature: {hottest_month}\\") # Plot plt.figure(figsize=(12, 6)) plt.plot(temperature_series.index, temperature_series, label=\'Daily Temperature\') plt.plot(rolling_mean.index, rolling_mean, label=\'7-Day Rolling Mean\', color=\'orange\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature\') plt.title(\'Daily Temperature and 7-Day Rolling Mean\') plt.legend() plt.show() # Export monthly mean temperatures to CSV monthly_mean.to_csv(\'monthly_mean_temperatures.csv\', header=[\'mean_temperature\']) # Example call to the function with the assumed file path process_temperature_data(\'temperature_data.csv\') ``` # Notes: - The provided solution assumes that all necessary imports are available. - Ensure proper exception handling and validation for the input file path and data. - Feel free to adjust the visualization parameters for better readability.","solution":"import pandas as pd import matplotlib.pyplot as plt def process_temperature_data(file_path): # Load data from CSV df = pd.read_csv(file_path, parse_dates=[\'date\']) # Handle duplicate dates by averaging the temperatures df = df.groupby(\'date\')[\'temperature\'].mean().reset_index() temperature_series = pd.Series(df[\'temperature\'].values, index=df[\'date\']) # Handle missing data by filling with mean temperature temperature_series.fillna(temperature_series.mean(), inplace=True) # Calculate 7-day rolling mean rolling_mean = temperature_series.rolling(window=7).mean() # Statistical analysis mean_temp = temperature_series.mean() median_temp = temperature_series.median() std_temp = temperature_series.std() var_temp = temperature_series.var() max_day = temperature_series.idxmax() min_day = temperature_series.idxmin() print(f\\"Mean Temperature: {mean_temp}\\") print(f\\"Median Temperature: {median_temp}\\") print(f\\"Standard Deviation: {std_temp}\\") print(f\\"Variance: {var_temp}\\") print(f\\"Day with Max Temperature: {max_day}\\") print(f\\"Day with Min Temperature: {min_day}\\") # Monthly aggregation monthly_mean = temperature_series.resample(\'M\').mean() hottest_month = monthly_mean.idxmax().month print(f\\"The month with the highest average temperature: {hottest_month}\\") # Plot plt.figure(figsize=(12, 6)) plt.plot(temperature_series.index, temperature_series, label=\'Daily Temperature\') plt.plot(rolling_mean.index, rolling_mean, label=\'7-Day Rolling Mean\', color=\'orange\') plt.xlabel(\'Date\') plt.ylabel(\'Temperature\') plt.title(\'Daily Temperature and 7-Day Rolling Mean\') plt.legend() plt.show() # Export monthly mean temperatures to CSV monthly_mean.to_csv(\'monthly_mean_temperatures.csv\', header=[\'mean_temperature\']) # Example call to the function with the assumed file path # process_temperature_data(\'temperature_data.csv\')"},{"question":"# Numeric Operations with Python Objects Objective: Implement a function named `perform_operations` that takes two integers and returns a dictionary containing the results of various numeric operations performed on them. You should use the provided C API functions through Python\'s `ctypes` or `cffi` library to access these functions. Function Signature: ```python def perform_operations(num1: int, num2: int) -> dict: pass ``` Inputs: - `num1` (int): The first integer. - `num2` (int): The second integer. Note: `num2` should not be zero where relevant (e.g., division). Outputs: - A dictionary with the results of the operations. The keys are string representations of the operations, and the values are the results. For example: ```python { \\"add\\": result_add, \\"subtract\\": result_subtract, \\"multiply\\": result_multiply, ... } ``` Operations to Perform: - Addition: `num1 + num2` - Subtraction: `num1 - num2` - Multiplication: `num1 * num2` - Floor Division: `num1 // num2` - True Division: `num1 / num2` - Modulus: `num1 % num2` - Power: `num1 ** num2` - Negation: `-num1`, `-num2` - Absolute: `abs(num1)`, `abs(num2)` - Bitwise AND: `num1 & num2` - Bitwise OR: `num1 | num2` - Bitwise XOR: `num1 ^ num2` - Left Shift: `num1 << num2` - Right Shift: `num1 >> num2` Example: ```python def perform_operations(num1: int, num2: int) -> dict: # Implementation using ctypes or cffi result = perform_operations(10, 3) print(result) ``` Constraints: - `num2` must not be zero for Division and Modulus operations. You must handle and document such cases appropriately. - Use the provided C API functions for all operations. Performance Considerations: - Ensure that the operations handle large integers gracefully. - Handle possible errors (such as division by zero) and reflect them in the output dictionary with an appropriate message. **Note**: It\'s assumed that you have the necessary setup to load the C API functions. Use `ctypes` for simplicity unless you prefer `cffi` and are more comfortable with it.","solution":"def perform_operations(num1: int, num2: int) -> dict: ops = {} # Basic Arithmetic Operations ops[\'add\'] = num1 + num2 ops[\'subtract\'] = num1 - num2 ops[\'multiply\'] = num1 * num2 # Division and Modulus if num2 != 0: ops[\'floor_division\'] = num1 // num2 ops[\'true_division\'] = num1 / num2 ops[\'modulus\'] = num1 % num2 else: ops[\'floor_division\'] = \'Error: Division by zero\' ops[\'true_division\'] = \'Error: Division by zero\' ops[\'modulus\'] = \'Error: Division by zero\' # Power ops[\'power\'] = num1 ** num2 # Unary Operations ops[\'negate_num1\'] = -num1 ops[\'negate_num2\'] = -num2 ops[\'absolute_num1\'] = abs(num1) ops[\'absolute_num2\'] = abs(num2) # Bitwise Operations ops[\'bitwise_and\'] = num1 & num2 ops[\'bitwise_or\'] = num1 | num2 ops[\'bitwise_xor\'] = num1 ^ num2 ops[\'left_shift\'] = num1 << num2 ops[\'right_shift\'] = num1 >> num2 return ops"},{"question":"**Objective:** Create a class `CustomZipImporter` that extends the functionality of the `zipimport.zipimporter` class to include additional diagnostic and logging features. Your class should provide detailed logs of each method call, including parameters and results, and handle exceptions gracefully with appropriate logging. **Task:** 1. Implement a class `CustomZipImporter` that inherits from `zipimport.zipimporter`. 2. Override the following methods to add logging: - `find_spec(fullname, target=None)` - `get_code(fullname)` - `get_data(pathname)` - `is_package(fullname)` 3. Ensure that each overridden method logs its entry, parameters, exit, and result. If an exception occurs, it should be caught and logged, and then the exception should be re-raised. 4. Implement a method `get_logs()` in the `CustomZipImporter` class that returns all logged messages as a list of strings. **Constraints:** - Do not use any external logging libraries; implement the logging feature using basic Python functionalities. - Ensure the log format includes timestamps and method names. - Do not modify the behavior of the original methods except for adding logging. **Example Usage:** ```python # Assuming example.zip contains a module named example_module.py import sys from custom_zipimporter import CustomZipImporter sys.path.insert(0, \'example.zip\') importer = CustomZipImporter(\'example.zip\') # Perform some import operations spec = importer.find_spec(\'example_module\') code = importer.get_code(\'example_module\') data = importer.get_data(\'example_module.py\') # Retrieve logs logs = importer.get_logs() for log in logs: print(log) ``` **Expected Output:** ```plaintext [2023-10-31T10:00:00] Entering method: find_spec with parameters: (\'example_module\', None) [2023-10-31T10:00:01] Exiting method: find_spec with result: <module spec object> [2023-10-31T10:00:02] Entering method: get_code with parameters: (\'example_module\',) [2023-10-31T10:00:03] Exiting method: get_code with result: <code object> [2023-10-31T10:00:04] Entering method: get_data with parameters: (\'example_module.py\',) [2023-10-31T10:00:05] Exiting method: get_data with result: <data of example_module.py ...> ``` **Notes:** - Include comprehensive docstrings and comments in your implementation. - Your solution will be evaluated based on correctness, completeness, logging details, and code quality.","solution":"import zipimport import datetime class CustomZipImporter(zipimport.zipimporter): A custom ZipImporter that extends zipimport.zipimporter to include logging and diagnostics. def __init__(self, path): super().__init__(path) self.logs = [] def log(self, message): timestamp = datetime.datetime.now().isoformat() self.logs.append(f\\"[{timestamp}] {message}\\") def find_spec(self, fullname, target=None): self.log(f\\"Entering method: find_spec with parameters: ({fullname}, {target})\\") try: spec = super().find_spec(fullname, target) self.log(f\\"Exiting method: find_spec with result: {spec}\\") return spec except Exception as e: self.log(f\\"Exception in method: find_spec with error: {e}\\") raise def get_code(self, fullname): self.log(f\\"Entering method: get_code with parameters: ({fullname},)\\") try: code = super().get_code(fullname) self.log(f\\"Exiting method: get_code with result: {code}\\") return code except Exception as e: self.log(f\\"Exception in method: get_code with error: {e}\\") raise def get_data(self, pathname): self.log(f\\"Entering method: get_data with parameters: ({pathname},)\\") try: data = super().get_data(pathname) self.log(f\\"Exiting method: get_data with result: data of length {len(data)}\\") return data except Exception as e: self.log(f\\"Exception in method: get_data with error: {e}\\") raise def is_package(self, fullname): self.log(f\\"Entering method: is_package with parameters: ({fullname},)\\") try: result = super().is_package(fullname) self.log(f\\"Exiting method: is_package with result: {result}\\") return result except Exception as e: self.log(f\\"Exception in method: is_package with error: {e}\\") raise def get_logs(self): return self.logs"},{"question":"You are tasked with analyzing two datasets using Seaborn\'s `histplot` functionality. The datasets to be used are the `penguins` and `planets` datasets, which can be loaded using `sns.load_dataset`. Your goal is to visualize the distributions of various features in these datasets using histograms, applying different customization options available in Seaborn. Part 1: Univariate Distributions 1. Load the `penguins` dataset. 2. Create a univariate histogram for the feature `flipper_length_mm`. 3. Add a kernel density estimate (KDE) to the histogram. 4. Experiment with different bin widths (e.g., 5 and 15) and check how the histogram changes. 5. Plot the histogram with the data placed along the y-axis instead of the x-axis. Part 2: Bivariate Distributions 1. Load the `planets` dataset. 2. Create a bivariate histogram for the features `year` and `distance`. 3. Visualize the histogram both in linear scale and in log scale. 4. Add a color bar to the plot representing the counts. Part 3: Categorized Histograms 1. From the `penguins` dataset, create histograms of `bill_length_mm` categorized by the `species`. 2. Plot two versions: one with overlapping bars and one with stacked bars. 3. Create a bivariate histogram using `bill_depth_mm` and `body_mass_g`, and differentiate the data points by `species`. Part 4: Custom Histograms 1. For the `planets` dataset, create a cumulative step histogram for the `distance` feature, categorized by `method`. 2. Normalize the histogram densities independently for each category. 3. Annotate this histogram with a colorbar indicating the density. # Function Signature ```python import seaborn as sns def analyze_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Part 1: Univariate Distributions sns.histplot(data=penguins, x=\\"flipper_length_mm\\", kde=True) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=5) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=15) sns.histplot(data=penguins, y=\\"flipper_length_mm\\") # Part 3: Categorized Histograms sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") def analyze_planets(): # Load the planets dataset planets = sns.load_dataset(\\"planets\\") # Part 2: Bivariate Distributions sns.histplot(planets, x=\\"year\\", y=\\"distance\\") sns.histplot(planets, x=\\"year\\", y=\\"distance\\", log_scale=True) sns.histplot(planets, x=\\"year\\", y=\\"distance\\", log_scale=True, cbar=True) # Part 4: Custom Histograms sns.histplot(planets, x=\\"distance\\", hue=\\"method\\", element=\\"step\\", cumulative=True, stat=\\"density\\", common_norm=False, cbar=True) ``` Your task is to write the function implementations for `analyze_penguins` and `analyze_planets` according to the specifications provided. # Constraints - Ensure all plots have appropriate labels and titles. - Use different bin widths to explore their effects on histogram shapes. - Use appropriate log and linear scales where necessary. - Handle the data to normalize distributions independently. # Expected Output The function should generate and display the required histograms for each part of the problem statement.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Part 1: Univariate Distributions plt.figure(figsize=(10, 8)) # Basic histogram with KDE plt.subplot(2, 2, 1) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", kde=True) plt.title(\\"Histogram with KDE\\") # Different bin widths plt.subplot(2, 2, 2) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=5) plt.title(\\"Histogram with Binwidth 5\\") plt.subplot(2, 2, 3) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", binwidth=15) plt.title(\\"Histogram with Binwidth 15\\") # Histogram along y-axis plt.subplot(2, 2, 4) sns.histplot(data=penguins, y=\\"flipper_length_mm\\") plt.title(\\"Histogram Y-axis \'flipper_length_mm\'\\") plt.tight_layout() plt.show() # Part 3: Categorized Histograms plt.figure(figsize=(12, 8)) # Overlapping bars plt.subplot(2, 1, 1) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", element=\\"step\\") plt.title(\\"Histogram of \'bill_length_mm\' by Species (Overlapping)\\") # Stacked bars plt.subplot(2, 1, 2) sns.histplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") plt.title(\\"Histogram of \'bill_length_mm\' by Species (Stacked)\\") plt.tight_layout() plt.show() # Bivariate histogram categorized by species sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\") plt.title(\\"Bivariate Histogram of \'bill_depth_mm\' and \'body_mass_g\' by Species\\") plt.show() def analyze_planets(): # Load the planets dataset planets = sns.load_dataset(\\"planets\\") # Part 2: Bivariate Distributions plt.figure(figsize=(12, 10)) # Bivariate histogram in linear scale plt.subplot(3, 1, 1) sns.histplot(planets, x=\\"year\\", y=\\"distance\\") plt.title(\\"Bivariate Histogram of \'year\' and \'distance\' (Linear)\\") # Bivariate histogram in log scale plt.subplot(3, 1, 2) sns.histplot(planets, x=\\"year\\", y=\\"distance\\", log_scale=True) plt.title(\\"Bivariate Histogram of \'year\' and \'distance\' (Log Scale)\\") # Bivariate histogram with color bar plt.subplot(3, 1, 3) sns.histplot(planets, x=\\"year\\", y=\\"distance\\", log_scale=True, cbar=True) plt.title(\\"Bivariate Histogram with Color Bar\\") plt.tight_layout() plt.show() # Part 4: Custom Histograms plt.figure(figsize=(10, 8)) sns.histplot(planets, x=\\"distance\\", hue=\\"method\\", element=\\"step\\", cumulative=True, stat=\\"density\\", common_norm=False, cbar=True) plt.title(\\"Cumulative Step Histogram of \'distance\' by Method\\") plt.show()"},{"question":"Objective: Your task is to demonstrate your understanding of PyTorch\'s MTIA backend by writing code that initializes the backend, manages devices and streams, and synchronizes operations across multiple streams. Problem Statement: 1. Write a function `initialize_mtia_backend()` that checks if the MTIA backend is available and initializes it. 2. Write a function `manage_streams_and_devices()` that: - Retrieves the number of available devices. - Sets the device to the first device (index 0). - Creates three different streams on this device. - Within each stream, launches a simple torch operation (like adding two tensors) on the device. - Synchronizes the operations across all three streams. Detailed Requirements: 1. **initialize_mtia_backend**: - Input: None - Output: Boolean value indicating whether the initialization was successful. 2. **manage_streams_and_devices**: - Input: None - Output: A dictionary containing: - \'device_count\': Number of devices available. - \'operations_successful\': Boolean indicating if operations within streams were successful. Constraints: - Use PyTorch\'s `torch.mtia` module for handling devices and streams. - Handle any potential errors gracefully using the `DeferredMtiaCallError`. Example: ```python # Assuming the required code for initialization and managing streams and devices is implemented as per the specifications. # Initialize MTIA Backend init_success = initialize_mtia_backend() print(init_success) # Output: True (if MTIA backend is available and initialized successfully) # Manage devices and streams result = manage_streams_and_devices() print(result) # Output: {\'device_count\': <number_of_devices>, \'operations_successful\': True} ``` Implement the functions `initialize_mtia_backend` and `manage_streams_and_devices` to meet the specifications outlined above.","solution":"import torch def initialize_mtia_backend(): Checks if the MTIA backend is available and initializes it. Returns: bool: True if MTIA backend is available and initialized successfully, False otherwise. try: if torch.has_mtia: torch.mtia.init() return True else: return False except Exception as e: print(f\\"Error initializing MTIA: {e}\\") return False def manage_streams_and_devices(): Manages devices and streams on MTIA. Retrieves the number of available devices, sets the device to the first device, creates streams, performs operations and synchronizes streams. Returns: dict: A dictionary with \'device_count\' and \'operations_successful\'. try: device_count = torch.mtia.device_count() if device_count == 0: raise RuntimeError(\\"No MTIA devices available.\\") device = torch.device(\\"mtia:0\\") torch.mtia.set_device(device) streams = [torch.mtia.Stream(device) for _ in range(3)] tensors = [torch.tensor([1.0, 2.0, 3.0], device=device) for _ in range(3)] results = [] for stream, tensor in zip(streams, tensors): with torch.mtia.stream(stream): result = tensor + tensor results.append(result) for stream in streams: stream.synchronize() return {\'device_count\': device_count, \'operations_successful\': all(result.is_cuda for result in results)} except Exception as e: print(f\\"Error managing streams and devices: {e}\\") return {\'device_count\': 0, \'operations_successful\': False}"},{"question":"# Custom Type Implementation in Python using C API Objective: As a student in this advanced Python course, your task is to implement a custom type in Python using the C API. This exercise will demonstrate your understanding of the `PyTypeObject` structure and how to create custom types with specific behaviors. Problem Statement: You are required to create a custom `Matrix` type that supports basic matrix operations. **Features to Implement:** 1. **Type Definition:** - Define a new type `Matrix` with the following functionality: - Store data in a 2-dimensional list. - Support addition, multiplication, and string representation. 2. **Initialization and Deallocation:** - Implement the `tp_init` and `tp_dealloc` slots to handle the initialization and cleanup of instances. 3. **Matrix Operations:** - Implement the `nb_add` and `nb_multiply` slots to support matrix addition and multiplication. 4. **String Representation:** - Implement the `tp_str` slot to provide a human-readable string representation of the matrix. 5. **Attribute Access:** - Implement the `tp_getattro` and `tp_setattro` slots to enable getting and setting matrix elements. Structure: - Your implementation should include: - A structure for the `Matrix` type. - Initialization and deallocation functions. - Functions for matrix operations. - Function to convert matrix to a string. Requirements: 1. **Input:** - The constructor should accept a list of lists representing the matrix data. 2. **Output:** - Operations like addition and multiplication should return a new `Matrix` object. - The string representation should be returned when `str()` is called on a `Matrix` object. 3. **Constraints:** - Ensure proper memory management to avoid leaks. - Handle errors gracefully, setting appropriate exceptions. Example Usage: ```python # Assuming your module is named `matrix_module` and the type is `Matrix` from matrix_module import Matrix # Creating matrix instances m1 = Matrix([[1, 2], [3, 4]]) m2 = Matrix([[5, 6], [7, 8]]) # Matrix addition m3 = m1 + m2 print(m3) # Should output a string representation of the matrix # Matrix multiplication m4 = m1 * m2 print(m4) # Should output a string representation of the matrix ``` Performance: - Ensure that your implementation handles large matrices efficiently. Submission: Provide the source code of your C extension along with instructions on how to compile and use it in Python.","solution":"class Matrix: def __init__(self, data): self.data = data def __add__(self, other): if not isinstance(other, Matrix): return NotImplemented rows = len(self.data) cols = len(self.data[0]) result = [] for i in range(rows): row = [] for j in range(cols): row.append(self.data[i][j] + other.data[i][j]) result.append(row) return Matrix(result) def __mul__(self, other): if not isinstance(other, Matrix): return NotImplemented rows_self = len(self.data) cols_self = len(self.data[0]) rows_other = len(other.data) cols_other = len(other.data[0]) if cols_self != rows_other: raise ValueError(\\"Matrix dimensions do not match for multiplication\\") result = [[0] * cols_other for _ in range(rows_self)] for i in range(rows_self): for j in range(cols_other): for k in range(cols_self): result[i][j] += self.data[i][k] * other.data[k][j] return Matrix(result) def __str__(self): return \'n\'.join([\' \'.join(map(str, row)) for row in self.data]) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value"},{"question":"<|Analysis Begin|> The provided documentation is for the `syslog` module, which provides an interface to the Unix \\"syslog\\" library routines. This module allows Python programs to send messages to the system logger. Here are the key functionalities and points in the documentation: 1. **Functions:** - `syslog.syslog(message)`: Sends a message to the system logger with default priority as `LOG_INFO`. - `syslog.syslog(priority, message)`: Sends a message with a specified priority. - `syslog.openlog([ident[, logoption[, facility]]])`: Configures logging options, such as default identifier, logging options, and facility. - `syslog.closelog()`: Resets syslog module values and closes the log. - `syslog.setlogmask(maskpri)`: Sets the priority mask. 2. **Constants:** - Various priority levels representing the severity of the messages (e.g., `LOG_EMERG`, `LOG_ALERT`, `LOG_CRIT`, `LOG_ERR`, etc.). - Different facilities representing the system component generating the message (e.g., `LOG_KERN`, `LOG_USER`, `LOG_MAIL`, etc.). - Log options that can be set (e.g., `LOG_PID`, `LOG_CONS`, `LOG_NDELAY`, etc.). 3. **Example Usage:** - Basic logging with default settings. - Logging with specific options and facility. A question can be crafted to assess a student\'s understanding and ability to implement these functionalities, focusing on using different priorities, facilities, options, and masks. <|Analysis End|> <|Question Begin|> You are tasked with creating a logging mechanism for a mock application that simulates operations. This mechanism should utilize the Unix `syslog` library via the `syslog` module in Python. Your implementation should support configuring different logging options, facilities, and message priorities. # Requirements: 1. Write a function `initialize_logger(ident: str, logoption: int, facility: int) -> None` that initializes the syslog with the given parameters. 2. Write a function `log_message(priority: int, message: str) -> None` that logs a message with the specified priority. 3. Write a function `set_priority_mask(priorities: List[int]) -> None` that sets the log mask to include only the given list of priority levels. 4. Write a function `close_logger() -> None` that closes the log. # Function Specifications: - `initialize_logger(ident, logoption, facility)`: - **Input**: - `ident` (str): A string to prepend to every message. - `logoption` (int): A bit field representing logging options. - `facility` (int): The default facility for messages without a specified facility. - **Output**: None - **Behavior**: Calls `syslog.openlog()` with the provided parameters. - `log_message(priority, message)`: - **Input**: - `priority` (int): The priority level of the message. - `message` (str): The message to be logged. - **Output**: None - **Behavior**: Calls `syslog.syslog()` with the specified priority and message. - `set_priority_mask(priorities)`: - **Input**: - `priorities` (List[int]): A list of priority levels to include in the log mask. - **Output**: None - **Behavior**: Calls `syslog.setlogmask()` with the calculated mask based on the provided priorities. - `close_logger()`: - **Input**: None - **Output**: None - **Behavior**: Calls `syslog.closelog()`, resetting the log settings. # Example Usage: ```python import syslog from typing import List # Function Implementations def initialize_logger(ident: str, logoption: int, facility: int) -> None: syslog.openlog(ident, logoption, facility) def log_message(priority: int, message: str) -> None: syslog.syslog(priority, message) def set_priority_mask(priorities: List[int]) -> None: mask = 0 for pri in priorities: mask |= syslog.LOG_MASK(pri) syslog.setlogmask(mask) def close_logger() -> None: syslog.closelog() # Usage initialize_logger(\'myapp\', syslog.LOG_PID | syslog.LOG_CONS, syslog.LOG_USER) log_message(syslog.LOG_INFO, \'Application started\') set_priority_mask([syslog.LOG_ERR, syslog.LOG_WARNING, syslog.LOG_INFO]) log_message(syslog.LOG_ERR, \'An error occurred\') close_logger() ``` # Constraints: - Ensure proper error handling if incorrect priorities, facilities, or options are provided. - The solution should be robust and handle multiple logging operations effectively. # Performance: - The function implementation should be efficient, with minimal overheads for logging operations.","solution":"import syslog from typing import List def initialize_logger(ident: str, logoption: int, facility: int) -> None: Initializes the syslog with the given parameters. Parameters: - ident: A string to prepend to every message. - logoption: A bit field representing logging options. - facility: The default facility for messages without a specified facility. syslog.openlog(ident, logoption, facility) def log_message(priority: int, message: str) -> None: Logs a message with the specified priority. Parameters: - priority: The priority level of the message. - message: The message to be logged. syslog.syslog(priority, message) def set_priority_mask(priorities: List[int]) -> None: Sets the log mask to include only the given list of priority levels. Parameters: - priorities: A list of priority levels to include in the log mask. mask = 0 for pri in priorities: mask |= syslog.LOG_MASK(pri) syslog.setlogmask(mask) def close_logger() -> None: Closes the syslog, resetting the log settings. syslog.closelog()"},{"question":"Question: Advanced Array Operations You are tasked with implementing a set of functions to manipulate numeric arrays using the `array` module in Python. This assessment will test your ability to work with arrays, apply different methods provided by the `array` module, and handle type-specific constraints. Below are the function requirements: 1. **Function: create_array(typecode, elements)** - **Input**: - `typecode` (str): A single-character string representing the type of elements in the array (e.g., `\'i\'` for signed int). - `elements` (list): A list of initial elements to populate the array. - **Output**: - Returns an array object initialized with the given `typecode` and `elements`. - **Constraints**: - Ensure that `elements` is a list of appropriate type matching the `typecode`. 2. **Function: array_operations(arr)** - **Input**: - `arr` (array): An array object of numeric values. - **Output**: - Returns a dictionary with the following keys: - `count_5`: The number of times the value `5` appears in the array. - `reversed_array`: The array after reversing the order of its elements. - `tobytes_representation`: The bytes representation of the array. - `buffer_info`: A tuple containing the memory address and the length in elements of the array\'s buffer. - `insertion`: The array after inserting the value `99` at position 2. - `removal`: The array after removing the first occurrence of the value `5`. 3. **Function: array_conversion(arr)** - **Input**: - `arr` (array): An array object of numeric values. - **Output**: - Returns a list containing the elements of the array. - **Constraints**: - The array `arr` can only contain numeric values (typecodes `\'b\'`, `\'B\'`, `\'h\'`, `\'H\'`, `\'i\'`, `\'I\'`, `\'l\'`, `\'L\'`, `\'q\'`, `\'Q\'`, `\'f\'`, `\'d\'`). # Example ```python # Test case for create_array arr = create_array(\'i\', [1, 5, 3, 5, 7]) print(arr) # Expected output: array(\'i\', [1, 5, 3, 5, 7]) # Test case for array_operations results = array_operations(arr) print(results) # Expected output: # { # \'count_5\': 2, # \'reversed_array\': array(\'i\', [7, 5, 3, 5, 1]), # \'tobytes_representation\': b\'x01x00x00x00x05x00x00x00x03x00x00x00x05x00x00x00x07x00x00x00\', # \'buffer_info\': (address, 5), # \'insertion\': array(\'i\', [1, 5, 99, 3, 5, 7]), # \'removal\': array(\'i\', [1, 3, 5, 7]) # } # Test case for array_conversion lst = array_conversion(arr) print(lst) # Expected output: [1, 5, 3, 5, 7] ``` # Notes - Be mindful of edge cases, such as invalid typecodes and empty arrays. - Ensure that your functions handle the type-specific constraints and raise appropriate exceptions where necessary.","solution":"import array def create_array(typecode, elements): Create an array with the given typecode and elements. return array.array(typecode, elements) def array_operations(arr): Perform various operations on the array and return the results in a dictionary. count_5 = arr.count(5) reversed_array = array.array(arr.typecode, reversed(arr)) tobytes_representation = arr.tobytes() buffer_info = arr.buffer_info() insertion_array = array.array(arr.typecode, arr) insertion_array.insert(2, 99) removal_array = array.array(arr.typecode, arr) if 5 in removal_array: removal_array.remove(5) return { \'count_5\': count_5, \'reversed_array\': reversed_array, \'tobytes_representation\': tobytes_representation, \'buffer_info\': buffer_info, \'insertion\': insertion_array, \'removal\': removal_array } def array_conversion(arr): Convert the array to a list. return list(arr)"},{"question":"# Floating Point Comparison Challenge In floating-point arithmetic, direct comparison of two float values can lead to unexpected results due to representation errors. To accurately compare two floating-point numbers for equality, one must take these approximations into account. **Task:** Write a Python function `compare_floats(a: float, b: float, tolerance: float = 1e-9) -> bool` that compares two floating-point numbers for near equality, considering a specified tolerance level to handle precision issues. # Function Signature: ```python def compare_floats(a: float, b: float, tolerance: float = 1e-9) -> bool: ``` # Input: - `a`: A floating-point number. - `b`: A floating-point number. - `tolerance`: An optional floating-point number representing the tolerance level for comparison. Default value is `1e-9`. # Output: - Returns `True` if the absolute difference between `a` and `b` is less than or equal to the specified `tolerance`, `False` otherwise. # Constraints: - Use the `math.isclose()` function is not allowed for this problem. - You must handle precision issues manually using the concept of representation errors explained in the documentation. - The values for `a` and `b` will be between `-1e10` and `1e10`. # Examples: ```python assert compare_floats(0.1 + 0.2, 0.3) == True assert compare_floats(0.1, 0.10000000000000001) == True assert compare_floats(1.000000001, 1.0, tolerance=1e-8) == True assert compare_floats(1.000000001, 1.0, tolerance=1e-10) == False ``` # Guidelines: 1. Calculate the absolute difference between `a` and `b`. 2. Determine if this difference is within the specified `tolerance` level. 3. Return `True` if the difference is within the tolerance; otherwise, return `False`. # Notes: - Take special care to consider edge cases involving very small and very large floating-point numbers as explained in the documentation. Implement the function `compare_floats` according to the given specifications.","solution":"def compare_floats(a: float, b: float, tolerance: float = 1e-9) -> bool: Compares two floating-point numbers for near equality based on a given tolerance. Args: a (float): First floating-point number. b (float): Second floating-point number. tolerance (float): The tolerance level for comparison. Default is 1e-9. Returns: bool: True if the absolute difference between `a` and `b` is within the tolerance, False otherwise. return abs(a - b) <= tolerance"},{"question":"# Question: You are tasked with creating a utility script that handles multiple types of compressed files. The script should be able to compress a given text file into various formats and decompress these files back to retrieve the original content. Requirements: 1. Implement a function `compress_file(input_file: str, output_file: str, format: str) -> None` that compresses the content of the `input_file` into `output_file` using the specified `format`. The `format` can be one of the following: `\'gzip\'`, `\'bz2\'`, `\'lzma\'`, `\'zip\'`. 2. Implement a function `decompress_file(input_file: str, output_file: str, format: str) -> None` that decompresses the `input_file` to `output_file` using the specified `format`. Constraints: - You may assume that the input files are small enough to fit into memory. - The `input_file` will always be a valid text file. - The `output_file` should be created or overwritten by the function. - Use appropriate libraries/modules as per the format requirement (`gzip`, `bz2`, `lzma`, `zipfile`). Function Signatures: ```python def compress_file(input_file: str, output_file: str, format: str) -> None: pass def decompress_file(input_file: str, output_file: str, format: str) -> None: pass ``` Example: ```python compress_file(\'example.txt\', \'example.gz\', \'gzip\') compress_file(\'example.txt\', \'example.bz2\', \'bz2\') compress_file(\'example.txt\', \'example.xz\', \'lzma\') compress_file(\'example.txt\', \'example.zip\', \'zip\') decompress_file(\'example.gz\', \'output.txt\', \'gzip\') decompress_file(\'example.bz2\', \'output.txt\', \'bz2\') decompress_file(\'example.xz\', \'output.txt\', \'lzma\') decompress_file(\'example.zip\', \'output.txt\', \'zip\') ``` Note: The `decompress_file` for `zip` format should extract the file named the same as the original input file (e.g., `example.txt`) from the ZIP archive. Testing: 1. Create a sample text file `example.txt` with some dummy content. 2. Compress this file using each available format (`gzip`, `bz2`, `lzma`, `zip`). 3. Decompress each compressed file back to a text file. 4. Verify that the decompressed files exactly match the original `example.txt`. Performance: - Solutions should efficiently handle file I/O operations. - Ensure all file streams are properly closed after operations.","solution":"import gzip import bz2 import lzma import zipfile import shutil def compress_file(input_file: str, output_file: str, format: str) -> None: with open(input_file, \'rb\') as f_in: if format == \'gzip\': with gzip.open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif format == \'bz2\': with bz2.open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif format == \'lzma\': with lzma.open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif format == \'zip\': with zipfile.ZipFile(output_file, \'w\') as z_out: z_out.write(input_file, arcname=input_file) else: raise ValueError(f\\"Unsupported format: {format}\\") def decompress_file(input_file: str, output_file: str, format: str) -> None: if format == \'gzip\': with gzip.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif format == \'bz2\': with bz2.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif format == \'lzma\': with lzma.open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif format == \'zip\': with zipfile.ZipFile(input_file, \'r\') as z_in: with z_in.open(z_in.namelist()[0]) as f_in, open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) else: raise ValueError(f\\"Unsupported format: {format}\\")"},{"question":"# Email Content Manager Implementation Problem Statement Your task is to implement a custom content manager class derived from `email.contentmanager.ContentManager`. This custom content manager, `CustomContentManager`, must handle specific MIME types and object types for its `get_content` and `set_content` methods. The manager should support the following: 1. **MIME Types Handling**: - For MIME type `text/custom`, the `get_content` should return the payload as a string by decoding it with UTF-8. - For MIME type `application/custom`, the `get_content` should return the payload as a base64 decoded bytes object. - For all other MIME types, the `get_content` should raise a `KeyError`. 2. **Object Types Handling**: - For strings, `set_content` should set the MIME type to `text/custom` and encode the string in UTF-8. - For bytes, `set_content` should set the MIME type to `application/custom` and encode the bytes in base64. - For all other object types, `set_content` should raise a `TypeError`. Additionally, implement appropriate handlers using `add_get_handler` and `add_set_handler` methods to register the types and MIME handlers mentioned above. Method Signatures ```python from email.contentmanager import ContentManager class CustomContentManager(ContentManager): def __init__(self): pass def get_content(self, msg, *args, **kw): pass def set_content(self, msg, obj, *args, **kw): pass def add_get_handler(self, key, handler): pass def add_set_handler(self, typekey, handler): pass ``` Constraints - MIME types to be handled: `text/custom`, `application/custom` - Object types to be handled: `str` and `bytes` - Encoding for `text/custom`: UTF-8 - Encoding for `application/custom`: base64 Example Usage ```python # Example MIME message object, you can use your own implementation or a suitable library class MIMEMessage: def __init__(self): self.headers = {} self.payload = None def set_payload(self, payload): self.payload = payload def get_payload(self): return self.payload def set_type(self, mime_type): self.headers[\'Content-Type\'] = mime_type # Instantiate the custom content manager manager = CustomContentManager() # Register custom handlers in the constructor manager.add_get_handler(\\"text/custom\\", manager.text_custom_get_handler) manager.add_get_handler(\\"application/custom\\", manager.application_custom_get_handler) manager.add_set_handler(str, manager.str_set_handler) manager.add_set_handler(bytes, manager.bytes_set_handler) # Create a MIME message and set content msg = MIMEMessage() manager.set_content(msg, \\"This is a custom text content\\") # Get content from MIME message content = manager.get_content(msg) print(content) # Output: This is a custom text content ``` Hints - Utilize the `get_content` and `set_content` methods\' registry mechanisms to dynamically dispatch functions based on MIME type and object type. - Make proper use of exceptions (`KeyError`, `TypeError`) to indicate unsupported MIME types or object types. Evaluation Criteria - Correct implementation of the `CustomContentManager` class methods. - Use of proper encoding/decoding mechanisms for the specified MIME types and object types. - Comprehensive test cases demonstrating the functionality of the `CustomContentManager`. Good luck!","solution":"from email.contentmanager import ContentManager import base64 class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.get_handlers = {} self.set_handlers = {} self.add_get_handler(\\"text/custom\\", self.text_custom_get_handler) self.add_get_handler(\\"application/custom\\", self.application_custom_get_handler) self.add_set_handler(str, self.str_set_handler) self.add_set_handler(bytes, self.bytes_set_handler) def get_content(self, msg, *args, **kw): content_type = msg.headers.get(\'Content-Type\') if content_type in self.get_handlers: return self.get_handlers[content_type](msg, *args, **kw) raise KeyError(f\'Unsupported MIME type {content_type}\') def set_content(self, msg, obj, *args, **kw): if type(obj) in self.set_handlers: self.set_handlers[type(obj)](msg, obj, *args, **kw) else: raise TypeError(f\'Unsupported object type {type(obj)}\') def add_get_handler(self, key, handler): self.get_handlers[key] = handler def add_set_handler(self, typekey, handler): self.set_handlers[typekey] = handler def text_custom_get_handler(self, msg, *args, **kw): return msg.get_payload().decode(\'utf-8\') def application_custom_get_handler(self, msg, *args, **kw): return base64.b64decode(msg.get_payload()) def str_set_handler(self, msg, obj, *args, **kw): msg.set_type(\'text/custom\') msg.set_payload(obj.encode(\'utf-8\')) def bytes_set_handler(self, msg, obj, *args, **kw): msg.set_type(\'application/custom\') msg.set_payload(base64.b64encode(obj))"},{"question":"# Advanced Sorting with Custom Objects You are given a list of employee records, where each record is a dictionary containing the following fields: - `name`: The employee\'s name (string) - `department`: The department the employee works in (string) - `salary`: The employee\'s monthly salary (integer) Your task is to implement a function `sort_employees`. This function will take the following parameters: 1. `employees` (list of dictionaries): The list of employee records. 2. `primary_key` (string): The primary field to sort by (either `\\"department\\"` or `\\"salary\\"`). 3. `secondary_key` (string): The secondary field to sort by (either `\\"name\\"` or `\\"salary\\"`). 4. `reverse_primary` (boolean): Whether to sort the primary key in descending order. 5. `reverse_secondary` (boolean): Whether to sort the secondary key in descending order. The function should return a sorted list of employee records based on the specified keys and their order. # Constraints - The `employees` list contains at least one record. - The `primary_key` and `secondary_key` will always be different. - The `name` field will have unique values. # Example ```python # Sample Employee Records employees = [ {\\"name\\": \\"Alice\\", \\"department\\": \\"HR\\", \\"salary\\": 5000}, {\\"name\\": \\"Bob\\", \\"department\\": \\"Engineering\\", \\"salary\\": 7000}, {\\"name\\": \\"Charlie\\", \\"department\\": \\"HR\\", \\"salary\\": 6000}, {\\"name\\": \\"David\\", \\"department\\": \\"Engineering\\", \\"salary\\": 7000}, {\\"name\\": \\"Eve\\", \\"department\\": \\"Marketing\\", \\"salary\\": 4000}, ] # Sample Function Calls sorted_employees = sort_employees(employees, primary_key=\\"department\\", secondary_key=\\"salary\\", reverse_primary=False, reverse_secondary=False) print(sorted_employees) sorted_employees = sort_employees(employees, primary_key=\\"salary\\", secondary_key=\\"name\\", reverse_primary=True, reverse_secondary=False) print(sorted_employees) ``` # Expected Output ```python [ {\\"name\\": \\"Bob\\", \\"department\\": \\"Engineering\\", \\"salary\\": 7000}, {\\"name\\": \\"David\\", \\"department\\": \\"Engineering\\", \\"salary\\": 7000}, {\\"name\\": \\"Alice\\", \\"department\\": \\"HR\\", \\"salary\\": 5000}, {\\"name\\": \\"Charlie\\", \\"department\\": \\"HR\\", \\"salary\\": 6000}, {\\"name\\": \\"Eve\\", \\"department\\": \\"Marketing\\", \\"salary\\": 4000}, ] [ {\\"name\\": \\"David\\", \\"department\\": \\"Engineering\\", \\"salary\\": 7000}, {\\"name\\": \\"Bob\\", \\"department\\": \\"Engineering\\", \\"salary\\": 7000}, {\\"name\\": \\"Charlie\\", \\"department\\": \\"HR\\", \\"salary\\": 6000}, {\\"name\\": \\"Alice\\", \\"department\\": \\"HR\\", \\"salary\\": 5000}, {\\"name\\": \\"Eve\\", \\"department\\": \\"Marketing\\", \\"salary\\": 4000}, ] ``` # Implementation Notes - Use the `operator.attrgetter` to facilitate multi-field sorting. - Ensure the order is stable to maintain the original sequence for employees with the same field values.","solution":"def sort_employees(employees, primary_key, secondary_key, reverse_primary=False, reverse_secondary=False): Sorts a list of employee records by primary and secondary keys and specified order. Parameters: employees (list of dict): List of employee records. primary_key (str): The primary field to sort by. secondary_key (str): The secondary field to sort by. reverse_primary (bool): Whether to sort the primary key in descending order. reverse_secondary (bool): Whether to sort the secondary key in descending order. Returns: list of dict: The sorted list of employee records. return sorted( employees, key=lambda x: (x[primary_key], x[secondary_key]), reverse=reverse_primary )[::-1] if reverse_secondary else sorted( employees, key=lambda x: (x[primary_key], x[secondary_key]), reverse=reverse_primary )"},{"question":"**Question: Implement Permutation Feature Importance Evaluation** Given a fitted regression model and a tabular dataset, implement a function to compute permutation feature importance and evaluate the contribution of each feature to the model\'s performance. Your task is to write a function `compute_permutation_feature_importance` that calculates and returns the feature importances along with the standard deviations. Additionally, handle different scoring metrics and address correlated features. # Function Signature ```python from typing import List, Tuple, Dict import numpy as np from sklearn.base import BaseEstimator from sklearn.utils import Bunch def compute_permutation_feature_importance(model: BaseEstimator, X: np.ndarray, y: np.ndarray, n_repeats: int, scoring: List[str], cluster_correlated_features: bool = False) -> Dict[str, Dict[str, Tuple[float, float]]]: pass ``` # Input - `model` (BaseEstimator): A fitted regression model. - `X` (np.ndarray): The feature matrix of shape (n_samples, n_features). - `y` (np.ndarray): The target array of shape (n_samples,). - `n_repeats` (int): Number of times the feature values are shuffled for evaluation. - `scoring` (List[str]): List of scoring metrics to evaluate importance. - `cluster_correlated_features` (bool): If True, clusters correlated features and computes importance for clusters instead of individual features. # Output - Returns a dictionary where keys are the scoring metrics, and each value is a dictionary mapping feature names (or cluster names) to (importance_mean, importance_std) tuples. # Constraints and Limitations - You must use sklearnâ€™s `permutation_importance` function for computing feature importances. - Ensure the function can handle multiple scoring metrics efficiently. - When `cluster_correlated_features` is True, use correlation-based clustering to determine feature clusters. # Example Usage ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance # Load dataset diabetes = load_diabetes() X_train, X_val, y_train, y_val = train_test_split(diabetes.data, diabetes.target, random_state=0) # Train model model = Ridge(alpha=1e-2).fit(X_train, y_train) # Compute permutation feature importance scoring_metrics = [\'r2\', \'neg_mean_squared_error\'] result = compute_permutation_feature_importance(model, X_val, y_val, n_repeats=30, scoring=scoring_metrics, cluster_correlated_features=True) # Print results for metric, importances in result.items(): print(f\\"Metric: {metric}\\") for feature, (importance_mean, importance_std) in importances.items(): print(f\\"{feature:<8}{importance_mean:.3f} +/- {importance_std:.3f}\\") ``` # Performance Requirements - The function should efficiently handle datasets with up to 1000 samples and 50 features. - Ensure that when clustering features, the correlation calculation and clustering process are performed using a computationally efficient approach. You are expected to follow good programming practices, including appropriate use of functions, modular code, and comments explaining key steps.","solution":"from typing import List, Tuple, Dict import numpy as np from sklearn.base import BaseEstimator from sklearn.utils import Bunch from sklearn.inspection import permutation_importance from sklearn.metrics import get_scorer from scipy.cluster.hierarchy import linkage, fcluster from scipy.spatial.distance import pdist def compute_permutation_feature_importance(model: BaseEstimator, X: np.ndarray, y: np.ndarray, n_repeats: int, scoring: List[str], cluster_correlated_features: bool = False) -> Dict[str, Dict[str, Tuple[float, float]]]: feature_names = np.arange(X.shape[1]) if cluster_correlated_features: correlation_matrix = np.corrcoef(X, rowvar=False) distance_matrix = 1 - np.abs(correlation_matrix) linkage_matrix = linkage(pdist(distance_matrix), method=\'complete\') clusters = fcluster(linkage_matrix, t=0.5, criterion=\'distance\') cluster_dict = {} for idx, feature in enumerate(feature_names): cluster = clusters[idx] if cluster not in cluster_dict: cluster_dict[cluster] = [] cluster_dict[cluster].append(feature) feature_sets = list(cluster_dict.values()) else: feature_sets = [[feature] for feature in feature_names] results = {} for metric_name in scoring: scorer = get_scorer(metric_name) clustered_importances = {} for idx, feature_set in enumerate(feature_sets): # Create a mask for the features in the cluster mask = np.array([f in feature_set for f in feature_names]) # Create a copy of X where all selected features are shuffled X_permuted = X.copy() np.random.shuffle(X_permuted[:, mask]) # Compute the performance with the shuffled features importance = permutation_importance(model, X_permuted, y, n_repeats=n_repeats, scoring=scorer, random_state=0) # Aggregate the results cluster_name = f\'Cluster {idx + 1}\' if cluster_correlated_features else f\'Feature {feature_set[0]}\' clustered_importances[cluster_name] = (np.mean(importance[\'importances_mean\']), np.std(importance[\'importances_mean\'])) results[metric_name] = clustered_importances return results"},{"question":"You are tasked with writing a function `sequence_operations` that processes and manipulates different types of Python sequences. This function should demonstrate proficiency with both immutable and mutable sequences, as well as various sequence operations. Implement the function `sequence_operations` with the following specification: **Function:** ```python def sequence_operations(lst: list[int], tpl: tuple[int], rg: range, substring: str) -> dict: pass ``` **Input:** 1. `lst`: A list of integers `[a, b, c, d]`. 2. `tpl`: A tuple of integers `(a, b, c, d)`. 3. `rg`: A `range` object. 4. `substring`: A string consisting of a word or phrase. **Output:** - A dictionary with the following keys and their corresponding values: 1. `original_list`: The original list `lst`. 2. `original_tuple`: The original tuple `tpl`. 3. `original_range`: A list representation of the range `rg`. 4. `list_sum`: Sum of all elements in the list `lst`. 5. `reversed_tuple`: The tuple `tpl` reversed. 6. `range_step_2`: A list representation of the range with a step of 2. 7. `substring_occurrences`: The number of times `substring` occurs in `lst` when `lst` is treated as a concatenated string of its elements. 8. `modified_list`: The list `lst` after removing the highest and lowest elements. 9. `sorted_tuple`: The tuple `tpl` sorted in ascending order. 10. `range_to_string`: A single string concatenation of all numbers in the range `rg`. **Constraints and Considerations:** 1. List `lst` will have exactly four integers. 2. Tuple `tpl` will also have exactly four integers. 3. Range `rg` can be of any size. 4. The `substring` will never be empty. 5. The function should handle lists, tuples, and ranges efficiently and make use of appropriate sequence methods to achieve the transformations. **Example:** ```python >>> sequence_operations([8, 15, 3, 7], (5, 2, 9, 6), range(10), \\"15\\") { \'original_list\': [8, 15, 3, 7], \'original_tuple\': (5, 2, 9, 6), \'original_range\': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \'list_sum\': 33, \'reversed_tuple\': (6, 9, 2, 5), \'range_step_2\': [0, 2, 4, 6, 8], \'substring_occurrences\': 1, \'modified_list\': [8, 7], \'sorted_tuple\': (2, 5, 6, 9), \'range_to_string\': \'0123456789\' } ``` **Tasks to be completed within the function:** 1. Validate and process the input sequences effectively. 2. Use Python\'s sequence operations, slicing, and methods to perform the transformations. 3. Ensure the code is efficient, readable, and makes the best use of Pythonâ€™s capabilities. 4. Add appropriate error handling where necessary.","solution":"def sequence_operations(lst: list[int], tpl: tuple[int], rg: range, substring: str) -> dict: Processes and manipulates different types of Python sequences. Args: - lst: A list of integers [a, b, c, d]. - tpl: A tuple of integers (a, b, c, d). - rg: A range object. - substring: A string consisting of a word or phrase. Returns: - A dictionary containing various manipulations of input sequences. list_sum = sum(lst) reversed_tuple = tuple(reversed(tpl)) range_step_2 = list(rg)[::2] lst_string = \'\'.join(map(str, lst)) substring_occurrences = lst_string.count(substring) modified_list = sorted(lst)[1:-1] sorted_tuple = tuple(sorted(tpl)) range_to_string = \'\'.join(map(str, rg)) return { \'original_list\': lst, \'original_tuple\': tpl, \'original_range\': list(rg), \'list_sum\': list_sum, \'reversed_tuple\': reversed_tuple, \'range_step_2\': range_step_2, \'substring_occurrences\': substring_occurrences, \'modified_list\': modified_list, \'sorted_tuple\': sorted_tuple, \'range_to_string\': range_to_string }"},{"question":"Objective: Implement a custom descriptor class called `LoggedAttributeAccess` that logs all attempts to access, set, or delete a managed attribute. Additionally, ensure that the descriptor can handle multiple attributes within the same class and that it logs each attribute using the correct attribute name. Requirements: - Implement the `LoggedAttributeAccess` descriptor class. - Use the logging module to log messages for accessing, setting, and deleting attributes. - Ensure that the descriptor correctly handles multiple attributes within a class. - Provide an example class `Person` that uses the `LoggedAttributeAccess` descriptor for attributes `name` and `age`. Input and Output: - The logging messages should follow the format: - Access: \\"Accessing {attribute_name} giving {value}\\" - Update: \\"Updating {attribute_name} to {new_value}\\" - Delete: \\"Deleting {attribute_name}\\" - Example class usage and expected logging outputs are provided below. Constraints: - Use Python\'s `logging` module for logging messages. - Assume that the class attributes managed by `LoggedAttributeAccess` are simple attributes (e.g., strings, integers). Example: ```python import logging logging.basicConfig(level=logging.INFO) class LoggedAttributeAccess: # Implement the descriptor methods class Person: name = LoggedAttributeAccess() age = LoggedAttributeAccess() def __init__(self, name, age): self.name = name self.age = age # Example usage: if __name__ == \\"__main__\\": john = Person(\\"John Doe\\", 30) print(john.name) john.age = 31 del john.name ``` Expected Logging Output: ``` INFO:root:Updating \'name\' to \'John Doe\' INFO:root:Updating \'age\' to 30 INFO:root:Accessing \'name\' giving \'John Doe\' INFO:root:Updating \'age\' to 31 INFO:root:Deleting \'name\' ``` Implementation: 1. Define the `LoggedAttributeAccess` class with appropriate `__get__`, `__set__`, and `__delete__` methods. 2. Ensure that the class `Person` correctly initializes and uses the descriptor for logging attribute access, updates, and deletions.","solution":"import logging logging.basicConfig(level=logging.INFO) class LoggedAttributeAccess: def __init__(self): self.attribute_name = None def __set_name__(self, owner, name): self.attribute_name = name def __get__(self, instance, owner): if instance is None: return self value = instance.__dict__.get(self.attribute_name) logging.info(f\\"Accessing \'{self.attribute_name}\' giving {value}\\") return value def __set__(self, instance, value): logging.info(f\\"Updating \'{self.attribute_name}\' to {value}\\") instance.__dict__[self.attribute_name] = value def __delete__(self, instance): logging.info(f\\"Deleting \'{self.attribute_name}\'\\") if self.attribute_name in instance.__dict__: del instance.__dict__[self.attribute_name] class Person: name = LoggedAttributeAccess() age = LoggedAttributeAccess() def __init__(self, name, age): self.name = name self.age = age # Example Usage if __name__ == \\"__main__\\": john = Person(\\"John Doe\\", 30) print(john.name) john.age = 31 del john.name"},{"question":"You are tasked with writing a Python function that ensures `pip` is installed and up-to-date in a specified environment. Specifically, you will implement a function that uses the `ensurepip` module to handle this task. Your function should take into account various installation options and handle potential errors gracefully. Function Signature ```python def setup_pip(root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False, verbosity: int = 0) -> str: pass ``` Parameters - `root` (str): Specifies an alternative root directory to install `pip` into. Default is `None`. - `upgrade` (bool): Indicates whether to upgrade an existing installation of `pip`. Default is `False`. - `user` (bool): Whether to install `pip` into the user site packages directory. Default is `False`. - `altinstall` (bool): If set to `True`, the script `pipX` will not be installed. Default is `False`. - `default_pip` (bool): If set to `True`, the `pip` script will also be installed. Default is `False`. - `verbosity` (int): Controls the output level from the bootstrapping operation. Default is `0`. Return - Returns a string indicating success or a specific error message. Constraints - The function should not abruptly terminate in case of errors but should return appropriate error messages. - If both `altinstall` and `default_pip` are set, return an error message: \\"Invalid configuration: Cannot set both altinstall and default_pip.\\" Example Usage ```python result = setup_pip(root=\'/custom/path\', upgrade=True, verbosity=1) print(result) # Should output a success message if pip is installed/upgraded successfully. ``` Notes - You may use the `ensurepip.bootstrap` function for the main functionality. - Handle any exceptions that may occur during the process and return meaningful error messages. Your goal is to ensure that the `pip` installation process is robust, clear, and well-handled for different environments and configurations.","solution":"import ensurepip import sys def setup_pip(root: str = None, upgrade: bool = False, user: bool = False, altinstall: bool = False, default_pip: bool = False, verbosity: int = 0) -> str: if altinstall and default_pip: return \\"Invalid configuration: Cannot set both altinstall and default_pip.\\" options = { \'root\': root, \'upgrade\': upgrade, \'user\': user, \'altinstall\': altinstall, \'default_pip\': default_pip, \'verbosity\': verbosity } # Construct the arguments while filtering out None values and converting booleans arguments = [key for key, value in options.items() if isinstance(value, bool) and value] if root: arguments.append(f\'--root={root}\') try: ensurepip.bootstrap(*arguments) return \\"pip install/upgrade successful.\\" except Exception as e: return f\\"Error occurred: {str(e)}\\""},{"question":"# Advanced Python Assignment: Working with Time Zones using the `zoneinfo` Module Problem Statement You are required to implement a function that does the following: 1. Takes a list of datetime strings and corresponding timezone strings. 2. Converts each datetime string to a timezone-aware datetime object using the given timezone string. 3. Adjusts each datetime object to account for possible daylight saving time transitions and ambiguous times using the `fold` attribute. 4. Serializes these datetime objects into a custom string format. 5. Provides a mechanism to deserialize these custom string formats back into timezone-aware datetime objects. Function Signature ```python from typing import List, Tuple from datetime import datetime from zoneinfo import ZoneInfo def convert_datetimes(datetime_zone_pairs: List[Tuple[str, str]]) -> List[str]: pass def deserialize_datetimes(serialized_datetimes: List[str]) -> List[datetime]: pass ``` Input - `datetime_zone_pairs`: A list of tuples, where each tuple contains a datetime string (in the format `\\"%Y-%m-%d %H:%M:%S\\"`) and a timezone string (e.g., `\\"America/Los_Angeles\\"`). Output 1. `convert_datetimes`: Returns a list of serialized datetime strings in the following format: `\\"YYYY-MM-DDTHH:MM:SSÂ±HH:MM [TimeZone]\\"`. 2. `deserialize_datetimes`: Returns a list of timezone-aware datetime objects corresponding to the serialized datetime strings. Example ```python datetime_zone_pairs = [ (\\"2020-10-31 12:00:00\\", \\"America/Los_Angeles\\"), (\\"2020-11-01 01:00:00\\", \\"America/Los_Angeles\\"), # Ambiguous time ] serialized_datetimes = convert_datetimes(datetime_zone_pairs) # Example serialized_datetimes: [\'2020-10-31T12:00:00-07:00 [America/Los_Angeles]\', \'2020-11-01T01:00:00-07:00 [America/Los_Angeles]\', \'2020-11-01T01:00:00-08:00 [America/Los_Angeles]\'] deserialized_datetimes = deserialize_datetimes(serialized_datetimes) # Should return datetime objects matching the originals but with correct timezone adjustments ``` Constraints - Only IANA timezone strings will be provided. - You may assume the datetime strings are in valid formats and timezone strings are correct. - Handle all daylight saving time transitions and ambiguous times correctly. Notes 1. Use `ZoneInfo` for handling the timezones. 2. Ensure that the `fold` attribute is correctly configured for ambiguous times. 3. Ensure that the serialized format includes the timezone details, and deserialization should correctly restore the timezone-aware datetime objects. 4. Consider potential performance implications when handling a large set of datetime-zone pairs.","solution":"from typing import List, Tuple from datetime import datetime from zoneinfo import ZoneInfo def convert_datetimes(datetime_zone_pairs: List[Tuple[str, str]]) -> List[str]: converted_datetimes = [] for dt_str, tz_str in datetime_zone_pairs: naive_dt = datetime.strptime(dt_str, \'%Y-%m-%d %H:%M:%S\') tz = ZoneInfo(tz_str) aware_dt = naive_dt.replace(tzinfo=tz) # Handle ambiguous times by checking if `fold` should be set try: aware_dt = tz.fromutc(naive_dt.replace(tzinfo=ZoneInfo(\'UTC\'))).replace(tzinfo=tz) except ValueError: # Handle the case fromutc fails by manually setting `fold` aware_dt_folded = aware_dt.replace(fold=1) aware_dt = min(aware_dt, aware_dt_folded) offset_str = aware_dt.strftime(\'%z\') formatted_dt = f\\"{aware_dt.strftime(\'%Y-%m-%dT%H:%M:%S\')}{offset_str[:3]}:{offset_str[3:]} [{tz_str}]\\" converted_datetimes.append(formatted_dt) return converted_datetimes def deserialize_datetimes(serialized_datetimes: List[str]) -> List[datetime]: deserialized_datetimes = [] for ser_dt in serialized_datetimes: dt_str, tz_str = ser_dt.rsplit(\' \', 1) tz_str = tz_str.strip(\'[]\') aware_dt = datetime.strptime(dt_str, \'%Y-%m-%dT%H:%M:%S%z\') aware_dt = aware_dt.replace(tzinfo=ZoneInfo(tz_str)) deserialized_datetimes.append(aware_dt) return deserialized_datetimes"},{"question":"You are required to write a Python function to manage an inventory system. This function will have nested functions and should correctly handle scope and exceptions. Function Specification ```python def manage_inventory(commands): Manage inventory based on a series of commands. Parameters: commands (list): A list of command strings. Each command can be one of the following: - \\"ADD item quantity\\": Adds the given quantity of the item to the inventory. - \\"REMOVE item quantity\\": Removes the given quantity of the item from the inventory. - \\"GET item\\": Returns the quantity of the item in the inventory. Returns: output: A list of results for \\"GET\\" commands. Each result should be a tuple (item, quantity). # Your code here # Example usage: commands = [ \\"ADD apple 10\\", \\"REMOVE apple 3\\", \\"GET apple\\", \\"ADD banana 5\\", \\"GET banana\\", \\"GET orange\\", \\"REMOVE apple 10\\", \\"GET apple\\" ] print(manage_inventory(commands)) # Expected output: [(\'apple\', 7), (\'banana\', 5), (\'orange\', 0), (\'apple\', 0)] ``` # Requirements 1. **Data Structure**: Use a dictionary to store inventory items and their quantities. 2. **Nested Functions & Scope**: - Define helper functions `add_item(item, quantity)`, `remove_item(item, quantity)`, and `get_item(item)`. - Ensure correct handling of adding, removing, and getting items using proper variable scope. 3. **Exception Handling**: - Implement exception handling to manage situations like removing more items than available or getting an item not present in the inventory. In such cases, quantity should be treated as 0 without raising an exception. # Constraints - `item` is a string without spaces. - `quantity` is a non-negative integer. - Handle invalid commands gracefully by ignoring them. # Performance The function should be efficient in both time and space, handling up to 10,000 commands.","solution":"def manage_inventory(commands): Manage inventory based on a series of commands. Parameters: commands (list): A list of command strings. Each command can be one of the following: - \\"ADD item quantity\\": Adds the given quantity of the item to the inventory. - \\"REMOVE item quantity\\": Removes the given quantity of the item from the inventory. - \\"GET item\\": Returns the quantity of the item in the inventory. Returns: output: A list of results for \\"GET\\" commands. Each result should be a tuple (item, quantity). inventory = {} def add_item(item, quantity): if item in inventory: inventory[item] += quantity else: inventory[item] = quantity def remove_item(item, quantity): if item in inventory: inventory[item] -= quantity if inventory[item] < 0: inventory[item] = 0 def get_item(item): return inventory.get(item, 0) output = [] for command in commands: parts = command.split() if len(parts) < 2: continue action = parts[0] item = parts[1] if action == \\"ADD\\" and len(parts) == 3 and parts[2].isdigit(): add_item(item, int(parts[2])) elif action == \\"REMOVE\\" and len(parts) == 3 and parts[2].isdigit(): remove_item(item, int(parts[2])) elif action == \\"GET\\" and len(parts) == 2: output.append((item, get_item(item))) return output"},{"question":"<|Analysis Begin|> The provided documentation is a tutorial on using regular expressions in Python via the `re` module. It covers a wide range of topics, suitable for both beginners and more advanced users. These topics include: 1. Simple Patterns and Matching Characters. 2. Repeating Patterns. 3. Using `re` module for various operations such as compiling regular expressions, performing matches, and handling special characters. 4. Utilizing methods provided by match objects to extract information about matched strings. 5. Module-level functions like `match()`, `search()`, `findall()`, and `finditer()`. 6. Various compilation flags that modify the behaviour of regular expressions. 7. More advanced concepts like lookahead assertions, grouping, and non-capturing groups. 8. Modifying strings using `split()`, `sub()`, and `subn()` methods. 9. Common pitfalls and performance tips. Given the documentation\'s detailed coverage, it is possible to design a challenging coding question that tests both fundamental and advanced understanding of regular expressions in Python. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Title: Validating and Extracting Information from Structured Text Problem Statement: You are provided with a string containing several structured data entries. Each entry follows a specific pattern, and your task is to extract and validate specific pieces of information using Python\'s `re` module. Each entry in the string follows this format: ``` \\"Product<product_id>: Name<product_name> - Price<product_price> USD.\\" ``` - `product_id` is a numerical identifier (e.g., 1234). - `product_name` is a string of characters that may include spaces (e.g., \\"Laptop Pro 15\\"). - `product_price` is a decimal number with two decimal places (e.g., 1299.99). Your task is to implement a function `extract_product_info(entries: str) -> List[Tuple[int, str, float]]` that takes a string of entries and returns a list of tuples, each containing `product_id` (int), `product_name` (str), and `product_price` (float). If any entry is invalid (i.e., does not match the format), it should be ignored. Constraints: 1. Each entry in the string is separated by a newline character `n`. 2. Valid entries should be treated case-insensitively. 3. Use regular expressions to validate and extract the required information. Example: ```python def extract_product_info(entries: str) -> List[Tuple[int, str, float]]: # Your code here entries = Product1234: NameLaptop Pro 15 - Price1299.99 USD. Product5678: NameGaming Mouse - Price49.99 USD. InvalidEntry - Price49.99 USD. Product9101: NameOffice Chair - Price199.99 USD. output = extract_product_info(entries) print(output) ``` **Expected Output:** ``` [(1234, \'Laptop Pro 15\', 1299.99), (5678, \'Gaming Mouse\', 49.99), (9101, \'Office Chair\', 199.99)] ``` Notes: - Focus on crafting a regular expression pattern that accurately captures the required information. - Ensure your function is robust and can handle invalid entries gracefully. - Consider performance and readability while writing the regular expression and processing the entries.","solution":"import re from typing import List, Tuple def extract_product_info(entries: str) -> List[Tuple[int, str, float]]: pattern = re.compile( r\\"Product(?P<product_id>d+): Name(?P<product_name>.+?) - Price(?P<product_price>d+.d{2}) USD.\\", re.IGNORECASE ) results = [] for match in pattern.finditer(entries): product_id = int(match.group(\'product_id\')) product_name = match.group(\'product_name\') product_price = float(match.group(\'product_price\')) results.append((product_id, product_name, product_price)) return results"},{"question":"# Python `os` Module Assessment Question At your company, there is a need for a Python script that performs a series of file operations for logging purposes. The script must meet the following requirements: 1. Create a directory structure for logs within the current working directory. 2. Write log information into a log file, handling file permissions appropriately. 3. Manage file descriptors efficiently to ensure optimal performance. 4. Ensure that environment variables are handled correctly and can be modified if necessary. 5. Implement appropriate error handling to gracefully handle various potential issues. # Requirements 1. **Create a Directory Structure**: - Create a directory named `logs`. - Inside `logs`, create subdirectories for each day the script runs (e.g., `2023-10-02`). 2. **Write Logs to a File**: - Within the subdirectory for the current day, create a log file named `logfile.txt`. - Write a specified log message to this file. - Ensure the log file is created with permissions such that it is readable and writable by the owner only. 3. **File Descriptor Management**: - Open the log file using low-level file descriptors. - Ensure the file descriptor is closed after writing to avoid resource leakage. 4. **Environment Variable Management**: - Ensure an environment variable `LOGGING` exists. If it does not, create it and set it to `1`. If it exists, verify its value. 5. **Error Handling**: - Catch and handle exceptions related to file operations and environment variable handling. Log any error encountered into a separate error log file within the same directory. # Input - `log_message` (str): A string containing the log message to write. # Output - No specific output. However, the script should perform the above operations successfully. # Example Implementations ```python import os import datetime def setup_logging_directory(): # Get current date as string current_date = datetime.datetime.now().strftime(\'%Y-%m-%d\') # Create logs directory if it does not exist logs_dir = \'logs\' if not os.path.exists(logs_dir): os.makedirs(logs_dir) # Create subdirectory for current date daily_dir = os.path.join(logs_dir, current_date) if not os.path.exists(daily_dir): os.makedirs(daily_dir) return daily_dir def write_log(log_message, daily_dir): log_file_path = os.path.join(daily_dir, \'logfile.txt\') # Open log file with appropriate mode and file descriptor fd = os.open(log_file_path, os.O_WRONLY | os.O_CREAT | os.O_APPEND, 0o600) try: os.write(fd, log_message.encode(\'utf-8\')) finally: os.close(fd) def manage_environment(): if \'LOGGING\' not in os.environ: os.environ[\'LOGGING\'] = \'1\' else: logging_value = os.environ[\'LOGGING\'] if logging_value != \'1\': raise EnvironmentError(f\'Expected LOGGING=1 but found LOGGING={logging_value}\') def log_error(error_message, daily_dir): error_log_path = os.path.join(daily_dir, \'error_log.txt\') fd = os.open(error_log_path, os.O_WRONLY | os.O_CREAT | os.O_APPEND, 0o600) try: os.write(fd, error_message.encode(\'utf-8\')) finally: os.close(fd) def main(log_message): try: # Setup logging directory daily_dir = setup_logging_directory() # Write log message write_log(log_message.strip() + \'n\', daily_dir) # Manage environment variable manage_environment() except Exception as e: # If any error occurs, log it log_error(str(e) + \'n\', daily_dir) # Example usage if __name__ == \\"__main__\\": log_message = \\"This is a test log message.\\" main(log_message) ``` # Constraints - The script must support both Unix and Windows systems. - You may not use high-level file operations (such as the open() function) for writing logs, instead rely on lower-level file descriptor management functions from the `os` module. - Ensure proper file permissions and environmental variable handling as described above. - Log any errors encountered during the execution of these operations.","solution":"import os import datetime def setup_logging_directory(): # Get current date as string current_date = datetime.datetime.now().strftime(\'%Y-%m-%d\') # Create logs directory if it does not exist logs_dir = \'logs\' if not os.path.exists(logs_dir): os.makedirs(logs_dir) # Create subdirectory for current date daily_dir = os.path.join(logs_dir, current_date) if not os.path.exists(daily_dir): os.makedirs(daily_dir) return daily_dir def write_log(log_message, daily_dir): log_file_path = os.path.join(daily_dir, \'logfile.txt\') # Open log file with appropriate mode and file descriptor fd = os.open(log_file_path, os.O_WRONLY | os.O_CREAT | os.O_APPEND, 0o600) try: os.write(fd, log_message.encode(\'utf-8\')) finally: os.close(fd) def manage_environment(): if \'LOGGING\' not in os.environ: os.environ[\'LOGGING\'] = \'1\' else: logging_value = os.environ[\'LOGGING\'] if logging_value != \'1\': raise EnvironmentError(f\'Expected LOGGING=1 but found LOGGING={logging_value}\') def log_error(error_message, daily_dir): error_log_path = os.path.join(daily_dir, \'error_log.txt\') fd = os.open(error_log_path, os.O_WRONLY | os.O_CREAT | os.O_APPEND, 0o600) try: os.write(fd, error_message.encode(\'utf-8\')) finally: os.close(fd) def main(log_message): try: # Setup logging directory daily_dir = setup_logging_directory() # Write log message write_log(log_message.strip() + \'n\', daily_dir) # Manage environment variable manage_environment() except Exception as e: # If any error occurs, log it log_error(str(e) + \'n\', daily_dir) # Example usage if __name__ == \\"__main__\\": log_message = \\"This is a test log message.\\" main(log_message)"},{"question":"**Objective:** Utilize the `runpy` module to create a Python script that can execute given modules or scripts dynamically and capture specific outputs. **Problem Statement:** You are required to implement a function `execute_module_or_script` which takes the name of a module and a path to a script (one of them will be `None`), executes the specified module or script, and returns the value of a specific variable (e.g., `\'result\'`) from the executed module\'s global namespace. # Function Signature ```python def execute_module_or_script(mod_name: str = None, path_name: str = None, var_name: str = \'result\') -> any: pass ``` # Parameters: - `mod_name` (str): The name of the module to execute. It should be an absolute module name. It can be `None` if `path_name` is provided. - `path_name` (str): The path to the script file to execute. It can be `None` if `mod_name` is provided. - `var_name` (str): The name of the variable to retrieve from the executed module or script\'s global namespace. # Constraints: - Either `mod_name` or `path_name` must be provided, but not both. - The variable specified by `var_name` must exist in the global namespace of the executed module or script. # Return: - The function should return the value of the variable `var_name` from the executed module\'s or script\'s global namespace. # Example Usage: ```python # Example 1: # Suppose we have a module named \'mymodule\' with a global variable `result` set to 42 # execute_module_or_script(mod_name=\'mymodule\') should return 42 # Example 2: # Suppose we have a script at \'path/to/script.py\' with a global variable `result` set to \'hello\' # execute_module_or_script(path_name=\'path/to/script.py\') should return \'hello\' ``` # Additional Notes: - You are encouraged to use the `runpy.run_module` and `runpy.run_path` functions to achieve the desired functionality. - Any necessary imports should be handled within the function. - Ensure that the function handles exceptions properly and provides meaningful error messages when necessary.","solution":"import runpy def execute_module_or_script(mod_name: str = None, path_name: str = None, var_name: str = \'result\') -> any: Executes the given module or script and returns the value of the specified variable from the global namespace. Parameters: - mod_name (str): The name of the module to execute. It should be an absolute module name. Default is None. - path_name (str): The path to the script file to execute. Default is None. - var_name (str): The name of the variable to retrieve from the executed module or script\'s global namespace. Default is \'result\'. Returns: - any: The value of the specified variable from the executed module or script. if (mod_name is None and path_name is None) or (mod_name is not None and path_name is not None): raise ValueError(\\"Either mod_name or path_name must be provided, but not both.\\") result_dict = None if mod_name is not None: result_dict = runpy.run_module(mod_name) elif path_name is not None: result_dict = runpy.run_path(path_name) if var_name not in result_dict: raise KeyError(f\\"The variable \'{var_name}\' is not found in the executed module/script.\\") return result_dict[var_name]"},{"question":"Problem Statement You are given a class `SortedList` that maintains a list in sorted order. Your task is to implement this class using the `bisect` module\'s functions to ensure that all insertions are done in sorted order. Additionally, you need to implement methods to find values using binary search, utilizing both `bisect_left` and `bisect_right`. Requirements 1. Implement a class `SortedList` with the following methods: - `__init__`: Initializes an empty list. - `insert(value)`: Inserts `value` into the list while maintaining sorted order. - `find(value)`: Returns the index of `value` if found, otherwise returns -1. - `find_lt(value)`: Returns the largest value in the list that is less than `value`. If such a value does not exist, return `None`. - `find_gt(value)`: Returns the smallest value in the list that is greater than `value`. If such a value does not exist, return `None`. Constraints - You must use the `bisect` module\'s functions to perform the insertion and searching operations. - The list should remain sorted at all times without manually sorting it after each insertion. - Your methods should have a time complexity of O(log n) for the search operations and O(n) for the insertion operations. Input and Output Format - Example: ```python sl = SortedList() sl.insert(5) sl.insert(3) sl.insert(8) sl.insert(6) assert sl.find(5) == 2 assert sl.find(7) == -1 assert sl.find_lt(6) == 5 assert sl.find_lt(3) == None assert sl.find_gt(6) == 8 assert sl.find_gt(8) == None ``` Example Function Signature ```python from bisect import bisect_left, bisect_right, insort_left class SortedList: def __init__(self): self._list = [] def insert(self, value): insort_left(self._list, value) def find(self, value): index = bisect_left(self._list, value) if index != len(self._list) and self._list[index] == value: return index return -1 def find_lt(self, value): index = bisect_left(self._list, value) if index > 0: return self._list[index - 1] return None def find_gt(self, value): index = bisect_right(self._list, value) if index != len(self._list): return self._list[index] return None ``` Implement the class and test the example input and output provided.","solution":"from bisect import bisect_left, bisect_right, insort_left class SortedList: def __init__(self): self._list = [] def insert(self, value): insort_left(self._list, value) def find(self, value): index = bisect_left(self._list, value) if index != len(self._list) and self._list[index] == value: return index return -1 def find_lt(self, value): index = bisect_left(self._list, value) if index > 0: return self._list[index - 1] return None def find_gt(self, value): index = bisect_right(self._list, value) if index != len(self._list): return self._list[index] return None"},{"question":"# Byte Manipulation Task Objective You are given a sequence of strings and tasks to perform on them to generate byte objects according to the rules described below. Your task is to implement a function that processes these tasks and returns the result as a byte object. Function Signature ```python def process_byte_tasks(tasks: list) -> bytes: pass ``` Input - `tasks`: A list of tuples where each tuple represents a task to perform on byte objects. Each tuple is of the form: `(\'operation\', \'arg1\', \'arg2\')` where \'operation\' is one of the strings `\'create\'`, `\'concat\'`, `\'format\'`. Output - A byte object resulting from performing all tasks in sequence. Constraints - `len(tasks) <= 1000` - `1 <= len(arg1) <= 100` - `1 <= len(arg2) <= 100` - `arg1` and `arg2` are always valid strings or integers as per the requirements of the operation. Operations 1. `\'create\'`: Creates a new byte object from `arg1`. - Example: `(\'create\', \'hello\', \'\')` => creates the byte object `b\'hello\'`. 2. `\'concat\'`: Concatenates the existing byte object with the byte object from `arg1`. - Example: If the current byte object is `b\'hello\'` and the task is `(\'concat\', \' world\', \'\')`, the result will be `b\'hello world\'`. 3. `\'format\'`: Creates a new byte object with a format string and arguments. - Example: `(\'format\', \'Hello %s!\', \'world\')` => creates the byte object `b\'Hello world!\'`. Example Usage ```python tasks = [ (\'create\', \'hello\', \'\'), (\'concat\', \' world\', \'\'), (\'format\', \'Number: %d\', \'10\') ] result = process_byte_tasks(tasks) print(result) # Output should be b\'hello worldNumber: 10\' ``` Notes - Pay attention to error handling; ensure your function raises appropriate exceptions if any invalid operations are encountered. - Efficiency is important given the constraints; ensure your solution is optimized for performance.","solution":"def process_byte_tasks(tasks: list) -> bytes: result = b\'\' for task in tasks: operation, arg1, arg2 = task if operation == \'create\': result = bytes(arg1, \'utf-8\') elif operation == \'concat\': result += bytes(arg1, \'utf-8\') elif operation == \'format\': # Convert the format and argument to the respective types if \'%d\' in arg1: formatted_str = arg1 % int(arg2) elif \'%s\' in arg1: formatted_str = arg1 % arg2 else: raise ValueError(f\\"Unsupported format specifier in {arg1}\\") result += bytes(formatted_str, \'utf-8\') else: raise ValueError(f\\"Unknown operation {operation}\\") return result"},{"question":"You are tasked with implementing a Python function that simulates how the Python interpreter processes different forms of input. Your function should identify whether the input is intended to be a complete program, file input, interactive input, or expression input, and then process it accordingly. Function Signature ```python def interpret_python_input(input_type: str, input_content: str) -> str: ``` Input - `input_type` (str): One of `\\"complete_program\\"`, `\\"file_input\\"`, `\\"interactive_input\\"`, or `\\"expression_input\\"`, indicating the form of the input. - `input_content` (str): A string containing the Python code to be processed. Output - `return` (str): The output or result of executing the input code. If the input code contains an error, return an appropriate error message. Constraints 1. The execution environment will have only built-in functions available and no additional user-defined modules or functions will be pre-loaded. 2. Ensure the function handles typical use cases and also edge cases (invalid syntax, runtime errors, and so on). Requirements - For `input_type` of `\\"complete_program\\"` or `\\"file_input\\"`, you may assume that the input content will be of valid complete Python programs. - For `input_type` of `\\"interactive_input\\"`, you should handle both simple statements and compound statements. - For `input_type` of `\\"expression_input\\"`, the function should handle single Python expressions. Example ```python # Example 1 print(interpret_python_input(\\"expression_input\\", \\"2+2\\")) # Output: \\"4\\" # Example 2 input_code = def add(a, b): return a + b result = add(5, 3) print(result) print(interpret_python_input(\\"complete_program\\", input_code)) # Output: \\"8\\" # Example 3 print(interpret_python_input(\\"interactive_input\\", \\"x = 10nx + 5n\\")) # Output: \\"15\\" ``` Hints - You can use built-in functions like `exec()` for complete programs and `eval()` for expressions. - To capture output from `print` statements, consider redirecting the standard output temporarily.","solution":"import sys import io def interpret_python_input(input_type: str, input_content: str) -> str: try: # Redirect standard output to capture print statements old_stdout = sys.stdout sys.stdout = io.StringIO() if input_type == \\"complete_program\\": exec(input_content) elif input_type == \\"file_input\\": exec(input_content) elif input_type == \\"interactive_input\\": lines = input_content.splitlines() for line in lines: if line.strip(): exec(line) elif input_type == \\"expression_input\\": result = eval(input_content) return str(result) else: return \\"Invalid input type\\" # Capture all printed output sys.stdout.seek(0) output = sys.stdout.read() return output.strip() except Exception as e: return f\\"Error: {e}\\" finally: # Reset standard output to its original value sys.stdout = old_stdout"},{"question":"# Custom Formatter Implementation You are tasked with implementing a custom formatter that extends Python\'s `Formatter` class to handle a new format syntax. This custom formatter should add functionality to apply transformations on replacement fields based on predefined tags. The transformations include: - **Upper**: Converts the string to uppercase. - **Lower**: Converts the string to lowercase. - **Reverse**: Reverses the string. Your custom formatter should support these transformations in addition to the existing features of Python\'s `Formatter`. Specifically, the custom transformations should be specified in the format string using the syntax `{field_name|transformation}`. Example Given the input data: ```python data = { \\"name\\": \\"John Doe\\", \\"profession\\": \\"developer\\" } ``` And the format string: ```python \\"Name: {name|upper}, Profession: {profession|reverse}\\" ``` The output should be: ```python \\"Name: JOHN DOE, Profession: repoleved\\" ``` # Your Task 1. Implement a class `CustomFormatter` that inherits from `string.Formatter`. 2. Override the necessary methods to implement the transformation functionality. 3. Ensure that your custom formatter can handle nested fields and default formatting as well. Input and Output Specification - **Input**: - A dictionary (possibly nested) containing the values for replacement fields. - A format string that may contain field names with optional transformations. - **Output**: - A formatted string with the specified transformations applied. Constraints - Assume the input dictionary keys and format string are well-formed and follow the specified rules. - Handle cases where the transformation tag is not recognized by keeping the original field value as it is. # Example Usage ```python formatter = CustomFormatter() data = { \\"name\\": \\"Alice\\", \\"profession\\": \\"Engineer\\", \\"location\\": \\"New York\\" } format_string = \\"Name: {name|upper}, Job: {profession|lower}, City: {location|reverse}\\" print(formatter.format(format_string, **data)) # Output: \\"Name: ALICE, Job: engineer, City: kroY weN\\" ``` Hints: - You might find it useful to override the `get_field` and `format_field` methods of the `Formatter` class. - Pay special attention to the format string parsing to correctly identify and apply transformations.","solution":"import string class CustomFormatter(string.Formatter): Custom Formatter that supports additional transformations like upper, lower, and reverse. def get_field(self, field_name, args, kwargs): field_name, transformation = self.parse_transformation(field_name) value = super().get_field(field_name, args, kwargs)[0] if transformation: value = self.apply_transformation(value, transformation) return value, field_name def parse_transformation(self, field_name): if \'|\' in field_name: field_name, transformation = field_name.split(\'|\') else: transformation = None return field_name, transformation def apply_transformation(self, value, transformation): if transformation == \\"upper\\": return str(value).upper() elif transformation == \\"lower\\": return str(value).lower() elif transformation == \\"reverse\\": return str(value)[::-1] else: return value # Example Usage # formatter = CustomFormatter() # # data = { # \\"name\\": \\"Alice\\", # \\"profession\\": \\"Engineer\\", # \\"location\\": \\"New York\\" # } # # format_string = \\"Name: {name|upper}, Job: {profession|lower}, City: {location|reverse}\\" # # print(formatter.format(format_string, **data)) # Output: \\"Name: ALICE, Job: engineer, City: kroY weN\\""},{"question":"# Multithreaded Prime Number Generator **Objective:** Implement a multithreaded Python program that generates prime numbers within a specified range and writes them to a file. Synchronization must be handled to ensure thread-safe writing to the file. **Requirements:** 1. **Threads:** - Create a class `PrimeThread` that extends `threading.Thread`. - Each thread will generate prime numbers within a given sub-range of the entire range. 2. **Synchronization:** - Use a `threading.Lock` to ensure that only one thread writes to the output file at a time. 3. **File Writing:** - Each prime number found should be written to the file `primes.txt`. 4. **Configuration:** - Allow the user to specify the number of threads and the range of numbers to search for primes. 5. **Constraints:** - Efficiently utilize threading to minimize runtime. - Handle exceptions and ensure all threads complete gracefully. **Input:** - Two integers, `start` and `end`, specifying the range [start, end). - An integer `num_threads`, specifying the number of threads to use. **Output:** - Write all prime numbers found in the specified range to the file `primes.txt`. # Implementation 1. **PrimeThread Class:** - Implement the initialization to accept sub-range endpoints and a lock. - Override the `run` method to perform prime number generation and file writing within the lock. 2. **Helper Functions:** - Implement a function to generate prime numbers. - Implement a function to divide the total range into sub-ranges based on the number of threads. 3. **Main Function:** - Parse input. - Create the specified number of `PrimeThread` objects, each with its sub-range and the shared lock. - Start and join all threads. - Ensure proper exception handling and cleanup. # Example Usage ```python import threading class PrimeThread(threading.Thread): def __init__(self, start, end, lock): threading.Thread.__init__(self) self.start_num = start self.end_num = end self.lock = lock def run(self): for num in range(self.start_num, self.end_num): if self.is_prime(num): with self.lock: with open(\'primes.txt\', \'a\') as f: f.write(f\\"{num}n\\") def is_prime(self, n): if n < 2: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def main(start, end, num_threads): lock = threading.Lock() thread_list = [] range_length = (end - start) // num_threads for i in range(num_threads): thread_start = start + i * range_length thread_end = start + (i + 1) * range_length if i != num_threads - 1 else end thread = PrimeThread(thread_start, thread_end, lock) thread_list.append(thread) thread.start() for thread in thread_list: thread.join() if __name__ == \\"__main__\\": main(1, 100, 4) # Example usage with range 1 to 100 and 4 threads ``` **Notes:** - Ensure to handle boundary conditions gracefully. - Make sure to handle files and threads properly to avoid deadlocks and race conditions.","solution":"import threading class PrimeThread(threading.Thread): def __init__(self, start, end, lock): threading.Thread.__init__(self) self.start_num = start self.end_num = end self.lock = lock def run(self): for num in range(self.start_num, self.end_num): if self.is_prime(num): with self.lock: with open(\'primes.txt\', \'a\') as f: f.write(f\\"{num}n\\") def is_prime(self, n): if n < 2: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def main(start, end, num_threads): lock = threading.Lock() thread_list = [] range_length = (end - start) // num_threads for i in range(num_threads): thread_start = start + i * range_length thread_end = start + (i + 1) * range_length if i != num_threads - 1 else end thread = PrimeThread(thread_start, thread_end, lock) thread_list.append(thread) thread.start() for thread in thread_list: thread.join() if __name__ == \\"__main__\\": main(1, 100, 4) # Example usage with range 1 to 100 and 4 threads"},{"question":"You are provided with a `Maildir` mailbox containing emails, and your task is to implement a function that processes these emails and categorizes them into different mailboxes based on the subject of each email. Specifically, emails should be categorized into three mailboxes: `Work`, `Personal`, and `Others`. **Requirements:** 1. Implement a function `categorize_emails(maildir_path, work_path, personal_path, others_path)` that categorizes emails. 2. The function should categorize emails based on the following rules: - Emails with \\"work\\" in the subject (case-insensitive) should be moved to the `Work` mailbox. - Emails with \\"personal\\" in the subject (case-insensitive) should be moved to the `Personal` mailbox. - All other emails should be moved to the `Others` mailbox. 3. Ensure that any concurrent access to the mailboxes is handled properly to avoid data corruption. 4. Implement proper error handling to manage malformed messages or other potential issues. **Function Signature:** ```python def categorize_emails(maildir_path: str, work_path: str, personal_path: str, others_path: str) -> None: pass ``` **Input:** - `maildir_path` (str): The path to the source `Maildir` mailbox. - `work_path` (str): The path to the `Work` `Maildir` mailbox. - `personal_path` (str): The path to the `Personal` `Maildir` mailbox. - `others_path` (str): The path to the `Others` `Maildir` mailbox. **Output:** - The function does not return a value but should properly categorize and move the emails to the respective mailboxes. **Example:** Assume the following paths: - Source `Maildir`: `/home/user/Maildir` - Work `Maildir`: `/home/user/Maildir_Work` - Personal `Maildir`: `/home/user/Maildir_Personal` - Others `Maildir`: `/home/user/Maildir_Others` ```python # Example usage: categorize_emails(\'/home/user/Maildir\', \'/home/user/Maildir_Work\', \'/home/user/Maildir_Personal\', \'/home/user/Maildir_Others\') ``` **Constraints:** - Ensure thread safety and proper locking mechanisms when accessing and modifying the mailboxes. - Handle errors gracefully, particularly malformed messages and missing mailboxes. - Optimize the function for performance, especially if the source `Maildir` contains a large number of emails. **Hints:** - Use the `mailbox.Maildir` class to interact with the mailboxes. - Utilize the `lock()` and `unlock()` methods to handle concurrent access. - Use the `get()` method to safely retrieve message subjects.","solution":"import mailbox import os import shutil from email.errors import MessageParseError def categorize_emails(maildir_path: str, work_path: str, personal_path: str, others_path: str) -> None: Categorizes emails from maildir_path into work_path, personal_path, and others_path mailboxes. Arguments: maildir_path -- the source Maildir path containing the emails. work_path -- the Maildir path where work-related emails will be moved. personal_path -- the Maildir path where personal emails will be moved. others_path -- the Maildir path where other emails will be moved. # Open the source Maildir src_maildir = mailbox.Maildir(maildir_path, factory=None, create=False) # Open the destination Maildirs work_maildir = mailbox.Maildir(work_path, factory=None, create=True) personal_maildir = mailbox.Maildir(personal_path, factory=None, create=True) others_maildir = mailbox.Maildir(others_path, factory=None, create=True) src_maildir.lock() try: for key, msg in src_maildir.items(): try: subject = msg.get(\'subject\', \'\').lower() if \'work\' in subject: destination = work_maildir elif \'personal\' in subject: destination = personal_maildir else: destination = others_maildir # Add the message to the destination mailbox and delete from source destination.add(msg) src_maildir.remove(key) except MessageParseError: print(f\\"Message with key {key} is malformed and will be skipped.\\") finally: src_maildir.unlock() # Flush and unlock all the maildirs work_maildir.flush() personal_maildir.flush() others_maildir.flush() work_maildir.unlock() personal_maildir.unlock() others_maildir.unlock()"},{"question":"**Kernel Density Estimation with scikit-learn** # Problem Statement You are provided with a dataset of 2D points representing a bimodal distribution. Your task is to implement and apply the `KernelDensity` estimator from scikit-learn to perform kernel density estimation on this dataset. You will experiment with different kernel types and bandwidth parameters to visualize the density estimates. # Function Signature ```python def perform_kde(X: np.ndarray, kernels: List[str], bandwidths: List[float]) -> Dict[str, Dict[str, np.ndarray]]: Perform Kernel Density Estimation on the given dataset using different kernels and bandwidths. Parameters: - X (np.ndarray): A 2D numpy array of shape (n_samples, 2) representing the dataset. - kernels (List[str]): A list of strings representing kernel types to be used in KDE. - bandwidths (List[float]): A list of floats representing bandwidth values to be used in KDE. Returns: Dict[str, Dict[str, np.ndarray]]: A nested dictionary where: - The outer keys are kernel types. - The inner keys are bandwidth values. - The values are 1D numpy arrays representing the log density of the samples (output of `score_samples` method). pass ``` # Requirements 1. **Input Format**: - `X` will be a 2D numpy array of shape `(n_samples, 2)` representing the dataset. - `kernels` is a list of strings representing different kernel types (`\'gaussian\'`, `\'tophat\'`, `\'epanechnikov\'`, `\'exponential\'`, `\'linear\'`, `\'cosine\'`). - `bandwidths` is a list of positive float values representing different bandwidth parameters. 2. **Output Format**: - A nested dictionary as specified in the function signature. Each combination of kernel type and bandwidth should have a corresponding log density array. 3. **Constraints**: - `X` will have at least 10 samples. - `kernels` will always contain valid kernel types. - `bandwidths` will always contain valid positive floats. 4. **Functionality**: - Use scikit-learn\'s `KernelDensity` for performing the KDE. - Calculate the log density score of the input samples for each combination of kernel and bandwidth. - Return the results in the specified dictionary format. # Example ```python # Example input X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [1, 1], [2, 1], [3, 2]]) kernels = [\'gaussian\', \'tophat\'] bandwidths = [0.5, 1.0] # Expected output (values will vary due to the nature of KDE): { \'gaussian\': { 0.5: np.array([-4.1, -4.2, ..., -3.9]), 1.0: np.array([-3.7, -3.8, ..., -3.5]) }, \'tophat\': { 0.5: np.array([-4.3, -4.4, ..., -3.8]), 1.0: np.array([-3.8, -3.9, ..., -3.5]) } } ``` Make sure to test and validate your function with different combinations of kernels and bandwidths. Visualize the results to see how the kernel and bandwidth parameters affect the density estimates.","solution":"from typing import List, Dict import numpy as np from sklearn.neighbors import KernelDensity def perform_kde(X: np.ndarray, kernels: List[str], bandwidths: List[float]) -> Dict[str, Dict[str, np.ndarray]]: Perform Kernel Density Estimation on the given dataset using different kernels and bandwidths. Parameters: - X (np.ndarray): A 2D numpy array of shape (n_samples, 2) representing the dataset. - kernels (List[str]): A list of strings representing kernel types to be used in KDE. - bandwidths (List[float]): A list of floats representing bandwidth values to be used in KDE. Returns: Dict[str, Dict[str, np.ndarray]]: A nested dictionary where: - The outer keys are kernel types. - The inner keys are bandwidth values. - The values are 1D numpy arrays representing the log density of the samples (output of `score_samples` method). result = {} for kernel in kernels: result[kernel] = {} for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(X) log_density = kde.score_samples(X) result[kernel][bandwidth] = log_density return result"},{"question":"# Coding Assessment: Implementing a Simple Neural Network Module in TorchScript Objective Implement a simple neural network module using TorchScript. Your task is to demonstrate your understanding of the fundamentals of TorchScript, including type annotations, class definitions, and managing module attributes. Problem Statement You are required to create a TorchScript module called `SimpleNet` that performs basic linear transformation and activation operations. The module should have the following requirements: - An input linear layer with a specified number of input and output features. - An optional dropout layer that can be enabled or disabled. - A ReLU activation function. - A method to compute forward pass through the network. - Use type annotations for all function parameters and attributes. Specifications 1. **Class `SimpleNet`**: - **Initializer (`__init__` method)**: - Parameters: - `in_features` (int): Number of input features. - `out_features` (int): Number of output features. - `dropout` (bool): Indicates whether to include a dropout layer. - The initializer should initialize a linear layer using `torch.nn.Linear` and a dropout layer using `torch.nn.Dropout` if `dropout` is True. - Use type annotations for all parameters and attributes. - **Forward Method (`forward` method)**: - Parameters: - `x` (torch.Tensor): Input tensor. - Returns: - A tensor after applying the linear layer, dropout (if enabled), and ReLU activation on input `x`. 2. **Example Usage**: ```python import torch @torch.jit.script class SimpleNet: def __init__(self, in_features: int, out_features: int, dropout: bool): self.linear = torch.nn.Linear(in_features, out_features) self.dropout = torch.nn.Dropout(0.5) if dropout else None self.relu = torch.nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.linear(x) if self.dropout is not None: x = self.dropout(x) x = self.relu(x) return x # Example usage net = SimpleNet(10, 5, True) input_tensor = torch.rand(2, 10) output = net.forward(input_tensor) print(output) ``` Constraints - The module should only use types and constructs supported by TorchScript. - Ensure the module and methods are properly annotated with type hints. - Make sure the dropout layer is only applied if `dropout` is set to True. Evaluation - Correct implementation of the `SimpleNet` class following the requirements. - Proper use of type annotations. - Correct handling of optional dropout layer based on the input parameter. - Accuracy and efficiency of forward pass logic. Submit your implementation of the `SimpleNet` class and a brief explanation of your approach.","solution":"import torch from torch import nn class SimpleNet(nn.Module): def __init__(self, in_features: int, out_features: int, dropout: bool) -> None: super(SimpleNet, self).__init__() self.linear = nn.Linear(in_features, out_features) self.dropout = nn.Dropout(0.5) if dropout else None self.relu = nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.linear(x) if self.dropout is not None: x = self.dropout(x) x = self.relu(x) return x"},{"question":"# Pandas DataFrame Assessment You are provided with two datasets in the form of CSV files: 1. `employees.csv` â€“ Contains employee details. 2. `salaries.csv` â€“ Contains employee salaries. Here are the contents of these files: employees.csv ``` id,name,department_id,hire_date 1,John Smith,101,2018-06-23 2,Jane Doe,102,2019-08-17 3,Bob Johnson,101,2019-05-19 4,Emily Davis,103,2020-09-22 5,Alice Brown,104,2017-11-12 ``` salaries.csv ``` employee_id,base_salary,bonus 1,60000,5000 2,65000,6000 3,55000,7000 4,70000,8000 5,62000,5500 6,48000,4000 ``` # Task Write a Python function using pandas that performs the following operations: 1. **Load Data**: Read the `employees.csv` and `salaries.csv` files into two DataFrames. 2. **Merge DataFrames**: Merge these two DataFrames on the appropriate columns (`id` from `employees` and `employee_id` from `salaries`). 3. **Handle Missing Data**: Identify any employees in the `employees` DataFrame who do not have corresponding salary information, and fill their `base_salary` and `bonus` columns with zeros. 4. **Compute Total Compensation**: Create a new column `total_compensation` that is the sum of `base_salary` and `bonus`. 5. **Department Summary**: Group the data by `department_id` and compute: - The average `base_salary`. - The total `bonus`. 6. **Date Handling**: Compute the number of years since each employee was hired. # Input 1. `employees.csv` 2. `salaries.csv` # Output 1. The merged DataFrame after handling missing data and adding the `total_compensation` column. 2. A summary DataFrame grouped by `department_id`. 3. The number of years since each employee was hired in a new DataFrame containing `name`, `department_id`, and `years_since_hire`. # Constraints - Files are guaranteed to exist and are correctly formatted. - Use pandas to perform all operations. # Function Signature ```python import pandas as pd def employee_analysis(employees_file: str, salaries_file: str): # Load the data employees_df = pd.read_csv(employees_file) salaries_df = pd.read_csv(salaries_file) # Merge the DataFrames merged_df = pd.merge(employees_df, salaries_df, how=\'left\', left_on=\'id\', right_on=\'employee_id\') # Fill missing salary data with zeros merged_df[[\'base_salary\', \'bonus\']] = merged_df[[\'base_salary\', \'bonus\']].fillna(0) # Compute total compensation merged_df[\'total_compensation\'] = merged_df[\'base_salary\'] + merged_df[\'bonus\'] # Department summary department_summary = merged_df.groupby(\'department_id\').agg( avg_base_salary=(\'base_salary\', \'mean\'), total_bonus=(\'bonus\', \'sum\') ).reset_index() # Compute years since hire current_year = pd.Timestamp.now().year merged_df[\'hire_date\'] = pd.to_datetime(merged_df[\'hire_date\']) merged_df[\'years_since_hire\'] = current_year - merged_df[\'hire_date\'].dt.year years_since_hire_df = merged_df[[\'name\', \'department_id\', \'years_since_hire\']] return merged_df, department_summary, years_since_hire_df # Example usage merged_df, department_summary, years_since_hire_df = employee_analysis(\'employees.csv\', \'salaries.csv\') print(merged_df) print(department_summary) print(years_since_hire_df) ```","solution":"import pandas as pd def employee_analysis(employees_file: str, salaries_file: str): # Load the data employees_df = pd.read_csv(employees_file) salaries_df = pd.read_csv(salaries_file) # Merge the DataFrames merged_df = pd.merge(employees_df, salaries_df, how=\'left\', left_on=\'id\', right_on=\'employee_id\') # Fill missing salary data with zeros merged_df[[\'base_salary\', \'bonus\']] = merged_df[[\'base_salary\', \'bonus\']].fillna(0) # Compute total compensation merged_df[\'total_compensation\'] = merged_df[\'base_salary\'] + merged_df[\'bonus\'] # Department summary department_summary = merged_df.groupby(\'department_id\').agg( avg_base_salary=(\'base_salary\', \'mean\'), total_bonus=(\'bonus\', \'sum\') ).reset_index() # Compute years since hire current_year = pd.Timestamp.now().year merged_df[\'hire_date\'] = pd.to_datetime(merged_df[\'hire_date\']) merged_df[\'years_since_hire\'] = current_year - merged_df[\'hire_date\'].dt.year years_since_hire_df = merged_df[[\'name\', \'department_id\', \'years_since_hire\']] return merged_df, department_summary, years_since_hire_df"},{"question":"# Email Manipulation and Analysis Tool Problem Statement You are required to create an email manipulation tool to process raw email data strings. The tool will perform the following tasks: 1. Parse a raw email string into a `Message` object. 2. Extract and print the values of specific headers (`From`, `Subject`, `To`, `Date`). 3. Check if the email is multipart and, if so, print the content type of each part. You should define a function `process_email(raw_email: str) -> None` that takes a raw email string as input and performs the tasks outlined above. Function Signature ```python def process_email(raw_email: str) -> None: pass ``` Input - `raw_email`: A string representing the raw email data, containing headers and body. Output - The function should print the following: - The `From` header value. - The `Subject` header value. - The `To` header value. - The `Date` header value. - Whether the email is multipart. - If multipart, the content type of each part. Constraints - Assume that the provided email string will always be a valid RFC 5322 formatted email. - The function should handle cases where some headers might be missing gracefully (e.g., print `\'Header not found\'` if a required header is missing). Example ```python raw_email = From: example@example.com To: receiver@example.com Subject: Test Email Date: Fri, 21 Nov 1997 09:55:06 -0600 Content-Type: multipart/mixed; boundary=\\"frontier\\" This is a message with multiple parts in MIME format. --frontier Content-Type: text/plain This is the body of the message. --frontier Content-Type: text/html <html> <body> <p>This is the HTML version of the message.</p> </body> </html> --frontier-- process_email(raw_email) ``` Expected Output: ``` From: example@example.com Subject: Test Email To: receiver@example.com Date: Fri, 21 Nov 1997 09:55:06 -0600 Is Multipart: True Part Content Type: text/plain Part Content Type: text/html ``` Implementation Notes 1. Use the `email.message.Message` class and its methods to parse and process the email string. 2. Make sure to handle potential missing headers by providing appropriate default values or messages. 3. For multipart messages, utilize the `walk()` method to iterate over each part and print its content type.","solution":"from email import message_from_string def process_email(raw_email: str) -> None: Processes a raw email string to extract and print specific headers and check if the email is multipart. If multipart, it prints the content type of each part. msg = message_from_string(raw_email) def get_header(header_name): return msg[header_name] if msg[header_name] else \'Header not found\' print(f\\"From: {get_header(\'From\')}\\") print(f\\"Subject: {get_header(\'Subject\')}\\") print(f\\"To: {get_header(\'To\')}\\") print(f\\"Date: {get_header(\'Date\')}\\") if msg.is_multipart(): print(\\"Is Multipart: True\\") for part in msg.walk(): if part.get_content_type(): print(f\\"Part Content Type: {part.get_content_type()}\\") else: print(\\"Is Multipart: False\\")"},{"question":"You are working for a data management company that handles a large number of files. To optimize storage and improve data transfer speeds, you need to compress and archive files using different algorithms and formats. Your task is to implement a function that takes a directory path as input, compresses all the files in the directory using a specified algorithm, and saves the compressed files in the desired format (gz, bz2, xz, zip, tar). Additionally, provide functionality to decompress these files back to their original format for further processing. # Function Signature ```python def compress_directory(directory_path: str, algorithm: str, archive_format: str) -> None: Compresses all files in the specified directory using the given algorithm and saves them in the specified archive format. Parameters: - directory_path (str): The path to the directory containing the files to be compressed. - algorithm (str): The compression algorithm to use (\'zlib\', \'gzip\', \'bz2\', \'lzma\'). - archive_format (str): The archive format to save the compressed files (\'gz\', \'bz2\', \'xz\', \'zip\', \'tar\'). Returns: - None def decompress_archive(archive_path: str, extract_to: str) -> None: Decompresses the given archive file back to the specified directory. Parameters: - archive_path (str): The path to the compressed archive file. - extract_to (str): The directory where the decompressed files should be extracted. Returns: - None ``` # Input 1. `directory_path` (str): A string representing the path to the directory containing the files to be compressed. 2. `algorithm` (str): A string specifying the compression algorithm to be used. Valid options are `\'zlib\'`, `\'gzip\'`, `\'bz2\'`, `\'lzma\'`. 3. `archive_format` (str): A string representing the archive format for saving the compressed files. Valid options are `\'gz\'`, `\'bz2\'`, `\'xz\'`, `\'zip\'`, `\'tar\'`. # Output 1. The `compress_directory` function does not return any value but creates compressed archive files in the specified format within the same directory. 2. The `decompress_archive` function does not return any value but extracts the contents of the archive file to the specified directory. # Constraints - Assume the directory contains only files and no subdirectories. - The function should handle any number of files within the directory. - Ensure the decompression function can handle the archives created by the compression function. # Example ```python # Example Usage directory_path = \'/path/to/directory\' algorithm = \'gzip\' archive_format = \'zip\' # Compress the directory compress_directory(directory_path, algorithm, archive_format) # Decompress the archive archive_path = \'/path/to/compressed/archive.zip\' extract_to = \'/path/to/extracted/files\' decompress_archive(archive_path, extract_to) ``` # Notes - Utilize the appropriate Python modules (`zlib`, `gzip`, `bz2`, `lzma`, `zipfile`, `tarfile`) to accomplish the task. - Ensure to handle file I/O operations and exceptions appropriately. - Include necessary imports and ensure your code follows best practices for readability and performance.","solution":"import os import shutil import zlib import gzip import bz2 import lzma import zipfile import tarfile def compress_directory(directory_path: str, algorithm: str, archive_format: str) -> None: Compresses all files in the specified directory using the given algorithm and saves them in the specified archive format. Parameters: - directory_path (str): The path to the directory containing the files to be compressed. - algorithm (str): The compression algorithm to use (\'zlib\', \'gzip\', \'bz2\', \'lzma\'). - archive_format (str): The archive format to save the compressed files (\'gz\', \'bz2\', \'xz\', \'zip\', \'tar\'). Returns: - None if archive_format == \'zip\': with zipfile.ZipFile(os.path.join(directory_path, f\\"archive.{archive_format}\\"), \'w\') as archive: for root, _, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) archive.write(file_path, os.path.relpath(file_path, directory_path)) elif archive_format in [\'tar\', \'gz\', \'bz2\', \'xz\']: mode = \'w:\' + archive_format if archive_format != \'gz\' else \'w:gz\' with tarfile.open(os.path.join(directory_path, f\\"archive.{archive_format}\\"), mode) as archive: archive.add(directory_path, arcname=os.path.basename(directory_path)) else: raise ValueError(\\"Unsupported archive format\\") def decompress_archive(archive_path: str, extract_to: str) -> None: Decompresses the given archive file back to the specified directory. Parameters: - archive_path (str): The path to the compressed archive file. - extract_to (str): The directory where the decompressed files should be extracted. Returns: - None if archive_path.endswith(\'.zip\'): with zipfile.ZipFile(archive_path, \'r\') as archive: archive.extractall(extract_to) elif archive_path.endswith((\'.tar\', \'.gz\', \'.bz2\', \'.xz\')): with tarfile.open(archive_path, \'r\') as archive: archive.extractall(extract_to) else: raise ValueError(\\"Unsupported archive format\\")"},{"question":"# Question 1: Implement a Custom ContentHandler As a programming assessment, you are required to implement a custom SAX ContentHandler for parsing an XML document that specifically focuses on extracting and printing the text contained within each XML element. Your task is to implement a class `MyContentHandler` that inherits from `xml.sax.handler.ContentHandler`. This class should correctly handle the following: 1. **Element Start**: Print a message indicating the start of an element along with its name. 2. **Element End**: Print a message indicating the end of an element along with its name. 3. **Character Data**: Print the character data contained within elements, ensuring to handle chunks of character data that might be split across multiple calls. 4. **Document Start and End**: Print messages indicating the start and end of the document. # Input The XML content to be parsed will be in the form of a string. You need to use the `xml.sax.parseString` method to parse this string using your custom handler. # Output Your program should print messages indicating the various events (start of document, start of element, character data, end of element, end of document) as they occur. # Constraints 1. The XML content will be well-formed. 2. The XML content might have nested elements. 3. The character data might be split across multiple calls to the `characters` method. # Example Given the following XML content: ```xml <?xml version=\\"1.0\\"?> <data> <greeting>Hello, World!</greeting> <note> <to>User</to> <message>Welcome to XML parsing with SAX.</message> </note> </data> ``` Your implementation should output: ``` Start Document Start Element: data Start Element: greeting Characters: Hello, World! End Element: greeting Start Element: note Start Element: to Characters: User End Element: to Start Element: message Characters: Welcome to XML parsing with SAX. End Element: message End Element: note End Element: data End Document ``` # Implementation ```python import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def startDocument(self): print(\\"Start Document\\") def endDocument(self): print(\\"End Document\\") def startElement(self, name, attrs): print(f\\"Start Element: {name}\\") def endElement(self, name): print(f\\"End Element: {name}\\") def characters(self, content): if content.strip(): # Skip purely whitespace content print(f\\"Characters: {content}\\") # Example usage xml_content = \'\'\' <?xml version=\\"1.0\\"?> <data> <greeting>Hello, World!</greeting> <note> <to>User</to> <message>Welcome to XML parsing with SAX.</message> </note> </data> \'\'\' handler = MyContentHandler() xml.sax.parseString(xml_content, handler) ``` Your task is to complete the implementation of the `MyContentHandler` class and ensure it behaves as described above.","solution":"import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def startDocument(self): print(\\"Start Document\\") def endDocument(self): print(\\"End Document\\") def startElement(self, name, attrs): print(f\\"Start Element: {name}\\") def endElement(self, name): print(f\\"End Element: {name}\\") def characters(self, content): if content.strip(): # Skip purely whitespace content print(f\\"Characters: {content}\\") def parse_xml(xml_content): handler = MyContentHandler() xml.sax.parseString(xml_content, handler)"},{"question":"# Memory-Mapped File Operations Problem Statement You are given the task of implementing a function `process_memory_mapped_file(file_path: str, write_content: bytes, find_sub: bytes) -> Tuple[Optional[bytes], int]`. This function should use Python\'s `mmap` module to perform multiple operations on a specified file. Function Specification **Input:** - `file_path` (str): The path to the file that will be memory-mapped. - `write_content` (bytes): A byte string to be written into the memory-mapped file. This content must be written starting at the beginning of the file and replace an equal number of bytes. - `find_sub` (bytes): A byte string to search for within the memory-mapped file. **Output:** - A tuple `(modified_content: Optional[bytes], find_index: int)`: - `modified_content` (Optional[bytes]): The content of the file after modifications. If the file cannot be modified (e.g., it is read-only), return `None`. - `find_index` (int): The index of the first occurrence of `find_sub` within the file after modifications. If `find_sub` is not found, return `-1`. **Constraints:** - The file specified by `file_path` is guaranteed to be at least as large as `write_content`. - The function should handle exceptions gracefully and return appropriate values where necessary. **Example:** ```python result = process_memory_mapped_file(\\"/path/to/file.txt\\", b\\"Hello World\\", b\\"World\\") # Suppose the file initially contains b\\"Initial Content\\". # After processing, the result should be (b\\"Hello World\\", 6) ``` Requirements 1. Use the `mmap` module to map the file. 2. Write `write_content` starting from the beginning of the file. 3. Search for `find_sub` in the modified file content. 4. Handle file access modes to ensure the file can be read from and written to effectively. **Performance Considerations:** The execution should be efficient in terms of memory use and runtime, leveraging the advantages of memory-mapped file operations. Implementation Tips: - Consider using context managers (`with` statements) for file and mmap operations to ensure resources are managed correctly. - Make sure to handle different file access modes for reading and writing. - Ensure to flush the changes to the underlying file after modifications.","solution":"import mmap from typing import Optional, Tuple def process_memory_mapped_file(file_path: str, write_content: bytes, find_sub: bytes) -> Tuple[Optional[bytes], int]: try: with open(file_path, \'r+b\') as f: # Create mmap object mm = mmap.mmap(f.fileno(), 0) try: # Write content to the beginning of the file mm[:len(write_content)] = write_content mm.flush() # Find the substring in the memory-mapped file find_index = mm.find(find_sub) # Read the modified content mm.seek(0) modified_content = mm.read() return modified_content, find_index finally: mm.close() except IOError: return None, -1"},{"question":"# Pandas Coding Assessment Question Objective: Assess the ability to handle missing data in a dataset using pandas, including detecting, filling, and managing missing values through different techniques. Problem Statement: You are given a DataFrame representing the sales data for a retail store. The DataFrame `sales_data` has the following columns: - `date`: The date of the sales transaction (can contain missing values). - `product_id`: The unique identifier for the product (cannot contain missing values). - `units_sold`: The number of units sold (can contain missing values, represented by different sentinel values). - `price_per_unit`: The price per unit for the product (can contain missing values, represented by different sentinel values). - `revenue`: The total revenue generated from the sale (should be calculated as `units_sold * price_per_unit`). # Tasks: 1. **Detect Missing Values**: - Identify and count the number of missing values in each column of the DataFrame. - Create a new column `has_missing` that indicates (`True` or `False`) whether any of the columns in that row have missing values. 2. **Handle Missing Values**: - Fill the missing values in the `date` column with the previous non-missing value in the column. If the first value is missing, leave it as is. - For `units_sold` and `price_per_unit` columns, fill the missing values with the mean of their respective columns. 3. **Calculate Revenue**: - Calculate the `revenue` for each row based on `units_sold * price_per_unit`. - If either `units_sold` or `price_per_unit` is missing, set the revenue to `NaN`. 4. **Report**: - Generate a summary report of the cleaned DataFrame, including: - The total number of rows. - The number of rows containing missing values in the original DataFrame. - The number of rows containing missing values after cleaning. - The mean and median revenue. # Constraints: - Use appropriate pandas functions to handle missing data. - Do not use any other external libraries except for pandas and numpy. # Input: - A pandas DataFrame `sales_data`. # Output: - A cleaned DataFrame with missing values handled as specified. - A summary report of the cleaned DataFrame. # Example Code for Input DataFrame: ```python import pandas as pd import numpy as np # Example sales data dataframe data = { \'date\': [\'2023-01-01\', \'2023-01-02\', np.nan, \'2023-01-04\', np.nan], \'product_id\': [101, 102, 103, 104, 105], \'units_sold\': [10, np.nan, 15, np.nan, 5], \'price_per_unit\': [20.5, 30.0, np.nan, 40.0, np.nan] } sales_data = pd.DataFrame(data) ``` Solution Guidelines: 1. **Detect Missing Values**: ```python missing_counts = sales_data.isna().sum() sales_data[\'has_missing\'] = sales_data.isna().any(axis=1) ``` 2. **Handle Missing Values**: ```python sales_data[\'date\'] = sales_data[\'date\'].fillna(method=\'ffill\') sales_data[\'units_sold\'] = sales_data[\'units_sold\'].fillna(sales_data[\'units_sold\'].mean()) sales_data[\'price_per_unit\'] = sales_data[\'price_per_unit\'].fillna(sales_data[\'price_per_unit\'].mean()) ``` 3. **Calculate Revenue**: ```python sales_data[\'revenue\'] = sales_data[\'units_sold\'] * sales_data[\'price_per_unit\'] ``` 4. **Generate Report**: ```python total_rows = len(sales_data) rows_with_missing_original = sales_data[\'has_missing\'].sum() rows_with_missing_after = sales_data.isna().any(axis=1).sum() mean_revenue = sales_data[\'revenue\'].mean() median_revenue = sales_data[\'revenue\'].median() report = { \'total_rows\': total_rows, \'rows_with_missing_original\': rows_with_missing_original, \'rows_with_missing_after\': rows_with_missing_after, \'mean_revenue\': mean_revenue, \'median_revenue\': median_revenue } ``` Please implement the above tasks and generate the requested summary report.","solution":"import pandas as pd import numpy as np def handle_sales_data(sales_data): Handle missing values in sales data and generate a summary report. Parameters: - sales_data (pd.DataFrame): Sales data with missing values. Returns: - cleaned_data (pd.DataFrame): The cleaned dataframe. - report (dict): A summary report of the cleaned dataframe. # Detect Missing Values sales_data[\'has_missing\'] = sales_data.isna().any(axis=1) # Handle Missing Values sales_data[\'date\'] = sales_data[\'date\'].fillna(method=\'ffill\') sales_data[\'units_sold\'] = sales_data[\'units_sold\'].fillna(sales_data[\'units_sold\'].mean()) sales_data[\'price_per_unit\'] = sales_data[\'price_per_unit\'].fillna(sales_data[\'price_per_unit\'].mean()) # Calculate Revenue sales_data[\'revenue\'] = sales_data[\'units_sold\'] * sales_data[\'price_per_unit\'] # Generate Report total_rows = len(sales_data) rows_with_missing_original = sales_data[\'has_missing\'].sum() rows_with_missing_after = sales_data.isna().any(axis=1).sum() mean_revenue = sales_data[\'revenue\'].mean() median_revenue = sales_data[\'revenue\'].median() report = { \'total_rows\': total_rows, \'rows_with_missing_original\': rows_with_missing_original, \'rows_with_missing_after\': rows_with_missing_after, \'mean_revenue\': mean_revenue, \'median_revenue\': median_revenue } return sales_data, report"},{"question":"**FTP File Management Task** You are required to implement a Python script that demonstrates your understanding of the `ftplib` module by performing the following tasks: 1. **Connect to an FTP Server**: Your script should connect to an FTP server using credentials (host, user, password) provided by the user. 2. **Login**: After connecting to the FTP server, login using the provided credentials. 3. **Change Directory**: Navigate to a specific directory on the FTP server as specified by the user. 4. **List Files**: Retrieve and print a list of files and directories in the current directory. 5. **Download a File**: Download a specified file from the current directory to a local file. 6. **Upload a File**: Upload a local file to the current directory on the FTP server. 7. **Rename a File**: Rename a file on the FTP server. 8. **Delete a File**: Delete a specified file from the FTP server. 9. **Exception Handling**: Handle any exceptions that might occur during these operations, such as connection errors, login failures, file not found, etc. 10. **Close Connection**: Ensure the connection is properly closed after the operations are completed. # Input Format - The script should accept the following inputs from the user: 1. `host`: FTP server address 2. `user`: FTP username 3. `passwd`: FTP password 4. `directory`: Directory on the FTP server to navigate to 5. `download_file`: Name of the file to download 6. `local_file_to_upload`: Path of the local file to upload 7. `rename_from`: Current name of the file to rename 8. `rename_to`: New name for the file 9. `delete_file`: Name of the file to delete # Output Format - The script should produce the following outputs: 1. A list of files and directories in the specified directory. 2. Confirmation messages for downloading, uploading, renaming, and deleting files. 3. Any error messages encountered during the operations. # Constraints - You must use the `ftplib` module for FTP operations. - Handle all exceptions gracefully and provide meaningful error messages to the user. - The script should be modular, with clearly defined functions for each task. - Ensure the script can be executed multiple times without requiring a restart of the FTP server. # Implementation Requirements Implement the following functions: 1. `connect_to_ftp(host, user, passwd)`: - Connect to the FTP server and login. - Return the FTP connection object. 2. `change_directory(ftp, directory)`: - Change to the specified directory on the FTP server. 3. `list_files(ftp)`: - List and print the files and directories in the current directory. 4. `download_file(ftp, download_file)`: - Download the specified file to the current local directory. 5. `upload_file(ftp, local_file_to_upload)`: - Upload the specified local file to the FTP server. 6. `rename_file(ftp, rename_from, rename_to)`: - Rename the specified file on the FTP server. 7. `delete_file(ftp, delete_file)`: - Delete the specified file from the FTP server. 8. `close_connection(ftp)`: - Close the FTP connection properly. # Example Usage ```python def main(): host = input(\\"Enter FTP server address: \\") user = input(\\"Enter FTP username: \\") passwd = input(\\"Enter FTP password: \\") directory = input(\\"Enter directory to navigate to: \\") download_file = input(\\"Enter the name of the file to download: \\") local_file_to_upload = input(\\"Enter the path of the local file to upload: \\") rename_from = input(\\"Enter the current name of the file to rename: \\") rename_to = input(\\"Enter the new name for the file: \\") delete_file = input(\\"Enter the name of the file to delete: \\") # Perform FTP operations try: ftp = connect_to_ftp(host, user, passwd) change_directory(ftp, directory) list_files(ftp) download_file(ftp, download_file) upload_file(ftp, local_file_to_upload) rename_file(ftp, rename_from, rename_to) delete_file(ftp, delete_file) except Exception as e: print(f\\"An error occurred: {e}\\") finally: close_connection(ftp) if __name__ == \\"__main__\\": main() ``` Implement the defined functions to complete the script functionality.","solution":"from ftplib import FTP, error_perm def connect_to_ftp(host, user, passwd): try: ftp = FTP(host) ftp.login(user=user, passwd=passwd) print(f\\"Connected to FTP server: {host}\\") return ftp except Exception as e: print(f\\"Error connecting to FTP server: {e}\\") raise def change_directory(ftp, directory): try: ftp.cwd(directory) print(f\\"Changed to directory: {directory}\\") except error_perm as e: print(f\\"Error changing directory: {e}\\") raise def list_files(ftp): try: files = ftp.nlst() print(\\"Files and directories in the current directory:\\") for file in files: print(file) except error_perm as e: print(f\\"Error listing files: {e}\\") raise def download_file(ftp, download_file): try: with open(download_file, \'wb\') as local_file: ftp.retrbinary(f\\"RETR {download_file}\\", local_file.write) print(f\\"Downloaded file: {download_file}\\") except error_perm as e: print(f\\"Error downloading file: {e}\\") raise def upload_file(ftp, local_file_to_upload): try: with open(local_file_to_upload, \'rb\') as local_file: ftp.storbinary(f\\"STOR {local_file_to_upload}\\", local_file) print(f\\"Uploaded file: {local_file_to_upload}\\") except error_perm as e: print(f\\"Error uploading file: {e}\\") raise def rename_file(ftp, rename_from, rename_to): try: ftp.rename(rename_from, rename_to) print(f\\"Renamed file from {rename_from} to {rename_to}\\") except error_perm as e: print(f\\"Error renaming file: {e}\\") raise def delete_file(ftp, delete_file): try: ftp.delete(delete_file) print(f\\"Deleted file: {delete_file}\\") except error_perm as e: print(f\\"Error deleting file: {e}\\") raise def close_connection(ftp): try: ftp.quit() print(\\"Closed FTP connection\\") except Exception as e: print(f\\"Error closing FTP connection: {e}\\") raise"},{"question":"Objective You need to demonstrate your understanding of the `copy` module\'s shallow and deep copy operations by implementing custom copy behavior for complex objects. Problem Statement You are provided with a class `Person` that maintains basic individual details and their contact information. You need to define custom shallow and deep copy behavior for this class. 1. Create a class `Person` with the following attributes: - `name`: a string representing the person\'s name. - `age`: an integer representing the person\'s age. - `contacts`: a list of dictionaries where each dictionary contains two keys: `\'type\'` (string, type of contact like \'email\', \'phone\') and `\'value\'` (string, the contact value). 2. Implement the `__copy__()` method for a shallow copy such that only the top-level object is copied, but the `contacts` list remains the same object reference in both the original and copied objects. 3. Implement the `__deepcopy__()` method for a deep copy such that the entire object, including the `contacts` list, is fully copied, ensuring that changes to the copied object\'s `contacts` list do not affect the original object. Input and Output Formats **Input:** 1. Create an instance of the `Person` class with appropriate attributes. 2. Make a shallow copy of the instance using the `copy` module\'s `copy()` function. 3. Make a deep copy of the instance using the `copy` module\'s `deepcopy()` function. 4. Modify the copied instances and show the differences. Constraints - The `name` attribute is a string with a maximum length of 100 characters. - The `age` attribute is an integer between 0 and 120. - The `contacts` list will have at most 10 dictionaries. - Each dictionary in `contacts` will have `type` as \'email\' or \'phone\' and `value` as a string with a maximum length of 50 characters. Example ```python import copy class Person: def __init__(self, name, age, contacts): self.name = name self.age = age self.contacts = contacts def __copy__(self): # Implement shallow copy new_instance = Person(self.name, self.age, self.contacts) return new_instance def __deepcopy__(self, memo): # Implement deep copy new_instance = Person(self.name, self.age, copy.deepcopy(self.contacts, memo)) return new_instance # Example usage: # Original instance person1 = Person(\'Alice\', 30, [{\'type\': \'email\', \'value\': \'alice@example.com\'}, {\'type\': \'phone\', \'value\': \'123-456-7890\'}]) # Shallow copy shallow_copy_person = copy.copy(person1) # Deep copy deep_copy_person = copy.deepcopy(person1) # Modify shallow copy shallow_copy_person.contacts[0][\'value\'] = \'alice.shallow@example.com\' # Modify deep copy deep_copy_person.contacts[0][\'value\'] = \'alice.deep@example.com\' print(\\"Original contacts:\\", person1.contacts) print(\\"Shallow copied contacts:\\", shallow_copy_person.contacts) print(\\"Deep copied contacts:\\", deep_copy_person.contacts) # Expected Output: # Original contacts: [{\'type\': \'email\', \'value\': \'alice.shallow@example.com\'}, {\'type\': \'phone\', \'value\': \'123-456-7890\'}] # Shallow copied contacts: [{\'type\': \'email\', \'value\': \'alice.shallow@example.com\'}, {\'type\': \'phone\', \'value\': \'123-456-7890\'}] # Deep copied contacts: [{\'type\': \'email\', \'value\': \'alice.deep@example.com\'}, {\'type\': \'phone\', \'value\': \'123-456-7890\'}] ``` Implement the class `Person` as described and demonstrate shallow and deep copy operations with expected modifications and output.","solution":"import copy class Person: def __init__(self, name, age, contacts): self.name = name self.age = age self.contacts = contacts def __copy__(self): # Implement shallow copy new_instance = Person(self.name, self.age, self.contacts) return new_instance def __deepcopy__(self, memo): # Implement deep copy new_instance = Person(self.name, self.age, copy.deepcopy(self.contacts, memo)) return new_instance"},{"question":"# Task: Implement a Custom Asynchronous Task Manager Objective: You are tasked with implementing a custom asynchronous task manager that processes tasks using a queue system. This task manager will implement a series of functions to enqueue tasks, process them, and provide a summary of the processed tasks. # Requirements: 1. **Class Definition**: - Define a class `AsyncTaskManager` that initializes an `asyncio.Queue` with a given `maxsize`. 2. **Adding Tasks**: - Implement a coroutine method `add_task(self, task)` that adds a task to the queue using `put`. 3. **Processing Tasks**: - Implement a coroutine method `process_tasks(self)`, which continuously processes tasks from the queue until the queue is empty, utilizing the `get` and `task_done` methods. - Each task will be a dictionary containing: ```python task = { \\"id\\": int, # Unique task identifier \\"duration\\": float # Time in seconds to simulate task processing } ``` - For demonstration, simulate task processing by awaiting `asyncio.sleep(task[\\"duration\\"])`. 4. **Summary**: - Implement a method `summary(self)` that returns the number of tasks processed and the total time spent processing the tasks. # Constraints: - The manager should handle up to `maxsize` tasks concurrently. Overflow tasks should wait until the queue has available space. - All methods should handle high concurrency efficiently using `asyncio`. - Provide methods\' type annotations. # Input and Output: - The class should handle asynchronous input/output as described. Summary is expected to be called after all tasks are processed. # Example Usage: ```python import asyncio class AsyncTaskManager: def __init__(self, maxsize: int): # Initialize Async Queue async def add_task(self, task: dict): # Add task to Queue async def process_tasks(self): # Process tasks in Queue def summary(self) -> dict: # Return summary of tasks processed async def main(): manager = AsyncTaskManager(maxsize=5) tasks = [ {\\"id\\": 1, \\"duration\\": 1.0}, {\\"id\\": 2, \\"duration\\": 0.5}, {\\"id\\": 3, \\"duration\\": 0.2} # Add as many tasks as needed for testing ] add_tasks = [manager.add_task(task) for task in tasks] await asyncio.gather(*add_tasks) await manager.process_tasks() result = manager.summary() print(result) # Expected format: {\\"tasks_processed\\": int, \\"total_duration\\": float} asyncio.run(main()) ``` # Assessment Criteria: - Correctly and efficiently using `asyncio.Queue`. - Proper implementation of coroutine functions and handling of async/await syntax. - Clear and concise method implementations. - Ability to process tasks concurrently and provide the required summary.","solution":"import asyncio class AsyncTaskManager: def __init__(self, maxsize: int): self.queue = asyncio.Queue(maxsize=maxsize) self.tasks_processed = 0 self.total_duration = 0.0 async def add_task(self, task: dict) -> None: await self.queue.put(task) async def process_tasks(self) -> None: while not self.queue.empty(): task = await self.queue.get() await asyncio.sleep(task[\\"duration\\"]) self.tasks_processed += 1 self.total_duration += task[\\"duration\\"] self.queue.task_done() def summary(self) -> dict: return { \\"tasks_processed\\": self.tasks_processed, \\"total_duration\\": self.total_duration }"},{"question":"# Asyncio Event Loop Compatibility Checking **Objective**: Write a Python function that determines the appropriate asyncio event loop for the given platform and version constraints. **Instructions**: 1. Implement a function `select_event_loop(platform_name: str, version: float) -> str` that: - Takes a platform name (e.g., `\'Windows\'`, `\'macOS\'`) and a version number (e.g., `10.15` for macOS, `10` for Windows). - Returns the name of the most compatible `asyncio` event loop as a string: `\'ProactorEventLoop\'`, `\'SelectorEventLoop\'`, `\'KqueueSelector\'`, `\'SelectSelector\'`, or `\'PollSelector\'`. - Considers the constraints and limitations from the provided documentation. **Constraints**: - If the platform is Windows: - Return `\'ProactorEventLoop\'` for all versions that support it. - Otherwise, return `\'SelectorEventLoop\'`. - If the platform is macOS: - For versions <= 10.8, return `\'SelectSelector\'`. - For versions > 10.8, return `\'KqueueSelector\'`. - For any unknown platform or unsupported configuration, return `\'UnknownEventLoop\'`. **Example**: ```python assert select_event_loop(\'Windows\', 10) == \'ProactorEventLoop\' assert select_event_loop(\'macOS\', 10.7) == \'SelectSelector\' assert select_event_loop(\'macOS\', 10.15) == \'KqueueSelector\' assert select_event_loop(\'Linux\', 5.4) == \'UnknownEventLoop\' ``` **Notes**: - You can use basic string comparisons and conditionals to achieve the desired results. - No need to delve into actual implementation of event loops, just return the appropriate strings based on the input. Good luck!","solution":"def select_event_loop(platform_name: str, version: float) -> str: Determines the appropriate asyncio event loop for the given platform and version constraints. Parameters: - platform_name (str): The platform name (\'Windows\', \'macOS\', etc.) - version (float): The version number of the operating system Returns: - str: The name of the most compatible asyncio event loop. if platform_name == \'Windows\': return \'ProactorEventLoop\' # Assuming ProactorEventLoop is supported on all Windows versions elif platform_name == \'macOS\': if version <= 10.8: return \'SelectSelector\' else: return \'KqueueSelector\' else: return \'UnknownEventLoop\'"},{"question":"# Advanced Coding Assessment Question on scikit-learn Context You are a data scientist working on a project that involves clustering and classification of image data represented as feature vectors. Your task is to compute the similarity between different images using various kernels and apply these similarities for clustering and classification purposes. Problem Statement Given a set of feature vectors `X`, you need to perform the following tasks: 1. Implement a function to compute the pairwise linear, polynomial, and RBF (Radial Basis Function) kernels between the feature vectors. 2. Implement a function to compute the similarity scores for a given feature vector against a dataset using the chosen kernel. 3. Use the computed similarity scores to classify a test dataset using a simple k-nearest neighbors approach. Instructions 1. Implement a function `compute_kernel` which takes the following parameters: - `X` (2D numpy array): An array of shape (n_samples, n_features) representing the dataset. - `kernel` (str): The type of kernel to use (\'linear\', \'polynomial\', \'rbf\'). - `**kwargs`: Additional keyword arguments for the specific kernel parameters. The function should return a 2D numpy array of shape (n_samples, n_samples) representing the pairwise kernel matrix. 2. Implement a function `compute_similarity` which takes the following parameters: - `X` (2D numpy array): An array of shape (n_samples, n_features) representing the dataset. - `x_query` (1D numpy array): A feature vector of shape (n_features,). - `kernel` (str): The type of kernel to use (\'linear\', \'polynomial\', \'rbf\'). - `**kwargs`: Additional keyword arguments for the specific kernel parameters. The function should return a 1D numpy array of shape (n_samples,) representing the similarity scores of `x_query` with each sample in `X`. 3. Implement a function `knn_classify` which takes the following parameters: - `X` (2D numpy array): An array of shape (n_samples, n_features) representing the training dataset. - `y` (1D numpy array): An array of shape (n_samples,) representing the labels of the training dataset. - `X_test` (2D numpy array): An array of shape (n_test_samples, n_features) representing the test dataset. - `kernel` (str): The type of kernel to use (\'linear\', \'polynomial\', \'rbf\'). - `k` (int): The number of nearest neighbors to consider for classification. - `**kwargs`: Additional keyword arguments for the specific kernel parameters. The function should return a 1D numpy array of shape (n_test_samples,) representing the predicted labels for the test dataset. Constraints - Use the predefined kernel functions provided by `sklearn.metrics.pairwise`. - Ensure that your implementation is efficient and can handle large datasets. Example Usage ```python import numpy as np X_train = np.array([[2, 3], [3, 5], [5, 8]]) y_train = np.array([0, 1, 0]) X_test = np.array([[1, 4], [4, 6]]) # Compute the RBF kernel matrix K_rbf = compute_kernel(X_train, kernel=\'rbf\', gamma=0.1) print(K_rbf) # Compute similarity scores for a query vector using the polynomial kernel x_query = np.array([3, 4]) similarity_scores = compute_similarity(X_train, x_query, kernel=\'polynomial\', degree=2, gamma=1, coef0=0) print(similarity_scores) # Classify the test dataset using k-NN with the linear kernel y_pred = knn_classify(X_train, y_train, X_test, kernel=\'linear\', k=1) print(y_pred) ```","solution":"import numpy as np from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, rbf_kernel def compute_kernel(X, kernel=\'linear\', **kwargs): Computes the pairwise kernel matrix for the given dataset. Parameters: X (numpy array): Dataset of shape (n_samples, n_features). kernel (str): The type of kernel to use (\'linear\', \'polynomial\', \'rbf\'). **kwargs: Additional keyword arguments for the specific kernel parameters. Returns: numpy array: Kernel matrix of shape (n_samples, n_samples). if kernel == \'linear\': return linear_kernel(X, **kwargs) elif kernel == \'polynomial\': return polynomial_kernel(X, **kwargs) elif kernel == \'rbf\': return rbf_kernel(X, **kwargs) else: raise ValueError(\\"Unknown kernel: {}\\".format(kernel)) def compute_similarity(X, x_query, kernel=\'linear\', **kwargs): Computes the similarity scores for a query vector against the dataset using the chosen kernel. Parameters: X (numpy array): Dataset of shape (n_samples, n_features). x_query (numpy array): Query vector of shape (n_features,). kernel (str): The type of kernel to use (\'linear\', \'polynomial\', \'rbf\'). **kwargs: Additional keyword arguments for the specific kernel parameters. Returns: numpy array: Similarity scores of shape (n_samples,). if kernel == \'linear\': return linear_kernel(X, x_query.reshape(1, -1), **kwargs).flatten() elif kernel == \'polynomial\': return polynomial_kernel(X, x_query.reshape(1, -1), **kwargs).flatten() elif kernel == \'rbf\': return rbf_kernel(X, x_query.reshape(1, -1), **kwargs).flatten() else: raise ValueError(\\"Unknown kernel: {}\\".format(kernel)) def knn_classify(X, y, X_test, kernel=\'linear\', k=1, **kwargs): Classifies the test dataset using k-NN with the chosen kernel. Parameters: X (numpy array): Training dataset of shape (n_samples, n_features). y (numpy array): Labels for the training dataset of shape (n_samples,). X_test (numpy array): Test dataset of shape (n_test_samples, n_features). kernel (str): The type of kernel to use (\'linear\', \'polynomial\', \'rbf\'). k (int): Number of nearest neighbors to consider. **kwargs: Additional keyword arguments for the specific kernel parameters. Returns: numpy array: Predicted labels for the test dataset of shape (n_test_samples,). y_pred = np.zeros(X_test.shape[0], dtype=int) for i, x_query in enumerate(X_test): similarity_scores = compute_similarity(X, x_query, kernel, **kwargs) nearest_neighbor_indices = np.argsort(similarity_scores)[-k:] nearest_labels = y[nearest_neighbor_indices] y_pred[i] = np.argmax(np.bincount(nearest_labels)) return y_pred"}]'),z={name:"App",components:{PoemCard:S},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},R={class:"search-container"},D={class:"card-container"},q={key:0,class:"empty-state"},F=["disabled"],L={key:0},N={key:1};function O(n,e,l,m,i,r){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",R,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>i.searchQuery=o),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>i.searchQuery="")}," âœ• ")):d("",!0)]),t("div",D,[(a(!0),s(b,null,v(r.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),s("div",q,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...o)=>r.loadMore&&r.loadMore(...o))},[i.isLoading?(a(),s("span",N,"Loading...")):(a(),s("span",L,"See more"))],8,F)):d("",!0)])}const M=p(z,[["render",O],["__scopeId","data-v-c645f7aa"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/61.md","filePath":"deepseek/61.md"}'),U={name:"deepseek/61.md"},H=Object.assign(U,{setup(n){return(e,l)=>(a(),s("div",null,[x(M)]))}});export{Y as __pageData,H as default};
