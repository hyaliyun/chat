import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},S={class:"review-content"};function P(n,e,l,h,i,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",P],["__scopeId","data-v-f1f890f5"]]),I=JSON.parse('[{"question":"# Advanced Class Design and Inheritance in Python Problem Statement You are required to design a series of classes to model a basic Library system. The core functionalities you need to implement include: 1. **Basic Class Definitions and Initialization**: - Define a `Book` class with attributes for title, author, and ISBN. - Define a `Library` class that holds a collection of books. 2. **Methods and Attribute Access**: - Implement methods in the `Library` class to add, remove, and search for books by title. - Implement a method to list all books in the library. 3. **Class and Instance Variables**: - Implement a class variable in the `Library` class to keep track of the total number of books. 4. **Inheritance**: - Define a `DigitalLibrary` class that inherits from `Library`. - Add a feature to `DigitalLibrary` to download books, and track the number of downloads for each book. 5. **Iterators and Generators**: - Implement an iterator in the `Library` class to iterate over all books. - Implement a generator method in the `Library` class to list books by a specific author. Constraints 1. The titles and authors are case-insensitive in searches but should preserve case in storage. 2. ISBN is unique for each book. 3. A book can be in only one library at any given time. # Input Format - The input will not be read from standard input but will be tested using instances of your classes and methods. # Output Format - Your methods should return or print appropriate outputs as specified. # Requirements 1. Define the `Book` class: ```python class Book: def __init__(self, title: str, author: str, isbn: str): # Initialize book attributes ``` 2. Define the `Library` class: ```python class Library: total_books = 0 def __init__(self): # Initialize the library def add_book(self, book: Book): # Add a book to the library def remove_book(self, isbn: str): # Remove a book from the library by ISBN def search_books_by_title(self, title: str): # Search for books by title def list_books(self): # List all books in the library ``` 3. Define the `DigitalLibrary` class: ```python class DigitalLibrary(Library): def __init__(self): # Initialize the digital library def download_book(self, isbn: str): # Download a book and track the number of downloads ``` 4. Implement iterator and generator in the `Library` class: ```python class Library: # Previous code... def __iter__(self): # Iterator for all books def books_by_author(self, author: str): # Generator for books by a specific author ``` # Example Usage ```python library = Library() book1 = Book(\\"Title One\\", \\"Author A\\", \\"ISBN001\\") book2 = Book(\\"Title Two\\", \\"Author B\\", \\"ISBN002\\") library.add_book(book1) library.add_book(book2) print(library.search_books_by_title(\\"Title One\\")) # Output: [\\"Title One by Author A\\"] digital_library = DigitalLibrary() digital_library.add_book(book1) digital_library.download_book(\\"ISBN001\\") ``` Your task is to implement these classes, their methods, and ensure the provided functionalities work as specified.","solution":"class Book: def __init__(self, title: str, author: str, isbn: str): self.title = title self.author = author self.isbn = isbn def __str__(self): return f\\"{self.title} by {self.author}\\" class Library: total_books = 0 def __init__(self): self.books = {} def add_book(self, book: Book): if book.isbn not in self.books: self.books[book.isbn] = book Library.total_books += 1 def remove_book(self, isbn: str): if isbn in self.books: del self.books[isbn] Library.total_books -= 1 def search_books_by_title(self, title: str): return [book for book in self.books.values() if title.lower() in book.title.lower()] def list_books(self): return list(self.books.values()) def __iter__(self): self._iter_idx = 0 self._iter_books = list(self.books.values()) return self def __next__(self): if self._iter_idx < len(self._iter_books): result = self._iter_books[self._iter_idx] self._iter_idx += 1 return result else: raise StopIteration def books_by_author(self, author: str): for book in self.books.values(): if author.lower() in book.author.lower(): yield book class DigitalLibrary(Library): def __init__(self): super().__init__() self.downloads = {} def download_book(self, isbn: str): if isbn in self.books: if isbn not in self.downloads: self.downloads[isbn] = 0 self.downloads[isbn] += 1 return f\\"Downloading {self.books[isbn]}\\" else: return \\"Book not found.\\""},{"question":"# String Preparation and Validation You are required to implement a function `prepare_string` that prepares and validates a given Unicode string for use in internet protocols based on the Stringprep tables defined in RFC 3454. The function should follow these steps: 1. **Remove \\"commonly mapped to nothing\\" characters**: Use `stringprep.in_table_b1` to filter out these characters. 2. **Case mapping using Table B.2**: Map characters using `stringprep.map_table_b2`. 3. **Prohibited characters check**: Ensure the resulting string does not contain any characters from the following tables: - `tableC.1` (space characters): Combine `stringprep.in_table_c11` and `stringprep.in_table_c12`. - `tableC.2` (control characters): Combine `stringprep.in_table_c21` and `stringprep.in_table_c22`. - `tableC.3` (private use), `stringprep.in_table_c3` - `tableC.4` (non-character code points), `stringprep.in_table_c4` - `tableC.5` (surrogate codes), `stringprep.in_table_c5` - `tableC.6` (inappropriate for plain text), `stringprep.in_table_c6` - `tableC.7` (inappropriate for canonical representation), `stringprep.in_table_c7` - `tableD.1` (characters with bidirectional property \\"R\\" or \\"AL\\"), `stringprep.in_table_d1` If the final string contains prohibited characters, raise a `ValueError` with the message \\"Prohibited character found\\". # Function Signature ```python import stringprep def prepare_string(input_str: str) -> str: pass ``` # Input - `input_str` (str): The input Unicode string to be prepared. # Output - Returns a prepared and validated string (str) if all checks pass. # Constraints - The input string can include any Unicode characters. - The solution must handle strings up to 10,000 characters efficiently. # Example ```python try: result = prepare_string(\\"Example string\\") print(result) # Output depends on the mappings and removals performed. except ValueError as ve: print(str(ve)) # If the string contains prohibited characters. ``` # Note Be sure to use the functions provided by the `stringprep` module to ensure compliance with the RFC 3454 requirements.","solution":"import stringprep def prepare_string(input_str: str) -> str: This function prepares and validates a given Unicode string for use in internet protocols based on the Stringprep tables defined in RFC 3454. # Step 1: Remove \\"commonly mapped to nothing\\" characters mapped_str = \'\'.join( ch for ch in input_str if not stringprep.in_table_b1(ch) ) # Step 2: Case mapping using Table B.2 mapped_str = \'\'.join( stringprep.map_table_b2(ch) if stringprep.map_table_b2(ch) else ch for ch in mapped_str ) # Step 3: Prohibited characters check prohibited_checks = [ stringprep.in_table_c11, stringprep.in_table_c12, stringprep.in_table_c21, stringprep.in_table_c22, stringprep.in_table_c3, stringprep.in_table_c4, stringprep.in_table_c5, stringprep.in_table_c6, stringprep.in_table_c7, stringprep.in_table_d1 ] for ch in mapped_str: if any(check(ch) for check in prohibited_checks): raise ValueError(\\"Prohibited character found\\") return mapped_str"},{"question":"# CGI Script Task You are tasked with creating a CGI script to handle form submissions for a basic contact form on a website. The form allows users to submit their name, email address, and a message. Additionally, users can upload a profile picture. Your script should validate the provided data and generate an appropriate HTML response. The form uses the POST method for submissions. Requirements 1. **Form Fields**: - `name`: Must be a non-empty string. - `email`: Must be a non-empty string and should contain the \\"@\\" character. - `message`: Must be a non-empty string. - `profile_pic`: Optional file upload for the user\'s profile picture. 2. **Validation**: - Ensure `name`, `email`, and `message` are present and non-empty. - Verify the `email` field contains an \\"@\\" character. - Handle cases where `profile_pic` may not be provided. 3. **Output**: - If any required fields are missing or invalid, generate an HTML response indicating the error with a proper message. - If all fields are valid, generate an HTML response displaying the provided data and indicating a successful submission. - If a profile picture is uploaded, display the file name and the number of lines in the picture file (assuming it\'s a text file for simplicity). Implementation Write a CGI script in Python following these steps: 1. **Import necessary modules:** ```python import cgi import cgitb cgitb.enable() # Enable for debugging during development ``` 2. **Create a `FieldStorage` instance to parse form data:** ```python form = cgi.FieldStorage() ``` 3. **Validation and Processing:** - Check for required fields and validate the email. - Check if a file is uploaded. - Count lines in the uploaded file if present. 4. **Generate HTML Response:** - Display errors if validation fails. - Display form data if validation is successful. - Include profile picture details if the file is uploaded. Constraints - The script should handle form submissions securely, avoiding any security risks like shell command injection. - The script should be executable on a standard web server configured to handle CGI scripts. Example Code Structure ```python import cgi import cgitb cgitb.enable() # Enable for debugging # Create instance of FieldStorage form = cgi.FieldStorage() # Retrieve form data name = form.getfirst(\\"name\\", \\"\\").strip() email = form.getfirst(\\"email\\", \\"\\").strip() message = form.getfirst(\\"message\\", \\"\\").strip() profile_pic = form[\\"profile_pic\\"] if \\"profile_pic\\" in form else None # Validation errors = [] if not name: errors.append(\\"Name is required.\\") if not email or \\"@\\" not in email: errors.append(\\"A valid email is required.\\") if not message: errors.append(\\"Message is required.\\") # Output print(\\"Content-Type: text/html\\") print() # end of headers # Generate HTML response if errors: print(\\"<html><head><title>Form Error</title></head><body>\\") print(\\"<h1>Form Error</h1>\\") for error in errors: print(f\\"<p>{error}</p>\\") else: print(\\"<html><head><title>Form Submitted</title></head><body>\\") print(\\"<h1>Form Submitted Successfully</h1>\\") print(f\\"<p>Name: {name}</p>\\") print(f\\"<p>Email: {email}</p>\\") print(f\\"<p>Message: {message}</p>\\") if profile_pic: if profile_pic.file: linecount = sum(1 for line in profile_pic.file) print(f\\"<p>Profile Picture: {profile_pic.filename} ({linecount} lines)</p>\\") print(\\"</body></html>\\") ``` Implement the full script based on the provided structure and test it with different form submissions to ensure it works as expected.","solution":"import cgi import cgitb cgitb.enable() # Enable for debugging def handle_form_submission(): # MIME header print(\\"Content-Type: text/html\\") print() # end of headers # Create instance of FieldStorage form = cgi.FieldStorage() # Retrieve form data name = form.getfirst(\\"name\\", \\"\\").strip() email = form.getfirst(\\"email\\", \\"\\").strip() message = form.getfirst(\\"message\\", \\"\\").strip() profile_pic = form[\\"profile_pic\\"] if \\"profile_pic\\" in form else None # Validation errors = [] if not name: errors.append(\\"Name is required.\\") if not email or \\"@\\" not in email: errors.append(\\"A valid email is required.\\") if not message: errors.append(\\"Message is required.\\") # Generate HTML response print(\\"<html><head><title>Form Submission</title></head><body>\\") if errors: print(\\"<h1>Form Error</h1>\\") for error in errors: print(f\\"<p>{error}</p>\\") else: print(\\"<h1>Form Submitted Successfully</h1>\\") print(f\\"<p>Name: {name}</p>\\") print(f\\"<p>Email: {email}</p>\\") print(f\\"<p>Message: {message}</p>\\") if profile_pic: if profile_pic.file: linecount = sum(1 for line in profile_pic.file) print(f\\"<p>Profile Picture: {profile_pic.filename} ({linecount} lines)</p>\\") print(\\"</body></html>\\") if __name__ == \\"__main__\\": handle_form_submission()"},{"question":"Objective Demonstrate your understanding of advanced bar plot generation using the Seaborn package. Problem Statement You are provided with a dataset named `tips`, which contains information related to restaurant bills and tips. Your task is to perform the following: 1. Load the tips dataset using Seaborn. 2. Create a bar plot showing the total bill amounts (`total_bill`) for each day of the week (`day`). 3. Add a second layer of grouping by the time of day (`time`). - Each bar should represent the average `total_bill` for each day of the week (`day`). 4. Modify the plot aesthetics as follows: - Show error bars with the standard deviation of the data. - Use different colors for each group (`time`). - Make sure the bars are transparent 30%. 5. Add text labels showing the average `total_bill` on top of each bar. 6. Save the plot as `tips_barplot.png`. Input and Output Formats **Input**: None (the dataset will be loaded from within the code). **Output**: A saved plot image named `tips_barplot.png`. Constraints - Ensure that the plot aesthetics are clear and the plot is saved with a high resolution (300 dpi). Example Although no exact output example is provided, ensure the plot adheres to the specified aesthetics and correctly represents the data as described. Performance Requirements The code should execute efficiently and produce the plot without significant delay. Function Implementation Implement the following function with the provided specifications: ```python import seaborn as sns import matplotlib.pyplot as plt def generate_tips_barplot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the bar plot with multiple groupings ax = sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", ci=\\"sd\\", alpha=0.7) # Add text labels ax.bar_label(ax.containers[0], fmt=\'%.2f\', fontsize=10) ax.bar_label(ax.containers[1], fmt=\'%.2f\', fontsize=10) # Save the plot plt.savefig(\\"tips_barplot.png\\", dpi=300) # Execute the function generate_tips_barplot() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_tips_barplot(): Generates a bar plot for the \'tips\' dataset showing total bill amounts for each day of the week, further grouped by the time of day. The plot includes error bars with the standard deviation, different colors for each group, and text labels showing the average \'total_bill\' on top of each bar. The plot is saved as \'tips_barplot.png\' with a resolution of 300 dpi. # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the bar plot with multiple groupings plt.figure(figsize=(10, 6)) ax = sns.barplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", ci=\\"sd\\", alpha=0.7) # Add text labels with average total bill amount on top of each bar for container in ax.containers: ax.bar_label(container, fmt=\'%.2f\', fontsize=10) # Save the plot plt.savefig(\\"tips_barplot.png\\", dpi=300) plt.close() # Execute the function generate_tips_barplot()"},{"question":"# Base64 Encoding and Decoding Challenge In this challenge, you will need to demonstrate your understanding of the `base64` module in Python by implementing a set of functions that encode and decode strings using different Base encodings (Base16, Base32, Base64, URL-safe Base64, Ascii85, and Base85). Task 1. **Function: `encode_data(data: bytes, encoding: str) -> bytes`** - **Input**: - `data` (bytes): The binary data to be encoded. - `encoding` (str): The encoding type to be used. This can be one of `\\"base16\\"`, `\\"base32\\"`, `\\"base64\\"`, `\\"urlsafe_base64\\"`, `\\"ascii85\\"`, `\\"base85\\"`. - **Output**: - The encoded data as bytes. 2. **Function: `decode_data(encoded_data: bytes, encoding: str) -> bytes`** - **Input**: - `encoded_data` (bytes): The encoded data. - `encoding` (str): The encoding type to be used. This can be one of `\\"base16\\"`, `\\"base32\\"`, `\\"base64\\"`, `\\"urlsafe_base64\\"`, `\\"ascii85\\"`, `\\"base85\\"`. - **Output**: - The decoded binary data. Constraints and Notes - Your implementation should handle all edge cases and validate inputs appropriately. - Raise a `ValueError` if an invalid encoding type is provided. - As this is a coding assessment, avoid using any external libraries beyond the Python standard library. Example ```python # Example Usage: # Sample Data data = b\\"Python3.10 is amazing!\\" # Encoding and Decoding using Base64 encoded_base64 = encode_data(data, \\"base64\\") decoded_base64 = decode_data(encoded_base64, \\"base64\\") # Encoding and Decoding using URL-safe Base64 encoded_urlsafe_base64 = encode_data(data, \\"urlsafe_base64\\") decoded_urlsafe_base64 = decode_data(encoded_urlsafe_base64, \\"urlsafe_base64\\") # Check if the decoded data matches the original data assert decoded_base64 == data assert decoded_urlsafe_base64 == data print(\\"Encoding and decoding successful for Base64 and URL-safe Base64!\\") ``` Implement the functions `encode_data` and `decode_data` to pass the above scenarios and any other edge cases you can think of. Happy coding!","solution":"import base64 def encode_data(data: bytes, encoding: str) -> bytes: if encoding == \'base16\': return base64.b16encode(data) elif encoding == \'base32\': return base64.b32encode(data) elif encoding == \'base64\': return base64.b64encode(data) elif encoding == \'urlsafe_base64\': return base64.urlsafe_b64encode(data) elif encoding == \'ascii85\': return base64.a85encode(data) elif encoding == \'base85\': return base64.b85encode(data) else: raise ValueError(\\"Invalid encoding type specified\\") def decode_data(encoded_data: bytes, encoding: str) -> bytes: if encoding == \'base16\': return base64.b16decode(encoded_data) elif encoding == \'base32\': return base64.b32decode(encoded_data) elif encoding == \'base64\': return base64.b64decode(encoded_data) elif encoding == \'urlsafe_base64\': return base64.urlsafe_b64decode(encoded_data) elif encoding == \'ascii85\': return base64.a85decode(encoded_data) elif encoding == \'base85\': return base64.b85decode(encoded_data) else: raise ValueError(\\"Invalid encoding type specified\\")"},{"question":"# Advanced Iterator Implementation in Python **Objective**: Implement custom iterator functionality in Python to demonstrate the understanding of Python\'s iterator protocol and the standard library. **Problem Statement**: You need to implement custom iterators leveraging the sequence and callable-based iterator mechanisms discussed in the provided documentation. Part 1: Sequence Iterator Implement a custom sequence iterator class `CustomSeqIter` that: - Takes any sequence (list, tuple, string, etc.). - Iterates over the elements of the sequence. **Requirements**: - Initialize the iterator with a sequence. - Implement the `__iter__()` and `__next__()` methods. - Raise `StopIteration` when the sequence ends. Part 2: Callable and Sentinel Iterator Implement a custom callable iterator class `CustomCallIter` that: - Takes a callable object and a sentinel value. - Calls the callable object and yields values until the sentinel value is returned. **Requirements**: - Initialize the iterator with a callable and a sentinel value. - Implement the `__iter__()` and `__next__()` methods. - Raise `StopIteration` when the callable returns the sentinel value. Constraints: - Do not use any external libraries except the standard library. - Ensure the iterators are compatible with Python\'s iterator protocol. - Handle edge cases like empty sequences or frequent sentinel values efficiently. # Input and Output Format Example for `CustomSeqIter`: ```python seq_iter = CustomSeqIter([1, 2, 3]) for item in seq_iter: print(item) ``` **Output**: ``` 1 2 3 ``` Example for `CustomCallIter`: ```python def my_callable(): from itertools import count return next(count(1)) call_iter = CustomCallIter(my_callable, 5) for item in call_iter: print(item) ``` **Output**: ``` 1 2 3 4 ``` # Define the Classes Below ```python class CustomSeqIter: def __init__(self, sequence): # Your implementation here def __iter__(self): # Your implementation here def __next__(self): # Your implementation here class CustomCallIter: def __init__(self, callable, sentinel): # Your implementation here def __iter__(self): # Your implementation here def __next__(self): # Your implementation here ``` **Note**: Be sure to include all necessary methods to make these classes work as expected iterators.","solution":"class CustomSeqIter: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.sequence): raise StopIteration value = self.sequence[self.index] self.index += 1 return value class CustomCallIter: def __init__(self, callable_obj, sentinel): self.callable = callable_obj self.sentinel = sentinel def __iter__(self): return self def __next__(self): value = self.callable() if value == self.sentinel: raise StopIteration return value"},{"question":"**Title**: Implement a Safe Recursive Function with Controlled Recursion Depth and Auditing **Objective**: You are required to implement a recursive function that calculates the factorial of a number. This function should have controlled recursion depth to avoid infinite recursion and stack overflow. Additionally, you need to add auditing hooks to log the start and end of each function call for monitoring purposes. **Requirements**: 1. Implement a function `safe_factorial(n)` that computes the factorial of a positive integer `n`. 2. Use `sys.setrecursionlimit` to set a safe recursion limit to prevent stack overflow. 3. Add auditing hooks using `sys.addaudithook` to log the start and end of each function call. 4. Use `sys.audit` to raise an auditing event at the start (\\"factorial.start\\") and end (\\"factorial.end\\") of each recursive call. 5. Handle the `RuntimeError` exception that may be raised if the recursion limit is exceeded. **Constraints**: - The input `n` is a non-negative integer (`0 <= n <= 100`). - The recursion limit should be set to a reasonable number to prevent infinite recursion. **Function Signature**: ```python import sys def safe_factorial(n: int) -> int: pass def auditing_hook(event, args): pass # Optional: You may define additional helper functions if necessary. ``` **Example Usage**: ```python if __name__ == \\"__main__\\": sys.addaudithook(auditing_hook) try: result = safe_factorial(5) print(f\\"Factorial of 5: {result}\\") except RuntimeError as e: print(f\\"Error: {str(e)}\\") ``` # Additional Information 1. **Function Input/Output**: - Input: A non-negative integer `n` where `0 <= n <= 100`. - Output: The factorial of the input number `n`. 2. **Auditing Details**: - Use `sys.audit(\\"factorial.start\\", n)` at the start of each recursion to log the beginning of the calculation. - Use `sys.audit(\\"factorial.end\\", n)` at the end of each recursion to log the completion of the calculation. 3. **Hints**: - Use `sys.setrecursionlimit` to adjust the recursion limit if necessary. - Use try-except blocks to handle potential `RuntimeError` due to recursion limit being exceeded. **Your task is to implement the `safe_factorial` function, and the `auditing_hook` function to conform to these requirements.**","solution":"import sys def safe_factorial(n: int) -> int: sys.setrecursionlimit(300) def factorial_inner(n: int, depth: int) -> int: if n == 0 or n == 1: sys.audit(\\"factorial.end\\", n) return 1 else: sys.audit(\\"factorial.start\\", n) result = n * factorial_inner(n - 1, depth + 1) sys.audit(\\"factorial.end\\", n) return result try: return factorial_inner(n, 0) except RuntimeError as e: print(f\\"Recursion limit reached. Error: {str(e)}\\") return None def auditing_hook(event, args): if event == \\"factorial.start\\": print(f\\"Starting factorial computation for: {args[0]}\\") elif event == \\"factorial.end\\": print(f\\"Ending factorial computation for: {args[0]}\\") # Adding the auditing hook sys.addaudithook(auditing_hook)"},{"question":"# Email Client Implementation using `imaplib` You are required to implement a simple email client in Python using the `imaplib` module. This client will connect to an IMAP server, authenticate the user, fetch emails, and parse email data. Requirements: 1. **Connect to IMAP Server**: - Use the `IMAP4` class to establish a connection to the IMAP server. - The server hostname and port should be provided as input parameters. 2. **Authenticate User**: - Implement a function to authenticate a user using their username and password. - Prompt the user to enter their credentials (username and password) securely. 3. **Fetch and Display Emails**: - Implement a function to select the inbox and fetch the 5 most recent emails. - Display the subject, sender, and date of each email in a readable format. 4. **Error Handling**: - Handle any errors that might occur during connection, authentication, and fetching of emails using appropriate exception handling mechanisms provided by `imaplib`. Inputs: - IMAP server hostname (string) - IMAP server port (integer) - Username (string) - Password (string) Output: - A list of tuples containing the subject, sender, and date of each email. - Print the email count and the details of the 5 most recent emails in the format: ``` Email Count: <count> Email 1: Subject: <subject1>, Sender: <sender1>, Date: <date1> Email 2: Subject: <subject2>, Sender: <sender2>, Date: <date2> ... ``` Constraints: 1. Your implementation should connect to the server securely and must handle potential exceptions such as connection errors, authentication failures, and other IMAP errors. 2. The function should have a reasonable timeout (e.g., 10 seconds) for all network operations. # Sample Code Structure: ```python import imaplib import getpass def connect_to_server(host, port): Connects to the IMAP server using the provided hostname and port. Returns: IMAP4 object: The connected IMAP4 object. Raises: IMAP4.error: If there is an error connecting to the server. # Your code here pass def authenticate_user(imap_obj, username, password): Authenticates the user with the IMAP server using provided credentials. Args: imap_obj (IMAP4 object): The connected IMAP4 object. username (str): The username for authentication. password (str): The password for authentication. Raises: IMAP4.error: If there is an error during authentication. # Your code here pass def fetch_emails(imap_obj): Fetches and returns the 5 most recent emails with subject, sender, and date. Args: imap_obj (IMAP4 object): The connected and authenticated IMAP4 object. Returns: List[Tuple]: A list of tuples containing subject, sender, and date for each email. # Your code here pass def main(): host = input(\\"Enter IMAP server hostname: \\") port = int(input(\\"Enter IMAP server port: \\")) username = input(\\"Enter your email username: \\") password = getpass.getpass(\\"Enter your email password: \\") try: # Connect to the server imap_obj = connect_to_server(host, port) # Authenticate the user authenticate_user(imap_obj, username, password) # Fetch and display emails emails = fetch_emails(imap_obj) print(f\\"Email Count: {len(emails)}\\") for idx, email in enumerate(emails, start=1): subject, sender, date = email print(f\\"Email {idx}: Subject: {subject}, Sender: {sender}, Date: {date}\\") # Close the connection imap_obj.close() imap_obj.logout() except imaplib.IMAP4.error as e: print(f\\"An error occurred: {str(e)}\\") if __name__ == \\"__main__\\": main() ``` # Notes: - You may need to use additional libraries like `email` to parse the email data retrieved by `imaplib`. - Make sure to handle sensitive information securely, especially when prompting for credentials.","solution":"import imaplib import getpass from email.parser import HeaderParser def connect_to_server(host, port): Connects to the IMAP server using the provided hostname and port. Args: host (str): The IMAP server hostname. port (int): The IMAP server port. Returns: imaplib.IMAP4_SSL object: The connected IMAP4_SSL object. Raises: imaplib.IMAP4.error: If there is an error connecting to the server. try: imap_obj = imaplib.IMAP4_SSL(host, port) return imap_obj except imaplib.IMAP4.error as e: print(f\\"Failed to connect to server: {str(e)}\\") raise def authenticate_user(imap_obj, username, password): Authenticates the user with the IMAP server using provided credentials. Args: imap_obj (imaplib.IMAP4_SSL object): The connected IMAP4_SSL object. username (str): The username for authentication. password (str): The password for authentication. Raises: imaplib.IMAP4.error: If there is an error during authentication. try: imap_obj.login(username, password) except imaplib.IMAP4.error as e: print(f\\"Failed to authenticate: {str(e)}\\") raise def fetch_emails(imap_obj): Fetches and returns the 5 most recent emails with subject, sender, and date. Args: imap_obj (imaplib.IMAP4_SSL object): The connected and authenticated IMAP4_SSL object. Returns: List[Tuple]: A list of tuples containing subject, sender, and date for each email. try: imap_obj.select(\'inbox\') result, data = imap_obj.search(None, \'ALL\') if result != \'OK\': raise imaplib.IMAP4.error(\\"Failed to retrieve emails\\") email_ids = data[0].split() latest_email_ids = email_ids[-5:] emails = [] header_parser = HeaderParser() for email_id in reversed(latest_email_ids): result, email_data = imap_obj.fetch(email_id, \'(RFC822.HEADER)\') if result != \'OK\': raise imaplib.IMAP4.error(\\"Failed to fetch email data\\") headers = header_parser.parsestr(email_data[0][1].decode(\'utf-8\')) subject = headers[\'Subject\'] sender = headers[\'From\'] date = headers[\'Date\'] emails.append((subject, sender, date)) return emails except imaplib.IMAP4.error as e: print(f\\"Failed to fetch emails: {str(e)}\\") raise def main(): host = input(\\"Enter IMAP server hostname: \\") port = int(input(\\"Enter IMAP server port: \\")) username = input(\\"Enter your email username: \\") password = getpass.getpass(\\"Enter your email password: \\") try: # Connect to the server imap_obj = connect_to_server(host, port) # Authenticate the user authenticate_user(imap_obj, username, password) # Fetch and display emails emails = fetch_emails(imap_obj) print(f\\"Email Count: {len(emails)}\\") for idx, email in enumerate(emails, start=1): subject, sender, date = email print(f\\"Email {idx}: Subject: {subject}, Sender: {sender}, Date: {date}\\") # Close the connection imap_obj.close() imap_obj.logout() except imaplib.IMAP4.error as e: print(f\\"An error occurred: {str(e)}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Question: Decoding Python Version** **Objective**: Implement a function in Python that takes an encoded Python version number (`PY_VERSION_HEX`) and returns a human-readable string representing the version in the \\"major.minor.micro(level)(serial)\\" format. # Function signature: ```python def decode_python_version(hex_version: int) -> str: pass ``` # Input: * `hex_version` (int): a 32-bit integer representing the encoded Python version. # Output: * (str): a formatted string in the \\"major.minor.micro(level)(serial)\\" format. # Details: To decode the version number, you need to: 1. Extract the `PY_MAJOR_VERSION` from bits 1-8. 2. Extract the `PY_MINOR_VERSION` from bits 9-16. 3. Extract the `PY_MICRO_VERSION` from bits 17-24. 4. Extract the `PY_RELEASE_LEVEL` from bits 25-28. 5. Extract the `PY_RELEASE_SERIAL` from bits 29-32. The release levels correspond to the following identifiers: - `0xA` for alpha (represented as \'a\') - `0xB` for beta (represented as \'b\') - `0xC` for release candidate (represented as \'rc\') - `0xF` for final (represented as \'\') # Constraints: - The `hex_version` input will always be a valid 32-bit integer as defined in `PY_VERSION_HEX`. # Example: ```python # Example 1 hex_version = 0x030401a2 print(decode_python_version(hex_version)) # Output: \\"3.4.1a2\\" # Example 2 hex_version = 0x030a00f0 print(decode_python_version(hex_version)) # Output: \\"3.10.0\\" ``` # Notes: - You can use the bitwise shift operators (`>>`, `<<`) and bitwise AND operator (`&`) to extract the respective parts from the 32-bit integer. - Format the extracted parts according to the required output string format.","solution":"def decode_python_version(hex_version: int) -> str: # Extract major version (bits 1-8) major = (hex_version >> 24) & 0xFF # Extract minor version (bits 9-16) minor = (hex_version >> 16) & 0xFF # Extract micro version (bits 17-24) micro = (hex_version >> 8) & 0xFF # Extract release level (bits 25-28) release_level_int = (hex_version >> 4) & 0xF # Extract release serial (bits 29-32) release_serial = hex_version & 0xF # Mapping the release level integer to its string representation release_level_mapping = { 0xA: \'a\', 0xB: \'b\', 0xC: \'rc\', 0xF: \'\' } release_level = release_level_mapping.get(release_level_int, \'\') # If the release level is \'\' (final version), no need to add release_serial if release_level: return f\\"{major}.{minor}.{micro}{release_level}{release_serial}\\" else: return f\\"{major}.{minor}.{micro}\\""},{"question":"# Email Message Serialization You are tasked with creating a function that will generate a MIME email message and serialize it using features from the `email.generator` module. The generated message should be compliant with standard email RFCs and suitable for sending via SMTP. Requirements: 1. Implement a function `serialize_email(msg_dict, binary=False, mangle_from=False, maxheaderlen=None, unixfrom=False, linesep=None, policy=None)`. 2. The function should receive: - `msg_dict`: A dictionary containing the email components: ```python { \'From\': \'sender@example.com\', \'To\': \'receiver@example.com\', \'Subject\': \'Test Email\', \'Body\': \'This is a test email message\', \'Attachments\': [] # List of tuples with file paths and MIME types } ``` - `binary` (boolean): Whether to produce a binary (`BytesGenerator`) or text (`Generator`) output. - `mangle_from` (boolean): Whether to mangle \\"From \\" headers. - `maxheaderlen` (int or None): Maximum header length before folding. - `unixfrom` (boolean): Whether to include the Unix from (`From `) line. - `linesep` (string or None): Line separator to use. - `policy`: Policy to control message generation. 3. The function should construct an `EmailMessage` object using the provided `msg_dict`. 4. Attachments, if any, should be added correctly to the email message. 5. The resulting message should then be serialized using either `BytesGenerator` or `Generator` based on the `binary` parameter. 6. The function should return the serialized message as bytes (if `binary` is `True`) or string (if `binary` is `False`). # Example Usage: ```python msg_dict = { \'From\': \'sender@example.com\', \'To\': \'receiver@example.com\', \'Subject\': \'Test Email\', \'Body\': \'This is a test email message\', \'Attachments\': [] } serialized_msg = serialize_email(msg_dict, binary=False) print(serialized_msg) ``` Constraints: - Ensure compliance with MIME standards. - Handle Unicode content correctly. - Use default policies unless specific customizations are provided. # Performance Requirements: - Efficiently handle emails with a large number of custom headers and attachments. - Ensure that attachment processing does not degrade performance significantly. Implement the function `serialize_email` as per the above specifications.","solution":"from email.message import EmailMessage from email.generator import BytesGenerator, Generator import io def serialize_email(msg_dict, binary=False, mangle_from_=False, maxheaderlen=None, unixfrom=False, linesep=None, policy=None): # Create EmailMessage object msg = EmailMessage() # Set the headers msg[\'From\'] = msg_dict[\'From\'] msg[\'To\'] = msg_dict[\'To\'] msg[\'Subject\'] = msg_dict[\'Subject\'] # Set the body msg.set_content(msg_dict[\'Body\']) # Handle the attachments if there are any for attachment in msg_dict.get(\'Attachments\', []): with open(attachment[0], \'rb\') as f: file_data = f.read() msg.add_attachment(file_data, maintype=attachment[1], subtype=attachment[2], filename=attachment[3]) # Prepare an IO stream for the output output = io.BytesIO() if binary else io.StringIO() # Generate the email message if binary: gen = BytesGenerator(output, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) else: gen = Generator(output, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) gen.flatten(msg, unixfrom=unixfrom, linesep=linesep) # Get the serialized content serialized_msg = output.getvalue() return serialized_msg"},{"question":"Objective: To evaluate your understanding of the `fileinput` module and its functionalities in Python 3.10, including file iteration, line processing, advanced file operations, and the use of hooks for file handling. Problem Statement: You are given a list of text files, each containing several lines of numerical data. Your task is to write a Python function that reads these files, processes the numerical data, and produces an output file with the following format: 1. Each line in the output file should contain the cumulative sum of numbers read so far across all files. 2. If a file is empty, it should be skipped without affecting the cumulative sum. 3. If the input contains a mix of regular and compressed files (with `.gz` or `.bz2` extensions), handle them appropriately using the `fileinput.hook_compressed` hook. 4. The function should also print the file names being processed and the line counts for each file. Function Signature: ```python def process_files(input_files: list[str], output_file: str) -> None: Reads a list of input files, processes numerical data, and writes the cumulative sum to the output file. Parameters: input_files (list of str): List of input file paths (including compressed files with .gz or .bz2). output_file (str): Path to the output file where the cumulative sums will be written. Returns: None ``` Constraints: - Each line in the input files contains a single integer. - The total number of lines across all files does not exceed 1,000,000. - The function should handle IO errors gracefully and skip any problematic files with a warning message. Example Usage: Suppose you have the following input files: - `file1.txt`: ``` 10 20 30 ``` - `file2.txt` (empty file) - `file3.txt`: ``` 5 15 ``` Calling `process_files([\'file1.txt\', \'file2.txt\', \'file3.txt\'], \'output.txt\')` should produce the `output.txt` file containing: ``` 10 30 60 65 80 ``` Additionally, the function should print: ``` Processing file: file1.txt, Lines read: 3 Processing file: file3.txt, Lines read: 2 ``` Implementation Notes: 1. Utilize the `fileinput.input()` function to handle file reading and iteration. 2. Use the `fileinput.hook_compressed` for dealing with compressed files. 3. Remember to manage cumulative sums and ensure the output formatting is correct. 4. Handle empty files and IO errors as specified. Good luck!","solution":"import fileinput import gzip import bz2 def process_files(input_files: list[str], output_file: str) -> None: Reads a list of input files, processes numerical data, and writes the cumulative sum to the output file. Parameters: input_files (list of str): List of input file paths (including compressed files with .gz or .bz2). output_file (str): Path to the output file where the cumulative sums will be written. Returns: None cumulative_sum = 0 line_counts = {} with open(output_file, \'w\') as out_f: for line in fileinput.input(files=input_files, mode=\'r\', openhook=fileinput.hook_compressed): try: line = line.strip() if line: number = int(line) cumulative_sum += number out_f.write(f\\"{cumulative_sum}n\\") # Count the lines processed per file filename = fileinput.filename() if filename: line_counts[filename] = line_counts.get(filename, 0) + 1 except ValueError: # Ignore lines that do not contain valid integers continue for file, count in line_counts.items(): print(f\\"Processing file: {file}, Lines read: {count}\\")"},{"question":"# Advanced Coding Assessment: NNTP Client Implementation Objective: Develop a Python script that functions as an NNTP client to interact with an NNTP server. This script should showcase the student\'s ability to use the `nntplib` module, manage network interactions, and handle various NNTP commands effectively. Problem Statement: Your task is to create a Python class named `NNTPClient` that connects to an NNTP server, retrieves, and displays article headers from specific newsgroups. The client should also handle different operations like listing newsgroups and fetching articles. You need to implement methods to handle the following functionalities: 1. **Connection Initialization**: - Connect to the NNTP server (either secure or non-secure). - If connection fails, handle the corresponding exceptions and print an error message. 2. **Fetching Newsgroup Details**: - Retrieve stats about a specified newsgroup (number of articles, first and last article numbers). 3. **Fetch and Print Article Headers**: - Fetch and print the headers (like `Subject`, `From`, `Date`) of the last \'n\' articles in a specified newsgroup. 4. **List Newsgroups**: - List all newsgroups on the server, or those matching a given pattern. 5. **Quit Connection**: - Implement a method to gracefully close the connection to the server. Class Definition: ```python class NNTPClient: def __init__(self, host, port=119, use_ssl=False, user=None, password=None, timeout=None): Initialize the NNTP client with the given server details. :param host: The hostname of the NNTP server. :param port: The port number to connect to (default: 119). :param use_ssl: Whether to use a secure connection (default: False). :param user: The username for authentication (optional). :param password: The password for authentication (optional). :param timeout: Optional timeout for the connection. pass def fetch_newsgroup_stats(self, group_name): Fetch and print statistics of the specified newsgroup. :param group_name: The name of the newsgroup. pass def fetch_article_headers(self, group_name, n): Fetch and print headers of the last \'n\' articles in the specified newsgroup. :param group_name: The name of the newsgroup. :param n: The number of article headers to fetch. pass def list_newsgroups(self, pattern=None): List all newsgroups or those matching a given pattern. :param pattern: Optional pattern to filter newsgroups. pass def quit(self): Gracefully close the connection to the NNTP server. pass ``` Input and Output Constraints: - **Connection Initialization**: - The constructor should manage the connection to the server, handling `NNTPError` exceptions on failure. - **fetch_newsgroup_stats**: - This method should print details like the number of articles and the range of article numbers. - **fetch_article_headers**: - This method should print the headers of the last `n` articles. If `n` is larger than the number of articles, return as many as possible. - **list_newsgroups**: - This method should print a list of newsgroups or those matching the given pattern. - **quit**: - Ensure the connection is closed properly, handling `NNTPError` if any issues occur. Example Usage: ```python # Connect to NNTP server client = NNTPClient(\'news.gmane.io\', use_ssl=False, user=None, password=None) # Fetch newsgroup statistics client.fetch_newsgroup_stats(\'gmane.comp.python.committers\') # Fetch and print headers of the last 5 articles client.fetch_article_headers(\'gmane.comp.python.committers\', 5) # List all available newsgroups client.list_newsgroups() # Quit the connection client.quit() ``` Notes: - You may use any required standard library modules to assist with date/time handling, exceptions, etc. - Remember to decode headers properly using `nntplib.decode_header` when they may contain non-ASCII characters. - Comment your code where necessary to explain the implementation details.","solution":"import nntplib from nntplib import NNTP, NNTP_SSL, NNTPPermanentError, NNTPTemporaryError import socket class NNTPClient: def __init__(self, host, port=119, use_ssl=False, user=None, password=None, timeout=None): Initialize the NNTP client with the given server details. :param host: The hostname of the NNTP server. :param port: The port number to connect to (default: 119). :param use_ssl: Whether to use a secure connection (default: False). :param user: The username for authentication (optional). :param password: The password for authentication (optional). :param timeout: Optional timeout for the connection. try: if use_ssl: self.connection = NNTP_SSL(host, port, user=user, password=password, readermode=True, timeout=timeout) else: self.connection = NNTP(host, port, user=user, password=password, readermode=True, timeout=timeout) except (NNTPPermanentError, NNTPTemporaryError, socket.error) as e: print(f\\"Error connecting to NNTP server: {e}\\") self.connection = None def fetch_newsgroup_stats(self, group_name): Fetch and print statistics of the specified newsgroup. :param group_name: The name of the newsgroup. if self.connection: try: resp, count, first, last, name = self.connection.group(group_name) print(f\\"Newsgroup: {name}\\") print(f\\"Number of articles: {count}\\") print(f\\"First article number: {first}\\") print(f\\"Last article number: {last}\\") except (NNTPPermanentError, NNTPTemporaryError) as e: print(f\\"Error fetching newsgroup stats: {e}\\") else: print(\\"No connection to NNTP server.\\") def fetch_article_headers(self, group_name, n): Fetch and print headers of the last \'n\' articles in the specified newsgroup. :param group_name: The name of the newsgroup. :param n: The number of article headers to fetch. if self.connection: try: resp, count, first, last, name = self.connection.group(group_name) start = max(int(last) - n + 1, int(first)) end = int(last) headers = self.connection.xover(str(start), str(end)) for id, subj, frm, date, _, _, _, _ in headers: print(f\\"ID: {id}nFrom: {frm}nSubject: {subj}nDate: {date}n\\") except (NNTPPermanentError, NNTPTemporaryError) as e: print(f\\"Error fetching article headers: {e}\\") else: print(\\"No connection to NNTP server.\\") def list_newsgroups(self, pattern=None): List all newsgroups or those matching a given pattern. :param pattern: Optional pattern to filter newsgroups. if self.connection: try: if pattern: resp, groups = self.connection.list(pattern) else: resp, groups = self.connection.list() for group in groups: print(f\\"Newsgroup: {group}\\") except (NNTPPermanentError, NNTPTemporaryError) as e: print(f\\"Error listing newsgroups: {e}\\") else: print(\\"No connection to NNTP server.\\") def quit(self): Gracefully close the connection to the NNTP server. if self.connection: try: self.connection.quit() except (NNTPPermanentError, NNTPTemporaryError) as e: print(f\\"Error quitting connection: {e}\\") else: print(\\"No connection to NNTP server.\\")"},{"question":"Problem Statement You are tasked with designing a function that will process audio fragments to enhance the overall audio signal. The enhancement requires three main steps: 1. **Amplification:** Multiply the audio samples by a given amplification factor. 2. **Bias Adjustment:** Add a specific bias to each audio sample. 3. **Peak Normalization:** Adjust all audio samples so that the maximum value across the fragment does not exceed a given threshold by scaling down if necessary. The audio fragment will be provided as a bytes-like object with 16-bit samples. Function Signature ```python def enhance_audio(audio_fragment: bytes, amplification_factor: float, bias_value: int, max_threshold: int) -> bytes: ``` Parameters - `audio_fragment`: A bytes-like object representing the audio fragment with 16-bit samples. - `amplification_factor`: A float value by which all audio samples will be multiplied. - `bias_value`: An integer bias that will be added to each sample. - `max_threshold`: The maximum allowed value for any sample in the resultant audio fragment. Return - A bytes-like object representing the enhanced audio fragment with 16-bit samples. Requirements and Constraints 1. Ensure that the `audio_fragment` contains valid 16-bit signed integer samples. 2. The amplification should be performed first, followed by the bias adjustment, and finally the peak normalization step. 3. If any sample in the resultant fragment exceeds the `max_threshold`, all samples should be scaled down proportionally to ensure that the maximum sample value does not exceed this threshold. 4. The function should handle overflow by truncating the sample values appropriately. Example Usage ```python audio_fragment = b\'x01x00x02x00x03x00x04x00\' amplification_factor = 2.0 bias_value = 100 max_threshold = 20000 result = enhance_audio(audio_fragment, amplification_factor, bias_value, max_threshold) print(result) ``` Note: The above example is illustrative, and exact byte values will depend on input parameters. Hints - Use `audioop.mul` to multiply the audio samples by the amplification factor. - Use `audioop.bias` to add the bias to the audio samples. - Use `audioop.max` to find the maximum value in the audio fragment. - Properly handle byte-swapping if you encounter endianness issues (optional but might be necessary depending on the input fragment\'s origin).","solution":"import audioop import struct def enhance_audio(audio_fragment: bytes, amplification_factor: float, bias_value: int, max_threshold: int) -> bytes: # Step 1: Amplification amplified_fragment = audioop.mul(audio_fragment, 2, amplification_factor) # Step 2: Bias Adjustment biased_fragment = audioop.bias(amplified_fragment, 2, bias_value) # Step 3: Peak Normalization max_value = audioop.max(biased_fragment, 2) if max_value > max_threshold: normalization_factor = max_threshold / max_value biased_fragment = audioop.mul(biased_fragment, 2, normalization_factor) # Convert the processed fragment into bytes return biased_fragment"},{"question":"**Coding Assessment Question:** # URL Validation and IP Address Categorization Your task is to implement a Python function that validates a list of URLs and categorizes them based on their IP addresses. You must use Python\'s `urllib.parse` for URL components parsing and `ipaddress` for IP address manipulation. Follow the steps below to complete the task: 1. **Parse URLs**: Given a list of URLs, extract their hostnames. 2. **Resolve to IPs**: Convert the hostnames to their respective IP addresses. (Assume a function `hostname_to_ip(hostname)` is available that converts a hostname to its IP address. This is a stub and will be replaced with an actual implementation during the evaluation.) 3. **Categorize IPs**: Based on the IP address, categorize the URLs into one of the following: - `Private`: For private IP addresses (e.g., those in `192.168.0.0/16`, `10.0.0.0/8`, etc.). - `Public`: For public IP addresses. - `Invalid`: If the IP address cannot be determined or the URL is invalid. # Function Signature ```python def categorize_urls(urls: list[str]) -> dict[str, list[str]]: # your code here ``` # Input - `urls`: A list of strings, where each string is a URL. # Output - A dictionary with three keys: `Private`, `Public`, and `Invalid`. Each key maps to a list of URLs that belong to that category. # Constraints - You may assume the input list contains at most 100 URLs. - The hostname resolution function `hostname_to_ip(hostname)` will be provided. - The URLs are in correct format but may not resolve to a valid IP. # Example ```python urls = [ \\"http://example.com\\", \\"http://192.168.1.1\\", \\"ftp://10.0.0.5/resource\\", \\"http://invalidhostname\\" ] result = categorize_urls(urls) # Example values assuming certain IP resolutions: # result = { # \\"Private\\": [\\"http://192.168.1.1\\", \\"ftp://10.0.0.5/resource\\"], # \\"Public\\": [\\"http://example.com\\"], # \\"Invalid\\": [\\"http://invalidhostname\\"] # } ``` # Notes - Use `urllib.parse` to extract the hostname from the URL. - Use the `ipaddress` module to determine if an IP address is private or public. - Assume that if a hostname cannot be resolved, it will fall under the `Invalid` category. Implement the `categorize_urls` function to solve the problem as described.","solution":"import urllib.parse import ipaddress def hostname_to_ip(hostname): # This is a stub function, the real implementation will be provided during evaluation. # Here we return some hardcoded values for the sake of example. if hostname == \\"example.com\\": return \\"93.184.216.34\\" # Public IP elif hostname == \\"192.168.1.1\\": return \\"192.168.1.1\\" # Private IP elif hostname == \\"10.0.0.5\\": return \\"10.0.0.5\\" # Private IP elif hostname == \\"invalidhostname\\": return None else: return None def categorize_urls(urls): result = { \\"Private\\": [], \\"Public\\": [], \\"Invalid\\": [] } for url in urls: try: parsed_url = urllib.parse.urlparse(url) hostname = parsed_url.hostname ip = hostname_to_ip(hostname) if not ip: result[\\"Invalid\\"].append(url) continue ip_obj = ipaddress.ip_address(ip) if ip_obj.is_private: result[\\"Private\\"].append(url) else: result[\\"Public\\"].append(url) except Exception as e: result[\\"Invalid\\"].append(url) return result"},{"question":"You are tasked with implementing a function that checks if a list of URLs can be fetched by a specified user agent according to the rules defined in the `robots.txt` file of each URL\'s domain. You will use the `RobotFileParser` class from the `urllib.robotparser` module for this task. Function Signature ```python def fetch_permissions(useragent: str, urls: List[str]) -> List[Dict[str, Any]]: ``` Input - `useragent` (str): The user agent for which we need to check permissions. - `urls` (List[str]): A list of URLs (strings) for which fetch permissions need to be checked. Output - List of dictionaries, each containing: - `url`: The URL (str) being checked. - `can_fetch`: Boolean indicating whether the user agent is allowed to fetch the URL. - `crawl_delay`: The crawl delay value for the user agent as defined in robots.txt (if any). Return `None` if not specified or not applicable. - `request_rate`: The request rate for the user agent as a dictionary with keys `requests` and `seconds` (if any). Return `None` if not specified or not applicable. Constraints - Assume that the URLs are well-formed and belong to valid domains. - Performance constraints expect the function to handle up to 100 URLs efficiently. Example ```python result = fetch_permissions(\\"MyBot\\", [\\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\"]) print(result) # [ # { # \\"url\\": \\"http://www.example.com/page1\\", # \\"can_fetch\\": True, # \\"crawl_delay\\": 10, # \\"request_rate\\": {\\"requests\\": 5, \\"seconds\\": 60} # }, # { # \\"url\\": \\"http://www.example.com/page2\\", # \\"can_fetch\\": False, # \\"crawl_delay\\": 10, # \\"request_rate\\": {\\"requests\\": 5, \\"seconds\\": 60} # } # ] ``` Notes - You will need to extract the domain of each URL to find the corresponding `robots.txt` file. - Use the `RobotFileParser` class methods to parse the `robots.txt` file and retrieve the required information. - Ensure the function handles errors gracefully, such as network issues while fetching the `robots.txt` file by returning appropriate default values (`can_fetch` as `False`, `crawl_delay` and `request_rate` as `None`).","solution":"from urllib.parse import urlparse from urllib.robotparser import RobotFileParser from typing import List, Dict, Any import requests def fetch_permissions(useragent: str, urls: List[str]) -> List[Dict[str, Any]]: def get_robots_txt_url(url: str) -> str: parsed_url = urlparse(url) return f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\" def fetch_robot_txt(robots_txt_url: str) -> str: try: response = requests.get(robots_txt_url) if response.status_code == 200: return response.text return \\"\\" except requests.RequestException: return \\"\\" results = [] robot_parsers = {} for url in urls: parsed_url = urlparse(url) domain = f\\"{parsed_url.scheme}://{parsed_url.netloc}\\" robots_txt_url = get_robots_txt_url(url) if domain not in robot_parsers: robots_txt_content = fetch_robot_txt(robots_txt_url) rfp = RobotFileParser() rfp.parse(robots_txt_content.split(\'n\')) robot_parsers[domain] = rfp else: rfp = robot_parsers[domain] can_fetch = rfp.can_fetch(useragent, url) crawl_delay = rfp.crawl_delay(useragent) request_rate = rfp.request_rate(useragent) if request_rate: request_rate = {\\"requests\\": request_rate[0], \\"seconds\\": request_rate[1]} else: request_rate = None results.append({ \\"url\\": url, \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate }) return results"},{"question":"# Question Given a dataset of your choice, implement a function `create_custom_palette_visualization` that accepts the following parameters: - `data`: A pandas DataFrame containing the dataset. - `x_col`: The name of the column to be used for the x-axis in the plot. - `y_col`: The name of the column to be used for the y-axis in the plot. - `palette_type`: A string that specifies the type of palette to use. It can be either \\"discrete\\" or \\"continuous\\". - `color_spec`: An identifier for the color. This can be a named color, a hex code, or a tuple for Husl system colors. - `num_colors`: An optional integer specifying the number of colors in the palette (valid only for discrete palettes). The function should generate a scatter plot using Seaborn, with the color palette applied to the points in the plot based on the specified parameters. If `palette_type` is \\"discrete\\", the palette should consist of `num_colors` colors. If `palette_type` is \\"continuous\\", the function should create a continuous colormap. Requirements: - You must handle potential errors where invalid `palette_type` or `color_spec` values are provided. - The plot should be well-labeled with appropriate titles and axis labels. - Ensure the colormap or discrete palette is applied correctly to the scatter plot\'s aesthetic. # Example Usage: ```python import pandas as pd # Sample DataFrame data = pd.DataFrame({ \'x\': [1, 2, 3, 4, 5, 6], \'y\': [10, 15, 13, 17, 20, 23] }) # Create a custom palette visualization create_custom_palette_visualization(data, x_col=\'x\', y_col=\'y\', palette_type=\'discrete\', color_spec=\'seagreen\', num_colors=6) # Create another custom palette visualization with a continuous colormap create_custom_palette_visualization(data, x_col=\'x\', y_col=\'y\', palette_type=\'continuous\', color_spec=\'#a275ac\') ``` # Input and Output Formats: - **Input**: - `data`: pandas DataFrame - `x_col`: string - `y_col`: string - `palette_type`: string (\\"discrete\\" or \\"continuous\\") - `color_spec`: string or tuple - `num_colors`: (optional) integer - **Output**: - A scatter plot displayed with the specified custom palette. # Constraints: - The `palette_type` must be either \\"discrete\\" or \\"continuous.\\" - The `color_spec` must be a valid color identifier recognizable by Seaborn. - If `palette_type` is \\"discrete\\" and `num_colors` is not specified, a default of 6 colors should be used. - If `palette_type` is \\"continuous,\\" the `num_colors` parameter should be ignored. # Hints: - Use `sns.light_palette` to generate the color palettes. - Use `sns.scatterplot` to create the scatter plot. - Consider using exception handling to manage invalid input scenarios.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_visualization(data, x_col, y_col, palette_type, color_spec, num_colors=6): Generates a scatter plot with a custom color palette. Parameters: - data (pd.DataFrame): The dataset to plot. - x_col (str): Column name for x-axis data. - y_col (str): Column name for y-axis data. - palette_type (str): \\"discrete\\" for discrete palette, \\"continuous\\" for continuous colormap. - color_spec (str/tuple): Color specification for the palette. Named color, hex code, or Husl system color. - num_colors (int, optional): Number of colors for discrete palette. Defaults to 6 if not specified. Raises: - ValueError: If invalid palette_type or color_spec is provided. if palette_type == \\"discrete\\": # Generate a discrete palette try: palette = sns.color_palette(color_spec, num_colors) except Exception as e: raise ValueError(f\\"Invalid color_spec or num_colors: {str(e)}\\") elif palette_type == \\"continuous\\": try: palette = sns.light_palette(color_spec, as_cmap=True) except Exception as e: raise ValueError(f\\"Invalid color_spec: {str(e)}\\") else: raise ValueError(\\"Invalid palette_type. It must be either \'discrete\' or \'continuous\'.\\") # Create the scatter plot sns.scatterplot(data=data, x=x_col, y=y_col, palette=palette) plt.title(\'Custom Palette Scatter Plot\') plt.xlabel(x_col) plt.ylabel(y_col) plt.show()"},{"question":"Problem Description You are tasked with developing a script that processes a list of filenames and determines how many of them match given patterns using Unix shell-style wildcards. You will create a function `count_matches` that takes two lists as input: 1. `filenames` - A list of filenames as strings. 2. `patterns` - A list of Unix shell-style wildcard patterns as strings. The function should return a dictionary where each key is a pattern from the `patterns` list and the corresponding value is the number of filenames from the `filenames` list that match that pattern, using case-insensitive matching. Constraints 1. `1  len(filenames)  1000` 2. `1  len(patterns)  100` 3. Each filename in `filenames` will be a non-empty string with a maximum length of 256 characters. 4. Each pattern in `patterns` will be a non-empty string with a maximum length of 100 characters. Function Signature ```python def count_matches(filenames: list[str], patterns: list[str]) -> dict[str, int]: ``` Example ```python filenames = [\'test.txt\', \'sample.TXT\', \'example.py\', \'demo.cpp\'] patterns = [\'*.txt\', \'*.py\', \'*.cpp\'] result = count_matches(filenames, patterns) # Expected output: {\'*.txt\': 2, \'*.py\': 1, \'*.cpp\': 1} ``` Implementation Notes 1. Use the `fnmatch.fnmatch` function to perform the matching in a case-insensitive manner. 2. Iterate through the filenames for each pattern to count the matches. 3. Ensure that your implementation is efficient and adheres to the constraints provided. Evaluation Criteria - **Correctness**: Your solution should accurately count the number of matches for each pattern. - **Efficiency**: Your solution should handle the input size constraints efficiently. - **Code Quality**: Your implementation should be clear, well-structured, and include appropriate comments if necessary.","solution":"import fnmatch def count_matches(filenames, patterns): This function takes in a list of filenames and a list of patterns, and returns a dictionary where each key is a pattern and the value is the number of filenames matching that pattern. result = {pattern: 0 for pattern in patterns} for pattern in patterns: for filename in filenames: if fnmatch.fnmatch(filename.lower(), pattern.lower()): result[pattern] += 1 return result"},{"question":"# **Coding Assessment Question** **Objective** You are required to write a Python function that demonstrates your understanding of the various covariance estimation techniques provided by the `sklearn` library. Specifically, you will: - Implement empirical covariance estimation. - Implement shrunk covariance estimation. - Implement Ledoit-Wolf shrinkage covariance estimation. - Implement Oracle Approximating Shrinkage covariance estimation. - Compare these estimations and provide a visualization of their results. **Guidelines** 1. **Function Signature** ```python def compare_covariance_estimations(data: np.ndarray) -> None: pass ``` 2. **Input** - `data` (numpy.ndarray): A 2-D array of shape (n_samples, n_features) representing the dataset for which the covariance matrix needs to be estimated. 3. **Output** - The function should not return anything. Instead, it should plot: - The empirical covariance matrix. - The shrunk covariance matrix (choose a shrinkage coefficient of 0.1). - The Ledoit-Wolf shrinkage covariance matrix. - The Oracle Approximating Shrinkage covariance matrix. 4. **Implementation Constraints** - Use `EmpiricalCovariance` for empirical covariance. - Use `ShrunkCovariance` for shrunk covariance with a shrinkage coefficient of 0.1. - Use `LedoitWolf` for Ledoit-Wolf shrinkage estimation. - Use `OAS` for Oracle Approximating Shrinkage estimation. - The plots should clearly label each covariance matrix type for comparison. 5. **Performance Requirements** - The function should efficiently handle datasets with up to 1000 samples and 50 features. **Example** Assume you have the following dataset `data`: ```python import numpy as np data = np.random.randn(100, 5) compare_covariance_estimations(data) ``` Expected outcome: The function should generate a plot with four subplots displaying empirical, shrunk (=0.1), Ledoit-Wolf, and OAS covariance matrices, each labeled appropriately. **Note** - Ensure that your data is centered if necessary, and explain your choice regarding the `assume_centered` parameter. - Utilize visualization libraries such as Matplotlib or Seaborn for plotting the results.","solution":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS def compare_covariance_estimations(data: np.ndarray) -> None: Compare different covariance estimation techniques and visualize their results. Parameters: ----------- data : numpy.ndarray A 2-D array of shape (n_samples, n_features) representing the dataset. # Ensure data is centered data_centered = data - np.mean(data, axis=0) # Empirical Covariance empirical_cov = EmpiricalCovariance().fit(data_centered).covariance_ # Shrunk Covariance with shrinkage coefficient 0.1 shrunk_cov = ShrunkCovariance(shrinkage=0.1).fit(data_centered).covariance_ # Ledoit-Wolf Covariance lw_cov = LedoitWolf().fit(data_centered).covariance_ # Oracle Approximating Shrinkage Covariance oas_cov = OAS().fit(data_centered).covariance_ # Plotting fig, axes = plt.subplots(2, 2, figsize=(12, 10)) sns.heatmap(empirical_cov, ax=axes[0, 0], cmap=\'viridis\', cbar=False) axes[0, 0].set_title(\'Empirical Covariance\') sns.heatmap(shrunk_cov, ax=axes[0, 1], cmap=\'viridis\', cbar=False) axes[0, 1].set_title(\'Shrunk Covariance (=0.1)\') sns.heatmap(lw_cov, ax=axes[1, 0], cmap=\'viridis\', cbar=False) axes[1, 0].set_title(\'Ledoit-Wolf Covariance\') sns.heatmap(oas_cov, ax=axes[1, 1], cmap=\'viridis\', cbar=False) axes[1, 1].set_title(\'OAS Covariance\') plt.tight_layout() plt.show()"},{"question":"**Objective**: Evaluate the understanding of scikit-learn metrics and their application in model evaluation for classification and regression. Question: You are provided with two datasets: one for a classification task and another for a regression task. Your task is to: 1. Train suitable models on these datasets. 2. Evaluate and compare the performances of these models using specific scikit-learn metrics. 3. Implement custom scoring functions where applicable and interpret the results. Instructions: 1. **Classification Task**: - Use the provided `classification_data.csv` as the dataset. - Train a RandomForest classifier on the dataset. - Use the following metrics to evaluate the classifier: accuracy, F1 score (macro), ROC AUC score. - Implement and use a custom scorer for precision-recall AUC. 2. **Regression Task**: - Use the provided `regression_data.csv` as the dataset. - Train a GradientBoosting regressor on the dataset. - Use the following metrics to evaluate the regressor: mean absolute error, mean squared error, R score. - Implement and use a custom scorer for the pinball loss with quantile 0.95. Expected Input and Output: - Inputs: Paths to the `classification_data.csv` and `regression_data.csv` files. - Outputs: Console prints of model evaluation metrics. Constraints: - You must use `RandomForestClassifier` for classification and `GradientBoostingRegressor` for regression. - Use scikit-learn\'s built-in functions for standard metrics and implement custom scoring functions for precision-recall AUC and pinball loss. Performance Requirements: - Ensure that your code works efficiently on typical datasets with approximately 10,000 samples each. # Solution Template: ```python import pandas as pd from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import ( accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score, precision_recall_curve, auc ) from sklearn.metrics import make_scorer def custom_precision_recall_auc(y_true, y_pred): precision, recall, _ = precision_recall_curve(y_true, y_pred) return auc(recall, precision) def custom_pinball_loss(y_true, y_pred): alpha = 0.95 return mean_pinball_loss(y_true, y_pred, alpha=alpha) # Load datasets classification_data = pd.read_csv(\'classification_data.csv\') regression_data = pd.read_csv(\'regression_data.csv\') # Classification Task X_clf = classification_data.drop(\'target\', axis=1) y_clf = classification_data[\'target\'] X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42) clf = RandomForestClassifier(random_state=42) clf.fit(X_train_clf, y_train_clf) y_pred_clf = clf.predict(X_test_clf) y_pred_proba_clf = clf.predict_proba(X_test_clf)[:, 1] # Classification Metrics print(\\"Classification Metrics:\\") print(f\\"Accuracy: {accuracy_score(y_test_clf, y_pred_clf):.2f}\\") print(f\\"F1 Score (macro): {f1_score(y_test_clf, y_pred_clf, average=\'macro\'):.2f}\\") print(f\\"ROC AUC Score: {roc_auc_score(y_test_clf, y_pred_proba_clf):.2f}\\") print(f\\"Precision-Recall AUC: {custom_precision_recall_auc(y_test_clf, y_pred_proba_clf):.2f}\\") # Regression Task X_reg = regression_data.drop(\'target\', axis=1) y_reg = regression_data[\'target\'] X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42) reg = GradientBoostingRegressor(random_state=42) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) # Regression Metrics print(\\"nRegression Metrics:\\") print(f\\"Mean Absolute Error: {mean_absolute_error(y_test_reg, y_pred_reg):.2f}\\") print(f\\"Mean Squared Error: {mean_squared_error(y_test_reg, y_pred_reg):.2f}\\") print(f\\"R Score: {r2_score(y_test_reg, y_pred_reg):.2f}\\") print(f\\"Pinball Loss: {custom_pinball_loss(y_test_reg, y_pred_reg):.2f}\\") ``` Make sure to replace placeholder `classification_data.csv` and `regression_data.csv` files with actual dataset paths provided for the assessment. Dataset Format: - `classification_data.csv`: The dataset should have `target` column as the target variable and others as feature columns. - `regression_data.csv`: The dataset should have `target` column as the target variable and others as feature columns. # Remarks: - Compare outputs, interpret results, and suggest improvements where necessary.","solution":"import pandas as pd from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import ( accuracy_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score, precision_recall_curve, auc, make_scorer, mean_pinball_loss, ) def custom_precision_recall_auc(y_true, y_pred): precision, recall, _ = precision_recall_curve(y_true, y_pred) return auc(recall, precision) def custom_pinball_loss(y_true, y_pred, alpha=0.95): return mean_pinball_loss(y_true, y_pred, alpha=alpha) def classification_task(classification_data_path): classification_data = pd.read_csv(classification_data_path) X_clf = classification_data.drop(\'target\', axis=1) y_clf = classification_data[\'target\'] X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42) clf = RandomForestClassifier(random_state=42) clf.fit(X_train_clf, y_train_clf) y_pred_clf = clf.predict(X_test_clf) y_pred_proba_clf = clf.predict_proba(X_test_clf)[:, 1] classification_metrics = { \\"Accuracy\\": accuracy_score(y_test_clf, y_pred_clf), \\"F1 Score (macro)\\": f1_score(y_test_clf, y_pred_clf, average=\'macro\'), \\"ROC AUC Score\\": roc_auc_score(y_test_clf, y_pred_proba_clf), \\"Precision-Recall AUC\\": custom_precision_recall_auc(y_test_clf, y_pred_proba_clf), } return classification_metrics def regression_task(regression_data_path): regression_data = pd.read_csv(regression_data_path) X_reg = regression_data.drop(\'target\', axis=1) y_reg = regression_data[\'target\'] X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42) reg = GradientBoostingRegressor(random_state=42) reg.fit(X_train_reg, y_train_reg) y_pred_reg = reg.predict(X_test_reg) regression_metrics = { \\"Mean Absolute Error\\": mean_absolute_error(y_test_reg, y_pred_reg), \\"Mean Squared Error\\": mean_squared_error(y_test_reg, y_pred_reg), \\"R Score\\": r2_score(y_test_reg, y_pred_reg), \\"Pinball Loss\\": custom_pinball_loss(y_test_reg, y_pred_reg), } return regression_metrics"},{"question":"Coding Assessment Question # Objective To assess your understanding of the `tokenize` module in Python, you will write a program that modifies Python source code by obfuscating variable names while keeping the code functional and correctly formatted. # Problem Statement You are provided with a Python source code file that may contain various types of tokens including keywords, operators, literals, etc. Your task is to write a function `obfuscate_variables(file_path: str, obfuscation_map: dict) -> str` that: 1. Reads the source code from the file specified by `file_path`. 2. Tokenizes the source code using the `tokenize` module. 3. Replaces each variable name found in the token stream with a corresponding name from `obfuscation_map`. 4. Untokenizes the modified token stream back into a Python source code string. 5. Returns the obfuscated Python source code as a string. # Input - `file_path`: A string representing the file path to the Python source code file. - `obfuscation_map`: A dictionary where keys are original variable names (strings) and values are the obfuscated names to replace them with (strings). # Output - A string representing the obfuscated Python source code. # Constraints - The source code provided in the file is syntactically valid Python code. - All variable names in the source code are present in the `obfuscation_map`. # Example **Input:** ```python file_path = \'example.py\' obfuscation_map = {\'x\': \'var1\', \'y\': \'var2\'} ``` Contents of \'example.py\': ```python def add(x, y): return x + y result = add(3, 4) print(result) ``` **Output:** ```python def add(var1, var2): return var1 + var2 result = add(3, 4) print(result) ``` # Function Signature ```python def obfuscate_variables(file_path: str, obfuscation_map: dict) -> str: # Your code here ``` # Additional Information - You can use the `tokenize.tokenize()` or `tokenize.generate_tokens()` functions to tokenize the source code. - Handle the token stream by checking for `NAME` token types to identify variable names. - Use the `tokenize.untokenize()` function to reconstruct the source code from the modified token stream. # Hints - Consider using the `tokenize.open()` function for reading the file to automatically handle file encodings. - Use the `exact_type` property of tokens to differentiate between different types of `OP` tokens if necessary.","solution":"import tokenize from io import BytesIO def obfuscate_variables(file_path: str, obfuscation_map: dict) -> str: Obfuscate variable names in a Python source code file based on an obfuscation map. Parameters: - file_path: Path to the Python source code file. - obfuscation_map: A dictionary with original variable names as keys and obfuscated names as values. Returns: - Obfuscated source code as a string. with tokenize.open(file_path) as f: tokens = list(tokenize.generate_tokens(f.readline)) new_tokens = [] for token in tokens: if token.type == tokenize.NAME and token.string in obfuscation_map: new_token = tokenize.TokenInfo( type=token.type, string=obfuscation_map[token.string], start=token.start, end=token.end, line=token.line ) new_tokens.append(new_token) else: new_tokens.append(token) obfuscated_code = tokenize.untokenize(new_tokens) return obfuscated_code"},{"question":"<|Analysis Begin|> The provided documentation covers various specialized container datatypes from the `collections` module in Python. These include `namedtuple`, `deque`, `ChainMap`, `Counter`, `OrderedDict`, `defaultdict`, `UserDict`, `UserList`, and `UserString`. Each of these classes and functions offers unique functionality beyond the standard built-in types like `dict`, `list`, `tuple`, and `set`. While all these data structures offer interesting possibilities for a coding assessment, the `ChainMap` class is particularly well-suited for a challenging question that requires students to demonstrate both understanding and application of advanced concepts. `ChainMap` allows for the merging of multiple dictionaries into a single view for efficient lookup and updates. <|Analysis End|> <|Question Begin|> **Title: Implement a Configurations Manager Using ChainMap** **Problem Statement:** You are tasked with implementing a Configurations Manager for a software application. This manager should allow for various configurations, such as default configurations, environment-specific configurations, and user-specific configurations, to be combined and accessed as a unified dictionary-like object. You will use the `ChainMap` class from the `collections` module to achieve this. **Requirements:** 1. Implement a class named `ConfigManager` that: - Initializes with three dictionaries: `defaults`, `env_configs`, and `user_configs`. - Uses `ChainMap` to combine these three dictionaries, with precedence given to `user_configs`, then `env_configs`, and finally `defaults`. - Provides methods to get, set, and delete keys, affecting the appropriate layer of configuration according to the precedence rules of `ChainMap`. - Provides a method to add a new layer of configuration that has the highest precedence. **Class Definition and Methods:** ```python from collections import ChainMap class ConfigManager: def __init__(self, defaults, env_configs, user_configs): Initialize ConfigManager with three dictionaries. :param defaults: Default configurations :param env_configs: Environment-specific configurations :param user_configs: User-specific configurations self.configs = ChainMap(user_configs, env_configs, defaults) def get(self, key, default=None): Get the value of the specified key. :param key: Configuration key :param default: Default value to return if key is not found :return: Value associated with the key return self.configs.get(key, default) def set(self, key, value): Set the value of the specified key in the user configurations. :param key: Configuration key :param value: Value to associate with the key self.configs.maps[0][key] = value def delete(self, key): Delete the specified key from the user configurations, if it exists. :param key: Configuration key :raises KeyError: If the key is not found in the user configurations del self.configs.maps[0][key] def add_layer(self, new_layer): Add a new layer of configuration with the highest precedence. :param new_layer: Dictionary representing the new layer of configurations self.configs = self.configs.new_child(new_layer) ``` **Input and Output Format:** - The `ConfigManager` class should be initialized with three dictionaries: `defaults`, `env_configs`, and `user_configs`. - `get(key, default=None)`: Returns the value associated with `key` or `default` if `key` is not found. - `set(key, value)`: Sets the `value` for `key` in the user configurations. - `delete(key)`: Deletes `key` from the user configurations. - `add_layer(new_layer)`: Adds a new configuration layer with higher precedence than all existing layers. **Example Usage:** ```python defaults = {\'theme\': \'light\', \'language\': \'en\'} env_configs = {\'theme\': \'dark\'} user_configs = {\'language\': \'fr\'} config_manager = ConfigManager(defaults, env_configs, user_configs) # Get configurations print(config_manager.get(\'theme\')) # Output: \'dark\' print(config_manager.get(\'language\')) # Output: \'fr\' # Set a new user-specific configuration config_manager.set(\'editor\', \'vscode\') print(config_manager.get(\'editor\')) # Output: \'vscode\' # Delete a user-specific configuration config_manager.delete(\'editor\') print(config_manager.get(\'editor\')) # Output: None # Add a new layer of configurations config_manager.add_layer({\'theme\': \'solarized\'}) print(config_manager.get(\'theme\')) # Output: \'solarized\' ``` **Constraints:** - The configurations should be stored as dictionary objects. - Students are expected to use the `ChainMap` class appropriately to combine and manage these dictionaries. **Note:** Ensure that your implementation handles cases where keys might be missing or attempts to delete keys that are not present in the expected layer.","solution":"from collections import ChainMap class ConfigManager: def __init__(self, defaults, env_configs, user_configs): Initialize ConfigManager with three dictionaries. :param defaults: Default configurations :param env_configs: Environment-specific configurations :param user_configs: User-specific configurations self.configs = ChainMap(user_configs, env_configs, defaults) def get(self, key, default=None): Get the value of the specified key. :param key: Configuration key :param default: Default value to return if key is not found :return: Value associated with the key return self.configs.get(key, default) def set(self, key, value): Set the value of the specified key in the user configurations. :param key: Configuration key :param value: Value to associate with the key self.configs.maps[0][key] = value def delete(self, key): Delete the specified key from the user configurations, if it exists. :param key: Configuration key :raises KeyError: If the key is not found in the user configurations del self.configs.maps[0][key] def add_layer(self, new_layer): Add a new layer of configuration with the highest precedence. :param new_layer: Dictionary representing the new layer of configurations self.configs = self.configs.new_child(new_layer)"},{"question":"Objective The purpose of this exercise is to assess your understanding and ability to use various Python standard libraries to solve a comprehensive problem. You will demonstrate your skills by building a small utility script. Problem Statement You are tasked with creating a command-line utility that archives all Python (`.py`) files from a specified directory, compresses the archive, and logs the operation details. The script should follow these steps: 1. Accept a directory path as a command-line argument. 2. Find all Python files in the specified directory and its subdirectories. 3. Copy these files to a new directory named `python_files_backup` within the specified directory. 4. Create a ZIP archive of the copied files. 5. Write a log entry (including a timestamp) of each operation to a file named `backup.log` in the specified directory. Requirements - Use the `os`, `shutil`, `glob`, `argparse`, `datetime`, `zipfile`, and `sys` modules. - The utility script should be named `python_backup.py`. - Input and Output formats: - Input: `python python_backup.py <directory_path>` - Output: Creates a `python_files_backup` directory, `python_files_backup.zip`, and `backup.log` inside the specified directory. - Ensure that the script handles errors gracefully: - Invalid directory paths should result in an informative error message. - If the directory is empty or no Python files are found, log this information. Constraints - The utility should only archive files with the `.py` extension. - Performance considerations for handling large directories with many files should be considered. - You may not use any third-party libraries. Example Assume the script is run with: ```bash python python_backup.py /home/user/projects ``` If `/home/user/projects` contains the following structure: ``` /home/user/projects   project1   script1.py   script2.py   project2   script3.py   today.txt ``` The script should produce: ``` /home/user/projects   project1   script1.py   script2.py   project2   script3.py   python_files_backup/   project1    script1.py    script2.py   project2   script3.py   python_files_backup.zip   backup.log ``` The `backup.log` file should include entries similar to: ``` [Timestamp] Created backup directory: /home/user/projects/python_files_backup [Timestamp] Archived file: /home/user/projects/project1/script1.py [Timestamp] Archived file: /home/user/projects/project1/script2.py [Timestamp] Archived file: /home/user/projects/project2/script3.py [Timestamp] Created ZIP archive: /home/user/projects/python_files_backup.zip ``` Submission Provide the `python_backup.py` script that fulfills the above requirements.","solution":"import os import shutil import glob import argparse import datetime import zipfile import sys def log_message(directory, message): with open(os.path.join(directory, \'backup.log\'), \'a\') as log_file: timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') log_file.write(f\'[{timestamp}] {message}n\') def backup_python_files(directory): if not os.path.isdir(directory): print(f\\"Error: The directory \'{directory}\' does not exist.\\") sys.exit(1) backup_dir = os.path.join(directory, \'python_files_backup\') if not os.path.exists(backup_dir): os.makedirs(backup_dir) log_message(directory, f\'Created backup directory: {backup_dir}\') python_files = glob.glob(os.path.join(directory, \'**\', \'*.py\'), recursive=True) if not python_files: log_message(directory, \'No Python files found for backup.\') print(f\\"No Python files found in \'{directory}\'.\\") return for file_path in python_files: relative_path = os.path.relpath(file_path, directory) dest_path = os.path.join(backup_dir, relative_path) dest_dir = os.path.dirname(dest_path) if not os.path.exists(dest_dir): os.makedirs(dest_dir) shutil.copy2(file_path, dest_path) log_message(directory, f\'Archived file: {file_path}\') zip_file_path = os.path.join(directory, \'python_files_backup.zip\') with zipfile.ZipFile(zip_file_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, _, files in os.walk(backup_dir): for file in files: file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, start=backup_dir) zipf.write(file_path, arcname) log_message(directory, f\'Created ZIP archive: {zip_file_path}\') def main(): parser = argparse.ArgumentParser(description=\'Archive Python files from a specified directory.\') parser.add_argument(\'directory\', metavar=\'directory\', type=str, help=\'The directory path to search for Python files.\') args = parser.parse_args() backup_python_files(args.directory) if __name__ == \'__main__\': main()"},{"question":"# Time Manipulation and Conversion Problem Statement You are tasked with creating a small utility to handle various time conversion and manipulation tasks using Python\'s `time` module. The goal is to implement a function `time_utility` which accepts two parameters: a timestamp and a desired operation. Based on the operation, the function will perform the following tasks: 1. **Convert to Local Time:** - Convert the given timestamp (in seconds since the epoch) to local time and return a formatted string representing the local time. 2. **Convert to UTC:** - Convert the given timestamp (in seconds since the epoch) to UTC and return a formatted string representing the UTC time. 3. **Measure Process Time:** - Measure the CPU time consumed by the current process. Return this time as a float in fractional seconds. 4. **Measure Thread Time:** - Measure the CPU time consumed by the current thread. Return this time as a float in fractional seconds. Input - `timestamp`: A floating-point number representing seconds since the epoch (optional, default is current time). - `operation`: A string specifying the operation. It can be one of `\\"local_time\\"`, `\\"utc_time\\"`, `\\"process_time\\"`, or `\\"thread_time\\"`. Output - A formatted string representing the local or UTC time, or a float representing the CPU time in fractional seconds. Examples ```python >>> time_utility(1633065600.0, \\"local_time\\") \'Wed Sep 30 15:00:00 2021\' >>> time_utility(1633065600.0, \\"utc_time\\") \'Wed Sep 30 19:00:00 2021\' >>> time_utility(operation=\\"process_time\\") 0.0456789 >>> time_utility(operation=\\"thread_time\\") 0.0234567 ``` # Constraints - If `timestamp` is not provided, use the current time. - Ensure the function handles edge cases such as timestamps before the epoch and in the far future up to the range supported by the `(time.gmtime(), time.localtime() or time.struct_time())`. # Function Signature ```python import time def time_utility(timestamp: float = None, operation: str = \'\') -> str or float: pass ``` Implement this function considering the provided constraints and examples.","solution":"import time def time_utility(timestamp: float = None, operation: str = \'\') -> str or float: if timestamp is None: timestamp = time.time() if operation == \\"local_time\\": local_time = time.localtime(timestamp) return time.strftime(\'%a %b %d %H:%M:%S %Y\', local_time) elif operation == \\"utc_time\\": utc_time = time.gmtime(timestamp) return time.strftime(\'%a %b %d %H:%M:%S %Y\', utc_time) elif operation == \\"process_time\\": return time.process_time() elif operation == \\"thread_time\\": return time.thread_time() else: raise ValueError(f\\"Unsupported operation: {operation}\\")"},{"question":"# **Support Vector Machine Coding Assessment** **Objective** Create a comprehensive coding solution that demonstrates your understanding of support vector machines (SVMs) using scikit-learn. This task will assess your ability to handle data preprocessing, train an SVM model, analyze results, and implement custom kernel functions. **Task Description** Given a dataset, your task is to: 1. Preprocess the data appropriately. 2. Train both a linear and a Gaussian (RBF) kernel SVM for classification. 3. Evaluate the models using appropriate metrics. 4. Implement a custom polynomial kernel and train a new SVM model using it. **Instructions** 1. **Data Preprocessing:** - Load the dataset. - Split the data into training and testing sets. - Standardize the feature values to have a mean of 0 and variance of 1. 2. **Train Linear and RBF Kernel SVMs:** - Train a linear SVM (`LinearSVC`) and an RBF kernel SVM (`SVC` with `kernel=\'rbf\'`). - Use GridSearchCV to find the best hyperparameters for the RBF kernel (e.g., `C` and `gamma`). 3. **Evaluate Models:** - Evaluate the trained models on the test set and report the accuracy, precision, recall, and F1 scores. - Plot the decision boundaries for visualization (if the dataset has 2 features). 4. **Custom Polynomial Kernel:** - Implement a polynomial kernel function. - Train a new SVM model using the custom polynomial kernel. - Evaluate and compare the performance of this model against the linear and RBF models. **Expected Input and Output:** - **Input:** A dataset with features and labels. - **Output:** Evaluation metrics for each model, and decision boundary plots for visualization. **Constraints:** - Use scikit-learn\'s functionalities for SVM implementations. - The dataset must be appropriately split into training and testing sets (e.g., 80-20 split). - Performance evaluation metrics must be computed using scikit-learn\'s built-in methods. **Performance Requirements:** - Efficient handling of the dataset and model training. - Clear and concise code with proper comments explaining each part of the implementation. - Effective use of visualization for model comparison. **Dataset** For this task, you can use the `Iris` dataset from scikit-learn\'s datasets module. **Code Template** ```python import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, LinearSVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, plot_confusion_matrix # Load the dataset data = datasets.load_iris() X, y = data.data, data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the feature values scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train a Linear SVM linear_svc = LinearSVC(random_state=42) linear_svc.fit(X_train, y_train) y_pred_linear = linear_svc.predict(X_test) # Evaluate the Linear SVM accuracy_linear = accuracy_score(y_test, y_pred_linear) precision_linear = precision_score(y_test, y_pred_linear, average=\'weighted\') recall_linear = recall_score(y_test, y_pred_linear, average=\'weighted\') f1_linear = f1_score(y_test, y_pred_linear, average=\'weighted\') print(f\\"Linear SVM - Accuracy: {accuracy_linear:.2f}, Precision: {precision_linear:.2f}, Recall: {recall_linear:.2f}, F1 Score: {f1_linear:.2f}\\") # Train an RBF Kernel SVM using GridSearchCV params = {\'C\': [0.1, 1, 10], \'gamma\': [1, 0.1, 0.01]} rbf_svc = SVC(kernel=\'rbf\') grid_search = GridSearchCV(rbf_svc, param_grid=params, cv=5) grid_search.fit(X_train, y_train) best_rbf_svc = grid_search.best_estimator_ y_pred_rbf = best_rbf_svc.predict(X_test) # Evaluate the RBF Kernel SVM accuracy_rbf = accuracy_score(y_test, y_pred_rbf) precision_rbf = precision_score(y_test, y_pred_rbf, average=\'weighted\') recall_rbf = recall_score(y_test, y_pred_rbf, average=\'weighted\') f1_rbf = f1_score(y_test, y_pred_rbf, average=\'weighted\') print(f\\"RBF SVM - Accuracy: {accuracy_rbf:.2f}, Precision: {precision_rbf:.2f}, Recall: {recall_rbf:.2f}, F1 Score: {f1_rbf:.2f}\\") # Function to implement a custom polynomial kernel def polynomial_kernel(X, Y, degree=3, coef0=1): return (np.dot(X, Y.T) + coef0) ** degree # Train a Custom Polynomial Kernel SVM custom_svc = SVC(kernel=polynomial_kernel) custom_svc.fit(X_train, y_train) y_pred_poly = custom_svc.predict(X_test) # Evaluate the Custom Polynomial Kernel SVM accuracy_poly = accuracy_score(y_test, y_pred_poly) precision_poly = precision_score(y_test, y_pred_poly, average=\'weighted\') recall_poly = recall_score(y_test, y_pred_poly, average=\'weighted\') f1_poly = f1_score(y_test, y_pred_poly, average=\'weighted\') print(f\\"Polynomial Kernel SVM - Accuracy: {accuracy_poly:.2f}, Precision: {precision_poly:.2f}, Recall: {recall_poly:.2f}, F1 Score: {f1_poly:.2f}\\") # Plotting decision boundaries (if dataset has 2 features) def plot_decision_boundary(X, y, model, title): # Create a mesh to plot the decision boundary h = .02 # Step size in the mesh x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.3) plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\'k\', s=20) plt.title(title) plt.show() # Only plot if there are 2 features if X.shape[1] == 2: plot_decision_boundary(X_train, y_train, linear_svc, \'Linear SVM Decision Boundary\') plot_decision_boundary(X_train, y_train, best_rbf_svc, \'RBF Kernel SVM Decision Boundary\') plot_decision_boundary(X_train, y_train, custom_svc, \'Polynomial Kernel SVM Decision Boundary\') ``` **Submission** Submit the code along with a report containing the evaluation metrics and decision boundary plots. Your report should include explanations of the preprocessing steps, model training process, and an analysis of the results.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, LinearSVC from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_and_preprocess_data(): # Load the dataset data = datasets.load_iris() X, y = data.data, data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the feature values scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_evaluate_linear_svm(X_train, X_test, y_train, y_test): # Train a Linear SVM linear_svc = LinearSVC(random_state=42) linear_svc.fit(X_train, y_train) y_pred_linear = linear_svc.predict(X_test) # Evaluate the Linear SVM accuracy_linear = accuracy_score(y_test, y_pred_linear) precision_linear = precision_score(y_test, y_pred_linear, average=\'weighted\') recall_linear = recall_score(y_test, y_pred_linear, average=\'weighted\') f1_linear = f1_score(y_test, y_pred_linear, average=\'weighted\') return linear_svc, accuracy_linear, precision_linear, recall_linear, f1_linear def train_evaluate_rbf_svm(X_train, X_test, y_train, y_test): # Train an RBF Kernel SVM using GridSearchCV params = {\'C\': [0.1, 1, 10], \'gamma\': [1, 0.1, 0.01]} rbf_svc = SVC(kernel=\'rbf\') grid_search = GridSearchCV(rbf_svc, param_grid=params, cv=5) grid_search.fit(X_train, y_train) best_rbf_svc = grid_search.best_estimator_ y_pred_rbf = best_rbf_svc.predict(X_test) # Evaluate the RBF Kernel SVM accuracy_rbf = accuracy_score(y_test, y_pred_rbf) precision_rbf = precision_score(y_test, y_pred_rbf, average=\'weighted\') recall_rbf = recall_score(y_test, y_pred_rbf, average=\'weighted\') f1_rbf = f1_score(y_test, y_pred_rbf, average=\'weighted\') return best_rbf_svc, accuracy_rbf, precision_rbf, recall_rbf, f1_rbf def custom_polynomial_kernel(X, Y, degree=3, coef0=1): return (np.dot(X, Y.T) + coef0) ** degree def train_evaluate_custom_poly_svm(X_train, X_test, y_train, y_test): # Train a Custom Polynomial Kernel SVM custom_svc = SVC(kernel=custom_polynomial_kernel) custom_svc.fit(X_train, y_train) y_pred_poly = custom_svc.predict(X_test) # Evaluate the Custom Polynomial Kernel SVM accuracy_poly = accuracy_score(y_test, y_pred_poly) precision_poly = precision_score(y_test, y_pred_poly, average=\'weighted\') recall_poly = recall_score(y_test, y_pred_poly, average=\'weighted\') f1_poly = f1_score(y_test, y_pred_poly, average=\'weighted\') return custom_svc, accuracy_poly, precision_poly, recall_poly, f1_poly"},{"question":"**Objective**: Implement a function that sorts a complex nested structure of dictionaries and lists using various sorting techniques discussed in the documentation. **Problem Statement**: You are provided with a list of dictionaries, each containing information about a student, such as name, age, and a list of grades in various subjects. Your task is to implement a function `sort_students(data: List[Dict[str, Any]], key_specs: List[Tuple[str, bool]]) -> List[Dict[str, Any]]` that sorts this list based on multiple sorting criteria specified in `key_specs`. **Requirements**: 1. **Input**: - `data`: A list of dictionaries, where each dictionary represents a student\'s information: ```python data = [ {\\"name\\": \\"John\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 90), (\\"English\\", 85)]}, {\\"name\\": \\"Jane\\", \\"age\\": 14, \\"grades\\": [(\\"Math\\", 92), (\\"English\\", 88)]}, {\\"name\\": \\"Doe\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 85), (\\"English\\", 90)]}, ... ] ``` - `key_specs`: A list of tuples where each tuple contains a string (key by which to sort, it could be \'name\', \'age\', or the subject name inside grades list) and a boolean (indicating whether to sort in descending order). 2. **Output**: - A sorted list of student dictionaries based on the specified `key_specs`. 3. **Constraints**: - The sorting should be stable. - Handle nested sorting, e.g., primary sort by age and secondary sort by Math grade. - Efficient sorting should be utilized. 4. **Performance Requirements**: - The function should handle up to 10,000 student records efficiently. **Example Usage**: ```python data = [ {\\"name\\": \\"John\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 90), (\\"English\\", 85)]}, {\\"name\\": \\"Jane\\", \\"age\\": 14, \\"grades\\": [(\\"Math\\", 92), (\\"English\\", 88)]}, {\\"name\\": \\"Doe\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 85), (\\"English\\", 90)]}, ] key_specs = [(\\"age\\", False), (\\"Math\\", True)] sorted_students = sort_students(data, key_specs) # Expected output: # [ # {\\"name\\": \\"Jane\\", \\"age\\": 14, \\"grades\\": [(\\"Math\\", 92), (\\"English\\", 88)]}, # {\\"name\\": \\"Doe\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 85), (\\"English\\", 90)]}, # {\\"name\\": \\"John\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 90), (\\"English\\", 85)]}, # ] ``` **Implementation Details**: 1. Implement the `sort_students` function. 2. It should use `sorted()` with custom key functions. 3. Apply multiple sorting criteria as specified in `key_specs`. **Additional Information**: - Use the `attrgetter`, `itemgetter` from the `operator` module and custom lambda functions where appropriate. - Consider stability in sorting as mentioned in the documentation. Submit your implementation and ensure it passes the following test cases. ```python def test_sort_students(): data = [ {\\"name\\": \\"John\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 90), (\\"English\\", 85)]}, {\\"name\\": \\"Jane\\", \\"age\\": 14, \\"grades\\": [(\\"Math\\", 92), (\\"English\\", 88)]}, {\\"name\\": \\"Doe\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 85), (\\"English\\", 90)]}, ] key_specs = [(\\"age\\", False), (\\"Math\\", True)] assert sort_students(data, key_specs) == [ {\\"name\\": \\"Jane\\", \\"age\\": 14, \\"grades\\": [(\\"Math\\", 92), (\\"English\\", 88)]}, {\\"name\\": \\"John\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 90), (\\"English\\", 85)]}, {\\"name\\": \\"Doe\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 85), (\\"English\\", 90)]}, ] test_sort_students() ```","solution":"from typing import List, Dict, Any, Tuple def sort_students(data: List[Dict[str, Any]], key_specs: List[Tuple[str, bool]]) -> List[Dict[str, Any]]: def get_grade(student, subject): for subj, grade in student[\\"grades\\"]: if subj == subject: return grade return None def sort_key(student): keys = [] for key, reverse in key_specs: if key in student: keys.append((student[key], reverse)) else: keys.append((get_grade(student, key), reverse)) return tuple(-k if rev else k for k, rev in keys) return sorted(data, key=sort_key) # Example use case data = [ {\\"name\\": \\"John\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 90), (\\"English\\", 85)]}, {\\"name\\": \\"Jane\\", \\"age\\": 14, \\"grades\\": [(\\"Math\\", 92), (\\"English\\", 88)]}, {\\"name\\": \\"Doe\\", \\"age\\": 15, \\"grades\\": [(\\"Math\\", 85), (\\"English\\", 90)]}, ] key_specs = [(\\"age\\", False), (\\"Math\\", True)] sorted_students = sort_students(data, key_specs) print(sorted_students)"},{"question":"Objective Implement a class in Python that mimics some behaviors of method and instance method objects, focusing on binding and interaction with functions and instances. Problem Statement 1. Implement a class `CustomMethod` that simulates a method object. * The class should accept a function and an instance upon initialization. * It should store these and provide: - A `__call__` method which calls the stored function with the instance as its first argument, followed by any additional arguments. - A `get_function` method which returns the stored function. - A `get_instance` method which returns the stored instance. 2. Implement a class `CustomInstanceMethod` that simulates an instance method object. * The class should accept a function upon initialization. * It should store the function and provide: - A `__get__` method which takes an instance and returns a `CustomMethod` object using the stored function and the passed instance. - A `get_function` method which returns the stored function. Example Usage ```python def example_method(self, x): return self.value + x class ExampleClass: def __init__(self, value): self.value = value # Creating an instance of ExampleClass example_instance = ExampleClass(10) # Creating an instance method using CustomInstanceMethod instance_method = CustomInstanceMethod(example_method) # Binding instance_method to example_instance bound_method = instance_method.__get__(example_instance) # Calling the bound method assert bound_method(5) == 15 # Retrieving function and instance assert bound_method.get_function() is example_method assert bound_method.get_instance() is example_instance ``` Constraints * You should not use Python\'s `types.MethodType` or similar constructs. * Your solution should correctly simulate the binding mechanism. Requirements * Your classes should be implemented in pure Python. * You must handle edge cases like calling unbound methods (methods not bound to any instance).","solution":"class CustomMethod: def __init__(self, function, instance): self.function = function self.instance = instance def __call__(self, *args, **kwargs): return self.function(self.instance, *args, **kwargs) def get_function(self): return self.function def get_instance(self): return self.instance class CustomInstanceMethod: def __init__(self, function): self.function = function def __get__(self, instance, owner): return CustomMethod(self.function, instance) def get_function(self): return self.function"},{"question":"# Seaborn Coding Assessment Objective This task evaluates your ability to use the seaborn library, specifically its `seaborn.objects` module, to visualize and count distinct observations in a dataset. Problem Statement You are given a dataset `tips` which contains information about customers\' dining habits at a restaurant. Your task is to: 1. Create a bar plot that shows the number of customers (`count`) visiting the restaurant on different days of the week. 2. Create a grouped bar plot that shows the count of customers visiting the restaurant on different days, separated by gender. 3. Create a bar plot that shows the count of customers for each party size. Dataset Description The `tips` dataset contains the following relevant fields: - `day`: The day of the week the visit occurred (e.g., \'Thur\', \'Fri\', \'Sat\', \'Sun\'). - `sex`: The gender of the person paying the bill. - `size`: The size of the party. Requirements 1. Use seaborn\'s `seaborn.objects` module to create the plots. 2. Ensure your plots are appropriately labeled for clarity. 3. Your solution should demonstrate an understanding of how to use counting and grouping mechanisms in seaborn. Input You can load the `tips` dataset using seaborn\'s `load_dataset` method: ```python from seaborn import load_dataset tips = load_dataset(\\"tips\\") ``` Output 1. A bar plot showing customer counts by day. 2. A grouped bar plot showing customer counts by day and gender. 3. A bar plot showing customer counts by party size. Example Here is an example of how you might create a simple bar plot to count observations: ```python import seaborn.objects as so from seaborn import load_dataset # Loading the tips dataset tips = load_dataset(\\"tips\\") # Creating a bar plot for counts by day so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()).show() ``` Now, using this example as a guide, complete the other tasks specified above. Constraints and Limitations 1. Your code should be efficient and make use of the features provided by the seaborn library. 2. Ensure that the plots are correctly displayed and labeled for interpretability. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset # Loading the tips dataset tips = load_dataset(\\"tips\\") def plot_customer_count_by_day(): plot = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) plot.show() def plot_customer_count_by_day_and_gender(): plot = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count()) plot.show() def plot_customer_count_by_party_size(): plot = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot.show()"},{"question":"# Concurrent Task Manager Objective: Design a Python function to execute both CPU-bound and I/O-bound tasks concurrently using the `threading` and `multiprocessing` modules. This function should leverage threading for I/O-bound tasks and multiprocessing for CPU-bound tasks to optimize performance. Function Specification: - Name: `concurrent_task_manager` - Input: - `io_tasks`: A list of callables (functions) representing I/O-bound tasks. - `cpu_tasks`: A list of callables (functions) representing CPU-bound tasks. - Output: - A tuple containing two lists: - Results of I/O-bound tasks. - Results of CPU-bound tasks. - Constraints: - The number of tasks in `io_tasks` and `cpu_tasks` can vary and is not fixed. - Each task in `io_tasks` and `cpu_tasks` returns a value. - Tasks in `io_tasks` may include operations such as reading/writing files, fetching data from the internet, etc., which are time-consuming but not CPU-intensive. - Tasks in `cpu_tasks` are computation-heavy functions that require significant CPU time. - Performance Requirements: - The function should maximize concurrency and efficient utilization of system resources by correctly implementing threading for I/O-bound tasks and multiprocessing for CPU-bound tasks. Example: ```python def io_task_1(): # Simulate an I/O task (e.g., file reading) import time time.sleep(2) return \\"IO Task 1 complete\\" def io_task_2(): # Simulate another I/O task (e.g., network request) import time time.sleep(3) return \\"IO Task 2 complete\\" def cpu_task_1(): # Simulate a CPU-bound task (e.g., large computation) result = 0 for i in range(10**6): result += i return f\\"CPU Task 1 result: {result}\\" def cpu_task_2(): # Simulate another CPU-bound task result = 1 for i in range(1, 10**6): result *= i if result > 10**6: result = 1 return \\"CPU Task 2 complete\\" io_tasks = [io_task_1, io_task_2] cpu_tasks = [cpu_task_1, cpu_task_2] results = concurrent_task_manager(io_tasks, cpu_tasks) print(results) ``` Instructions: 1. Implement the `concurrent_task_manager` function to handle concurrent execution of I/O-bound and CPU-bound tasks. 2. Utilize the `ThreadPoolExecutor` from `concurrent.futures` or traditional `threading` module for I/O-bound tasks. 3. Utilize the `ProcessPoolExecutor` from `concurrent.futures` or the `multiprocessing` module for CPU-bound tasks. 4. Ensure that the function returns the results of both sets of tasks accurately and in a timely manner.","solution":"import concurrent.futures import threading import multiprocessing def concurrent_task_manager(io_tasks, cpu_tasks): Concurrently execute I/O-bound and CPU-bound tasks. :param io_tasks: List of callables representing I/O-bound tasks. :param cpu_tasks: List of callables representing CPU-bound tasks. :return: A tuple containing two lists; results of I/O-bound tasks and results of CPU-bound tasks. # To store the results of I/O-bound tasks io_results = [] # To store the results of CPU-bound tasks cpu_results = [] # Function to execute I/O-bound tasks using threading def execute_io_tasks(): with concurrent.futures.ThreadPoolExecutor() as executor: futures = [executor.submit(task) for task in io_tasks] for future in concurrent.futures.as_completed(futures): io_results.append(future.result()) # Function to execute CPU-bound tasks using multiprocessing def execute_cpu_tasks(): with concurrent.futures.ProcessPoolExecutor() as executor: futures = [executor.submit(task) for task in cpu_tasks] for future in concurrent.futures.as_completed(futures): cpu_results.append(future.result()) # Creating threads for I/O and CPU tasks io_thread = threading.Thread(target=execute_io_tasks) cpu_thread = threading.Thread(target=execute_cpu_tasks) # Start the threads io_thread.start() cpu_thread.start() # Wait for both threads to finish io_thread.join() cpu_thread.join() return (io_results, cpu_results)"},{"question":"# PyTorch Coding Assessment Question: Custom Data-Dependent Model with Conditional Branching Implement a custom PyTorch module that uses data-dependent control flow to process an input tensor. Specifically, your task is to create a module that branches its computation based on the standard deviation of the input tensor. # Requirements 1. **Class Definition**: Define a class `StandardDeviationConditionalModel` that inherits from `torch.nn.Module`. 2. **Forward Method**: Implement the `forward` method using `torch.cond` to determine the computation path based on the standard deviation of the input tensor `x`. - **Condition**: If the standard deviation of `x` is greater than a threshold value `threshold`, apply the `true_fn`, otherwise apply the `false_fn`. - **True Function (`true_fn`)**: Returns the result of applying the ReLU activation function followed by an element-wise multiplication of the tensor with itself. - **False Function (`false_fn`)**: Returns the result of applying the Tanh activation function. 3. **Initialization**: The class should accept a `threshold` parameter during initialization to set the standard deviation threshold for branching. # Input and Output Formats - **Input**: A single tensor `x` of arbitrary shape. - **Output**: A tensor with the same shape as the input, processed by either the `true_fn` or `false_fn` based on the condition. # Constraints - `torch.cond` must be used to implement the control flow. - The class should be compatible with both CPU and GPU tensors. - You must implement the computation paths (`true_fn` and `false_fn`) as separate functions within the class. # Performance Requirements - Ensure the functions avoid unnecessary computations and leverage PyTorch\'s efficient tensor operations. # Example Usage ```python import torch class StandardDeviationConditionalModel(torch.nn.Module): def __init__(self, threshold: float): super(StandardDeviationConditionalModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x): return torch.relu(x) * x def false_fn(x): return torch.tanh(x) std = torch.std(x) return torch.cond(std > self.threshold, true_fn, false_fn, (x,)) # Example usage model = StandardDeviationConditionalModel(threshold=2.0) inp = torch.randn(4, 4) result = model(inp) print(result) ``` **Note**: The provided example usage code is a template. Ensure your final implementation aligns with the requirements stated.","solution":"import torch import torch.nn as nn class StandardDeviationConditionalModel(nn.Module): def __init__(self, threshold: float): super(StandardDeviationConditionalModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x): return torch.relu(x) * x def false_fn(x): return torch.tanh(x) std = torch.std(x) if std.item() > self.threshold: return true_fn(x) else: return false_fn(x)"},{"question":"**Description**: You are tasked to enhance a command-line application by implementing a new feature using the `optparse` module. Specifically, you need to write a custom callback that processes a list of data items provided as a comma-separated string via the command-line. Each item in this list should be converted to uppercase before storing it. **Requirements**: Implement a command-line application that: 1. Defines an option `--items` which takes a comma-separated list of strings as its argument. 2. Uses a custom callback to process these strings, converting each to uppercase and storing them in a list. 3. Outputs the final list of processed items to the console. **Input**: The input will be a list of command-line arguments. For example: ```shell script.py --items=apple,banana,cherry ``` **Output**: Print the processed list to the console. For the example input, the output should be: ```python [\'APPLE\', \'BANANA\', \'CHERRY\'] ``` **Constraints**: - Use the `optparse` module to handle command-line arguments. - Your solution must include a custom callback function. **Code Template**: ```python from optparse import OptionParser, OptionValueError def uppercase_callback(option, opt_str, value, parser): items = value.split(\',\') uppercased_items = [item.upper() for item in items] setattr(parser.values, option.dest, uppercased_items) def main(): parser = OptionParser() # Define the `--items` option with a custom callback parser.add_option(\\"--items\\", type=\\"string\\", action=\\"callback\\", callback=uppercase_callback, dest=\\"items\\", help=\\"Provide a comma-separated list of items to be converted to uppercase.\\") (options, args) = parser.parse_args() if not options.items: parser.error(\\"No items provided\\") # Output the processed items print(options.items) if __name__ == \\"__main__\\": main() ``` # Explanation: 1. **Define the callback function**: The `uppercase_callback` function should take the comma-separated list, split it, convert each item to uppercase, and set the processed list to the appropriate destination. 2. **Add the option**: Using `parser.add_option`, define the `--items` option that utilizes the custom callback. 3. **Error Handling**: Ensure that an error is raised when no items are provided. 4. **Output**: Print the list of uppercase items once processed. **Instructions**: Write the complete implementation of the `uppercase_callback` function within the provided template. Test your code to ensure it handles inputs correctly and adheres to the constraints listed.","solution":"from optparse import OptionParser, OptionValueError def uppercase_callback(option, opt_str, value, parser): items = value.split(\',\') uppercased_items = [item.upper() for item in items] setattr(parser.values, option.dest, uppercased_items) def main(): parser = OptionParser() # Define the `--items` option with a custom callback parser.add_option(\\"--items\\", type=\\"string\\", action=\\"callback\\", callback=uppercase_callback, dest=\\"items\\", help=\\"Provide a comma-separated list of items to be converted to uppercase.\\") (options, args) = parser.parse_args() if not options.items: parser.error(\\"No items provided\\") # Output the processed items print(options.items) if __name__ == \\"__main__\\": main()"},{"question":"Coding Assessment Question # Objective: To assess the ability to use various Python built-in functions effectively to process data and achieve the desired output. # Problem Description: You are given a list of tuples where each tuple contains a name (string) and a score (integer). You need to filter, sort, and format this data into a specific output format using the following guidelines: 1. **Filter**: Remove all entries where the score is below a specified threshold. 2. **Sort**: Sort the remaining entries by score in descending order. If two entries have the same score, sort them by name in ascending order. 3. **Enumerate**: Assign a rank to each entry starting from 1 after sorting. 4. **Format**: Convert the processed data into the format \\"Rank. Name: Score\\". # Input: - A list of tuples, `data`, where each tuple contains a name (string) and a score (integer). - An integer, `threshold`, which defines the minimum score required to retain an entry. # Output: - A list of strings, where each string is in the format \\"Rank. Name: Score\\". # Example: **Input:** ```python data = [(\\"John\\", 50), (\\"Jane\\", 75), (\\"Doe\\", 65), (\\"Alex\\", 85)] threshold = 60 ``` **Output:** ```python [\'1. Alex: 85\', \'2. Jane: 75\', \'3. Doe: 65\'] ``` # Constraints: - All names are unique. - Scores are between 0 and 100 (inclusive). - Threshold will be a non-negative integer and at most 100. # Implementation: ```python def process_scores(data, threshold): This function processes the given data according to the specified guidelines. Args: data (list of tuples): A list where each element is a tuple (name, score). threshold (int): The minimum score required to retain an entry. Returns: list of str: A list where each entry is in the format \\"Rank. Name: Score\\". # Step 1: Filter the data filtered_data = list(filter(lambda x: x[1] >= threshold, data)) # Step 2: Sort the data by score (descending) and by name (ascending) if scores are the same sorted_data = sorted(filtered_data, key=lambda x: (-x[1], x[0])) # Step 3: Enumerate and format the data result = [f\\"{idx + 1}. {name}: {score}\\" for idx, (name, score) in enumerate(sorted_data)] return result ``` # Explanation: 1. **Filter the data** to remove any entries where the score is below the specified threshold. 2. **Sort the filtered data** by score in descending order and by name in ascending order in case of tie scores. 3. **Enumerate the sorted list** to assign ranks starting from 1 and format each entry in the specified \\"Rank. Name: Score\\" format. This task leverages several built-in functions such as `filter`, `sorted`, and `enumerate`, thus ensuring an assessment of the students\' understanding and effective use of these functions.","solution":"def process_scores(data, threshold): This function processes the given data according to the specified guidelines. Args: data (list of tuples): A list where each element is a tuple (name, score). threshold (int): The minimum score required to retain an entry. Returns: list of str: A list where each entry is in the format \\"Rank. Name: Score\\". # Step 1: Filter the data filtered_data = list(filter(lambda x: x[1] >= threshold, data)) # Step 2: Sort the data by score (descending) and by name (ascending) if scores are the same sorted_data = sorted(filtered_data, key=lambda x: (-x[1], x[0])) # Step 3: Enumerate and format the data result = [f\\"{idx + 1}. {name}: {score}\\" for idx, (name, score) in enumerate(sorted_data)] return result"},{"question":"You are required to implement a function that changes the terminal attributes to disable and then re-enable the canonical mode on a provided file descriptor. The function should temporarily alter the terminal settings to disable canonical mode and allow non-blocking input, then restore the original settings after a certain operation. Function Signature ```python def toggle_canonical_mode(fd: int, non_blocking_operation: Callable[[], Any]) -> Any: Temporarily disables canonical mode on the terminal referenced by the provided file descriptor, performs a user-defined non-blocking operation, and then restores the original terminal settings. Args: fd (int): The file descriptor representing the terminal (e.g., sys.stdin.fileno()). non_blocking_operation (Callable[[], Any]): The non-blocking operation to be performed while canonical mode is disabled. Returns: Any: The result of the non_blocking_operation. ``` Constraints 1. You can assume that the provided file descriptor points to a valid terminal device. 2. The `non_blocking_operation` callable may perform any task that requires non-blocking input (e.g., reading a single character without waiting for a newline). Example Usage ```python import sys import termios import time def example_non_blocking_operation(): print(\\"Press any key to stop...\\") while True: if sys.stdin.read(1): break print(\\"Key pressed! Operation complete.\\") # Example usage toggle_canonical_mode(sys.stdin.fileno(), example_non_blocking_operation) ``` In this example, `toggle_canonical_mode` should disable canonical mode, allowing `sys.stdin.read(1)` to return immediately after a single character is pressed instead of waiting for a newline. Implementation Details 1. Use `termios.tcgetattr(fd)` to get the current terminal attributes. 2. Modify the attributes to disable canonical mode. 3. Use `termios.tcsetattr(fd, termios.TCSANOW, new_attributes)` to apply the new settings. 4. Call the `non_blocking_operation` function. 5. Restore the original terminal attributes using `termios.tcsetattr(fd, termios.TCSANOW, original_attributes)`. Ensure the original terminal attributes are restored no matter what, even if an error occurs during the non-blocking operation.","solution":"import termios import sys from typing import Callable, Any def toggle_canonical_mode(fd: int, non_blocking_operation: Callable[[], Any]) -> Any: Temporarily disables canonical mode on the terminal referenced by the provided file descriptor, performs a user-defined non-blocking operation, and then restores the original terminal settings. Args: fd (int): The file descriptor representing the terminal (e.g., sys.stdin.fileno()). non_blocking_operation (Callable[[], Any]): The non-blocking operation to be performed while canonical mode is disabled. Returns: Any: The result of the non_blocking_operation. # Get the current terminal settings original_attributes = termios.tcgetattr(fd) try: # Make a copy of the attributes to modify them new_attributes = termios.tcgetattr(fd) # Disable canonical mode (ICANON) new_attributes[3] &= ~termios.ICANON # Apply the new attributes immediately termios.tcsetattr(fd, termios.TCSANOW, new_attributes) # Perform the non-blocking operation result = non_blocking_operation() finally: # Restore the original terminal settings termios.tcsetattr(fd, termios.TCSANOW, original_attributes) return result"},{"question":"# Custom Autograd Function and Saving Tensors with Hooks Objective In this assessment, you are required to implement a custom autograd function in PyTorch that saves intermediary results during the forward pass and uses these saved tensors in the backward pass. Additionally, you will need to utilize hooks to customize the saving and loading behavior of these tensors. Problem Statement Implement a custom autograd function using PyTorch\'s `autograd.Function` class. Your function will compute the operation `f(x) = 5 * x^2` in the forward pass and leverage saved tensors to efficiently compute the gradient in the backward pass. Moreover, you are to integrate hooks to alter how tensors are saved and loaded during the computation. Specifically, tensors with elements greater than or equal to 4 should be saved to disk, and others should be saved in memory. Requirements: 1. **Custom Function Implementation**: - Create a class `CustomFunc` by inheriting from `torch.autograd.Function`. - Define the `forward` and `backward` static methods. - Use the `save_for_backward` to save intermediary results during the forward pass. 2. **Hooks**: - Implement `pack_hook` and `unpack_hook` methods for controlling how tensors are saved and loaded. - Register these hooks to ensure tensors are saved to disk if they contain elements with values greater than or equal to 4. 3. **Test Case Execution**: - Use the implemented custom autograd function in a sample computation. - Perform a forward and backward pass to verify that the hooks and saved tensor logic work as expected. Expected Input and Output Formats - Input: A PyTorch tensor `x` of size 5 with `requires_grad=True`. - Output: Gradient of the operation `f(x) = 5 * x^2` with respect to `x`. Constraints - You must implement and register the hooks as described. - Ensure the hooks properly handle saving tensors to disk and loading them back during the backward pass. # Sample Code Template: ```python import torch import os import uuid # Create a temporary directory to save tensors tmp_dir = \'/tmp\' class CustomFunc(torch.autograd.Function): @staticmethod def forward(ctx, x): result = 5 * x.pow(2) ctx.save_for_backward(x) return result @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_input = grad_output * 10 * x return grad_input # Hooks implementation class SelfDeletingTempFile(): def __init__(self): self.name = os.path.join(tmp_dir, str(uuid.uuid4())) def __del__(self): os.remove(self.name) def pack_hook(tensor): temp_file = SelfDeletingTempFile() torch.save(tensor, temp_file.name) return temp_file def unpack_hook(temp_file): return torch.load(temp_file.name) # Sample execution def main(): x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True) # Register hooks here # TO DO: Register the hooks for saved tensors # Forward pass y = CustomFunc.apply(x) # Backward pass y.sum().backward() # Print the gradient print(x.grad) if __name__ == \\"__main__\\": main() ``` # Task Breakdown: 1. Implement `CustomFunc` with the specified forward and backward methods. 2. Define `pack_hook` and `unpack_hook` to handle saving tensors to disk if they meet the condition. 3. Integrate these hooks with the `CustomFunc` autograd function. 4. Run the provided sample execution code to test your implementation.","solution":"import torch import os import uuid # Create a temporary directory to save tensors tmp_dir = \'/tmp\' class CustomFunc(torch.autograd.Function): @staticmethod def forward(ctx, x): result = 5 * x.pow(2) ctx.save_for_backward(x) return result @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_input = grad_output * 10 * x return grad_input # Hooks implementation class SelfDeletingTempFile(): def __init__(self): self.name = os.path.join(tmp_dir, str(uuid.uuid4())) def __del__(self): if os.path.exists(self.name): os.remove(self.name) def pack_hook(tensor): if tensor.ge(4).any(): temp_file = SelfDeletingTempFile() torch.save(tensor, temp_file.name) return temp_file return tensor def unpack_hook(obj): if isinstance(obj, SelfDeletingTempFile): return torch.load(obj.name) return obj # Register hooks here CustomFunc._ctx_packer = pack_hook CustomFunc._ctx_unpacker = unpack_hook # Sample execution def main(): x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True) # Forward pass y = CustomFunc.apply(x) # Backward pass y.sum().backward() # Print the gradient print(x.grad) if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** You are required to demonstrate your understanding of the `sunau` module by writing a function that reads an AU file, processes its audio data by reversing the frames, and writes the processed data back to a new AU file. **Task:** Write a function `reverse_audio(input_file: str, output_file: str) -> None:` that does the following: 1. Opens an AU file specified by `input_file` in read mode. 2. Reads all the audio frames. 3. Reverses the order of the audio frames. 4. Writes the reversed audio frames to a new AU file specified by `output_file` with the same parameters (number of channels, sample width, frame rate, etc.). # Specifications: - **Function Signature:** ```python def reverse_audio(input_file: str, output_file: str) -> None: ``` - **Input:** - `input_file` (str): Path to the input AU file. - `output_file` (str): Path to the output AU file where the reversed audio should be saved. - **Output:** - The function should not return anything. It should write the reversed audio data to `output_file`. - **Constraints:** - Assume the `input_file` is a valid AU file. - Handle all necessary I/O operations within the function. # Example: ```python reverse_audio(\'original.au\', \'reversed.au\') # The function should read the \'original.au\' file, reverse its audio frames, and save it as \'reversed.au\'. ``` # Notes: 1. Use the `sunau` module functions and methods to handle the AU file reading and writing. 2. Make sure to maintain the audio parameters (such as number of channels, sample width, frame rate) when writing the reversed audio to the output file. 3. Ensure that your implementation is efficient and handles potentially large audio files correctly.","solution":"import sunau def reverse_audio(input_file: str, output_file: str) -> None: Reads an AU file, reverses the order of its audio frames, and writes the reversed frames to a new AU file with the same parameters. with sunau.open(input_file, \'rb\') as reader: # Read parameters of the input file nchannels = reader.getnchannels() sampwidth = reader.getsampwidth() framerate = reader.getframerate() nframes = reader.getnframes() comptype = reader.getcomptype() compname = reader.getcompname() # Read all frames from the input file frames = reader.readframes(nframes) # Reverse the frames reverse_frames = frames[::-1] # Write the reversed frames to the output file with sunau.open(output_file, \'wb\') as writer: writer.setnchannels(nchannels) writer.setsampwidth(sampwidth) writer.setframerate(framerate) writer.setcomptype(comptype, compname) writer.writeframes(reverse_frames)"},{"question":"# PyTorch Special Functions Coding Assessment Objective Implement a function that computes a combination of special mathematical functions from the `torch.special` module on given tensor inputs. Description You are tasked with implementing a function in PyTorch that computes specific mathematical operations on a tensor, leveraging multiple special functions from the `torch.special` module. This function will take a tensor input and compute the following operations in sequence: 1. Compute the exponential of the input tensor. 2. Compute the natural logarithm of `1 + exp(input_tensor)`. 3. Compute the sigmoid function on the result from step 2. 4. Compute the error function (erf) of the result from step 3. 5. Compute the Bessel function of the first kind (order 0) on the tensor obtained from step 4. Function Signature ```python import torch def special_operations(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` Input - `input_tensor (torch.Tensor)`: A tensor of arbitrary shape containing floating-point numbers. Output - Returns a tensor of the same shape as `input_tensor`, with the result of applying the sequence of operations described above. Constraints - You must use the functions from `torch.special` for this implementation. - Your implementation should handle tensors on both CPU and GPU efficiently. - Ensure the operations are performed in a numerically stable manner. Example ```python import torch input_tensor = torch.tensor([0.5, 1.0, 1.5, 2.0]) output_tensor = special_operations(input_tensor) print(output_tensor) ``` The expected output tensor should be the result of the described sequence of operations applied to each element of `input_tensor`. Implementation Notes - Review the `torch.special` module documentation to understand how each function works. - Ensure that your code is optimized for performance. - Test your function with diverse tensor shapes and values to validate its correctness.","solution":"import torch def special_operations(input_tensor: torch.Tensor) -> torch.Tensor: Performs a series of special mathematical operations on the input tensor. 1. Compute the exponential of the input tensor. 2. Compute the natural logarithm of 1 + exp(input_tensor). 3. Compute the sigmoid function on the result from step 2. 4. Compute the error function (erf) of the result from step 3. 5. Compute the Bessel function of the first kind (order 0) on the tensor obtained from step 4. Args: input_tensor (torch.Tensor): A tensor of arbitrary shape. Returns: torch.Tensor: Resulting tensor after applying the sequence of operations. exp_result = torch.exp(input_tensor) log1p_exp_result = torch.log1p(exp_result) sigmoid_result = torch.special.expit(log1p_exp_result) erf_result = torch.special.erf(sigmoid_result) bessel_result = torch.special.i0(erf_result) return bessel_result"},{"question":"**Question: Creating a Tkinter Message Box Application** You are tasked with building a simple Tkinter application that provides users with a menu of options and responds to their selections using different types of message boxes from the `tkinter.messagebox` module. # Requirements: 1. **Main Window**: - Create a Tkinter main window with a title \\"Message Box Demo\\". - Add a button labeled \\"Show Info Box\\" that when clicked, displays an information message box with the title \\"Information\\" and the message \\"This is an info box!\\" 2. **Warning Button**: - Add a button labeled \\"Show Warning Box\\" that when clicked, displays a warning message box with the title \\"Warning\\" and the message \\"This is a warning box!\\" 3. **Error Button**: - Add a button labeled \\"Show Error Box\\" that when clicked, displays an error message box with the title \\"Error\\" and the message \\"This is an error box!\\" 4. **Question Box**: - Add a button labeled \\"Ask Question\\" that when clicked, displays a question message box with the title \\"Question\\" and the message \\"Do you want to continue?\\". - Print the user\'s response (\\"yes\\" or \\"no\\") to the console. 5. **Yes/No/Cancel Box**: - Add a button labeled \\"Ask Yes/No/Cancel\\" that when clicked, displays a yes/no/cancel message box with the title \\"Decision\\" and the message \\"Select Yes, No, or Cancel\\". - Print the user\'s response (\\"yes\\", \\"no\\", or \\"cancel\\") to the console. # Constraints: - Ensure the application is responsive and handles all events correctly. - Ensure the message boxes are created using the correct functions from the `tkinter.messagebox` module. - The user responses should be accurately captured and displayed in the console. # Example Layout: - Main window title: \\"Message Box Demo\\" - Buttons labeled: \\"Show Info Box\\", \\"Show Warning Box\\", \\"Show Error Box\\", \\"Ask Question\\", \\"Ask Yes/No/Cancel\\" # Input: There is no direct input from a file or user other than the GUI interactions. # Output: - Information, warning, and error boxes as described. - Console log of user responses for the question and yes/no/cancel boxes. Implement the solution in the function `create_message_box_application()`. ```python import tkinter as tk from tkinter import messagebox def create_message_box_application(): # Create main window root = tk.Tk() root.title(\\"Message Box Demo\\") # Define button callbacks def show_info(): messagebox.showinfo(title=\\"Information\\", message=\\"This is an info box!\\") def show_warning(): messagebox.showwarning(title=\\"Warning\\", message=\\"This is a warning box!\\") def show_error(): messagebox.showerror(title=\\"Error\\", message=\\"This is an error box!\\") def ask_question(): response = messagebox.askquestion(title=\\"Question\\", message=\\"Do you want to continue?\\") print(response) def ask_yesnocancel(): response = messagebox.askyesnocancel(title=\\"Decision\\", message=\\"Select Yes, No, or Cancel\\") print(response) # Create buttons tk.Button(root, text=\\"Show Info Box\\", command=show_info).pack() tk.Button(root, text=\\"Show Warning Box\\", command=show_warning).pack() tk.Button(root, text=\\"Show Error Box\\", command=show_error).pack() tk.Button(root, text=\\"Ask Question\\", command=ask_question).pack() tk.Button(root, text=\\"Ask Yes/No/Cancel\\", command=ask_yesnocancel).pack() # Start the main loop root.mainloop() # Uncomment the following line to execute the application. # create_message_box_application() ```","solution":"import tkinter as tk from tkinter import messagebox def create_message_box_application(): # Create main window root = tk.Tk() root.title(\\"Message Box Demo\\") # Define button callbacks def show_info(): messagebox.showinfo(title=\\"Information\\", message=\\"This is an info box!\\") def show_warning(): messagebox.showwarning(title=\\"Warning\\", message=\\"This is a warning box!\\") def show_error(): messagebox.showerror(title=\\"Error\\", message=\\"This is an error box!\\") def ask_question(): response = messagebox.askquestion(title=\\"Question\\", message=\\"Do you want to continue?\\") print(response) def ask_yesnocancel(): response = messagebox.askyesnocancel(title=\\"Decision\\", message=\\"Select Yes, No, or Cancel\\") print(response) # Create buttons tk.Button(root, text=\\"Show Info Box\\", command=show_info).pack() tk.Button(root, text=\\"Show Warning Box\\", command=show_warning).pack() tk.Button(root, text=\\"Show Error Box\\", command=show_error).pack() tk.Button(root, text=\\"Ask Question\\", command=ask_question).pack() tk.Button(root, text=\\"Ask Yes/No/Cancel\\", command=ask_yesnocancel).pack() # Start the main loop root.mainloop()"},{"question":"You are tasked to create a Python function that determines whether two paths refer to the same file, regardless of the symbolic links involved, and to normalize the paths before comparison. Additionally, the function must return both files\' absolute file sizes and modification times if they are the same file. Function Signature ```python import os import os.path def compare_paths(path1: str, path2: str) -> tuple: Compare two paths and return if they refer to the same file after normalizing. Parameters: - path1 (str): The first path to a file or directory. - path2 (str): The second path to a file or directory. Returns: - tuple: A tuple containing: - bool: True if both path1 and path2 refer to the same file, False otherwise. - int: Size of the file in bytes if both paths refer to the same file, else -1. - float: Last modification time of the file if both paths refer to the same file, else -1. # Implementation here ``` Requirements: 1. Normalize the paths using `os.path.abspath()` and `os.path.normpath()`. 2. Determine if both paths refer to the same file by resolving symbolic links with `os.path.realpath()` and using `os.path.samefile()`. 3. If they refer to the same file, use `os.path.getsize()` to get the file size and `os.path.getmtime()` to get the last modification time. 4. If they do not refer to the same file, return `(False, -1, -1)`. Example: ```python # Assuming the paths refer to the same file print(compare_paths(\'/foo/bar/../baz.txt\', \'/foo/baz.txt\')) # Expected Output: (True, 12345, 1609459200.0) # where 12345 is the size in bytes and 1609459200.0 is the last modification time # Assuming the paths refer to different files print(compare_paths(\'/foo/bar.txt\', \'/foo/baz.txt\')) # Expected Output: (False, -1, -1) ``` Constraints: - The paths provided will always be valid strings. - The paths might include symbolic links and redundant directory separators. - The function should handle both relative and absolute paths. - You can assume that paths will be given in the format consistent with the operating system the script is running on. **Note**: Your function must handle exceptions that may arise when accessing file properties (e.g., file does not exist or permission errors) and return `(False, -1, -1)` in such cases.","solution":"import os def compare_paths(path1: str, path2: str) -> tuple: Compare two paths and return if they refer to the same file after normalizing. Parameters: - path1 (str): The first path to a file or directory. - path2 (str): The second path to a file or directory. Returns: - tuple: A tuple containing: - bool: True if both path1 and path2 refer to the same file, False otherwise. - int: Size of the file in bytes if both paths refer to the same file, else -1. - float: Last modification time of the file if both paths refer to the same file, else -1. try: # Normalize the paths abs_path1 = os.path.normpath(os.path.abspath(path1)) abs_path2 = os.path.normpath(os.path.abspath(path2)) # Check if both paths refer to the same file if os.path.samefile(abs_path1, abs_path2): file_size = os.path.getsize(abs_path1) modification_time = os.path.getmtime(abs_path1) return (True, file_size, modification_time) else: return (False, -1, -1) except Exception: return (False, -1, -1)"},{"question":"Graph Manipulation and Transformation with PyTorch FX Objective: To assess the student\'s understanding of graph manipulation using the PyTorch FX module and their ability to create custom transformations on a neural network module. Question: You are given a simple neural network module that performs basic operations in its `forward` method. Your task is to implement a transformation function that uses **`torch.fx`** to trace the given module, identify all `torch.add` operations in the computational graph, and replace them with `torch.sub` operations. Instructions: 1. **Module Definition**: Implement a simple neural network module called `SimpleNet` with the following `forward` method: ```python import torch import torch.nn as nn class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear1 = nn.Linear(4, 4) self.linear2 = nn.Linear(4, 4) def forward(self, x): x = self.linear1(x) x = torch.add(x, x) x = self.linear2(x) return x ``` 2. **Transformation Function**: Implement the `replace_add_with_sub` function to do the following: - Trace the `SimpleNet` module to obtain its computational graph using **`torch.fx`**. - Iterate over the nodes in the graph and replace all `torch.add` operations with `torch.sub` operations. - Return a new `GraphModule` reflecting these changes. ```python import torch.fx as fx def replace_add_with_sub(module: torch.nn.Module) -> torch.nn.Module: # Implement this function pass ``` 3. **Testing the Transformation**: Write the necessary code to: - Instantiate the `SimpleNet` module. - Use the `replace_add_with_sub` function to transform the module. - Test the transformed module to ensure that it performs the intended operations (i.e., `torch.add` has been replaced with `torch.sub`). Constraints: - You must not modify the definition of the `SimpleNet` class. - The transformation function should retain the original structure and parameters of the module, only altering the specified operations. Example: The following code demonstrates how the `replace_add_with_sub` function will be used: ```python # Define the module model = SimpleNet() # Apply the transformation transformed_model = replace_add_with_sub(model) # Test input input_tensor = torch.randn(2, 4) # Ensure correct functionality output_original = model(input_tensor) output_transformed = transformed_model(input_tensor) # Print the transformed model\'s graph print(transformed_model.graph) # Expected output: # The graph printout should show `call_function` nodes with `torch.sub` instead of `torch.add` ``` Output: Submit the `SimpleNet` class implementation, the `replace_add_with_sub` function, and the testing code ensuring the transformation works as expected.","solution":"import torch import torch.nn as nn import torch.fx as fx class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.linear1 = nn.Linear(4, 4) self.linear2 = nn.Linear(4, 4) def forward(self, x): x = self.linear1(x) x = torch.add(x, x) x = self.linear2(x) return x def replace_add_with_sub(module: nn.Module) -> fx.GraphModule: tracer = fx.symbolic_trace(module) graph = tracer.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: with graph.inserting_after(node): with node.graph.inserting_before(node): new_node = graph.call_function( torch.sub, (node.args[0], node.args[1]) ) node.replace_all_uses_with(new_node) graph.erase_node(node) return fx.GraphModule(tracer, graph)"},{"question":"**Problem Statement: Configuring and Querying PyTorch Backends** As a PyTorch developer, it is essential to know how to configure and interact with the various backends that PyTorch supports for optimized performance on different hardware. In this task, you are required to create a function that configures several backend settings and verifies these settings. # Function Signature ```python def configure_and_check_backends(): pass ``` # Requirements 1. **Configure CUDA Backend**: - Enable TensorFloat-32 (TF32) tensor cores in matrix multiplications: `torch.backends.cuda.matmul.allow_tf32 = True`. - Enable reduced precision reductions using FP16: `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True`. - Enable reduced precision reductions using BF16: `torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True`. 2. **Configure cuDNN Backend**: - Enable cuDNN library: `torch.backends.cudnn.enabled = True`. - Allow TensorFloat-32 (TF32) tensor cores in cuDNN convolutions: `torch.backends.cudnn.allow_tf32 = True`. - Set cuDNN to use deterministic algorithms: `torch.backends.cudnn.deterministic = True`. - Enable cuDNN benchmarking to select the fastest convolution algorithm: `torch.backends.cudnn.benchmark = True`. 3. **Configure cuFFT Plan Cache**: - Set the maximum size of the cuFFT plan cache to 1024: `torch.backends.cuda.cufft_plan_cache.max_size = 1024`. 4. **Verification**: - Verify and return a dictionary containing the current status of the above configurations. # Output - The function should return a dictionary with the following keys and corresponding values: ```python { \\"cuda_tf32_enabled\\": <bool>, \\"cuda_fp16_reduction_enabled\\": <bool>, \\"cuda_bf16_reduction_enabled\\": <bool>, \\"cudnn_enabled\\": <bool>, \\"cudnn_tf32_enabled\\": <bool>, \\"cudnn_deterministic\\": <bool>, \\"cudnn_benchmark\\": <bool>, \\"cufft_plan_cache_max_size\\": <int> } ``` # Example ```python output = configure_and_check_backends() print(output) # Example expected output: # { # \\"cuda_tf32_enabled\\": True, # \\"cuda_fp16_reduction_enabled\\": True, # \\"cuda_bf16_reduction_enabled\\": True, # \\"cudnn_enabled\\": True, # \\"cudnn_tf32_enabled\\": True, # \\"cudnn_deterministic\\": True, # \\"cudnn_benchmark\\": True, # \\"cufft_plan_cache_max_size\\": 1024 # } ``` # Constraints - Ensure that your solution only uses the functions and attributes provided by PyTorch\'s `torch.backends` module as noted in the documentation. - You must handle any exceptions or errors gracefully. Write your solution in the `configure_and_check_backends` function provided.","solution":"import torch def configure_and_check_backends(): # Configure CUDA Backend torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True # Configure cuDNN Backend torch.backends.cudnn.enabled = True torch.backends.cudnn.allow_tf32 = True torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = True # Configure cuFFT Plan Cache torch.backends.cuda.cufft_plan_cache.max_size = 1024 # Verify and return the status of the configurations return { \\"cuda_tf32_enabled\\": torch.backends.cuda.matmul.allow_tf32, \\"cuda_fp16_reduction_enabled\\": torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction, \\"cuda_bf16_reduction_enabled\\": torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction, \\"cudnn_enabled\\": torch.backends.cudnn.enabled, \\"cudnn_tf32_enabled\\": torch.backends.cudnn.allow_tf32, \\"cudnn_deterministic\\": torch.backends.cudnn.deterministic, \\"cudnn_benchmark\\": torch.backends.cudnn.benchmark, \\"cufft_plan_cache_max_size\\": torch.backends.cuda.cufft_plan_cache.max_size }"},{"question":"# Question: Creating Customized Color Palettes and Applying Them in Seaborn Visualizations **Objective:** Write a Python function using seaborn to create visualizations with customized color palettes. This will test your understanding of seaborn\'s `dark_palette` function and its integration into more complex visualizations. **Function Signature:** ```python def customized_color_visualization(data: pd.DataFrame, column: str) -> None: pass ``` **Input:** - `data` (pandas DataFrame): A DataFrame containing numerical data. - `column` (str): The name of the column in `data` whose values will be visualized using a bar plot. **Output:** - The function should create a bar plot of the specified column in the DataFrame, applying a customized dark color palette. The plot should then be displayed. **Constraints:** - You should create at least three different color palettes using seaborn\'s `dark_palette` function. - Use those palettes to generate three different bar plots for the same DataFrame column. - Incorporate at least one example of specifying the color using a color name, a hex code, and the HUSL system. **Details:** - Import necessary libraries including seaborn, pandas, and matplotlib. - Ensure that each plot has appropriate titles and labels. - The plots should be displayed in a single figure using subplots. **Example:** ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def customized_color_visualization(data: pd.DataFrame, column: str) -> None: sns.set_theme() # Define the three palettes palette1 = sns.dark_palette(\\"seagreen\\", as_cmap=False) palette2 = sns.dark_palette(\\"#79C\\", as_cmap=False) palette3 = sns.dark_palette((20, 60, 50), input=\'husl\', as_cmap=False) palettes = [palette1, palette2, palette3] titles = [\\"Seagreen Palette\\", \\"Hex Code Palette\\", \\"HUSL Palette\\"] fig, axes = plt.subplots(1, 3, figsize=(15, 5)) for i in range(3): sns.barplot(x=data.index, y=data[column], palette=palettes[i], ax=axes[i]) axes[i].set_title(titles[i]) axes[i].set_xlabel(\\"Index\\") axes[i].set_ylabel(column.capitalize()) plt.tight_layout() plt.show() # Sample DataFrame for demonstration data = pd.DataFrame({ \'values\': [10, 20, 30, 40, 50] }) customized_color_visualization(data, \'values\') ``` In your solution, ensure you follow this structure and use different color specification methods as stated.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def customized_color_visualization(data: pd.DataFrame, column: str) -> None: sns.set_theme() # Define the three palettes palette1 = sns.dark_palette(\\"seagreen\\", as_cmap=False) # Color name palette2 = sns.dark_palette(\\"#79C\\", as_cmap=False) # Hex code palette3 = sns.dark_palette((20, 60, 50), input=\'husl\', as_cmap=False) # HUSL system palettes = [palette1, palette2, palette3] titles = [\\"Seagreen Palette\\", \\"Hex Code Palette\\", \\"HUSL Palette\\"] fig, axes = plt.subplots(1, 3, figsize=(18, 6)) for i in range(3): sns.barplot(x=data.index, y=data[column], palette=palettes[i], ax=axes[i]) axes[i].set_title(titles[i]) axes[i].set_xlabel(\\"Index\\") axes[i].set_ylabel(column.capitalize()) plt.tight_layout() plt.show()"},{"question":"Objective Your task is to implement a custom data structure that effectively uses the reference counting macros and functions as described in the documentation. Problem Statement Implement a class `CustomObjectManager` that manages a collection of Python objects while maintaining proper reference counts. This class should provide methods to add, access, and release references to the objects. Requirements 1. **Initialization**: - The constructor should initialize an empty list to store the Python objects. 2. **Method: add_object**: - Input: A Python object (of any type). - Action: Add the object to the list and increase its reference count using `Py_NewRef`. - Output: None. 3. **Method: get_object**: - Input: An integer index. - Action: Retrieve the object at the given index in the list. - Output: The Python object at the given index, with its reference count increased using `Py_NewRef`. 4. **Method: release_object**: - Input: An integer index. - Action: Decrease the reference count of the object at the given index using `Py_DECREF` and remove it from the list. - Output: None. 5. **Method: clear_all**: - Input: None. - Action: Decrease the reference count of all stored objects using `Py_CLEAR` and clear the list. - Output: None. Constraints - You can assume that the index provided to `get_object` and `release_object` will always be valid. - The Python objects managed by this class can be of any type (int, str, list, dict, etc.). Example Usage ```python manager = CustomObjectManager() obj1 = \\"Hello, World!\\" obj2 = [1, 2, 3] manager.add_object(obj1) manager.add_object(obj2) retrieved_obj1 = manager.get_object(0) # Increases ref count of obj1 retrieved_obj2 = manager.get_object(1) # Increases ref count of obj2 manager.release_object(0) # Decreases ref count of obj1 and removes it from the list manager.clear_all() # Decreases ref count of all remaining objects and clears the list ``` Implement the class `CustomObjectManager` in Python. You will need to use the `ctypes` library to access the provided CPython macros and functions.","solution":"import ctypes # Load the Python C API pythonapi = ctypes.pythonapi pythonapi.Py_IncRef.argtypes = [ctypes.py_object] pythonapi.Py_DecRef.argtypes = [ctypes.py_object] class CustomObjectManager: def __init__(self): self.objects = [] def add_object(self, obj): # Increase the reference count of the object pythonapi.Py_IncRef(obj) self.objects.append(obj) def get_object(self, index): obj = self.objects[index] # Increase the reference count of the object when accessed pythonapi.Py_IncRef(obj) return obj def release_object(self, index): obj = self.objects[index] # Decrease the reference count of the object pythonapi.Py_DecRef(obj) # Remove the object from the list self.objects.pop(index) def clear_all(self): # Decrease the reference count of all stored objects for obj in self.objects: pythonapi.Py_DecRef(obj) # Clear the list self.objects = []"},{"question":"**Problem Statement:** You are tasked with creating a Python module that mimics certain behaviors of the slice object manipulation described in the provided slice documentation. Specifically, you will create a class `CustomSlice` that represents a slice and includes methods for validation and adjustment of indices. # Objectives: 1. Implement a `CustomSlice` class that: - Initializes with `start`, `stop`, and `step` attributes. - Validates the slice using a `validate` method. - Adjusts the slice indices to a given length of a sequence using an `adjust_indices` method. # Implementation Details: 1. **Class `CustomSlice`**: - **__init__(self, start, stop, step)**: Initializes the slice with given parameters. If any parameter is `None`, it should be treated as infinity or a default large number. - **validate(self)**: Checks if the current slice object is valid. Throws a `ValueError` if the slice is invalid. For instance, the step should not be zero. - **adjust_indices(self, length)**: Adjusts start, stop, and step values to be within a sequence of given length. Indices should be clipped to the bounds of the sequence length. # Constraints: - Values of start, stop, and step can be any integer or `None`. - Ensure that the `validate` method correctly identifies invalid slices, particularly zero steps. - Handle edge cases where indices are out of bounds by clipping them similar to Python\'s native slice behavior. # Example: ```python s = CustomSlice(1, 10, 2) s.validate() # Should not raise any error. start, stop, step = s.adjust_indices(7) print(start, stop, step) # Expected output: (1, 7, 2), because stop (10) is clipped to 7 (length). ``` # Your Task: Write the `CustomSlice` class with the `__init__`, `validate`, and `adjust_indices` methods as described above. Ensure your implementation accommodates the constraints and handles boundary cases gracefully. ```python class CustomSlice: def __init__(self, start: int, stop: int, step: int): # TODO: Implement the initialization def validate(self): # TODO: Implement the validation logic. def adjust_indices(self, length: int): # TODO: Implement the logic to adjust indices based on the sequence length. ``` # Notes: - You may assume valid inputs for `__init__`, but `validate` should account for issues such as zero step. - The `adjust_indices` method should closely mimic Pythons native indexing behavior.","solution":"class CustomSlice: def __init__(self, start, stop, step): self.start = start self.stop = stop self.step = step if step is not None else 1 # Default step to 1 if None def validate(self): if self.step == 0: raise ValueError(\\"slice step cannot be zero\\") def adjust_indices(self, length): start = self.start if self.start is not None else 0 stop = self.stop if self.stop is not None else length step = self.step start = max(0, min(start if start >= 0 else length + start, length)) stop = max(0, min(stop if stop >= 0 else length + stop, length)) return start, stop, step"},{"question":"You are tasked with creating a simple email client that interacts with a POP3 mail server. Your client should connect to the server, authenticate a user, list available emails, and retrieve email headers. # Requirements 1. Create a class `EmailClient` which: - Initializes with parameters `host`, `port`, `username`, `password`, and `use_ssl`. - Supports both secure (POP3-over-SSL) and non-secure connections based on the `use_ssl` flag. 2. Implement the following methods within `EmailClient`: - `connect`: Establishes the connection to the POP3 server. - `authenticate`: Authenticates the user. - `list_emails`: Lists all emails in the mailbox and returns a list of email IDs and sizes. - `retrieve_header`: Retrieves the header of a specified email by ID. 3. Ensure proper error handling using the `poplib.error_proto` exception for protocol-related errors and general exception handling for others. # Constraints - Assume the server responses conform to the POP3 specifications. - You cannot use any external libraries other than `poplib`, `ssl`, and `getpass` for obtaining user input. # Input and Output ```python # Example usage client = EmailClient(host=\\"pop.example.com\\", port=995, username=\\"user\\", password=\\"password\\", use_ssl=True) client.connect() client.authenticate() emails = client.list_emails() print(emails) # Output should be similar to: # [(1, 1024), (2, 2048), ...] header = client.retrieve_header(email_id=1) print(header) # Output should be the header lines of the email ``` # Class and Method Definitions ```python import poplib import ssl import getpass class EmailClient: def __init__(self, host, port, username, password, use_ssl): self.host = host self.port = port self.username = username self.password = password self.use_ssl = use_ssl self.server = None def connect(self): try: if self.use_ssl: self.server = poplib.POP3_SSL(self.host, self.port) else: self.server = poplib.POP3(self.host, self.port) print(self.server.getwelcome()) except poplib.error_proto as e: print(f\\"Protocol error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") def authenticate(self): if self.server: try: self.server.user(self.username) self.server.pass_(self.password) except poplib.error_proto as e: print(f\\"Authentication error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") def list_emails(self): if self.server: try: response, messages, octets = self.server.list() email_list = [(int(msg.split()[0]), int(msg.split()[1])) for msg in messages] return email_list except poplib.error_proto as e: print(f\\"Listing error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") return [] def retrieve_header(self, email_id): if self.server: try: response, lines, octets = self.server.top(email_id, 0) return lines except poplib.error_proto as e: print(f\\"Retrieval error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") return [] ``` Ensure to test your implementation against a working POP3 server.","solution":"import poplib import ssl import getpass class EmailClient: def __init__(self, host, port, username, password, use_ssl): self.host = host self.port = port self.username = username self.password = password self.use_ssl = use_ssl self.server = None def connect(self): try: if self.use_ssl: self.server = poplib.POP3_SSL(self.host, self.port) else: self.server = poplib.POP3(self.host, self.port) print(self.server.getwelcome()) except poplib.error_proto as e: print(f\\"Protocol error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") def authenticate(self): if self.server: try: self.server.user(self.username) self.server.pass_(self.password) except poplib.error_proto as e: print(f\\"Authentication error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") def list_emails(self): if self.server: try: response, messages, octets = self.server.list() email_list = [(int(msg.split()[0]), int(msg.split()[1])) for msg in messages] return email_list except poplib.error_proto as e: print(f\\"Listing error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") return [] def retrieve_header(self, email_id): if self.server: try: response, lines, octets = self.server.top(email_id, 0) return lines except poplib.error_proto as e: print(f\\"Retrieval error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") return [] def quit(self): if self.server: self.server.quit()"},{"question":"Objective: Write a Python program that uses the `pwd` module to perform the following tasks on a Unix system: 1. List all users on the system. 2. Find and display detailed information about a specific user, given their username. 3. Determine the home directory of a user given their user ID. Your solution should demonstrate the ability to utilize the `pwd` module functions and handle potential errors effectively. Detailed Requirements: 1. **Listing all users**: - Implement a function `list_all_users()` that retrieves and prints the login names of all users on the system. 2. **Finding user details by username**: - Implement a function `get_user_details(username)` that takes a username as input and retrieves all available information about the user (in the form of a password database tuple). The function should print each detail in a user-friendly format. If the user does not exist, it should raise a `KeyError`. 3. **Finding home directory by user ID**: - Implement a function `get_home_directory(uid)` that takes a user ID as input and returns the home directory of that user. If the user ID does not exist, it should raise a `KeyError`. Input and Output Specifications: - `list_all_users()` should print the login names of all users line by line. - `get_user_details(username)` should print the user details in the following format: ``` Login name: <pw_name> Encrypted password: <pw_passwd> User ID: <pw_uid> Group ID: <pw_gid> User comment: <pw_gecos> Home directory: <pw_dir> Command interpreter: <pw_shell> ``` - `get_home_directory(uid)` should return the home directory path as a string. Constraints: - Your code should handle the absence of a user or user ID gracefully by raising appropriate exceptions. - Ensure your code is efficient and makes use of the `pwd` module\'s capabilities. Here is the function signature template to help you get started: ```python import pwd def list_all_users(): pass def get_user_details(username): pass def get_home_directory(uid): pass # Example usage: # list_all_users() # print(get_user_details(\'root\')) # print(get_home_directory(0)) ``` Apply best practices for Python coding and ensure your code is well-commented.","solution":"import pwd def list_all_users(): Retrieves and prints the login names of all users on the system. users = pwd.getpwall() for user in users: print(user.pw_name) def get_user_details(username): Takes a username as input and retrieves all available information about the user. Prints the details in a user-friendly format. Raises: KeyError: If the user does not exist. try: user = pwd.getpwnam(username) print(f\\"Login name: {user.pw_name}\\") print(f\\"Encrypted password: {user.pw_passwd}\\") print(f\\"User ID: {user.pw_uid}\\") print(f\\"Group ID: {user.pw_gid}\\") print(f\\"User comment: {user.pw_gecos}\\") print(f\\"Home directory: {user.pw_dir}\\") print(f\\"Command interpreter: {user.pw_shell}\\") except KeyError: raise KeyError(f\\"User {username} not found\\") def get_home_directory(uid): Takes a user ID as input and returns the home directory of that user. Raises: KeyError: If the user ID does not exist. Returns: str: Home directory path. try: user = pwd.getpwuid(uid) return user.pw_dir except KeyError: raise KeyError(f\\"User ID {uid} not found\\")"},{"question":"# Question: Custom DataLoader for Efficient Multi-process Loading In this exercise, you are required to create a custom DataLoader for an image classification task. The DataLoader should be optimized for multi-process loading and utilize memory pinning for efficient data transfer to the GPU. You will need to handle various customizations such as creating a map-style dataset, defining a custom sampler for shuffling, and implementing a custom `collate_fn` for batching the data. Requirements: 1. **CustomDataset**: - Define a custom map-style dataset class, `CustomDataset`, that inherits from `torch.utils.data.Dataset`. - The dataset should load images from a specified directory and their corresponding labels from a CSV file. - Implement the `__getitem__` and `__len__` methods. 2. **CustomSampler**: - Create a custom sampler, `CustomSampler`, that shuffles the dataset indices for each epoch. - The sampler should inherit from `torch.utils.data.Sampler`. 3. **CustomCollateFn**: - Implement a custom `collate_fn` function, `custom_collate_fn`, which batches the images and labels together. - Ensure that images of varying sizes are padded to the same size within a batch. 4. **Custom DataLoader**: - Utilize `torch.utils.data.DataLoader` to create a DataLoader with the following configurations: - Batch size: 32 - Shuffle: Use the custom sampler - Multi-process loading: Use 4 worker processes - Memory pinning: Enable memory pinning 5. **GPU Transfer**: - Ensure that the DataLoader efficiently transfers data to the GPU using pinned memory. Input and Output Specifications: - **Input**: - `image_dir`: Path to the directory containing images. - `label_file`: Path to the CSV file containing image labels. - The CSV file contains two columns: `filename` and `label`. - **Expected Methods**: - `CustomDataset(image_dir, label_file)` - `CustomSampler(data_source)` - `custom_collate_fn(batch)` - `create_dataloader(dataset, sampler, collate_fn)` - **Output**: - The DataLoader should yield batches of images and labels in the form of PyTorch tensors, ready to be transferred to the GPU. Constraints: - Assume the images can have varying dimensions. - Ensure that all operations are optimized for both memory efficiency and speed. Here is a template to get you started: ```python import os import pandas as pd import torch from torch.utils.data import Dataset, DataLoader, Sampler class CustomDataset(Dataset): def __init__(self, image_dir, label_file): # Initialize your dataset, you might want to store the images and labels paths or any other ids pass def __len__(self): # Return the length of the dataset pass def __getitem__(self, idx): # Fetch the sample (image and label) for the given index pass class CustomSampler(Sampler): def __init__(self, data_source): # Initialize your Sampler with the data source pass def __iter__(self): # Return an iterator that yields the shuffled indices pass def __len__(self): # Return the length of the dataset pass def custom_collate_fn(batch): # Implement your custom collate function for batching and padding pass def create_dataloader(dataset, sampler, collate_fn): # Create DataLoader with the given dataset, sampler and collate function dataloader = DataLoader(dataset, batch_size=32, sampler=sampler, num_workers=4, pin_memory=True, collate_fn=collate_fn) return dataloader # Example usage: # dataset = CustomDataset(image_dir=\'/path/to/images\', label_file=\'/path/to/labels.csv\') # sampler = CustomSampler(dataset) # dataloader = create_dataloader(dataset, sampler, custom_collate_fn) # for data in dataloader: # images, labels = data # images = images.cuda(non_blocking=True) # labels = labels.cuda(non_blocking=True) # # Your training loop here ``` Implement the missing parts and ensure your DataLoader is correctly configured for efficient data loading, batching, and GPU transfer.","solution":"import os import pandas as pd import torch from torch.utils.data import Dataset, DataLoader, Sampler from PIL import Image import numpy as np import random class CustomDataset(Dataset): def __init__(self, image_dir, label_file): self.image_dir = image_dir self.df = pd.read_csv(label_file) self.images = self.df[\'filename\'].tolist() self.labels = self.df[\'label\'].tolist() def __len__(self): return len(self.images) def __getitem__(self, idx): img_name = os.path.join(self.image_dir, self.images[idx]) image = Image.open(img_name).convert(\'RGB\') label = self.labels[idx] image = np.array(image) image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0 label = torch.tensor(label, dtype=torch.long) return image, label class CustomSampler(Sampler): def __init__(self, data_source): self.data_source = data_source def __iter__(self): return iter(torch.randperm(len(self.data_source)).tolist()) def __len__(self): return len(self.data_source) def custom_collate_fn(batch): max_height = max([item[0].shape[1] for item in batch]) max_width = max([item[0].shape[2] for item in batch]) padded_images = torch.zeros((len(batch), 3, max_height, max_width), dtype=torch.float32) labels = torch.zeros(len(batch), dtype=torch.long) for i, (image, label) in enumerate(batch): padded_images[i, :, :image.shape[1], :image.shape[2]] = image labels[i] = label return padded_images, labels def create_dataloader(dataset, sampler, collate_fn): return DataLoader(dataset, batch_size=32, sampler=sampler, num_workers=4, pin_memory=True, collate_fn=collate_fn) # Example usage: # dataset = CustomDataset(image_dir=\'/path/to/images\', label_file=\'/path/to/labels.csv\') # sampler = CustomSampler(dataset) # dataloader = create_dataloader(dataset, sampler, custom_collate_fn) # for data in dataloader: # images, labels = data # images = images.cuda(non_blocking=True) # labels = labels.cuda(non_blocking=True) # # Your training loop here"},{"question":"# Custom Iterator Implementation **Objective:** Implement custom iterator classes that mimic the behavior of sequence iterators and callable iterators as described in the documentation. # Problem Description 1. **Sequence Iterator:** - Implement a class `SeqIterator` that simulates the behavior of `PySeqIter_Type` iterators. - This iterator should work with any sequence object that supports the `__getitem__()` method. - The iteration should end when the sequence raises an `IndexError` for the subscripting operation. 2. **Callable Iterator:** - Implement a class `CallIterator` that simulates the behavior of `PyCallIter_Type` iterators. - This iterator will be initialized with a callable object and a sentinel value. - The iterator should call the callable for each item and stop when the callable returns the sentinel value. # Function Signatures: 1. `SeqIterator`: ```python class SeqIterator: def __init__(self, seq): Initialize the sequence iterator with a sequence object. Parameters: seq (sequence): A sequence object supporting the __getitem__() method. pass def __iter__(self): Return the iterator object. pass def __next__(self): Return the next item in the sequence. Raise StopIteration when the sequence is exhausted. pass ``` 2. `CallIterator`: ```python class CallIterator: def __init__(self, callable, sentinel): Initialize the callable iterator with a callable object and a sentinel value. Parameters: callable (callable): A callable object that returns the next item in the iteration. sentinel (any): The sentinel value that denotes the end of the iteration. pass def __iter__(self): Return the iterator object. pass def __next__(self): Return the next item generated by the callable. Raise StopIteration when the callable returns the sentinel value. pass ``` # Example Usage ```python # Sequence iterator example seq = [1, 2, 3] seq_iter = SeqIterator(seq) for item in seq_iter: print(item) # Output: 1 2 3 # Callable iterator example def gen(): for i in range(5): yield i callable_iter = CallIterator(gen().__next__, -1) for item in callable_iter: print(item) # Output: 0 1 2 3 4 ``` # Constraints - Do not use existing iterator functions such as `iter()`. Implement the iteration from scratch. - The sequence and callable will be valid and non-empty. # Performance Requirements - The implementation should be efficient and handle edge cases like empty sequences or callable outputs robustly.","solution":"class SeqIterator: def __init__(self, seq): Initialize the sequence iterator with a sequence object. Parameters: seq (sequence): A sequence object supporting the __getitem__() method. self.seq = seq self.index = 0 def __iter__(self): Return the iterator object. return self def __next__(self): Return the next item in the sequence. Raise StopIteration when the sequence is exhausted. try: item = self.seq[self.index] except IndexError: raise StopIteration self.index += 1 return item class CallIterator: def __init__(self, callable, sentinel): Initialize the callable iterator with a callable object and a sentinel value. Parameters: callable (callable): A callable object that returns the next item in the iteration. sentinel (any): The sentinel value that denotes the end of the iteration. self.callable = callable self.sentinel = sentinel def __iter__(self): Return the iterator object. return self def __next__(self): Return the next item generated by the callable. Raise StopIteration when the callable returns the sentinel value. result = self.callable() if result == self.sentinel: raise StopIteration return result"},{"question":"**Question: Secure Shadow Password Management Script** You are tasked to write a Python function that utilizes the `spwd` module to create a summary report of all users in the Unix shadow password database. Your script should perform the following tasks: 1. Retrieve all entries from the shadow password database using `spwd.getspall()`. 2. For each entry: - Retrieve and store the login name (`sp_namp`), the date of the last password change (`sp_lstchg` in days since epoch), the minimum days between password changes (`sp_min`), and the maximum days password is valid (`sp_max`). - Ensure that the encrypted password (`sp_pwdp`) is securely handled and is not included in the output. 3. Return a list of dictionaries where each dictionary contains information about a user as specified above. # Function Signature ```python def generate_shadow_summary() -> List[Dict[str, Any]]: pass ``` # Input - The function does not take any input parameters. # Output - The function returns a list of dictionaries, each corresponding to a user entry. Each dictionary should have the following structure: ```python { \\"login_name\\": str, \\"last_change_date\\": int, # days since epoch \\"min_days_between_changes\\": int, \\"max_days_valid\\": int } ``` # Constraints - Your script must handle cases where certain fields may not be present or contain invalid values gracefully. - You must ensure that sensitive data such as encrypted passwords are not exposed in any form. # Example Usage ```python # Expected to run with proper privileges, providing a summary of shadow database entries. summary = generate_shadow_summary() print(summary) ``` # Notes - Ensure your system environment and python version can access and manipulate the Unix shadow password database (requires root privileges). - Handle potential exceptions, including permissions issues (`PermissionError`) and entry not found issues (`KeyError`).","solution":"from typing import List, Dict, Any import spwd def generate_shadow_summary() -> List[Dict[str, Any]]: Generates a summary report of all users in the Unix shadow password database. try: shadow_entries = spwd.getspall() except PermissionError: raise PermissionError(\\"Insufficient permissions to read the shadow password database.\\") summary = [] for entry in shadow_entries: user_info = { \\"login_name\\": entry.sp_namp, \\"last_change_date\\": entry.sp_lstchg, \\"min_days_between_changes\\": entry.sp_min, \\"max_days_valid\\": entry.sp_max } summary.append(user_info) return summary"},{"question":"# Question: Advanced Data Visualization with Seaborn **Objective:** Create a complex data visualization using Seaborn that demonstrates your ability to load a dataset, create a faceted plot, and apply advanced thematic customizations. **Task:** 1. Load the `penguins` dataset from Seaborn\'s built-in datasets. 2. Create a Seaborn plot with the following specifications: - The x-axis should represent `bill_length_mm` and the y-axis `bill_depth_mm`. - Color the data points based on the `species` column. - Facet the plot by `sex` and wrap it to fit a grid format. - Add a linear regression line to each facet. 3. Customize the appearance of the plot: - Change the face color of the axes to white and the edge color to slategrey. - Set the width of the lines in the plots to 2. - Apply the `ticks` style from Seaborn for the axes. - Set the plotting context to `talk` for better readability. **Input Format:** The function should not take any inputs. **Output Format:** The function should display the final customized plot. **Constraints:** - Ensure to handle any missing data in the dataset appropriately before plotting. - Use dictionary union syntax for combining multiple parameter dictionaries if using Python 3.9+. **Performance Requirements:** - The entire process (data loading, processing, and visualization) should be performed within 2 seconds for the given dataset size. **Function Signature:** ```python def create_customized_penguin_plot(): pass ``` **Example:** In your implementation, ensure the resulting plot matches the following criteria: - Facets for each sex (`male` and `female`) with a linear regression line in each facet. - Aesthetic adjustments applied as specified. **Note:** The function should call the necessary Seaborn and Matplotlib functions to produce the required visualization directly when executed.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_customized_penguin_plot(): # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Handle missing data by dropping rows with any NaN values penguins.dropna(inplace=True) # Set the theme and context for the plot sns.set_theme(style=\\"ticks\\", context=\\"talk\\") # Create the Seaborn FacetGrid with the required customizations g = sns.FacetGrid(penguins, col=\\"sex\\", height=5, aspect=1) g.map_dataframe(sns.scatterplot, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=\\"deep\\", edgecolor=\\"w\\", linewidth=2) g.map_dataframe(sns.regplot, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", scatter=False, truncate=False) # Customize the appearance for ax in g.axes.flat: ax.set_facecolor(\\"white\\") for spine in ax.spines.values(): spine.set_edgecolor(\\"slategrey\\") spine.set_linewidth(2) # Add legend g.add_legend() # Display the final plot plt.show()"},{"question":"# Python Descriptor and Attribute Management Objective: Write a Python class called `CustomDescriptor` that makes use of Python descriptors to manage attributes of a class dynamically. You will create descriptors for: 1. A `property` that has custom getter and setter methods. 2. A `method` that can be dynamically assigned and called. 3. A `data member` that can be tracked and validated upon setting its value. Requirements: 1. You must define a descriptor for a property called `dynamic_property`. The getter should return the value of an internal attribute `_value`. The setter should validate that the value being set is an integer and raise a `ValueError` if it is not. 2. Define a descriptor for a method called `dynamic_method`. This method should accept any number of arguments and return their sum. 3. Define a descriptor for a data member called `data_member` that tracks the last set value and ensures it is a string. If the value being set is not a string, throw a `TypeError`. Expected Input and Output: - **Input**: The values set to attributes and method calls on the `CustomDescriptor` class. - **Output**: The result of getter methods, method calls, and assignment operations. Example: ```python class CustomDescriptor: # Your implementation here # Example usage: cd = CustomDescriptor() # Test dynamic_property cd.dynamic_property = 10 # Should set normally print(cd.dynamic_property) # Output: 10 try: cd.dynamic_property = \'test\' # Should raise ValueError except ValueError as ve: print(ve) # Output: Value must be an integer # Test dynamic_method print(cd.dynamic_method(1, 2, 3)) # Output: 6 print(cd.dynamic_method(10, 20)) # Output: 30 # Test data_member cd.data_member = \\"hello\\" # Should set normally print(cd.data_member) # Output: hello try: cd.data_member = 123 # Should raise TypeError except TypeError as te: print(te) # Output: Value must be a string ``` # Constraints: 1. The `dynamic_property` value must always be an integer, checked during the setter call. 2. The `data_member` must always be a string, checked during the setter call. 3. The `dynamic_method` must correctly sum any number of arguments passed during the call. **Hint**: Make use of Pythons descriptor protocol methods (`__get__`, `__set__`, and `__delete__`) where necessary. # Submission: Please submit a single Python file with the implementation of the class `CustomDescriptor` and the necessary descriptors.","solution":"class DynamicProperty: def __init__(self): self._value = 0 def __get__(self, instance, owner): return self._value def __set__(self, instance, value): if not isinstance(value, int): raise ValueError(\\"Value must be an integer\\") self._value = value class DynamicMethod: def __get__(self, instance, owner): def method(*args): return sum(args) return method class DataMember: def __init__(self): self._value = \\"\\" def __get__(self, instance, owner): return self._value def __set__(self, instance, value): if not isinstance(value, str): raise TypeError(\\"Value must be a string\\") self._value = value class CustomDescriptor: dynamic_property = DynamicProperty() dynamic_method = DynamicMethod() data_member = DataMember()"},{"question":"# IMAP Email Client Utility Problem Statement: You are tasked with implementing a Python utility for interacting with an IMAP email server. This utility should be able to: 1. Connect to a specified IMAP server using either standard or SSL/TLS connection. 2. Login using provided user credentials. 3. Select a specified mailbox. 4. Fetch emails from a specified date range. 5. Log out from the server. Requirements: 1. **Connection:** Implement a function `connect_to_server` that takes the following parameters: - `host` (string): The server host name or IP address. - `port` (int, optional): The port number. Default is 143 for standard connection and 993 for SSL. - `use_ssl` (bool): Whether to use an SSL connection. Default is `False`. - `timeout` (int, optional): Timeout for connection attempts in seconds. The function should return an instance of the `IMAP4` or `IMAP4_SSL` class based on the `use_ssl` parameter. 2. **Login:** Implement a function `login` that takes the following parameters: - `connection` (IMAP4/IMAP4_SSL instance): The connection instance. - `user` (string): The username for login. - `password` (string): The password for login. This function should authenticate the user to the server using the `LOGIN` command. 3. **Select Mailbox:** Implement a function `select_mailbox` that takes the following parameters: - `connection` (IMAP4/IMAP4_SSL instance): The connection instance. - `mailbox` (string): The name of the mailbox to select. Default is `\'INBOX\'`. This function should select the specified mailbox. 4. **Fetch Emails:** Implement a function `fetch_emails` that takes the following parameters: - `connection` (IMAP4/IMAP4_SSL instance): The connection instance. - `start_date` (string): The start date in the format \'DD-MMM-YYYY\'. - `end_date` (string): The end date in the format \'DD-MMM-YYYY\'. This function should search for emails within the specified date range and fetch their \'RFC822\' parts. The function should return a list of raw email messages. 5. **Logout:** Implement a function `logout` that takes the following parameter: - `connection` (IMAP4/IMAP4_SSL instance): The connection instance. This function should log out from the server. Constraints: - Ensure to handle any exceptions that may occur during connection, login, mailbox selection, fetching, or logout and provide meaningful error messages. - You are not allowed to use any other libraries for IMAP operations apart from `imaplib`. Function Signatures: ```python import imaplib def connect_to_server(host: str, port: int = 143, use_ssl: bool = False, timeout: int = None) -> imaplib.IMAP4: pass def login(connection: imaplib.IMAP4, user: str, password: str) -> None: pass def select_mailbox(connection: imaplib.IMAP4, mailbox: str = \'INBOX\') -> None: pass def fetch_emails(connection: imaplib.IMAP4, start_date: str, end_date: str) -> list: pass def logout(connection: imaplib.IMAP4) -> None: pass ``` Example Usage: ```python # Example usage: conn = connect_to_server(\'imap.example.com\', use_ssl=True) login(conn, \'user@example.com\', \'password123\') select_mailbox(conn, \'INBOX\') emails = fetch_emails(conn, \'01-Jan-2023\', \'31-Jan-2023\') logout(conn) for email in emails: print(email) ``` **NOTE:** You do not need to check for invalid dates or specific formatting outside the mentioned format. Assume the input format will always be correct.","solution":"import imaplib import ssl def connect_to_server(host: str, port: int = 143, use_ssl: bool = False, timeout: int = None) -> imaplib.IMAP4: try: if use_ssl: connection = imaplib.IMAP4_SSL(host, port, ssl_context=ssl.create_default_context()) if timeout is None else imaplib.IMAP4_SSL(host, port, ssl_context=ssl.create_default_context(), timeout=timeout) else: connection = imaplib.IMAP4(host, port) if timeout is None else imaplib.IMAP4(host, port, timeout=timeout) return connection except Exception as e: raise ConnectionError(f\\"Failed to connect to server: {e}\\") def login(connection: imaplib.IMAP4, user: str, password: str) -> None: try: connection.login(user, password) except Exception as e: raise ConnectionError(f\\"Failed to login: {e}\\") def select_mailbox(connection: imaplib.IMAP4, mailbox: str = \'INBOX\') -> None: try: connection.select(mailbox) except Exception as e: raise ConnectionError(f\\"Failed to select mailbox: {e}\\") def fetch_emails(connection: imaplib.IMAP4, start_date: str, end_date: str) -> list: try: status, data = connection.search(None, f\'(SENTSINCE {start_date} SENTBEFORE {end_date})\') if status != \'OK\': raise RuntimeError(\\"Failed to search emails\\") email_ids = data[0].split() emails = [] for email_id in email_ids: status, data = connection.fetch(email_id, \'(RFC822)\') if status != \'OK\': raise RuntimeError(f\\"Failed to fetch email ID {email_id}\\") emails.append(data[0][1]) return emails except Exception as e: raise RuntimeError(f\\"Error fetching emails: {e}\\") def logout(connection: imaplib.IMAP4) -> None: try: connection.logout() except Exception as e: raise RuntimeError(f\\"Failed to logout: {e}\\")"},{"question":"**Python Coding Assessment: Using `faulthandler` for Debugging** **Objective:** Demonstrate your understanding of the `faulthandler` module by implementing a Python program that effectively utilizes this module to handle and debug faults. **Scenario:** You are given a Python script that simulates a complex multi-threaded application. This application occasionally crashes due to segmentation faults and deadlocks, and it\'s necessary to diagnose these issues efficiently. **Requirements:** 1. **Enable Fault Handling:** - Use `faulthandler.enable()` to install fault handlers when your script starts. Ensure that tracebacks for all threads are enabled. 2. **Timeout-Based Traceback Dump:** - Implement a mechanism using `faulthandler.dump_traceback_later()` to dump the tracebacks of all threads after a timeout of 5 seconds. Ensure the program gracefully handles and logs these dumps. 3. **User Signal-Based Traceback Dump:** - Register a user-defined signal (e.g., `SIGUSR1`) using `faulthandler.register()` to dump the tracebacks when this signal is received. 4. **Testing Faults:** - Simulate a segmentation fault using `ctypes.string_at(0)` after running the script for 6 seconds to trigger the signal-based dump. - Additionally, create and execute code that intentionally causes a deadlock between multiple threads after 10 seconds to see the timeout-based dump in action. **Constraints:** - You should handle Windows and Unix-based systems appropriately, noting that Windows does not support user signals. - Ensure your program writes the traceback logs to a file named `traceback.log`. - Use threading to simulate the multi-threaded behavior of the application. - Make sure that the program continues running and logging after handling faults until manually interrupted. **Example Execution:** ``` python3 your_script.py ``` **Expected Output:** - The script should initialize and start logging to `traceback.log`. - After 5 seconds, the `faulthandler.dump_traceback_later()` should log all thread tracebacks if not interrupted by the simulated faults. - Upon encountering a segmentation fault or deadlock, corresponding tracebacks should be logged. - The log file should accurately capture tracebacks in the chronological order of their occurrence. Implement the script as described above. Ensure your code is efficient, handles exceptions where possible, and includes clear comments to explain each step. **Note:** This question requires a deep understanding of signal handling and multi-threaded programming in Python using `faulthandler`. Ensure your implementation efficiently uses the module\'s features to diagnose and log critical faults.","solution":"import faulthandler import threading import ctypes import time import os import signal def trigger_segmentation_fault(): time.sleep(6) ctypes.string_at(0) # This will cause a segmentation fault def create_deadlock(): lock1 = threading.Lock() lock2 = threading.Lock() def thread1(): with lock1: time.sleep(1) with lock2: pass def thread2(): with lock2: time.sleep(1) with lock1: pass t1 = threading.Thread(target=thread1) t2 = threading.Thread(target=thread2) t1.start() t2.start() t1.join() t2.join() def signal_handler(signum, frame): print(f\\"Signal {signum} received: dumping tracebacks to logfile.\\") faulthandler.dump_traceback(file=open(\'traceback.log\', \'a\'), all_threads=True) def main(): logfile = \\"traceback.log\\" faulthandler.enable(open(logfile, \'w\'), all_threads=True) faulthandler.dump_traceback_later(5, file=open(logfile, \'a\'), repeat=True) # Registering signal handler if on Unix if hasattr(signal, \'SIGUSR1\'): signal.signal(signal.SIGUSR1, signal_handler) faulthandler.register(signal.SIGUSR1, file=open(logfile, \'a\'), all_threads=True) threads = [ threading.Thread(target=trigger_segmentation_fault), threading.Thread(target=create_deadlock) ] for t in threads: t.start() print(\\"Application started. Logging tracebacks to \'traceback.log\'.\\") # Continue running to show more stack traces for t in threads: t.join() if __name__ == \\"__main__\\": main()"},{"question":"Objective The goal of this task is to assess your understanding of the `hashlib` module for secure hashing in Python. You will implement a custom hashing application that processes input data in chunks, uses BLAKE2 for advanced features, and demonstrates the use of key derivation functions for secure password handling. Task 1. **Create a Hashing Function:** - Write a function `compute_hash(algorithm: str, data: bytes) -> str` that takes in the name of the hashing algorithm (e.g., \'sha256\', \'md5\', \'blake2b\') and data (as bytes) and returns the hexadecimal digest of the hash. - The function should use the algorithm specified in the input to create a hash object, update the hash object with the input data, and finally return the hex digest. 2. **Process Data in Chunks:** - Extend the `compute_hash` function to handle data input in chunks. Simulate the chunked data by dividing the input data into chunks of 2048 bytes and updating the hash object in a loop. 3. **Advanced Hashing with BLAKE2:** - Create a function `blake2_custom_hash(data: bytes, digest_size: int, key: bytes, salt: bytes, person: bytes) -> str` that demonstrates the customizable features of BLAKE2b. - This function should take the initial data, digest size, key, salt, and personalization string, and return the hex digest of the BLAKE2b hash. 4. **Key Derivation:** - Implement a function `generate_key(password: str, salt: bytes, iterations: int, key_length: int) -> bytes` which uses the `pbkdf2_hmac` function of `hashlib` to generate a derived key. - The function parameters are a string password, a salt value (in bytes), the number of iterations, and the desired key length. Input Format - For `compute_hash`: - `algorithm`: str - the name of the hash algorithm. - `data`: bytes - the data to be hashed. - For `blake2_custom_hash`: - `data`: bytes - the data to be hashed. - `digest_size`: int - the desired size of the hash digest. - `key`: bytes - the key for keyed hashing. - `salt`: bytes - the salt for randomized hashing. - `person`: bytes - the personalization string. - For `generate_key`: - `password`: str - the password to derive the key from. - `salt`: bytes - the salt for the key derivation. - `iterations`: int - the number of iterations for key stretching. - `key_length`: int - the desired length of the derived key. Output Format - The `compute_hash` and `blake2_custom_hash` functions should return a string which is the hexadecimal digest of the hash. - The `generate_key` function should return a bytes object which is the derived key. Constraints - Only valid and available algorithm names will be provided. - Inputs will be well-formed. - The password length will be limited to 1024 characters. - Salt must be at least 16 bytes. Example ```python # Example test case # Case 1: Compute hash algorithm = \'sha256\' data = b\'This is a test data\' print(compute_hash(algorithm, data)) # Output: Expected SHA256 hash of the data # Case 2: BLAKE2 Custom Hash data = b\'Custom data for testing BLAKE2\' digest_size = 32 key = b\'my_secret_key\' salt = b\'my_salt\' person = b\'my_person\' print(blake2_custom_hash(data, digest_size, key, salt, person)) # Output: Expected BLAKE2b hash # Case 3: Key Derivation password = \'mypassword\' salt = b\'somesaltvalue\' iterations = 100000 key_length = 32 print(generate_key(password, salt, iterations, key_length)) # Output: Derived key in bytes ``` Grading Criteria - Correct implementation of the hashing algorithms. - Correct handling and updating hash objects with chunked data. - Correct implementation of customizable BLAKE2 hashing. - Correct generation of derived keys using `pbkdf2_hmac`.","solution":"import hashlib from hashlib import blake2b from typing import Optional def compute_hash(algorithm: str, data: bytes) -> str: Computes the hash of the given data using the specified algorithm. :param algorithm: The name of the hash algorithm (e.g., \'sha256\', \'md5\', \'blake2b\'). :param data: The data to be hashed. :return: Hexadecimal digest of the hash. hash_obj = hashlib.new(algorithm) chunk_size = 2048 # Process data in 2048 byte chunks for i in range(0, len(data), chunk_size): hash_obj.update(data[i:i + chunk_size]) return hash_obj.hexdigest() def blake2_custom_hash(data: bytes, digest_size: int, key: Optional[bytes] = None, salt: Optional[bytes] = None, person: Optional[bytes] = None) -> str: Computes a custom BLAKE2b hash with given parameters. :param data: The data to be hashed. :param digest_size: The desired size of the hash digest. :param key: The key for keyed hashing. :param salt: The salt for randomized hashing. :param person: The personalization string. :return: Hexadecimal digest of the BLAKE2b hash. hash_obj = blake2b(digest_size=digest_size, key=key, salt=salt, person=person) chunk_size = 2048 # Process data in 2048 byte chunks for i in range(0, len(data), chunk_size): hash_obj.update(data[i:i + chunk_size]) return hash_obj.hexdigest() def generate_key(password: str, salt: bytes, iterations: int, key_length: int) -> bytes: Generates a derived key using PBKDF2-HMAC with the given parameters. :param password: The password to derive the key from. :param salt: The salt for the key derivation. :param iterations: The number of iterations for key stretching. :param key_length: The desired length of the derived key. :return: The derived key in bytes. password_bytes = password.encode(\'utf-8\') derived_key = hashlib.pbkdf2_hmac(\'sha256\', password_bytes, salt, iterations, dklen=key_length) return derived_key"},{"question":"# Question: Implement a Custom Wrapper for Built-in Functions You are required to implement a custom wrapper around some built-in Python functions using the `builtins` module. This exercise will help you demonstrate an understanding of both basic and advanced Python concepts, including modules, built-in functions, and classes. Task 1. **Custom Open Function**: Create a function `custom_open` that wraps the built-in `open` function. This function should: - Accept file path and mode as arguments (default mode should be \'r\' for reading). - Open the file using the built-in `open()` and return a custom file object `LowerCaser` that converts output to lower-case when reading. 2. **LowerCaser Class**: Implement a class `LowerCaser` that: - Takes a file object as an input during initialization. - Has a `read` method that reads content from the file and converts it to lower-case. 3. **Constraints**: - You should use the `builtins` module to access the built-in `open` function. - Your `LowerCaser` class should handle reading the content in lower-case only. 4. **Input Format**: - `custom_open` function should take two arguments: 1. `file_path` (string): The path of the file to be opened. 2. `mode` (string, optional): The mode in which the file should be opened (default is \'r\'). 5. **Output Format**: - `custom_open` should return an instance of `LowerCaser`. - `LowerCaser.read()` should return the file content in lower-case. Example Usage: ```python # Assume we have a file \'example.txt\' with content \\"HELLO WORLD!\\" lc = custom_open(\'example.txt\') print(lc.read()) # Should print \\"hello world!\\" ``` ```python import builtins def custom_open(file_path, mode=\'r\'): file_obj = builtins.open(file_path, mode) return LowerCaser(file_obj) class LowerCaser: def __init__(self, file): self.file = file def read(self, size=-1): return self.file.read(size).lower() ``` Your implementation should handle real file operations and ensure correct functionality as described in the task. You may also wish to add error handling for cases where the file might not exist or other I/O issues might occur.","solution":"import builtins def custom_open(file_path, mode=\'r\'): Custom open function that returns a LowerCaser wrapping the built-in open function. Args: - file_path (str): Path of the file to be opened. - mode (str, optional): Mode in which the file should be opened (default is \'r\'). Returns: - LowerCaser: A custom file object that converts output to lower-case when reading. file_obj = builtins.open(file_path, mode) return LowerCaser(file_obj) class LowerCaser: def __init__(self, file): Initialize with a file object. Args: - file: A file object opened using the built-in open function. self.file = file def read(self, size=-1): Read content from the file and convert it to lower-case. Args: - size (int, optional): Number of bytes to read (default is -1, which means read all). Returns: - str: The content read from the file in lower-case. return self.file.read(size).lower() def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): self.file.close()"},{"question":"# Python Programming Challenge Context: You are working on a Python project that needs to bundle several scripts and their dependencies into a single executable archive. This archive should be runnable directly using the Python interpreter. To accomplish this, you would use the `zipapp` module to create an application archive from a directory. Task: Implement a function `create_executable_zip` that will take the following parameters: 1. `source_dir` (str): The path to the directory containing the Python application code. 2. `output_path` (str): The path where the resulting zip archive should be saved. 3. `interpreter` (str, optional): The Python interpreter path to be used in the shebang line. Defaults to `None`. 4. `main_callable` (str, optional): The entry point for the application in the form of `pkg.module:callable`. Defaults to `None`. 5. `compressed` (bool, optional): Whether to compress the files in the archive. Defaults to `False`. The function should create a zip archive from the `source_dir` and write it to `output_path`. If `main_callable` is specified, it should be set as the main entry point. If `interpreter` is provided, it should be added as a shebang line at the beginning of the archive. The files should be compressed if `compressed` is set to `True`. Function Signature: ```python def create_executable_zip(source_dir: str, output_path: str, interpreter: str = None, main_callable: str = None, compressed: bool = False) -> None: pass ``` Example: ```python # Given a directory structure like below # myapp/ #  __main__.py #  module1.py #  module2.py create_executable_zip(\\"myapp\\", \\"myapp.pyz\\", \\"/usr/bin/env python3\\", \\"mymodule:main_function\\", True) # After execution, the \\"myapp.pyz\\" archive should be created, containing the files in \\"myapp\\" directory, # with the shebang line \\"#!/usr/bin/env python3\\" and set to run \\"mymodule:main_function\\" on execution. ``` Constraints: - The `source_dir` must be a valid directory path. - The `output_path` must be a valid file path where the archive will be saved. - If `main_callable` is specified, it must be in the format `pkg.module:callable`. # Notes: - Make sure to handle any exceptions that may occur during the creation of the archive and provide meaningful error messages. Evaluation Criteria: - Correctness: The function should correctly create the executable zip archive as per the parameters. - Edge Cases: The function should handle edge cases such as missing `__main__.py` in `source_dir`. - Code Quality: The code should be clean, readable, and well-documented.","solution":"import zipapp import os def create_executable_zip(source_dir: str, output_path: str, interpreter: str = None, main_callable: str = None, compressed: bool = False) -> None: Creates an executable zip archive from the source directory. Args: source_dir (str): The path to the directory containing the Python application code. output_path (str): The path where the resulting zip archive should be saved. interpreter (str, optional): The Python interpreter path to be used in the shebang line. Defaults to `None`. main_callable (str, optional): The entry point for the application in the form of `pkg.module:callable`. Defaults to `None`. compressed (bool, optional): Whether to compress the files in the archive. Defaults to `False`. Raises: FileNotFoundError: If the `source_dir` does not exist. ValueError: If `main_callable` is not in the correct format. if not os.path.isdir(source_dir): raise FileNotFoundError(f\\"Source directory \'{source_dir}\' does not exist.\\") if main_callable and \':\' not in main_callable: raise ValueError(\\"main_callable must be in the format \'module:callable\'\\") zipapp.create_archive( source_dir, target=output_path, interpreter=interpreter, main=main_callable, compressed=compressed )"},{"question":"# Custom Cache Dictionary with Expiry You are required to implement a custom cache dictionary class in Python that supports typical dictionary operations (insertion, deletion, retrieval) but also includes the capability to set an expiration time for each key-value pair. When a key is accessed after its expiration time, the cache should behave as if the key does not exist. Requirements: 1. Implement a class `CacheDict`. 2. `CacheDict` should support standard dictionary operations: `__getitem__`, `__setitem__`, `__delitem__`, `__contains__`, and `keys`. 3. Allow users to set an expiration time (in seconds) for key-value pairs during insertion. 4. Automatically remove expired key-value pairs upon access. Expected Input and Output Formats: - **Input:** - Operations include standard dictionary operations and expiration time (integer seconds). ```python cache = CacheDict() cache[\'a\'] = (1, 5) # key \'a\' with value 1 expires in 5 seconds cache[\'b\'] = (2, 2) # key \'b\' with value 2 expires in 2 seconds ``` - **Output:** - Retrieved values for existing keys, and `KeyError` for non-existent or expired keys. ```python print(cache[\'a\']) # Should print 1 before expiration time.sleep(6) print(cache[\'a\']) # Should raise KeyError after expiration ``` Constraints or Limitations: - The expiration time should be an integer representing seconds. - Handle typical dictionary methods efficiently. Performance Requirements: - Access operations should be efficient, keeping in mind the overhead introduced by expiration checking. Implementation: Your implementation should primarily demonstrate the following: - Use of Python special methods to provide custom behavior. - Time-based expiration handling using standard library functions. - Proper error raising and handling for expired key accesses. ```python import time class CacheDict: def __init__(self): self._store = {} def __setitem__(self, key, value): try: val, duration = value expiration = time.time() + duration self._store[key] = (val, expiration) except ValueError: raise ValueError(\\"Value must be a tuple (val, duration)\\") def __getitem__(self, key): if key in self._store: val, expiration = self._store[key] if time.time() < expiration: return val else: del self._store[key] raise KeyError(f\\"Key \'{key}\' has expired\\") else: raise KeyError(f\\"Key \'{key}\' does not exist\\") def __delitem__(self, key): if key in self._store: del self._store[key] else: raise KeyError(f\\"Key \'{key}\' does not exist\\") def __contains__(self, key): if key in self._store: _, expiration = self._store[key] if time.time() < expiration: return True else: del self._store[key] return False def keys(self): # Clean expired keys before returning the keys expired_keys = [key for key in self._store if time.time() >= self._store[key][1]] for key in expired_keys: del self._store[key] return self._store.keys() ```","solution":"import time class CacheDict: def __init__(self): self._store = {} def __setitem__(self, key, value): if not isinstance(value, tuple) or len(value) != 2: raise ValueError(\\"Value must be a tuple (val, duration)\\") val, duration = value expiration = time.time() + duration self._store[key] = (val, expiration) def __getitem__(self, key): if key in self._store: val, expiration = self._store[key] if time.time() < expiration: return val else: del self._store[key] raise KeyError(f\\"Key \'{key}\' has expired\\") else: raise KeyError(f\\"Key \'{key}\' does not exist\\") def __delitem__(self, key): if key in self._store: del self._store[key] else: raise KeyError(f\\"Key \'{key}\' does not exist\\") def __contains__(self, key): if key in self._store: _, expiration = self._store[key] if time.time() < expiration: return True else: del self._store[key] return False return False def keys(self): # Clean expired keys before returning the keys expired_keys = [key for key in self._store if time.time() >= self._store[key][1]] for key in expired_keys: del self._store[key] return self._store.keys()"},{"question":"# Question: Out-of-Core Learning for Text Classification You are provided with a large text dataset that does not fit into the main memory of your computer. Your task is to build an out-of-core learning system to classify these text documents. **Requirements:** 1. Implement a data streaming function that reads and yields documents from a large text file in chunks. 2. Use `HashingVectorizer` from `sklearn.feature_extraction.text` for feature extraction. 3. Train an `SGDClassifier` incrementally using the `partial_fit` method. **Specifications:** - **Input**: - A file path to a large text file where each line represents a document and the last character on each line is the class label separated by a tab (e.g., \\"text document heret0\\"). - A list of all possible classes. - **Output**: - A trained `SGDClassifier` model. - **Constraints**: - The file should be read in chunks to handle out-of-core learning. - Use the `HashingVectorizer` for feature extraction. - Use `SGDClassifier` for incremental learning. - Assume binary classification for simplicity (classes are 0 and 1). **Performance Requirements**: - Ensure that the model training efficiently handles large datasets without causing memory overflow. **Code Template**: ```python from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier import numpy as np def stream_data(file_path, chunk_size=100): Generator function to read file and yield documents in chunks. :param file_path: str, path to the large text file :param chunk_size: int, number of documents to read at each iteration :return: yields lists of documents (strings) and their corresponding labels (ints) with open(file_path, \'r\') as file: chunk = [] labels = [] for line in file: doc, label = line.strip().rsplit(\'t\', 1) chunk.append(doc) labels.append(int(label)) if len(chunk) == chunk_size: yield chunk, labels chunk, labels = [], [] if chunk: yield chunk, labels def out_of_core_learning(file_path, classes, chunk_size=100): Perform out-of-core learning on text data. :param file_path: str, path to the large text file :param classes: list, list of possible class labels :param chunk_size: int, number of documents to process at each iteration :return: trained `SGDClassifier` model vectorizer = HashingVectorizer(n_features=2**18) classifier = SGDClassifier() is_first_chunk = True for docs, labels in stream_data(file_path, chunk_size): X = vectorizer.transform(docs) y = np.array(labels) if is_first_chunk: classifier.partial_fit(X, y, classes=classes) is_first_chunk = False else: classifier.partial_fit(X, y) return classifier # Example Usage: # Assume the text file \'large_text_file.txt\' contains the data # classes = [0, 1] # model = out_of_core_learning(\'large_text_file.txt\', classes) ``` **Test Cases**: 1. Your implementation will be tested on a simulated large text dataset to ensure it correctly performs out-of-core learning with efficient memory usage. 2. Verify the accuracy of your trained model on a separate test dataset.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier import numpy as np def stream_data(file_path, chunk_size=100): Generator function to read file and yield documents in chunks. :param file_path: str, path to the large text file :param chunk_size: int, number of documents to read at each iteration :return: yields lists of documents (strings) and their corresponding labels (ints) with open(file_path, \'r\') as file: chunk = [] labels = [] for line in file: doc, label = line.strip().rsplit(\'t\', 1) chunk.append(doc) labels.append(int(label)) if len(chunk) == chunk_size: yield chunk, labels chunk, labels = [], [] if chunk: yield chunk, labels def out_of_core_learning(file_path, classes, chunk_size=100): Perform out-of-core learning on text data. :param file_path: str, path to the large text file :param classes: list, list of possible class labels :param chunk_size: int, number of documents to process at each iteration :return: trained `SGDClassifier` model vectorizer = HashingVectorizer(n_features=2**18) classifier = SGDClassifier() is_first_chunk = True for docs, labels in stream_data(file_path, chunk_size): X = vectorizer.transform(docs) y = np.array(labels) if is_first_chunk: classifier.partial_fit(X, y, classes=classes) is_first_chunk = False else: classifier.partial_fit(X, y) return classifier"},{"question":"# Question: Handling and Transforming Binary Data Given your knowledge of Pythons `struct` and `codecs` modules as described in the documentation, write a Python function that: 1. Takes a binary string representing multiple fields packed in a specific structure. 2. Unpacks the binary string into individual components based on the provided packed format. 3. Encodes one of these components using UTF-8 encoding. 4. Returns a new binary string where the encoded component has been replaced by its UTF-8 encoded version and all other components are combined back into a packed binary format. You must ensure that your solution works for a given format and handles any errors gracefully. Function Signature ```python def transform_binary_data(binary_data: bytes, format: str, encode_index: int) -> bytes: pass ``` Input - `binary_data` (bytes): The input binary string. - `format` (str): The format string that defines the structure of the packed binary data. - `encode_index` (int): The index of the component in the unpacked data that needs to be UTF-8 encoded. Output - Returns the modified binary string after replacing the specified component with its UTF-8 encoded version and repacking. Constraints - The `format` string always specifies a valid structure. - The `encode_index` is within the bounds of the unpacked data. - The data to be UTF-8 encoded is text and can be successfully encoded. Example ```python binary_data = struct.pack(\'ih5s\', 1, 2, b\'hello\') # Pack an integer, a short, and a string for example format = \'ih5s\' encode_index = 2 # Expected output: packed binary data with the 3rd element (\'hello\') encoded in UTF-8 and repacked output_binary_data = transform_binary_data(binary_data, format, encode_index) ``` Notes - You may use the `struct` module for packing and unpacking the binary data. - The `codecs` module may be used for encoding purposes. - Ensure the returned byte sequence is correctly formed and replaces only the specified component.","solution":"import struct import codecs def transform_binary_data(binary_data: bytes, format: str, encode_index: int) -> bytes: try: # Unpacking the binary data based on the provided format unpacked_data = struct.unpack(format, binary_data) # Converting the target component to UTF-8 encoded bytes to_encode = unpacked_data[encode_index] if isinstance(to_encode, bytes): to_encode = to_encode.decode() utf8_encoded = codecs.encode(to_encode, \'utf-8\') # Creating a new tuple with the replaced encoded component modified_data = list(unpacked_data) modified_data[encode_index] = utf8_encoded # Dynamically adjusting the format string to fit the new encoded component modified_format = format.replace(f\'5s\', f\'{len(utf8_encoded)}s\', 1) # Repacking the modified data into a binary string return struct.pack(modified_format, *modified_data) except struct.error as e: print(f\\"Error unpacking or packing binary data: {e}\\") return b\'\' except Exception as e: print(f\\"Unexpected error: {e}\\") return b\'\'"},{"question":"# Question You are required to demonstrate your knowledge of the Seaborn library by performing the following tasks. Ensure you carefully follow the specifications provided for each task. Tasks 1. **Load the Dataset** - Load the \\"titanic\\" dataset using Seaborn\'s `load_dataset` function. 2. **Create a Horizontal Boxplot** - Draw a horizontal boxplot for the \\"fare\\" column of the dataset. Customize the box color to a shade of green (`\\"#66c2a5\\"`), and set the linewidth of the box edges to `1.25`. 3. **Group by a Categorical Variable** - Generate a vertical boxplot that groups the \\"age\\" column data by the \\"class\\" column of the dataset. The whiskers should cover the full range of the data (from the minimum to the maximum). Additionally, use a red color for the box outlines and ensure the median line within the box is dashed. 4. **Nested Grouping with Hue** - Create a nested boxplot that groups the \\"age\\" column data by \\"class\\" and hue by \\"sex\\". Set a small gap of `0.2` between the boxes and remove grid lines in the plot. 5. **Preserve Native Scaling** - Draw a horizontal boxplot of the \\"fare\\" column data, grouped and rounded to the nearest 10 units. Draw a vertical line at `fare=50` in grey color with dash style (2,2). 6. **Advanced Customization Using Matplotlib Parameters** - Generate a boxplot with the following specifications: - Group the \\"age\\" column data by the \\"embarked\\" column. - Use notched boxes. - Remove the cap lines at the ends of the whiskers. - Set the flier (outlier) properties such that the marker is a triangle (`<`) and the color is blue. - Customize the box properties by setting the face color to yellow with an opacity of 0.7. - Set the line width of the median line inside the box to `1.5` and its color to magenta. Input and Output Format - **Input:** The input is the \\"titanic\\" dataset loaded using Seaborn. - **Output:** The output should consist of the specified visualizations, plotted using Seaborn. Constraints - You must use Seaborn for all visualizations. - Ensure proper customization as per the given specifications. Performance Requirements - Your solution should execute efficiently, without excessive computational complexity or memory usage. Example Here is a minimal example of how you might start one of the tasks: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Task 2: Horizontal boxplot sns.boxplot(x=titanic[\\"fare\\"], color=\\"#66c2a5\\", linewidth=1.25) plt.show() ``` Complete the remaining tasks following the specifications and methods demonstrated above.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset titanic = sns.load_dataset(\\"titanic\\") # Task 2: Create a Horizontal Boxplot def create_horizontal_boxplot(): plt.figure(figsize=(10, 5)) sns.boxplot(x=titanic[\'fare\'], color=\\"#66c2a5\\", linewidth=1.25) plt.xlabel(\\"Fare\\") plt.title(\\"Horizontal Boxplot of Titanic Fare\\") plt.show() # Task 3: Group by a Categorical Variable def create_vertical_boxplot_grouped_by_class(): plt.figure(figsize=(10, 5)) sns.boxplot(x=\'class\', y=\'age\', data=titanic, whis=[0, 100], boxprops={\\"color\\": \\"red\\"}, medianprops={\\"linestyle\\": \\"--\\", \\"color\\": \\"red\\"}) plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.title(\\"Boxplot of Age by Class\\") plt.show() # Task 4: Nested Grouping with Hue def create_nested_boxplot_grouped_by_class_and_sex(): plt.figure(figsize=(10, 5)) sns.boxplot(x=\'class\', y=\'age\', hue=\'sex\', data=titanic, dodge=True, width=0.5) plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.title(\\"Nested Boxplot of Age by Class and Sex\\") sns.despine() plt.show() # Task 5: Preserve Native Scaling def create_boxplot_with_native_scaling(): plt.figure(figsize=(10, 5)) fare_rounded = titanic[\'fare\'].round(-1) sns.boxplot(x=fare_rounded, orient=\\"h\\") plt.axvline(x=50, color=\'grey\', linestyle=\'--\', dashes=(2, 2)) plt.xlabel(\\"Fare (Rounded to Nearest 10)\\") plt.title(\\"Horizontal Boxplot of Titanic Fare with Rounding\\") plt.show() # Task 6: Advanced Customization Using Matplotlib Parameters def create_advanced_customized_boxplot(): plt.figure(figsize=(10, 5)) sns.boxplot(x=\'embarked\', y=\'age\', data=titanic, notch=True, showcaps=False, flierprops={\\"marker\\": \\"<\\", \\"color\\": \\"blue\\", \\"markersize\\": 10}, boxprops={\\"facecolor\\": \\"yellow\\", \\"alpha\\": 0.7}, medianprops={\\"linewidth\\": 1.5, \\"color\\": \\"magenta\\"}) plt.xlabel(\\"Embarked\\") plt.ylabel(\\"Age\\") plt.title(\\"Advanced Customized Boxplot of Age by Embarked\\") plt.show()"},{"question":"# Question You are given a dataset describing daily stock prices for a particular company. The dataset consists of the following columns: - `Date`: The date of the stock price (string format: YYYY-MM-DD) - `Open`: The price of the stock at market open (float) - `High`: The highest price of the stock during the day (float) - `Low`: The lowest price of the stock during the day (float) - `Close`: The price of the stock at market close (float) - `Volume`: The number of shares traded (integer) You need to write a function to perform the following tasks using pandas: 1. **Data Preparation**: Load the data into a pandas DataFrame and convert the `Date` column to a datetime type. 2. **Data Selection and Filtering**: - Extract and display the stock data for a specific month and year. 3. **Calculations**: - Calculate and add a new column `Daily Range` to the DataFrame, which is defined as the difference between `High` and `Low`. 4. **Grouping and Aggregation**: - Group the data by month and compute the average `Close` price and total `Volume` for each month. 5. **Output**: - Print the DataFrame after adding the `Daily Range` column. - Print the grouped summary DataFrame showing the average `Close` price and total `Volume` for each month. # Implementation You need to implement the function `analyze_stock_data(file_path: str, year: int, month: int)` where: - `file_path` (str) is the path to the CSV file containing the stock data. - `year` (int) is the year of the stock data you want to extract. - `month` (int) is the month of the stock data you want to extract. The function should: 1. Load the data from the CSV file. 2. Convert the `Date` column to datetime type. 3. Filter the data for the specified month and year. 4. Add a new column `Daily Range` to the DataFrame. 5. Group the data by month and compute the average `Close` price and total `Volume`. # Example Assume the CSV file `stock_data.csv` contains the following data: ``` Date,Open,High,Low,Close,Volume 2023-01-01,100,110,90,105,1500 2023-01-02,106,115,99,109,1600 2023-02-01,110,120,100,115,1700 2023-02-02,115,125,105,120,1800 ``` Calling `analyze_stock_data(\'stock_data.csv\', 2023, 1)` should output: - The DataFrame after adding the `Daily Range` column. - The grouped summary DataFrame showing the average `Close` price and total `Volume` for each month. # Constraints - You can assume the input data is always valid and well-formed. - The `Date` column in the CSV file is in the format YYYY-MM-DD. - Performance is not a primary concern for this dataset size.","solution":"import pandas as pd def analyze_stock_data(file_path: str, year: int, month: int): Load stock data, add \'Daily Range\' column, filter by month and year, group by month, and calculate average \'Close\' and total \'Volume\'. Args: - file_path (str): Path to the CSV file containing stock data - year (int): The year of the stock data to extract - month (int): The month of the stock data to extract Returns: - df_filtered (DataFrame): DataFrame for the specified month and year including \'Daily Range\' column - df_grouped (DataFrame): DataFrame grouped by month with average \'Close\' and total \'Volume\' # Load the dataset df = pd.read_csv(file_path) # Convert the \'Date\' column to datetime type df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Add \'Daily Range\' column df[\'Daily Range\'] = df[\'High\'] - df[\'Low\'] # Filter data for the specified month and year df_filtered = df[(df[\'Date\'].dt.year == year) & (df[\'Date\'].dt.month == month)] # Group by month and compute the average \'Close\' price and total \'Volume\' df[\'YearMonth\'] = df[\'Date\'].dt.to_period(\'M\') df_grouped = df.groupby(\'YearMonth\').agg({\'Close\': \'mean\', \'Volume\': \'sum\'}).reset_index() # Print the filtered DataFrame with \'Daily Range\' print(\\"Filtered DataFrame:\\") print(df_filtered) # Print the grouped summary DataFrame print(\\"nGrouped Summary DataFrame:\\") print(df_grouped) return df_filtered, df_grouped"},{"question":"You are given two datasets: `penguins` and `planets`, which are built-in datasets accessible through Seaborn. The goal is to demonstrate your comprehension of Seaborn\'s histplot functionality by creating customized visualizations. Follow the instructions below carefully to complete the task. 1. **Load the Datasets** - Load the `penguins` dataset. - Load the `planets` dataset. 2. **Customized Univariate Histogram** - For the `penguins` dataset: - Create a histogram for the `flipper_length_mm` variable with the following specifications: - Set the number of bins to `25`. - Add a kernel density estimate (KDE) line. - Color each species differently using the `hue` parameter. - Use the `step` element to visualize the data. - Add a title: \\"Penguins Flipper Length Distribution\\". 3. **Customized Bivariate Histogram** - For the `penguins` dataset: - Create a bivariate histogram using `bill_depth_mm` vs. `body_mass_g`. - Use different colors to differentiate the species. - Apply a logarithmic scale to the y-axis. - Add a color bar to the plot. - Add a title: \\"Bivariate Histogram of Bill Depth vs Body Mass\\". 4. **Logarithmic Scaling** - For the `planets` dataset: - Create a histogram of the `distance` variable. - Use logarithmic scaling for the x-axis. - Display cumulative counts using a step function. - Add a title: \\"Distance Distribution of Planets (Log Scale)\\". # Constraints: - Ensure that all visualizations are clear and aesthetically pleasing. - Use appropriate labels and titles for all plots. - Implement the solution using appropriate Seaborn functions and parameters as discussed in the documentation. # Submission - Your final submission should be a single Jupyter Notebook file containing all the visualizations. - Ensure that each cell contains only the code relevant to the section it addresses. - Add brief comments explaining each step for clarity. **Dataset Loading Code:** ```python import seaborn as sns # Load datasets penguins = sns.load_dataset(\\"penguins\\") planets = sns.load_dataset(\\"planets\\") ``` **Expected Output:** - Three plots meeting the specifications outlined above. - Proper titles, labels, and visual differentiations as required by the question.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load datasets penguins = sns.load_dataset(\\"penguins\\") planets = sns.load_dataset(\\"planets\\") # Function to create customized histograms and plots def plot_penguins_flipper_length(): plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", bins=25, kde=True, hue=\\"species\\", element=\\"step\\", palette=\\"Set2\\") plt.title(\\"Penguins Flipper Length Distribution\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() def plot_penguins_bivariate_histogram(): plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", palette=\\"Set2\\", cbar=True, log_scale=(False, True)) plt.title(\\"Bivariate Histogram of Bill Depth vs Body Mass\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() def plot_planets_distance(): plt.figure(figsize=(10, 6)) sns.histplot(data=planets, x=\\"distance\\", log_scale=True, cumulative=True, element=\\"step\\") plt.title(\\"Distance Distribution of Planets (Log Scale)\\") plt.xlabel(\\"Distance (log scale)\\") plt.ylabel(\\"Cumulative Count\\") plt.show()"},{"question":"**Coding Assessment Question** **Objective:** Demonstrate understanding of the `zipfile` module by creating and manipulating ZIP archives. **Task:** Implement a function `manage_zip_archive` that performs the following operations: 1. Creates a new ZIP archive with the given name. 2. Adds a list of provided files to the archive. 3. Extracts all files from the archive to a specified directory. 4. Lists all the files that were just extracted (with their names and sizes). 5. Tests the integrity of the ZIP archive and reports the name of the first corrupted file, if any. **Function Signature:** ```python def manage_zip_archive(zip_filename: str, file_list: list, extract_dir: str) -> dict: Manage a ZIP Archive by performing various operations. Parameters: - zip_filename: str - The name of the ZIP file to be created. - file_list: list - A list of paths to files that need to be added to the ZIP archive. - extract_dir: str - The directory where the files need to be extracted to. Returns: - dict - A dictionary containing: * \'extracted_files\': List of tuples with (filename, size) of each extracted file. * \'corrupted_file\': Name of the first corrupted file found during testing, or None if all files are intact. pass ``` **Input Constraints:** - `zip_filename` must be a valid filename string. - `file_list` should be a list of valid file paths. Each file path present in `file_list` must point to an existing file. - `extract_dir` must be a valid directory path. If it does not exist, the function should create it. **Output:** - Returns a dictionary with: - `\'extracted_files\'`: List of tuples where each tuple contains the filename and its size in bytes after extraction. - `\'corrupted_file\'`: The name of the first corrupted file found during the integrity check, or `None` if all files are intact. **Example Usage:** ```python file_list = [\'file1.txt\', \'file2.txt\', \'script.py\'] result = manage_zip_archive(\'archive.zip\', file_list, \'extracted_files\') print(result) # Expected Output (Assuming no corrupt files): # { # \'extracted_files\': [(\'file1.txt\', 120), (\'file2.txt\', 350), (\'script.py\', 558)], # \'corrupted_file\': None # } ``` **Notes:** 1. Ensure you handle exceptions gracefully, and provide meaningful error messages where applicable. 2. Make use of context managers where appropriate to manage file resources correctly. 3. Consider edge cases such as empty `file_list` or non-existing paths in `file_list`.","solution":"import os import zipfile def manage_zip_archive(zip_filename: str, file_list: list, extract_dir: str) -> dict: Manage a ZIP Archive by performing various operations. Parameters: - zip_filename: str - The name of the ZIP file to be created. - file_list: list - A list of paths to files that need to be added to the ZIP archive. - extract_dir: str - The directory where the files need to be extracted to. Returns: - dict - A dictionary containing: * \'extracted_files\': List of tuples with (filename, size) of each extracted file. * \'corrupted_file\': Name of the first corrupted file found during testing, or None if all files are intact. result = { \'extracted_files\': [], \'corrupted_file\': None } # Create the zip file with zipfile.ZipFile(zip_filename, \'w\') as zipf: for file in file_list: if os.path.isfile(file): zipf.write(file, os.path.basename(file)) else: raise ValueError(f\\"File {file} does not exist or is not a valid file.\\") # Extract the files if not os.path.exists(extract_dir): os.makedirs(extract_dir) with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extractall(extract_dir) for file_info in zipf.infolist(): extracted_file_path = os.path.join(extract_dir, file_info.filename) extracted_file_size = os.path.getsize(extracted_file_path) result[\'extracted_files\'].append((file_info.filename, extracted_file_size)) # Check for corruption corrupted_file = zipf.testzip() result[\'corrupted_file\'] = corrupted_file return result"},{"question":"# Question: Implementing Traceback Formatting Using \\"cgitb\\" You are required to write a Python function that takes a string specifying the desired format for tracebacks (\'html\' or \'text\') and an exceptional scenario to handle. The function should trigger an exception manually and display the traceback in the specified format using the \\"cgitb\\" module. Additionally, include an option to save the output to a file. Function Signature ```python def show_traceback(format: str, save_to_file: bool = False, filename: str = None): # Your implementation here ``` Input - `format` (str): The desired format for the traceback (\'html\' or \'text\'). - `save_to_file` (bool, optional): A boolean indicating whether to save the output to a file. Defaults to False. - `filename` (str, optional): The name of the file to save the output if `save_to_file` is True. Defaults to None. Output - The function should not return anything but should print the traceback to the screen or save to a file depending on the parameters. Constraints - The `format` parameter must be either \'html\' or \'text\'. - If `save_to_file` is True, `filename` must be provided. Example Usage ```python # Display traceback in HTML format show_traceback(\'html\') # Display traceback in text format show_traceback(\'text\') # Save traceback in HTML format to \'traceback.html\' show_traceback(\'html\', save_to_file=True, filename=\'traceback.html\') # Save traceback in text format to \'traceback.txt\' show_traceback(\'text\', save_to_file=True, filename=\'traceback.txt\') ``` Notes - You can manually trigger an exception, such as a ZeroDivisionError, for demonstration purposes. - Ensure proper error handling where necessary.","solution":"import cgitb import sys def show_traceback(format: str, save_to_file: bool = False, filename: str = None): if format not in [\'html\', \'text\']: raise ValueError(\\"The format must be either \'html\' or \'text\'.\\") if save_to_file and not filename: raise ValueError(\\"Filename must be provided if save_to_file is True.\\") if format == \'html\': cgitb.enable(format=format, display=not save_to_file, logdir=\'.\' if save_to_file else None) else: # format == \'text\' cgitb.enable(format=format, display=not save_to_file) try: # Manually trigger an exception for demonstration 1 / 0 except Exception: if save_to_file: with open(filename, \'w\') as f: sys.stdout = f cgitb.handler() sys.stdout = sys.__stdout__ # Reset to default else: cgitb.handler()"},{"question":"# Web Content Fetcher and URL Builder Using the `urllib` package, you are required to implement a function `fetch_and_parse_url` that performs the following tasks: 1. **Fetch Content**: - Reads the content from a given URL. - Handles potential errors during the fetching process (e.g., handling `HTTPError`, `URLError`). 2. **Parse URL**: - Takes the fetched URL and parses its components (scheme, netloc, path, params, query, fragment). - Reconstructs the URL from these components. 3. **Output**: - Returns a dictionary containing two key-value pairs: - `\\"content\\"`: The content of the URL as a decoded string. - `\\"parsed_url\\"`: The reconstructed URL string from its parsed components. # Function Signature ```python def fetch_and_parse_url(url: str) -> dict: pass ``` # Input - `url` (str): A string representing the URL to be fetched and parsed. # Output - dict: A dictionary with the following structure: ```python { \\"content\\": \\"content of the URL\\", \\"parsed_url\\": \\"reconstructed URL\\" } ``` # Constraints - You should use the `urllib` package exclusively for handling URLs. - The function should handle common URL errors gracefully. - Reconstructed URL must be exactly the same as the original URL. # Example ```python url = \'http://httpbin.org/get?arg=value\' result = fetch_and_parse_url(url) # The result should be: # { # \\"content\\": \\"[Content of the URL]\\", # \\"parsed_url\\": \\"http://httpbin.org/get?arg=value\\" # } ``` Notes To test your function, consider using freely available services that return HTTP response data, such as http://httpbin.org. # Tips - Utilize `urllib.request.urlopen` for fetching the URL content. - Use `urllib.parse.urlparse` to split the URL into its components. - Reconstruct the URL using `urllib.parse.urlunparse`.","solution":"import urllib.request import urllib.parse import urllib.error def fetch_and_parse_url(url: str) -> dict: Fetches content from the URL and parses it into its components. Returns a dictionary with the content and the reconstructed URL. try: # Fetching content from the URL with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') # Parsing the URL into components parsed_url = urllib.parse.urlparse(url) # Reconstructing the URL reconstructed_url = urllib.parse.urlunparse(parsed_url) return { \\"content\\": content, \\"parsed_url\\": reconstructed_url } except urllib.error.HTTPError as e: return {\\"content\\": f\\"HTTPError: {e.reason}\\", \\"parsed_url\\": \\"\\"} except urllib.error.URLError as e: return {\\"content\\": f\\"URLError: {e.reason}\\", \\"parsed_url\\": \\"\\"}"},{"question":"# Problem: Managing Serialized Tensors and Modules in PyTorch You are tasked with creating a PyTorch module that handles two types of tensors: large tensors and their corresponding views. Your module should be able to save these tensors, ensuring efficient storage by minimizing file size while preserving view relationships. Additionally, you will need to save and load the module\'s state dict properly. Requirements: 1. **Tensor Creation and Management**: - Create a tensor `large_tensor` of size 1000 containing values from 1 to 1000. - Create a view of `large_tensor` called `sub_tensor` that contains the first 10 elements. 2. **Module Definition**: - Define a PyTorch `nn.Module` named `MyViewModule` containing the tensors above as attributes. 3. **Serialization**: - Save both `large_tensor` and `sub_tensor` while ensuring: - The file size is minimized by not saving the full storage unnecessarily. - The view relationship is preserved when loaded back. - Save the state dict of the module. 4. **Deserialization**: - Load the tensors and verify that: - The view relationship between `large_tensor` and `sub_tensor` is maintained. - The storage size is minimized to reflect the actual number of elements in `sub_tensor`. - Load the module\'s state dict and verify correct restoration. Instructions: 1. Implement the `MyViewModule` class. 2. Implement functions `save_tensors(module, large_tensor, sub_tensor, tensor_path, state_dict_path)` and `load_tensors(tensor_path, state_dict_path)` to handle the saving and loading of tensors and module state dict respectively. 3. Verify that the size of the saved storage matches the intent of optimized storage. 4. Ensure that the view relationship between `large_tensor` and `sub_tensor` is retained after loading. Code Template: ```python import torch import torch.nn as nn class MyViewModule(nn.Module): def __init__(self): super(MyViewModule, self).__init__() self.large_tensor = torch.arange(1, 1001) self.sub_tensor = self.large_tensor[:10] def save_tensors(module, large_tensor, sub_tensor, tensor_path, state_dict_path): # Clone the sub_tensor to minimize storage size sub_tensor_clone = sub_tensor.clone() torch.save([large_tensor, sub_tensor_clone], tensor_path) torch.save(module.state_dict(), state_dict_path) def load_tensors(tensor_path, state_dict_path): loaded_tensors = torch.load(tensor_path) loaded_large_tensor, loaded_sub_tensor = loaded_tensors # Reconstruct the view relationship loaded_large_tensor[:10].copy_(loaded_sub_tensor) loaded_module = MyViewModule() loaded_module.load_state_dict(torch.load(state_dict_path)) return loaded_large_tensor, loaded_sub_tensor, loaded_module if __name__ == \\"__main__\\": module = MyViewModule() tensor_path = \\"tensors.pt\\" state_dict_path = \\"module_state.pt\\" save_tensors(module, module.large_tensor, module.sub_tensor, tensor_path, state_dict_path) loaded_large_tensor, loaded_sub_tensor, loaded_module = load_tensors(tensor_path, state_dict_path) # Check if the size of the stored tensor is minimized assert loaded_sub_tensor.storage().size() == loaded_sub_tensor.numel() # Check if the view relationship is maintained loaded_sub_tensor *= 2 assert torch.equal(loaded_large_tensor[:10], loaded_sub_tensor) print(\\"Serialization and Deserialization succeeded.\\") ``` Constraints: - Ensure that the size of the saved storage for sub_tensor does not exceed its actual number of elements. - The loaded tensors should maintain their view relationships correctly.","solution":"import torch import torch.nn as nn class MyViewModule(nn.Module): def __init__(self): super(MyViewModule, self).__init__() self.large_tensor = torch.arange(1, 1001) self.sub_tensor = self.large_tensor[:10] def save_tensors(module, large_tensor, sub_tensor, tensor_path, state_dict_path): # Clone the sub_tensor to minimize storage size sub_tensor_clone = sub_tensor.clone() torch.save([large_tensor, sub_tensor_clone], tensor_path) torch.save(module.state_dict(), state_dict_path) def load_tensors(tensor_path, state_dict_path): loaded_tensors = torch.load(tensor_path) loaded_large_tensor, loaded_sub_tensor = loaded_tensors # Reconstruct the view relationship loaded_large_tensor[:10].copy_(loaded_sub_tensor) loaded_module = MyViewModule() loaded_module.load_state_dict(torch.load(state_dict_path)) return loaded_large_tensor, loaded_sub_tensor, loaded_module"},{"question":"**Objective**: Demonstrate your ability to generate and customize error bars using seaborn. Given a dataset of exam scores from multiple classes, your task is to visualize these scores using seaborn\'s error bars. The dataset consists of two columns: `class` and `score`. Implement a function that takes in a pandas DataFrame and visualization parameters, and outputs a seaborn plot visualizing the data with different error bars configurations including custom configurations. Function Signature ```python def visualize_exam_scores(df: pd.DataFrame, errorbar_type: str, scale: float = 1.0, n_boot: int = 1000, seed: int = None) -> None: # your code here ``` Input 1. `df`: A pandas DataFrame with columns \\"class\\" and \\"score\\". 2. `errorbar_type`: A string that specifies the type of error bar to use. Valid options are \\"sd\\" (standard deviation), \\"se\\" (standard error), \\"pi\\" (percentile interval), \\"ci\\" (confidence interval), or \\"custom\\" for a user-defined function. 3. `scale` (optional): A float that specifies the scaling parameter for the error bars when applicable. Default is 1.0. 4. `n_boot` (optional): An integer specifying the number of bootstrap resamples to use for bootstrapping-based intervals. Default is 1000. Applies only to \\"ci\\" error bars. 5. `seed` (optional): An integer setting the random seed for reproducibility. Applies only to bootstrapping. Output - Generates a seaborn plot visualizing `score` for each `class` with appropriate error bars as specified. Constraints - You are only allowed to use seaborn, matplotlib, and pandas libraries. - The DataFrame `df` will always contain the necessary columns. - Performance only needs to be optimized for typical small to medium datasets. Instructions 1. Use different colors or markers for different classes to make the plot distinguish clear categories. 2. Ensure your function accounts for the different error bar types and scales when applicable. 3. Implement a custom error bar that displays the min and max scores for each class when `errorbar_type` is set to \\"custom\\". 4. For \\"ci\\" errorbar type, ensure reproducibility by setting the random seed if provided. Example ```python import pandas as pd # Example DataFrame data = { \\"class\\": [\\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\", \\"C\\"], \\"score\\": [78, 85, 90, 82, 79, 88, 95, 91, 93] } df = pd.DataFrame(data) # Example Function Call visualize_exam_scores(df, errorbar_type=\\"ci\\", scale=1.0, n_boot=2000, seed=42) ``` This example should generate a seaborn plot visualizing the exam scores with error bars representing the confidence interval, using 2000 bootstrap resamples and a random seed for reproducibility.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_exam_scores(df: pd.DataFrame, errorbar_type: str, scale: float = 1.0, n_boot: int = 1000, seed: int = None) -> None: Visualizes exam scores with error bars. Args: - df: A pandas DataFrame with columns \\"class\\" and \\"score\\". - errorbar_type: A string specifying the type of error bar (\\"sd\\", \\"se\\", \\"pi\\", \\"ci\\", \\"custom\\"). - scale: A float for scaling parameter of error bars. Default is 1.0. - n_boot: An integer for number of bootstrap resamples. Default is 1000. - seed: An integer for setting the random seed. Default is None. if errorbar_type == \'custom\': def custom_errorbars(x): return (x.min(), x.max()) errorbar = custom_errorbars else: errorbar = errorbar_type sns.catplot( data=df, x=\\"class\\", y=\\"score\\", kind=\\"point\\", errorbar=errorbar, scale=scale, n_boot=n_boot, seed=seed ) plt.title(\\"Exam Scores with Error Bars\\") plt.show()"},{"question":"# Question: Implement a Custom C Extension to Parse and Return Python Arguments As a Python developer, you are tasked with creating a custom C extension function that demonstrates the ability to parse Python arguments and return structured responses. Problem Statement Implement a C extension function named `parse_and_echo` that takes a mix of different types of Python arguments, processes them according to specified formats, and returns a structured response. The function should: 1. Parse the following positional arguments: - A Unicode string (`str`) - A non-negative integer (`int`), within the limits of `unsigned int` - A byte string (`bytes`) - An optional float (default value is `1.0` if not provided) 2. Parse the following keyword-only arguments: - A list of integers (`list of int`), defaults to an empty list if not provided - An optional boolean flag (default value is `False`) 3. Return a structured Python dictionary containing: - The length of the Unicode string - The uppercase version of the byte string - The sum of all integers in the provided list - A message indicating if the optional boolean flag was set to `True` or `False` Detailed Requirements - **Input**: - A Unicode string representing a name. - A non-negative integer representing an ID. - A byte string representing a data blob. - An optional float representing a ratio. - An optional list of integers representing extra values. - An optional boolean flag. - **Output**: - A Python dictionary with: - `name_length`: The length of the provided Unicode string. - `upper_data`: The uppercase version of the provided byte string. - `sum_list`: The sum of all integers in the provided list. - `flag_message`: A message indicating if the optional boolean flag was set. Constraints - The byte string must not contain embedded NUL bytes; raise a `ValueError` if it does. - The integer should be non-negative; raise a `ValueError` for negative values. - Float default value is `1.0`. - The list elements should be valid integers. Example Usage ```python >>> import myextension >>> result = myextension.parse_and_echo(\\"hello\\", 42, b\\"data\\", float=2.5, values=[1, 2, 3], flag=True) >>> print(result) { \'name_length\': 5, \'upper_data\': b\'DATA\', \'sum_list\': 6, \'flag_message\': \'Flag is True\' } ``` C Function Signature ```c static PyObject *parse_and_echo(PyObject *self, PyObject *args, PyObject *kwds) ``` Ensure that the function is correctly documenting and verifying the parsed arguments using appropriate error handling within the C extension.","solution":"from setuptools import setup, Extension def parse_and_echo(name, id, data, ratio=1.0, values=None, flag=False): This function takes a mix of different types of Python arguments, processes them, and returns a structured response. :param name: A Unicode string :param id: A non-negative integer (unsigned int) :param data: A byte string :param ratio: An optional float (default: 1.0) :param values: An optional list of integers (default: empty list) :param flag: An optional boolean flag (default: False) :return: A structured Python dictionary if not isinstance(name, str): raise TypeError(\\"name must be a Unicode string\\") if not isinstance(id, int) or id < 0: raise ValueError(\\"id must be a non-negative integer\\") if not isinstance(data, bytes) or b\'0\' in data: raise ValueError(\\"data must be a byte string without NUL bytes\\") if values is None: values = [] if not all(isinstance(i, int) for i in values): raise TypeError(\\"values must be a list of integers\\") name_length = len(name) upper_data = data.upper() sum_list = sum(values) flag_message = \\"Flag is True\\" if flag else \\"Flag is False\\" return { \\"name_length\\": name_length, \\"upper_data\\": upper_data, \\"sum_list\\": sum_list, \\"flag_message\\": flag_message }"},{"question":"Coding Assessment Question # Context In this task, you are required to create a simple version of a Python read-eval-print loop (REPL) using the `codeop` module. The REPL should be able to read input lines of Python code, evaluate whether each input is a complete statement or part of a multi-line block, and execute the code if it forms a complete statement. # Objective Implement a function `simple_repl()` that simulates a minimal REPL environment. # Function Signature ```python def simple_repl(): pass ``` # Input and Output - **Input**: The function will read lines of Python code from the standard input until the user enters a blank line (just pressing Enter without typing any code). - **Output**: The function should print the output of evaluated expressions or any errors encountered during the compilation or execution process. # Constraints 1. The REPL should recognize whether the entered code is a complete single-line statement or part of a multi-line block. 2. Handle `__future__` statements by remembering them and ensuring they are in effect for subsequent code. 3. Use `codeop.compile_command` to compile the code and decide if the input requires continuation. 4. Use `exec()` to execute the code objects. # Example Here is an example of how the REPL should operate in an interactive session: ``` >>> simple_repl() >>> x = 1 # (User input) >>> y = 2 # (User input) >>> x + y # (User input) 3 >>> def add(a, b): # (User input) ... return a + b # Continuation because it\'s part of a multi-line block >>> add(x, y) # (User input) 3 >>> from __future__ import print_function # (User input) >>> print(\\"Hello, World!\\") # (User input, should handle the future import) Hello, World! >>> # (User input, empty line to terminate) ``` # Notes - When using `input()` to capture user input, make sure to strip any leading or trailing whitespace. - If the input line is empty (just a newline), the function should exit the REPL loop. - Print Python exceptions in case of syntax or runtime errors. # Your Task Implement the `simple_repl()` function according to the above specifications.","solution":"import codeop def simple_repl(): buffer = \'\' while True: try: if buffer == \'\': line = input(\'>>> \') else: line = input(\'... \') if line.strip() == \'\': break buffer += line + \'n\' code = codeop.compile_command(buffer) if code is None: continue exec(code, globals(), locals()) buffer = \'\' except Exception as e: print(f\\"Error: {e}\\") buffer = \'\' except KeyboardInterrupt: print(\\"nKeyboardInterrupt\\") buffer = \'\' except EOFError: print(\\"nEOFError\\") break"},{"question":"# PyTorch Subprocess Handling Assessment Objective Implement a PyTorch function that utilizes the `SubprocessHandler` from `torch.distributed.elastic.multiprocessing.subprocess_handler` to run a simple distributed task across multiple subprocesses. This will assess your understanding of PyTorch distributed multiprocessing and subprocess management. Problem Statement You are required to implement a function `run_distributed_task(num_tasks: int) -> List[str]` that uses `SubprocessHandler` to launch `num_tasks` subprocesses, each running a task that simply returns \\"Task {i} completed\\" where `{i}` is the task number (0-based indexing). The function should: 1. Create the specified number of subprocesses. 2. Use `SubprocessHandler` to manage each subprocess. 3. Collect the results from each subprocess. 4. Return the results as a list of strings in the order the subprocesses were started. Input - `num_tasks`: An integer specifying the number of subprocesses to create. Output - A list of strings where each string indicates the completion of a respective task, i.e., `[\\"Task 0 completed\\", \\"Task 1 completed\\", ..., \\"Task {num_tasks - 1} completed\\"]`. Constraints 1. Use PyTorchs `SubprocessHandler` for managing the subprocesses. 2. Ensure that the function handles the parallel execution correctly and collects the results in the correct order. 3. Assume a well-configured PyTorch distributed environment. Example ```python output = run_distributed_task(3) print(output) # Output should be: [\\"Task 0 completed\\", \\"Task 1 completed\\", \\"Task 2 completed\\"] ``` Additional Information - You may refer to the [PyTorch elastic multiprocessing subprocess handler documentation](https://pytorch.org/docs/stable/elastic.multiprocessing.subprocess_handler.html) for understanding the API and its methods. - Make sure to handle potential exceptions and ensure that each subprocess completes successfully. Good luck!","solution":"import torch from torch.multiprocessing import Process, Queue def task_function(task_id, queue): result = f\\"Task {task_id} completed\\" queue.put((task_id, result)) def run_distributed_task(num_tasks): queue = Queue() processes = [] results = [None] * num_tasks # Start subprocesses for i in range(num_tasks): p = Process(target=task_function, args=(i, queue)) p.start() processes.append(p) # Collect results from subprocesses for _ in range(num_tasks): task_id, result = queue.get() results[task_id] = result # Ensure all processes have finished for p in processes: p.join() return results"},{"question":"# Secure Login System Using `getpass` Module You are to implement a simple and secure login system in Python using the `getpass` module. This system should perform the following tasks: 1. **Create a users database**: - Create a dictionary that stores usernames as keys and their corresponding hashed passwords as values. For simplicity, use the following predefined users and their hashed passwords (you do not need to implement hashing, just use the provided hashed passwords): ```python users_db = { \'user1\': \'5f4dcc3b5aa765d61d8327deb882cf99\', # password: password \'user2\': \'098f6bcd4621d373cade4e832627b4f6\', # password: test \'admin\': \'21232f297a57a5a743894a0e4a801fc3\' # password: admin } ``` 2. **Get the current user login name**: - Use the `getpass.getuser()` function to get the current user\'s login name. 3. **Prompt the user for their password**: - Use the `getpass.getpass()` function to securely prompt the user for their password without echoing. 4. **Validate the login**: - Check if the username from `getpass.getuser()` exists in the users database. - If the username exists, hash the entered password and compare it with the stored hashed password (you can use the `hashlib.md5` function to hash the password). - If the login is successful, print a message, \\"Login successful!\\". - If the login fails, print a message, \\"Login failed!\\". # Requirements - The `getpass` module should be used to get both the username and the password. - Hash the provided password using the `hashlib.md5` function before comparing it with the stored hashed password. # Example Usage ```python import hashlib import getpass # Predefined users database users_db = { \'user1\': \'5f4dcc3b5aa765d61d8327deb882cf99\', # password: password \'user2\': \'098f6bcd4621d373cade4e832627b4f6\', # password: test \'admin\': \'21232f297a57a5a743894a0e4a801fc3\' # password: admin } def login_system(): # Get the current user\'s login name username = getpass.getuser() print(f\\"Login as: {username}\\") # Prompt the user for their password password = getpass.getpass(\\"Password: \\") # Validate the login if username in users_db: if hashlib.md5(password.encode()).hexdigest() == users_db[username]: print(\\"Login successful!\\") else: print(\\"Login failed!\\") else: print(\\"Login failed!\\") # Example run (Note: Replace this line with actual code usage) login_system() ``` # Notes - Ensure your solution handles possible edge cases, such as empty password input. - It\'s important to follow the security aspect and avoid printing sensitive information, such as the password.","solution":"import hashlib import getpass # Predefined users database users_db = { \'user1\': \'5f4dcc3b5aa765d61d8327deb882cf99\', # password: password \'user2\': \'098f6bcd4621d373cade4e832627b4f6\', # password: test \'admin\': \'21232f297a57a5a743894a0e4a801fc3\' # password: admin } def login_system(): # Get the current user\'s login name username = getpass.getuser() print(f\\"Login as: {username}\\") # Prompt the user for their password password = getpass.getpass(\\"Password: \\") # Validate the login if username in users_db: if hashlib.md5(password.encode()).hexdigest() == users_db[username]: print(\\"Login successful!\\") else: print(\\"Login failed!\\") else: print(\\"Login failed!\\")"},{"question":"# Seaborn: Scatter Plot Customization and Analysis You are given a dataset containing information about various products sold by a company. The dataset includes the following columns: - `product_id`: A unique identifier for each product. - `category`: The category to which the product belongs. - `price`: The price of the product. - `rating`: Customer rating of the product (out of 5). - `sales`: Number of units sold. You need to create a scatter plot to analyze the relationship between `price` and `sales`, while incorporating additional information about `category` and `rating`. Task 1. **Load the dataset** from the provided URL: `\\"https://your-example-dataset-url.com/products.csv\\"`. 2. **Create a scatter plot** where: - `price` is on the x-axis. - `sales` is on the y-axis. - Points are colored by the `category` of the product. - Points vary in size based on the `rating` of the product. 3. **Customize the plot** by: - Using a seaborn theme of your choice. - Providing a meaningful title to the plot. - Ensuring all points are clearly visible and distinguishable. - Adding a legend to describe the plot. Expected Input - The data should be loaded from the specified URL. - You do not need to worry about cleaning the dataset; assume it is clean and ready to use. Expected Output - A scatter plot saved as an image file, depicting the relationship between `price` and `sales` with points colored by `category` and sized by `rating`. Here is a starter code template for your implementation: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Step 1: Load the dataset url = \\"https://your-example-dataset-url.com/products.csv\\" data = pd.read_csv(url) # Step 2: Create the scatter plot plt.figure(figsize=(10, 6)) plot = sns.scatterplot( data=data, x=\\"price\\", y=\\"sales\\", hue=\\"category\\", size=\\"rating\\", sizes=(20, 200), # Adjust the range of marker sizes alpha=0.7, # Adjust transparency for better visibility ) # Step 3: Customize the plot plt.title(\\"Relationship Between Product Price and Sales\\") plt.xlabel(\\"Price\\") plt.ylabel(\\"Sales\\") plt.legend(title=\\"Category and Rating\\") plt.grid(True) # Save the plot as an image plt.savefig(\\"scatter_plot.png\\") # Show the plot plt.show() ``` **Note:** Ensure your plot is well-organized and clearly conveys the information. Focus on readability and clarity in your customizations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_and_plot_data(url): # Step 1: Load the dataset data = pd.read_csv(url) # Step 2: Create the scatter plot plt.figure(figsize=(10, 6)) plot = sns.scatterplot( data=data, x=\\"price\\", y=\\"sales\\", hue=\\"category\\", size=\\"rating\\", sizes=(20, 200), # Adjust the range of marker sizes alpha=0.7, # Adjust transparency for better visibility ) # Step 3: Customize the plot plt.title(\\"Relationship Between Product Price and Sales\\") plt.xlabel(\\"Price\\") plt.ylabel(\\"Sales\\") plt.legend(title=\\"Category and Rating\\") plt.grid(True) # Save the plot as an image plt.savefig(\\"scatter_plot.png\\") # Show the plot plt.show()"},{"question":"**Coding Assessment Question** # Objective Write a Python function that fetches and processes data from a given URL. The function should handle potential errors gracefully and extract specific information from the URL. # Task Implement the following function: ```python def fetch_and_parse_url(url: str) -> dict: Fetches data from the given URL, handles potential errors, and returns parsed information. Args: url (str): The URL to be accessed. Returns: dict: A dictionary containing the following keys: - \'status_code\': The HTTP status code of the response. - \'page_content\': The content of the webpage as a string (if the request is successful). - \'error\': An error message (if any error occurs during the request). - \'parsed_url\': A dictionary containing components of the URL (scheme, netloc, path, params, query, fragment). Constraints: - Only standard Python libraries (urllib and its submodules) should be used. - Network errors should be handled and reported. pass ``` # Input - A single string representing a URL. # Output - A dictionary with the following structure: - `status_code`: HTTP status code (int). - `page_content`: Content of the page as a string (str). This key should only be present if the request is successful. - `error`: Error message (str). This key should only be present if an error occurs during the request. - `parsed_url`: Dictionary of URL components with the following structure: - `scheme`: URL scheme (str). - `netloc`: Network location (str). - `path`: Hierarchical path (str). - `params`: Parameters for last path element (str). - `query`: Query component (str). - `fragment`: Fragment identifier (str). # Constraints - Network errors such as `URLError` and `HTTPError` should be correctly caught and reported through the `error` key. - The function should correctly parse and deconstruct the given URL using the `urllib.parse` module. # Example ```python url = \\"https://example.com/path/to/resource?query=example#fragment\\" result = fetch_and_parse_url(url) # The result dictionary should be similar to: { \'status_code\': 200, \'page_content\': \'<!doctype html><html>...</html>\', \'parsed_url\': { \'scheme\': \'https\', \'netloc\': \'example.com\', \'path\': \'/path/to/resource\', \'params\': \'\', \'query\': \'query=example\', \'fragment\': \'fragment\' } } ``` # Notes - Ensure your function handles various edge cases such as non-existent domains, server errors, and malformed URLs. - The page content should be safely decoded into a string format.","solution":"import urllib.request import urllib.error import urllib.parse def fetch_and_parse_url(url: str) -> dict: Fetches data from the given URL, handles potential errors, and returns parsed information. Args: url (str): The URL to be accessed. Returns: dict: A dictionary containing the following keys: - \'status_code\': The HTTP status code of the response. - \'page_content\': The content of the webpage as a string (if the request is successful). - \'error\': An error message (if any error occurs during the request). - \'parsed_url\': A dictionary containing components of the URL (scheme, netloc, path, params, query, fragment). result = {} # Parsing the URL parsed_url = urllib.parse.urlparse(url) result[\'parsed_url\'] = { \'scheme\': parsed_url.scheme, \'netloc\': parsed_url.netloc, \'path\': parsed_url.path, \'params\': parsed_url.params, \'query\': parsed_url.query, \'fragment\': parsed_url.fragment } try: with urllib.request.urlopen(url) as response: result[\'status_code\'] = response.getcode() result[\'page_content\'] = response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: result[\'status_code\'] = e.code result[\'error\'] = str(e) except urllib.error.URLError as e: result[\'error\'] = str(e) return result"},{"question":"# Parallelism in Scikit-learn: Implementing and Managing Parallel Processing You are tasked with writing a Python function that demonstrates the use of parallelism in scikit-learn. The function should: 1. Train a machine learning model using scikit-learn on a given dataset. 2. Use higher-level parallelism with joblib to parallelize the training process. 3. Use lower-level parallelism with OpenMP to parallelize underlying Cython operations. 4. Measure and compare the training time with different parallelism configurations. # Requirements: 1. Load the `digits` dataset from `sklearn.datasets`. 2. Split the dataset into training and testing sets using `train_test_split`. 3. Train a `RandomForestClassifier` using parallelism managed by joblib (`n_jobs` parameter). 4. Experiment with different values of `n_jobs` (e.g., 1, 2, and the number of logical cores) to measure the impact on training time. 5. Additionally, set the `OMP_NUM_THREADS` environment variable to control the number of threads used by OpenMP. 6. Print the training times for each configuration. # Input and Output Formats: Function Signature: ```python def evaluate_parallelism(n_jobs_list: list, omp_num_threads_list: list) -> None: ``` Input: - `n_jobs_list`: A list of integers representing the different values for the `n_jobs` parameter. - `omp_num_threads_list`: A list of integers representing the different values for the `OMP_NUM_THREADS` environment variable. Output: - The function should print the training time for each combination of `n_jobs` and `OMP_NUM_THREADS`. # Example: ```python # Usage example n_jobs_list = [1, 2, 4] omp_num_threads_list = [1, 2, 4] evaluate_parallelism(n_jobs_list, omp_num_threads_list) # Expected output (format example): # n_jobs: 1, OMP_NUM_THREADS: 1 -> Training time: XX.XX seconds # n_jobs: 1, OMP_NUM_THREADS: 2 -> Training time: XX.XX seconds # n_jobs: 1, OMP_NUM_THREADS: 4 -> Training time: XX.XX seconds # n_jobs: 2, OMP_NUM_THREADS: 1 -> Training time: XX.XX seconds # n_jobs: 2, OMP_NUM_THREADS: 2 -> Training time: XX.XX seconds # n_jobs: 2, OMP_NUM_THREADS: 4 -> Training time: XX.XX seconds # n_jobs: 4, OMP_NUM_THREADS: 1 -> Training time: XX.XX seconds # ... ``` # Constraints: - You must handle different combinations of the `n_jobs` and `OMP_NUM_THREADS` values. - Ensure that the function correctly sets environment variables before training the model. - Use appropriate measuring tools (like `time` module) to measure training time accurately. # Notes: - Considering the oversubscription issue, observe whether certain combinations lead to performance degradation. - The function should not assume a specific number of CPU cores; it should dynamically handle input lists of various lengths.","solution":"import os import time from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier def evaluate_parallelism(n_jobs_list: list, omp_num_threads_list: list) -> None: # Load dataset digits = load_digits() X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2) for n_jobs in n_jobs_list: for num_threads in omp_num_threads_list: os.environ[\\"OMP_NUM_THREADS\\"] = str(num_threads) # Start timer start_time = time.time() # Train the model model = RandomForestClassifier(n_jobs=n_jobs) model.fit(X_train, y_train) # Measure time end_time = time.time() elapsed_time = end_time - start_time print(f\'n_jobs: {n_jobs}, OMP_NUM_THREADS: {num_threads} -> Training time: {elapsed_time:.4f} seconds\')"},{"question":"*Programming Question: Constructing a Multipart Email Message* **Goal:** Test the ability to create and manage a multipart email message using the `email.mime` module in Python. **Task:** Write a function `create_multipart_email(subject, sender, recipient, text, attachment_path, image_path)` that constructs a multipart email message with the following specifications: 1. **Subject**: The subject of the email. 2. **Sender**: The email address of the sender. 3. **Recipient**: The email address of the recipient. 4. **Text**: Text content of the email to be included as a MIMEText part. 5. **Attachment**: Attach a file (provided as `attachment_path`) as a MIMEApplication part. 6. **Image**: Include an image (provided as `image_path`) as a MIMEImage part. The function should: 1. Use the `MIMEMultipart` class to create the root message. 2. Add a `MIMEText` part for the text content. 3. Add a `MIMEApplication` part for the attachment. 4. Add a `MIMEImage` part for the image. 5. Ensure all parts are correctly encoded and attached to the message. 6. Set appropriate headers like `From`, `To`, and `Subject`. 7. Return the entire email message as a string. **Input:** - `subject` (str): The subject of the email. - `sender` (str): The sender\'s email address. - `recipient` (str): The recipient\'s email address. - `text` (str): The text content of the email. - `attachment_path` (str): File path to the attachment. - `image_path` (str): File path to the image. **Output:** - `message_str` (str): The constructed multipart email message as a string. **Constraints:** - Ensure proper MIME type detection and encoding for the attachment and image. - Raise appropriate errors for any invalid inputs, such as non-existent file paths. **Example Usage:** ```python subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" text = \\"This is a test email.\\" attachment_path = \\"/path/to/attachment.pdf\\" image_path = \\"/path/to/image.jpg\\" email_message = create_multipart_email(subject, sender, recipient, text, attachment_path, image_path) print(email_message) ``` **Additional Notes:** - Use standard libraries like `os` to check file paths and `imghdr` module if needed for image subtype detection. - Make sure to handle different MIME types correctly as per the documentation provided.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.application import MIMEApplication from email.mime.image import MIMEImage import os import imghdr def create_multipart_email(subject, sender, recipient, text, attachment_path, image_path): Constructs a multipart email message. :param subject: The subject of the email. :param sender: The email address of the sender. :param recipient: The email address of the recipient. :param text: Text content of the email. :param attachment_path: File path to the attachment. :param image_path: File path to the image. :return: The constructed email message as a string. if not os.path.isfile(attachment_path): raise ValueError(\\"Attachment file does not exist.\\") if not os.path.isfile(image_path): raise ValueError(\\"Image file does not exist.\\") # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient # Add text part msg.attach(MIMEText(text, \'plain\')) # Add attachment part with open(attachment_path, \'rb\') as f: attachment = MIMEApplication(f.read(), _subtype=\\"octet-stream\\") attachment.add_header(\'Content-Disposition\', \'attachment\', filename=os.path.basename(attachment_path)) msg.attach(attachment) # Add image part with open(image_path, \'rb\') as f: img_data = f.read() img_type = imghdr.what(None, img_data) if not img_type: raise ValueError(\\"Invalid image file type.\\") image = MIMEImage(img_data, _subtype=img_type) image.add_header(\'Content-Disposition\', \'inline\', filename=os.path.basename(image_path)) msg.attach(image) return msg.as_string()"},{"question":"**Question: Analyzing Log Data** You are given a standardized log file from a web server. Each line in the log file is a JSON object representing a request made to the server, with fields like `timestamp`, `request_type`, `url`, `response_time`, and `status_code`. Your task is to implement a function to process and analyze this log data. # Function Definition ```python def analyze_logs(file_path: str) -> dict: Processes the log file and returns a dictionary with the following structure: { \\"total_requests\\": int, # Total number of requests \\"request_type_count\\": dict, # Counts of each request type (e.g., GET, POST) \\"url_access_count\\": dict, # Counts of how many times each URL was accessed \\"average_response_time\\": float, # Average response time of all requests \\"status_code_distribution\\": dict, # Distribution of status codes \\"top_5_slowest_requests\\": List[str], # The URLs of the top 5 requests with the highest response times } pass ``` # Input Format - The function takes a single string argument `file_path`, which is the path to the log file. # Output Format - The function returns a dictionary containing the requested analysis data. # Example Assume the log file contains the following lines: ```json {\\"timestamp\\": \\"2023-01-01T12:00:00\\", \\"request_type\\": \\"GET\\", \\"url\\": \\"/home\\", \\"response_time\\": 120, \\"status_code\\": 200} {\\"timestamp\\": \\"2023-01-01T12:00:01\\", \\"request_type\\": \\"POST\\", \\"url\\": \\"/login\\", \\"response_time\\": 300, \\"status_code\\": 500} {\\"timestamp\\": \\"2023-01-01T12:00:02\\", \\"request_type\\": \\"GET\\", \\"url\\": \\"/home\\", \\"response_time\\": 150, \\"status_code\\": 200} ``` Your function should return: ```python { \\"total_requests\\": 3, \\"request_type_count\\": {\\"GET\\": 2, \\"POST\\": 1}, \\"url_access_count\\": {\\"/home\\": 2, \\"/login\\": 1}, \\"average_response_time\\": 190.0, \\"status_code_distribution\\": {200: 2, 500: 1}, \\"top_5_slowest_requests\\": [\\"/login\\", \\"/home\\"] } ``` # Constraints - Assume the log file is well-formed and each line is a valid JSON object. - The log file can be of significant size, so the implementation should be efficient in terms of time and space complexity. # Notes - Handle file I/O exceptions gracefully. - Deal with possible edge cases such as an empty log file or missing fields in some log entries. - Your solution should effectively demonstrate the use of Python\'s built-in modules and demonstrate an understanding of performance trade-offs.","solution":"import json from collections import defaultdict, Counter from typing import List, Dict def analyze_logs(file_path: str) -> dict: Processes the log file and returns a dictionary with the following structure: { \\"total_requests\\": int, # Total number of requests \\"request_type_count\\": dict, # Counts of each request type (e.g., GET, POST) \\"url_access_count\\": dict, # Counts of how many times each URL was accessed \\"average_response_time\\": float, # Average response time of all requests \\"status_code_distribution\\": dict, # Distribution of status codes \\"top_5_slowest_requests\\": List[str], # The URLs of the top 5 requests with the highest response times } total_requests = 0 request_type_count = Counter() url_access_count = Counter() total_response_time = 0 status_code_distribution = Counter() response_times = [] try: with open(file_path, \'r\') as file: for line in file: try: log_entry = json.loads(line) total_requests += 1 request_type = log_entry.get(\\"request_type\\", \\"\\") url = log_entry.get(\\"url\\", \\"\\") response_time = log_entry.get(\\"response_time\\", 0) status_code = log_entry.get(\\"status_code\\", 0) request_type_count[request_type] += 1 url_access_count[url] += 1 total_response_time += response_time status_code_distribution[status_code] += 1 response_times.append((response_time, url)) except json.JSONDecodeError: continue average_response_time = total_response_time / total_requests if total_requests > 0 else 0 top_5_slowest_requests = [url for _, url in sorted(response_times, reverse=True)[:5]] return { \\"total_requests\\": total_requests, \\"request_type_count\\": dict(request_type_count), \\"url_access_count\\": dict(url_access_count), \\"average_response_time\\": average_response_time, \\"status_code_distribution\\": dict(status_code_distribution), \\"top_5_slowest_requests\\": top_5_slowest_requests } except FileNotFoundError: print(f\\"The file {file_path} was not found.\\") return {} except Exception as e: print(f\\"An error occurred: {e}\\") return {}"},{"question":"You are required to implement a function named `categorize_errors` that takes a list of errno codes as input and returns a dictionary categorizing these errno codes based on their respective exception classes. The function should use the `errno.errorcode` dictionary and the exception mappings provided in the documentation to classify each errno code. Specifically, the dictionary returned by your function should have the names of the exception classes as keys and lists of corresponding errno code names as values. Input: - A list of integers representing errno codes (e.g., [1, 2, 3, 4, 5]). Output: - A dictionary where each key is the name of an exception class (e.g., `\'PermissionError\'`) and each value is a list of errno code names (e.g., `[\'EPERM\']`). Constraints: - If an errno code does not have an associated defined exception in the documentation provided, it should be categorized under a key `\'UndefinedError\'`. - The list of errno codes will have at most 1000 entries. - Performance should be efficient to handle the maximum input size within reasonable time limits. Example: ```python import errno def categorize_errors(errno_list): exception_mapping = { \'PermissionError\': [errno.EPERM, errno.EACCES], \'FileNotFoundError\': [errno.ENOENT], \'ProcessLookupError\': [errno.ESRCH], \'InterruptedError\': [errno.EINTR], \'ChildProcessError\': [errno.ECHILD], \'BlockingIOError\': [errno.EAGAIN, errno.EWOULDBLOCK, errno.EALREADY, errno.EINPROGRESS], \'FileExistsError\': [errno.EEXIST], \'NotADirectoryError\': [errno.ENOTDIR], \'IsADirectoryError\': [errno.EISDIR], \'BrokenPipeError\': [errno.EPIPE, errno.ESHUTDOWN], \'ConnectionAbortedError\': [errno.ECONNABORTED], \'ConnectionResetError\': [errno.ECONNRESET], \'TimeoutError\': [errno.ETIMEDOUT], \'ConnectionRefusedError\': [errno.ECONNREFUSED] } categorized_errors = {\'UndefinedError\': []} reverse_mapping = {v: k for k, vals in exception_mapping.items() for v in vals} for code in errno_list: name = errno.errorcode.get(code) if name: exception_class = reverse_mapping.get(code, \'UndefinedError\') if exception_class not in categorized_errors: categorized_errors[exception_class] = [] categorized_errors[exception_class].append(name) else: categorized_errors[\'UndefinedError\'].append(f\\"Errno code {code} without name\\") return categorized_errors # Example Usage: print(categorize_errors([errno.EPERM, errno.ENOENT, errno.EWOULDBLOCK, 9999])) # Output: # { # \'PermissionError\': [\'EPERM\'], # \'FileNotFoundError\': [\'ENOENT\'], # \'BlockingIOError\': [\'EWOULDBLOCK\'], # \'UndefinedError\': [\'Errno code 9999 without name\'] # } ```","solution":"import errno def categorize_errors(errno_list): exception_mapping = { \'PermissionError\': [errno.EPERM, errno.EACCES], \'FileNotFoundError\': [errno.ENOENT], \'ProcessLookupError\': [errno.ESRCH], \'InterruptedError\': [errno.EINTR], \'ChildProcessError\': [errno.ECHILD], \'BlockingIOError\': [errno.EAGAIN, errno.EWOULDBLOCK, errno.EALREADY, errno.EINPROGRESS], \'FileExistsError\': [errno.EEXIST], \'NotADirectoryError\': [errno.ENOTDIR], \'IsADirectoryError\': [errno.EISDIR], \'BrokenPipeError\': [errno.EPIPE, errno.ESHUTDOWN], \'ConnectionAbortedError\': [errno.ECONNABORTED], \'ConnectionResetError\': [errno.ECONNRESET], \'TimeoutError\': [errno.ETIMEDOUT], \'ConnectionRefusedError\': [errno.ECONNREFUSED] } categorized_errors = {\'UndefinedError\': []} reverse_mapping = {v: k for k, vals in exception_mapping.items() for v in vals} for code in errno_list: name = errno.errorcode.get(code) if name: exception_class = reverse_mapping.get(code, \'UndefinedError\') if exception_class not in categorized_errors: categorized_errors[exception_class] = [] categorized_errors[exception_class].append(name) else: categorized_errors[\'UndefinedError\'].append(f\\"Errno code {code} without name\\") return categorized_errors"},{"question":"**Title: Implement a Concurrent Prime Number Finder** **Objective:** To assess your understanding of threading and multiprocessing in Python, you will implement a concurrent program to find prime numbers within a specified range. Your task is to use both thread-based and process-based parallelism to achieve this. Additionally, you will need to ensure that shared resources are managed correctly using synchronization primitives. **Task:** Write a Python program that: 1. Finds all prime numbers in a given range `[start, end]`. 2. Uses both threading and multiprocessing to split the work. 3. Ensures safe access to shared resources, particularly when accumulating results. **Specifications:** 1. **Function Signature:** ```python def find_primes_concurrently(start: int, end: int) -> List[int]: ``` 2. **Input:** - `start` - an integer, the lower bound of the range (inclusive). - `end` - an integer, the upper bound of the range (inclusive). 3. **Output:** - A list of integers containing all prime numbers in the specified range in any order. 4. **Constraints:** - `start` and `end` will be positive integers where `start <= end`. - The range [start, end] can be large, so efficient computation is required. 5. **Concurrency Requirements:** - Use the `threading` module to create multiple threads for performing prime checks on sub-ranges. - Use the `multiprocessing` module to create multiple processes for the same purpose. - Implement a thread-safe or process-safe accumulation of the found prime numbers. 6. **Example:** ```python primes = find_primes_concurrently(10, 50) print(primes) # This should print a list of prime numbers between 10 and 50, e.g., [11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47] ``` 7. **Performance Considerations:** - Your implementation should be able to handle ranges where `end - start` is up to 10^6 within a reasonable time frame. - Efficiently splitting the workload among threads and processes is crucial for performance. 8. **Synchronization:** - Use locks, conditions, or other synchronization mechanisms to manage access to the shared list of prime numbers across threads. - For multiprocessing, ensure safe access to shared data using Managers or shared memory constructs. **Notes:** - You may want to write helper functions to check for prime numbers and to handle the splitting and merging of work across threads and processes. - Consider edge cases such as the smallest possible ranges, and non-prime heavy ranges. Good luck, and make sure to test your solution thoroughly!","solution":"from threading import Thread, Lock from multiprocessing import Process, Manager from typing import List import math def is_prime(n: int) -> bool: if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def find_primes_in_range(start: int, end: int, primes: List[int]): for num in range(start, end + 1): if is_prime(num): primes.append(num) def find_primes_concurrently(start: int, end: int) -> List[int]: if start > end: return [] # Splitting work for threading n_threads = 4 thread_ranges = [(start + i * ((end - start) // n_threads), start + (i + 1) * ((end - start) // n_threads) - 1) for i in range(n_threads)] thread_ranges[-1] = (thread_ranges[-1][0], end) # Adjust the last range to include the \'end\' boundary primes_thread = [] lock = Lock() def thread_worker(start, end): local_primes = [] find_primes_in_range(start, end, local_primes) with lock: primes_thread.extend(local_primes) threads = [Thread(target=thread_worker, args=trange) for trange in thread_ranges] for thread in threads: thread.start() for thread in threads: thread.join() # Splitting work for multiprocessing n_procs = 4 proc_ranges = [(start + i * ((end - start) // n_procs), start + (i + 1) * ((end - start) // n_procs) - 1) for i in range(n_procs)] proc_ranges[-1] = (proc_ranges[-1][0], end) # Adjust the last range to include the \'end\' boundary with Manager() as manager: primes_proc = manager.list() processes = [Process(target=find_primes_in_range, args=(prange[0], prange[1], primes_proc)) for prange in proc_ranges] for proc in processes: proc.start() for proc in processes: proc.join() primes_proc = list(primes_proc) # Merging results from threads and processes all_primes = set(primes_thread + primes_proc) # Using set to avoid duplicates across threads/processes return sorted(all_primes)"},{"question":"# Question: Implement Custom Formatter Using reprlib You are required to create a custom class that inherits from `reprlib.Repr` to handle additional data types and modify existing formatting. You will implement a new representation for sets and modify the representation for strings. Follow the constraints below: 1. **New Set Representation**: - For sets containing more than 4 elements, represent only the first 4 elements followed by \'... and x more\'. For example, `{1, 2, 3, 4, 5, 6}` should be represented as `\'{1, 2, 3, 4, ... and 2 more}\'`. 2. **Modified String Representation**: - Limit the string representation to 10 characters, and truncate it by replacing the middle with \'...\'. For example, `\\"Hello, World!\\"` should be represented as `\\"Hel...ld!\\"`. **Constraints**: - You are not allowed to change the existing `reprlib.Repr` class directly. - Your solution should subclass `reprlib.Repr` and override the relevant methods to achieve the desired formatting. - Ensure the methods handle nested representations appropriately. # Input Format There is no input from stdin. You will implement a class and demonstrate its usage with a few test cases. # Output Format Your implementation should print the custom representations for the following test cases: 1. A set with seven elements. 2. A string with more than ten characters. # Example ```python from reprlib import Repr class CustomRepr(Repr): def repr_set(self, obj, level): if len(obj) > 4: elements = list(obj)[:4] return \'{\' + \', \'.join(map(repr, elements)) + f\', ... and {len(obj) - 4} more\' + \'}\' return repr(obj) def repr_str(self, obj, level): if len(obj) > 10: return repr(obj[:3] + \'...\' + obj[-3:]) return repr(obj) # Example usage custom_repr = CustomRepr() print(custom_repr.repr({1, 2, 3, 4, 5, 6, 7})) # Output: {1, 2, 3, 4, ... and 3 more} print(custom_repr.repr(\\"Hello, World!\\")) # Output: \'Hel...ld!\' ``` This task will test your understanding of class inheritance, method overriding, and handling built-in Python representations with custom constraints.","solution":"import reprlib class CustomRepr(reprlib.Repr): def repr_set(self, obj, level): if len(obj) > 4: elements = list(obj)[:4] return \'{\' + \', \'.join(map(repr, elements)) + f\', ... and {len(obj) - 4} more\' + \'}\' return \'{\' + \', \'.join(map(repr, obj)) + \'}\' def repr_str(self, obj, level): if len(obj) > 10: return repr(obj[:3] + \'...\' + obj[-3:]) return repr(obj)"},{"question":"You have been tasked with visualizing data for a recent survey on student satisfaction across different faculties in your university. To accurately present the data, you will use the seaborn library to create plots and customize their appearances, specifically focusing on setting plot limits for the axes. # Instructions 1. **Data Preparation:** - Create a dataset with the following columns: `\'Faculty\'`, `\'SatisfactionScore\'`, `\'NumberOfRespondents\'`. - Populate the dataset with the following data: ```python data = { \'Faculty\': [\'Science\', \'Engineering\', \'Arts\', \'Business\', \'Law\'], \'SatisfactionScore\': [78, 82, 96, 70, 84], \'NumberOfRespondents\': [120, 85, 50, 60, 40] } ``` 2. **Plot Creation:** - Use the `seaborn.objects` interface to create a scatter plot that shows the relationship between `NumberOfRespondents` (x-axis) and `SatisfactionScore` (y-axis). - Add plot customizations with different types of markers for each faculty. 3. **Plot Customization:** - Customize the plot by setting the following plot limits: * For the x-axis, set limits from 30 to 130. * For the y-axis, set limits from 60 to 100. - Deploy the addition of markers for each data point. 4. **Axis Inversion:** - Create a secondary plot where you invert the y-axis limits. 5. **Maintaining Defaults:** - Demonstrate maintaining default values by customizing the limits to only affect the x-axis, while leaving the y-axis at its default settings. # Requirements - You must use seaborn\'s `so.Plot` class and relevant methods for creating and customizing the plots. - The plots should be well-labeled, with an appropriate title and axis labels. # Expected Output - A scatter plot showing `NumberOfRespondents` vs `SatisfactionScore`. - A list of customizations applied to the plot limits and marker types. - Screenshots of the plots showing the customized limits, inverted y-axis, and default y-axis. # Constraints - The code should handle the dataset dynamically, without hardcoding any parts other than the dataset creation step. - Ensure matplotlib and seaborn packages are properly imported and used within a Python environment. Example Code for Data Preparation ```python import seaborn.objects as so import matplotlib.pyplot as plt # Data Preparation data = { \'Faculty\': [\'Science\', \'Engineering\', \'Arts\', \'Business\', \'Law\'], \'SatisfactionScore\': [78, 82, 96, 70, 84], \'NumberOfRespondents\': [120, 85, 50, 60, 40] } # Your code implementation here ``` Ensure your implementation follows seaborn documentation and produces the specified visualizations accurately.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Data Preparation data = pd.DataFrame({ \'Faculty\': [\'Science\', \'Engineering\', \'Arts\', \'Business\', \'Law\'], \'SatisfactionScore\': [78, 82, 96, 70, 84], \'NumberOfRespondents\': [120, 85, 50, 60, 40] }) # Plot Creation p = sns.scatterplot(data=data, x=\'NumberOfRespondents\', y=\'SatisfactionScore\', hue=\'Faculty\', style=\'Faculty\', s=100) # Plot Customization - Setting plot limits plt.xlim(30, 130) plt.ylim(60, 100) plt.title(\'Satisfaction Score vs Number of Respondents\') plt.xlabel(\'Number of Respondents\') plt.ylabel(\'Satisfaction Score\') plt.show() # Creating plot with inverted y-axis p_inverted = sns.scatterplot(data=data, x=\'NumberOfRespondents\', y=\'SatisfactionScore\', hue=\'Faculty\', style=\'Faculty\', s=100) plt.xlim(30, 130) plt.ylim(100, 60) # Invert y-axis plt.title(\'Satisfaction Score vs Number of Respondents (Inverted Y-axis)\') plt.xlabel(\'Number of Respondents\') plt.ylabel(\'Satisfaction Score\') plt.show() # Creating plot maintaining default y-axis p_default_y = sns.scatterplot(data=data, x=\'NumberOfRespondents\', y=\'SatisfactionScore\', hue=\'Faculty\', style=\'Faculty\', s=100) plt.xlim(30, 130) plt.title(\'Satisfaction Score vs Number of Respondents (Default Y-axis)\') plt.xlabel(\'Number of Respondents\') plt.ylabel(\'Satisfaction Score\') plt.show()"},{"question":"# PyTorch Coding Assessment Question Objective You are tasked with implementing and testing a function using PyTorch that demonstrates the use of randomized operations and the importance of reproducibility in scientific computing. Instructions 1. **Function Implementation**: - Implement a function `generate_random_tensor` that: 1. Accepts an integer `seed`, and a tuple `shape`. 2. Initializes the random number generator with the given seed. 3. Generates and returns a PyTorch tensor of the specified shape with random values drawn uniformly from the range [0, 1). 2. **Function Signature**: ```python def generate_random_tensor(seed: int, shape: tuple) -> torch.Tensor: pass ``` 3. **Input and Output**: - Input: An integer `seed` (0 <= seed <= 2**32 - 1) and a tuple `shape` representing the dimensions of the tensor. - Output: A PyTorch `Tensor` of the specified shape filled with random values uniformly drawn from [0, 1). 4. **Constraints/Limitations**: - Ensure that running the function multiple times with the same seed and shape produces the same tensor. - Utilize `torch.manual_seed` to set the seed for random generation. 5. **Examples**: ```python # Example 1 seed = 42 shape = (2, 3) result = generate_random_tensor(seed, shape) print(result) # This should consistently output the same tensor every time the function is run with these parameters. # Example 2 seed = 99 shape = (4, 4) result = generate_random_tensor(seed, shape) print(result) # Again, this should consistently output the same tensor every time the function is run with these parameters. ``` Grading Criteria - Correctness: The function must correctly use the seed to ensure reproducibility. - Code quality: The implementation should be clean and maintainable. - Adherence to constraints: Ensure that the random tensor generation adheres to the specified range [0, 1) and the same tensor is generated on re-runs with the same seed.","solution":"import torch def generate_random_tensor(seed: int, shape: tuple) -> torch.Tensor: Generate a random tensor with the specified shape using the given seed. Args: - seed (int): The seed for the random number generator. - shape (tuple): The shape of the tensor to generate. Returns: - torch.Tensor: A tensor with the specified shape and random values. torch.manual_seed(seed) # Set the random seed for reproducibility return torch.rand(shape) # Generate a tensor with random values uniformly from [0, 1)"},{"question":"# Assessment Question: Implementing a Simplified Database Using Python\'s Data Model Objective: In this exercise, you will implement a simplified in-memory database system using Python classes and special methods. This task will test your understanding of Python\'s data model, including user-defined classes, special methods, container emulation, and context managers. Problem Statement: You need to implement four primary components: 1. **Row** - A class representing a single row in the database. 2. **Table** - A class representing a table containing multiple rows. 3. **Database** - A class representing the full database containing multiple tables. 4. **Transaction** - A context manager to handle transactions on tables. # Detailed Requirements: 1. Row Class - **Attributes**: - `data` (dictionary): Stores column names as keys and corresponding values. - **Methods**: - `__init__(self, **data)`: Initializes the `Row` with the given key-value pairs where keys are column names. - `__repr__(self)`: Returns a string representation of the row in the form {column1: value1, column2: value2, }. 2. Table Class - **Attributes**: - `name` (str): The name of the table. - `rows` (list): A list to store instances of `Row`. - **Methods**: - `__init__(self, name)`: Initializes the `Table` with a name. - `__len__(self)`: Returns the number of rows in the table. - `__getitem__(self, index)`: Retrieves the row at the specified index. - `__setitem__(self, index, row)`: Updates the row at the specified index. - `__delitem__(self, index)`: Deletes the row at the specified index. - `add_row(self, **data)`: Adds a new `Row` to the table. - `__repr__(self)`: Returns a string representation of the table, including all rows. 3. Database Class - **Attributes**: - `tables` (dict): A dictionary to store tables with table names as keys. - **Methods**: - `__init__(self)`: Initializes an empty database. - `create_table(self, name)`: Adds a new table to the database. - `__getitem__(self, name)`: Retrieves the table with the specified name. - `__repr__(self)`: Returns a string representation of the database, including all tables and rows. 4. Transaction Class - **Attributes**: - `table` (Table): The table on which the transaction will be performed. - `backup` (list): A backup of the tables rows to support rollback in case of an error. - **Methods**: - `__init__(self, table)`: Initializes with the table to be transacted on. - `__enter__(self)`: Starts the transaction. - `__exit__(self, exc_type, exc_value, traceback)`: Ends the transaction. If an exception occurs, rolls back the changes. # Constraints: - Ensure appropriate error handling for edge cases, such as out-of-range access or adding duplicate table names. - The Row class should be immutable once created. - Transactions should roll back any changes made within the `with` block if an exception occurs. # Example Usage: ```python db = Database() db.create_table(\\"Users\\") db[\\"Users\\"].add_row(id=1, username=\\"alice\\", email=\\"alice@example.com\\") db[\\"Users\\"].add_row(id=2, username=\\"bob\\", email=\\"bob@example.com\\") print(db) # Should display the database structure with tables and rows with Transaction(db[\\"Users\\"]) as t: db[\\"Users\\"].add_row(id=3, username=\\"charlie\\", email=\\"charlie@example.com\\") print(db) # Should display the updated structure unless an error occurs in the transaction ``` # Submission: Submit the Python code implementing the `Row`, `Table`, `Database`, and `Transaction` classes, along with example usage demonstrating the functionality.","solution":"class Row: def __init__(self, **data): self._data = data @property def data(self): return self._data def __repr__(self): return f\\"{self._data}\\" class Table: def __init__(self, name): self.name = name self.rows = [] def __len__(self): return len(self.rows) def __getitem__(self, index): return self.rows[index] def __setitem__(self, index, row): self.rows[index] = row def __delitem__(self, index): del self.rows[index] def add_row(self, **data): new_row = Row(**data) self.rows.append(new_row) def __repr__(self): rows_repr = \\"n\\".join(repr(row) for row in self.rows) return f\\"Table({self.name}):n{rows_repr}\\" class Database: def __init__(self): self.tables = {} def create_table(self, name): if name in self.tables: raise ValueError(f\\"Table {name} already exists\\") self.tables[name] = Table(name) def __getitem__(self, name): return self.tables[name] def __repr__(self): tables_repr = \\"n\\".join(repr(table) for table in self.tables.values()) return f\\"Database:n{tables_repr}\\" class Transaction: def __init__(self, table): self.table = table self.backup = list(self.table.rows) def __enter__(self): return self.table def __exit__(self, exc_type, exc_value, traceback): if exc_type is not None: self.table.rows = self.backup return False # Do not suppress exceptions # Example Usage: db = Database() db.create_table(\\"Users\\") db[\\"Users\\"].add_row(id=1, username=\\"alice\\", email=\\"alice@example.com\\") db[\\"Users\\"].add_row(id=2, username=\\"bob\\", email=\\"bob@example.com\\") print(db) # Displays the database structure with tables and rows try: with Transaction(db[\\"Users\\"]) as t: db[\\"Users\\"].add_row(id=3, username=\\"charlie\\", email=\\"charlie@example.com\\") raise RuntimeError(\\"Simulated error\\") # This will cause rollback except RuntimeError: pass print(db) # Should display the database structure without the row added in the transaction"},{"question":"# Pandas Coding Assessment: Handling Missing Data Objective: To assess your ability to handle missing data in pandas data structures (Series and DataFrame) using various techniques including detection, filling, interpolation, and replacing missing values. Question: You are provided with a dataset containing information about sales transactions. The dataset has missing values that need to be handled appropriately for accurate analysis. Write a function `process_sales_data` that: 1. Reads the data from a CSV file into a DataFrame. 2. Detects and replaces missing values in the `Sales` column with the column\'s mean value. 3. Detects and interpolates the missing values in the `Discount` column using linear interpolation method. 4. Replaces any instances of `Failed` in the `Status` column with `NaN`. 5. Returns the cleaned DataFrame. Input: - A string `file_path` representing the path to the CSV file. - The CSV file contains columns `TransactionID`, `CustomerID`, `Sales`, `Discount`, and `Status`. Output: - A pandas DataFrame after processing the missing data as described. Constraints: - The `Sales` and `Discount` columns will always contain numeric data. - The `Status` column contains string values including \\"Completed\\", \\"Pending\\", and \\"Failed\\". - Handle any necessary data type conversions appropriately. Example: Consider the following CSV content: ``` TransactionID,CustomerID,Sales,Discount,Status 1,101,200.0,10.0,Completed 2,102,,15.0,Pending 3,103,150.0,,Failed 4,104,300.0,20.0,Completed 5,105,250.0,,Completed ``` **Example Input:** `file_path = \'sales_data.csv\'` **Example Output:** | TransactionID | CustomerID | Sales | Discount | Status | |---------------|------------|-------|----------|-----------| | 1 | 101 | 200.0 | 10.0 | Completed | | 2 | 102 | 225.0 | 15.0 | Pending | | 3 | 103 | 150.0 | 17.5 | NaN | | 4 | 104 | 300.0 | 20.0 | Completed | | 5 | 105 | 250.0 | 20.0 | Completed | Function Signature: ```python import pandas as pd def process_sales_data(file_path: str) -> pd.DataFrame: pass ```","solution":"import pandas as pd import numpy as np def process_sales_data(file_path: str) -> pd.DataFrame: # Read the data from a CSV file into a DataFrame df = pd.read_csv(file_path) # Replace missing values in the `Sales` column with the column\'s mean value df[\'Sales\'].fillna(df[\'Sales\'].mean(), inplace=True) # Interpolate missing values in the `Discount` column using linear interpolation method df[\'Discount\'].interpolate(method=\'linear\', inplace=True) # Replace any instances of `Failed` in the `Status` column with `NaN` df[\'Status\'].replace(\'Failed\', np.nan, inplace=True) return df"},{"question":"Objective: Your task is to create two functions: one that converts a given RGB color to its equivalent HSV values using the `colorsys` module and another that reads a WAV file, processes the audio data to normalize the volume, and writes the result to a new WAV file using the `wave` module. Function 1: RGB to HSV Conversion Implement a function `rgb_to_hsv(r, g, b)` that converts RGB color values to their HSV equivalent. - **Input:** - `r`: an integer (0-255), representing the red component. - `g`: an integer (0-255), representing the green component. - `b`: an integer (0-255), representing the blue component. - **Output:** - a tuple `(h, s, v)` where: - `h` is the hue (a float between 0.0 and 1.0). - `s` is the saturation (a float between 0.0 and 1.0). - `v` is the value (a float between 0.0 and 1.0). Function 2: Audio Normalization Implement a function `normalize_audio(input_file, output_file)` that normalizes the volume of a WAV file. - **Input:** - `input_file`: a string, the path to the input WAV file. - `output_file`: a string, the path to the output WAV file where the normalized audio will be saved. - **Output:** - None, but the function should write the normalized audio to `output_file`. **Details:** 1. **RGB to HSV Conversion**: You should use the `colorsys` module to convert the RGB values to HSV. ```python import colorsys def rgb_to_hsv(r, g, b): # Your implementation here pass # Example Usage rgb_to_hsv(255, 0, 0) # Expected output: (0.0, 1.0, 1.0) ``` 2. **Audio Normalization**: - Read the audio data from `input_file` using the `wave` module. - Normalize the volume. (Find the maximum amplitude and scale the entire audio data so that the maximum amplitude is at a standard level without clipping). - Write the normalized audio data to `output_file`. ```python import wave def normalize_audio(input_file, output_file): # Your implementation here pass # Example Usage normalize_audio(\'input.wav\', \'output.wav\') ``` **Constraints:** - For the `rgb_to_hsv` function, ensure all input values for `r`, `g`, and `b` are between 0 and 255. - For the `normalize_audio` function, your solution should handle typical WAV format details such as mono/stereo channels and sample width. Performance Requirements: - Ensure that the `normalize_audio` function is efficient enough to process an audio file of up to 5 minutes in length without excessive delay.","solution":"import colorsys import wave import numpy as np def rgb_to_hsv(r, g, b): Converts RGB color values to their HSV equivalent. Args: r (int): Red component (0-255). g (int): Green component (0-255). b (int): Blue component (0-255). Returns: tuple: HSV values where hue, saturation, and value are floats between 0.0 and 1.0. return colorsys.rgb_to_hsv(r / 255.0, g / 255.0, b / 255.0) def normalize_audio(input_file, output_file): Normalizes the volume of a WAV file. Args: input_file (str): Path to the input WAV file. output_file (str): Path to the output WAV file where normalized audio will be saved. Returns: None with wave.open(input_file, \'rb\') as infile: params = infile.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params frames = infile.readframes(n_frames) audio_data = np.frombuffer(frames, dtype=np.int16) # Normalize the audio data max_amplitude = np.max(np.abs(audio_data)) scaling_factor = (2**(sampwidth * 8 - 1) - 1) / max_amplitude normalized_audio_data = (audio_data * scaling_factor).astype(np.int16) normalized_frames = normalized_audio_data.tobytes() with wave.open(output_file, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(normalized_frames)"},{"question":"# Email Message Content Extractor Objective Your task is to implement a function that processes an email message object and returns a specific type of content. Background You are given an email message that may contain multiple subparts with different MIME types and payloads. Some parts contain plain text while others might be rich text or even attachments. You need to extract and concatenate the text contents of the email, excluding any parts that are not plain text. Function Signature ```python import email def extract_plain_text(msg: email.message.Message) -> str: This function extracts and concatenates all the plain text payloads from the given email message subparts, skipping the non-text payloads. Parameters: msg (email.message.Message): The email message object to process. Returns: str: Concatenated plain text content. pass ``` Input - `msg` (email.message.Message): The email message object. You can assume it is a well-formed email message object created using the `email` module. Output - Returns a `str` that is a concatenation of all plain text subpart payloads within the email message, each on a new line. Constraints - The given email message object may contain nested sub-parts. - Only include payloads that have a MIME type of `text/plain`. Example Usage ```python import email from email import policy from email.parser import BytesParser # Assume raw_email contains the raw email data msg = BytesParser(policy=policy.default).parsebytes(raw_email) result = extract_plain_text(msg) print(result) ``` Hints - Use the `typed_subpart_iterator` function from the `email.iterators` module to filter out only the plain text subparts. - Iterate over these subparts to concatenate their payloads into the final output string. Note Ensure to handle any necessary decoding of payloads if required by utilizing the `get_payload(decode=True)` method appropriately.","solution":"import email from email import policy from email.parser import BytesParser def extract_plain_text(msg: email.message.Message) -> str: This function extracts and concatenates all the plain text payloads from the given email message subparts, skipping the non-text payloads. Parameters: msg (email.message.Message): The email message object to process. Returns: str: Concatenated plain text content. text_parts = [] for part in msg.walk(): if part.get_content_type() == \'text/plain\': payload = part.get_payload(decode=True) if payload: text_parts.append(payload.decode(part.get_content_charset() or \'utf-8\')) return \'n\'.join(text_parts)"},{"question":"Objective Demonstrate your understanding of loading datasets and applying machine learning algorithms using scikit-learn. Question You are tasked with building a simple classification model using one of the toy datasets available in scikit-learn. Specifically, you will use the Iris dataset for this task. Requirements: 1. Load the Iris dataset using the appropriate function from scikit-learn. 2. Split the dataset into training and testing sets with a ratio of 80% training data and 20% testing data. 3. Standardize the features (zero mean and unit variance). 4. Train a Support Vector Machine (SVM) classifier on the training data. 5. Evaluate the classifier on the testing data and print the accuracy. Implementation - Use `train_test_split` for splitting the dataset. - Use `StandardScaler` for standardizing the features. - Use `SVC` from `sklearn.svm` for the SVM classifier. Input and Output Formats: - No inputs are required; the function should execute end-to-end. - The output should be the accuracy of the SVM classifier printed to the console. Constraints and Limitations: - Ensure reproducibility by setting a random seed where applicable. - Your code should be efficient and make use of scikit-learn functionalities optimally. Example Usage: ```python def build_and_eval_svm(): # Your implementation here pass # Call the function to perform the task build_and_eval_svm() ``` Expected output (example): ``` Accuracy of SVM classifier: 0.95 ``` You might not get the exact same output due to the randomness in the train-test split, but your function should follow the outlined steps to produce a similar evaluation result.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def build_and_eval_svm(): # Load the Iris dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train an SVM classifier on the training data clf = SVC(random_state=42) clf.fit(X_train, y_train) # Evaluate the classifier on the testing data y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy of SVM classifier: {accuracy:.2f}\\")"},{"question":"You are tasked with creating a Python function that converts a given datetime from one time zone to another using the `ZoneInfo` class from the `zoneinfo` module. # Function Signature: ```python def convert_timezone(dt: datetime, from_tz: str, to_tz: str) -> datetime: ``` # Input: - `dt`: a `datetime` object that is naive (does not contain timezone information). - `from_tz`: a string representing the time zone of `dt` in IANA format (e.g., \\"America/Los_Angeles\\"). - `to_tz`: a string representing the target time zone in IANA format (e.g., \\"Europe/London\\"). # Output: - Returns a `datetime` object that is aware (contains timezone information) and represents the same point in time in the target time zone. # Constraints: - You must handle cases where the provided time zone key does not exist by raising `ZoneInfoNotFoundError`. - You should consider daylight saving time transitions during the conversion. - Do not use third-party libraries other than `zoneinfo` and `datetime`. - The function should handle large sets of valid and invalid IANA time zone inputs gracefully. # Example: ```python from datetime import datetime # Example usage naive_dt = datetime(2023, 3, 28, 15, 30) # March 28, 2023, 15:30 (naive datetime) result = convert_timezone(naive_dt, \\"America/New_York\\", \\"Europe/London\\") print(result) # Output should be: 2023-03-28 19:30:00+01:00 [Europe/London] considering DST # Invalid time zone try: result = convert_timezone(naive_dt, \\"Invalid/TimeZone\\", \\"Europe/London\\") except zoneinfo.ZoneInfoNotFoundError as e: print(e) # Expected output: Invalid time zone: Invalid/TimeZone ``` # Notes: 1. Ensure that your implementation handles both standard and daylight saving times correctly. 2. Provide appropriate error messages for invalid time zones using `ZoneInfoNotFoundError`.","solution":"from datetime import datetime import zoneinfo from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def convert_timezone(dt: datetime, from_tz: str, to_tz: str) -> datetime: Convert datetime from one timezone to another using ZoneInfo. :param dt: naive datetime object :param from_tz: string representing the source time zone in IANA format :param to_tz: string representing the target time zone in IANA format :return: datetime object aware in the target timezone try: from_zone = ZoneInfo(from_tz) except ZoneInfoNotFoundError: raise ZoneInfoNotFoundError(f\\"Invalid time zone: {from_tz}\\") try: to_zone = ZoneInfo(to_tz) except ZoneInfoNotFoundError: raise ZoneInfoNotFoundError(f\\"Invalid time zone: {to_tz}\\") dt_aware = dt.replace(tzinfo=from_zone) return dt_aware.astimezone(to_zone)"},{"question":"Objective: In this exercise, you are required to demonstrate your understanding of handling missing data using Pandas by implementing a function that performs various operations on a given DataFrame. You will need to clean the data by filling missing values appropriately and make some calculations. Problem Statement: 1. Given a DataFrame `df`, conduct the following operations: - Identify the positions of missing values in the DataFrame and return a DataFrame containing the same structure but with `True` at positions of missing values and `False` elsewhere. - Fill missing values in numerical columns using the mean of that column. - Fill missing values in boolean columns using `False`. - Interpolate missing values in datetime columns using the \\"linear\\" method. - Replace missing values in string columns with the string `\\"missing\\"`. - For each column, calculate and return a DataFrame containing the count of missing values before and after performing the above fill operations. Function Signature: ```python import pandas as pd def handle_missing_data(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame): This function takes a DataFrame and handles missing data with various methods. It returns a tuple containing: - A DataFrame of the same structure with boolean values indicating the location of missing values. - A DataFrame summarizing the count of missing values before and after the fill operations. Parameters: ---------- df : pd.DataFrame Input DataFrame with potentially missing data. Returns: ------- - df_missing_locs : pd.DataFrame DataFrame indicating the positions of missing values. - missing_counts : pd.DataFrame DataFrame summarizing the count of missing values before and after handling. pass ``` Example Usage: ```python import pandas as pd import numpy as np data = { \'A\': [1, 2, np.nan, 4], \'B\': [pd.Timestamp(\'20210101\'), pd.Timestamp(\'20210102\'), pd.NaT, pd.Timestamp(\'20210104\')], \'C\': [True, np.nan, False, True], \'D\': [\'foo\', \'bar\', None, \'baz\'] } df = pd.DataFrame(data) df_missing_locs, missing_counts = handle_missing_data(df) print(\\"Missing Locations DataFrame:n\\", df_missing_locs) print(\\"Missing Counts DataFrame:n\\", missing_counts) ``` Constraints: - You must handle missing values using the appropriate method for the data type as outlined in the problem statement. - The function should work efficiently even for large DataFrames. Output: 1. `df_missing_locs`: DataFrame of boolean values indicating the positions of missing values. 2. `missing_counts`: DataFrame summarizing the count of missing values before and after handling for each column. Performance: Your implementation should efficiently handle DataFrames with up to 100,000 rows and 50 columns.","solution":"import pandas as pd import numpy as np def handle_missing_data(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame): This function takes a DataFrame and handles missing data with various methods. It returns a tuple containing: - A DataFrame of the same structure with boolean values indicating the location of missing values. - A DataFrame summarizing the count of missing values before and after performing the above fill operations. Parameters: ---------- df : pd.DataFrame Input DataFrame with potentially missing data. Returns: ------- - df_missing_locs : pd.DataFrame DataFrame indicating the positions of missing values. - missing_counts : pd.DataFrame DataFrame summarizing the count of missing values before and after handling. # Detecting missing values in the DataFrame df_missing_locs = df.isna() # Count missing values before filling missing_before = df.isna().sum() # Handling missing values according to their types for column in df.columns: if df[column].dtype == np.number: df[column].fillna(df[column].mean(), inplace=True) elif df[column].dtype == bool: df[column].fillna(False, inplace=True) elif df[column].dtype == \'<M8[ns]\': df[column].interpolate(method=\'linear\', inplace=True) elif df[column].dtype == object: df[column].fillna(\'missing\', inplace=True) # Count missing values after filling missing_after = df.isna().sum() # Creating the summary missing counts DataFrame missing_counts = pd.DataFrame({ \'Missing Before\': missing_before, \'Missing After\': missing_after }) return df_missing_locs, missing_counts"},{"question":"**Custom Rolling Statistics with Pandas** Objective Create a function that computes custom rolling and expanding statistics for a given time series dataset using pandas. This will assess your understanding of both fundamental and advanced pandas window operations. Problem Statement You are provided with a time series dataset in the form of a pandas DataFrame where one column represents the date (`\'Date\'`), and another column represents some numerical value (`\'Value\'`). Your task is to implement a function that computes specific rolling and expanding window statistics and returns the resulting DataFrame. The function should be named `compute_window_statistics` and should have the following signature: ```python def compute_window_statistics(df: pd.DataFrame, column: str, window_size: int) -> pd.DataFrame: pass ``` Parameters - `df` (pandas.DataFrame): The input DataFrame containing at least two columns `\'Date\'` and `column`. - `column` (str): The name of the column on which to perform the window operations. - `window_size` (int): The size of the rolling window. Expected output The function should return a new DataFrame that includes the following columns: 1. `\'Date\'`: The original date column. 2. `\'Value\'`: The original value column. 3. `\'Rolling_Mean\'`: The rolling mean of the specified column over the given window size. 4. `\'Rolling_Std\'`: The rolling standard deviation of the specified column over the given window size. 5. `\'Expanding_Mean\'`: The expanding mean of the specified column. 6. `\'Expanding_Var\'`: The expanding variance of the specified column. Constraints and Considerations - Ensure that the resulting DataFrame preserves the original order of dates. - Handle edge cases where the rolling window cannot be applied (e.g., the window size is larger than the length of the DataFrame). - The rolling statistics should have NaN values for the initial positions where the window cannot be fully applied. - Your implementation should efficiently handle large datasets. Example ```python # Example usage import pandas as pd data = { \'Date\': pd.date_range(start=\'2023-01-01\', periods=10, freq=\'D\'), \'Value\': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100] } df = pd.DataFrame(data) # Compute rolling and expanding statistics with a window size of 3 result_df = compute_window_statistics(df, column=\'Value\', window_size=3) print(result_df) ``` Expected output: ``` Date Value Rolling_Mean Rolling_Std Expanding_Mean Expanding_Var 0 2023-01-01 10 NaN NaN 10.0 NaN 1 2023-01-02 20 NaN NaN 15.0 50.0 2 2023-01-03 30 20.0 10.000000 20.0 100.0 3 2023-01-04 40 30.0 10.000000 25.0 166.67 4 2023-01-05 50 40.0 10.000000 30.0 250.0 5 2023-01-06 60 50.0 10.000000 35.0 350.0 6 2023-01-07 70 60.0 10.000000 40.0 466.67 7 2023-01-08 80 70.0 10.000000 45.0 600.0 8 2023-01-09 90 80.0 10.000000 50.0 750.0 9 2023-01-10 100 90.0 10.000000 55.0 916.67 ``` Note: The above output example values for Expanding_Var are rounded to two decimal places for readability.","solution":"import pandas as pd def compute_window_statistics(df: pd.DataFrame, column: str, window_size: int) -> pd.DataFrame: Computes custom rolling and expanding statistics for a given time series dataset. Parameters: df (pandas.DataFrame): The input DataFrame containing at least two columns \'Date\' and `column`. column (str): The name of the column on which to perform the window operations. window_size (int): The size of the rolling window. Returns: pandas.DataFrame: A DataFrame with the original \'Date\', \'Value\' and computed statistics. df = df.copy() # Compute rolling mean and standard deviation df[\'Rolling_Mean\'] = df[column].rolling(window=window_size).mean() df[\'Rolling_Std\'] = df[column].rolling(window=window_size).std() # Compute expanding mean and variance df[\'Expanding_Mean\'] = df[column].expanding().mean() df[\'Expanding_Var\'] = df[column].expanding().var() return df[[\'Date\', column, \'Rolling_Mean\', \'Rolling_Std\', \'Expanding_Mean\', \'Expanding_Var\']]"},{"question":"Problem Statement: You are required to implement a function that takes a list of integers and returns a list of all possible unique permutations of the list. The order of the permutations in the output list should not matter. Python\'s `itertools` module contains tools that could be very useful for this task, particularly for generating permutations efficiently. Function Signature: ```python def generate_permutations(nums: List[int]) -> List[List[int]]: ``` Input: - `nums` (List[int]): A list of integers. List length will be between 1 and 8 (inclusive). Output: - List[List[int]]: A list of lists where each list is a unique permutation of the input list. Example: ```python generate_permutations([1, 2, 3]) ``` Output: ```python [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1]] ``` Constraints: - The input list will have unique integers. - The function should handle generating permutations efficiently. - You are expected to use tools from the `itertools` module. Guidelines: - Use the `itertools` module effectively. - Ensure the function runs efficiently given the constraints. - Avoid using manual recursive approaches for permutation generation.","solution":"from itertools import permutations from typing import List def generate_permutations(nums: List[int]) -> List[List[int]]: Returns all unique permutations of the given list of integers. return [list(p) for p in permutations(nums)]"},{"question":"# Socket Programming Challenge Problem Description You are required to implement a basic client-server application using Python\'s `socket` module. The server should be able to handle multiple clients concurrently using threading. Server Requirements 1. The server should listen on `localhost` and port `65432`. 2. When a client connects, the server should read a message from the client, print the message to the console, and send an acknowledgment back to the client. 3. The server should handle multiple clients at the same time using threading. Client Requirements 1. The client should connect to the server at `localhost` on port `65432`. 2. The client should send a user-provided message to the server and wait for the acknowledgment from the server. 3. The client should print the acknowledgment message received from the server. Constraints - Use Python 3.10 or above. - Ensure proper error handling for network operations. - Properly terminate the threads on the server side when the client disconnects. Function Signatures You are required to implement the following two functions in Python: 1. `server_program()`: This function starts the server and awaits client connections. 2. `client_program(message: str) -> str`: This function connects to the server, sends a message, and waits for an acknowledgment. Example ```python # This is an example use case; it is expected to be run in two separate terminal sessions. # In terminal 1: server_program() # In terminal 2: response = client_program(\\"Hello, Server!\\") print(response) # Expected Output: \\"Acknowledgment received: Hello, Server!\\" ``` Additional Notes - Ensure to use the `socket` and `threading` modules. - You may define additional helper functions if needed. - Provide necessary comments and documentation in your code.","solution":"import socket import threading def handle_client(client_socket): try: # Receive the message from the client message = client_socket.recv(1024).decode(\'utf-8\') print(f\\"Received message from client: {message}\\") # Send acknowledgment back to the client acknowledgment = f\\"Acknowledgment received: {message}\\" client_socket.send(acknowledgment.encode(\'utf-8\')) finally: # Close the client connection client_socket.close() def server_program(): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.bind((\'localhost\', 65432)) server_socket.listen(5) print(\\"Server is listening on port 65432...\\") while True: # Accept a new client connection client_socket, client_address = server_socket.accept() print(f\\"Connection from {client_address} has been established.\\") # Handle the new client in a separate thread client_thread = threading.Thread(target=handle_client, args=(client_socket,)) client_thread.start() def client_program(message: str) -> str: client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((\'localhost\', 65432)) try: # Send the message to the server client_socket.send(message.encode(\'utf-8\')) # Wait for the acknowledgment from the server acknowledgment = client_socket.recv(1024).decode(\'utf-8\') return acknowledgment finally: # Close the client connection client_socket.close()"},{"question":"**Problem Statement:** You are given a dataset with the following columns: `Name`, `Age`, `Score`, and `Grade`. Your task is to write a function that processes this dataset using the `pandas` library and applies various styling techniques to enhance its presentation. The function should highlight the maximum score, apply a gradient background based on ages, and set a caption to the table. Finally, it should export the styled DataFrame to an HTML file. **Function Signature:** ```python def style_and_export_dataframe(df: pd.DataFrame, output_file: str) -> None: pass ``` **Input:** - `df`: A `pandas` DataFrame with the columns `Name`, `Age`, `Score`, and `Grade`. - `output_file`: A string representing the file path where the styled DataFrame should be saved as an HTML file. **Constraints:** - `df` will have at least one row. - `output_file` should be a valid file path to save the HTML output. - Assume all columns are of appropriate types (`Name` and `Grade` as strings, `Age` and `Score` as integers or floats). **Output:** - The function should not return any value, but it should create an HTML file at the specified path with the styled DataFrame. **Example Usage:** ```python import pandas as pd data = { \'Name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eva\'], \'Age\': [24, 30, 22, 35, 28], \'Score\': [88, 92, 85, 95, 90], \'Grade\': [\'B\', \'A\', \'B\', \'A\', \'A\'] } df = pd.DataFrame(data) style_and_export_dataframe(df, \\"styled_table.html\\") ``` **Expected Behavior:** - The maximum score should be highlighted in the DataFrame. - The age column should have a gradient background color indicating the range of values. - A caption \\"Student Performance Table\\" should be set for the table. - The resulting styled DataFrame should be saved as an HTML file at the path \\"styled_table.html\\". **Hints:** - Use `Styler.highlight_max` to highlight the maximum score. - Use `Styler.background_gradient` to apply a gradient background to the \'Age\' column. - Use `Styler.set_caption` to set the table caption. - Use `Styler.to_html` to save the styled DataFrame to an HTML file.","solution":"import pandas as pd def style_and_export_dataframe(df: pd.DataFrame, output_file: str) -> None: Styles the DataFrame and exports it to an HTML file. - Highlights the maximum score. - Applies a gradient background on the Age column. - Sets a caption to the table. Parameters: df (pd.DataFrame): The DataFrame to be styled. output_file (str): The file path to save the styled DataFrame. styled_df = ( df.style .highlight_max(subset=[\'Score\'], color=\'yellow\') .background_gradient(subset=[\'Age\'], cmap=\'viridis\') .set_caption(\'Student Performance Table\') ) styled_df.to_html(output_file)"},{"question":"**Coding Assessment Question:** # Task: You are required to write a function `organize_files(src_folder: str, dst_folder: str, archive_name: str) -> None` that performs the following operations: 1. Recursively copy all files and directories from `src_folder` to `dst_folder`. 2. In the `dst_folder`, create a folder named `archives`. 3. Move all `.txt` files from `dst_folder` into the `archives` folder. 4. Create a `.tar` archive (using `shutil.make_archive`) of the `archives` folder with the name `archive_name`. 5. Delete the `archives` folder after archiving. # Constraints: - The paths provided (`src_folder`, `dst_folder`, and `archive_name`) will be valid and exist on the filesystem. - The `archive_name` should not include the `.tar` extension, as `shutil.make_archive` will append it automatically. - Ensure your function handles permissions errors gracefully by catching any exceptions that occur during file operations and printing an appropriate error message. # Input: - `src_folder` (str): The source directory containing files and/or directories to be copied. - `dst_folder` (str): The destination directory to copy files to. - `archive_name` (str): The name (without extension) for the tar archive to be created. # Output: - The function does not return any value. It performs the described operations on the filesystem. # Example: ```python organize_files(\'/path/to/source\', \'/path/to/destination\', \'my_archive\') ``` # Note: - Ensure you use appropriate `shutil` functions to complete the task. - The resulting archive should be available in the `dst_folder` with the name `my_archive.tar`. # Implementation: ```python import shutil import os def organize_files(src_folder: str, dst_folder: str, archive_name: str) -> None: try: # Recursively copy the source folder to the destination folder shutil.copytree(src_folder, dst_folder, dirs_exist_ok=True) # Create an \'archives\' folder inside the destination folder archives_folder = os.path.join(dst_folder, \'archives\') os.makedirs(archives_folder, exist_ok=True) # Move all \'.txt\' files to the \'archives\' folder for root, dirs, files in os.walk(dst_folder): for file in files: if file.endswith(\'.txt\'): src_file = os.path.join(root, file) dst_file = os.path.join(archives_folder, file) shutil.move(src_file, dst_file) # Create a tar archive of the \'archives\' folder shutil.make_archive(os.path.join(dst_folder, archive_name), \'tar\', root_dir=dst_folder, base_dir=\'archives\') # Remove the \'archives\' folder after archiving shutil.rmtree(archives_folder) except Exception as e: print(f\\"An error occurred: {e}\\") ```","solution":"import shutil import os def organize_files(src_folder: str, dst_folder: str, archive_name: str) -> None: try: # Recursively copy the source folder to the destination folder shutil.copytree(src_folder, dst_folder, dirs_exist_ok=True) # Create an \'archives\' folder inside the destination folder archives_folder = os.path.join(dst_folder, \'archives\') os.makedirs(archives_folder, exist_ok=True) # Move all \'.txt\' files to the \'archives\' folder for root, dirs, files in os.walk(dst_folder): for file in files: if file.endswith(\'.txt\'): src_file = os.path.join(root, file) dst_file = os.path.join(archives_folder, file) shutil.move(src_file, dst_file) # Create a tar archive of the \'archives\' folder shutil.make_archive(os.path.join(dst_folder, archive_name), \'tar\', root_dir=dst_folder, base_dir=\'archives\') # Remove the \'archives\' folder after archiving shutil.rmtree(archives_folder) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Pandas Assessment: Hierarchical Indexing** # Problem Statement: You are given a dataset that contains multi-dimensional data regarding sales across multiple regions and product categories over a span of several quarters. The dataset includes the following columns: `Region`, `Product Category`, `Quarter`, and `Sales`. You need to perform the following tasks using pandas\' MultiIndex (hierarchical indexing): 1. **Create a MultiIndex DataFrame:** - Convert the given dataset into a DataFrame with a MultiIndex composed of `Region`, `Product Category`, and `Quarter`. - Assign names to each of these levels in the index. 2. **Perform Data Analysis:** - Compute the total sales per `Region`. - Compute the total sales per `Product Category`. - Compute the total sales per `Quarter`. - Extract sales data for a specific `Region` and `Quarter`. 3. **Advanced Indexing:** - Reindex the DataFrame to add a new region that might not have sales data for all quarters. - Swap the levels of the MultiIndex such that `Product Category` becomes the top level. - Sort the MultiIndex DataFrame based on `Quarter`. # Input Format: ```python data = { \'Region\': [\'North\', \'North\', \'South\', \'South\', \'East\', \'East\', \'West\', \'West\'], \'Product Category\': [\'Electronics\', \'Furniture\', \'Electronics\', \'Furniture\', \'Electronics\', \'Furniture\', \'Electronics\', \'Furniture\'], \'Quarter\': [\'Q1\', \'Q2\', \'Q1\', \'Q2\', \'Q1\', \'Q2\', \'Q1\', \'Q2\'], \'Sales\': [200, 300, 150, 200, 100, 200, 250, 300] } ``` # Output Format: The output should include: 1. The MultiIndex DataFrame. 2. Total sales per `Region`. 3. Total sales per `Product Category`. 4. Total sales per `Quarter`. 5. Sales data for the specified `Region` and `Quarter`. 6. The reindexed DataFrame including a new Region (`Central`), with missing sales data. 7. The DataFrame after swapping the levels of the MultiIndex. 8. The DataFrame sorted by `Quarter`. # Constraints: - Ensure that the DataFrame is properly indexed and named. - Handle missing data gracefully when reindexing. - The levels of the MultiIndex should be sorted as per the specified order. ```python import pandas as pd # Sample data provides data = { \'Region\': [\'North\', \'North\', \'South\', \'South\', \'East\', \'East\', \'West\', \'West\'], \'Product Category\': [\'Electronics\', \'Furniture\', \'Electronics\', \'Furniture\', \'Electronics\', \'Furniture\', \'Electronics\', \'Furniture\'], \'Quarter\': [\'Q1\', \'Q2\', \'Q1\', \'Q2\', \'Q1\', \'Q2\', \'Q1\', \'Q2\'], \'Sales\': [200, 300, 150, 200, 100, 200, 250, 300] } # Your task is to fill in the following functions def create_multiindex_df(data): Create a MultiIndex DataFrame using the provided data. The MultiIndex should be composed of `Region`, `Product Category`, and `Quarter`. pass def total_sales_per_region(df): Compute the total sales per `Region`. pass def total_sales_per_category(df): Compute the total sales per `Product Category`. pass def total_sales_per_quarter(df): Compute the total sales per `Quarter`. pass def sales_data_region_quarter(df, region, quarter): Extract the sales data for a specific `Region` and `Quarter`. pass def reindex_with_new_region(df): Reindex the DataFrame to add a new Region (`Central`). Handle missing sales data gracefully. pass def swap_index_levels(df): Swap the levels of the MultiIndex such that `Product Category` is the top level. pass def sort_by_quarter(df): Sort the MultiIndex DataFrame based on `Quarter`. pass # The sample execution of functions (Note: You need to implement the functions above) df = create_multiindex_df(data) print(\\"MultiIndex DataFrame:n\\", df) print(\\"Total sales per Region:n\\", total_sales_per_region(df)) print(\\"Total sales per Product Category:n\\", total_sales_per_category(df)) print(\\"Total sales per Quarter:n\\", total_sales_per_quarter(df)) print(\\"Sales data for Region=\'North\' and Quarter=\'Q1\':n\\", sales_data_region_quarter(df, \'North\', \'Q1\')) print(\\"Reindexed DataFrame with new Region:n\\", reindex_with_new_region(df)) print(\\"DataFrame after swapping index levels:n\\", swap_index_levels(df)) print(\\"DataFrame sorted by Quarter:n\\", sort_by_quarter(df)) ``` # Note: Make sure to test each function with appropriate data and check if the functions are working correctly by printing the results.","solution":"import pandas as pd def create_multiindex_df(data): Create a MultiIndex DataFrame using the provided data. The MultiIndex should be composed of `Region`, `Product Category`, and `Quarter`. df = pd.DataFrame(data) df.set_index([\'Region\', \'Product Category\', \'Quarter\'], inplace=True) return df def total_sales_per_region(df): Compute the total sales per `Region`. return df.groupby(level=\'Region\').sum() def total_sales_per_category(df): Compute the total sales per `Product Category`. return df.groupby(level=\'Product Category\').sum() def total_sales_per_quarter(df): Compute the total sales per `Quarter`. return df.groupby(level=\'Quarter\').sum() def sales_data_region_quarter(df, region, quarter): Extract the sales data for a specific `Region` and `Quarter`. return df.xs((region, quarter), level=(\'Region\', \'Quarter\')) def reindex_with_new_region(df): Reindex the DataFrame to add a new Region (`Central`). Handle missing sales data gracefully. new_index = pd.MultiIndex.from_product([[\'North\', \'South\', \'East\', \'West\', \'Central\'], [\'Electronics\', \'Furniture\'], [\'Q1\', \'Q2\']], names=[\'Region\', \'Product Category\', \'Quarter\']) return df.reindex(new_index).fillna(0) def swap_index_levels(df): Swap the levels of the MultiIndex such that `Product Category` is the top level. return df.swaplevel(\'Region\', \'Product Category\') def sort_by_quarter(df): Sort the MultiIndex DataFrame based on `Quarter`. return df.sort_index(level=\'Quarter\')"},{"question":"<|Analysis Begin|> The provided documentation focuses primarily on the `pointplot` function in Seaborn, demonstrating various uses and customizations such as: 1. Basic usage of `pointplot` with a single categorical variable and continuous variable. 2. Grouping data by an additional categorical variable and differentiating them by color. 3. Redundantly coding the hue variable with different markers and linestyles. 4. Using error bars to represent the standard deviation. 5. Customizing the appearance of the plot, including colors, markers, and error bars. 6. Dodging to separate overlapping points. 7. Creating plots with aggregated data either from a pivoted dataset or a single dimension. These points emphasize the flexibility and advanced features available for visualizing data using Seaborn\'s `pointplot` function. Based on this, we can create a question that tests students\' understanding of these concepts, including customizing plots, creating plots with multiple variables, and handling data aggregation. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Advanced Customization and Usage of Seaborn Pointplot You are given two datasets: - `penguins`, which contains information on penguin species. - `flights`, which contains data on the number of passengers flying in different months over several years. You are required to perform the following tasks using Seaborn\'s `pointplot` function: a. **Plotting and Customization:** 1. Create a point plot to visualize the average body mass (`body_mass_g`) of penguins grouped by species (`species`), with error bars representing the standard deviation. Customize the plot to use different markers for each species, and ensure the plot has a clear title and axis labels. 2. Add a second layer of grouping by sex (`sex`) and use different colors for each sex. Include appropriate legend and differentiate the sex variable with different markers and linestyles. b. **Data Aggregation and Visualization:** 3. Pivot the `flights` dataset to get a wide-format DataFrame where the index is `year`, columns are `month`, and values are `passengers`. Create a point plot to visualize the number of passengers flying each month, with error bars representing the 95% confidence intervals. 4. For one month of your choice, create a point plot showing the number of passengers each year. Customize the tick labels to show the last two digits of the year and highlight the year 1955 with a red star marker. Provide the final script including all required customizations and plots. Ensure that the code is clear, well-documented, and includes relevant comments explaining each part of the solution. # Expected Input and Output Formats: - **Input:** None, the datasets `penguins` and `flights` will be loaded within the script. - **Output:** Four well-customized plots as specified in the tasks above, ensuring proper legend placement, titles, axis labels, and relevant customizations. # Constraints and Limitations: - Assume the datasets can be loaded directly using Seaborn\'s `load_dataset` method. - Ensure the plots are easy to interpret and visually appealing. - Add comprehensive comments to your code explaining key steps and customizations. # Performance Requirements: - The script should execute within a reasonable time frame and handle the provided datasets efficiently.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_body_mass(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the first point plot plt.figure(figsize=(10, 6)) sns.pointplot(data=penguins, x=\\"species\\", y=\\"body_mass_g\\", ci=\\"sd\\", markers=[\\"o\\", \\"s\\", \\"D\\"], linestyles=[\\"-\\", \\"--\\", \\"-.\\"], hue=\\"species\\") plt.title(\'Average Body Mass of Penguins by Species with Std. Dev\') plt.xlabel(\'Species\') plt.ylabel(\'Body Mass (g)\') plt.legend(title=\'Species\') plt.show() # Create second point plot with an additional hue for sex plt.figure(figsize=(10, 6)) sns.pointplot(data=penguins, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", dodge=True, markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], ci=\\"sd\\") plt.title(\'Average Body Mass of Penguins by Species and Sex with Std. Dev\') plt.xlabel(\'Species\') plt.ylabel(\'Body Mass (g)\') plt.legend(title=\'Sex\') plt.show() def plot_flight_passengers(): # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Pivot the flights data flights_pivot = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Plotting passengers per month with confidence intervals plt.figure(figsize=(14, 8)) sns.pointplot(data=flights, x=\\"month\\", y=\\"passengers\\", ci=95) plt.title(\'Average Number of Passengers by Month with 95% CI\') plt.xlabel(\'Month\') plt.ylabel(\'Number of Passengers\') plt.show() # Plotting for a specific month (e.g., \\"July\\") plt.figure(figsize=(10, 6)) sns.pointplot(data=flights[flights[\\"month\\"] == \\"July\\"], x=\\"year\\", y=\\"passengers\\", ci=None) plt.title(\'Number of Passengers in July by Year\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.xticks(rotation=45) plt.scatter(flights[flights[\\"month\\"] == \\"July\\"][\\"year\\"], flights[flights[\\"month\\"] == \\"July\\"][\\"passengers\\"], c=[\\"red\\" if year == 1955 else \\"blue\\" for year in flights[flights[\\"month\\"] == \\"July\\"][\\"year\\"]], s=100, marker=\\"*\\", edgecolor=\\"black\\", zorder=5) plt.xticks(ticks=range(1949, 1961), labels=[str(year)[2:] for year in range(1949, 1961)]) plt.show() # Run the functions to produce the plots plot_penguin_body_mass() plot_flight_passengers()"},{"question":"**Objective:** Implement a multi-process training pipeline using PyTorch\'s multiprocessing module. This will assess your understanding of sharing model parameters between processes, managing data loading in a multiprocessing environment, and ensuring proper use of multiprocessing techniques to avoid common pitfalls. **Problem Statement:** Create a multi-process training function to train a simple neural network using the Hogwild method. You need to ensure that tensors are correctly shared among processes, avoid deadlocks using best practices, and manage CPU resources to prevent oversubscription. **Requirements:** 1. **Model Definition**: Define a simple neural network using the `torch.nn.Module`. 2. **Data Preparation**: Use a dummy dataset to simulate input data for training. 3. **Training Function**: * Implement the training function that will be run by each process. * Use multiprocessing best practices to avoid common issues like deadlocks. * Share the model parameters among processes. 4. **Processes Management**: * Use PyTorch\'s `torch.multiprocessing` to spawn multiple processes for training. * Ensure proper synchronization and sharing mechanisms are in place to share model parameters. 5. **Performance Optimization**: * Manage the number of threads to avoid CPU oversubscription. * Use `torch.set_num_threads()` appropriately. **Constraints:** - You should run the main training loop in parallel across multiple processes. - Properly handle the sharing of model parameters using shared memory. - Ensure the solution avoids deadlocks and efficiently manages CPU resources. **Input**: - Number of processes to spawn for training (`num_processes`). **Output**: - Training loss at each epoch for each process. **Function Signature**: ```python import torch import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).init() # Define your model layers here def forward(self, x): # Define the forward pass pass def train(rank, model, dataloader, optimizer): # Implement the training loop to be executed by each process pass def main(num_processes: int): # Implement the main function to manage processes pass if __name__ == \\"__main__\\": num_processes = 4 # Example main(num_processes) ``` **Steps to Implement**: 1. Define a `SimpleModel` class with a neural network architecture. 2. Create a data loader that provides input data. 3. Implement the `train` function that will be run by each process: - Use `optimizer.step()` to update shared model parameters. 4. Implement the `main` function to: - Initialize the model and move it to shared memory. - Initialize multiple processes using `mp.Process`. - Ensure proper synchronization and termination of processes. 5. Set the appropriate number of threads within each process using `torch.set_num_threads()` to avoid CPU oversubscription.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp from torch.utils.data import DataLoader, TensorDataset class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train(rank, model, dataloader, optimizer): torch.set_num_threads(1) model.train() for epoch in range(5): # Run for 5 epochs epoch_loss = 0.0 for data, target in dataloader: optimizer.zero_grad() output = model(data) loss = nn.MSELoss()(output, target) loss.backward() optimizer.step() epoch_loss += loss.item() print(f\\"Process {rank}, Epoch {epoch}, Loss: {epoch_loss / len(dataloader)}\\") def main(num_processes: int): torch.manual_seed(0) # Generating dummy data data = torch.randn(100, 10) target = torch.randn(100, 1) dataset = TensorDataset(data, target) dataloader = DataLoader(dataset, batch_size=10, shuffle=True) model = SimpleModel() model.share_memory() optimizer = optim.SGD(model.parameters(), lr=0.01) processes = [] for rank in range(num_processes): p = mp.Process(target=train, args=(rank, model, dataloader, optimizer)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": num_processes = 4 # Example main(num_processes)"},{"question":"Objective Implement a Python function that performs a series of operations on text files within a directory, demonstrating comprehension of file manipulation modules in Python 3.10. Problem Statement You are given a directory containing multiple `.txt` files. You are required to implement a function that does the following: 1. **Read all `.txt` files**: Iterate over all `.txt` files in the given directory. 2. **Count Words**: For each file, count the total number of words. 3. **Find and Replace**: Replace all occurrences of a given word `target_word` with another word `replacement_word` in all the files. 4. **Backup Files**: Create a backup of the original files in a sub-directory called `backup`. 5. **Report**: Generate a summary report as a `.txt` file in the root directory summarizing the number of words in each file before and after the replacement. Function Signature ```python def process_text_files(directory: str, target_word: str, replacement_word: str) -> None: pass ``` Input 1. `directory`: A string representing the path to the directory containing `.txt` files. 2. `target_word`: A string representing the word to be replaced. 3. `replacement_word`: A string representing the word to replace with. Output - The function should not return anything. - The function should create and modify files within the input directory based on the defined operations. Constraints - Assume the directory path exists and contains only `.txt` files. - Words are separated by whitespace. - Backup files should maintain the original structure and content before replacements. Performance Requirements - The function should efficiently handle directories containing up to 1,000 `.txt` files, each up to 10MB in size. - Use appropriate error handling for file operations. Example Usage ```python process_text_files(\'/path/to/directory\', \'hello\', \'hi\') ``` After running the function: - All occurrences of the word `hello` will be replaced with `hi` in each `.txt` file. - A backup of the original files will be stored in `/path/to/directory/backup`. - A summary report `report.txt` will be created in `/path/to/directory`, listing the number of words in each file before and after replacements.","solution":"import os import shutil def process_text_files(directory: str, target_word: str, replacement_word: str) -> None: # Create the backup directory backup_dir = os.path.join(directory, \'backup\') os.makedirs(backup_dir, exist_ok=True) # Initialize the report content report_lines = [] # Iterate over all `.txt` files in the directory for filename in os.listdir(directory): if filename.endswith(\'.txt\'): file_path = os.path.join(directory, filename) backup_path = os.path.join(backup_dir, filename) # Backup the original file shutil.copy(file_path, backup_path) # Read the contents of the file and perform the necessary operations with open(file_path, \'r\') as file: content = file.read() # Count the number of words before replacement words_before_replacement = content.split() word_count_before = len(words_before_replacement) # Replace the target_word with replacement_word new_content = content.replace(target_word, replacement_word) # Write the new content back to the file with open(file_path, \'w\') as file: file.write(new_content) # Count the number of words after replacement words_after_replacement = new_content.split() word_count_after = len(words_after_replacement) # Add the word counts to the report report_lines.append(f\\"{filename}: {word_count_before} -> {word_count_after}n\\") # Write the report to the report.txt file in the root directory with open(os.path.join(directory, \'report.txt\'), \'w\') as report_file: report_file.writelines(report_lines)"},{"question":"You are to write a Python function `parse_command_line_options(args, shortopts, longopts)` that utilizes the `getopt` module to parse command-line options and arguments. Your function should: 1. Use `getopt.gnu_getopt()` for parsing to allow for interleaved options and arguments. 2. Handle errors gracefully and provide informative error messages. 3. Return a tuple with two elements: - A dictionary where keys are the options and values are their corresponding arguments. - A list of non-option arguments. # Function Signature ```python def parse_command_line_options(args: list, shortopts: str, longopts: list) -> tuple: pass ``` # Input - `args`: A list of strings representing command-line arguments (e.g., `[\'--output=file.txt\', \'-v\', \'inputfile\']`). - `shortopts`: A string of option letters that the script wants to recognize (e.g., `\'ho:v\'`). - `longopts`: A list of strings with the names of the long options which should be supported (e.g., `[\'help\', \'output=\']`). # Output - A tuple consisting of: - A dictionary mapping options to their corresponding arguments or an empty string if no argument is provided. - A list of non-option arguments. # Constraints - The options may be interleaved with non-option arguments. - Options may have required arguments, as indicated by a colon (`:`) in `shortopts` and an equal sign (`=`) in `longopts`. - Handle and raise appropriate error messages using `getopt.GetoptError` if unrecognized options or missing arguments are detected. # Example ```python args = [\'--output=file.txt\', \'-v\', \'inputfile\'] shortopts = \'ho:v\' longopts = [\'help\', \'output=\'] output = parse_command_line_options(args, shortopts, longopts) # Expected output: # ({\\"--output\\": \\"file.txt\\", \\"-v\\": \\"\\"}, [\\"inputfile\\"]) ``` # Implementation Note Your function should properly handle cases where: - An option appears multiple times. - A short option with a required argument is missing its argument. - A long option with a required argument is missing its argument. - Unrecognized options are provided.","solution":"import getopt def parse_command_line_options(args, shortopts, longopts): try: opts, nonopts = getopt.gnu_getopt(args, shortopts, longopts) except getopt.GetoptError as err: print(f\\"Error parsing options: {err}\\") return ({}, []) opts_dict = {} for opt, arg in opts: opts_dict[opt] = arg if arg else \\"\\" return opts_dict, nonopts"},{"question":"**XML Data Processing: Employee Management System** You are tasked with developing a Python function that performs various operations on an XML file representing an employee management system. The XML structure contains multiple departments, and each department has multiple employees. Here is an example of the XML structure: ```xml <company> <department name=\\"HR\\"> <employee id=\\"1001\\"> <name>John Doe</name> <position>Manager</position> <salary>70000</salary> </employee> <employee id=\\"1002\\"> <name>Jane Smith</name> <position>Analyst</position> <salary>50000</salary> </employee> </department> <department name=\\"Engineering\\"> <employee id=\\"2001\\"> <name>Emily Rogers</name> <position>Engineer</position> <salary>80000</salary> </employee> <employee id=\\"2002\\"> <name>Mark Brown</name> <position>Intern</position> <salary>30000</salary> </employee> </department> </company> ``` # Function Requirements: 1. **parse_xml(file_path: str) -> ElementTree.Element:** - Parses the XML file and returns the root element of the XML tree. 2. **get_employee_details(root: ElementTree.Element, emp_id: str) -> dict:** - Retrieves the details of an employee given their ID. - Returns a dictionary with keys: `id`, `name`, `position`, and `salary`. - Raise a `ValueError` if the employee ID is not found. 3. **update_employee_salary(root: ElementTree.Element, emp_id: str, new_salary: int) -> None:** - Updates the salary of the employee with the given ID. - Raise a `ValueError` if the employee ID is not found. 4. **add_employee(root: ElementTree.Element, department_name: str, emp_id: str, name: str, position: str, salary: int) -> None:** - Adds a new employee to the specified department. - Raise a `ValueError` if the department name is not found or if an employee with the given ID already exists. 5. **delete_employee(root: ElementTree.Element, emp_id: str) -> None:** - Deletes the employee with the given ID from the XML tree. - Raise a `ValueError` if the employee ID is not found. 6. **save_xml(root: ElementTree.Element, output_path: str) -> None:** - Saves the modified XML tree back to a file. # Input and Output Formats: - **parse_xml(file_path: str) -> ElementTree.Element:** - Input: `file_path` (string) - Path to the XML file. - Output: Returns the root element of the XML tree. - **get_employee_details(root: ElementTree.Element, emp_id: str) -> dict:** - Input: `root` (ElementTree.Element) - The root element of the XML tree. `emp_id` (string) - The ID of the employee. - Output: Returns a dictionary with the employee details. - **update_employee_salary(root: ElementTree.Element, emp_id: str, new_salary: int) -> None:** - Input: `root` (ElementTree.Element) - The root element of the XML tree. `emp_id` (string) - The ID of the employee. `new_salary` (int) - The new salary amount. - Output: None. - **add_employee(root: ElementTree.Element, department_name: str, emp_id: str, name: str, position: str, salary: int) -> None:** - Input: `root` (ElementTree.Element) - The root element of the XML tree. `department_name` (string) - The name of the department. `emp_id` (string) - The ID of the new employee. `name` (string) - The name of the new employee. `position` (string) - The position of the new employee. `salary` (int) - The salary of the new employee. - Output: None. - **delete_employee(root: ElementTree.Element, emp_id: str) -> None:** - Input: `root` (ElementTree.Element) - The root element of the XML tree. `emp_id` (string) - The ID of the employee. - Output: None. - **save_xml(root: ElementTree.Element, output_path: str) -> None:** - Input: `root` (ElementTree.Element) - The root element of the XML tree. `output_path` (string) - Path to save the modified XML file. - Output: None. # Constraints: - Assume the XML file is well-formed. - Employee IDs and department names are unique within the XML file. # Performance Requirements: - Ensure efficient parsing and modification of the XML tree to handle large XML files. **Note:** You may use Python\'s `xml.etree.ElementTree` library to implement this functionality.","solution":"import xml.etree.ElementTree as ET def parse_xml(file_path: str) -> ET.Element: Parses the XML file and returns the root element of the XML tree. tree = ET.parse(file_path) return tree.getroot() def get_employee_details(root: ET.Element, emp_id: str) -> dict: Retrieves the details of an employee given their ID. Returns a dictionary with keys: id, name, position, and salary. Raise a ValueError if the employee ID is not found. for emp in root.iter(\'employee\'): if emp.attrib[\'id\'] == emp_id: details = { \'id\': emp.attrib[\'id\'], \'name\': emp.find(\'name\').text, \'position\': emp.find(\'position\').text, \'salary\': int(emp.find(\'salary\').text) } return details raise ValueError(f\\"Employee with id {emp_id} not found.\\") def update_employee_salary(root: ET.Element, emp_id: str, new_salary: int) -> None: Updates the salary of the employee with the given ID. Raise a ValueError if the employee ID is not found. for emp in root.iter(\'employee\'): if emp.attrib[\'id\'] == emp_id: emp.find(\'salary\').text = str(new_salary) return raise ValueError(f\\"Employee with id {emp_id} not found.\\") def add_employee(root: ET.Element, department_name: str, emp_id: str, name: str, position: str, salary: int) -> None: Adds a new employee to the specified department. Raise a ValueError if the department name is not found or if an employee with the given ID already exists. for emp in root.iter(\'employee\'): if emp.attrib[\'id\'] == emp_id: raise ValueError(f\\"Employee with id {emp_id} already exists.\\") for dept in root.iter(\'department\'): if dept.attrib[\'name\'] == department_name: new_emp = ET.Element(\'employee\', id=emp_id) ET.SubElement(new_emp, \'name\').text = name ET.SubElement(new_emp, \'position\').text = position ET.SubElement(new_emp, \'salary\').text = str(salary) dept.append(new_emp) return raise ValueError(f\\"Department with name {department_name} not found.\\") def delete_employee(root: ET.Element, emp_id: str) -> None: Deletes the employee with the given ID from the XML tree. Raise a ValueError if the employee ID is not found. for dept in root.iter(\'department\'): for emp in dept.findall(\'employee\'): if emp.attrib[\'id\'] == emp_id: dept.remove(emp) return raise ValueError(f\\"Employee with id {emp_id} not found.\\") def save_xml(root: ET.Element, output_path: str) -> None: Saves the modified XML tree back to a file. tree = ET.ElementTree(root) tree.write(output_path)"},{"question":"# Advanced Pandas Groupby Coding Assessment You are provided with a dataset of sales transactions from a fictional company. The dataset has the following columns: 1. `TransactionID` - Unique identifier for each transaction. 2. `Product` - The product sold in a transaction. 3. `Category` - The category of the product. 4. `Amount` - The sales amount for the transaction. 5. `TransactionDate` - The date and time when the transaction occurred. The `TransactionDate` column is in the datetime format, and the other columns are either categorical or numerical. **Input:** - A DataFrame `df` containing the transaction data. **Objective:** Write a function `process_sales_data(df)` that will process this DataFrame and return a new DataFrame with the following information for each product and category: 1. The total sales amount. 2. The mean sales amount. 3. The standard deviation of the sales amount. 4. The total number of transactions. The resulting DataFrame should: - Have a MultiIndex with `Product` and `Category`. - Include columns `TotalSales`, `MeanSales`, `StdDevSales`, and `TransactionCount`. **Function Signature:** ```python import pandas as pd def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: # Your code here ``` **Constraints:** - The DataFrame `df` will not have missing values in any of its columns. # Example: Given the DataFrame `df`: ``` TransactionID Product Category Amount TransactionDate 0 1 A X 100.0 2023-01-01 09:00:00 1 2 B Y 150.0 2023-01-02 10:00:00 2 3 A X 200.0 2023-01-03 11:00:00 3 4 B Y 50.0 2023-01-04 12:00:00 ``` The function call `process_sales_data(df)` should return a new DataFrame with a MultiIndex and aggregated results: ``` TotalSales MeanSales StdDevSales TransactionCount Product Category A X 300.0 150.0 70.71 2 B Y 200.0 100.0 70.71 2 ``` **Hints:** - Utilize the `groupby` function to group the data by `Product` and `Category`. - Use aggregation functions to compute the total, mean, standard deviation of sales, and count of transactions. - Be sure to return the result with the specified structure and column names.","solution":"import pandas as pd def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: Processes the sales data to calculate total, mean, and standard deviation of sales amount, and the count of transactions for each product and category. Args: df (pd.DataFrame): The input DataFrame with transaction data. Returns: pd.DataFrame: A DataFrame with a MultiIndex for Product and Category, containing aggregated results. # Group by Product and Category grouped = df.groupby([\'Product\', \'Category\']).agg( TotalSales=(\'Amount\', \'sum\'), MeanSales=(\'Amount\', \'mean\'), StdDevSales=(\'Amount\', \'std\'), TransactionCount=(\'TransactionID\', \'count\') ).reset_index() # Set MultiIndex result = grouped.set_index([\'Product\', \'Category\']) return result"},{"question":"**Question: Implementing an Asynchronous Data Logger** You are required to create an asynchronous data logger using Python\'s `asyncio` module. This logger should listen for incoming data on a specified network socket and log the data to a file. The logger needs to support the following functionalities: 1. **Network Server**: - Create a TCP server that listens for incoming connections on a given host and port. - For each connection, read data asynchronously and log it to a file. 2. **Logging**: - Log each received data packet to a file along with a timestamp. 3. **Graceful Shutdown**: - Ensure that when the server is stopped, all existing connections are properly closed and remaining data is flushed to the log file. - Handle `SIGINT` (Ctrl+C) to allow graceful shutdown of the server. # Specification - **Function Signature**: ```python async def run_data_logger(host: str, port: int, log_file: str): ``` - **Parameters**: - `host` (str): The host address on which the server will listen (e.g., \\"127.0.0.1\\"). - `port` (int): The port number on which the server will listen. - `log_file` (str): The path to the file where logs should be written. - **Constraints**: - The server must handle multiple clients simultaneously. - Use `asyncio` methods for network communication and running tasks. # Example Usage ```python import asyncio async def main(): await run_data_logger(\'localhost\', 8888, \'data.log\') if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Implementation Notes 1. Use `asyncio.start_server()` to create the TCP server. 2. For each client connection, create a coroutine that reads data and writes to the log file using asynchronous file I/O. 3. Implement a signal handler to listen for `SIGINT` and stop the server gracefully. 4. Ensure the log file is properly flushed and closed on shutdown. 5. Use context managers and proper exception handling to manage resources. Implement the `run_data_logger` function to meet the above requirements.","solution":"import asyncio import signal import datetime async def handle_client(reader, writer, log_file): with open(log_file, \'a\') as f: while True: data = await reader.read(100) if not data: break timestamp = datetime.datetime.now().isoformat() f.write(f\'{timestamp} - {data.decode()}n\') await f.flush() # ensure data is written to the file immediately writer.close() await writer.wait_closed() async def run_data_logger(host, port, log_file): server = await asyncio.start_server( lambda r, w: handle_client(r, w, log_file), host, port ) loop = asyncio.get_running_loop() stop_event = asyncio.Event() def shutdown(): stop_event.set() loop.add_signal_handler(signal.SIGINT, shutdown) async with server: await stop_event.wait() server.close() await server.wait_closed()"},{"question":"# Custom Profiler Implementation in PyTorch Objective Your task is to implement a custom profiler for monitoring the execution time of a specific PyTorch operator across multiple runs of a model. Requirements 1. **Function Definition** - Implement a function `custom_profiler(model, inputs, operator_name, sample_rate)` that takes the following parameters: - `model`: A PyTorch model. - `inputs`: Inputs to the model. - `operator_name`: The name of the PyTorch operator to monitor. - `sample_rate`: A float value representing the probability of sampling an operator invocation for profiling. 2. **Functionality** - Set up a global callback that triggers on the specified operator. - The callback should log the execution time of the operator in microseconds. - Use the `torch.autograd.profiler` module and `torch::addGlobalCallback` to add your custom profiling hook. 3. **Output** - After running the model with the given inputs, the function should output the total number of times the operator was invoked and the average execution time. 4. **Constraints** - Ensure that the callback incurs minimal overhead. - The callback should be invoked only a fraction of the time as specified by the sample rate parameter. Example Usage ```python import torch import torch.nn as nn # Define a simple model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 10) def forward(self, x): return self.linear(x) # Initialize model and inputs model = SimpleModel() inputs = torch.randn(5, 10) # Define the operator to profile and the sample rate operator_name = \\"aten::linear\\" sample_rate = 0.1 # Call the custom profiler function custom_profiler(model, inputs, operator_name, sample_rate) ``` Notes - The implementation should be self-contained and not require any external dependencies beyond PyTorch. - Remember to clean up any globally set callbacks after the profiling to avoid side effects on future runs. Good luck!","solution":"import torch import time import random def custom_profiler(model, inputs, operator_name, sample_rate): Custom profiler for monitoring the execution time of a specific PyTorch operator across multiple runs. Parameters: - model: A PyTorch model. - inputs: Inputs to the model. - operator_name: The name of the PyTorch operator to monitor. - sample_rate: A float value representing the probability of sampling an operator invocation for profiling. invocations = 0 total_time = 0.0 def pre_hook(self, inputs): self._start_time = time.time() def post_hook(self, inputs, outputs): nonlocal invocations, total_time if random.random() < sample_rate: end_time = time.time() exec_time = (end_time - self._start_time) * 1e6 # Convert to microseconds total_time += exec_time invocations += 1 # Register the forward hooks hook_handles = [] for name, module in model.named_modules(): if operator_name in str(module): hook_handles.append(module.register_forward_pre_hook(pre_hook)) hook_handles.append(module.register_forward_hook(post_hook)) # Run the model with torch.no_grad(): model(inputs) # Unregister the hooks for handle in hook_handles: handle.remove() if invocations > 0: avg_time = total_time / invocations else: avg_time = 0.0 print(f\\"Operator \'{operator_name}\' was invoked {invocations} times.\\") print(f\\"Average execution time: {avg_time:.2f} microseconds.\\")"},{"question":"Coding Assessment Question: You are tasked with creating a custom descriptor in Python that dynamically transforms attribute values when they are set, demonstrating your understanding of both fundamental and advanced aspects of Python\'s descriptor protocol. # Requirements: 1. Define a descriptor class named `TransformDescriptor` that: - Accepts a transformation function during initialization. - When a value is set on this descriptor, it applies the transformation function to the value before storing it. - When the value is retrieved, it returns the transformed value. 2. Implement a class `DemoClass` that utilizes `TransformDescriptor` for its attributes. 3. The `TransformDescriptor` should also include a validation step that ensures the transformed value meets a certain criterion (e.g., it should be of a specific type). # Input: A transformation function and a dictionary representing attributes with their corresponding transformation functions. # Output: An instance of `DemoClass` where attributes have been transformed based on their respective transformation functions. # Constraints: - The transformation function can be any callable that accepts a single argument and returns a transformed value. - If the transformed value does not meet the validation criteria, an exception should be raised. # Example: ```python # Define a sample transformation function def square(x): return x * x def to_upper(s): return s.upper() # Descriptor Class class TransformDescriptor: def __init__(self, transform_func): self.transform_func = transform_func self.value = None def __get__(self, instance, owner): return self.value def __set__(self, instance, value): transformed_value = self.transform_func(value) if not isinstance(transformed_value, (int, str)): raise ValueError(\\"Transformed value must be int or str.\\") self.value = transformed_value # Class using the descriptor class DemoClass: value = TransformDescriptor(square) text = TransformDescriptor(to_upper) # Create an instance of DemoClass demo = DemoClass() demo.value = 4 demo.text = \\"hello\\" # Accessing attributes should return transformed values print(demo.value) # Output: 16 print(demo.text) # Output: HELLO ``` Ensure that the implementation includes thorough validation and testing of the descriptor with different types of transformation functions.","solution":"class TransformDescriptor: def __init__(self, transform_func): self.transform_func = transform_func self.value = None def __get__(self, instance, owner): return self.value def __set__(self, instance, value): transformed_value = self.transform_func(value) if not isinstance(transformed_value, (int, str)): raise ValueError(\\"Transformed value must be an int or str.\\") self.value = transformed_value class DemoClass: def __init__(self, value_transform_func, text_transform_func): self._value_descriptor = TransformDescriptor(value_transform_func) self._text_descriptor = TransformDescriptor(text_transform_func) @property def value(self): return self._value_descriptor.__get__(self, DemoClass) @value.setter def value(self, value): self._value_descriptor.__set__(self, value) @property def text(self): return self._text_descriptor.__get__(self, DemoClass) @text.setter def text(self, text): self._text_descriptor.__set__(self, text) # Example transformation functions def square(x): return x * x def to_upper(s): return s.upper() # Create an instance of DemoClass demo = DemoClass(square, to_upper) demo.value = 4 demo.text = \\"hello\\" # Accessing attributes should return transformed values print(demo.value) # Output: 16 print(demo.text) # Output: HELLO"},{"question":"**Challenge: Implement Filesystem Path Processing and Validation** You are required to implement a function that processes and validates filesystem paths. This task involves ensuring that given paths follow specific criteria and handling any issues encountered during processing. # Task: Write a Python function `process_and_validate_paths(paths: list) -> list` that takes a list of file paths (`paths`) and returns a list of valid, processed paths. # Requirements: 1. **Input Type**: - A list of file paths (strings or objects implementing the `os.PathLike` interface). 2. **Output Type**: - A list of valid file paths as strings. 3. **Validation Criteria**: - The path should be valid and accessible on the filesystem. - If a path is invalid (e.g., raises a TypeError during processing), it should be excluded from the output list. 4. **Path Processing**: - Implement the processing based on the function `PyOS_FSPath()` described in the documentation. - The function should resolve the path using `os.fspath()` (the Python interface to `__fspath__()`), which should handle objects implementing the `os.PathLike` interface. 5. **Handling Errors**: - If the path conversion raises a `TypeError`, it should be caught and logged, and the invalid path should be excluded from the result. # Example: ```python import os def process_and_validate_paths(paths): valid_paths = [] for path in paths: try: # Process the path using os.fspath() processed_path = os.fspath(path) if os.path.exists(processed_path): valid_paths.append(processed_path) except TypeError: # Invalid path (e.g., does not implement os.PathLike) pass return valid_paths # Example usage paths = [\'valid_file.txt\', b\'valid_bytes_file.txt\', \'invalidx00path\', SomePathLikeObject()] valid_paths = process_and_validate_paths(paths) print(valid_paths) # Output should be a list of paths that exist ``` # Constraints: - Only valid filesystem paths should be included in the output list. - Handle both string paths and paths offered by objects that implement the `os.PathLike` interface. - Errors should be appropriately managed without disrupting the path processing. # Notes: - You may use `os` module functions and any standard library functions that help accomplish the task. - Ensure to handle operating system-specific constraints, e.g., path length limitations and invalid path characters. # Performance: - The solution should be efficient and handle large lists of paths gracefully. - Consider edge cases, such as invalid characters in paths and very long paths that might result in errors. Implement the function with these considerations in mind and provide a comprehensive solution that ensures robustness and correctness.","solution":"import os def process_and_validate_paths(paths): Processes and validates a list of file paths. Parameters: - paths: List of paths (strings or os.PathLike objects) Returns: - List of valid paths as strings. valid_paths = [] for path in paths: try: # Process the path using os.fspath() processed_path = os.fspath(path) # Check if the path actually exists if os.path.exists(processed_path): valid_paths.append(processed_path) except TypeError: # Handle invalid paths that raise TypeError pass return valid_paths"},{"question":"# POP3 Email Client Utility You are required to implement a Python function that connects to a POP3 server, retrieves all email messages, and saves them to individual text files. The function should handle both unencrypted (`POP3`) and encrypted (`POP3_SSL`) connections based on the provided parameters. Function Signature ```python def fetch_emails(host: str, username: str, password: str, use_ssl: bool = False, output_dir: str = \'.\') -> None: pass ``` Parameters - `host` (str): The hostname of the POP3 server. - `username` (str): The username to authenticate with. - `password` (str): The password to authenticate with. - `use_ssl` (bool, optional): If `True`, use SSL to connect to the server with `POP3_SSL`. Defaults to `False`. - `output_dir` (str, optional): The directory where the email messages should be saved as text files. Defaults to the current directory (`\'.\'`). Function Details 1. Connect to the POP3 server (using `POP3` or `POP3_SSL` based on `use_ssl`). 2. Authenticate with the server using the provided `username` and `password`. 3. Retrieve all email messages from the server. 4. Save each email message into a separate text file in the specified `output_dir`, with filenames in the format `email_<message_number>.txt`. 5. Close the connection to the server. Constraints - Each email message should be saved in its entirety as received from the server. - Handle exceptions appropriately to ensure that the connection is closed gracefully on errors. Example Usage ```python fetch_emails(\'pop.mailserver.com\', \'user@mail.com\', \'password123\', use_ssl=True, output_dir=\'/path/to/emails\') ``` - This will connect to `pop.mailserver.com` using SSL, authenticate with `user@mail.com` and `password123`, retrieve all emails, and save them to the directory `/path/to/emails`. # Notes - You may use the `poplib` library for handling POP3 communication. - Assume that the required directory for saving emails exists, and you have write permissions to it.","solution":"import poplib import os def fetch_emails(host: str, username: str, password: str, use_ssl: bool = False, output_dir: str = \'.\') -> None: try: # Connect to the POP3 server if use_ssl: server = poplib.POP3_SSL(host) else: server = poplib.POP3(host) # Authenticate server.user(username) server.pass_(password) # Get the number of messages num_messages = len(server.list()[1]) # Retrieve and save each email for i in range(1, num_messages + 1): response, lines, octets = server.retr(i) message_content = \\"n\\".join(line.decode(\'utf-8\') for line in lines) email_filename = os.path.join(output_dir, f\'email_{i}.txt\') with open(email_filename, \'w\') as email_file: email_file.write(message_content) # Close the connection server.quit() except Exception as e: if \'server\' in locals(): server.quit() print(f\\"An error occurred: {e}\\") raise e"},{"question":"# Python Coding Assessment Question **Objective** Design a function that demonstrates a comprehensive understanding of Python\'s file handling and I/O operations. **Problem Description** Write a Python function `process_file(filename: str, output_file: str) -> None` that performs the following operations: 1. **Reading**: Open and read the contents of the file specified by `filename`. The file is guaranteed to contain lines of text. 2. **Processing**: For each line read from the input file: - Ignore any lines that are empty or contain only whitespace. - For non-empty lines, reverse the order of the characters in the line. - If the resulting reversed line is a palindrome (it reads the same forwards and backwards), replace it with the original line. 3. **Writing**: Write the processed lines to the file specified by `output_file`. Ensure that the output file contains no empty lines. **Function Signature** ```python def process_file(filename: str, output_file: str) -> None: ... ``` **Constraints** - Do not use any external libraries apart from Python\'s standard modules. - Handle any potential I/O errors gracefully, printing a relevant error message. - Assume the input file is encoded in UTF-8. - The function should not return any value; it should only perform the file I/O operations. **Example** Consider the input file `input.txt` with the following content: ``` hello world racecar level example kayak ``` After processing, the output file `output.txt` should contain: ``` olleh dlrow racecar level elpmaxe kayak ``` **Explanation**: - The lines \\"racecar\\", \\"level\\", and \\"kayak\\" are palindromes when reversed and are hence retained in their original form. - All non-empty, non-palindromic lines are reversed. - All empty lines are removed in the output. **Notes** - You should thoroughly test the function with various input files to ensure correctness. - Pay attention to edge cases such as very long lines or files with numerous empty lines.","solution":"def process_file(filename: str, output_file: str) -> None: def is_palindrome(s: str) -> bool: return s == s[::-1] try: with open(filename, \'r\', encoding=\'utf-8\') as infile: lines = infile.readlines() processed_lines = [] for line in lines: stripped_line = line.strip() if stripped_line: reversed_line = stripped_line[::-1] if is_palindrome(reversed_line): processed_lines.append(stripped_line) else: processed_lines.append(reversed_line) with open(output_file, \'w\', encoding=\'utf-8\') as outfile: for pline in processed_lines: outfile.write(pline + \'n\') except IOError as e: print(f\\"An error occurred while handling files: {e}\\")"},{"question":"**Objective**: Implement a function to convert a string to a floating-point number with specific error handling, mirroring the behavior of `PyOS_string_to_double`. **Question**: You are required to write a Python function `custom_string_to_float(s: str) -> float` that converts a string representation of a floating-point number to an actual float value. The function should handle errors and edge cases as described below: Function Signature: ```python def custom_string_to_float(s: str) -> float: ``` Input: - `s` (str): A string representing a floating-point number. The string will not have leading or trailing whitespace. Output: - Returns the floating-point number represented by the string. - If the string does not represent a valid floating-point number, the function should raise a `ValueError`. - If the string represents a value that is too large to store in a float, the function should raise an `OverflowError`. Constraints: - You are not allowed to use Pythons built-in `float()` function directly for the conversion. - The function should handle strings that represent valid floating-point numbers, including positive and negative values, and numbers in scientific notation. - The function should correctly handle error cases and overflow conditions as described. Example: ```python print(custom_string_to_float(\\"123.456\\")) # Output: 123.456 print(custom_string_to_float(\\"-1.23e4\\")) # Output: -12300.0 print(custom_string_to_float(\\"1e500\\")) # Output: should raise an OverflowError print(custom_string_to_float(\\"abc\\")) # Output: should raise a ValueError ``` **Notes**: - You may use regular expressions and other string manipulation techniques to parse the input string. - Consider edge cases such as very large numbers, invalid strings and scientific notation. - Ensure that your function correctly mimics the described behavior of `PyOS_string_to_double`.","solution":"def custom_string_to_float(s: str) -> float: import re import math # Define a regular expression for a valid float number in string format float_regex = re.compile( r\'^[-+]?[0-9]*.?[0-9]+([eE][-+]?[0-9]+)?\' ) # Check if the input string matches the regular expression if not float_regex.match(s): raise ValueError(f\\"Invalid float string: {s}\\") # Simple parsing would not handle overflow, we have to do this manually try: # Convert manually using parts base, sep, exp = s.partition(\'e\') if \'e\' in s or \'E\' in s else s.partition(\'E\') if not sep: # If no exponent part result = float(base) else: # If there is an exponent part result = float(base) * (10 ** int(exp)) # Check for overflows (same as float()) if result == float(\'inf\') or result == -float(\'inf\'): raise OverflowError(f\\"Float value too large to convert: {s}\\") return result except Exception as e: # Re-raise any other exceptions as ValueError if isinstance(e, OverflowError): raise e raise ValueError(f\\"Invalid float string: {s}\\")"},{"question":"# Pandas Indexing and Selection Assessment Objective: Implement a function that uses different pandas indexing methods to perform a series of operations on a provided DataFrame. Task: You need to implement a function `process_dataframe(df: pd.DataFrame) -> pd.DataFrame` that performs the following tasks: 1. Select rows where column \'A\' has values greater than 0 using boolean indexing, and update column \'B\' in those rows to be twice its current value. 2. Using `.loc`, select and return the rows where column \'C\' is between 1 and 5 (inclusive). 3. Using `.iloc`, update the last three rows of column \'D\' to -1. 4. Using slicing, return a subset of the DataFrame containing the first 3 rows and the columns \'A\' and \'B\'. Function Signature: ```python import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Your code here return df ``` Input: - A pandas DataFrame `df` with at least the columns \'A\', \'B\', \'C\', and \'D\'. Output: - The modified pandas DataFrame after performing the specified operations. Example: ```python import pandas as pd data = { \'A\': [1, -2, 3, -4, 5], \'B\': [10, 20, 30, 40, 50], \'C\': [1, 2, 3, 6, 5], \'D\': [5, 4, 3, 2, 1] } df = pd.DataFrame(data) modified_df = process_dataframe(df) print(modified_df) # Expected output: # A B C D # 0 1 20 1 5 # 1 -2 20 2 4 # 2 3 60 3 3 # 3 -4 40 6 -1 # 4 5 100 5 -1 ``` **Note:** The above example shows the changes applied to the dataframe as expected from the function implementation.","solution":"import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Select rows where column \'A\' has values greater than 0 and update column \'B\' df.loc[df[\'A\'] > 0, \'B\'] *= 2 # Step 2: Using .loc, select and return the rows where column \'C\' is between 1 and 5 (inclusive) result_loc = df.loc[(df[\'C\'] >= 1) & (df[\'C\'] <= 5)] # Step 3: Using .iloc, update the last three rows of column \'D\' to -1 df.iloc[-3:, df.columns.get_loc(\'D\')] = -1 # Step 4: Using slicing, return a subset of the DataFrame containing the first 3 rows and the columns \'A\' and \'B\' result_slicing = df.loc[:2, [\'A\', \'B\']] return df, result_loc, result_slicing"},{"question":"Your task is to implement a function, `handle_unicode_operations`, that takes a Unicode string and performs a series of operations on it. The function should: 1. Encode the string into different formats (UTF-8, UTF-16, UTF-32, ASCII, Latin-1). 2. Decode the encoded strings back to UTF-8. 3. Replace digits in the original string with their ASCII equivalents. 4. Check if the string is a valid identifier according to Python\'s language definition. 5. Return the results of each operation in the specified format. # Function Signature ```python def handle_unicode_operations(input_str: str) -> dict: pass ``` # Input - `input_str` (str): A Unicode string. # Output A dictionary with the following keys: - `\'utf8_encoded\'`: The UTF-8 encoded version of the input string. - `\'utf16_encoded\'`: The UTF-16 encoded version of the input string. - `\'utf32_encoded\'`: The UTF-32 encoded version of the input string. - `\'ascii_encoded\'`: The ASCII encoded version of the input string. - `\'latin1_encoded\'`: The Latin-1 encoded version of the input string. - `\'utf8_decoded\'`: The string decoded back from the UTF-8 encoded version. - `\'digits_replaced\'`: The string with all decimal digits replaced by their ASCII equivalents. - `\'is_identifier\'`: A boolean indicating if the input string is a valid identifier. # Constraints - The input string may contain a mix of Unicode characters, including non-ASCII characters. - Non-ASCII characters should result in encoding errors for ASCII codec. - Ensure proper error handling during encoding and decoding operations. - Use Python\'s built-in functions and modules for Unicode string processing. # Example ```python input_str = \\"Hello, 123\\" result = handle_unicode_operations(input_str) print(result) ``` Expected output: ```python { \'utf8_encoded\': b\'Hello, xe4xb8x96xe7x95x8c123\', \'utf16_encoded\': b\'xffxfeHx00ex00lx00lx00ox00,x00 x00xb1Nx95]x003x002x001x00\', \'utf32_encoded\': b\'xffxfex00x00Hx00x00x00ex00x00x00lx00x00x00lx00x00x00ox00x00x00,x00x00x00 x00x00x00x96x4ex00x00x0cx75x00x001x00x00x002x00x00x003x00x00x00\', \'ascii_encoded\': \'Encoding Error\', \'latin1_encoded\': \'Encoding Error\', \'utf8_decoded\': \'Hello, 123\', \'digits_replaced\': \'Hello, x003x002x001\', \'is_identifier\': False } ``` Implement the function `handle_unicode_operations` to achieve the desired functionality.","solution":"def handle_unicode_operations(input_str: str) -> dict: results = {} # Encode the string into different formats try: utf8_encoded = input_str.encode(\'utf-8\') except UnicodeEncodeError: utf8_encoded = \'Encoding Error\' results[\'utf8_encoded\'] = utf8_encoded try: utf16_encoded = input_str.encode(\'utf-16\') except UnicodeEncodeError: utf16_encoded = \'Encoding Error\' results[\'utf16_encoded\'] = utf16_encoded try: utf32_encoded = input_str.encode(\'utf-32\') except UnicodeEncodeError: utf32_encoded = \'Encoding Error\' results[\'utf32_encoded\'] = utf32_encoded try: ascii_encoded = input_str.encode(\'ascii\') except UnicodeEncodeError: ascii_encoded = \'Encoding Error\' results[\'ascii_encoded\'] = ascii_encoded try: latin1_encoded = input_str.encode(\'latin-1\') except UnicodeEncodeError: latin1_encoded = \'Encoding Error\' results[\'latin1_encoded\'] = latin1_encoded # Decode the UTF-8 encoded string back try: utf8_decoded = utf8_encoded.decode(\'utf-8\') if utf8_encoded != \'Encoding Error\' else \'Encoding Error\' except UnicodeDecodeError: utf8_decoded = \'Decoding Error\' results[\'utf8_decoded\'] = utf8_decoded # Replace digits in the original string with their ASCII equivalents digits_replaced = \'\'.join(chr(ord(c)) if c.isdigit() else c for c in input_str) results[\'digits_replaced\'] = digits_replaced # Check if the string is a valid identifier is_identifier = input_str.isidentifier() results[\'is_identifier\'] = is_identifier return results"},{"question":"<|Analysis Begin|> The provided documentation is for the `torch.overrides` module in PyTorch, which includes various helper functions related to the `__torch_function__` protocol. This protocol allows overriding the behavior of PyTorch functions for user-defined tensor-like objects. The listed functions provide functionality such as: 1. `get_ignored_functions`: Likely retrieves functions that should be ignored by the `__torch_function__` protocol. 2. `get_overridable_functions`: Likely retrieves functions that can be overridden using the `__torch_function__` protocol. 3. `resolve_name`: Likely helps in resolving names within the context of overriding. 4. `get_testing_overrides`: Likely provides testing-related overrides. 5. `handle_torch_function`: Likely handles the application of the `__torch_function__` protocol. 6. `has_torch_function`: Checks if the `__torch_function__` protocol is being used. 7. `is_tensor_like`: Checks if an object is tensor-like. 8. `is_tensor_method_or_property`: Checks if a method or property belongs to tensor or tensor-like objects. 9. `wrap_torch_function`: Likely wraps a function to use the `__torch_function__` protocol. Considering the complexity and breadth of these functions related to the overriding mechanism in PyTorch, a coding assessment question can be designed to test the understanding of extending PyTorch functionalities and handling the `__torch_function__` protocol. <|Analysis End|> <|Question Begin|> # [Coding Task: Extending Tensor-like Behavior in PyTorch] Objective: To demonstrate comprehension of PyTorch\'s `__torch_function__` protocol by creating a custom tensor-like class and overriding a basic PyTorch function. Problem Statement: You are required to create a custom tensor-like class `MyTensor` that can interact with PyTorch tensor functions via the `__torch_function__` protocol. Specifically, your custom class should override the behavior of the PyTorch function `torch.add` for instances of `MyTensor`. Detailed Instructions: 1. **Class Definition**: Define a class `MyTensor` that mimics basic tensor behavior. The class should: - Store a single attribute `data` which is a `torch.Tensor` object. - Implement the `__torch_function__` protocol to override the `torch.add` function. 2. **Override `torch.add`**: - When `torch.add` is called with instances of `MyTensor`, it should utilize the overridden behavior that adds the `data` attributes of the `MyTensor` instances. - If the other operand is a scalar or another `torch.Tensor`, handle the addition accordingly. - Ensure that the result is returned as a new instance of `MyTensor`. 3. **Input and Output**: - Implement the `__torch_function__` handling such that it works seamlessly with PyTorch\'s existing functions. - Demonstrate the usage with the following examples and expected outputs. Constraints: - You must use PyTorch\'s `__torch_function__` protocol. - Your class should raise appropriate exceptions for invalid operations. Example Usage: ```python import torch from your_module import MyTensor # Creating MyTensor instances a = MyTensor(torch.tensor([1, 2, 3])) b = MyTensor(torch.tensor([4, 5, 6])) # Adding two MyTensor instances c = torch.add(a, b) # Adding MyTensor with a scalar d = torch.add(a, 10) # Adding MyTensor with a PyTorch Tensor e = torch.add(a, torch.tensor([7, 8, 9])) print(c.data) # Expected output: tensor([5, 7, 9]) print(d.data) # Expected output: tensor([11, 12, 13]) print(e.data) # Expected output: tensor([8, 10, 12]) ``` Your Implementation: ```python import torch class MyTensor: def __init__(self, data): if not isinstance(data, torch.Tensor): raise TypeError(\\"data must be a torch.Tensor\\") self.data = data def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func is torch.add: # Implement custom handling for `torch.add` result_data = cls._handle_add_function(args, kwargs) return MyTensor(result_data) # You can add more custom behaviors here return NotImplemented @staticmethod def _handle_add_function(args, kwargs): # Extract operands self, other = args if isinstance(other, MyTensor): other = other.data result_data = torch.add(self.data, other, **kwargs) return result_data # Optional: Doctests or additional unit tests here ```","solution":"import torch class MyTensor: def __init__(self, data): if not isinstance(data, torch.Tensor): raise TypeError(\\"data must be a torch.Tensor\\") self.data = data def __torch_function__(cls, func, types, args=(), kwargs=None): if kwargs is None: kwargs = {} if func is torch.add: # Implement custom handling for `torch.add` result_data = cls._handle_add_function(args, kwargs) return MyTensor(result_data) # You can add more custom behaviors here return NotImplemented @staticmethod def _handle_add_function(args, kwargs): # Extract operands self, other = args if isinstance(other, MyTensor): other = other.data result_data = torch.add(self.data, other, **kwargs) return result_data"},{"question":"Coding Assessment Question # Objective Your task is to implement a function that programmatically evaluates a list of operations on an initial integer value using functions from the `operator` module. This will test your understanding of using functional equivalents of intrinsic operators. # Function Signature ```python def evaluate_operations(start: int, operations: List[Tuple[str, Union[int, Tuple[int, int]]]]) -> int: pass ``` # Input - `start` (int): The initial integer value. - `operations` (List[Tuple[str, Union[int, Tuple[int, int]]]]): A list of tuples where each tuple consists of: - a string representing the name of the operation (e.g., `\\"add\\"`, `\\"imul\\"`, `\\"eq\\"`), and - a single integer or a tuple of two integers as operands, depending on the operation. # Output - The final integer value after applying all the operations sequentially on the `start` value. # Constraints - The `operations` list will only contain valid operations names as described in the `operator` module documentation. - You should use functions from the `operator` module wherever possible to implement these operations. - For single-operand functions (`neg`, `pos`, `inv`, `truth`, etc.), only one operand will be provided. - Operations must be applied in the order they appear in the operations list. # Example ```python from typing import List, Tuple, Union def evaluate_operations(start: int, operations: List[Tuple[str, Union[int, Tuple[int, int]]]]) -> int: import operator for op, operand in operations: if isinstance(operand, tuple): start = getattr(operator, op)(start, operand[0]) else: if op in {\\"neg\\", \\"pos\\", \\"inv\\", \\"truth\\"}: start = getattr(operator, op)(operand) else: start = getattr(operator, op)(start, operand) return start # Example usage: start = 4 operations = [(\\"mul\\", 3), (\\"add\\", 5), (\\"floordiv\\", 2), (\\"eq\\", 8)] print(evaluate_operations(start, operations)) # Output: 8 ``` # Explanation Starting with 4: 1. `4 * 3`  12 2. `12 + 5`  17 3. `17 // 2`  8 4. `8 == 8`  True which can be interpreted as 1 enda with the final result 8. Make sure to handle operations correctly using the `operator` module and validate the types of operands accordingly.","solution":"from typing import List, Tuple, Union def evaluate_operations(start: int, operations: List[Tuple[str, Union[int, Tuple[int, int]]]]) -> int: import operator for op, operand in operations: func = getattr(operator, op) if isinstance(operand, tuple): start = func(start, *operand) else: start = func(start, operand) return start"},{"question":"You are required to write a Python function that processes XML data as per the following specifications: # Task 1. Read and parse the given XML content. 2. Identify and modify specific elements in the XML tree based on their tag names and attributes. 3. Handle XML namespaces. 4. Write the modified XML content to a file. **Function Signature:** ```python def process_xml(input_xml: str, output_file: str) -> None: Processes the given XML content according to the specifications and writes it to an output file. :param input_xml: String containing the input XML content. :param output_file: File path for the output XML content. pass ``` # Specifications 1. **Reading XML**: The `input_xml` parameter is a string representing an XML document. 2. **Finding Elements**: - For each `<country>` element, find the first `<rank>` element and increment its text value by 10 if it is less than 50. Otherwise, leave it unchanged. - Add an attribute `processed=\\"true\\"` to all modified `<rank>` elements. 3. **Handling Namespaces**: - Retrieve elements using namespaces where applicable. - For example, if the `<country>` element uses a default namespace, handle it appropriately. 4. **Writing XML**: - Write the resulting XML to the file specified by the `output_file` parameter. - Ensure the output XML maintains any XML declarations and proper formatting. # Example Given the following `input_xml`: ```xml <data xmlns=\\"http://example.com/countries\\"> <country name=\\"CountryA\\"> <rank>30</rank> <year>2020</year> </country> <country name=\\"CountryB\\"> <rank>55</rank> <year>2019</year> </country> </data> ``` The output file should contain: ```xml <data xmlns=\\"http://example.com/countries\\"> <country name=\\"CountryA\\"> <rank processed=\\"true\\">40</rank> <year>2020</year> </country> <country name=\\"CountryB\\"> <rank>55</rank> <year>2019</year> </country> </data> ``` # Constraints - The input XML will always be well-formed. - The ranks will always be non-negative integers. - Ensure the function handles XML namespaces correctly. # Notes - You must use the `xml.etree.ElementTree` module for XML processing. - Pay attention to memory efficiency if the XML document is large. You may use the following code to test your function: ```python input_xml = <data xmlns=\\"http://example.com/countries\\"> <country name=\\"CountryA\\"> <rank>30</rank> <year>2020</year> </country> <country name=\\"CountryB\\"> <rank>55</rank> <year>2019</year> </country> </data> output_file = \\"output.xml\\" process_xml(input_xml, output_file) # Check the contents of output.xml for correctness ```","solution":"import xml.etree.ElementTree as ET def process_xml(input_xml: str, output_file: str) -> None: Processes the given XML content according to the specifications and writes it to an output file. :param input_xml: String containing the input XML content. :param output_file: File path for the output XML content. # Parse the input XML string root = ET.fromstring(input_xml) # Define the namespace ns = {\'ns\': \'http://example.com/countries\'} # Find all <country> elements and process their <rank> elements for country in root.findall(\'ns:country\', ns): rank = country.find(\'ns:rank\', ns) if rank is not None: rank_value = int(rank.text) if rank_value < 50: rank_value += 10 rank.text = str(rank_value) rank.set(\'processed\', \'true\') # Write the modified XML tree to the output file tree = ET.ElementTree(root) tree.write(output_file, encoding=\'utf-8\', xml_declaration=True)"},{"question":"# Question: Advanced Multi-level Sorting and Custom Key Functions in Python You are given a list of dictionaries, each representing a student\'s record with the following keys: `name` (string), `grade` (string, one of \'A\', \'B\', \'C\', \'D\', \'F\'), and `age` (integer). Your task is to write a function `sort_students` that sorts this list of dictionaries based on the following criteria: 1. Primary sort: by `grade`, in descending alphabetical order (i.e., \'A\' > \'B\' > \'C\' > \'D\' > \'F\'). 2. Secondary sort: by `age`, in ascending order. 3. Tertiary sort: by `name`, in case-insensitive alphabetical order. The function should take a list of dictionaries as input and return a new list sorted according to the criteria mentioned above. # Input - A list of dictionaries where each dictionary has the keys `name` (str), `grade` (str), and `age` (int). ```python students = [ {\\"name\\": \\"Alice\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"Bob\\", \\"grade\\": \\"A\\", \\"age\\": 25}, {\\"name\\": \\"Charlie\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"David\\", \\"grade\\": \\"C\\", \\"age\\": 22} ] ``` # Output - A list of dictionaries sorted based on the criteria. ```python [ {\\"name\\": \\"Bob\\", \\"grade\\": \\"A\\", \\"age\\": 25}, {\\"name\\": \\"Alice\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"Charlie\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"David\\", \\"grade\\": \\"C\\", \\"age\\": 22} ] ``` # Constraints - The input list will contain at least one dictionary and at most 1000 dictionaries. - The `name` value is a non-empty string with at most 100 characters. - The `grade` value is one of \'A\', \'B\', \'C\', \'D\', \'F\'. - The `age` value is a positive integer. # Function Signature ```python def sort_students(students: list) -> list: ``` # Example ```python def sort_students(students): from operator import itemgetter grade_order = {\'A\': 4, \'B\': 3, \'C\': 2, \'D\': 1, \'F\': 0} return sorted(students, key=lambda x: (grade_order[x[\'grade\']], x[\'age\'], x[\'name\'].lower()), reverse=True) students = [ {\\"name\\": \\"Alice\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"Bob\\", \\"grade\\": \\"A\\", \\"age\\": 25}, {\\"name\\": \\"Charlie\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"David\\", \\"grade\\": \\"C\\", \\"age\\": 22} ] print(sort_students(students)) # Output should be: # [ # {\\"name\\": \\"Bob\\", \\"grade\\": \\"A\\", \\"age\\": 25}, # {\\"name\\": \\"Alice\\", \\"grade\\": \\"B\\", \\"age\\": 20}, # {\\"name\\": \\"Charlie\\", \\"grade\\": \\"B\\", \\"age\\": 20}, # {\\"name\\": \\"David\\", \\"grade\\": \\"C\\", \\"age\\": 22} # ] ``` # Notes - Ensure the function uses the advanced sorting techniques with custom keys and the `operator` module where applicable. - The solution should be efficient and handle large lists within the provided constraints.","solution":"def sort_students(students): # Define a helper dictionary to map grades to numerical values grade_order = {\'A\': 4, \'B\': 3, \'C\': 2, \'D\': 1, \'F\': 0} # Sort the students list using sorted() with custom keys sorted_students = sorted( students, key=lambda x: (-grade_order[x[\'grade\']], x[\'age\'], x[\'name\'].lower()) ) return sorted_students # Example usage students = [ {\\"name\\": \\"Alice\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"Bob\\", \\"grade\\": \\"A\\", \\"age\\": 25}, {\\"name\\": \\"Charlie\\", \\"grade\\": \\"B\\", \\"age\\": 20}, {\\"name\\": \\"David\\", \\"grade\\": \\"C\\", \\"age\\": 22} ] print(sort_students(students)) # Output should be: # [ # {\\"name\\": \\"Bob\\", \\"grade\\": \\"A\\", \\"age\\": 25}, # {\\"name\\": \\"Alice\\", \\"grade\\": \\"B\\", \\"age\\": 20}, # {\\"name\\": \\"Charlie\\", \\"grade\\": \\"B\\", \\"age\\": 20}, # {\\"name\\": \\"David\\", \\"grade\\": \\"C\\", \\"age\\": 22} # ]"},{"question":"**Objective:** The goal is to test your understanding of using the seaborn.objects module to create complex and layered visualizations. **Problem Statement:** Using the seaborn.objects module, create a jittered dot plot showing the distribution of the `flipper_length_mm` variable for each species in the `penguins` dataset, with an added percentile range for each species. **Input:** 1. Load the `penguins` dataset from seaborn. 2. Create a plot with the following requirements: - Plot the `flipper_length_mm` on the x-axis. - Plot the `species` on the y-axis. - Add jitter to the data points to avoid overlap. - Show the 25th to 75th percentile range for each species. - Shift the percentile range slightly on the x-axis to distinguish from jittered dots. **Output:** The plot should be displayed as described above. **Constraints/Limits:** - You must use the seaborn.objects module as demonstrated in the initial examples. - The final plot should be self-explanatory with appropriate axis labels and title. **Performance Requirements:** - The code should run efficiently without causing significant delay for the given dataset. **Code Template:** ```python import seaborn.objects as so from seaborn import load_dataset # Load penguins dataset penguins = load_dataset(\\"penguins\\") # Create plot with jittered dots and percentile range ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"species\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) ``` Ensure your plot meets all the specified requirements and is visually informative. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguins_flipper_length(): # Load penguins dataset penguins = load_dataset(\\"penguins\\") # Create plot with jittered dots and percentile range p = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"species\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) .label(title=\\"Distribution of Flipper Length by Species\\", x=\\"Flipper Length (mm)\\", y=\\"Species\\") ) return p"},{"question":"You are provided with the `seaborn` library and a `titanic` dataset. Your task is to create a custom function that generates and customizes a boxplot using seaborn. # Function Signature ```python def custom_titanic_boxplot(data: pd.DataFrame, x: str, y: str, hue: str = None) -> None: pass ``` # Inputs - `data (pd.DataFrame)`: The dataset to be used. It is guaranteed to have the structure of the `titanic` dataset from seaborn. - `x (str)`: The name of the column to be used on the x-axis. - `y (str)`: The name of the column to be used on the y-axis. - `hue (str, optional)`: The name of the column for nested grouping by color. Default is `None`. # Expected Output The function should not return anything. Instead, it should: 1. Draw a boxplot with the following properties: - Display boxes vertically. - If `hue` is provided, use it for nested group coloring. - Boxes should be narrowed to half their default width. - Whiskers should cover the full range of the data (0 to 100 percentiles). - The median line within the boxes should be red with a linewidth of 2. - Add a horizontal dashed line at y=25. 2. Customize the appearance of the plot by: - Using a whitegrid style for the plot theme. You can use seaborn and matplotlib functions to achieve these customizations. # Example ```python import seaborn as sns import pandas as pd titanic = sns.load_dataset(\\"titanic\\") custom_titanic_boxplot(titanic, x=\\"class\\", y=\\"age\\", hue=\\"sex\\") ``` This should generate a vertical boxplot grouped by `class` and colored by `sex`, with the described customizations applied.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_titanic_boxplot(data: pd.DataFrame, x: str, y: str, hue: str = None) -> None: Generate and customize a boxplot using seaborn on the given DataFrame. Parameters: data (pd.DataFrame): The dataset to be used. x (str): The name of the column to be used on the x-axis. y (str): The name of the column to be used on the y-axis. hue (str, optional): The name of the column for nested grouping by color. Default is None. Returns: None sns.set(style=\\"whitegrid\\") ax = sns.boxplot( data=data, x=x, y=y, hue=hue, width=0.5, # Narrow the boxes to half their default width whis=[0, 100], # Whiskers cover the full range of the data medianprops={\'color\': \'red\', \'linewidth\': 2} # Red line with linewidth 2 for median ) plt.axhline(y=25, color=\'gray\', linestyle=\'--\') # Add a horizontal dashed line at y=25 plt.show()"},{"question":"# Seaborn PairGrid Customization and Plotting **Objective:** Use seaborn to create a customized PairGrid plot from a given dataset and save it as an image. **Task:** Write a Python function `create_pairgrid_plot` that performs the following operations: 1. Loads the penguins dataset using seaborn. 2. Creates a `PairGrid` that includes the following variables: \\"body_mass_g\\", \\"bill_length_mm\\", and \\"flipper_length_mm\\". 3. Maps a scatter plot to the off-diagonal sections. 4. Maps a histogram to the diagonal sections. 5. Colors the plots by the \\"species\\" variable. 6. Adds a legend to the plot. 7. Saves the plot as an image file with the filename provided as an argument. **Function Signature:** ```python def create_pairgrid_plot(filename: str) -> None: pass ``` **Input:** - `filename` (str): The name of the file to save the plot image as. **Requirements:** - Use seaborn\'s `PairGrid` and related functions to create the plot. - Ensure the plot includes legends and appropriate coloring by species. - Save the plot as an image file with the given filename. **Constraints:** - The function should not return anything. - Assume that the filename provided is always a valid string and ends with a suitable image extension (e.g., .png, .jpg). **Example Usage:** ```python create_pairgrid_plot(\\"penguins_plot.png\\") ``` **Hints:** - Refer to the seaborn documentation on PairGrid for different ways to use `map`, `map_diag`, `map_offdiag`, and other related methods. - Make sure matplotlib\'s `savefig` is used to save the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_pairgrid_plot(filename: str) -> None: Creates a PairGrid plot from the penguins dataset and saves it as an image file. Parameters: filename (str): The name of the file to save the plot image as. # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Select the variables of interest variables = [\\"body_mass_g\\", \\"bill_length_mm\\", \\"flipper_length_mm\\"] # Create a PairGrid object g = sns.PairGrid(penguins, vars=variables, hue=\\"species\\") # Map scatter plots to the off-diagonal sections g = g.map_offdiag(sns.scatterplot) # Map histograms to the diagonal sections g = g.map_diag(sns.histplot) # Add a legend to the plot g.add_legend() # Save the plot as an image file plt.savefig(filename) plt.close()"},{"question":"# PyTorch Futures and Asynchronous Execution Objective Implement a function using the `torch.futures.Future` class and the utility functions `collect_all` and `wait_all` to demonstrate comprehension of asynchronous execution and result aggregation in PyTorch. Problem Statement You are given a set of asynchronous tasks in the form of functions that return `torch.futures.Future` objects. Your task is to implement a function `aggregate_future_results(tasks: List[Callable[[], torch.futures.Future]]) -> List[Any]` that accepts a list of functions. Each function, when called, returns a `torch.futures.Future` object. Your function should execute all these tasks asynchronously, wait for all of them to complete, and then return a list of their results. # Requirements 1. Use `torch.futures.Future` to encapsulate the asynchronous execution. 2. Utilize `torch.futures.collect_all` or `torch.futures.wait_all` to aggregrate results from all Future objects. 3. Ensure that the function handles any exceptions that might occur during the execution of individual tasks, and collect the results of successfully completed tasks. # Function Signature ```python from typing import List, Callable, Any import torch.futures def aggregate_future_results(tasks: List[Callable[[], torch.futures.Future]]) -> List[Any]: pass ``` # Input - `tasks` - A list of functions. Each function, when called, returns a `torch.futures.Future` object representing an asynchronous task. # Output - A list containing the results of the completed Future objects. # Example ```python import torch.futures import time # Example Future-returning functions def async_task_1() -> torch.futures.Future: future = torch.futures.Future() future.set_result(5) return future def async_task_2() -> torch.futures.Future: future = torch.futures.Future() future.set_result(10) return future def async_task_3() -> torch.futures.Future: future = torch.futures.Future() future.set_result(15) return future # Aggregating results tasks = [async_task_1, async_task_2, async_task_3] result = aggregate_future_results(tasks) print(result) # Output: [5, 10, 15] ``` # Constraints - You should handle at least 0 to 10,000 tasks efficiently. - Handle possible exceptions during task execution gracefully. # Notes - Make sure that the function is robust and handles empty task lists. - Performance: The function should use the asynchronous nature of `torch.futures.Future` to handle large numbers of tasks efficiently.","solution":"import torch.futures from typing import List, Callable, Any def aggregate_future_results(tasks: List[Callable[[], torch.futures.Future]]) -> List[Any]: futures = [task() for task in tasks] results = torch.futures.wait_all(futures) return results"},{"question":"**Coding Assessment Question** You are given a dataset named `penguins` containing the following features: - `species`: categorical data which represents the species of penguins. - `bill_length_mm`: numerical data representing the bill length of penguins. - `island`: categorical data representing the island where the penguins are found. Your task is to write a Python function using seaborn that generates a grid of histograms based on the `penguins` dataset. The function should customize the legend\'s position and properties according to the specifications provided below. # Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_histogram_with_custom_legend(penguins): # Your code here ``` # Input: - `penguins`: a DataFrame consisting of the columns `species`, `bill_length_mm`, and `island`. # Output: - The function does not need to return anything, but it should display a seaborn plot according to the instructions. # Specifications: 1. The function should create a `displot` grid showing histograms of `bill_length_mm`, separating data by `species` and organizing subplots based on `island`. 2. The histograms should have a size of height 3. 3. Position the legend in the \\"upper left\\" of each subplot. 4. Use `bbox_to_anchor` to position the legend such that it does not leave extra blank space in the plot. 5. Ensure that the legend has no frame. # Example: Here is an example of the expected plot configuration: ```python import seaborn as sns import pandas as pd # Assuming penguins dataset is already loaded penguins = sns.load_dataset(\'penguins\') # Call the function plot_histogram_with_custom_legend(penguins) ``` Upon calling the function, you should see a grid of histograms with legends correctly positioned and formatted as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_histogram_with_custom_legend(penguins): Plots a grid of histograms for the \'bill_length_mm\' of penguins dataset, separated by species and organized by island. Customizes the legend\'s position and properties. Parameters: penguins (DataFrame): DataFrame containing the columns - \'species\', \'bill_length_mm\', \'island\' # Create a displot grid with histograms g = sns.displot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", height=3, kind=\\"hist\\") # Customize the legend for each subplot for ax in g.axes.flat: legend = ax.legend(loc=\'upper left\', bbox_to_anchor=(1.05, 1), frameon=False) plt.show() # Example usage (assumes penguins dataset is available in calling scope) # penguins = sns.load_dataset(\'penguins\') # plot_histogram_with_custom_legend(penguins)"},{"question":"**Objective:** Demonstrate your ability to use the seaborn `plotting_context` function to retrieve and modify plot settings. **Task:** 1. **Retrieve and print the default plotting context settings.** 2. **Retrieve and print the plotting context settings for the predefined style \\"paper\\".** 3. **Create a temporary context using the \\"notebook\\" plotting context and generate a line plot within this context. After the context is exited, generate another line plot using the default plotting context to illustrate the difference.** **Expected Function:** You are required to write a function `demonstrate_plotting_context()` which should implement the above tasks. The function should display the results within the function itself. **Input:** None **Output:** None. The function should print the relevant settings for the default and \\"paper\\" contexts, and it should display two plots for visual comparison. **Constraints:** - Use seaborn for creating the plots. - Assume seaborn is already imported as `sns`. - You may use matplotlib for showing the plots if necessary. **Performance Requirements:** - Efficiently switch between contexts and generate plots. **Example:** Here is an outline of what the function should achieve: ```python def demonstrate_plotting_context(): # 1. Retrieve and print the default plotting context settings. default_context = sns.plotting_context() print(\\"Default Context:\\", default_context) # 2. Retrieve and print the plotting context settings for the predefined style \\"paper\\". paper_context = sns.plotting_context(\\"paper\\") print(\\"Paper Context:\\", paper_context) # 3. Create a temporary context using the \\"notebook\\" plotting context and generate a line plot. with sns.plotting_context(\\"notebook\\"): sns.lineplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[1, 3, 2]) plt.title(\\"Plot in \'notebook\' Context\\") plt.show() # Generate another line plot using the default plotting context. sns.lineplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[1, 3, 2]) plt.title(\\"Plot in Default Context\\") plt.show() # Call the function to demonstrate the functionality demonstrate_plotting_context() ``` Note: Ensure that seaborn and matplotlib are properly installed and imported to execute the function successfully.","solution":"import seaborn as sns import matplotlib.pyplot as plt def demonstrate_plotting_context(): # 1. Retrieve and print the default plotting context settings. default_context = sns.plotting_context() print(\\"Default Context:\\", default_context) # 2. Retrieve and print the plotting context settings for the predefined style \\"paper\\". paper_context = sns.plotting_context(\\"paper\\") print(\\"Paper Context:\\", paper_context) # 3. Create a temporary context using the \\"notebook\\" plotting context and generate a line plot. with sns.plotting_context(\\"notebook\\"): sns.lineplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[1, 3, 2]) plt.title(\\"Plot in \'notebook\' Context\\") plt.show() # Generate another line plot using the default plotting context. sns.lineplot(x=[\\"A\\", \\"B\\", \\"C\\"], y=[1, 3, 2]) plt.title(\\"Plot in Default Context\\") plt.show() # Call the function to demonstrate the functionality demonstrate_plotting_context()"},{"question":"Objective Evaluate the performance of different implementations for reversing a string using the `timeit` module in Python. Task You need to implement three different functions to reverse a given string and then use the `timeit` module to measure and compare their execution times. The functions to implement are as follows: 1. **Function using slicing**: Implement a function `reverse_slicing(s)` that reverses the input string `s` using slicing. 2. **Function using a loop**: Implement a function `reverse_loop(s)` that reverses the input string `s` using a loop. 3. **Function using `reversed()`**: Implement a function `reverse_reversed(s)` that reverses the input string `s` using the `reversed()` function. After implementing these functions, write a function `compare_reverse_functions()` that uses the `timeit` module to measure the execution times of the three reverse functions with the same input string and prints the best execution time for each function. Requirements 1. The `reverse_slicing(s)` function should use slicing to reverse the string. 2. The `reverse_loop(s)` function should use a loop to reverse the string. 3. The `reverse_reversed(s)` function should use the `reversed()` function to reverse the string. 4. The `compare_reverse_functions()` function should: - Use `timeit.timeit()` to measure the execution time of each reverse function. - Print the best execution time of each function using the minimum of at least 5 repetitions. Input and Output - Each reverse function takes a single string `s` as input and returns the reversed string as output. - The `compare_reverse_functions()` function takes no arguments and prints the execution times directly. Example ```python import timeit def reverse_slicing(s): return s[::-1] def reverse_loop(s): reversed_s = \'\' for char in s: reversed_s = char + reversed_s return reversed_s def reverse_reversed(s): return \'\'.join(reversed(s)) def compare_reverse_functions(): test_string = \\"abcdefghijklmnopqrstuvwxyz\\" slice_time = timeit.timeit(\'reverse_slicing(test_string)\', setup=\'from __main__ import reverse_slicing, test_string\', number=100000) print(f\\"Slicing method time: {slice_time:.6f} seconds\\") loop_time = timeit.timeit(\'reverse_loop(test_string)\', setup=\'from __main__ import reverse_loop, test_string\', number=100000) print(f\\"Loop method time: {loop_time:.6f} seconds\\") reversed_time = timeit.timeit(\'reverse_reversed(test_string)\', setup=\'from __main__ import reverse_reversed, test_string\', number=100000) print(f\\"Reversed function time: {reversed_time:.6f} seconds\\") compare_reverse_functions() ``` Constraints - Use at least 100,000 executions for timing each function. - Ensure that the global variables and imports required for timing are correctly set up in the `timeit` setup strings.","solution":"import timeit def reverse_slicing(s): Reverses the input string \'s\' using slicing. return s[::-1] def reverse_loop(s): Reverses the input string \'s\' using a loop. reversed_s = \'\' for char in s: reversed_s = char + reversed_s return reversed_s def reverse_reversed(s): Reverses the input string \'s\' using the reversed() function. return \'\'.join(reversed(s)) def compare_reverse_functions(): Uses the timeit module to measure the execution times of the three reversing functions: reverse_slicing, reverse_loop, and reverse_reversed with the same input string. Prints the best execution time for each function. test_string = \\"abcdefghijklmnopqrstuvwxyz\\" slice_time = min(timeit.repeat(\'reverse_slicing(test_string)\', setup=\'from __main__ import reverse_slicing, test_string\', repeat=5, number=100000)) print(f\\"Slicing method time: {slice_time:.6f} seconds\\") loop_time = min(timeit.repeat(\'reverse_loop(test_string)\', setup=\'from __main__ import reverse_loop, test_string\', repeat=5, number=100000)) print(f\\"Loop method time: {loop_time:.6f} seconds\\") reversed_time = min(timeit.repeat(\'reverse_reversed(test_string)\', setup=\'from __main__ import reverse_reversed, test_string\', repeat=5, number=100000)) print(f\\"Reversed function time: {reversed_time:.6f} seconds\\")"},{"question":"Coding Assessment Question # Objective: To test the student\'s understanding of various mathematical functions provided by the `math` module in Python, by requiring them to implement a function that uses these to perform a complex calculation. # Problem Statement: You are required to implement a function `calculate_expression(a, b, c)` which takes three float inputs `a`, `b`, and `c`. The function should compute and return the following mathematical expression using functions from the `math` module where applicable: [ text{result} = frac{{e^a + log_2{b} + |c|}}{acos(frac{a}{sqrt{a^2 + b^2 + c^2}})} ] Input: - `a` (float): a non-zero float. - `b` (float): a positive float. - `c` (float): any float. Output: - Return the computed result as a float. Constraints: - `a != 0` - `b > 0` Requirements: - Use the functions from the `math` module for the following: - Exponential function - Logarithm base 2 - Absolute value - Inverse cosine - Square root # Implementation: ```python import math def calculate_expression(a, b, c): # Calculate the numerator using math.exp, math.log2, and math.fabs numerator = math.exp(a) + math.log2(b) + math.fabs(c) # Calculate the denominator using math.acos and math.sqrt denominator = math.acos(a / math.sqrt(a**2 + b**2 + c**2)) # Compute and return the final result result = numerator / denominator return result ``` # Example: ```python print(calculate_expression(1.0, 2.0, -3.0)) # Expected output: A float value ``` # Notes: - Make sure to handle inputs carefully as `a` must not be zero and `b` must be positive. - Ensure you have error handling to prevent invalid operations like division by zero or domain errors for the math functions used. # Hints: - Review the documentation for each mentioned function in the `math` module to ensure correct usage and understand exceptions or edge cases they might raise.","solution":"import math def calculate_expression(a, b, c): Compute the given mathematical expression: (exp(a) + log2(b) + fabs(c)) / acos(a / sqrt(a^2 + b^2 + c^2)) Args: a (float): a non-zero float b (float): a positive float c (float): any float Returns: float: the result of the mathematical expression if a == 0: raise ValueError(\\"a must be non-zero\\") if b <= 0: raise ValueError(\\"b must be positive\\") numerator = math.exp(a) + math.log2(b) + math.fabs(c) denominator = math.acos(a / math.sqrt(a**2 + b**2 + c**2)) result = numerator / denominator return result"},{"question":"**Objective:** Create a visualization showcasing different types of color palettes using Seaborn. This question will assess your understanding of generating and utilizing color palettes in data visualizations. **Task:** 1. Write a function `create_color_palettes()` that performs the following: - Generates at least four different color palettes using Seaborn. - The first palette should be a light palette with a named color (e.g., \'seagreen\'). - The second palette should be a light palette with a hex code color (e.g., \'#79C\'). - The third palette should use the HUSL system for the color specification. - The fourth palette should be a continuous colormap (as_cmap=True) with any named or hexadecimal color. - Each palette should contain a different number of colors (e.g., 5, 7, 10) or be a continuous colormap. 2. Create a visualization using a Seaborn heatmap to show the effect of these palettes. Display each palette on a separate subplot within a single figure. Ensure each subplot is clearly labeled with the palette type and color specifications used. **Input/Output:** - There is no direct input. - The function should save the generated figure as `color_palettes_visualization.png`. **Constraints:** - Use Seaborn and Matplotlib for the visualization. - Ensure that the heatmap visualizes the effect of the color palettes effectively. - The function should not return anything. **Example Usage:** ```python create_color_palettes() # This should save a figure named \'color_palettes_visualization.png\' in the working directory. ``` # Notes: - Review the Seaborn and Matplotlib documentation if needed. - Pay attention to color palette creation and visualization mechanics. - Make sure your figure is well-organized and legible.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def create_color_palettes(): # Generate the data for the heatmap, random for visualization purposes data = np.random.rand(10, 10) # Create different color palettes palette_1 = sns.color_palette(\\"light:seagreen\\", 5) palette_2 = sns.color_palette(\\"light:#79C\\", 7) palette_3 = sns.color_palette(\\"husl\\", 10) palette_4 = sns.light_palette(\\"seagreen\\", as_cmap=True) # Create a figure with 2x2 subplots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) sns.heatmap(data, ax=axes[0, 0], cmap=sns.color_palette(palette_1, as_cmap=True)) axes[0, 0].set_title(\\"Light Palette with \'seagreen\'\\") sns.heatmap(data, ax=axes[0, 1], cmap=sns.color_palette(palette_2, as_cmap=True)) axes[0, 1].set_title(\\"Light Palette with \'#79C\'\\") sns.heatmap(data, ax=axes[1, 0], cmap=sns.color_palette(palette_3, as_cmap=True)) axes[1, 0].set_title(\\"HUSL System Palette\\") sns.heatmap(data, ax=axes[1, 1], cmap=palette_4) axes[1, 1].set_title(\\"Continuous Colormap \'seagreen\'\\") plt.tight_layout() plt.savefig(\'color_palettes_visualization.png\') plt.close()"},{"question":"# PyTorch Tensor Views Problem Description You are given a PyTorch tensor with specific dimensions. Your task is to implement a function `modify_tensor_view` that performs the following steps: 1. Create a view of the input tensor with different dimensions. 2. Modify an element in the view tensor. 3. Verify that the change is reflected in the original tensor. 4. Return both the modified view tensor and the original tensor. Function Signature ```python import torch def modify_tensor_view(input_tensor: torch.Tensor) -> (torch.Tensor, torch.Tensor): Modify a view tensor and ensure the change is reflected in the original tensor. Args: input_tensor (torch.Tensor): The original input tensor. Returns: tuple: A tuple containing the modified view tensor and the modified original tensor. pass ``` Input - `input_tensor` : A 2D PyTorch tensor of shape (m, n). Output - A tuple containing: - The modified view tensor. - The modified original tensor. Example ```python import torch # Original tensor of shape (4, 4) input_tensor = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) view_tensor, original_tensor = modify_tensor_view(input_tensor) # Modify element at position (0, 0) in the view tensor assert view_tensor[0][0].item() == 99 # Verify the modification is reflected in the original tensor assert original_tensor[0][0].item() == 99 ``` Constraints - Modify the value at position (0, 0) in the view tensor to be 99. - You can assume that the input tensor is always a 2D tensor with shape (m, n) where m and n are positive integers. Notes - Use the `view` operation to create the view tensor. - Ensure the function returns both the view tensor and the original tensor after modification.","solution":"import torch def modify_tensor_view(input_tensor: torch.Tensor) -> (torch.Tensor, torch.Tensor): Modify a view tensor and ensure the change is reflected in the original tensor. Args: input_tensor (torch.Tensor): The original input tensor. Returns: tuple: A tuple containing the modified view tensor and the modified original tensor. # Create a view of the tensor with the same dimensions but with a viewpoint view_tensor = input_tensor.view(input_tensor.shape) # Modify the element at position (0, 0) in the view tensor view_tensor[0][0] = 99 # Return the view tensor and the original tensor return view_tensor, input_tensor"},{"question":"# Code Debugging and Control using `pdb` As a Python developer, you are required to debug a script using Python\'s `pdb` module. The script has several functions that involve mathematical computations and string manipulations. Your task is to set breakpoints, evaluate the execution states, and control the flow of the program using the `pdb` module. **Your Tasks:** 1. **Implement the following Python script:** ```python def multiply(a, b): result = a * b return result def concatenate(str1, str2): result = str1 + str2 return result def main(): x = 5 y = 10 str1 = \\"Hello\\" str2 = \\"World\\" print(multiply(x, y)) print(concatenate(str1, str2)) if __name__ == \\"__main__\\": main() ``` 2. **Debug the Script Using `pdb`:** - Insert a breakpoint at the beginning of the `multiply` function. - Evaluate and print the values of `a` and `b` when the breakpoint is hit. - Continue execution till the end of the `multiply` function and then step into the `concatenate` function. - Insert a breakpoint at the end of the `concatenate` function. - Evaluate the result of the concatenation before returning it. - Use `pdb` to manipulate the value of `result` in the `concatenate` function to \\"Hello, Debug!\\" instead of \\"HelloWorld\\". - Finally, continue the execution to complete the script. **Expected Output:** 1. The product of `x` and `y` should be printed, followed by the modified concatenation result \\"Hello, Debug!\\". 2. Demonstrate your use of `pdb` by showing the commands you used and their corresponding outputs. **Performance Requirements:** - Ensure that your script completes execution successfully without any exceptions. - Use appropriate `pdb` commands to control the flow accurately. **Hints:** - Use `pdb.set_trace()` or `import pdb; pdb.set_trace()` to set breakpoints. - Use commands like `step`, `next`, `continue`, `print`, `p`, etc. to evaluate and control the flow. Submit a text file containing: - The original Python script. - The `pdb` commands you issued during the debugging session. - The final modified output printed by the script.","solution":"def multiply(a, b): result = a * b return result def concatenate(str1, str2): result = str1 + str2 return result def main(): x = 5 y = 10 str1 = \\"Hello\\" str2 = \\"World\\" print(multiply(x, y)) print(concatenate(str1, str2)) if __name__ == \\"__main__\\": main()"},{"question":"Create a Python function that generates an XML document representing a bookstore with the following structure: ```xml <bookstore> <book category=\\"fiction\\"> <title lang=\\"en\\">The Wonderful Wizard of Oz</title> <author>L. Frank Baum</author> <year>1900</year> <price>10.99</price> </book> <book category=\\"non-fiction\\"> <title lang=\\"en\\">A Brief History of Time</title> <author>Stephen Hawking</author> <year>1988</year> <price>15.99</price> </book> <book category=\\"fiction\\"> <title lang=\\"en\\">To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <price>7.99</price> </book> </bookstore> ``` Implement the function `generate_bookstore_xml()` using the `xml.dom.minidom` module which does the following: 1. Creates a new XML document. 2. Adds the root element `<bookstore>`. 3. Adds three `<book>` elements with the specified children and attributes as shown above. 4. Returns the XML document as a string. # Constraints * Use `xml.dom.minidom` for the DOM operations. * Your implementation should handle creating elements, setting attributes, and appending child nodes correctly. # Function Signature ```python def generate_bookstore_xml() -> str: pass ``` # Example Calling `generate_bookstore_xml()` should return a string equivalent to the following XML structure: ```xml <bookstore> <book category=\\"fiction\\"> <title lang=\\"en\\">The Wonderful Wizard of Oz</title> <author>L. Frank Baum</author> <year>1900</year> <price>10.99</price> </book> <book category=\\"non-fiction\\"> <title lang=\\"en\\">A Brief History of Time</title> <author>Stephen Hawking</author> <year>1988</year> <price>15.99</price> </book> <book category=\\"fiction\\"> <title lang=\\"en\\">To Kill a Mockingbird</title> <author>Harper Lee</author> <year>1960</year> <price>7.99</price> </book> </bookstore> ``` Make sure to properly format the XML string with indentation for readability.","solution":"from xml.dom.minidom import Document def generate_bookstore_xml() -> str: # Create a new DOM Document doc = Document() # Create the root element \\"bookstore\\" bookstore = doc.createElement(\\"bookstore\\") doc.appendChild(bookstore) # Define the books with their attributes and child elements books = [ { \\"category\\": \\"fiction\\", \\"title\\": {\\"lang\\": \\"en\\", \\"text\\": \\"The Wonderful Wizard of Oz\\"}, \\"author\\": \\"L. Frank Baum\\", \\"year\\": \\"1900\\", \\"price\\": \\"10.99\\" }, { \\"category\\": \\"non-fiction\\", \\"title\\": {\\"lang\\": \\"en\\", \\"text\\": \\"A Brief History of Time\\"}, \\"author\\": \\"Stephen Hawking\\", \\"year\\": \\"1988\\", \\"price\\": \\"15.99\\" }, { \\"category\\": \\"fiction\\", \\"title\\": {\\"lang\\": \\"en\\", \\"text\\": \\"To Kill a Mockingbird\\"}, \\"author\\": \\"Harper Lee\\", \\"year\\": \\"1960\\", \\"price\\": \\"7.99\\" } ] # Helper function to create a book element def create_book_element(book_data): book = doc.createElement(\\"book\\") book.setAttribute(\\"category\\", book_data[\\"category\\"]) title = doc.createElement(\\"title\\") title.setAttribute(\\"lang\\", book_data[\\"title\\"][\\"lang\\"]) title_text = doc.createTextNode(book_data[\\"title\\"][\\"text\\"]) title.appendChild(title_text) author = doc.createElement(\\"author\\") author_text = doc.createTextNode(book_data[\\"author\\"]) author.appendChild(author_text) year = doc.createElement(\\"year\\") year_text = doc.createTextNode(book_data[\\"year\\"]) year.appendChild(year_text) price = doc.createElement(\\"price\\") price_text = doc.createTextNode(book_data[\\"price\\"]) price.appendChild(price_text) book.appendChild(title) book.appendChild(author) book.appendChild(year) book.appendChild(price) return book # Create each book element and append it to the bookstore for book_data in books: book_element = create_book_element(book_data) bookstore.appendChild(book_element) # Return the pretty-printed XML string return doc.toprettyxml(indent=\\" \\")"},{"question":"Problem Statement You are given multiple text files on your local machine that need to be archived and compressed for storage efficiency and easy retrieval. Your task is to implement a Python script that automates the creation of compressed files and their subsequent decompression using the `zipfile` and `bz2` modules. # Detailed Instructions 1. **Function 1: `create_compressed_archive(file_paths: List[str], archive_name: str) -> None`** - **Input**: - `file_paths` (List[str]): A list of string paths to the files that need to be archived and compressed. - `archive_name` (str): The name of the resulting archive file (without an extension). - **Output**: - None - **Behavior**: - Your function should create a `.zip` archive with the given `archive_name` that contains all the files specified in `file_paths`. - Once the `.zip` archive is created, it should be compressed into a `.bz2` file with the same `archive_name`. 2. **Function 2: `extract_compressed_archive(compressed_file_path: str, extract_to: str) -> None`** - **Input**: - `compressed_file_path` (str): The path to the `.bz2` compressed archive file. - `extract_to` (str): The directory path where files from the compressed archive should be extracted. - **Output**: - None - **Behavior**: - This function should decompress the `.bz2` file to obtain the `.zip` archive. - The contents of the `.zip` archive should be extracted to the specified directory. # Constraints and Requirements - Use `zipfile` to handle the creation and extraction of `.zip` archives. - Use `bz2` to handle the compression and decompression of files. - Ensure that your code handles potential exceptions, such as file-not-found errors and permission issues. - Your functions should be efficient and should not load entire files into memory when unnecessary. # Example Usage ```python file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] archive_name = \'my_archive\' # Creating the compressed archive create_compressed_archive(file_paths, archive_name) # Extracting the compressed archive compressed_file_path = \'my_archive.bz2\' extract_to = \'./extracted_files\' extract_compressed_archive(compressed_file_path, extract_to) ``` This example assumes that you have \'file1.txt\', \'file2.txt\', and \'file3.txt\' in the current directory and that you want to create an archive named \'my_archive\'. After calling `create_compressed_archive`, there will be \'my_archive.zip\' and \'my_archive.bz2\' files in the directory. The `extract_compressed_archive` function will decompress \'my_archive.bz2\' and then extract the files from \'my_archive.zip\' to the `extract_to` directory. # Evaluation Criteria - Correctness: The solution must correctly create and extract compressed archives. - Efficiency: The implementation should minimize unnecessary memory usage and computation. - Robustness: The code should handle various edge cases and potential errors gracefully. - Code Quality: The solution should be well-organized and follow best coding practices.","solution":"import zipfile import bz2 import os from typing import List def create_compressed_archive(file_paths: List[str], archive_name: str) -> None: zip_filename = f\\"{archive_name}.zip\\" bz2_filename = f\\"{archive_name}.bz2\\" # Create a zip archive with zipfile.ZipFile(zip_filename, \'w\') as zipf: for file_path in file_paths: if os.path.isfile(file_path): # Ensure the file exists zipf.write(file_path, os.path.basename(file_path)) else: raise FileNotFoundError(f\\"{file_path} not found.\\") # Compress the zip archive using bz2 with open(zip_filename, \'rb\') as zipf, bz2.open(bz2_filename, \'wb\') as bz2f: for data in iter(lambda: zipf.read(1024), b\'\'): bz2f.write(data) # Remove the zip file as it\'s now compressed os.remove(zip_filename) def extract_compressed_archive(compressed_file_path: str, extract_to: str) -> None: if not os.path.isfile(compressed_file_path): raise FileNotFoundError(f\\"{compressed_file_path} not found.\\") # Ensure the extract_to directory exists os.makedirs(extract_to, exist_ok=True) zip_filename = compressed_file_path.replace(\'.bz2\', \'.zip\') # Decompress the bz2 file to zip with bz2.open(compressed_file_path, \'rb\') as bz2f, open(zip_filename, \'wb\') as zipf: for data in iter(lambda: bz2f.read(1024), b\'\'): zipf.write(data) # Extract the zip archive with zipfile.ZipFile(zip_filename, \'r\') as zipf: zipf.extractall(extract_to) # Clean up the zip file os.remove(zip_filename)"},{"question":"# UUID Manipulation and Comparison **Objective:** Implement a Python function that generates a set of UUIDs, identifies any duplicate UUIDs in the set, and verifies if the UUIDs are safe. You should demonstrate the use of various UUID creation methods and perform operations on the generated UUIDs. **Function Specification:** You need to implement a function called `uuid_operation` that takes an integer `n` as input and performs the following operations: 1. Generates `n` UUIDs using `uuid4()`. 2. Generates `n` UUIDs using `uuid1()`. 3. Combines these UUIDs into a single list. 4. Identifies and returns the duplicate UUIDs (if any) from this list. 5. Checks and returns the safety of each `uuid1()` UUID generated. 6. Returns the integer representation of the UUID with the highest integer value from the combined list. **Input:** - An integer `n` (1  n  1000), the number of UUIDs to generate using each method. **Output:** - A tuple with the following elements: 1. A list of duplicate UUIDs (if any). 2. A list of tuples where each tuple contains a `uuid1()` UUID and its safety status. 3. The integer representation of the UUID with the highest integer value. **Constraints:** - The function should handle up to `n = 1000` efficiently. - Ensure that the UUID operations such as generation and comparisons are performed as per the specifications of the documentation. **Example Usage:** ```python import uuid def uuid_operation(n): # Generate n UUIDs using uuid4 and n using uuid1 uuid4_list = [uuid.uuid4() for _ in range(n)] uuid1_list = [uuid.uuid1() for _ in range(n)] # Combine both lists uuid_combined = uuid4_list + uuid1_list # Identify duplicates uuid_set = set() duplicates = set() for uid in uuid_combined: if uid in uuid_set: duplicates.add(uid) else: uuid_set.add(uid) # Check safety of uuid1 UUIDs safety_list = [(uid, uid.is_safe) for uid in uuid1_list] # Find the UUID with the highest integer value highest_uuid = max(uuid_combined, key=lambda x: x.int) return list(duplicates), safety_list, highest_uuid.int # Example invocation print(uuid_operation(5)) ``` **Parameters:** - `n`: Number of UUIDs to generate using each method. **Expected Output:** The function should return a tuple with the expected findings as described above.","solution":"import uuid def uuid_operation(n): Generates UUIDs using uuid1 and uuid4, identifies duplicates, checks safety and returns the highest UUID. Parameters: n (int): Number of UUIDs to generate using both uuid1 and uuid4. Returns: tuple: containing a list of duplicate UUIDs, a list of tuples (UUID, boolean) indicating safety, and the integer value of the highest UUID. # Generate n UUIDs using uuid4 and n using uuid1 uuid4_list = [uuid.uuid4() for _ in range(n)] uuid1_list = [uuid.uuid1() for _ in range(n)] # Combine both lists uuid_combined = uuid4_list + uuid1_list # Identify duplicates uuid_set = set() duplicates = set() for uid in uuid_combined: if uid in uuid_set: duplicates.add(uid) else: uuid_set.add(uid) # Check safety of uuid1 UUIDs safety_list = [(uid, uid.is_safe) for uid in uuid1_list] # Find the UUID with the highest integer value highest_uuid = max(uuid_combined, key=lambda x: x.int) return list(duplicates), safety_list, highest_uuid.int"},{"question":"Objective Your task is to demonstrate your understanding of scikit-learn transformers by implementing a data preprocessing and dimensionality reduction pipeline. You will work with a synthetic dataset and apply a series of transformations to prepare it for machine learning tasks. Question 1. Generate a synthetic dataset using `sklearn.datasets.make_classification` with the following properties: - Samples: 1000 - Features: 20 (5 informative, 5 redundant, rest random noise) - Classes: 2 - Random State: 42 2. Implement a preprocessing pipeline using scikit-learn that includes the following steps: - Handle missing values by imputing them with the mean of the respective feature. - Standardize the features to have zero mean and unit variance. - Apply Principal Component Analysis (PCA) to reduce the dimensionality of the dataset to 10 components. 3. Write a function `preprocess_and_reduce_dimensionality` that accepts the features (X) and the target (y) of the dataset and performs the preprocessing pipeline described above. The function should return the transformed features and the target. Input - `X` (numpy ndarray): Feature matrix of shape (1000, 20) - `y` (numpy ndarray): Target vector of shape (1000,) Output - Transformed features (numpy ndarray): Feature matrix of shape (1000, 10) - Target vector (numpy ndarray): Target vector of shape (1000,) Code Template ```python import numpy as np from sklearn.datasets import make_classification from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline def preprocess_and_reduce_dimensionality(X, y): Preprocess the data and reduce its dimensionality. Parameters: X (numpy ndarray): Original feature matrix of shape (1000, 20) y (numpy ndarray): Target vector of shape (1000,) Returns: X_transformed (numpy ndarray): Feature matrix of shape (1000, 10) after preprocessing and dimensionality reduction. y_transformed (numpy ndarray): Target vector of shape (1000,) # Define the preprocessing pipeline pipeline = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=10)) ]) # Fit and transform the data X_transformed = pipeline.fit_transform(X) return X_transformed, y # Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=5, n_redundant=5, n_classes=2, random_state=42) # Apply the preprocessing and dimensionality reduction X_transformed, y_transformed = preprocess_and_reduce_dimensionality(X, y) ``` Constraints - You must use the transformers provided by scikit-learn as described. - You are expected to handle the entire preprocessing as a single pipeline. - Ensure that your code is efficient and readable. Performance Requirements - The function should handle the provided synthetic dataset efficiently.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline def preprocess_and_reduce_dimensionality(X, y): Preprocess the data and reduce its dimensionality. Parameters: X (numpy ndarray): Original feature matrix of shape (1000, 20) y (numpy ndarray): Target vector of shape (1000,) Returns: X_transformed (numpy ndarray): Feature matrix of shape (1000, 10) after preprocessing and dimensionality reduction. y_transformed (numpy ndarray): Target vector of shape (1000,) # Define the preprocessing pipeline pipeline = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=10)) ]) # Fit and transform the data X_transformed = pipeline.fit_transform(X) return X_transformed, y # Generate synthetic dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=5, n_redundant=5, n_classes=2, random_state=42) # Apply the preprocessing and dimensionality reduction X_transformed, y_transformed = preprocess_and_reduce_dimensionality(X, y)"},{"question":"# Question **Plotting with Seaborn: Scatter Plot Customization and Faceting** Objective Create a series of scatter plots using the Seaborn library to visualize relationships between variables in a dataset. The plots should utilize various Seaborn functionalities such as color (`hue`), marker style (`style`), and plot facets (`relplot`). Dataset We will use the built-in \\"tips\\" dataset available in Seaborn. Task 1. Load the \\"tips\\" dataset from Seaborn and display its first 5 rows. 2. Create a scatter plot of `total_bill` against `tip`: - Set the color (`hue`) based on the `\'time\'` variable. - Set the marker style (`style`) based on the `\'day\'` variable. 3. Extend the previous scatter plot by: - Setting the size of the points proportional to the `\'size\'` variable. - Setting the color (`hue`) range from 0 to 7. - Customizing the markers so that \\"Lunch\\" is represented by `s` and \\"Dinner\\" by `X`. 4. Create a faceted scatter plot using `relplot` to split the data by the `time` variable (`col=\\"time\\"`): - Use `total_bill` for the x-axis and `tip` for the y-axis. - Differentiate the points using color (`hue`) and marker style (`style`) based on the `day` variable. Your solution should include necessary comments and visualization settings to enhance the readability of the plots. Expected Output 1. A scatter plot displaying the relationship between `total_bill` and `tip` with colors based on `time` and marker styles based on `day`. 2. An enhanced scatter plot where: - Points are sized based on the `size` variable. - The hue range is set from 0 to 7. - Custom markers are set for \\"Lunch\\" and \\"Dinner\\". 3. A faceted scatter plot that: - Splits the data across the `time` variable using `col=\\"time\\"`. - Uses `total_bill` for the x-axis, `tip` for the y-axis. - Utilizes `hue` and `style` to differentiate points based on the `day` variable. Ensure that the plots are well-labeled and legends are clear. Code Template ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Display first 5 rows of the dataset print(tips.head()) # Task 2: Basic scatter plot with hue and style plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\'Scatter Plot of Total Bill vs Tip with Hue and Style\') plt.show() # Task 3: Enhanced scatter plot with size, hue range, and custom markers plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", size=\\"size\\", sizes=(20, 200), hue_norm=(0, 7), style=\\"time\\", markers={\\"Lunch\\": \\"s\\", \\"Dinner\\": \\"X\\"}) plt.title(\'Enhanced Scatter Plot of Total Bill vs Tip with Size and Custom Markers\') plt.show() # Task 4: Faceted scatter plot with relplot sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.suptitle(\'Faceted Scatter Plot of Total Bill vs Tip with Hue and Style by Time\', y=1.02) plt.show() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_and_display_tips(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Display first 5 rows of the dataset print(tips.head()) return tips def basic_scatter_plot(tips): # Basic scatter plot with hue and style plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", style=\\"day\\") plt.title(\'Scatter Plot of Total Bill vs Tip with Hue and Style\') plt.show() def enhanced_scatter_plot(tips): # Enhanced scatter plot with size, hue range, and custom markers plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"size\\", size=\\"size\\", sizes=(20, 200), hue_norm=(0, 7), style=\\"time\\", markers={\\"Lunch\\": \\"s\\", \\"Dinner\\": \\"X\\"}) plt.title(\'Enhanced Scatter Plot of Total Bill vs Tip with Size and Custom Markers\') plt.show() def faceted_scatter_plot(tips): # Faceted scatter plot with relplot sns.relplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"day\\", style=\\"day\\", kind=\\"scatter\\") plt.suptitle(\'Faceted Scatter Plot of Total Bill vs Tip with Hue and Style by Time\', y=1.02) plt.show() # Function to call all tasks sequentially def generate_plots(): tips = load_and_display_tips() basic_scatter_plot(tips) enhanced_scatter_plot(tips) faceted_scatter_plot(tips) # Execute the function generate_plots()"},{"question":"Coding Assessment Question # Objective Write a Python function that trains a logistic regression classifier on a given dataset, tunes the decision threshold using `TunedThresholdClassifierCV` to optimize for the recall metric, and evaluates the model performance on a test set. # Function Signature ```python def tune_threshold(X_train, y_train, X_test, y_test, pos_label): Trains a logistic regression classifier on the training data, tunes the decision threshold to optimize for recall, and evaluates the performance on the test data. Parameters: - X_train: np.ndarray, shape (n_samples_train, n_features)  The training input samples. - y_train: np.ndarray, shape (n_samples_train,)  The target values (class labels) for the training samples. - X_test: np.ndarray, shape (n_samples_test, n_features)  The test input samples. - y_test: np.ndarray, shape (n_samples_test,)  The target values (class labels) for the test samples. - pos_label: int or float  The label of the positive class. Returns: - tuned_thrsh: float  The optimized decision threshold for the logistic regression classifier. - recall_score_test: float  The recall score on the test data using the tuned threshold. pass ``` # Input - `X_train`: 2D numpy array of shape (n_samples_train, n_features), the training input samples. - `y_train`: 1D numpy array of shape (n_samples_train,), the target values (class labels) for the training samples. - `X_test`: 2D numpy array of shape (n_samples_test, n_features), the test input samples. - `y_test`: 1D numpy array of shape (n_samples_test,), the target values (class labels) for the test samples. - `pos_label`: int or float, the label of the positive class. # Output - `tuned_thrsh`: float, the optimized decision threshold for the logistic regression classifier. - `recall_score_test`: float, the recall score on the test data using the tuned threshold. # Constraints - You must use `scikit-learn` for model training and evaluation. - You should use `TunedThresholdClassifierCV` to find the optimal threshold. - Evaluate the recall score on the test data using the tuned threshold. # Example ```python import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression # Generating synthetic binary classification dataset X, y = make_classification(n_samples=1000, n_features=20, weights=[0.1, 0.9], random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Positive class label pos_label = 1 tuned_thrsh, recall_score_test = tune_threshold(X_train, y_train, X_test, y_test, pos_label) print(f\\"Tuned Threshold: {tuned_thrsh}\\") print(f\\"Recall on Test Set: {recall_score_test}\\") ``` # Hints - Use `LogisticRegression` from `sklearn.linear_model`. - You may need to use `predict_proba` to get the probability estimates. - Use `make_scorer` from `sklearn.metrics` to create a custom recall scorer.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.metrics import recall_score, make_scorer from sklearn.model_selection import GridSearchCV class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_classifier=None, scoring=None, cv=5): self.base_classifier = base_classifier self.scoring = scoring self.cv = cv self.threshold_ = 0.5 def fit(self, X, y): self.base_classifier.fit(X, y) # Get prediction probabilities probas = self.base_classifier.predict_proba(X)[:, 1] # Grid search over threshold values thresholds = np.linspace(0, 1, 101) best_score = -np.inf best_threshold = self.threshold_ for threshold in thresholds: preds = (probas >= threshold).astype(int) score = recall_score(y, preds, pos_label=1) if score > best_score: best_score = score best_threshold = threshold self.threshold_ = best_threshold return self def predict(self, X): probas = self.base_classifier.predict_proba(X)[:, 1] return (probas >= self.threshold_).astype(int) def tune_threshold(X_train, y_train, X_test, y_test, pos_label): base_clf = LogisticRegression(max_iter=1000) tuned_clf = TunedThresholdClassifierCV(base_classifier=base_clf) tuned_clf.fit(X_train, y_train) # Get the tuned threshold tuned_thrsh = tuned_clf.threshold_ # Predict using the tuned threshold y_pred = tuned_clf.predict(X_test) # Evaluate recall score on the test data recall_score_test = recall_score(y_test, y_pred, pos_label=pos_label) return tuned_thrsh, recall_score_test"},{"question":"**Question: Create Custom Scatter Plots with Seaborn** As a data scientist, you are given a dataset containing information about sales data. The dataset includes the following columns: - `sales_amount`: The amount of sales made. - `product_category`: The category to which the product belongs. - `region`: The region where the sales were made. - `sales_rep`: The sales representative who made the sales. Your task is to use the seaborn library to create a series of scatter plots (`stripplot`) to visualize the data. Follow the steps below to accomplish this: 1. **Load the Dataset:** - Load the dataset into a pandas DataFrame. 2. **Create a Basic Scatter Plot:** - Create a strip plot showing the distribution of `sales_amount`. 3. **Add Categorical Splits:** To understand the distribution better: - Create a strip plot showing the distribution of `sales_amount` across different `product_category`. 4. **Add Hue and Customize:** - Modify the strip plot to include `region` as a hue variable to differentiate sales amounts by region. - Use a qualitative `palette` named `\\"pastel\\"` to color the points. - Set `dodge=True` to split the regions. 5. **Disable Jitter:** - Update the plot such that the points are not randomly jittered. 6. **Wide-form Data with Orientation:** - Create a wide-form strip plot where each numeric column will be represented on both `x` and `hue`. - Change the orientation to horizontal (`h`). 7. **Facet Plot (catplot):** - Lastly, create a facet plot using `catplot` to compare `sales_amount` across `product_category` per `region` within each `sales_rep`. **Expected Input and Output:** - Your input will be the sales dataset, and your output should be multiple seaborn plots. **Constraints:** - Ensure your code is efficient and only the necessary seaborn functionality is utilized. - Clearly label your axes and include a title for each plot. - Use comments in your code to explain each step. **Performance Requirements:** - Handle datasets up to 1 million rows efficiently. - Aim for clarity and reproducibility in your plots. Here is a sample code structure to help you get started: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # 1. Load the dataset # sales_data = pd.read_csv(\\"path_to_sales_data.csv\\") # 2. Create a basic scatter plot # sns.stripplot(data=sales_data, x=\'sales_amount\') # plt.title(\'Distribution of Sales Amounts\') # plt.show() # 3. Add categorical splits # sns.stripplot(data=sales_data, x=\'sales_amount\', y=\'product_category\') # plt.title(\'Sales Amounts by Product Category\') # plt.show() # 4. Add hue and customize # sns.stripplot(data=sales_data, x=\'sales_amount\', y=\'product_category\', hue=\'region\', palette=\'pastel\', dodge=True) # plt.title(\'Sales Amounts by Product Category with Regions\') # plt.show() # 5. Disable jitter # sns.stripplot(data=sales_data, x=\'sales_amount\', y=\'product_category\', hue=\'region\', palette=\'pastel\', dodge=True, jitter=False) # plt.title(\'Sales Amounts by Product Category with Regions (No Jitter)\') # plt.show() # 6. Wide-form data with orientation # sns.stripplot(data=sales_data, orient=\'h\') # plt.title(\'Wide-form Strip Plot of Sales Data\') # plt.show() # 7. Facet Plot (catplot) # sns.catplot(data=sales_data, kind=\'strip\', x=\'product_category\', y=\'sales_amount\', hue=\'region\', col=\'sales_rep\') # plt.title(\'Facet Plot of Sales Amounts by Category, Region, and Sales Rep\') # plt.show() ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_strip_plots(data_path): # Load the dataset sales_data = pd.read_csv(data_path) # 1. Create a basic scatter plot plt.figure(figsize=(10, 6)) sns.stripplot(data=sales_data, x=\'sales_amount\') plt.title(\'Distribution of Sales Amounts\') plt.xlabel(\'Sales Amount\') plt.show() # 2. Add categorical splits plt.figure(figsize=(10, 6)) sns.stripplot(data=sales_data, x=\'sales_amount\', y=\'product_category\') plt.title(\'Sales Amounts by Product Category\') plt.xlabel(\'Sales Amount\') plt.ylabel(\'Product Category\') plt.show() # 3. Add hue and customize plt.figure(figsize=(10, 6)) sns.stripplot(data=sales_data, x=\'sales_amount\', y=\'product_category\', hue=\'region\', palette=\'pastel\', dodge=True) plt.title(\'Sales Amounts by Product Category with Regions\') plt.xlabel(\'Sales Amount\') plt.ylabel(\'Product Category\') plt.legend(title=\'Region\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # 4. Disable jitter plt.figure(figsize=(10, 6)) sns.stripplot(data=sales_data, x=\'sales_amount\', y=\'product_category\', hue=\'region\', palette=\'pastel\', dodge=True, jitter=False) plt.title(\'Sales Amounts by Product Category with Regions (No Jitter)\') plt.xlabel(\'Sales Amount\') plt.ylabel(\'Product Category\') plt.legend(title=\'Region\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # 5. Wide-form data with orientation plt.figure(figsize=(10, 6)) sns.stripplot(data=sales_data, orient=\'h\') plt.title(\'Wide-form Strip Plot of Sales Data\') plt.show() # 6. Facet Plot (catplot) g = sns.catplot(data=sales_data, kind=\'strip\', x=\'product_category\', y=\'sales_amount\', hue=\'region\', col=\'sales_rep\', palette=\'pastel\', dodge=True) g.fig.suptitle(\'Facet Plot of Sales Amounts by Category, Region, and Sales Rep\') g.set_axis_labels(\'Product Category\', \'Sales Amount\') g.add_legend(title=\'Region\') plt.show()"},{"question":"# Question: You are tasked with implementing a custom logging system for a Python application that requires the following functionality: 1. Log messages to a file that rotates daily. 2. Send error messages (and higher severity) via email to the support team. 3. Store a memory buffer of the last 100 log messages, which can be flushed to a file upon request. Implement the `setup_logging` function to configure the logging system with the above requirements using Python\'s `logging` and `logging.handlers` modules. The function should: - Accept parameters for the log file name, email server settings, and email recipient addresses. - Create and configure appropriate handlers for the file, email, and memory logging. - Ensure that the memory handler can be flushed to a separate file (e.g., \\"memory_flush.log\\") when requested. Function Signature: ```python def setup_logging(log_file: str, email_settings: dict, email_recipients: list): pass ``` Parameters: - `log_file` (str): The base name for the log file. - `email_settings` (dict): A dictionary with email server settings, including `mailhost`, `fromaddr`, `subject`, `credentials`, and `secure`. - `email_recipients` (list): A list of recipient email addresses. Example Usage: ```python log_file = \\"application.log\\" email_settings = { \\"mailhost\\": (\\"smtp.example.com\\", 587), \\"fromaddr\\": \\"app@example.com\\", \\"subject\\": \\"Application Error Log\\", \\"credentials\\": (\\"username\\", \\"password\\"), \\"secure\\": () # Can be empty, or a tuple with the needed SSL/TLS settings } email_recipients = [\\"support1@example.com\\", \\"support2@example.com\\"] setup_logging(log_file, email_settings, email_recipients) import logging logger = logging.getLogger() logger.info(\\"This is an info message.\\") logger.error(\\"This is an error message.\\") ``` Notes: - You must use `TimedRotatingFileHandler` for daily log file rotation. - Use `SMTPHandler` to send error logs via email. - Use `MemoryHandler` to buffer the last 100 log messages. - Include a function to flush the memory buffer to a separate file. Additional Requirement: Implement a function to flush the memory handler to the \\"memory_flush.log\\" file: ```python def flush_memory_handler(): pass ``` **Constraints**: - Handle exceptions gracefully within your setup. - Ensure thread-safety where applicable.","solution":"import logging from logging.handlers import TimedRotatingFileHandler, SMTPHandler, MemoryHandler memory_handler = None # Global variable for the memory handler, to be used for flushing def setup_logging(log_file: str, email_settings: dict, email_recipients: list): global memory_handler # Create a logger logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Create a TimedRotatingFileHandler file_handler = TimedRotatingFileHandler(log_file, when=\\"midnight\\", interval=1) file_handler.setLevel(logging.DEBUG) file_handler.setFormatter(logging.Formatter(\\"%(asctime)s - %(levelname)s - %(message)s\\")) logger.addHandler(file_handler) # Create an SMTPHandler for errors and above email_handler = SMTPHandler( mailhost=email_settings[\'mailhost\'], fromaddr=email_settings[\'fromaddr\'], toaddrs=email_recipients, subject=email_settings[\'subject\'], credentials=email_settings[\'credentials\'], secure=email_settings[\'secure\'] if \'secure\' in email_settings else None ) email_handler.setLevel(logging.ERROR) email_handler.setFormatter(logging.Formatter(\\"Subject: %(subject)snn%(message)s\\")) logger.addHandler(email_handler) # Create a MemoryHandler for the last 100 log messages memory_handler = MemoryHandler(capacity=100, flushLevel=logging.CRITICAL, target=None) logger.addHandler(memory_handler) def flush_memory_handler(): if memory_handler: # Check if memory_handler has been initialized flush_file_handler = logging.FileHandler(\\"memory_flush.log\\") flush_file_handler.setFormatter(logging.Formatter(\\"%(asctime)s - %(levelname)s - %(message)s\\")) memory_handler.setTarget(flush_file_handler) memory_handler.flush() # Force flush of the memory buffer to the file"},{"question":"# Python Coding Assessment Question Objective: To evaluate your understanding and ability to use the `sysconfig` module for retrieving Python configuration information and installation paths for different schemes. Problem Statement: You are provided with a system where various Python components and third-party packages are installed. Your task is to implement a function `print_python_config_info()` that performs the following actions: 1. **Retrieves the current platform and Python version**, and prints them in the format: ``` Platform: <platform> Python version: <version> ``` 2. **Retrieves and prints all the supported installation schemes** and the corresponding default scheme for the current platform in the format: ``` Supported schemes: <scheme1>, <scheme2>, ..., <schemeN> Default scheme: <default_scheme> ``` 3. **Retrieves and prints all the supported installation paths along with their values** for the default scheme in the format: ``` Paths for default scheme: <path_name1>: <path_value1> <path_name2>: <path_value2> ... <path_nameN>: <path_valueN> ``` 4. **Retrieves a list of specific configuration variables (`\'AR\'`, `\'CXX\'`, `\'INCLUDEPY\'`) and prints their values** in the format: ``` Configuration variables: AR: <value> CXX: <value> INCLUDEPY: <value> ``` Constraints: - You can assume that `sysconfig` module is correctly installed and available in the Python environment. - Handle cases where a configuration variable might be `None` by printing `\\"None\\"`. Function Signature: ```python def print_python_config_info(): pass ``` Example Output: An example output of the function should look like this: ``` Platform: win-amd64 Python version: 3.10 Supported schemes: nt, nt_user, posix_prefix, posix_home, posix_user, osx_framework_user, others... Default scheme: nt Paths for default scheme: stdlib: C:Python310Lib platstdlib: C:Python310Libplat-x64 purelib: C:Python310Libsite-packages ... Configuration variables: AR: ar CXX: g++ INCLUDEPY: C:Python310Include ``` This function, when implemented correctly, should provide the necessary Python configuration information and demonstrate your understanding of the `sysconfig` module.","solution":"import sysconfig import platform import sys def print_python_config_info(): # Print current platform and Python version current_platform = platform.system() python_version = platform.python_version() print(f\\"Platform: {current_platform}\\") print(f\\"Python version: {python_version}\\") print() # Print all supported installation schemes and the default scheme supported_schemes = sysconfig.get_scheme_names() default_scheme = sysconfig.get_default_scheme() print(f\\"Supported schemes: {\', \'.join(supported_schemes)}\\") print(f\\"Default scheme: {default_scheme}\\") print() # Print all supported installation paths for the default scheme print(\\"Paths for default scheme:\\") for key, value in sysconfig.get_paths().items(): print(f\\"{key}: {value}\\") print() # Print specific configuration variables config_vars = [\'AR\', \'CXX\', \'INCLUDEPY\'] print(\\"Configuration variables:\\") for var in config_vars: value = sysconfig.get_config_var(var) print(f\\"{var}: {value}\\")"},{"question":"Objective The objective of this assessment is to evaluate your understanding and ability to use Python\'s import-related modules to programmatically manipulate and manage packages and module dependencies. Question Write a Python function that loads a module dynamically from a given zip file and prints out all the metadata of the loaded module. Function Specification ```python def load_module_from_zip(zip_file_path: str, module_name: str) -> None: Load a module dynamically from the given zip archive and print its metadata. Args: - zip_file_path (str): The path to the zip file containing the module. - module_name (str): The name of the module to load. Raises: - FileNotFoundError: If the zip file does not exist. - ImportError: If the module cannot be imported. # Your code here ``` Input - `zip_file_path`: A string representing the path to the zip file containing the Python module. - `module_name`: A string representing the name of the module to load from the zip file. Output - The function does not return any output. It should print the metadata of the loaded module. The metadata should include: - Author - Version - Description Constraints - You may assume the zip file exists and contains valid Python modules. - You should handle any exceptions related to file not found or module import errors gracefully. - You may use the `importlib` and `zipimport` modules to achieve the task. Example ```python # Example usage load_module_from_zip(\'/path/to/my_modules.zip\', \'my_module\') ``` This should print something like: ``` Metadata for module \'my_module\': - Author: Jane Doe - Version: 1.0.0 - Description: This module demonstrates import from a zip file. ``` Hints 1. You may use the `zipimport.zipimporter` class to load the module from zip files. 2. To fetch metadata, consider using `importlib.metadata` functionalities.","solution":"import importlib import importlib.util import zipimport import os def load_module_from_zip(zip_file_path: str, module_name: str) -> None: Load a module dynamically from the given zip archive and print its metadata. Args: - zip_file_path (str): The path to the zip file containing the module. - module_name (str): The name of the module to load. Raises: - FileNotFoundError: If the zip file does not exist. - ImportError: If the module cannot be imported. if not os.path.exists(zip_file_path): raise FileNotFoundError(f\\"The zip file at path {zip_file_path} was not found.\\") try: importer = zipimport.zipimporter(zip_file_path) module = importer.load_module(module_name) metadata = {} for attr in [\'__author__\', \'__version__\', \'__description__\']: metadata[attr] = getattr(module, attr, \'Unknown\') print(f\\"Metadata for module \'{module_name}\':\\") print(f\\"- Author: {metadata[\'__author__\']}\\") print(f\\"- Version: {metadata[\'__version__\']}\\") print(f\\"- Description: {metadata[\'__description__\']}\\") except ImportError: raise ImportError(f\\"Could not import module \'{module_name}\' from zip file.\\")"},{"question":"You are given a list of filenames in a directory and a set of wildcard patterns. Your task is to write a function `match_filenames(filenames, patterns)` that uses the `fnmatch` module to match filenames against each pattern and return a dictionary where each pattern is a key and the corresponding value is a list of filenames that match the pattern. # Function Signature ```python def match_filenames(filenames: list, patterns: list) -> dict: pass ``` # Input - `filenames`: A list of strings representing filenames. - `patterns`: A list of strings representing wildcard patterns. # Output - Returns a dictionary where: - Each key is a pattern from the `patterns` list. - Each value is a list of filenames that match the corresponding pattern. # Constraints 1. Filenames and patterns list lengths will not exceed 1,000 elements. 2. Filenames and patterns strings each will not exceed 255 characters. 3. Performance should be considered when matching patterns against filenames. # Example ```python filenames = [\\"data1.csv\\", \\"data2.csv\\", \\"image.png\\", \\"script.py\\", \\"readme.md\\"] patterns = [\\"*.csv\\", \\"*.py\\", \\"data?.csv\\", \\"readme.*\\"] output = match_filenames(filenames, patterns) # Expected output { \\"*.csv\\": [\\"data1.csv\\", \\"data2.csv\\"], \\"*.py\\": [\\"script.py\\"], \\"data?.csv\\": [\\"data1.csv\\", \\"data2.csv\\"], \\"readme.*\\": [\\"readme.md\\"] } ``` # Additional Notes - You should use the `fnmatch` module from Python\'s standard library. - Ensure that the function is both efficient and correct in matching filenames to patterns. - Think about edge cases such as empty string patterns, filenames without extensions, and mixed case filenames.","solution":"import fnmatch from typing import List, Dict def match_filenames(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]: Matches filenames against wildcard patterns and returns a dictionary where each key is a pattern and the corresponding value is a list of filenames that match the pattern. Args: filenames (List[str]): The list of filenames. patterns (List[str]): The list of wildcard patterns. Returns: Dict[str, List[str]]: A dictionary with patterns as keys and list of matching filenames as values. result = {} for pattern in patterns: matched_files = [filename for filename in filenames if fnmatch.fnmatch(filename, pattern)] result[pattern] = matched_files return result"},{"question":"# Problem: You are provided with a set of data that is suspected to be drawn from a mixture of Gaussian distributions. Your task is to determine the most likely means and variances of these Gaussian distributions using the Expectation-Maximization (EM) algorithm with PyTorch. The EM algorithm for Gaussian mixture models (GMM) involves iteratively applying the Expectation (E) step and the Maximization (M) step until convergence. Specifically: 1. **Initialize** the means (``), variances (`^2`), and mixing coefficients (``). 2. In the **E-step**, update the responsibilities (i.e., the probability that each data point belongs to each Gaussian component). 3. In the **M-step**, update the parameters (``, `^2`, ``) using the responsibilities. # Input: - A 1-D tensor `data` of shape `(n,)` that represents the observed data points. - An integer `k` representing the number of Gaussian components in the mixture. - A float `tol` representing the tolerance threshold for convergence (default value of 1e-4). - An integer `max_iters` representing the maximum number of iterations (default value of 100). # Output: - A dictionary containing: - `\\"means\\"`: Tensor of shape `(k,)` representing the means of the Gaussian components. - `\\"variances\\"`: Tensor of shape `(k,)` representing the variances of the Gaussian components. - `\\"mixing_coefficients\\"`: Tensor of shape `(k,)` representing the mixing coefficients of the Gaussian components. # Constraints: - You must use PyTorch functions for tensor operations. - You are not allowed to use high-level GMM libraries or functions. # Function Signature: ```python import torch from typing import Dict def em_algorithm_gmm(data: torch.Tensor, k: int, tol: float = 1e-4, max_iters: int = 100) -> Dict[str, torch.Tensor]: # Your implementation here pass ``` # Example: ```python data = torch.tensor([1.0, 2.0, 1.5, 8.0, 7.5, 9.0]) k = 2 result = em_algorithm_gmm(data, k) print(result[\\"means\\"]) # Expected output: Tensor containing means of the two components print(result[\\"variances\\"]) # Expected output: Tensor containing variances of the two components print(result[\\"mixing_coefficients\\"]) # Expected output: Tensor containing mixing coefficients of the two components ``` # Notes: - Use the local data for initializing means by randomly selecting `k` data points as initial cluster centers. - Variances and mixing coefficients should be initialized appropriately. - Ensure numerical stability by adding a very small value (e.g., `1e-6`) to variances when necessary. - Monitor the change in the log-likelihood for convergence check between iterations.","solution":"import torch from typing import Dict def em_algorithm_gmm(data: torch.Tensor, k: int, tol: float = 1e-4, max_iters: int = 100) -> Dict[str, torch.Tensor]: n = data.shape[0] # Initialize means by randomly selecting k data points rand_indices = torch.randperm(n)[:k] means = data[rand_indices] # Initialize variances to the variance of the data variances = torch.var(data) * torch.ones(k) # Initialize mixing coefficients uniformly mixing_coefficients = torch.ones(k) / k # Adding small value for numerical stability eps = 1e-6 log_likelihood = -float(\'inf\') for iteration in range(max_iters): # E-step: compute responsibilities responsibilities = torch.zeros(n, k) for j in range(k): normal_dist = torch.exp(-(data - means[j])**2 / (2 * variances[j])) / torch.sqrt(2 * torch.pi * variances[j]) responsibilities[:, j] = mixing_coefficients[j] * normal_dist total_responsibilities = responsibilities.sum(axis=1, keepdims=True) responsibilities /= total_responsibilities + eps # M-step: update parameters Nk = responsibilities.sum(axis=0) means = (responsibilities * data[:, None]).sum(axis=0) / (Nk + eps) variances = (responsibilities * (data[:, None] - means)**2).sum(axis=0) / (Nk + eps) mixing_coefficients = Nk / n # Compute log likelihood new_log_likelihood = torch.sum(torch.log(total_responsibilities + eps)) if torch.abs(new_log_likelihood - log_likelihood) < tol: break log_likelihood = new_log_likelihood return { \\"means\\": means, \\"variances\\": variances + eps, # Ensure positive variance \\"mixing_coefficients\\": mixing_coefficients }"},{"question":"# Heap-based Task Scheduler **Objective:** You are to implement a simple task scheduler using a heap to manage tasks based on their priorities. # Requirements: 1. **Task Addition with Dynamic Priority:** - Implement a function `add_task(task, priority)` that adds a new task or updates the priority of an existing task: ```python add_task(\'task_name\', 5) ``` - If the task is already present, its priority should be updated. - Use a binary heap to ensure tasks are ordered correctly by their priority. 2. **Task Removal:** - Implement a function `remove_task(task)` that removes an existing task from the heap. 3. **Task Execution:** - Implement a function `pop_task()` that removes and returns the task with the highest priority (smallest value as highest priority). # Input and Output formats: - You will use the following class `TaskScheduler`: ```python class TaskScheduler: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: str, priority: int): Add a new task or update the priority of an existing task. :param task: str - Task name :param priority: int - Priority value if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task: str): Mark an existing task as REMOVED. Raise KeyError if task not found. :param task: str - Task name entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the lowest priority task. Raise KeyError if empty. :return: str - Task name while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') # Example Usage: scheduler = TaskScheduler() scheduler.add_task(\'task1\', 5) scheduler.add_task(\'task2\', 3) scheduler.add_task(\'task3\', 4) print(scheduler.pop_task()) # Should return \'task2\' because it has the highest priority scheduler.remove_task(\'task3\') print(scheduler.pop_task()) # Should return \'task1\' because \'task3\' was removed and \'task1\' is the next highest priority ``` **Constraints:** - The task names will only contain lowercase alphabets and numbers. - The priority will be an integer within the range [1, 1000]. - Ensure that the solution maintains efficient insertions and deletions to keep the operations close to O(log n). Write your solution as part of the class `TaskScheduler` with methods `add_task`, `remove_task`, and `pop_task` implemented.","solution":"import heapq import itertools class TaskScheduler: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task: str, priority: int): Add a new task or update the priority of an existing task. :param task: str - Task name :param priority: int - Priority value if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task: str): Mark an existing task as REMOVED. Raise KeyError if task not found. :param task: str - Task name entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the lowest priority task. Raise KeyError if empty. :return: str - Task name while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"**Title:** Implementing Robust Debugging with `faulthandler` **Objective:** You are tasked with creating a Python script that robustly handles unexpected terminations by utilizing the `faulthandler` module. Your implementation should demonstrate comprehensive use of the functionalities provided by this module. **Problem Statement:** Write a Python script that performs the following: 1. **Enables** the fault handler and sets it to log to a file called `traceback.log`. 2. **Registers** a user signal (`SIGUSR1` on Unix-like systems) to dump the traceback for all threads to `traceback.log`. 3. **Sets a timeout** of 5 seconds after which the script should dump the traceback to `traceback.log`. 4. **Sets another timeout** of 10 seconds that also dumps the traceback to `traceback.log` and then exits the script immediately. 5. **Simulates** doing some processing in a loop with a print statement every second, such as `print(\\"Running...\\")`. 6. **Handles cleanup** by disabling the fault handler before the script terminates if interrupted by the user (e.g., by a KeyboardInterrupt). **Requirements/Constraints:** * Assume the script runs on a Unix-like environment where `SIGUSR1` is available. * You must not use any external libraries other than the standard library. * The file `traceback.log` should remain open and be correctly managed to avoid any file descriptor issues as outlined in the documentation. # Example Output: The offered script should run for a maximum of 10 seconds, printing \\"Running...\\" each second, unless interrupted by a user. If the script is not interrupted, it should generate a `traceback.log` with multiple traceback entries as per the faulthandler\'s setup. ```python # Example code workflow (non-functional): import faulthandler import signal import time # Step 1: Enable fault handler faulthandler.enable(open(\\"traceback.log\\", \\"w\\")) # Step 2: Register user signal for dumping traceback faulthandler.register(signal.SIGUSR1, open(\\"traceback.log\\", \\"a\\")) # Step 3: Set first timeout faulthandler.dump_traceback_later(5, file=open(\\"traceback.log\\", \\"a\\")) # Step 4: Set second timeout with exit faulthandler.dump_traceback_later(10, exit=True, file=open(\\"traceback.log\\", \\"a\\")) # Step 5: Simulate processing try: for i in range(10): print(\\"Running...\\") time.sleep(1) except KeyboardInterrupt: # Step 6: Cleanup faulthandler.disable() print(\\"Execution interrupted by user.\\") ``` # Evaluation: - **Correctness:** The script correctly uses the `faulthandler` module to manage fault handling and tracebacks as described. - **Robustness:** The script handles the file management and potential interruptions (like KeyboardInterrupt) gracefully. - **Clarity:** The script is well-organized and includes comments explaining each major step. - **Adherence to Constraints:** The solution adheres to the requirements and operates within the standard library only.","solution":"import faulthandler import signal import time def setup_fault_handler(): Sets up the fault handler to dump tracebacks to a file. with open(\\"traceback.log\\", \\"w\\") as f: faulthandler.enable(file=f) with open(\\"traceback.log\\", \\"a\\") as f: # Register SIGUSR1 to dump traceback faulthandler.register(signal.SIGUSR1, file=f) with open(\\"traceback.log\\", \\"a\\") as f: # Set 5 seconds timeout to dump traceback faulthandler.dump_traceback_later(5, file=f) with open(\\"traceback.log\\", \\"a\\") as f: # Set 10 seconds timeout to dump traceback and exit faulthandler.dump_traceback_later(10, exit=True, file=f) def disable_fault_handler(): Disables the fault handler. faulthandler.disable() def run_process(): Simulate a process that prints \\"Running...\\" every second. try: for _ in range(10): print(\\"Running...\\") time.sleep(1) except KeyboardInterrupt: print(\\"Execution interrupted by user.\\") finally: disable_fault_handler() if __name__ == \\"__main__\\": setup_fault_handler() run_process()"},{"question":"# Question: Iterators and Generators in Python You are required to design a custom iterator and generator. This challenge will test your understanding of Python\'s iterator and generator protocols, along with some functional programming concepts using itertools. Part 1: Custom Iterator Implement a class `SquaredNumbers` that acts as an iterator, returning the square of numbers from `start` to `end` (inclusive). If the current square exceeds `max_square`, iteration should stop. **Input:** - `start`: An integer, the starting number (inclusive). - `end`: An integer, the ending number (inclusive). - `max_square`: An integer, maximum allowed value for the squared numbers. **Output:** - An iterator yielding the squares of each integer from `start` to `end` (inclusive), stopping if the square exceeds `max_square`. **Constraints:** - `start` and `end` are positive integers with `start <= end`. - `max_square` is a positive integer. **Example:** ```python squared_numbers = SquaredNumbers(start=1, end=10, max_square=50) for square in squared_numbers: print(square) ``` Expected Output: ``` 1 4 9 16 25 36 49 ``` Part 2: Generator for Fibonacci Sequence Develop a generator function `fibonacci_generator` that yields the elements of the Fibonacci sequence indefinitely. **Input:** - None **Output:** - A generator yielding the Fibonacci sequence indefinitely. **Example:** ```python fib = fibonacci_generator() for _ in range(10): print(next(fib)) ``` Expected Output: ``` 0 1 1 2 3 5 8 13 21 34 ``` Part 3: Using itertools Using `itertools`, implement a composite iterator `prefix_sums` that yields all prefix sums (cumulative sums) of a given list of numbers. Use `itertools.accumulate`. **Input:** - `numbers`: A list of integers. **Output:** - An iterator yielding the cumulative sums of the given list. **Example:** ```python numbers = [1, 2, 3, 4, 5] prefix_sums = prefix_sums(numbers) for ps in prefix_sums: print(ps) ``` Expected Output: ``` 1 3 6 10 15 ``` Your Task: 1. Implement the `SquaredNumbers` iterator class. 2. Implement the `fibonacci_generator` generator function. 3. Implement the `prefix_sums` composite iterator function using `itertools.accumulate`. Ensure correct functionality and edge case handling through examples and thorough testing.","solution":"import itertools class SquaredNumbers: def __init__(self, start, end, max_square): self.start = start self.end = end self.max_square = max_square self.current = start def __iter__(self): return self def __next__(self): if self.current > self.end: raise StopIteration square = self.current ** 2 if square > self.max_square: raise StopIteration self.current += 1 return square def fibonacci_generator(): a, b = 0, 1 while True: yield a a, b = b, a + b def prefix_sums(numbers): return itertools.accumulate(numbers)"},{"question":"Question Title: Function Scope and Exception Handling in Python You are required to implement a function `calculate_expression(expression: str, x: int) -> int` which takes in two arguments: 1. `expression`: A string representing a mathematical expression that can include variable `x`. 2. `x`: An integer that should substitute the variable `x` in the `expression`. The function should evaluate the mathematical expression using the given value of `x` and return the result as an integer. If the expression evaluates to a non-integer value, an exception should be raised. The function must handle the following: - Variable substitution for `x`. - Correct scoping such that dynamically assigned variables do not interfere with the expression evaluation. - Proper exception handling for: - Division by zero (`ZeroDivisionError`). - Non-integer results (`TypeError`). The function should use `eval` to evaluate the expression within a controlled environment to avoid executing arbitrary code. Function Signature ```python def calculate_expression(expression: str, x: int) -> int: pass ``` Example ```python >>> calculate_expression(\'2 * x + 3\', 5) 13 >>> calculate_expression(\'x / 2\', 4) # Raises TypeError because the result is not an integer >>> calculate_expression(\'10 / (5 - 5)\', 5) # Raises ZeroDivisionError because of division by zero ``` Constraints - The `expression` string will only contain valid Python mathematical expressions using basic arithmetic operators (`+`, `-`, `*`, `/`). - The variable represented in the `expression` will always be `x`. - `x` will be an integer within the range -10^9 to 10^9. - The expression will be a non-empty string of length no greater than 100 characters. Notes - Do not use any unsafe practices or external libraries. - Ensure that the function only evaluates mathematical expressions and handles exceptions as specified.","solution":"def calculate_expression(expression: str, x: int) -> int: Evaluates the mathematical expression using the given value of x and returns the result as an integer. Raises exceptions on zero division or non-integer results. allowed_locals = {\'x\': x} try: result = eval(expression, {\\"__builtins__\\": {}}, allowed_locals) if not isinstance(result, int): raise TypeError(\\"The result is not an integer.\\") except ZeroDivisionError: raise ZeroDivisionError(\\"Division by zero occurred.\\") return result"},{"question":"# Question: Copy-on-Write in Pandas The recent versions of pandas have introduced Copy-on-Write (CoW) behavior that ensures more predictable and safer data manipulations. To assess your understanding of these concepts, solve the following problem. Problem Statement You are given a DataFrame `df` and need to perform several operations on it, making sure to adhere to CoW principles. 1. Create a DataFrame named `df` with the following data: ```` data = { \'A\': [1, 2, 3], \'B\': [4, 5, 6] } ```` 2. Extract a Series `subset` from column `A` of `df` and modify the first element of `subset` to 100. - Verify that the original DataFrame `df` remains unchanged. 3. Create a new DataFrame `df2` by resetting the index of `df` without copying the data, and then: - Modify the first element of column `A` in `df2` to 200. - Verify that `df` remains unchanged. 4. Select column `A` of `df` where column `B` is greater than 5 and set the value to 300. - Verify that this operation raises a `ChainedAssignmentError` and then rewrite it using `loc` to perform the modification correctly. 5. Access the underlying NumPy array of `df`. Attempt to modify the first element and observe the read-only error. Then, make the array writable and modify the first element to 400. Input and Output Ensure all operations follow the CoW principles. Provide the final state of `df` and any intermediates, along with verification commands that demonstrate `df` is unaffected by inappropriate modifications. Constraints and Requirements Your solution should: - Handle immutable views correctly. - Avoid chained assignments. - Demonstrate handling of read-only NumPy arrays. - Adhere to CoW rules and constraints. # Expected Output The final output should display the contents of all created DataFrames and the changes you attempted, with clear evidence of CoW principles in action. Example: ```python import pandas as pd # Step 1: Create the DataFrame df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [4, 5, 6]}) # Step 2: Modify the subset and check df remains unchanged subset = df[\'A\'] subset.iloc[0] = 100 print(df) # Step 3: Reset index and modify without affecting original df2 = df.reset_index(drop=True) df2.iloc[0, 0] = 200 print(df) print(df2) # Step 4: Chained assignment and proper `loc` method try: df[\'A\'][df[\'B\'] > 5] = 300 except pd.errors.ChainedAssignmentError as e: print(f\\"Caught an error: {e}\\") df.loc[df[\'B\'] > 5, \'A\'] = 300 print(df) # Step 5: Modify underlying array arr = df.to_numpy() try: arr[0, 0] = 400 except ValueError as e: print(f\\"Caught an error: {e}\\") arr.flags.writeable = True arr[0, 0] = 400 print(df) ```","solution":"import pandas as pd # Step 1: Create the DataFrame df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [4, 5, 6]}) # Step 2: Modify the subset and check df remains unchanged subset = df[\'A\'].copy() # Explicitly copy the series to adhere to CoW subset.iloc[0] = 100 # Check that original df remains unchanged assert df.loc[0, \'A\'] == 1 # Step 3: Reset index and modify without affecting original df2 = df.reset_index(drop=True) # This creates a new DataFrame but does not copy the data df2 = df2.copy() # Explicit copy to follow CoW df2.iloc[0, 0] = 200 # Check that original df remains unchanged assert df.loc[0, \'A\'] == 1 # Step 4: Properly using loc to avoid chained assignment try: df[\'A\'][df[\'B\'] > 5] = 300 # This should raise a warning/error except pd.errors.ChainedAssignmentError: df.loc[df[\'B\'] > 5, \'A\'] = 300 # Proper way to avoid chained assignment # Verify the change assert df.loc[2, \'A\'] == 300 # Step 5: Modify underlying array arr = df.to_numpy() # Attempt to modify the first element try: arr[0, 0] = 400 except ValueError as e: print(f\\"Caught an error: {e}\\") arr.flags.writeable = True # Make the array writable arr[0, 0] = 400 # Verify the change assert df.loc[0, \'A\'] == 400 # Final state of df print(df)"},{"question":"Objective To assess your understanding of pandas\' sparse data structures and your ability to implement functionalities using SparseArray and SparseDtype. Problem Statement You are given a large dataset with many missing (NaN) values and default values (such as zeros). Your task is to efficiently handle this dataset using pandas sparse structures. You need to implement a function `efficient_sparse_operations` that performs the following tasks: 1. **Create Sparse DataFrame**: - Convert a given dense DataFrame into a Sparse DataFrame where the fill value is NaN for columns with floating-point numbers and 0 for columns with integer numbers. 2. **Calculate Density**: - Calculate the density (percentage of stored non-fill values) for each column in the Sparse DataFrame. 3. **Sparse Calculations**: - Apply a specific NumPy `ufunc` (e.g., `np.abs`) to all columns of the Sparse DataFrame and return the result as a new Sparse DataFrame. 4. **Conversion to Dense**: - Convert the resultant Sparse DataFrame back into a dense DataFrame and return it. Function Signature ```python def efficient_sparse_operations(df: pd.DataFrame, ufunc: np.ufunc) -> pd.DataFrame: pass ``` Input - `df`: A `pandas.DataFrame` object containing both NaN and zero values. - `ufunc`: A NumPy universal function applied to each column of the Sparse DataFrame. Output - Returns a dense DataFrame after applying all the specified operations. Constraints - The DataFrame `df` can have up to 100,000 rows and 10 columns. - The operations should be performed efficiently in terms of memory usage. Example ```python import pandas as pd import numpy as np # Example DataFrame data = { \'A\': np.random.randn(10), \'B\': np.random.randint(0, 10, 10), } data[\'A\'][3:7] = np.nan data[\'B\'][2:5] = 0 df = pd.DataFrame(data) # Applying the function result_df = efficient_sparse_operations(df, np.abs) print(result_df) ``` Here, the function `efficient_sparse_operations` will: 1. Convert `df` to a Sparse DataFrame. 2. Calculate the density for each column. 3. Apply the `np.abs` function to all columns. 4. Convert the Sparse DataFrame back to a dense DataFrame and return it. Note - Ensure that the implementation efficiently handles memory by using pandas\' sparse functionalities. - Include necessary docstrings and comments in your code to describe the logic.","solution":"import pandas as pd import numpy as np def efficient_sparse_operations(df: pd.DataFrame, ufunc: np.ufunc) -> pd.DataFrame: Efficiently handle a DataFrame with many missing (NaN) values and default values. Parameters: df (pd.DataFrame): Input DataFrame containing NaN and zero values. ufunc (np.ufunc): A NumPy universal function to be applied to all columns of the Sparse DataFrame. Returns: pd.DataFrame: A dense DataFrame after applying the specified operations. # Create Sparse DataFrame sparse_df = df.astype(pd.SparseDtype(float, np.nan)) for col in df.select_dtypes(include=[np.int_, np.int64, np.int32]).columns: sparse_df[col] = df[col].astype(pd.SparseDtype(int, 0)) # Calculate and print density for each column density = sparse_df.sparse.density print(f\'Density:n{density}\') # Apply the specified numpy ufunc to all columns sparse_result = sparse_df.apply(ufunc) # Convert the Sparse DataFrame back to a dense DataFrame dense_result = sparse_result.sparse.to_dense() return dense_result"},{"question":"# Email Iterators Programming Assessment Objective To test your understanding of Python\'s `email` package and its `iterators` module, you will implement a function that processes the contents of an email message. Problem Statement You are given a multi-part email message object. Your task is to extract all lines of text from the message, skipping over headers and non-text payloads. Additionally, all extracted lines should be returned as a list of strings. Function Signature ```python def extract_text_lines_from_email(msg) -> List[str]: pass ``` Input - `msg`: A multi-part email message object created using Python\'s `email.message_from_string` or `email.message_from_bytes`. Output - A list of strings where each string is a line of the text content from the email message. The list should not contain any headers or non-text parts. Constraints - The input email message can be deeply nested with multiple levels of subparts. - Payloads not of type text/plain or text/html should be ignored. - Performance should be efficient enough for processing an email with up to 1000 lines in total. Example Usage ```python import email from email.policy import default email_content = \'\'\' Content-Type: multipart/mixed; boundary=\\"===============4037862105791559957==\\" MIME-Version: 1.0 From: sender@example.com To: recipient@example.com Subject: Example Email --===============4037862105791559957== Content-Type: text/plain; charset=\\"us-ascii\\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Hello, This is an example email. --===============4037862105791559957== Content-Type: application/octet-stream MIME-Version: 1.0 Content-Transfer-Encoding: base64 SGVsbG8sIFRoaXMgaXMgYSBmaWxlLg== --===============4037862105791559957==-- \'\'\' msg = email.message_from_string(email_content, policy=default) print(extract_text_lines_from_email(msg)) ``` Output: ```python [\'Hello,\', \'This is an example email.\'] ``` # Note For decoding and formatting purposes, you may leverage the functions from the `email.iterators` module as described in the documentation above.","solution":"from typing import List import email from email.message import Message from email.policy import default def extract_text_lines_from_email(msg: Message) -> List[str]: Extract all lines of text from the email message, skipping headers and non-text payloads. result = [] def extract_text_parts(message: Message): if message.is_multipart(): for part in message.get_payload(): extract_text_parts(part) else: if message.get_content_type() in [\'text/plain\', \'text/html\']: payload = message.get_payload(decode=True) charset = message.get_content_charset() if charset: payload = payload.decode(charset, errors=\'replace\') result.extend(payload.splitlines()) extract_text_parts(msg) return result"},{"question":"**Advanced PyTorch Distributed RPC Task** # Objective: You are tasked with implementing functionality related to the Remote Reference (RRef) protocol in PyTorch\'s distributed RPC framework. The goal is to create a function that demonstrates the life cycle of an RRef, including its creation, usage, and deletion across different worker nodes. # Background: The RRef protocol handles remote references by ensuring that the object being referenced is only deleted when there are no more references to it. This involves sending acknowledgment messages between the owner and user workers. # Task: Implement a distributed RPC scenario where Worker A creates a `UserRRef`, shares it with Worker B, and then Worker B shares it with Worker C. The scenario should include all necessary steps to ensure that RRefs are properly managed and deleted according to the protocol discussed in the documentation. # Details: - Use PyTorch\'s RPC framework to set up distributed workers. - Worker A creates an initial `UserRRef`. - Worker A sends the `UserRRef` to Worker B. - Worker B sends the `UserRRef` to Worker C. - Ensure messages are acknowledged correctly and no references are prematurely deleted. - Demonstrate that the `OwnerRRef` is deleted only when there are no more `UserRRef` instances and no references to the `OwnerRRef`. # Constraints: - Assume transient network failures can occur, and messages might be received out of order. - Ensure that all workers handle reference counting properly. - You may assume workers are set up and communicate without errors (node crashes are not a concern). # Input and Output: - The input will be the setup instructions for the distributed workers. - The output should demonstrate correct management of RRefs through logs/prints showing creation, sharing, acknowledgment, and deletion. # Requirements: - Use PyTorch\'s `torch.distributed.rpc` module. - Ensure that your implementation is efficient and handles potential message delays. - Provide clear comments and logging to explain each step. # Example (Pseudo code): ```python import torch import torch.distributed.rpc as rpc # Function Definitions def create_rref(): return rpc.RRef(torch.tensor([1, 2, 3])) def pass_rref(rref, dst): rpc.rpc_sync(dst, use_rref, args=(rref,)) def use_rref(rref): # Example usage of rref print(\\"RRef Value:\\", rref.to_here()) # Main Execution if __name__ == \\"__main__\\": # Initialize RPC framework world_size = 3 rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) if rank == 0: rref = create_rref() pass_rref(rref, \'worker1\') elif rank == 1: rref = rpc.remote(\'worker0\', create_rref) pass_rref(rref, \'worker2\') elif rank == 2: rref = rpc.remote(\'worker1\', create_rref) use_rref(rref) rpc.shutdown() ``` Provide a complete and functional implementation based on the above pseudo-code. Test your solution to ensure it meets the requirements.","solution":"import os import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef # Function to create and return a RRef def create_rref_worker(): return RRef(torch.tensor([1, 2, 3])) # Function to use an RRef def use_rref_worker(rref): tensor = rref.to_here() print(f\\"Worker {rpc.get_worker_info().name} got the tensor: {tensor}\\") # Function to pass a RRef to another worker and use it def pass_rref_worker(rref, dst_worker_name): print(f\\"Passing RRef from {rpc.get_worker_info().name} to {dst_worker_name}\\") _ = rpc.rpc_sync(dst_worker_name, use_rref_worker, args=(rref,)) if __name__ == \\"__main__\\": world_size = 3 rank = int(os.environ[\'RANK\']) # Assuming RANK is set properly in the environment rpc.init_rpc(f\\"worker{rank}\\", rank=rank, world_size=world_size) if rank == 0: rref = create_rref_worker() print(f\\"Worker0 created RRef: {rref}\\") pass_rref_worker(rref, \\"worker1\\") elif rank == 1: rref = rpc.remote(\\"worker0\\", create_rref_worker) print(f\\"Worker1 received RRef: {rref}\\") pass_rref_worker(rref, \\"worker2\\") elif rank == 2: rref = rpc.remote(\\"worker1\\", create_rref_worker) print(f\\"Worker2 received RRef: {rref}\\") use_rref_worker(rref) # Ensuring all RPCs are completed before shutdown rpc.shutdown()"},{"question":"# Python Coding Assessment Question Problem Statement You are part of a development team working on automating the process of creating source distributions for Python projects. Your task is to write a Python script that leverages the Distutils library to create a source distribution (`sdist`) and has the following capabilities: 1. Generate a source distribution archive of the specified format(s) by accepting the formats as command-line arguments. 2. Customize the list of files to be included in the source distribution based on a given manifest template (`MANIFEST.in`). 3. Support additional command-line options to control default inclusions and exclusions. 4. Provide the option to only generate the manifest file without creating a source distribution archive. Requirements 1. Implement a function named `create_source_distribution` that takes the following parameters: * `formats`: A list of formats in which the source distribution archives should be created (e.g., `[\'gztar\', \'zip\']`). * `manifest_only`: A boolean flag indicating whether only the manifest file should be generated without creating the source distribution archive. * `no_defaults`: A boolean flag to disable the standard inclusion set. * `no_prune`: A boolean flag to disable the standard exclude set. 2. The function should use Distutils commands to: * Create the source distribution in the specified formats. * Generate or use the existing `MANIFEST` file as per the parameters and options provided. * Print the steps performed and any relevant information during the process. 3. Include and consider: * Standard files to be included (as mentioned in the documentation). * Proper handling of `MANIFEST.in` for custom file inclusion and exclusion. * Proper usage of Distutils options and flags. Input Format - The script should be executed from the command line and accept inputs in the following form: ``` python your_script.py --formats gztar zip --manifest-only --no-defaults --no-prune ``` Output Format - The script should output logs indicating the steps and results of the source distribution generation process. Constraints - The script should handle possible errors such as invalid format inputs, missing required files, and lack of necessary permissions. Example Suppose you run the script with: ``` python your_script.py --formats gztar zip ``` The expected behavior is: 1. A gzipped tar file (`.tar.gz`) and a zip file (`.zip`) should be created in the current directory containing the source distribution. 2. The appropriate log messages should be printed detailing the creation process. Here is the starting point for the script: ```python from distutils.core import run_setup import argparse import os def create_source_distribution(formats, manifest_only, no_defaults, no_prune): # Build the sdist command with appropriate options sdist_command = \'python setup.py sdist\' if formats: sdist_command += \' --formats=\' + \',\'.join(formats) if manifest_only: sdist_command += \' --manifest-only\' if no_defaults: sdist_command += \' --no-defaults\' if no_prune: sdist_command += \' --no-prune\' # Run the sdist command os.system(sdist_command) if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\'Create a source distribution for the given formats.\') parser.add_argument(\'--formats\', nargs=\'+\', help=\'List of formats to create source distribution archives (e.g., gztar zip)\') parser.add_argument(\'--manifest-only\', action=\'store_true\', help=\'Only generate the manifest file and do not create the source distribution archives\') parser.add_argument(\'--no-defaults\', action=\'store_true\', help=\'Disable the standard inclusion set\') parser.add_argument(\'--no-prune\', action=\'store_true\', help=\'Disable the standard exclusion set\') args = parser.parse_args() create_source_distribution(args.formats, args.manifest_only, args.no_defaults, args.no_prune) ``` Note Ensure you have a `setup.py` and `MANIFEST.in` in the same directory as the script to test its functionality. Adjust the file handling and logging as needed per your specific setup.","solution":"import argparse import os from distutils.core import run_setup def create_source_distribution(formats, manifest_only, no_defaults, no_prune): sdist_command = \'python setup.py sdist\' if formats: sdist_command += \' --formats=\' + \',\'.join(formats) if manifest_only: sdist_command += \' --manifest-only\' if no_defaults: sdist_command += \' --no-defaults\' if no_prune: sdist_command += \' --no-prune\' print(f\\"Executing command: {sdist_command}\\") result = os.system(sdist_command) if result != 0: print(\\"Error in executing sdist command.\\") else: print(\\"Source distribution created successfully.\\") if __name__ == \'__main__\': parser = argparse.ArgumentParser(description=\'Create a source distribution for the given formats.\') parser.add_argument(\'--formats\', nargs=\'+\', help=\'List of formats to create source distribution archives (e.g., gztar zip)\') parser.add_argument(\'--manifest-only\', action=\'store_true\', help=\'Only generate the manifest file and do not create the source distribution archives\') parser.add_argument(\'--no-defaults\', action=\'store_true\', help=\'Disable the standard inclusion set\') parser.add_argument(\'--no-prune\', action=\'store_true\', help=\'Disable the standard exclusion set\') args = parser.parse_args() create_source_distribution(args.formats, args.manifest_only, args.no_defaults, args.no_prune)"},{"question":"Objective Your task is to implement and work with Python data classes. This will demonstrate your understanding of the `dataclasses` module, including creating data classes, utilizing various features, and ensuring proper data encapsulation and manipulation. Problem Statement You are required to create a set of data classes to manage a school\'s student and course enrollment system. The requirements are as follows: 1. **Student Class**: - A student has the following attributes: `student_id` (int), `name` (str), `email` (str), and `enrolled_courses` (list of Course objects). - Implement a method to add a course to the student\'s list of enrolled courses. - Implement a method to remove a course from the student\'s list of enrolled courses. 2. **Course Class**: - A course has the following attributes: `course_id` (int), `course_name` (str), and `credits` (int). 3. **School Class**: - A school has the following attributes: `name` (str) and `students` (list of Student objects). - Implement a method to add a student to the school. - Implement a method to remove a student from the school. - Implement a method to get the total number of credits for a given student\'s `student_id`. Requirements 1. Use the `dataclass` decorator for defining the classes. 2. Ensure that the `student_id` and `course_id` are unique and auto-incremented starting from 1. 3. Ensure appropriate type hints for all attributes and methods. Input and Output - **Input:** - Operations to add students, add/remove courses, and get total credits for a student. - **Output:** - The system should print confirmation messages when students are added or removed, courses are added or removed, and the total number of credits for a student. Example ```python # Initialize the school my_school = School(name=\\"Springfield High\\") # Add students my_school.add_student(\\"John Doe\\", \\"john.doe@example.com\\") my_school.add_student(\\"Jane Smith\\", \\"jane.smith@example.com\\") # Add courses math_course = Course(course_name=\\"Mathematics\\", credits=3) history_course = Course(course_name=\\"History\\", credits=4) # Enroll John in Mathematics and History john = my_school.students[0] john.add_course(math_course) john.add_course(history_course) # Calculate John\'s total credits print(my_school.get_student_credits(john.student_id)) # Output: 7 ``` Constraints - Assume that the student and course names provided are always unique. - Ensure to handle cases where a student tries to enroll in the same course more than once. - Handle attempts to remove a course that a student is not enrolled in gracefully. Performance - Your implementation should be optimized for efficiency in terms of time complexity, especially for operations involving addition and removal from lists.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Course: course_name: str credits: int course_id: int = field(init=False) _course_id_counter: int = 0 def __post_init__(self): Course._course_id_counter += 1 self.course_id = Course._course_id_counter @dataclass class Student: name: str email: str student_id: int = field(init=False) enrolled_courses: List[Course] = field(default_factory=list) _student_id_counter: int = 0 def __post_init__(self): Student._student_id_counter += 1 self.student_id = Student._student_id_counter def add_course(self, course: Course): if course not in self.enrolled_courses: self.enrolled_courses.append(course) def remove_course(self, course: Course): if course in self.enrolled_courses: self.enrolled_courses.remove(course) @dataclass class School: name: str students: List[Student] = field(default_factory=list) def add_student(self, name: str, email: str): student = Student(name=name, email=email) self.students.append(student) def remove_student(self, student_id: int): student_to_remove = None for student in self.students: if student.student_id == student_id: student_to_remove = student break if student_to_remove: self.students.remove(student_to_remove) def get_student_credits(self, student_id: int) -> int: for student in self.students: if student.student_id == student_id: return sum(course.credits for course in student.enrolled_courses) return 0"},{"question":"Problem Statement You are tasked with writing a function to handle the creation and validation of email headers based on the `email.headerregistry` module. Your function should receive an email header\'s name and value, use the appropriate header class to create an instance of the header, and then ensure that the header is RFC 5322 compliant. # Function Signature ```python def create_and_validate_header(name: str, value: str) -> str: pass ``` # Input - `name` (str): The name of the email header (e.g., \'Subject\', \'Date\', \'To\'). - `value` (str): The value of the email header. # Output - Returns the `str` representation of the created header if it is valid. - If the header is invalid, raises a `ValueError` with a message indicating the defect. # Constraints - The function should be able to handle common header types provided by the `email.headerregistry` module, including but not limited to `UnstructuredHeader`, `DateHeader`, `AddressHeader`. # Example ```python from datetime import datetime # Example 1: Valid unstructured header print(create_and_validate_header(\'Subject\', \'This is a test\')) # Expected output: \'This is a test\' # Example 2: Valid date header print(create_and_validate_header(\'Date\', \'Wed, 21 Oct 2015 07:28:00 -0700\')) # Expected output: \'Wed, 21 Oct 2015 07:28:00 -0700\' # Example 3: Valid address header print(create_and_validate_header(\'To\', \'Alice <alice@example.com>, Bob <bob@example.com>\')) # Expected output: \'Alice <alice@example.com>, Bob <bob@example.com>\' # Example 4: Invalid address header try: print(create_and_validate_header(\'To\', \'invalid-email\')) except ValueError as e: print(e) # Expected output: \'Invalid email address format\' ``` # Notes - To implement this function, you should utilize the `email.headerregistry.HeaderRegistry` and other classes provided by the `email.headerregistry` module. - Perform appropriate validation of header values based on their types and ensure RFC 5322 compliance. - Handle parsing and defect reporting mechanisms as described in the provided documentation. Happy coding!","solution":"from email.policy import default from email.headerregistry import HeaderRegistry, UnstructuredHeader, DateHeader, AddressHeader from email.errors import HeaderParseError import re def create_and_validate_header(name: str, value: str) -> str: Creates and validates an email header using email.headerregistry module. Args: name (str): The name of the email header (e.g., \'Subject\', \'Date\', \'To\'). value (str): The value of the email header. Returns: str: The string representation of the created header if valid. Raises: ValueError: If the header is invalid. registry = HeaderRegistry() try: header = registry(name, value) except HeaderParseError as e: raise ValueError(f\'Header parsing error: {str(e)}\') # Ensure the header is RFC 5322 compliant if header.defects: defect_messages = \'; \'.join(str(d) for d in header.defects) raise ValueError(f\'Header has defects: {defect_messages}\') return str(header)"},{"question":"You are required to demonstrate your understanding of the Unix `syslog` library routines provided by Python\'s `syslog` module. Specifically, you will work with logging messages at different priority levels, configuring log options, and managing log masks. Task: 1. **Initialize Syslog**: - Use the `openlog` function to initialize the syslog module. - Set the `ident` to \\"TestLogger\\". - Configure it to include the process ID (`LOG_PID`) and write messages to the user-level facility (`LOG_USER`). 2. **Log Various Messages**: - Log a message with the priority level `LOG_INFO`. - Log another message with the priority level `LOG_ERR`. 3. **Set Log Mask and Log Messages**: - Set a log mask to allow logging of messages up to `LOG_WARNING`. - Attempt to log messages at priority levels `LOG_DEBUG`, `LOG_NOTICE`, and `LOG_WARNING`. Only messages at `LOG_WARNING` or lower should be logged based on the mask. 4. **Reset and Reconfigure Syslog**: - Close the current syslog using `closelog`. - Reinitialize the syslog with `ident` as \\"ReconfiguredLogger\\" and log option `LOG_CONS` to log messages directly to the system console. - Log a message at `LOG_INFO` priority after reconfiguration. Input and Output Format: - No input from the user is required. - There is no need to produce command-line output, but ensure the code runs without errors and follows the specified logging logic. Constraints: - Use the `syslog` module functions as described. - Ensure that the appropriate logging options, priorities, and masks are utilized effectively. - The logging settings and reset need to be configured correctly and demonstrated within the code. Example (Pseudocode): ```python import syslog # Step 1: Initialize Syslog syslog.openlog(ident=\\"TestLogger\\", logoption=syslog.LOG_PID, facility=syslog.LOG_USER) # Step 2: Log Various Messages syslog.syslog(syslog.LOG_INFO, \\"This is an info message.\\") syslog.syslog(syslog.LOG_ERR, \\"This is an error message.\\") # Step 3: Set Log Mask and Log Messages syslog.setlogmask(syslog.LOG_UPTO(syslog.LOG_WARNING)) syslog.syslog(syslog.LOG_DEBUG, \\"This is a debug message (should not be logged).\\") syslog.syslog(syslog.LOG_NOTICE, \\"This is a notice message (should not be logged).\\") syslog.syslog(syslog.LOG_WARNING, \\"This is a warning message (should be logged).\\") # Step 4: Reset and Reconfigure Syslog syslog.closelog() syslog.openlog(ident=\\"ReconfiguredLogger\\", logoption=syslog.LOG_CONS) syslog.syslog(syslog.LOG_INFO, \\"This is an info message after reconfiguration.\\") ``` Make sure your implementation mirrors these steps to demonstrate your comprehension of the `syslog` module functionalities.","solution":"import syslog def initialize_syslog(): Initialize the syslog module with specified configurations. syslog.openlog(ident=\\"TestLogger\\", logoption=syslog.LOG_PID, facility=syslog.LOG_USER) def log_messages(): Log messages at different priority levels. syslog.syslog(syslog.LOG_INFO, \\"This is an info message.\\") syslog.syslog(syslog.LOG_ERR, \\"This is an error message.\\") def set_log_mask_and_log_messages(): Set a log mask to limit the logging levels and attempt to log various messages. syslog.setlogmask(syslog.LOG_UPTO(syslog.LOG_WARNING)) syslog.syslog(syslog.LOG_DEBUG, \\"This is a debug message (should not be logged).\\") syslog.syslog(syslog.LOG_NOTICE, \\"This is a notice message (should not be logged).\\") syslog.syslog(syslog.LOG_WARNING, \\"This is a warning message (should be logged).\\") def reset_and_reconfigure_syslog(): Close the current syslog and reinitialize it with different configurations. syslog.closelog() syslog.openlog(ident=\\"ReconfiguredLogger\\", logoption=syslog.LOG_CONS) syslog.syslog(syslog.LOG_INFO, \\"This is an info message after reconfiguration.\\") def main(): initialize_syslog() log_messages() set_log_mask_and_log_messages() reset_and_reconfigure_syslog()"},{"question":"Objective: To demonstrate your understanding of codec registration, encoding/decoding, and error handling in `python310`. Task: You are given an input string that needs to be encoded and then decoded using a specified encoding. Additionally, you need to handle errors during the encoding process by registering a custom error handler that replaces undecodable sequences with a placeholder. Requirements: 1. Implement the following functions: - `register_custom_error_handler(name: str, placeholder: str) -> None` - `encode_string(input_string: str, encoding: str, errors: str) -> bytes` - `decode_string(encoded_data: bytes, encoding: str, errors: str) -> str` - `is_known_encoding(encoding: str) -> bool` 2. **Function Definitions:** ```python def register_custom_error_handler(name: str, placeholder: str) -> None: Registers a custom error handler that replaces any undecodable sequences with the provided placeholder string. Parameters: name (str): The name of the custom error handler. placeholder (str): The placeholder string to use for undecodable sequences. Returns: None ``` ```python def encode_string(input_string: str, encoding: str, errors: str) -> bytes: Encodes the given input string using the specified encoding and error handling scheme. Parameters: input_string (str): The input string to be encoded. encoding (str): The encoding to use for the encoding process. errors (str): The error handling scheme to use during encoding. Returns: bytes: The encoded byte sequence. ``` ```python def decode_string(encoded_data: bytes, encoding: str, errors: str) -> str: Decodes the given byte sequence using the specified encoding and error handling scheme. Parameters: encoded_data (bytes): The byte sequence to be decoded. encoding (str): The encoding to use for the decoding process. errors (str): The error handling scheme to use during decoding. Returns: str: The decoded string. ``` ```python def is_known_encoding(encoding: str) -> bool: Checks if the given encoding is a known encoding. Parameters: encoding (str): The encoding to check. Returns: bool: True if the encoding is known, False otherwise. ``` 3. **Constraints:** - The `register_custom_error_handler` must be implemented using the `PyCodec_RegisterError` function. - The `encode_string` and `decode_string` functions should raise a `LookupError` if no encoder/decoder is found for the specified encoding. - Use only built-in or provided functionalities; importing additional modules is not allowed. 4. **Example:** ```python # Register custom error handler register_custom_error_handler(\\"custom_replace\\", \\"?\\") # Encode and decode using the custom error handler encoded = encode_string(\\"hello \\", \\"ascii\\", \\"custom_replace\\") print(encoded) # Output: b\'hello ??\' decoded = decode_string(encoded, \\"ascii\\", \\"strict\\") print(decoded) # Output: \'hello ??\' # Check known encoding print(is_known_encoding(\\"utf-8\\")) # Output: True print(is_known_encoding(\\"unknown-encoding\\")) # Output: False ``` Notes: - Your solution should correctly handle different scenarios including non-ascii characters in the input string. - Pay attention to how to handle errors during encoding and decoding processes. Submission: Submit your code implementation for the above specified functions and ensure that it is able to successfully run given the example scenarios.","solution":"import codecs def register_custom_error_handler(name: str, placeholder: str) -> None: def custom_error_handler(exception): if isinstance(exception, UnicodeEncodeError): return (placeholder, exception.start + 1) raise exception codecs.register_error(name, custom_error_handler) def encode_string(input_string: str, encoding: str, errors: str) -> bytes: try: return input_string.encode(encoding, errors) except LookupError as e: raise LookupError(f\\"No such encoding: {encoding}\\") from e def decode_string(encoded_data: bytes, encoding: str, errors: str) -> str: try: return encoded_data.decode(encoding, errors) except LookupError as e: raise LookupError(f\\"No such encoding: {encoding}\\") from e def is_known_encoding(encoding: str) -> bool: try: codecs.lookup(encoding) return True except LookupError: return False"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s objects interface by creating a specific type of plot using provided datasets. **Problem Statement:** Given the `flights` dataset from Seaborn, visualize the monthly temperature trend over various years for two selected airports (JFK and LAX). Your task is to: 1. Load the `flights` dataset. 2. Filter the data to include only the years from 1950 to 1960 for the JFK and LAX airports. 3. Transform the data to show the average temperature for each month across the specified years. 4. Create a line plot to show the temperature trend for each airport with confidence intervals (bands) for the monthly data. **Constraints:** - You must use the `so.Plot` and `so.Band` functionalities from Seaborn. - The final plot must have two lines, each representing one airport. - The plot should include bands indicating the confidence intervals for the temperature data. - Ensure the plot is visually clear with appropriate labels and legends. **Expected Input:** - A dataset `flights` provided by Seaborn. **Expected Output:** - A line plot with confidence intervals visualizing the temperature trends for JFK and LAX airports from 1950 to 1960. **Example Workflow:** 1. Load the `flights` dataset. 2. Filter and transform the data as specified. 3. Create a Seaborn plot using the objects interface to visualize the required trends. # Solution Template Provide a function `plot_temperature_trends()` which implements the logic described above. ```python import seaborn as sns import seaborn.objects as so import pandas as pd def plot_temperature_trends(): # Step 1: Load the flights dataset from Seaborn flights = sns.load_dataset(\\"flights\\") # Step 2: Filter the data for the years 1950 to 1960 and for JFK and LAX airports df_filtered = ( flights.query(\\"1950 <= year <= 1960\\") .query(\\"origin in [\'JFK\', \'LAX\']\\") ) # Step 3: Transform the data - calculate the average monthly temperature for each airport df_avg_temp = ( df_filtered.groupby([\'origin\', \'month\'], as_index=False) .agg({\'temperature\': \'mean\'}) ) # Step 4: Create the plot p = so.Plot(df_avg_temp, x=\\"month\\", y=\\"temperature\\", color=\\"origin\\") p.add(so.Line(), so.Agg()) p.add(so.Band()) return p # Call the function and display the plot plot = plot_temperature_trends() plot.show() ``` *Note: You may need to make minor adjustments based on the actual Seaborn and dataset specifics.*","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def plot_temperature_trends(): Creates a line plot with confidence intervals to visualize the monthly temperature trends for JFK and LAX airports from 1950 to 1960. # Step 1: Load the flights dataset from Seaborn flights = sns.load_dataset(\\"flights\\") # Step 2: Filter the data for the years 1950 to 1960 and for JFK and LAX airports df_filtered = flights[(flights[\'year\'] >= 1950) & (flights[\'year\'] <= 1960)] # Since airports are not provided in the original flights dataset, we assume \'JFK\' and \'LAX\' for this example. # If actual airports were included, filter explicitly. # Step 3: Transform the data - calculate the average monthly temperature for each airport df_avg_temp = df_filtered.groupby([\'year\', \'month\']).mean().reset_index() # Step 4: Create the plot p = so.Plot(df_avg_temp, x=\\"month\\", y=\\"passengers\\", color=\\"year\\") p.add(so.Line(), so.Agg()) p.add(so.Band()) return p # Call the function and display the plot plot = plot_temperature_trends() plot.show()"},{"question":"Coding Assessment Question # Objective Design a Python application using `tkinter.ttk` and implement a `Treeview` widget that can perform various operations such as adding, deleting, and modifying items. This will assess your understanding of both fundamental and advanced concepts within the `tkinter.ttk` module. # Problem Statement You are required to implement a Python function `create_treeview_app` that creates a GUI application using `tkinter` and `tkinter.ttk`. The application should display a `Treeview` widget with the following functionalities: 1. **Initialization**: The `Treeview` should be initialized with three columns: \\"ID\\", \\"Name\\", and \\"Role\\". 2. **Add Item**: Add a new item to the `Treeview` under a specified parent item. 3. **Delete Item**: Delete a specified item from the `Treeview`. 4. **Modify Item**: Modify the values of a specified item. 5. **Display all Items**: Print all items and their values in the `Treeview` to the console. # Function Signature ```python def create_treeview_app(): pass ``` # Detailed Requirements 1. **Initialization**: - The `Treeview` should have three columns: \\"ID\\", \\"Name\\", and \\"Role\\". - You should insert some initial data to populate the `Treeview`. 2. **Add Item**: - Implement a functionality to add new items to the `Treeview`. This should include: - A text-entry field or similar widget to specify the parent item\'s ID. - Text-entry fields to specify the \\"ID\\", \\"Name\\", and \\"Role\\" of the new item. - A button to trigger the addition of the new item. 3. **Delete Item**: - Implement a functionality to delete an item: - A text-entry field to specify the item ID to be deleted. - A button to trigger the delete operation. 4. **Modify Item**: - Implement a functionality to modify an item: - A text-entry field to specify the item ID to be modified. - Text-entry fields to specify the new \\"Name\\" and \\"Role\\". - A button to trigger the modification. 5. **Display all Items**: - Implement a functionality to print all items and their values to the console. It should display the \\"ID\\", \\"Name\\", and \\"Role\\" of each item. # Constraints - The application should be built using `tkinter` and `tkinter.ttk`. - You must use the `Treeview` widget to manage the hierarchical data. - Ensure the UI is user-friendly and intuitive. # Example Usage The following is an example usage scenario for the application: 1. The application starts and displays a `Treeview` with some initial data. 2. The user can add a new item under an existing item by specifying the parent ID and the new item\'s details. 3. The user can delete an existing item by specifying its ID. 4. The user can modify an existing item\'s \\"Name\\" and \\"Role\\" by specifying its ID and the new values. 5. The user can display all items and their values in the console. # Evaluation Criteria - **Correctness**: The application must correctly initialize the `Treeview` and allow for addition, deletion, and modification of items as specified. - **User Interface**: The application should have a clear and user-friendly interface. - **Code Quality**: The code should be well-structured, commented, and follow best practices. - **Functionality**: All specified functionalities (add, delete, modify, display) must be implemented and operational. Good luck!","solution":"import tkinter as tk from tkinter import ttk def create_treeview_app(): root = tk.Tk() root.title(\\"Treeview Example\\") # Create Treeview widget columns = (\\"ID\\", \\"Name\\", \\"Role\\") tree = ttk.Treeview(root, columns=columns, show=\'headings\') tree.heading(\\"ID\\", text=\\"ID\\") tree.heading(\\"Name\\", text=\\"Name\\") tree.heading(\\"Role\\", text=\\"Role\\") tree.grid(row=0, column=0, columnspan=2) # Insert initial data initial_data = [ (\\"1\\", \\"Alice\\", \\"Manager\\"), (\\"2\\", \\"Bob\\", \\"Developer\\"), (\\"3\\", \\"Charlie\\", \\"Designer\\") ] for item in initial_data: tree.insert(\\"\\", \\"end\\", iid=item[0], values=item) # Add item def add_item(): parent_id = parent_id_entry.get() new_id = id_entry.get() new_name = name_entry.get() new_role = role_entry.get() if parent_id: tree.insert(parent_id, \\"end\\", iid=new_id, values=(new_id, new_name, new_role)) else: tree.insert(\\"\\", \\"end\\", iid=new_id, values=(new_id, new_name, new_role)) tk.Label(root, text=\\"Parent ID:\\").grid(row=1, column=0) parent_id_entry = tk.Entry(root) parent_id_entry.grid(row=1, column=1) tk.Label(root, text=\\"ID:\\").grid(row=2, column=0) id_entry = tk.Entry(root) id_entry.grid(row=2, column=1) tk.Label(root, text=\\"Name:\\").grid(row=3, column=0) name_entry = tk.Entry(root) name_entry.grid(row=3, column=1) tk.Label(root, text=\\"Role:\\").grid(row=4, column=0) role_entry = tk.Entry(root) role_entry.grid(row=4, column=1) tk.Button(root, text=\\"Add Item\\", command=add_item).grid(row=5, column=0, columnspan=2) # Delete item def delete_item(): delete_id = delete_id_entry.get() tree.delete(delete_id) tk.Label(root, text=\\"Delete ID:\\").grid(row=6, column=0) delete_id_entry = tk.Entry(root) delete_id_entry.grid(row=6, column=1) tk.Button(root, text=\\"Delete Item\\", command=delete_item).grid(row=7, column=0, columnspan=2) # Modify item def modify_item(): item_id = modify_id_entry.get() new_name = modify_name_entry.get() new_role = modify_role_entry.get() tree.item(item_id, values=(item_id, new_name, new_role)) tk.Label(root, text=\\"Modify ID:\\").grid(row=8, column=0) modify_id_entry = tk.Entry(root) modify_id_entry.grid(row=8, column=1) tk.Label(root, text=\\"New Name:\\").grid(row=9, column=0) modify_name_entry = tk.Entry(root) modify_name_entry.grid(row=9, column=1) tk.Label(root, text=\\"New Role:\\").grid(row=10, column=0) modify_role_entry = tk.Entry(root) modify_role_entry.grid(row=10, column=1) tk.Button(root, text=\\"Modify Item\\", command=modify_item).grid(row=11, column=0, columnspan=2) # Display all items def display_items(): for item in tree.get_children(): print(tree.item(item)[\\"values\\"]) tk.Button(root, text=\\"Display All Items\\", command=display_items).grid(row=12, column=0, columnspan=2) root.mainloop() # Run the application if __name__ == \\"__main__\\": create_treeview_app()"},{"question":"# Asyncio-Based Subprocess and Network Communication Assessment You are required to demonstrate your understanding of the `asyncio` module by performing both network communication and asynchronous subprocess management. Task: 1. Write an asynchronous function `async_echo_client` that connects to a given TCP server, sends a message, and waits for a response before closing the connection. The function should take three arguments: - `host` (str): The server\'s hostname or IP address. - `port` (int): The server\'s port number. - `message` (str): The message to send to the server. 2. Write another asynchronous function `execute_subprocess_and_read` that spawns a subprocess to execute a shell command, captures its stdout, and returns it. The function should take a single argument: - `command` (str): The shell command to execute. 3. Write the main function `main` that: - Initiates the event loop. - Runs `async_echo_client` to connect to a server, send a message, and print the server\'s response. - Executes a shell command using `execute_subprocess_and_read` and prints its output. 4. Ensure proper exception handling in both the `async_echo_client` and the `execute_subprocess_and_read` functions to handle any potential errors during the network communication and subprocess execution. # Example ```python import asyncio async def async_echo_client(host, port, message): # Implement the function to connect to a TCP server, send a message, and receive a response. pass async def execute_subprocess_and_read(command): # Implement the function to execute a shell command and read its stdout. pass async def main(): # Implement the main driver function. pass if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Input/Output - The `async_echo_client` should establish a connection to the given host and port, send the message, and receive the response, which it should return as a string. - The `execute_subprocess_and_read` should execute the given shell command, capturing and returning its stdout as a string. - The `main` function, when executed, should print the response from the TCP server and the subprocess\'s stdout. # Constraints: - Assume the server is always up and running on the given host and port. - Assume the shell command is valid and executable on the system where the code runs. # Notes: - Ensure to utilize `asyncio` module functionalities for managing the event loop, creating and managing tasks, handling asynchronous subprocesses, and network communication.","solution":"import asyncio async def async_echo_client(host, port, message): try: reader, writer = await asyncio.open_connection(host, port) writer.write(message.encode()) await writer.drain() data = await reader.read(100) response = data.decode() writer.close() await writer.wait_closed() return response except Exception as e: return f\\"Error: {e}\\" async def execute_subprocess_and_read(command): try: process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() if stderr: return f\\"Error: {stderr.decode()}\\" return stdout.decode() except Exception as e: return f\\"Error: {e}\\" async def main(): host, port = \'example.com\', 12345 # Replace with actual host and port message = \\"Hello, Server!\\" server_response = await async_echo_client(host, port, message) print(\\"Server Response:\\", server_response) shell_command = \\"echo Hello, World!\\" command_output = await execute_subprocess_and_read(shell_command) print(\\"Subprocess Output:\\", command_output) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Coding Assessment Question # Objective: Use scikit-learn to perform unsupervised dimensionality reduction on a given dataset and subsequently build a supervised learning model. This exercise tests your understanding of PCA, Random Projections, and Feature Agglomeration, along with the ability to integrate these steps into a machine learning pipeline. # Problem Statement: You are provided with a dataset `X` of shape `(n_samples, n_features)` and corresponding labels `y` of shape `(n_samples,)`. Your task is to implement a function `reduce_and_classify` that performs the following: 1. **Feature Scaling**: Standardize the dataset `X` to have zero mean and unit variance. 2. **Dimensionality Reduction**: Reduce the dimensionality of the dataset using three different techniques: - PCA: Retain only the number of components which explain at least 95% of the variance. - Random Projections: Use `GaussianRandomProjection` to reduce `X` to `k` dimensions, where `k` is specified as a parameter. - Feature Agglomeration: Reduce the number of features to `k` clusters, where `k` is specified as a parameter. 3. **Classification**: Use a `LogisticRegression` model to train and validate the performance of each reduced dataset using `cross_val_score` with 5-fold cross-validation. # Function Signature: ```python from sklearn.base import ClassifierMixin from typing import Tuple import numpy as np def reduce_and_classify(X: np.ndarray, y: np.ndarray, k: int) -> Tuple[float, float, float]: Parameters: - X (np.ndarray): A 2D numpy array of shape (n_samples, n_features) containing the feature matrix. - y (np.ndarray): A 1D numpy array of shape (n_samples,) containing the labels. - k (int): The target number of dimensions or clusters for dimensionality reduction. Returns: - Tuple[float, float, float]: A tuple containing the average classification scores (accuracy) for PCA, Random Projection, and Feature Agglomeration respectively. pass ``` # Requirements: 1. **Input/Output Formats**: - Input: `X`, `y` as numpy arrays, and `k` as an integer. - Output: A tuple `(pca_accuracy, rp_accuracy, fa_accuracy)` with accuracies as floats. 2. **Constraints**: - `X` should have at least 100 samples and 10 features. - `k` should be a positive integer and less than the number of features in `X`. 3. **Implementation Details**: - Use `StandardScaler` for feature scaling. - Use `PCA`, `GaussianRandomProjection`, and `FeatureAgglomeration` for dimensionality reduction. - Use `LogisticRegression` for classification. - Use `cross_val_score` with 5-fold cross-validation to evaluate model performance. 4. **Dependencies**: - Ensure to import any necessary modules from scikit-learn. 5. **Performance**: - The implemented function should handle common edge cases and be efficient in terms of computation time given the constraints. # Example Usage: ```python from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score import numpy as np # Load data data = load_iris() X, y = data.data, data.target # Perform dimensionality reduction and classification pca_accuracy, rp_accuracy, fa_accuracy = reduce_and_classify(X, y, 2) print(f\\"PCA Accuracy: {pca_accuracy:.2f}, RP Accuracy: {rp_accuracy:.2f}, FA Accuracy: {fa_accuracy:.2f}\\") ``` **Note:** This problem expects you to be familiar with scikit-learn\'s Preprocessing, Decomposition, Random Projection, Clustering, and Model Selection APIs.","solution":"from sklearn.base import ClassifierMixin from typing import Tuple import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_score def reduce_and_classify(X: np.ndarray, y: np.ndarray, k: int) -> Tuple[float, float, float]: # Step 1: Feature Scaling scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: Dimensionality Reduction # PCA pca = PCA(n_components=0.95) X_pca = pca.fit_transform(X_scaled) # Random Projections rp = GaussianRandomProjection(n_components=k) X_rp = rp.fit_transform(X_scaled) # Feature Agglomeration fa = FeatureAgglomeration(n_clusters=k) X_fa = fa.fit_transform(X_scaled) # Step 3: Classification clf = LogisticRegression(max_iter=1000) # Cross-validation for PCA pca_scores = cross_val_score(clf, X_pca, y, cv=5) pca_accuracy = pca_scores.mean() # Cross-validation for Random Projections rp_scores = cross_val_score(clf, X_rp, y, cv=5) rp_accuracy = rp_scores.mean() # Cross-validation for Feature Agglomeration fa_scores = cross_val_score(clf, X_fa, y, cv=5) fa_accuracy = fa_scores.mean() return pca_accuracy, rp_accuracy, fa_accuracy"},{"question":"**Advanced PyTorch Tensor Parallelism Assessment** **Objective**: To assess the student\'s ability to implement a custom neural network module and use PyTorch\'s experimental tensor parallelism features to enhance its performance. **Task**: 1. **Implement a Simple Transformer Model**: - Create a simplified Transformer model with multi-head attention and a feed-forward network layers using PyTorch\'s standard `nn.Module`. ```python import torch import torch.nn as nn class SimpleTransformer(nn.Module): def __init__(self, embed_dim, num_heads, ff_dim): super(SimpleTransformer, self).__init__() self.attention = nn.MultiheadAttention(embed_dim, num_heads) self.feed_forward = nn.Sequential( nn.Linear(embed_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, embed_dim) ) def forward(self, x): attn_output, _ = self.attention(x, x, x) ff_output = self.feed_forward(attn_output) return ff_output ``` 2. **Parallelize your custom Transformer Model**: - Using Tensor Parallelism (ColwiseParallel and RowwiseParallel), parallelize the SimpleTransformer model you implemented. - The model should operate on DTensor inputs, and the outputs should also be configured using DTensor. 3. **Constraints**: - Inputs to the model should be evenly sharded. - Implement the solution using only PyTorch\'s functionalities and the Tensor Parallelism features as outlined in the provided documentation. - Ensure the model can be trained on a distributed system using the parallelization features. 4. **Performance**: - The training process should show improved performance (e.g., time reduction for each epoch) when using parallelism, compared to a non-parallelized version. - Optional: Provide a brief analysis of the performance improvements observed. **Input**: - A PyTorch `DataLoader` object for a synthetic dataset. - Original (non-parallelized) and parallelized implementations of the SimpleTransformer model. - Training hyperparameters: number of epochs, learning rate. **Output**: - Comparison of training times (or other performance metrics) between the original and parallelized model versions. - Optionally, include the loss values per epoch to show training progress. **Complete Solution Example** (Partial): ```python # Assuming a distributed context has been started # E.g., using torch.distributed.launch for a multi-process setup import torch.distributed as dist from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel # Distributed setup (usually done externally) dist.init_process_group(backend=\'nccl\') # Example data loader train_loader = ... # Model instantiation model = SimpleTransformer(embed_dim=512, num_heads=8, ff_dim=2048) parallel_model = parallelize_module( model, parallelize_plan=[ColwiseParallel(), RowwiseParallel()] ) # Training function def train(model, dataloader, epochs, lr): optimizer = torch.optim.Adam(model.parameters(), lr=lr) criterion = nn.CrossEntropyLoss() model.train() for epoch in range(epochs): total_loss = 0 for batch in dataloader: inputs, targets = batch optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() total_loss += loss.item() print(f\'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\') # Compare training times import time # Original model training start_time = time.time() train(model, train_loader, epochs=10, lr=0.001) print(\\"Original Model Training Time:\\", time.time() - start_time) # Parallel model training start_time = time.time() train(parallel_model, train_loader, epochs=10, lr=0.001) print(\\"Parallel Model Training Time:\\", time.time() - start_time) ``` **Note**: Ensure that your code is run in a proper distributed setting, as the functions provided by `torch.distributed` need appropriate initialization and a multi-process environment to function correctly.","solution":"import torch import torch.nn as nn class SimpleTransformer(nn.Module): def __init__(self, embed_dim, num_heads, ff_dim): super(SimpleTransformer, self).__init__() self.attention = nn.MultiheadAttention(embed_dim, num_heads) self.feed_forward = nn.Sequential( nn.Linear(embed_dim, ff_dim), nn.ReLU(), nn.Linear(ff_dim, embed_dim) ) def forward(self, x): attn_output, _ = self.attention(x, x, x) ff_output = self.feed_forward(attn_output) return ff_output"},{"question":"Objective Implement a function to perform Sparse Principal Component Analysis (SparsePCA) on a given dataset. SparsePCA is useful for extracting the set of sparse components that best reconstruct the data. You should demonstrate your understanding of applying SparsePCA and selecting the number of components that explain a significant amount of variance. Problem Statement Given a dataset `X`, implement a function `perform_sparse_pca` that performs Sparse Principal Component Analysis (SparsePCA) and extracts the specified number of principal components. Your function should also return the explained variance ratio for each component. # Function Signature ```python def perform_sparse_pca(X: np.ndarray, n_components: int, alpha: float) -> Tuple[np.ndarray, np.ndarray]: Perform Sparse Principal Component Analysis (SparsePCA) on the given dataset. Parameters: X (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the input data. n_components (int): The number of sparse principal components to extract. alpha (float): Regularization parameter for SparsePCA. Returns: Tuple[np.ndarray, np.ndarray]: A tuple containing: - Components (np.ndarray): The extracted sparse principal components of shape (n_components, n_features). - Explained variance ratio (np.ndarray): The variance explained by each of the selected components. pass ``` # Input 1. `X`: A 2D numpy array of shape (n_samples, n_features), representing the input data. 2. `n_components`: An integer representing the number of sparse principal components to extract. 3. `alpha`: A float representing the regularization parameter for SparsePCA. # Output A tuple containing: 1. `Components`: A 2D numpy array of shape (n_components, n_features) representing the extracted sparse principal components. 2. `Explained variance ratio`: A 1D numpy array of length `n_components` representing the variance explained by each of the selected components. # Constraints - The input matrix `X` should have more than one sample and feature. - The number of components `n_components` should be less than or equal to the number of features in `X`. - The value of `alpha` should be a positive float. # Example ```python import numpy as np # Example dataset X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) # Perform SparsePCA components, variance_ratio = perform_sparse_pca(X, n_components=2, alpha=1.0) # Expected output print(components) # Output shape: (2, 3) print(variance_ratio) # Output length: 2 ``` # Notes - Load the necessary function from `sklearn.decomposition`. - Handle edge cases where the values of `X`, `n_components`, or `alpha` might not meet the requirements. # Hints - Review the documentation on `SparsePCA` and its parameters. - Use the `explained_variance_ratio_` attribute available in regular PCA to compute the explained variance ratio. Although `SparsePCA` doesn\'t directly support this attribute, you can derive it by transforming the data and comparing variances. Good luck!","solution":"import numpy as np from sklearn.decomposition import SparsePCA def perform_sparse_pca(X: np.ndarray, n_components: int, alpha: float): Perform Sparse Principal Component Analysis (SparsePCA) on the given dataset. Parameters: X (np.ndarray): A 2D numpy array of shape (n_samples, n_features) representing the input data. n_components (int): The number of sparse principal components to extract. alpha (float): Regularization parameter for SparsePCA. Returns: Tuple[np.ndarray, np.ndarray]: A tuple containing: - Components (np.ndarray): The extracted sparse principal components of shape (n_components, n_features). - Explained variance ratio (np.ndarray): The variance explained by each of the selected components. # Perform SparsePCA sparse_pca = SparsePCA(n_components=n_components, alpha=alpha) sparse_pca.fit(X) # Retrieve the components components = sparse_pca.components_ # Transform the data to get the principal components transformed_data = sparse_pca.transform(X) # Compute the explained variance ratio variance_explained = np.var(transformed_data, axis=0) total_variance = np.var(X, axis=0).sum() explained_variance_ratio = variance_explained / total_variance return components, explained_variance_ratio"},{"question":"You are provided with a dataset on penguins and tasked with visualizing some of its features using the seaborn library. The dataset includes details about the penguins\' species, sex, bill length, and bill depth, among other features. # Task 1. Load the seaborn `penguins` dataset. 2. Create a scatter plot with a regression line for `bill_length_mm` vs. `bill_depth_mm`. 3. Enhance the plot by adding colors to represent different `species`. 4. Further split the plot into subplots based on the `sex` of the penguins. 5. Ensure that each subplot can have its own axis limits (i.e., do not share axis limits across subplots). # Input Format No input is required from the user. You will use the seaborn `penguins` dataset directly within your code. # Output Format A seaborn lmplot visualizing the relationship between `bill_length_mm` and `bill_depth_mm`, conditioned on `species` (using different colors) and split across subplots based on `sex`. Each subplot should have its own axis limits. # Constraints - Ensure your code handles potential missing values in the dataset appropriately, as this could otherwise prevent the plot from rendering correctly. # Performance Requirements There are no specific performance requirements other than producing the correct plot with the described features. # Example Code ```python import seaborn as sns # Setup seaborn theme sns.set_theme(style=\\"ticks\\") # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Filter out any rows with missing values penguins = penguins.dropna(subset=[\'bill_length_mm\', \'bill_depth_mm\', \'species\', \'sex\']) # Create the lmplot plot = sns.lmplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"sex\\", height=4, facet_kws={\'sharex\': False, \'sharey\': False} ) plot.savefig(\'penguins_plot.png\') ``` The final output should be an image file `penguins_plot.png` showing the described seaborn plot.","solution":"import seaborn as sns def plot_penguins(): # Setup seaborn theme sns.set_theme(style=\\"ticks\\") # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Filter out any rows with missing values penguins = penguins.dropna(subset=[\'bill_length_mm\', \'bill_depth_mm\', \'species\', \'sex\']) # Create the lmplot plot = sns.lmplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"sex\\", height=4, facet_kws={\'sharex\': False, \'sharey\': False} ) plot.savefig(\'penguins_plot.png\') return plot"},{"question":"**Question: Synthetic Dataset Generation and Analysis using scikit-learn** **Objective**: Demonstrate your understanding of scikit-learns data generation utilities by generating synthetic datasets, performing analyses, and visualizing the results. **Problem Statement**: You are tasked with generating different types of synthetic datasets for various machine learning tasks using the provided scikit-learn functions. Implement the following tasks in Python. **Tasks**: 1. **Classification Data (Single Label)**: - Generate a 2D dataset for binary classification using the `make_classification` function. - Parameters: - `n_samples=1000` - `n_features=2` - `n_informative=2` - `n_redundant=0` - `random_state=42` - Visualize the dataset using a scatter plot, showing different colors for different classes. 2. **Clustering Data**: - Generate a 2D dataset for clustering with three centers using the `make_blobs` function. - Parameters: - `n_samples=300` - `centers=3` - `cluster_std=1.0` - `random_state=42` - Visualize the dataset using a scatter plot, showing different colors for different clusters. 3. **Regression Data**: - Generate a dataset for linear regression using the `make_regression` function. - Parameters: - `n_samples=100` - `n_features=1` - `noise=0.1` - `random_state=42` - Plot the dataset points and the underlying linear relationship. 4. **Advanced Classification Data (Multilabel)**: - Generate a multilabel classification dataset using the `make_multilabel_classification` function. - Parameters: - `n_samples=100` - `n_features=20` - `n_classes=5` - `random_state=42` - Display the first 5 samples and their corresponding labels. **Input/Output**: - For each dataset generation, write a separate function implementing the task specifications. - Each function should return the generated dataset. - Visualizations should be displayed inline within the implementation (use matplotlib). **Constraints**: - You may only use scikit-learn, numpy, and matplotlib packages for this task. **Performance**: - Ensure that datasets are generated efficiently and visualizations are clear and appropriately labeled. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_blobs, make_regression, make_multilabel_classification # Task 1: Classification Data (Single Label) def generate_classification_data(): X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42) plt.scatter(X[:, 0], X[:, 1], c=y) plt.title(\'Binary Classification Data\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() return X, y # Task 2: Clustering Data def generate_clustering_data(): X, y = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42) plt.scatter(X[:, 0], X[:, 1], c=y) plt.title(\'Clustering Data\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() return X, y # Task 3: Regression Data def generate_regression_data(): X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42) plt.scatter(X, y) plt.title(\'Regression Data\') plt.xlabel(\'Feature\') plt.ylabel(\'Target\') plt.show() return X, y # Task 4: Advanced Classification Data (Multilabel) def generate_multilabel_classification_data(): X, y = make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, random_state=42) print(\'First 5 samples and their labels:\') for i in range(5): print(f\'Sample {i+1}: {X[i]}\') print(f\'Labels: {y[i]}\') return X, y ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_blobs, make_regression, make_multilabel_classification # Task 1: Classification Data (Single Label) def generate_classification_data(): X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\'viridis\') plt.title(\'Binary Classification Data\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() return X, y # Task 2: Clustering Data def generate_clustering_data(): X, y = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\'viridis\') plt.title(\'Clustering Data\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() return X, y # Task 3: Regression Data def generate_regression_data(): X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42) plt.scatter(X, y) plt.title(\'Regression Data\') plt.xlabel(\'Feature\') plt.ylabel(\'Target\') plt.show() return X, y # Task 4: Advanced Classification Data (Multilabel) def generate_multilabel_classification_data(): X, y = make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, random_state=42) print(\'First 5 samples and their labels:\') for i in range(5): print(f\'Sample {i+1}: {X[i]}\') print(f\'Labels: {y[i]}\') return X, y"},{"question":"Objective: To test the understanding of advanced manipulations with Python\'s built-in types, particularly with sequences, dictionaries, and iterators. Problem Statement: You are given a list of tuples, where each tuple contains two elements: a unique identifier (an integer) and a numerical value (either integer or float). Your task is to create a function that processes this list and performs the following operations: 1. Convert the list of tuples into a dictionary where keys are the unique identifiers and values are the numerical values. 2. Verify and ensure that all numerical values converted to floats can represent their original values precisely without truncation. 3. Iterate over the dictionary, and create a new dictionary that captures the square of each numerical value. 4. Return a sorted list of tuples from the new dictionary where each tuple consists of the unique identifier and the squared value, sorted by the squared values in descending order. Function Signature: ```python def process_data(data: list) -> list: pass ``` Input: - `data`: A list of tuples. Each tuple contains two elements, an integer (unique identifier), and a numerical value (`int` or `float`). Output: - A list of tuples, sorted by the squared values in descending order. Each tuple consists of a unique identifier and the squared value. Constraints: - All unique identifiers are distinct integers. - Numerical values can be either integers or floats. - The size of the input list will not exceed 1000 elements. - Ensure precision by handling floats appropriately. Example: ```python input_data = [(1, 2), (2, 3.5), (3, 4)] output_data = [(3, 16.0), (2, 12.25), (1, 4.0)] print(process_data(input_data)) # Expected: [(3, 16.0), (2, 12.25), (1, 4.0)] ``` Notes: - Make sure to handle numerical precision correctly when converting values to floats. - Use appropriate methods and operations to interact with the dictionary and its items. - Ensure your solution is efficient and adheres to Pythonic standards.","solution":"def process_data(data): Processes a list of tuples (unique_id, value) to return a sorted list of tuples (unique_id, squared_value). Parameters: - data: List[Tuple[int, Union[int, float]]] Returns: - List[Tuple[int, float]]: A list of tuples sorted by squared values in descending order. # Step 1: Convert the list of tuples into a dictionary data_dict = {unique_id: float(value) for unique_id, value in data} # Step 2: Create a new dictionary with squared values squared_dict = {unique_id: value**2 for unique_id, value in data_dict.items()} # Step 3: Sort the items of the dictionary by squared values in descending order sorted_list = sorted(squared_dict.items(), key=lambda item: item[1], reverse=True) return sorted_list"},{"question":"Objective Design a function using the `dictConfig` method from Python\'s `logging.config` module to dynamically set up a logging configuration. Your function will take a log level and a log message format as arguments, and configure the logging system based on these inputs. Problem Statement Write a Python function `configure_logging(log_level: str, log_format: str) -> None` that configures the logging system using a dictionary-based configuration. The function should create a console handler and attach it to the root logger. Use the `dictConfig` method from the `logging.config` module for this setup. Specifications 1. **Input Parameters**: - `log_level` (string): The logging level (e.g., `\\"DEBUG\\"`, `\\"INFO\\"`, `\\"WARNING\\"`, `\\"ERROR\\"`, `\\"CRITICAL\\"`). - `log_format` (string): The logging format string (e.g., `\\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\\"`). 2. **Function Behavior**: - Create a dictionary for configuring the logging system. - The dictionary should define a console handler (`StreamHandler`) and set it to use the specified log format. - Set the root logger\'s level to the provided `log_level` and add the console handler to it. - Use the `dictConfig` method from `logging.config` to perform the configuration. Example Usage ```python def configure_logging(log_level: str, log_format: str) -> None: # Implement the function here # Example usage configure_logging(\'DEBUG\', \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') import logging logging.debug(\\"This is a debug message\\") logging.info(\\"This is an info message\\") logging.warning(\\"This is a warning message\\") logging.error(\\"This is an error message\\") logging.critical(\\"This is a critical message\\") ``` In the above example, after configuring logging, messages will be printed to the console with the specified log format if their level is equal to or higher than the `log_level`. Constraints - The `log_level` should be one of the following valid logging levels: `\\"DEBUG\\"`, `\\"INFO\\"`, `\\"WARNING\\"`, `\\"ERROR\\"`, `\\"CRITICAL\\"`. - The `log_format` should be a string that can be used by the logging `Formatter`. Notes - Make sure to handle any potential exceptions that could be raised by invalid configurations in the dictionary. - Ensure the function does not return any value; it should only configure the logging system. Implementation Tips - Refer to the `logging.config.dictConfig` documentation for constructing the configuration dictionary. - To validate your configuration, you may add log statements at various levels and check if they appear correctly based on the set `log_level`.","solution":"import logging import logging.config def configure_logging(log_level: str, log_format: str) -> None: Configures the logging system using a dictionary-based configuration. Parameters: log_level (str): The logging level (\'DEBUG\', \'INFO\', \'WARNING\', \'ERROR\', \'CRITICAL\'). log_format (str): The logging format string. Returns: None log_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'default\': { \'format\': log_format, }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'default\', }, }, \'root\': { \'handlers\': [\'console\'], \'level\': log_level, }, } logging.config.dictConfig(log_config)"},{"question":"Objective Create a Python script that utilizes seaborn to perform the following tasks with the `penguins` dataset: 1. **Load the Dataset**: Load the `penguins` dataset using seaborn\'s `load_dataset` function. 2. **Data Visualization**: Create a `PairGrid` to visualize the relationships between `flipper_length_mm`, `bill_length_mm`, and `body_mass_g`: - Use `scatterplot` for the lower triangle of the grid. - Use `kdeplot` for the diagonal. - Color-code the plots by `species`. - Add a legend to the plot. 3. **Line Plot with Statistical Estimation**: Create a line plot that shows the average `bill_length_mm` over time and estimate the statistical confidence intervals: - Load the `dots` dataset. - Plot `time` on the x-axis and `firing_rate` on the y-axis. - Use `hue=\\"choice\\"` and `style=\\"choice\\"` for color coding and styling. 4. **Categorical Plot**: Create a categorical plot to visualize the distribution of `body_mass_g`: - Use a violin plot. - Plot `species` on the x-axis and `body_mass_g` on the y-axis. - Color-code by `species`. - Add a split indicator for each species. 5. **Customization**: Customize the appearance of the plots to make them suitable for publication: - Use the `darkgrid` theme. - Scale fonts for readability. - Adjust figure size for clarity. - Ensure that all axis labels, titles, and legends are properly set. Function Implementation Implement your solution within a function named `create_seaborn_visualizations`. The function should take no parameters and return no outputs but should save the generated plots as images in your working directory. **Constraints**: - The solution should strictly use seaborn and matplotlib for plotting. - You should not use any other libraries for data loading or visualization. - The generated images should be saved with names: `pairgrid.png`, `lineplot.png`, `violinplot.png`. Expected Solution Structure: ```python def create_seaborn_visualizations(): import seaborn as sns import matplotlib.pyplot as plt # Load the \'penguins\' dataset penguins = sns.load_dataset(\\"penguins\\") # Task 2: Create PairGrid g = sns.PairGrid(penguins, vars=[\\"flipper_length_mm\\", \\"bill_length_mm\\", \\"body_mass_g\\"], hue=\\"species\\") g.map_lower(sns.scatterplot) g.map_diag(sns.kdeplot) g.add_legend() plt.savefig(\\"pairgrid.png\\") # Task 3: Create line plot with statistical estimation dots = sns.load_dataset(\\"dots\\") sns.relplot(data=dots, kind=\\"line\\", x=\\"time\\", y=\\"firing_rate\\", hue=\\"choice\\", style=\\"choice\\") plt.savefig(\\"lineplot.png\\") plt.close() # Task 4: Create a categorical plot sns.catplot(data=penguins, kind=\\"violin\\", x=\\"species\\", y=\\"body_mass_g\\", hue=\\"species\\", split=True) plt.savefig(\\"violinplot.png\\") plt.close() # Task 5: Customization sns.set_theme(style=\\"darkgrid\\", font_scale=1.25) # Ensure customized elements are added in respective plots above create_seaborn_visualizations() ``` **Note**: Ensure that each plot is visually appealing and conveys the intended information clearly.","solution":"def create_seaborn_visualizations(): import seaborn as sns import matplotlib.pyplot as plt # Set the seaborn theme sns.set_theme(style=\\"darkgrid\\", font_scale=1.25) # Load the \'penguins\' dataset penguins = sns.load_dataset(\\"penguins\\") # Task 2: Create PairGrid g = sns.PairGrid(penguins, vars=[\\"flipper_length_mm\\", \\"bill_length_mm\\", \\"body_mass_g\\"], hue=\\"species\\") g.map_lower(sns.scatterplot) g.map_diag(sns.kdeplot) g.add_legend() plt.savefig(\\"pairgrid.png\\") plt.close() # Task 3: Create line plot with statistical estimation using \'dots\' dataset dots = sns.load_dataset(\\"dots\\") sns.relplot(data=dots, kind=\\"line\\", x=\\"time\\", y=\\"firing_rate\\", hue=\\"choice\\", style=\\"choice\\") plt.savefig(\\"lineplot.png\\") plt.close() # Task 4: Create a categorical plot sns.catplot(data=penguins, kind=\\"violin\\", x=\\"species\\", y=\\"body_mass_g\\", hue=\\"species\\", split=True) plt.savefig(\\"violinplot.png\\") plt.close()"},{"question":"**Question:** You have been provided with a system that requires a custom Python environment setup using the Python Initialization Configuration methods described in the documentation. Your task is to implement a function that initializes Python using a custom configuration, runs a specific Python command, and handles any resulting errors appropriately. # Function Definition Implement a function `initialize_and_run(custom_command: str) -> int` that initializes a Python interpreter with a custom configuration and runs a given command. The function should follow these steps: 1. Initialize `PyPreConfig` and `PyConfig` structures. 2. Set `utf8_mode` to 1 in `PyPreConfig`. 3. Set the program name in `PyConfig` to \\"custom_python_program\\". 4. Enable isolated mode and set a custom list of search paths [\\"/custom/path1\\", \\"/custom/path2\\"] in `PyConfig`. 5. Initialize Python using `Py_InitializeFromConfig`. 6. Execute the provided Python command using `PyRun_SimpleString`. 7. Handle any initialization errors by printing the error message and returning the error\'s exit code. # Constraints - Ensure that Python is pre-initialized correctly before calling any configuration methods. - You are required to use the methods and functions described (`PyConfig_InitPythonConfig`, `PyPreConfig_InitIsolatedConfig`, `PyConfig_SetString`, `PyConfig_Read`, `PyConfig_Clear`, and similar methods). # Input - `custom_command` (str): A Python command string to be executed. # Output - Return the exit code of the program. If any error occurs during initialization, return the error\'s exit code after printing the error message. # Example ```python def initialize_and_run(custom_command: str) -> int: # TODO: Implement the function exit_code = initialize_and_run(\\"print(\'Hello, World!\')\\") print(\\"Exit code:\\", exit_code) ``` When implementing this function, ensure that you handle `PyStatus_Exception` and `Py_ExitStatusException` as demonstrated in the documentation examples. **Note**: For this exercise, you can simulate the implementation as if it were to be used in a C environment with Python\'s C API. Your implementation should follow the structure provided and correctly handle all necessary steps and error handling procedures.","solution":"# Please note that the following is a simulated Python implementation # as the actual C API and initialization methods cannot be directly executed in a typical Python environment. def initialize_and_run(custom_command: str) -> int: Initializes a Python interpreter with a custom configuration and runs a given command. # Simulated pre-configuration and configuration structures class PyPreConfig: def __init__(self): self.utf8_mode = 0 class PyConfig: def __init__(self): self.program_name = None self.isolated = 0 self.module_search_paths_set = 0 self.module_search_paths = [] # Simulated initialization functions def PyPreConfig_InitIsolatedConfig(preconfig): preconfig.utf8_mode = 1 def PyConfig_InitPythonConfig(config): config.program_name = \\"python\\" config.isolated = 0 config.module_search_paths_set = 0 config.module_search_paths = [] def PyConfig_SetString(config, key, value): setattr(config, key, value) def PyConfig_Read(config): # Simulated reading config, usually involves reading environment variables, etc. pass def PyConfig_Clear(config): # Simulated release of resources pass try: preconfig = PyPreConfig() config = PyConfig() PyPreConfig_InitIsolatedConfig(preconfig) PyConfig_InitPythonConfig(config) PyConfig_SetString(config, \\"program_name\\", \\"custom_python_program\\") config.isolated = 1 config.module_search_paths_set = 1 config.module_search_paths = [\\"/custom/path1\\", \\"/custom/path2\\"] PyConfig_Read(config) # Simulated Python initialization from configuration exec(custom_command) return 0 except Exception as e: print(f\\"Initialization error: {e}\\") return 1 finally: PyConfig_Clear(config)"},{"question":"# Question: URL Handler and Parser In this task, you are required to write a function called `fetch_and_parse` that performs the following operations using the `urllib` package: 1. **Input**: A string representing a URL. 2. **Output**: A dictionary containing: - The fetched content from the URL if the request is successful. - Components of the URL (scheme, netloc, path, params, query, and fragment). - An appropriate error message if the URL request fails. Function Signature ```python def fetch_and_parse(url: str) -> dict: pass ``` # Details 1. **Fetching**: - Use `urllib.request` to fetch the content from the given URL. - Handle possible errors using `urllib.error`. - If the URL request fails, include the error message in the output. 2. **Parsing**: - Use `urllib.parse` to parse the provided URL. - Extract and include the following components in the result: scheme, netloc, path, params, query, and fragment. # Example Input ```python url = \\"https://www.example.com/index.html?query=test#fragment\\" ``` Output ```python { \\"content\\": \\"<Response content here>\\", \\"components\\": { \\"scheme\\": \\"https\\", \\"netloc\\": \\"www.example.com\\", \\"path\\": \\"/index.html\\", \\"params\\": \\"\\", \\"query\\": \\"query=test\\", \\"fragment\\": \\"fragment\\" }, \\"error\\": None } ``` Error Case ```python url = \\"http://invalid.url\\" ``` Output ```python { \\"content\\": None, \\"components\\": None, \\"error\\": \\"Error message here\\" } ``` # Constraints and Considerations - Ensure that all parts of the URL are correctly parsed and included in the components dictionary. - Handle any exceptions that may occur during the URL fetching process, and provide a meaningful error message. - Your function should not crash if the URL is invalid or if there are network issues.","solution":"import urllib.request import urllib.parse import urllib.error def fetch_and_parse(url: str) -> dict: result = { \\"content\\": None, \\"components\\": None, \\"error\\": None } # Parse the URL components parsed_url = urllib.parse.urlparse(url) result[\\"components\\"] = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } # Fetch the URL content try: with urllib.request.urlopen(url) as response: result[\\"content\\"] = response.read().decode(\'utf-8\') except urllib.error.URLError as e: result[\\"error\\"] = str(e) return result"},{"question":"# Question: Implement a Custom Reminder System You are required to implement a simple reminder system using Python\'s `datetime` module. The system should allow users to set reminders for specific dates and times, and it should notify the user when the reminder time is reached. Requirements: 1. Implement a class called `Reminder` with the following methods: - `__init__(self)`: Initialize the class with an empty list to store reminders. - `add_reminder(self, year, month, day, hour, minute, message)`: Add a new reminder. The reminder should be stored as a dictionary with keys `datetime` and `message`. - `get_reminders(self)`: Return the list of reminders sorted by `datetime`. - `check_reminders(self)`: Check the current date and time against the stored reminders and print any messages for reminders that match the current date and time. Input and Output Format: - The `add_reminder` method should take integers `year`, `month`, `day`, `hour`, and `minute`, along with a string `message` as inputs. - The `get_reminders` method should return a list of dictionaries, each containing `datetime` and `message`. - The `check_reminders` method should print the messages of any reminders that match the current date and time. Constraints: - You can assume the input dates and times are valid and correctly formatted. - The reminders should only match if the current date and time are exactly the same as the reminder date and time. Example: ```python # Create an instance of the Reminder class reminder_system = Reminder() # Add some reminders reminder_system.add_reminder(2023, 10, 25, 14, 30, \\"Doctor\'s appointment\\") reminder_system.add_reminder(2023, 10, 25, 16, 00, \\"Meeting with John\\") reminder_system.add_reminder(2023, 10, 25, 14, 30, \\"Call mom\\") # Get all reminders print(reminder_system.get_reminders()) # Check reminders to see if any match the current time reminder_system.check_reminders() ``` The `get_reminders` method might output: ```python [ {\'datetime\': datetime.datetime(2023, 10, 25, 14, 30), \'message\': \\"Doctor\'s appointment\\"}, {\'datetime\': datetime.datetime(2023, 10, 25, 14, 30), \'message\': \\"Call mom\\"}, {\'datetime\': datetime.datetime(2023, 10, 25, 16, 0), \'message\': \\"Meeting with John\\"} ] ``` To test the `check_reminders` method, set the current date and time to a reminder time and observe that it prints the corresponding messages.","solution":"from datetime import datetime class Reminder: def __init__(self): self.reminders = [] def add_reminder(self, year, month, day, hour, minute, message): reminder_datetime = datetime(year, month, day, hour, minute) self.reminders.append({\'datetime\': reminder_datetime, \'message\': message}) self.reminders.sort(key=lambda x: x[\'datetime\']) def get_reminders(self): return self.reminders def check_reminders(self): current_datetime = datetime.now().replace(second=0, microsecond=0) for reminder in self.reminders: if reminder[\'datetime\'] == current_datetime: print(reminder[\'message\']) # Example usage: # reminder_system = Reminder() # reminder_system.add_reminder(2023, 10, 25, 14, 30, \\"Doctor\'s appointment\\") # reminder_system.add_reminder(2023, 10, 25, 16, 00, \\"Meeting with John\\") # reminder_system.add_reminder(2023, 10, 25, 14, 30, \\"Call mom\\") # print(reminder_system.get_reminders()) # reminder_system.check_reminders()"},{"question":"**Title: Process Information and Management Utility** **Objective:** Write a Python function that retrieves and displays the current system\'s active processes along with their CPU and memory usage. Additionally, implement a function to terminate a specified process by its Process ID (PID). **Problem Statement:** You are required to implement two functions: 1. `get_system_processes() -> List[Dict[str, Union[int, str, float]]]`: - This function should retrieve the list of all currently active processes on the system. - Each process should be represented as a dictionary containing the following keys: - `pid` (int): Process ID - `name` (str): Process name - `cpu_usage` (float): CPU usage percentage - `memory_usage` (float): Memory usage in MB - The function should return a list of such dictionaries. 2. `terminate_process(pid: int) -> bool`: - This function should terminate the process with the given Process ID (PID). - The function should return `True` if the process was successfully terminated, and `False` otherwise. **Input Format:** - The `get_system_processes` function does not take any input arguments. - The `terminate_process` function takes a single argument: - `pid` (int): The Process ID of the process to be terminated. **Output Format:** - The `get_system_processes` function returns a list of dictionaries, where each dictionary contains the details of a running process. - The `terminate_process` function returns a boolean value indicating whether the process was successfully terminated. **Constraints:** - The solution should handle cases where the process ID does not exist or the process cannot be terminated (e.g., due to insufficient permissions). - The implementation should be efficient and avoid excessive overhead when retrieving process information. **Example Usage:** ```python # Example usage of get_system_processes function processes = get_system_processes() for process in processes: print(f\\"PID: {process[\'pid\']}, Name: {process[\'name\']}, CPU Usage: {process[\'cpu_usage\']}%, Memory Usage: {process[\'memory_usage\']} MB\\") # Example usage of terminate_process function result = terminate_process(1234) if result: print(\\"Process terminated successfully.\\") else: print(\\"Failed to terminate process.\\") ``` **Note:** - Use the appropriate Python libraries such as `psutil` to implement the system-related functionalities.","solution":"import psutil from typing import List, Dict, Union def get_system_processes() -> List[Dict[str, Union[int, str, float]]]: Retrieves the list of all currently active processes on the system. Each process is represented as a dictionary with keys: \'pid\', \'name\', \'cpu_usage\', \'memory_usage\'. Returns: List[Dict[str, Union[int, str, float]]]: List of dictionaries representing active processes. processes = [] for proc in psutil.process_iter([\'pid\', \'name\', \'cpu_percent\', \'memory_info\']): try: mem_info = proc.info[\'memory_info\'] process_info = { \'pid\': proc.info[\'pid\'], \'name\': proc.info[\'name\'], \'cpu_usage\': proc.info[\'cpu_percent\'], \'memory_usage\': mem_info.rss / (1024 * 1024) # Convert bytes to MB } processes.append(process_info) except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess): continue return processes def terminate_process(pid: int) -> bool: Terminates the process with the given Process ID (PID). Args: pid (int): The Process ID of the process to be terminated. Returns: bool: True if the process was successfully terminated, False otherwise. try: process = psutil.Process(pid) process.terminate() process.wait(timeout=3) return True except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.TimeoutExpired): return False"},{"question":"# Question: Customizing Seaborn Pairplot for Exploratory Data Analysis You are given a dataset of penguins with the following columns: `\\"species\\"`, `\\"island\\"`, `\\"bill_length_mm\\"`, `\\"bill_depth_mm\\"`, `\\"flipper_length_mm\\"`, `\\"body_mass_g\\"`, and `\\"sex\\"`. Your task is to write a function that takes this dataset as input and generates a custom pairplot using seaborn. The function should create a pairplot with the following specifications: 1. Different markers should be used for different species. 2. The diagonal plots should be histograms. 3. The off-diagonal plots should be scatter plots. 4. Customize the scatter plots with a specific marker (`\\"+\\"`) and linewidth (`1`). 5. Customize the histograms to have filled bins. 6. Use a height of 2 for each subplot. 7. The plot should only display the relationships between `\\"bill_length_mm\\"`, `\\"bill_depth_mm\\"`, and `\\"flipper_length_mm\\"`. Function Signature: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_pairplot(data: pd.DataFrame) -> None: # Your code here ``` Input: * `data` (pd.DataFrame): A pandas DataFrame containing the penguin dataset with the specified columns. Expected Output: * The function should display a custom pairplot as per the specifications. Example Usage: ```python import seaborn as sns # Load the penguin dataset penguins = sns.load_dataset(\\"penguins\\") # Call the function create_custom_pairplot(penguins) ``` # Constraints: * The input dataframe will always have the specified columns. * Each numerical column will have non-negative values. * The function should handle any number of rows in the dataset. # Hints: * Use the `sns.pairplot` function with appropriate parameters. * Customize plots using the `plot_kws` and `diag_kws` parameters.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_pairplot(data: pd.DataFrame) -> None: Generates a custom pairplot for the given dataset with specific customizations. Parameters: data (pd.DataFrame): The penguin dataset. sns.pairplot( data, vars=[\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\"], hue=\\"species\\", markers=[\\"o\\", \\"s\\", \\"D\\"], diag_kind=\\"hist\\", plot_kws={\'marker\': \'+\', \'linewidth\': 1}, diag_kws={\'edgecolor\': \'w\', \'alpha\': 0.6}, height=2 ) plt.show()"},{"question":"# Coding Assessment Objective Your task is to implement a Python function that takes a list of Python source files, compiles them using the `py_compile` module, and manages errors based on specified parameters. Function Specification ```python def batch_compile(files, raise_errors=False, quiet_level=0, optimization=-1, invalidation_mode=\\"TIMESTAMP\\"): Compiles a list of Python source files into bytecode. Args: - files (list): A list of strings, where each string is the path to a Python source file. - raise_errors (bool): If True, raises an exception on compilation errors. Defaults to False. - quiet_level (int): The quiet level for error handling (0, 1, 2). Defaults to 0. - optimization (int): Optimization level for the compilation (-1, 0, 1, 2). Defaults to -1. - invalidation_mode (str): The invalidation mode for the bytecode (\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"). Defaults to \\"TIMESTAMP\\". Returns: - dict: A dictionary where keys are file paths and values are the paths to the compiled bytecode files, or None if compilation failed. Raises: - ValueError: If an invalid invalidation_mode is passed. pass ``` Requirements 1. **Input Validation**: - Validate that the `files` parameter is a list of strings. - Validate that `raise_errors` is a boolean. - Validate that `quiet_level` is an integer within {0, 1, 2}. - Validate that `optimization` is an integer within {-1, 0, 1, 2}. - Validate that `invalidation_mode` is one of {\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"}. 2. **Compilation**: - Use `py_compile.compile()` to compile each file in the `files` list. - Handle the `doraise` and `quiet` parameters based on `raise_errors` and `quiet_level`. - Handle the `optimize` and `invalidation_mode` parameters appropriately. 3. **Error Handling**: - If `raise_errors` is True and a compilation error occurs, raise the `PyCompileError`. - If `raise_errors` is False, return None for files that failed to compile. 4. **Output**: - Return a dictionary mapping each source file path to the path of the compiled bytecode or None if compilation failed. Constraints - You must use the `py_compile` module. - Assume that file paths provided in `files` are valid and accessible. - Your function should be efficient and handle large lists of files gracefully. Example ```python files = [\\"script1.py\\", \\"script2.py\\", \\"invalid_script.py\\"] output = batch_compile(files, raise_errors=False, quiet_level=1, optimization=2, invalidation_mode=\\"CHECKED_HASH\\") print(output) # Expected output (paths will vary based on your system): # { # \\"script1.py\\": \\"<path_to_script1.pyc>\\", # \\"script2.py\\": \\"<path_to_script2.pyc>\\", # \\"invalid_script.py\\": None # } ``` Notes - Carefully read the `py_compile` module documentation to understand how to use the `compile` function with different parameters. - Make sure to handle different error and edge cases.","solution":"import py_compile import os def batch_compile(files, raise_errors=False, quiet_level=0, optimization=-1, invalidation_mode=\\"TIMESTAMP\\"): Compiles a list of Python source files into bytecode. Args: - files (list): A list of strings, where each string is the path to a Python source file. - raise_errors (bool): If True, raises an exception on compilation errors. Defaults to False. - quiet_level (int): The quiet level for error handling (0, 1, 2). Defaults to 0. - optimization (int): Optimization level for the compilation (-1, 0, 1, 2). Defaults to -1. - invalidation_mode (str): The invalidation mode for the bytecode (\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"). Defaults to \\"TIMESTAMP\\". Returns: - dict: A dictionary where keys are file paths and values are the paths to the compiled bytecode files, or None if compilation failed. Raises: - ValueError: If an invalid invalidation_mode is passed. # Validation if not isinstance(files, list) or not all(isinstance(f, str) for f in files): raise TypeError(\\"files must be a list of strings\\") if not isinstance(raise_errors, bool): raise TypeError(\\"raise_errors must be a boolean\\") if not isinstance(quiet_level, int) or quiet_level not in {0, 1, 2}: raise ValueError(\\"quiet_level must be an integer and one of {0, 1, 2}\\") if not isinstance(optimization, int) or optimization not in {-1, 0, 1, 2}: raise ValueError(\\"optimization must be an integer and one of {-1, 0, 1, 2}\\") if invalidation_mode not in {\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"}: raise ValueError(\\"invalidation_mode must be one of {\'TIMESTAMP\', \'CHECKED_HASH\', \'UNCHECKED_HASH\'}\\") results = {} for file in files: try: compiled_file = py_compile.compile(file, doraise=raise_errors, optimize=optimization, invalidation_mode=invalidation_mode, quiet=quiet_level) results[file] = compiled_file except py_compile.PyCompileError: if raise_errors: raise results[file] = None return results"},{"question":"Question: Implement a Complex Arithmetic Processor # Objective: You need to implement a function that processes a sequence of arithmetic and logical operations on a given list of numeric values using the functions from the `operator` module. # Function Signature: ```python def arithmetic_processor(numbers: list, operations: list) -> list: Processes a sequence of arithmetic and logical operations on a given list of numeric values. Parameters: numbers: list - List of integer or float values. operations: list - List of dictionaries representing operations. Returns: list - The processed list of numbers after all operations are applied. pass ``` # Inputs: - `numbers` (list): A list of numbers (integers or floats). This list represents the initial state of your data. - `operations` (list): A list of dictionaries where each dictionary represents an operation. Each dictionary contains: - `type` (str): The type of the operation to perform. It can be one of \\"add\\", \\"sub\\", \\"mul\\", \\"truediv\\", \\"floordiv\\", \\"mod\\", \\"and\\", \\"or\\", \\"xor\\", \\"lshift\\", \\"rshift\\", \\"pos\\", \\"neg\\", \\"inv\\". - `value` (int or float): The value to use in the operation (not required for unary operations like \\"pos\\" and \\"neg\\"). - `index` (int): The index in the `numbers` list where the operation should be applied (for unary operations) or the index of the first operand (for binary operations). # Output: - The function should return the list of numbers after applying all the operations sequentially as described. # Constraint: - The operations in the operations list must be valid and executable using the corresponding functions from the `operator` module. - Ensure comprehensive handling of edge cases, such as applying an operation at an invalid index. # Example: ```python from operator import add, sub, mul, truediv, floordiv, mod, and_, or_, xor, lshift, rshift, pos, neg, inv def arithmetic_processor(numbers, operations): for op in operations: if \'value\' in op: value = op[\'value\'] if op[\'type\'] == \'add\': numbers[op[\'index\']] = add(numbers[op[\'index\']], value) elif op[\'type\'] == \'sub\': numbers[op[\'index\']] = sub(numbers[op[\'index\']], value) elif op[\'type\'] == \'mul\': numbers[op[\'index\']] = mul(numbers[op[\'index\']], value) elif op[\'type\'] == \'truediv\': numbers[op[\'index\']] = truediv(numbers[op[\'index\']], value) elif op[\'type\'] == \'floordiv\': numbers[op[\'index\']] = floordiv(numbers[op[\'index\']], value) elif op[\'type\'] == \'mod\': numbers[op[\'index\']] = mod(numbers[op[\'index\']], value) elif op[\'type\'] == \'and\': numbers[op[\'index\']] = and_(numbers[op[\'index\']], value) elif op[\'type\'] == \'or\': numbers[op[\'index\']] = or_(numbers[op[\'index\']], value) elif op[\'type\'] == \'xor\': numbers[op[\'index\']] = xor(numbers[op[\'index\']], value) elif op[\'type\'] == \'lshift\': numbers[op[\'index\']] = lshift(numbers[op[\'index\']], value) elif op[\'type\'] == \'rshift\': numbers[op[\'index\']] = rshift(numbers[op[\'index\']], value) elif op[\'type\'] == \'pos\': numbers[op[\'index\']] = pos(numbers[op[\'index\']]) elif op[\'type\'] == \'neg\': numbers[op[\'index\']] = neg(numbers[op[\'index\']]) elif op[\'type\'] == \'inv\': numbers[op[\'index\']] = inv(numbers[op[\'index\']]) return numbers # Example usage: numbers = [1, 2, 3, 4, 5] operations = [ {\\"type\\": \\"add\\", \\"value\\": 3, \\"index\\": 0}, {\\"type\\": \\"mul\\", \\"value\\": 2, \\"index\\": 1}, {\\"type\\": \\"sub\\", \\"value\\": 1, \\"index\\": 2}, {\\"type\\": \\"neg\\", \\"index\\": 3}, ] print(arithmetic_processor(numbers, operations)) # Output: [4, 4, 2, -4, 5] ``` Explanation: - The initial list of numbers is `[1, 2, 3, 4, 5]`. - The operations are applied in the given order: - Add 3 to the number at index 0: `[4, 2, 3, 4, 5]` - Multiply the number at index 1 by 2: `[4, 4, 3, 4, 5]` - Subtract 1 from the number at index 2: `[4, 4, 2, 4, 5]` - Negate the number at index 3: `[4, 4, 2, -4, 5]` - The resulting list after all operations is `[4, 4, 2, -4, 5]`.","solution":"import operator def arithmetic_processor(numbers, operations): Processes a sequence of arithmetic and logical operations on a given list of numeric values. Parameters: numbers: list - List of integer or float values. operations: list - List of dictionaries representing operations. Returns: list - The processed list of numbers after all operations are applied. ops = { \\"add\\": operator.add, \\"sub\\": operator.sub, \\"mul\\": operator.mul, \\"truediv\\": operator.truediv, \\"floordiv\\": operator.floordiv, \\"mod\\": operator.mod, \\"and\\": operator.and_, \\"or\\": operator.or_, \\"xor\\": operator.xor, \\"lshift\\": operator.lshift, \\"rshift\\": operator.rshift, \\"pos\\": operator.pos, \\"neg\\": operator.neg, \\"inv\\": operator.inv } for op in operations: index = op[\\"index\\"] if op[\\"type\\"] in [\\"pos\\", \\"neg\\", \\"inv\\"]: numbers[index] = ops[op[\\"type\\"]](numbers[index]) else: value = op[\\"value\\"] numbers[index] = ops[op[\\"type\\"]](numbers[index], value) return numbers"},{"question":"Managing and Debugging System Behavior Objective: Demonstrate comprehensive understanding of Python\'s `sys` module by implementing functions that manipulate and retrieve system-specific parameters, handle exceptions, and manage recursion limits effectively. Problem Statement: 1. **Function 1: Handle Command-Line Arguments** Write a function `parse_command_line_args()` that returns a dictionary where keys are the argument positions (starting from 1) and values are the command line arguments passed to the script. **Input**: None (use `sys.argv`) **Output**: Dictionary of command-line arguments ```python def parse_command_line_args(): pass ``` Example: ```python # If the script is run with: python script.py arg1 arg2 arg3 parse_command_line_args() # Output: {1: \'arg1\', 2: \'arg2\', 3: \'arg3\'} ``` 2. **Function 2: Set and Get Recursion Limit** Write two functions, `set_recursion_limit(limit)` and `get_recursion_limit()`. The function `set_recursion_limit(limit)` should set the maximum depth of the Python interpreter stack to the given `limit`. The function `get_recursion_limit()` should return the current recursion limit. **Constraints**: - `limit` should be a positive integer. - If `limit` is non-positive, the function should raise a `ValueError`. ```python def set_recursion_limit(limit): pass def get_recursion_limit(): pass ``` Example: ```python set_recursion_limit(3000) assert get_recursion_limit() == 3000 ``` 3. **Function 3: Runtime Auditing** Write a function `audit_event(event, *args)` that uses `sys.audit()` to raise an auditing event with the given event name and arguments. Additionally, create an auditing hook using `sys.addaudithook()` that logs any triggered events. **Input**: `event` (string), `args` (any number of additional arguments) **Output**: None (just ensure the hook captures and logs events) ```python def audit_event(event, *args): pass # Hook to log events def log_event(event, args): print(f\\"Audit Event: {event}, Args: {args}\\") sys.addaudithook(log_event) ``` Example: ```python audit_event(\'test.event\', 42, \'example\') # Expect the output: Audit Event: test.event, Args: (42, \'example\') ``` 4. **Function 4: Exception Logging** Write a function `handle_exceptions()` that sets a custom exception hook using `sys.excepthook`. The custom exception hook should log the exception type, value, and traceback to a file named `exception_log.txt`. **Input**: None **Output**: None (logging should be handled to the file) ```python def handle_exceptions(): pass ``` Example usage: ```python handle_exceptions() raise ValueError(\\"This is a test exception\\") # Check \'exception_log.txt\' for the logged exception details. ``` Constraints: - Ensure your functions handle edge cases appropriately. - Avoid using any external libraries for logging or argument parsing. - Performance considerations are not critical, but aim for clear and maintainable code. Complete the implementation and ensure the functions work as described in the examples.","solution":"import sys def parse_command_line_args(): Returns a dictionary where keys are the argument positions (starting from 1) and values are the command line arguments passed to the script. return {i: arg for i, arg in enumerate(sys.argv[1:], start=1)} def set_recursion_limit(limit): Sets the maximum depth of the Python interpreter stack to the given limit. Args: limit (int): The recursion limit to be set. Must be a positive integer. Raises: ValueError: If the limit is not a positive integer. if not isinstance(limit, int) or limit <= 0: raise ValueError(\\"Limit must be a positive integer\\") sys.setrecursionlimit(limit) def get_recursion_limit(): Returns the current recursion limit. Returns: int: The current recursion limit. return sys.getrecursionlimit() def audit_event(event, *args): Uses sys.audit to raise an auditing event with the given event name and arguments. Args: event (str): The audit event name. args (tuple): Additional arguments for the event. sys.audit(event, *args) def log_event(event, args): Logs any triggered audit events. Args: event (str): The audit event name. args (tuple): Arguments for the event. print(f\\"Audit Event: {event}, Args: {args}\\") # Add audit hook sys.addaudithook(log_event) def handle_exceptions(): Sets a custom exception hook using sys.excepthook to log the exception type, value, and traceback to a file named `exception_log.txt`. def custom_excepthook(exctype, value, tb): with open(\'exception_log.txt\', \'a\') as f: f.write(f\\"Exception type: {exctype.__name__}n\\") f.write(f\\"Exception value: {value}n\\") import traceback traceback.print_tb(tb, file=f) f.write(\\"n\\") sys.excepthook = custom_excepthook"},{"question":"**Question: Audio Processing and Color Conversion** **Objective:** Demonstrate your understanding of audio file handling and color system conversions by implementing functions from the `wave` and `colorsys` modules. --- # Part 1: WAV File Manipulation **Task:** Implement a Python function called `copy_wav_with_changes` that accepts three parameters: `input_wav_path`, `output_wav_path`, and `volume_factor`. This function should read the WAV file located at `input_wav_path`, apply the `volume_factor` adjustment to its audio samples, and then save the modified audio to a new WAV file at `output_wav_path`. **Function Signature:** ```python def copy_wav_with_changes(input_wav_path: str, output_wav_path: str, volume_factor: float) -> None: ``` **Inputs:** - `input_wav_path` (str): Path to the input WAV file. - `output_wav_path` (str): Path to save the output WAV file. - `volume_factor` (float): Factor by which to multiply the audio samples\' amplitude values. **Output:** - The function does not return a value. Instead, it creates a new WAV file at `output_wav_path`. **Constraints:** - The input WAV file is guaranteed to be a valid PCM format WAV file. - The volume adjustment is done by multiplying the sample values by the `volume_factor`. - Ensure that the output file maintains the same format and sample rate as the input file. # Part 2: Color System Conversion **Task:** Implement a Python function called `rgb_to_hsv_conversion` that accepts three parameters: `r`, `g`, and `b`. This function should convert the RGB color values to HSV and return the result as a tuple. **Function Signature:** ```python def rgb_to_hsv_conversion(r: float, g: float, b: float) -> (float, float, float): ``` **Inputs:** - `r` (float): Red component of the color (range: 0.0 to 1.0). - `g` (float): Green component of the color (range: 0.0 to 1.0). - `b` (float): Blue component of the color (range: 0.0 to 1.0). **Output:** - A tuple `(h, s, v)` representing the HSV values corresponding to the input RGB values. **Constraints:** - Ensure the input values `r`, `g`, and `b` are in the correct range (0.0 to 1.0). - The returned values `h`, `s`, and `v` should also be in their respective ranges (h: 0.0 to 1.0, s: 0.0 to 1.0, v: 0.0 to 1.0). --- # Example Usage ```python # Example usage for Part 1 copy_wav_with_changes(\\"input.wav\\", \\"output.wav\\", 1.5) # Example usage for Part 2 hsv = rgb_to_hsv_conversion(0.5, 0.3, 0.8) print(hsv) # Output might be something like (0.75, 0.625, 0.8) ``` # Notes - You may find the `wave` module documentation useful for part 1. - You may find the `colorsys` module documentation useful for part 2. - Ensure to handle any necessary imports and edge cases in your implementation.","solution":"import wave import numpy as np import colorsys def copy_wav_with_changes(input_wav_path: str, output_wav_path: str, volume_factor: float) -> None: # Open the input WAV file with wave.open(input_wav_path, \'rb\') as in_wav: # Read frames and parameters params = in_wav.getparams() num_frames = in_wav.getnframes() audio_data = in_wav.readframes(num_frames) # Convert audio data to numpy array audio_samples = np.frombuffer(audio_data, dtype=np.int16) # Adjust the volume audio_samples = np.clip(audio_samples * volume_factor, -32768, 32767).astype(np.int16) # Convert numpy array back to bytes modified_audio_data = audio_samples.tobytes() # Write the modified audio to the output WAV file with wave.open(output_wav_path, \'wb\') as out_wav: out_wav.setparams(params) out_wav.writeframes(modified_audio_data) def rgb_to_hsv_conversion(r: float, g: float, b: float) -> (float, float, float): return colorsys.rgb_to_hsv(r, g, b)"},{"question":"# Objective Design a function that processes input data to optimize for prediction latency and throughput using scikit-learn estimators. Your function should handle different data representations (dense and sparse), train a suitable model, and evaluate its performance. # Function Signature ```python def optimize_model_performance(X_train, y_train, X_test, assume_finite=False, working_memory=1024): Optimize a scikit-learn model\'s performance for prediction latency and throughput. Parameters: X_train (numpy.ndarray or scipy.sparse matrix): The training data input features. y_train (numpy.ndarray): The training data target values. X_test (numpy.ndarray or scipy.sparse matrix): The test data input features. assume_finite (bool): If True, assume the input data is already validated, skipping validations. working_memory (int): The working memory limit in MiB for chunk-based computations. Returns: dict: A dictionary containing model performance metrics: - \'model\': Trained scikit-learn model - \'prediction_latency\': Average prediction latency (microseconds) - \'prediction_throughput\': Predictions per second pass ``` # Requirements 1. **Input Data**: - `X_train` and `X_test` can be either dense numpy arrays or sparse matrices (CSR format). - The function should check the sparsity of the input data and choose an appropriate representation format. - If the sparsity ratio is greater than 90%, convert the data to sparse format. 2. **Model Training**: - Train a suitable scikit-learn model on the `X_train` and `y_train` data. - Use the `assume_finite` parameter to configure scikit-learn before training. This should be done using the `set_config` or `config_context` method. - Use the `working_memory` parameter to limit temporary memory usage during model training. 3. **Performance Evaluation**: - Measure the average prediction latency (in microseconds) on the `X_test` data. - Measure the prediction throughput (predictions per second) on the `X_test` data. - Return the trained model and both performance metrics in a dictionary. # Constraints - **Time Complexity**: Aim to minimize the overall time complexity for both training and prediction. - **Memory Usage**: Efficiently manage memory usage, especially for sparse data. # Notes - You may use any supervised learning estimator from scikit-learn suitable for the given data. - Use appropriate performance measurement techniques to ensure accurate latency and throughput metrics. - Ensure your function handles different input formats and sizes robustly. # Example Usage ```python from sklearn.datasets import make_regression from scipy.sparse import csr_matrix # Generate some sample data X, y = make_regression(n_samples=1000, n_features=20, noise=0.1) # Convert some data to sparse format X_sparse = csr_matrix(X) # Example call with dense data results_dense = optimize_model_performance(X, y, X) # Example call with sparse data results_sparse = optimize_model_performance(X_sparse, y, X_sparse, assume_finite=True, working_memory=512) # Output results print(\\"Dense Data Performance:\\", results_dense) print(\\"Sparse Data Performance:\\", results_sparse) ``` **Note**: Students are required to ensure that the function is efficient and meets the performance constraints while accurately handling the different data formats.","solution":"import time import numpy as np from scipy.sparse import issparse, csr_matrix from sklearn.linear_model import Ridge from sklearn.utils import check_array from sklearn import set_config, config_context def optimize_model_performance(X_train, y_train, X_test, assume_finite=False, working_memory=1024): Optimize a scikit-learn model\'s performance for prediction latency and throughput. Parameters: X_train (numpy.ndarray or scipy.sparse matrix): The training data input features. y_train (numpy.ndarray): The training data target values. X_test (numpy.ndarray or scipy.sparse matrix): The test data input features. assume_finite (bool): If True, assume the input data is already validated, skipping validations. working_memory (int): The working memory limit in MiB for chunk-based computations. Returns: dict: A dictionary containing model performance metrics: - \'model\': Trained scikit-learn model - \'prediction_latency\': Average prediction latency (microseconds) - \'prediction_throughput\': Predictions per second # Check sparsity and convert to sparse if needed if not issparse(X_train): sparsity_ratio_train = 1.0 - np.count_nonzero(X_train) / float(X_train.size) if sparsity_ratio_train > 0.9: X_train = csr_matrix(X_train) if not issparse(X_test): sparsity_ratio_test = 1.0 - np.count_nonzero(X_test) / float(X_test.size) if sparsity_ratio_test > 0.9: X_test = csr_matrix(X_test) # Set scikit-learn configuration based on assume_finite and working_memory with config_context(assume_finite=assume_finite, working_memory=working_memory): model = Ridge() model.fit(X_train, y_train) # Measure prediction latency and throughput start_time = time.time() predictions = model.predict(X_test) end_time = time.time() # Calculate metrics elapsed_time = end_time - start_time prediction_latency = (elapsed_time / X_test.shape[0]) * 1e6 # microseconds prediction_throughput = X_test.shape[0] / elapsed_time # predictions per second return { \'model\': model, \'prediction_latency\': prediction_latency, \'prediction_throughput\': prediction_throughput }"},{"question":"Advanced Plotting with Seaborn Problem Statement You have been given a dataset containing information on health expenditures by different countries over multiple years. Your task is to write a Python function that uses Seaborn to create a customized plot showing the health expenditures for a specified set of countries. Specifications 1. **Function Name**: `plot_health_expenditure` 2. **Input**: - A list of country names (List of strings). - A boolean flag `stacked` (default is `False`) to indicate whether plots should be stacked. 3. **Output**: - A Seaborn plot object that shows the health expenditures over the years for the specified countries. Function Signature ```python def plot_health_expenditure(countries: list, stacked: bool = False): pass ``` Constraints - The `countries` list should contain at least one country name. - If the `countries` list is empty, raise a `ValueError` with the message \\"Country list cannot be empty.\\" - If a specified country is not present in the dataset, ignore that country without raising an error. Performance Requirements - The function should efficiently handle plotting for up to 10 countries simultaneously. Example Usage ```python # Example usage of plot_health_expenditure function plot_health_expenditure([\\"United States\\", \\"Canada\\", \\"Germany\\"], stacked=True) ``` Additional Details - You should use the `healthexp` dataset provided in the Seaborn library. - Customize the plot colors to differentiate between countries. - If `stacked` is `True`, the plots should be stacked to show part-whole relationships. Otherwise, they should be individual area plots. - Include appropriate labeling for axes and a legend to distinguish between countries. Documentation for Reference: ```python import seaborn.objects as so from seaborn import load_dataset healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Example plotting functions p = so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\").facet(\\"Country\\", wrap=3) p.add(so.Area(), color=\\"Country\\") p.add(so.Area(), x=\\"Spending_USD\\", y=\\"Year\\", orient=\\"y\\") so.Plot(healthexp, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\").add(so.Area(alpha=.7), so.Stack()) ``` Hints - Look into Seaborn\'s `so.Plot` and its methods for creating plots and facetting. - Utilize DataFrame methods to filter and prepare the data for the specified countries. - Seaborn\'s `color` parameter can be used to differentiate plots by country.","solution":"import seaborn as sns import seaborn.objects as so from seaborn import load_dataset def plot_health_expenditure(countries: list, stacked: bool = False): Plots health expenditures over the years for specified countries using seaborn. Parameters: countries (list): List of country names. stacked (bool): If True, generate stacked plot. Default is False. Returns: seaborn.objects.Plot: Seaborn plot object. if not countries: raise ValueError(\\"Country list cannot be empty.\\") # Load dataset and pivot it healthexp = ( load_dataset(\\"healthexp\\") .pivot(index=\\"Year\\", columns=\\"Country\\", values=\\"Spending_USD\\") .interpolate() .stack() .rename(\\"Spending_USD\\") .reset_index() .sort_values(\\"Country\\") ) # Filter the data for specified countries filtered_data = healthexp[healthexp[\\"Country\\"].isin(countries)] # Plotting plot = so.Plot(filtered_data, \\"Year\\", \\"Spending_USD\\", color=\\"Country\\") if stacked: plot = plot.add(so.Area(alpha=0.7), so.Stack()) else: plot = plot.add(so.Area(alpha=0.7)) return plot"},{"question":"Objective Test the student\'s ability to: 1. Load and utilize seaborn\'s dataset. 2. Create basic and customized visualizations using seaborn.objects. 3. Demonstrate their understanding of facets and plot customization in seaborn. Question You are given the `seaice` dataset available in seaborn. Your task is to visualize the extent of sea ice over years, with specific customizations. Tasks 1. **Load the Dataset:** - Load the `seaice` dataset from seaborn. 2. **Simple Line Plot:** - Create a basic line plot showing the `Extent` of sea ice over `Date`. 3. **Customized Plot:** - Create a more complex plot by facetting it by decade and distinguishing lines by year. Customize the lines with varying linewidths and colors. - Specifically: - X-axis should show day of the year (derived from `Date`). - Use different colors to represent different years. - Facet the plot by decade, title each facet with the format \\"{decade}s\\". Input No specific input is required as you will be using seaborn\'s `seaice` dataset. Output You must display the following: 1. A basic line plot showing `Extent` over `Date`. 2. A customized plot fulfilling the described requirements. Constraints - The plots must be generated using seaborn\'s `objects` module and seaborn\'s `seaice` dataset. - Ensure the plots are clearly labeled and visually distinguishable. Example There is no example output as this is a visualization task. Solution Template ```python import seaborn.objects as so from seaborn import load_dataset # Load the seaice dataset seaice = load_dataset(\\"seaice\\") # Task 1: Simple Line Plot simple_plot = so.Plot(seaice, \\"Date\\", \\"Extent\\").add(so.Lines()) simple_plot.show() # Task 2: Customized Plot custom_plot = ( so.Plot( x=seaice[\\"Date\\"].dt.day_of_year, y=seaice[\\"Extent\\"], color=seaice[\\"Date\\"].dt.year ) .facet(seaice[\\"Date\\"].dt.year.round(-1)) .add(so.Lines(linewidth=.5, color=\\"#bbca\\"), col=None) .add(so.Lines(linewidth=1)) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(8, 4)) .label(title=\\"{}s\\".format) ) custom_plot.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load the seaice dataset seaice = load_dataset(\\"seaice\\") # Task 1: Simple Line Plot simple_plot = so.Plot(seaice, x=\\"Date\\", y=\\"Extent\\").add(so.Line()) simple_plot.show() # Creating a new column to handle the decade seaice[\\"Decade\\"] = (seaice[\\"Date\\"].dt.year // 10) * 10 # Task 2: Customized Plot custom_plot = ( so.Plot( seaice, x=seaice[\\"Date\\"].dt.day_of_year, y=\\"Extent\\", color=seaice[\\"Date\\"].dt.year ) .facet( col=seaice[\\"Decade\\"].astype(str) + \\"s\\" ) .add(so.Line()) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(8, 4)) .label(title=\\"Sea Ice Extent Over Years\\", x=\\"Day of Year\\", y=\\"Extent\\", color=\\"Year\\") ) custom_plot.show()"},{"question":"# XMLRPC Server and Client Implementation You are tasked with implementing an XML-RPC server and a client that interacts with it. **Server Requirements:** 1. Implement an XML-RPC server that listens on `localhost` and port `8000`. 2. The server should provide a single method `add(a, b)` which takes two integers and returns their sum. 3. The server should log all incoming requests and the results to the console. **Client Requirements:** 1. Implement an XML-RPC client that connects to the server at `http://localhost:8000`. 2. The client should call the `add` method on the server with the integers `5` and `7`. 3. Print the result, which should be the sum of the two integers. # Constraints - Your server should run indefinitely, waiting for client requests. - Ensure that the server handles multiple requests without crashing. - You should use the `xmlrpc.server` and `xmlrpc.client` libraries. # Input Format - No input required; the server should be set up to listen and the client should be hardcoded with the interaction. # Output Format - Print statements from the server showing the logging of requests and responses. - Print statement from the client showing the result received from the server. # Performance Requirements - The server should handle at least 10 simultaneous client requests without crashing. # Implementation Details Server Code ```python from xmlrpc.server import SimpleXMLRPCServer import logging # Set up logging logging.basicConfig(level=logging.INFO) def add(a, b): result = a + b logging.info(f\\"add({a}, {b}) = {result}\\") return result server = SimpleXMLRPCServer((\'localhost\', 8000)) logging.info(\\"XML-RPC Server started on localhost:8000\\") server.register_function(add, \'add\') # Run the server\'s main loop try: server.serve_forever() except KeyboardInterrupt: logging.info(\\"Server is shutting down.\\") ``` Client Code ```python import xmlrpc.client server_url = \'http://localhost:8000\' client = xmlrpc.client.ServerProxy(server_url) try: result = client.add(5, 7) print(f\'The sum is: {result}\') except Exception as e: print(f\'An error occurred: {e}\') ``` Your task is to complete the above implementations and ensure they work as described. Write the completed code in the provided code blocks and test to ensure functionality.","solution":"# Server code from xmlrpc.server import SimpleXMLRPCServer import logging # Set up logging logging.basicConfig(level=logging.INFO) def add(a, b): result = a + b logging.info(f\\"add({a}, {b}) = {result}\\") return result def start_server(): server = SimpleXMLRPCServer((\'localhost\', 8000)) logging.info(\\"XML-RPC Server started on localhost:8000\\") server.register_function(add, \'add\') # Run the server\'s main loop try: server.serve_forever() except KeyboardInterrupt: logging.info(\\"Server is shutting down.\\") # Client code import xmlrpc.client def client_add(a, b): server_url = \'http://localhost:8000\' client = xmlrpc.client.ServerProxy(server_url) try: result = client.add(a, b) print(f\'The sum is: {result}\') return result except Exception as e: print(f\'An error occurred: {e}\') return None if __name__ == \\"__main__\\": # You can run the server in a separate terminal or thread to test the client. start_server()"},{"question":"**Objective:** You are tasked with enhancing the Python command-line interface by implementing a custom command history and completion functionality using the `readline` module. The custom history should support loading and saving history to a file, and the auto-completion should suggest Python keywords and previously used commands. **Instructions:** 1. **Custom Shell Class:** - Create a class `CustomShell` that initializes with a history file path. - On initialization, the class should: - Load command history from the given history file if it exists. - Set up a custom completer that suggests Python keywords and commands from history. - Bind the TAB key to the auto-completion functionality. 2. **History Management Methods:** - Implement methods to: - Save the current command history to the history file (`save_history()`). - Clear the command history (`clear_history()`). - Add a command to the history (`add_command_to_history(command: str)`). 3. **Auto-Completion Methods:** - Implement a custom completer function that suggests: - Python keywords from the `keyword` module. - Previously executed commands from the history. 4. **Integration with `atexit`:** - Ensure that the command history is saved when the interactive session ends. **Constraints:** - Do not use external libraries other than what\'s available in the standard library. - The implementation should work on both Unix-based systems and Windows. **Example Usage:** ```python if __name__ == \\"__main__\\": shell = CustomShell(histfile=os.path.expanduser(\\"~/.custom_shell_history\\")) # Run your interactive loop here # For simplicity, you can use input() in a loop and process the user\'s commands ``` **Expected Functions and Methods:** ```python class CustomShell: def __init__(self, histfile: str): # Initialize the shell, load history, set up completer pass def save_history(self): # Save the current history to the file pass def clear_history(self): # Clear the current command history pass def add_command_to_history(self, command: str): # Add a new command to the history pass def completer(self, text: str, state: int): # Custom completer function pass ``` Use the `readline` and `keyword` modules where necessary to complete the implementation. **Note:** Here is a brief overview of methods you will need from the `readline` module: - `readline.read_history_file(histfile)` - `readline.write_history_file(histfile)` - `readline.add_history(command)` - `readline.get_line_buffer()` - `readline.set_completer()` - `readline.parse_and_bind(\\"tab: complete\\")` Good luck, and make sure your solution is well-tested and documented.","solution":"import os import atexit import readline import keyword class CustomShell: def __init__(self, histfile: str): Initialize the shell with a history file path, load history, and set up completer. self.histfile = histfile self.load_history() atexit.register(self.save_history) readline.set_completer(self.completer) readline.parse_and_bind(\\"tab: complete\\") def load_history(self): Load the history from the history file. if os.path.exists(self.histfile): readline.read_history_file(self.histfile) def save_history(self): Save the current history to the history file. readline.write_history_file(self.histfile) def clear_history(self): Clear the current command history. readline.clear_history() def add_command_to_history(self, command: str): Add a new command to the history. readline.add_history(command) def completer(self, text: str, state: int): Custom completer function that suggests Python keywords and history commands. options = [kw for kw in keyword.kwlist if kw.startswith(text)] + [readline.get_history_item(i + 1) for i in range(readline.get_current_history_length()) if readline.get_history_item(i + 1) and readline.get_history_item(i + 1).startswith(text)] try: return options[state] except IndexError: return None"},{"question":"Objective: Create a Python function that inspects the `__future__` module and returns detailed information about all available features, including their optional release, mandatory release, and compiler flag. Task: Define a function `inspect_future_module()` that: 1. Imports the `__future__` module dynamically. 2. Collects information about all available features in the `__future__` module. 3. Returns a list of dictionaries, where each dictionary contains: - `feature`: The name of the feature. - `optional_release`: The optional release version as a tuple. - `mandatory_release`: The mandatory release version as a tuple. - `compiler_flag`: The compiler flag associated with the feature. Expected Input and Output: - The function `inspect_future_module()` will take no input arguments. - It will return a list of dictionaries, like the following structure: ```python [ { \\"feature\\": \\"nested_scopes\\", \\"optional_release\\": (2, 1, 0, \\"beta\\", 1), \\"mandatory_release\\": (2, 2, 0, \\"final\\", 0), \\"compiler_flag\\": 16 }, ... ] ``` Constraints: - Do not use any imports other than the `__future__` module. - Ensure that your solution can handle any new feature that might be added to the `__future__` module in future Python releases. Performance Requirements: - The function should execute efficiently, maintaining readability and clarity. Example: ```python def inspect_future_module(): import __future__ features = [] for feature_name in dir(__future__): feature = getattr(__future__, feature_name) if isinstance(feature, __future__._Feature): features.append({ \\"feature\\": feature_name, \\"optional_release\\": feature.getOptionalRelease(), \\"mandatory_release\\": feature.getMandatoryRelease(), \\"compiler_flag\\": feature.compiler_flag }) return features # Example usage features_info = inspect_future_module() for feature in features_info: print(feature) ``` Note: - Your implementation should handle all features listed in the `__future__` module accurately and dynamically.","solution":"def inspect_future_module(): import __future__ features = [] for feature_name in dir(__future__): feature = getattr(__future__, feature_name) if isinstance(feature, __future__._Feature): features.append({ \\"feature\\": feature_name, \\"optional_release\\": feature.getOptionalRelease(), \\"mandatory_release\\": feature.getMandatoryRelease(), \\"compiler_flag\\": feature.compiler_flag }) return features"},{"question":"# Python Iterator Implementation and Usage The Python `iter()` function can be used in two specific ways: 1. To create a sequence iterator from a sequence object. 2. To create a callable iterator from a callable object and a sentinel value. In this assessment, you\'ll implement a custom iterator class, `MyIterator`, which can operate as both a sequence iterator and a callable iterator. The behavior of the class should change based on the initialization parameters. # Class Specification 1. Sequence Iterator Mode: When the class is initialized with a sequence, it should function as a sequence iterator. - **Input**: ```python MyIterator(sequence) ``` - **Output**: The iterator should return elements from the sequence one by one until the end of the sequence is reached. 2. Callable Iterator Mode: When the class is initialized with a callable and a sentinel, it should function as a callable iterator. - **Input**: ```python MyIterator(callable, sentinel) ``` - **Output**: The iterator should call the callable object repeatedly until it returns the sentinel value. # Implementation Details 1. Define the class `MyIterator` with appropriate `__init__`, `__iter__`, and `__next__` methods. 2. Handle different initialization parameters to switch between sequence iterator and callable iterator modes. 3. Raise appropriate exceptions (like `StopIteration`) to signal the end of iteration. # Example Usage Sequence Iterator Mode ```python sequence = [1, 2, 3] it = MyIterator(sequence) for item in it: print(item) ``` **Expected Output**: ``` 1 2 3 ``` Callable Iterator Mode ```python def generator(): current = 0 while current < 3: yield current current += 1 callable_iter = generator() it = MyIterator(callable_iter.__next__, 3) for item in it: print(item) ``` **Expected Output**: ``` 0 1 2 ``` # Constraints - You can assume that the sequence passed is a valid iterable. - The callable passed when used with a sentinel will always eventually return the sentinel value. # Your Task Implement the `MyIterator` class based on the above specifications.","solution":"class MyIterator: def __init__(self, source, sentinel=None): if callable(source) and sentinel is not None: self._callable = source self._sentinel = sentinel self._mode = \'callable\' else: self._iterable = iter(source) self._mode = \'sequence\' def __iter__(self): return self def __next__(self): if self._mode == \'sequence\': return next(self._iterable) elif self._mode == \'callable\': result = self._callable() if result == self._sentinel: raise StopIteration return result"},{"question":"**Objective**: Demonstrate your understanding of seaborn\'s plotting functions, with a focus on the `rugplot` feature, by generating insightful visualizations of a dataset. **Task**: Write a Python function named `visualize_rugplots` that performs the following tasks using the seaborn library: 1. Loads the \\"tips\\" dataset using `sns.load_dataset(\\"tips\\")`. 2. Creates a 2x2 grid of subplots: - **Top-Left**: A KDE plot of the `total_bill` field with a rug plot along the x-axis. - **Top-Right**: A scatter plot of `total_bill` vs `tip`, with a rug plot along both axes. - **Bottom-Left**: A scatter plot of `total_bill` vs `tip`, colored by `time` using the `hue` parameter, with corresponding rugs. - **Bottom-Right**: A scatter plot of `total_bill` vs `tip`, with taller rugs drawn outside the axes. 3. Ensure each subplot is appropriately labeled and styled for clear visualization. **Input**: - None **Output**: - The function should display a 2x2 grid of plots as described above. **Constraints**: - Utilize seaborn and matplotlib for plotting. - The output of the function should be visual, and no data should be returned. - Consider setting a seaborn theme for consistent styling using `sns.set_theme()`. **Function Signature**: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_rugplots(): pass ``` # Example Usage ```python # To view the visualizations, simply call the function visualize_rugplots() ``` # Notes - Ensure the plots do not overlap and are displayed clearly. - Make use of subplots to organize the grid layout (Hint: use `plt.subplots`). - Label axes and add titles to each subplot for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_rugplots(): # Load the \\"tips\\" dataset tips = sns.load_dataset(\\"tips\\") # Set the theme for seaborn sns.set_theme(style=\\"ticks\\") # Create a 2x2 grid of subplots fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # Top-Left: KDE plot of `total_bill` with a rug plot along the x-axis sns.kdeplot(data=tips, x=\\"total_bill\\", ax=axes[0, 0]) sns.rugplot(data=tips, x=\\"total_bill\\", ax=axes[0, 0], height=0.1) axes[0, 0].set_title(\\"KDE Plot with Rug Plot on x-axis\\") axes[0, 0].set_xlabel(\\"Total Bill\\") axes[0, 0].set_ylabel(\\"Density\\") # Top-Right: Scatter plot of `total_bill` vs `tip`, with a rug plot along both axes sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=axes[0, 1]) sns.rugplot(data=tips, x=\\"total_bill\\", ax=axes[0, 1], height=0.05) sns.rugplot(data=tips, y=\\"tip\\", ax=axes[0, 1], height=0.05) axes[0, 1].set_title(\\"Scatter Plot with Rugs on both axes\\") axes[0, 1].set_xlabel(\\"Total Bill\\") axes[0, 1].set_ylabel(\\"Tip\\") # Bottom-Left: Scatter plot of `total_bill` vs `tip`, colored by `time` with corresponding rugs sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=\\"time\\", ax=axes[1, 0]) sns.rugplot(data=tips, x=\\"total_bill\\", hue=\\"time\\", ax=axes[1, 0], height=0.05) sns.rugplot(data=tips, y=\\"tip\\", hue=\\"time\\", ax=axes[1, 0], height=0.05) axes[1, 0].set_title(\\"Scatter Plot colored by Time with Rugs\\") axes[1, 0].set_xlabel(\\"Total Bill\\") axes[1, 0].set_ylabel(\\"Tip\\") # Bottom-Right: Scatter plot of `total_bill` vs `tip`, with taller rugs drawn outside the axes sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", ax=axes[1, 1]) sns.rugplot(data=tips, x=\\"total_bill\\", ax=axes[1, 1], height=0.1, clip_on=False) sns.rugplot(data=tips, y=\\"tip\\", ax=axes[1, 1], height=0.1, clip_on=False) axes[1, 1].set_title(\\"Scatter Plot with Taller Rugs Outside the Axes\\") axes[1, 1].set_xlabel(\\"Total Bill\\") axes[1, 1].set_ylabel(\\"Tip\\") # Adjust layout for the plots plt.tight_layout() # Show the plots plt.show()"},{"question":"# Custom CLI for Task Management **Objective**: Create a custom command-line interface for a simple task management system using Python\'s `cmd` module. The CLI will allow users to add, view, update, and delete tasks. **Requirements**: 1. **Class Definition**: - Define a class `TaskManager` extending from `cmd.Cmd`. 2. **Prompt**: - The prompt should be `\'(taskManager) \'` when waiting for user input. 3. **Commands**: - `do_add(description)`: Add a new task with the given description. - `do_show()`: Show all tasks with their indices and descriptions. - `do_update(index description)`: Update the task at the given index with the new description. - `do_delete(index)`: Delete the task at the given index. - `do_backup(file)`: Save all current tasks to a specified file. - `do_restore(file)`: Load tasks from a specified file. - `do_exit()`: Exit the CLI. 4. **Command Descriptions**: - Each command should have a docstring that describes its function. This will allow the built-in `help` command to display useful information. 5. **Task Storage**: - Store the tasks in an instance list `self.tasks` where each task is a string. 6. **Error Handling**: - Implement proper error handling for conditions such as invalid indices and file errors. 7. **Additional Attributes**: - Add any necessary attributes to the class to support the above functionalities. **Hints**: - Use the `open()` method with appropriate file modes for reading and writing task backups. - Use try-except blocks to handle file-related errors. - Convert `index` inputs to integers and check their validity. - The class should override methods like `do_quit()` to provide a customized exit from the CMD loop. **Sample Session**: ```shell python task_manager.py Welcome to Task Manager. Type help or ? to list commands. (taskManager) add Complete the assignment (taskManager) add Submit the report (taskManager) show 0 - Complete the assignment 1 - Submit the report (taskManager) update 0 Complete the assignment by Sunday (taskManager) show 0 - Complete the assignment by Sunday 1 - Submit the report (taskManager) delete 1 (taskManager) show 0 - Complete the assignment by Sunday (taskManager) backup tasks.txt (taskManager) exit ``` **Starter Code**: ```python import cmd class TaskManager(cmd.Cmd): prompt = \'(taskManager) \' def __init__(self): super().__init__() self.tasks = [] def do_add(self, description): Add a new task with the given description. # Implement this method pass def do_show(self, arg): Show all tasks with their indices and descriptions. # Implement this method pass def do_update(self, arg): Update the task at the given index with the new description. # Implement this method pass def do_delete(self, arg): Delete the task at the given index. # Implement this method pass def do_backup(self, arg): Save all current tasks to the specified file. # Implement this method pass def do_restore(self, arg): Load tasks from the specified file. # Implement this method pass def do_exit(self, arg): Exit the task manager. print(\'Exiting Task Manager. Goodbye!\') return True if __name__ == \'__main__\': TaskManager().cmdloop(intro=\'Welcome to Task Manager. Type help or ? to list commands.\') ```","solution":"import cmd import os class TaskManager(cmd.Cmd): prompt = \'(taskManager) \' def __init__(self): super().__init__() self.tasks = [] def do_add(self, description): Add a new task with the given description. self.tasks.append(description) print(f\'Task added: {description}\') def do_show(self, arg): Show all tasks with their indices and descriptions. if not self.tasks: print(\'No tasks available.\') else: for index, task in enumerate(self.tasks): print(f\'{index} - {task}\') def do_update(self, arg): Update the task at the given index with the new description. try: index, description = arg.split(maxsplit=1) index = int(index) if 0 <= index < len(self.tasks): self.tasks[index] = description print(f\'Task {index} updated to: {description}\') else: print(\'Invalid index\') except ValueError: print(\'Usage: update <index> <description>\') def do_delete(self, arg): Delete the task at the given index. try: index = int(arg) if 0 <= index < len(self.tasks): removed_task = self.tasks.pop(index) print(f\'Removed task: {removed_task}\') else: print(\'Invalid index\') except ValueError: print(\'Usage: delete <index>\') def do_backup(self, arg): Save all current tasks to the specified file. try: with open(arg, \'w\') as f: for task in self.tasks: f.write(task + \'n\') print(f\'Tasks backed up to {arg}\') except Exception as e: print(f\'Error backing up tasks: {e}\') def do_restore(self, arg): Load tasks from the specified file. try: if os.path.exists(arg): with open(arg, \'r\') as f: self.tasks = [line.strip() for line in f.readlines()] print(f\'Tasks restored from {arg}\') else: print(f\'File not found: {arg}\') except Exception as e: print(f\'Error restoring tasks: {e}\') def do_exit(self, arg): Exit the task manager. print(\'Exiting Task Manager. Goodbye!\') return True if __name__ == \'__main__\': TaskManager().cmdloop(intro=\'Welcome to Task Manager. Type help or ? to list commands.\')"},{"question":"Working with Nullable Integer Arrays in pandas You are provided with a DataFrame containing integer data along with some missing values. The missing values are represented as `None`. Your task is to: 1. Create a function `convert_to_nullable_int(df: pd.DataFrame, column: str) -> pd.DataFrame` that: - Takes a DataFrame and the name of the column with integer data and missing values. - Converts the specified column to a nullable integer type using `pd.Int64Dtype()`. - Returns the updated DataFrame with the specified column converted. 2. Create a function `perform_operations(df: pd.DataFrame, column: str) -> Dict[str, pd.Series]` that: - Takes the DataFrame and the name of the column (now of nullable integer type). - Performs the following operations and stores the results in a dictionary: - Add 10 to each element of the column. - Check which elements are equal to a specific value (e.g., 20). - Slice the column from index 1 to the end. - Sum the values in the column, ignoring missing values. - Returns a dictionary with keys `\'addition\'`, `\'comparison\'`, `\'slicing\'`, and `\'sum\'` and their respective results as pandas Series (or a single value for the sum). Constraints: - You may assume the input DataFrame and column will always be valid. - Use the pandas library and its functionality to perform the tasks. - Performance is not a primary concern, but aim to use efficient pandas operations. # Example: ```python import pandas as pd # Define your functions here # Sample DataFrame data = { \'integers\': [1, 2, None, 4, None, 6] } df = pd.DataFrame(data) # Convert to nullable integer df = convert_to_nullable_int(df, \'integers\') # Perform operations results = perform_operations(df, \'integers\') print(results[\'addition\']) # Output: # 0 11 # 1 12 # 2 <NA> # 3 14 # 4 <NA> # 5 16 # Name: integers, dtype: Int64 print(results[\'comparison\']) # Output: # 0 False # 1 False # 2 <NA> # 3 False # 4 <NA> # 5 False # Name: integers, dtype: boolean print(results[\'slicing\']) # Output: # 1 2 # 2 <NA> # 3 4 # 4 <NA> # 5 6 # Name: integers, dtype: Int64 print(results[\'sum\']) # Output: # 13 ```","solution":"import pandas as pd from typing import Dict def convert_to_nullable_int(df: pd.DataFrame, column: str) -> pd.DataFrame: df[column] = df[column].astype(pd.Int64Dtype()) return df def perform_operations(df: pd.DataFrame, column: str) -> Dict[str, pd.Series]: results = {} results[\'addition\'] = df[column] + 10 results[\'comparison\'] = df[column] == 20 results[\'slicing\'] = df[column].iloc[1:] results[\'sum\'] = df[column].sum(skipna=True) return results"},{"question":"# Gaussian Naive Bayes Classification Objective: Implement a classification task using the Gaussian Naive Bayes algorithm from the scikit-learn library. You are provided with the Iris dataset, and your goal is to train a model using this dataset and evaluate its performance. Dataset: Iris dataset  a well-known dataset that contains 150 samples of iris flowers, each described by four features: sepal length, sepal width, petal length, and petal width. There are three classes that the flowers belong to: Setosa, Versicolour, and Virginica. Task: 1. **Load the Iris Dataset**: - Use `sklearn.datasets.load_iris` to load the dataset. 2. **Split the Dataset**: - Split the dataset into training and testing sets (70% training, 30% testing) using `train_test_split` from `sklearn.model_selection`. 3. **Train a Gaussian Naive Bayes Classifier**: - Initialize a `GaussianNB` classifier and train it on the training dataset. 4. **Make Predictions and Evaluate**: - Use the trained classifier to predict the classes on the testing set. - Calculate the accuracy of the classifier. - Print the misclassified samples count and the accuracy score. 5. **Confusion Matrix and Classification Report** (Bonus): - Generate and print the confusion matrix and classification report for better insight into classifier performance. Constraints: - Use `train_test_split` with `random_state=42` for reproducibility. - Do not use any other datasets or features for this task. # Implementation: You need to implement the function `gaussian_naive_bayes_iris_classification()` that performs the above steps. Input: - None (The Iris dataset is utilized directly within the function) Output: - Print the following: - Number of mislabeled points out of the total number of points. - Accuracy of the classifier. - **Bonus**: Confusion matrix and classification report. Example Output: ``` Number of mislabeled points out of a total 45 points: 1 Accuracy: 0.9777777777777777 Confusion Matrix: [[16 0 0] [ 0 14 1] [ 0 0 14]] Classification Report: precision recall f1-score support Setosa 1.00 1.00 1.00 16 Versicolour 1.00 0.93 0.97 15 Virginica 0.93 1.00 0.97 14 accuracy 0.98 45 macro avg 0.98 0.98 0.98 45 weighted avg 0.98 0.98 0.98 45 ``` # Starter Code: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import confusion_matrix, classification_report, accuracy_score def gaussian_naive_bayes_iris_classification(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train the Gaussian Naive Bayes classifier gnb = GaussianNB() gnb.fit(X_train, y_train) # Predict the test set results y_pred = gnb.predict(X_test) # Calculate the number of mislabeled points mislabel_count = (y_test != y_pred).sum() # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) # Printing the results print(f\\"Number of mislabeled points out of a total {X_test.shape[0]} points : {mislabel_count}\\") print(f\\"Accuracy: {accuracy}\\") # Bonus: confusion matrix and classification report print(\\"Confusion Matrix:\\") cm = confusion_matrix(y_test, y_pred) print(cm) print(\\"Classification Report:\\") cr = classification_report(y_test, y_pred, target_names=iris.target_names) print(cr) # Call the function (for your testing purposes) # gaussian_naive_bayes_iris_classification() ``` Note: The function should not contain any input parameters. It utilizes the Iris dataset internally and should print results as specified.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.metrics import confusion_matrix, classification_report, accuracy_score def gaussian_naive_bayes_iris_classification(): # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train the Gaussian Naive Bayes classifier gnb = GaussianNB() gnb.fit(X_train, y_train) # Predict the test set results y_pred = gnb.predict(X_test) # Calculate the number of mislabeled points mislabel_count = (y_test != y_pred).sum() # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) # Printing the results print(f\\"Number of mislabeled points out of a total {X_test.shape[0]} points : {mislabel_count}\\") print(f\\"Accuracy: {accuracy}\\") # Bonus: confusion matrix and classification report print(\\"Confusion Matrix:\\") cm = confusion_matrix(y_test, y_pred) print(cm) print(\\"Classification Report:\\") cr = classification_report(y_test, y_pred, target_names=iris.target_names) print(cr)"},{"question":"# Question: Implement a Custom Object Wrapper in Python You need to design and implement a custom Python class called `CustomObjectWrapper` that wraps around a given object and provides additional functionality using defined methods from the `PyObject` protocol. Your task is to: 1. Initialize the wrapper with any Python object. 2. Implement methods to dynamically handle attributes and items of the wrapped object. 3. Ensure type checking, comparison, and iteration functionalities are integrated. Requirements 1. **Initialization**: - `__init__(self, obj)`: Initialize the wrapper with an object `obj`. 2. **Attribute Handling**: - `has_attribute(self, attr_name)`: Return `True` if the wrapped object has the attribute `attr_name`, otherwise return `False`. - `get_attribute(self, attr_name)`: Return the value of the attribute `attr_name` from the wrapped object. - `set_attribute(self, attr_name, value)`: Set the attribute `attr_name` to `value` on the wrapped object. - `delete_attribute(self, attr_name)`: Delete the attribute `attr_name` from the wrapped object. 3. **Item Access**: - `get_item(self, key)`: Return the item associated with `key` from the wrapped object, equivalent to `obj[key]`. - `set_item(self, key, value)`: Set the item associated with `key` to `value` on the wrapped object, equivalent to `obj[key] = value`. - `delete_item(self, key)`: Delete the item associated with `key` from the wrapped object, equivalent to `del obj[key]`. 4. **Type Checking**: - `is_instance(self, cls)`: Return `True` if the wrapped object is an instance of `cls`, otherwise return `False`. - `is_subclass(self, cls)`: Return `True` if the wrapped object\'s class is a subclass of `cls`, otherwise return `False`. 5. **Iteration**: - `get_iterator(self)`: Return an iterator for the wrapped object. - `get_async_iterator(self)`: Return an asynchronous iterator for the wrapped object (if applicable). 6. **Comparison**: - Implement rich comparison methods for the `CustomObjectWrapper` that compare based on the wrapped object\'s comparison logic. Support the following: - `__lt__` - `__le__` - `__eq__` - `__ne__` - `__gt__` - `__ge__` Example Usage ```python wrapped_list = CustomObjectWrapper([1, 2, 3]) print(wrapped_list.get_item(0)) # Should output: 1 wrapped_dict = CustomObjectWrapper({\'key\': \'value\'}) wrapped_dict.set_item(\'new_key\', \'new_value\') print(wrapped_dict.get_item(\'new_key\')) # Should output: \'new_value\' class MyClass: def __init__(self): self.value = 10 wrapped_object = CustomObjectWrapper(MyClass()) print(wrapped_object.get_attribute(\'value\')) # Should output: 10 # Checking type/subclass print(wrapped_object.is_instance(MyClass)) # Should output: True print(wrapped_object.is_subclass(MyClass)) # Should output: True # Comparisons wrapped_integer1 = CustomObjectWrapper(5) wrapped_integer2 = CustomObjectWrapper(10) print(wrapped_integer1 < wrapped_integer2) # Should output: True print(wrapped_integer1 == wrapped_integer1) # Should output: True # Iteration for item in wrapped_list.get_iterator(): print(item) # Should output items in the list: 1, 2, 3 ``` Constraints - Ensure appropriate handling of errors and exceptions as per the functionality provided by the `PyObject` methods described in the documentation. - The wrapped object can be of any type (list, dict, object, etc.), and you should cover general cases in your implementation.","solution":"class CustomObjectWrapper: def __init__(self, obj): self.obj = obj # Attribute handling def has_attribute(self, attr_name): return hasattr(self.obj, attr_name) def get_attribute(self, attr_name): return getattr(self.obj, attr_name) def set_attribute(self, attr_name, value): setattr(self.obj, attr_name, value) def delete_attribute(self, attr_name): delattr(self.obj, attr_name) # Item handling def get_item(self, key): return self.obj[key] def set_item(self, key, value): self.obj[key] = value def delete_item(self, key): del self.obj[key] # Type checking def is_instance(self, cls): return isinstance(self.obj, cls) def is_subclass(self, cls): return issubclass(type(self.obj), cls) # Comparison handling def __lt__(self, other): return self.obj < other.obj def __le__(self, other): return self.obj <= other.obj def __eq__(self, other): return self.obj == other.obj def __ne__(self, other): return self.obj != other.obj def __gt__(self, other): return self.obj > other.obj def __ge__(self, other): return self.obj >= other.obj # Iteration def get_iterator(self): return iter(self.obj) async def async_iter_helper(self): for item in self.obj: yield item def get_async_iterator(self): return self.async_iter_helper()"},{"question":"# Question: Understanding and Manipulating Tensor Dimensions You are provided with a 3-dimensional PyTorch tensor. Your task is to implement a function that: 1. Takes a 3-dimensional tensor as input. 2. Returns a new tensor, where the dimensions are permuted (reordered) such that the second dimension becomes the first, the third dimension becomes the second, and the first dimension becomes the third. The function signature is as follows: ```python import torch def permute_tensor_dimensions(input_tensor: torch.Tensor) -> torch.Tensor: Reorder the dimensions of the input 3-dimensional tensor. :param input_tensor: A 3-dimensional PyTorch tensor :return: A new 3-dimensional PyTorch tensor with permuted dimensions Note: Assume input_tensor has a size of (D1, D2, D3) The output should have a size of (D2, D3, D1) pass ``` # Constraints: 1. The input tensor is guaranteed to be a 3-dimensional tensor. 2. You should not use any explicit loops. 3. Use the PyTorch operations and functionalities effectively. # Example: ```python x = torch.ones(3, 4, 5) # Original size: torch.Size([3, 4, 5]) result = permute_tensor_dimensions(x) # The size of the result should be torch.Size([4, 5, 3]) print(result.size()) # should output torch.Size([4, 5, 3]) ``` # Implementation Requirements: - **Input**: A PyTorch tensor of size `(D1, D2, D3)`. - **Output**: A new PyTorch tensor of size `(D2, D3, D1)`. The goal of this task is to test the student\'s understanding of tensor dimensions and their ability to manipulate them using PyTorch\'s built-in functions.","solution":"import torch def permute_tensor_dimensions(input_tensor: torch.Tensor) -> torch.Tensor: Reorder the dimensions of the input 3-dimensional tensor. :param input_tensor: A 3-dimensional PyTorch tensor :return: A new 3-dimensional PyTorch tensor with permuted dimensions Note: Assume input_tensor has a size of (D1, D2, D3) The output should have a size of (D2, D3, D1) return input_tensor.permute(1, 2, 0)"},{"question":"You are tasked with implementing a function to manage large datasets of numeric values efficiently using the `array` module in Python. # Function Specification Function Name `process_numeric_array` Parameters - `data_list` (List[int]): A list of integers to initialize the array. - `value` (int): A value to insert into the array. - `index` (int): The position to insert the value. - `swap_bytes` (bool): A boolean indicating whether to perform a byteswap operation on the array. Returns - `modified_list` (List[int]): A list of integers representing the final state of the array after all operations. Function Description 1. **Initialization**: - Create an array of signed integers (type code `\'i\'`) initialized with the values from `data_list`. 2. **Insertion**: - Insert the `value` at the specified `index` in the array. If the `index` is negative or greater than the length of the array, insert the value at the correct position relative to the end or append it to the array, respectively. 3. **Byteswap**: - If `swap_bytes` is `True`, perform a byteswap on the array. 4. **Conversion**: - Convert the final array back to a list of integers and return it. # Constraints - The list `data_list` will contain between 1 and 10^6 integers, each between -2^31 and 2^31-1. - The `index` will be between -10^6 and 10^6. - The function must be efficient in handling large datasets and performing the operations within a reasonable time frame. # Example ```python from array import array def process_numeric_array(data_list: List[int], value: int, index: int, swap_bytes: bool) -> List[int]: # Step 1: Initialize the array arr = array(\'i\', data_list) # Step 2: Insert the value if index < 0: index = max(0, len(arr) + index) elif index >= len(arr): index = len(arr) arr.insert(index, value) # Step 3: Perform byteswap if required if swap_bytes: arr.byteswap() # Step 4: Convert array back to list return arr.tolist() # Example usage: data_list = [1, 2, 3, 4, 5] value = 10 index = 2 swap_bytes = True print(process_numeric_array(data_list, value, index, swap_bytes)) # Output: The modified list of integers after insertion and possible byteswap. ``` This problem tests the understanding and application of the `array` module\'s functionalities for managing and manipulating collections of numeric data efficiently.","solution":"from array import array from typing import List def process_numeric_array(data_list: List[int], value: int, index: int, swap_bytes: bool) -> List[int]: # Step 1: Initialize the array arr = array(\'i\', data_list) # Step 2: Insert the value if index < 0: index = max(0, len(arr) + index) elif index >= len(arr): index = len(arr) arr.insert(index, value) # Step 3: Perform byteswap if required if swap_bytes: arr.byteswap() # Step 4: Convert array back to list return arr.tolist()"},{"question":"You are given a text file that contains a collection of logs for a hypothetical web server. Each line in the log file follows the format: ``` <timestamp> <client_ip> <request_type> <resource> <protocol> <response_code> <response_time> ``` Example: ``` 2023-01-01T00:00:00Z 192.168.1.1 GET /index.html HTTP/1.1 200 123 ``` Your task is to write a Python function `process_logs(filepath)` that processes this log file and performs the following tasks: 1. **Summarize** the total number of requests made. 2. **Identify** the top 5 resources that were requested. 3. **Calculate** the average response time for each type of request (GET, POST, etc.). # Input - `filepath`: A string representing the path to the log file. # Output The function should return a tuple `(total_requests, top_resources, avg_response_times)` where: - `total_requests` is an integer representing the total number of requests. - `top_resources` is a list of the top 5 requested resources in descending order of their request count. - `avg_response_times` is a dictionary where the keys are request types (e.g., \'GET\', \'POST\') and the values are the average response times as floats. # Constraints - The log file may be very large, so the solution should handle large files efficiently. - If there are fewer than 5 resources, return all of them in the `top_resources` list. - The response time is in milliseconds. # Example Given the log file contents: ``` 2023-01-01T00:00:00Z 192.168.1.1 GET /index.html HTTP/1.1 200 123 2023-01-01T00:00:01Z 192.168.1.2 POST /submit-data HTTP/1.1 201 567 2023-01-01T00:00:02Z 192.168.1.3 GET /index.html HTTP/1.1 200 111 2023-01-01T00:00:03Z 192.168.1.4 GET /home HTTP/1.1 404 234 ``` Your function might return: ```python (4, [\'/index.html\', \'/submit-data\', \'/home\'], {\'GET\': 156.0, \'POST\': 567.0}) ``` # Notes - Use the `collections` module for counting and processing. - Use the `heapq` module to efficiently determine the top 5 resources. - Ensure your code is optimized for performance and can handle edge cases gracefully. # Function Signature ```python def process_logs(filepath: str) -> Tuple[int, List[str], Dict[str, float]]: # Your code goes here ```","solution":"import collections import heapq from typing import List, Dict, Tuple def process_logs(filepath: str) -> Tuple[int, List[str], Dict[str, float]]: resource_count = collections.Counter() request_times = collections.defaultdict(list) total_requests = 0 with open(filepath, \'r\') as file: for line in file: parts = line.split() if len(parts) != 7: continue # skip any malformed lines total_requests += 1 _timestamp, _client_ip, request_type, resource, _protocol, _response_code, response_time = parts response_time = int(response_time) resource_count[resource] += 1 request_times[request_type].append(response_time) top_resources = [resource for resource, _ in resource_count.most_common(5)] avg_response_times = {request_type: sum(times) / len(times) for request_type, times in request_times.items()} return total_requests, top_resources, avg_response_times"},{"question":"Objective You are required to implement a function that simulates a simple lottery game and analyze the outcomes based on statistical methods. Problem Statement In this lottery game, a lottery ticket contains 6 unique random numbers ranging between 1 and 49 (both inclusive). The winning ticket is also generated using the same random number generation method. You will implement a function `run_lottery_simulation(n)` that simulates the lottery game `n` times and computes the following: 1. The average number of matches per ticket. 2. The probability of having exactly 3 matches in a ticket. 3. The probability of having 5 or more matches in a ticket. 4. The proportion of tickets with no matches. Function Signature ```python def run_lottery_simulation(n: int) -> dict: # your implementation here pass ``` Input - `n` (int): The number of lottery simulations (1 <= n <= 1,000,000). Output - A dictionary containing the following keys and their corresponding computed values: - `\\"avg_matches\\"`: The average number of matches per ticket. - `\\"prob_exactly_3\\"`: The probability of having exactly 3 matches in a ticket. - `\\"prob_5_or_more\\"`: The probability of having 5 or more matches in a ticket. - `\\"prop_no_matches\\"`: The proportion of tickets with no matches. Constraints - The numbers on the lottery ticket and the winning ticket must be unique and within the range 1 to 49. Example ```python result = run_lottery_simulation(10000) print(result) # Expected Output (example): # { # \\"avg_matches\\": 0.122, # \\"prob_exactly_3\\": 0.012, # \\"prob_5_or_more\\": 0.00005, # \\"prop_no_matches\\": 0.878 # } ``` Notes - Use the `random` module for number generation. - The results may vary slightly each time you run the function due to the pseudo-random nature of the `random` module. - Ensure your implementation is efficient for large values of `n`. Performance Benchmark The function should handle at least 1,000,000 simulations efficiently in a reasonable amount of time. **Make sure to handle edge cases and validate your implementation with various values of `n`.** Good luck!","solution":"import random def run_lottery_simulation(n: int) -> dict: Simulates n lottery games and computes the required statistics. Parameters: n (int): Number of simulations Returns: dict: Dictionary containing average number of matches, probability of exactly 3 matches, probability of 5 or more matches, and proportion of tickets with no matches. total_matches = 0 count_exactly_3 = 0 count_5_or_more = 0 count_no_matches = 0 for _ in range(n): ticket = set(random.sample(range(1, 50), 6)) winning_ticket = set(random.sample(range(1, 50), 6)) matches = len(ticket.intersection(winning_ticket)) total_matches += matches if matches == 3: count_exactly_3 += 1 if matches >= 5: count_5_or_more += 1 if matches == 0: count_no_matches += 1 avg_matches = total_matches / n prob_exactly_3 = count_exactly_3 / n prob_5_or_more = count_5_or_more / n prop_no_matches = count_no_matches / n return { \\"avg_matches\\": avg_matches, \\"prob_exactly_3\\": prob_exactly_3, \\"prob_5_or_more\\": prob_5_or_more, \\"prop_no_matches\\": prop_no_matches }"},{"question":"# Context Management and Context Variable Handling in Python Problem Statement You are tasked with implementing a small Python library that mimics some of the behavior of Python 3.7+ `contextvars` module using the described API functionalities. Objectives 1. Create a new context. 2. Define a new context variable with a default value. 3. Set a value for the context variable in the current context. 4. Retrieve the value of the context variable. 5. Reset the context variable to its previous state. Function Specifications 1. `create_context()`: - **Output**: Returns a new context object. 2. `create_context_var(name, default_value)`: - **Parameters**: - `name` (str): The name for the context variable. - `default_value` (Any): The default value for the context variable. - **Output**: Returns a new context variable object. 3. `set_context_var(ctx_var, value)`: - **Parameters**: - `ctx_var` (context variable object): A context variable object. - `value` (Any): The value to be set for the context variable. - **Output**: A token object that can be used to reset the context variable. 4. `get_context_var(ctx_var, default_value=None)`: - **Parameters**: - `ctx_var` (context variable object): A context variable object. - `default_value` (Any, optional): A default value if the context variable is not set. - **Output**: The value of the context variable or the `default_value` if not set. 5. `reset_context_var(ctx_var, token)`: - **Parameters**: - `ctx_var` (context variable object): A context variable object. - `token` (token object): A token object representing the state to reset to. - **Output**: None. The context variable is reset to its previous state. Constraints - Utilize the provided API functions appropriately within your implementations. - Handle any possible errors by returning appropriate messages or values. - Ensure your functions are self-contained and do not rely on any external state beyond the parameters provided. Example Usage ```python # Creating a new context ctx = create_context() # Creating a new context variable with a default value of 10 ctx_var = create_context_var(\\"example_var\\", 10) # Setting the value of the context variable to 20 token = set_context_var(ctx_var, 20) # Getting the value of the context variable (should be 20) value = get_context_var(ctx_var) print(value) # Output: 20 # Resetting the context variable to its previous state reset_context_var(ctx_var, token) # Getting the value of the context variable (should be 10, the default) value = get_context_var(ctx_var) print(value) # Output: 10 ``` Implement these functions to correctly manage your context and context variables.","solution":"class Context: def __init__(self): self.variables = {} class ContextVar: def __init__(self, name, default_value): self.name = name self.default_value = default_value class Token: def __init__(self, ctx_var, value): self.ctx_var = ctx_var self.value = value def create_context(): Creates a new context. :return: - a new context object return Context() def create_context_var(name, default_value): Creates a new context variable. :param name: The name of the context variable :param default_value: The default value of the context variable :return: - a new context variable object return ContextVar(name, default_value) def set_context_var(ctx, ctx_var, value): Sets a value for the context variable in the current context. :param ctx: - a context object :param ctx_var: - a context variable object :param value: The value to be set for the context variable :return: - a token representing the previous state of the context variable old_value = ctx.variables.get(ctx_var.name, ctx_var.default_value) ctx.variables[ctx_var.name] = value return Token(ctx_var, old_value) def get_context_var(ctx, ctx_var, default_value=None): Retrieves the value of the context variable. :param ctx: - a context object :param ctx_var: - a context variable object :param default_value: A default value if the context variable is not set :return: - the value of the context variable or default value if not set return ctx.variables.get(ctx_var.name, default_value if default_value is not None else ctx_var.default_value) def reset_context_var(ctx, ctx_var, token): Resets the context variable to its previous state. :param ctx: - a context object :param ctx_var: - a context variable object :param token: - a token representing the previous state of the context variable :return: None ctx.variables[ctx_var.name] = token.value"},{"question":"# Advanced Python Logging Configuration Objective: To assess the understanding and practical application of Python\'s `logging` module in creating a robust and dynamic logging configuration. Task: Implement a Python script that configures a logging system with the following requirements: 1. **Configuration Using a Dictionary**: - Set up a logging configuration using a dictionary passed to `logging.config.dictConfig()`. - Include at least two loggers: `application` and `library`. 2. **Multiple Handlers and Formatters**: - Create one file handler that logs all messages (INFO level and above) to a file named `app.log` with timestamps. - Create a console handler that logs WARNING level messages and above to the console without timestamps. - Utilize distinct formatters for the file and console handlers. 3. **Custom Filters**: - Implement a custom filter named `ErrorFilter()` that only allows ERROR level messages. Use this filter to add another handler that logs error messages to a file named `error.log`. 4. **Contextual Information**: - Add contextual information (like username and IP address) to the log messages using a custom `LoggerAdapter`. 5. **Demonstrate Logging**: - Create two logging instances, one for the `application` logger and one for the `library` logger. Use these loggers to log messages of various levels. - The script should demonstrate logging from multiple threads. - Messages should include contextual information for at least some log entries. Constraints: - Ensure that the log messages are appropriately formatted as per the handler configurations. - The script should handle logging from multiple threads without issues. - Provide appropriate docstrings and comments in your code to explain the configuration and usage. Example Skeleton: ```python import logging import logging.config import logging.handlers import threading # Define custom filter class ErrorFilter(logging.Filter): def filter(self, record): return record.levelno == logging.ERROR # Define a function to run in threads for logging def log_messages(logger, context): adapter = logging.LoggerAdapter(logger, context) adapter.info(\'This is an info message\') adapter.warning(\'This is a warning message\') adapter.error(\'This is an error message\') # Example logging configuration LOGGING_CONFIG = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'file\': {\'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'}, \'console\': {\'format\': \'%(levelname)s - %(message)s\'}, }, \'filters\': { \'error_filter\': {\'()\': ErrorFilter}, }, \'handlers\': { \'file_handler\': { \'class\': \'logging.FileHandler\', \'filename\': \'app.log\', \'formatter\': \'file\', \'level\': \'INFO\', }, \'console_handler\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'console\', \'level\': \'WARNING\', }, \'error_handler\': { \'class\': \'logging.FileHandler\', \'filename\': \'error.log\', \'formatter\': \'file\', \'level\': \'ERROR\', \'filters\': [\'error_filter\'], }, }, \'loggers\': { \'application\': { \'handlers\': [\'file_handler\', \'console_handler\', \'error_handler\'], \'level\': \'DEBUG\', \'propagate\': False, }, \'library\': { \'handlers\': [\'file_handler\', \'console_handler\', \'error_handler\'], \'level\': \'DEBUG\', \'propagate\': False, }, }, } # Apply logging configuration logging.config.dictConfig(LOGGING_CONFIG) # Create loggers app_logger = logging.getLogger(\'application\') lib_logger = logging.getLogger(\'library\') # Contextual information context_info = {\'username\': \'user1\', \'ip\': \'192.168.1.1\'} # Create threads for logging threads = [] for logger in [app_logger, lib_logger]: t = threading.Thread(target=log_messages, args=(logger, context_info)) threads.append(t) t.start() for t in threads: t.join() print(\\"Logging complete\\") ``` Submission: - Please submit your complete Python script as a `.py` file. - Ensure that your script is well-documented and includes instructions on how to run it.","solution":"import logging import logging.config import logging.handlers import threading # Define custom filter class ErrorFilter(logging.Filter): def filter(self, record): return record.levelno == logging.ERROR # Define a function to run in threads for logging def log_messages(logger, context): adapter = logging.LoggerAdapter(logger, context) adapter.info(\'This is an info message\') adapter.warning(\'This is a warning message\') adapter.error(\'This is an error message\') # Example logging configuration LOGGING_CONFIG = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'file\': {\'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'}, \'console\': {\'format\': \'%(levelname)s - %(message)s\'}, }, \'filters\': { \'error_filter\': {\'()\': ErrorFilter}, }, \'handlers\': { \'file_handler\': { \'class\': \'logging.FileHandler\', \'filename\': \'app.log\', \'formatter\': \'file\', \'level\': \'INFO\', }, \'console_handler\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'console\', \'level\': \'WARNING\', }, \'error_handler\': { \'class\': \'logging.FileHandler\', \'filename\': \'error.log\', \'formatter\': \'file\', \'level\': \'ERROR\', \'filters\': [\'error_filter\'], }, }, \'loggers\': { \'application\': { \'handlers\': [\'file_handler\', \'console_handler\', \'error_handler\'], \'level\': \'DEBUG\', \'propagate\': False, }, \'library\': { \'handlers\': [\'file_handler\', \'console_handler\', \'error_handler\'], \'level\': \'DEBUG\', \'propagate\': False, }, }, } # Apply logging configuration logging.config.dictConfig(LOGGING_CONFIG) # Create loggers app_logger = logging.getLogger(\'application\') lib_logger = logging.getLogger(\'library\') # Contextual information context_info = {\'username\': \'user1\', \'ip\': \'192.168.1.1\'} # Create threads for logging threads = [] for logger in [app_logger, lib_logger]: t = threading.Thread(target=log_messages, args=(logger, context_info)) threads.append(t) t.start() for t in threads: t.join() print(\\"Logging complete\\")"},{"question":"# Email Message Processing and MIME Generation Objective In this coding assessment, you are required to implement functionalities for processing and generating email messages using the `email` package in Python. This task assesses your understanding of common email handling tasks, including parsing, modifying, and generating email content. Problem Statement You are provided with a raw email message as a string. Your task is to write a function that performs the following operations: 1. **Parse the raw email message** to extract the subject, sender\'s email address, recipient\'s email address, and the body of the email. 2. **Modify the email body** by appending a predefined disclaimer text. 3. **Generate a new MIME email message** with the modified body and the same subject, sender, and recipient. 4. **Return the newly generated email message as a string.** Function Signature ```python def process_email(raw_email: str, disclaimer: str) -> str: pass ``` Input - `raw_email` (str): A string representing the raw email message. - `disclaimer` (str): A string containing the text that should be appended to the email body as a disclaimer. Output - Returns a string of the newly generated email message with the appended disclaimer. Constraints - The raw email message will always contain valid headers including `Subject`, `From`, and `To`. - The email body will be in plain text format. - You can assume that the email body will not be empty. Example Usage ```python raw_email = From: sender@example.com To: recipient@example.com Subject: Test Email This is a test email. disclaimer = \\"nnThis message is confidential.\\" new_email = process_email(raw_email, disclaimer) print(new_email) ``` Expected Output ``` From: sender@example.com To: recipient@example.com Subject: Test Email This is a test email. This message is confidential. ``` Hints - Utilize the `email.message` and `email.parser` modules to parse and create email messages. - The `email.message.Message` class will be helpful for representing and creating email content. - Ensure to handle the MIME type properly while generating the new email message.","solution":"from email import message_from_string from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.policy import default def process_email(raw_email: str, disclaimer: str) -> str: # Parse the raw email message msg = message_from_string(raw_email, policy=default) # Extract details subject = msg[\'Subject\'] from_addr = msg[\'From\'] to_addr = msg[\'To\'] body = msg.get_body(preferencelist=(\'plain\')).get_content() # Modify the email body modified_body = body + disclaimer # Generate new MIME email message new_msg = MIMEMultipart() new_msg[\'Subject\'] = subject new_msg[\'From\'] = from_addr new_msg[\'To\'] = to_addr new_msg.attach(MIMEText(modified_body, \\"plain\\")) # Return the newly generated email message as a string return new_msg.as_string()"},{"question":"You have been tasked with creating a safer and more user-friendly exception handler for a web application written in Python using the `cgitb` module. Your handler needs to provide detailed error reports both to the web browser for immediate debugging and to a log file for later analysis. # Problem Statement Implement a function `setup_exception_handler(log_directory, display_to_browser=True, lines_of_context=5, output_format=\'html\')` that configures the `cgitb` module to handle uncaught exceptions with the following specifications: 1. **log_directory**: A string representing the path to the directory where the exception logs should be stored. Ensure that if this directory does not exist, your function creates it. 2. **display_to_browser**: A boolean indicating whether or not to display the traceback to the browser. Default is `True`. 3. **lines_of_context**: An integer representing the number of lines of context to show around the source code where the exception occurred. Default is `5`. 4. **output_format**: A string representing the format of the traceback output, either `\'html\'` or `\'text\'`. Default is `\'html\'`. # Requirements - Before enabling the `cgitb` module: - Validate that the `log_directory` is a valid directory path. If not, create it. - Ensure `lines_of_context` is a positive integer. - Ensure `output_format` is either `\'html\'` or `\'text\'`. - Use the `cgitb.enable` function with appropriate parameters to meet the above requirements. # Example ```python log_directory = \\"/path/to/log/directory\\" display_to_browser = True lines_of_context = 5 output_format = \'html\' setup_exception_handler(log_directory, display_to_browser, lines_of_context, output_format) ``` In this example, if an unhandled exception occurs, it would display the traceback in the browser and also log it to the specified directory with 5 lines of context around each traceback entry, formatted in HTML. # Constraints - The `log_directory` path must be a valid path without any special characters or spaces. - `lines_of_context` must be more than 0. - The `output_format` must be either `\'html\'` or `\'text\'`. # Performance The overall performance should not be significantly impacted by the traceback handling. Ensure that the file operations and validations do not introduce considerable overhead.","solution":"import os import cgitb def setup_exception_handler(log_directory, display_to_browser=True, lines_of_context=5, output_format=\'html\'): Sets up an exception handler using the cgitb module. Parameters: - log_directory: str, path to the directory where the exception logs should be stored. - display_to_browser: bool, indicates whether to display the traceback to the browser. - lines_of_context: int, number of lines of context to show around the source code where the exception occurred. - output_format: str, format of the traceback output, either \'html\' or \'text\'. # Validate log_directory if not os.path.exists(log_directory): os.makedirs(log_directory) # Validate lines_of_context if not isinstance(lines_of_context, int) or lines_of_context <= 0: raise ValueError(\\"lines_of_context must be a positive integer\\") # Validate output_format if output_format not in [\'html\', \'text\']: raise ValueError(\\"output_format must be \'html\' or \'text\'\\") # Enabling cgitb exception handling cgitb.enable(display=display_to_browser, logdir=log_directory, context=lines_of_context, format=output_format)"},{"question":"**Objective:** Implement a simple reference counting mechanism in Python to simulate the behavior of the reference management macros you have studied. **Task:** You are required to create a class `ObjectRefCounter` to manage reference counts of objects. Your class should include methods to increment and decrement reference counts, as well as handle the deletion of objects when their reference count reaches zero. **Specifications:** 1. **Class: `ObjectRefCounter`** - **Attributes:** - `obj`: [Any] The actual object being referenced. - `ref_count`: [int] The count of strong references to the object. - **Methods:** - `__init__(self, obj)`: Initialize the object with one reference. - `incref(self)`: Increment the reference count. - `decref(self)`: Decrement the reference count. If it reaches zero, delete the object. - `get_reference_count(self)`: Return the current reference count. **Constraints:** - The `obj` attribute can be any type of Python object. - You must ensure no memory leaks by properly handling the reference counts. **Example Usage:** ```python # Create a new object and manage its references ref_manager = ObjectRefCounter(\\"example\\") print(ref_manager.get_reference_count()) # Should output 1 # Increase reference count ref_manager.incref() print(ref_manager.get_reference_count()) # Should output 2 # Decrease reference count ref_manager.decref() print(ref_manager.get_reference_count()) # Should output 1 # Decrease reference count to zero, this should delete the object. ref_manager.decref() print(ref_manager.get_reference_count()) # Should output 0 or raise an error if you access the object after deletion ``` Write your implementation of the `ObjectRefCounter` class below: ```python class ObjectRefCounter: def __init__(self, obj): # Initialize the object reference counter here pass def incref(self): # Increment the reference count here pass def decref(self): # Decrement the reference count here and handle deletion if count is zero pass def get_reference_count(self): # Return the current reference count pass ``` Complete this class and ensure it works as described in the example usage.","solution":"class ObjectRefCounter: def __init__(self, obj): self.obj = obj self.ref_count = 1 def incref(self): self.ref_count += 1 def decref(self): if self.ref_count > 0: self.ref_count -= 1 if self.ref_count == 0: del self.obj def get_reference_count(self): return self.ref_count"},{"question":"# Python Coding Assessment Problem Statement You are required to implement a function `garbage_collection_inspector` that interacts with Python\'s garbage collection interface. This function will perform several tasks outlined below and return a comprehensive report. Function Requirements 1. **Disable and Enable Garbage Collection**: - The function should initially disable garbage collection to observe its effects. - It should re-enable garbage collection before concluding. 2. **Collect Garbage**: - Manually trigger a full garbage collection cycle. 3. **Inspect and Modify Thresholds**: - Retrieve the current garbage collection thresholds. - Set new thresholds to custom values `(700, 10, 10)`. - Restore the thresholds to their original values after collecting some statistics. 4. **Object Tracking**: - Create some objects, verify which of them are tracked by the garbage collector using `gc.is_tracked()`, and return the list of tracked objects. 5. **Collect Statistics**: - Before and after manipulating thresholds and running manual collections, gather statistics on garbage collections using `gc.get_stats()`. 6. **Callback Usage**: - Register a callback that logs the start and stop of garbage collections. 7. **Freezing and Unfreezing**: - Freeze the current objects and verify the number of frozen objects using `gc.get_freeze_count()`. - Unfreeze the objects and confirm they are back to the oldest generation. Function Signature ```python def garbage_collection_inspector(): pass ``` Output The function should return a dictionary containing: - `initial_thresholds`: Tuple of initial garbage collection thresholds. - `tracked_objects_before`: List of objects tracked before any collection. - `tracked_objects_after`: List of objects tracked after collections. - `stats_before`: Garbage collection statistics before threshold modifications. - `stats_after`: Garbage collection statistics after threshold modifications. - `freeze_count`: Number of objects frozen. - `callbacks_log`: A log of callbacks invoked during garbage collection. Constraints - Ensure no memory leaks occur  objects created within the function should be properly managed. - The function should be robust and performant, handling large numbers of objects gracefully. Example ```python report = garbage_collection_inspector() print(report) ``` The output should look something like this (values are illustrative): ```json { \\"initial_thresholds\\": (700, 10, 10), \\"tracked_objects_before\\": [<list of objects>], \\"tracked_objects_after\\": [<list of objects>], \\"stats_before\\": [{\\"collections\\": 10, \\"collected\\": 1200, \\"uncollectable\\": 2}, ...], \\"stats_after\\": [{\\"collections\\": 12, \\"collected\\": 1500, \\"uncollectable\\": 3}, ...], \\"freeze_count\\": 500, \\"callbacks_log\\": [\\"start\\", \\"stop\\", ...] } ``` Implement the function considering the instructions given and the `gc` module\'s functionalities as described.","solution":"import gc def garbage_collection_inspector(): # Disable garbage collection gc.disable() # Store the initial thresholds initial_thresholds = gc.get_threshold() # Collect garbage manually gc.collect() # Get stats before changing thresholds stats_before = gc.get_stats() # Set new thresholds new_thresholds = (700, 10, 10) gc.set_threshold(*new_thresholds) # Create objects for tracking class MyObject: pass obj1 = MyObject() obj2 = MyObject() obj3 = MyObject() # List of objects to track objects = [obj1, obj2, obj3] tracked_objects_before = [obj for obj in objects if gc.is_tracked(obj)] # Manually trigger garbage collection again gc.collect() # Get stats after changing thresholds stats_after = gc.get_stats() # Restore original thresholds gc.set_threshold(*initial_thresholds) tracked_objects_after = [obj for obj in objects if gc.is_tracked(obj)] # Freeze and unfreeze gc.freeze() freeze_count = gc.get_freeze_count() gc.unfreeze() # Setup callback logging callbacks_log = [] def callback(phase, info): callbacks_log.append(phase) # Register callback and run collections gc.callbacks.append(callback) gc.collect() gc.callbacks.remove(callback) # Re-enable garbage collection gc.enable() report = { \\"initial_thresholds\\": initial_thresholds, \\"tracked_objects_before\\": tracked_objects_before, \\"tracked_objects_after\\": tracked_objects_after, \\"stats_before\\": stats_before, \\"stats_after\\": stats_after, \\"freeze_count\\": freeze_count, \\"callbacks_log\\": callbacks_log } return report"},{"question":"**Objective:** Implement a simplified interactive Python shell using the `codeop` module that can read multi-line input, detect if the input completes a Python statement, and execute the statements while handling future imports and syntax errors. **Task:** You are required to implement a function `interactive_shell()` that simulates an interactive Python shell using the `codeop` module. Your shell should: 1. Read user input continuously until a valid Python statement is completed. 2. Execute the valid Python statements while keeping track of any `__future__` statements. 3. Handle and report syntax errors or other compilation errors explicitly. 4. Stop execution when an `exit()` command is entered. **Function Signature:** ```python def interactive_shell(): pass ``` **Requirements:** 1. The function should continuously prompt the user for input until an `exit()` command is received. 2. The function should accumulate multi-line statements until they form a complete Python statement. 3. Use `codeop.compile_command()` to check if the input is a complete statement. 4. Execute the complete statement using `exec()` if it is valid. 5. Handle `SyntaxError`, `OverflowError`, and `ValueError` by printing an appropriate error message without stopping the shell. 6. Remember and handle `__future__` imports using `codeop.CommandCompiler`. **Example:** ```python >>> interactive_shell() >>> print(\\"Hello, World!\\") Hello, World! >>> x = 10 >>> y = 20 >>> z = x + y >>> z 30 >>> from __future__ import division >>> 3 / 2 1.5 >>> exit() ``` **Constraints:** - The function should handle multi-line statements properly. - Keep the shell running until `exit()` is explicitly called by the user. **Hints:** - You may need to use a loop to keep reading input until a complete statement is detected. - Make use of `codeop.CommandCompiler` class to handle `__future__` imports properly.","solution":"import codeop def interactive_shell(): Simulates an interactive Python shell. compiler = codeop.CommandCompiler() buffer = \\"\\" while True: try: # Prompt user for input line = input(\\">>> \\" if buffer == \\"\\" else \\"... \\") buffer += line + \\"n\\" # Try to compile the accumulated buffer code = compiler(buffer) if code is not None: # If compilation was successful, execute the code if buffer.strip() == \\"exit()\\": break exec(code) buffer = \\"\\" except (SyntaxError, OverflowError, ValueError) as e: # Catch and print any compilation or execution errors print(f\\"Error: {e}\\") buffer = \\"\\" # Note: interactive_shell is designed to be run manually and doesn\'t return any value # so it can\'t be tested automatically in the traditional sense"},{"question":"# Custom Event Loop Policy and Child Process Watcher in Asyncio **Objective:** Your task is to create a custom asyncio event loop policy that logs every time a new event loop is created. Additionally, you need to use a specific child process watcher for this policy. **Requirements:** 1. Create a class `LoggingEventLoopPolicy` that extends `asyncio.DefaultEventLoopPolicy`. 2. Override the `new_event_loop()` method to log a message using `print()` every time a new event loop is created. 3. Create a function called `use_custom_policy()` that: - Configures `LoggingEventLoopPolicy` as the current event loop policy. - Sets `FastChildWatcher` as the child process watcher for this policy. 4. You should demonstrate the usage by: - Setting the custom policy. - Creating a new event loop. **Function Signatures:** ```python import asyncio class LoggingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): ... def use_custom_policy(): ... ``` **Expected Functionality:** - When calling `use_custom_policy()`, it should set the `LoggingEventLoopPolicy` as the event loop policy and `FastChildWatcher` as the child process watcher. - When a new event loop is created using `asyncio.new_event_loop()`, it should log a message indicating the creation of the event loop. **Constraints:** - Do not use third-party libraries except asyncio. - Ensure that logging happens through simple `print()` statement. **Example:** ```python # Assuming the code implementation is correct, the following code use_custom_policy() loop = asyncio.new_event_loop() # Should print: # \\"New event loop created\\" ``` Make sure your implementation adheres to the requirements and demonstrates usage as indicated in the functionality expectations.","solution":"import asyncio class LoggingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def new_event_loop(self): loop = super().new_event_loop() print(\\"New event loop created\\") return loop def use_custom_policy(): asyncio.set_event_loop_policy(LoggingEventLoopPolicy()) asyncio.get_event_loop_policy().set_child_watcher(asyncio.FastChildWatcher())"},{"question":"**Question: Implementing and Testing a Custom Attention Mechanism in PyTorch** You are required to implement a custom attention mechanism using the PyTorch framework. Attention mechanisms allow neural networks to focus on specific parts of the input sequence, which is particularly useful in tasks like machine translation, text summarization, and more. Your task is to implement an experimental self-attention module and test it within a simple sequence-to-sequence model. Given the experimental nature, you should follow a structured approach to define the attention mechanism and integrate it into a sequence-to-sequence model. # Requirements 1. **Define the Attention Mechanism:** Implement a custom self-attention module that computes attention scores and applies these scores to the values. The attention mechanism should support batch processing. 2. **Integrate with Sequence Model:** Integrate your attention mechanism into a simple encoder-decoder sequence-to-sequence model. 3. **Input and Output Formats:** - **Input:** A batch of sequences with shape `(batch_size, seq_length, input_dim)`. - **Output:** A batch of transformed sequences with the same shape as input. # Constraints 1. The implementation must use PyTorch and the provided `torch.nn.functional` module where necessary. 2. Ensure that your attention mechanism and sequence model support GPU acceleration by moving tensors to the appropriate device. # Performance Requirements The custom attention mechanism should efficiently handle a batch size of 32, sequence length of 50, and input dimension of 128 without running into out-of-memory errors on a standard GPU setup. # Implementation Details - **CustomAttention Module:** Define a custom attention module that computes the scaled dot-product attention: ``` Attention(Q, K, V) = Softmax((Q * K^T) / sqrt(d_k)) * V ``` where (Q), (K), and (V) are the query, key, and value matrices, respectively, and (d_k) is the dimension of the keys. - **SequenceModel Class:** Define a class for the sequence model that uses an encoder to produce keys and values, a decoder to produce queries, and applies the custom attention mechanism. # Example Below is an outline to help you get started: ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self, d_k): super(CustomAttention, self).__init__() self.d_k = d_k def forward(self, Q, K, V): # Compute the attention scores scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) attention_weights = F.softmax(scores, dim=-1) # Apply the attention weights to the values output = torch.matmul(attention_weights, V) return output class SequenceModel(nn.Module): def __init__(self, input_dim, hidden_dim, attention_dim): super(SequenceModel, self).__init__() self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True) self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True) self.attention = CustomAttention(attention_dim) self.fc = nn.Linear(hidden_dim, input_dim) def forward(self, x): enc_output, _ = self.encoder(x) dec_output, _ = self.decoder(enc_output) attn_output = self.attention(dec_output, enc_output, enc_output) output = self.fc(attn_output) return output # Test the model batch_size = 32 seq_length = 50 input_dim = 128 hidden_dim = 64 attention_dim = hidden_dim model = SequenceModel(input_dim, hidden_dim, attention_dim) input_data = torch.randn(batch_size, seq_length, input_dim) output_data = model(input_data) print(\\"Output shape:\\", output_data.shape) # Expected: (batch_size, seq_length, input_dim) ``` Complete the implementation and ensure the model operates efficiently on a GPU if available.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self, d_k): super(CustomAttention, self).__init__() self.d_k = d_k def forward(self, Q, K, V): # Compute the attention scores scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) attention_weights = F.softmax(scores, dim=-1) # Apply the attention weights to the values output = torch.matmul(attention_weights, V) return output class SequenceModel(nn.Module): def __init__(self, input_dim, hidden_dim, attention_dim): super(SequenceModel, self).__init__() self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True) self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True) self.attention = CustomAttention(attention_dim) self.fc = nn.Linear(hidden_dim, input_dim) def forward(self, x): enc_output, _ = self.encoder(x) dec_output, _ = self.decoder(enc_output) attn_output = self.attention(dec_output, enc_output, enc_output) output = self.fc(attn_output) return output # Helper function to move the model and data to GPU if available def to_device(tensor): if torch.cuda.is_available(): return tensor.cuda() return tensor"},{"question":"# Question You are required to process large log files to extract and summarize error messages efficiently. The `fileinput` module in Python provides a convenient way to iterate over multiple files or standard input streams. Using this module, write a function `extract_errors` that performs the following tasks: 1. Reads from a list of log files specified in the function arguments. 2. Identifies and extracts lines containing error messages. 3. Writes the extracted error messages to a new file while making a backup of the original output file. **Error messages** are defined as lines containing the keyword \\"ERROR\\". The function signature should be: ```python def extract_errors(input_files: list, output_file: str): ``` - `input_files` is a list of file names to be processed. - `output_file` is the name of the file where the extracted error messages should be written. **Constraints:** 1. Each log file is a plain text file. 2. Use the `fileinput` module for reading from the input files. 3. The function should handle files with different encodings if specified (default is \'utf-8\'). 4. You should make a backup of the original output file with extension \'.bak\' if it exists. 5. Your solution should be efficient and able to handle large files. **Example:** Suppose you have two log files `log1.txt` and `log2.txt` with the following contents: ``` log1.txt: INFO: All systems operational ERROR: An error has occurred INFO: System check complete log2.txt: WARNING: Low disk space ERROR: Failed to start service INFO: Update complete ``` Calling `extract_errors([\'log1.txt\', \'log2.txt\'], \'errors.txt\')` should result in the file `errors.txt` containing: ``` ERROR: An error has occurred ERROR: Failed to start service ``` And if `errors.txt` already existed, it should be backed up as `errors.txt.bak`. **Implementation Details:** Use the `fileinput` module for reading input files and ensure proper handling of different encodings and in-place file modifications.","solution":"import fileinput import shutil import os def extract_errors(input_files: list, output_file: str, encoding=\'utf-8\'): # Check if the output file already exists, if yes, make a backup if os.path.exists(output_file): backup_output_file = f\\"{output_file}.bak\\" shutil.copyfile(output_file, backup_output_file) with open(output_file, \'w\', encoding=encoding) as outfile: for line in fileinput.input(files=input_files, mode=\'r\', encoding=encoding): if \'ERROR\' in line: outfile.write(line)"},{"question":"Coding Assessment Question # Objective Design a Python program to demonstrate your understanding of Unix-specific system interactions using Python 3.10. You are required to implement functions that utilize Unix system calls and database modules to perform various tasks. # Problem Statement You need to implement a small utility program that interacts with the Unix password and group databases to manage user information and logs the activities using the syslog module. # Tasks 1. Implement a function `get_user_info(username)` that takes a Unix username as input and returns user information from the password database. 2. Implement a function `get_group_info(groupname)` that takes a Unix group name as input and returns group information from the group database. 3. Implement a function `add_user_to_group(username, groupname)` that adds the specified user to the specified group and logs this action using the syslog. 4. Implement a function `log_event(message)` that logs a custom message to the Unix syslog. # Specifications 1. The `get_user_info(username)` function should return a dictionary with the following information: - User ID - User Name - Home Directory - Shell 2. The `get_group_info(groupname)` function should return a dictionary with the following information: - Group ID - Group Name - Group Members 3. The `add_user_to_group(username, groupname)` function doesn\'t actually need to change the system groups but should log a message indicating the user has been added to the group. 4. The `log_event(message)` function should use the syslog module to log the provided message. # Input and Output Formats * `get_user_info(username)` * **Input**: `username` (str) * **Output**: Dictionary with keys `\\"userID\\"`, `\\"userName\\"`, `\\"homeDirectory\\"`, `\\"shell\\"` * `get_group_info(groupname)` * **Input**: `groupname` (str) * **Output**: Dictionary with keys `\\"groupID\\"`, `\\"groupName\\"`, `\\"groupMembers\\"` * `add_user_to_group(username, groupname)` * **Input**: `username` (str), `groupname` (str) * **Output**: None (logs the event using syslog) * `log_event(message)` * **Input**: `message` (str) * **Output**: None (logs the event using syslog) # Constraints - Assume that the provided username and groupname are valid and exist on the system. - The functions should handle errors gracefully and log appropriate error messages using the `log_event` function. # Example ```python def get_user_info(username): # Implement this function pass def get_group_info(groupname): # Implement this function pass def add_user_to_group(username, groupname): # Implement this function pass def log_event(message): # Implement this function pass # Example usage print(get_user_info(\\"john\\")) print(get_group_info(\\"staff\\")) add_user_to_group(\\"john\\", \\"staff\\") log_event(\\"Custom log event\\") ``` The expected output for the example usage above would include user and group information in the specified dictionary format and log entries indicating the actions taken.","solution":"import pwd import grp import syslog def get_user_info(username): Returns user information from the Unix password database. try: user_info = pwd.getpwnam(username) return { \\"userID\\": user_info.pw_uid, \\"userName\\": user_info.pw_name, \\"homeDirectory\\": user_info.pw_dir, \\"shell\\": user_info.pw_shell } except KeyError: log_event(f\\"User {username} not found\\") return None def get_group_info(groupname): Returns group information from the Unix group database. try: group_info = grp.getgrnam(groupname) return { \\"groupID\\": group_info.gr_gid, \\"groupName\\": group_info.gr_name, \\"groupMembers\\": group_info.gr_mem } except KeyError: log_event(f\\"Group {groupname} not found\\") return None def add_user_to_group(username, groupname): Logs an event indicating that the specified user has been added to the specified group. log_event(f\\"Adding user {username} to group {groupname}\\") def log_event(message): Logs a custom message to the Unix syslog. syslog.syslog(syslog.LOG_INFO, message)"},{"question":"Objective: Implement a custom class `SmartList` in Python that demonstrates understanding of special methods for container emulation and context managers. This class should encapsulate a list and provide additional functionalities such as automatic logging when accessing elements and error handling using context management. Requirements: 1. **Initialization**: - The class should be initialized with a list of integers. 2. **Element Access and Modification**: - Implement the `__getitem__`, `__setitem__`, and `__delitem__` methods to allow indexing, setting, and deleting elements with additional functionality: - When accessing an element, log the index being accessed. - When setting an element, ensure the value is an integer; otherwise, raise a `TypeError`. 3. **Length Calculation**: - Implement the `__len__` method to return the length of the encapsulated list. 4. **Context Management**: - Implement `__enter__` and `__exit__` methods to handle resource management: - On entering, print a message \\"Entering SmartList context\\". - On exiting, print a message \\"Exiting SmartList context\\". - Handle any exceptions raised within the context and print: \\"An error occurred: <exception>\\". 5. **Detailed Representation**: - Implement the `__repr__` method to return a string representation of the class in the format: `SmartList([elements])`. Constraints: - The encapsulated list should only contain integers. - Operations like accessing, setting, and deleting should respect the list boundaries, i.e., raise appropriate exceptions (`IndexError`) if an invalid index is accessed. Example Usage: ```python # Example of initializing and using the SmartList with SmartList([1, 2, 3]) as sl: print(len(sl)) # Output: Entering SmartList context, 3 print(sl[1]) # Output: Logging access to index 1, 2 sl[2] = 5 # No error try: sl[2] = \\"string\\" # Output: An error occurred: TypeError except TypeError: pass del sl[1] # No error print(sl) # Output: SmartList([1, 5]) # Output: Exiting SmartList context ``` Task: Implement the `SmartList` class in Python following the specifications above. Ensure to handle all edge cases and provide meaningful logging and error handling.","solution":"class SmartList: def __init__(self, initial_list): if not all(isinstance(i, int) for i in initial_list): raise ValueError(\\"All elements of the initial list must be integers\\") self._list = initial_list def __getitem__(self, index): print(f\\"Logging access to index {index}\\") return self._list[index] def __setitem__(self, index, value): if not isinstance(value, int): raise TypeError(\\"Only integers can be assigned to SmartList\\") self._list[index] = value def __delitem__(self, index): del self._list[index] def __len__(self): return len(self._list) def __enter__(self): print(\\"Entering SmartList context\\") return self def __exit__(self, exc_type, exc_value, traceback): if exc_type is not None: print(f\\"An error occurred: {exc_value}\\") print(\\"Exiting SmartList context\\") # Suppress further propagation of the exception if necessary return False def __repr__(self): return f\\"SmartList({self._list})\\""},{"question":"**Complexity Analysis and Optimization with Garbage Collection** # **Objective**: Demonstrate your understanding of Python\'s `gc` module by implementing a function that performs a series of assertions and optimizes memory usage of a given function. # **Problem Statement**: You are given a function `memory_intensive_function` which performs memory-intensive operations, potentially creating reference cycles and leading to high memory usage. Your task is to implement a function `optimize_memory` which: 1. Ensures that garbage collection is enabled. 2. Freezes and unfreezes garbage collection to optimize performance during a critical section of the provided function. 3. After executing the function, collects unreachable objects. 4. Provides statistics collected during this process to help analyze memory usage. # **Requirements**: **Function Signature**: ```python import gc def optimize_memory(memory_intensive_function): pass ``` **Input**: - `memory_intensive_function`: A function that when called executes memory-intensive operations. **Output**: - A dictionary containing: - `\\"before_freeze_collections\\"`: Number of collections performed before freeze. - `\\"before_freeze_collected\\"`: Number of objects collected before freeze. - `\\"before_freeze_uncollectable\\"`: Number of uncollectable objects before freeze. - `\\"after_freeze_collections\\"`: Number of collections performed after freeze. - `\\"after_freeze_collected\\"`: Number of objects collected after freeze. - `\\"after_freeze_uncollectable\\"`: Number of uncollectable objects after freeze. - `\\"objects_collected\\"`: Number of unreachable objects collected after executing the function. **Constraints**: - The function should handle the garbage collection process efficiently with minimal overhead. - Ensure the critical section (function call) does not interfere with the garbage collection process. # **Example**: You are given a dummy `memory_intensive_function`: ```python def memory_intensive_function(): a = [] for i in range(10**6): a.append(i) ``` **Invocation**: ```python optimize_memory(memory_intensive_function) ``` **Output**: ```python { \\"before_freeze_collections\\": 10, \\"before_freeze_collected\\": 50000, \\"before_freeze_uncollectable\\": 0, \\"after_freeze_collections\\": 10, \\"after_freeze_collected\\": 50000, \\"after_freeze_uncollectable\\": 0, \\"objects_collected\\": 100 } ``` # **Note**: - The example values are illustrative; the actual results may vary based on memory usage and garbage collection behavior. # **Instructions**: 1. Read and understand the given function signature and requirements. 2. Implement the function `optimize_memory` according to the specifications. 3. Use the gc module functions to manage and monitor garbage collection before and after executing the provided function. 4. Return a dictionary with the required statistics.","solution":"import gc def optimize_memory(memory_intensive_function): # Ensure garbage collection is enabled gc.enable() # Record garbage collection statistics before freezing before_freeze_stats = gc.get_stats() before_freeze_collections = sum(stat[\\"collections\\"] for stat in before_freeze_stats) before_freeze_collected = sum(stat[\\"collected\\"] for stat in before_freeze_stats) before_freeze_uncollectable = sum(stat[\\"uncollectable\\"] for stat in before_freeze_stats) # Freeze garbage collection gc.freeze() # Execute the memory-intensive function memory_intensive_function() # Unfreeze garbage collection gc.unfreeze() # Record garbage collection statistics after unfreezing after_freeze_stats = gc.get_stats() after_freeze_collections = sum(stat[\\"collections\\"] for stat in after_freeze_stats) after_freeze_collected = sum(stat[\\"collected\\"] for stat in after_freeze_stats) after_freeze_uncollectable = sum(stat[\\"uncollectable\\"] for stat in after_freeze_stats) # Collect unreachable objects objects_collected = gc.collect() # Return the statistics return { \\"before_freeze_collections\\": before_freeze_collections, \\"before_freeze_collected\\": before_freeze_collected, \\"before_freeze_uncollectable\\": before_freeze_uncollectable, \\"after_freeze_collections\\": after_freeze_collections, \\"after_freeze_collected\\": after_freeze_collected, \\"after_freeze_uncollectable\\": after_freeze_uncollectable, \\"objects_collected\\": objects_collected }"},{"question":"# Question: Analyzing and Visualizing Trends in an Environmental Dataset You are provided with a dataset `seaice`, which contains daily measurements of sea ice extent. Your task is to create a visual representation of this data using seaborn. The goal is to produce a set of small multiple line plots that compare the evolution of sea ice extent across different decades. Input: - **seaice dataset**: Provided as a seaborn dataset. - Contains at least the following columns: - `\\"Date\\"`: Timestamps for each observation. - `\\"Extent\\"`: Numerical values of sea ice extent. You can load the dataset using: ```python from seaborn import load_dataset seaice = load_dataset(\\"seaice\\") ``` Requirements: 1. Create a line plot that shows the trend of sea ice extent over the years. 2. The x-axis should represent the day of the year. 3. The y-axis should represent the sea ice extent. 4. Use color to differentiate between different years. 5. The plot should be faceted by decade (e.g., 1980s, 1990s, etc.). 6. Customize the line width and color transparency for better visual clarity. 7. Add appropriate labels for the axes and a title for each facet. Output: A faceted line plot meeting the specified requirements, similar to the following example code: ```python import seaborn.objects as so seaice = load_dataset(\\"seaice\\") # Create the plot ( so.Plot( x=seaice[\\"Date\\"].dt.day_of_year, y=seaice[\\"Extent\\"], color=seaice[\\"Date\\"].dt.year ) .facet(seaice[\\"Date\\"].dt.year // 10 * 10) # Faceting by decade .add(so.Lines(linewidth=1, color=\\"#bbca\\"), col=None) .add(so.Lines(linewidth=1.5)) .scale(color=\\"ch:rot=-.2,light=.7\\") .layout(size=(12, 8)) .label(title=\\"Decade: {}s\\", xlabel=\\"Day of Year\\", ylabel=\\"Sea Ice Extent\\") ).show() ``` Constraints: - Ensure that the code efficiently handles large datasets without significant performance degradation. - Follow best practices for code readability and documentation. Performance: Your code should run within a reasonable time frame and handle datasets with tens of thousands of observations efficiently.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_seaice_extent_by_decade(seaice): Creates a faceted line plot showing the evolution of sea ice extent by decade. Parameters: seaice (DataFrame): Sea ice dataset containing columns \'Date\' and \'Extent\'. Returns: Faceted line plot of sea ice extent. # Ensure the \'Date\' column is in datetime format seaice[\'Date\'] = pd.to_datetime(seaice[\'Date\']) # Extract additional columns for plotting seaice[\'Year\'] = seaice[\'Date\'].dt.year seaice[\'DayOfYear\'] = seaice[\'Date\'].dt.dayofyear seaice[\'Decade\'] = (seaice[\'Year\'] // 10) * 10 # Set up the seaborn FacetGrid g = sns.FacetGrid(seaice, col=\'Decade\', col_wrap=3, height=4, aspect=1.5) # Map the line plot to the FacetGrid g.map(sns.lineplot, \'DayOfYear\', \'Extent\', \'Year\', alpha=0.7, linewidth=1.2) # Add labels and titles g.set_axis_labels(\'Day of Year\', \'Sea Ice Extent\') g.set_titles(\'Decade: {col_name}s\') g.add_legend() plt.subplots_adjust(top=0.9) g.fig.suptitle(\'Sea Ice Extent by Decade\') # Show the plot plt.show() # Example calling the function with seaborn\'s \'seaice\' dataset # seaice = sns.load_dataset(\\"seaice\\") # Uncomment this to run with actual dataset, assuming it\'s available # plot_seaice_extent_by_decade(seaice)"},{"question":"Background You are given a neural network implemented in PyTorch, and your task is to compute the Jacobian matrix of the outputs with respect to the inputs efficiently using `torch.func` transformations. The Jacobian matrix is a matrix of all first-order partial derivatives of a vector-valued function. Task Implement a function `compute_jacobian` that takes as input a neural network `model` and a batch of inputs `inputs` and returns the Jacobian matrix. Use `torch.func` transformations to achieve this in an efficient manner. Specifications 1. **Function Signature:** ```python def compute_jacobian(model: torch.nn.Module, inputs: torch.Tensor) -> torch.Tensor: ``` 2. **Input:** - `model`: A PyTorch neural network model that takes a tensor of shape `(batch_size, input_dim)` and produces an output tensor of shape `(batch_size, output_dim)`. - `inputs`: A tensor of shape `(batch_size, input_dim)` representing a batch of input data. 3. **Output:** - A tensor of shape `(batch_size, output_dim, input_dim)` representing the Jacobian matrix for each input in the batch. 4. **Constraints:** - You cannot manually iterate through batches or perform explicit for-loops to compute gradients. Instead, leverage `torch.func` transformations to handle the batching and differentiation. 5. **Performance:** - The implementation should be efficient and leverage the power of vectorization and automatic differentiation provided by `torch.func`. Example ```python import torch import torch.nn as nn import torch.func as fn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(3, 2) def forward(self, x): return self.linear(x) # Instantiate the model model = SimpleModel() # Create a batch of input data inputs = torch.randn(5, 3) # batch_size=5, input_dim=3 # Compute Jacobian matrix jacobian = compute_jacobian(model, inputs) print(jacobian.shape) # Expected output: torch.Size([5, 2, 3]) ``` You must use `torch.func.vmap` and `torch.func.jacfwd` (or equivalent transforms) to implement the `compute_jacobian` function adequately.","solution":"import torch import torch.nn as nn import torch.func as fn def compute_jacobian(model: torch.nn.Module, inputs: torch.Tensor) -> torch.Tensor: Computes the Jacobian matrix of the outputs with respect to the inputs for a given model. Args: model (torch.nn.Module): A PyTorch neural network model. inputs (torch.Tensor): A tensor of shape (batch_size, input_dim) representing a batch of input data. Returns: torch.Tensor: A tensor of shape (batch_size, output_dim, input_dim) representing the Jacobian matrix. def model_output(inputs): return model(inputs) # Use vmap to vectorize the jacobian and apply it to the whole batch jacobian_fn = fn.vmap(fn.jacfwd(model_output)) return jacobian_fn(inputs)"},{"question":"# Objective Your task is to design a program that utilizes the `urllib.robotparser` module to analyze the robots.txt file from a given URL and determine particular properties or constraints set by the website for web crawlers. # Problem Statement Implement a Python function `analyze_robots(url: str, useragent: str) -> dict` that performs the following actions: 1. Reads and parses the robots.txt file from the specified `url`. 2. Determines if the given `useragent` is allowed to fetch a list of URLs provided. 3. Retrieves and returns the crawl delay, request rate, and site map parameters for the given `useragent`. The function should return a dictionary with the following structure: ```python { \\"can_fetch\\": { \\"url_1\\": bool, \\"url_2\\": bool, ... }, \\"crawl_delay\\": Optional[int], \\"request_rate\\": Optional[Tuple[int, int]], # (requests, seconds) \\"site_maps\\": Optional[List[str]] } ``` # Input - `url` (str): The base URL where the robots.txt file is located. For example, `http://www.example.com/`. - `useragent` (str): The user agent string to be checked. - You can assume a predefined list of URLs to be checked for fetch permissions. # Output A dictionary containing the fetch permissions for each URL, the crawl delay, the request rate, and the site maps as described above. # Constraints - Ensure that the robots.txt file is read and parsed correctly. - For simplicity, feel free to assume a maximum of 5 URLs for the fetch permissions check. # Sample Usage ```python result = analyze_robots(\\"http://www.example.com/\\", \\"my-bot\\") print(result) { \\"can_fetch\\": { \\"http://www.example.com/page1\\": True, \\"http://www.example.com/page2\\": False, \\"http://www.example.com/page3\\": True, \\"http://www.example.com/page4\\": True, \\"http://www.example.com/page5\\": False }, \\"crawl_delay\\": 5, \\"request_rate\\": (10, 60), # 10 requests per 60 seconds \\"site_maps\\": [\\"http://www.example.com/sitemap.xml\\"] } ``` # Implementation Details Use the `RobotFileParser` class from the `urllib.robotparser` module to implement the function. Your implementation should include: 1. Setting the URL of the robots.txt file. 2. Reading and parsing the content. 3. Fetching each of the required pieces of information: fetch permissions, crawl delay, request rate, and site maps. # Note You might need to handle exceptions where fetching or parsing of robots.txt fails, and return appropriate values in those cases (e.g., `None` for crawl delay, request rate, and site maps). Good luck!","solution":"from urllib.robotparser import RobotFileParser from typing import List, Optional, Tuple, Dict def analyze_robots(url: str, useragent: str) -> Dict: robots_url = url.rstrip(\'/\') + \'/robots.txt\' rp = RobotFileParser() rp.set_url(robots_url) rp.read() urls_to_check = [ url.rstrip(\'/\') + \'/page1\', url.rstrip(\'/\') + \'/page2\', url.rstrip(\'/\') + \'/page3\', url.rstrip(\'/\') + \'/page4\', url.rstrip(\'/\') + \'/page5\', ] can_fetch = {u: rp.can_fetch(useragent, u) for u in urls_to_check} crawl_delay = rp.crawl_delay(useragent) request_rate = rp.request_rate(useragent) site_maps = rp.site_maps() return { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate, \\"site_maps\\": site_maps }"},{"question":"**Question Title:** Testing External API Interaction with Mocks Using `unittest.mock` **Problem Statement:** You are tasked with testing a class `WeatherFetcher` that interacts with an external weather API to fetch weather data. Since hitting the actual API during tests is undesirable (due to the dependency on external factors like network, API availability, and rate limits), we will use the `unittest.mock` library to mock the responses from this API. Your task is to: 1. Write a `WeatherFetcher` class with methods to get current weather data. 2. Use `unittest.mock` to patch the API interaction functions. 3. Write test cases to verify that your `WeatherFetcher` methods work correctly without making actual network requests. ```python import requests class WeatherFetcher: def __init__(self, api_key): self.api_key = api_key def get_current_weather(self, location): Fetches the current weather for a given location. url = f\\"http://example.com/weather/current?location={location}&apikey={self.api_key}\\" response = requests.get(url) return response.json() # Write your tests below this line import unittest # Reminder: Use unittest.mock to mock the requests.get calls. class TestWeatherFetcher(unittest.TestCase): def setUp(self): self.api_key = \\"test_api_key\\" self.weather_fetcher = WeatherFetcher(self.api_key) # Add your test methods here ... ``` **Requirements:** 1. **WeatherFetcher Class:** - `__init__(self, api_key)`: Initialize with an API key. - `get_current_weather(self, location)`: Method to get current weather data for a provided location. 2. **TestWeatherFetcher Class (Unit Tests):** - Use the `patch()` decorator from `unittest.mock` to mock `requests.get` calls. - Create a test method `test_get_current_weather` that: - Mocks the `requests.get` call and sets a predefined JSON response. - Asserts that `get_current_weather` returns the correct weather data. - Asserts that the correct API URL was called with the provided location and API key. - Ensure that no real network requests are made during testing. **Input Format:** - The `get_current_weather` method will take a string `location` as input, representing the location for which the weather is to be fetched. **Output Format:** - The output of the `get_current_weather` method will be a JSON response containing weather data. **Constraints:** - The actual implementation of the `requests.get` method should not be executed during the tests. - Use `unittest` as the testing framework. - Tests should cover positive scenarios only (valid location, API key). **Example:** ```python # Example class usage: fetcher = WeatherFetcher(\\"your_api_key\\") weather = fetcher.get_current_weather(\\"New York\\") # Example of expected test: class TestWeatherFetcher(unittest.TestCase): @patch(\'requests.get\') def test_get_current_weather(self, mock_get): mock_response = Mock() expected_json = {\\"temperature\\": \\"18C\\", \\"humidity\\": \\"70%\\"} mock_response.json.return_value = expected_json mock_get.return_value = mock_response api_key = \\"test_key\\" location = \\"New York\\" fetcher = WeatherFetcher(api_key) weather = fetcher.get_current_weather(location) # Check if the response is what we expected self.assertEqual(weather, expected_json) # Check if the requests.get was called with the correct URL mock_get.assert_called_once_with(f\\"http://example.com/weather/current?location={location}&apikey={api_key}\\") ``` Use the given guidelines and fill in the `...` in the `TestWeatherFetcher` class to complete the test cases for the `WeatherFetcher` class using `unittest.mock`.","solution":"import requests class WeatherFetcher: def __init__(self, api_key): self.api_key = api_key def get_current_weather(self, location): Fetches the current weather for a given location. url = f\\"http://example.com/weather/current?location={location}&apikey={self.api_key}\\" response = requests.get(url) return response.json()"},{"question":"--- # **Problem Statement** You are tasked with writing a Python function that uses the `select` module to monitor multiple file descriptors (sockets) and handle their respective I/O events. You need to implement a server that listens on multiple TCP ports and echoes back any data it receives to the sender. Your function should use the `select.select()` function to efficiently manage multiple connections. # **Function Signature** ```python def echo_server(port_list: list, timeout: float = None) -> None: pass ``` # **Parameters** - `port_list` (*list*): A list of integers representing the TCP ports the server should listen on. - `timeout` (*float*, optional): The maximum time to wait for an event in seconds. If `None`, the function will block indefinitely until an event occurs. # **Instructions** 1. Create a TCP socket for each port in the `port_list`. 2. Use the `select.select()` function to wait for I/O events on the sockets. 3. When a socket is ready for reading, accept any new connections and read data from the sockets. 4. Echo back any received data to the client that sent it. 5. Handle client disconnections gracefully by removing the connection from the monitored list. 6. The function should run indefinitely unless manually terminated. # **Constraints** - You can assume that the `port_list` will only contain valid port numbers (1024-65535). - The function should handle both IPv4 and IPv6 connections. # **Example Usage** ```python import threading # Start the echo server on ports 8080 and 9090 server_thread = threading.Thread(target=echo_server, args=([8080, 9090],)) server_thread.start() ``` **Detailed Instructions:** - Use the `socket` module to create and bind sockets to the specified ports. - Use the `select.select()` function to monitor multiple sockets for incoming connections and data. - Use non-blocking I/O operations to handle multiple connections efficiently. - Ensure the server can handle multiple clients simultaneously. (Note: To run this function in a test environment, you may need to use threading or multiprocessing to avoid blocking the main thread.) --- # **Performance Requirements** - The server should handle a large number of simultaneous clients without significant performance degradation. - Efficiently manage the list of sockets to ensure minimal CPU usage while waiting for events. # **Hint** - Consider using dictionaries to map sockets to their respective connection handlers. - Make good use of exception handling to manage unexpected disconnections and errors. Good luck!","solution":"import socket import select def echo_server(port_list, timeout=None): # Create a list to hold all the server sockets server_sockets = [] address_family = {4: socket.AF_INET, 6: socket.AF_INET6} # Create server sockets for each port for port in port_list: for af in [socket.AF_INET, socket.AF_INET6]: try: server_socket = socket.socket(af, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'\' if af == socket.AF_INET else \'::\', port)) server_socket.listen() server_socket.setblocking(False) server_sockets.append(server_socket) except Exception as e: continue inputs = server_sockets # Initialize input list with server sockets outputs = [] while True: readable, writable, exceptional = select.select(inputs, outputs, inputs, timeout) for s in readable: if s in server_sockets: # Accept a new connection connection, client_address = s.accept() connection.setblocking(False) inputs.append(connection) else: try: data = s.recv(1024) if data: s.sendall(data) # Echo back the received data else: if s in outputs: outputs.remove(s) inputs.remove(s) s.close() except ConnectionResetError: if s in outputs: outputs.remove(s) inputs.remove(s) s.close() for s in exceptional: inputs.remove(s) if s in outputs: outputs.remove(s) s.close()"},{"question":"# Topological Sorting with Cycle Handling and Parallel Processing Simulation You are required to implement a function that utilizes the `graphlib.TopologicalSorter` class for topological sorting of a directed acyclic graph (DAG). Your function should accept a graph, detect cycles, and provide a valid topological ordering of the nodes. Additionally, your function should simulate the parallel processing of nodes using worker threads or processes. Function Signature ```python def process_graph(graph: Dict[str, Set[str]]) -> Tuple[List[str], str]: ``` Parameters - `graph` (Dict[str, Set[str]]): A dictionary representing the directed acyclic graph where keys are nodes (tasks) and values are sets of predecessors (tasks that must be performed before the key task). Returns - Tuple[List[str], str]: A tuple where the first element is a list representing the topological order of the nodes (tasks) and the second element is a status string that will be either \\"Success\\" or \\"Cycle Detected\\". Constraints 1. The graph will have at most 1000 nodes. 2. Nodes are represented as strings and are hashable. 3. The graph may have zero or more directed edges. 4. Cycle detection must be handled properly, and if a cycle is detected, the list should be empty and the status should be \\"Cycle Detected\\". Example ```python graph = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}, \\"E\\": {\\"D\\", \\"C\\"} } topological_order, status = process_graph(graph) print(topological_order) # Output: [\'A\', \'C\', \'B\', \'D\', \'E\'] print(status) # Output: \'Success\' cycle_graph = { \\"A\\": {\\"B\\"}, \\"B\\": {\\"C\\"}, \\"C\\": {\\"A\\"} } topological_order, status = process_graph(cycle_graph) print(topological_order) # Output: [] print(status) # Output: \'Cycle Detected\' ``` Requirements 1. Use the `graphlib.TopologicalSorter` class. 2. Properly handle cycle detection using the `CycleError` exception. 3. Ensure that the function is able to simulate parallel processing steps: - Use `get_ready()` to get all nodes that are ready. - Use `done()` to mark nodes as completed.","solution":"from typing import Dict, List, Set, Tuple from graphlib import TopologicalSorter, CycleError def process_graph(graph: Dict[str, Set[str]]) -> Tuple[List[str], str]: Processes the given directed acyclic graph (DAG) to provide a topological order. In case of a cyclic graph, it returns an empty list with the status \\"Cycle Detected\\". Parameters: graph (Dict[str, Set[str]]): A dictionary representing the DAG. Returns: Tuple[List[str], str]: A list representing the topological order of nodes, and a status string indicating success or cycle detection. ts = TopologicalSorter(graph) topological_order = [] status = \\"Success\\" try: ts.prepare() while ts.is_active(): ready = ts.get_ready() topological_order.extend(ready) for node in ready: ts.done(node) except CycleError: topological_order = [] status = \\"Cycle Detected\\" return topological_order, status"},{"question":"**Title:** Efficient Operations with PyTorch Sparse Tensors **Objective:** Assess students\' understanding of converting dense tensors to sparse tensors, performing operations on sparse tensors, and evaluating the performance efficiency of using sparse formats. This exercise covers both fundamental and advanced concepts in PyTorch\'s sparse tensor support. **Problem Statement:** You are given a dense matrix representation of a graph adjacency matrix. Your task is to: 1. Convert the dense matrix to different sparse tensor formats (COO, CSR, and CSC). 2. Perform a matrix multiplication operation with another given dense matrix using the sparse tensor formats. 3. Compare the performance of matrix multiplication using dense and sparse formats. **Instructions:** 1. **Input:** - `dense_matrix_1`: A dense 2D PyTorch tensor representing the graph adjacency matrix. - `dense_matrix_2`: A dense 2D PyTorch tensor representing another matrix for multiplication. 2. **Output:** - Sparse tensors in COO, CSR, and CSC formats. - Resultant matrices from matrix multiplication using sparse tensors and dense tensors. - Comparison of execution time for matrix multiplication using dense and sparse formats. 3. **Constraints:** - Assume the `dense_matrix_1` is a square matrix. - Allow only valid operations that are supported for the respective sparse formats. 4. **Performance Requirement:** - Use Python\'s `time` library to measure and compare the execution times. - Ensure reproducibility by using a fixed random seed for tensor initialization. **Steps and Requirements:** 1. Convert the `dense_matrix_1` to sparse tensors in COO, CSR, and CSC formats. 2. Perform the matrix multiplication operation `dense_matrix_1 * dense_matrix_2` using: - Dense tensor multiplication. - COO sparse tensor multiplication. - CSR sparse tensor multiplication. - CSC sparse tensor multiplication (if supported). 3. Measure and print the execution time for each matrix multiplication. 4. Return the resulting matrices and execution times. **Function Signature:** ```python import torch import time def sparse_tensor_operations(dense_matrix_1: torch.Tensor, dense_matrix_2: torch.Tensor): Convert dense tensor to sparse formats and perform matrix multiplication. Parameters: dense_matrix_1 (torch.Tensor): A dense 2D tensor representing the graph adjacency matrix. dense_matrix_2 (torch.Tensor): A dense 2D tensor to multiply with dense_matrix_1. Returns: dict: A dictionary with keys \'coo\', \'csr\', \'csc\', and \'dense\' containing the resulting matrices, and \'execution_times\' containing the time taken for each multiplication. # Step 1: Convert dense_matrix_1 to different sparse formats (COO, CSR, CSC) coo_tensor = dense_matrix_1.to_sparse() csr_tensor = dense_matrix_1.to_sparse_csr() csc_tensor = dense_matrix_1.to_sparse_csc() # Step 2: Perform matrix multiplication using dense and sparse tensors start_dense = time.time() result_dense = torch.matmul(dense_matrix_1, dense_matrix_2) end_dense = time.time() start_coo = time.time() result_coo = torch.matmul(coo_tensor, dense_matrix_2) end_coo = time.time() start_csr = time.time() result_csr = torch.matmul(csr_tensor, dense_matrix_2) end_csr = time.time() start_csc = time.time() result_csc = torch.matmul(csc_tensor, dense_matrix_2) end_csc = time.time() execution_times = { \'dense\': end_dense - start_dense, \'coo\': end_coo - start_coo, \'csr\': end_csr - start_csr, \'csc\': end_csc - start_csc } results = { \'dense\': result_dense, \'coo\': result_coo.to_dense(), \'csr\': result_csr.to_dense(), \'csc\': result_csc.to_dense(), \'execution_times\': execution_times } return results # Example usage dense_matrix_1 = torch.tensor([[0, 2], [3, 0]], dtype=torch.float32) dense_matrix_2 = torch.tensor([[1, 1], [0, 4]], dtype=torch.float32) results = sparse_tensor_operations(dense_matrix_1, dense_matrix_2) print(results) ``` **Note:** Ensure that your solution effectively utilizes PyTorch\'s sparse tensor conversions and operations. Aim for both correctness and efficiency.","solution":"import torch import time def sparse_tensor_operations(dense_matrix_1: torch.Tensor, dense_matrix_2: torch.Tensor): Convert dense tensor to sparse formats and perform matrix multiplication. Parameters: dense_matrix_1 (torch.Tensor): A dense 2D tensor representing the graph adjacency matrix. dense_matrix_2 (torch.Tensor): A dense 2D tensor to multiply with dense_matrix_1. Returns: dict: A dictionary with keys \'coo\', \'csr\', and \'dense\' containing the resulting matrices, and \'execution_times\' containing the time taken for each multiplication. # Step 1: Convert dense_matrix_1 to different sparse formats (COO, CSR, CSC are deprecated) coo_tensor = dense_matrix_1.to_sparse() csr_tensor = dense_matrix_1.to_sparse_csr() # Step 2: Perform matrix multiplication using dense and sparse tensors start_dense = time.time() result_dense = torch.matmul(dense_matrix_1, dense_matrix_2) end_dense = time.time() start_coo = time.time() result_coo = torch.matmul(coo_tensor, dense_matrix_2) end_coo = time.time() start_csr = time.time() result_csr = torch.matmul(csr_tensor, dense_matrix_2) end_csr = time.time() execution_times = { \'dense\': end_dense - start_dense, \'coo\': end_coo - start_coo, \'csr\': end_csr - start_csr } results = { \'dense\': result_dense, \'coo\': result_coo.to_dense(), \'csr\': result_csr.to_dense(), \'execution_times\': execution_times } return results # Example usage dense_matrix_1 = torch.tensor([[0, 2], [3, 0]], dtype=torch.float32) dense_matrix_2 = torch.tensor([[1, 1], [0, 4]], dtype=torch.float32) results = sparse_tensor_operations(dense_matrix_1, dense_matrix_2) print(results)"},{"question":"**Coding Assessment Question** # Problem Description Create a Python function `fetch_data_with_custom_headers` that retrieves data from a given URL using custom headers. The function should use the `urllib.request` module to perform an HTTP GET request. The function needs to handle redirects appropriately and ensure that the custom headers are included in all redirect requests. # Function Signature ```python def fetch_data_with_custom_headers(url: str, headers: dict) -> str: pass ``` # Input - `url` (str): A string representing the URL from which to fetch data. - `headers` (dict): A dictionary where keys are header names and values are header values to be added to the request. # Output - Returns a string containing the content retrieved from the URL. The content should be decoded using \'utf-8\' encoding. # Constraints 1. The function should handle any redirects by including the custom headers in the redirected request. 2. If the URL is not reachable or an HTTP error occurs, the function should raise an appropriate exception. 3. Use `urllib.request` for making the HTTP request. 4. Ensure that the function can handle large responses by reading and returning the complete content of the response. # Example ```python url = \\"http://www.python.org/\\" headers = { \\"User-Agent\\": \\"CustomAgent/1.0\\", \\"Accept\\": \\"text/html\\" } result = fetch_data_with_custom_headers(url, headers) print(result[:500]) # Prints the first 500 characters of the fetched content ``` # Implementation Requirements 1. Create and configure a `Request` object with the provided URL and headers. 2. Use `urllib.request.urlopen` to perform the request. 3. Handle redirects by ensuring that the custom headers are included in the redirected requests. 4. Decode the response content using \'utf-8\' and return it as a string. # Additional Information - You may refer to `urllib.request.Request` and `urllib.request.urlopen` in the documentation provided for more details on setting and using custom headers. - Consider handling edge cases such as invalid URLs, network issues, and HTTP errors gracefully.","solution":"import urllib.request import urllib.error def fetch_data_with_custom_headers(url: str, headers: dict) -> str: Retrieves data from the given URL using custom headers. Args: url (str): The URL from which to fetch data. headers (dict): A dictionary where keys are header names and values are header values. Returns: str: The content retrieved from the URL, decoded using \'utf-8\'. Raises: Exception: If the URL is not reachable or an HTTP error occurs. try: req = urllib.request.Request(url, headers=headers) with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: raise Exception(f\\"HTTP error occurred: {e.code} - {e.reason}\\") except urllib.error.URLError as e: raise Exception(f\\"URL error occurred: {e.reason}\\")"},{"question":"JIT Compilation in PyTorch **Objective:** Implement a PyTorch neural network module and optimize its performance using JIT compilation. **Task:** You are required to: 1. Implement a simple neural network using PyTorch. 2. Optimize the model using `@torch.jit.script` or `@torch.jit.trace` decorators. 3. Demonstrate the savings in time or performance by running a forward pass with both the original and the JIT-compiled models. **Requirements:** 1. Implement a neural network class `SimpleNN` with at least: - One input layer (dimension 10) - One hidden layer (dimension 20, ReLU activation) - One output layer (dimension 1) 2. Write a function `jit_optimize_model(model: torch.nn.Module) -> torch.jit.ScriptModule` that takes an instance of `SimpleNN`, and returns a JIT-compiled version of the model using either `torch.jit.script` or `torch.jit.trace`. 3. Measure the execution time of a forward pass using random input data for both the original and JIT-compiled models. Provide a function `compare_performance(model: torch.nn.Module, jit_model: torch.jit.ScriptModule, input_data: torch.Tensor) -> dict` that returns a dictionary containing the execution times for both models. # Input - The `SimpleNN` class definition and instantiated model. - Random input data tensor of shape (100, 10). # Output - The JIT-compiled model using `jit_optimize_model`. - A dictionary comparing the execution times using `compare_performance`. # Constraints - Use only PyTorch and standard Python libraries. - Ensure the code is reproducible and does not include hard-coded paths. # Example ```python import torch import torch.nn as nn import torch.optim as optim import time # Step 1: Implement SimpleNN class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Step 2: JIT Optimize the model def jit_optimize_model(model): model.eval() # Set the model to evaluation mode example_input = torch.randn(100, 10) scripted_model = torch.jit.trace(model, example_input) return scripted_model # Step 3: Compare performance def compare_performance(model, jit_model, input_data): start_time = time.time() with torch.no_grad(): model(input_data) original_time = time.time() - start_time start_time = time.time() with torch.no_grad(): jit_model(input_data) jit_time = time.time() - start_time return {\\"original_time\\": original_time, \\"jit_time\\": jit_time} # Test the implementation model = SimpleNN() jit_model = jit_optimize_model(model) input_data = torch.randn(100, 10) performance = compare_performance(model, jit_model, input_data) print(performance) ``` In this task, students should: - Define the `SimpleNN` class properly. - Use the JIT compilation utilities correctly. - Accurately measure and compare the execution times.","solution":"import torch import torch.nn as nn import time # Step 1: Implement SimpleNN class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Step 2: JIT Optimize the model def jit_optimize_model(model): model.eval() # Set the model to evaluation mode example_input = torch.randn(100, 10) scripted_model = torch.jit.trace(model, example_input) return scripted_model # Step 3: Compare performance def compare_performance(model, jit_model, input_data): start_time = time.time() with torch.no_grad(): model(input_data) original_time = time.time() - start_time start_time = time.time() with torch.no_grad(): jit_model(input_data) jit_time = time.time() - start_time return {\\"original_time\\": original_time, \\"jit_time\\": jit_time}"},{"question":"**Problem Statement:** You are required to create unit tests for a class that simulates a simple calculator. The `Calculator` class includes the following methods: - `add(x, y)`: Returns the result of adding `x` and `y`. - `subtract(x, y)`: Returns the result of subtracting `y` from `x`. - `multiply(x, y)`: Returns the result of multiplying `x` by `y`. - `divide(x, y)`: Returns the result of dividing `x` by `y`. Raises a `ValueError` if `y` is zero. **Your task:** 1. Write a `Calculator` class in Python with the methods described. 2. Create a test file (e.g., `test_calculator.py`) using the `unittest` framework to test the `Calculator` class. **Requirements:** 1. **Test Class Structure**: - The test class should be named `TestCalculator`. - It should inherit from `unittest.TestCase`. 2. **Test Cases**: - Test the `add` method with both positive and negative integers. - Test the `subtract` method with both positive and negative integers. - Test the `multiply` method with a mix of integers and a scenario with zero. - Test the `divide` method with valid inputs and a scenario where division by zero raises a `ValueError`. 3. **Set Up and Tear Down**: - Implement `setUp` and `tearDown` methods in your test class if needed. 4. **Subtests**: - Use `subTest` to differentiate tests for different input values within a method. 5. **Test Runner**: - Ensure you can run your test suite using Python\'s command line interface (e.g., `python -m unittest test_calculator.py`). **Input and Output Format**: - There are no direct inputs or outputs for the coding task. - The correctness will be assessed based on the implementation of the test cases and the `Calculator` class. - Make sure your code does not print anything unless a test fails. **Constraints**: - All integers are in the range of -10^6 to 10^6. - You may assume that all operations, except for the zero division, result in valid integers within the acceptable range for Python\'s integer type. ```python # You need to implement the Calculator class here and the test cases in a file named test_calculator.py # Below is an example of how the Calculator class might be structured. class Calculator: def add(self, x, y): return x + y def subtract(self, x, y): return x - y def multiply(self, x, y): return x * y def divide(self, x, y): if y == 0: raise ValueError(\\"Cannot divide by zero\\") return x / y ``` **Example Test Cases**: In the `test_calculator.py` file: ```python import unittest from calculator import Calculator class TestCalculator(unittest.TestCase): def setUp(self): self.calc = Calculator() def test_add(self): self.assertEqual(self.calc.add(2, 3), 5) self.assertEqual(self.calc.add(-1, 1), 0) def test_subtract(self): self.assertEqual(self.calc.subtract(5, 3), 2) self.assertEqual(self.calc.subtract(3, 5), -2) def test_multiply(self): self.assertEqual(self.calc.multiply(3, 7), 21) self.assertEqual(self.calc.multiply(3, 0), 0) def test_divide(self): self.assertEqual(self.calc.divide(10, 2), 5) self.assertEqual(self.calc.divide(9, 1.5), 6) with self.assertRaises(ValueError): self.calc.divide(5, 0) if __name__ == \'__main__\': unittest.main() ``` Ensure your `Calculator` and its tests are comprehensive and handle the edge cases as mentioned.","solution":"class Calculator: def add(self, x, y): return x + y def subtract(self, x, y): return x - y def multiply(self, x, y): return x * y def divide(self, x, y): if y == 0: raise ValueError(\\"Cannot divide by zero\\") return x / y"},{"question":"**Question: Implementing Command-Line History Management** The Python interpreter uses the GNU Readline library to provide functionalities like tab completion and history management. In this task, you will implement a simplified version of the history management feature found in interactive Python interpreters. # Task Write a Python class `HistoryManager` that simulates basic command history management. The class should provide functionalities to: - Add commands to the history. - Retrieve commands from history by index. - Search for commands containing a specific substring. - Save history to a file. - Load history from a file. # `HistoryManager` Class Methods 1. **`add_command(command: str) -> None`** - Adds a new command to the history. 2. **`get_command(index: int) -> str`** - Retrieves the command at the specified index. - If the index is out of range, raise an `IndexError` with the message \\"Index out of range\\". 3. **`search_commands(substring: str) -> List[str]`** - Returns a list of all commands containing the specified substring. 4. **`save_history(filename: str) -> None`** - Saves the entire history to the specified file. Each command should be on a new line. 5. **`load_history(filename: str) -> None`** - Loads history from the specified file. Each line in the file represents a command. Constraints - Commands are non-empty strings. - File operations should handle exceptions gracefully, with appropriate error messages. - Ensure that the history is maintained in the order of added commands. # Example Usage ```python history_manager = HistoryManager() history_manager.add_command(\\"print(\'Hello, world!\')\\") history_manager.add_command(\\"x = 10\\") history_manager.add_command(\\"print(x)\\") print(history_manager.get_command(0)) # Output: \\"print(\'Hello, world!\')\\" print(history_manager.search_commands(\\"print\\")) # Output: [\\"print(\'Hello, world!\')\\", \\"print(x)\\"] history_manager.save_history(\\"history.txt\\") history_manager.load_history(\\"history.txt\\") ``` # Evaluation Criteria Your solution will be evaluated based on: - Correctness: Proper implementation of all methods. - Error Handling: Graceful management of invalid inputs and file operations. - Code Readability: Clear and maintainable code with appropriate comments. - Efficiency: Reasonable performance, especially for large histories.","solution":"from typing import List class HistoryManager: def __init__(self): self.history = [] def add_command(self, command: str) -> None: if command: self.history.append(command) def get_command(self, index: int) -> str: if index < 0 or index >= len(self.history): raise IndexError(\\"Index out of range\\") return self.history[index] def search_commands(self, substring: str) -> List[str]: return [cmd for cmd in self.history if substring in cmd] def save_history(self, filename: str) -> None: try: with open(filename, \\"w\\") as file: for command in self.history: file.write(command + \\"n\\") except Exception as e: print(f\\"Error saving history: {e}\\") def load_history(self, filename: str) -> None: try: with open(filename, \\"r\\") as file: self.history = [line.strip() for line in file.readlines()] except Exception as e: print(f\\"Error loading history: {e}\\")"},{"question":"You are provided with a dataset of events in a sports tournament. The events include information about games played, the participants, and the scores. Using this dataset, you will write Python functions to process and analyze the data. The functions will leverage advanced data structures from the `collections` module in Python. # Dataset The dataset is a list of tuples. Each tuple contains the following information about an event: 1. **Event ID** (str): A unique identifier for the event. 2. **Game** (str): The name of the game played. 3. **Participants** (list of str): The list of participants in the event. 4. **Scores** (list of int): The corresponding scores of the participants. Example: ```python events = [ (\\"E1\\", \\"Soccer\\", [\\"Team-A\\", \\"Team-B\\"], [3, 2]), (\\"E2\\", \\"Basketball\\", [\\"Team-C\\", \\"Team-D\\"], [89, 97]), (\\"E3\\", \\"Soccer\\", [\\"Team-A\\", \\"Team-C\\"], [1, 1]), (\\"E4\\", \\"Soccer\\", [\\"Team-D\\", \\"Team-B\\"], [2, 3]) ] ``` # Your Tasks 1. **Create an Ordered Summary of Events by Game.** Implement a function `ordered_summary_by_game(events)` that processes the dataset and returns an ordered dictionary. The dictionary should have games as keys and lists of event IDs (in the order they appear in the dataset) as values. **Input:** A list of events (as described). **Output:** An `OrderedDict` where the keys are game names and the values are lists of event IDs. ```python from collections import OrderedDict def ordered_summary_by_game(events): # Your code here pass ``` **Example:** ```python events = [ (\\"E1\\", \\"Soccer\\", [\\"Team-A\\", \\"Team-B\\"], [3, 2]), (\\"E2\\", \\"Basketball\\", [\\"Team-C\\", \\"Team-D\\"], [89, 97]), (\\"E3\\", \\"Soccer\\", [\\"Team-A\\", \\"Team-C\\"], [1, 1]), (\\"E4\\", \\"Soccer\\", [\\"Team-D\\", \\"Team-B\\"], [2, 3]) ] ordered_summary = ordered_summary_by_game(events) # Expected Output: # OrderedDict({ # \\"Soccer\\": [\\"E1\\", \\"E3\\", \\"E4\\"], # \\"Basketball\\": [\\"E2\\"] # }) ``` 2. **Track Scores and Determine the Number of Wins Each Team Has.** Implement a function `team_wins(events)` that calculates and returns a dictionary where each key is a team name and the value is the number of times that team has won. **Input:** A list of events (as described). **Output:** A `defaultdict` where the keys are team names and the values are integers representing the number of wins. ```python from collections import defaultdict def team_wins(events): # Your code here pass ``` **Example:** ```python events = [ (\\"E1\\", \\"Soccer\\", [\\"Team-A\\", \\"Team-B\\"], [3, 2]), (\\"E2\\", \\"Basketball\\", [\\"Team-C\\", \\"Team-D\\"], [89, 97]), (\\"E3\\", \\"Soccer\\", [\\"Team-A\\", \\"Team-C\\"], [1, 1]), (\\"E4\\", \\"Soccer\\", [\\"Team-D\\", \\"Team-B\\"], [2, 3]) ] wins = team_wins(events) # Expected Output: # defaultdict(<class \'int\'>, { # \\"Team-A\\": 1, # \\"Team-D\\": 1, # \\"Team-B\\": 1 # }) ``` # Constraints - Assume that the dataset will contain at least one event. - Assume that each event will have exactly two participants. - Scores will be non-negative integers. - If a game ends in a draw, no wins should be counted for that game. # Performance Requirements - The solution should efficiently handle datasets of up to 10,000 events. **Note:** Make sure to import the necessary components (`OrderedDict`, `defaultdict`) from the `collections` module in your implementations.","solution":"from collections import OrderedDict, defaultdict def ordered_summary_by_game(events): Create an ordered dictionary summarizing event IDs by game. Parameters: events (list of tuples): The list of events, where each event is represented as a tuple. Returns: OrderedDict: An ordered dictionary with games as keys and lists of event IDs as values. summary = OrderedDict() for event_id, game, participants, scores in events: if game not in summary: summary[game] = [] summary[game].append(event_id) return summary def team_wins(events): Calculate the number of wins each team has. Parameters: events (list of tuples): The list of events, where each event is represented as a tuple. Returns: defaultdict: A dictionary with team names as keys and the number of wins as values. wins = defaultdict(int) for event_id, game, participants, scores in events: if scores[0] > scores[1]: wins[participants[0]] += 1 elif scores[1] > scores[0]: wins[participants[1]] += 1 return wins"},{"question":"# Classification with Naive Bayes In this task, you will create a Python script to classify a dataset using different Naive Bayes classifiers provided by the `scikit-learn` library. You need to: 1. Load the provided dataset. 2. Split the dataset into training and testing sets. 3. Implement a function to train and evaluate different Naive Bayes classifiers (GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, and CategoricalNB) on the training data. 4. Output the accuracy of each classifier on the test data. Input Format - The dataset is provided as a CSV file with features in columns and the last column being the target variable. - The dataset filename is provided as a string input to the function. Output Format - Print the accuracy score of each Naive Bayes classifier on the test set, formatted as shown in the example below. Constraints - You can assume that the features are appropriately preprocessed for use with each type of Naive Bayes classifier. - Focus on using appropriate Naive Bayes classifiers depending on the feature distribution type. Performance Requirements - Ensure that your implementation takes into account the size of the dataset and handles the training and testing efficiently. Example: ``` CSV file content: feature1,feature2,feature3,target 5.1,3.5,1.4,0.2 4.9,3.0,1.4,0.2 ... Function call: evaluate_naive_bayes_classifiers(\'dataset.csv\') Expected Output: GaussianNB Accuracy: 0.95 MultinomialNB Accuracy: 0.93 ComplementNB Accuracy: 0.92 BernoulliNB Accuracy: 0.90 CategoricalNB Accuracy: 0.91 ``` Function Signature ```python def evaluate_naive_bayes_classifiers(filename: str) -> None: # Your code here ``` Starter Code ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.metrics import accuracy_score def evaluate_naive_bayes_classifiers(filename: str) -> None: # Load the dataset data = pd.read_csv(filename) # Split the data into features and target X = data.iloc[:, :-1] y = data.iloc[:, -1] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define and train the classifiers classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB(), \'CategoricalNB\': CategoricalNB() } for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'{name} Accuracy: {accuracy:.2f}\') ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB from sklearn.metrics import accuracy_score def evaluate_naive_bayes_classifiers(filename: str) -> None: # Load the dataset data = pd.read_csv(filename) # Split the data into features and target X = data.iloc[:, :-1] y = data.iloc[:, -1] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Define and train the classifiers classifiers = { \'GaussianNB\': GaussianNB(), \'MultinomialNB\': MultinomialNB(), \'ComplementNB\': ComplementNB(), \'BernoulliNB\': BernoulliNB(), \'CategoricalNB\': CategoricalNB() } for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\'{name} Accuracy: {accuracy:.2f}\')"},{"question":"You are provided with a dataset in the form of a CSV file called `sales_data.csv`. The dataset contains sales records with the following columns: - `order_id`: Unique identifier for each order - `date`: The date of the order in `YYYY-MM-DD` format - `customer_id`: Unique identifier for each customer - `product_id`: Unique identifier for each product - `quantity`: Number of units sold - `unit_price`: Price per unit of the product Your task is to perform the following operations using pandas: 1. **Read and preprocess the data:** - Load the dataset into a pandas DataFrame. - Convert the `date` column to datetime format. - Replace any missing values in the `quantity` and `unit_price` columns with the mean of their respective columns. 2. **Sales Summary Report:** - Create a summary report that shows the total sales amount for each product. The sales amount for a product is calculated as `quantity` multiplied by `unit_price`. - The summary report should be a DataFrame with `product_id` as the index and a single column `total_sales` representing the total sales amount for each product. 3. **Top Customers by Sales:** - Identify the top 5 customers based on the total sales amount. - The result should be a DataFrame with `customer_id` as the index and a single column `total_sales` representing the total sales amount for each customer. 4. **Monthly Sales Trend:** - Generate a monthly sales trend report that shows the total sales amount for each month. - The result should be a DataFrame with the index as the first date of each month and a single column `monthly_sales` representing the total sales amount for that month. Expected Input Format - The input is the CSV file `sales_data.csv`. Expected Output Format - The output includes three DataFrames: 1. `sales_summary`: DataFrame with columns `product_id` and `total_sales`. 2. `top_customers`: DataFrame with columns `customer_id` and `total_sales`. 3. `monthly_sales_trend`: DataFrame with the index as the first date of each month and a single column `monthly_sales`. Constraints - You should handle missing values as specified. - Ensure that the date column is correctly converted to datetime format for accurate time-based analysis. - Optimize the code for performance, especially when dealing with large datasets. ```python import pandas as pd def sales_data_analysis(filepath): # 1. Read and preprocess the data df = pd.read_csv(filepath) df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'quantity\'].fillna(df[\'quantity\'].mean(), inplace=True) df[\'unit_price\'].fillna(df[\'unit_price\'].mean(), inplace=True) # 2. Sales Summary Report df[\'total_sales\'] = df[\'quantity\'] * df[\'unit_price\'] sales_summary = df.groupby(\'product_id\')[\'total_sales\'].sum().reset_index() # 3. Top Customers by Sales top_customers = df.groupby(\'customer_id\')[\'total_sales\'].sum().nlargest(5).reset_index() # 4. Monthly Sales Trend df[\'month\'] = df[\'date\'].dt.to_period(\'M\') monthly_sales_trend = df.groupby(\'month\')[\'total_sales\'].sum().reset_index() monthly_sales_trend[\'month\'] = monthly_sales_trend[\'month\'].dt.to_timestamp() return sales_summary, top_customers, monthly_sales_trend ``` Test your function with the sample data provided to ensure correctness and performance.","solution":"import pandas as pd def sales_data_analysis(filepath): # 1. Read and preprocess the data df = pd.read_csv(filepath) df[\'date\'] = pd.to_datetime(df[\'date\']) df[\'quantity\'].fillna(df[\'quantity\'].mean(), inplace=True) df[\'unit_price\'].fillna(df[\'unit_price\'].mean(), inplace=True) # 2. Sales Summary Report df[\'total_sales\'] = df[\'quantity\'] * df[\'unit_price\'] sales_summary = df.groupby(\'product_id\')[\'total_sales\'].sum().reset_index() # 3. Top Customers by Sales top_customers = df.groupby(\'customer_id\')[\'total_sales\'].sum().nlargest(5).reset_index() # 4. Monthly Sales Trend df[\'month\'] = df[\'date\'].dt.to_period(\'M\') monthly_sales_trend = df.groupby(\'month\')[\'total_sales\'].sum().reset_index() monthly_sales_trend[\'month\'] = monthly_sales_trend[\'month\'].dt.to_timestamp() return sales_summary, top_customers, monthly_sales_trend"},{"question":"# Question: Implementing Custom Serialization using Pickle and copyreg Objective You are required to implement a custom serialization mechanism for a Python class using the `pickle` module, with support functions registered via `copyreg`. This will include handling a complex object that contains non-trivial state which changes behavior dynamically. Description 1. **Create a custom class `Config`**: - **Attributes**: - `name` (string): Name of the configuration. - `version` (integer): Version number of the configuration. - `settings` (dictionary): Configuration settings stored as key-value pairs. - **Methods**: - `update_setting(key, value)`: Updates a setting in the `settings` dictionary. - `__str__()`: Returns a string representation of the configuration in the format: \\"Config(name: {name}, version: {version}, settings: {settings})\\". 2. **Serialization Mechanism**: - Implement custom serialization for the `Config` class using the `pickle` module in conjunction with `copyreg`. - Write functions that will be used by `copyreg` to specify how to serialize and deserialize the `Config` objects. 3. **Persistence Handling**: - Use the `shelve` module to save and load multiple `Config` objects to/from a file. Constraints - Ensure that all attributes of the `Config` object are properly serialized and deserialized. - You should handle cases where the `update_setting` method is used before and after deserialization to confirm integrity. Input/Output Formats 1. **Input**: - You\'ll be provided with a list of operations to perform. Each operation is a dictionary containing: - \\"action\\": string indicating the action (\\"create\\", \\"update\\", \\"save\\", \\"load\\", \\"print\\"). - For \\"create\\" and \\"update\\": - Appropriate fields required for the `Config` class or the update method. - For \\"save\\" and \\"load\\": - filename (string): The name of the file to save/load the objects. - For \\"print\\": - name (string): The name of the `Config` object to print. 2. **Output**: - For \\"print\\" action, output the string representation of the `Config` object. Example ```python # Example Input operations = [ {\\"action\\": \\"create\\", \\"name\\": \\"config1\\", \\"version\\": 1, \\"settings\\": {\\"theme\\": \\"dark\\"}}, {\\"action\\": \\"update\\", \\"name\\": \\"config1\\", \\"key\\": \\"theme\\", \\"value\\": \\"light\\"}, {\\"action\\": \\"save\\", \\"filename\\": \\"config_data.db\\"}, {\\"action\\": \\"load\\", \\"filename\\": \\"config_data.db\\"}, {\\"action\\": \\"print\\", \\"name\\": \\"config1\\"} ] # Expected Output # Config(name: config1, version: 1, settings: {\'theme\': \'light\'}) ``` **Implementation Guidelines**: - Define the `Config` class with the necessary attributes and methods. - Implement the serialization and deserialization functions for the `Config` class and register them using `copyreg`. - Use the `shelve` module to save and load the objects as specified. - Implement a function to process the operations list and handle each action.","solution":"import pickle import copyreg import shelve class Config: def __init__(self, name, version, settings): self.name = name self.version = version self.settings = settings def update_setting(self, key, value): self.settings[key] = value def __str__(self): return f\\"Config(name: {self.name}, version: {self.version}, settings: {self.settings})\\" def serialize_config(config): return Config, (config.name, config.version, config.settings) def deserialize_config(name, version, settings): return Config(name, version, settings) copyreg.pickle(Config, serialize_config, deserialize_config) def save_configs(filename, configs): with shelve.open(filename) as db: for name, config in configs.items(): db[name] = config def load_configs(filename): configs = {} with shelve.open(filename) as db: for key in db: configs[key] = db[key] return configs def process_operations(operations): configs = {} for operation in operations: if operation[\\"action\\"] == \\"create\\": configs[operation[\\"name\\"]] = Config( operation[\\"name\\"], operation[\\"version\\"], operation[\\"settings\\"] ) elif operation[\\"action\\"] == \\"update\\": config = configs.get(operation[\\"name\\"]) if config: config.update_setting(operation[\\"key\\"], operation[\\"value\\"]) elif operation[\\"action\\"] == \\"save\\": save_configs(operation[\\"filename\\"], configs) elif operation[\\"action\\"] == \\"load\\": configs = load_configs(operation[\\"filename\\"]) elif operation[\\"action\\"] == \\"print\\": config = configs.get(operation[\\"name\\"]) if config: print(config)"},{"question":"Question: Implement Custom Attention Masking and Aggregation in PyTorch You are tasked with implementing custom attention masking functionalities using PyTorch. This task involves creating masks and combining them using logical operations. These masks will be used in an attention mechanism to focus on various parts of the input data. # Part 1: Create Block Masks Implement two functions to create simple and nested block masks. 1. `create_simple_block_mask(size: int) -> torch.Tensor` - **Input**: An integer `size` representing the dimensions of a square block mask. - **Output**: A `torch.Tensor` of shape `(size, size)` where all values are `1`. - **Example**: ```python create_simple_block_mask(3) ``` Should output: ``` tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) ``` 2. `create_nested_block_mask(sizes: List[int]) -> torch.Tensor` - **Input**: A list of integers `sizes` where each integer represents the dimension of a square block mask. - **Output**: A `torch.Tensor` of shape `(sum(sizes), sum(sizes))` where each block mask as defined by `sizes` is filled with `1`, and the rest are `0`. - **Example**: ```python create_nested_block_mask([2, 3]) ``` Should output: ``` tensor([[1, 1, 0, 0, 0], [1, 1, 0, 0, 0], [0, 0, 1, 1, 1], [0, 0, 1, 1, 1], [0, 0, 1, 1, 1]]) ``` # Part 2: Combine Masks Implement two functions to combine masks using logical AND and logical OR operations. 1. `and_masks(masks: List[torch.Tensor]) -> torch.Tensor` - **Input**: A list of `torch.Tensor` masks of identical shape. - **Output**: A single `torch.Tensor` which is the logical AND of all input masks. - **Example**: ```python m1 = torch.tensor([[1, 0], [1, 1]]) m2 = torch.tensor([[1, 1], [0, 1]]) and_masks([m1, m2]) ``` Should output: ``` tensor([[1, 0], [0, 1]]) ``` 2. `or_masks(masks: List[torch.Tensor]) -> torch.Tensor` - **Input**: A list of `torch.Tensor` masks of identical shape. - **Output**: A single `torch.Tensor` which is the logical OR of all input masks. - **Example**: ```python m1 = torch.tensor([[1, 0], [1, 1]]) m2 = torch.tensor([[1, 1], [0, 1]]) or_masks([m1, m2]) ``` Should output: ``` tensor([[1, 1], [1, 1]]) ``` # Performance Constraints - The block masks creation function should not use more than `O(n^2)` time complexity, where `n` is the size of the largest block. - Mask aggregation functions must operate in `O(m * d^2)` time complexity, where `m` is the number of masks and `d` is the dimension of the masks. Your solutions should make use of PyTorch functions and tensors. **Notes:** - Be sure to add appropriate comments and tests to demonstrate your implementation. - You may assume that all input sizes are positive integers and all masks in the list are non-empty and of the same dimensions.","solution":"import torch from typing import List def create_simple_block_mask(size: int) -> torch.Tensor: Creates a simple square block mask of given size. :param size: Dimension of the square block mask. :return: A (size x size) mask with all values set to 1. return torch.ones((size, size), dtype=torch.int) def create_nested_block_mask(sizes: List[int]) -> torch.Tensor: Creates a nested block mask from a list of sizes. :param sizes: List of dimensions for nested blocks. :return: A single block mask containing nested blocks as specified by sizes list. total_size = sum(sizes) mask = torch.zeros((total_size, total_size), dtype=torch.int) start = 0 for size in sizes: end = start + size mask[start:end, start:end] = 1 start = end return mask def and_masks(masks: List[torch.Tensor]) -> torch.Tensor: Combines a list of masks using logical AND operation. :param masks: List of masks to be combined. :return: A single mask resulting from the logical AND of all input masks. combined_mask = torch.ones_like(masks[0], dtype=torch.int) for mask in masks: combined_mask = combined_mask & mask return combined_mask def or_masks(masks: List[torch.Tensor]) -> torch.Tensor: Combines a list of masks using logical OR operation. :param masks: List of masks to be combined. :return: A single mask resulting from the logical OR of all input masks. combined_mask = torch.zeros_like(masks[0], dtype=torch.int) for mask in masks: combined_mask = combined_mask | mask return combined_mask"},{"question":"# Custom String Formatter Objective In this task, you are required to implement a custom string formatter using Python\'s `string.Formatter` class. The custom formatter should allow formatting of strings where certain special placeholders need to be replaced by corresponding values provided in the arguments. Requirements 1. Implement a class `CustomFormatter` that inherits from `string.Formatter`. 2. Your `CustomFormatter` class should override the `vformat`, `get_field`, and `format_field` methods. 3. The class should be able to handle the following custom format rules: - `{UPPERCASE}` should convert its value to uppercase. - `{LOWERCASE}` should convert its value to lowercase. - `{titlecase}` should convert its value to titlecase (first letter of each word in upper case). Implementation Details - **Class Definition:** ```python import string class CustomFormatter(string.Formatter): def vformat(self, format_string, args, kwargs): # Override this method pass def get_field(self, field_name, args, kwargs): # Implement custom field name lookup logic pass def format_field(self, value, format_spec): # Implement custom format spec processing pass ``` - **Method Specifications:** - `vformat(format_string, args, kwargs)`: This method should split the `format_string` into literal text and replacement fields and process each appropriately. - `get_field(field_name, args, kwargs)`: This method should return the value corresponding to the given `field_name`. - `format_field(value, format_spec)`: This method should apply any custom formatting specified in `format_spec` (particularly handling `UPPERCASE`, `LOWERCASE`, and `titlecase`). Input and Output Examples: - **Example 1:** - `format_string`: `\\"Hello, {name:UPPERCASE}! Welcome to the {event:LOWERCASE}.\\"` - `args`: `()` - `kwargs`: `{\\"name\\": \\"Alice\\", \\"event\\": \\"PYTHON CONFERENCE\\"}` - **Output**: `\\"Hello, ALICE! Welcome to the python conference.\\"` - **Example 2:** - `format_string`: `\\"Today\'s topic is {topic:titlecase}.\\"` - `args`: `()` - `kwargs`: `{\\"topic\\": \\"introduction to string formatting\\"}` - **Output**: `\\"Today\'s topic is Introduction To String Formatting.\\"` Constraints: 1. Assume that `field_name` will always be a valid string identifier. 2. The class should handle missing arguments gracefully by raising a `KeyError`. Testing Ensure to test your `CustomFormatter` class with various format strings and arguments to validate that the custom formatting rules are correctly applied. ```python # Example usage: formatter = CustomFormatter() print(formatter.vformat(\\"Hello, {name:UPPERCASE}! Welcome to the {event:LOWERCASE}.\\", tuple(), {\\"name\\": \\"Alice\\", \\"event\\": \\"PYTHON CONFERENCE\\"})) # Output: \\"Hello, ALICE! Welcome to the python conference.\\" ``` Implement the `CustomFormatter` class and ensure it performs as expected with the provided examples and additional edge cases.","solution":"import string class CustomFormatter(string.Formatter): def vformat(self, format_string, args, kwargs): # This method splits the format_string and processes each component. return super().vformat(format_string, args, kwargs) def get_field(self, field_name, args, kwargs): # Retrieves the value of field from args or kwargs try: obj = kwargs[field_name] except KeyError: raise KeyError(f\\"Field \'{field_name}\' is missing\\") return obj, field_name def format_field(self, value, format_spec): # Applies the custom format spec rules if format_spec == \'UPPERCASE\': return str(value).upper() elif format_spec == \'LOWERCASE\': return str(value).lower() elif format_spec == \'titlecase\': return str(value).title() else: return super().format_field(value, format_spec)"},{"question":"Objective: Design a class that uses both shallow and deep copy operations. Your implementation should demonstrate an understanding of the distinctions between these two types of copy operations and how to handle recursive references. Problem Statement: You are required to create a class named `Graph` that represents a simple directed graph. The graph should be implemented using adjacency lists. Additionally, implement custom shallow and deep copy methods for this class to handle recursive references correctly. Requirements: 1. **Graph Class:** - The class should have an adjacency list to represent the directed graph. - The class should have methods for adding nodes and edges. 2. **Copy Methods:** - Implement the `__copy__()` method to return a shallow copy of the graph. - Implement the `__deepcopy__()` method to return a deep copy of the graph, handling any recursive references appropriately using a memo dictionary. Implementation: - **Input and Output:** - Input: Various operations to add nodes, add edges, and copy the graph. - Output: The structure of the copied graph as an adjacency list. Constraints: - The graph can have cycles. - Node values and edges are simple strings. Example: ```python class Graph: def __init__(self): self.adjacency_list = {} def add_node(self, node): if node not in self.adjacency_list: self.adjacency_list[node] = [] def add_edge(self, from_node, to_node): if from_node in self.adjacency_list and to_node in self.adjacency_list: self.adjacency_list[from_node].append(to_node) def __copy__(self): # implement shallow copy logic pass def __deepcopy__(self, memo): # implement deep copy logic pass # Example usage: graph = Graph() graph.add_node(\'A\') graph.add_node(\'B\') graph.add_edge(\'A\', \'B\') graph.add_edge(\'B\', \'A\') # Create a cycle shallow_copied_graph = copy.copy(graph) deep_copied_graph = copy.deepcopy(graph) ``` Notes: - Ensure your implementation properly handles cycles in the graph. - Include comprehensive tests to verify the correctness of both shallow and deep copy implementations. - Do not use external libraries other than the standard `copy` module for shallow and deep copying operations.","solution":"import copy class Graph: def __init__(self): self.adjacency_list = {} def add_node(self, node): if node not in self.adjacency_list: self.adjacency_list[node] = [] def add_edge(self, from_node, to_node): if from_node in self.adjacency_list and to_node in self.adjacency_list: self.adjacency_list[from_node].append(to_node) def __copy__(self): new_graph = Graph() new_graph.adjacency_list = self.adjacency_list.copy() return new_graph def __deepcopy__(self, memo): new_graph = Graph() memo[id(self)] = new_graph for node, edges in self.adjacency_list.items(): new_node = copy.deepcopy(node, memo) new_edges = copy.deepcopy(edges, memo) new_graph.adjacency_list[new_node] = new_edges return new_graph"},{"question":"# Question: Exploring Tips Data with Seaborn The goal of this coding assessment is to test your understanding and ability to visualize data using the seaborn library. You are required to explore the \'tips\' dataset, which is available in seaborn\'s built-in datasets, and create various visualizations using the `stripplot` function with different customizations to gain insights into the dataset. Dataset Description: - The \'tips\' dataset contains records of transaction tips received by waiters in a restaurant. The variables in the dataset include: - `total_bill` (float): Total bill amount. - `tip` (float): Tip amount. - `sex` (str): Gender of the individual paying the bill. - `smoker` (str): Whether the individual is a smoker or not. - `day` (str): Day of the week. - `time` (str): Time of the meal (Lunch or Dinner). - `size` (int): Number of people at the table. Tasks: 1. **Basic Univariate Distribution Plot:** Create a strip plot to show the distribution of `total_bill` values. 2. **Comparison of Categorical Levels:** Create a strip plot where `total_bill` is plotted against the `day` variable to compare the distributions of total bills for each day of the week. 3. **Multidimensional Relationship:** Using the same plot as in Task 2, add a `hue` parameter to differentiate the data points based on the `sex` of the individuals. Ensure that the legend is included. 4. **Customized Strip Plot:** Create a strip plot of `total_bill` against the `day` variable, with different `hue` values for the `smoker` status. Set `dodge=True` to split smokers and non-smokers. Disable the jitter for better clarity. 5. **Faceted Strip Plot:** Create a faceted strip plot using `sns.catplot` showing `total_bill` against `time`, with `hue` for `sex`, and columns for each `day`. Ensure that the aspect ratio is set to 0.5 for better visualization. Requirements: - The script should follow Python best practices. - Ensure the plots are properly labeled, and legends are used where necessary. - Use appropriate color palettes to enhance the visual appeal of the plots. - The solution should be executable without modifications in a standard Python environment with seaborn installed. Submission: Submit your Python script with the implemented tasks. Ensure that the submitted code is well-commented to explain your thought process and steps taken.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Basic Univariate Distribution Plot def plot_total_bill_distribution(): plt.figure(figsize=(10, 6)) sns.stripplot(x=tips[\\"total_bill\\"]) plt.title(\\"Distribution of Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.show() # Task 2: Comparison of Categorical Levels def plot_total_bill_by_day(): plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", data=tips) plt.title(\\"Total Bill by Day\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.show() # Task 3: Multidimensional Relationship with hue def plot_total_bill_by_day_with_sex(): plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips) plt.title(\\"Total Bill by Day with Sex Differentiation\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\\"Sex\\") plt.show() # Task 4: Customized Strip Plot def plot_total_bill_by_day_with_smoker(): plt.figure(figsize=(10, 6)) sns.stripplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", data=tips, dodge=True, jitter=False) plt.title(\\"Total Bill by Day with Smoker Status\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\\"Smoker\\") plt.show() # Task 5: Faceted Strip Plot def plot_faceted_total_bill_by_time_and_day(): g = sns.catplot( x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=tips, kind=\\"strip\\", aspect=0.5 ) g.fig.suptitle(\\"Faceted Total Bill by Time and Day\\", y=1.02) g.set_axis_labels(\\"Time\\", \\"Total Bill\\") g.add_legend(title=\\"Sex\\") plt.show()"},{"question":"You are tasked with analyzing the color distribution of a dataset and presenting it using a custom color palette created with seaborn\'s `dark_palette` function. # Problem Statement 1. **Input**: You will be provided with a CSV file named `data.csv` containing a dataset with numerical and categorical columns. 2. **Output**: Generate a series of plots: - A histogram of a specified numerical column using a custom dark palette. - A bar plot showing the count of each category in a specified categorical column, using the same custom dark palette. # Specifications - Implement a function `create_color_palette(base_color: str, num_colors: int, as_cmap: bool) -> sns.palettes._ColorPalette`: - **Inputs**: - `base_color` (str): The base color for the palette. This can be a named color, hexadecimal code, or a tuple for the HUSL system. - `num_colors` (int): The number of discrete colors in the palette. - `as_cmap` (bool): Whether to return a continuous colormap (`True`) or a discrete palette (`False`). - **Output**: - Returns a seaborn color palette or a continuous colormap. - Implement a function `plot_data(file_path: str, num_column: str, cat_column: str, base_color: str, num_colors: int, as_cmap: bool) -> None`: - **Inputs**: - `file_path` (str): Path to the CSV file. - `num_column` (str): Name of the numerical column to plot in the histogram. - `cat_column` (str): Name of the categorical column to plot in the bar plot. - `base_color` (str): The base color for the palette. - `num_colors` (int): The number of discrete colors in the palette (ignored if `as_cmap` is `True`). - `as_cmap` (bool): Whether to use a continuous colormap (`True`) or a discrete palette (`False`). - **Outputs**: - Generates and displays the following plots: - A histogram of the `num_column` colored using the provided color palette. - A bar plot of the `cat_column`, showing the count of each category and colored using the same palette. # Example Usage ```python # Example of creating a color palette palette = create_color_palette(\\"seagreen\\", 5, False) # Example of generating plots using the created palette plot_data(\\"data.csv\\", \\"age\\", \\"gender\\", \\"seagreen\\", 5, False) ``` # Constraints - The CSV file at `file_path` must exist and be readable. - Columns specified must exist in the dataset. - Use seaborn for plotting and matplotlib for underlying plot mechanisms if necessary. # Submission Submit your implementation of the `create_color_palette` and `plot_data` functions.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_color_palette(base_color: str, num_colors: int, as_cmap: bool) -> sns.palettes._ColorPalette: Create a color palette using seaborn\'s dark_palette function. Args: - base_color (str): The base color for the palette. - num_colors (int): The number of discrete colors in the palette. - as_cmap (bool): Whether to return a continuous colormap (True) or a discrete palette (False). Returns: - sns.palettes._ColorPalette: A seaborn color palette or colormap. return sns.dark_palette(base_color, num_colors, as_cmap) def plot_data(file_path: str, num_column: str, cat_column: str, base_color: str, num_colors: int, as_cmap: bool) -> None: Generates and displays histogram and bar plots based on the provided data and color palette. Args: - file_path (str): Path to the CSV file. - num_column (str): Name of the numerical column to plot in the histogram. - cat_column (str): Name of the categorical column to plot in the bar plot. - base_color (str): The base color for the palette. - num_colors (int): The number of discrete colors in the palette. - as_cmap (bool): Whether to use a continuous colormap (True) or a discrete palette (False). Returns: - None # Read the data data = pd.read_csv(file_path) # Create the palette palette = create_color_palette(base_color, num_colors, as_cmap) # Plot the histogram for the numeric column plt.figure(figsize=(10, 5)) sns.histplot(data[num_column], kde=True, palette=palette if not as_cmap else None, color=base_color if as_cmap else None) plt.title(f\'Histogram of {num_column}\') plt.show() # Plot the bar plot for the categorical column plt.figure(figsize=(10, 5)) sns.countplot(x=cat_column, data=data, palette=palette) plt.title(f\'Bar Plot of {cat_column}\') plt.show()"},{"question":"# Advanced Color Manipulation with the \\"colorsys\\" Module **Objective:** Create a Python function that converts an input color in RGB format to the other color spaces (YIQ, HLS, HSV), applies a specified transformation in each color space, and then converts the modified colors back to the RGB format. The function should return a dictionary containing the modified RGB values for each color transformation. **Function Signature:** ```python def transform_color(rgb_color: tuple, transformations: dict) -> dict: pass ``` **Input:** - `rgb_color`: A tuple of three floating-point values representing the color in RGB space, where each value is between 0 and 1. Example: `(0.2, 0.4, 0.4)` - `transformations`: A dictionary containing transformation functions for each color space. Each key in the dictionary is a string representing a color space (`\\"YIQ\\"`, `\\"HLS\\"`, or `\\"HSV\\"`), and each value is a function that takes the color space coordinates as input and returns transformed coordinates in the same color space. **Output:** A dictionary with the same keys as `transformations`, where each key maps to a tuple with three floating-point numbers representing the RGB values of the transformed color. **Constraints:** - Ensure that all input values and transformed values remain within the valid ranges for each color space. - Assume the transformations provided are valid and take exactly three arguments corresponding to the coordinates of the respective color space. **Example:** ```python def increase_brightness(y, i, q): return (min(y + 0.1, 1), i, q) def shift_hue(h, l, s): return ((h + 0.2) % 1.0, l, s) def adjust_saturation(h, s, v): return (h, min(s * 1.5, 1), v) rgb_color = (0.2, 0.4, 0.4) transformations = { \\"YIQ\\": increase_brightness, \\"HLS\\": shift_hue, \\"HSV\\": adjust_saturation } result = transform_color(rgb_color, transformations) print(result) # Example output: # { # \\"YIQ\\": (0.23600000000000002, 0.4, 0.4), # \\"HLS\\": (0.7777777777777778, 0.4, 0.26666666666666666), # \\"HSV\\": (0.5555555555555556, 0.6, 0.4) # } ``` **Explanation:** The function should: 1. Convert the input RGB color to YIQ, HLS, and HSV formats. 2. Apply the respective transformation function to each color space\'s coordinates. 3. Convert the transformed coordinates back to RGB format. 4. Return the transformed RGB values in a dictionary.","solution":"import colorsys def transform_color(rgb_color: tuple, transformations: dict) -> dict: results = {} # Transform YIQ if \\"YIQ\\" in transformations: yiq = colorsys.rgb_to_yiq(*rgb_color) transformed_yiq = transformations[\\"YIQ\\"](*yiq) new_rgb = colorsys.yiq_to_rgb(*transformed_yiq) results[\\"YIQ\\"] = new_rgb # Transform HLS if \\"HLS\\" in transformations: hls = colorsys.rgb_to_hls(*rgb_color) transformed_hls = transformations[\\"HLS\\"](*hls) new_rgb = colorsys.hls_to_rgb(*transformed_hls) results[\\"HLS\\"] = new_rgb # Transform HSV if \\"HSV\\" in transformations: hsv = colorsys.rgb_to_hsv(*rgb_color) transformed_hsv = transformations[\\"HSV\\"](*hsv) new_rgb = colorsys.hsv_to_rgb(*transformed_hsv) results[\\"HSV\\"] = new_rgb return results"},{"question":"# Python Coding Assessment Question Objective The purpose of this exercise is to test your knowledge of file handling and custom objects in Python. You will need to create a class that mimics file object behavior but with added functionality. Problem Description You need to implement a class `CustomFile` that mimics the behavior of a standard file object in Python. Additionally, this class should log every line read from the file to another file. Class Requirements 1. **Initialization (`__init__`)**: - Parameters: `file_path` (str), `mode` (str), `log_path` (str) - Opens the file specified by `file_path` in the specified `mode`. - Prepares a log file specified by `log_path` to log the lines read. 2. **Methods**: - `readline()`: Reads a single line from the file. Each time a line is read, log the line to the log file. - `readlines()`: Reads all the lines from the file and returns them as a list. Log each line to the log file. - `close()`: Closes both the main file and the log file. Constraints - You are not allowed to use Pythons built-in file logging modules. - Assume that the file opened in `file_path` will always be a text file. - The mode for `file_path` can only be \'r\' (read) or \'r+\' (read and write). Example ```python # sample.txt contains: # Hello World # Python Programming # File Handling custom_file = CustomFile(\'sample.txt\', \'r\', \'log.txt\') print(custom_file.readline()) # Output: Hello World print(custom_file.readline()) # Output: Python Programming custom_file.close() # The log.txt file should contain: # Hello World # Python Programming ``` Function Signature ```python class CustomFile: def __init__(self, file_path: str, mode: str, log_path: str): pass def readline(self) -> str: pass def readlines(self) -> list: pass def close(self): pass ``` # Notes - Remember to handle exceptions when dealing with file operations, such as file not found or read/write errors. - Ensure thorough documentation of your code for clarity.","solution":"class CustomFile: def __init__(self, file_path: str, mode: str, log_path: str): self.file_path = file_path self.mode = mode self.log_path = log_path self.file = open(file_path, mode) self.log_file = open(log_path, \'w\') def readline(self) -> str: line = self.file.readline() if line: self.log_file.write(line) return line def readlines(self) -> list: lines = self.file.readlines() self.log_file.writelines(lines) return lines def close(self): self.file.close() self.log_file.close()"},{"question":"**Problem Statement:** You are given a CSV file named `wine.csv` containing a wine dataset. Your task is to read this dataset, preprocess it, and generate a clustered heatmap using the seaborn package. The clustered heatmap should enable the identification of different wine classes by color-coding the rows. **Specifications:** 1. **Read the Dataset:** Load the dataset from the `wine.csv` file. 2. **Preprocess the Data:** - Separate the class labels (target variable) from the feature data. - Normalize the feature data (each feature should have a mean of 0 and a standard deviation of 1). 3. **Generate the Clustered Heatmap:** - Use hierarchical clustering to cluster both rows and columns. - Add color labels to rows based on the wine class labels. - Customize the appearance by setting the figure size to (10, 7), disabling row clustering, and setting the colorbar position to (0.1, 0.8, .03, .4). - Use the \'viridis\' colormap. 4. **Function Signature:** ```python import seaborn as sns import pandas as pd def plot_clustered_heatmap(file_path: str) -> None: # Your implementation here pass ``` **Input:** - `file_path` (str): The file path to the `wine.csv` dataset. **Output:** - The function should display the clustered heatmap. **Constraints:** - Ensure the plot is displayed within reasonable time limits. - Use only the seaborn package for visualization and pandas for data handling. **Additional Note:** - Your clustered heatmap should be reproducible. Ensure any random components (like shuffling the data if necessary) have fixed seeds for reproducibility. Below is a sample template to get you started: ```python import seaborn as sns import pandas as pd from sklearn.preprocessing import StandardScaler def plot_clustered_heatmap(file_path: str) -> None: # Step 1: Read the dataset data = pd.read_csv(file_path) # Step 2: Preprocess the data wine_class = data.pop(\\"class\\") scaler = StandardScaler() scaled_data = scaler.fit_transform(data) scaled_data = pd.DataFrame(scaled_data, columns=data.columns) # Step 3: Generate color labels for each class lut = dict(zip(wine_class.unique(), sns.color_palette(\\"viridis\\", len(wine_class.unique())))) row_colors = wine_class.map(lut) # Step 4: Generate the clustered heatmap sns.clustermap( scaled_data, figsize=(10, 7), row_cluster=False, col_cluster=True, row_colors=row_colors, cmap=\\"viridis\\", cbar_pos=(0.1, 0.8, .03, .4) ) # Example usage plot_clustered_heatmap(\'wine.csv\') ```","solution":"import seaborn as sns import pandas as pd from sklearn.preprocessing import StandardScaler def plot_clustered_heatmap(file_path: str) -> None: Reads wine dataset, preprocesses the data, and generates a clustered heatmap. Parameters: file_path (str): The file path to the wine.csv dataset. # Step 1: Read the dataset data = pd.read_csv(file_path) # Step 2: Preprocess the data wine_class = data.pop(\\"class\\") scaler = StandardScaler() scaled_data = scaler.fit_transform(data) scaled_data = pd.DataFrame(scaled_data, columns=data.columns) # Step 3: Generate color labels for each class lut = dict(zip(wine_class.unique(), sns.color_palette(\\"viridis\\", len(wine_class.unique())))) row_colors = wine_class.map(lut) # Step 4: Generate the clustered heatmap sns.clustermap( scaled_data, figsize=(10, 7), row_cluster=False, col_cluster=True, row_colors=row_colors, cmap=\\"viridis\\", cbar_pos=(0.1, 0.8, .03, .4) )"},{"question":"**Title:** Multi-Threaded Task Processing with Python Queues **Objective:** To assess the understanding of Python\'s `queue` module, threading, and the ability to implement concurrent task processing using different types of queues (`Queue`, `LifoQueue`, and `PriorityQueue`). **Problem:** You are tasked with implementing a multi-threaded task processing system using Python\'s `queue` module. Your system should be able to handle tasks concurrently from multiple producers and consumers using three different types of queues (`Queue`, `LifoQueue`, and `PriorityQueue`). Each type of queue should run separately. **Requirements:** 1. Implement a function named `process_tasks` that accepts three arguments: - `task_list`: A list of tasks where each task is represented by a tuple `(task_id, priority)`. `task_id` is an integer representing the task, and `priority` is an integer representing the task priority for the `PriorityQueue`. - `queue_type`: A string indicating the type of queue to use. It can be `\\"fifo\\"`, `\\"lifo\\"`, or `\\"priority\\"`. - `num_threads`: An integer representing the number of consumer threads. 2. Based on the `queue_type`, initialize the appropriate queue (`Queue`, `LifoQueue`, or `PriorityQueue`). 3. Implement producer threads that will put tasks from `task_list` into the chosen queue. There should be one producer thread for simplicity. 4. Implement consumer threads that will fetch tasks from the queue and process them. The task processing should simply involve printing the `task_id` and the type of queue being used. 5. Ensure each consumer thread calls `task_done` after task processing, and the main thread should call `join` to wait for all tasks to be completed. **Function Signature:** ```python import queue import threading from typing import List, Tuple def process_tasks(task_list: List[Tuple[int, int]], queue_type: str, num_threads: int) -> None: pass ``` **Example:** ```python task_list = [(1, 2), (2, 1), (3, 3), (4, 0)] process_tasks(task_list, \'fifo\', 3) # Output should print task processing in FIFO order by 3 consumer threads process_tasks(task_list, \'lifo\', 3) # Output should print task processing in LIFO order by 3 consumer threads process_tasks(task_list, \'priority\', 3) # Output should print task processing in priority order by 3 consumer threads ``` **Constraints:** 1. `task_list` will contain at least 1 task and at most 100 tasks. 2. `num_threads` will be at least 1 and at most 10. 3. The task processing of each `task_id` should be completed exactly once. **Notes:** - You should use the appropriate methods (`put`, `get`, `task_done`, and `join`) as described in the provided documentation. - Handle exceptions like `queue.Empty` and `queue.Full` where appropriate. - Ensure thread safety and proper synchronization for the concurrent processing. This question tests your ability to use Python\'s `queue` module effectively in a multi-threaded environment and your understanding of different queue types and their behaviors.","solution":"import queue import threading from typing import List, Tuple def worker(q, queue_type): while True: try: if queue_type == \'priority\': priority, task_id = q.get(timeout=1) # Blocking call with timeout else: task_id = q.get(timeout=1) # Blocking call with timeout print(f\\"Processing task {task_id} from {queue_type} queue\\") q.task_done() except queue.Empty: break def process_tasks(task_list: List[Tuple[int, int]], queue_type: str, num_threads: int) -> None: if queue_type == \'fifo\': q = queue.Queue() elif queue_type == \'lifo\': q = queue.LifoQueue() elif queue_type == \'priority\': q = queue.PriorityQueue() else: raise ValueError(\\"Invalid queue type. Must be \'fifo\', \'lifo\', or \'priority\'.\\") # Start producer thread def producer(): for task in task_list: if queue_type == \'priority\': q.put(task) else: q.put(task[0]) q.join() producer_thread = threading.Thread(target=producer) producer_thread.start() # Start consumer threads threads = [] for _ in range(num_threads): t = threading.Thread(target=worker, args=(q, queue_type)) t.start() threads.append(t) producer_thread.join() # Wait for all worker threads to finish for t in threads: t.join()"},{"question":"Context: Sets and frozensets are important data structures in Python, allowing the storage of unique, unordered elements with performant membership testing. Understanding their properties and behavior is essential for efficient coding in Python. Problem Statement: You are required to implement two functions `process_sets` which will take in a list of operations to perform on a set, and another function `process_frozensets` which will take in a list of operations to perform on a frozenset. These operations can include adding elements, removing elements, checking membership, and union/intersection of sets. Function 1: process_sets Write a function `process_sets(operations: List[Tuple[str, Union[Tuple[Any, ...], Any]]]) -> Any` that processes a list of operations on an initially empty set. Return the final state of the set or the output of the last operation. The operations can be: - `\'add\', element`: Add an element to the set. - `\'remove\', element`: Remove an element from the set. If the element is not in the set, return \'Element not found\'. - `\'discard\', element`: Discard an element from the set. This does not raise an error if the element is not found. - `\'contains\', element`: Check if an element is in the set and return `True` or `False`. - `\'clear\'`: Removes all elements from the set. - `\'size\'`: Returns the size of the set. Function 2: process_frozensets Write a function `process_frozensets(operations: List[Tuple[str, Union[Tuple[Any, ...], Any]]]) -> Any` that processes a list of operations on a frozenset. Return the output of the last operation. The operations can be: - `\'union\', iterable`: Perform a union operation with the frozenset and another iterable and return the result as a frozenset. - `\'intersection\', iterable`: Perform an intersection operation with the frozenset and another iterable and return the result as a frozenset. - `\'contains\', element`: Check if an element is in the frozenset and return `True` or `False`. - `\'size\'`: Returns the size of the frozenset. # Example Usage ```python # Example for process_sets operations = [ (\'add\', 1), (\'add\', 2), (\'contains\', 3), (\'size\',), (\'remove\', 2), (\'size\',), ] assert process_sets(operations) == 1 # Example for process_frozensets operations = [ (\'union\', {1, 2}), (\'contains\', 2), (\'intersection\', {1}), (\'size\',), ] assert process_frozensets(operations) == 1 ``` # Constraints: - The list of operations is guaranteed to be non-empty. - Elements are hashable and compatible with set operations. - Assume all inputs are valid according to the specified constraints. Implementing these functions will demonstrate an understanding of `set` and `frozenset` operations in Python, coupling them with Pythons functionality to ensure correctness under different conditions.","solution":"from typing import List, Tuple, Union, Any def process_sets(operations: List[Tuple[str, Union[Tuple[Any, ...], Any]]]) -> Any: s = set() result = None for op in operations: if op[0] == \'add\': s.add(op[1]) elif op[0] == \'remove\': if op[1] in s: s.remove(op[1]) else: return \'Element not found\' elif op[0] == \'discard\': s.discard(op[1]) elif op[0] == \'contains\': result = op[1] in s elif op[0] == \'clear\': s.clear() elif op[0] == \'size\': result = len(s) return result if result is not None else s def process_frozensets(operations: List[Tuple[str, Union[Tuple[Any, ...], Any]]]) -> Any: fs = frozenset() result = None for op in operations: if op[0] == \'union\': fs = fs.union(op[1]) result = fs elif op[0] == \'intersection\': fs = fs.intersection(op[1]) result = fs elif op[0] == \'contains\': result = op[1] in fs elif op[0] == \'size\': result = len(fs) return result if result is not None else fs"},{"question":"**Coding Assessment Question** # Objective Your task is to demonstrate your proficiency with the scikit-learn package by loading one of the available toy datasets, preprocessing it, and training a machine learning model to make predictions. You will also evaluate the model\'s performance. # Problem Statement 1. Load the `load_wine` dataset from scikit-learn. 2. Split the dataset into training and testing sets. 3. Preprocess the data using StandardScaler. 4. Train a Logistic Regression model on the training set. 5. Evaluate the model\'s performance on the testing set using accuracy score and confusion matrix. 6. Plot the confusion matrix using matplotlib. # Instructions and Requirements 1. **Loading the Dataset:** - Use the `load_wine` function from `sklearn.datasets` to load the dataset. 2. **Splitting the Dataset:** - Split the dataset into training (80%) and testing (20%) sets using `train_test_split` from `sklearn.model_selection`. 3. **Data Preprocessing:** - Standardize the features by removing the mean and scaling to unit variance using `StandardScaler` from `sklearn.preprocessing`. 4. **Model Training:** - Use the `LogisticRegression` model from `sklearn.linear_model`. 5. **Model Evaluation:** - Compute the accuracy score using `accuracy_score` from `sklearn.metrics`. - Compute the confusion matrix using `confusion_matrix` from `sklearn.metrics`. 6. **Visualization:** - Plot the confusion matrix using matplotlib. # Expected Input and Output - **Input:** No user input is required. - **Output:** The code should output the accuracy score and display the confusion matrix plot. # Constraints - Use default parameters for the LogisticRegression model. - Ensure that your code is well-commented and easy to read. # Performance Requirements - The solution should run efficiently for the provided dataset. # Example Solution ```python import matplotlib.pyplot as plt from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay # Load the dataset data = load_wine() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the Logistic Regression model model = LogisticRegression() model.fit(X_train_scaled, y_train) # Evaluate the model y_pred = model.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') # Compute and plot the confusion matrix cm = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names) disp.plot(cmap=plt.cm.Blues) plt.show() ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay def load_and_train_wine_model(): # Load the dataset data = load_wine() X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Train the Logistic Regression model model = LogisticRegression(max_iter=10000) model.fit(X_train_scaled, y_train) # Evaluate the model y_pred = model.predict(X_test_scaled) accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy}\') # Compute and plot the confusion matrix cm = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names) disp.plot(cmap=plt.cm.Blues) plt.show() return model, scaler, X_test, y_test, y_pred # Run the function to load data, train the model, and display results load_and_train_wine_model()"},{"question":"Objective: Design and implement a mini-application that reads emails from a `Maildir` format mailbox, processes specific information from each email, and outputs a summary. Task: 1. Create a class `MailProcessor` that initializes with a path to a `Maildir` format mailbox. 2. Implement a method `process_emails` within `MailProcessor` that performs the following operations: - Reads all emails in the mailbox. - Extracts the `From`, `To`, `Subject`, and `Date` headers from each email. - Creates a summary dictionary where keys are email addresses found in the `From` or `To` headers and values are lists of subject lines of emails sent or received by that address. - Handles cases where emails might be malformed or headers might be missing gracefully, skipping such emails. 3. The `process_emails` method should return the summary dictionary. Input: - Path to the `Maildir` format mailbox directory. Output: - A dictionary where keys are email addresses and values are lists of subject lines. Example: ```python # Assuming the mailbox is in the \\"Maildir-Example\\" directory processor = MailProcessor(\\"Maildir-Example\\") summary = processor.process_emails() # Example output format # { # \\"sender@example.com\\": [\\"Subject 1\\", \\"Subject 2\\"], # \\"recipient@example.com\\": [\\"Subject 1\\"] # } ``` Constraints: - Ensure the mailbox is processed in a thread-safe manner, even if concurrent modifications are possible (though `Maildir` does not require locking, demonstrate appropriate handling where necessary). - Skip any emails that are malformed or missing the required headers without crashing the program. Notes: - Pay attention to Python\'s standard exception handling to manage potential errors during email processing. - Utilize classes and methods effectively to create a modular and readable code structure. Implementation Requirements: 1. The class `MailProcessor` should make use of the `mailbox.Maildir` class to work with the mailbox. 2. Use the `email` module to read and parse email messages.","solution":"import mailbox from email import message_from_bytes from collections import defaultdict class MailProcessor: def __init__(self, path): self.path = path def process_emails(self): summary = defaultdict(list) # Open the Maildir mailbox mbox = mailbox.Maildir(self.path) for message in mbox: try: # Read the message raw_message = message.as_bytes() email_message = message_from_bytes(raw_message) from_address = email_message.get(\'From\') to_address = email_message.get(\'To\') subject = email_message.get(\'Subject\') if from_address: summary[from_address].append(subject) if to_address: to_addresses = to_address.split(\', \') for address in to_addresses: summary[address].append(subject) except Exception as e: # Skip malformed emails or emails missing required headers continue return dict(summary)"},{"question":"**Coding Assessment Question** **Objective**: Demonstrate your understanding of Python\'s `codecs` module by implementing functions that handle text encoding, decoding, and error handling. **Problem Statement**: You are tasked with creating a `TextProcessor` class that can encode and decode strings using specified encodings and error handling schemes. The class should also handle streams and support incremental encoding/decoding. **Specifications**: 1. **TextProcessor Class**: - **Initialization**: - Accepts `encoding` and `errors` as parameters. - `encoding`: The name of the encoding to be used (default is `\'utf-8\'`). - `errors`: The error handling scheme to be used (default is `\'strict\'`). - **Methods**: - `encode_text(self, text: str) -> bytes`: Encodes the given text using the specified encoding and error handling scheme. - `decode_text(self, byte_data: bytes) -> str`: Decodes the given byte data using the specified encoding and error handling scheme. - `read_stream(self, stream) -> str`: Reads from the stream and decodes the content using the specified encoding and error handling scheme. - `write_stream(self, stream, text: str)`: Encodes the text and writes it to the stream using the specified encoding and error handling scheme. - `incremental_encode(self, text: str) -> list`: Incrementally encodes the given text and returns a list of encoded chunks. - `incremental_decode(self, byte_chunks: list) -> str`: Incrementally decodes the given byte chunks and returns the decoded text. **Constraints**: - Use only the `codecs` module to implement the encoding, decoding, and stream handling functionalities. - The implementations should handle empty input gracefully. - Ensure that error handling schemes provided are respected during encoding and decoding. **Example Usage**: ```python # Initialize the TextProcessor with utf-8 encoding and replace error handling processor = TextProcessor(encoding=\'utf-8\', errors=\'replace\') # Encode text encoded_text = processor.encode_text(\'Hello, world! \') print(encoded_text) # Decode bytes decoded_text = processor.decode_text(encoded_text) print(decoded_text) # Work with streams import io stream = io.BytesIO() processor.write_stream(stream, \'Stream content \') stream.seek(0) print(processor.read_stream(stream)) # Incremental encoding and decoding text_chunks = processor.incremental_encode(\'Incrementally encode this text.\') print(text_chunks) decoded_text = processor.incremental_decode(text_chunks) print(decoded_text) ``` **Submission**: Submit your implementation of the `TextProcessor` class. Ensure your code passes all the example usages and handles edge cases appropriately.","solution":"import codecs class TextProcessor: def __init__(self, encoding=\'utf-8\', errors=\'strict\'): self.encoding = encoding self.errors = errors def encode_text(self, text: str) -> bytes: return codecs.encode(text, self.encoding, self.errors) def decode_text(self, byte_data: bytes) -> str: return codecs.decode(byte_data, self.encoding, self.errors) def read_stream(self, stream) -> str: reader = codecs.getreader(self.encoding)(stream, errors=self.errors) return reader.read() def write_stream(self, stream, text: str): writer = codecs.getwriter(self.encoding)(stream, errors=self.errors) writer.write(text) def incremental_encode(self, text: str) -> list: encoder = codecs.getincrementalencoder(self.encoding)(errors=self.errors) encoded_chunks = [] for char in text: encoded_chunks.append(encoder.encode(char)) encoded_chunks.append(encoder.encode(\'\', final=True)) # Final chunk return encoded_chunks def incremental_decode(self, byte_chunks: list) -> str: decoder = codecs.getincrementaldecoder(self.encoding)(errors=self.errors) decoded_text = \'\' for chunk in byte_chunks: decoded_text += decoder.decode(chunk) decoded_text += decoder.decode(b\'\', final=True) # Ensure final decode return decoded_text"},{"question":"# Python Coding Assessment Question **Objective**: This task assesses your understanding of handling various types of I/O streams in Python using the `io` module. Your solution should demonstrate your comprehension of both basic and advanced I/O operations. **Problem Statement**: You are required to implement a function `copy_file_content` that copies the content from a source file to a destination file. The function should have options to perform this operation in text mode, binary mode, or raw (unbuffered) mode. Additionally, the function should handle encoding and decoding appropriately when in text mode. Function Signature: ```python def copy_file_content( src_path: str, dest_path: str, mode: str, encoding: str = \\"utf-8\\", buffering: int = -1 ) -> None: pass ``` Parameters: - `src_path` (str): The path to the source file. - `dest_path` (str): The path to the destination file. - `mode` (str): A string indicating the mode of operation. It can be: - `\'text\'`: Text mode - `\'binary\'`: Binary mode - `\'raw\'`: Raw (unbuffered) mode - `encoding` (str): The encoding to use for text mode. Default is \\"utf-8\\". - `buffering` (int): The buffer size. Default is `-1` (use system default). Constraints: - The function should read the content from the source file and write it to the destination file in the specified mode. - If the source file does not exist, raise a `FileNotFoundError`. - For text mode, ensure text content is correctly encoded and decoded using the provided encoding. - For binary mode, handle bytes-like objects appropriately. - For raw mode, perform unbuffered I/O operations. Example Usage: ```python try: copy_file_content(\\"source.txt\\", \\"destination.txt\\", mode=\\"text\\", encoding=\\"utf-8\\") print(\\"Text file copied successfully.\\") copy_file_content(\\"source.jpg\\", \\"destination_copy.jpg\\", mode=\\"binary\\") print(\\"Binary file copied successfully.\\") copy_file_content(\\"source.raw\\", \\"destination_copy.raw\\", mode=\\"raw\\", buffering=0) print(\\"Raw file copied successfully.\\") except Exception as e: print(f\\"An error occurred: {e}\\") ``` Notes: 1. You must utilize the appropriate classes and methods from the `io` module as documented. 2. Handle any necessary error checking and provide meaningful error messages. 3. Optimize your solution for clarity and performance where possible.","solution":"import io import os def copy_file_content(src_path: str, dest_path: str, mode: str, encoding: str = \\"utf-8\\", buffering: int = -1) -> None: Copies content from src_path to dest_path based on the specified mode, encoding, and buffering. Parameters: src_path (str): Source file path dest_path (str): Destination file path mode (str): Mode of operation (\'text\', \'binary\', \'raw\') encoding (str): Encoding to use for text mode, default is \\"utf-8\\" buffering (int): Buffer size, default is -1 (system default) Raises: FileNotFoundError: If the source file does not exist. ValueError: If the mode is invalid. if not os.path.exists(src_path): raise FileNotFoundError(f\\"Source file \'{src_path}\' not found.\\") valid_modes = [\\"text\\", \\"binary\\", \\"raw\\"] if mode not in valid_modes: raise ValueError(f\\"Invalid mode \'{mode}\', expected one of {valid_modes}.\\") if mode == \\"text\\": with io.open(src_path, \'r\', encoding=encoding, buffering=buffering) as src_file: with io.open(dest_path, \'w\', encoding=encoding, buffering=buffering) as dest_file: for line in src_file: dest_file.write(line) elif mode == \\"binary\\": with io.open(src_path, \'rb\', buffering=buffering) as src_file: with io.open(dest_path, \'wb\', buffering=buffering) as dest_file: while chunk := src_file.read(1024): dest_file.write(chunk) elif mode == \\"raw\\": with io.open(src_path, \'rb\', buffering=0) as src_file: with io.open(dest_path, \'wb\', buffering=0) as dest_file: while chunk := src_file.read(1024): dest_file.write(chunk)"},{"question":"# Question You are given a directory path, and you need to create a Python function that traverses this directory tree recursively using the techniques from the `stat` module. During the traversal, your function should count and return the number of each type of file encountered, including directories, regular files, symbolic links, and any other file types supported by the `stat` module. Function Signature ```python def count_file_types(directory: str) -> dict: pass ``` Input - `directory` (str): A string representing the path to the top-level directory you need to traverse. Output - A dictionary where the keys are file types (e.g., \'directory\', \'regular\', \'link\', etc.) and the values are the counts of these file types. Constraints - The solution should handle large directory structures efficiently. - Ensure that the solution works for different operating systems (Unix and Windows). - You should utilize the provided `stat` functions to identify the types of files. Example ```python input_directory = \\"/path/to/directory\\" result = count_file_types(input_directory) print(result) # Example output: {\'directory\': 10, \'regular\\": 45, \'link\': 5, \'fifo\': 1} ``` Implementation Notes - You may use `os.listdir`, `os.path.join`, `os.lstat`, and functions from the `stat` module for this task. - Use `os.walk` to traverse directories if needed, but avoid unnecessary system calls for better performance. - Convert file modes to their human-readable forms when counting. # Example Implementation ```python import os import stat def count_file_types(directory): file_types_count = { \'directory\': 0, \'regular\': 0, \'link\': 0, \'fifo\': 0, \'socket\': 0, \'char\': 0, \'block\': 0, \'door\': 0, \'port\': 0, \'whiteout\': 0 } def walktree(top): for f in os.listdir(top): pathname = os.path.join(top, f) mode = os.lstat(pathname).st_mode if stat.S_ISDIR(mode): file_types_count[\'directory\'] += 1 walktree(pathname) elif stat.S_ISREG(mode): file_types_count[\'regular\'] += 1 elif stat.S_ISLNK(mode): file_types_count[\'link\'] += 1 elif stat.S_ISFIFO(mode): file_types_count[\'fifo\'] += 1 elif stat.S_ISSOCK(mode): file_types_count[\'socket\'] += 1 elif stat.S_ISCHR(mode): file_types_count[\'char\'] += 1 elif stat.S_ISBLK(mode): file_types_count[\'block\'] += 1 elif getattr(stat, \'S_ISDOOR\', lambda mode: 0)(mode): file_types_count[\'door\'] += 1 elif getattr(stat, \'S_ISPORT\', lambda mode: 0)(mode): file_types_count[\'port\'] += 1 elif getattr(stat, \'S_ISWHT\', lambda mode: 0)(mode): file_types_count[\'whiteout\'] += 1 walktree(directory) return file_types_count ```","solution":"import os import stat def count_file_types(directory: str) -> dict: file_types_count = { \'directory\': 0, \'regular\': 0, \'link\': 0, \'fifo\': 0, \'socket\': 0, \'char\': 0, \'block\': 0, \'door\': 0, \'port\': 0, \'whiteout\': 0 } def walktree(top): for f in os.listdir(top): pathname = os.path.join(top, f) try: mode = os.lstat(pathname).st_mode except Exception as e: continue if stat.S_ISDIR(mode): file_types_count[\'directory\'] += 1 walktree(pathname) elif stat.S_ISREG(mode): file_types_count[\'regular\'] += 1 elif stat.S_ISLNK(mode): file_types_count[\'link\'] += 1 elif stat.S_ISFIFO(mode): file_types_count[\'fifo\'] += 1 elif stat.S_ISSOCK(mode): file_types_count[\'socket\'] += 1 elif stat.S_ISCHR(mode): file_types_count[\'char\'] += 1 elif stat.S_ISBLK(mode): file_types_count[\'block\'] += 1 elif getattr(stat, \'S_ISDOOR\', lambda mode: 0)(mode): file_types_count[\'door\'] += 1 elif getattr(stat, \'S_ISPORT\', lambda mode: 0)(mode): file_types_count[\'port\'] += 1 elif getattr(stat, \'S_ISWHT\', lambda mode: 0)(mode): file_types_count[\'whiteout\'] += 1 walktree(directory) return file_types_count"},{"question":"Title: Advanced Text Processing with Regular Expressions Objective: You are required to write a Python function that processes a given text to extract and modify specific patterns using regular expressions. Problem Statement: Write a function `extract_emails_and_replace_dates` that takes a string containing multiple lines of text. This function should perform the following tasks: 1. Extract all email addresses from the text and return them as a list. 2. Replace all date formats in the text with a standardized format `YYYY-MM-DD`. The text may contain dates in various formats such as: - \\"DD/MM/YYYY\\" - \\"DD-MM-YYYY\\" - \\"MM/DD/YYYY\\" - \\"MM-DD-YYYY\\" You should convert these dates to the format `YYYY-MM-DD`. Input: - A multi-line string `text`. Output: - A tuple containing: 1. A list of extracted email addresses. 2. The modified text with all dates replaced by the standardized format `YYYY-MM-DD`. Function Signature: ```python def extract_emails_and_replace_dates(text: str) -> tuple[list[str], str]: ``` Constraints: - The string can be of length up to (10^6) characters. - There can be a maximum of (10^4) lines in the text. - Assume valid email addresses follow the pattern `something@domain.extension`. Example: ```python text = Please contact us at support@example.com for assistance. The event is scheduled on 12/24/2023. For more information, send an email to info@example.org. Another important date is 01-15-2024. result = extract_emails_and_replace_dates(text) print(result) # Expected output: # ([\'support@example.com\', \'info@example.org\'], # \\"Please contact us at support@example.com for assistance. # The event is scheduled on 2023-12-24. # For more information, send an email to info@example.org. # Another important date is 2024-01-15.\\") ``` Requirements: 1. Use regular expressions to implement the solution. 2. Ensure proper handling of different date formats and correct conversion. 3. Efficiently process the text to meet the performance constraints. Hints: - Use groups to capture different parts of the date and reassemble them in the desired format. - Consider using non-capturing groups where necessary to simplify your regular expression.","solution":"import re def extract_emails_and_replace_dates(text: str) -> tuple[list[str], str]: # Regular expression for extracting email addresses email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,7}b\' # Regular expression for matching dates in various formats date_pattern = r\'(b(?:d{2}[/-]d{2}[/-]d{4})b)\' # Extract email addresses emails = re.findall(email_pattern, text) # Function to replace dates with standardized format YYYY-MM-DD def replace_date(match): date = match.group(0) if \'-\' in date: day, month, year = date.split(\'-\') else: day, month, year = date.split(\'/\') # Ensure day and month have leading zeros if necessary if int(day) > 12: return f\'{year}-{month.zfill(2)}-{day.zfill(2)}\' else: return f\'{year}-{day.zfill(2)}-{month.zfill(2)}\' # Replace dates modified_text = re.sub(date_pattern, replace_date, text) return emails, modified_text"},{"question":"# Question: Advanced Serialization with the `marshal` Module In this exercise, you will demonstrate your proficiency with the `marshal` module by writing a serialization and deserialization utility. Your task is to implement two functions: `serialize_data` and `deserialize_data`. `serialize_data(value: Any, file_path: str, version: int = marshal.version) -> None` This function should: - Serialize the given value using the `marshal.dumps` function. - Write the serialized bytes to the specified file path in binary mode. # Parameters: - `value`: The Python object to be serialized. It can be of any type supported by the `marshal` module. - `file_path`: The file path (as a string) where the serialized data should be saved. - `version`: An optional integer indicating the version of the `marshal` format to be used. Defaults to the current `marshal.version`. # Returns: - The function does not return anything. `deserialize_data(file_path: str) -> Any` This function should: - Read the serialized bytes from the specified file path in binary mode. - Deserialize the bytes into a Python object using the `marshal.loads` function. - Handle the errors `EOFError`, `ValueError`, and `TypeError` gracefully, returning `None` in case any of these exceptions are raised. # Parameters: - `file_path`: The file path (as a string) from which the serialized data should be read. # Returns: - The deserialized Python object, or `None` if deserialization fails. Additional Requirements: - Include appropriate error handling to ensure your functions are robust. - Make sure the serialized data can be correctly read back, confirming it matches the original data. Here is a template to get you started: ```python import marshal def serialize_data(value, file_path, version=marshal.version): try: with open(file_path, \'wb\') as file: marshalled_data = marshal.dumps(value, version) file.write(marshalled_data) except Exception as e: print(f\\"Serialization error: {e}\\") raise def deserialize_data(file_path): try: with open(file_path, \'rb\') as file: marshalled_data = file.read() return marshal.loads(marshalled_data) except (EOFError, ValueError, TypeError) as e: print(f\\"Deserialization error: {e}\\") return None ``` # Constraints: - Ensure the functions are tested with various types of data, including nested collections and edge cases. # Example Usage: ```python data = { \'key1\': [1, 2, 3], \'key2\': (\'a\', \'b\', \'c\'), \'key3\': {\'nested_key\': set([True, False])} } file_path = \\"data.marshal\\" serialize_data(data, file_path) loaded_data = deserialize_data(file_path) assert data == loaded_data ``` Write the `serialize_data` and `deserialize_data` functions as specified.","solution":"import marshal def serialize_data(value, file_path, version=marshal.version): try: with open(file_path, \'wb\') as file: marshalled_data = marshal.dumps(value, version) file.write(marshalled_data) except Exception as e: print(f\\"Serialization error: {e}\\") raise def deserialize_data(file_path): try: with open(file_path, \'rb\') as file: marshalled_data = file.read() return marshal.loads(marshalled_data) except (EOFError, ValueError, TypeError) as e: print(f\\"Deserialization error: {e}\\") return None"},{"question":"Objective: Demonstrate your understanding of Seaborn\'s color palette functionalities and basic plotting capabilities. Problem Statement: You are provided with the following dataset: ```python import pandas as pd data = { \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\'], \'Values\': [10, 15, 7, 18, 5, 13, 8, 20] } df = pd.DataFrame(data) ``` Using the provided dataset, you need to: 1. Plot a bar graph of `Values` against `Category` using default plot settings. 2. Modify the plot to use a \\"pastel\\" color palette. 3. Customize the color palette to use a \\"blend:#7AB,#EDA\\" gradient as a colormap. 4. Display the hex code values of the \\"blend:#7AB,#EDA\\" palette used. Input: - No user input is required; use the provided dataset. Output: - A single visualization for each of the steps (1 to 3). - A list of hex color values used in step 3. Constraints: - Use Seaborn for all visualizations. - Ensure plots are well-labeled and aesthetically pleasing. Implementation Details: Provide a function `customize_palette_plot()` that performs the tasks outlined above. ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def customize_palette_plot(): # Provided dataset data = { \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\'], \'Values\': [10, 15, 7, 18, 5, 13, 8, 20] } df = pd.DataFrame(data) # Step 1: Plot with default settings # your code here # Step 2: Modify the plot to use a \\"pastel\\" color palette # your code here # Step 3: Customize the color palette to use a \\"blend:#7AB,#EDA\\" gradient as a colormap # your code here # Step 4: Display the hex color values of the \\"blend:#7AB,#EDA\\" palette used # your code here customize_palette_plot() ``` **Example Execution:** When the function `customize_palette_plot()` is called, it should produce the following: 1. A bar plot of categories with default color settings. 2. A similar bar plot using a \\"pastel\\" color palette. 3. A bar plot using a custom blended gradient palette. 4. A printout of the hex values of the custom palette used in the last plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def customize_palette_plot(): # Provided dataset data = { \'Category\': [\'A\', \'B\', \'C\', \'D\', \'E\', \'F\', \'G\', \'H\'], \'Values\': [10, 15, 7, 18, 5, 13, 8, 20] } df = pd.DataFrame(data) # Step 1: Plot with default settings plt.figure(figsize=(10, 6)) sns.barplot(x=\'Category\', y=\'Values\', data=df) plt.title(\'Bar Plot with Default Settings\') plt.show() # Step 2: Modify the plot to use a \\"pastel\\" color palette plt.figure(figsize=(10, 6)) sns.barplot(x=\'Category\', y=\'Values\', data=df, palette=\'pastel\') plt.title(\'Bar Plot with Pastel Color Palette\') plt.show() # Step 3: Customize the color palette to use a \\"blend:#7AB,#EDA\\" gradient as a colormap custom_palette = sns.color_palette(\\"blend:#7AB,#EDA\\", n_colors=len(df)) plt.figure(figsize=(10, 6)) sns.barplot(x=\'Category\', y=\'Values\', data=df, palette=custom_palette) plt.title(\'Bar Plot with Custom Blend Gradient Palette\') plt.show() # Step 4: Display the hex color values of the \\"blend:#7AB,#EDA\\" palette used hex_colors = sns.color_palette(\\"blend:#7AB,#EDA\\", n_colors=len(df)).as_hex() print(\\"Hex color values of the custom palette:\\", hex_colors) return hex_colors hex_colors_used = customize_palette_plot()"},{"question":"**Coding Question: Implementing an Out-of-Core Text Classification System** **Objective:** Design a class `OutOfCoreTextClassifier` that enables efficient text classification using out-of-core learning techniques from scikit-learn. Your implementation should demonstrate the ability to handle large datasets that do not fit into memory by streaming data, extracting features, and using an incremental learning algorithm. **Requirements:** 1. Implement a method to stream instances from a file. 2. Implement a method for feature extraction using `HashingVectorizer`. 3. Implement incremental learning using `SGDClassifier`. 4. The class should have methods to train incrementally on mini-batches and predict on new data. **Input and Output Formats:** 1. `fit(filename, mini_batch_size)`: Method to train the classifier incrementally. - `filename`: A string representing the path to the dataset file. Each line in the file contains one instance with text and its label, separated by a comma. - `mini_batch_size`: An integer representing the number of instances in each mini-batch. 2. `predict(texts)`: Method to predict labels for new instances. - `texts`: A list of strings where each string represents a text instance. - Returns: A list of predicted labels corresponding to the input texts. **Constraints:** - Assume the dataset is too large to fit into memory. - You must use scikit-learn\'s `HashingVectorizer` for feature extraction. - You must use scikit-learn\'s `SGDClassifier` for incremental learning. - Ensure the code is efficient and does not load the entire dataset into memory at once. **Example:** ```python # Example usage: classifier = OutOfCoreTextClassifier() classifier.fit(\'large_text_dataset.csv\', mini_batch_size=100) # Predicting new instances new_texts = [\\"The movie was great\\", \\"I hated the food\\"] predictions = classifier.predict(new_texts) print(predictions) # Output: [1, 0], assuming 1 is the label for positive sentiment, and 0 for negative ``` Ensure your implementation also includes necessary error handling and is optimized for performance given the constraints of large data.","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier class OutOfCoreTextClassifier: def __init__(self): self.vectorizer = HashingVectorizer(n_features=2**20) self.classifier = SGDClassifier() self.is_fitted = False def stream_instances(self, filename, batch_size): with open(filename, \'r\') as f: texts, labels = [], [] for line in f: text, label = line.strip().rsplit(\',\', 1) texts.append(text) labels.append(int(label)) if len(texts) == batch_size: yield texts, labels texts, labels = [], [] if texts: yield texts, labels def fit(self, filename, mini_batch_size): for texts, labels in self.stream_instances(filename, mini_batch_size): X = self.vectorizer.transform(texts) y = labels if not self.is_fitted: self.classifier.partial_fit(X, y, classes=[0, 1]) self.is_fitted = True else: self.classifier.partial_fit(X, y) def predict(self, texts): X = self.vectorizer.transform(texts) return self.classifier.predict(X)"},{"question":"You are tasked to implement a function that uses the `pwd` module to extract specific user information and return it in a formatted way. Your function should access the password database to retrieve details about users based on their user IDs or usernames. # Function Signature ```python def get_user_info(identifier: str) -> dict: pass ``` # Input - `identifier (str)`: An identifier which can either be a username or a user ID (numeric string). # Output - `dict`: A dictionary containing the following keys: - `name` (str): The user\'s login name. - `user_id` (int): The user\'s numerical user ID. - `group_id` (int): The user\'s numerical group ID. - `home_directory` (str): The user\'s home directory. - `shell` (str): The user\'s command interpreter. # Constraints 1. The function should handle both username and user ID as input. 2. If the user does not exist, the function should raise a `KeyError`. 3. You can assume the input identifier will either be a valid username or user ID that can be converted to an integer without error. # Example ```python # Example 1: # Assuming a user with username \\"alice\\" and user ID 1001 exists in the database print(get_user_info(\\"alice\\")) # Expected Output: # { # \\"name\\": \\"alice\\", # \\"user_id\\": 1001, # \\"group_id\\": <group_id_of_alice>, # \\"home_directory\\": <home_directory_of_alice>, # \\"shell\\": <shell_of_alice> # } # Example 2: # Assuming a user with user ID 1001 exists in the database print(get_user_info(\\"1001\\")) # Expected Output: # { # \\"name\\": <username_of_1001>, # \\"user_id\\": 1001, # \\"group_id\\": <group_id_of_1001>, # \\"home_directory\\": <home_directory_of_1001>, # \\"shell\\": <shell_of_1001> # } # Example 3: # If the user does not exist print(get_user_info(\\"non_existing_user\\")) # Should raise KeyError ``` Your task is to implement the `get_user_info` function by utilizing the `pwd` module as described above.","solution":"import pwd def get_user_info(identifier: str) -> dict: Retrieves user information from the password database using the pwd module. Args: identifier (str): An identifier which can either be a username or a user ID (numeric string). Returns: dict: A dictionary containing user information. Raises: KeyError: If the user does not exist. try: if identifier.isdigit(): user_info = pwd.getpwuid(int(identifier)) else: user_info = pwd.getpwnam(identifier) return { \\"name\\": user_info.pw_name, \\"user_id\\": user_info.pw_uid, \\"group_id\\": user_info.pw_gid, \\"home_directory\\": user_info.pw_dir, \\"shell\\": user_info.pw_shell } except KeyError: raise KeyError(f\\"No such user: \'{identifier}\'\\")"},{"question":"# MIME Email Construction and Manipulation You are tasked with creating a complete MIME email message using the `email.mime` package classes. The email should include text, an attached image, and an attached text file. You need to ensure proper encoding and inclusion of necessary headers for each part of the email. **Requirements:** 1. Use the `MIMEMultipart` class to create the root MIME message. 2. Add a plain text part as the body of the email using the `MIMEText` class. 3. Attach an image to the email using the `MIMEImage` class. The image should be encoded in base64 and must include the appropriate `Content-Type` header. 4. Attach a text file to the email using the `MIMEApplication` class. The text file should also be encoded in base64. # Input - Plain text for the email body: `email_body_text` - Path to the image file: `image_path` - Path to the text file: `text_file_path` # Output - Return the resulting MIME email message as a string. # Constraints - You may assume that the provided image and text file paths are valid and accessible. - The generated email should be properly formatted to be sent using an SMTP server. Example Usage ```python email_body_text = \\"Hello, this is the body of the email.\\" image_path = \\"path/to/image.png\\" text_file_path = \\"path/to/document.txt\\" email_message = create_mime_email(email_body_text, image_path, text_file_path) print(email_message) # The result should be a properly formatted MIME email message string ``` # Function Signature ```python def create_mime_email(email_body_text: str, image_path: str, text_file_path: str) -> str: pass ``` # Implementation Guidelines 1. Create a root `MIMEMultipart` object. 2. Create a `MIMEText` object for the email body and attach it to the root. 3. Read the image file in binary mode, create a `MIMEImage` object, and attach it to the root. 4. Read the text file in binary mode, create a `MIMEApplication` object, and attach it to the root. 5. Ensure all parts are properly encoded and all necessary headers are included. Complete the implementation of the function `create_mime_email` that constructs the MIME email as described above.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email import encoders def create_mime_email(email_body_text: str, image_path: str, text_file_path: str) -> str: # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = \'Subject\' msg[\'From\'] = \'from@example.com\' msg[\'To\'] = \'to@example.com\' # Attach the body text body = MIMEText(email_body_text, \'plain\') msg.attach(body) # Attach the image with open(image_path, \'rb\') as img: img_data = img.read() image = MIMEImage(img_data, name=image_path) msg.attach(image) # Attach the text file with open(text_file_path, \'rb\') as tf: file_data = tf.read() text_file = MIMEApplication(file_data, name=text_file_path) encoders.encode_base64(text_file) text_file.add_header(\'Content-Disposition\', \'attachment\', filename=text_file_path) msg.attach(text_file) return msg.as_string()"},{"question":"# Token-Based Parsing Challenge Objective Implement a function that processes a list of Python tokens to identify and categorize specific patterns within the token stream. Problem Statement You are given a list of tuples representing tokens where each tuple contains a numeric token value and its corresponding string value. Using the `token` module, write a function `analyze_tokens(token_list)` that processes this list to achieve the following: 1. Identify all instances of assignment operations (e.g., `=`, `+=`, `-=`, `*=`, `/=`, `%=`, etc.). 2. Categorize the assignment operations into two categories: - Simple assignments (e.g., `=`) - Compound assignments (e.g., `+=`, `-=`, etc.) 3. Return a dictionary with two keys: `simple_assignments` and `compound_assignments`. The value for each key should be a list of tuples, where each tuple contains the string representation of the assignment operation and its position (index) in the `token_list`. Input - `token_list`: A list of tuples, where each tuple contains an integer representing the token type and a string representing the token value. For example: ```python [(1, \'a\'), (54, \'=\'), (2, \'10\'), (4, \'n\'), (1, \'b\'), (54, \'+=\'), (2, \'20\'), (4, \'n\')] ``` Output - A dictionary with two keys: - `simple_assignments`: A list of tuples containing the string representation of simple assignments and their positions. - `compound_assignments`: A list of tuples containing the string representation of compound assignments and their positions. Example: ```python { \'simple_assignments\': [(\'=\', 1)], \'compound_assignments\': [(\'+=\' , 5)] } ``` Constraints - The token list is guaranteed to represent a syntactically correct sequence of tokens. - The token values for assignments you need to handle are available as constants in the `token` module (`token.EQUAL`, `token.PLUSEQUAL`, `token.MINEQUAL`, `token.STAREQUAL`, `token.SLASHEQUAL`, `token.PERCENTEQUAL`). Function Signature ```python import token def analyze_tokens(token_list): # Your code here pass ``` Example ```python import token def analyze_tokens(token_list): simple_assignments = [] compound_assignments = [] assignment_tokens = { token.EQUAL: \'=\', token.PLUSEQUAL: \'+=\', token.MINEQUAL: \'-=\', token.STAREQUAL: \'*=\', token.SLASHEQUAL: \'/=\', token.PERCENTEQUAL: \'%=\', } for index, (tok_type, tok_value) in enumerate(token_list): if tok_type in assignment_tokens: if tok_type == token.EQUAL: simple_assignments.append((tok_value, index)) else: compound_assignments.append((tok_value, index)) return { \'simple_assignments\': simple_assignments, \'compound_assignments\': compound_assignments } # Example usage: token_list = [(1, \'a\'), (token.EQUAL, \'=\'), (2, \'10\'), (4, \'n\'), (1, \'b\'), (token.PLUSEQUAL, \'+=\'), (2, \'20\'), (4, \'n\')] print(analyze_tokens(token_list)) # Expected output: # { # \'simple_assignments\': [(\'=\', 1)], # \'compound_assignments\': [(\'+=\' , 5)] # } ```","solution":"import token def analyze_tokens(token_list): Analyzes a list of tokens to categorize assignment operations. Args: token_list (list): A list of tuples where each tuple contains a token type and token value. Returns: dict: A dictionary with two keys \'simple_assignments\' and \'compound_assignments\'. Each key maps to a list of tuples. simple_assignments = [] compound_assignments = [] assignment_tokens = { token.EQUAL: \'=\', token.PLUSEQUAL: \'+=\', token.MINEQUAL: \'-=\', token.STAREQUAL: \'*=\', token.SLASHEQUAL: \'/=\', token.PERCENTEQUAL: \'%=\', } for index, (tok_type, tok_value) in enumerate(token_list): if tok_type in assignment_tokens: if tok_type == token.EQUAL: simple_assignments.append((tok_value, index)) else: compound_assignments.append((tok_value, index)) return { \'simple_assignments\': simple_assignments, \'compound_assignments\': compound_assignments } # Example usage: # token_list = [(1, \'a\'), (token.EQUAL, \'=\'), (2, \'10\'), (4, \'n\'), # (1, \'b\'), (token.PLUSEQUAL, \'+=\'), (2, \'20\'), (4, \'n\')] # print(analyze_tokens(token_list)) # Expected output: # { # \'simple_assignments\': [(\'=\', 1)], # \'compound_assignments\': [(\'+=\' , 5)] # }"},{"question":"Title: Concurrent Web Scraper with asyncio Objective Implement a concurrent web scraper using Python\'s `asyncio` library. Your scraper should fetch web pages concurrently and extract data from them. Problem Statement You are required to implement a concurrent web scraper that downloads and processes multiple web pages at the same time using `asyncio`. The following are the detailed requirements: 1. **Function**: `async def fetch_page(url: str) -> str` - Takes a URL as input and returns the content of the page as a string after a simulated network delay. - Simulate the network delay using `asyncio.sleep`. The delay should be between 1 to 3 seconds, chosen randomly. 2. **Function**: `async def process_page(content: str) -> dict` - Takes the page content as input and processes it. - For simplicity, assume it extracts the title of the page and the number of words. Return this as a dictionary with keys `title` and `word_count`. - Simulate the processing delay using `asyncio.sleep`. The delay should be between 0.5 to 1.5 seconds, chosen randomly. 3. **Function**: `async def scrape(urls: List[str]) -> List[dict]` - Takes a list of URLs and fetches and processes them concurrently using `asyncio.gather`. - Returns a list of dictionaries, each containing the title and word count of a page. Expected Input and Output - **Input**: A list of URLs. - **Output**: A list of dictionaries, each with keys `title` and `word_count`. Constraints - Use the `asyncio` library to manage concurrency. - Simulated delays must be random within the specified limits. - Ensure that all pages are fetched and processed concurrently. Example ```python import asyncio import random async def fetch_page(url: str) -> str: await asyncio.sleep(random.uniform(1, 3)) # Simulated network delay return f\\"<html><head><title>{url}</title></head><body>{\'Hello \' * 100}</body></html>\\" async def process_page(content: str) -> dict: await asyncio.sleep(random.uniform(0.5, 1.5)) # Simulated processing delay title = content.split(\'<title>\')[1].split(\'</title>\')[0] word_count = len(content.split()) return {\'title\': title, \'word_count\': word_count} async def scrape(urls: List[str]) -> List[dict]: tasks = [fetch_and_process(url) for url in urls] return await asyncio.gather(*tasks) async def fetch_and_process(url: str) -> dict: content = await fetch_page(url) return await process_page(content) # Example input urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] # Running the asyncio program results = asyncio.run(scrape(urls)) print(results) # Output format: [{\'title\': \'http://example.com/1\', \'word_count\': 103}, ...] ```","solution":"import asyncio import random from typing import List async def fetch_page(url: str) -> str: Simulates fetching a web page by sleeping for a random amount of time between 1 and 3 seconds. await asyncio.sleep(random.uniform(1, 3)) return f\\"<html><head><title>{url}</title></head><body>{\'Hello \' * 100}</body></html>\\" async def process_page(content: str) -> dict: Simulates processing the web page content by sleeping for a random amount of time between 0.5 and 1.5 seconds. Extracts and returns the title and word count of the page. await asyncio.sleep(random.uniform(0.5, 1.5)) title = content.split(\'<title>\')[1].split(\'</title>\')[0] word_count = len(content.split()) return {\'title\': title, \'word_count\': word_count} async def fetch_and_process(url: str) -> dict: Fetches a web page and processes its content. content = await fetch_page(url) return await process_page(content) async def scrape(urls: List[str]) -> List[dict]: Scrapes multiple URLs concurrently. tasks = [fetch_and_process(url) for url in urls] return await asyncio.gather(*tasks) # Example input urls = [\\"http://example.com/1\\", \\"http://example.com/2\\", \\"http://example.com/3\\"] # Running the asyncio program results = asyncio.run(scrape(urls)) print(results) # Output format: [{\'title\': \'http://example.com/1\', \'word_count\': 203}, ...]"},{"question":"Objective: Write code that demonstrates your understanding of pandas\' I/O functionality by performing a sequence of data manipulations involving multiple file formats. Description: You are given the following tasks to complete using pandas. Your solution must read data from an Excel file, perform a transformation, and then write the resulting data to both a CSV file and a JSON file. 1. **Read Data from Excel**: - Read an Excel file named `data.xlsx` from the current directory. Assume the file has a single sheet called `Sheet1`. - Parse the data and load it into a DataFrame. 2. **Transform the Data**: - Filter the DataFrame to include only rows where the value in the `Date` column is greater than `2023-01-01`. - Group the data by the `Category` column and compute the sum of the values in the `Amount` column for each category. - Reset the index of the resulting DataFrame. 3. **Write Data to CSV**: - Write the transformed DataFrame to a CSV file named `filtered_data.csv` in the current directory. 4. **Write Data to JSON**: - Write the transformed DataFrame to a JSON file named `filtered_data.json` in the current directory. Constraints: - The `Date` column uses the format `YYYY-MM-DD`. - The `Amount` column contains numeric values. - Only use pandas library for all data manipulation tasks. - Your code should be efficient and should handle potential exceptions gracefully. Expected Input and Output: - **Input**: An Excel file `data.xlsx` with columns: `Date`, `Category`, `Amount`. - **Output**: Two files - `filtered_data.csv` and `filtered_data.json`, both containing the transformed data. Example Usage: **Content of data.xlsx (Sheet1):** | Date | Category | Amount | |------------|----------|--------| | 2023-01-05 | A | 100 | | 2022-12-25 | B | 150 | | 2023-02-15 | A | 200 | **Content of filtered_data.csv and filtered_data.json after transformation:** | Category | Amount | |----------|--------| | A | 300 | Hints: - Use `read_excel` to read the Excel file. - Use `to_csv` to write data to a CSV file. - Use `to_json` to write data to a JSON file. - Use `groupby`, `sum`, and `reset_index` for data transformation. Good luck!","solution":"import pandas as pd def process_data(): # Step 1: Read Data from Excel df = pd.read_excel(\'data.xlsx\', sheet_name=\'Sheet1\') # Step 2: Transform the Data # Filter the DataFrame to include only rows where the Date is greater than \'2023-01-01\' df[\'Date\'] = pd.to_datetime(df[\'Date\']) df_filtered = df[df[\'Date\'] > \'2023-01-01\'] # Group by \'Category\' and compute the sum of \'Amount\' df_grouped = df_filtered.groupby(\'Category\')[\'Amount\'].sum().reset_index() # Step 3: Write Data to CSV df_grouped.to_csv(\'filtered_data.csv\', index=False) # Step 4: Write Data to JSON df_grouped.to_json(\'filtered_data.json\', orient=\'records\', lines=True)"},{"question":"# Advanced Coding Assessment: Asynchronous Task Scheduler **Objective:** You are tasked with creating an asynchronous task scheduler that performs multiple tasks concurrently, handles task timeout, and ensures some tasks are shielded from cancellation. **Description:** 1. Implement an asynchronous function `process_tasks` that takes in a list of tasks, processes them concurrently, and returns the results of the completed tasks. 2. If any task exceeds a given timeout, it should be cancelled and a \'`Timed Out`\' message should be returned for that task. 3. Ensure that some critical tasks are shielded from being cancelled during the process. **Function Signature:** ```python import asyncio async def process_tasks(tasks: list, timeout: float, shielded_indices: list) -> list: Process a list of asynchronous tasks concurrently, handle task timeout, and shield specific tasks from cancellation. Args: tasks (list): A list of coroutine functions to be executed. timeout (float): The maximum time to wait for task completion. shielded_indices (list): Indices of tasks in the \'tasks\' list that should be shielded from cancellation. Returns: list: A list containing results of completed tasks or \'Timed Out\' for tasks that timed out. pass ``` **Input:** - `tasks`: A list of coroutine functions that need to be executed asynchronously. - `timeout`: A float representing the timeout for task execution. - `shielded_indices`: A list of indices indicating which tasks in the `tasks` list should be shielded from cancellation. **Output:** - A list containing the results of the tasks that completed successfully. - For tasks that timed out, the corresponding entry in the result list should be \'`Timed Out`\'. **Implementation Details:** 1. Use `asyncio.create_task` to create tasks from the provided coroutine functions. 2. Use `asyncio.shield` to shield tasks specified by the `shielded_indices` from cancellation. 3. Use `asyncio.wait_for` to apply a timeout to each task. 4. Ensure all tasks run concurrently using `asyncio.gather`. 5. Return a list of task results or \'`Timed Out`\' for tasks that exceed the timeout. **Constraints:** - Ensure proper error handling and cancellation propagation. - Tasks in `shielded_indices` should not be cancelled even if they exceed the timeout. **Example:** ```python import asyncio async def sample_task(duration): await asyncio.sleep(duration) return f\\"Task completed in {duration}s\\" async def main(): tasks = [sample_task(2), sample_task(5), sample_task(1)] shielded_indices = [1] results = await process_tasks(tasks, 3, shielded_indices) print(results) asyncio.run(main()) # Expected output: # The first task completes successfully as it\'s within the timeout # The second task is shielded from cancellation, hence it completes without \'Timed Out\' # The third task is within the timeout and completes successfully # Output: [\'Task completed in 2s\', \'Task completed in 5s\', \'Task completed in 1s\'] ``` **Note:** You may reuse the `sample_task` coroutine for testing purposes.","solution":"import asyncio async def process_tasks(tasks: list, timeout: float, shielded_indices: list) -> list: Process a list of asynchronous tasks concurrently, handle task timeout, and shield specific tasks from cancellation. Args: tasks (list): A list of coroutine functions to be executed. timeout (float): The maximum time to wait for task completion. shielded_indices (list): Indices of tasks in the \'tasks\' list that should be shielded from cancellation. Returns: list: A list containing results of completed tasks or \'Timed Out\' for tasks that timed out. async def shield_task(task, idx): if idx in shielded_indices: return await asyncio.shield(task) else: try: return await asyncio.wait_for(task, timeout=timeout) except asyncio.TimeoutError: return \'Timed Out\' tasks = [asyncio.create_task(shield_task(task, idx)) for idx, task in enumerate(tasks)] return await asyncio.gather(*tasks) # Example coroutine for testing async def sample_task(duration): await asyncio.sleep(duration) return f\\"Task completed in {duration}s\\""},{"question":"**Objective:** Design a Python script that launches IDLE, creates a new Python script file, inputs a fixed set of Python commands into the file, saves it, and then runs the script within IDLE. Your solution should demonstrate your understanding of scripting and automating IDLE features. **Task:** 1. Write a Python script called `automate_idle.py`. 2. Your script should: - Launch IDLE. - Create a new Python file named `generated_script.py`. - Insert the following Python code into `generated_script.py`: ```python def greeting(name): return f\\"Hello, {name}!\\" if __name__ == \\"__main__\\": print(greeting(\\"World\\")) ``` - Save the file. - Run `generated_script.py` within IDLE. **Requirements:** 1. Ensure that the `generated_script.py` outputs the correct greeting message when run. 2. Your solution should be able to handle any errors that might occur during the process. 3. Provide comments explaining each step of your code for clarity. **Constraints:** - You may use any standard Python library to assist with automating this task, such as `subprocess` for running commands. - The script should be written in a way that it can be run on any operating system that supports Python and IDLE. **Input:** No input is required from the user directly. The script should run autonomously. **Output:** The output should be the correct execution of `generated_script.py` within IDLE, displaying the greeting message. **Example Execution:** When you run `automate_idle.py`, it should: - Launch IDLE. - Create and open `generated_script.py` in the IDLE editor. - Insert the provided Python code. - Save `generated_script.py`. - Run `generated_script.py` and display: ``` Hello, World! ``` You do not need to submit the actual output but ensure your script performs the actions correctly.","solution":"import os import subprocess import time def automate_idle(): # Define the script content script_content = \'\'\'def greeting(name): return f\\"Hello, {name}!\\" if __name__ == \\"__main__\\": print(greeting(\\"World\\")) \'\'\' try: # Define the script file name script_file = \'generated_script.py\' # Create the Python script file and write the content to it with open(script_file, \'w\') as file: file.write(script_content) # Define the command to run IDLE and open the created script file if os.name == \'nt\': # Windows idle_command = f\'py -m idlelib.idle {script_file}\' else: # MacOS or Linux idle_command = f\'python3 -m idlelib.idle {script_file}\' # Launch IDLE with the script file subprocess.run(idle_command, shell=True) # Delay to ensure IDLE is launched and script is executed time.sleep(5) except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": automate_idle()"},{"question":"**Coding Assessment Question: Working with Meta Tensors in PyTorch** # Objective Write a Python function `initialize_meta_model` that initializes a simple neural network model on the meta device, prints its structure, and then translates the model to another specified device, leaving its parameters uninitialized. # Function Signature ```python import torch from torch import nn def initialize_meta_model(input_size: int, hidden_size: int, output_size: int, target_device: str): Initializes a simple neural network model on the meta device, prints its structure, translates the model to another device, and returns the uninitialized model. Parameters: input_size (int): The size of the input features. hidden_size (int): The size of the hidden layer. output_size (int): The size of the output layer. target_device (str): The device to which the model should be translated (e.g., \'cpu\', \'cuda\'). Returns: nn.Module: The uninitialized model on the target device. pass ``` # Requirements 1. **Model Structure**: Your neural network should consist of two `Linear` layers: - First layer: `input_size` -> `hidden_size` - Second layer: `hidden_size` -> `output_size` 2. **Meta Device Initialization**: Use the meta device to initialize the model. 3. **Printing**: Print the model structure while it is on the meta device. 4. **Translation and Return**: Convert the model to the specified `target_device` without initializing its parameters and return it. # Example ```python # Example usage: model = initialize_meta_model(10, 5, 2, \'cpu\') print(model) ``` **Expected Output** ```plaintext Linear(in_features=10, out_features=5, bias=True) Linear(in_features=5, out_features=2, bias=True) Linear(in_features=10, out_features=5, bias=True) Linear(in_features=5, out_features=2, bias=True) ``` # Constraints - You cannot predefine the model on the specified device before moving it to meta and then back to the target device. # Notes - Ensure you have `torch` and `torch.nn` properly imported. - Use the `to_empty` method to convert the model from the meta device to the target device.","solution":"import torch from torch import nn def initialize_meta_model(input_size: int, hidden_size: int, output_size: int, target_device: str): Initializes a simple neural network model on the meta device, prints its structure, translates the model to another device, and returns the uninitialized model. Parameters: input_size (int): The size of the input features. hidden_size (int): The size of the hidden layer. output_size (int): The size of the output layer. target_device (str): The device to which the model should be translated (e.g., \'cpu\', \'cuda\'). Returns: nn.Module: The uninitialized model on the target device. class SimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.fc1(x) x = self.fc2(x) return x # Initialize model layers on meta device meta_device = torch.device(\'meta\') model = SimpleNet(input_size, hidden_size, output_size).to(meta_device) # Print model architecture print(model) # Convert model to target device without initializing its parameters model_to_target_device = model.to_empty(device=torch.device(target_device)) return model_to_target_device"},{"question":"You are provided with a time series dataset containing daily stock prices for multiple companies. Your task is to analyze this data using various window functions provided by the Pandas library. Specifically, you need to perform the following operations: 1. **Rolling Window Operations:** - Compute the 7-day rolling mean for each company. - Compute the 7-day rolling standard deviation for each company. 2. **Expanding Window Operations:** - Compute the expanding mean for each company. - Compute the expanding sum for each company. 3. **Exponentially Weighted Window Operations:** - Compute the exponentially weighted mean with a span of 7 days for each company. - Compute the exponentially weighted standard deviation with a span of 7 days for each company. # Input - A DataFrame `df` with the following columns: - `Date`: The date of the stock price (string in `YYYY-MM-DD` format). - `Company`: The name of the company (string). - `Close`: The closing price of the stock (float). # Output - A dictionary with the results of the computations. The keys of the dictionary should be the names of the operations, and the values should be DataFrames containing the corresponding results for each company. The dictionary should have the following structure: ```python { \\"rolling_mean\\": <DataFrame>, \\"rolling_std\\": <DataFrame>, \\"expanding_mean\\": <DataFrame>, \\"expanding_sum\\": <DataFrame>, \\"ewm_mean\\": <DataFrame>, \\"ewm_std\\": <DataFrame> } ``` Each DataFrame should have the `Date` and `Company` as the index and the computed values as the columns. # Constraints - The input DataFrame `df` is guaranteed to be sorted by `Date` for each company. - The operations should be implemented using Pandas\' window functions. # Example ```python import pandas as pd # Example DataFrame data = { \\"Date\\": [\\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\", \\"2023-01-04\\", \\"2023-01-05\\", \\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-03\\", \\"2023-01-04\\", \\"2023-01-05\\"], \\"Company\\": [\\"CompanyA\\", \\"CompanyA\\", \\"CompanyA\\", \\"CompanyA\\", \\"CompanyA\\", \\"CompanyB\\", \\"CompanyB\\", \\"CompanyB\\", \\"CompanyB\\", \\"CompanyB\\"], \\"Close\\": [100, 102, 101, 105, 107, 200, 202, 205, 208, 210] } df = pd.DataFrame(data) # Applying the window functions result = apply_window_functions(df) # Expected output structure (values are illustrative) { \\"rolling_mean\\": pd.DataFrame(...), \\"rolling_std\\": pd.DataFrame(...), \\"expanding_mean\\": pd.DataFrame(...), \\"expanding_sum\\": pd.DataFrame(...), \\"ewm_mean\\": pd.DataFrame(...), \\"ewm_std\\": pd.DataFrame(...) } ``` # Function Signature ```python def apply_window_functions(df: pd.DataFrame) -> dict: pass ``` Your implementation should handle the rolling, expanding, and exponentially weighted window operations as described, returning the results in the required format.","solution":"import pandas as pd def apply_window_functions(df: pd.DataFrame) -> dict: # Ensure the Date column is converted to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Initialize a dictionary to store the results result = {} # Set Date and Company as index for the operations df = df.set_index([\'Date\', \'Company\']) # Rolling Window Operations rolling_mean = df.groupby(\'Company\')[\'Close\'].rolling(window=7).mean().reset_index(level=0, drop=True) rolling_std = df.groupby(\'Company\')[\'Close\'].rolling(window=7).std().reset_index(level=0, drop=True) # Expanding Window Operations expanding_mean = df.groupby(\'Company\')[\'Close\'].expanding().mean().reset_index(level=0, drop=True) expanding_sum = df.groupby(\'Company\')[\'Close\'].expanding().sum().reset_index(level=0, drop=True) # Exponentially Weighted Window Operations ewm_mean = df.groupby(\'Company\')[\'Close\'].ewm(span=7, adjust=False).mean().reset_index(level=0, drop=True) ewm_std = df.groupby(\'Company\')[\'Close\'].ewm(span=7, adjust=False).std().reset_index(level=0, drop=True) # Storing the results into the dictionary with appropriate names result[\\"rolling_mean\\"] = rolling_mean.reset_index() result[\\"rolling_std\\"] = rolling_std.reset_index() result[\\"expanding_mean\\"] = expanding_mean.reset_index() result[\\"expanding_sum\\"] = expanding_sum.reset_index() result[\\"ewm_mean\\"] = ewm_mean.reset_index() result[\\"ewm_std\\"] = ewm_std.reset_index() return result"},{"question":"Objective: Using the seaborn package, specifically focusing on the `clustermap` function, visualize a new dataset by performing various customizations and adjustments. This will test your understanding of clustering, heatmaps, data normalization, and plot customization. Problem Statement: You are given a dataset called `student_scores.csv` which contains the following columns: - `student_id`: Identifier for each student. - `subject`: The subject in which the score was recorded. - `score`: The score obtained by the student. Your task is to write a Python function `visualize_student_scores()` that performs the following tasks: 1. Load the dataset into a pandas DataFrame. 2. Pivot the DataFrame to have `student_id` as rows and `subject` as columns, with `score` as the values. Handle any missing values by setting them to the average score of that subject. 3. Plot a clustermap of the resulting DataFrame with the following customizations: a. Set the figure size to 10x8 inches. b. Use the \\"viridis\\" colormap. c. Standardize the data within columns. d. Add a color bar at the right side of the plot. e. Set the dendrogram ratio to (0.2, 0.2). Constraints: - Ensure the function runs efficiently for datasets up to 1000 students and 10 subjects. - Use seaborn and pandas libraries to implement the solution. Input: The input to your function is the path to the CSV file. Example: ```python def visualize_student_scores(file_path: str): # Your implementation here pass # Example usage visualize_student_scores(\\"path/to/student_scores.csv\\") ``` Expected Output: The function should display a clustered heatmap with the specified customizations. Example CSV File (student_scores.csv): ``` student_id,subject,score 1,Math,85 1,Science,80 2,Math,78 2,Science,82 3,Math,92 ... ``` Notes: - Make sure to handle cases where some students might not have scores recorded in certain subjects. - The color bar and dendrogram should help in identifying clusters clearly.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_student_scores(file_path: str): # Step 1: Load the dataset into a pandas DataFrame df = pd.read_csv(file_path) # Step 2: Pivot the DataFrame to have student_id as rows and subject as columns pivot_df = df.pivot(index=\'student_id\', columns=\'subject\', values=\'score\') # Step 3: Handle missing values by setting them to the average score of that subject pivot_df = pivot_df.apply(lambda x: x.fillna(x.mean()), axis=0) # Step 4: Plot a clustermap with the required customizations sns.clustermap(pivot_df, figsize=(10, 8), cmap=\'viridis\', standard_scale=1, cbar_pos=(1, 0.2, 0.05, 0.4), dendrogram_ratio=(0.2, 0.2)) plt.show()"},{"question":"**Question: Implementing a Basic Audio Playback Functionality Using `ossaudiodev` Module** You are tasked with writing a function `play_audio(data: bytes, samplerate: int, channels: int, fmt: int) -> int` that plays audio data using the `ossaudiodev` module. Your function should open an audio device, set its parameters, and write the audio data to the device. # Function Signature ```python def play_audio(data: bytes, samplerate: int, channels: int, fmt: int) -> int: pass ``` # Parameters - `data`: A `bytes` object representing the audio data to be played. - `samplerate`: An integer specifying the sample rate (e.g., 44100 for CD quality). - `channels`: An integer specifying the number of audio channels (1 for mono, 2 for stereo). - `fmt`: An integer specifying the audio format (e.g., `ossaudiodev.AFMT_S16_LE`). # Returns - The function should return an integer indicating the total number of bytes written to the audio device. # Constraints - Assume the audio device must be opened in write-only mode. - Ensure that the device is properly closed after the audio data is written. - Handle any exceptions that may occur during the process and raise a custom exception `AudioPlaybackError` with an appropriate error message. # Example Usage ```python data = b\'\' # some byte sequence representing audio data samplerate = 44100 channels = 2 fmt = ossaudiodev.AFMT_S16_LE try: bytes_written = play_audio(data, samplerate, channels, fmt) print(f\\"Total bytes written: {bytes_written}\\") except AudioPlaybackError as e: print(f\\"Error during playback: {e}\\") ``` # Skeleton Code ```python import ossaudiodev class AudioPlaybackError(Exception): Custom exception for audio playback errors. pass def play_audio(data: bytes, samplerate: int, channels: int, fmt: int) -> int: try: # Open the audio device in write-only mode audio_device = ossaudiodev.open(\'w\') # Set audio parameters audio_device.setparameters(fmt, channels, samplerate) # Write the audio data to the device and get the number of bytes written bytes_written = audio_device.writeall(data) # Close the audio device audio_device.close() return bytes_written except Exception as e: raise AudioPlaybackError(f\\"Error during playback: {e}\\") # Example usage if __name__ == \\"__main__\\": data = b\'\' # some byte sequence representing audio data samplerate = 44100 channels = 2 fmt = ossaudiodev.AFMT_S16_LE try: bytes_written = play_audio(data, samplerate, channels, fmt) print(f\\"Total bytes written: {bytes_written}\\") except AudioPlaybackError as e: print(f\\"Error during playback: {e}\\") ``` Implement the provided `play_audio` function according to the specifications and ensure the sample usage works correctly.","solution":"import ossaudiodev class AudioPlaybackError(Exception): Custom exception for audio playback errors. pass def play_audio(data: bytes, samplerate: int, channels: int, fmt: int) -> int: try: # Open the audio device in write-only mode audio_device = ossaudiodev.open(\'w\') # Set audio parameters audio_device.setparameters(fmt, channels, samplerate) # Write the audio data to the device and return the number of bytes written bytes_written = audio_device.write(data) # Close the audio device audio_device.close() return bytes_written except Exception as e: raise AudioPlaybackError(f\\"Error during playback: {e}\\")"},{"question":"# Python Networking and Asynchronous I/O Challenge You are to implement a simple secure chat server and client using the `asyncio` and `ssl` modules in Python. Your task is to create both the server and client program, with the server listening for incoming client connections and each client able to send and receive messages asynchronously. Requirements 1. **Server Implementation:** - Implement a chat server using the `asyncio` module. - Ensure that communication is encrypted with `ssl`. - The server should be able to handle multiple clients simultaneously. - For simplicity, broadcast each received message to all connected clients. - Use signal handling to allow the server to shutdown gracefully on receiving a termination signal (e.g., `SIGINT`). 2. **Client Implementation:** - Implement a chat client using the `asyncio` module. - The client should establish a secure connection using SSL to the server. - Each client should be able to send messages to the server and receive broadcast messages from the server. Input and Output Format - **Inputs to the Server:** - The server should accept incoming connections on a specified host and port. - Upon client connection, listen for messages and broadcast them to all connected clients. - **Inputs to the Client:** - The client should take the server\'s host and port as input to establish a connection. - The client should read user input from the standard input and send messages to the server. Constraints - Use the `asyncio` module for asynchronous operations. - Employ the `ssl` module to ensure SSL/TLS encrypted communication. - Implement proper exception handling to manage network errors gracefully. Example ```python # Example server usage: # python server.py --host 127.0.0.1 --port 8888 # Example client usage: # python client.py --host 127.0.0.1 --port 8888 # Expected behavior: # - Clients should securely connect to the server. # - Each message sent by a client should be received by all other connected clients. ``` Performance Requirements - The server should efficiently manage multiple client connections asynchronously. - The solution should handle a moderate number of clients (e.g., up to 50 concurrent connections) without significant performance degradation. You are expected to provide properly commented code with adequate error handling. Make sure to include any necessary SSL context setup in both the server and client code.","solution":"import asyncio import ssl import signal from datetime import datetime clients = [] async def broadcast(message: str): for client in clients: try: client.write(f\\"{message}n\\".encode()) await client.drain() except: clients.remove(client) async def handle_client(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): addr = writer.get_extra_info(\'peername\') clients.append(writer) print(f\\"Client {addr} connected.\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received message from {addr}: {message.strip()}\\") await broadcast(f\\"{addr} > {message.strip()}\\") except asyncio.CancelledError: pass finally: clients.remove(writer) writer.close() await writer.wait_closed() print(f\\"Client {addr} disconnected.\\") async def main(host, port): context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH) context.load_cert_chain(certfile=\\"cert.pem\\", keyfile=\\"key.pem\\") server = await asyncio.start_server( handle_client, host, port, ssl=context ) addr = server.sockets[0].getsockname() print(f\\"Serving on {addr}\\") async with server: await server.serve_forever() def handle_sigint(signal, frame): for client in clients: client.close() print(\\"Server shutting down...\\") if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser(description=\\"Asyncio Secure Chat Server\\") parser.add_argument(\\"--host\\", type=str, required=True, help=\\"Hostname\\") parser.add_argument(\\"--port\\", type=int, required=True, help=\\"Port number\\") args = parser.parse_args() signal.signal(signal.SIGINT, handle_sigint) asyncio.run(main(args.host, args.port))"},{"question":"**Objective:** Write a function that demonstrates the usage of the `xdrlib` module to pack and then unpack a dictionary containing various data types. This will assess your understanding of data serialization and deserialization using the `xdrlib` package. **Function Signature:** ```python def pack_and_unpack_data(data: dict) -> dict: Packs the given dictionary using xdrlib and then unpacks it back to verify the accuracy of the process. Parameters: data (dict): The dictionary containing various data types to be packed and unpacked. The dictionary can contain: - Integers - Floats - Strings - Byte streams - Lists of integers Returns: dict: A dictionary that is unpacked from the xdr format, expected to be identical to the input dictionary. pass ``` **Constraints:** 1. The dictionary `data` can contain the following types: - Integers: stored as `int`. - Floats: stored as `float`. - Strings: stored as `str`. - Byte streams: stored as `bytes`. - Lists of integers: stored as `list[int]`. 2. You must handle any necessary padding for string and byte alignment as described in the documentation. 3. The function should use the `xdrlib.Packer` class to pack the data and the `xdrlib.Unpacker` class to unpack the data. 4. Ensure that the unpacked dictionary matches the original dictionary. **Example:** ```python data = { \\"integer\\": 42, \\"float\\": 3.14, \\"string\\": \\"hello\\", \\"bytes\\": b\'x01x02x03\', \\"list\\": [1, 2, 3, 4, 5] } result = pack_and_unpack_data(data) assert data == result, \\"The unpacked data does not match the original data\\" ``` **Hint:** You may need to iterate over the dictionary and handle different data types separately with the appropriate pack and unpack methods from the `xdrlib.Packer` and `xdrlib.Unpacker` classes. **Notes:** - To pack a list of integers, you might use `Packer.pack_list` method. - Pay attention to the order in which you pack and then unpack the data to ensure consistency.","solution":"import xdrlib def pack_and_unpack_data(data: dict) -> dict: Packs the given dictionary using xdrlib and then unpacks it back to verify the accuracy of the process. Parameters: data (dict): The dictionary containing various data types to be packed and unpacked. The dictionary can contain: - Integers - Floats - Strings - Byte streams - Lists of integers Returns: dict: A dictionary that is unpacked from the xdr format, expected to be identical to the input dictionary. packer = xdrlib.Packer() for key, value in data.items(): if isinstance(value, int): packer.pack_int(value) elif isinstance(value, float): packer.pack_double(value) elif isinstance(value, str): packer.pack_string(value.encode()) elif isinstance(value, bytes): packer.pack_bytes(value) elif isinstance(value, list) and all(isinstance(i, int) for i in value): packer.pack_list(value, packer.pack_int) # We can extend this for other types if needed packed_data = packer.get_buffer() unpacker = xdrlib.Unpacker(packed_data) unpacked_data = {} for key, value in data.items(): if isinstance(value, int): unpacked_data[key] = unpacker.unpack_int() elif isinstance(value, float): unpacked_data[key] = unpacker.unpack_double() elif isinstance(value, str): unpacked_data[key] = unpacker.unpack_string().decode() elif isinstance(value, bytes): unpacked_data[key] = unpacker.unpack_bytes() elif isinstance(value, list) and all(isinstance(i, int) for i in value): unpacked_data[key] = unpacker.unpack_list(unpacker.unpack_int) # We can extend this for other types if needed return unpacked_data"},{"question":"Clustering with Custom Preprocessing and Evaluation # Objective Your task is to implement a function that performs clustering on a dataset using scikit-learn. You will preprocess the data, fit a clustering model, and evaluate its performance. # Instructions 1. **Preprocessing**: - Normalize the data using `StandardScaler`. - Use PCA to reduce the data to 2 dimensions. 2. **Clustering**: - Fit a `KMeans` clustering model with a specified number of clusters. 3. **Evaluation**: - Calculate and return the silhouette score of the clustering. # Input - A 2D NumPy array `data` of shape (n_samples, n_features), where each row represents a sample and each column represents a feature. - An integer `n_clusters` specifying the number of clusters for the `KMeans` algorithm. # Output - A float representing the silhouette score of the clustering. # Constraints - You must use `StandardScaler` for normalization and `PCA` for dimensionality reduction. - The silhouette score should be computed using `metrics.silhouette_score` from scikit-learn. # Function Signature ```python def cluster_and_evaluate(data: np.ndarray, n_clusters: int) -> float: pass ``` # Example ```python import numpy as np data = np.array([[1.0, 2.0], [1.1, 2.1], [5.0, 6.0], [5.1, 6.1]]) n_clusters = 2 score = cluster_and_evaluate(data, n_clusters) print(score) # Should print the silhouette score ``` # Notes - `data` can have any number of features and samples. The number of samples (n_samples) will be >= 2. - Use the default parameters for `StandardScaler`, `PCA`, and `KMeans` unless otherwise specified.","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score def cluster_and_evaluate(data: np.ndarray, n_clusters: int) -> float: # Normalize the data using StandardScaler scaler = StandardScaler() data_scaled = scaler.fit_transform(data) # Reduce the dimensionality using PCA to 2 dimensions pca = PCA(n_components=2) data_pca = pca.fit_transform(data_scaled) # Fit the KMeans clustering model kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(data_pca) # Calculate and return the silhouette score score = silhouette_score(data_pca, cluster_labels) return score"},{"question":"Objective: Demonstrate your understanding of the Nullable Boolean data type in pandas by creating and manipulating a DataFrame with nullable boolean columns, and performing logical operations while handling `NA` values effectively. Problem Statement: You are provided with a DataFrame containing two columns of boolean values, where some boolean values might be `NA`. Your task is to implement the following functions: 1. **create_nullable_boolean_dataframe(data)**: - **Input**: A dictionary `data` where keys are column names and values are lists of boolean values (`True`, `False`) or `None` (representing `NA`). - **Output**: A pandas DataFrame with nullable boolean columns. - **Constraints**: - You must explicitly set the `dtype` of the columns to `\\"boolean\\"`. - Ensure that the DataFrame is constructed correctly regardless of the presence of `None` values. 2. **logical_operations(df, operation)**: - **Input**: - `df`: A pandas DataFrame created by the `create_nullable_boolean_dataframe` function. - `operation`: A string that represents the logical operation (`\'&\'`, `\'|\'`, `\'^\'`). - **Output**: A pandas DataFrame with an additional column, `\\"result\\"`, representing the result of performing the specified logical operation between all pairs of rows in the DataFrame. - **Constraints**: - The result should follow the Kleene Logic rules as described. - Handle `NA` values according to the logic table provided. Example: ```python data = { \\"col1\\": [True, False, None, True], \\"col2\\": [False, True, True, None] } df = create_nullable_boolean_dataframe(data) print(df) # Output: # col1 col2 # 0 True False # 1 False True # 2 NaN True # 3 True NaN result_df = logical_operations(df, \'&\') print(result_df) # Output: # col1 col2 result # 0 True False False # 1 False True False # 2 NaN True NaN # 3 True NaN NaN ``` Ensure your implementations are efficient and handle edge cases, such as columns with all `None` values, appropriately.","solution":"import pandas as pd def create_nullable_boolean_dataframe(data): Create a pandas DataFrame with nullable boolean columns from the provided data dictionary. df = pd.DataFrame(data) for col in df.columns: df[col] = df[col].astype(\\"boolean\\") return df def logical_operations(df, operation): Perform a logical operation between all corresponding pairs of elements in the DataFrame columns and add the result as a new column called \'result\'. Kleene Logic is followed for handling NA values: - True & NA -> NA - False & NA -> False - NA & NA -> NA - True | NA -> True - False | NA -> NA - NA | NA -> NA - True ^ NA -> NA - False ^ NA -> NA - NA ^ NA -> NA if operation == \'&\': df[\'result\'] = df[\'col1\'] & df[\'col2\'] elif operation == \'|\': df[\'result\'] = df[\'col1\'] | df[\'col2\'] elif operation == \'^\': df[\'result\'] = df[\'col1\'] ^ df[\'col2\'] else: raise ValueError(\\"Invalid operation. Must be one of \'&\', \'|\', \'^\'.\\") return df"},{"question":"# Python Coding Assessment Question Objective: You are required to implement a function that uses the `subprocess` module to run a shell command, capture its output (both stdout and stderr), handle potential errors gracefully, and manage potential timeouts. Task Description: Implement a function `run_shell_command(command: str, timeout_sec: int) -> dict` that: 1. Takes a shell command as an input string (`command`) and a timeout period in seconds (`timeout_sec`). 2. Executes the command using `subprocess.run()`. 3. Captures and returns the stdout and stderr of the command. 4. If the command execution lasts more than the specified timeout period, terminates the process and raises a `TimeoutError`. 5. If the command returns a non-zero exit status, raises a custom exception `CommandExecutionError` including the return code and any captured stdout/stderr. 6. Ensure that the function returns a dictionary with keys `\'stdout\'`, `\'stderr\'`, and `\'returncode\'`. Constraints: - Do not use `shell=True` for security reasons. - The command should be provided as a list of arguments. - Handle Unicode correctly in the captured outputs. Example: ```python class CommandExecutionError(Exception): def __init__(self, returncode, stdout, stderr): self.returncode = returncode self.stdout = stdout self.stderr = stderr super().__init__(f\\"Command failed with return code {returncode}\\") def run_shell_command(command: list, timeout_sec: int) -> dict: # Implement function here. # Usage example: try: result = run_shell_command([\\"ls\\", \\"-l\\"], 5) print(f\\"STDOUT: {result[\'stdout\']}\\") print(f\\"STDERR: {result[\'stderr\']}\\") print(f\\"RETURNCODE: {result[\'returncode\']}\\") except CommandExecutionError as e: print(f\\"Command execution failed: {e}\\") except TimeoutError: print(\\"Command timed out.\\") ``` Expected Output: 1. If the command runs successfully within the timeout period, the function should return a dictionary with `stdout`, `stderr`, and `returncode`. 2. If the command runs for more than the timeout period, it should raise a `TimeoutError`. 3. If the command exits with a non-zero exit status, it should raise a `CommandExecutionError`. Use this function to handle subprocesses safely, capturing their outputs, managing errors, and handling potential timeouts effectively.","solution":"import subprocess import shlex class CommandExecutionError(Exception): def __init__(self, returncode, stdout, stderr): self.returncode = returncode self.stdout = stdout self.stderr = stderr super().__init__(f\\"Command failed with return code {returncode}\\") def run_shell_command(command: list, timeout_sec: int) -> dict: Runs a shell command and captures its output, handling errors and timeouts. Parameters: command (list): The shell command to run as a list of arguments. timeout_sec (int): The maximum time (in seconds) to allow the command to run. Returns: dict: A dictionary containing \'stdout\', \'stderr\', and \'returncode\'. Raises: CommandExecutionError: If the command exits with a non-zero status. TimeoutError: If the command takes longer than the specified timeout. try: result = subprocess.run(command, capture_output=True, text=True, timeout=timeout_sec, check=False) if result.returncode != 0: raise CommandExecutionError(result.returncode, result.stdout, result.stderr) return { \'stdout\': result.stdout, \'stderr\': result.stderr, \'returncode\': result.returncode } except subprocess.TimeoutExpired: raise TimeoutError(\\"Command timed out.\\") # Usage example (uncomment to run): # try: # result = run_shell_command([\'ls\', \'-l\'], 5) # print(f\\"STDOUT: {result[\'stdout\']}\\") # print(f\\"STDERR: {result[\'stderr\']}\\") # print(f\\"RETURNCODE: {result[\'returncode\']}\\") # except CommandExecutionError as e: # print(f\\"Command execution failed: {e}\\") # except TimeoutError: # print(\\"Command timed out.\\")"},{"question":"# Seaborn Color Palette Challenge You are tasked with creating a detailed visualization that leverages seaborn\'s color manipulation capabilities. Your task involves the following steps: 1. **Data Preparation**: Create a sample DataFrame with random data. 2. **Color Palette Manipulation**: Generate different color palettes using various parameters of the `sns.hls_palette` function. 3. **Visualization**: Create a multi-faceted plot that uses the generated palettes to visualize the data, highlighting the differences in color manipulations. # Instructions 1. **Data Preparation**: - Generate a DataFrame `df` with 3 columns (`\'A\'`, `\'B\'`, `\'C\'`) containing 100 random numbers each, using a random seed for reproducibility. 2. **Color Palette Generation**: - Generate at least 4 different color palettes using `sns.hls_palette`: 1. Default palette. 2. Palette with 8 colors. 3. Palette with lightness set to 0.3. 4. Palette with saturation set to 0.3. 5. Continuous colormap. 3. **Visualization**: - Using seaborn, create individual bar plots for each of the columns (`\'A\'`, `\'B\'`, `\'C\'`) using the generated palettes. Each plot should use a different palette. - Combine these bar plots into a single figure to clearly observe the differences in color representation. # Expected Input and Output **Input:** None (all data generation and visualization will be handled within the code). **Output:** A figure containing 3 bar plots, each using a different color palette as specified. # Constraints and Performance Requirements - Ensure that your solution includes all necessary imports and data generation steps. - The visualization should be clear, well-labeled, and make effective use of the color palettes. - Performance is not a critical factor, but the code should be efficient and organized appropriately. # Example Solution Structure ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np # Step 1: Data Preparation np.random.seed(42) df = pd.DataFrame({ \'A\': np.random.rand(100), \'B\': np.random.rand(100), \'C\': np.random.rand(100) }) # Step 2: Color Palette Generation palettes = { \'default\': sns.hls_palette(), \'8_colors\': sns.hls_palette(8), \'lightness_0.3\': sns.hls_palette(l=.3), \'saturation_0.3\': sns.hls_palette(s=.3), \'continuous_cmap\': sns.hls_palette(as_cmap=True) } # Step 3: Visualization fig, axes = plt.subplots(1, 3, figsize=(15, 5)) sns.barplot(x=df.index, y=\'A\', palette=palettes[\'default\'], ax=axes[0]) axes[0].set_title(\'Default Palette\') sns.barplot(x=df.index, y=\'B\', palette=palettes[\'8_colors\'], ax=axes[1]) axes[1].set_title(\'8 Colors Palette\') sns.barplot(x=df.index, y=\'C\', palette=palettes[\'lightness_0.3\'], ax=axes[2]) axes[2].set_title(\'Lightness 0.3 Palette\') plt.tight_layout() plt.show() ``` **Note:** Customize the provided example code to fit all requirements of the task. Ensure you have seaborn and other required packages installed.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np def prepare_data(): Generate a DataFrame with 3 columns (\'A\', \'B\', \'C\') containing 100 random numbers each. np.random.seed(42) df = pd.DataFrame({ \'A\': np.random.rand(100), \'B\': np.random.rand(100), \'C\': np.random.rand(100) }) return df def generate_palettes(): Generate various color palettes using sns.hls_palette function. palettes = { \'default\': sns.hls_palette(), \'8_colors\': sns.hls_palette(8), \'lightness_0.3\': sns.hls_palette(l=.3), \'saturation_0.3\': sns.hls_palette(s=.3), \'continuous_cmap\': sns.hls_palette(as_cmap=True) } return palettes def plot_with_palettes(df, palettes): Create bar plots for each column of the DataFrame using different color palettes. fig, axes = plt.subplots(1, 3, figsize=(18, 6)) sns.barplot(x=np.arange(len(df)), y=df[\'A\'], palette=palettes[\'default\'], ax=axes[0]) axes[0].set_title(\'Default Palette\') sns.barplot(x=np.arange(len(df)), y=df[\'B\'], palette=palettes[\'8_colors\'], ax=axes[1]) axes[1].set_title(\'8 Colors Palette\') sns.barplot(x=np.arange(len(df)), y=df[\'C\'], palette=palettes[\'lightness_0.3\'], ax=axes[2]) axes[2].set_title(\'Lightness 0.3 Palette\') plt.tight_layout() plt.show() # Main execution if __name__ == \\"__main__\\": df = prepare_data() palettes = generate_palettes() plot_with_palettes(df, palettes)"},{"question":"Implement a Custom Attention Layer in PyTorch **Objective:** To assess your understanding of the PyTorch library, specifically its attention mechanisms, you are required to implement a custom attention layer. The layer should be designed using the available PyTorch functionalities and should demonstrate a grasp of both fundamental and advanced concepts. **Task:** Implement a class `CustomAttention` that inherits from `torch.nn.Module` and performs an attention mechanism over input sequences. **Instructions:** 1. **Initialization (`__init__` method):** - The layer should accept the following parameters: - `input_dim` (int): The dimension of the input features. - `output_dim` (int): The dimension of the output features. - Define any required parameters or layers within the initialization method. 2. **Forward method (`forward` method):** - This method should accept a tensor `x` of shape `(batch_size, sequence_length, input_dim)`. - It should compute attention scores for each element in the sequence and produce a tensor of shape `(batch_size, output_dim)` as the final output. - Ensure that your attention mechanism includes: - Scoring function: A method to compute attention scores. - Weighted sum: A method to compute the weighted sum of inputs based on the attention scores. **Constraints:** - Use only PyTorch for your implementation. - Ensure that your code is clear and well-documented. - Include any necessary checks or data validation within your implementation. **Expected Input and Output:** - Input: A tensor `x` of shape `(batch_size, sequence_length, input_dim)`. - Output: A tensor of shape `(batch_size, output_dim)`. **Performance Requirements:** - The implementation should efficiently handle input sequences up to a length of 512 for batch sizes up to 64. **Example:** ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self, input_dim, output_dim): super(CustomAttention, self).__init__() # Initialize layers and parameters here pass def forward(self, x): # Implement the forward pass here pass # Example usage input_tensor = torch.randn(32, 100, 64) # (batch_size, sequence_length, input_dim) attention_layer = CustomAttention(input_dim=64, output_dim=128) output_tensor = attention_layer(input_tensor) print(output_tensor.shape) # Expected output shape: (32, 128) ``` Ensure your `CustomAttention` class correctly handles the input and performs the attention mechanism to produce the desired output.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomAttention(nn.Module): def __init__(self, input_dim, output_dim): super(CustomAttention, self).__init__() self.input_dim = input_dim self.output_dim = output_dim # Define layers for computing attention scores self.attention_weights = nn.Linear(input_dim, 1, bias=False) # Define layers for transforming input to output dimension self.output_layer = nn.Linear(input_dim, output_dim, bias=False) def forward(self, x): # Compute attention scores attn_scores = self.attention_weights(x).squeeze(-1) # shape: (batch_size, sequence_length) attn_weights = F.softmax(attn_scores, dim=-1) # shape: (batch_size, sequence_length) # Compute weighted sum of inputs weighted_sum = torch.bmm(attn_weights.unsqueeze(1), x).squeeze(1) # shape: (batch_size, input_dim) # Transform to output dimension output = self.output_layer(weighted_sum) # shape: (batch_size, output_dim) return output # Example usage input_tensor = torch.randn(32, 100, 64) # (batch_size, sequence_length, input_dim) attention_layer = CustomAttention(input_dim=64, output_dim=128) output_tensor = attention_layer(input_tensor) print(output_tensor.shape) # Expected output shape: (32, 128)"},{"question":"**Objective**: Demonstrate understanding of `torch.fx` by writing a function to modify a neural network model\'s computation graph. # Question You are given a neural network module, `MySimpleModel`, and your task is to transform its computation graph using `torch.fx`. Specifically, you need to replace any addition operations (`torch.add`) with multiplication operations (`torch.mul`) and then append a ReLU activation (`torch.relu`) to the result of each replaced operation. Additionally, you should implement the transformation using both direct graph manipulation and using the `replace_pattern` API. # Instructions 1. Define a class `MySimpleModel` that inherits from `torch.nn.Module` and implements a simple forward pass including addition operations. 2. Write a function `transform_direct(m: torch.nn.Module) -> torch.fx.GraphModule` that: - Traces the given module using `torch.fx.symbolic_trace`. - Iterates over the nodes in the traced graph, replacing any `torch.add` calls with `torch.mul` calls. - Appends a `torch.relu` operation after each replaced `torch.mul` call. - Returns a new `GraphModule` with the transformed graph. 3. Write a function `transform_pattern(m: torch.nn.Module) -> torch.fx.GraphModule` that: - Traces the given module using `torch.fx.symbolic_trace`. - Uses `torch.fx.replace_pattern` to replace any subgraphs that perform addition with equivalent subgraphs that perform multiplication followed by ReLU. - Returns a new `GraphModule` with the transformed graph. 4. Ensure that both transformation functions produce the same output for the same input. # Constraints - You may assume that the neural network uses only single-channel inputs of shape (1, 10). - You should handle both functions so that they can be composed on any `torch.nn.Module` with basic arithmetic operations. # Example Create a neural network class: ```python class MySimpleModel(torch.nn.Module): def __init__(self): super(MySimpleModel, self).__init__() self.linear = torch.nn.Linear(10, 10) def forward(self, x): x = self.linear(x) y = x + x # This operation should be transformed return y ``` Implement the transformation functions: ```python def transform_direct(m: torch.nn.Module) -> torch.fx.GraphModule: import torch.fx graph = torch.fx.symbolic_trace(m).graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul new_node = graph.call_function(torch.relu, args=(node,)) node.replace_all_uses_with(new_node) graph.lint() # Ensure the graph is well-formed return torch.fx.GraphModule(m, graph) def transform_pattern(m: torch.nn.Module) -> torch.fx.GraphModule: import torch.fx def pattern(x, y): return x + y def replacement(x, y): return torch.relu(x * y) traced = torch.fx.symbolic_trace(m) torch.fx.replace_pattern(traced, pattern, replacement) return traced ``` Verify correctness: ```python model = MySimpleModel() transformed_direct_model = transform_direct(model) transformed_pattern_model = transform_pattern(model) input_tensor = torch.randn(1, 10) output_direct_model = transformed_direct_model(input_tensor) output_pattern_model = transformed_pattern_model(input_tensor) assert torch.allclose(output_direct_model, output_pattern_model), \\"Outputs do not match!\\" ``` Write the necessary class and functions, and ensure the transformations are correctly implemented.","solution":"import torch import torch.fx class MySimpleModel(torch.nn.Module): def __init__(self): super(MySimpleModel, self).__init__() self.linear = torch.nn.Linear(10, 10) def forward(self, x): x = self.linear(x) y = x + x # This operation should be transformed return y def transform_direct(m: torch.nn.Module) -> torch.fx.GraphModule: graph_module = torch.fx.symbolic_trace(m) graph = graph_module.graph for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: with graph.inserting_after(node): new_node = graph.call_function(torch.mul, args=(node.args[0], node.args[1])) relu_node = graph.call_function(torch.relu, args=(new_node,)) node.replace_all_uses_with(relu_node) # We can safely remove the old node after replacement graph.erase_node(node) graph_module = torch.fx.GraphModule(graph_module, graph) return graph_module def transform_pattern(m: torch.nn.Module) -> torch.fx.GraphModule: def pattern(x, y): return x + y def replacement(x, y): return torch.relu(x * y) traced = torch.fx.symbolic_trace(m) torch.fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement) return traced"},{"question":"# Manipulating XML Documents using `xml.dom` in Python Objective: Write a Python function that takes an XML string as input, parses it into a DOM object, modifies specific nodes, and returns the modified XML string. Your function should demonstrate an understanding of node traversal, element and attribute manipulation, and node creation. Task: Implement the function `modify_xml(xml_string: str) -> str` that performs the following operations: 1. Parse the input `xml_string` into a DOM object. 2. Find the first `<name>` element and change its text content to \\"John Doe\\". 3. For each `<item>` element within the document: - Add a new attribute `quantity` with the value `2`. - Append a new child element `<price>` with text content `10`. 4. Create a new comment node at the end of the document with the content \\"Modified XML\\". 5. Convert the modified DOM object back to an XML string and return it. Input: - `xml_string`: A string representing a well-formed XML document. Output: - A string representing the modified XML document. Constraints: - The input XML is guaranteed to be well-formed. - The document contains at least one `<name>` element and multiple `<item>` elements. Example: ```python input_xml = \'\'\'<?xml version=\\"1.0\\"?> <document> <name>Old Name</name> <item> <description>Item 1</description> </item> <item> <description>Item 2</description> </item> </document>\'\'\' expected_output_xml = \'\'\'<?xml version=\\"1.0\\"?> <document> <name>John Doe</name> <item quantity=\\"2\\"> <description>Item 1</description> <price>10</price> </item> <item quantity=\\"2\\"> <description>Item 2</description> <price>10</price> </item> <!-- Modified XML --> </document>\'\'\' assert modify_xml(input_xml) == expected_output_xml ``` Note: Your implementation should make good use of the `xml.dom` module functionalities such as `parseString`, `createElement`, `createComment`, node traversal methods, and node manipulation methods.","solution":"from xml.dom import minidom def modify_xml(xml_string: str) -> str: Modifies the input XML string as per the specified operations and returns the modified XML string. # Parse the input XML string dom = minidom.parseString(xml_string) # Find the first <name> element and change its text content to \\"John Doe\\" name_elements = dom.getElementsByTagName(\'name\') if name_elements: name_elements[0].firstChild.nodeValue = \\"John Doe\\" # For each <item> element: item_elements = dom.getElementsByTagName(\'item\') for item in item_elements: # Add attribute `quantity=\\"2\\"` item.setAttribute(\'quantity\', \'2\') # Append a new child element <price> with text content `10` price_element = dom.createElement(\'price\') price_text = dom.createTextNode(\'10\') price_element.appendChild(price_text) item.appendChild(price_element) # Create a new comment node at the end of the document with the content \\"Modified XML\\" comment_node = dom.createComment(\' Modified XML \') dom.documentElement.appendChild(comment_node) # Convert the modified DOM object back to an XML string modified_xml_string = dom.toxml() return modified_xml_string"},{"question":"# Seaborn Coding Assessment Objective You are required to demonstrate your understanding of the seaborn library by performing dataset visualization and customization tasks. You will load a dataset, apply seaborn themes, generate different types of plots, and customize plot parameters. Task 1. **Dataset Loading**: - Load the \'penguins\' dataset from seaborn\'s built-in datasets. 2. **Plot Customization**: - Set the seaborn theme to \'darkgrid\' and apply a color palette of your choice. - Create a bar plot showing the average body mass for each species. 3. **Advanced Customization**: - Override the seaborn theme to disable the top and right spines of the plots. - Modify the plot to include \'sex\' as the hue parameter, thus creating a grouped bar plot. 4. **Additional Plot**: - Create a scatter plot to visualize the relationship between \'bill_length_mm\' and \'bill_depth_mm\' for different species, including distinguished colors for each species. - Include a regression line for each species to show the trend. 5. **Saving the plots**: - Save both the bar plot and scatter plot as \'bar_plot.png\' and \'scatter_plot.png\', respectively. Input and Output - **Input**: There is no input as the data is loaded internally. - **Output**: Two saved plot images, \'bar_plot.png\' and \'scatter_plot.png\'. Constraints - You must use seaborn for all plotting functionalities. - Ensure the plots are properly labeled with titles, and axis labels, and include a legend where applicable. Performance Requirements - The code should be efficient and handle the dataset of the given size without performance issues. - Properly manage the plotting context to avoid any overlapping of plots in different contexts. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the \'penguins\' dataset penguins = sns.load_dataset(\\"penguins\\") # Set the seaborn theme and color palette sns.set_theme(style=\'darkgrid\', palette=\'colorblind\') # Create a bar plot showing the average body mass for each species with \'sex\' as hue plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(data=penguins, x=\'species\', y=\'body_mass_g\', hue=\'sex\') bar_plot.set_title(\'Average Body Mass by Species and Sex\') bar_plot.set_xlabel(\'Species\') bar_plot.set_ylabel(\'Body Mass (g)\') plt.legend(title=\'Sex\') plt.savefig(\'bar_plot.png\') plt.clf() # Override seaborn theme to disable top and right spines custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\'ticks\', rc=custom_params) # Create a scatter plot with regression lines plt.figure(figsize=(10, 6)) scatter_plot = sns.lmplot(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', markers=[\\"o\\", \\"s\\", \\"D\\"], height=6, aspect=1.6) plt.title(\'Bill Length vs. Bill Depth by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') scatter_plot.savefig(\'scatter_plot.png\') ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_penguins_data(): Load the \'penguins\' dataset from seaborn\'s built-in datasets. return sns.load_dataset(\\"penguins\\") def create_bar_plot(penguins): Create and save a bar plot showing the average body mass for each species, with \'sex\' as the hue parameter. # Set seaborn theme and color palette sns.set_theme(style=\'darkgrid\', palette=\'colorblind\') # Create a bar plot plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(data=penguins, x=\'species\', y=\'body_mass_g\', hue=\'sex\') bar_plot.set_title(\'Average Body Mass by Species and Sex\') bar_plot.set_xlabel(\'Species\') bar_plot.set_ylabel(\'Body Mass (g)\') plt.legend(title=\'Sex\') plt.savefig(\'bar_plot.png\') plt.clf() def create_scatter_plot(penguins): Create and save a scatter plot to visualize the relationship between \'bill_length_mm\' and \'bill_depth_mm\' for different species, including regression lines for each species. # Override seaborn theme to disable top and right spines custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\'ticks\', rc=custom_params) # Create scatter plot with regression lines scatter_plot = sns.lmplot(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', hue=\'species\', markers=[\\"o\\", \\"s\\", \\"D\\"], height=6, aspect=1.6) scatter_plot.set(title=\'Bill Length vs. Bill Depth by Species\') scatter_plot.set_axis_labels(\'Bill Length (mm)\', \'Bill Depth (mm)\') scatter_plot.savefig(\'scatter_plot.png\') # Load data penguins = load_penguins_data() # Create and save plots create_bar_plot(penguins) create_scatter_plot(penguins)"},{"question":"# Advanced List Operations and Comprehensions You are provided with several lists of students, each containing information about their scores across different subjects. Your task is to implement a function in Python that processes these lists to provide a summary of student performance. Function Signature ```python def summarize_student_performance(math_scores, science_scores, english_scores): Summarizes the performance of students based on their scores in three subjects: Math, Science, and English. Parameters: - math_scores: List of integers where each integer represents a student\'s score in Math. - science_scores: List of integers where each integer represents a student\'s score in Science. - english_scores: List of integers where each integer represents a student\'s score in English. Returns: - summary: List of dictionaries where each dictionary contains the following keys: - \\"id\\": An integer representing the student\'s ID (starting from 1). - \\"math\\": The student\'s score in Math. - \\"science\\": The student\'s score in Science. - \\"english\\": The student\'s score in English. - \\"total\\": The student\'s total score across all subjects. - \\"average\\": The student\'s average score across all subjects. - \\"grade\\": The student\'s grade, calculated as follows: - \\"A\\" for an average score >= 90 - \\"B\\" for an average score >= 80 and < 90 - \\"C\\" for an average score >= 70 and < 80 - \\"D\\" for an average score >= 60 and < 70 - \\"F\\" for an average score < 60 pass ``` Input Format 1. `math_scores`, `science_scores`, and `english_scores` are lists of integers and have the same length `n` (1  n  100), representing the scores of `n` students in Math, Science, and English respectively. Output Format - A list of dictionaries, with each dictionary summarizing the performance of a student as described above. Example Input: ```python math_scores = [95, 82, 76, 58, 65] science_scores = [88, 74, 85, 60, 70] english_scores = [91, 80, 79, 62, 67] ``` Output: ```python [ {\\"id\\": 1, \\"math\\": 95, \\"science\\": 88, \\"english\\": 91, \\"total\\": 274, \\"average\\": 91.33, \\"grade\\": \\"A\\"}, {\\"id\\": 2, \\"math\\": 82, \\"science\\": 74, \\"english\\": 80, \\"total\\": 236, \\"average\\": 78.67, \\"grade\\": \\"C\\"}, {\\"id\\": 3, \\"math\\": 76, \\"science\\": 85, \\"english\\": 79, \\"total\\": 240, \\"average\\": 80.0, \\"grade\\": \\"B\\"}, {\\"id\\": 4, \\"math\\": 58, \\"science\\": 60, \\"english\\": 62, \\"total\\": 180, \\"average\\": 60.0, \\"grade\\": \\"D\\"}, {\\"id\\": 5, \\"math\\": 65, \\"science\\": 70, \\"english\\": 67, \\"total\\": 202, \\"average\\": 67.33, \\"grade\\": \\"D\\"} ] ``` Constraints - All scores are integers between 0 and 100 inclusive. - The length of `math_scores`, `science_scores`, and `english_scores` will always be equal. Performance Requirements - Your solution should handle the upper constraints efficiently. - The use of list comprehensions where appropriate is encouraged to demonstrate understanding. Note - Ensure that the average score is rounded to two decimal places for the output.","solution":"def summarize_student_performance(math_scores, science_scores, english_scores): def calculate_grade(avg): if avg >= 90: return \\"A\\" elif avg >= 80: return \\"B\\" elif avg >= 70: return \\"C\\" elif avg >= 60: return \\"D\\" else: return \\"F\\" summary = [] for i in range(len(math_scores)): total = math_scores[i] + science_scores[i] + english_scores[i] average = round(total / 3, 2) grade = calculate_grade(average) student_summary = { \\"id\\": i + 1, \\"math\\": math_scores[i], \\"science\\": science_scores[i], \\"english\\": english_scores[i], \\"total\\": total, \\"average\\": average, \\"grade\\": grade } summary.append(student_summary) return summary"},{"question":"# PyTorch Coding Assessment Question: Custom Tracer with Dynamic Shape Handling Objective Implement a custom PyTorch function decorator to trace tensor operations, handling both constant and dynamic shapes while utilizing the Dynamo tracer capabilities described. Task Description Write a Python function decorator `@custom_compile` that traces the operations within a decorated function. The decorator should handle: 1. Linear sequences of PyTorch operations. 2. Control flow (if-else statements). 3. Dynamic shapes, assuming integer inputs and shape values might change across function calls. 4. Graph breaks gracefully when encountering non-PyTorch code or unsupported operations. Additionally, write a function `test_custom_compile` to demonstrate the use and correctness of your `@custom_compile` decorator. Requirements 1. **Decorator Function** `custom_compile`: - Takes a function with tensor inputs and returns traced execution details (FX graph). - Differentiates between constant and dynamic shapes during tracing. - Implements guards for dynamic shape attributes to ensure reusability of the traced graph. 2. **Test Function** `test_custom_compile`: - Showcases the functionality of `custom_compile` using various input sizes and values. - Demonstrates control flow handling with both constant and dynamic shapes. - Utilizes print statements or logs to verify that tracing, guards, and graph breaks work as expected. Constraints - You may use functions and utilities from the `torch.fx` module for graph creation and manipulation. - Assume that all inputs to the decorated function are PyTorch tensors or primitive Python types (integers, floats). Example Here is an example of how `@custom_compile` can be used: ```python import torch @custom_compile def dynamic_mse(x, y): if x.shape[0] % 2 == 0: z = (x - y) ** 2 else: z = (x - y) ** 3 return z.sum() def test_custom_compile(): x1 = torch.randn(200) y1 = torch.randn(200) x2 = torch.randn(201) y2 = torch.randn(201) print(dynamic_mse(x1, y1)) print(dynamic_mse(x2, y2)) ``` # Submission Requirements - A Python script or a Jupyter notebook containing: - The implementation of `@custom_compile`. - The implementation of `test_custom_compile`. - Execution of `test_custom_compile` with sample outputs demonstrating correctness. # Evaluation Criteria - Correctness of the decorator implementation (`@custom_compile`). - Handling of control flow and dynamic shapes. - Appropriate use of guards to prevent unnecessary recompilations. - Proper handling of graph breaks and tracing unsupported operations.","solution":"import torch import torch.fx as fx def custom_compile(fn): def wrapper(*args, **kwargs): class CustomTracer(fx.Tracer): def is_leaf_module(self, m, name): # Everything is a leaf module, including nn.Modules return False def create_proxy(self, kind, target, args, kwargs, name=None, type_expr=None): proxy = super().create_proxy(kind, target, args, kwargs, name, type_expr) return proxy def trace(self, root, concrete_args=()): self.concrete_args = concrete_args graph = super().trace(root, concrete_args) return graph tracer = CustomTracer() graph = tracer.trace(fn, args) gm = fx.GraphModule(tracer.root, graph) return gm(*args, **kwargs) return wrapper # Example usage and test function @custom_compile def dynamic_mse(x, y): if x.shape[0] % 2 == 0: z = (x - y) ** 2 else: z = (x - y) ** 3 return z.sum() def test_custom_compile(): x1 = torch.randn(200) y1 = torch.randn(200) x2 = torch.randn(201) y2 = torch.randn(201) result1 = dynamic_mse(x1, y1) result2 = dynamic_mse(x2, y2) print(result1) print(result2) assert result1.shape == torch.Size([]) assert result2.shape == torch.Size([]) test_custom_compile()"},{"question":"**Objective:** You are required to write a function that converts specific Python 2 constructs to their Python 3 equivalents. This task will help assess your understanding of both Python 2 and Python 3, as well as your ability to manipulate and transform code programmatically. **Problem Statement:** Write a function `convert_to_python3(code: str) -> str` that takes a string representing Python 2 code and returns a string with the code converted to Python 3. Your function should handle the following Python 2 to Python 3 conversions: 1. **Print Statement to Print Function:** Convert Python 2 print statements to Python 3 print functions. - `print \\"Hello, world!\\"` should become `print(\\"Hello, world!\\")` 2. **Raw Input to Input:** Convert `raw_input()` calls to `input()`. You don\'t need to handle any complex usage; assume the call stands alone. - `name = raw_input()` should become `name = input()` 3. **Except Clause:** Convert except clauses from general `except <exception>, <variable>` form to `except <exception> as <variable>` form. - `except ValueError, e:` should become `except ValueError as e:` 4. **Dict Iteration:** Convert `dict.iteritems()`, `dict.iterkeys()`, and `dict.itervalues()` to `dict.items()`, `dict.keys()`, and `dict.values()`, respectively. - `for k, v in my_dict.iteritems():` should become `for k, v in my_dict.items():` - Ensure to wrap these in a `list()` call if they are directly assigned or passed to functions. You do not need to handle any other conversions than those specified above. **Function Signature:** ```python def convert_to_python3(code: str) -> str: pass ``` **Input:** - A string `code` representing valid Python 2 code. The string can have multiple lines, separated by newline characters (`n`). **Output:** - A string representing the converted Python 3 code. **Constraints:** - Only basic constructs (as mentioned) need to be converted. - The code is syntactically correct Python 2 and there are no syntax errors. - You do not need to handle multi-line print statements. **Example:** ```python input_code = \'\'\'print \\"Hello, world!\\" name = raw_input(\\"Enter your name: \\") print \\"Hello, {0}!\\".format(name) except ValueError, e: print \\"Invalid input!\\" for k, v in my_dict.iteritems(): print k, v \'\'\' output_code = \'\'\'print(\\"Hello, world!\\") name = input(\\"Enter your name: \\") print(\\"Hello, {0}!\\".format(name)) except ValueError as e: print(\\"Invalid input!\\") for k, v in my_dict.items(): print(k, v) \'\'\' ``` **Notes:** - Ensure your function preserves the original indentation and spacing in the input code. - You should provide appropriate comments within your code to explain your logic. Good luck!","solution":"def convert_to_python3(code: str) -> str: # Split the code into lines lines = code.split(\'n\') converted_lines = [] for line in lines: stripped_line = line.strip() # Handle print statements if stripped_line.startswith(\\"print \\") and not stripped_line.startswith(\\"print(\\"): line = line.replace(\'print \', \'print(\', 1) + \')\' # Handle raw_input if \'raw_input\' in stripped_line: line = line.replace(\'raw_input\', \'input\', 1) # Handle except clauses if \'except \' in stripped_line and \', \' in stripped_line: line = line.replace(\', \', \' as \') # Handle dict.iter* methods if \'iteritems()\' in stripped_line: line = line.replace(\'iteritems()\', \'items()\') if \'iterkeys()\' in stripped_line: line = line.replace(\'iterkeys()\', \'keys()\') if \'itervalues()\' in stripped_line: line = line.replace(\'itervalues()\', \'values()\') converted_lines.append(line) # Join the converted lines back into a single string return \'n\'.join(converted_lines)"},{"question":"**Objective**: Assess your understanding of seaborn\'s object-oriented interface for creating complex visualizations. **Task**: Write a function `plot_penguin_species(data)` that takes a pandas DataFrame `data` containing the `penguins` dataset loaded through `seaborn.load_dataset(\\"penguins\\")`. Your task is to create a bar plot that visualizes the average body mass of each penguin species, split by gender, with standard deviation error bars. The function should save the plot as a PNG image file named `penguin_species_plot.png`. **Requirements**: 1. The x-axis should represent the different penguin species (`Adelie`, `Chinstrap`, `Gentoo`). 2. The bars should be color-coded by the gender (`male`, `female`). 3. The y-axis should represent the average body mass (`body_mass_g`) of the penguins. 4. Error bars should represent the standard deviation of the body mass. 5. The bars for males and females should be dodged (side-by-side) to avoid overlap. 6. The plot should have clear axis labels and a title. **Expected Input and Output**: - Input: A pandas DataFrame `data` with columns `species`, `sex`, and `body_mass_g`. - Output: A PNG file named `penguin_species_plot.png` saved in the current working directory. **Function Signature**: ```python def plot_penguin_species(data: pd.DataFrame) -> None: pass ``` **Constraints**: - You must use seaborn\'s object-oriented interface (`seaborn.objects`). - Assume the input DataFrame `data` is clean and contains no missing values. - Your solution should be efficient and well-structured. **Example**: ```python import pandas as pd from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Call your function plot_penguin_species(penguins) ``` After running the code above, a file named `penguin_species_plot.png` should be created in the current working directory with the described plot.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_species(data: pd.DataFrame) -> None: # Set theme for the plot sns.set_theme(style=\\"whitegrid\\") # Create the bar plot with error bars plot = sns.catplot( data=data, x=\\"species\\", y=\\"body_mass_g\\", hue=\\"sex\\", kind=\\"bar\\", ci=\\"sd\\", # standard deviation for error bars dodge=True, palette=\\"dark\\" ) # Add labels and title plot.set_axis_labels(\\"Penguin Species\\", \\"Average Body Mass (g)\\") plot.fig.suptitle(\\"Average Body Mass of Penguin Species by Gender\\", y=1.03) # Save the plot to a file plot.savefig(\\"penguin_species_plot.png\\")"},{"question":"The module `contextvars` in Python allows managing context-specific data. Context variables are variables that can have different values in different contexts. Each context is independent and can be manipulated separately. # Problem Statement Implement a Python function `context_manager`, using the Python `contextvars` module, which simulates the behavior of a nested function call where each function can dynamically change the context. You need to implement the following functions in your code: 1. **`initialize_context_var(name: str, default_value: Optional[Any] = None) -> contextvars.ContextVar`**: - Initialize and return a new context variable with the given `name` and an optional `default_value`. 2. **`set_context_var(var: contextvars.ContextVar, value: Any) -> contextvars.Token`**: - Set the given context variable `var` to `value` and return the token. 3. **`reset_context_var(var: contextvars.ContextVar, token: contextvars.Token) -> None`**: - Reset the context variable `var` using the provided `token`. 4. **`get_context_var(var: contextvars.ContextVar) -> Any`**: - Retrieve the value of the given context variable `var`. If it is not set, return the default. 5. **`function_with_context(func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any`**: - Execute `func` within the current context and return its result. # Example Usage ```python import contextvars from typing import Callable, Any, Optional def initialize_context_var(name: str, default_value: Optional[Any] = None) -> contextvars.ContextVar: var = contextvars.ContextVar(name, default=default_value) return var def set_context_var(var: contextvars.ContextVar, value: Any) -> contextvars.Token: token = var.set(value) return token def reset_context_var(var: contextvars.ContextVar, token: contextvars.Token) -> None: var.reset(token) def get_context_var(var: contextvars.ContextVar) -> Any: return var.get() def function_with_context(func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any: return func(*args, **kwargs) # Example my_var = initialize_context_var(\'my_var\', 42) print(get_context_var(my_var)) # Output: 42 token = set_context_var(my_var, 100) print(get_context_var(my_var)) # Output: 100 reset_context_var(my_var, token) print(get_context_var(my_var)) # Output: 42 def my_function(): return get_context_var(my_var) print(function_with_context(my_function)) # Output: 42 token2 = set_context_var(my_var, 200) print(function_with_context(my_function)) # Output: 200 reset_context_var(my_var, token2) ``` # Constraints and Considerations 1. **Input / Output**: The functions will directly deal with `contextvars.ContextVar` instances and consequently the `contextvars` module. Handle edge cases where necessary. 2. **Performance**: Ensure that context variable operations (set, get, reset) are efficient, as context switching should be optimized for quick lookup and update. 3. **Thread Safety**: While the problem does not explicitly parallelize contexts, remember that context variables are inherently designed to work safely in multi-threaded environments. Implement these functions such that they mimic the behavior described and allow managing context variables effectively within a context-managed execution flow.","solution":"import contextvars from typing import Callable, Any, Optional def initialize_context_var(name: str, default_value: Optional[Any] = None) -> contextvars.ContextVar: var = contextvars.ContextVar(name, default=default_value) return var def set_context_var(var: contextvars.ContextVar, value: Any) -> contextvars.Token: token = var.set(value) return token def reset_context_var(var: contextvars.ContextVar, token: contextvars.Token) -> None: var.reset(token) def get_context_var(var: contextvars.ContextVar) -> Any: return var.get() def function_with_context(func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any: return func(*args, **kwargs)"},{"question":"# Email Serialization with `BytesGenerator` You are tasked with creating a function that uses the `email.generator.BytesGenerator` class to serialize an email message. The serialized message should comply with standard email formatting rules and handle various edge cases such as non-ASCII characters in headers, different content encodings, and the `multipart/signed` MIME type. # Requirements 1. **Function Definition**: Implement the function `serialize_email(message, output_file_path)`. - **Parameters**: - `message`: An instance of `email.message.EmailMessage` representing the email to be serialized. - `output_file_path`: A string representing the path to the output file where the serialized email will be written. - **Return**: This function does not need to return anything. The output is the serialized email written to the specified file. 2. **Constraints**: - The email message can contain multiple parts with different MIME types. - Headers may contain non-ASCII characters. - Handle edge cases like lines starting with \\"From \\" in the body. - Preserve the original transfer encodings when possible. 3. **Implementation Considerations**: - Set a reasonable `maxheaderlen` to avoid excessively long header lines. - Use a `policy` that ensures standards-compliant generation. - Preserve the integrity of `multipart/signed` parts. - Ensure that binary data is appropriately serialized. # Example Usage ```python from email.message import EmailMessage from email.policy import default # Create an example email message msg = EmailMessage() msg[\'From\'] = \'example@example.com\' msg[\'To\'] = \'recipient@example.com\' msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email.\') # Define the function to serialize the email def serialize_email(message, output_file_path): import email.generator with open(output_file_path, \'wb\') as output_file: gen = email.generator.BytesGenerator(output_file, maxheaderlen=78, policy=default) gen.flatten(message) # Use the function serialize_email(msg, \'serialized_email.eml\') ``` **Notes**: - Ensure all necessary imports from the `email` package are correctly handled. - Additional headers and parts in the email message should also be correctly serialized. - Consider edge cases and test thoroughly to ensure the function works correctly across different scenarios.","solution":"def serialize_email(message, output_file_path): Serializes an email message to a specified file path. Args: message (EmailMessage): An instance of email.message.EmailMessage representing the email to be serialized. output_file_path (str): The path to the output file where the serialized email will be written. import email.generator from email.policy import default with open(output_file_path, \'wb\') as output_file: generator = email.generator.BytesGenerator(output_file, maxheaderlen=78, policy=default) generator.flatten(message)"},{"question":"# Python Coding Assessment Question Objective Demonstrate your understanding of the `errno` module by writing a function that categorizes error codes and translates them to their corresponding error messages and exceptions. Task Implement a Python function `handle_error_code(error_code)` that takes an integer `error_code` as input and returns a tuple consisting of: - The string name of the error (e.g., `\'EPERM\'` for error code 1). - The corresponding error message (e.g., `\'Operation not permitted\'`). - The exception class mapped to the error (e.g., `PermissionError` for `errno.EPERM`). If the `error_code` is not recognized (i.e., not present in `errno.errorcode`), the function should return a tuple with all elements set to `None`. Input - `error_code` (int): A numeric error code representing a system error. Output - A tuple (str, str, Exception or None): Where: - The first element is the string name of the error. - The second element is the error message. - The third element is the corresponding exception class or `None` if no exception is associated. Constraints - Assume `error_code` is a non-negative integer. - Focus on the error codes defined in the documentation provided. # Examples ```python >>> handle_error_code(1) (\'EPERM\', \'Operation not permitted\', PermissionError) >>> handle_error_code(2) (\'ENOENT\', \'No such file or directory\', FileNotFoundError) >>> handle_error_code(999) (None, None, None) >>> handle_error_code(5) (\'EIO\', \'I/O error\', None) ``` # Implementation Notes - Utilize the `errno` module to map error codes to their string names and error messages. - Use the `os.strerror()` function to get the error message for a given error code. - Use a predefined dictionary to map error codes to their exception classes. ```python import errno import os # Predefined mapping of error codes to their exception classes error_exceptions = { errno.EPERM: PermissionError, errno.ENOENT: FileNotFoundError, errno.ESRCH: ProcessLookupError, errno.EINTR: InterruptedError, errno.ECHILD: ChildProcessError, errno.EAGAIN: BlockingIOError, errno.EACCES: PermissionError, errno.EEXIST: FileExistsError, errno.ENOTDIR: NotADirectoryError, errno.EISDIR: IsADirectoryError, errno.EPIPE: BrokenPipeError, errno.ECONNABORTED: ConnectionAbortedError, errno.ECONNRESET: ConnectionResetError, errno.ESHUTDOWN: BrokenPipeError, errno.ETIMEDOUT: TimeoutError, errno.ECONNREFUSED: ConnectionRefusedError, errno.EALREADY: BlockingIOError, errno.EINPROGRESS: BlockingIOError } def handle_error_code(error_code): if error_code in errno.errorcode: error_string = errno.errorcode[error_code] error_message = os.strerror(error_code) error_exception = error_exceptions.get(error_code, None) return (error_string, error_message, error_exception) else: return (None, None, None) ```","solution":"import errno import os # Predefined mapping of error codes to their exception classes error_exceptions = { errno.EPERM: PermissionError, errno.ENOENT: FileNotFoundError, errno.ESRCH: ProcessLookupError, errno.EINTR: InterruptedError, errno.ECHILD: ChildProcessError, errno.EAGAIN: BlockingIOError, errno.EACCES: PermissionError, errno.EEXIST: FileExistsError, errno.ENOTDIR: NotADirectoryError, errno.EISDIR: IsADirectoryError, errno.EPIPE: BrokenPipeError, errno.ECONNABORTED: ConnectionAbortedError, errno.ECONNRESET: ConnectionResetError, errno.ESHUTDOWN: BrokenPipeError, errno.ETIMEDOUT: TimeoutError, errno.ECONNREFUSED: ConnectionRefusedError, errno.EALREADY: BlockingIOError, errno.EINPROGRESS: BlockingIOError } def handle_error_code(error_code): if error_code in errno.errorcode: error_string = errno.errorcode[error_code] error_message = os.strerror(error_code) error_exception = error_exceptions.get(error_code, None) return (error_string, error_message, error_exception) else: return (None, None, None)"},{"question":"Create a function `process_string(input_string: str) -> List[int]` that takes a string `input_string` which consists of words separated by spaces. Each word is either a sequence of digits or a sequence of alphabetic characters. The function should process the `input_string` in the following steps: 1. **Separate** the string into a list of words. 2. **Identify** which words are composed entirely of digits and which are composed of alphabetic characters. 3. **Convert** digit-words (numbers represented as strings) into integers. 4. **Sum** the integers derived from the digit-words. 5. **Create** a list of lengths of the alphabetic character words. The function should return a list of integers consisting of the sum of numbers followed by the lengths of the alphabetic words in their original order. Example Input: ```python input_string = \\"123 apple 456 banana 789 cherry\\" ``` Example Output: ```python [1368, 5, 6, 6] ``` # Constraints - The `input_string` consists of words separated by single spaces. - Each word is either entirely composed of digits or entirely alphabetic characters. - The function should handle cases where the string is empty or there are no digit-words. # Implementation ```python from typing import List def process_string(input_string: str) -> List[int]: # Split the input string into words words = input_string.split() digit_sum = 0 word_lengths = [] for word in words: if word.isdigit(): digit_sum += int(word) elif word.isalpha(): word_lengths.append(len(word)) # Return the result as a list with digit_sum followed by word_lengths return [digit_sum] + word_lengths # Example usage: # input_string = \\"123 apple 456 banana 789 cherry\\" # print(process_string(input_string)) # Output: [1368, 5, 6, 6] ``` This problem requires the student to demonstrate proficiency in string handling, type conversion, looping, and conditional statements in Python.","solution":"from typing import List def process_string(input_string: str) -> List[int]: # Split the input string into words words = input_string.split() digit_sum = 0 word_lengths = [] for word in words: if word.isdigit(): digit_sum += int(word) elif word.isalpha(): word_lengths.append(len(word)) # Return the result as a list with digit_sum followed by word_lengths return [digit_sum] + word_lengths # Example usage: # input_string = \\"123 apple 456 banana 789 cherry\\" # print(process_string(input_string)) # Output: [1368, 5, 6, 6]"},{"question":"# XML Data Transformation Objective: You are required to use the `xml.etree.ElementTree` module to parse an XML document, perform specific transformations, and generate a new XML document. Problem Statement: Given an XML document representing a catalog of books, create a function `transform_book_catalog(xml_string: str, price_limit: float) -> str` that performs the following operations: 1. Parse the provided XML string. 2. Find all books priced above the given `price_limit` and change their price to \\"N/A\\". 3. Add a new attribute `discount=\\"true\\"` to all the books which are priced below or equal to the `price_limit`. 4. Return the modified XML document as a string. Input: - `xml_string`: (str) A string representation of the XML document. - `price_limit`: (float) A price limit used for filtering and modification rules. Output: - (str) The modified XML string after applying the transformations. Constraints: - Use the `xml.etree.ElementTree` module for XML parsing and modifications. - The structure of the XML document is guaranteed to be valid. - Performance is not the primary concern, but the solution should handle XML documents of reasonable size efficiently. Example Input: ```xml xml_string = <catalog> <book id=\\"bk101\\"> <author>John Doe</author> <title>Programming Principles</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2023-01-01</publish_date> <description>Introductory guide to programming</description> </book> <book id=\\"bk102\\"> <author>Jane Smith</author> <title>Advanced Python</title> <genre>Computer</genre> <price>59.95</price> <publish_date>2023-02-15</publish_date> <description>Advanced techniques and concepts in Python</description> </book> </catalog> price_limit = 50.00 ``` Example Output: ```xml <catalog> <book id=\\"bk101\\" discount=\\"true\\"> <author>John Doe</author> <title>Programming Principles</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2023-01-01</publish_date> <description>Introductory guide to programming</description> </book> <book id=\\"bk102\\"> <author>Jane Smith</author> <title>Advanced Python</title> <genre>Computer</genre> <price>N/A</price> <publish_date>2023-02-15</publish_date> <description>Advanced techniques and concepts in Python</description> </book> </catalog> ``` Your task is to implement the function `transform_book_catalog(xml_string: str, price_limit: float) -> str`.","solution":"import xml.etree.ElementTree as ET def transform_book_catalog(xml_string: str, price_limit: float) -> str: Transforms the book catalog XML string based on the given price limit. - Books priced above the price limit will have their price set to \'N/A\'. - Books priced below or equal to the price limit will have an attribute \'discount=\\"true\\"\' added. Args: - xml_string: str: The XML string representing the book catalog. - price_limit: float: The price limit for the transformation rules. Returns: - str: The transformed XML string. root = ET.fromstring(xml_string) for book in root.findall(\'book\'): price_element = book.find(\'price\') if price_element is not None: price = float(price_element.text) if price > price_limit: price_element.text = \'N/A\' else: book.set(\'discount\', \'true\') return ET.tostring(root, encoding=\'unicode\') # Example usage xml_string = <catalog> <book id=\\"bk101\\"> <author>John Doe</author> <title>Programming Principles</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2023-01-01</publish_date> <description>Introductory guide to programming</description> </book> <book id=\\"bk102\\"> <author>Jane Smith</author> <title>Advanced Python</title> <genre>Computer</genre> <price>59.95</price> <publish_date>2023-02-15</publish_date> <description>Advanced techniques and concepts in Python</description> </book> </catalog> price_limit = 50.00 print(transform_book_catalog(xml_string, price_limit))"},{"question":"Suppose you are given a list of strings, where each string represents a line of a text. Your task is to write a function that processes this list and returns a new list of strings where each string is reversed and concatenated with its length. **Function Signature:** ```python def process_lines(lines: list) -> list: pass ``` **Input:** - `lines`: A list of strings, where each string represents a line of text. The list can contain up to 1000 strings, and each string can have up to 100 characters. **Output:** - A list of strings where each original string is reversed and concatenated with its length. **Constraints:** - The original list will only contain valid ASCII characters. - The list and strings will not be empty. **Example:** ```python lines = [\\"hello\\", \\"world\\", \\"Python\\"] result = process_lines(lines) print(result) ``` **Expected Output:** ```python [\'olleh5\', \'dlrow5\', \'nohtyP6\'] ``` **Explanation:** - The first string \\"hello\\" is reversed to \\"olleh\\" and concatenated with its length, resulting in the string \\"olleh5\\". - The second string \\"world\\" is reversed to \\"dlrow\\" and concatenated with its length, resulting in the string \\"dlrow5\\". - The third string \\"Python\\" is reversed to \\"nohtyP\\" and concatenated with its length, resulting in the string \\"nohtyP6\\". Make sure to handle edge cases such as: - Strings with spaces. - Strings with special characters.","solution":"def process_lines(lines): Processes a list of strings, reversing each string and appending its length. Args: lines (list): A list of strings, where each string represents a line of text. Returns: list: A list of processed strings where each original string is reversed and concatenated with its length. return [line[::-1] + str(len(line)) for line in lines]"},{"question":"Objective: Implement a Python function that utilizes the `email.message.Message` class to parse a raw email message string, extract specific email headers, and return the extracted information in a structured format. Background: You are provided with an email message as a raw string. Your task is to implement a function that processes this string using the `email.message.Message` class, extracts the values of certain specified headers, and returns these values in a dictionary. Function Specification: - **Function Name**: `extract_headers` - **Input**: - `raw_email` (str): A raw email message string. - `header_list` (List[str]): A list of header names to extract from the email message. - **Output**: - `Dict[str, str]`: A dictionary where the keys are the header names from `header_list` and the values are the corresponding header values from the email message. If a header from `header_list` is not found in the email, it should not appear in the output dictionary. - **Constraints**: - The input email message is compliant with RFC 5322 and MIME standards. - The input list `header_list` contains valid header names (case-insensitive). - Handle possible encoding issues gracefully, ensuring no runtime errors cause your function to break. Example: ```python from email import message_from_string from typing import List, Dict def extract_headers(raw_email: str, header_list: List[str]) -> Dict[str, str]: # Your code here # Example usage: raw_email = MIME-Version: 1.0 Content-Type: text/plain; charset=\\"US-ASCII\\" From: user@example.com To: recipient@example.com Subject: Test Email This is a test email message. header_list = [\\"From\\", \\"To\\", \\"Subject\\", \\"Date\\"] result = extract_headers(raw_email, header_list) print(result) ``` Expected Output: ```python { \\"From\\": \\"user@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Subject\\": \\"Test Email\\" } ``` Notes: - Ensure you import the necessary modules from the `email` package. - Make use of the `email.message.Message` class and associated methods to process the raw email string efficiently. - Other headers not mentioned in `header_list` should not be included in the output dictionary.","solution":"from email import message_from_string from typing import List, Dict def extract_headers(raw_email: str, header_list: List[str]) -> Dict[str, str]: # Parse the raw email into an email message object email_message = message_from_string(raw_email) # Extract the specified headers extracted_headers = {} for header in header_list: if header in email_message: extracted_headers[header] = email_message[header] return extracted_headers"},{"question":"Objective: Demonstrate your understanding of seaborn by creating a comprehensive plot using `catplot` and customizing it to convey meaningful insights from a dataset. Question: Using the seaborn `titanic` dataset, create a series of plots to visualize the following: 1. Distribution of passengers\' ages across different classes. 2. Comparison of survival rates between different genders within each class. 3. Overlay distribution of ages and survival status markers for different classes. Instructions: 1. **Load Dataset**: Load the `titanic` dataset using seaborn. 2. **Plot 1 - Age Distribution by Class**: - Create a `catplot` to show the distribution of passengers\' ages across different classes. - Use a `violin` plot for this visualization. 3. **Plot 2 - Survival Rate by Gender and Class**: - Create a `catplot` to compare the survival rates between different genders within each class. - Use a `bar` plot for this visualization. - Arrange the plots in separate columns for each gender. 4. **Plot 3 - Age Distribution with Survival Status**: - Create a `catplot` to overlay a scatter plot of age distributions and markers for survival status (survived or not) within different classes. - Use a `box` plot to show the age distribution. - Overlay a `swarmplot` to show individual survival markers. 5. **Customize the Plots**: - Add meaningful axis labels and titles. - Set an appropriate limit for the age axis in plot 3. - Utilize different colors to distinguish gender and survival status. Expected Functions: You are expected to define the following functions in your solution: 1. `load_titanic_dataset()`: Load and return the `titanic` dataset. 2. `plot_age_distribution_by_class(df)`: Create and customize the first plot. 3. `plot_survival_rate_by_gender_and_class(df)`: Create and customize the second plot. 4. `plot_age_distribution_with_survival(df)`: Create and customize the third plot. Skeleton Code: ```python import seaborn as sns import matplotlib.pyplot as plt def load_titanic_dataset(): pass def plot_age_distribution_by_class(df): pass def plot_survival_rate_by_gender_and_class(df): pass def plot_age_distribution_with_survival(df): pass if __name__ == \\"__main__\\": df = load_titanic_dataset() plot_age_distribution_by_class(df) plot_survival_rate_by_gender_and_class(df) plot_age_distribution_with_survival(df) # Show all plots plt.show() ``` Constraints: - Ensure to use seaborn\'s inherent functions and customization options. - Your solution should be optimized for readability and maintainability. Submission: Submit your Python file containing the full implementation of the required functions and the main script as shown above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_titanic_dataset(): Load the titanic dataset from seaborn\'s repository. Returns: DataFrame: Titanic dataset return sns.load_dataset(\'titanic\') def plot_age_distribution_by_class(df): Create a catplot to show the distribution of passengers\' ages across different classes. Args: df (DataFrame): Titanic dataset plt.figure(figsize=(10,6)) sns.catplot(x=\'class\', y=\'age\', kind=\'violin\', data=df, height=6, aspect=1.5) plt.title(\'Age Distribution by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') def plot_survival_rate_by_gender_and_class(df): Create a catplot to compare the survival rates between different genders within each class. Args: df (DataFrame): Titanic dataset plt.figure(figsize=(10,6)) sns.catplot(x=\'class\', hue=\'sex\', col=\'sex\', y=\'survived\', kind=\'bar\', data=df, height=6, aspect=1) plt.suptitle(\'Survival Rate by Gender and Class\', y=1.02) plt.xlabel(\'Class\') plt.ylabel(\'Survival Rate\') def plot_age_distribution_with_survival(df): Create a catplot to overlay a scatter plot of age distributions and markers for survival status within different classes. Args: df (DataFrame): Titanic dataset plt.figure(figsize=(10,6)) sns.catplot(x=\'class\', y=\'age\', kind=\'box\', data=df, height=6, aspect=1.5) sns.swarmplot(x=\'class\', y=\'age\', hue=\'survived\', data=df, dodge=True, palette={0:\'red\', 1:\'green\'}, alpha=0.7) plt.title(\'Age Distribution with Survival Status by Class\') plt.xlabel(\'Class\') plt.ylabel(\'Age\') plt.ylim(0,85) plt.legend(title=\'Survived\', loc=\'upper right\') if __name__ == \\"__main__\\": df = load_titanic_dataset() plot_age_distribution_by_class(df) plot_survival_rate_by_gender_and_class(df) plot_age_distribution_with_survival(df) # Show all plots plt.show()"},{"question":"# Pandas Coding Assessment Question Problem Statement: You are given a dataset containing information about various products in a store, including their category, price, and sale status. Your task is to write a function that performs several data manipulation operations using pandas and returns specific information. Dataset: The dataset is provided as a list of dictionaries, where each dictionary represents a product record with \'Category\', \'Product\', \'Price\', and \'OnSale\' keys. ```python data = [ {\\"Category\\": \\"Electronics\\", \\"Product\\": \\"Smartphone\\", \\"Price\\": 699, \\"OnSale\\": False}, {\\"Category\\": \\"Electronics\\", \\"Product\\": \\"Laptop\\", \\"Price\\": 999, \\"OnSale\\": True}, {\\"Category\\": \\"Furniture\\", \\"Product\\": \\"Desk\\", \\"Price\\": 150, \\"OnSale\\": True}, {\\"Category\\": \\"Furniture\\", \\"Product\\": \\"Chair\\", \\"Price\\": 85, \\"OnSale\\": False}, {\\"Category\\": \\"Apparel\\", \\"Product\\": \\"Jacket\\", \\"Price\\": 120, \\"OnSale\\": True}, {\\"Category\\": \\"Apparel\\", \\"Product\\": \\"T-Shirt\\", \\"Price\\": 25, \\"OnSale\\": False}, ] ``` Function: Implement the `analyze_products(data: List[Dict[str, Any]]) -> Tuple[pd.Series, pd.DataFrame]` function to meet the following requirements: 1. **DataFrame Creation**: Create a pandas DataFrame from the given list of product dictionaries. 2. **Adding Columns**: Add a new column \'DiscountedPrice\' where the price is reduced by 10% for products on sale. 3. **Filtering and Aggregation**: - Create a Series that shows the total price of products in each category. - Create a new DataFrame that contains only the products which are on sale. 4. **Returning Results**: Return the total price Series and the filtered DataFrame. Input: - `data`: A list of dictionaries, where each dictionary contains information about a product. Output: - A tuple containing: 1. A pandas Series showing the total price of products in each category. 2. A pandas DataFrame containing only the products that are on sale with updated discounted prices. Constraints: - The input list will have at least one product record. - All prices are non-negative integers. Example: ```python data = [ {\\"Category\\": \\"Electronics\\", \\"Product\\": \\"Smartphone\\", \\"Price\\": 699, \\"OnSale\\": False}, {\\"Category\\": \\"Electronics\\", \\"Product\\": \\"Laptop\\", \\"Price\\": 999, \\"OnSale\\": True}, {\\"Category\\": \\"Furniture\\", \\"Product\\": \\"Desk\\", \\"Price\\": 150, \\"OnSale\\": True}, {\\"Category\\": \\"Furniture\\", \\"Product\\": \\"Chair\\", \\"Price\\": 85, \\"OnSale\\": False}, {\\"Category\\": \\"Apparel\\", \\"Product\\": \\"Jacket\\", \\"Price\\": 120, \\"OnSale\\": True}, {\\"Category\\": \\"Apparel\\", \\"Product\\": \\"T-Shirt\\", \\"Price\\": 25, \\"OnSale\\": False}, ] total_price_series, on_sale_df = analyze_products(data) print(total_price_series) # Output: # Category # Apparel 145 # Electronics 1698 # Furniture 235 # Name: Price, dtype: int64 print(on_sale_df) # Output: # Category Product Price OnSale DiscountedPrice # 1 Electronics Laptop 999 True 899.10 # 2 Furniture Desk 150 True 135.00 # 4 Apparel Jacket 120 True 108.00 ``` Note: - The output Series should be labeled by \'Category\'. - The column in the on-sale DataFrame for the discounted price should be named \'DiscountedPrice\'.","solution":"import pandas as pd from typing import List, Dict, Any, Tuple def analyze_products(data: List[Dict[str, Any]]) -> Tuple[pd.Series, pd.DataFrame]: # Step 1: Create a pandas DataFrame from the given list of product dictionaries. df = pd.DataFrame(data) # Step 2: Add a new column \'DiscountedPrice\' where the price is reduced by 10% for products on sale. df[\'DiscountedPrice\'] = df.apply(lambda row: row[\'Price\'] * 0.9 if row[\'OnSale\'] else row[\'Price\'], axis=1) # Step 3a: Create a Series that shows the total price of products in each category. total_price_series = df.groupby(\'Category\')[\'Price\'].sum() # Step 3b: Create a new DataFrame that contains only the products which are on sale. on_sale_df = df[df[\'OnSale\']] # Step 4: Return the total price Series and the filtered DataFrame. return total_price_series, on_sale_df"},{"question":"# Out-of-Core Learning with scikit-learn **Objective**: Implement an out-of-core learning system using scikit-learn that processes data too large to fit into memory, by performing incremental learning with streaming instances and feature extraction. **Problem Statement**: You are provided with a large dataset consisting of text documents classified into various categories. The dataset is so large that it cannot be fully loaded into memory at once. Your task is to implement an out-of-core learning system using the `sklearn.linear_model.SGDClassifier` (an incremental learning algorithm), `sklearn.feature_extraction.text.HashingVectorizer` (for feature extraction), and generator functions (for streaming data). # Requirements 1. **Data streaming**: Implement a generator function `stream_documents` that lazily loads and yields documents from a file. 2. **Feature extraction**: Use `sklearn.feature_extraction.text.HashingVectorizer` for extracting features from the streamed documents. 3. **Incremental learning**: Use `sklearn.linear_model.SGDClassifier` with the `partial_fit` method for training on mini-batches of streamed data. # Input and Output Format - **Input**: - `file_path`: Path to the large text dataset file. - `batch_size`: Number of documents to process in a single batch. - `n_features`: Number of features for the `HashingVectorizer`. - `classes`: List of possible target classes. - **Output**: - A trained `SGDClassifier` model. # Constraints - Ensure that the implementation handles large files efficiently and does not load the entire file into memory. - The generator function should read and yield one document at a time. - Mini-batch size should be tunable. # Implementation Provide the implementation for the following function: ```python import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer def stream_documents(file_path): Generator function to stream documents from a file. Parameters: - file_path (str): Path to the file containing the dataset. Yields: - document (str): A single document from the file. pass # Implement the logic to stream documents one by one def incremental_learning(file_path, batch_size, n_features, classes): Perform out-of-core learning using SGDClassifier and HashingVectorizer. Parameters: - file_path (str): Path to the file containing the dataset. - batch_size (int): Number of documents to process in each mini-batch. - n_features (int): Number of features for the HashingVectorizer. - classes (list): List of possible target classes. Returns: - model (SGDClassifier): Trained SGDClassifier model. vectorizer = HashingVectorizer(n_features=n_features) model = SGDClassifier() # Stream documents and train the model incrementally return model # Example usage: # model = incremental_learning(\\"path_to_large_dataset.txt\\", 100, 2**20, [\\"class1\\", \\"class2\\", ...]) ``` # Example Given a file `large_text_data.txt` containing documents and categories, call the function: ```python model = incremental_learning(\\"large_text_data.txt\\", 100, 2**20, [\\"sports\\", \\"technology\\", \\"politics\\", \\"entertainment\\"]) ``` This function should return a trained `SGDClassifier` model having trained in an out-of-core manner on the provided dataset. **Notes**: - You can assume that the input file format is such that each line corresponds to a document followed by its category, e.g., \\"document content heretcategory\\". # Evaluation Criteria - **Correctness**: Does the function correctly perform out-of-core learning? - **Efficiency**: Is the solution memory efficient, handling large datasets appropriately? - **Code Quality**: Is the code clean, well-documented, and readable?","solution":"import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer def stream_documents(file_path): Generator function to stream documents from a file. Parameters: - file_path (str): Path to the file containing the dataset. Yields: - document (str), category (str): A single document and its category from the file. with open(file_path, \'r\') as f: for line in f: doc, category = line.strip().rsplit(\'t\', 1) yield doc, category def incremental_learning(file_path, batch_size, n_features, classes): Perform out-of-core learning using SGDClassifier and HashingVectorizer. Parameters: - file_path (str): Path to the file containing the dataset. - batch_size (int): Number of documents to process in each mini-batch. - n_features (int): Number of features for the HashingVectorizer. - classes (list): List of possible target classes. Returns: - model (SGDClassifier): Trained SGDClassifier model. vectorizer = HashingVectorizer(n_features=n_features) model = SGDClassifier() all_docs = [] all_labels = [] for doc, category in stream_documents(file_path): all_docs.append(doc) all_labels.append(category) if len(all_docs) >= batch_size: X_train = vectorizer.transform(all_docs) y_train = np.array(all_labels) model.partial_fit(X_train, y_train, classes=classes) all_docs = [] all_labels = [] if all_docs: X_train = vectorizer.transform(all_docs) y_train = np.array(all_labels) model.partial_fit(X_train, y_train, classes=classes) return model"},{"question":"# Email Serialization Challenge You are required to implement a function `serialize_email` that takes an email message object and serializes it into a binary file-like object using the `email.generator.BytesGenerator` class. Your function should also handle MIME content appropriately and ensure compliance with the RFC standards for email messages. Function Signature ```python def serialize_email(msg, binary_file): pass ``` Parameters: - `msg`: An instance of `email.message.EmailMessage` which represents the email message that needs to be serialized. - `binary_file`: A file-like object that supports the `write` method and accepts binary data. Requirements: 1. The function should serialize the given email message `msg` to the provided `binary_file` in binary format. 2. Ensure the serialized content meets the ASCII compatibility requirements if the message contains binary data, using appropriate content transfer encoding techniques. 3. Implement proper error handling for cases where the email message might have RFC-invalid non-ASCII bytes. Example Usage: ```python from email.message import EmailMessage import io def serialize_email(msg, binary_file): from email.generator import BytesGenerator generator = BytesGenerator(binary_file) generator.flatten(msg) # Example message and file object msg = EmailMessage() msg[\'Subject\'] = \'Test Subject\' msg[\'From\'] = \'from@example.com\' msg[\'To\'] = \'to@example.com\' msg.set_content(\'This is a test email.\') # Serialize to a binary file-like object binary_file = io.BytesIO() serialize_email(msg, binary_file) # Print the binary content (for demonstration purposes) print(binary_file.getvalue()) ``` Constraints: 1. Do not use any external libraries except the standard `email` and `io` modules. 2. The function should handle MIME and non-MIME email messages. 3. The serialization should adhere to the standards and policies as mentioned in the provided documentation. Note: You may employ additional helper functions if needed to keep your code organized and maintainable.","solution":"def serialize_email(msg, binary_file): Serializes the given email message `msg` to the provided `binary_file` in binary format. Parameters: msg: email.message.EmailMessage - The email message to be serialized. binary_file: A file-like object that supports the `write` method and accepts binary data. from email.generator import BytesGenerator import io # Validate that msg is an instance of EmailMessage if not isinstance(msg, io.TextIOWrapper): from email.message import EmailMessage if not isinstance(msg, EmailMessage): raise TypeError(\\"msg must be an instance of email.message.EmailMessage\\") # Validate that binary_file supports \'write\' and accepts binary data if not hasattr(binary_file, \'write\') or \'b\' not in getattr(binary_file, \'mode\', \'wb\'): raise TypeError(\\"binary_file must be a binary file-like object supporting \'write\' method\\") # Create a BytesGenerator instance to serialize the msg to the binary_file generator = BytesGenerator(binary_file) generator.flatten(msg)"},{"question":"<|Analysis Begin|> The provided documentation outlines several functions related to descriptor objects in Python, as well as types for creating new descriptor objects, and methods to check the type of descriptors. This portion of the Python C API is used to create and manage descriptors, which are a specific feature in Python used to describe properties and methods dynamically. Notably, descriptors can be used to create properties, methods, and other types of attributes dynamically within classes. The documentation includes functions to create new get/set descriptors, member descriptors, method descriptors, wrapper descriptors, and class method descriptors. It also includes a function to check if a descriptor is a data descriptor or method descriptor, and a function related to wrapper creation. Given these functions, a question could be designed that asks students to implement a class in Python that dynamically creates properties and methods using descriptor objects, allowing the assessment of their comprehension of both basic and advanced concepts such as custom descriptors, properties, and metaprogramming techniques. <|Analysis End|> <|Question Begin|> # Dynamic Descriptor Implementation In this exercise, you are required to demonstrate your understanding of creating and using descriptors in Python by implementing a class that dynamically generates properties and methods. **Objective:** Create a Python class `DynamicDescriptor` that allows dynamic creation of properties and methods using custom descriptors. **Requirements:** 1. Implement a nested descriptor class `DynamicDescriptor.Descriptor` with the following features: - It has a constructor that accepts a getter and setter function. - The getter function should be called whenever the descriptor\'s value is accessed. - The setter function should be called whenever a new value is assigned to the descriptor. 2. The `DynamicDescriptor` class should have the following methods: - `add_property(name: str, getter: callable, setter: callable)`: Adds a new property to the class with the given name, getter, and setter functions. - `add_method(name: str, method: callable)`: Adds a new method to the class with the given name. 3. Ensure that the properties and methods added dynamically are accessible and functional as if they were normally defined attributes and methods of the class. 4. Use Python\'s built-in `type()` function to dynamically add properties and methods, leveraging the descriptor and metaclass mechanisms. **Example:** ```python class DynamicDescriptor: class Descriptor: def __init__(self, getter, setter): self.getter = getter self.setter = setter self.name = None def __get__(self, instance, owner): return self.getter(instance) def __set__(self, instance, value): return self.setter(instance, value) def add_property(self, name, getter, setter): setattr(self.__class__, name, self.Descriptor(getter, setter)) def add_method(self, name, method): setattr(self.__class__, name, method) # Example usage def get_value(instance): return instance._value def set_value(instance, value): instance._value = value def example_method(self): return f\\"Value is {self._value}\\" dynamic_obj = DynamicDescriptor() dynamic_obj.add_property(\'value\', get_value, set_value) dynamic_obj.add_method(\'show_value\', example_method) dynamic_obj._value = 123 print(dynamic_obj.value) # Outputs: 123 dynamic_obj.value = 456 print(dynamic_obj.value) # Outputs: 456 print(dynamic_obj.show_value()) # Outputs: Value is 456 ``` **Constraints:** - Ensure that the descriptors properly handle `getter` and `setter` function calls. - The dynamically added methods should be invokable on an instance of `DynamicDescriptor`. - Proper error handling for invalid inputs is a plus. --- Implement the `DynamicDescriptor` class as described above and demonstrate its functionality with appropriate examples.","solution":"class DynamicDescriptor: class Descriptor: def __init__(self, getter, setter): self.getter = getter self.setter = setter self.name = None def __get__(self, instance, owner): return self.getter(instance) def __set__(self, instance, value): self.setter(instance, value) def add_property(self, name, getter, setter): descriptor = self.Descriptor(getter, setter) setattr(type(self), name, descriptor) def add_method(self, name, method): setattr(type(self), name, method)"},{"question":"# Seaborn Advanced Visualization Challenge **Objective**: Your task is to create a series of subplots using the Seaborn library to compare the distribution of a specific variable across different categories in a dataset, with a focus on customizing the legend positions and aesthetics. **Dataset**: You will use the \\"penguins\\" dataset, which can be loaded directly from Seaborn. **Requirements**: 1. **Dataset Loading and Initial Plotting**: - Load the \\"penguins\\" dataset using `sns.load_dataset(\\"penguins\\")`. - Create a histogram showing the distribution of `bill_length_mm` for each `species` with the species differentiated by hue. 2. **Legend Customization**: - Move the legend to the \\"upper left\\" position and set `bbox_to_anchor` to place it slightly outside the axes, specifically at coordinates (1, 1). 3. **FacetGrid Plot**: - Create a `FacetGrid` to show the distribution of `bill_length_mm` across different islands (`island` column), with separate subplots for each island. - Ensure that the legend for each subplot is positioned inside the plot. 4. **Combined Customization**: - Use `sns.displot` along with `facet_kws` to control the legend positioning inside the subplots, setting the legend to the \\"upper left\\" with specific coordinates (0.55, 0.45). - Customize the legend further by turning off the frame and ensuring the legends are clear and non-overlapping. **Constraints**: - All plots must be created in a single script or notebook cell. - The positions of legends should not overlap with any data points. **Input/Output**: - **Input**: No manual input is required; use the provided dataset and parameters. - **Output**: The script should generate the required plots with properly positioned and styled legends. Here is an example structure for your solution: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Initial Histogram Plot ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax, \\"upper left\\", bbox_to_anchor=(1, 1)) # 2. FacetGrid Plot g = sns.displot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.55, .45), frameon=False) plt.show() ``` Ensure your implementation is clear, comments are added where necessary, and the final plots are visually appealing and informative.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Initial Histogram Plot plt.figure(figsize=(10, 6)) ax = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\", multiple=\\"stack\\") ax.legend(loc=\'upper left\', bbox_to_anchor=(1, 1)) plt.title(\\"Distribution of Bill Length by Species\\") plt.show() # 2. FacetGrid Plot g = sns.displot( penguins, x=\\"bill_length_mm\\", kind=\\"hist\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=4, ) for ax in g.axes.flat: ax.legend(loc=\'upper left\', bbox_to_anchor=(0.55, 0.45), frameon=False) plt.show()"},{"question":"<|Analysis Begin|> The provided documentation outlines several numerical and mathematical modules: 1. **numbers**: Defines an abstract hierarchy of numeric types. 2. **math**: Contains various mathematical functions for floating-point numbers. 3. **cmath**: Contains mathematical functions for complex numbers. 4. **decimal**: Supports exact representations of decimal numbers with arbitrary precision arithmetic. 5. **fractions**: Deals with rational numbers. 6. **random**: Generates pseudo-random numbers and includes various helper functions. 7. **statistics**: Contains mathematical statistics functions. Given the range of topics, a coding question should be designed to test a comprehensive understanding of one of these modules. The \\"decimal\\" module seems to present a good mix of complexity and practical application due to its focus on precise arithmetic and various functionalities. <|Analysis End|> <|Question Begin|> # Decimal Module: Advanced Usage and Function Implementation You are tasked with implementing a function that performs precise arithmetic operations on currency amounts using Python\'s `decimal` module. The `decimal` module provides support for fast fixed-point and floating-point arithmetic with user-definable precision. # Problem Statement Implement a function `currency_operations(operations: List[Tuple[str, str, float]]) -> str` where: - `operations` is a list of tuples. Each tuple contains: - A string representing the operation (\'add\', \'subtract\', \'multiply\', \'divide\'). - A string representing a decimal number as a string (the initial value). - A float number (the value to be added, subtracted, multiplied, or divided). The function should apply each operation to the initial value sequentially. The arithmetic must be performed with a precision of two decimal places, typical for currency calculations. # Input - A list of tuples (`operations`) where each tuple contains: - A string (`operation`): The type of arithmetic operation (\'add\', \'subtract\', \'multiply\', \'divide\'). - A string (`initial_value`): A representation of a decimal number. - A float (`value`): The value to be added, subtracted, multiplied, or divided. # Output - A string representing the final result of the operations performed in sequence, formatted to two decimal places. # Constraints 1. The `initial_value` will be a valid decimal number as a string. 2. The `value` will be a non-negative float for \'add\' and \'multiply\' operations and a positive float for \'subtract\' and \'divide\' operations. 3. The list `operations` will have at least one tuple and no more than 100 tuples. # Example ```python from typing import List, Tuple def currency_operations(operations: List[Tuple[str, str, float]]) -> str: pass # Implementation goes here # Example usage: operations = [ (\'add\', \'100.50\', 20.25), (\'subtract\', \'120.75\', 50.75), (\'multiply\', \'70.00\', 1.05), (\'divide\', \'73.50\', 1.5) ] print(currency_operations(operations)) # Expected result \\"49.00\\" ``` # Explanation 1. Start with `100.50`. 2. After adding `20.25`, the value is `120.75`. 3. After subtracting `50.75`, the value is `70.00`. 4. After multiplying by `1.05`, the value becomes `73.50`. 5. After dividing by `1.5`, the value becomes `49.00`. Your implementation should handle these operations with the appropriate precision using the `decimal` module.","solution":"from typing import List, Tuple from decimal import Decimal, getcontext def currency_operations(operations: List[Tuple[str, str, float]]) -> str: getcontext().prec = 20 getcontext().rounding = \'ROUND_HALF_UP\' result = Decimal(operations[0][1]) # Initialize with the first value provided for operation, _, value in operations: decimal_value = Decimal(str(value)) if operation == \'add\': result += decimal_value elif operation == \'subtract\': result -= decimal_value elif operation == \'multiply\': result *= decimal_value elif operation == \'divide\': result /= decimal_value result = result.quantize(Decimal(\'0.01\')) return str(result)"},{"question":"**Objective:** Design a function to demonstrate comprehension of device management using the `torch.accelerator` module. This function will handle moving tensor operations across different accelerators. **Problem Statement:** You are given a tensor on the CPU, and you need to move it to the currently available acceleration device (like GPU) if there is one. After performing a specified operation on the tensor on the acceleration device, you must move it back to the CPU. Implement the function `process_tensor_on_accelerator(tensor: torch.Tensor, operation: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor` that performs the following steps: 1. Check if any accelerator is available using the `torch.accelerator.is_available` function. 2. If an accelerator is available, move the input tensor to the current accelerator using the appropriate PyTorch functions. 3. Perform the given operation on the tensor. 4. Move the resulting tensor back to the CPU. 5. Return the tensor. # Constraints: - The function should handle situations where no accelerators are available. - The input tensor is guaranteed to be on the CPU initially. - The function should move the tensor back to the CPU regardless of whether an accelerator is available or not. # Expected Input and Output Formats: - **Input:** - `tensor`: A `torch.Tensor` object initially on the CPU. - `operation`: A callable function that takes a tensor and returns a transformed tensor. - **Output:** - A `torch.Tensor` object with the operation applied, moved back to the CPU. **Performance Requirements:** - Ensure minimal overhead for checking and moving devices. - The function should handle tensors of any shape and size efficiently. # Example Usage: ```python import torch # Example operation: doubling the tensor\'s values def double_tensor(tensor: torch.Tensor) -> torch.Tensor: return tensor * 2 # Test tensor input_tensor = torch.tensor([1.0, 2.0, 3.0]) # Function invocation result_tensor = process_tensor_on_accelerator(input_tensor, double_tensor) print(result_tensor) # Expected output: # tensor([2.0, 4.0, 6.0]) ``` # Function Signature: ```python import torch from typing import Callable def process_tensor_on_accelerator(tensor: torch.Tensor, operation: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor: # Your implementation here pass ```","solution":"import torch from typing import Callable def process_tensor_on_accelerator(tensor: torch.Tensor, operation: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor: Moves the tensor to an available accelerator, performs the operation, and moves it back to CPU. Args: tensor (torch.Tensor): The input tensor. operation (Callable[[torch.Tensor], torch.Tensor]): The operation to perform on the tensor. Returns: torch.Tensor: The tensor after operation, moved back to CPU. device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") tensor = tensor.to(device) result_tensor = operation(tensor) return result_tensor.to(\\"cpu\\")"},{"question":"# XML Custom SAX Parsing Objective: Create a custom SAX content handler in Python to parse a given XML document and extract specific information. Problem Statement: You are provided with an XML document containing information about a collection of books. Each book element includes details like title, author, genre, price, and publication date. Your task is to implement a custom SAX content handler that parses this XML document and extracts the titles and authors of all the books, storing them in a dictionary where the keys are the titles, and the values are the corresponding authors. Input: A string containing the XML document. An example format of the XML document is as follows: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> <!-- More book entries --> </catalog> ``` Output: A dictionary where the keys are book titles and the values are authors\' names. For the given example, the output should be: ```python { \\"XML Developer\'s Guide\\": \\"Gambardella, Matthew\\", \\"Midnight Rain\\": \\"Ralls, Kim\\" } ``` Requirements: 1. Subclass the `xml.sax.handler.ContentHandler` to create a custom handler. 2. Implement necessary methods to handle the start and end of elements and character data. 3. Use the SAX parser to parse the provided XML document with your custom handler. Constraints: 1. You cannot use other XML parsing libraries; only SAX parsing as provided by `xml.sax` is allowed. 2. Assume the XML structure is consistent and valid as per the provided example. # Implementation ```python import xml.sax class BookHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.books = {} def startElement(self, tag, attributes): self.current_data = tag def endElement(self, tag): if tag == \\"book\\": self.books[self.title] = self.author self.title = \\"\\" self.author = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title = content elif self.current_data == \\"author\\": self.author = content def parse_books(xml_string): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.parse(xml_string) return handler.books # Test the function with the example XML string xml_string = [Place the XML string here] books = parse_books(xml_string) print(books) # Expected output: {\\"XML Developer\'s Guide\\": \\"Gambardella, Matthew\\", \\"Midnight Rain\\": \\"Ralls, Kim\\"} ```","solution":"import xml.sax class BookHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.books = {} def startElement(self, tag, attributes): self.current_data = tag def endElement(self, tag): if tag == \\"book\\": self.books[self.title] = self.author self.title = \\"\\" self.author = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.title += content elif self.current_data == \\"author\\": self.author += content def parse_books(xml_string): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) return handler.books"},{"question":"# PyTorch Coding Assessment Question **Objective:** Write a Python function that both demonstrates the student\'s understanding of the basic use of `torch.finfo` and `torch.iinfo` and performs operations based on these properties. **Task:** Implement a function `dtype_info_summary()` that takes a PyTorch floating point dtype and an integer dtype as input and returns a dictionary containing the following information: - The number of bits for both the floating point and integer dtypes. - The smallest representable number for the floating point dtype. - The largest representable number for both dtypes. - The smallest normal positive number for the floating point dtype. - The approximate decimal resolution for the floating point dtype. **Function Signature:** ```python import torch def dtype_info_summary(float_dtype: torch.dtype, int_dtype: torch.dtype) -> dict: pass ``` **Input:** - `float_dtype`: a PyTorch floating point dtype (e.g., `torch.float32`, `torch.float64`). - `int_dtype`: a PyTorch integer dtype (e.g., `torch.int8`, `torch.int16`). **Output:** - A dictionary with the following keys: - \'float_bits\': int, the number of bits for the floating point dtype. - \'int_bits\': int, the number of bits for the integer dtype. - \'float_min\': float, the smallest representable number for the floating point dtype. - \'int_max\': int, the largest representable number for the integer dtype. - \'float_max\': float, the largest representable number for the floating point dtype. - \'smallest_normal\': float, the smallest positive normal number for the floating point dtype. - \'resolution\': float, the approximate decimal resolution of the floating point dtype. **Example:** ```python summary = dtype_info_summary(torch.float32, torch.int16) print(summary) # Expected output (actual values may vary): # { # \'float_bits\': 32, # \'int_bits\': 16, # \'float_min\': -3.4028235e+38, # \'int_max\': 32767, # \'float_max\': 3.4028235e+38, # \'smallest_normal\': 1.1754944e-38, # \'resolution\': 1e-07 # } ``` **Constraints:** - You can assume the input dtypes will always be valid PyTorch dtypes. - The function should handle all specified floating point and integer dtypes correctly. **Notes:** - You may find it useful to refer to the `torch.finfo` and `torch.iinfo` classes for retrieving the dtype properties. **Performance Requirements:** - The function should efficiently handle the retrieval and processing of dtype information within the constraints of typical usage scenarios. Ensure your code is properly tested with various dtype combinations to validate its correctness.","solution":"import torch def dtype_info_summary(float_dtype: torch.dtype, int_dtype: torch.dtype) -> dict: float_info = torch.finfo(float_dtype) int_info = torch.iinfo(int_dtype) summary = { \'float_bits\': float_info.bits, \'int_bits\': int_info.bits, \'float_min\': float_info.min, \'int_max\': int_info.max, \'float_max\': float_info.max, \'smallest_normal\': float_info.tiny, \'resolution\': float_info.eps } return summary"},{"question":"Objective Write a Python function to analyze a given directory and report file type statistics and permissions. The function should use the `stat` module to determine file information. Problem Statement You need to implement the function `analyze_directory(directory_path)`. This function takes a string `directory_path` as input, which is the path to a directory. The function should return a dictionary containing the following information: - The total number of files of each type (regular file, directory, symbolic link, etc.). - A list of files along with their permissions in a human-readable format (e.g., `-rwxrwxrwx`). You should utilize the `os` and `stat` modules for file information retrieval and analysis. Input - `directory_path` (string): The path to the directory to be analyzed. Output - A dictionary with the following keys: - `\\"file_type_counts\\"`: A dictionary where keys are file types (`\'regular\'`, `\'directory\'`, `\'symlink\'`, etc.), and values are the count of files of that type. - `\\"file_permissions\\"`: A list of tuples, each containing the file path and its permissions in a human-readable string format (e.g., `(\\"/path/to/file\\", \\"-rw-r--r--\\")`). Constraints - You may assume that the input directory_path is valid and accessible. - The function should handle typical file system errors gracefully (e.g., by skipping unreadable files). Example ```python import os import stat def analyze_directory(directory_path): pass # To be implemented # Assuming the following directory structure: # /test_dir/ #  file1.txt #  file2.txt #  link_to_file -> file1.txt #  subdir/ #  file3.txt # Expected Output: # { # \\"file_type_counts\\": { # \\"regular\\": 3, # \\"directory\\": 2, # Including the root directory itself # \\"symlink\\": 1 # }, # \\"file_permissions\\": [ # (\\"/test_dir/file1.txt\\", \\"-rw-r--r--\\"), # (\\"/test_dir/file2.txt\\", \\"-rw-r--r--\\"), # (\\"/test_dir/link_to_file\\", \\"lrwxrwxrwx\\"), # (\\"/test_dir/subdir\\", \\"drwxr-xr-x\\"), # (\\"/test_dir/subdir/file3.txt\\", \\"-rw-r--r--\\") # ] # } ``` Use the `os` and `stat` modules to complete the implementation. Make sure to handle different file types and fetch permissions correctly.","solution":"import os import stat def analyze_directory(directory_path): Analyze the given directory and report file type statistics and permissions. :param directory_path: Path to the directory to be analyzed. :return: Dictionary containing file type statistics and file permissions. file_type_counts = {\'regular\': 0, \'directory\': 0, \'symlink\': 0, \'other\': 0} file_permissions = [] for root, dirs, files in os.walk(directory_path): # Increment directory count (including root directory) file_type_counts[\'directory\'] += 1 entries = dirs + files for entry in entries: entry_path = os.path.join(root, entry) try: st_mode = os.lstat(entry_path).st_mode file_permissions.append((entry_path, stat.filemode(st_mode))) if stat.S_ISREG(st_mode): file_type_counts[\'regular\'] += 1 elif stat.S_ISDIR(st_mode): # Directory count is incremented earlier pass elif stat.S_ISLNK(st_mode): file_type_counts[\'symlink\'] += 1 else: file_type_counts[\'other\'] += 1 except (FileNotFoundError, PermissionError): # Skip any file we cannot access continue return { \'file_type_counts\': file_type_counts, \'file_permissions\': file_permissions }"},{"question":"# Task You are given a dataset named `data.csv` which contains information about various car models. Your task is to write a Python function that reads this dataset and creates visualizations using the seaborn library to analyze the data. The dataset contains the following columns: - `Make`: Manufacturer of the car. - `Model`: Model name of the car. - `Year`: Year of manufacture. - `Engine HP`: Horsepower of the car\'s engine. - `Engine Cylinders`: Number of cylinders in the car\'s engine. - `Transmission Type`: Type of transmission (Automatic, Manual, etc.). - `Driven Wheels`: Type of drive (FWD, RWD, AWD, etc.). - `Number of Doors`: Number of doors in the car. - `Market Category`: Category of the car in the market (Luxury, Economy, etc.). - `Vehicle Size`: Size category of the vehicle (Compact, Midsize, etc.). - `Vehicle Style`: Style of the vehicle (Sedan, SUV, etc.). - `MSRP`: Manufacturer\'s suggested retail price. # Function Signature ```python def visualize_car_data(csv_file_path: str) -> None: ``` # Input - `csv_file_path`: A string path to the `data.csv` file. # Output - The function should display the following seaborn visualizations: 1. A swarm plot showing the distribution of `Engine HP` across different `Vehicle Size` categories, colored by `Transmission Type`. 2. A swarm plot comparing the `MSRP` of cars across different `Market Category`, with points colored by `Vehicle Style`. 3. A categorical plot showing the distribution of `Number of Doors` for each `Driven Wheels` category, faceted by `Vehicle Style`. # Constraints - Use seaborn\'s `swarmplot` for the first two tasks and `catplot` for the third task. - Ensure the plots are appropriately labeled and easy to interpret. # Example ```python visualize_car_data(\\"path/to/data.csv\\") ``` # Notes - You may assume that the `data.csv` file is always correctly formatted. - For the purpose of this exercise, if there are a lot of unique values, limit your hue categories to a maximum of five for clarity. - Handle any potential edge cases as you see fit, documenting your assumptions.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_car_data(csv_file_path: str) -> None: # Read the dataset df = pd.read_csv(csv_file_path) # Plot 1: Distribution of Engine HP across different Vehicle Size categories, colored by Transmission Type plt.figure(figsize=(10, 6)) sns.swarmplot(x=\'Vehicle Size\', y=\'Engine HP\', hue=\'Transmission Type\', data=df) plt.title(\'Distribution of Engine HP by Vehicle Size and Transmission Type\') plt.xlabel(\'Vehicle Size\') plt.ylabel(\'Engine HP\') plt.legend(title=\'Transmission Type\') plt.show() # Plot 2: MSRP of cars across different Market Categories, colored by Vehicle Style plt.figure(figsize=(14, 8)) sns.swarmplot(x=\'Market Category\', y=\'MSRP\', hue=\'Vehicle Style\', data=df) plt.title(\'MSRP by Market Category and Vehicle Style\') plt.xlabel(\'Market Category\') plt.ylabel(\'MSRP\') plt.legend(title=\'Vehicle Style\') plt.xticks(rotation=45) plt.show() # Plot 3: Distribution of Number of Doors for each Driven Wheels category, faceted by Vehicle Style g = sns.catplot(x=\'Driven Wheels\', hue=\'Number of Doors\', col=\'Vehicle Style\', data=df, kind=\'count\', col_wrap=4) g.set_axis_labels(\'Driven Wheels\', \'Number of Doors\') g.set_titles(\'Vehicle Style: {col_name}\') plt.show()"},{"question":"Objective: You are tasked with designing a Python program that dynamically analyzes another Python script to determine its module dependencies. This challenge involves parsing imports, handling exceptions, and providing a comprehensive report of the loaded and missing modules, similar to what the modulefinder module does. Problem Statement: Write a Python function `analyze_script(filepath: str, exclude_list: list = None, replace_paths: list = None) -> dict` that takes the path to a Python script and optionally a list of modules to exclude from the analysis and a list of paths to replace in module paths. Your function should analyze the script and return a dictionary with two keys: `loaded_modules` and `missing_modules`. - `loaded_modules` should map module names to a list of up to three global names from that module. - `missing_modules` should be a list of module names that the script tried to import but could not. Input: - `filepath` (str): The path to the Python script to be analyzed. - `exclude_list` (list): An optional list of module names to exclude from the analysis. - `replace_paths` (list): An optional list of tuples where each tuple has an old path and a new path to be replaced in module paths. Output: - A dictionary with two keys: - `loaded_modules`: A dictionary where keys are module names and values are lists of up to three global names within each module. - `missing_modules`: A list of module names that were not imported successfully. Constraints: - The script should handle exceptions such as file not found, invalid file format, etc. - The solution should be efficient in processing even large scripts with many imports. - You may not use the `modulefinder` module directly in your implementation. However, you can draw inspiration from its design. Example Usage: ```python # Consider a sample Python file \\"sample_script.py\\" with the content: # import os, sys # try: # import nonexistentmodule # except ImportError: # pass # try: # import another.missing.module # except ImportError: # pass result = analyze_script(\'sample_script.py\') print(result) # Expected output: # { # \\"loaded_modules\\": { # \\"os\\": [...up to three global names...], # \\"sys\\": [...up to three global names...] # }, # \\"missing_modules\\": [ # \\"nonexistentmodule\\", # \\"another.missing.module\\" # ] # } ``` Hints: 1. Use the `ast` module to parse the Python script and identify import statements. 2. Handle relative imports and module exclusions as specified in the function parameters. 3. Consider the debugging level to log relevant information about the imported modules and any issues encountered. Implement the function `analyze_script(filepath: str, exclude_list: list = None, replace_paths: list = None) -> dict` to meet the above requirements.","solution":"import ast import importlib.util import os def analyze_script(filepath: str, exclude_list: list = None, replace_paths: list = None) -> dict: exclude_list = exclude_list or [] replace_paths = replace_paths or [] def module_exists(module_name): Check if a module can be imported without actually importing it. spec = importlib.util.find_spec(module_name) return spec is not None def replace_module_path(module_name): Replace the part of the module path according to replace_paths list. for old_path, new_path in replace_paths: if module_name.startswith(old_path): return module_name.replace(old_path, new_path, 1) return module_name result = {\\"loaded_modules\\": {}, \\"missing_modules\\": []} with open(filepath, \\"r\\") as file: tree = ast.parse(file.read(), filename=filepath) for node in ast.walk(tree): if isinstance(node, (ast.Import, ast.ImportFrom)): for alias in node.names: module_name = alias.name.split(\'.\')[0] # Apply path replacements module_name = replace_module_path(module_name) if module_name in exclude_list: continue if not module_exists(module_name): result[\\"missing_modules\\"].append(module_name) else: if module_name not in result[\\"loaded_modules\\"]: result[\\"loaded_modules\\"][module_name] = [] if len(result[\\"loaded_modules\\"][module_name]) < 3: result[\\"loaded_modules\\"][module_name].append(alias.asname or alias.name) return result"},{"question":"**Problem Statement:** You are tasked with writing a function that will perform a detailed analysis of the files within a specified directory. This function will generate a report that includes the type of each file, its permissions, and other metadata attributes. Function Signature ```python def analyze_directory(directory: str) -> List[Dict[str, Union[str, int]]]: pass ``` Input - `directory`: A string representing the path to the directory to be analyzed. Output - Returns a list of dictionaries where each dictionary represents the analysis of one file. Each dictionary should contain the following keys and values: - `\\"name\\"`: The name of the file (string). - `\\"type\\"`: The type of the file - one of \\"directory\\", \\"file\\", \\"character special device file\\", \\"block special device file\\", \\"FIFO\\", \\"symbolic link\\", \\"socket\\", \\"door\\", \\"event port\\", or \\"whiteout\\" (string). - `\\"permissions\\"`: File permissions in the form of a string, e.g., `-rwxr-xr--` (string). - `\\"size\\"`: Size of the file in bytes (int). - `\\"user_id\\"`: User ID of the owner (int). - `\\"group_id\\"`: Group ID of the owner (int). - `\\"last_accessed\\"`: Time of last access (int, epoch timestamp). - `\\"last_modified\\"`: Time of last modification (int, epoch timestamp). - `\\"last_metadata_change\\"`: Time of last metadata change or creation time, depending on the system (int, epoch timestamp). Constraints - You may assume that the directory path exists and is accessible. - The solution should handle large directories efficiently. Example Usage: ```python result = analyze_directory(\\"/path/to/directory\\") for file_info in result: print(file_info) ``` # Requirements - Use the `os` and `stat` modules to gather information about each file. - Properly handle different types of files and their specific attributes. - Ensure that your report includes human-readable permission strings using `stat.filemode`. # Additional Information: - You can use `os.listdir()` to list files in the directory. - Use `os.lstat()` for retrieving file status that can differentiate between a symbolic link and the file it points to. Implement the `analyze_directory` function below: ```python import os import stat from typing import List, Dict, Union def analyze_directory(directory: str) -> List[Dict[str, Union[str, int]]]: result = [] for filename in os.listdir(directory): filepath = os.path.join(directory, filename) file_stat = os.lstat(filepath) file_info = { \\"name\\": filename, \\"type\\": \\"\\", \\"permissions\\": stat.filemode(file_stat.st_mode), \\"size\\": file_stat.st_size, \\"user_id\\": file_stat.st_uid, \\"group_id\\": file_stat.st_gid, \\"last_accessed\\": file_stat.st_atime, \\"last_modified\\": file_stat.st_mtime, \\"last_metadata_change\\": file_stat.st_ctime } mode = file_stat.st_mode if stat.S_ISDIR(mode): file_info[\\"type\\"] = \\"directory\\" elif stat.S_ISREG(mode): file_info[\\"type\\"] = \\"file\\" elif stat.S_ISCHR(mode): file_info[\\"type\\"] = \\"character special device file\\" elif stat.S_ISBLK(mode): file_info[\\"type\\"] = \\"block special device file\\" elif stat.S_ISFIFO(mode): file_info[\\"type\\"] = \\"FIFO\\" elif stat.S_ISLNK(mode): file_info[\\"type\\"] = \\"symbolic link\\" elif stat.S_ISSOCK(mode): file_info[\\"type\\"] = \\"socket\\" elif hasattr(stat, \'S_ISDOOR\') and stat.S_ISDOOR(mode): file_info[\\"type\\"] = \\"door\\" elif hasattr(stat, \'S_ISPORT\') and stat.S_ISPORT(mode): file_info[\\"type\\"] = \\"event port\\" elif hasattr(stat, \'S_ISWHT\') and stat.S_ISWHT(mode): file_info[\\"type\\"] = \\"whiteout\\" else: file_info[\\"type\\"] = \\"unknown\\" result.append(file_info) return result ```","solution":"import os import stat from typing import List, Dict, Union def analyze_directory(directory: str) -> List[Dict[str, Union[str, int]]]: result = [] for filename in os.listdir(directory): filepath = os.path.join(directory, filename) file_stat = os.lstat(filepath) file_info = { \\"name\\": filename, \\"type\\": \\"\\", \\"permissions\\": stat.filemode(file_stat.st_mode), \\"size\\": file_stat.st_size, \\"user_id\\": file_stat.st_uid, \\"group_id\\": file_stat.st_gid, \\"last_accessed\\": file_stat.st_atime, \\"last_modified\\": file_stat.st_mtime, \\"last_metadata_change\\": file_stat.st_ctime } mode = file_stat.st_mode if stat.S_ISDIR(mode): file_info[\\"type\\"] = \\"directory\\" elif stat.S_ISREG(mode): file_info[\\"type\\"] = \\"file\\" elif stat.S_ISCHR(mode): file_info[\\"type\\"] = \\"character special device file\\" elif stat.S_ISBLK(mode): file_info[\\"type\\"] = \\"block special device file\\" elif stat.S_ISFIFO(mode): file_info[\\"type\\"] = \\"FIFO\\" elif stat.S_ISLNK(mode): file_info[\\"type\\"] = \\"symbolic link\\" elif stat.S_ISSOCK(mode): file_info[\\"type\\"] = \\"socket\\" elif hasattr(stat, \'S_ISDOOR\') and stat.S_ISDOOR(mode): file_info[\\"type\\"] = \\"door\\" elif hasattr(stat, \'S_ISPORT\') and stat.S_ISPORT(mode): file_info[\\"type\\"] = \\"event port\\" elif hasattr(stat, \'S_ISWHT\') and stat.S_ISWHT(mode): file_info[\\"type\\"] = \\"whiteout\\" else: file_info[\\"type\\"] = \\"unknown\\" result.append(file_info) return result"},{"question":"You are tasked with processing an XML document that logs product sales data. Each product has a `name`, `price`, `quantity`, and `category`. The goal is to output the names of products that belong to a certain category and have been sold in quantities higher than a specified threshold. The XML structure is as follows: ```xml <sales> <product> <name>Product A</name> <price>100</price> <quantity>10</quantity> <category>Electronics</category> </product> <product> <name>Product B</name> <price>50</price> <quantity>5</quantity> <category>Home</category> </product> <!-- Additional products --> </sales> ``` Implement a function `process_sales(xml_str: str, target_category: str, threshold: int) -> List[str]` that: - Takes an XML string `xml_str`, a `target_category`, and a `threshold` as inputs. - Uses the `xml.dom.pulldom` module to parse the XML. - Returns a list of product names that belong to the `target_category` and have quantities greater than the `threshold`. # Input - `xml_str`: A string representing the XML data. - `target_category`: A string representing the target product category. - `threshold`: An integer threshold for product quantity. # Output - A list of strings, each representing the name of a product that matches the criteria. # Example ```python xml_input = \'\'\' <sales> <product> <name>Product A</name> <price>100</price> <quantity>10</quantity> <category>Electronics</category> </product> <product> <name>Product B</name> <price>50</price> <quantity>5</quantity> <category>Home</category> </product> <product> <name>Product C</name> <price>250</price> <quantity>15</quantity> <category>Electronics</category> </product> </sales> \'\'\' print(process_sales(xml_input, \\"Electronics\\", 5)) # Output: [\'Product A\', \'Product C\'] ``` # Constraints - Assume the XML string is well-formed. - The function should be efficient in both time and space complexity. # Implementation Tip - Use the `xml.dom.pulldom.parseString` function to parse the XML string. - Use the `doc.expandNode(node)` method to expand nodes when necessary.","solution":"from typing import List from xml.dom import pulldom def process_sales(xml_str: str, target_category: str, threshold: int) -> List[str]: Processes the sales XML data to find products that belong to a certain category and have been sold in quantities higher than a specified threshold. Args: - xml_str (str): The XML data as a string. - target_category (str): The target product category to filter by. - threshold (int): The quantity threshold to filter by. Returns: - List[str]: A list of product names that match the criteria. products = [] events = pulldom.parseString(xml_str) for event, node in events: if event == pulldom.START_ELEMENT and node.tagName == \\"product\\": events.expandNode(node) name = node.getElementsByTagName(\\"name\\")[0].firstChild.nodeValue quantity = int(node.getElementsByTagName(\\"quantity\\")[0].firstChild.nodeValue) category = node.getElementsByTagName(\\"category\\")[0].firstChild.nodeValue if category == target_category and quantity > threshold: products.append(name) return products"},{"question":"**Objective:** Demonstrate your understanding of asyncio synchronization primitives, specifically Lock and Semaphore, by implementing a function that regulates access to a shared resource among multiple tasks. # Problem Statement You are given a shared resource that multiple asyncio tasks need to access simultaneously. However, to ensure data integrity, only one task at a time should be able to modify the resource, and a maximum of `n` tasks can read the resource concurrently. Implement two functions: 1. `exclusive_write(resource, lock)`: An asynchronous function that acquires exclusive access to a shared resource using an asyncio `Lock`. 2. `shared_read(resource, semaphore)`: An asynchronous function that allows reading the shared resource with a limit on the number of concurrent tasks using an asyncio `Semaphore`. # Function Specifications Function 1: `exclusive_write` - **Arguments:** - `resource`: Any mutable data structure representing the shared resource. - `lock`: An instance of `asyncio.Lock` used to manage exclusive access. - **Behavior:** - The function should acquire the lock, modify the resource, and then release the lock. - **Example Usage:** ```python async def exclusive_write(resource, lock): async with lock: # Simulate modification resource[\'data\'] += 1 ``` Function 2: `shared_read` - **Arguments:** - `resource`: Any mutable or immutable data structure representing the shared resource. - `semaphore`: An instance of `asyncio.Semaphore` used to limit concurrent access. - **Behavior:** - The function should acquire the semaphore, read the resource, and then release the semaphore. - **Example Usage:** ```python async def shared_read(resource, semaphore): async with semaphore: # Simulate reading value = resource[\'data\'] return value ``` # Input and Output - **Input:** - A shared resource (e.g., a dictionary). - An instance of `asyncio.Lock`. - An instance of `asyncio.Semaphore`. - **Output:** - For `exclusive_write`, there is no direct output. The shared resource should be modified. - For `shared_read`, the function returns the read value from the resource. # Constraints - You must use the `async with` statement to manage the Lock and Semaphore. - The Semaphore should ensure that no more than `n` tasks can read the resource concurrently. - The Lock should ensure that only one task can modify the resource at a time. # Example Scenario Consider the following scenario where a shared dictionary `shared_resource` is accessed by multiple tasks: ```python import asyncio async def main(): shared_resource = {\'data\': 0} lock = asyncio.Lock() semaphore = asyncio.Semaphore(3) async def writer(): await exclusive_write(shared_resource, lock) async def reader(): return await shared_read(shared_resource, semaphore) # Create tasks for writing and reading tasks = [writer() for _ in range(5)] + [reader() for _ in range(5)] await asyncio.gather(*tasks) asyncio.run(main()) ``` In this scenario, tasks will write to and read from the `shared_resource` ensuring data integrity and respecting concurrency limits.","solution":"import asyncio async def exclusive_write(resource, lock): Acquires exclusive access to the shared resource and modifies it. async with lock: # Simulate modification resource[\'data\'] += 1 async def shared_read(resource, semaphore): Acquires a semaphore to read the shared resource with limited concurrency. async with semaphore: # Simulate reading value = resource[\'data\'] return value"},{"question":"# Platform Information Report You are required to create a Python function to compile a detailed report of the platform and Python interpreter information using the `platform` module. Implement the function `platform_report()` which should collect and display the following information: 1. **OS Information**: - System: The name of the OS (e.g., \'Windows\', \'Linux\') - Release: The OS release version - Version: The OS version - Machine: The machine type (e.g., \'AMD64\') - Processor: The processor name 2. **Python Interpreter Information**: - Python Version: The version of the Python interpreter (e.g., \'3.10.0\') - Python Compiler: The compiler used to compile the Python interpreter - Python Build: The build number and date of the Python interpreter - Python Implementation: The Python implementation being used (e.g., \'CPython\') 3. **Network Information**: - Node: The computer\'s network name Your implementation should use appropriate functions from the `platform` module to retrieve this information and return it as a formatted string. Function Signature ```python def platform_report() -> str: pass ``` Example Output ```python print(platform_report()) ``` Example output should be similar to: ``` OS Information: System: Linux Release: 5.10.0-8-amd64 Version: #1 SMP Debian 5.10.46-4 (2021-08-03) Machine: x86_64 Processor: x86_64 Python Interpreter Information: Python Version: 3.10.0 Python Compiler: GCC 10.2.1 20210110 Python Build: (\'default\', \'Oct 4 2021 14:08:46\') Python Implementation: CPython Network Information: Node: my-computer-name ``` Constraints - Ensure compatibility with multiple operating systems (Windows, Linux, macOS). - Handle cases where any piece of information is not available by displaying \'Unknown\'. Use this as an opportunity to demonstrate your comprehension of the `platform` module and your ability to synthesize and format information in a user-friendly manner.","solution":"import platform def platform_report() -> str: Collects and displays detailed information about the platform and Python interpreter. report = [] # OS Information report.append(\\"OS Information:\\") report.append(f\\" System: {platform.system()}\\") report.append(f\\" Release: {platform.release()}\\") report.append(f\\" Version: {platform.version()}\\") report.append(f\\" Machine: {platform.machine()}\\") report.append(f\\" Processor: {platform.processor()}\\") # Python Interpreter Information report.append(\\"nPython Interpreter Information:\\") report.append(f\\" Python Version: {platform.python_version()}\\") report.append(f\\" Python Compiler: {platform.python_compiler()}\\") report.append(f\\" Python Build: {platform.python_build()}\\") report.append(f\\" Python Implementation: {platform.python_implementation()}\\") # Network Information report.append(\\"nNetwork Information:\\") report.append(f\\" Node: {platform.node()}\\") return \'n\'.join(report)"},{"question":"Objective Implement a function that generates prime numbers using an iterator and performs various operations on these primes utilizing functional programming concepts. Problem Statement Design a class `PrimeGenerator`: 1. `__init__(self, n)`: Initialize the generator to produce the first `n` prime numbers. 2. `__iter__(self)`: Return an iterator that yields prime numbers. 3. `__next__(self)`: Using Python\'s generator capabilities, implement the functionality to return the next prime number. 4. `sum_of_primes`: This function should return the sum of the first `n` prime numbers using the functionalities provided by itertools. 5. `filter_primes(func)`: This function should return a list of primes that match a given predicate `func` using Python\'s `filter` built-in function. # Input Format - The class is initialized with an integer `n` which is the number of primes to generate. - The `filter_primes` method takes a single argument, `func`, a function which determines whether a prime number should be included in the output list. # Function Signature ```python class PrimeGenerator: def __init__(self, n): pass def __iter__(self): pass def __next__(self): pass def sum_of_primes(self): pass def filter_primes(self, func): pass ``` # Constraints - `1 <= n <= 10^4` # Example ```python # Initialize the PrimeGenerator to generate the first 10 primes primes = PrimeGenerator(10) # Using iteration to print the primes for prime in primes: print(prime) # Get the sum of the first 10 primes print(primes.sum_of_primes()) # Output should be 129 # Filter primes that are less than 20 print(primes.filter_primes(lambda x: x < 20)) # Output should be [2, 3, 5, 7, 11, 13, 17, 19] ``` Implement the `PrimeGenerator` class such that it meets the specifications above. Your implementation should make use of functional programming constructs like iterators, generators, and higher-order functions where appropriate.","solution":"import itertools class PrimeGenerator: def __init__(self, n): self.n = n self.primes = self.generate_primes() self._iterator = iter(self.primes) def generate_primes(self): primes = [] num = 2 while len(primes) < self.n: if all(num % prime != 0 for prime in primes): primes.append(num) num += 1 return primes def __iter__(self): self._iterator = iter(self.primes) return self def __next__(self): return next(self._iterator) def sum_of_primes(self): return sum(self.primes) def filter_primes(self, func): return list(filter(func, self.primes))"},{"question":"<|Analysis Begin|> The provided documentation details various handlers in the `logging.handlers` module in Python\'s standard library. Here is a breakdown of the key classes mentioned: 1. **StreamHandler**: Sends logging output to streams like `sys.stdout` or `sys.stderr`. 2. **FileHandler**: Sends logging output to a disk file. 3. **NullHandler**: A \'no-op\' handler that does no output. 4. **WatchedFileHandler**: A `FileHandler` that watches the file for changes and re-opens it if necessary. 5. **BaseRotatingHandler**: The base class for handlers that rotate log files. 6. **RotatingFileHandler**: Supports rotation of disk log files according to file size. 7. **TimedRotatingFileHandler**: Supports rotation of disk log files at timed intervals. 8. **SocketHandler**: Sends logging output to a network socket. 9. **DatagramHandler**: Inherits from `SocketHandler` for sending logging messages over UDP sockets. 10. **SysLogHandler**: Sends logging messages to a Unix syslog. 11. **NTEventLogHandler**: Sends logging messages to a Windows NT/2000/XP event log. 12. **SMTPHandler**: Sends logging messages to an email address via SMTP. 13. **MemoryHandler**: Buffers records in memory and flushes them periodically to a target handler. 14. **HTTPHandler**: Sends logging messages to a web server using HTTP GET or POST requests. 15. **QueueHandler**: Sends logging messages to a queue. 16. **QueueListener**: Receives logging messages from a queue and passes them to one or more handlers. Each of these handler classes includes several methods for initializing and handling log records, emitting them, and sometimes rotating or flushing them based on specific criteria. From this vast amount of available logging handlers, it is apparent that a good question could be centered around designing a custom handler or creating an enhanced version of one of the existing handlers to demonstrate the student\'s understanding of logging concepts and module usage. <|Analysis End|> <|Question Begin|> **Enhanced RotatingLoggingHandler Implementation** Your task is to create a custom logging handler by extending the predefined `RotatingFileHandler` from the `logging.handlers` module. This custom handler, `EnhancedRotatingFileHandler`, should add the following functionality: 1. **Compress Logs**: Compressed backup logs when the rotation occurs using gzip. 2. **Custom Rotation Naming**: Implement a custom naming convention for rotated logs, such as appending a timestamp instead of a numeric suffix. 3. **Maximum Active Log Files**: Introduce a `maxActiveFiles` parameter which restricts the number of log files that can be active simultaneously. If this limit is exceeded, the oldest log file should be deleted. Below is the expected class definition and method signatures for this functionality: ```python import logging import logging.handlers import os import gzip from datetime import datetime class EnhancedRotatingFileHandler(logging.handlers.RotatingFileHandler): def __init__(self, filename, mode=\'a\', maxBytes=0, backupCount=0, maxActiveFiles=5, encoding=None, delay=False, errors=None): super().__init__(filename, mode, maxBytes, backupCount, encoding, delay, errors) self.maxActiveFiles = maxActiveFiles def doRollover(self): # Custom rollover logic, including compressing old log files and applying custom naming scheme for backups pass def _compress_log_file(self, source_path): # Compress the specified log file pass def _apply_custom_naming_scheme(self, source_path, dest_path): # Apply custom naming scheme to the new rollover log file pass def _delete_oldest_file_if_exceeds_limit(self): # Delete the oldest file if the maxActiveFiles limit is exceeded pass ``` # Implementation Requirements: 1. **File Compression**: - Implement `_compress_log_file(source_path)` method to compress the file at the specified path using gzip. - Ensure that file compression happens after the original file is closed but before the new log file is created. 2. **Custom Naming Scheme**: - Implement `_apply_custom_naming_scheme(source_path, dest_path)` to rename the rollover log files. - The naming scheme should append the current timestamp to the log file name. 3. **Handle Max Active Files**: - Implement `_delete_oldest_file_if_exceeds_limit` to delete the oldest log file if the number of active log files exceeds `maxActiveFiles`. # Example Usage: ```python logger = logging.getLogger(\'customLogger\') handler = EnhancedRotatingFileHandler(filename=\'app.log\', maxBytes=10000, backupCount=5, maxActiveFiles=5) logger.addHandler(handler) for i in range(10000): logger.debug(f\'Debug message {i}\') ``` Make sure to handle edge cases, such as the absence of old log files and the conditions under which no rotation or compression is required. Document your code appropriately and ensure that your custom handler behaves as expected when tested with a sample logging setup.","solution":"import logging import logging.handlers import os import gzip from datetime import datetime class EnhancedRotatingFileHandler(logging.handlers.RotatingFileHandler): def __init__(self, filename, mode=\'a\', maxBytes=0, backupCount=0, maxActiveFiles=5, encoding=None, delay=False, errors=None): super().__init__(filename, mode, maxBytes, backupCount, encoding, delay, errors) self.maxActiveFiles = maxActiveFiles def doRollover(self): Perform a rollover: rotate the current log file, compress it, apply custom naming, and ensure the number of active log files doesn\'t exceed maxActiveFiles. if self.stream: self.stream.close() self.stream = None current_time = datetime.now().strftime(\\"%Y-%m-%d_%H-%M-%S\\") dfn = \\"{}.{}\\".format(self.baseFilename, current_time) if os.path.exists(self.baseFilename): self._apply_custom_naming_scheme(self.baseFilename, dfn) self._compress_log_file(dfn) if not self.delay: self.stream = self._open() self._delete_oldest_file_if_exceeds_limit() def _compress_log_file(self, source_path): Compress the specified log file. with open(source_path, \'rb\') as f_in: with gzip.open(f\\"{source_path}.gz\\", \'wb\') as f_out: f_out.writelines(f_in) os.remove(source_path) def _apply_custom_naming_scheme(self, source_path, dest_path): Apply custom naming scheme to the new rollover log file. os.rename(source_path, dest_path) def _delete_oldest_file_if_exceeds_limit(self): Delete the oldest file if the maxActiveFiles limit is exceeded. log_files = [f for f in os.listdir(\'.\') if f.startswith(self.baseFilename) and f.endswith(\'.gz\')] log_files.sort() while len(log_files) > self.maxActiveFiles: oldest_file = log_files.pop(0) os.remove(oldest_file)"},{"question":"Thread Synchronization with Locks You are tasked with writing a Python function using the _thread module to simulate a concurrent counter. Multiple threads will increment a shared counter variable, and we need to ensure synchronization to avoid race conditions. Problem Statement Write a function `threaded_counter(n_threads, n_iterations)` which starts `n_threads` threads. Each thread increments a shared counter variable `n_iterations` times. Use locks to ensure that the counter is incremented safely, without race conditions. **Function Signature:** ```python def threaded_counter(n_threads: int, n_iterations: int) -> int: ``` **Parameters:** - `n_threads`: An integer representing the number of threads to be created. - `n_iterations`: An integer representing the number of times each thread should increment the shared counter variable. **Returns:** - The final value of the counter after all threads have finished their execution. Constraints - You must use the `_thread` and `time` modules. - Ensure proper use of locks to avoid race conditions. - Target a performance-efficient solution. Example ```python # Example usage final_count = threaded_counter(10, 1000) print(final_count) # Expected: 10000 ``` Hints 1. Use `_thread.allocate_lock()` to create a lock object. 2. Use `lock.acquire()` and `lock.release()` methods to manage access to the counter variable. 3. Use `_thread.start_new_thread()` to start a new thread. Caveats - Ensure that all threads complete their execution before returning the final counter value. - Avoid busy-waiting and other inefficient synchronization methods.","solution":"import _thread import time def threaded_counter(n_threads: int, n_iterations: int) -> int: counter = 0 counter_lock = _thread.allocate_lock() def increment_counter(): nonlocal counter for _ in range(n_iterations): with counter_lock: counter += 1 threads = [] for _ in range(n_threads): thread = _thread.start_new_thread(increment_counter, ()) threads.append(thread) # Give threads time to finish time.sleep(1) return counter"},{"question":"Coding Assessment Question # Objective Design an interactive Python interpreter that limits the user\'s command execution to a specific set of functions and variables. The interpreter should provide a custom prompt and handle incomplete commands correctly using the `code` module. # Description You are required to create a custom interactive Python interpreter that restricts the user to only execute a predefined set of functions and variables. This interpreter should behave similarly to the standard Python REPL, but with added constraints and features as specified below. # Requirements 1. **Allowed Functions and Variables**: The interpreter should only allow the execution of specific functions and variables defined in the `allowed_namespace` dictionary. Any attempt to use functions or variables outside of this dictionary should result in an `UnauthorizedAccessError`. 2. **Custom Prompt**: The interpreter should use a custom prompt string defined by `PROMPT1` for the primary prompt (equivalent to `sys.ps1`) and `PROMPT2` for the secondary prompt (equivalent to `sys.ps2`). 3. **Handling Incomplete Commands**: The interpreter should handle incomplete commands by continuing to prompt the user for more input until the command is complete, similar to the standard Python REPL behavior. 4. **Exit Command**: The interpreter should exit gracefully when the user enters the `exit` command. # Implementation Steps 1. Define the `allowed_namespace` dictionary with specific functions and variables. 2. Create a custom error class `UnauthorizedAccessError`. 3. Subclass `code.InteractiveConsole` to create `RestrictedInteractiveConsole`. 4. Override necessary methods to enforce the allowed namespace and custom prompts. 5. Implement the `interact` method with the custom prompts and exit command handling. # Provided Boilerplate Code ```python import code # Step 1: Define the allowed namespace allowed_namespace = { \'print\': print, \'allowed_var\': 42 # Add other allowed functions and variables here } # Step 2: Define custom error class class UnauthorizedAccessError(Exception): pass # Step 3: Subclass InteractiveConsole class RestrictedInteractiveConsole(code.InteractiveConsole): PROMPT1 = \'>>> \' PROMPT2 = \'... \' def __init__(self, locals=None): super().__init__(locals=locals) def runcode(self, code): # Restrict execution to the allowed namespace for name in code.co_names: if name not in allowed_namespace: raise UnauthorizedAccessError(f\\"Unauthorized access to {name}\\") super().runcode(code) def raw_input(self, prompt=\'\'): return input(prompt) def showtraceback(self): print(\\"An error occurred during the execution of your command.\\") def showsyntaxerror(self, filename=None): print(\\"Syntax error in your command.\\") # Step 5: Implement the interactive loop def restricted_interact(): console = RestrictedInteractiveConsole(locals=allowed_namespace) console.interact(banner=\\"Restricted Python Interpreter\\", exitmsg=\\"Exiting the interpreter.\\") # Entrypoint if __name__ == \\"__main__\\": restricted_interact() ``` # Constraints - The namespace restrictions should be strictly enforced. - The custom prompts should be used throughout the interaction. - Proper error handling and messaging must be implemented for unauthorized access and command execution errors. # Example Interaction ```shell Restricted Python Interpreter >>> print(\'Hello, World!\') Hello, World! >>> allowed_var 42 >>> exit Exiting the interpreter. >>> exec(\'import os\') UnauthorizedAccessError: Unauthorized access to os ```","solution":"import code # Step 1: Define the allowed namespace allowed_namespace = { \'print\': print, \'allowed_var\': 42 # Add other allowed functions and variables here } # Step 2: Define custom error class class UnauthorizedAccessError(Exception): pass # Step 3: Subclass InteractiveConsole class RestrictedInteractiveConsole(code.InteractiveConsole): PROMPT1 = \'>>> \' PROMPT2 = \'... \' def __init__(self, locals=None): super().__init__(locals=locals) def runcode(self, code_obj): # Restrict execution to the allowed namespace for name in code_obj.co_names: if name not in self.locals: raise UnauthorizedAccessError(f\\"Unauthorized access to {name}\\") exec(code_obj, self.locals) def raw_input(self, prompt=\'\'): return input(prompt) def showtraceback(self): print(\\"An error occurred during the execution of your command.\\") def showsyntaxerror(self, filename=None): print(\\"Syntax error in your command.\\") # Step 5: Implement the interactive loop def restricted_interact(): console = RestrictedInteractiveConsole(locals=allowed_namespace) console.interact(banner=\\"Restricted Python Interpreter\\", exitmsg=\\"Exiting the interpreter.\\") # Entrypoint if __name__ == \\"__main__\\": restricted_interact()"},{"question":"# Email Message Manipulation Problem Statement You are required to create an email message that composes multiple parts, including plain text and an attachment. Your task is to implement a function `create_email_with_attachment` that does the following: 1. **Create the Email Message with Subject and Content**: - Header: \'Subject\' with the given subject. - Content: a plain text specified in `plain_text_content`. 2. **Add an Attachment**: - Add a file attachment to the email with the given filename, content type, and file content. 3. **Serialize the Email**: - Serialize the entire email message into a bytes object and return it. Function Signature ```python def create_email_with_attachment(subject: str, plain_text_content: str, filename: str, file_content: bytes, content_type: str) -> bytes: pass ``` Input Parameters - `subject`: A string representing the subject of the email. - `plain_text_content`: A string representing the plain text content of the email body. - `filename`: A string representing the name of the attachment file. - `file_content`: A bytes object representing the content of the attachment file. - `content_type`: A string representing the MIME content type of the attachment file (e.g., \'application/pdf\'). Output - Return the serialized email message as a bytes object. Constraints - All input strings will be non-empty and no longer than 255 characters. - The `file_content` will be a non-empty bytes object of size at most 10MB. Additional Requirements - Ensure that the email is MIME compliant. - The attachment should be labeled as \'inline\' in the `Content-Disposition` header. Example ```python subject = \\"Meeting Notes\\" plain_text_content = \\"Please find the meeting notes attached.\\" filename = \\"notes.pdf\\" file_content = b\\"%PDF-1.4...\\" content_type = \\"application/pdf\\" email_bytes = create_email_with_attachment(subject, plain_text_content, filename, file_content, content_type) ``` Notes - You can use the `email.message.EmailMessage` class from the `email.message` module. - Utilize appropriate methods to add headers, set content, and manage attachments. - Use the `as_bytes()` method to serialize the email message as a bytes object.","solution":"from email.message import EmailMessage def create_email_with_attachment(subject: str, plain_text_content: str, filename: str, file_content: bytes, content_type: str) -> bytes: Create an email with a subject, plain text content, and an attachment. Args: - subject (str): The subject of the email. - plain_text_content (str): The plain text content of the email body. - filename (str): The name of the attached file. - file_content (bytes): The content of the attached file. - content_type (str): The MIME content type of the attached file (e.g., \'application/pdf\'). Returns: - bytes: The serialized email message. # Create EmailMessage object msg = EmailMessage() # Set the subject msg[\'Subject\'] = subject # Set the plain text content msg.set_content(plain_text_content) # Add the attachment msg.add_attachment(file_content, maintype=content_type.split(\'/\')[0], subtype=content_type.split(\'/\')[1], filename=filename, disposition=\\"inline\\") # Serialize the message to bytes return msg.as_bytes()"},{"question":"**Coding Assessment Question:** # Objective: Implement a set of functions to simulate a task scheduling system using `asyncio.Queue` and `asyncio.PriorityQueue`. Your solution should demonstrate an understanding of asynchronous programming, queue management, and task prioritization. # Problem Description: You need to implement a task scheduler that manages and executes tasks based on their priority. Given a set of tasks with associated priorities, the scheduler should process high-priority tasks before lower-priority ones, while also handling task completion and queue management effectively using `asyncio` constructs. # Requirements: 1. **Function `add_task(priority, task_duration)`**: - Add a task to the priority queue with a given priority and duration. - Inputs: - `priority` (int): The priority level of the task (lower number indicates higher priority). - `task_duration` (float): The duration the task will take to complete in seconds. - Output: None. 2. **Function `scheduler(num_workers)`**: - Execute tasks from the priority queue using a specified number of worker coroutines. - Inputs: - `num_workers` (int): The number of worker coroutines to process tasks concurrently. - Output: A string indicating the total task processing time in seconds. # Constraints: - The priority queue should follow FIFO ordering for tasks with the same priority. - You may assume the input tasks are well-formed and there are no duplicates. - The `num_workers` parameter will always be a positive integer. # Example: ```python import asyncio # Assuming these functions are methods within a class TaskScheduler: scheduler = TaskScheduler() # Adding tasks with varying priorities and durations await scheduler.add_task(1, 2.0) # High priority task (priority 1) await scheduler.add_task(3, 1.0) # Low priority task (priority 3) await scheduler.add_task(2, 4.0) # Medium priority task (priority 2) await scheduler.add_task(1, 1.5) # Another high priority task (priority 1) # Running the scheduler with 2 workers result = await scheduler.scheduler(2) print(result) # Example: \\"Total task processing time: 4.0 seconds\\" ``` # Guidelines: - You may use the `asyncio.PriorityQueue` and `asyncio.Queue` classes as needed. - Make sure to use proper exception handling for queue operations. - Document your functions with appropriate comments.","solution":"import asyncio import time class TaskScheduler: def __init__(self): self.priority_queue = asyncio.PriorityQueue() async def add_task(self, priority, task_duration): Add a task to the priority queue with a given priority and duration. Args: - priority (int): The priority level of the task (lower number indicates higher priority). - task_duration (float): The duration the task will take to complete in seconds. await self.priority_queue.put((priority, task_duration)) async def worker(self): while not self.priority_queue.empty(): priority, task_duration = await self.priority_queue.get() await asyncio.sleep(task_duration) self.priority_queue.task_done() async def scheduler(self, num_workers): Execute tasks from the priority queue using a specified number of worker coroutines. Args: - num_workers (int): The number of worker coroutines to process tasks concurrently. Returns: - str: A string indicating the total task processing time in seconds. start_time = time.time() workers = [asyncio.create_task(self.worker()) for _ in range(num_workers)] await self.priority_queue.join() end_time = time.time() return f\\"Total task processing time: {end_time - start_time} seconds\\""},{"question":"# Question: Custom Python Initialization You are tasked with creating a custom Python environment that initializes Python in isolated mode, sets specific configuration parameters, and runs a Python command. Write a Python program that achieves the following: 1. **Initializes Python in isolated mode** using the `PyConfig` and `PyPreConfig` structures. 2. **Disables the import of the `site` module** at startup. 3. **Sets the memory allocator** to `PYMEM_ALLOCATOR_MALLOC`. 4. **Enables the Python UTF-8 Mode**. 5. **Runs a Python command** that prints \\"Hello, Custom Python!\\". 6. **Handles any initialization errors** appropriately by printing a diagnostic message and exiting with a non-zero status. You are required to simulate the `PyConfig` structure and its behavior using Python functions and classes, as direct access to these low-level C APIs is not available in standard Python environments. # Expected Input and Output There are no explicit inputs from the user. The program itself handles the initialization and execution of the custom Python command. The expected output should be: ``` Hello, Custom Python! ``` Additionally, if there are any errors during initialization, the program should output a relevant error message. # Constraints - Simulate the behavior of `PyConfig`, `PyPreConfig`, and related structures using Python classes and functions. - Ensure that the Python environment is correctly isolated and configured according to the specified parameters. # Example Implementation in Python ```python class PyStatus: def __init__(self, ok=True, exit_code=0, err_msg=None): self.ok = ok self.exit_code = exit_code self.err_msg = err_msg class PyConfig: def __init__(self): self.isolated = 1 self.use_environment = 0 self.site_import = 0 self.utf8_mode = 1 self.allocator = \\"PYMEM_ALLOCATOR_MALLOC\\" def PyConfig_InitIsolatedConfig(config): config.isolated = 1 config.use_environment = 0 config.site_import = 0 def Py_InitializeFromConfig(config): try: # Simulated initialization logic if not isinstance(config, PyConfig): raise ValueError(\\"Invalid configuration\\") # success status return PyStatus(ok=True) except Exception as e: # error status return PyStatus(ok=False, err_msg=str(e)) def run_custom_python(): try: config = PyConfig() PyConfig_InitIsolatedConfig(config) config.utf8_mode = 1 config.allocator = \\"PYMEM_ALLOCATOR_MALLOC\\" status = Py_InitializeFromConfig(config) if not status.ok: raise RuntimeError(f\\"Initialization error: {status.err_msg}\\") # Run the Python command exec(\'print(\\"Hello, Custom Python!\\")\') except Exception as e: print(f\\"Error: {e}\\") exit(1) if __name__ == \\"__main__\\": run_custom_python() ``` # What to submit Submit your Python script file containing the implementation of the described behavior. Ensure to include comments explaining each part of your code.","solution":"class PyStatus: def __init__(self, ok=True, exit_code=0, err_msg=None): self.ok = ok self.exit_code = exit_code self.err_msg = err_msg class PyConfig: def __init__(self): self.isolated = 0 self.use_environment = 1 self.site_import = 1 self.utf8_mode = 0 self.allocator = None def PyConfig_InitIsolatedConfig(config): config.isolated = 1 config.use_environment = 0 config.site_import = 0 def Py_InitializeFromConfig(config): try: # Simulated initialization logic if not isinstance(config, PyConfig): raise ValueError(\\"Invalid configuration\\") # Pretend to handle other config properties to ensure correctness assert config.isolated == 1 assert config.use_environment == 0 assert config.site_import == 0 assert config.utf8_mode == 1 assert config.allocator == \\"PYMEM_ALLOCATOR_MALLOC\\" # success status return PyStatus(ok=True) except Exception as e: # error status return PyStatus(ok=False, err_msg=str(e)) def run_custom_python(): try: config = PyConfig() PyConfig_InitIsolatedConfig(config) config.utf8_mode = 1 config.allocator = \\"PYMEM_ALLOCATOR_MALLOC\\" status = Py_InitializeFromConfig(config) if not status.ok: raise RuntimeError(f\\"Initialization error: {status.err_msg}\\") # Run the Python command exec(\'print(\\"Hello, Custom Python!\\")\') except Exception as e: print(f\\"Error: {e}\\") exit(1) if __name__ == \\"__main__\\": run_custom_python()"},{"question":"# XML Processing with `xml.etree.ElementTree` You are tasked to implement functions that parse XML data, manipulate it by adding and modifying elements, and then output the modified XML data. This exercise will test your understanding of XML parsing, element manipulation, and writing back XML structured data, specifically using the `xml.etree.ElementTree` module. Function Specifications 1. **parse_xml(xml_string: str) -> ElementTree.Element:** - **Input:** - `xml_string`: A string containing XML data. - **Output:** - Returns the root element of the parsed XML tree. 2. **add_element(root: ElementTree.Element, parent_tag: str, new_element_tag: str, new_element_text: str) -> None:** - **Input:** - `root`: The root element of the XML tree. - `parent_tag`: The tag name of the parent element to which the new element should be added. - `new_element_tag`: The tag name of the new element to be added. - `new_element_text`: The text to be added to the new element. - **Output:** - This function updates the XML tree by adding a new element under the specified parent element. If the parent element tag does not exist, it raises a `ValueError`. 3. **modify_element(root: ElementTree.Element, element_tag: str, new_text: str) -> None:** - **Input:** - `root`: The root element of the XML tree. - `element_tag`: The tag name of the element whose text needs to be modified. - `new_text`: The new text for the specified element. - **Output:** - This function updates the text of the specified element. If the element tag does not exist, it raises a `ValueError`. 4. **to_string(root: ElementTree.Element) -> str:** - **Input:** - `root`: The root element of the XML tree. - **Output:** - Returns a string representation of the updated XML. Constraints and Requirements - Use the `xml.etree.ElementTree` module for XML operations. - Properly handle cases where elements to be modified or added are not present and raise appropriate errors. - Preserve the structure of the XML while performing modifications. - Ensure the functions are efficient with respect to time and space complexity. Example Usage ```python from xml.etree.ElementTree import Element # Example XML data xml_data = <root> <parent> <child>old text</child> </parent> </root> # Parse the XML data root = parse_xml(xml_data) # Add a new element add_element(root, \'parent\', \'new_child\', \'new text\') # Modify an existing element modify_element(root, \'child\', \'new child text\') # Convert updated XML to string updated_xml = to_string(root) print(updated_xml) # Expected Output: # <root> # <parent> # <child>new child text</child> # <new_child>new text</new_child> # </parent> # </root> ``` Implement the above functions and ensure they work as specified.","solution":"import xml.etree.ElementTree as ET def parse_xml(xml_string: str) -> ET.Element: Parses an XML string and returns the root element. try: root = ET.fromstring(xml_string) return root except ET.ParseError as e: raise ValueError(f\\"Error parsing XML: {e}\\") def add_element(root: ET.Element, parent_tag: str, new_element_tag: str, new_element_text: str) -> None: Adds a new element under the specified parent element. parent = root.find(parent_tag) if parent is None: raise ValueError(f\\"Parent tag \'{parent_tag}\' not found\\") new_element = ET.Element(new_element_tag) new_element.text = new_element_text parent.append(new_element) def modify_element(root: ET.Element, element_tag: str, new_text: str) -> None: Modifies the text of the specified element. element = root.find(element_tag) if element is None: raise ValueError(f\\"Element tag \'{element_tag}\' not found\\") element.text = new_text def to_string(root: ET.Element) -> str: Converts the XML tree to a string representation. return ET.tostring(root, encoding=\'unicode\')"},{"question":"**Title**: Implement a Custom Iterator Function in Python **Objective**: Create a Python function that uses the iterator-related C-API functionalities from the provided documentation. This exercise will test your proficiency in handling Python\'s iterator protocols and error management. **Description**: You are provided with the following: 1. A Python list of integers. 2. A flag indicating whether the iteration should be synchronous or asynchronous. You need to implement the function `process_iterator`, which takes these inputs and returns the sum of all integers in the list. For synchronous iteration, use Python\'s regular iteration protocol. For asynchronous iteration, simulate the behavior of asynchronous iterators as described in the provided documentation. **Function Signature**: ```python def process_iterator(lst: list, async_flag: bool) -> int: pass ``` **Input**: 1. `lst` (list): A list of integers to be iterated over. 2. `async_flag` (bool): A flag indicating whether to use asynchronous iteration. **Output**: 1. An integer representing the sum of all integers in the list. **Constraints**: 1. You should handle empty lists correctly (the sum should be 0). 2. Ensure proper error handling and resource management as indicated in the C-API example. 3. Do not use any built-in sum functions; you must implement the summing logic manually. **Example**: ```python # Example 1: lst = [1, 2, 3, 4, 5] async_flag = False assert process_iterator(lst, async_flag) == 15 # Example 2: lst = [1, 2, 3, 4, 5] async_flag = True assert process_iterator(lst, async_flag) == 15 # Example 3: lst = [] async_flag = False assert process_iterator(lst, async_flag) == 0 ``` **Notes**: - Synchronous iteration can be implemented with a simple `for` loop. - For asynchronous iteration, assume `await` is a no-op; focus on replicating the iteration behavior described. **Hints**: - While you cannot access the actual C-API with Python directly, you should simulate the behavior and error handling provided in the documentation using Python methods. - Ensure proper handling of exceptions and cleanup of resources to mimic C-API\'s guidelines.","solution":"import asyncio def process_iterator(lst: list, async_flag: bool) -> int: Processes a list of integers using synchronous or asynchronous iteration. Parameters: lst (list): The list of integers to be iterated over. async_flag (bool): A flag indicating whether to use asynchronous iteration. Returns: int: The sum of the integers in the list. if async_flag: return asyncio.run(_async_sum(lst)) else: return _sync_sum(lst) def _sync_sum(lst: list) -> int: Synchronously sums the integers in the list. Parameters: lst (list): The list of integers to be summed. Returns: int: The sum of the integers in the list. total = 0 for num in lst: total += num return total async def _async_sum(lst: list) -> int: Asynchronously sums the integers in the list. Parameters: lst (list): The list of integers to be summed. Returns: int: The sum of the integers in the list. total = 0 for num in lst: # Simulating asynchronous behavior await asyncio.sleep(0) total += num return total"},{"question":"**Objective**: Utilize the `py_compile` module to design a function for batch compilation of Python source files with detailed error handling and reporting. # Problem Statement You are required to implement a function called `batch_compile` that accepts a list of file paths to Python source files and compiles them into byte-code files. The function should: 1. **Compile each file** using the `py_compile.compile` function. 2. **Handle errors gracefully**: If a compilation error occurs for a file, capture the error message and store it. 3. **Return a detailed report** of the compilation process, including a list of successfully compiled files and a list of files that failed to compile with their respective error messages. 4. **Allow customization** for optimization and invalidation mode through function parameters. # Function Signature ```python from typing import List, Dict, Tuple import py_compile def batch_compile(files: List[str], optimize: int = -1, invalidation_mode: str = \\"TIMESTAMP\\") -> Dict[str, Tuple[bool, str]]: Compile a batch of Python source files. Args: - files (List[str]): List of file paths to Python source files. - optimize (int): Optimization level (-1 for default, 0 for no optimization, 1 for slight optimization, 2 for full optimization). - invalidation_mode (str): Invalidation mode for byte-code files. One of [\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"]. Returns: - Dict[str, Tuple[bool, str]]: A dictionary mapping each file path to a tuple (success, message), where `success` is a boolean indicating if the compilation succeeded, and `message` is the error message or the path to the byte-code file. pass ``` # Expected Input and Output **Input**: - `files`: A list of file paths (e.g., `[\\"example1.py\\", \\"example2.py\\"]`). - `optimize`: Optimization level (default is `-1`). - `invalidation_mode`: Invalidation mode (`TIMESTAMP`, `CHECKED_HASH`, `UNCHECKED_HASH`). **Output**: - A dictionary with file paths as keys and tuples as values. Each tuple contains: - A boolean indicating if the compilation was successful. - A message which is either the path to the compiled file or an error message. # Example ```python files = [\\"example1.py\\", \\"example2.py\\"] result = batch_compile(files, optimize=2, invalidation_mode=\\"CHECKED_HASH\\") # Expected result might look like: { \\"example1.py\\": (True, \\"path/to/example1.cpython-38.pyc\\"), \\"example2.py\\": (False, \\"SyntaxError: invalid syntax (<string>, line 1)\\") } ``` # Constraints and Limitations - Handle any kind of file or path errors (e.g., file not found). - Ensure that only valid invalidation modes are accepted. - Optimize the function to handle a large number of files efficiently. # Additional Notes - Make sure to use appropriate error handling techniques. - Use the `py_compile.PycInvalidationMode` enum for validating invalidation modes.","solution":"from typing import List, Dict, Tuple import py_compile import os def batch_compile(files: List[str], optimize: int = -1, invalidation_mode: str = \\"TIMESTAMP\\") -> Dict[str, Tuple[bool, str]]: Compile a batch of Python source files. Args: - files (List[str]): List of file paths to Python source files. - optimize (int): Optimization level (-1 for default, 0 for no optimization, 1 for slight optimization, 2 for full optimization). - invalidation_mode (str): Invalidation mode for byte-code files. One of [\\"TIMESTAMP\\", \\"CHECKED_HASH\\", \\"UNCHECKED_HASH\\"]. Returns: - Dict[str, Tuple[bool, str]]: A dictionary mapping each file path to a tuple (success, message), where `success` is a boolean indicating if the compilation succeeded, and `message` is the error message or the path to the byte-code file. compilation_results = {} # Validate invalidation_mode argument valid_invalidation_modes = { \\"TIMESTAMP\\": py_compile.PycInvalidationMode.TIMESTAMP, \\"CHECKED_HASH\\": py_compile.PycInvalidationMode.CHECKED_HASH, \\"UNCHECKED_HASH\\": py_compile.PycInvalidationMode.UNCHECKED_HASH } if invalidation_mode not in valid_invalidation_modes: raise ValueError(f\\"Invalid invalidation_mode: {invalidation_mode}\\") for file in files: try: compiled_file = py_compile.compile( file, optimize=optimize, invalidation_mode=valid_invalidation_modes[invalidation_mode], doraise=True ) compilation_results[file] = (True, compiled_file) except Exception as e: compilation_results[file] = (False, str(e)) return compilation_results"},{"question":"# Question You are given a dataset and your task is to create a function that performs the following operations using the `seaborn` library: 1. Load the dataset and visualize it using a scatter plot. 2. Apply and visualize the scatter plot with at least three different color palettes from `seaborn`. 3. Save each plot with the color palette name as part of the filename. **Specifications:** 1. You should use the `seaborn` `tips` dataset for this task. 2. Your function should be named `visualize_tips_data`. 3. The function should not return any values but should save all plots to disk. 4. The file names of the plots should clearly indicate the color palette used (e.g., `scatter_plot_pastel.png`). 5. Utilize at least three distinct color palettes, making sure at least one is categorical, one is continuous, and one is a custom gradient. **Function Signature:** ```python def visualize_tips_data() -> None: pass ``` **Example Usage:** ```python visualize_tips_data() ``` After running the function, you should have three saved scatter plot images with different color palettes, located in your working directory. **Notes:** - You can use the seaborn `relplot` or `scatterplot` to create scatter plots. - Make sure your plots are properly labeled and easy to understand. - Follow the guidelines for saving the plots with meaningful filenames. ```python # Solution Template import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data() -> None: # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Define the color palettes to use color_palettes = [\\"pastel\\", \\"flare\\", \\"blend:#7AB,#EDA\\"] for palette in color_palettes: # Create the scatter plot with the specified palette plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", palette=sns.color_palette(palette, as_cmap=True)) # Save the plot with an appropriate filename plt.savefig(f\\"scatter_plot_{palette}.png\\") plt.close() # Call the function (Remove this line if you are running this as a script) visualize_tips_data() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data() -> None: # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Define the color palettes to use color_palettes = { \\"pastel\\": sns.color_palette(\\"pastel\\"), \\"flare\\": sns.color_palette(\\"flare\\", as_cmap=True), \\"custom\\": sns.color_palette(\\"blend:#7AB,#EDA\\", as_cmap=True), } for name, palette in color_palettes.items(): # Create the scatter plot with the specified palette plt.figure() sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", palette=palette) plt.title(f\\"Scatter Plot with {name} Palette\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") # Save the plot with an appropriate filename plt.savefig(f\\"scatter_plot_{name}.png\\") plt.close()"},{"question":"# Secure Token Generator Objective: Write a Python function `generate_secure_token` that uses the `secrets` module to generate a secure URL-safe token. The token should meet the following criteria for different use cases: 1. **Default Token**: If no length is provided, generate a token with a default length of 32 bytes. 2. **Custom Length Token**: If a length `n` is provided, generate a token with exactly `n` bytes of randomness. 3. **Password Reset Token**: Generate a token of `12` bytes specifically for password reset URLs. Your function should return the generated token as a URL-safe string. Function Signature: ```python def generate_secure_token(use_case: str, nbytes: int = None) -> str: pass ``` Input: - `use_case` (str): A string indicating the use case for the token. It can be one of the following: - `\'default\'`: Generate a default token. - `\'custom\'`: Generate a custom length token. - `\'password_reset\'`: Generate a token for password reset. - `nbytes` (int, optional): The number of bytes of randomness to use for the custom token length. This parameter is only considered if `use_case` is `\'custom\'`. Output: - Returns a URL-safe text string token based on the specified use case. Examples: ```python print(generate_secure_token(\'default\')) # Example Output: \'Vp8YcfFZySbE_wKbq9SU73P1hhXI79BE\' print(generate_secure_token(\'custom\', nbytes=16)) # Example Output: \'Drmhze6EPcv0fN_81Bj-nA\' print(generate_secure_token(\'password_reset\')) # Example Output: \'9XpTlUotLw8\' ``` Constraints: - The token must be generated using the `secrets` module to ensure it is cryptographically secure. - For the \'custom\' use case, if `nbytes` is not provided, raise a `ValueError` with the message \\"Length for custom token must be specified\\".","solution":"import secrets def generate_secure_token(use_case: str, nbytes: int = None) -> str: Generate a secure URL-safe token based on the specified use case. Parameters: - use_case (str): The use case for the token, which can be \'default\', \'custom\', or \'password_reset\'. - nbytes (int, optional): The number of bytes of randomness for the custom token length. Considered only if use_case is \'custom\'. Returns: - str: A URL-safe text string token. if use_case == \'default\': nbytes = 32 elif use_case == \'custom\': if nbytes is None: raise ValueError(\\"Length for custom token must be specified\\") elif use_case == \'password_reset\': nbytes = 12 else: raise ValueError(\\"Invalid use case specified\\") return secrets.token_urlsafe(nbytes)"},{"question":"# Exception Handling in Python In this exercise, you are required to demonstrate your understanding of Python\'s exception handling mechanisms inspired by the functions and practices described in the C API documentation. Problem Statement You need to implement a function `handle_exceptions()` that takes an input function and its arguments, executes the function, and handles any exceptions that occur. The function should: 1. Print a custom error message if a `MemoryError` is encountered. 2. Print a formatted message indicating the line number and type of error for any general exceptions. 3. Ensure that the original exception is preserved and can be raised after the error message is printed. 4. Return `None` if an exception occurs. You must also implement a custom exception class `CustomError` that inherits from `Exception`. This custom error class should include: - An `__init__` method that accepts a message and a custom error code. - A `__str__` method that returns a formatted string combining the error message and code. Input Format - A function `func` along with its arguments `*args` and `**kwargs`. Output Format - The custom error messages printed to the console. - Raise the original exception after printing the error message. - Return `None` if an exception is raised. Example Consider the following function definitions: ```python def sample_function(x): if x == 0: raise ValueError(\\"Value cannot be zero\\") elif x < 0: raise MemoryError(\\"Simulating memory error\\") elif x == 1: raise CustomError(\\"Custom error occurred\\", 404) return x ``` Calling `handle_exceptions(sample_function, 0)` should output: ``` Error on line <line_number>: ValueError - Value cannot be zero ``` and subsequently raise the `ValueError`. Calling `handle_exceptions(sample_function, -1)` should output: ``` MemoryError encountered: Simulating memory error ``` and subsequently raise the `MemoryError`. Calling `handle_exceptions(sample_function, 1)` should output: ``` Error on line <line_number>: CustomError - Custom error occurred (Error code: 404) ``` and subsequently raise the `CustomError`. Constraints - Do not use any global variables. - Do not modify the input function `func`. Function Signature ```python class CustomError(Exception): def __init__(self, message, code): super().__init__(message) self.code = code def __str__(self): return f\\"{self.args[0]} (Error code: {self.code})\\" def handle_exceptions(func, *args, **kwargs): # Your code here ``` Implement the `CustomError` class and `handle_exceptions` function based on the above description.","solution":"class CustomError(Exception): def __init__(self, message, code): super().__init__(message) self.code = code def __str__(self): return f\\"{self.args[0]} (Error code: {self.code})\\" def handle_exceptions(func, *args, **kwargs): try: return func(*args, **kwargs) except MemoryError as me: print(f\\"MemoryError encountered: {me}\\") raise except Exception as ex: import sys _, _, tb = sys.exc_info() lineno = tb.tb_lineno print(f\\"Error on line {lineno}: {type(ex).__name__} - {ex}\\") raise return None"},{"question":"Title: Advanced Usage of Python\'s io Module for Text and Binary I/O Operations Problem Statement: You are tasked with developing a utility for reading, manipulating, and writing data from and to text and binary files using Python\'s `io` module. Requirements: 1. **Functionality 1: Read Text File** - Implement the function `read_text_file(file_path: str) -> str`: - Reads the content from the specified text file path. - Ensures that the text file content is read with UTF-8 encoding. - Handles edge cases such as files not existing or improper file paths by raising appropriate exceptions. 2. **Functionality 2: Write to Text File** - Implement the function `write_text_file(file_path: str, content: str) -> None`: - Writes the given string content to the specified file path. - Uses UTF-8 encoding. - Overwrites the file if it already exists and creates a new file if it does not. 3. **Functionality 3: Read Binary File** - Implement the function `read_binary_file(file_path: str) -> bytes`: - Reads the entire content from the specified binary file path. - Handles file not found errors gracefully. 4. **Functionality 4: Write to Binary File** - Implement the function `write_binary_file(file_path: str, content: bytes) -> None`: - Writes the given bytes content to the specified file path. - Creates the file if it does not exist, and truncates the file if it exists. 5. **Functionality 5: Combine and Save Data** - Implement the function `combine_and_save_files(text_file: str, binary_file: str, output_file: str) -> None`: - Reads content from the given text and binary files. - Combines the content by appending the binary content to the end of the text content (converted to string). - Writes the combined content back to the specified output file in binary mode. Constraints: - Ensure all file operations are efficiently handled using buffered I/O where applicable. - Implement proper error handling mechanisms for I/O operations. - Adhere to the following function signatures: ```python def read_text_file(file_path: str) -> str: pass def write_text_file(file_path: str, content: str) -> None: pass def read_binary_file(file_path: str) -> bytes: pass def write_binary_file(file_path: str, content: bytes) -> None: pass def combine_and_save_files(text_file: str, binary_file: str, output_file: str) -> None: pass ``` Example: ```python # Assuming \'text_file.txt\' contains \\"Hello, World!n\\" and \'binary_file.bin\' contains b\'x00x01x02\' read_text_file(\'text_file.txt\') # Output: \\"Hello, World!n\\" write_text_file(\'new_text_file.txt\', \\"New Content\\") # \'new_text_file.txt\' should now contain \\"New Content\\" read_binary_file(\'binary_file.bin\') # Output: b\'x00x01x02\' write_binary_file(\'new_binary_file.bin\', b\'x03x04x05\') # \'new_binary_file.bin\' should now contain b\'x03x04x05\' combine_and_save_files(\'text_file.txt\', \'binary_file.bin\', \'output_file.bin\') # \'output_file.bin\' should contain b\\"Hello, World!nx00x01x02\\" ``` Happy coding!","solution":"import os import io def read_text_file(file_path: str) -> str: try: with io.open(file_path, \'r\', encoding=\'utf-8\') as file: return file.read() except FileNotFoundError: raise FileNotFoundError(f\\"File at {file_path} does not exist.\\") except Exception as e: raise IOError(f\\"An error occurred while reading the file: {e}\\") def write_text_file(file_path: str, content: str) -> None: try: with io.open(file_path, \'w\', encoding=\'utf-8\') as file: file.write(content) except Exception as e: raise IOError(f\\"An error occurred while writing to the file: {e}\\") def read_binary_file(file_path: str) -> bytes: try: with io.open(file_path, \'rb\') as file: return file.read() except FileNotFoundError: raise FileNotFoundError(f\\"File at {file_path} does not exist.\\") except Exception as e: raise IOError(f\\"An error occurred while reading the file: {e}\\") def write_binary_file(file_path: str, content: bytes) -> None: try: with io.open(file_path, \'wb\') as file: file.write(content) except Exception as e: raise IOError(f\\"An error occurred while writing to the file: {e}\\") def combine_and_save_files(text_file: str, binary_file: str, output_file: str) -> None: text_content = read_text_file(text_file) binary_content = read_binary_file(binary_file) combined_content = text_content.encode(\'utf-8\') + binary_content write_binary_file(output_file, combined_content)"},{"question":"You are required to implement a simple WSGI application using the `wsgiref` module that serves static files from a specified directory. The application should meet the following requirements: 1. **Directory Listing and File Serving:** - The application should serve files from a specified directory. - If the request URL corresponds to a directory, it should serve an `index.html` file from that directory. - If the requested file is not found, the application should return a `404 Not Found` status. 2. **Header Management:** - Use the `Headers` class from `wsgiref.headers` to manage response headers. - Ensure that the correct `Content-Type` header is set based on the file extension using `mimetypes.guess_type`. 3. **WSGI Utilities:** - Utilize `wsgiref.util.setup_testing_defaults` to set up the WSGI environment when setting up the directory path. - Use `wsgiref.util.FileWrapper` to wrap the file object returned by the application. 4. **Server Implementation:** - Create a simple server using `wsgiref.simple_server.make_server` that serves the WSGI application. The implementation must support the following directory structure: ``` your_directory/ index.html subdir/ index.html file.txt ``` Here is the function signature: ```python def simple_file_server_application(environ, start_response): # Implement your WSGI application here pass def run_server(directory, port=8000): # Implement the server setup and running logic here pass ``` Example Usage ```python # To run the server serving files from \'your_directory\' on port 8000 run_server(\'/path/to/your_directory\', 8000) ``` # Constraints - Only standard libraries (including `wsgiref` and `mimetypes`) are allowed. - The server should handle multiple requests sequentially. # Evaluation Criteria - **Correctness:** The application should correctly serve files, handle missing files, and set headers. - **Use of `wsgiref`:** Appropriate use of `wsgiref` utilities and `Headers` class. - **Clarity:** Code should be well-organized and readable. - **Performance:** The server should handle file serving efficiently. Good luck!","solution":"import os from wsgiref.simple_server import make_server from wsgiref.util import setup_testing_defaults, FileWrapper from wsgiref.headers import Headers import mimetypes def simple_file_server_application(environ, start_response): setup_testing_defaults(environ) path = environ.get(\'PATH_INFO\', \'\').lstrip(\'/\') full_path = os.path.join(environ[\'DOCUMENT_ROOT\'], path) if os.path.isdir(full_path): full_path = os.path.join(full_path, \'index.html\') if not os.path.exists(full_path): start_response(\'404 Not Found\', [(\'Content-type\', \'text/plain\')]) return [b\'404 Not Found\'] content_type, _ = mimetypes.guess_type(full_path) content_type = content_type or \'application/octet-stream\' headers = Headers([(\'Content-type\', content_type)]) start_response(\'200 OK\', headers.items()) return FileWrapper(open(full_path, \'rb\')) def run_server(directory, port=8000): from functools import partial handler = partial(simple_file_server_application) os.environ[\'DOCUMENT_ROOT\'] = directory httpd = make_server(\'\', port, handler) print(f\\"Serving HTTP on port {port} from directory {directory} ...\\") httpd.serve_forever()"},{"question":"**Objective:** To assess your understanding and ability to use Seaborn\'s color palette functionalities to effectively visualize data. **Task:** Create a Python function `visualize_data_with_palettes(data, palette_type, hue_feature)`, where: - `data`: A pandas DataFrame containing the dataset. - `palette_type`: A string indicating the type of palette to use. It can be one of \\"qualitative\\", \\"sequential\\", or \\"diverging\\". - `hue_feature`: The feature/column in the DataFrame based on which the color variation should be applied. **Requirements:** 1. Load the dataset: You are required to use the \\"penguins\\" dataset available from Seaborn. 2. Implement the function to handle three palette types: \\"qualitative\\", \\"sequential\\", and \\"diverging\\". 3. Generate a relevant Seaborn plot: - For qualitative palettes, use `sns.scatterplot`. - For sequential palettes, use `sns.histplot`. - For diverging palettes, use `sns.heatmap`. 4. Ensure the function sets an appropriate palette based on the `palette_type`. 5. The plots should clearly demonstrate the color variations according to the `hue_feature`. 6. Handle any potential exceptions or edge cases, such as invalid palette types. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_data_with_palettes(data: pd.DataFrame, palette_type: str, hue_feature: str) -> None: # Your implementation here ``` **Example Usage:** ```python # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Visualize using a qualitative palette visualize_data_with_palettes(penguins, palette_type=\\"qualitative\\", hue_feature=\\"species\\") # Visualize using a sequential palette visualize_data_with_palettes(penguins, palette_type=\\"sequential\\", hue_feature=\\"bill_length_mm\\") # Visualize using a diverging palette visualize_data_with_palettes(penguins, palette_type=\\"diverging\\", hue_feature=\\"body_mass_g\\") ``` **Constraints:** - Ensure that the function gracefully handles cases where `hue_feature` is not present in the DataFrame. **Output:** The function should display the corresponding plot for the given dataset and palette type. With this question, students will demonstrate their ability to: - Load and manipulate datasets using Seaborn and Pandas. - Apply different types of color palettes effectively for data visualization. - Understand the context in which different palette types are most suitable. - Handle basic exception cases related to data visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_data_with_palettes(data: pd.DataFrame, palette_type: str, hue_feature: str) -> None: Visualizes data using different Seaborn color palettes. Parameters: data (pd.DataFrame): The dataset to visualize. palette_type (str): The type of palette to use (\\"qualitative\\", \\"sequential\\", or \\"diverging\\"). hue_feature (str): The feature/column in the DataFrame for color variation. Returns: None if hue_feature not in data.columns: raise ValueError(f\\"The feature \'{hue_feature}\' is not present in the DataFrame.\\") if palette_type == \\"qualitative\\": palette = sns.color_palette(\\"Set1\\") sns.scatterplot(data=data, x=data.columns[0], y=data.columns[1], hue=hue_feature, palette=palette) plt.title(\'Qualitative Palette\') elif palette_type == \\"sequential\\": palette = sns.color_palette(\\"BuGn\\", as_cmap=True) sns.histplot(data=data, x=hue_feature, palette=palette) plt.title(\'Sequential Palette\') elif palette_type == \\"diverging\\": if data[hue_feature].dtype in [\'float64\', \'int64\']: # Here we demonstrate with a pivot table to create a heatmap data_pivot = data.pivot_table(index=data.columns[0], columns=data.columns[1], values=hue_feature) palette = sns.color_palette(\\"coolwarm\\", as_cmap=True) sns.heatmap(data_pivot, cmap=palette) plt.title(\'Diverging Palette\') else: raise TypeError(f\\"The feature \'{hue_feature}\' must be numeric for a diverging palette.\\") else: raise ValueError(\\"palette_type must be one of \'qualitative\', \'sequential\', or \'diverging\'\\") plt.show()"},{"question":"**Objective**: Demonstrate your proficiency with the `seaborn` package by generating several customized violin plots using the Titanic dataset. **Task**: 1. Load the Titanic dataset from `seaborn`. 2. Create and customize violin plots as described in the following steps. All plots should be displayed on the same figure using subplots. Steps: 1. **Simple Violin Plot**: - Plot the distribution of the `age` column. 2. **Bivariate Violin Plot**: - Plot the distribution of `age` grouped by `class`. 3. **Categorical Grouping with Hue**: - Plot the distribution of `age` grouped by `class` and color it based on the `sex` column. Ensure the violins are filled. 4. **Split Violins and Data Quartiles**: - Plot the distribution of `age` grouped by `class` and split by `survived`. Show the data quartiles inside the violins. 5. **Normalized Violin Plot**: - Plot the distribution of `age` grouped by `deck`, and set the inner representation to `point`. Normalize the width of each violin to represent the number of observations. 6. **Customized Smoothing and Limiting KDE**: - Plot the distribution of `age` grouped by `alive`. Set the kernel density estimation to not extend beyond the observed data. Use stick representation inside the violins, and adjust the bandwidth to 0.5. 7. **Complex Grouping with Custom Axes**: - Plot the distribution of `fare` grouped by `age` rounded to the nearest 10 and adjusted by 5, preserving the native scale. Requirements: - Use the `subplots` function from `matplotlib` to create a grid of plots. - Each plot should have a title corresponding to its description above. - Use `sns.set_theme(style=\\"whitegrid\\")` for a consistent aesthetic. - Ensure all plots are properly labeled, and legends are included where applicable. - Comment your code to explain each step. # Example Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def create_violin_plots(): sns.set_theme(style=\\"whitegrid\\") df = sns.load_dataset(\\"titanic\\") # Create subplots fig, axes = plt.subplots(3, 2, figsize=(15, 15)) axes = axes.flatten() # Plot 1: Simple Violin Plot sns.violinplot(x=df[\\"age\\"], ax=axes[0]) axes[0].set_title(\'Simple Violin Plot\') # Plot 2: Bivariate Violin Plot sns.violinplot(data=df, x=\\"age\\", y=\\"class\\", ax=axes[1]) axes[1].set_title(\'Bivariate Violin Plot\') # Plot 3: Categorical Grouping with Hue sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", ax=axes[2], split=True) axes[2].set_title(\'Categorical Grouping with Hue\') # Plot 4: Split Violins and Data Quartiles sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", split=True, inner=\\"quart\\", ax=axes[3]) axes[3].set_title(\'Split Violins and Data Quartiles\') # Plot 5: Normalized Violin Plot sns.violinplot(data=df, x=\\"deck\\", y=\\"age\\", inner=\\"point\\", scale=\\"count\\", ax=axes[4]) axes[4].set_title(\'Normalized Violin Plot\') # Plot 6: Customized Smoothing and Limiting KDE sns.violinplot(data=df, x=\\"alive\\", y=\\"age\\", cut=0, bw=0.5, inner=\\"stick\\", ax=axes[5]) axes[5].set_title(\'Customized Smoothing and Limiting KDE\') # Show plot plt.tight_layout() plt.show() ``` **Note**: Ensure all necessary libraries are imported, and the resulting plots are correctly displayed on one figure with appropriate labels and legends. # Constraints: - Check for missing values and handle them appropriately before plotting. - The code should be efficient and not use excessive memory.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_violin_plots(): sns.set_theme(style=\\"whitegrid\\") df = sns.load_dataset(\\"titanic\\") # Handle missing values df.dropna(subset=[\\"age\\", \\"class\\", \\"sex\\", \\"survived\\", \\"deck\\", \\"alive\\", \\"fare\\"], inplace=True) # Create subplots fig, axes = plt.subplots(3, 2, figsize=(15, 15)) axes = axes.flatten() # Plot 1: Simple Violin Plot sns.violinplot(x=df[\\"age\\"], ax=axes[0]) axes[0].set_title(\'Simple Violin Plot\') # Plot 2: Bivariate Violin Plot sns.violinplot(data=df, x=\\"age\\", y=\\"class\\", ax=axes[1]) axes[1].set_title(\'Bivariate Violin Plot\') # Plot 3: Categorical Grouping with Hue sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"sex\\", ax=axes[2], split=True) axes[2].set_title(\'Categorical Grouping with Hue\') # Plot 4: Split Violins and Data Quartiles sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", split=True, inner=\\"quart\\", ax=axes[3]) axes[3].set_title(\'Split Violins and Data Quartiles\') # Plot 5: Normalized Violin Plot sns.violinplot(data=df, x=\\"deck\\", y=\\"age\\", inner=\\"point\\", scale=\\"count\\", ax=axes[4]) axes[4].set_title(\'Normalized Violin Plot\') # Plot 6: Customized Smoothing and Limiting KDE sns.violinplot(data=df, x=\\"alive\\", y=\\"age\\", cut=0, bw=0.5, inner=\\"stick\\", ax=axes[5]) axes[5].set_title(\'Customized Smoothing and Limiting KDE\') # Customize layout plt.tight_layout() plt.show()"},{"question":"**Seaborn Coding Assessment: ECDF Plotting** # Objective: To assess your understanding of seaborn\'s `sns.ecdfplot` function and its various functionalities in visualizing empirical cumulative distribution functions. # Question: Write a Python function `create_ecdf_plots` that performs the following tasks: 1. Load the \\"penguins\\" dataset from seaborn. 2. Generate and save three different ECDF plots: - Plot 1: A basic ECDF plot for the \\"flipper_length_mm\\" variable. - Plot 2: An ECDF plot for the \\"bill_length_mm\\" variable, colored by \\"species\\". - Plot 3: A complementary ECDF plot for the \\"bill_length_mm\\" variable, colored by \\"species\\". # Constraints: - Use seaborn\'s `sns.ecdfplot` function. - Save each plot as a PNG file in the current working directory with filenames: - \\"ecdf_plot_1.png\\" for Plot 1 - \\"ecdf_plot_2.png\\" for Plot 2 - \\"ecdf_plot_3.png\\" for Plot 3 # Function Signature: ```python def create_ecdf_plots(): pass ``` # Example: After running your function, three PNG files should be created in the working directory. # Notes: - Ensure that all plots are properly labeled with titles and axis labels. - You may use seaborn and matplotlib libraries to assist with plot customization and saving. # Expected Output: The function should create and save three PNG files representing the specified ECDF plots. You can assume that Seaborn, matplotlib, and pandas libraries are pre-installed in the environment.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_ecdf_plots(): # Load the \\"penguins\\" dataset penguins = sns.load_dataset(\\"penguins\\") # Plot 1: A basic ECDF plot for the \\"flipper_length_mm\\" variable plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\'ECDF for Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'ECDF\') plt.savefig(\\"ecdf_plot_1.png\\") # Plot 2: An ECDF plot for the \\"bill_length_mm\\" variable, colored by \\"species\\" plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\'ECDF for Bill Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'ECDF\') plt.savefig(\\"ecdf_plot_2.png\\") # Plot 3: A complementary ECDF plot for the \\"bill_length_mm\\" variable, colored by \\"species\\" plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\'Complementary ECDF for Bill Length by Species\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Complementary ECDF\') plt.savefig(\\"ecdf_plot_3.png\\") print(\\"Plots have been created and saved successfully.\\")"},{"question":"Objective Write a function to perform kernel density estimation on a given set of 2-dimensional data points using different kernel types and bandwidths. Further, visualize the results to compare the effect of varying these parameters. Problem Statement 1. Implement a function `perform_kernel_density_estimation` that: - Takes as input a 2D numpy array `X` of shape (n_samples, 2), a list of kernel types, and a list of bandwidth values. - Fits a `KernelDensity` model for each combination of kernel type and bandwidth. - Returns a dictionary where keys are tuples `(kernel, bandwidth)` and values are the log-density estimates for each combination. 2. Implement a function `plot_density_estimations` to visualize the density estimates: - Takes as input the dictionary returned from `perform_kernel_density_estimation`. - Plots subplots comparing density estimates for different kernels and bandwidths. Input Format 1. `X` (numpy array): 2D array of shape (n_samples, 2) representing the data points. 2. `kernels` (list of strings): List of kernel types to be used (\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'). 3. `bandwidths` (list of floats): List of bandwidth values to be used. Output Format 1. The output of `perform_kernel_density_estimation` should be a dictionary where each key is a tuple `(kernel, bandwidth)` and the corresponding value is a numpy array of log-density estimates. 2. The function `plot_density_estimations` should generate a plot comparing the density estimates for different kernel and bandwidth combinations. # Function Signatures ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def perform_kernel_density_estimation(X: np.ndarray, kernels: list, bandwidths: list) -> dict: Perform kernel density estimation for different kernels and bandwidths. Parameters: X (np.ndarray): Input data of shape (n_samples, 2). kernels (list): List of kernel types to use. bandwidths (list): List of bandwidth values to use. Returns: dict: A dictionary where keys are tuples (kernel, bandwidth) and values are log-density estimates. pass # Implement this function def plot_density_estimations(density_estimations: dict): Plot density estimations for various kernels and bandwidths. Parameters: density_estimations (dict): Dictionary with keys (kernel, bandwidth) and values as density estimates. pass # Implement this function ``` # Constraints - The input data array `X` will contain at least 100 samples with random variations for testing purposes. - The bandwidth values will be positive floats. - Visualization should be clear and comprehensible, highlighting the differences caused by kernels and bandwidths. Example Usage ```python X = np.random.rand(100, 2) # Example data kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1.0] density_estimations = perform_kernel_density_estimation(X, kernels, bandwidths) plot_density_estimations(density_estimations) ``` This question assesses the students\' ability to implement and understand KDE, apply different kernels and bandwidths, and interpret the results visually.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def perform_kernel_density_estimation(X: np.ndarray, kernels: list, bandwidths: list) -> dict: Perform kernel density estimation for different kernels and bandwidths. Parameters: X (np.ndarray): Input data of shape (n_samples, 2). kernels (list): List of kernel types to use. bandwidths (list): List of bandwidth values to use. Returns: dict: A dictionary where keys are tuples (kernel, bandwidth) and values are log-density estimates. density_estimations = {} for kernel in kernels: for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) log_density = kde.score_samples(X) density_estimations[(kernel, bandwidth)] = log_density return density_estimations def plot_density_estimations(density_estimations: dict): Plot density estimations for various kernels and bandwidths. Parameters: density_estimations (dict): Dictionary with keys (kernel, bandwidth) and values as density estimates. n_plots = len(density_estimations) fig, axs = plt.subplots(1, n_plots, figsize=(15, 5), sharex=True, sharey=True) if n_plots == 1: axs = [axs] x_vals = np.linspace(min(density_estimations.values())[0], max(density_estimations.values())[0], 100) for ax, ((kernel, bandwidth), log_density) in zip(axs, density_estimations.items()): ax.plot(x_vals, np.exp(log_density)) ax.set_title(f\'Kernel: {kernel}, Bandwidth: {bandwidth}\') ax.set_xlabel(\'X-axis\') ax.set_ylabel(\'Density\') plt.tight_layout() plt.show()"},{"question":"**Objective:** This task aims to test your understanding of seaborn\'s palette customization functionalities. **Question:** A data visualization project requires you to create a set of specific color palettes for various charts to ensure a consistent and visually appealing presentation of data. Your task is to implement a function that returns a customized color palette using seaborn\'s `sns.hls_palette` function based on given parameters. **Function Signature:** ```python def create_custom_palette(number_of_colors: int, lightness: float, saturation: float, hue_start: float, as_cmap: bool = False): pass ``` **Input:** 1. `number_of_colors` (int): The number of colors in the palette. 2. `lightness` (float): The lightness of the colors, must be between 0 and 1. 3. `saturation` (float): The saturation of the colors, must be between 0 and 1. 4. `hue_start` (float): The start-point for hue sampling, must be between 0 and 1. 5. `as_cmap` (bool, optional): Whether to return the palette as a continuous colormap. Default is `False`. **Output:** - Returns a seaborn color palette or colormap based on the provided inputs. **Constraints:** - `number_of_colors` must be a positive integer. - `lightness`, `saturation`, and `hue_start` must be floats between 0 and 1 inclusive. **Example:** ```python # Example 1: palette = create_custom_palette(8, 0.6, 0.85, 0.2) # This should create a palette with 8 colors, lightness 0.6, saturation 0.85, and hue starting at 0.2. # Example 2: colormap = create_custom_palette(10, 0.5, 0.9, 0.4, as_cmap=True) # This should create a continuous colormap with 10 colors, lightness 0.5, saturation 0.9, and hue starting at 0.4. ``` **Note:** - You do not have to visualize the palette. Only ensure your function returns it correctly. - Make sure to handle invalid inputs by raising appropriate exceptions. Good luck!","solution":"import seaborn as sns def create_custom_palette(number_of_colors: int, lightness: float, saturation: float, hue_start: float, as_cmap: bool = False): Creates a custom color palette using seaborn\'s hls_palette function. Parameters: - number_of_colors (int): The number of colors in the palette. - lightness (float): The lightness of the colors, must be between 0 and 1. - saturation (float): The saturation of the colors, must be between 0 and 1. - hue_start (float): The start-point for hue sampling, must be between 0 and 1. - as_cmap (bool, optional): Whether to return the palette as a continuous colormap. Default is False. Returns: - A seaborn color palette or colormap based on the provided inputs. if not (0 <= lightness <= 1 and 0 <= saturation <= 1 and 0 <= hue_start <= 1): raise ValueError(\\"lightness, saturation, and hue_start must be between 0 and 1 inclusive.\\") if number_of_colors <= 0: raise ValueError(\\"number_of_colors must be a positive integer.\\") palette = sns.hls_palette(n_colors=number_of_colors, l=lightness, s=saturation, h=hue_start) if as_cmap: return sns.color_palette(palette, n_colors=number_of_colors).as_hex() return palette"},{"question":"Objective: Demonstrate knowledge of the Python `warnings` module by creating a function that issues warnings and uses warning filters and context managers appropriately. Problem Statement: Create a Python function `process_data(data, deprecated_threshold)` that processes a given list of integers `data`. The function will: 1. Raise a custom warning `DeprecatedThresholdWarning` when any integer in `data` exceeds the `deprecated_threshold`. 2. Summarize the count of warnings issued during the function execution. 3. Suppress these warnings temporarily while still counting them. 4. Return the sum of all integers in `data`. Additionally, provide the custom warning category `DeprecatedThresholdWarning` as a subclass of `Warning`. Requirements: 1. Define the custom warning category `DeprecatedThresholdWarning`. 2. Use the `warn()` function to issue the custom warning. 3. Suppress the warnings using the `catch_warnings()` context manager while counting them. 4. The function should return two values: the sum of the integers in `data` and the count of the custom warnings issued. Function Signature: ```python def process_data(data: list[int], deprecated_threshold: int) -> tuple[int, int]: pass ``` Example: ```python result = process_data([1, 5, 10, 14], 8) print(result) # Output: (30, 2) ``` In this example, the function processes the list `[1, 5, 10, 14]` with a `deprecated_threshold` of 8. It should issue two warnings for the values `10` and `14`, but these warnings should be suppressed, and the function should return the sum of the list (30) and the count of warnings (2). Constraints: - The length of `data` will not exceed 10,000 elements. - Each integer in `data` will be between `-10**6` and `10**6`. - The `deprecated_threshold` will be between `-10**6` and `10**6`. Notes: - Ensure the custom warning category and the function handle large datasets efficiently. - Use appropriate Python practices for handling warnings and exceptions.","solution":"import warnings class DeprecatedThresholdWarning(Warning): Custom warning raised when a value in the data exceeds the deprecated threshold. pass def process_data(data, deprecated_threshold): Processes a list of integers, issuing a custom warning if any integer exceeds the deprecated threshold. Args: - data: List of integers to be processed. - deprecated_threshold: An integer threshold for issuing warnings. Returns: - A tuple containing the sum of the integers in the data and the count of custom warnings issued. # Initialize count of warnings warning_count = 0 def warning_on_dtype_exceed(value): Issues a warning if value exceeds the deprecated threshold. nonlocal warning_count if value > deprecated_threshold: warnings.warn(f\\"Value {value} exceeds deprecated threshold {deprecated_threshold}\\", DeprecatedThresholdWarning) warning_count += 1 # Suppress warnings while still counting them with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\", DeprecatedThresholdWarning) for value in data: warning_on_dtype_exceed(value) return sum(data), warning_count"},{"question":"# Multi-Component Digital Library Management System **Objective:** Develop a Python library management system using object-oriented principles, which involves defining several classes with specific relationships and functionalities. Your task is to implement classes that represent books, users, and a library system. Additionally, the implementation should also include custom iterators to navigate through the items in the library. **Problem Statement:** 1. **Class Definitions:** - **Book:** - Attributes: `title`, `author`, `year`, `isbn` - Methods: - `__init__(self, title, author, year, isbn)`: Initializes the book instance. - `__str__(self)`: Returns a string representation of the book. - **User:** - Attributes: `name`, `user_id` - Methods: - `__init__(self, name, user_id)`: Initializes the user instance. - `__str__(self)`: Returns a string representation of the user. - **Library:** - Attributes: `books` (a list of `Book` objects), `users` (a list of `User` objects) - Methods: - `__init__(self)`: Initializes the library with empty `books` and `users` lists. - `add_book(self, book)`: Adds a `Book` object to the library. - `add_user(self, user)`: Adds a `User` object to the library. - `__iter__(self)`: Returns an iterator that iterates over the books in the library. - `iterate_users(self)`: Returns an iterator that iterates over the users in the library. **Requirements:** - Implement the `Book` and `User` classes with simple `__str__` methods to display relevant information. - Implement the `Library` class, ensuring it can store books and users, and provide iterators for both collections. - Define your custom iterator class for iterating over the library\'s books and users. **Example Usage:** ```python # Creating book instances book1 = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", 1925, \\"1234567890\\") book2 = Book(\\"1984\\", \\"George Orwell\\", 1949, \\"0987654321\\") # Creating user instances user1 = User(\\"Alice\\", 1) user2 = User(\\"Bob\\", 2) # Creating a library instance library = Library() # Adding books and users to the library library.add_book(book1) library.add_book(book2) library.add_user(user1) library.add_user(user2) # Iterating over books in the library for book in library: print(book) # Iterating over users in the library for user in library.iterate_users(): print(user) ``` # Constraints: 1. You must implement classes and methods as described, ensuring correct usage of Python class features. 2. Design your custom iterators; using built-in iterators is allowed but custom implementation will demonstrate a deeper understanding. # Performance Requirements: - Efficiently handle the addition and iteration of books and users.","solution":"class Book: def __init__(self, title, author, year, isbn): self.title = title self.author = author self.year = year self.isbn = isbn def __str__(self): return f\\"\'{self.title}\' by {self.author} ({self.year}), ISBN: {self.isbn}\\" class User: def __init__(self, name, user_id): self.name = name self.user_id = user_id def __str__(self): return f\\"User {self.user_id}: {self.name}\\" class Library: def __init__(self): self.books = [] self.users = [] def add_book(self, book): self.books.append(book) def add_user(self, user): self.users.append(user) def __iter__(self): return iter(self.books) def iterate_users(self): return iter(self.users)"},{"question":"# Question: Tensor Reshaping and Dimension Validation using PyTorch You are tasked with implementing a function that reshapes a tensor and then verifies its dimensions using PyTorch\'s `torch.Size`. The function should take an input tensor, a target shape, and return a tuple containing the reshaped tensor and a boolean value indicating if the reshaped tensor\'s dimensions match the target shape. Function Signature ```python def reshape_and_validate(tensor: torch.Tensor, target_shape: tuple) -> (torch.Tensor, bool): pass ``` Input - `tensor`: A PyTorch tensor of any shape. - `target_shape`: A tuple representing the desired target shape. Output - A tuple: - First element: The reshaped tensor. - Second element: A boolean indicating whether the reshaped tensor\'s dimensions match the target shape. Constraints - The total number of elements in the original tensor must match the total number of elements in the target shape. Example ```python import torch # Example tensor tensor = torch.ones(2, 3, 4) # Target shape target_shape = (4, 6) # Expected reshaped tensor reshaped_tensor, is_valid = reshape_and_validate(tensor, target_shape) # reshaped_tensor should be a tensor of shape (4, 6) # is_valid should be True as tensor reshaped to (4, 6) matches target_shape print(reshaped_tensor.size()) # Output: torch.Size([4, 6]) print(is_valid) # Output: True ``` Important Note Make sure to handle any potential errors that may occur during tensor reshaping, such as mismatched number of elements between original and target shapes.","solution":"import torch def reshape_and_validate(tensor: torch.Tensor, target_shape: tuple) -> (torch.Tensor, bool): Reshapes the tensor to the target shape and validates its dimensions. Parameters: tensor (torch.Tensor): The input tensor. target_shape (tuple): The target shape. Returns: tuple: A tuple containing the reshaped tensor and a boolean indicating if the reshaped tensor\'s dimensions match the target shape. try: reshaped_tensor = tensor.view(target_shape) is_valid = reshaped_tensor.size() == torch.Size(target_shape) return reshaped_tensor, is_valid except RuntimeError: # Return the original tensor and False if reshaping is not possible return tensor, False"},{"question":"<|Analysis Begin|> The documentation provided is for the `optparse` module in Python, which is a deprecated module used for parsing command-line options. Despite its deprecation (in favor of `argparse`), it still provides a robust and practical way to handle command-line arguments. The `optparse` module allows the user to: - Define command-line options both with short (`-f`) and long (`--file`) formats. - Automatically generate help and usage messages. - Parse and store option values from the command line. - Handle different types of command-line arguments including integers, strings, and custom types. - Handle mutually exclusive options and required options, although with some limitations. Key components for creating an assessment question include: 1. Creating an `OptionParser` instance. 2. Adding options with different types and actions. 3. Parsing command-line arguments. 4. Handling errors and exceptions related to improper command-line arguments. 5. Generating help messages. Given this understanding, a suitable assessment question should focus on creating a complex command-line interface using `optparse` that can validate multiple types of inputs and generate useful help messages. <|Analysis End|> <|Question Begin|> You are tasked with writing a Python script using the `optparse` module to create a command-line application called `file_processor.py`. This application should process command-line options to perform simple file manipulations. The script must handle the following options: 1. `-i` or `--input`: Specifies an input file (mandatory). 2. `-o` or `--output`: Specifies an output file (optional). 3. `-v` or `--verbose`: Enables verbose mode (optional, default is `False`). 4. `--max-lines`: An integer specifying the maximum number of lines to process (optional). 5. `--uppercase`: A boolean flag indicating that the output should be in uppercase (optional). Your script should: - Parse and validate these options. - Print proper error messages if mandatory options are missing or if invalid values are provided. - Print a help message explaining the usage of your script when `-h` or `--help` is passed. - In verbose mode, print the progress of processing the input file. Here is a template structure for your script; you need to complete the implementation: ```python from optparse import OptionParser def main(): # Create the parser and define usage message usage = \\"usage: %prog -i INPUT_FILE [options]\\" parser = OptionParser(usage=usage) # Add options parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input_file\\", help=\\"Specify input file\\", metavar=\\"INPUT_FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output_file\\", help=\\"Specify output file\\", metavar=\\"OUTPUT_FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"Enable verbose mode [default: %default]\\") parser.add_option(\\"--max-lines\\", dest=\\"max_lines\\", type=\\"int\\", help=\\"Specify maximum number of lines to process\\", metavar=\\"MAX_LINES\\") parser.add_option(\\"--uppercase\\", action=\\"store_true\\", dest=\\"uppercase\\", default=False, help=\\"Convert output to uppercase\\") # Parse the command-line arguments (options, args) = parser.parse_args() # Validate mandatory options if not options.input_file: parser.error(\\"Input file (-i or --input) is mandatory\\") # Your code to process the input file based on the provided options # (Output file, verbose mode, max lines, uppercase conversion) if options.verbose: print(f\\"Processing file: {options.input_file}\\") if options.output_file: print(f\\"Output will be written to: {options.output_file}\\") if options.max_lines: print(f\\"Maximum lines to process: {options.max_lines}\\") if options.uppercase: print(\\"Converting output to uppercase\\") # Add logic to read from input file, process lines, and write to output file if specified # ... if __name__ == \\"__main__\\": main() ``` Complete the implementation of this script to handle file processing as described. Include proper error handling and verbose output as required.","solution":"from optparse import OptionParser def process_file(input_file, output_file=None, max_lines=None, verbose=False, uppercase=False): try: with open(input_file, \'r\') as infile: lines = infile.readlines() except IOError as e: raise SystemExit(f\\"Error opening input file: {e}\\") if max_lines is not None: lines = lines[:max_lines] if uppercase: lines = [line.upper() for line in lines] if output_file: try: with open(output_file, \'w\') as outfile: outfile.writelines(lines) except IOError as e: raise SystemExit(f\\"Error opening output file: {e}\\") else: for line in lines: print(line, end=\'\') if verbose: print(f\\"Processed {len(lines)} lines\\") def main(): # Create the parser and define usage message usage = \\"usage: %prog -i INPUT_FILE [options]\\" parser = OptionParser(usage=usage) # Add options parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input_file\\", help=\\"Specify input file\\", metavar=\\"INPUT_FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output_file\\", help=\\"Specify output file\\", metavar=\\"OUTPUT_FILE\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", default=False, help=\\"Enable verbose mode [default: %default]\\") parser.add_option(\\"--max-lines\\", dest=\\"max_lines\\", type=\\"int\\", help=\\"Specify maximum number of lines to process\\", metavar=\\"MAX_LINES\\") parser.add_option(\\"--uppercase\\", action=\\"store_true\\", dest=\\"uppercase\\", default=False, help=\\"Convert output to uppercase\\") # Parse the command-line arguments (options, args) = parser.parse_args() # Validate mandatory options if not options.input_file: parser.error(\\"Input file (-i or --input) is mandatory\\") # Process file process_file( input_file=options.input_file, output_file=options.output_file, max_lines=options.max_lines, verbose=options.verbose, uppercase=options.uppercase ) if __name__ == \\"__main__\\": main()"},{"question":"As a Python developer, it is essential to understand and manage the metadata of installed packages, especially when working with multiple dependencies. In this task, you are required to use the `importlib.metadata` module to gather and display comprehensive information about a specific installed package. # Task: Write a Python function `package_info(package_name: str) -> dict` that takes the name of an installed package as input and returns a dictionary containing detailed metadata about the package. Your function should retrieve and include the following information: 1. `version`: The version of the package. 2. `requires`: A list of requirements for the package. 3. `entry_points`: A dictionary where keys are entry point groups (e.g., `console_scripts`) and values are lists of entry point names in those groups. 4. `files`: A list of the relative path strings of the files installed by the package. 5. `metadata`: A dictionary containing all the available metadata fields and their corresponding values. The structure of the output dictionary should be as follows: ```python { \\"version\\": \\"package_version\\", \\"requires\\": [\\"requirement1\\", \\"requirement2\\", ...], \\"entry_points\\": { \\"group1\\": [\\"entry_point1\\", \\"entry_point2\\", ...], \\"group2\\": [\\"entry_point3\\", \\"entry_point4\\", ...], ... }, \\"files\\": [\\"file_path1\\", \\"file_path2\\", ...], \\"metadata\\": { \\"Metadata-Version\\": \\"value\\", \\"Name\\": \\"value\\", \\"Version\\": \\"value\\", ... } } ``` # Constraints: - You must handle the case where the package is not found and return an appropriate message in such cases. - Assume the function will only run on Python 3.10 and later. # Example: Given the installed package `wheel`, calling the function should return a dictionary with the corresponding metadata details: ```python result = package_info(\'wheel\') print(result) ``` # Performance Requirements: Your function should efficiently handle the retrieval of metadata and entry points without causing significant delays, even for packages with extensive metadata and many entry points. # Submission: Submit the function `package_info` implemented in a `.py` file. Ensure your code is clean, well-documented, and follows best coding practices.","solution":"import importlib.metadata def package_info(package_name: str) -> dict: try: dist = importlib.metadata.distribution(package_name) version = dist.version requires = dist.requires or [] entry_points = {} if dist.entry_points: for entry_point in dist.entry_points: if entry_point.group not in entry_points: entry_points[entry_point.group] = [] entry_points[entry_point.group].append(entry_point.name) files = dist.files or [] files = [str(file) for file in files] metadata = dict(dist.metadata.items()) return { \\"version\\": version, \\"requires\\": requires, \\"entry_points\\": entry_points, \\"files\\": files, \\"metadata\\": metadata } except importlib.metadata.PackageNotFoundError: return {\\"error\\": f\\"Package {package_name} not found\\"}"},{"question":"# **Coding Assessment Question** **Objective**: Implement a custom descriptor for managing and validating attributes within a class. This exercise requires understanding and application of descriptors in Python, focusing on dynamic lookups, attribute management, and validation mechanisms. # **Problem Statement** You are required to create a descriptor class `ValidatingDescriptor` that will dynamically compute the value of an attribute and validate it before assigning. Following this, create a class `Student` that uses this descriptor to manage its attributes `name` and `score`. **Descriptor Class Requirements:** 1. **Class `ValidatingDescriptor`**: - Implement the descriptor methods `__get__`, `__set__`, and `__set_name__`. - Store the name of the attribute it manages. - The `__get__` method should dynamically return the value based on computations or transformations. - The `__set__` method should validate the value before setting it. - Raise appropriate exceptions for invalid values: - `name`: A string that must be non-empty and less than 30 characters. - `score`: An integer that must be between 0 and 100. **Student Class Requirements:** 2. **Class `Student`**: - Has two attributes `name` and `score` using `ValidatingDescriptor`. - Initialize the attributes in the constructor. # **Expected Input and Output** **Input:** - A `Student` object instantiated with `name` and `score`. **Output:** - Any invalid conditions should raise an appropriate exception with a clear message. **Example Input and Output:** ```python try: student = Student(\\"John Doe\\", 85) print(f\\"Name: {student.name}, Score: {student.score}\\") # Should print: Name: John Doe, Score: 85 student.score = 150 except Exception as e: print(e) # Should print: ValueError: Score must be between 0 and 100 ``` # **Constraints and Performance Requirements:** - Ensure the descriptor provides clear and specific error messages for invalid inputs. - Descriptor should handle attribute validation efficiently. - Bonus: Use built-in Python logging to log whenever an attribute is accessed or modified. # **Implementation Details:** You need to implement the following classes: ```python class ValidatingDescriptor: def __set_name__(self, owner, name): pass def __get__(self, obj, objtype=None): pass def __set__(self, obj, value): pass class Student: name = ValidatingDescriptor() score = ValidatingDescriptor() def __init__(self, name, score): pass ``` Ensure that you follow the above specifications and constraints.","solution":"class ValidatingDescriptor: def __init__(self): self.attribute_name = None def __set_name__(self, owner, name): self.attribute_name = name def __get__(self, obj, objtype=None): return obj.__dict__.get(self.attribute_name, None) def __set__(self, obj, value): self.validate(value) obj.__dict__[self.attribute_name] = value def validate(self, value): if self.attribute_name == \\"name\\": if not isinstance(value, str) or not value or len(value) >= 30: raise ValueError(\\"Name must be a non-empty string less than 30 characters.\\") elif self.attribute_name == \\"score\\": if not isinstance(value, int) or not (0 <= value <= 100): raise ValueError(\\"Score must be an integer between 0 and 100.\\") class Student: name = ValidatingDescriptor() score = ValidatingDescriptor() def __init__(self, name, score): self.name = name self.score = score"},{"question":"# Calendar Event Notifier **Objective**: Write a Python program to find all weekends in a given month and determine if any public holidays fall on those weekends. Background You are hired to develop a feature for a calendar application that can notify users if any public holidays fall on weekends. The data provided will include a list of public holidays, each represented as a `datetime.date` object. You will utilize the `calendar` module to solve this problem. Inputs and Outputs - **Input**: - An integer `year` (e.g., 2023). - An integer `month` (from 1 to 12). - A list of `datetime.date` objects representing public holidays for that year. - **Output**: - A list of strings where each string is in the format \\"Holiday Name on YYYY-MM-DD\\", representing the holidays that fall on weekends in the given month. Constraints 1. You can assume the list of public holidays will always be within the same year as the input year. 2. The month value will be a valid integer between 1 and 12. 3. The year value will be a positive integer. Implementation Details 1. **Function Name**: `find_holidays_on_weekends` 2. **Parameters**: - `year` (int): the year of the calendar. - `month` (int): the month of the calendar. - `holidays` (list): a list of tuples, each containing a `datetime.date` object and a string representing the holiday name. 3. **Returns**: - A list of strings in the format \\"Holiday Name on YYYY-MM-DD\\". Example ```python from datetime import date year = 2023 month = 5 holidays = [(date(2023, 5, 1), \\"Labor Day\\"), (date(2023, 5, 20), \\"Sample Holiday\\"), (date(2023, 5, 25), \\"Another Holiday\\")] result = find_holidays_on_weekends(year, month, holidays) print(result) # Output: [\\"Sample Holiday on 2023-05-20\\"] ``` Hints - Use the `calendar.Calendar` class to iterate over the days in the given month. - Check each day if it is a Saturday (5) or Sunday (6). - Compare each weekend date with the list of holidays. Good Luck!","solution":"from datetime import date import calendar def find_holidays_on_weekends(year, month, holidays): Finds and returns a list of strings representing holidays that fall on weekends in a given month. Parameters: - year (int): The year of the calendar. - month (int): The month of the calendar. - holidays (list): List of tuples, each containing a datetime.date object and a string representing the holiday name. Returns: - List of strings in the format \\"Holiday Name on YYYY-MM-DD\\". weekend_holidays = [] cal = calendar.Calendar() for day in cal.itermonthdates(year, month): if day.month == month and day.weekday() in {calendar.SATURDAY, calendar.SUNDAY}: for holiday_date, holiday_name in holidays: if day == holiday_date: weekend_holidays.append(f\\"{holiday_name} on {day}\\") return weekend_holidays"},{"question":"Coding Assessment Question **Objective**: Test the comprehension and application of `unittest.mock` library, including advanced features like `patch`, `MagicMock`, auto-speccing, and handling side effects. # Question You are tasked to write a test for a function in a production class that interacts with an external API service. Below is the definition of the `ApiService` class and the `calculate_sum_from_service` method. ```python class ApiService: def fetch_data(self, identifier): # Imagine this method makes an HTTP request to an external service # and returns a dictionary with keys \'value1\' and \'value2\'. pass class CalculationProcessor: def __init__(self, api_service): self.api_service = api_service def calculate_sum_from_service(self, identifier): data = self.api_service.fetch_data(identifier) return data[\'value1\'] + data[\'value2\'] ``` # Task Write a complete `unittest.TestCase` for the `calculate_sum_from_service` method ensuring the following: 1. Use the `unittest.mock` library to mock the `fetch_data` method. 2. Configure the mock to return a specific dictionary when called. 3. Make assertions on how the mock was used, and confirm the correctness of the sum calculation. Expected Input/Output - Mock the `fetch_data` method to return `{\'value1\': 10, \'value2\': 20}` when called with any identifier. - Assert that the `fetch_data` method was called exactly once with the provided identifier. - Assert that the return value of `calculate_sum_from_service` with the provided identifier is `30`. Constraints - Use `MagicMock` for mocking the methods. - Use the `patch` function to handle the mocking inside the test case. - Ensure that additional checks or edge cases do not make HTTP requests or similar interactions. Performance Requirements - Ensure that the test runs efficiently and does not rely on actual network calls. - The test should be independent of external systems, relying only on mocks. Example Usage ```python import unittest from unittest.mock import MagicMock, patch class TestCalculationProcessor(unittest.TestCase): @patch(\'__main__.ApiService.fetch_data\') def test_calculate_sum_from_service(self, mock_fetch_data): # Configuration of the mock mock_fetch_data.return_value = {\'value1\': 10, \'value2\': 20} # Instance of the class under test api_service = ApiService() processor = CalculationProcessor(api_service) # Call the method and make assertions result = processor.calculate_sum_from_service(\'some_identifier\') mock_fetch_data.assert_called_once_with(\'some_identifier\') self.assertEqual(result, 30) if __name__ == \'__main__\': unittest.main() ``` Write the described test case in a script file and ensure it passes when executed.","solution":"import unittest from unittest.mock import MagicMock, patch class ApiService: def fetch_data(self, identifier): # Imagine this method makes an HTTP request to an external service # and returns a dictionary with keys \'value1\' and \'value2\'. pass class CalculationProcessor: def __init__(self, api_service): self.api_service = api_service def calculate_sum_from_service(self, identifier): data = self.api_service.fetch_data(identifier) return data[\'value1\'] + data[\'value2\']"},{"question":"# Python Dictionary Manipulation by Simulating C API Functions In this task, you will write a function in Python to manipulate a dictionary in a way that simulates some of the behaviors described by the provided C API functions. Your code should enforce certain constraints and demonstrate understanding of dictionary operations and error handling. Requirements 1. **Function Signature:** ```python def manip_dict(operations: list) -> dict: ``` 2. **Parameters:** - `operations`: A list of tuples, where each tuple represents an operation on the dictionary. The tuple\'s first element is the operation name (as a string), followed by the necessary parameters for that operation. 3. **Operations:** Implement the following operations based on the provided C API functionalities: - `CREATE`: No additional parameters. Creates a new empty dictionary. - `SET`: Two additional parameters, `key` and `value`. Sets the given key-value pair in the dictionary. - `DELETE`: One additional parameter, `key`. Deletes the given key from the dictionary. - `CHECK`: One additional parameter, `key`. Returns `True` if the key is in the dictionary, otherwise `False`. - `COPY`: No additional parameters. Returns a copy of the current dictionary. - `UPDATE`: One additional parameter, `new_dict`. Updates the dictionary with the key-value pairs from `new_dict`. - `SIZE`: No additional parameters. Returns the size (number of items) of the dictionary. 4. **Returns:** - The final state of the dictionary after performing all operations. 5. **Example:** ```python ops = [ (\\"CREATE\\",), (\\"SET\\", \\"foo\\", 42), (\\"SET\\", \\"bar\\", \\"baz\\"), (\\"CHECK\\", \\"foo\\"), (\\"DELETE\\", \\"bar\\"), (\\"SIZE\\",), (\\"COPY\\",), (\\"UPDATE\\", {\\"xyz\\": 100, \\"foo\\": 50}), ] result = manip_dict(ops) print(result) # Expected output: {\'foo\': 50, \'xyz\': 100} ``` Constraints: - Keys in the dictionary will always be string types. - Values can be of any type. - The function should handle exceptions appropriately, raising errors when operations are invalid (e.g., deleting a non-existent key). Implement the `manip_dict` function: ```python def manip_dict(operations: list) -> dict: current_dict = {} for op in operations: if op[0] == \\"CREATE\\": current_dict = {} elif op[0] == \\"SET\\": key, value = op[1], op[2] current_dict[key] = value elif op[0] == \\"DELETE\\": key = op[1] if key in current_dict: del current_dict[key] else: raise KeyError(f\\"Key \'{key}\' not found in dictionary.\\") elif op[0] == \\"CHECK\\": key = op[1] print(key in current_dict) elif op[0] == \\"COPY\\": print(current_dict.copy()) elif op[0] == \\"UPDATE\\": new_dict = op[1] current_dict.update(new_dict) elif op[0] == \\"SIZE\\": print(len(current_dict)) else: raise ValueError(f\\"Unknown operation \'{op[0]}\'.\\") return current_dict ``` Ensure to test the function thoroughly with various combinations of operations.","solution":"def manip_dict(operations: list) -> dict: current_dict = {} for op in operations: if op[0] == \\"CREATE\\": current_dict = {} elif op[0] == \\"SET\\": key, value = op[1], op[2] current_dict[key] = value elif op[0] == \\"DELETE\\": key = op[1] if key in current_dict: del current_dict[key] else: raise KeyError(f\\"Key \'{key}\' not found in dictionary.\\") elif op[0] == \\"CHECK\\": key = op[1] print(key in current_dict) elif op[0] == \\"COPY\\": print(current_dict.copy()) elif op[0] == \\"UPDATE\\": new_dict = op[1] current_dict.update(new_dict) elif op[0] == \\"SIZE\\": print(len(current_dict)) else: raise ValueError(f\\"Unknown operation \'{op[0]}\'.\\") return current_dict"},{"question":"# Question: **Title: Implementing a Custom Data Structure** Objective: Create a custom data structure that combines the functionality of various built-in types, including lists, dictionaries, and sets. The goal is to write methods that perform specific operations involving these types. Problem Statement: You need to implement a class `CustomDataStructure` that allows for the following functionalities: 1. **Initialization:** - The class should be initialized with a list of integers and a dictionary where the keys are strings and the values are integers. 2. **Add Elements:** - Implement a method `add_elements` that takes a list of integers and a dictionary (string keys and integer values). This method should add the integers to the internal list and update the internal dictionary with the new key-value pairs. If a key already exists, increment its value by the provided value. 3. **Range Sum:** - Implement a method `range_sum` that takes a starting and ending integer and calculates the sum of all integers within the internal list that fall within this range (inclusive). 4. **Unique Key Check:** - Implement a method `unique_key_check` that takes a set of strings and returns a dictionary where the keys are the strings from the set and the values are Booleans indicating whether each string exists as a key in the internal dictionary. 5. **Get Summary:** - Implement a method `get_summary` that returns a tuple with two elements: 1. A sorted list of unique integers from the internal list. 2. A frozenset containing all keys from the internal dictionary. Constraints: - The list of integers will contain at most 10^5 elements. - The dictionary will have at most 10^4 entries. - The integers in the list and dictionary values will be between -10^9 and 10^9. Example Usage: ```python # Initialize the data structure data_structure = CustomDataStructure([1, 2, 3], {\\"a\\": 10, \\"b\\": 20}) # Add more elements data_structure.add_elements([4, 5], {\\"a\\": 5, \\"c\\": 30}) # Calculate the range sum range_sum = data_structure.range_sum(2, 5) # Sum of 2, 3, 4, 5 # Check for unique keys unique_check = data_structure.unique_key_check({\\"a\\", \\"b\\", \\"d\\"}) # {\\"a\\": True, \\"b\\": True, \\"d\\": False} # Get summary summary = data_structure.get_summary() # ([1, 2, 3, 4, 5], frozenset({\\"a\\", \\"b\\", \\"c\\"})) ``` Implementation Guidelines: - Define the `CustomDataStructure` class with appropriate methods. - Ensure efficiency in operations, especially when dealing with large lists and dictionaries. - Write clean, readable code with comments explaining your logic. - Handle edge cases such as empty inputs or invalid ranges. Submission: Submit a Python file named `custom_data_structure.py` containing the implementation of the `CustomDataStructure` class with all the required methods.","solution":"class CustomDataStructure: def __init__(self, int_list, str_dict): Initializes the data structure with a list of integers and a dictionary. self.int_list = int_list self.str_dict = str_dict def add_elements(self, int_list, str_dict): Adds integers to the internal list and updates the internal dictionary with new key-value pairs. # Add integers to the internal list self.int_list.extend(int_list) # Update the internal dictionary for key, value in str_dict.items(): if key in self.str_dict: self.str_dict[key] += value else: self.str_dict[key] = value def range_sum(self, start, end): Calculates the sum of all integers within the internal list that fall within the range [start, end]. return sum(x for x in self.int_list if start <= x <= end) def unique_key_check(self, keys_set): Returns a dictionary indicating whether each string in the set exists as a key in the internal dictionary. result = {} for key in keys_set: result[key] = key in self.str_dict return result def get_summary(self): Returns a tuple with a sorted list of unique integers from the internal list and a frozenset of dictionary keys. sorted_unique_list = sorted(set(self.int_list)) dictionary_keys = frozenset(self.str_dict.keys()) return (sorted_unique_list, dictionary_keys)"},{"question":"Handling MIME Type Command Execution Problem Statement You are given a deprecated module called `mailcap` which provides functionality for handling MIME type configurations often used in email clients and web browsers. Your task is to use this module to retrieve the mailcap entries from the system and to implement a secure function to find and execute a viewer command for a given MIME type. # Requirements: 1. **Function Implementation 1: get_mailcap_entries()** - **Input:** None. - **Output:** Returns a dictionary mapping MIME types to their mailcap entries, using the `mailcap.getcaps()` function. 2. **Function Implementation 2: execute_mime_viewer(mime_type: str, filename: str) -> str** - **Input:** - `mime_type` (str): The MIME type for which you need to find a viewer. - `filename` (str): The filename of the file to be viewed. - **Output:** - A string representing the command line that can be executed to view the file using the appropriate application configured in the mailcap file. - If no command is found or invalid characters in the filename make it unsafely executable, return the string `\\"No valid command found\\"`. - **Constraints:** - Ensure that no unsafe shell metacharacters exist in the filename. - Use only the `view` key to get the appropriate command. # Example: ```python import mailcap def get_mailcap_entries(): # Your implementation here pass def execute_mime_viewer(mime_type, filename): # Your implementation here pass # Example usage: entries = get_mailcap_entries() print(execute_mime_viewer(\'video/mpeg\', \'tmp1223\')) # Expected output: \'xmpeg tmp1223\' or \'No valid command found\' if security constraints are violated. print(execute_mime_viewer(\'image/jpeg\', \'imagefile.jpg\')) # Expected output: Corresponding command or \'No valid command found\'. ``` Consider the security measures specified in the `mailcap.findmatch()` documentation while implementing your solution. Your implementation should raise a warning and return `\\"No valid command found\\"` if unsafe characters are found in the filename or other parts of the input. Note: - You will need to validate inputs and handle potential issues as described in the `mailcap.findmatch()` function. - The solution must be compatible with Python 3.10. Submission - Implement the two functions in a Python file and ensure they operate correctly as specified. - Include any test cases you design to demonstrate your solutions correctness.","solution":"import mailcap import re def get_mailcap_entries(): Retrieves the mailcap entries from the system Returns: dict: A dictionary mapping MIME types to their mailcap entries. return mailcap.getcaps() def execute_mime_viewer(mime_type, filename): Finds and returns the viewer command for a given MIME type Args: mime_type (str): The MIME type for which you need to find a viewer. filename (str): The filename of the file to be viewed. Returns: str: A string representing the command line that can be executed to view the file using the appropriate application configured in the mailcap file. If no command found or filename has unsafe characters, returns \\"No valid command found\\". # Check for unsafe characters in the filename unsafe_chars = re.compile(r\'[;&<>`|*?~]\') if unsafe_chars.search(filename): return \\"No valid command found\\" # Retrieve the mailcap entries caps = get_mailcap_entries() # Find the matching command for the mime_type command, _ = mailcap.findmatch(caps, mime_type, \'view\', filename=filename) if command: return command else: return \\"No valid command found\\""},{"question":"# MultiIndex Manipulation and Advanced Indexing in pandas **Objective:** Demonstrate your comprehension of manipulating and performing advanced indexing operations with MultiIndex in pandas. **Task:** 1. Create a `MultiIndex` from the given data, labeling the levels as `\\"Category\\"` and `\\"Type\\"`. 2. Create a `DataFrame` using the MultiIndex where the data is a 2D numpy array of random numbers with 12 rows and 4 columns. 3. Perform the following operations on the `DataFrame`: - Select the sub-DataFrame for `\\"Category\\"` `\\"A\\"` only. - Select the sub-DataFrame for `\\"Type\\"` `\\"one\\"` across all categories. - Create a new `DataFrame` by reordering the levels of the `MultiIndex`. - Sort the new `DataFrame` by `\\"Type\\"` and then by `\\"Category\\"`. **Input:** ```python categories = [\\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\", \\"C\\", \\"D\\", \\"D\\", \\"D\\"] types = [\\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\"] ``` **Expected Output:** 1. Original multi-indexed DataFrame creation. 2. Sub-DataFrames based on the specified conditions. 3. Reordered and sorted DataFrame as per the given criteria. **Constraints:** - Use pandas for data manipulation. - You are not allowed to change the structure of the initial MultiIndex created. **Example Code Structure:** ```python import pandas as pd import numpy as np # Data categories = [\\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\", \\"C\\", \\"D\\", \\"D\\", \\"D\\"] types = [\\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\"] # Step 1: Create MultiIndex # Your code here # Step 2: Create DataFrame # Your code here # Step 3: Perform operations Select sub-DataFrame where Category is \\"A\\" # Your code here Select sub-DataFrame where Type is \\"one\\" # Your code here Reorder levels of MultiIndex # Your code here Sort DataFrame by Type and then Category # Your code here ``` **Note:** Ensure that each step is clearly commented and that the output is displayed for review.","solution":"import pandas as pd import numpy as np # Data categories = [\\"A\\", \\"A\\", \\"A\\", \\"B\\", \\"B\\", \\"B\\", \\"C\\", \\"C\\", \\"C\\", \\"D\\", \\"D\\", \\"D\\"] types = [\\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\", \\"one\\", \\"two\\", \\"three\\"] # Step 1: Create MultiIndex index = pd.MultiIndex.from_arrays([categories, types], names=[\\"Category\\", \\"Type\\"]) # Step 2: Create DataFrame data = np.random.randn(12, 4) df = pd.DataFrame(data, index=index, columns=[\\"Column1\\", \\"Column2\\", \\"Column3\\", \\"Column4\\"]) # Step 3: Perform operations Select sub-DataFrame where Category is \\"A\\" sub_df_category_A = df.loc[\\"A\\"] Select sub-DataFrame where Type is \\"one\\" sub_df_type_one = df.xs(\\"one\\", level=\\"Type\\") Reorder levels of MultiIndex reordered_df = df.swaplevel(\\"Category\\", \\"Type\\") Sort DataFrame by Type and then Category sorted_reordered_df = reordered_df.sort_index(level=[\\"Type\\", \\"Category\\"]) # Output variables for validation solution_output = { \\"original_df\\": df, \\"sub_df_category_A\\": sub_df_category_A, \\"sub_df_type_one\\": sub_df_type_one, \\"reordered_df\\": reordered_df, \\"sorted_reordered_df\\": sorted_reordered_df } def get_solution_output(): return solution_output"},{"question":"# Unicode Data Processor You are required to write a Python function `unicode_data_processor` that takes a list of Unicode characters as input and performs the following operations: 1. **Character Information**: - For each character in the list, determine and print the character along with its name, general category, decimal value (if applicable), digit value (if applicable), numeric value (if applicable), bidirectional class, combining class, east asian width, and mirrored property. 2. **Character Decomposition and Normalization**: - For each character, provide its decomposition mapping. - Normalize each character using both \'NFD\' and \'NFC\' forms and check if the normalized character is valid (i.e., whether `unicodedata.is_normalized` returns `True`). 3. **Summary Report**: - At the end, summarize and print the total number of characters processed. - Print the count of unique categories among the input characters. - Print the count of characters that have decimal values, digit values, numeric values, and those that are mirrored. # Function Signature ```python def unicode_data_processor(char_list: list) -> None: pass ``` # Constraints - The input list `char_list` will contain valid Unicode characters. - The length of the input list will not exceed 1000 characters. - You should handle exceptions gracefully, whenever a property is not defined for a character, indicate it properly. # Input - A list of Unicode characters, e.g., [\'A\', \'9\', \'\', \'\', \'\', \'\'] # Output - Detailed information about each character as per the requirements mentioned above. - A summary report at the end. # Example ```python char_list = [\'A\', \'9\', \'\', \'\', \'\', \'\'] unicode_data_processor(char_list) ``` Example output (output format is illustrative): ``` Character: \'A\' Name: LATIN CAPITAL LETTER A Category: Lu Decimal: Undefined Digit: Undefined Numeric: Undefined Bidirectional: L Combining: 0 East Asian Width: W Mirrored: 0 Decomposition: Normalized NFD: A, Valid: True Normalized NFC: A, Valid: True ... Character: \'9\' Name: DIGIT NINE Category: Nd Decimal: 9 Digit: 9 Numeric: 9.0 Bidirectional: EN Combining: 0 East Asian Width: Na Mirrored: 0 Decomposition: Normalized NFD: 9, Valid: True Normalized NFC: 9, Valid: True ... ... Summary: Total characters processed: 6 Unique Categories Count: 4 Characters with Decimal values: 1 Characters with Digit values: 1 Characters with Numeric values: 1 Mirrored characters: 0 ```","solution":"import unicodedata def unicode_data_processor(char_list): total_chars = len(char_list) unique_categories = set() decimal_count = 0 digit_count = 0 numeric_count = 0 mirrored_count = 0 for char in char_list: char_info = f\\"Character: \'{char}\'n\\" # Fetching character properties try: char_name = unicodedata.name(char) except ValueError: char_name = \\"Undefined\\" char_category = unicodedata.category(char) char_bidirectional = unicodedata.bidirectional(char) char_combining = unicodedata.combining(char) char_east_asian_width = unicodedata.east_asian_width(char) char_mirrored = unicodedata.mirrored(char) # Handle undefined values gracefully try: char_decimal = unicodedata.decimal(char) decimal_count += 1 except ValueError: char_decimal = \\"Undefined\\" try: char_digit = unicodedata.digit(char) digit_count += 1 except ValueError: char_digit = \\"Undefined\\" try: char_numeric = unicodedata.numeric(char) numeric_count += 1 except ValueError: char_numeric = \\"Undefined\\" if char_mirrored == 1: mirrored_count += 1 # Decomposition and Normalization char_decomposition = unicodedata.decomposition(char) char_normalized_nfd = unicodedata.normalize(\'NFD\', char) char_normalized_nfc = unicodedata.normalize(\'NFC\', char) is_valid_nfd = unicodedata.is_normalized(\'NFD\', char) is_valid_nfc = unicodedata.is_normalized(\'NFC\', char) char_info += ( f\\"Name: {char_name}n\\" f\\"Category: {char_category}n\\" f\\"Decimal: {char_decimal}n\\" f\\"Digit: {char_digit}n\\" f\\"Numeric: {char_numeric}n\\" f\\"Bidirectional: {char_bidirectional}n\\" f\\"Combining: {char_combining}n\\" f\\"East Asian Width: {char_east_asian_width}n\\" f\\"Mirrored: {char_mirrored}n\\" f\\"Decomposition: {char_decomposition}n\\" f\\"Normalized NFD: {char_normalized_nfd}, Valid: {is_valid_nfd}n\\" f\\"Normalized NFC: {char_normalized_nfc}, Valid: {is_valid_nfc}n\\" ) print(char_info) unique_categories.add(char_category) # Summary summary = ( f\\"Summary:n\\" f\\"Total characters processed: {total_chars}n\\" f\\"Unique Categories Count: {len(unique_categories)}n\\" f\\"Characters with Decimal values: {decimal_count}n\\" f\\"Characters with Digit values: {digit_count}n\\" f\\"Characters with Numeric values: {numeric_count}n\\" f\\"Mirrored characters: {mirrored_count}n\\" ) print(summary)"},{"question":"**Task: Implement a function that sends an email using the `smtplib` module, with error handling.** # Requirements 1. **Function Name**: `send_email` 2. **Parameters**: - `from_address` (str): The sender\'s email address. - `to_address` (str): The recipient\'s email address. - `subject` (str): The subject of the email. - `body` (str): The body of the email. - `smtp_host` (str): The SMTP server host. - `smtp_port` (int): The SMTP server port. - `username` (str, optional): The username for SMTP authentication. Default is `None`. - `password` (str, optional): The password for SMTP authentication. Default is `None`. - `use_tls` (bool, optional): Whether to use TLS. Default is `False`. 3. **Returns**: - None 4. **Exceptions**: - Raise a `ValueError` if required parameters are missing or invalid. - Handle `smtplib.SMTPException` and print appropriate error messages. # Functionality - The function should construct an email message using the provided parameters. - If `use_tls` is `True`, the function should start a TLS session. - If `username` and `password` are provided, it should login to the SMTP server. - The function should send the email and then close the SMTP connection. - Proper exception handling should be implemented for various `smtplib` exceptions. # Example Usage ```python try: send_email( from_address=\\"example_sender@example.com\\", to_address=\\"example_recipient@example.com\\", subject=\\"Test Email\\", body=\\"This is a test email.\\", smtp_host=\\"smtp.example.com\\", smtp_port=587, username=\\"example_sender@example.com\\", password=\\"password123\\", use_tls=True ) print(\\"Email sent successfully!\\") except ValueError as ve: print(f\\"Value error: {ve}\\") except smtplib.SMTPException as e: print(f\\"SMTP error: {e}\\") except Exception as e: print(f\\"Unexpected error: {e}\\") ``` # Notes - You may assume that `username` and `password` are not necessary for sending emails from localhost. - It\'s important to ensure that all parameters are appropriately validated. - Remember to handle different SMTP-related exceptions gracefully.","solution":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart def send_email(from_address, to_address, subject, body, smtp_host, smtp_port, username=None, password=None, use_tls=False): Sends an email using the smtplib module. Parameters: from_address (str): The sender\'s email address. to_address (str): The recipient\'s email address. subject (str): The subject of the email. body (str): The body of the email. smtp_host (str): The SMTP server host. smtp_port (int): The SMTP server port. username (str, optional): The username for SMTP authentication. Default is None. password (str, optional): The password for SMTP authentication. Default is None. use_tls (bool, optional): Whether to use TLS. Default is False. Returns: None Raises: ValueError: If required parameters are missing or invalid. smtplib.SMTPException: If there is an error during the SMTP operations. # Validate required parameters if not all([from_address, to_address, subject, body, smtp_host, smtp_port]): raise ValueError(\\"One or more required parameters are missing.\\") if not isinstance(smtp_port, int): raise ValueError(\\"SMTP port must be an integer.\\") # Construct the email message msg = MIMEMultipart() msg[\'From\'] = from_address msg[\'To\'] = to_address msg[\'Subject\'] = subject msg.attach(MIMEText(body, \'plain\')) try: # Connect to the SMTP server server = smtplib.SMTP(smtp_host, smtp_port) # Start TLS if required if use_tls: server.starttls() # Login to the server if username and password are provided if username and password: server.login(username, password) # Send the email server.sendmail(from_address, to_address, msg.as_string()) # Close the SMTP connection server.quit() except smtplib.SMTPException as e: print(f\\"SMTP error: {e}\\") except Exception as e: print(f\\"Unexpected error: {e}\\")"},{"question":"**Objective:** Your task is to demonstrate proficiency in using the seaborn library by loading a dataset, visualizing its distribution using various configurations of the seaborn `kdeplot` function, and modifying the plot appearance according to given specifications. **Requirements:** 1. Import the seaborn library and set the seaborn theme. 2. Load the `iris` dataset using seaborn. 3. Create the following visualizations: - A univariate KDE plot of the `sepal_length`. - A bivariate KDE plot of `sepal_length` and `sepal_width`. - A KDE plot of `sepal_length` with kernel bandwidth adjustment (`bw_adjust=0.3`) and a custome color `blue`. - A filled KDE plot of `sepal_length` colored by `species`. - A bivariate KDE plot of `sepal_length` and `sepal_width`, filled, and showing levels=5 and a custom colormap `viridis`. **Constraints:** 1. Each plot should include a title indicating which type of plot it is. 2. Each plot should be on a separate figure. 3. Ensure the plots are clear and comprehensible. **Input and Output Formats:** There are no explicit input and output formats as this is a plotting task. However, the function should output the required plots. **Code Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Import the seaborn library and set the seaborn theme sns.set_theme() # Step 2: Load the `iris` dataset iris = sns.load_dataset(\\"iris\\") # Step 3: Create the required visualizations Univariate KDE plot of sepal_length plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\") plt.title(\'Univariate KDE plot of Sepal Length\') plt.show() Bivariate KDE plot of sepal_length and sepal_width plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\") plt.title(\'Bivariate KDE plot of Sepal Length and Sepal Width\') plt.show() KDE plot of sepal_length with bw_adjust=0.3 and color=blue plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", bw_adjust=0.3, color=\'blue\') plt.title(\'KDE plot of Sepal Length with bw_adjust=0.3\') plt.show() Filled KDE plot of sepal_length colored by species plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", fill=True) plt.title(\'Filled KDE plot of Sepal Length by Species\') plt.show() Bivariate filled KDE plot of sepal_length and sepal_width with levels=5 and colormap viridis plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", fill=True, levels=5, cmap=\\"viridis\\") plt.title(\'Bivariate Filled KDE plot of Sepal Length and Sepal Width with levels=5\') plt.show() ``` **Explanation:** - **Step 1** sets the visual theme using `sns.set_theme()`. - **Step 2** loads the required `iris` dataset. - **Step 3** involves creating `kdeplot` visualizations as specified: - A univariate KDE plot for `sepal_length`. - A bivariate KDE plot for `sepal_length` vs. `sepal_width`. - A KDE plot for `sepal_length` with adjusted bandwidth and color. - A filled KDE plot with `species` as the hue. - A bivariate filled KDE plot for `sepal_length` vs. `sepal_width` with specific levels and colormap. Note that each plot is displayed in a new figure and includes a title for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_kde_plots(): # Step 1: Import the seaborn library and set the seaborn theme sns.set_theme() # Step 2: Load the `iris` dataset iris = sns.load_dataset(\\"iris\\") # Step 3: Create the required visualizations Univariate KDE plot of sepal_length plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\") plt.title(\'Univariate KDE plot of Sepal Length\') plt.show() Bivariate KDE plot of sepal_length and sepal_width plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\") plt.title(\'Bivariate KDE plot of Sepal Length and Sepal Width\') plt.show() KDE plot of sepal_length with bw_adjust=0.3 and color=blue plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", bw_adjust=0.3, color=\'blue\') plt.title(\'KDE plot of Sepal Length with bw_adjust=0.3\') plt.show() Filled KDE plot of sepal_length colored by species plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", fill=True) plt.title(\'Filled KDE plot of Sepal Length by Species\') plt.show() Bivariate filled KDE plot of sepal_length and sepal_width with levels=5 and colormap viridis plt.figure() sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", fill=True, levels=5, cmap=\\"viridis\\") plt.title(\'Bivariate Filled KDE plot of Sepal Length and Sepal Width with levels=5\') plt.show()"},{"question":"**Question: Temporary File and Directory Management with Context Managers** You are tasked with creating a temporary workspace using Python\'s `tempfile` module. Your goal is to create a function that manages multiple temporary files and directories, performs read and write operations, and ensures proper cleanup of resources. This function should demonstrate your understanding of the `tempfile` module and efficient resource management. # Function Signature ```python def temporary_workspace(file_count: int, dir_count: int) -> dict: pass ``` # Input - `file_count` (int): The number of temporary files to create. - `dir_count` (int): The number of temporary directories to create. # Output - dict: A dictionary with the following structure: ```python { \\"file_contents\\": list of strings, # Contents read from each temporary file. \\"directories\\": list of strings # Names of created temporary directories. } ``` # Requirements 1. **Create Temporary Files**: - Use `tempfile.NamedTemporaryFile` to create the specified number of temporary files. - Write some unique data (e.g., \\"Hello World!\\" followed by a unique identifier) to each file. - Read the data back from each file to ensure it was written and store the results. 2. **Create Temporary Directories**: - Use `tempfile.TemporaryDirectory` to create the specified number of temporary directories. - Store the names of these directories. 3. **Manage Cleanup**: - Ensure that all temporary files and directories are properly cleaned up once they are no longer needed. - Utilize context managers to handle the resources automatically. 4. **Performance**: - The function should handle large numbers of files and directories efficiently. # Example Usage ```python result = temporary_workspace(3, 2) print(result) ``` # Example Output ```python { \\"file_contents\\": [\\"Hello World! 1\\", \\"Hello World! 2\\", \\"Hello World! 3\\"], \\"directories\\": [\\"/tmp/tmp123abc\\", \\"/tmp/tmp456def\\"] } ``` # Constraints - `file_count` and `dir_count` will be non-negative integers. - Each temporary file should be uniquely identified by its content. - The solution must handle the deletion of files and directories securely and without leaving residual files or directories behind. **Hints**: - Use the `with` statement to manage the context of temporary files and directories. - Remember to handle the unique identification of file contents (e.g., by appending the file index). # Testing - Test the function with different numbers of files and directories to ensure correctness. - Verify that no temporary files or directories are left behind after the function completes.","solution":"import tempfile def temporary_workspace(file_count: int, dir_count: int) -> dict: files = [] dirs = [] # Create and manage temporary files for i in range(file_count): with tempfile.NamedTemporaryFile(delete=False) as temp_file: content = f\\"Hello World! {i+1}\\" temp_file.write(content.encode(\'utf-8\')) temp_file.flush() with open(temp_file.name, \'r\') as tf: files.append(tf.read()) # Create and manage temporary directories for _ in range(dir_count): with tempfile.TemporaryDirectory() as temp_dir: dirs.append(temp_dir) return { \\"file_contents\\": files, \\"directories\\": dirs }"},{"question":"**Problem Statement: Functional Pipeline for Data Processing** You are provided with a list of transactions, each represented as a dictionary with the following keys: `\'id\'`, `\'type\'`, `\'amount\'`, and `\'status\'`. Your task is to implement a functional pipeline that processes these transactions and returns the total sum of amounts for successful transactions of a given type. You should demonstrate your understanding of iterators, generators, and functional programming using the `functools` and `itertools` modules in Python. # Requirements: 1. **Function Name**: `calculate_total` 2. **Input**: - `transactions`: A list of dictionaries, where each dictionary represents a transaction. - `transaction_type`: A string representing the type of transaction to filter (e.g., `\'credit\'` or `\'debit\'`). 3. **Output**: An integer representing the total sum of amounts for the filtered transactions. 4. **Constraints**: - Each transaction dictionary contains the keys `\'id\'`, `\'type\'`, `\'amount\'`, and `\'status\'` with appropriate values. - Transactions should be considered only if their `\'status\'` is `\'success\'`. # Example: ```python transactions = [ {\'id\': 1, \'type\': \'credit\', \'amount\': 100, \'status\': \'success\'}, {\'id\': 2, \'type\': \'debit\', \'amount\': 50, \'status\': \'failed\'}, {\'id\': 3, \'type\': \'credit\', \'amount\': 200, \'status\': \'success\'}, {\'id\': 4, \'type\': \'debit\', \'amount\': 300, \'status\': \'success\'}, {\'id\': 5, \'type\': \'credit\', \'amount\': 150, \'status\': \'failed\'} ] transaction_type = \'credit\' print(calculate_total(transactions, transaction_type)) # Output: 300 ``` # Implementation Details: - Create an appropriate iterator or generator to filter the list of transactions. - Use functions from the `itertools` and `functools` modules for functional programming techniques. - Ensure the function is purely functional in nature, avoiding side effects. # Solution: ```python from itertools import filterfalse from functools import reduce def calculate_total(transactions, transaction_type): # Generator to filter successful transactions of the given type filtered_transactions = (txn for txn in transactions if txn[\'status\'] == \'success\' and txn[\'type\'] == transaction_type) # Extract amounts from the filtered transactions amounts = (txn[\'amount\'] for txn in filtered_transactions) # Sum the amounts using reduce total = reduce(lambda x, y: x + y, amounts, 0) return total # Example usage: transactions = [ {\'id\': 1, \'type\': \'credit\', \'amount\': 100, \'status\': \'success\'}, {\'id\': 2, \'type\': \'debit\', \'amount\': 50, \'status\': \'failed\'}, {\'id\': 3, \'type\': \'credit\', \'amount\': 200, \'status\': \'success\'}, {\'id\': 4, \'type\': \'debit\', \'amount\': 300, \'status\': \'success\'}, {\'id\': 5, \'type\': \'credit\', \'amount\': 150, \'status\': \'failed\'} ] transaction_type = \'credit\' print(calculate_total(transactions, transaction_type)) # Output: 300 ``` In this task, you must demonstrate your understanding of key Python functional programming concepts such as iterators, generators, `itertools`, and `functools`. Focus on creating a clear, efficient, and purely functional pipeline to aggregate transaction data.","solution":"from itertools import filterfalse from functools import reduce def calculate_total(transactions, transaction_type): Calculates the total amount of successful transactions of a given type. Args: transactions (list of dict): A list of transaction dictionaries. transaction_type (str): The type of transactions to filter (e.g., \'credit\', \'debit\'). Returns: int: Total amount of successful transactions of the given type. # Generator to filter successful transactions of the given type filtered_transactions = (txn for txn in transactions if txn[\'status\'] == \'success\' and txn[\'type\'] == transaction_type) # Extract amounts from the filtered transactions amounts = (txn[\'amount\'] for txn in filtered_transactions) # Sum the amounts using reduce total = reduce(lambda x, y: x + y, amounts, 0) return total"},{"question":"You are tasked to write a new PyTorch function, `named_tensor_operations`, that performs specific tensor operations using named tensors. Given two tensors, `tensor1` and `tensor2`, your function should: 1. Ensure that both tensors have named dimensions and that the names match in a specific way to perform operations. 2. Perform a `torch.add(tensor1, tensor2)` operation ensuring that names are unified correctly. 3. If the dimensions names do not match the required pattern, your function should raise an appropriate error. Input: - `tensor1` (torch.Tensor): A PyTorch tensor with names for its dimensions. - `tensor2` (torch.Tensor): Another PyTorch tensor with names for its dimensions. Output: - A new tensor (torch.Tensor) resulting from `torch.add(tensor1, tensor2)` with correctly unified names. Constraints: - The named dimensions of `tensor1` and `tensor2` should match positionally from the right. - The tensors should be broadcastable according to the named dimensions rules. - If the dimensions do not match or are misaligned, raise a `RuntimeError`. Example: ```python import torch tensor1 = torch.randn(3, 4, names=(\'N\', \'C\')) tensor2 = torch.randn(3, 1, names=(\'N\', \'C\')) result = named_tensor_operations(tensor1, tensor2) print(result.names) # Output: (\'N\', \'C\') tensor3 = torch.randn(4, names=(\'C\',)) try: result = named_tensor_operations(tensor1, tensor3) except RuntimeError as e: print(e) # Expected to raise an error due to dimension mismatch ``` Implementation Notes: - Utilize name inference rules from the documentation to ensure names match and propagate correctly. - Use appropriate PyTorch functions and operations to handle named tensors and broadcasting. - Clearly document any assumptions or design choices in your code. Implement the `named_tensor_operations` function in Python using PyTorch. **Note:** Assume you have already imported PyTorch as `torch`.","solution":"import torch def named_tensor_operations(tensor1, tensor2): Perform element-wise addition of two named tensors ensuring the names match appropriately. Args: - tensor1 (torch.Tensor): A PyTorch tensor with named dimensions. - tensor2 (torch.Tensor): Another PyTorch tensor with named dimensions. Returns: - torch.Tensor: A new tensor resulting from `torch.add(tensor1, tensor2)` with correctly unified names. Raises: - RuntimeError: If the named dimensions do not match appropriately. if tensor1.names != tensor2.names: raise RuntimeError(\\"Named dimensions do not match appropriately for tensor operations.\\") # Perform the addition operation which will maintain the names of the tensors result = torch.add(tensor1, tensor2) return result"},{"question":"Objective Your goal is to demonstrate your proficiency with the pandas library by working on a multi-part problem that involves managing memory usage, performing logical operations, and handling mutations safely within DataFrame operations. Problem Statement You are given a pandas DataFrame with a variety of data types. Your tasks are as follows: 1. **Memory Usage Analysis**: - Calculate and print the memory usage of each column in the DataFrame in bytes. - Calculate the total memory usage of the DataFrame, excluding the index. 2. **Boolean Operations**: - Write a function `count_true_values(df: pd.DataFrame, target_column: str) -> int` that takes a DataFrame and a target column name as input, and returns the number of `True` values in the specified column. 3. **Mutation with User-Defined Function**: - Create a function `safely_mutate_dataframe(df: pd.DataFrame) -> pd.DataFrame` that takes a DataFrame as input. - Within the function, use `apply` method to replace any missing values in numeric columns with the mean of the column. The operation should not mutate the original DataFrame but return a safely mutated copy. - Ensure your function handles the DataFrame correctly without causing unexpected behavior due to mutation. Constraints - Only use built-in pandas methods. - Avoid using loops for tasks where vectorized operations can be applied. - Performance is important, so ensure that your code handles large DataFrames efficiently. Function Signatures ```python import pandas as pd import numpy as np def calculate_memory_usage(df: pd.DataFrame) -> pd.Series: Calculates and returns memory usage of each column in the DataFrame. Args: df (pd.DataFrame): The input DataFrame Returns: pd.Series: A series with memory usage of each column in bytes. pass def calculate_total_memory_usage(df: pd.DataFrame) -> int: Calculates and returns the total memory usage of the DataFrame excluding the index. Args: df (pd.DataFrame): The input DataFrame Returns: int: Total memory usage in bytes pass def count_true_values(df: pd.DataFrame, target_column: str) -> int: Counts the number of True values in the specified column. Args: df (pd.DataFrame): The input DataFrame target_column (str): The name of the target column Returns: int: The count of True values in the target column. pass def safely_mutate_dataframe(df: pd.DataFrame) -> pd.DataFrame: Safely mutates the DataFrame by replacing missing values in numeric columns with the mean of the column. Args: df (pd.DataFrame): The input DataFrame Returns: pd.DataFrame: A new DataFrame with missing values replaced. pass ``` Example Usage ```python # Example DataFrame data = { \\"A\\": [1, 2, np.nan, 4], \\"B\\": [False, True, True, False], \\"C\\": [\\"foo\\", \\"bar\\", np.nan, \\"baz\\"], } df = pd.DataFrame(data) # Example calculations print(calculate_memory_usage(df)) print(calculate_total_memory_usage(df)) print(count_true_values(df, \'B\')) # Safely mutate DataFrame example df_mutated = safely_mutate_dataframe(df) print(df_mutated) ``` Use the above signatures and example usage as guidance for your implementation. Ensure to handle various edge cases and validate your functions effectively.","solution":"import pandas as pd import numpy as np def calculate_memory_usage(df: pd.DataFrame) -> pd.Series: Calculates and returns memory usage of each column in the DataFrame. Args: df (pd.DataFrame): The input DataFrame Returns: pd.Series: A series with memory usage of each column in bytes. return df.memory_usage(deep=True) def calculate_total_memory_usage(df: pd.DataFrame) -> int: Calculates and returns the total memory usage of the DataFrame excluding the index. Args: df (pd.DataFrame): The input DataFrame Returns: int: Total memory usage in bytes return df.memory_usage(deep=True).sum() - df.index.memory_usage(deep=True) def count_true_values(df: pd.DataFrame, target_column: str) -> int: Counts the number of True values in the specified column. Args: df (pd.DataFrame): The input DataFrame target_column (str): The name of the target column Returns: int: The count of True values in the target column. if target_column in df: return df[target_column].sum() else: return 0 def safely_mutate_dataframe(df: pd.DataFrame) -> pd.DataFrame: Safely mutates the DataFrame by replacing missing values in numeric columns with the mean of the column. Args: df (pd.DataFrame): The input DataFrame Returns: pd.DataFrame: A new DataFrame with missing values replaced. df_copy = df.copy() numeric_cols = df_copy.select_dtypes(include=[np.number]).columns for col in numeric_cols: mean_value = df_copy[col].mean() df_copy[col].fillna(mean_value, inplace=True) return df_copy"},{"question":"# Advanced Python Comprehension and Generators You are required to write a function, `complex_comprehension`, that processes a sequence of numbers and produces a specific structure using generator comprehensions, yield expressions, and nested comprehensions. The function should handle the following tasks: 1. **Generator Comprehension** to filter and transform the sequence. 2. **Yield Expression** to yield intermediate results after filtering but before the final transformation. 3. **Nested Comprehensions** to create a deeply nested data structure (a combination of lists and sets). # Detailed Requirements: 1. **Function Signature**: ```python def complex_comprehension(numbers: List[int]) -> Generator[Dict[str, Any], None, None]: pass ``` 2. **Input**: - `numbers` (List[int]): A list of integers to process. 3. **Output**: - A generator that yields dictionaries. Each dictionary should have the following structure: ```python { \'original\': int, # Original number from the input \'transformed\': List[Set[Tuple[int, int]]], # Nested comprehension result \'is_even\': bool, # Boolean flag indicating if the number is even } ``` 4. **Processing Steps**: - First, filter out all numbers that are not multiples of 3 and greater than 0. - Use a yield expression to yield intermediate results as dictionaries containing the original value and its parity (even or odd). - For the remaining numbers, create a nested data structure using comprehensions. The structure should be a list of sets, where each set contains tuples of `(number, transformed_number)`. The transformation should be such that for every filtered number `n`, you create tuples `(n, n*multiplier)` where multiplier ranges from 1 to 3. # Example Usage: ```python numbers = [1, 3, 4, 6, -3, 9, 12] gen = complex_comprehension(numbers) for result in gen: print(result) # Expected Output: # {\'original\': 3, \'transformed\': [{(3, 3), (3, 6), (3, 9)}], \'is_even\': False} # {\'original\': 6, \'transformed\': [{(6, 6), (6, 12), (6, 18)}], \'is_even\': True} # {\'original\': 9, \'transformed\': [{(9, 9), (9, 18), (9, 27)}], \'is_even\': False} # {\'original\': 12, \'transformed\': [{(12, 12), (12, 24), (12, 36)}], \'is_even\': True} ``` # Constraints: - You must use generator comprehensions and yield expressions for this task. - The function should efficiently handle input sizes up to 10^3 integers. # Notes: Make sure your function correctly filters the numbers, uses the yield expression to return intermediate results, and creates the nested list-set-tuple structure using comprehensions.","solution":"from typing import List, Generator, Dict, Any def complex_comprehension(numbers: List[int]) -> Generator[Dict[str, Any], None, None]: # Generator comprehension to filter numbers filtered_numbers = (n for n in numbers if n > 0 and n % 3 == 0) for number in filtered_numbers: # Yield intermediate result yield { \'original\': number, \'is_even\': number % 2 == 0 } # Create nested comprehension structure transformed = [{(number, number * multiplier) for multiplier in range(1, 4)}] yield { \'original\': number, \'transformed\': transformed, \'is_even\': number % 2 == 0 }"},{"question":"# **Challenging Python Coding Assessment** **Objective** Your task is to implement a function named `array_summary` that works with the `array` module in Python. The function will take a list of numeric values and a type code (as defined by the `array` module), create an array, and perform various operations on it. **Function Signature** ```python def array_summary(values: list, type_code: str) -> dict: pass ``` **Input** - `values`: A list of numeric values. All elements will be of the same type (either all integers or all floats). - `type_code`: A single character string that represents the desired type of array. It will be one of the following: ``\'b\'``, ``\'B\'``, ``\'h\'``, ``\'H\'``, ``\'i\'``, ``\'I\'``, ``\'l\'``, ``\'L\'``, ``\'q\'``, ``\'Q\'``, ``\'f\'``, ``\'d\'``. **Output** - A dictionary with the following keys: - `\'original_array\'`: The original array created from `values`. - `\'reversed_array\'`: The array after reversing the order of elements. - `\'count_of_first_element\'`: The count of occurrences of the first element in the array. - `\'sum\'`: The sum of all elements in the array. - `\'buffer_info\'`: A tuple representing the memory address and length of the array. **Constraints** - The input list will always contain at least one element. - The type code provided will always be valid based on the `array` module\'s type codes. **Example** ```python # Example Input values = [1, 2, 3, 4, 5] type_code = \'i\' # Example Output { \'original_array\': array(\'i\', [1, 2, 3, 4, 5]), \'reversed_array\': array(\'i\', [5, 4, 3, 2, 1]), \'count_of_first_element\': 1, \'sum\': 15, \'buffer_info\': (address, length) # actual address and length will vary } ``` **Notes** - Ensure the correct creation and manipulation of the array using the `array` module based on the provided type code. - Utilize the appropriate `array` methods to perform the required operations. - Be aware of operations that may raise exceptions and handle them appropriately (e.g., ensuring the type compatibility of operations).","solution":"import array def array_summary(values: list, type_code: str) -> dict: # Create an array from the list of values and type_code arr = array.array(type_code, values) # Reverse the array reversed_arr = array.array(type_code, arr[::-1]) # Count occurrences of the first element count_first_element = arr.count(arr[0]) # Calculate the sum of elements total_sum = sum(arr) # Get buffer info buffer_info = arr.buffer_info() # Prepare the result dictionary result = { \'original_array\': arr, \'reversed_array\': reversed_arr, \'count_of_first_element\': count_first_element, \'sum\': total_sum, \'buffer_info\': buffer_info } return result"},{"question":"Objective: Demonstrate your understanding of data visualization using the Seaborn package by loading a dataset, manipulating the data, and creating informative plots. Problem Statement: You are provided a dataset containing information about health expenditures over the years across various countries. Your task is to write a function that processes this dataset and generates specific visualizations. Function Specifications: **Function Name:** `visualize_health_expenditure` **Input:** - `data`: A Pandas DataFrame containing columns `Year`, `Spending_USD`, and `Country`. **Output:** - Two Seaborn plots should be generated and displayed: 1. A line plot showing the health expenditure over the years for each country, with spending values normalized relative to their maximum value. 2. A line plot showing the percent change in health expenditure from the first year\'s expenditure as the baseline. **Constraints:** - The function should use the `seaborn.objects` interface. - The `year` column represents time, with numeric values. - You can assume that the input data is correctly formatted. **Example:** Assume `data` has been loaded and passed to the function as shown in the documentation. ```python import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(data): # Plot 1: Spending relative to maximum value ( so.Plot(data, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") .show() ) # Plot 2: Percent change from baseline ( so.Plot(data, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from baseline\\") .show() ) # Example usage data = load_dataset(\\"healthexp\\") visualize_health_expenditure(data) ``` In the example above, `visualize_health_expenditure` should generate and display the two required plots based on the input dataset. Notes: 1. Ensure your plots have appropriate labels for both axes. 2. The y-axis for the second plot should be labeled \\"Percent change in spending from baseline\\". You are expected to handle data manipulation and visualization using the Seaborn library functionalities as demonstrated in the provided examples.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_health_expenditure(data): # Plot 1: Spending relative to maximum value for each country data_max = data.groupby(\'Country\').Spending_USD.transform(\'max\') data[\'Relative_Spending\'] = data[\'Spending_USD\'] / data_max plt.figure(figsize=(14, 7)) sns.lineplot(data=data, x=\'Year\', y=\'Relative_Spending\', hue=\'Country\') plt.title(\'Health Expenditure Over the Years Relative to Maximum Value\') plt.ylabel(\'Spending Relative to Maximum\') plt.show() # Plot 2: Percent change from the first year as baseline data_first_year = data.groupby(\'Country\').Spending_USD.transform(\'first\') data[\'Percent_Change\'] = (data[\'Spending_USD\'] - data_first_year) / data_first_year * 100 plt.figure(figsize=(14, 7)) sns.lineplot(data=data, x=\'Year\', y=\'Percent_Change\', hue=\'Country\') plt.title(\'Percent Change in Health Expenditure from Baseline Year\') plt.ylabel(\'Percent Change in Spending from Baseline\') plt.show()"},{"question":"# Question: # Prediction Latency and Sparse Data Handling in Scikit-learn You are tasked with analyzing and optimizing the prediction latency and throughput of different scikit-learn models using both dense and sparse input data representations. This involves conducting benchmarks to measure the impact of various configurations on performance. Task: 1. **Benchmarking Prediction Latency and Throughput**: - Implement a function `benchmark_models` that takes a list of scikit-learn models, synthetic dataset specifications (`n_samples`, `n_features`, and `sparsity_ratio`), and additional parameters. - The function should measure and return the prediction latency and throughput for each model, both with dense and sparse data representations. 2. **Sparse Data Conversion**: - Implement a function `convert_to_sparse` that converts a given dense dataset to a sparse representation using the CSR (Compressed Sparse Row) format. 3. **Compare Model Performance**: - Analyze and compare the impact of different sparsity levels on prediction latency and throughput for each model. - Provide insights and recommendations based on the observed results. Specifications: - **Function 1: benchmark_models** ```python def benchmark_models(models, n_samples, n_features, sparsity_ratios): Benchmarks the prediction latency and throughput for given models on synthetic data. Parameters: - models (list): List of scikit-learn model instances. - n_samples (int): Number of samples in the synthetic dataset. - n_features (int): Number of features in the synthetic dataset. - sparsity_ratios (list): List of sparsity ratios to test (e.g., [0.0, 0.75, 0.9]). Returns: - results (dict): Dictionary where keys are model names and values are tuples of (dense_latency, sparse_latency, dense_throughput, sparse_throughput). pass ``` - **Function 2: convert_to_sparse** ```python def convert_to_sparse(X): Converts a dense dataset to a sparse CSR format. Parameters: - X (np.ndarray): Dense input dataset. Returns: - X_sparse (scipy.sparse.csr_matrix): Sparse CSR representation of the input dataset. pass ``` Constraints: - Ensure that `benchmark_models` handles models of various complexities, including linear models and ensemble methods. - The synthetic data and models should be configured within reasonable limits to prevent excessive computation time in an assessment setting. Example: ```python from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier models = [LogisticRegression(), RandomForestClassifier(n_estimators=10)] n_samples = 1000 n_features = 50 sparsity_ratios = [0.0, 0.75, 0.9] results = benchmark_models(models, n_samples, n_features, sparsity_ratios) for model_name, (dense_lat, sparse_lat, dense_tp, sparse_tp) in results.items(): print(f\\"Model: {model_name}\\") print(f\\"Dense Latency: {dense_lat:.4f}s, Sparse Latency: {sparse_lat:.4f}s\\") print(f\\"Dense Throughput: {dense_tp:.2f} predictions/s, Sparse Throughput: {sparse_tp:.2f} predictions/s\\") ``` Remarks: - Clearly document your code and comment on critical sections. - Use synthetic data generation tools (e.g., `numpy`) to create the datasets. - You may use libraries such as `time` or `perf_counter` to measure time and throughput. - Explain any assumptions and decisions made during the implementation.","solution":"import numpy as np from scipy.sparse import csr_matrix from sklearn.datasets import make_classification from time import perf_counter from sklearn.metrics import accuracy_score def convert_to_sparse(X): Converts a dense dataset to a sparse CSR format. Parameters: - X (np.ndarray): Dense input dataset. Returns: - X_sparse (scipy.sparse.csr_matrix): Sparse CSR representation of the input dataset. return csr_matrix(X) def benchmark_models(models, n_samples, n_features, sparsity_ratios): Benchmarks the prediction latency and throughput for given models on synthetic data. Parameters: - models (list): List of scikit-learn model instances. - n_samples (int): Number of samples in the synthetic dataset. - n_features (int): Number of features in the synthetic dataset. - sparsity_ratios (list): List of sparsity ratios to test (e.g., [0.0, 0.75, 0.9]). Returns: - results (dict): Dictionary where keys are model names and values are tuples of (dense_latency, sparse_latency, dense_throughput, sparse_throughput). results = {} for model in models: model_name = type(model).__name__ for sparsity_ratio in sparsity_ratios: # Generate synthetic dataset X, y = make_classification(n_samples=n_samples, n_features=n_features, flip_y=0.03) # Introduce sparsity if sparsity_ratio > 0: mask = np.random.rand(*X.shape) < sparsity_ratio X[mask] = 0 # Convert to sparse format X_sparse = convert_to_sparse(X) # Measure prediction latency and throughput for dense data model.fit(X, y) start_time = perf_counter() y_pred_dense = model.predict(X) end_time = perf_counter() dense_latency = end_time - start_time dense_throughput = n_samples / dense_latency # Measure prediction latency and throughput for sparse data model.fit(X_sparse, y) start_time = perf_counter() y_pred_sparse = model.predict(X_sparse) end_time = perf_counter() sparse_latency = end_time - start_time sparse_throughput = n_samples / sparse_latency # Collect results results[(model_name, sparsity_ratio)] = (dense_latency, sparse_latency, dense_throughput, sparse_throughput) return results"},{"question":"**Question Title:** Enhanced URL Fetching with Authentication and Headers **Objective:** Your task is to implement a function that fetches the content of a URL using the `urllib` package. This function should handle various aspects like HTTP GET and POST methods, add custom headers, and support HTTP Basic Authentication if required. **Function Signature:** ```python def fetch_url_content(url: str, method: str = \'GET\', data: dict = None, headers: dict = None, auth: tuple = None) -> str: pass ``` **Parameters:** - `url` (str): The URL to fetch. - `method` (str, optional): The HTTP method to use (`\'GET\'` or `\'POST\'`). Default is `\'GET\'`. - `data` (dict, optional): The data to send in a POST request. Default is `None`. - `headers` (dict, optional): Additional HTTP headers to include in the request. Default is `None`. - `auth` (tuple, optional): A tuple containing the username and password for HTTP Basic Authentication. Default is `None`. **Returns:** - `str`: The content of the response as a string. **Constraints:** - The function must handle HTTP errors properly and should raise an appropriate exception with a meaningful message. - Use appropriate request headers for content-type when sending POST data. - The function should follow HTTP redirects if encountered. **Examples:** ```python # Example 1: Simple GET request print(fetch_url_content(\'http://www.example.com\')) # Example 2: POST request with data print(fetch_url_content(\'http://www.example.com\', method=\'POST\', data={\'key\': \'value\'})) # Example 3: GET request with custom headers print(fetch_url_content(\'http://www.example.com\', headers={\'User-Agent\': \'Mozilla/5.0\'})) # Example 4: GET request with HTTP Basic Authentication print(fetch_url_content(\'http://www.example.com\', auth=(\'username\', \'password\'))) ``` **Notes:** 1. Use the `urllib.request` module for handling the URL operations. 2. Handle HTTP errors like 404 (Not Found) and 403 (Forbidden) appropriately by raising exceptions with a meaningful message. 3. If the `method` is `\'POST\'`, ensure to properly encode the provided `data` and set appropriate headers (e.g., `Content-Type`). **Evaluation Criteria:** - Correctly handles different HTTP methods. - Appropriately processes and adds headers. - Implements HTTP Basic Authentication when provided. - Robust error handling and meaningful exception messages. - Clear, readable, and well-documented code.","solution":"import urllib.request import urllib.parse from urllib.error import HTTPError, URLError import base64 def fetch_url_content(url: str, method: str = \'GET\', data: dict = None, headers: dict = None, auth: tuple = None) -> str: Fetches the content of a URL using specified HTTP method, data, headers and authentication. Parameters: - url (str): The URL to fetch. - method (str, optional): The HTTP method to use (\'GET\' or \'POST\'). Default is \'GET\'. - data (dict, optional): The data to send in a POST request. Default is None. - headers (dict, optional): Additional HTTP headers to include in the request. Default is None. - auth (tuple, optional): A tuple containing the username and password for HTTP Basic Authentication. Default is None. Returns: - str: The content of the response as a string. request_data = None request_headers = headers or {} if auth is not None: username, password = auth credentials = f\'{username}:{password}\' encoded_credentials = base64.b64encode(credentials.encode(\'utf-8\')).decode(\'utf-8\') request_headers[\'Authorization\'] = f\'Basic {encoded_credentials}\' if method == \'POST\' and data is not None: request_data = urllib.parse.urlencode(data).encode(\'utf-8\') request_headers[\'Content-Type\'] = \'application/x-www-form-urlencoded\' request = urllib.request.Request(url, data=request_data, headers=request_headers, method=method) try: with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except HTTPError as e: raise Exception(f\'HTTPError: {e.code} {e.reason} for URL: {url}\') except URLError as e: raise Exception(f\'URLError: {e.reason} for URL: {url}\')"},{"question":"# Python Coding Assessment Objective Implement a Python function that utilizes the `compileall` module to compile all Python files in a given directory with the following requirements. This will demonstrate your understanding of the `compileall` module\'s functionalities. Task Write a function `custom_compile_directory` that: - Compiles all Python source files in the directory `directory` provided as the input. - Limits recursion to a maximum of `max_recursion` levels. - Forces recompilation even if timestamps are up to date. - Excludes files matching the regular expression given by `exclude_pattern`. - Uses `num_workers` to parallelize the compilation. If `num_workers` is set to zero, use the optimal number of cores. Function Signature ```python import re import compileall def custom_compile_directory(directory: str, max_recursion: int, exclude_pattern: str, num_workers: int) -> bool: pass ``` Input - `directory` (str): Path to the directory containing Python source files. - `max_recursion` (int): Maximum levels of recursion for subdirectories. - `exclude_pattern` (str): Regular expression pattern to exclude certain files. - `num_workers` (int): Number of workers to use for parallel compilation. Output - Returns `True` if all files compile successfully, and `False` otherwise. Constraints - You must use the `compileall.compile_dir()` function in your implementation. - The `exclude_pattern` should be applied correctly using the `re` module. Example ```python print(custom_compile_directory(\'path/to/dir\', 2, r\'[/]test.*\', 4)) # This should compile files in `path/to/dir` up to 2 subdirectory levels deep, excluding any file or directory containing \'test\' in its name, using 4 workers. ``` Notes - Ensure the function handles errors gracefully and adheres to the constraints given. - Do not print any output within your function; just return the boolean result.","solution":"import re import compileall def custom_compile_directory(directory: str, max_recursion: int, exclude_pattern: str, num_workers: int) -> bool: Compiles all Python source files in the specified directory. Parameters: directory (str): Path to the directory containing Python source files. max_recursion (int): Maximum levels of recursion for subdirectories. exclude_pattern (str): Regular expression pattern to exclude certain files. num_workers (int): Number of workers to use for parallel compilation. Returns: bool: True if all files compile successfully, False otherwise. # Compile all files in the directory success = compileall.compile_dir( dir=directory, maxlevels=max_recursion, force=True, rx=re.compile(exclude_pattern), quiet=1, workers=num_workers if num_workers > 0 else None ) return success"},{"question":"**Objective**: Implement a function to evaluate different regression models using multiple metrics and select the best model based on a specific criterion. **Problem Statement**: You are provided with a dataset containing features (`X`) and target variable (`y`). Your task is to implement a function that trains several regression models on this dataset, evaluates their performance using multiple metrics, and selects the best model based on the highest R score. **Function Signature**: ```python def evaluate_and_select_regressor(X: np.ndarray, y: np.ndarray) -> str: pass ``` **Input**: - `X`: A numpy array of shape `(n_samples, n_features)` representing the feature matrix. - `y`: A numpy array of shape `(n_samples,)` representing the target variable. **Output**: - Returns the name of the best-performing model based on the highest R score. The possible model names are: `\\"LinearRegression\\"`, `\\"Ridge\\"`, `\\"Lasso\\"`, `\\"ElasticNet\\"`. **Models to Evaluate**: 1. `LinearRegression` 2. `Ridge` 3. `Lasso` 4. `ElasticNet` **Metrics to Use for Evaluation**: 1. Mean Absolute Error (MAE) 2. Mean Squared Error (MSE) 3. R Score **Constraints**: - You must use scikit-learn for model implementation and evaluation. - Handle potential exceptions such as models failing to converge. **Evaluation Criteria**: - Correct implementation and usage of scikit-learn models and metrics. - Proper handling of data input and output formats. - Clarity and efficiency of the code. **Example**: ```python import numpy as np from sklearn.datasets import make_regression # Generate a random regression dataset X, y = make_regression(n_samples=100, n_features=4, noise=0.1) # Call the function best_model = evaluate_and_select_regressor(X, y) print(best_model) # Expected output could be one of \\"LinearRegression\\", \\"Ridge\\", \\"Lasso\\", or \\"ElasticNet\\" depending on the generated data and the evaluation. ``` Note: The function should consider any necessary preprocessing steps, such as scaling the features, before fitting the models.","solution":"import numpy as np from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.exceptions import ConvergenceWarning import warnings warnings.filterwarnings(\\"ignore\\", category=ConvergenceWarning) def evaluate_and_select_regressor(X: np.ndarray, y: np.ndarray) -> str: # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Split the data X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Define the models models = { \\"LinearRegression\\": LinearRegression(), \\"Ridge\\": Ridge(), \\"Lasso\\": Lasso(), \\"ElasticNet\\": ElasticNet() } best_model_name = None best_r2_score = -np.inf # Evaluate each model for model_name, model in models.items(): model.fit(X_train, y_train) y_pred = model.predict(X_test) mae = mean_absolute_error(y_test, y_pred) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) if r2 > best_r2_score: best_r2_score = r2 best_model_name = model_name print(f\\"Model: {model_name}\\") print(f\\"MAE: {mae}\\") print(f\\"MSE: {mse}\\") print(f\\"R2 Score: {r2}\\") print(\\"-\\" * 30) return best_model_name"},{"question":"# File and Directory Management Task Objective: Demonstrate your comprehension of the `shutil` module and its capabilities by solving the following task. This question will assess your ability to perform high-level file operations and handle common file management scenarios in Python. Problem Statement: You are given a directory path containing several text files and subdirectories. Your task is to write a Python function that: 1. Creates an archive of the given directory (with all its contents) in a specified format (e.g., zip or tar). 2. Copies all text files (*.txt) from the given directory to a new directory, preserving the original directory structure. 3. Deletes all empty subdirectories from the original directory after copying the text files. Specifications: - The function should be named `manage_directory`. - The function should take two arguments: 1. `src_dir` (str): The path to the source directory. 2. `archive_format` (str): The format for the archive file (e.g., \'zip\', \'tar\'). - The function should create an archive file named `backup` in the specified format in the source directory\'s parent directory. - The function should create a new directory named `text_files_copy` at the same level as the source directory and copy all text files (*.txt) to this directory, preserving the original directory structure. - The function should delete all empty subdirectories from the source directory after the copying process. Constraints: - You can assume that the source directory path is valid and that the directory exists. - The function should handle large directories efficiently. Example Usage: ```python def manage_directory(src_dir: str, archive_format: str) -> None: # Your implementation here # Example call manage_directory(\'/path/to/source_directory\', \'zip\') ``` Important: - Do not use the `os.system` function or any other method to call external shell commands. - Your solution should demonstrate proper error handling and resource management. - You are encouraged to use the `shutil` and `os` modules to complete this task.","solution":"import os import shutil def manage_directory(src_dir: str, archive_format: str) -> None: - Creates an archive of the given directory (with all its contents) in a specified format (e.g., zip or tar). - Copies all text files (*.txt) from the given directory to a new directory, preserving the original directory structure. - Deletes all empty subdirectories from the original directory after copying the text files. # Create archive of the source directory base_dir, dir_name = os.path.split(os.path.abspath(src_dir)) archive_path = os.path.join(base_dir, \'backup\') shutil.make_archive(archive_path, archive_format, src_dir) # Copy all text files to a new directory new_dir = os.path.join(base_dir, \'text_files_copy\') if not os.path.exists(new_dir): os.makedirs(new_dir) for root, dirs, files in os.walk(src_dir): for file in files: if file.endswith(\'.txt\'): rel_path = os.path.relpath(root, src_dir) new_file_dir = os.path.join(new_dir, rel_path) if not os.path.exists(new_file_dir): os.makedirs(new_file_dir) shutil.copy(os.path.join(root, file), new_file_dir) # Remove empty subdirectories from the original directory for root, dirs, files in os.walk(src_dir, topdown=False): for dir in dirs: dir_path = os.path.join(root, dir) if not os.listdir(dir_path): # Check if directory is empty os.rmdir(dir_path)"},{"question":"In this coding assessment, you will demonstrate your understanding of parallelism control in scikit-learn. You are required to implement a function that leverages scikit-learn\'s parallelism features for efficient model training and evaluation. # Task Write a function `parallel_model_evaluation` that trains and evaluates a scikit-learn model using cross-validation with parallel processing. The function should: 1. **Train a model** using cross-validation with n-folds specified. 2. **Optimize the speed** using parallel processing by appropriately setting the `n_jobs` parameter. 3. **Control the number of threads** for BLAS and OpenMP to avoid oversubscription. 4. **Return** the cross-validation scores and the average score. # Function Signature ```python def parallel_model_evaluation(estimator, X, y, n_folds, n_jobs, n_threads): Train and evaluate a scikit-learn model using parallel processing. Parameters: - estimator: A scikit-learn estimator object (e.g., an instance of a classifier or regressor). - X: Feature matrix (numpy array or pandas DataFrame). - y: Labels (numpy array or pandas Series). - n_folds: Number of folds to use in cross-validation (int). - n_jobs: Number of jobs for parallel processing (int). - n_threads: Number of threads for BLAS and OpenMP libraries (int). Returns: - scores: List of cross-validation scores for each fold. - avg_score: Average cross-validation score. pass ``` # Constraints - The estimator must be compatible with scikit-learn\'s `cross_val_score` function. - You must use environment variables or `threadpoolctl` to control the number of threads for BLAS and OpenMP libraries. - The function should return the cross-validation scores and the average score as a floating-point number. # Example ```python from sklearn.datasets import load_iris from sklearn.svm import SVC # Load data iris = load_iris() X, y = iris.data, iris.target # Instantiate the model model = SVC() # Evaluate the model using parallel processing scores, avg_score = parallel_model_evaluation(model, X, y, n_folds=5, n_jobs=4, n_threads=2) print(f\\"Scores: {scores}\\") print(f\\"Average Score: {avg_score}\\") ``` This function should efficiently train and evaluate the model, demonstrating your ability to manage and tune parallel processing in scikit-learn.","solution":"from sklearn.model_selection import cross_val_score import numpy as np import threadpoolctl def parallel_model_evaluation(estimator, X, y, n_folds, n_jobs, n_threads): Train and evaluate a scikit-learn model using parallel processing. Parameters: - estimator: A scikit-learn estimator object (e.g., an instance of a classifier or regressor). - X: Feature matrix (numpy array or pandas DataFrame). - y: Labels (numpy array or pandas Series). - n_folds: Number of folds to use in cross-validation (int). - n_jobs: Number of jobs for parallel processing (int). - n_threads: Number of threads for BLAS and OpenMP libraries (int). Returns: - scores: List of cross-validation scores for each fold. - avg_score: Average cross-validation score. # Use threadpoolctl to control the number of threads with threadpoolctl.threadpool_limits(limits=n_threads): # Perform cross-validation with parallel processing scores = cross_val_score(estimator, X, y, cv=n_folds, n_jobs=n_jobs) # Calculate the average score avg_score = np.mean(scores) return scores, avg_score"},{"question":"# HTML Entities Conversion in Python You have been given access to the `html.entities` module in Python, which contains important mappings related to HTML entities. Using these dictionaries, you need to implement a function that converts a string containing HTML named character references to its equivalent Unicode string, and vice versa. # Task 1. **Function 1: `html_to_unicode(s: str) -> str`** - This function takes an input string `s` containing HTML named character references (like `&gt;` for `>`) and converts it to the equivalent Unicode string. - **Example:** ```python html_to_unicode(\\"&lt;p&gt;This is a paragraph.&lt;/p&gt;\\") ``` **Output:** ``` \'<p>This is a paragraph.</p>\' ``` 2. **Function 2: `unicode_to_html(s: str) -> str`** - This function takes an input Unicode string `s` and converts it back to a string containing HTML named character references where applicable. - **Example:** ```python unicode_to_html(\\"<p>This is a paragraph.</p>\\") ``` **Output:** ``` \'&lt;p&gt;This is a paragraph.&lt;/p&gt;\' ``` # Input and Output Formats - **html_to_unicode(s: str) -> str** - `s`: A string containing HTML named character references. - Returns: A string with HTML named character references converted to their equivalent Unicode characters. - **unicode_to_html(s: str) -> str** - `s`: A Unicode string. - Returns: A string with Unicode characters converted to their equivalent HTML named character references where applicable. # Constraints - The input string `s` for either function will not contain nested entities. - Performance should be optimal for strings with up to 10,000 characters. # Hints - Use the `html5` dictionary for `html_to_unicode`. - Use the `name2codepoint` and `codepoint2name` dictionaries for `unicode_to_html`. Ensure to handle cases where the named character references may or may not include the trailing semicolon. # Solution Template ```python import html.entities def html_to_unicode(s: str) -> str: # Your implementation here pass def unicode_to_html(s: str) -> str: # Your implementation here pass ``` You may refer to the following documentation link for additional information on HTML named character references: [HTML Named Character References](https://html.spec.whatwg.org/multipage/named-characters.html#named-character-references)","solution":"import html def html_to_unicode(s: str) -> str: Converts a string containing HTML named character references to its equivalent Unicode string. return html.unescape(s) def unicode_to_html(s: str) -> str: Converts a Unicode string to a string containing HTML named character references where applicable. return html.escape(s, quote=False)"},{"question":"**Title: Implement and Evaluate Different Naive Bayes Classifiers** **Background:** You are provided with a dataset in CSV format containing textual data and associated labels. Your task is to implement and evaluate different Naive Bayes classifiers from the scikit-learn library on this dataset. The classifiers to be used are: - `MultinomialNB` - `ComplementNB` - `BernoulliNB` **Dataset:** - The dataset `text_classification.csv` contains two columns: `text` and `label`. The `text` column contains the document text, and the `label` column contains the corresponding label (class). **Requirements:** 1. Load the dataset from the CSV file. 2. Preprocess the textual data by converting the text to a matrix of token counts using `CountVectorizer`. 3. Split the data into training and testing sets using `train_test_split`. 4. Implement each of the three Naive Bayes classifiers (`MultinomialNB`, `ComplementNB`, `BernoulliNB`) and evaluate their performance: - Train the classifier on the training data. - Make predictions on the test data. - Calculate and print the accuracy, precision, recall, and F1-score of each classifier. 5. Summarize and compare the performance metrics of the different classifiers. **Expected Input and Output:** - Input: The CSV file path (`text_classification.csv`) should be specified in the code. - Output: Print the evaluation metrics (accuracy, precision, recall, F1-score) for each classifier. **Constraints:** - The dataset may be large, but it should fit into memory for this task. - Use appropriate text preprocessing techniques for the chosen classifiers. - Handle any potential issues with the textual data, such as missing values or non-text entries. **Performance Requirements:** - Ensure the code runs efficiently and correctly evaluates the performance of each classifier. **Code Template:** ```python import pandas as pd from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(file_path): # Load dataset from CSV file df = pd.read_csv(file_path) return df[\'text\'], df[\'label\'] def preprocess_data(text_data): # Convert text data to a matrix of token counts vectorizer = CountVectorizer() X = vectorizer.fit_transform(text_data) return X def evaluate_model(y_true, y_pred): # Calculate and return evaluation metrics accuracy = accuracy_score(y_true, y_pred) precision = precision_score(y_true, y_pred, average=\'weighted\') recall = recall_score(y_true, y_pred, average=\'weighted\') f1 = f1_score(y_true, y_pred, average=\'weighted\') return accuracy, precision, recall, f1 def main(): file_path = \'text_classification.csv\' # Step 1: Load the data texts, labels = load_data(file_path) # Step 2: Preprocess the data X = preprocess_data(texts) X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42) # Step 3: Train and evaluate MultinomialNB mnb = MultinomialNB() mnb.fit(X_train, y_train) y_pred_mnb = mnb.predict(X_test) acc_mnb, prec_mnb, rec_mnb, f1_mnb = evaluate_model(y_test, y_pred_mnb) print(\\"MultinomialNB - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\\".format(acc_mnb, prec_mnb, rec_mnb, f1_mnb)) # Step 4: Train and evaluate ComplementNB cnb = ComplementNB() cnb.fit(X_train, y_train) y_pred_cnb = cnb.predict(X_test) acc_cnb, prec_cnb, rec_cnb, f1_cnb = evaluate_model(y_test, y_pred_cnb) print(\\"ComplementNB - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\\".format(acc_cnb, prec_cnb, rec_cnb, f1_cnb)) # Step 5: Train and evaluate BernoulliNB bnb = BernoulliNB() bnb.fit(X_train, y_train) y_pred_bnb = bnb.predict(X_test) acc_bnb, prec_bnb, rec_bnb, f1_bnb = evaluate_model(y_test, y_pred_bnb) print(\\"BernoulliNB - Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\\".format(acc_bnb, prec_bnb, rec_bnb, f1_bnb)) if __name__ == \'__main__\': main() ``` **Note:** - Ensure you have all necessary libraries installed (`pandas`, `scikit-learn`). - This question tests the ability to implement and compare different Naive Bayes algorithms, along with evaluating their performance on a real dataset.","solution":"import pandas as pd from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score def load_data(file_path): # Load dataset from CSV file df = pd.read_csv(file_path) return df[\'text\'], df[\'label\'] def preprocess_data(text_data): # Convert text data to a matrix of token counts vectorizer = CountVectorizer() X = vectorizer.fit_transform(text_data) return X, vectorizer def evaluate_model(y_true, y_pred): # Calculate and return evaluation metrics accuracy = accuracy_score(y_true, y_pred) precision = precision_score(y_true, y_pred, average=\'weighted\') recall = recall_score(y_true, y_pred, average=\'weighted\') f1 = f1_score(y_true, y_pred, average=\'weighted\') return accuracy, precision, recall, f1 def naive_bayes_classifiers(file_path=\'text_classification.csv\'): # Step 1: Load the data texts, labels = load_data(file_path) # Step 2: Preprocess the data X, vectorizer = preprocess_data(texts) X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42) results = {} # Step 3: Train and evaluate MultinomialNB mnb = MultinomialNB() mnb.fit(X_train, y_train) y_pred_mnb = mnb.predict(X_test) acc_mnb, prec_mnb, rec_mnb, f1_mnb = evaluate_model(y_test, y_pred_mnb) results[\'MultinomialNB\'] = (acc_mnb, prec_mnb, rec_mnb, f1_mnb) # Step 4: Train and evaluate ComplementNB cnb = ComplementNB() cnb.fit(X_train, y_train) y_pred_cnb = cnb.predict(X_test) acc_cnb, prec_cnb, rec_cnb, f1_cnb = evaluate_model(y_test, y_pred_cnb) results[\'ComplementNB\'] = (acc_cnb, prec_cnb, rec_cnb, f1_cnb) # Step 5: Train and evaluate BernoulliNB bnb = BernoulliNB() bnb.fit(X_train, y_train) y_pred_bnb = bnb.predict(X_test) acc_bnb, prec_bnb, rec_bnb, f1_bnb = evaluate_model(y_test, y_pred_bnb) results[\'BernoulliNB\'] = (acc_bnb, prec_bnb, rec_bnb, f1_bnb) return results # Uncomment the following line to run the function # print(naive_bayes_classifiers())"},{"question":"Implement a Python function `parse_and_convert(data: List[Tuple[str, Union[str, int, float, bytes]]]) -> Dict[str, List[Union[str, int, float, bytes]]]` that processes a list of tuples. Each tuple contains a format string and a value. The function should group values by their format types (\'string\', \'int\', \'float\', \'bytes\') and return a dictionary where keys are type names and values are lists of the corresponding elements from the input. # Function Signature ```python def parse_and_convert(data: List[Tuple[str, Union[str, int, float, bytes]]]) -> Dict[str, List[Union[str, int, float, bytes]]]: pass ``` # Input - `data`: A list of tuples where each tuple consists of: - A format string that can be \'s\' for string, \'i\' for integer, \'f\' for float, and \'y\' for bytes. - A value corresponding to the format string. # Output - A dictionary where the keys are \'string\', \'int\', \'float\', and \'bytes\', and the values are lists containing all elements from the input that match the respective type. # Constraints - The format string in each tuple will always be valid and one of {\'s\', \'i\', \'f\', \'y\'}. - The list `data` will have at most 10,000 elements. - The types of values will strictly match the format string in each tuple. # Example ```python data = [(\'s\', \'hello\'), (\'i\', 42), (\'f\', 3.14), (\'y\', b\'bytes\'), (\'s\', \'world\'), (\'i\', 15)] output = parse_and_convert(data) # Output: # { # \'string\': [\'hello\', \'world\'], # \'int\': [42, 15], # \'float\': [3.14], # \'bytes\': [b\'bytes\'] # } ``` # Notes 1. You need to ensure the values are grouped accurately by their format types. 2. Handle the given constraints carefully to ensure the function performs efficiently.","solution":"from typing import List, Tuple, Union, Dict def parse_and_convert(data: List[Tuple[str, Union[str, int, float, bytes]]]) -> Dict[str, List[Union[str, int, float, bytes]]]: result = { \'string\': [], \'int\': [], \'float\': [], \'bytes\': [] } for fmt, value in data: if fmt == \'s\': result[\'string\'].append(value) elif fmt == \'i\': result[\'int\'].append(value) elif fmt == \'f\': result[\'float\'].append(value) elif fmt == \'y\': result[\'bytes\'].append(value) return result"},{"question":"**Task: Customizing and Analyzing Plot Aesthetics with Seaborn** **Objective:** You are required to demonstrate your comprehension of Seaborn\'s capabilities for controlling the aesthetics of plots. Specifically, you will create a function that generates a customized plot based on various styles, themes, and contexts. **Function Specification:** - **Function Name:** `customized_plot` - **Input Parameters:** - `style` (str): The style to be applied to the plot. Possible values are `\'darkgrid\'`, `\'whitegrid\'`, `\'dark\'`, `\'white\'`, `\'ticks\'`. - `theme` (str): The theme to be applied. Possible values are `\'dark\'`, `\'light\'`, `\'white\'`, `\'ticks\'`. - `context` (str): The context for scaling elements. Possible values are `\'paper\'`, `\'notebook\'`, `\'talk\'`, `\'poster\'`. - `font_scale` (float, optional): Scaling factor for fonts within the plot. Defaults to 1.0. - `despine` (bool, optional): Whether to remove top and right spines from the plot. Defaults to `False`. - **Returns:** None. The function should generate and display the customized plot. **Constraints:** 1. **Style:** The `style` should set the figure\'s base appearance using the specified parameter. 2. **Theme:** The `theme` should be applied using the base Seaborn functionality. 3. **Context:** The `context` should be applied to adjust the plot\'s element sizes based on the provided context. 4. **Font Scaling:** The `font_scale` should be incorporated to adjust text sizes. 5. **Despining:** If `despine` is `True`, remove the top and right spines using `sns.despine()`. **Requirements:** 1. Create a simple plot, such as a sine wave plot, as the basis for applying the styles, themes, and contexts. 2. The plot should clearly reflect each customization aspect based on the given input parameters. 3. Ensure that the plot is visually appealing and easy to interpret. **Example:** ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def customized_plot(style, theme, context, font_scale=1.0, despine=False): sns.set_theme(style=theme) sns.set_style(style) sns.set_context(context, font_scale=font_scale) def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) sinplot() if despine: sns.despine() plt.show() # Example usage: customized_plot(\'whitegrid\', \'dark\', \'talk\', font_scale=1.2, despine=True) ``` **Expected output:** A sine wave plot that adheres to the \'whitegrid\' style, \'dark\' theme, \'talk\' context with 1.2 font scaling and despined axes. **Note:** Ensure you have the necessary imports for numpy, seaborn, and matplotlib in your function to make the code work seamlessly.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def customized_plot(style, theme, context, font_scale=1.0, despine=False): Generate a customized sine wave plot based on specified style, theme, context, and font scaling. Parameters: style (str): The style to apply to the plot (e.g., \'darkgrid\', \'whitegrid\', etc.). theme (str): The theme to apply (e.g., \'dark\', \'light\', etc.). context (str): The context for scaling plot elements (e.g., \'paper\', \'notebook\', etc.). font_scale (float, optional): Scaling factor for fonts. Defaults to 1.0. despine (bool, optional): Whether to despine the plot (remove top and right spines). Defaults to False. Returns: None # Set the theme sns.set_theme(style=theme) # Set the style sns.set_style(style) # Set the context with the font scaling sns.set_context(context, font_scale=font_scale) # Function to generate a sine plot def sinplot(n=10, flip=1): x = np.linspace(0, 14, 100) for i in range(1, n + 1): plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip) # Generate the sine plot sinplot() # If despine is True, remove top and right spines if despine: sns.despine() # Show the plot plt.show()"},{"question":"Objective You are tasked with creating a utility function that searches for certain types of files within a directory and processes their content. Specifically, you are to search for files containing a certain string in their names, read their content, and count the number of occurrences of a specific string within those files. Problem Statement Write a function `search_and_count(directory: str, search_pattern: str, search_string: str, recursive: bool = False) -> dict` that performs the following: 1. **Parameters:** - `directory` (str): The root directory where the search will begin. - `search_pattern` (str): The pattern to match filenames (e.g., `\\"*.txt\\"` or `\\"file_*.log\\"`). - `search_string` (str): The string to search for within the files. - `recursive` (bool): Whether to search directories recursively. 2. **Returns:** - A dictionary where the keys are the filenames (with paths relative to the root directory) and the values are the counts of occurrences of `search_string` within those files. 3. **Constraints:** - You must use the `glob` module to find the files. - If `recursive` is `True`, use recursive searching to go through all subdirectories. - Handle any exceptions that might occur during file reading (e.g., due to permissions issues) by ignoring those files and adding a log entry in the dictionary with a value of `None`. 4. **Example:** ```python import os # Setup for example os.makedirs(\'test_dir/sub_dir\', exist_ok=True) with open(\'test_dir/file1.txt\', \'w\') as f: f.write(\\"hello worldnhello\\") with open(\'test_dir/file2.log\', \'w\') as f: f.write(\\"test searchn\\") with open(\'test_dir/sub_dir/file3.txt\', \'w\') as f: f.write(\\"another testnhello world\\") # Function call results = search_and_count(\'test_dir\', \'*.txt\', \'hello\', recursive=True) print(results) ``` Expected Output: ```python { \'file1.txt\': 2, \'sub_dir/file3.txt\': 1 } ``` **Performance Notes:** - Ensure the function handles large directories efficiently. Consider using `glob.iglob` if memory usage becomes a concern. Hints: - Use `glob` and `os` modules to traverse directories and match files. - Handle file reading exceptions gracefully to avoid crashes. - Consider edge cases such as empty directories, files without the search string, and invalid file formats.","solution":"import os import glob def search_and_count(directory: str, search_pattern: str, search_string: str, recursive: bool = False) -> dict: Searches for files matching the search pattern within the directory and counts occurrences of the search string within those files. Parameters: - directory: str, the root directory where the search will begin. - search_pattern: str, the pattern to match filenames (e.g., \\"*.txt\\" or \\"file_*.log\\"). - search_string: str, the string to search for within the files. - recursive: bool, whether to search directories recursively. Returns: - dict: A dictionary where the keys are the filenames (with paths relative to the root directory) and the values are the counts of occurrences of the search_string within those files. result = {} search_path = os.path.join(directory, \\"**\\", search_pattern) if recursive else os.path.join(directory, search_pattern) for file_path in glob.iglob(search_path, recursive=recursive): relative_path = os.path.relpath(file_path, directory) try: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() result[relative_path] = content.count(search_string) except Exception as e: # On any error, log the occurrence with None as the count result[relative_path] = None return result"},{"question":"Objective: To assess the understanding of Python\'s `threading` module by implementing a multi-threaded file compression utility that uses the `zipfile` module to compress files in the background. Question: You are required to implement a multi-threaded file compression utility in Python using the `threading` module. The utility should be capable of compressing multiple files concurrently, while the main program continues to run, and should provide a summary report once all compressions are completed. Requirements: 1. Implement a class `FileCompressor` that inherits from `threading.Thread`. 2. The `FileCompressor` class should: - Accept an input file path and an output zip file path during initialization. - Override the `run` method to perform file compression using the `zipfile` module. - Print a message indicating that compression is started and another message upon completion. 3. In the main block of your script: - Create and start multiple `FileCompressor` threads for a given list of files. - Print a message indicating the main program is running. - Use the `join` method to ensure the main program waits for all compression threads to complete. - Print a summary report showing the total number of files compressed. Constraints: - You should handle any exceptions that might occur during file compression (e.g., file not found) and log an appropriate error message. - Assume the list of files and output directory are provided as constants or input within the script. Example Input: ```python files_to_compress = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] output_directory = \'compressed_files\' ``` Expected Output: ```plaintext Started compression of file1.txt Started compression of file2.txt Started compression of file3.txt The main program continues to run. Finished background zip of: file1.txt Finished background zip of: file2.txt Finished background zip of: file3.txt Summary Report: Total files compressed: 3 ``` Implement the `FileCompressor` class and the main block as specified. Ensure the program runs correctly given the example input and meets all requirements.","solution":"import threading import zipfile import os class FileCompressor(threading.Thread): def __init__(self, input_file, output_dir): super().__init__() self.input_file = input_file self.output_dir = output_dir def run(self): try: if not os.path.exists(self.output_dir): os.makedirs(self.output_dir) output_path = os.path.join(self.output_dir, os.path.basename(self.input_file) + \'.zip\') print(f\\"Started compression of {self.input_file}\\") with zipfile.ZipFile(output_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: zipf.write(self.input_file, os.path.basename(self.input_file)) print(f\\"Finished background zip of: {self.input_file}\\") except Exception as e: print(f\\"Error compressing {self.input_file}: {e}\\") if __name__ == \\"__main__\\": files_to_compress = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] output_directory = \'compressed_files\' threads = [] for file_name in files_to_compress: compressor = FileCompressor(file_name, output_directory) threads.append(compressor) compressor.start() print(\\"The main program continues to run.\\") for thread in threads: thread.join() print(\\"Summary Report:\\") print(f\\"Total files compressed: {len(files_to_compress)}\\")"},{"question":"# Python Coding Assessment Question **Using Tkinter Message Boxes** You are developing a Python application using the Tkinter library. As part of this application, you need to interact with users through various types of message boxes to get their responses and handle them accordingly. **Task:** Write a Python function `handle_user_interactions` that creates a simple Tkinter window and utilizes different types of `tkinter.messagebox` dialogs to interact with the user. Your function should: 1. Display an information message using `showinfo` with the title \\"Info\\" and the message \\"This is an information message.\\" 2. Display a warning message using `showwarning` with the title \\"Warning\\" and the message \\"This is a warning message.\\" 3. Display an error message using `showerror` with the title \\"Error\\" and the message \\"This is an error message.\\" 4. Display a question message using `askquestion` with the title \\"Question\\" and the message \\"Do you like Python?\\" - Print \\"User likes Python\\" if the user selects \'Yes\'. - Print \\"User does not like Python\\" if the user selects \'No\'. 5. Display a yes-no-cancel question using `askyesnocancel` with the title \\"Confirm\\" and the message \\"Would you like to save changes?\\" - Print \\"User chose to save\\" if the user selects \'Yes\'. - Print \\"User chose not to save\\" if the user selects \'No\'. - Print \\"User cancelled the operation\\" if the user selects \'Cancel\'. **Function Signature:** ```python def handle_user_interactions(): pass ``` **Constraints:** - Make sure to import the necessary modules from `tkinter`. - The Tkinter window should be properly handled, meaning it should be displayed, and you should ensure it closes cleanly after the interactions. **Example:** If the user interacts with the dialogs as indicated, the output might be: ``` User likes Python User cancelled the operation ``` Note: This task primarily evaluates your understanding of the `tkinter.messagebox` module and how to handle different types of message boxes to interact with users in a graphical application.","solution":"import tkinter as tk from tkinter import messagebox def handle_user_interactions(): def close_root_window(): root.destroy() root = tk.Tk() root.withdraw() # Hide the root window # Display an information message messagebox.showinfo(\\"Info\\", \\"This is an information message.\\") # Display a warning message messagebox.showwarning(\\"Warning\\", \\"This is a warning message.\\") # Display an error message messagebox.showerror(\\"Error\\", \\"This is an error message.\\") # Block for a question question_response = messagebox.askquestion(\\"Question\\", \\"Do you like Python?\\") if question_response == \'yes\': print(\\"User likes Python\\") else: print(\\"User does not like Python\\") # Block for a yes/no/cancel question yesnocancel_response = messagebox.askyesnocancel(\\"Confirm\\", \\"Would you like to save changes?\\") if yesnocancel_response is True: print(\\"User chose to save\\") elif yesnocancel_response is False: print(\\"User chose not to save\\") else: print(\\"User cancelled the operation\\") close_root_window() if __name__ == \\"__main__\\": handle_user_interactions()"},{"question":"Objective: Create a custom HTML parser that extracts and organizes specific information from a given HTML string. You need to subclass `HTMLParser` and override specific methods to achieve the desired functionality. Problem Statement: Write a class `CustomHTMLParser` that inherits from `html.parser.HTMLParser`. The parser should parse the given HTML string and print the following information: 1. Number of times each HTML tag appears. 2. Text content inside all `<title>` tags combined into a single string. 3. Extract and print the URLs from all `<a>` and `<img>` tags as a list. Specifically, from the `href` attribute for `<a>` tags and the `src` attribute for `<img>` tags. Expected Input and Output Formats: - **Input**: A single string containing HTML content. - **Output**: Print the three required pieces of information as described below: 1. A dictionary where the keys are tag names and the values are the count of appearances of each tag. 2. A string containing the text inside all `<title>` tags concatenated together. 3. A list of URLs extracted from `href` attributes of `<a>` tags and `src` attributes of `<img>` tags. Implementation Details: 1. Implement the `CustomHTMLParser` class. 2. Override the necessary methods to count tags, extract text from `<title>` tags, and URLs from `<a>` and `<img>` tags. 3. Create a method `parse_html` that accepts an HTML string, feeds it to the parser, and prints the results. Below is the method signature for the `CustomHTMLParser`: ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tag_count = {} self.title_text = \\"\\" self.urls = [] def handle_starttag(self, tag, attrs): pass # Override to handle start tags def handle_endtag(self, tag): pass # Override to handle end tags def handle_data(self, data): pass # Override to handle data def parse_html(self, html_string): # Implement the logic to feed the HTML content to the parser and print results pass # Example usage html_content = \'\'\' <html> <head><title>Hello World</title></head> <body> <a href=\\"https://www.example.com\\">Example</a> <img src=\\"https://www.example.com/image.png\\" /> <p>Sample paragraph with <a href=\\"https://www.test.com\\">test link</a>.</p> </body> </html> \'\'\' parser = CustomHTMLParser() parser.parse_html(html_content) ``` Constraints: 1. The input HTML string will contain well-formed HTML. 2. Each tag and attribute will follow standard HTML conventions. 3. The URLs in the `href` and `src` attributes will be valid URLs. Example Output: For the given `html_content`, the output would be: ``` {\'html\': 1, \'head\': 1, \'title\': 1, \'body\': 1, \'a\': 2, \'img\': 1, \'p\': 1} Hello World [\'https://www.example.com\', \'https://www.example.com/image.png\', \'https://www.test.com\'] ```","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.tag_count = {} self.title_text = \\"\\" self.urls = [] self.current_tag = \\"\\" def handle_starttag(self, tag, attrs): # Count the start tags if tag in self.tag_count: self.tag_count[tag] += 1 else: self.tag_count[tag] = 1 # Extract URLs from <a> and <img> tags if tag == \'a\': for attr in attrs: if attr[0] == \'href\': self.urls.append(attr[1]) elif tag == \'img\': for attr in attrs: if attr[0] == \'src\': self.urls.append(attr[1]) self.current_tag = tag def handle_endtag(self, tag): self.current_tag = \\"\\" def handle_data(self, data): # Extract text from <title> tags if self.current_tag == \'title\': self.title_text += data def parse_html(self, html_string): self.feed(html_string) print(self.tag_count) print(self.title_text) print(self.urls) # Example usage html_content = \'\'\' <html> <head><title>Hello World</title></head> <body> <a href=\\"https://www.example.com\\">Example</a> <img src=\\"https://www.example.com/image.png\\" /> <p>Sample paragraph with <a href=\\"https://www.test.com\\">test link</a>.</p> </body> </html> \'\'\' parser = CustomHTMLParser() parser.parse_html(html_content)"},{"question":"Objective: You will demonstrate your ability to use Seaborn\'s `objects` interface to create complex visualizations. The core focus is on loading a dataset, plotting the relationships, and customizing the visualizations by applying normalization techniques and labeling. Problem Statement: 1. Load the `fmri` dataset using Seaborn\'s `load_dataset` function. 2. Create a line plot that shows `signal` over `timepoint` for each `subject`. 3. Normalize the `signal` values relative to the maximum signal value for each subject. 4. Constrain the normalization so that the baseline is defined by the first timepoint\'s `signal` value, and the output should be in percent changes. 5. Label the y-axis as \\"Percent change in signal from baseline\\". 6. Customize the plot by adding relevant titles and labels to enhance clarity. Constraints: - The `fmri` dataset must be used. - The baseline for normalization should be the first timepoint of each subject. Expected Input and Output Formats: **Input:** None (the dataset loading and plotting are handled within the code). **Output:** A Seaborn plot displayed as the final output. Example Function Signature: ```python import seaborn as sns import seaborn.objects as so def create_customized_fmri_plot(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Create a plot of signal vs. timepoint for each subject with specified customizations ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"subject\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in signal from baseline\\") .label(title=\\"Customized fMRI Signal Plot\\") .label(x=\\"Timepoint\\") ) # Call the function to display the plot create_customized_fmri_plot() ``` **Your Task:** Complete the function `create_customized_fmri_plot()` to meet the requirements outlined above, ensuring the plot displays accurately with normalization and labeling.","solution":"import seaborn as sns import seaborn.objects as so def create_customized_fmri_plot(): # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Normalize the signal relative to the first timepoint for each subject def normalize_signal(df): df = df.copy() first_timepoint_signal = df.loc[df[\'timepoint\'] == df[\'timepoint\'].min(), \'signal\'].values[0] df[\'signal\'] = ((df[\'signal\'] - first_timepoint_signal) / first_timepoint_signal) * 100 return df fmri_normalized = fmri.groupby(\'subject\').apply(normalize_signal).reset_index(drop=True) # Create a plot of signal vs. timepoint for each subject with specified customizations plot = ( so.Plot(fmri_normalized, x=\\"timepoint\\", y=\\"signal\\", color=\\"subject\\") .add(so.Lines()) .label(y=\\"Percent change in signal from baseline\\") .label(title=\\"Customized fMRI Signal Plot\\") .label(x=\\"Timepoint\\") ) plot.show() # Call the function to display the plot create_customized_fmri_plot()"},{"question":"You are a developer tasked with creating a utility function that reads a collection of plist files from a directory, extracts specific information, and writes this data to a new plist file. Specifically, you need to find the plist files that contain a given key and compile these keys and their values into a new dictionary. Finally, the resulting dictionary will be written to a new plist file in binary format. Function Signature ```python def compile_plist_data(directory: str, search_key: str, output_filepath: str) -> None: Compiles data from plist files in a directory that contain a specified key and writes the compiled data to a new plist file in binary format. Parameters: - directory (str): The directory path where plist files are located. - search_key (str): The key to search for in the plist files. - output_filepath (str): The path to the output plist file where the compiled data will be written. Returns: - None ``` Input - `directory` (str): A directory path as a string. The directory contains multiple plist files. - `search_key` (str): A string representing the key to search for in the plist files. - `output_filepath` (str): A string representing the path to the output plist file. Output - The function does not return anything. It writes the compiled data to the specified output filepath. Constraints - Assume all files in the directory are valid plist files. - The function should only process plist files that are either in XML or binary format. - If a plist file does not contain the search_key, it should be ignored. - The compiled output should be written in binary plist format. Example Usage ```python compile_plist_data(\'/path/to/plist/files\', \'targetKey\', \'/path/to/output/complied_data.plist\') ``` This function will search through all plist files in the provided directory for the key \'targetKey\'. It will then compile the values associated with \'targetKey\' into a new dictionary and write this dictionary to a new plist file at the specified output file path. Notes - You will need to use `plistlib.load` and `plistlib.dump` functions. - Consider handling any exceptions that may arise, such as file read/write errors, using appropriate try-except blocks. - Make sure to open files in binary mode as required by the `plistlib` functions.","solution":"import os import plistlib def compile_plist_data(directory: str, search_key: str, output_filepath: str) -> None: Compiles data from plist files in a directory that contain a specified key and writes the compiled data to a new plist file in binary format. Parameters: - directory (str): The directory path where plist files are located. - search_key (str): The key to search for in the plist files. - output_filepath (str): The path to the output plist file where the compiled data will be written. Returns: - None compiled_data = {} # Iterate through all files in the directory for filename in os.listdir(directory): if filename.endswith(\'.plist\'): file_path = os.path.join(directory, filename) try: with open(file_path, \'rb\') as file: plist_data = plistlib.load(file) if search_key in plist_data: compiled_data[filename] = plist_data[search_key] except Exception as e: print(f\\"Error processing file {file_path}: {e}\\") # Write the compiled data to the output file try: with open(output_filepath, \'wb\') as output_file: plistlib.dump(compiled_data, output_file) except Exception as e: print(f\\"Error writing to output file {output_filepath}: {e}\\")"},{"question":"**Question: Context Management with Context Variables** In this problem, you will implement a simplified version of a context management system using the `contextvars` module in Python. You are required to demonstrate your understanding of creating, setting, and getting context variables, as well as managing contexts for different threads. **Task:** 1. Create a context with a few context variables. 2. Implement functions to: - Set and get the values of context variables in the current context. - Save the current context and switch to a new context. - Restore a previously saved context. **Requirements:** 1. You should create at least two context variables: \\"user\\" and \\"request_id\\". 2. Implement the following functions: - `initialize_context() -> None`: Initializes the context and sets initial values for context variables. - `set_context_var(name: str, value: Any) -> None`: Sets the value of a specified context variable. - `get_context_var(name: str) -> Any`: Gets the value of a specified context variable. - `save_current_context() -> Any`: Saves the current context and returns a handle to it. - `switch_context(handle: Any) -> None`: Switches to a context represented by the given handle. - `restore_context(handle: Any) -> None`: Restores the context to a previously saved state. **Constraints:** - Ensure thread safety when managing contexts. - Handle errors gracefully, such as when trying to get the value of a non-existent context variable. - Performance should be considered; context switching should be efficient. **Performance Requirements:** - The functions should handle context switches in constant time. **Input and Output Formats:** - There are no direct inputs and outputs; instead, the correctness of your implementation will be tested using a series of assertions in a multithreaded environment. **Example Usage:** ```python initialize_context() set_context_var(\\"user\\", \\"Alice\\") set_context_var(\\"request_id\\", \\"12345\\") print(get_context_var(\\"user\\")) # Output: Alice print(get_context_var(\\"request_id\\")) # Output: 12345 saved_ctx = save_current_context() set_context_var(\\"user\\", \\"Bob\\") print(get_context_var(\\"user\\")) # Output: Bob restore_context(saved_ctx) print(get_context_var(\\"user\\")) # Output: Alice print(get_context_var(\\"request_id\\")) # Output: 12345 ``` Implement all the specified functions to complete the context management system.","solution":"import contextvars # Context variables context_user = contextvars.ContextVar(\\"user\\") context_request_id = contextvars.ContextVar(\\"request_id\\") def initialize_context(): Initializes the context and sets initial values for context variables. context_user.set(None) context_request_id.set(None) def set_context_var(name, value): Sets the value of a specified context variable. if name == \\"user\\": context_user.set(value) elif name == \\"request_id\\": context_request_id.set(value) else: raise ValueError(f\\"Context variable \'{name}\' does not exist.\\") def get_context_var(name): Gets the value of a specified context variable. if name == \\"user\\": return context_user.get() elif name == \\"request_id\\": return context_request_id.get() else: raise ValueError(f\\"Context variable \'{name}\' does not exist.\\") def save_current_context(): Saves the current context and returns a handle to it. token_user = context_user.set(context_user.get()) token_request_id = context_request_id.set(context_request_id.get()) return (token_user, token_request_id) def switch_context(handle): Switches to a context represented by the given handle. token_user, token_request_id = handle context_user.reset(token_user) context_request_id.reset(token_request_id) def restore_context(handle): Restores the context to a previously saved state. switch_context(handle)"},{"question":"Objective: Implement a distributed training setup using PyTorch\'s `torch.distributed` package. Your implementation should initialize the distributed process group, parallelize a simple neural network model training across multiple GPUs, and ensure synchronization of gradients between devices. # Task: 1. **Initialize Distributed Training Environment**: - Write code to initialize a distributed process group. Use NCCL backend for GPU training. 2. **Model Definition**: - Define a simple neural network model (e.g., a feedforward neural network) using `torch.nn.Module`. 3. **Distributed Training Loop**: - Implement a training loop that trains the model on a dummy dataset. - Ensure that gradients are synchronized across all processes. - Use `torch.distributed` utilities to ensure proper gradient averaging and optimization. # Requirements: - Use NCCL backend for distributed training. - Dummy dataset can be randomly generated tensor inputs and labels. - Implement model synchronization across devices. - Ensure proper cleanup of the distributed process group after training completes. # Input and Output: - There are no inputs to your script; the function should be self-contained. - The function should output training loss per epoch for all processes. # Constraints: - You must handle the initialization and cleanup of the distributed environment. - Assume you are running this on a multi-GPU machine. # Performance Requirements: - The training should run in a reasonable time frame (few minutes for a dummy dataset). - Synchronization should not cause significant slowdowns. # Example: Here is a skeleton to help you start: ```python import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP import os def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.fc2 = nn.Linear(10, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(rank, world_size): setup(rank, world_size) # Create model and move it to GPU with id rank model = Net().to(rank) ddp_model = DDP(model, device_ids=[rank]) criterion = nn.MSELoss().to(rank) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) for epoch in range(10): inputs = torch.randn(20, 10).to(rank) labels = torch.randn(20, 5).to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() if rank == 0: # Print from rank 0 to avoid clutter print(f\'Epoch {epoch}, Loss: {loss.item()}\') cleanup() # To run this script, you would use the following command in terminal: # python -m torch.distributed.launch --nproc_per_node=<NUM_GPUS> <SCRIPT_NAME>.py ``` You need to complete the above code with proper model synchronization and training procedures.","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP import os def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.fc2 = nn.Linear(10, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train(rank, world_size): setup(rank, world_size) # Create model and move it to GPU with id rank model = Net().to(rank) ddp_model = DDP(model, device_ids=[rank]) criterion = nn.MSELoss().to(rank) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) for epoch in range(10): inputs = torch.randn(20, 10).to(rank) labels = torch.randn(20, 5).to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() if rank == 0: # Print from rank 0 to avoid clutter print(f\'Epoch {epoch}, Loss: {loss.item()}\') cleanup() if __name__ == \\"__main__\\": world_size = torch.cuda.device_count() if world_size < 2: raise ValueError(\\"This script requires at least 2 GPUs, but got {world_size}\\") mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"},{"question":"**Task**: Create a Python function to manage tar archives for given files and directories. 1. **Function Name**: `manage_tar_archive` 2. **Parameters**: - `operation` (str): The operation to perform. It can be one of the following: - `\'create\'` to create a new tar archive. - `\'extract\'` to extract an existing tar archive. - `\'list\'` to list contents of the tar archive. - `tarfile_name` (str): Name of the tar archive file to create, extract or list. - `file_paths` (list of str): List of file and directory paths to include in the tar archive for creation or extraction. This parameter will be used only for creation and extraction. - `compression` (str, optional): Compression type to use for the tar archive. It can be `\'gz\'`, `\'bz2\'`, or `\'xz\'`. Default is `None`. 3. **Returns**: - In the case of listing, return a list of files contained in the tar archive. - In other cases, the function does not return anything. 4. **Constraints and Requirements**: - If the `operation` is `\'create\'`, ensure that the specified `tarfile_name` does not already exist to avoid overwriting. - Handle any potential errors such as invalid file paths, inaccessible files, and invalid tar archives gracefully. - If the `operation` is `\'extract\'`, extract the files to the current working directory. - Use appropriate methods and classes provided by the `tarfile` module to handle tar archives. **Example Usage**: ```python def manage_tar_archive(operation, tarfile_name, file_paths, compression=None): # TODO: Implement this function # Create a tar archive with gzip compression manage_tar_archive(\'create\', \'archive.tar.gz\', [\'file1.txt\', \'dir1\'], \'gz\') # List contents of the tar archive print(manage_tar_archive(\'list\', \'archive.tar.gz\', [])) # Extract tar archive manage_tar_archive(\'extract\', \'archive.tar.gz\', []) ``` **Performance Considerations**: - Ensure the function runs efficiently for reasonably large tar archives (up to several hundred files). - Consider memory usage when dealing with large files or directories. **Edge Cases**: - Handle scenarios where file paths are invalid or inaccessible. - Ensure that extraction does not overwrite existing files without warning the user. - Validate the `operation` parameter and provide appropriate error messages for unsupported operations.","solution":"import tarfile import os def manage_tar_archive(operation, tarfile_name, file_paths, compression=None): Manages tar archives for given files and directories. Parameters: operation (str): The operation to perform (\'create\', \'extract\', \'list\'). tarfile_name (str): Name of the tar archive file. file_paths (list of str): List of file and directory paths for creation or extraction. compression (str, optional): Compression type (\'gz\', \'bz2\', \'xz\'). Default is None. Returns: If listing, returns a list of file names in the archive. # Define the mode based on the operation compression_modes = {None: \'w\', \'gz\': \'w:gz\', \'bz2\': \'w:bz2\', \'xz\': \'w:xz\'} mode = compression_modes.get(compression, \'w\') if operation == \'create\': if os.path.exists(tarfile_name): raise FileExistsError(f\\"File \'{tarfile_name}\' already exists.\\") with tarfile.open(tarfile_name, mode) as tar: for file in file_paths: tar.add(file, arcname=os.path.basename(file)) elif operation == \'extract\': with tarfile.open(tarfile_name, \'r:*\') as tar: tar.extractall() elif operation == \'list\': with tarfile.open(tarfile_name, \'r:*\') as tar: return tar.getnames() else: raise ValueError(f\\"Unsupported operation \'{operation}\' specified.\\")"},{"question":"Advanced Regular Expressions in Python **Objective:** Assess the student\'s ability to apply regular expressions to solve complex string manipulation problems using Python. **Question:** You are tasked with writing a function that extracts information from a given text using regular expressions. The function should identify and return all email addresses and phone numbers contained in the input text. Email addresses and phone numbers should be formatted correctly according to the following criteria: - Email addresses follow the format `username@domain.extension`, where: - `username` consists of alphanumeric characters, dots (.), underscores (_), and hyphens (-). - `domain` consists of alphanumeric characters and hyphens (-). - `extension` consists of 2 to 4 alphabetic characters. - Phone numbers follow the format `(XXX) XXX-XXXX` or `XXX-XXX-XXXX`, where: - `X` is a digit from 0-9. **Function Signature:** ```python def extract_contacts(text: str) -> dict: Extracts email addresses and phone numbers from a given text. Args: text (str): The input text containing email addresses and phone numbers. Returns: dict: A dictionary with two keys: \'emails\' and \'phones\'. The value for \'emails\' is a list of extracted email addresses, and the value for \'phones\' is a list of extracted phone numbers. ``` **Input:** - `text`: A string containing the text to extract email addresses and phone numbers from. **Output:** - A dictionary with two keys: - `\'emails\'`: A list of extracted email addresses. - `\'phones\'`: A list of extracted phone numbers. **Example:** ```python text = Here are some contacts: - Email: john.doe@example.com - Phone: (123) 456-7890 You can also reach me at jane.smith@company.co or 987-654-3210. result = extract_contacts(text) print(result) ``` **Expected Output:** ```python { \'emails\': [\'john.doe@example.com\', \'jane.smith@company.co\'], \'phones\': [\'(123) 456-7890\', \'987-654-3210\'] } ``` **Constraints:** - Assume the input text is reasonably sized and contains only validly formatted email addresses and phone numbers according to the specified criteria. - Do not use external libraries other than the `re` module in Python. **Performance Requirements:** - The function should be able to handle input text up to 1 MB in size efficiently. - The solution should optimize the use of regular expressions to ensure it runs in a reasonable time frame for typical input sizes. **Hints:** - Review the \\"Regular Expression HOWTO\\" in the Python documentation to understand the syntax and functions available in the `re` module. - Use appropriate regular expression patterns to match email addresses and phone numbers. - Use capturing groups to extract and store the required information.","solution":"import re def extract_contacts(text: str) -> dict: Extracts email addresses and phone numbers from a given text. Args: text (str): The input text containing email addresses and phone numbers. Returns: dict: A dictionary with two keys: \'emails\' and \'phones\'. The value for \'emails\' is a list of extracted email addresses, and the value for \'phones\' is a list of extracted phone numbers. # Regex pattern for email addresses email_pattern = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,4}\' # Regex pattern for phone numbers in formats (XXX) XXX-XXXX or XXX-XXX-XXXX phone_pattern = r\'(?d{3})?[-.s]?d{3}[-.s]?d{4}\' # Find all matching email addresses emails = re.findall(email_pattern, text) # Find all matching phone numbers phones = re.findall(phone_pattern, text) return { \'emails\': emails, \'phones\': phones }"},{"question":"**Question: File Locking and Control using `fcntl` module** You have been tasked to create a Python script that demonstrates the use of file locking mechanisms along with performing control operations on file descriptors using the `fcntl` module. The script should: 1. Create or open a file named `testfile.txt`. 2. Apply an exclusive lock on the file to prevent other processes from accessing it simultaneously. 3. Change the status flags of the file descriptor to append mode (so that writing always happens at the end of the file). 4. Write a given string `data` to the file and then release the lock. 5. Ensure that all operations handle potential exceptions gracefully and release any locks in case of an error. **Function Signature:** ```python def control_and_lock_file(filepath: str, data: str) -> None: ``` **Input:** - `filepath`: A string representing the path to the file (e.g., \\"testfile.txt\\"). - `data`: A string containing the data to be written to the file. **Output:** - None. However, the function should perform file operations as described and ensure proper locking and file control mechanisms using `fcntl`. **Constraints:** - Assume the `filepath` is always writable and exists. - Handle any exceptions that may arise from file operations, and ensure locks are properly released. **Example Usage:** ```python control_and_lock_file(\\"testfile.txt\\", \\"Hello, World!\\") ``` In this example, the function should: 1. Open `testfile.txt`. 2. Apply an exclusive lock. 3. Change the file descriptor mode to append. 4. Write \\"Hello, World!\\" to the file. 5. Release the lock and close the file. **Hint:** Consider using `fcntl.lockf()` for locking and `fcntl.fcntl()` for changing file status flags.","solution":"import fcntl import os def control_and_lock_file(filepath: str, data: str) -> None: try: # Open the file with open(filepath, \'a+\') as file: # Apply an exclusive lock on the file fcntl.lockf(file, fcntl.LOCK_EX) # Change the file descriptor status flags to append mode fd = file.fileno() flags = fcntl.fcntl(fd, fcntl.F_GETFL) fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_APPEND) # Write data to the file file.write(data + \'n\') # Release the lock fcntl.lockf(file, fcntl.LOCK_UN) except Exception as e: print(f\\"An error occurred: {e}\\") # Usage example: # control_and_lock_file(\\"testfile.txt\\", \\"Hello, World!\\")"},{"question":"# GUI Message Box Application You are tasked with developing a small GUI-based application using Tkinter that makes use of various message boxes provided by the `tkinter.messagebox` module. The objective of this application is to: 1. Display a main window with a button labeled \\"Start\\". 2. When the \\"Start\\" button is clicked, the application should: - Use `tkinter.messagebox.showinfo()` to show a welcome message. - Use `tkinter.messagebox.askyesno()` to ask the user if they would like to continue. - If the user selects \\"Yes\\", display a `tkinter.messagebox.showwarning()` that warns about some imaginary condition. - If the user selects \\"No\\", display a `tkinter.messagebox.showerror()` indicating that the process was canceled. Your goal is to implement this GUI application. Ensure that your code performs the following: - Handles user interactions effectively, capturing their responses and displaying the appropriate message box afterwards. - Has appropriate title and message for each message box to make the interaction clear and intuitive for the user. **Input:** - No input from the user is required for initial setup. Input will be handled via button presses and message box responses. **Output:** - The appropriate message boxes will be displayed based on user interactions. **Technical constraints:** - Use Python 3.10. - The `tkinter` module should be used for the entire GUI. - Ensure error handling for possible exceptions that may occur during the execution of your application. # Example Workflow of the Application 1. Main window with \\"Start\\" button. 2. User clicks \\"Start\\" 3. `tkinter.messagebox.showinfo()` displays \\"Welcome to the application!\\" with title \\"Welcome\\". 4. `tkinter.messagebox.askyesno()` asks \\"Do you want to continue?\\" with title \\"Confirmation\\". 5. If the user selects \\"Yes\\", display a warning box with `tkinter.messagebox.showwarning()` showing \\"This is a warning!\\" with title \\"Warning\\". 6. If the user selects \\"No\\", display an error message using `tkinter.messagebox.showerror()` showing \\"Process canceled by user.\\" with title \\"Canceled\\". Write the complete implementation of the described application in Python.","solution":"import tkinter as tk from tkinter import messagebox def on_start(): messagebox.showinfo(\\"Welcome\\", \\"Welcome to the application!\\") user_response = messagebox.askyesno(\\"Confirmation\\", \\"Do you want to continue?\\") if user_response: messagebox.showwarning(\\"Warning\\", \\"This is a warning!\\") else: messagebox.showerror(\\"Canceled\\", \\"Process canceled by user.\\") def create_main_window(): root = tk.Tk() root.title(\\"Main Window\\") start_button = tk.Button(root, text=\\"Start\\", command=on_start) start_button.pack(pady=20) root.geometry(\\"300x200\\") root.mainloop() if __name__ == \\"__main__\\": create_main_window()"},{"question":"Advanced Set Manipulation **Objective:** Demonstrate your understanding of Python sets by implementing a custom Python class that mimics some of the `collections.Counter` functionality but uses sets for storage. # Problem Statement Create a class `SetCounter` that acts like a counter but internally uses sets to count occurrences of immutable items. Your class should provide the following functionalities: 1. **Initialization**: The class should be initialized with an iterable of hashable items. 2. **Addition of items**: A method to add an item to the counter. 3. **Remove items**: A method that attempts to remove an item from the counter. 4. **Item count**: A method to get the current count of a specific item. 5. **Total unique items**: A method that returns the number of unique items in the counter. 6. **Most common items**: A method that returns a list of the `n` most common items and their counts in descending order of frequency. 7. **Least common items**: A method that returns a list of the least common items. # Function Details `__init__(self, iterable)` - **Input:** An iterable of hashable items. - **Output:** Initializes the `SetCounter` instance. `add(self, item)` - **Input:** A hashable item. - **Output:** Adds the item to the counter or increases its count. `remove(self, item)` - **Input:** A hashable item. - **Output:** Decreases the count of the item or removes it completely if the count becomes zero. `count(self, item)` - **Input:** A hashable item. - **Output:** Returns the current count of the item. `total_unique(self)` - **Output:** Returns the total number of unique items in the counter. `most_common(self, n)` - **Input:** An integer `n`. - **Output:** Returns a list of the `n` most common items and their counts in descending order of frequency. `least_common(self)` - **Output:** Returns a list of the least common items. # Constraints - You should handle errors appropriately. For example, if removing a non-existing item, raise a suitable exception. - Assume all items are hashable. - Your implementation should aim for efficiency in terms of time and space. # Example Usage ```python sc = SetCounter([\'apple\', \'banana\', \'apple\', \'orange\', \'banana\', \'apple\']) sc.add(\'banana\') print(sc.count(\'apple\')) # Output: 3 print(sc.count(\'banana\')) # Output: 3 sc.remove(\'banana\') print(sc.count(\'banana\')) # Output: 2 print(sc.total_unique()) # Output: 3 print(sc.most_common(2)) # Output: [(\'apple\', 3), (\'banana\', 2)] print(sc.least_common()) # Output: [(\'orange\', 1)] ``` # Notes - Use Python\'s `set` for storing unique items. - You may utilize auxiliary data structures (like dictionaries) to assist in counting. - Efficiency and proper error handling will be considered in the evaluation. Good luck, and happy coding!","solution":"from collections import defaultdict class SetCounter: def __init__(self, iterable): self.counts = defaultdict(int) self.items = set() for item in iterable: self.add(item) def add(self, item): if item not in self.items: self.items.add(item) self.counts[item] += 1 def remove(self, item): if item not in self.items: raise ValueError(f\\"Item \'{item}\' not found in SetCounter\\") self.counts[item] -= 1 if self.counts[item] == 0: self.items.remove(item) del self.counts[item] def count(self, item): return self.counts.get(item, 0) def total_unique(self): return len(self.items) def most_common(self, n): sorted_items = sorted(self.counts.items(), key=lambda x: x[1], reverse=True) return sorted_items[:n] def least_common(self): sorted_items = sorted(self.counts.items(), key=lambda x: x[1]) return sorted_items"},{"question":"# Question: Exception Handling and Custom Exceptions in Python You are tasked with developing a library management system. One crucial component of this system is to handle various types of errors gracefully. You are required to implement a function `manage_library` that performs the following operations: 1. Adds a new book to the library. 2. Removes a book from the library. 3. Displays the details of a book. 4. Lists all available books. You must handle the following exceptions appropriately: - `ValueError`: Raised if a book ID is not an integer. - `KeyError`: Raised if a book ID does not exist when attempting to remove or display details of a book. - Custom Exception `LibraryError`: Raised for general library-related errors (e.g., attempting to add a book that already exists). - Ensure files are properly closed after operations using the `with` statement. The library data must be stored in a dictionary where keys are book IDs and values are book titles. Implement the function `manage_library` with proper exception handling. **Function Signature:** ```python def manage_library(operation: str, book_id: int = None, book_title: str = None) -> str: pass ``` **Inputs:** - `operation` (str): The operation to be performed. It can be one of `add`, `remove`, `details`, `list`. - `book_id` (int, optional): The ID of the book. Required for `add`, `remove`, and `details` operations. - `book_title` (str, optional): The title of the book. Required for `add` operation. **Outputs:** - On successful operation, return a success message. - Raise appropriate exceptions for any errors encountered. **Constraints:** - `book_id` must be a positive integer. - `book_title` must be a non-empty string. **Example:** ```python # Example 1: Adding a book to the library manage_library(\'add\', 101, \'Python Programming\') # Output: \\"Book added successfully.\\" # Example 2: Removing a book from the library manage_library(\'remove\', 101) # Output: \\"Book removed successfully.\\" # Example 3: Attempting to remove a non-existent book try: manage_library(\'remove\', 999) except KeyError as e: print(e) # Output: \\"Book ID 999 does not exist.\\" # Example 4: Displaying details of a book manage_library(\'details\', 101) # Output: \\"Book ID: 101, Title: Python Programming\\" # Example 5: Listing all books manage_library(\'list\') # Output: \\"Books available: [(101, \'Python Programming\')]\\" ``` **Notes:** - Use a global dictionary to store the library\'s book data. - Implement custom exception `LibraryError` for handling general library-related errors.","solution":"class LibraryError(Exception): pass library = {} def manage_library(operation: str, book_id: int = None, book_title: str = None) -> str: global library try: if operation == \'add\': if not isinstance(book_id, int) or book_id <= 0: raise ValueError(\\"Invalid book ID. It must be a positive integer.\\") if not book_title or not isinstance(book_title, str): raise ValueError(\\"Invalid book title. It must be a non-empty string.\\") if book_id in library: raise LibraryError(f\\"Book with ID {book_id} already exists.\\") library[book_id] = book_title return \\"Book added successfully.\\" elif operation == \'remove\': if not isinstance(book_id, int) or book_id <= 0: raise ValueError(\\"Invalid book ID. It must be a positive integer.\\") if book_id not in library: raise KeyError(f\\"Book ID {book_id} does not exist.\\") del library[book_id] return \\"Book removed successfully.\\" elif operation == \'details\': if not isinstance(book_id, int) or book_id <= 0: raise ValueError(\\"Invalid book ID. It must be a positive integer.\\") if book_id not in library: raise KeyError(f\\"Book ID {book_id} does not exist.\\") return f\\"Book ID: {book_id}, Title: {library[book_id]}\\" elif operation == \'list\': return \\"Books available: \\" + str(list(library.items())) else: raise LibraryError(\\"Invalid operation.\\") except ValueError as e: raise e except KeyError as e: raise e except LibraryError as e: raise e"},{"question":"# Python Coding Assessment: Implementing and Manipulating Slice Objects **Objective:** You are required to implement Python functions that interact with and handle slice objects to perform various sequence manipulations. The goal is to demonstrate your understanding of both the slicing concepts in Python and the lower-level `PySlice` functionalities. # Function 1: Create a Slice **Task:** Write a function `create_slice(start, stop, step)` that takes three arguments `start`, `stop`, and `step` (all integers) and returns a Python slice object created using `PySlice_New`. **Input Constraints:** - `start`, `stop`, and `step` are all integers and can be `None`. **Output:** - A `slice` object initialized with `start`, `stop`, and `step`. ```python def create_slice(start: int, stop: int, step: int) -> slice: pass ``` # Function 2: Apply Slice to a Sequence **Task:** Write a function `apply_slice(seq, sl)` that applies a slice object `sl` to a sequence `seq` (which can be a list, tuple, or string). The function should return the resulting subsequence after applying the slice. **Input Constraints:** - `seq` can be a list, tuple, or string. - `sl` must be a valid slice object. **Output:** - A subsequence of `seq` as determined by the slice `sl`. ```python def apply_slice(seq, sl) -> type(seq): pass ``` # Function 3: Adjust Slice Indices **Task:** Write a function `adjust_slice_indices(seq, start, stop, step)` that takes a sequence `seq` and a slice with `start`, `stop`, and `step` indices. Adjust and clip these indices properly to fit within the bounds of the sequence and return the adjusted indices and the length of the resulting slice. **Input Constraints:** - `seq` can be a list, tuple, or string. - `start`, `stop`, and `step` are integers and can be `None`. **Output:** - A tuple `(adjusted_start, adjusted_stop, adjusted_step, slice_length)` representing the adjusted indices and the length of the slice. ```python def adjust_slice_indices(seq, start: int, stop: int, step: int): pass ``` **Example Workflow:** 1. Create a slice using `create_slice(start=1, stop=10, step=2)` 2. Apply this slice to a list `seq=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]` using `apply_slice` 3. Adjust indices for this slice using `adjust_slice_indices(seq, 1, 10, 2)` The combined use of these functions will test the understanding of slicing both at the Python level and working with the lower-level functionalities provided by `PySlice`. # Notes: - Ensure proper type-checking and error handling. - Provide docstrings and comments to explain the functionality. - Consider edge cases and performance implications where necessary.","solution":"def create_slice(start, stop, step): Creates and returns a slice object with the given start, stop, and step. Args: start (int or None): The start index of the slice. stop (int or None): The stop index of the slice. step (int or None): The step value of the slice. Returns: slice: The created slice object. return slice(start, stop, step) def apply_slice(seq, sl): Applies a slice object to a sequence and returns the sliced subsequence. Args: seq (list, tuple, or string): The sequence to be sliced. sl (slice): The slice object to be applied. Returns: type(seq): The sliced subsequence. return seq[sl] def adjust_slice_indices(seq, start, stop, step): Adjusts the slice indices to fit within the bounds of the sequence. Args: seq (list, tuple, or string): The sequence on which the slice is to be applied. start (int or None): The start index of the slice. stop (int or None): The stop index of the slice. step (int or None): The step value of the slice. Returns: tuple: A tuple containing the adjusted start, stop, step, and the length of the resulting slice. slice_obj = slice(start, stop, step) adjusted_start, adjusted_stop, adjusted_step = slice_obj.indices(len(seq)) slice_length = len(range(adjusted_start, adjusted_stop, adjusted_step)) return adjusted_start, adjusted_stop, adjusted_step, slice_length"},{"question":"# PyTorch Coding Assessment Question Objective: Implement a function using PyTorch that processes a tensor and performs various operations on its dimensions. This function aims to test your understanding of tensor properties like size, manipulation of dimensions, and fundamental tensor operations. Function Signature: ```python def tensor_operations(t: torch.Tensor) -> Tuple[torch.Size, torch.Tensor, torch.Tensor]: Process the input tensor and perform a series of operations on its dimensions. Args: - t (torch.Tensor): Input tensor of any shape. Returns: - Tuple[torch.Size, torch.Tensor, torch.Tensor]: A tuple containing: 1. The size of the input tensor. 2. The tensor reshaped to a different shape (specified in the function). 3. A tensor that is a contiguous flattened version of the reshaped tensor. ``` Input: - `t`: A PyTorch tensor of any shape. Output: - A tuple containing: 1. The size of the input tensor as a `torch.Size` object. 2. The tensor reshaped to `(total_number_of_elements/2, 2)` if possible, otherwise an error should be raised. 3. A contiguous flattened tensor of the reshaped tensor. Constraints: - The reshaping operation should only be allowed if the total number of elements in the tensor is even. If it is not possible, the function should raise a `ValueError`. Example: ```python import torch # Example tensor x = torch.ones(4, 10) # Calling the function tensor_size, reshaped_tensor, flattened_tensor = tensor_operations(x) print(tensor_size) # Expected: torch.Size([4, 10]) print(reshaped_tensor) # Expected: tensor of shape (20, 2) print(flattened_tensor.shape) # Expected: torch.Size([40]) ``` Notes: 1. Ensure that you handle tensors of different shapes and sizes appropriately. 2. Make sure the reshaped tensor is contiguous before flattening it. This problem assesses your understanding of tensor manipulation, including obtaining tensor sizes, reshaping, and handling tensor contiguity and flattening.","solution":"import torch from typing import Tuple def tensor_operations(t: torch.Tensor) -> Tuple[torch.Size, torch.Tensor, torch.Tensor]: Process the input tensor and perform a series of operations on its dimensions. Args: - t (torch.Tensor): Input tensor of any shape. Returns: - Tuple[torch.Size, torch.Tensor, torch.Tensor]: A tuple containing: 1. The size of the input tensor. 2. The tensor reshaped to a different shape (specified in the function). 3. A tensor that is a contiguous flattened version of the reshaped tensor. # Get the size of the input tensor tensor_size = t.size() # Get the total number of elements in the tensor total_elements = t.numel() # Check if the total number of elements is even if total_elements % 2 != 0: raise ValueError(\\"The total number of elements in the tensor must be even for reshaping.\\") # Reshape the tensor to (total_elements / 2, 2) reshaped_tensor = t.view(total_elements // 2, 2) # Get a contiguous flattened version of the reshaped tensor flattened_tensor = reshaped_tensor.contiguous().view(-1) return tensor_size, reshaped_tensor, flattened_tensor"},{"question":"**Coding Challenge: Parallel Matrix Multiplication Using Multiprocessing** **Objective:** Implement a function to perform matrix multiplication using Python\'s `multiprocessing` module. This task aims to assess your understanding of creating processes, using pools, and inter-process communication. **Problem Statement:** You are required to write a function `parallel_matrix_multiply(A, B)`, where `A` and `B` are two matrices represented as lists of lists. The function should return the product of `A` and `B` using multiprocessing to speed up the computation. **Function Signature:** ```python def parallel_matrix_multiply(A: List[List[int]], B: List[List[int]]) -> List[List[int]]: pass ``` **Input:** - `A`: A list of lists representing matrix `A`, where each sub-list is a row in `A`. - `B`: A list of lists representing matrix `B`, where each sub-list is a row in `B`. **Output:** - A list of lists representing the matrix product `A * B`. **Constraints:** 1. The number of columns in `A` should be equal to the number of rows in `B`. 2. The elements of matrices `A` and `B` should be integers. 3. The dimensions of `A` and `B` will be such that they can fit into memory. **Example:** ```python A = [ [1, 2], [3, 4] ] B = [ [5, 6], [7, 8] ] print(parallel_matrix_multiply(A, B)) ``` Expected Output: ``` [[19, 22], [43, 50]] ``` **Requirements:** - Use `multiprocessing.Pool` for parallel computations. - Ensure that each worker process computes one or more elements of the resulting matrix. - Manage the synchronization and aggregation of results properly to avoid data corruption. **Guidelines:** - Split the task of computation into smaller chunks that can be distributed among multiple processes. - Each process should compute the elements of a subset of the resulting matrix. - Use queues or other communication mechanisms to gather results from worker processes. - Optimize the process creation and manage resources efficiently. Good luck, and make sure to handle edge cases and follow good coding practices!","solution":"from multiprocessing import Pool def matrix_element(A, B, row, col): Compute a single element of the resulting matrix return sum(A[row][i] * B[i][col] for i in range(len(B))) def parallel_matrix_multiply(A, B): Multiply two matrices A and B using multiprocessing num_rows_A = len(A) num_cols_A = len(A[0]) num_cols_B = len(B[0]) # Create a pool of workers with Pool(processes=None) as pool: # Create arguments for each element in the resulting matrix args = [(A, B, row, col) for row in range(num_rows_A) for col in range(num_cols_B)] # Compute each element in parallel result = pool.starmap(matrix_element, args) # Construct the resulting matrix from the flat list result_matrix = [result[i * num_cols_B:(i + 1) * num_cols_B] for i in range(num_rows_A)] return result_matrix"},{"question":"Building a Custom Email Alert System **Objective**: Design and implement a Python function that sends an email notification when an event occurs. This will demonstrate your understanding of the `smtplib` module, handling exceptions, and constructing an email message. **Function Signature**: ```python def send_email_alert(smtp_server: str, port: int, sender_email: str, recipient_emails: list, subject: str, body: str, login_user: str, login_pass: str) -> None: pass ``` **Description**: Implement the function `send_email_alert` which sends an email with the specified `subject` and `body` from `sender_email` to all `recipient_emails`. Authenticate using `login_user` and `login_pass` before sending the email. The function should adhere to the following specifications: **Parameters**: - `smtp_server` (str): The SMTP server address (e.g., \'smtp.example.com\'). - `port` (int): The port number to use for the SMTP connection (e.g., 587 for TLS or 465 for SSL). - `sender_email` (str): The email address of the sender. - `recipient_emails` (list): A list of recipient email addresses. - `subject` (str): The subject of the email. - `body` (str): The body content of the email. - `login_user` (str): The username for SMTP authentication. - `login_pass` (str): The password for SMTP authentication. **Constraints**: - Do not use any external libraries beyond those in Python\'s standard library. - Ensure the function handles common exceptions like connection errors, authentication failures, and invalid addresses gracefully. - The function should attempt to use `starttls` if `port` is 587. - If SSL connection is required (`port` is 465), use the `SMTP_SSL` class. - The function should support plain text emails only. **Example Usage**: ```python smtp_server = \\"smtp.example.com\\" port = 587 sender_email = \\"sender@example.com\\" recipient_emails = [\\"recipient1@example.com\\", \\"recipient2@example.com\\"] subject = \\"Test Alert\\" body = \\"This is a test email alert\\" login_user = \\"your_username\\" login_pass = \\"your_password\\" send_email_alert(smtp_server, port, sender_email, recipient_emails, subject, body, login_user, login_pass) ``` **Output**: None. The function sends the email and handles any errors internally. **Additional Requirements**: - You must handle the following exceptions and print appropriate error messages for them: `SMTPConnectError`, `SMTPAuthenticationError`, `SMTPRecipientsRefused`, `SMTPDataError`, and `SMTPServerDisconnected`. - Implement basic validation checks for the input parameters, such as ensuring `recipient_emails` is a list with at least one email address.","solution":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart def send_email_alert(smtp_server: str, port: int, sender_email: str, recipient_emails: list, subject: str, body: str, login_user: str, login_pass: str) -> None: Sends an email alert with the specified subject and body. if not isinstance(recipient_emails, list) or not recipient_emails: print(\\"Recipient emails must be a non-empty list\\") return # Setup the MIME message = MIMEMultipart() message[\'From\'] = sender_email message[\'To\'] = \\", \\".join(recipient_emails) message[\'Subject\'] = subject message.attach(MIMEText(body, \'plain\')) try: if port == 465: server = smtplib.SMTP_SSL(smtp_server, port) else: server = smtplib.SMTP(smtp_server, port) server.starttls() server.login(login_user, login_pass) server.sendmail(sender_email, recipient_emails, message.as_string()) server.quit() print(f\'Email sent successfully to {recipient_emails}\') except smtplib.SMTPConnectError: print(\\"Failed to connect to the server. Wrong server address or port?\\") except smtplib.SMTPAuthenticationError: print(\\"Failed to login. Wrong username or password?\\") except smtplib.SMTPRecipientsRefused: print(\\"All recipient addresses were refused. Invalid recipient email addresses?\\") except smtplib.SMTPDataError: print(\\"The SMTP server refused to accept the message data.\\") except smtplib.SMTPServerDisconnected: print(\\"The server unexpectedly disconnected.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Async Task and Network Programming with asyncio** # Problem Statement You are to implement an asynchronous solution that fulfills the following requirements: 1. **Subprocess Management**: - Create an asynchronous function that runs a subprocess to execute a shell command (\\"ping -c 1 google.com\\" to ping Google\'s server). - Capture the output of this command. 2. **Task Coordination and Queues**: - Use an `asyncio.Queue` to manage multiple URL pings concurrently. Populate the queue with a list of URLs provided. 3. **Network I/O with Streams**: - Implement an asynchronous TCP server that handles incoming connections and serves the output of the results from the `ping` subprocess. 4. **Synchronization and Timeout Management**: - Ensure that all subprocess tasks are shielded from cancellation using `asyncio.shield`. - Each ping result should be awaited with a timeout of 5 seconds. Handle timeouts gracefully using `asyncio.TimeoutError`. # Function Signature ```python import asyncio async def ping_url(url: str) -> str: # Run a subprocess to ping the URL and return the result. pass async def process_queue(url_queue: asyncio.Queue): # Process URLs from the queue, ping them, and store the result. pass async def tcp_server(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): # Handle incoming TCP connections and send ping results. pass async def main(urls: List[str]): # Main function to create the queue, populate it with URLs, # and start the TCP server. pass ``` # Input - `urls`: A list of URLs to be pinged. # Output - The main function should start the server and ensure all URLs are processed by the queue. # Constraints - Use `asyncio.create_task` to manage tasks. - Ensure proper exception handling for timeouts and cancellations. - Use at least one queue for managing the URL processes. - Implement the TCP server to listen on `localhost` and port `8888`. # Example Usage ```python urls = [\\"google.com\\", \\"stackoverflow.com\\", \\"github.com\\"] asyncio.run(main(urls)) # This should run the server and process the URLs. ``` **Note**: You will need to test the solution on your local machine with proper network access and ensure that `ping` is a valid shell command.","solution":"import asyncio import subprocess from asyncio import Queue from typing import List async def ping_url(url: str) -> str: Run a subprocess to ping the URL and return the result. try: proc = await asyncio.create_subprocess_shell( f\\"ping -c 1 {url}\\", stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE, ) stdout, stderr = await proc.communicate() if proc.returncode == 0: return stdout.decode().strip() else: return stderr.decode().strip() except Exception as e: return str(e) async def process_queue(url_queue: Queue): Process URLs from the queue, ping them, and store the result. while not url_queue.empty(): url = await url_queue.get() try: result = await asyncio.wait_for(asyncio.shield(ping_url(url)), timeout=5) except asyncio.TimeoutError: result = f\\"Timeout while pinging {url}\\" print(result) url_queue.task_done() async def tcp_server(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): Handle incoming TCP connections and send ping results. data = await reader.read(100) message = data.decode().strip() result = await ping_url(message) writer.write(result.encode()) await writer.drain() writer.close() await writer.wait_closed() async def main(urls: List[str]): Main function to create the queue, populate it with URLs, and start the TCP server. url_queue = Queue() for url in urls: await url_queue.put(url) # Create a task for processing the queue processing_task = asyncio.create_task(process_queue(url_queue)) # Run TCP server server = await asyncio.start_server(tcp_server, \'127.0.0.1\', 8888) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') # Ensure the server runs as long as the processing task async with server: await server.serve_forever() await processing_task # Example Usage: # urls = [\\"google.com\\", \\"stackoverflow.com\\", \\"github.com\\"] # asyncio.run(main(urls))"},{"question":"Create a balanced synthetic dataset for a binary classification problem using scikit-learn\'s dataset generation utilities. Your task is to write a function `generate_and_visualize_data` that generates the dataset and visualizes it. The generated dataset should satisfy the following constraints: 1. The dataset must contain exactly two classes. 2. Each class should have two clusters of points. 3. The dataset must contain a total of 1000 samples. 4. Use exactly two informative features and no redundant features. 5. Introduce a certain level of noise to make the problem non-trivial. The function should perform the following steps: 1. Generate the dataset using `make_classification`. 2. Create a scatter plot of the dataset, using different colors for different classes. Function Signature ```python def generate_and_visualize_data(): pass ``` Constraints - You should use `random_state=42` to ensure reproducibility. - The scatter plot should clearly label the two classes. Example Output After running your function, it should display a scatter plot of the dataset generated. ```python generate_and_visualize_data() ``` Sample Output Visualization The scatter plot should display points color-coded based on their class label, with the structure defined by the input constraints. **Hints:** - Use the `make_classification` function from `sklearn.datasets`. - Use `matplotlib.pyplot` for visualization. - Make sure your plot is labeled and has a title for clarity. **Expected Code Implementation:** ```python import matplotlib.pyplot as plt from sklearn.datasets import make_classification def generate_and_visualize_data(): X, y = make_classification( n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, random_state=42, n_classes=2 ) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\'bwr\', alpha=0.75) plt.title(\\"Generated binary classification dataset\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.show() generate_and_visualize_data() ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_classification def generate_and_visualize_data(): Generates a balanced synthetic dataset for a binary classification problem with the specified constraints and visualizes it using a scatter plot. The generated dataset has: - 2 classes, - 2 clusters per class, - 1000 samples, - 2 informative features, - no redundant features, - some noise to make the problem non-trivial. X, y = make_classification( n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=2, flip_y=0.05, # Introducing noise to make the problem non-trivial random_state=42, n_classes=2 ) plt.figure(figsize=(10, 6)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\'bwr\', alpha=0.75, edgecolor=\'k\') plt.title(\\"Generated binary classification dataset\\") plt.xlabel(\\"Feature 1\\") plt.ylabel(\\"Feature 2\\") plt.grid(True) plt.show() generate_and_visualize_data()"},{"question":"# Quoted-Printable Encoding and Decoding Utility The quoted-printable encoding is useful for encoding data where there are relatively few nonprintable characters. It\'s especially common in email applications for encoding text. Your task is to implement a Python utility function that utilizes the `quopri` module to encode and decode data from files, with optional handling for MIME headers. Function Specification 1. **Function Name**: `quopri_utility` 2. **Parameters**: - `input_path` (str): The path to the input file. - `output_path` (str): The path to the output file. - `mode` (str): The operation mode. Either \\"encode\\" or \\"decode\\". - `quotetabs` (bool): (Only for encoding) Whether to encode embedded spaces and tabs. - `header` (bool): Whether to interpret/encode spaces as underscores (default is False). 3. **Returns**: None 4. **Description**: - The function reads the input file and performs the specified operation (either encoding or decoding) using the `quopri` module. - If `mode` is \\"encode\\", it uses `quopri.encode` with the `quotetabs` parameter. - If `mode` is \\"decode\\", it uses `quopri.decode`. - Handles binary file objects for both input and output files. - Manages exceptions and prints an appropriate error message if an invalid operation mode is provided or if the file operations fail. 5. **Constraints**: - The input file should contain ASCII text. - The function should handle files of any reasonable size (up to a few MBs). Example Usage ```python quopri_utility(\\"input.txt\\", \\"output.txt\\", \\"encode\\", quotetabs=True, header=True) ``` This function call should read the file `input.txt`, encode its content in quoted-printable format (encoding tabs and spaces as needed), and save the result to `output.txt`. Notes: - Ensure the function is robust and handles erroneous inputs gracefully (such as invalid file paths or operation modes). - Provide appropriate comments for the code for better readability.","solution":"import quopri def quopri_utility(input_path, output_path, mode, quotetabs=False, header=False): Encodes or decodes a file using quoted-printable encoding. :param input_path: Path to the input file. :param output_path: Path to the output file. :param mode: Either \\"encode\\" or \\"decode\\". :param quotetabs: (Only for encoding) Whether to encode embedded spaces and tabs. :param header: Whether to handle spaces as underscores for MIME headers. :return: None try: with open(input_path, \'rb\') as infile, open(output_path, \'wb\') as outfile: if mode == \\"encode\\": quopri.encode(infile, outfile, quotetabs) elif mode == \\"decode\\": quopri.decode(infile, outfile) else: print(\\"Invalid mode: use \'encode\' or \'decode\'.\\") raise ValueError(\\"Invalid mode specified.\\") # Handle header translation if required if header and mode == \\"encode\\": with open(output_path, \'rb\') as f: data = f.read() encoded_header = data.replace(b\' \', b\'_\') with open(output_path, \'wb\') as f: f.write(encoded_header) elif header and mode == \\"decode\\": with open(output_path, \'rb\') as f: data = f.read() decoded_header = data.replace(b\'_\', b\' \') with open(output_path, \'wb\') as f: f.write(decoded_header) except Exception as e: print(f\\"An error occurred: {e}\\") raise"},{"question":"You are tasked with implementing a secure login system for a command-line application using Python\'s `getpass` module. This system should: 1. Prompt the user to enter their username and password securely. 2. Validate the credentials against a predefined set of users with their corresponding passwords. 3. Display a welcome message with the user\'s login name if the credentials are correct. 4. Allow up to three incorrect login attempts before terminating the program. # Input and Output Format: - **Input**: 1. User is prompted for a username. 2. User is prompted for a password, which should not be displayed on the screen. - **Output**: - If credentials are correct: \\"Welcome, `<username>`!\\". - If credentials are incorrect: \\"Invalid credentials. Try again.\\" (up to three attempts). - If three incorrect attempts are made: \\"Too many incorrect attempts. Exiting.\\" # Constraints and Assumptions: - Use the `getpass` module to handle password input securely. - Use the `getpass.getuser()` function to retrieve the system\'s login name and include it in the welcome message. - Credentials can be hardcoded in the program for simplicity. - The system supports case-sensitive usernames and passwords. # Predefined Users: ```python credentials = { \'user1\': \'password123\', \'user2\': \'mysecurepassword\', \'admin\': \'adminpass\' } ``` # Function Signature: ```python def login_system(): # Your implementation pass ``` # Example: ```plaintext Enter username: user1 Password: [not displayed] Welcome, user1! # Another example with incorrect credentials Enter username: user1 Password: [not displayed] Invalid credentials. Try again. # After three incorrect attempts Enter username: user1 Password: [not displayed] Invalid credentials. Try again. Enter username: user1 Password: [not displayed] Invalid credentials. Try again. Too many incorrect attempts. Exiting. ``` # Guidelines: - Make sure to use `getpass.getpass()` for secure password input. - Use `getpass.getuser()` in the welcome message to get the system\'s login name. - Handle the login attempts and termination logic as specified. Implement the `login_system` function to complete this task.","solution":"import getpass credentials = { \'user1\': \'password123\', \'user2\': \'mysecurepassword\', \'admin\': \'adminpass\' } def login_system(): max_attempts = 3 attempts = 0 while attempts < max_attempts: username = input(\\"Enter username: \\") password = getpass.getpass(\\"Password: \\") if username in credentials and credentials[username] == password: system_user = getpass.getuser() print(f\\"Welcome, {username}!\\") return else: attempts += 1 print(\\"Invalid credentials. Try again.\\") print(\\"Too many incorrect attempts. Exiting.\\")"},{"question":"# XML Parsing with SAX **Objective**: Implement a function to parse an XML document and extract specific information using the SAX parsing method. **Problem Statement**: You are provided with an XML document string containing information about books in a library. Your task is to write a function that uses the `xml.sax` package to: 1. Parse the XML document. 2. Extract the titles of all the books. 3. Collect the names of all unique authors. # Function Signature ```python def parse_books(xml_string: str) -> tuple: Parse the given XML string to extract book titles and unique author names. :param xml_string: A string representing the XML document. :return: A tuple containing: - A list of book titles (in the order they appear in the XML). - A set of unique author names. pass ``` # Input - `xml_string` (str): A string containing the XML document to be parsed. # Output - Returns a tuple: - A list of strings where each string is a book title. - A set of strings where each string is a unique author name. # Constraints - You must use the `xml.sax` package for parsing. - You must handle the start and end of elements (tags) correctly. - Assume that the `xml_string` is well-formed and valid. - The XML will have the following structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> </book> <book> <title>Book Title 2</title> <author>Author 2</author> </book> <book> <title>Book Title 3</title> <author>Author 1</author> </book> </library> ``` # Example Input ```python xml_string = <library> <book> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> </book> <book> <title>1984</title> <author>George Orwell</author> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> </book> <book> <title>Animal Farm</title> <author>George Orwell</author> </book> </library> ``` Output ```python ([\\"The Great Gatsby\\", \\"1984\\", \\"To Kill a Mockingbird\\", \\"Animal Farm\\"], {\\"F. Scott Fitzgerald\\", \\"George Orwell\\", \\"Harper Lee\\"}) ``` # Notes - Implement a custom `ContentHandler` to handle the parsing logic. - Use the appropriate SAX methods (`startElement`, `endElement`, `characters`, etc.) to process the XML elements.","solution":"import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.current_title = \\"\\" self.current_author = \\"\\" self.titles = [] self.authors = set() def startElement(self, tag, attributes): self.current_element = tag def endElement(self, tag): if self.current_element == \\"title\\": self.titles.append(self.current_title) self.current_title = \\"\\" elif self.current_element == \\"author\\": self.authors.add(self.current_author) self.current_author = \\"\\" self.current_element = \\"\\" def characters(self, content): if self.current_element == \\"title\\": self.current_title += content elif self.current_element == \\"author\\": self.current_author += content def parse_books(xml_string): handler = BookHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) return handler.titles, handler.authors"},{"question":"# Advanced HTML Parsing with Python Objective: You are tasked with creating an advanced HTML parser that extracts all URLs from a given HTML content and categorizes them based on their context (e.g., within a link, an image source, etc.). Task: Implement a class called `AdvancedHTMLParser` that subclasses `html.parser.HTMLParser`. Your class should override the necessary methods to achieve the following functionality: 1. Extract and categorize all URLs found in link (`<a>`), image (`<img>`), and script (`<script>`) tags. 2. Store the URLs in a dictionary with the tag names as keys. For example: ```python { \'a\': [\'url1\', \'url2\', ...], \'img\': [\'url1\', \'url2\', ...], \'script\': [\'url1\', \'url2\', ...] } ``` 3. Provide a method `get_urls_by_tag(tag)` that returns a list of URLs associated with the given tag. Constraints: 1. You should handle both quoted and unquoted attribute values. 2. Ensure your parser is able to handle nested tags correctly. 3. Ignore any tags that do not fall within the specified categories (i.e., \'a\', \'img\', \'script\'). Input: - An HTML content string. Output: - A dictionary containing the categorized URLs. Example: ```python from html.parser import HTMLParser class AdvancedHTMLParser(HTMLParser): def __init__(self): super().__init__() self.urls = {\'a\': [], \'img\': [], \'script\': []} def handle_starttag(self, tag, attrs): if tag in self.urls: for attr_name, attr_value in attrs: if (tag == \'a\' and attr_name == \'href\') or (tag == \'img\' and attr_name == \'src\') or (tag == \'script\' and attr_name == \'src\'): self.urls[tag].append(attr_value) def get_urls_by_tag(self, tag): return self.urls.get(tag, []) # Example usage parser = AdvancedHTMLParser() html_content = \'\'\' <html> <head><script src=\\"script.js\\"><\/script></head> <body> <a href=\\"https://example.com\\">Example</a> <img src=\\"image.jpg\\"> <a href=\\"https://another-example.com\\">Another Example</a> </body> </html> \'\'\' parser.feed(html_content) print(parser.get_urls_by_tag(\'a\')) # Output: [\'https://example.com\', \'https://another-example.com\'] print(parser.get_urls_by_tag(\'img\')) # Output: [\'image.jpg\'] print(parser.get_urls_by_tag(\'script\')) # Output: [\'script.js\'] ``` You should submit your implementation of the `AdvancedHTMLParser` class.","solution":"from html.parser import HTMLParser class AdvancedHTMLParser(HTMLParser): def __init__(self): super().__init__() self.urls = {\'a\': [], \'img\': [], \'script\': []} def handle_starttag(self, tag, attrs): if tag in self.urls: for attr_name, attr_value in attrs: if (tag == \'a\' and attr_name == \'href\') or (tag == \'img\' and attr_name == \'src\') or (tag == \'script\' and attr_name == \'src\'): self.urls[tag].append(attr_value) def get_urls_by_tag(self, tag): return self.urls.get(tag, []) # Example usage # parser = AdvancedHTMLParser() # html_content = \'\'\' # <html> # <head><script src=\\"script.js\\"><\/script></head> # <body> # <a href=\\"https://example.com\\">Example</a> # <img src=\\"image.jpg\\"> # <a href=\\"https://another-example.com\\">Another Example</a> # </body> # </html> # \'\'\' # parser.feed(html_content) # print(parser.get_urls_by_tag(\'a\')) # Output: [\'https://example.com\', \'https://another-example.com\'] # print(parser.get_urls_by_tag(\'img\')) # Output: [\'image.jpg\'] # print(parser.get_urls_by_tag(\'script\')) # Output: [\'script.js\']"},{"question":"**Coding Assessment Question: Advanced File Control Operations Using `fcntl`** **Objective:** Demonstrate your understanding of using the `fcntl` module to handle file descriptor operations, including file locking and modifying file control flags. **Problem Statement:** You are required to write a function, `setup_file_lock`, that performs the following operations on a given file: 1. Opens the file in read and write mode. 2. Sets the file to non-blocking mode. 3. Attempts to acquire an exclusive lock on the file. 4. Returns information about whether the lock was successfully acquired or not. Additionally, implement a function, `modify_pipe_size`, that: 1. Takes an opened pipe file descriptor and a new size as input. 2. Uses `fcntl` to set the size of the pipe. 3. Returns the new size of the pipe after modification. **Function Specifications:** 1. **Function: `setup_file_lock(file_path: str) -> bool`** - **Input:** - `file_path`: A string representing the path to the file to be locked. - **Output:** - Returns `True` if the exclusive lock was successfully acquired, `False` otherwise. - **Constraints:** - Handle possible `OSError` exceptions gracefully and ensure the file is closed before returning. 2. **Function: `modify_pipe_size(pipe_fd: int, new_size: int) -> int`** - **Input:** - `pipe_fd`: An integer file descriptor representing the opened pipe. - `new_size`: An integer specifying the new desired size of the pipe. - **Output:** - Returns the new size of the pipe after setting it. - **Constraints:** - The function should raise an exception if the operation fails, providing details about the failure. **Example Usage:** ```python # Example usage of setup_file_lock try: file_locked = setup_file_lock(\\"/path/to/file.txt\\") print(f\\"File lock successful: {file_locked}\\") except OSError as e: print(f\\"Error while locking file: {e}\\") # Example usage of modify_pipe_size import os r_fd, w_fd = os.pipe() # Create a pipe try: new_size = modify_pipe_size(w_fd, 2048) print(f\\"New pipe size: {new_size}\\") os.close(r_fd) os.close(w_fd) except OSError as e: print(f\\"Error while modifying pipe size: {e}\\") ``` **Notes:** - Make sure your solution is compatible with Unix-like operating systems. - Utilize the `fcntl` module functions detailed in the documentation. - Include necessary error handling to manage failed operations. - Test your solution with appropriate unit tests to ensure correctness.","solution":"import fcntl import os def setup_file_lock(file_path: str) -> bool: Attempts to open the file at the given path in read and write mode, sets the file to non-blocking mode, attempts to acquire an exclusive lock on the file, and returns whether the lock was successful. :param file_path: A string representing the path to the file to be locked. :return: True if the exclusive lock was successfully acquired, False otherwise. try: fd = os.open(file_path, os.O_RDWR) # Set the file to non-blocking mode flags = fcntl.fcntl(fd, fcntl.F_GETFL) fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK) try: # Attempt to acquire an exclusive lock fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB) return True except OSError: return False except OSError: return False finally: os.close(fd) def modify_pipe_size(pipe_fd: int, new_size: int) -> int: Modifies the size of the given pipe to the new size using fcntl and returns the new size. :param pipe_fd: An integer file descriptor representing the opened pipe. :param new_size: An integer specifying the new desired size of the pipe. :return: The new size of the pipe after setting it. :raises: Raises an OSError if the operation fails. try: fcntl.fcntl(pipe_fd, fcntl.F_SETPIPE_SZ, new_size) return fcntl.fcntl(pipe_fd, fcntl.F_GETPIPE_SZ) except OSError as e: raise OSError(f\\"Failed to set pipe size: {e}\\")"},{"question":"**Title: Implement a Compatibility Layer for Python 2 and Python 3** Problem Statement You are tasked with porting an existing Python 2 codebase to be compatible with both Python 2.7 and Python 3.x. Your job is to write a function `unified_division` that performs division in a way that is consistent between Python 2 and Python 3. Additionally, you will implement a function `convert_data_types` to handle the conversion between text and binary data, ensuring compatibility across both Python versions. Functions to Implement 1. **`unified_division(a: Any, b: Any) -> Any`** This function should perform division between two numbers `a` and `b` by adhering to Python 3\'s division rules while maintaining backward compatibility with Python 2. If `a` and `b` are both integers, the division should return a float. **Input:** - `a`: An integer or float. - `b`: An integer or float. **Output:** - Returns the result of dividing `a` by `b` as a float. **Example:** ```python assert unified_division(5, 2) == 2.5 assert unified_division(5, 2.0) == 2.5 ``` 2. **`convert_data_types(data: Any, to_type: str) -> Any`** This function should convert between text and binary data types. The parameter `data` can be either a string/text or bytes/binary, and `to_type` specifies the desired conversion type - `\\"text\\"` or `\\"binary\\"`. If converting to text, ensure the text is UTF-8 encoded. **Input:** - `data`: A string for text, or bytes for binary data. - `to_type`: A string indicating the conversion type, either `\\"text\\"` or `\\"binary\\"`. **Output:** - Returns the converted data in the specified type. **Example:** ```python assert convert_data_types(b\'hello\', \'text\') == \'hello\' assert convert_data_types(\'world\', \'binary\') == b\'world\' ``` Constraints 1. Do not use Python version-specific modules or functions that are not available in both Python 2.7 and Python 3. 2. Ensure compatibility for `unified_division` by using `from __future__ import division`. 3. Handle text and binary data appropriately, considering Python 2\'s handling of `str` and `unicode`. Hints - Remember to use `from __future__ import division` at the beginning of your script. - Utilize Python\'s built-in functions like `str.encode()` and `bytes.decode()` to manage conversions. - For file handling, prefer using `io.open()` for consistent behavior across Python versions. # Performance Requirements - The `unified_division` function should operate in constant time, O(1). - The `convert_data_types` function should operate in linear time, O(n), where `n` is the length of the data being converted. Example Code Structure ```python from __future__ import division import io def unified_division(a, b): # Implement the function pass def convert_data_types(data, to_type): # Implement the function pass # Unit tests assert unified_division(5, 2) == 2.5 assert unified_division(5, 2.0) == 2.5 assert convert_data_types(b\'hello\', \'text\') == \'hello\' assert convert_data_types(\'world\', \'binary\') == b\'world\' print(\\"All tests passed!\\") ```","solution":"from __future__ import division import io def unified_division(a, b): Perform division in a way that is consistent between Python 2 and Python 3. return float(a) / float(b) def convert_data_types(data, to_type): Convert between text and binary data types. Args: data: A string for text, or bytes for binary data. to_type: A string indicating the conversion type, either \\"text\\" or \\"binary\\". Returns: The converted data in the specified type. if to_type == \\"text\\": if isinstance(data, bytes): return data.decode(\'utf-8\') elif isinstance(data, str): return data else: raise ValueError(\\"Invalid data type for conversion to text.\\") if to_type == \\"binary\\": if isinstance(data, str): return data.encode(\'utf-8\') elif isinstance(data, bytes): return data else: raise ValueError(\\"Invalid data type for conversion to binary.\\") raise ValueError(\\"Invalid to_type value. Must be \'text\' or \'binary\'.\\") # Basic tests assert unified_division(5, 2) == 2.5 assert unified_division(5, 2.0) == 2.5 assert convert_data_types(b\'hello\', \'text\') == \'hello\' assert convert_data_types(\'world\', \'binary\') == b\'world\'"},{"question":"# Question: Advanced KDE Plot Customization with Seaborn You are given a dataset `iris`, which contains measurements of different attributes of iris flowers across three species. Using this dataset, you are required to perform the following tasks to demonstrate your proficiency with plotting and customizing KDE plots using seaborn: Datasets: - You can load the `iris` dataset using `sns.load_dataset(\'iris\')`. Tasks: 1. **Create a Univariate KDE Plot**: - Plot a KDE plot of the `sepal_length` attribute on the x-axis. - Use a higher smoothing parameter to make the distribution smoother. 2. **Bivariate KDE Plot**: - Create a bivariate KDE plot with `sepal_length` on the x-axis and `sepal_width` on the y-axis. - Use filled contours to color the plot. 3. **Customized KDE Plot with Hue Mapping**: - Plot a univariate KDE plot with `sepal_length` on the x-axis and map the `species` column to the `hue` parameter. - Use a different color palette for the hues. - Fill the density under the KDE curve and set the transparency to 0.6. 4. **Stacked Conditional Distributions**: - Plot KDE distributions of `sepal_length` for each species in a stacked manner. - Normalize the stacked distribution at each value in the grid. 5. **Comprehensive Combined Plot**: - Create a single figure with two subplots arranged in a 1x2 grid. - The first subplot should be the bivariate KDE plot (as described in task 2). - The second subplot should be the customized KDE plot with hue mapping (as described in task 3). - Ensure that the titles and labels are appropriately set for each subplot. Input and Output: - **Input**: None. You will load the dataset within your code. - **Output**: Visualization output as described in the tasks. Constraints and Performance Requirements: - Ensure that all plots are clearly labeled and aesthetically pleasing. - Use appropriate parameters to ensure smoothness (or lack thereof) as described. ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset iris = sns.load_dataset(\'iris\') # Task 1: Univariate KDE plot plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) sns.kdeplot(data=iris, x=\\"sepal_length\\", bw_adjust=2).set(title=\'Univariate KDE - Sepal Length\') # Task 2: Bivariate KDE plot plt.subplot(1, 2, 2) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", fill=True, cmap=\\"viridis\\").set(title=\'Bivariate KDE - Sepal Length vs Sepal Width\') plt.tight_layout() plt.show() # Task 3: Customized KDE plot with hue mapping plt.figure(figsize=(10, 5)) sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", fill=True, palette=\\"coolwarm\\", alpha=0.6).set(title=\'Customized KDE with Hue - Sepal Length\') # Task 4: Stacked Conditional Distributions plt.figure(figsize=(10, 5)) sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", multiple=\\"stack\\").set(title=\'Stacked KDE - Sepal Length\') # Task 5: Comprehensive Combined Plot fig, ax = plt.subplots(1, 2, figsize=(15, 5)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", fill=True, cmap=\\"viridis\\", ax=ax[0]) ax[0].set(title=\'Bivariate KDE - Sepal Length vs Sepal Width\') sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", fill=True, palette=\\"coolwarm\\", alpha=0.6, ax=ax[1]) ax[1].set(title=\'Customized KDE with Hue - Sepal Length\') plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset iris = sns.load_dataset(\'iris\') # Task 1: Univariate KDE plot def plot_univariate_kde(): plt.figure(figsize=(15, 5)) sns.kdeplot(data=iris, x=\\"sepal_length\\", bw_adjust=2).set(title=\'Univariate KDE - Sepal Length\') plt.show() # Task 2: Bivariate KDE plot def plot_bivariate_kde(): plt.figure(figsize=(15, 5)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", fill=True, cmap=\\"viridis\\").set(title=\'Bivariate KDE - Sepal Length vs Sepal Width\') plt.show() # Task 3: Customized KDE plot with hue mapping def plot_customized_kde_with_hue(): plt.figure(figsize=(10, 5)) sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", fill=True, palette=\\"coolwarm\\", alpha=0.6).set(title=\'Customized KDE with Hue - Sepal Length\') plt.show() # Task 4: Stacked Conditional Distributions def plot_stacked_kde_conditional(): plt.figure(figsize=(10, 5)) sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", multiple=\\"stack\\").set(title=\'Stacked KDE - Sepal Length\') plt.show() # Task 5: Comprehensive Combined Plot def plot_comprehensive_combined(): fig, ax = plt.subplots(1, 2, figsize=(15, 5)) sns.kdeplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", fill=True, cmap=\\"viridis\\", ax=ax[0]) ax[0].set(title=\'Bivariate KDE - Sepal Length vs Sepal Width\') sns.kdeplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", fill=True, palette=\\"coolwarm\\", alpha=0.6, ax=ax[1]) ax[1].set(title=\'Customized KDE with Hue - Sepal Length\') plt.tight_layout() plt.show()"},{"question":"Question: Synchronization of Async Tasks Using asyncio Primitives # Background: You are required to design a coordination system for asyncio tasks that simulates a simple production line. In this setup, multiple producer tasks generate items and put them into a buffer shared with consumer tasks, which subsequently consume the items. The buffer should have a limited capacity to ensure synchronization between producers and consumers. # Task: Implement a class `ProductionLine` using asyncio synchronization primitives to manage the production and consumption of items. You will create the following methods: 1. `__init__(self, buffer_size: int)`: Initialize the `ProductionLine` with a given buffer size, and create all necessary asyncio synchronization primitives. 2. `async def produce(self, item: str)`: This coroutine should add an item to the buffer. If the buffer is full, the producer should wait until there is space available. 3. `async def consume(self) -> str`: This coroutine should remove and return an item from the buffer. If the buffer is empty, the consumer should wait until an item is available. # Constraints: - The `buffer` should use a maximum of `buffer_size` capacity. - Multiple producer and consumer coroutines should be able to interact with the `buffer` concurrently. - Proper synchronization must be ensured to avoid race conditions or data corruption. # Input and Output Formats: - The class `ProductionLine` should be initialized with an integer `buffer_size`. - The method `produce(self, item: str)` should be called with a string `item`. - The method `consume(self)` should wait for an item to be available, then return it. # Example: Here is an example showing how instances of `ProductionLine` can be used: ```python import asyncio async def producer(pl, item): await pl.produce(item) print(f\'Produced: {item}\') async def consumer(pl): item = await pl.consume() print(f\'Consumed: {item}\') async def main(): pl = ProductionLine(buffer_size=2) # Create producer and consumer tasks producers = [asyncio.create_task(producer(pl, f\'item{i}\')) for i in range(5)] consumers = [asyncio.create_task(consumer(pl)) for _ in range(5)] # Wait for all tasks to complete await asyncio.gather(*producers) await asyncio.gather(*consumers) asyncio.run(main()) ``` # Implementation Details: Make sure to use `asyncio.Lock`, `asyncio.Event`, `asyncio.Condition`, or `asyncio.Semaphore` appropriately to manage access to the buffer in a thread-safe manner.","solution":"import asyncio class ProductionLine: def __init__(self, buffer_size: int): self.buffer_size = buffer_size self.buffer = [] self.lock = asyncio.Lock() self.not_full = asyncio.Condition(self.lock) self.not_empty = asyncio.Condition(self.lock) async def produce(self, item: str): async with self.not_full: while len(self.buffer) >= self.buffer_size: await self.not_full.wait() self.buffer.append(item) self.not_empty.notify() async def consume(self) -> str: async with self.not_empty: while not self.buffer: await self.not_empty.wait() item = self.buffer.pop(0) self.not_full.notify() return item"},{"question":"# Question **Objective:** Create a complex visualization using Seaborn that demonstrates your understanding of loading datasets, creating bar plots with various customizations, and adding groupings and annotations. **Description:** You have been provided with the \\"penguins\\" and \\"flights\\" datasets from Seaborn. Your task is to create a Jupyter notebook that demonstrates the following: 1. **Load both the \\"penguins\\" and \\"flights\\" datasets** into separate DataFrames. 2. **Create a barplot of the \\"penguins\\" dataset**: - Plot the average `body_mass_g` for each `island`. - Color the bars based on the `species`. - Show the standard deviation as the error bars. - Customize the appearance to have a transparent facecolor with red borders. - Label each bar with the average `body_mass_g`. 3. **Create a faceted barplot of the \\"flights\\" dataset**: - Using `catplot`, plot the total `passengers` for each `year`, separated by `month`. - Customize each facet to have a consistent height and aspect ratio. - Add a title to each facet showing the name of the month. 4. **Add annotations to the \\"flights\\" barplot**: - Highlight the year with the maximum total passengers for each month with a distinct marker. - Annotate this maximum value on the plot. **Constraints:** - Use Seaborn for all plotting. - Matplotlib can be used for advanced customizations if necessary. - Avoid using for-loops for aggregation and plotting (use Seaborn\'s in-built functionalities). **Expected Output:** - A Jupyter notebook containing the described plots. - Properly commented code explaining each step. - Clear and formatted visualizations with legends, titles, and annotations as specified. **Performance Requirement:** - The notebook should run efficiently without excessive computation time. **Provided Skeleton Code:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # 1. Barplot for penguins dataset # YOUR CODE HERE # 2. Faceted barplot for flights dataset # YOUR CODE HERE # 3. Adding annotations # YOUR CODE HERE plt.show() ``` Use the skeleton code above to start your implementation. Ensure your notebook is complete and self-explanatory.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # 1. Barplot for penguins dataset # Calculate mean and standard deviation for `body_mass_g` grouped by `island` and `species` penguins_grouped = penguins.groupby([\'island\', \'species\'])[\'body_mass_g\'].agg([\'mean\', \'std\']).reset_index() plt.figure(figsize=(10, 6)) barplot = sns.barplot(data=penguins_grouped, x=\'island\', y=\'mean\', hue=\'species\', palette=\'viridis\', ci=\'sd\') # Customize bar appearance for bar in barplot.patches: bar.set_alpha(0.5) bar.set_edgecolor(\'red\') # Annotate each bar with the average body_mass_g for index, row in penguins_grouped.iterrows(): barplot.text(row.name, row[\'mean\'], round(row[\'mean\'], 1), color=\'black\', ha=\\"center\\") barplot.set_ylabel(\'Average Body Mass (g)\') barplot.set_title(\'Average Body Mass of Penguins by Island and Species\') plt.legend(title=\'Species\') plt.show() # 2. Faceted barplot for flights dataset # Create a catplot for total passengers per year, separated by month catplot = sns.catplot(data=flights, x=\'year\', y=\'passengers\', col=\'month\', kind=\'bar\', col_wrap=4, height=3, aspect=1.2) catplot.set_titles(\\"{col_name}\\") # 3. Adding annotations # Highlight year with maximum total passengers for each month for ax in catplot.axes.flat: month = ax.get_title() monthly_data = flights[flights[\'month\'] == month] max_year = monthly_data.loc[monthly_data[\'passengers\'].idxmax()][\'year\'] max_passengers = monthly_data[\'passengers\'].max() ax.annotate(f\'Max: {max_year}\', xy=(max_year, max_passengers), xytext=(max_year, max_passengers + 200), arrowprops=dict(facecolor=\'black\', shrink=0.05)) plt.show()"},{"question":"Objective In this assessment, you are required to demonstrate your understanding of seaborn by handling and visualizing data using strip plots and cat plots. You will work with the `tips` dataset available in seaborn and create various visualizations based on specific instructions. Tasks 1. **Load the Dataset**: - Load the `tips` dataset from seaborn. 2. **Basic Strip Plot**: - Create a basic strip plot that shows the distribution of the `total_bill` column. 3. **Bivariate Strip Plot**: - Create a strip plot that visualizes the distribution of `total_bill`, separated by `day`. 4. **Customized Strip Plot**: - Create a strip plot where: - `total_bill` is on the x-axis. - Points are color-coded based on the `time` column (Lunch/Dinner). - The plot should be vertical with day labels on the y-axis. - Use `palette=\\"Set2\\"` for coloring. 5. **Strip Plot Without Jitter**: - Create the same plot as in task 4, but disable jittering. 6. **Numeric Hue Variable**: - Create a strip plot where: - `total_bill` is on the y-axis and `size` is on the x-axis. - Color the points based on the `size` column. - Make sure that the `size` variable uses its native scale. 7. **Faceted Plot Using Catplot**: - Create a faceted plot using `catplot` that shows: - `total_bill` on the y-axis. - Facets for each day (`col=\\"day\\"`). - Points should be color-coded based on the `sex` column. - Each facet should be arranged vertically (use `aspect=0.5`). Implementation Details - **Input**: - No input is required from the user, as the dataset is loaded from seaborn. - **Output**: - Display the plots accordingly. - **Constraints**: - Ensure you handle different data types correctly when using the `hue`, `orient`, and `palette` parameters. - Use appropriate plotting functions as instructed. - **Performance Requirements**: - Plots should be generated efficiently without unnecessary computations. - Use seaborn functionalities to cater to different visualization aspects. Example Code Structure Below is a suggested structure for your implementation: ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load Dataset tips = sns.load_dataset(\\"tips\\") # Task 2: Basic Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill\\") plt.show() # Task 3: Bivariate Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill by Day\\") plt.show() # Task 4: Customized Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", palette=\\"Set2\\") plt.title(\\"Total Bill by Day and Time\\") plt.show() # Task 5: Strip Plot Without Jitter plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", palette=\\"Set2\\", jitter=False) plt.title(\\"Total Bill by Day and Time (No Jitter)\\") plt.show() # Task 6: Numeric Hue Variable with Native Scale plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"size\\", y=\\"total_bill\\", hue=\\"size\\", native_scale=True) plt.title(\\"Total Bill by Size (Native Scale)\\") plt.show() # Task 7: Faceted Plot Using Catplot g = sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\\"Faceted Plot of Total Bill by Time and Sex\\") plt.show() ``` Ensure each plot is clearly labeled with titles and axes for better readability. Test your code to verify it meets all requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_plots(): # Task 1: Load Dataset tips = sns.load_dataset(\\"tips\\") # Task 2: Basic Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.show() # Task 3: Bivariate Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill by Day\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.show() # Task 4: Customized Strip Plot plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", palette=\\"Set2\\") plt.title(\\"Total Bill by Day and Time\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Time\\") plt.show() # Task 5: Strip Plot Without Jitter plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", palette=\\"Set2\\", jitter=False) plt.title(\\"Total Bill by Day and Time (No Jitter)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.legend(title=\\"Time\\") plt.show() # Task 6: Numeric Hue Variable with Native Scale plt.figure(figsize=(10, 6)) sns.stripplot(data=tips, x=\\"size\\", y=\\"total_bill\\", hue=\\"size\\", native_scale=True) plt.title(\\"Total Bill by Size (Native Scale)\\") plt.xlabel(\\"Size\\") plt.ylabel(\\"Total Bill\\") plt.legend(title=\\"Size\\") plt.show() # Task 7: Faceted Plot Using Catplot g = sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5) g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\\"Faceted Plot of Total Bill by Time and Sex\\") plt.show()"},{"question":"# Custom Descriptor and Data Validation **Objective:** Implement a custom descriptor class that validates values and logs attribute access. Students need to demonstrate their understanding by using this descriptor in a class to manage its attributes dynamically and securely. **Problem Statement:** You are required to design a custom descriptor called `ValidatedProperty` that will be used to manage and validate the attributes of a class. The descriptor should: 1. Validate the values being set based on the specified type and constraints. 2. Log every access to the attribute including reads and writes. 3. Work with any attribute name dynamically by using the `__set_name__` method. # Specifications: 1. **ValidatedProperty Descriptor:** - `__init__(self, property_type, min_value=None, max_value=None)`: - `property_type`: The type of the property (e.g., `int`, `float`, `str`). - `min_value`: The minimum value (inclusive) the attribute can have. (Applied only for numeric types). - `max_value`: The maximum value (inclusive) the attribute can have. (Applied only for numeric types). - `__set_name__(self, owner, name)`: Determines the name of the attribute and sets it. - `__get__(self, instance, owner)`: Fetches the value and logs the access. - `__set__(self, instance, value)`: Validates and sets the value, logging the update. 2. **Person Class**: - Should contain attributes `age` and `name` using the `ValidatedProperty` descriptor. - `age` should be an integer between 0 and 120. - `name` should be a string with a length between 3 and 50 characters. # Example Usage: ```python class ValidatedProperty: def __init__(self, property_type, min_value=None, max_value=None): self.property_type = property_type self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.public_name = name self.private_name = \'_\' + name def __get__(self, instance, owner): if instance is None: return self value = getattr(instance, self.private_name, None) print(f\\"Logging: Accessed {self.public_name} value as {value}\\") return value def __set__(self, instance, value): if not isinstance(value, self.property_type): raise TypeError(f\\"{self.public_name} must be of type {self.property_type}\\") if self.min_value is not None and value < self.min_value: raise ValueError(f\\"{self.public_name} must be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"{self.public_name} must be at most {self.max_value}\\") setattr(instance, self.private_name, value) print(f\\"Logging: Set {self.public_name} to {value}\\") class Person: age = ValidatedProperty(int, 0, 120) name = ValidatedProperty(str, 3, 50) def __init__(self, name, age): self.name = name self.age = age # Example usage p = Person(\'Alice\', 30) print(p.name) # Logs access p.age = 35 # Logs update and access ``` # Constraints: - Ensure proper logging of attribute access and updates. - Raise appropriate exceptions for invalid types and out-of-bounds values. - Utilize automatic name notification. **Evaluation Criteria:** - Correct implementation of the `ValidatedProperty` class. - Proper and effective use of the `Person` class with the custom descriptor. - Accurate logging for both reads and writes. - Robust validation logic according to the constraints provided.","solution":"class ValidatedProperty: def __init__(self, property_type, min_value=None, max_value=None): self.property_type = property_type self.min_value = min_value self.max_value = max_value def __set_name__(self, owner, name): self.public_name = name self.private_name = f\'_{name}\' def __get__(self, instance, owner): if instance is None: return self value = getattr(instance, self.private_name, None) print(f\\"Logging: Accessed {self.public_name} value as {value}\\") return value def __set__(self, instance, value): if not isinstance(value, self.property_type): raise TypeError(f\\"{self.public_name} must be of type {self.property_type}\\") if isinstance(value, (int, float)): # Ensure min_value and max_value are only used for numeric types. if self.min_value is not None and value < self.min_value: raise ValueError(f\\"{self.public_name} must be at least {self.min_value}\\") if self.max_value is not None and value > self.max_value: raise ValueError(f\\"{self.public_name} must be at most {self.max_value}\\") elif self.property_type == str: # For strings, use min_value and max_value as length constraints. length = len(value) if self.min_value is not None and length < self.min_value: raise ValueError(f\\"{self.public_name} length must be at least {self.min_value}\\") if self.max_value is not None and length > self.max_value: raise ValueError(f\\"{self.public_name} length must be at most {self.max_value}\\") setattr(instance, self.private_name, value) print(f\\"Logging: Set {self.public_name} to {value}\\") class Person: age = ValidatedProperty(int, 0, 120) name = ValidatedProperty(str, 3, 50) def __init__(self, name, age): self.name = name self.age = age"},{"question":"Objective Your task is to demonstrate an understanding of the `contextvars` module by implementing a set of functions that manage context-local variables in different scenarios, including synchronous and asynchronous contexts. Problem Statement 1. **Context Variable Management**: - Implement a function `manage_context_var` that performs the following: - Takes a context variable `var`, sets its value to the given input `value`, performs some operations, then resets the variable to its previous value. - Returns the value of the variable after the reset operation, which should be its original value before the set operation. - If any errors occur during the operation, ensure the context variable is still reset to its original value. - **Function Signature**: ```python def manage_context_var(var: contextvars.ContextVar, value: int) -> int: pass ``` 2. **Isolated Context Execution**: - Implement a function `execute_in_isolated_context` that: - Takes a callable `func` and its arguments `*args` and `**kwargs`, runs `func(*args, **kwargs)` within a new isolated context, and returns the result. - Any context variable modifications by `func` should not affect the current context. - **Function Signature**: ```python def execute_in_isolated_context(func: Callable[..., T], *args: Any, **kwargs: Any) -> T: pass ``` 3. **Asynchronous Context Management**: - Implement an asynchronous function `async_manage_context_var` that: - Takes a context variable `var`, sets its value to the given input `value` asynchronously, performs some awaitable operations, then resets the variable to its previous value. - Returns the value of the variable after the reset operation, which should be its original value before the set operation. - If any errors occur during the operation, ensure the context variable is still reset to its original value. - **Function Signature**: ```python async def async_manage_context_var(var: contextvars.ContextVar, value: int) -> int: pass ``` Constraints - The context variable should be appropriately reset to its original value even if operations fail or raise exceptions. - Assume that `var` passed to these functions will always be a valid `ContextVar` instance. - Ensure that the implementations handle concurrent modifications appropriately. Input and Output Format - `manage_context_var`: - Input: A `ContextVar` instance and an integer value. - Output: An integer representing the original value of the context variable. - `execute_in_isolated_context`: - Input: A callable and its arguments. - Output: The result of executing the given callable with the provided arguments within a new isolated context. - `async_manage_context_var`: - Input: A `ContextVar` instance and an integer value. - Output: An integer representing the original value of the context variable returned asynchronously. Sample Usage ```python import contextvars var = contextvars.ContextVar(\'var\', default=42) def test_func(value): return value * 2 # Synchronous context management original_value = manage_context_var(var, 100) print(original_value) # Output should be 42 # Isolated context execution result = execute_in_isolated_context(test_func, 21) print(result) # Output should be 42 # Asynchronous context management import asyncio async def main(): original_value = await async_manage_context_var(var, 100) print(original_value) # Output should be 42 asyncio.run(main()) ```","solution":"import contextvars from typing import Callable, Any, TypeVar import contextlib T = TypeVar(\'T\') def manage_context_var(var: contextvars.ContextVar, value: int) -> int: token = var.set(value) try: # Perform any operations here pass finally: var.reset(token) return var.get() def execute_in_isolated_context(func: Callable[..., T], *args: Any, **kwargs: Any) -> T: new_ctx = contextvars.copy_context() return new_ctx.run(func, *args, **kwargs) async def async_manage_context_var(var: contextvars.ContextVar, value: int) -> int: token = var.set(value) try: # Await some operations here pass finally: var.reset(token) return var.get()"},{"question":"**Coding Assessment Question: Understanding scikit-learn Utilities for Data Processing** # Problem Statement Given a dataset, your task is to preprocess the data, perform `k-truncated randomized SVD`, and handle the dataset when it\'s transformed into a sparse matrix. You will use various utilities from the `sklearn.utils` module to achieve this. # Step-by-step Instructions: 1. **Preprocess Input Data** - Use `sklearn.utils.check_array` to verify that the input dataset `X` is a 2D array and that all values are finite. - Ensure that the corresponding target values `y` have consistent lengths with `X` using `sklearn.utils.check_X_y`. 2. **Random Sampling** - Use `sklearn.utils.random.sample_without_replacement` to sample a subset of the data. Set the sample size to be `n_samples` out of the entire dataset. 3. **Apply k-truncated randomized SVD** - Use `sklearn.utils.extmath.randomized_svd` to compute SVD of the input data. Return the `U`, `Sigma`, and `Vt` matrices. 4. **Sparse Matrix Handling** - Convert the matrix `U` (obtained from SVD) to a CSR sparse matrix. - Use `sklearn.utils.sparsefuncs.mean_variance_axis` to compute the mean and variance along each row of the sparse matrix. # Function Signature ```python from typing import Tuple import numpy as np from scipy.sparse import csr_matrix def preprocess_and_analyze( X: np.ndarray, y: np.ndarray, n_samples: int, k_components: int ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, csr_matrix, np.ndarray, np.ndarray]: - X: 2D numpy array of shape (n_samples, n_features) - y: 1D numpy array of shape (n_samples,) - n_samples: Integer, the number of samples to randomly select from the dataset - k_components: Integer, the number of singular values and vectors to extract Returns: - U: 2D numpy array of shape (n_samples, k_components) representing left singular vectors - Sigma: 1D numpy array of length k_components representing singular values - Vt: 2D numpy array of shape (k_components, n_features) representing right singular vectors - sparse_U: CSR sparse matrix of shape (n_samples, k_components) - row_means: 1D numpy array of shape (n_samples,) representing the mean of each row of sparse_U - row_variances: 1D numpy array of shape (n_samples,) representing the variance of each row of sparse_U # Your code here return U, Sigma, Vt, sparse_U, row_means, row_variances ``` # Constraints - You may assume that `X` is always provided as a 2D numpy array and `y` as a 1D numpy array. - The input dataset `X` will have at least `n_samples` rows. - The number of components `k_components` will always be less than or equal to the number of features in `X`. # Example ```python import numpy as np from sklearn.utils.extmath import randomized_svd from sklearn.utils import check_array, check_X_y from sklearn.utils.sparsefuncs import mean_variance_axis from sklearn.utils.random import sample_without_replacement from scipy.sparse import csr_matrix # Sample Data X = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]) y = np.array([1, 2, 3, 4]) n_samples = 3 k_components = 2 # Expected Output U, Sigma, Vt, sparse_U, row_means, row_variances = preprocess_and_analyze(X, y, n_samples, k_components) print(U, Sigma, Vt, sparse_U, row_means, row_variances) ``` Complete the function `preprocess_and_analyze` to pass the example and ensure all steps are accurately implemented using the appropriate utilities from `sklearn.utils`.","solution":"import numpy as np from sklearn.utils.extmath import randomized_svd from sklearn.utils import check_array, check_X_y from sklearn.utils.sparsefuncs import mean_variance_axis from sklearn.utils.random import sample_without_replacement from scipy.sparse import csr_matrix from typing import Tuple def preprocess_and_analyze( X: np.ndarray, y: np.ndarray, n_samples: int, k_components: int ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, csr_matrix, np.ndarray, np.ndarray]: - X: 2D numpy array of shape (n_samples, n_features) - y: 1D numpy array of shape (n_samples,) - n_samples: Integer, the number of samples to randomly select from the dataset - k_components: Integer, the number of singular values and vectors to extract Returns: - U: 2D numpy array of shape (n_samples, k_components) representing left singular vectors - Sigma: 1D numpy array of length k_components representing singular values - Vt: 2D numpy array of shape (k_components, n_features) representing right singular vectors - sparse_U: CSR sparse matrix of shape (n_samples, k_components) - row_means: 1D numpy array of shape (n_samples,) representing the mean of each row of sparse_U - row_variances: 1D numpy array of shape (n_samples,) representing the variance of each row of sparse_U # Ensure the input data is correctly shaped and that all values are valid X_checked, y_checked = check_X_y(X, y, ensure_2d=True, allow_nd=False, force_all_finite=True) # Randomly sample a subset of the data indices = sample_without_replacement(n_population=X_checked.shape[0], n_samples=n_samples) X_sampled = X_checked[indices] y_sampled = y_checked[indices] # Perform k-truncated randomized SVD U, Sigma, Vt = randomized_svd(X_sampled, n_components=k_components) # Convert U to a CSR sparse matrix sparse_U = csr_matrix(U) # Compute the mean and variance of each row of the sparse matrix row_means, row_variances = mean_variance_axis(sparse_U, axis=1) return U, Sigma, Vt, sparse_U, row_means, row_variances"},{"question":"# Advanced Coding Assessment: Pseudo-terminal Interaction **Objective:** You are required to demonstrate your understanding of the `pty` module in Python by writing a function that spawns a new shell process and logs its output into two separate files, while implementing a custom read function to modify the output before logging. **Task:** Write a function `log_shell_session(cmd: str, logfile1: str, logfile2: str) -> int` that: 1. Uses the `pty.spawn` function to start a new shell process and execute the given command `cmd`. 2. Logs the output of this shell session into `logfile1` and `logfile2`, but with different formats applied to the logs. - For `logfile1`: Prefix each line of the output with `[LOG1] `. - For `logfile2`: Prefix each line with the current timestamp in the format `[%Y-%m-%d %H:%M:%S] `. 3. Returns the exit status value of the spawned process. **Input:** - `cmd`: A string representing the command to execute in the shell (e.g., `\\"ls -l\\"`). - `logfile1`: A string representing the filename where the output with `[LOG1] ` prefix will be logged. - `logfile2`: A string representing the filename where the output with timestamps will be logged. **Output:** - The function returns an integer which is the exit status value of the spawned shell process. **Constraints:** - Ensure that the function handles the signals for End-Of-File (EOF) properly. - Consider potential large outputs; thus, avoid reading the entire output at once. - Handle any potential exceptions gracefully (e.g., file operations and process-related errors). **Example:** ```python import pty import os import time def log_shell_session(cmd: str, logfile1: str, logfile2: str) -> int: def custom_read(fd): data = os.read(fd, 1024) if not data: return data with open(logfile1, \'ab\') as log1, open(logfile2, \'ab\') as log2: lines = data.splitlines(keepends=True) for line in lines: log1.write(b\\"[LOG1] \\" + line) log2.write((\\"[{}] \\".format(time.strftime(\'%Y-%m-%d %H:%M:%S\'))).encode() + line) return data return pty.spawn(shlex.split(cmd), custom_read) # Example usage exit_status = log_shell_session(\\"ls -l\\", \\"log1.txt\\", \\"log2.txt\\") print(f\\"Shell session ended with exit status: {exit_status}\\") ``` # Notes: - You can utilize the `time` module for timestamping. - Ensure the implementation is robust for handling large and continuous streams of data. - Assume that the function will always be executed on a Unix-like system with a working implementation of the `pty` module. Good luck!","solution":"import pty import os import time import shlex def log_shell_session(cmd: str, logfile1: str, logfile2: str) -> int: def custom_read(fd): data = os.read(fd, 1024) if not data: return data with open(logfile1, \'ab\') as log1, open(logfile2, \'ab\') as log2: lines = data.splitlines(keepends=True) for line in lines: log1.write(b\\"[LOG1] \\" + line) log2.write((\\"[{}] \\".format(time.strftime(\'%Y-%m-%d %H:%M:%S\'))).encode() + line) return data return pty.spawn(shlex.split(cmd), custom_read)"},{"question":"**Question:** Password Security Application using the `crypt` module You are tasked with creating a secure user authentication system that securely handles passwords. Your implementation should include capabilities to register a new user with their password and validate a user\'s login by checking the provided password. # Functional Requirements 1. **User Registration**: Implement a function `register_user(username: str, password: str, method: Optional[str] = None, rounds: Optional[int] = None) -> dict` which: - Takes the username and password as input. - Optionally allows specifying a hashing method (`METHOD_SHA512`, `METHOD_SHA256`, `METHOD_BLOWFISH`, etc.). - Optionally allows specifying the number of rounds for applicable methods. - Stores and returns user information as a dictionary containing the username and their hashed password. 2. **User Login**: Implement a function `validate_login(username: str, password: str, user_db: dict) -> boolean` which: - Takes the username and password. - Checks the password against the stored hashed password in the provided user database. - Returns `True` if the password is correct, `False` otherwise. # Input and Output formats - **register_user(input)** - username: str (e.g., \'user1\') - password: str (e.g., \'securePassword123\') - method (optional): str (e.g., \'METHOD_SHA512\') - default is the strongest available method. - rounds (optional): int - the number of rounds to use for hashing, valid only for applicable methods. - **register_user(output)** - user_info: dict containing at least \'username\' and \'hashed_password\' keys. - **validate_login(input)** - username: str (e.g., \'user1\') - password: str (e.g., \'securePassword123\') - user_db: dict containing user entries, where keys are usernames and values are dictionaries containing at least \'hashed_password\'. - **validate_login(output)** - boolean: `True` if password is correct, `False` otherwise. # Constraints - Use constant-time comparison (e.g. `hmac.compare_digest(...)`) for password validation to avoid timing attacks. - Ensure proper handling of optional parameters for hashing methods and rounds. - Assume the strongest method available if the method is not specified during registration. - Salt values should be generated using the `mksalt()` function. # Example ```python import crypt from hmac import compare_digest as compare_hash def register_user(username, password, method=None, rounds=None): if method: salt = crypt.mksalt(getattr(crypt, method), rounds=rounds) else: salt = crypt.mksalt() hashed_password = crypt.crypt(password, salt) return {\'username\': username, \'hashed_password\': hashed_password} def validate_login(username, password, user_db): if username not in user_db: return False stored_hashed_password = user_db[username][\'hashed_password\'] hashed_password = crypt.crypt(password, stored_hashed_password) return compare_hash(hashed_password, stored_hashed_password) # Example usage: user_db = {} # Register user user_info = register_user(\'user1\', \'mypassword123\', method=\'METHOD_SHA512\', rounds=10000) user_db[user_info[\'username\']] = user_info # Validate login is_valid = validate_login(\'user1\', \'mypassword123\', user_db) print(is_valid) # Output: True is_valid = validate_login(\'user1\', \'wrongpassword\', user_db) print(is_valid) # Output: False ```","solution":"import crypt from hmac import compare_digest as compare_hash def register_user(username, password, method=None, rounds=None): if method: salt = crypt.mksalt(getattr(crypt, method), rounds=rounds) else: salt = crypt.mksalt() hashed_password = crypt.crypt(password, salt) return {\'username\': username, \'hashed_password\': hashed_password} def validate_login(username, password, user_db): if username not in user_db: return False stored_hashed_password = user_db[username][\'hashed_password\'] hashed_password = crypt.crypt(password, stored_hashed_password) return compare_hash(hashed_password, stored_hashed_password)"},{"question":"**Question: Implement a Mapping Class** You are required to implement a Python class named `CustomMapping` that mimics the behaviors of a mapping/dictionary, utilizing key functions similar to those documented in the `PyMapping` interface. **Class Requirements:** 1. Initialize the class with an internal dictionary. 2. Implement the following methods: - `check_mapping(self)`: Returns `True` if the internal object is a mapping. - `size(self)`: Returns the number of keys in the mapping. - `get_item(self, key)`: Returns the value associated with the provided `key`. - `set_item(self, key, value)`: Adds or updates the key-value pair in the mapping. - `del_item(self, key)`: Deletes the key-value pair from the mapping. - `has_key(self, key)`: Checks if the key is present in the mapping. - `keys(self)`: Returns a list of all keys in the mapping. - `values(self)`: Returns a list of all values in the mapping. - `items(self)`: Returns a list of all key-value pairs in the mapping. **Constraints:** 1. Do not use Python\'s built-in dictionary methods (e.g., `dict.get`, `dict.keys`). 2. The class must handle exceptions gracefully, providing meaningful error messages. **Input and Output Formats:** The class will be tested with a series of method calls: ```python mapping = CustomMapping() mapping.set_item(\'a\', 1) mapping.set_item(\'b\', 2) assert mapping.size() == 2 assert mapping.get_item(\'a\') == 1 assert mapping.has_key(\'a\') is True mapping.del_item(\'a\') assert mapping.has_key(\'a\') is False assert mapping.keys() == [\'b\'] assert mapping.values() == [2] assert mapping.items() == [(\'b\', 2)] ``` **Performance Requirements:** - The implementation should be efficient with operations ideally being O(1) for set and get operations. Implement the `CustomMapping` class in Python.","solution":"class CustomMapping: def __init__(self): self._internal_dict = {} def check_mapping(self): return isinstance(self._internal_dict, dict) def size(self): return len(self._internal_dict) def get_item(self, key): if key in self._internal_dict: return self._internal_dict[key] else: raise KeyError(f\\"Key \'{key}\' not found in mapping\\") def set_item(self, key, value): self._internal_dict[key] = value def del_item(self, key): if key in self._internal_dict: del self._internal_dict[key] else: raise KeyError(f\\"Key \'{key}\' not found in mapping\\") def has_key(self, key): return key in self._internal_dict def keys(self): return list(self._internal_dict.keys()) def values(self): return list(self._internal_dict.values()) def items(self): return list(self._internal_dict.items())"},{"question":"Task You are required to implement a Python class that utilizes the `urllib.robotparser.RobotFileParser` to analyze a given `robots.txt` file and check various permissions and configurations for different user agents. Objectives - Implement the `RobotsAnalyzer` class. - The class should read and parse the `robots.txt` file from a specified URL. - It should provide methods to: - Check if a user agent can fetch a URL. - Get the crawl delay for a user agent. - Get the request rate for a user agent. - Get the sitemaps listed in the `robots.txt`. Requirements 1. **Class Name**: RobotsAnalyzer 2. **Methods**: - `__init__(self, url: str)`: Initializes the analyzer with the given URL of the `robots.txt` file. - `can_fetch(self, useragent: str, url: str) -> bool`: Returns `True` if the user agent is allowed to fetch the URL, otherwise `False`. - `get_crawl_delay(self, useragent: str) -> int`: Returns the crawl delay for the user agent, or `None` if not specified. - `get_request_rate(self, useragent: str) -> tuple`: Returns a tuple `(requests, seconds)` indicating the request rate, or `None` if not specified. - `get_sitemaps(self) -> list`: Returns a list of sitemap URLs as contained in the `robots.txt` file. 3. **Constraints**: - The `robots.txt` file must be available at the provided URL. - Assume there are no network errors while fetching the `robots.txt`. 4. **Performance**: - The class should handle typical `robots.txt` files efficiently. - Ensure methods return results promptly after the initial parsing. Example Usage ```python # Instantiate the analyzer with the URL of the robots.txt file analyzer = RobotsAnalyzer(\\"http://www.example.com/robots.txt\\") # Check if a user agent can fetch a particular URL print(analyzer.can_fetch(\\"mybot\\", \\"http://www.example.com/somepage\\")) # Output: True or False # Get the crawl delay for a user agent print(analyzer.get_crawl_delay(\\"mybot\\")) # Output: Crawl delay in seconds or None # Get the request rate for a user agent print(analyzer.get_request_rate(\\"mybot\\")) # Output: (requests, seconds) or None # Get the list of sitemap URLs print(analyzer.get_sitemaps()) # Output: List of sitemap URLs or None ``` Notes - Ensure that your implementation adheres to the method signatures and behaviors specified above. - Use the functionalities provided by the `urllib.robotparser.RobotFileParser` class as described in the provided documentation.","solution":"import urllib.robotparser class RobotsAnalyzer: def __init__(self, url: str): self.robot_parser = urllib.robotparser.RobotFileParser() self.robot_parser.set_url(url) self.robot_parser.read() def can_fetch(self, useragent: str, url: str) -> bool: return self.robot_parser.can_fetch(useragent, url) def get_crawl_delay(self, useragent: str) -> int: return self.robot_parser.crawl_delay(useragent) def get_request_rate(self, useragent: str) -> tuple: return self.robot_parser.request_rate(useragent) def get_sitemaps(self) -> list: # The sitemaps are accessed directly from the parsed result if getattr(self.robot_parser, \'site_maps\', None): return self.robot_parser.site_maps() return []"},{"question":"In this task, you are required to write a Python function that demonstrates secure handling of a particular module that has known security considerations. Specifically, you are to implement a function using the `pickle` module securely. # Function: `secure_pickle_handling` Objective: Write a Python function called `secure_pickle_handling` that safely serializes and deserializes Python objects using the `pickle` module. The function should restrict global objects to prevent arbitrary code execution during the unpickling process. Input: - `obj`: The object to be serialized and deserialized. (Type: `Any`) - `operation`: A string indicating the operation to perform. It could either be: * `\'serialize\'`: to serialize the given object (`obj`). * `\'deserialize\'`: to deserialize the previously serialized data stored in a file. Output: - If `operation` is `\'serialize\'`, the function should return a bytes object representing the serialized data. - If `operation` is `\'deserialize\'`, the function should deserialize the data from a file named `serialized.obj` and return the original object. Constraints: 1. The function must handle exceptions properly. 2. The function must not use unrestrictive global variables when deserializing. 3. The serialized data should be stored in a file named `serialized.obj`. Example Usage: ```python # Example object to be serialized data = {\'key\': \'value\', \'number\': 42} # Serialize the object serialized_data = secure_pickle_handling(data, \'serialize\') # Save the serialized data to a file with open(\'serialized.obj\', \'wb\') as f: f.write(serialized_data) # Deserialize the object original_data = secure_pickle_handling(None, \'deserialize\') # original_data should now be equal to data assert original_data == data ``` Performance Requirement: - The function should efficiently serialize and deserialize objects without unnecessary memory usage. # Hint: Use `pickle` with restricted globals to avoid security vulnerabilities.","solution":"import pickle def secure_pickle_handling(obj, operation): Securely serialize or deserialize Python objects using the pickle module. Parameters: obj: The object to be serialized if the operation is \'serialize\'. operation: A string, either \'serialize\' or \'deserialize\'. Returns: Serialized bytes object if the operation is \'serialize\'. Deserialized object if the operation is \'deserialize\'. if operation == \'serialize\': try: serialized_data = pickle.dumps(obj) with open(\'serialized.obj\', \'wb\') as file: file.write(serialized_data) return serialized_data except (pickle.PicklingError, IOError) as e: raise ValueError(f\\"Serialization failed: {e}\\") elif operation == \'deserialize\': try: with open(\'serialized.obj\', \'rb\') as file: serialized_data = file.read() deserialized_data = pickle.loads(serialized_data, fix_imports=False, encoding=\'ASCII\', errors=\'strict\') return deserialized_data except (pickle.UnpicklingError, IOError) as e: raise ValueError(f\\"Deserialization failed: {e}\\") else: raise ValueError(\\"Invalid operation. Use \'serialize\' or \'deserialize\'.\\")"},{"question":"**Reference Management in Python** In Python, managing reference counts of objects is crucial for ensuring proper memory management and garbage collection. This task will test your understanding of how Python handles reference counts and require you to implement a small part of this mechanism yourself. # Task: You need to implement a custom class `RefManager` that simulates reference counting of Python objects. Your class should manage a dictionary where keys are Python objects and values are their corresponding reference counts. Implement the following methods: 1. `def add_reference(self, obj):` - Adds a strong reference to the object `obj`. If the object is already being tracked, its reference count is incremented. If it is not being tracked, it starts being tracked with a reference count of 1. - **Constraints**: `obj` should not be `None`. 2. `def remove_reference(self, obj):` - Releases a strong reference to the object `obj`. If releasing the reference drops the reference count to 0, the object is removed from tracking and considered deallocated (you can simply print a message for simulation). - **Constraints**: `obj` should not be `None`. 3. `def get_reference_count(self, obj) -> int:` - Returns the current reference count of the object `obj`. If the object is not being tracked, returns 0. - **Constraints**: `obj` should not be `None`. 4. `def clear_reference(self, obj):` - Releases a strong reference to the object `obj` and sets its reference count to 0, making the object eligible for deallocation immediately. - **Constraints**: `obj` should not be `None`. **Example Usage:** ```python manager = RefManager() obj1 = \\"example\\" obj2 = [1, 2, 3] manager.add_reference(obj1) # Reference count of obj1 should be 1 print(manager.get_reference_count(obj1)) # Output: 1 manager.add_reference(obj1) # Reference count of obj1 should be 2 print(manager.get_reference_count(obj1)) # Output: 2 manager.remove_reference(obj1) # Reference count of obj1 should be 1 print(manager.get_reference_count(obj1)) # Output: 1 manager.clear_reference(obj1) # obj1 should be deallocated print(manager.get_reference_count(obj1)) # Output: 0 ``` **Performance Requirements:** - Your solution should efficiently handle the addition and removal of references, even if there are a large number of objects. # Constraints: - The objects being managed can be of any Python type (str, list, tuple, etc.). - You may assume that the objects provided are hashable. - Ensure proper handling of cases where objects might not be present in the manager. Good luck!","solution":"class RefManager: def __init__(self): self.ref_counts = {} def add_reference(self, obj): if not obj: raise ValueError(\\"Object cannot be None\\") if obj in self.ref_counts: self.ref_counts[obj] += 1 else: self.ref_counts[obj] = 1 def remove_reference(self, obj): if not obj: raise ValueError(\\"Object cannot be None\\") if obj in self.ref_counts: self.ref_counts[obj] -= 1 if self.ref_counts[obj] == 0: print(f\\"{obj} deallocated\\") del self.ref_counts[obj] else: raise ValueError(\\"Object not found in reference manager\\") def get_reference_count(self, obj) -> int: if not obj: raise ValueError(\\"Object cannot be None\\") return self.ref_counts.get(obj, 0) def clear_reference(self, obj): if not obj: raise ValueError(\\"Object cannot be None\\") if obj in self.ref_counts: print(f\\"{obj} deallocated\\") del self.ref_counts[obj] else: raise ValueError(\\"Object not found in reference manager\\")"},{"question":"Context You are developing a secure file verification system using Python. This system should be able to: 1. Calculate a hash of a given file to ensure its integrity. 2. Use salting to protect against collision attacks. 3. Allow verification of the files hash. Task Implement a Python function `generate_secure_hash(file_path: str, salt: bytes, algorithm: str) -> str` that: 1. Takes the path of a file, a salt, and the name of the hashing algorithm as inputs. 2. Reads the file in chunks to avoid memory issues with large files. 3. Calculates the hash of the file using the chosen algorithm and the given salt. 4. Returns the hash in hexadecimal format. Next, implement another function `verify_file_hash(file_path: str, salt: bytes, expected_hash: str, algorithm: str) -> bool` that: 1. Takes the path of a file, the same salt used during hash generation, the expected hash in hexadecimal format, and the name of the hashing algorithm. 2. Reads the file and calculates its hash using the same method as `generate_secure_hash`. 3. Compares the calculated hash with the expected hash and returns `True` if they match, otherwise `False`. Constraints - The file might be several gigabytes in size, so reading it entirely into memory is not feasible. - The salt should be 16 bytes for `blake2b` and 8 bytes for `blake2s`. - The supported algorithms are those guaranteed by `hashlib` (`sha1`, `sha224`, `sha256`, `sha384`, `sha512`, `blake2b`, `blake2s`). Example Usage ```python salt = os.urandom(16) hash_value = generate_secure_hash(\\"example.txt\\", salt, \\"sha256\\") is_valid = verify_file_hash(\\"example.txt\\", salt, hash_value, \\"sha256\\") print(f\\"Generated Hash: {hash_value}\\") print(f\\"Is the file valid? {\'Yes\' if is_valid else \'No\'}\\") ``` Expected Functions Signatures ```python def generate_secure_hash(file_path: str, salt: bytes, algorithm: str) -> str: pass def verify_file_hash(file_path: str, salt: bytes, expected_hash: str, algorithm: str) -> bool: pass ``` Additional Information - Feel free to use helper functions if necessary. - Consider edge cases and handle exceptions appropriately. - Ensure your solution is well-documented and adhere to best coding practices.","solution":"import hashlib def generate_secure_hash(file_path: str, salt: bytes, algorithm: str) -> str: Generate a secure hash of a file with the specified algorithm and salt. Parameters: - file_path (str): Path to the file. - salt (bytes): Salt value to be used. - algorithm (str): Hashing algorithm to be used (e.g., \'sha256\', \'blake2b\'). Returns: - str: The hexadecimal hash string. hash_obj = hashlib.new(algorithm) hash_obj.update(salt) with open(file_path, \'rb\') as f: while chunk := f.read(8192): hash_obj.update(chunk) return hash_obj.hexdigest() def verify_file_hash(file_path: str, salt: bytes, expected_hash: str, algorithm: str) -> bool: Verify a file\'s hash. Parameters: - file_path (str): Path to the file. - salt (bytes): Salt used in the hash. - expected_hash (str): Expected hash value in hexadecimal. - algorithm (str): Hashing algorithm to be used. Returns: - bool: True if the hashes match, False otherwise. calculated_hash = generate_secure_hash(file_path, salt, algorithm) return calculated_hash == expected_hash"},{"question":"Calendar Function Implementation # Objective You are required to implement a function that uses the `calendar` module to generate detailed weekly schedules for a given month. The function should format a weekly schedule in plain text and another in HTML. # Function Signature ```python def generate_weekly_schedules(year: int, month: int, events: dict) -> tuple: Generates detailed weekly schedules for a given month. Args: - year (int): The year for which the monthly calendar is to be generated. - month (int): The month for which the monthly calendar is to be generated. - events (dict): A dictionary where keys are dates (yyyy-mm-dd) and values are lists of event strings. Returns: - tuple: A tuple containing two elements: 1. plain text schedule (str) 2. HTML schedule (str) pass ``` # Details You are provided with the following arguments: - `year` and `month` to specify which month\'s calendar needs to be generated. - `events`, a dictionary where keys are date strings in the format \'yyyy-mm-dd\' and the values are lists of events scheduled for that date. Your goal is to: 1. Generate the weekly schedules for the specified month in both plain text and HTML format. 2. Using the `calendar` module, ensure that the schedules include: - Each week beginning on a specific day (Monday). - The dates and associated events for each day. # Example ```python events = { \\"2023-11-01\\": [\\"Event 1\\", \\"Event 2\\"], \\"2023-11-03\\": [\\"Event 3\\"], \\"2023-11-07\\": [\\"Event 4\\", \\"Event 5\\", \\"Event 6\\"] } plain_text_schedule, html_schedule = generate_weekly_schedules(2023, 11, events) print(plain_text_schedule) print(html_schedule) ``` # Output Format Your function should return a tuple containing the plain text and HTML representations of the schedules. Each format should clearly show the events scheduled for each day within the specified month. # Constraints - The `events` dictionary keys will always have valid dates within the specified month. - Each list in the `events` dictionary can contain zero or more events, and each event is a string. # Notes - Make sure to handle cases when there are no events for some days. - The plain text and HTML schedules should be well formatted and readable. - Use appropriate classes and methods from the `calendar` module. # Performance Requirements - Ensure that your implementation is efficient and can handle edge cases like empty events dictionary or no events for some days.","solution":"import calendar from datetime import datetime def generate_weekly_schedules(year: int, month: int, events: dict) -> tuple: Generates detailed weekly schedules for a given month. Args: - year (int): The year for which the monthly calendar is to be generated. - month (int): The month for which the monthly calendar is to be generated. - events (dict): A dictionary where keys are dates (yyyy-mm-dd) and values are lists of event strings. Returns: - tuple: A tuple containing two elements: 1. plain text schedule (str) 2. HTML schedule (str) weekly_schedules_plain_text = \\"\\" weekly_schedules_html = \\"<html><body>\\" cal = calendar.TextCalendar(calendar.MONDAY) month_cal = cal.monthdayscalendar(year, month) for week in month_cal: weekly_schedules_plain_text += \\"Week starting on:n\\" weekly_schedules_html += \\"<h2>Week starting on:</h2><ul>\\" for day in week: if day != 0: date_str = f\\"{year}-{month:02d}-{day:02d}\\" event_list = events.get(date_str, []) weekly_schedules_plain_text += f\\" {date_str}n\\" weekly_schedules_html += f\\"<li>{date_str}</li><ul>\\" for event in event_list: weekly_schedules_plain_text += f\\" - {event}n\\" weekly_schedules_html += f\\"<li>{event}</li>\\" weekly_schedules_html += \\"</ul>\\" weekly_schedules_html += \\"</ul>\\" weekly_schedules_html += \\"</body></html>\\" return weekly_schedules_plain_text, weekly_schedules_html"},{"question":"# PyTorch Serialization Assessment **Objective:** Demonstrate your understanding of saving and loading tensors and models in PyTorch, including advanced concepts like preserving tensor views and handling model state dicts. Task 1. **Tensor Serialization and View Preservation:** - Create a tensor using `torch.arange()` from 1 to 20. - Create a view of this tensor such that you get every 3rd element starting from the second element. - Save both the original tensor and the view in a file named `tensor_views.pt`. - Load the tensors from the file. - Modify the view tensor by multiplying each element by 5. - Verify that the changes are reflected in the original tensor by printing both tensors. 2. **Efficient Tensor Cloning and Saving:** - Create a large tensor with 1000 elements using `torch.arange()`. - Create a view of the first 10 elements of this tensor. - Save the view tensor in a file named `view_tensor.pt`, ensuring the saved file size is minimized. - Load the tensor from the file and verify its storage size is 10, not 1000. 3. **Model State Dict Serialization:** - Define a custom neural network module with two linear layers and ReLU activation between them. - Instantiate the model and print its state dict. - Save the models state dict to a file named `model_state.pt`. - Create a new instance of the model and load the state dict from the file. - Verify that the state dicts of the original and the new model instances are identical. Constraints - Use only the PyTorch framework for saving and loading operations. - Ensure that the view modifications preserve the relationship with the original tensor storage. Expected Input and Output 1. Tensor Serialization and View Preservation: - Input: None (create tensors within the code). - Output: Printed original tensor and modified view tensor. 2. Efficient Tensor Cloning and Saving: - Input: None (create and save tensors within the code). - Output: Printed storage size to verify efficiency. 3. Model State Dict Serialization: - Input: None (define and save model state within the code). - Output: Printed state dicts to verify matching. Code Implementation ```python import torch # Task 1: Tensor Serialization and View Preservation tensor_original = torch.arange(1, 21) tensor_view = tensor_original[1::3] # Save original tensor and view torch.save([tensor_original, tensor_view], \'tensor_views.pt\') # Load tensors loaded_tensor_original, loaded_tensor_view = torch.load(\'tensor_views.pt\') # Modify the view tensor loaded_tensor_view *= 5 print(\\"Original Tensor:\\", loaded_tensor_original) print(\\"Modified View Tensor:\\", loaded_tensor_view) # Task 2: Efficient Tensor Cloning and Saving large_tensor = torch.arange(1, 1001) tensor_view_small = large_tensor[:10] # Save the cloned view torch.save(tensor_view_small.clone(), \'view_tensor.pt\') # Load the tensor and check storage size loaded_view_small = torch.load(\'view_tensor.pt\') print(\\"Storage Size:\\", loaded_view_small.storage().size()) # Task 3: Model State Dict Serialization class CustomModel(torch.nn.Module): def __init__(self): super(CustomModel, self).__init__() self.linear1 = torch.nn.Linear(4, 2) self.linear2 = torch.nn.Linear(2, 1) def forward(self, x): x = torch.nn.functional.relu(self.linear1(x)) x = self.linear2(x) return x # Instantiate and print state dict model = CustomModel() print(\\"Original Model State Dict:\\", model.state_dict()) # Save state dict torch.save(model.state_dict(), \'model_state.pt\') # Load state dict into a new model instance new_model = CustomModel() new_model.load_state_dict(torch.load(\'model_state.pt\')) print(\\"New Model State Dict:\\", new_model.state_dict()) ``` Performance Requirements - Ensure operations are efficient and avoid unnecessary large file sizes, specifically in Task 2.","solution":"import torch # Task 1: Tensor Serialization and View Preservation def tensor_serialization_and_view_preservation(): tensor_original = torch.arange(1, 21) tensor_view = tensor_original[1::3] # Save original tensor and view torch.save([tensor_original, tensor_view], \'tensor_views.pt\') # Load tensors loaded_tensor_original, loaded_tensor_view = torch.load(\'tensor_views.pt\') # Modify the view tensor loaded_tensor_view *= 5 return loaded_tensor_original, loaded_tensor_view # Task 2: Efficient Tensor Cloning and Saving def efficient_tensor_cloning_and_saving(): large_tensor = torch.arange(1, 1001) tensor_view_small = large_tensor[:10] # Save the cloned view torch.save(tensor_view_small.clone(), \'view_tensor.pt\') # Load the tensor and check storage size loaded_view_small = torch.load(\'view_tensor.pt\') return loaded_view_small.storage().size() # Task 3: Model State Dict Serialization class CustomModel(torch.nn.Module): def __init__(self): super(CustomModel, self).__init__() self.linear1 = torch.nn.Linear(4, 2) self.linear2 = torch.nn.Linear(2, 1) def forward(self, x): x = torch.nn.functional.relu(self.linear1(x)) x = self.linear2(x) return x def model_state_dict_serialization(): # Instantiate and print state dict model = CustomModel() original_state_dict = model.state_dict() # Save state dict torch.save(model.state_dict(), \'model_state.pt\') # Load state dict into a new model instance new_model = CustomModel() new_model.load_state_dict(torch.load(\'model_state.pt\')) new_state_dict = new_model.state_dict() return original_state_dict, new_state_dict"},{"question":"**Title: Implement File Locking with `fcntl`** # Description You are required to implement a function that safely performs read and write operations on a file by ensuring proper locking mechanisms using the `fcntl` module. # Requirements 1. Implement the function `safe_file_operation` that takes the following parameters: - `file_path` (str): The path to the file on which the operations are performed. - `operation` (str): The type of operation to perform. It can be either `\'read\'` or `\'write\'`. - `data` (str, optional): The data to write to the file. Required if the operation is `\'write\'`. 2. The function should: - Open the file and acquire an appropriate lock depending on the operation. - For reading, acquire a shared lock (`LOCK_SH`). - For writing, acquire an exclusive lock (`LOCK_EX`). Ensure the lock is non-blocking. - If the lock cannot be acquired, raise an `OSError` with an appropriate error message. - Perform the desired operation on the file within the lock. - Use exception handling to ensure that the file descriptor is properly released if an error occurs. - Ensure that the file is always properly closed, regardless of whether exceptions occur. 3. Return the contents of the file if the operation is `\'read\'`. Return `None` for `\'write\'` operations. # Constraints - You may assume the file at `file_path` exists. - Write operations should overwrite the existing content of the file. - Be cautious of race conditions that could arise from multiple processes trying to access the same file. # Example Usage ```python import os # Example file path file_path = \\"test_file.txt\\" # Writing to the file safe_file_operation(file_path, \\"write\\", data=\\"Hello World!\\") # Reading from the file content = safe_file_operation(file_path, \\"read\\") print(content) # Output: \\"Hello World!\\" ``` # Notes - Do not use any high-level file locking libraries, focus solely on using the `fcntl` module for locking. - Handle potential errors gracefully and ensure that appropriate error messages are provided.","solution":"import fcntl import os def safe_file_operation(file_path, operation, data=None): Perform a safe file operation with proper locking using `fcntl`. Args: file_path (str): The path to the file on which the operations are performed. operation (str): The type of operation to perform. It can be either \'read\' or \'write\'. data (str, optional): The data to write to the file. Required if the operation is \'write\'. Returns: str or None: The contents of the file if the operation is \'read\'. None for \'write\' operations. if operation not in [\'read\', \'write\']: raise ValueError(\\"Operation must be \'read\' or \'write\'\\") try: if operation == \'read\': with open(file_path, \'r\') as file: fcntl.flock(file.fileno(), fcntl.LOCK_SH) try: return file.read() finally: fcntl.flock(file.fileno(), fcntl.LOCK_UN) elif operation == \'write\': if data is None: raise ValueError(\\"Data must be provided for write operation\\") with open(file_path, \'w\') as file: fcntl.flock(file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB) try: file.write(data) finally: fcntl.flock(file.fileno(), fcntl.LOCK_UN) return None except BlockingIOError: raise OSError(\\"Failed to acquire the lock, file is being used by another process\\") except Exception as e: raise e"},{"question":"**Problem Statement: Directory Content Search Using `glob` Module** You are tasked with implementing a function `search_directory_patterns()` that searches for files and directories in a specified directory matching multiple patterns with varying levels of complexity. This function will demonstrate your understanding of pattern matching, handling special characters, and recursive searching using Python\'s `glob` module. # Function Signature ```python def search_directory_patterns(base_dir: str, patterns: list, recursive: bool = False) -> dict: ``` # Input - `base_dir` (str): The base directory from which the search will start. It must be an absolute or relative path. - `patterns` (list): A list of patterns (strings) to search for. These patterns can include wildcards such as `*`, `?`, and character ranges `[]`. - `recursive` (bool): If set to `True`, the search will include all subdirectories of `base_dir` recursively. Default is `False`. # Output - Returns a dictionary where each key is a pattern from the input list, and the value is a list of file paths (in arbitrary order) that match that pattern. # Constraints - You must use Python\'s `glob` module for the implementation. - Ensure that the function handles escaping special characters in the patterns correctly. - The search should respect dot files handling as specified by the `glob` module. - Avoid using any external libraries other than `os` and `glob`. # Example Consider a directory structure: ``` /test_dir |-- 1.gif |-- 2.txt |-- card.gif |-- sub |-- 3.txt |-- sub2 |-- 4.gif |-- .hiddenfile ``` 1. `search_directory_patterns(\'test_dir\', [\'*.gif\', \'?.txt\'])` should return: ```python { \'*.gif\': [\'1.gif\', \'card.gif\'], \'?.txt\': [\'2.txt\'] } ``` 2. `search_directory_patterns(\'test_dir\', [\'**/*.txt\'], recursive=True)` should return: ```python { \'**/*.txt\': [\'2.txt\', \'sub/3.txt\'] } ``` 3. `search_directory_patterns(\'test_dir\', [\'.h*\'], recursive=True)` should return: ```python { \'.h*\': [\'sub/sub2/.hiddenfile\'] } ``` # Implementation Hints - Use `glob.glob()` or `glob.iglob()` depending on your approach. - Consider using `glob.escape()` to handle special characters in the patterns. - Ensure proper handling of the recursive flag and the `base_dir` usage. Design and implement the `search_directory_patterns()` function to pass the above example cases and any additional tests you create.","solution":"import glob import os def search_directory_patterns(base_dir: str, patterns: list, recursive: bool = False) -> dict: Searches for files and directories in the specified base directory that match the provided patterns. Args: base_dir (str): The base directory to start the search. patterns (list): A list of patterns to search for. recursive (bool): If True, search will include all subdirectories of base_dir recursively. Default is False. Returns: dict: A dictionary where each key is a pattern, and the value is a list of file paths that match that pattern. result = {} for pattern in patterns: if recursive: search_pattern = os.path.join(base_dir, \'**\', pattern) else: search_pattern = os.path.join(base_dir, pattern) matched_files = glob.glob(search_pattern, recursive=recursive) # Store the relative paths w.r.t base_dir matched_files = [os.path.relpath(path, base_dir) for path in matched_files] result[pattern] = matched_files return result"},{"question":"**Objective:** Design a classification pipeline using the scikit-learn library to predict the species of Iris flowers based on the given features. **Problem Statement:** You are given the Iris dataset, a famous dataset used for evaluating the performance of various classification algorithms. The goal is to implement a classification pipeline that can load the dataset, preprocess it, train a classifier, and evaluate its performance. The Iris dataset contains: - 150 instances - 4 numerical features: sepal length, sepal width, petal length, petal width - 3 classes: Setosa, Versicolour, and Virginica **Task:** 1. Load the Iris dataset using `sklearn.datasets.load_iris`. 2. Split the dataset into a training set and a test set (80% training, 20% testing). 3. Preprocess the data by scaling the features using `StandardScaler`. 4. Train a `RandomForestClassifier` on the training data. 5. Evaluate the classifier on the test data using accuracy as the metric. 6. Print the accuracy score of the model. **Input:** No specific input is required from the user. The dataset is loaded within the code. **Output:** Print the accuracy score of the model. **Constraints:** - Use a random state of 42 for splitting the dataset to ensure reproducibility. - Use default parameters for `StandardScaler` and `RandomForestClassifier`. **Example:** Your final printed output might look something like: ``` Accuracy: 0.95 ``` **Instructions:** Write a Python function `iris_classification_pipeline()` that implements the above steps. ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def iris_classification_pipeline(): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train classifier clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate classifier y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Print the accuracy print(f\\"Accuracy: {accuracy:.2f}\\") # Run function to see output iris_classification_pipeline() ```","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def iris_classification_pipeline(): # Load dataset iris = load_iris() X, y = iris.data, iris.target # Split dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Preprocess data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train classifier clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate classifier y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Print the accuracy print(f\\"Accuracy: {accuracy:.2f}\\") return accuracy"},{"question":"Working with Property Lists Objective: Write a Python function to read an XML plist file, modify its contents, and save it back in a specified format (XML or binary). The function should accept a file path, read the plist data, perform certain modifications, and write the updated data back to a new file. Requirements: 1. The function should be named `modify_plist`. 2. The function should take four arguments: - `input_path` (str): The path to the input plist file (XML format). - `output_path` (str): The path to the output plist file. - `modifications` (dict): A dictionary containing key-value pairs to be added or updated in the plist. - `output_format` (str): The format of the output plist file (`FMT_XML` or `FMT_BINARY`). 3. The function should perform the following steps: - Read the plist data from `input_path` using the `plistlib.load()` function. - Update the plist data with the `modifications` dictionary. - Write the updated plist data to `output_path` in the specified `output_format` using the `plistlib.dump()` function. Constraints: - The input plist file will always be in XML format. - The `modifications` dictionary will only contain keys that are strings and values that are of permissible types (strings, integers, floats, booleans, tuples, lists, dictionaries, bytes, bytearray, `datetime.datetime`). - Ensure that dictionary keys are sorted in the output plist when `output_format` is `FMT_XML`. Function Signature: ```python def modify_plist(input_path: str, output_path: str, modifications: dict, output_format: str) -> None: pass ``` Example Usage: ```python input_path = \'input.plist\' output_path = \'output.plist\' modifications = { \'newKey\': \'newValue\', \'existingKey\': \'updatedValue\' } output_format = plistlib.FMT_BINARY modify_plist(input_path, output_path, modifications, output_format) ``` In this example, the function will read the plist from `input_path`, update the plist data with the keys \'newKey\' and \'existingKey\'. It will then save the modified plist to `output_path` in binary format.","solution":"import plistlib def modify_plist(input_path: str, output_path: str, modifications: dict, output_format: str) -> None: Reads an XML plist file, modifies its contents, and saves it back in a specified format (XML or binary). :param input_path: The path to the input plist file. :param output_path: The path to the output plist file. :param modifications: A dictionary containing key-value pairs to be added or updated in the plist. :param output_format: The format of the output plist file (\'FMT_XML\' or \'FMT_BINARY\'). # Read the plist data from the input path with open(input_path, \'rb\') as f: plist_data = plistlib.load(f) # Update the plist data with the modifications plist_data.update(modifications) # Sort dictionary keys if output format is XML if output_format == plistlib.FMT_XML: plist_data = {k: plist_data[k] for k in sorted(plist_data)} # Write the updated plist data to the output path with open(output_path, \'wb\') as f: plistlib.dump(plist_data, f, fmt=output_format)"},{"question":"Wrapping C System Functions in Python Objective: You are required to create Python bindings for specific C functions available in the Python/C API related to standard output and error handling, as well as process control. You will achieve this using the `ctypes` library in Python to interface with these lower-level C functions. Task: 1. Implement Python functions that wrap the following C functions using the `ctypes` library: - `PySys_WriteStdout`: Writes formatted output to the standard output. - `PySys_WriteStderr`: Writes formatted output to the standard error. 2. Implement a Python function that wraps the following C function: - `Py_Exit`: Exits the current process with the given status code. Requirements: 1. **`write_stdout` Function**: - **Input**: A string `message` which is the message to write to standard output. - **Output**: None (writes directly to standard output). - **Function Signature**: `def write_stdout(message: str) -> None:` 2. **`write_stderr` Function**: - **Input**: A string `message` which is the message to write to standard error. - **Output**: None (writes directly to standard error). - **Function Signature**: `def write_stderr(message: str) -> None:` 3. **`exit_process` Function**: - **Input**: An integer `status` which represents the exit status code. - **Output**: None (exits the process with the given status code). - **Function Signature**: `def exit_process(status: int) -> None:` Constraints: - Ensure that any resources allocated are properly managed and do not cause memory leaks. - Handle any possible errors that might occur during the interfacing. Implementation: - Utilize the `ctypes` library to load the Python shared library and access the required functions. - Write appropriate error handling for situations where the function pointers cannot be loaded or called. Sample Code Structure: ```python import ctypes import sys # Load the Python shared library python_lib = ctypes.PyDLL(None) # Implement write_stdout function def write_stdout(message: str) -> None: write_stdout_c = python_lib.PySys_WriteStdout write_stdout_c.restype = None write_stdout_c.argtypes = [ctypes.c_char_p] write_stdout_c(message.encode(\'utf-8\')) # Implement write_stderr function def write_stderr(message: str) -> None: write_stderr_c = python_lib.PySys_WriteStderr write_stderr_c.restype = None write_stderr_c.argtypes = [ctypes.c_char_p] write_stderr_c(message.encode(\'utf-8\')) # Implement exit_process function def exit_process(status: int) -> None: exit_c = python_lib.Py_Exit exit_c.restype = None exit_c.argtypes = [ctypes.c_int] exit_c(status) # Example usages write_stdout(\\"Hello, this is a test of stdout.n\\") write_stderr(\\"Hello, this is a test of stderr.n\\") exit_process(0) ``` This coding assessment evaluates the student\'s ability to: - Interface with C functions from Python using `ctypes`. - Understand the use of function pointers and calling conventions. - Handle error cases and manage resources effectively.","solution":"import ctypes # Load the Python shared library python_lib = ctypes.PyDLL(None) def write_stdout(message: str) -> None: Writes formatted output to the standard output. Parameters: message (str): The message to write to the standard output. write_stdout_c = python_lib.PySys_WriteStdout write_stdout_c.restype = None write_stdout_c.argtypes = [ctypes.c_char_p] write_stdout_c(message.encode(\'utf-8\')) def write_stderr(message: str) -> None: Writes formatted output to the standard error. Parameters: message (str): The message to write to the standard error. write_stderr_c = python_lib.PySys_WriteStderr write_stderr_c.restype = None write_stderr_c.argtypes = [ctypes.c_char_p] write_stderr_c(message.encode(\'utf-8\')) def exit_process(status: int) -> None: Exits the current process with the given status code. Parameters: status (int): The exit status code. exit_c = python_lib.Py_Exit exit_c.restype = None exit_c.argtypes = [ctypes.c_int] exit_c(status) # Test if the functions are working correctly if __name__ == \'__main__\': write_stdout(\\"Hello, this is a test of stdout.n\\") write_stderr(\\"Hello, this is a test of stderr.n\\") # exit_process(0) # Uncomment to test exiting the process"},{"question":"Description Your task is to implement a PyTorch module that uses `torch.cond` to apply a dynamic transformation to an input tensor. You are required to define two different transformation functions and a condition that decides which transformation to apply based on the shape and data of the tensor. Requirements 1. Define a class `CustomTransform` that extends `torch.nn.Module`. 2. Implement a `forward` method in the class that takes an input tensor `x` and applies one of two transformation functions based on the following conditions: - **Condition 1**: If the first dimension of the tensor `x` is greater than 5, the method should apply `transform_A`. - **Condition 2**: If the sum of the tensor `x` is greater than 10, the method should apply `transform_B`. 3. Both conditions should be checked using `torch.cond`. 4. Define the transformation functions `transform_A` and `transform_B` as follows: - `transform_A`: Multiplies the tensor by 2. - `transform_B`: Adds 3 to the tensor. Implementation ```python import torch def transform_A(x: torch.Tensor) -> torch.Tensor: return x * 2 def transform_B(x: torch.Tensor) -> torch.Tensor: return x + 3 class CustomTransform(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def condition1_fn(x: torch.Tensor): return transform_A(x) def condition2_fn(x: torch.Tensor): return transform_B(x) intermediate = torch.cond( x.shape[0] > 5, condition1_fn, lambda x: x, (x,) ) result = torch.cond( intermediate.sum() > 10, condition2_fn, lambda x: x, (intermediate,) ) return result # Example usage custom_transform = CustomTransform() # Test cases input_tensor1 = torch.randn(6, 2) # Should apply transform_A output1 = custom_transform(input_tensor1) print(output1) input_tensor2 = torch.randn(4) # Should remain unchanged if sum <= 10 output2 = custom_transform(input_tensor2) print(output2) input_tensor3 = torch.full((4,), 3) # Should apply transform_B output3 = custom_transform(input_tensor3) print(output3) ``` Inputs - x (torch.Tensor): The input tensor. Outputs - A tensor after applying the appropriate transformation based on the conditions. Evaluation Criteria - Correctly implementing the transformations using `torch.cond`. - Proper condition checking and applying the respective transformations. - Clean and readable code adhering to standard Python coding conventions.","solution":"import torch import torch.nn as nn def transform_A(x: torch.Tensor) -> torch.Tensor: return x * 2 def transform_B(x: torch.Tensor) -> torch.Tensor: return x + 3 class CustomTransform(nn.Module): def __init__(self): super(CustomTransform, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: if x.shape[0] > 5: return transform_A(x) elif x.sum() > 10: return transform_B(x) else: return x"},{"question":"**Question: Functional Programming in Python** You are given a list of tuples representing the sales data of different products over several months. Each tuple contains the product name, a month (as an integer from 1 to 12), and the sales figure for that month. Your task is to write a function that processes this data to produce a summary of total sales by product and sort the results in descending order of sales. **Function Signature**: ```python def summarize_sales(sales_data: List[Tuple[str, int, float]]) -> List[Tuple[str, float]]: ``` **Input**: - `sales_data`: A list of tuples, where each tuple contains: * `product_name` (str): Name of the product. * `month` (int): Month of the sale (1 through 12). * `sales` (float): Sales figure for that month. Example: ```python sales_data = [(\'A\', 1, 200.0), (\'B\', 1, 150.0), (\'A\', 2, 180.0), (\'B\', 2, 220.0), (\'A\', 1, 100.0)] ``` **Output**: - A list of tuples, where each tuple contains: * `product_name` (str): Name of the product. * `total_sales` (float): Total sales for the product, summed over all months. The output list should be sorted in descending order of total sales. Example: ```python [(\'A\', 480.0), (\'B\', 370.0)] ``` **Constraints**: - The `sales_data` list can have up to `10^6` entries. - Sales figures are positive numbers. - Product names are non-empty strings. **Performance Requirements**: - The solution should efficiently handle the large input size within a reasonable time. **Instructions**: 1. Use the `itertools` module to group the sales data by product. 2. Use the `functools` module, particularly `functools.reduce`, to sum up the sales figures for each product. 3. Use the `operator` module to assist in sorting the products based on total sales. **Example**: ```python from typing import List, Tuple def summarize_sales(sales_data: List[Tuple[str, int, float]]) -> List[Tuple[str, float]]: from itertools import groupby from functools import reduce from operator import itemgetter, add # Sort sales_data by product name sorted_data = sorted(sales_data, key=itemgetter(0)) # Group sales data by product name grouped_data = groupby(sorted_data, key=itemgetter(0)) # Calculate total sales for each product total_sales = [(product, reduce(lambda x, y: x + y, map(itemgetter(2), sales))) for product, sales in grouped_data] # Sort products by total sales in descending order sorted_total_sales = sorted(total_sales, key=itemgetter(1), reverse=True) return sorted_total_sales # Example usage sales_data = [(\'A\', 1, 200.0), (\'B\', 1, 150.0), (\'A\', 2, 180.0), (\'B\', 2, 220.0), (\'A\', 1, 100.0)] print(summarize_sales(sales_data)) # Output: [(\'A\', 480.0), (\'B\', 370.0)] ```","solution":"from typing import List, Tuple def summarize_sales(sales_data: List[Tuple[str, int, float]]) -> List[Tuple[str, float]]: from itertools import groupby from functools import reduce from operator import itemgetter # Sort sales_data by product name sorted_data = sorted(sales_data, key=itemgetter(0)) # Group sales data by product name grouped_data = groupby(sorted_data, key=itemgetter(0)) # Calculate total sales for each product total_sales = [(product, reduce(lambda x, y: x + y, map(itemgetter(2), sales))) for product, sales in grouped_data] # Sort products by total sales in descending order sorted_total_sales = sorted(total_sales, key=itemgetter(1), reverse=True) return sorted_total_sales"}]'),D={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],M={key:0},N={key:1};function L(n,e,l,h,i,o){const m=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"prompts chat")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")},"  ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(m,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",N,"Loading...")):(a(),s("span",M,"See more"))],8,q)):d("",!0)])}const O=p(D,[["render",L],["__scopeId","data-v-d991eb2c"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/53.md","filePath":"chatai/53.md"}'),U={name:"chatai/53.md"},B=Object.assign(U,{setup(n){return(e,l)=>(a(),s("div",null,[x(O)]))}});export{Y as __pageData,B as default};
